Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 740?747, Vancouver, October 2005. c?2005 Association for Computational Linguistics
BLANC1: Learning Evaluation Metrics for MT
Lucian Vlad Lita and Monica Rogati and Alon Lavie
Carnegie Mellon University
{llita,mrogati,alavie}@cs.cmu.edu
Abstract
We introduce BLANC, a family of dy-
namic, trainable evaluation metrics for ma-
chine translation. Flexible, parametrized
models can be learned from past data and
automatically optimized to correlate well
with human judgments for different cri-
teria (e.g. adequacy, fluency) using dif-
ferent correlation measures. Towards this
end, we discuss ACS (all common skip-
ngrams), a practical algorithm with train-
able parameters that estimates reference-
candidate translation overlap by comput-
ing a weighted sum of all common skip-
ngrams in polynomial time. We show that
the BLEU and ROUGE metric families are
special cases of BLANC, and we compare
correlations with human judgments across
these three metric families. We analyze the
algorithmic complexity of ACS and argue
that it is more powerful in modeling both
local meaning and sentence-level structure,
while offering the same practicality as the
established algorithms it generalizes.
1 Introduction
Although recent MT evaluation methods show
promising correlations to human judgments in terms
of adequacy and fluency, there is still considerable
room for improvement (Culy and Riehemann, 2003).
Most of these studies have been performed at a sys-
tem level and have not investigated metric robust-
ness at a lower granularity. Moreover, even though
the emphasis on adequacy vs. fluency is application-
dependent, automatic evaluation metrics do not dis-
tinguish between the need to optimize correlation
with regard to one or the other.
Machine translation automatic evaluation metrics
face two important challenges: the lack of powerful
features to capture both sentence level structure and
local meaning, and the difficulty of designing good
functions for combining these features into meaning-
ful quality estimation algorithms.
In this paper, we introduce BLANC1, an automatic
MT evaluation metric family that is a generaliza-
tion of popular and successful metric families cur-
rently used in the MT community (BLEU, ROUGE, F-
measure etc.). We describe an efficient, polynomial-
time algorithm for BLANC, and show how it can be
optimized to target adequacy, fluency or any other
criterion. We compare our metric?s performance
with traditional and recent automatic evaluation met-
rics. We also describe the parameter conditions under
which BLANC can emulate them.
Throughout the remainder of this paper, we dis-
tinguish between two components of automatic MT
evaluation: the statistics computed on candidate
and reference translations and the function used in
defining evaluation metrics and generating transla-
tion scores. Commonly used statistics include bag-
of-words overlap, edit distance, longest common sub-
sequence, ngram overlap, and skip-bigram overlap.
Preferred functions are various combinations of pre-
cision and recall (Soricut and Brill, 2004), including
1Since existing evaluation metrics (e.g. BLEU, ROUGE) are
special cases of our metric family, it is only natural to name it
Broad Learning and Adaptation for Numeric Criteria (BLANC) ?
white light contains light of all frequencies
740
weighted precision and F-measures (Van-Rijsbergen,
1979).
BLANC implements a practical algorithm with
learnable parameters for automatic MT evaluation
which estimates the reference-candidate translation
overlap by computing a weighted sum of common
subsequences (also known as skip-ngrams). Com-
mon skip-ngrams are sequences of words in their
sentence order that are found both in the reference
and candidate translations. By generalizing and sep-
arating the overlap statistics from the function used
to combine them, and by identifying the latter as a
learnable component, BLANC subsumes the ngram
based evaluation metrics as special cases and can
better reflect the need of end applications for ade-
quacy/fluency tradeoffs .
1.1 Related Work
Initial work in evaluating translation quality focused
on edit distance-based metrics (Su et al, 1992; Akiba
et al, 2001). In the MT context, edit distance (Lev-
enshtein, 1965) represents the amount of word inser-
tions, deletions and substitutions necessary to trans-
form a candidate translation into a reference trans-
lation. Another evaluation metric based on edit dis-
tance is the Word Error Rate (Niessen et al, 2000)
which computes the normalized edit distance. BLEU
is a weighted precision evaluation metric introduced
by IBM (Papineni et al, 2001). BLEU and its exten-
sions/variants (e.g. NIST (Doddington, 2002)) have
become de-facto standards in the MT community and
are consistently being used for system optimization
and tuning. These methods rely on local features
and do not explicitly capture sentence-level features,
although implicitly longer n-gram matches are re-
warded in BLEU. The General Text Matcher (GTM)
(Turian et al, 2003) is another MT evaluation method
that rewards longer ngrams instead of assigning them
equal weight.
(Lin and Och, 2004) recently proposed a set of
metrics (ROUGE) for MT evaluation. ROUGE-L is a
longest common subsequence (LCS) based automatic
evaluation metric for MT. The intuition behind it is
that long common subsequences reflect a large over-
lap between a candidate translation and a reference
translation. ROUGE-W is also based on LCS, but
assigns higher weights to sequences that have fewer
gaps. However, these metrics still do not distinguish
among translations with the same LCS but different
number of shorter sized subsequences, also indica-
tive of overlap. ROUGE-S attempts to correct this
problem by combining the precision/recall of skip-
bigrams of the reference and candidate translations.
However, by using skip-ngrams with n?=2, we might
be able to capture more information encoded in the
higher level sentence structure. With BLANC, we
propose a way to exploit local contiguity in a man-
ner similar to BLEU and also higher level structure
similar to ROUGE type metrics.
2 Approach
We have designed an algorithm that can perform a
full overlap search over variable-size, non-contiguous
word sequences (skip-ngrams) efficiently. At first
glance, in order to perform this search, one has to
first exhaustively generate all skip-ngrams in the can-
didate and reference segments and then assess the
overlap. This approach is highly prohibitive since the
number of possible sequences is exponential in the
number of words in the sentence. Our algorithm ?
ACS (all common skip-ngrams) ? directly constructs
the set of overlapping skip-ngrams through incremen-
tal composition of word-level matches. With ACS,
we can reduce computation complexity to a fifth de-
gree polynomial in the number of words.
Through the ACS algorithm, BLANC is not limited
only to counting skip-ngram overlap: the contribu-
tion of different skip-ngrams to the overall score is
based on a set of features. ACS computes the over-
lap between two segments of text and also allows
local and global features to be computed during the
overlap search. These local and global features are
subsequently used to train evaluation models within
the BLANC family. We introduce below several sim-
ple skip-ngram-based features and show that special-
case parameter settings for these features emulate the
computation of existing ngram-based metrics. In or-
der to define the relative significance of a particular
skip-ngram found by the ACS algorithm, we employ
an exponential model for feature integration.
2.1 Weighted Skip-Ngrams
We define skip-ngrams as sequences of n words taken
in sentence order allowing for arbitrary gaps. In algo-
rithms literature skip-ngrams are equivalent to subse-
quences. As special cases, skip-ngrams with n=2 are
741
referred to as skip-bigrams and skip-ngrams with no
gaps between the words are simply ngrams. A sen-
tence S of size |S| has C(|S|, n) = |S|!(|S|?n)!n! skip-
ngrams.
For example, the sentence ?To be or not to be? has
C(6, 2) = 15 corresponding skip-bigrams including
?be or?, ?to to?, and three occurrences of ?to be?.
It also has C(6, 4) = 15 corresponding skip-4grams
(n = 4) including ?to be to be? and ?to or not to?.
Consider the following sample reference and can-
didate translations:
R0: machine translated text is evaluated automatically
K1: machine translated stories are chosen automatically
K2: machine and human together can forge a friendship that
cannot be translated into words automatically
K3: machine code is being translated automatically
The skip-ngram ?machine translated automati-
cally? appears in both the reference R0 and all candi-
date translations. Arguably, a skip-bigram that con-
tains few gaps is likely to capture local structure
or meaning. At the same time, skip-ngrams spread
across a sentence are also very useful since they may
capture part of the high level sentence structure.
We define a weighting feature function for skip-
ngrams that estimates how likely they are to capture
local meaning and sentence structure. The weighting
function ? for a skip-ngram w1 ..wn is defined as:
?(w1..wn) = e???G(w1..wn) (1)
where ? ? 0 is a decay parameter and G(w1..wn)
measures the overall gap of the skip-ngram w1..wn in
a specific sentence. This overall skip-ngram weight
can be decomposed into the weights of its constituent
skip-bigrams:
?(w1..wn) = e???G(w1,..,wn) (2)
= e???
Pn?1
i=1 G(wi,wi+1)
=
n?1
?
i=1
?(wi wi+1) (3)
In equation 3, ?(wi wi+1) is the number of words
between wi and wi+1 in the sentence. In the example
above, the skip-ngram ?machine translated automat-
ically? has weight e?3? for sentence K1 and weight
e?12? = 1 for sentence K2.
In our initial experiments the gap G has been ex-
pressed as a linear function, but different families of
functions can be explored and their corresponding pa-
rameters learned. The parameter ? dictates the be-
havior of the weighting function. When ? = 0 ?
equals e0 = 1, rendering gap sizes irrelevant. In this
case, skip-ngrams are given the same weight as con-
tiguous ngrams. When ? is very large, ? approaches
0 if there are any gaps in the skip-ngram and is 1 if
there are no gaps. This setting has the effect of con-
sidering only contiguous ngrams and discarding all
skip-ngrams with gaps.
In the above example, although the skip-ngram
?machine translated automatically? has the same cu-
mulative gap in both in K1 and K3, the occurrence in
K1 has is a gap distribution that more closely reflects
that of the reference skip-ngram in R0. To model gap
distribution differences between two occurrences of a
skip-ngram, we define a piece-wise distance function
?XY between two sentences x and y. For two succes-
sive words in the skip-ngram, the distance function is
defined as:
?XY (w1w2) = e???|GX(w1,w2)?GY (w1,w2)| (4)
where ? ? 0 is a decay parameter. Intuitively, the
? parameter is used to reward better aligned skip-
ngrams. Similar to the ? function, the overall ?XY
distance between two occurrences of a skip-ngram
with n > 1 is:
?XY (w1..wn) =
n?1
?
i=1
?XY (wiwi+1) (5)
Note that equation 5 takes into account pairs of skip-
ngrams skip in different places by summing over
piecewise differences. Finally, using an exponen-
tial model, we assign an overall score to the matched
skip-ngram. The skip-ngram scoring function Sxy al-
lows independent features to be incorporated into the
overall score:
Sxy(wi..wk) = ?(wi..wk) ? ?xy(wi..wk)
?e?1f1(wi..wk) ? ... ? e?hfh(wi..wk) (6)
where features f1..fh can be functions based on the
syntax, semantics, lexical or morphological aspects
of the skip-ngram. Note that different models for
combining skip-ngram features can be used in con-
junction with ACS.
742
2.2 Multiple References
In BLANC we incorporate multiple references in a
manner similar to the ROUGE metric family. We
compute the precision and recall of each size skip-
ngrams for individual references. Based on these we
combine the maximum precision and maximum re-
call of the candidate translation obtained using all
reference translations and use them to compute an ag-
gregate F-measure.
The F-measure parameter ?F is modeled by
BLANC. In our experiments we optimized ?F indi-
vidually for fluency and adequacy.
2.3 The ACS Algorithm
We present a practical algorithm for extracting All
Common Skip-ngrams (ACS) of any size that appear
in the candidate and reference translations. For clar-
ity purposes, we present the ACS algorithm as it
relates to the MT problem: find all common skip-
ngrams (ACS) of any size in two sentences X and Y :
wSKIP ? Acs(?, ?,X, Y ) (7)
= {wSKIP1..wSKIPmin(|X|,|Y |)} (8)
where wSkipn is the set of all skip-ngrams of size n
and is defined as:
wSKIPn = {?w1..wn? | wi ? X,wi ? Y,?i ? [1..n]
and wi ? wj ,?i < j ? [1..n]}
Given two sentences X and Y we observe a match
(w, x, y) if word w is found in sentence X at index x
and in sentence Y at index y:
(w, x, y) ? {0 ? x ? |X|, 0 ? y ? |Y |,
w ? V, and X[x] = Y [y] = w} (9)
where V is the vocabulary with a finite set of words.
In the following subsections, we present the fol-
lowing steps in the ACS algorithm:
1. identify all matches ? find matches and generate
corresponding nodes in the dependency graph
2. generate dependencies ? construct edges ac-
cording to pairwise match dependencies
3. propagate common subsequences ? count
all common skip-ngrams using corresponding
weights and distances
In the following sections we use the following exam-
ple to illustrate the intermediate steps of ACS.
X. ?to be or not to be?
Y. ?to exist or not be?
2.3.1 Step 1: Identify All Matches
In this step we identify all word matches (w, x, y)
in sentences X and Y . Using the example above, the
intermediate inputs and outputs of this step are:
Input: X. ?to be or not to be?
Y. ?to exist or not be?
Output: (to,1,1); (to,5,1); (or,3,3); (be,2,5); . . .
For each match we create a corresponding node N
in a dependency graph. With each node we associate
the actual word matched and its corresponding index
positions in both sentences.
2.3.2 Step 2: Generate Dependencies
A dependency N1 ? N2 occurs when the two
corresponding matches (w1, x1, y1) and (w2, x2, y2)
can form a valid common skip-bigram: i.e. when
x1 < x2 and y1 < y2. Note that the matches can
cover identical words, but their indices cannot be the
same (x1 6= x2 and y1 6= y2) since a skip-bigram
requires two different word matches.
In order to facilitate the generation of all common
subsequences, the graph is populated with the
appropriate dependency edges:
for each node N in DAG
for each node M 6=N in DAG
if N(x)?M(x) and N(y)?M(y)
create edge E: N?M
compute ?XY (E)
compute ?(E)
This step incorporates the concepts of skip-ngram
weight and distance into the graph. With each edge
E : N1 ? N2 we associate step-wise weight and dis-
tance information for the corresponding skip-bigram
formed by matches (w1, x1, y1) and (w2, x2, y2).
Note that rather than counting all skip-ngrams,
which would be exponential in the worst case sce-
nario, we only construct a structure of match depen-
dencies (i.e. skip-bigrams). As in dynamic program-
ming, in order to avoid exponential complexity, we
compute individual skip-ngram scores only once.
2.3.3 Step 3: Propagate Common Subsequences
In this last step, the ACS algorithm counts all com-
mon skip-ngrams using corresponding weights and
distances. In the general case, this step is equiva-
lent measuring the overlap of the two sentences X
and Y . As a special case, if no features are used, the
743
ACS algorithm is equivalent to counting the number
of common skip-ngrams regardless of gap sizes.
// depth first search (DFS)
for each node N in DAG
compute node N?s depth
// initialize skip-ngram counts
for each node N in DAG
vN [1]? 1
for i=2 to LCS(X,Y)
vN [i] = 0
// compute ngram counts
for d=1 to MAXDEPTH
for each node N of depth d in DAG
for each edge E: N?M
for i=2 to d
vM [i] += Sxy(?(E), ?(E), vN [i-1])
After algorithm ACS is run, the number of skip-
ngrams (weighted skip-ngram score) of size k is sim-
ply the sum of the number of skip-ngrams of size k
ending in each node N ?s corresponding match:
wSKIPk =
?
Ni?DAG
vNi [k] (10)
2.3.4 ACS Complexity and Feasibility
In the worst case scenario, both sentences X and Y
are composed of exactly the same repeated word: X
= ?w w w w .. ? and Y = ?w w w w ..?. We let m = |X|
and n = |Y |. In this case, the number of matches is
M = n ? m. Therefore, Step 1 has worst case time
and space complexity of O(m ? n). However, em-
pirical data suggest that there are far fewer matches
than in the worst-case scenario and the actual space
requirements are drastically reduced. Even in the
worst-case scenario, if we assume the average sen-
tences is fewer than 100 words, the number of nodes
in the DAG would only be 10, 000. Step 2 of the al-
gorithm consists of creating edges in the dependency
graph. In the worst case scenario, the number of di-
rected edges is O(M2) and furthermore if the sen-
tences are uniformly composed of the same repeated
word as seen above, the worst-case time and space
complexity is m(m+1)/2 ?n(n+1)/2 = O(m2n2).
In Step 3 of the algorithm, the DFS complexity for
computing of node depths is O(M) and the complex-
ity of LCS(X,Y ) is O(m ? n). The dominant step
is the propagation of common subsequences (skip-
ngram counts). Let l be the size of the LCS. The up-
per bound on the size of the longest common subse-
quence is min(|X|, |Y |) = min(m,n). In the worst
case scenario, for each node we propagate l count val-
ues (the size of vector v) to all other nodes in the
DAG. Therefore, the time complexity for Step 3 is
O(M2 ? l) = O(m2n2l) (fifth degree polynomial).
3 BLANC as a Generalization of BLEU and
ROUGE
Due to its parametric nature, the All Common Sub-
sequences algorithm can emulate the ngram compu-
tation of several popular MT evaluation metrics. The
weighting function ? allows skip-ngrams with differ-
ent gap sizes to be assigned different weights. Param-
eter ? controls the shape of the weighting function.
In one extreme scenario, if we allow ? to take
very large values, the net effect is that all contiguous
ngrams of any size will have corresponding weights
of e0 = 1 while all other skip-ngrams will have
weights that are zero. In this case, the distance
function will only apply to contiguous ngrams which
have the same size and no gaps. Therefore, the dis-
tance function will also be 1. The overall result is
that the ACS algorithm collects contiguous common
ngram counts for all ngram sizes. This is equivalent
to computing the ngram overlap between two sen-
tences, which is equivalent to the ngram computa-
tion performed BLEU metric. In addition to comput-
ing ngram overlap, BLEU incorporates a thresholding
(clipping) on ngram counts based on reference trans-
lations, as well as a brevity penalty which makes sure
the machine-produced translations are not too short.
In BLANC, this is replaced by standard F-measure,
which research (Turian et al, 2003) has shown it can
be used successfully in MT evaluation.
Another scenario consists of setting the ? and ?
parameters to 0. In this case, all skip-ngrams are as-
signed the same weight value of 1 and skip-ngram
matches are also assigned the same distance value of
1 regardless of gap sizes and differences in gap sizes.
This renders all skip-ngrams equivalent and the ACS
algorithm is reduced to counting the skip-ngram over-
lap between two sentences. Using these counts, pre-
cision and recall-based metrics such as the F-measure
can be computed. If we let the ? and ? parameters to
be zero, disregard redundant matches, and compute
744
0 50 100
0
50
100
150
200
Arabic 2003
Sentence Length
#s
en
te
nc
es
0 50 100
0
50
100
150
200
250
300
350
Chinese 2003
Sentence Length
#s
en
te
nc
es
0 50 100
100
102
104
ACS #Matches
Sentence Length
Av
g 
#M
at
ch
es
0 50 100
100
105
ACS #Edges
Sentence Length
Av
g 
#E
dg
es
0 50 100
100
105
1010
ACS #Feature Calls
Sentence Length
Av
g 
#T
ot
al
Arabic
Chinese
Worst Case
Figure 1: Empirical and theoretical behavior of ACS on 2003 machine translation evaluation data (semilog scale).
the ACS only for skip-ngrams of size 2, the ACS algo-
rithm is equivalent to the ROUGE-S metric (Lin and
Och, 2004). This case represents a specific parameter
setting in the ACS skip-ngram computation.
The longest common subsequence statistic has also
been successfully used for automatic machine trans-
lation evaluation in the ROUGE-L (Lin and Och,
2004) algorithm. In BLANC, if we set both ? and
? parameters to zero, the net result is a set of skip-
bigram (common subsequence) overlap counts for all
skip-bigram sizes. Although dynamic programming
or suffix trees can be used to compute the LCS much
faster, under this parameter setting the ACS algorithm
can also produce the longest common subsequence:
LCS(X,Y )? argmax
k
ACS(wSKIPk) > 0
where Acs(wSKIPk) is the number of common
skip-ngrams (common subsequences) produced by
the ACS algorithm.
ROUGE-W (Lin and Och, 2004) relies on a
weighted version of the longest common subse-
quence, under which longer contiguous subsequences
are assigned a higher weight than subsequences that
incorporate gaps. ROUGE-W uses the polynomial
function xa in the weighted LCS computation. This
setting can also be simulated by BLANC by adjusting
the parameters ? to reward tighter skip-ngrams and ?
to assign a very high score to similar size gaps. In-
tuitively, ? is used to reward skip-ngrams that have
smaller gaps, while ? is used to reward better aligned
skip-ngram overlap.
4 Scalability & Data Exploration
In Figure 1 we show theoretical and empirical prac-
tical behavior for the ACS algorithm on the 2003
TIDES machine translation evaluation data for Ara-
bic and Chinese. Sentence length distribution is
somewhat similar for the two languages ? only a very
small amount of text segments have more than 50
tokens. We show the ACS graph size in the worst
case scenario, and the empirical average number of
matches for both languages as a function of sentence
length. We also show (on a log scale) the upper bound
on time/space complexity in terms of total number
of feature computations. Even though the worst-
case scenario is tractable (polynomial), the empirical
amount of computation is considerably smaller in the
form of polynomials of lower degree. In Figure 1,
sentence length is the average between reference and
candidate lengths.
Finally, we also show the total number of fea-
ture computations involved in performing a full over-
lap search and computing a numeric score for the
745
reference-candidate translation pair. We have exper-
imented with the ACS algorithm using a worst-case
scenario where all words are exactly the same for a
fifty words reference translation and candidate trans-
lation. In practice when considering real sentences
the number of matches is very small. In this setting,
the algorithm takes less than two seconds on a low-
end desktop system when working on the worst case
scenario, and less then a second for all candidate-
reference pairs in the TIDES 2003 dataset. This re-
sult renders the ACS algorithm very practical for au-
tomatic MT evaluation.
5 Experiments & Results
In the dynamic metric BLANC, we have implemented
the ACS algorithm using several parameters includ-
ing the aggregate gap size ?, the displacement feature
?, a parameter for regulating skip-ngram size contri-
bution, and the F-measure ?F parameter.
Until recently, most experiments that evaluate au-
tomatic metrics correlation to human judgments have
been performed at a system level. In such experi-
ments, human judgments are aggregated across sen-
tences for each MT system and compared to aggre-
gate scores for automatic metrics. While high scor-
ing metrics in this setting are useful for understand-
ing relative system performance, not all of them are
robust enough for evaluating the quality of machine
translation output at a lower granularity. Sentence-
level translation quality estimation is very useful
when MT is used as a component in a pipeline of text-
processing applications (e.g. question answering).
The fact that current automatic MT evaluation met-
rics including BLANC do not correlate well with hu-
man judgments at the sentence level, does not mean
we should ignore this need and focus only on system
level evaluation. On the contrary, further research is
required to improve these metrics. Due to its train-
able nature, and by allowing additional features to be
incorporated into its model, BLANC has the potential
to address this issue.
For comparison purposes with previous literature,
we have also performed experiments at system level
for Arabic. The datasets used consist of the MT trans-
lation outputs from all systems available through the
Tides 2003 evaluation (663 sentences) for training
and Tides 2004 evaluation (1353 sentences) for test-
ing.
We compare (Table 1) the performance of BLANC
on Arabic translation output with the performance
of more established evaluation metrics: BLEU and
NIST, and also with more recent metrics: ROUGE-
L and ROUGE-S (using an unlimited size skip win-
dow), which have been shown to correlate well with
human judgments at system level ? as confirmed by
our results. We have performed experiments in which
case information is preserved as well as experiments
that ignore case information. Since the results are
very similar, we only show here experiments under
the former condition. In order to maintain consis-
tency, when using any metric we apply the same pre-
processing provided by the MTEval script. When
computing the correlation between metrics and hu-
man judgments, we only keep strictly positive scores.
While this is not fully equivalent to BLEU smooth-
ing, it partially mitigates the same problem of zero
count ngrams for short sentences. In future work we
plan to implement smoothing for all metrics, includ-
ing BLANC.
We train BLANC separately for adequacy and flu-
ency, as well as for system level and segment level
correlation with human judgments. The BLANC pa-
rameters are currently trained using a simple hill-
climbing procedure and using several starting points
in order to decrease the chance of reaching a local
maximum.
BLANC proves to be robust across criteria and
granularity levels. As expected, different parameter
values of BLANC optimize different criteria (e.g. ad-
equacy and fluency). We have observed that train-
ing BLANC for adequacy results in more bias to-
wards recall (?F =3) compared to training it for flu-
ency (?F =2). This confirms our intuition that a dy-
namic, parametric metric is justified for automatic
evaluation.
6 Conclusions & Future Work
In previous sections we have defined simple distance
functions. More complex functions can also be incor-
porated in ACS. Skip-ngrams in the candidate sen-
tence might be rewarded if they contain fewer gaps in
the candidate sentence and penalized if they contain
more. Different distance functions could also be used
in ACS, including functions based on surface-form
features and part-of-speech features.
Most of the established MT evaluation methods are
746
Tides 2003 Arabic
System Level Segment Level
Method Adequacy Fluency Adequacy Fluency
BLEU 0.950 0.934 0.382 0.286
NIST 0.962 0.939 0.439 0.304
ROUGE-L 0.974 0.926 0.440 0.328
ROUGE-S 0.949 0.935 0.360 0.328
BLANC 0.988 0.979 0.492 0.391
Tides 2004 Arabic
System Level Segment Level
Method Adequacy Fluency Adequacy Fluency
BLEU 0.978 0.994 0.446 0.337
NIST 0.987 0.952 0.529 0.358
ROUGE-L 0.981 0.985 0.538 0.412
ROUGE-S 0.937 0.980 0.367 0.408
BLANC 0.982 0.994 0.565 0.438
Table 1: Pearson correlation of several metrics with human judgments at system level and segment level for fluency and adequacy.
static functions according to which automatic evalu-
ation scores are computed. In this paper, we have
laid the foundation for a more flexible, parametric ap-
proach that can be trained using existing MT data and
that can be optimized for highest agreement with hu-
man assessors, for different criteria.
We have introduced ACS, a practical algorithm
with learnable parameters for automatic MT evalu-
ation and showed that ngram computation of popu-
lar evaluation methods can be emulated through dif-
ferent parameters by ACS. We have computed time
and space bounds for the ACS algorithm and argued
that while it is more powerful in modeling local and
sentence structure, it offers the same practicality as
established algorithms.
In our experiments, we trained and tested BLANC
on data from consecutive years, and therefore tai-
lored the metric for two different operating points
in MT system performance. In this paper we show
that BLANC correlates well with human performance
when trained on previous year data for both sentence
and system level.
In the future, we plan to investigate the stability
and performance of BLANC and also apply it to auto-
matic summarization evaluation. We plan to optimize
the BLANC parameters for different criteria in addi-
tion to incorporating syntactic and semantic features
(e.g. ngrams, word classes, part-of-speech).
In previous sections we have defined simple dis-
tance functions. More complex functions can also
be incorporated in ACS. Skip-ngrams in the candi-
date sentence might be rewarded if they contain fewer
gaps in the candidate sentence and penalized if they
contain more. Different distance functions could also
be used in ACS, including functions based on surface-
form features and part-of-speech features.
Looking beyond the BLANC metric, this paper
makes the case for the need to shift to trained, dy-
namic evaluation metrics which can adapt to individ-
ual optimization criteria and correlation functions.
We plan to make available an implementation of
BLANC at http://www.cs.cmu.edu/ llita/blanc.
References
Y. Akiba, K. Iamamurfa, and E. Sumita. 2001. Using
multiple edit distances to automatically rank machine
translation output. MT Summit VIII.
C. Culy and S.Z. Riehemann. 2003. The limits of n-
gram translation evaluation metrics. Machine Transla-
tion Summit IX.
George Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram co-occurrence
statistics. Human Language Technology Conference
(HLT).
V.I. Levenshtein. 1965. Binary codes capable of cor-
recting deletions, insertions, and reversals. Doklady
Akademii Nauk SSSR.
C.Y. Lin and F.J. Och. 2004. Automatic evaluation of
machine translation quality using longest common sub-
sequence and skip bigram statistics. ACL.
S. Niessen, F.J. Och, G. Leusch, and H. Ney. 2000. An
evaluation tool for machine translation: Fast evaluation
for mt research. LREC.
K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2001.
Bleu: a method for automatic evaluation of machine
translation. IBM Research Report.
R. Soricut and E. Brill. 2004. A unified framework for
automatic evaluation using n-gram co-occurence statis-
tics. ACL.
K.Y. Su, M.W. Wu, and J.S. Chang. 1992. A new quanti-
tative quality measure for machine translation systems.
COLING.
J.P. Turian, L. Shen, and I.D. Melamed. 2003. Evaluation
of machine translation and its evaluation. MT Summit
IX.
C.J. Van-Rijsbergen. 1979. Information retrieval.
747
tRuEcasIng
Lucian Vlad Lita ?
Carnegie Mellon
llita@cs.cmu.edu
Abe Ittycheriah
IBM T.J. Watson
abei@us.ibm.com
Salim Roukos
IBM T.J. Watson
roukos@us.ibm.com
Nanda Kambhatla
IBM T.J. Watson
nanda@us.ibm.com
Abstract
Truecasing is the process of restoring
case information to badly-cased or non-
cased text. This paper explores truecas-
ing issues and proposes a statistical, lan-
guage modeling based truecaser which
achieves an accuracy of ?98% on news
articles. Task based evaluation shows a
26% F-measure improvement in named
entity recognition when using truecasing.
In the context of automatic content ex-
traction, mention detection on automatic
speech recognition text is also improved
by a factor of 8. Truecasing also en-
hances machine translation output legibil-
ity and yields a BLEU score improvement
of 80.2%. This paper argues for the use of
truecasing as a valuable component in text
processing applications.
1 Introduction
While it is true that large, high quality text corpora
are becoming a reality, it is also true that the digital
world is flooded with enormous collections of low
quality natural language text. Transcripts from var-
ious audio sources, automatic speech recognition,
optical character recognition, online messaging and
gaming, email, and the web are just a few exam-
ples of raw text sources with content often produced
in a hurry, containing misspellings, insertions, dele-
tions, grammatical errors, neologisms, jargon terms
? Work done at IBM TJ Watson Research Center
etc. We want to enhance the quality of such sources
in order to produce better rule-based systems and
sharper statistical models.
This paper focuses on truecasing, which is the
process of restoring case information to raw text.
Besides text rEaDaBILiTY, truecasing enhances the
quality of case-carrying data, brings into the pic-
ture new corpora originally considered too noisy for
various NLP tasks, and performs case normalization
across styles, sources, and genres.
Consider the following mildly ambiguous sen-
tence ?us rep. james pond showed up riding an it
and going to a now meeting?. The case-carrying al-
ternative ?US Rep. James Pond showed up riding an
IT and going to a NOW meeting? is arguably better
fit to be subjected to further processing.
Broadcast news transcripts contain casing errors
which reduce the performance of tasks such as
named entity tagging. Automatic speech recognition
produces non-cased text. Headlines, teasers, section
headers - which carry high information content - are
not properly cased for tasks such as question answer-
ing. Truecasing is an essential step in transforming
these types of data into cleaner sources to be used by
NLP applications.
?the president? and ?the President? are two viable
surface forms that correctly convey the same infor-
mation in the same context. Such discrepancies are
usually due to differences in news source, authors,
and stylistic choices. Truecasing can be used as a
normalization tool across corpora in order to pro-
duce consistent, context sensitive, case information;
it consistently reduces expressions to their statistical
canonical form.
In this paper, we attempt to show the benefits of
truecasing in general as a valuable building block
for NLP applications rather than promoting a spe-
cific implementation. We explore several truecasing
issues and propose a statistical, language modeling
based truecaser, showing its performance on news
articles. Then, we present a straight forward appli-
cation of truecasing on machine translation output.
Finally, we demonstrate the considerable benefits of
truecasing through task based evaluations on named
entity tagging and automatic content extraction.
1.1 Related Work
Truecasing can be viewed in a lexical ambiguity res-
olution framework (Yarowsky, 1994) as discriminat-
ing among several versions of a word, which hap-
pen to have different surface forms (casings). Word-
sense disambiguation is a broad scope problem that
has been tackled with fairly good results generally
due to the fact that context is a very good pre-
dictor when choosing the sense of a word. (Gale
et al, 1994) mention good results on limited case
restoration experiments on toy problems with 100
words. They also observe that real world problems
generally exhibit around 90% case restoration accu-
racy. (Mikheev, 1999) also approaches casing dis-
ambiguation but models only instances when capi-
talization is expected: first word in a sentence, after
a period, and after quotes. (Chieu and Ng, 2002)
attempted to extract named entities from non-cased
text by using a weaker classifier but without focus-
ing on regular text or case restoration.
Accents can be viewed as additional surface forms
or alternate word casings. From this perspective, ei-
ther accent identification can be extended to truecas-
ing or truecasing can be extended to incorporate ac-
cent restoration. (Yarowsky, 1994) reports good re-
sults with statistical methods for Spanish and French
accent restoration.
Truecasing is also a specialized method for
spelling correction by relaxing the notion of casing
to spelling variations. There is a vast literature on
spelling correction (Jones and Martin, 1997; Gold-
ing and Roth, 1996) using both linguistic and statis-
tical approaches. Also, (Brill and Moore, 2000) ap-
ply a noisy channel model, based on generic string
to string edits, to spelling correction.
2 Approach
In this paper we take a statistical approach to true-
casing. First we present the baseline: a simple,
straight forward unigram model which performs rea-
sonably well in most cases. Then, we propose a bet-
ter, more flexible statistical truecaser based on lan-
guage modeling.
From a truecasing perspective we observe four
general classes of words: all lowercase (LC), first
letter uppercase (UC), all letters uppercase (CA), and
mixed case word MC). The MC class could be fur-
ther refined into meaningful subclasses but for the
purpose of this paper it is sufficient to correctly iden-
tify specific true MC forms for each MC instance.
We are interested in correctly assigning case la-
bels to words (tokens) in natural language text. This
represents the ability to discriminate between class
labels for the same lexical item, taking into account
the surrounding words. We are interested in casing
word combinations observed during training as well
as new phrases. The model requires the ability to
generalize in order to recognize that even though the
possibly misspelled token ?lenon? has never been
seen before, words in the same context usually take
the UC form.
2.1 Baseline: The Unigram Model
The goal of this paper is to show the benefits of true-
casing in general. The unigram baseline (presented
below) is introduced in order to put task based eval-
uations in perspective and not to be used as a straw-
man baseline.
The vast majority of vocabulary items have only
one surface form. Hence, it is only natural to adopt
the unigram model as a baseline for truecasing. In
most situations, the unigram model is a simple and
efficient model for surface form restoration. This
method associates with each surface form a score
based on the frequency of occurrence. The decoding
is very simple: the true case of a token is predicted
by the most likely case of that token.
The unigram model?s upper bound on truecasing
performance is given by the percentage of tokens
that occur during decoding under their most frequent
case. Approximately 12% of the vocabulary items
have been observed under more than one surface
form. Hence it is inevitable for the unigram model
to fail on tokens such as ?new?. Due to the over-
whelming frequency of its LC form, ?new? will take
this particular form regardless of what token follows
it. For both ?information? and ?york? as subsequent
words, ?new? will be labeled as LC. For the latter
case, ?new? occurs under one of its less frequent sur-
face forms.
2.2 Truecaser
The truecasing strategy that we are proposing seeks
to capture local context and bootstrap it across a
sentence. The case of a token will depend on the
most likely meaning of the sentence - where local
meaning is approximated by n-grams observed dur-
ing training. However, the local context of a few
words alone is not enough for case disambiguation.
Our proposed method employs sentence level con-
text as well.
We capture local context through a trigram lan-
guage model, but the case label is decided at a sen-
tence level. A reasonable improvement over the un-
igram model would have been to decide the word
casing given the previous two lexical items and their
corresponding case content. However, this greedy
approach still disregards global cues. Our goal is
to maximize the probability of a larger text segment
(i.e. a sentence) occurring under a certain surface
form. Towards this goal, we first build a language
model that can provide local context statistics.
2.2.1 Building a Language Model
Language modeling provides features for a label-
ing scheme. These features are based on the prob-
ability of a lexical item and a case content condi-
tioned on the history of previous two lexical items
and their corresponding case content:
Pmodel(w3|w2, w1) = ?trigramP (w3|w2, w1)
+ ?bigramP (w3|w2)
+ ?unigramP (w3)
+ ?uniformP0 (1)
where trigram, bigram, unigram, and uniform prob-
abilities are scaled by individual ?is which are
learned by observing training examples. wi repre-
sents a word with a case tag treated as a unit for
probability estimation.
2.2.2 Sentence Level Decoding
Using the language model probabilities we de-
code the case information at a sentence level. We
construct a trellis (figure 1) which incorporates all
the sentence surface forms as well as the features
computed during training. A node in this trellis con-
sists of a lexical item, a position in the sentence, a
possible casing, as well as a history of the previous
two lexical items and their corresponding case con-
tent. Hence, for each token, all surface forms will
appear as nodes carrying additional context infor-
mation. In the trellis, thicker arrows indicate higher
transition probabilities.
Figure 1: Given individual histories, the decodings
delay and DeLay, are most probable - perhaps in the
context of ?time delay? and respectively ?Senator
Tom DeLay?
The trellis can be viewed as a Hidden Markov
Model (HMM) computing the state sequence
which best explains the observations. The states
(q1, q2, ? ? ? , qn) of the HMM are combinations of
case and context information, the transition proba-
bilities are the language model (?) based features,
and the observations (O1O2 ? ? ?Ot) are lexical items.
During decoding, the Viterbi algorithm (Rabiner,
1989) is used to compute the highest probability
state sequence (q?? at sentence level) that yields the
desired case information:
q?? = argmaxqi1qi2???qitP (qi1qi2 ? ? ? qit|O1O2 ? ? ?Ot, ?)
(2)
where P (qi1qi2 ? ? ? qit|O1O2 ? ? ?Ot, ?) is the proba-
bility of a given sequence conditioned on the obser-
vation sequence and the model parameters. A more
sophisticated approach could be envisioned, where
either the observations or the states are more expres-
sive. These alternate design choices are not explored
in this paper.
Testing speed depends on the width and length of
the trellis and the overall decoding complexity is:
Cdecoding = O(SMH+1) where S is the sentence
size, M is the number of surface forms we are will-
ing to consider for each word, and H is the history
size (H = 3 in the trigram case).
2.3 Unknown Words
In order for truecasing to be generalizable it must
deal with unknown words ? words not seen during
training. For large training sets, an extreme assump-
tion is that most words and corresponding casings
possible in a language have been observed during
training. Hence, most new tokens seen during de-
coding are going to be either proper nouns or mis-
spellings. The simplest strategy is to consider all
unknown words as being of the UC form (i.e. peo-
ple?s names, places, organizations).
Another approach is to replace the less frequent
vocabulary items with case-carrying special tokens.
During training, the word mispeling is replaced with
by UNKNOWN LC and the word Lenon with UN-
KNOWN UC. This transformation is based on the
observation that similar types of infrequent words
will occur during decoding. This transformation cre-
ates the precedent of unknown words of a particular
format being observed in a certain context. When a
truly unknown word will be seen in the same con-
text, the most appropriate casing will be applied.
This was the method used in our experiments. A
similar method is to apply the case-carrying special
token transformation only to a small random sam-
ple of all tokens, thus capturing context regardless
of frequency of occurrence.
2.4 Mixed Casing
A reasonable truecasing strategy is to focus on to-
ken classification into three categories: LC, UC, and
CA. In most text corpora mixed case tokens such as
McCartney, CoOl, and TheBeatles occur with mod-
erate frequency. Some NLP tasks might prefer map-
ping MC tokens starting with an uppercase letter into
the UC surface form. This technique will reduce the
feature space and allow for sharper models. How-
ever, the decoding process can be generalized to in-
clude mixed cases in order to find a closer fit to the
true sentence. In a clean version of the AQUAINT
(ARDA) news stories corpus, ? 90% of the tokens
occurred under the most frequent surface form (fig-
ure 2).
Figure 2: News domain casing distribution
The expensive brute force approach will consider
all possible casings of a word. Even with the full
casing space covered, some mixed cases will not be
seen during training and the language model prob-
abilities for n-grams containing certain words will
back off to an unknown word strategy. A more fea-
sible method is to account only for the mixed case
items observed during training, relying on a large
enough training corpus. A variable beam decod-
ing will assign non-zero probabilities to all known
casings of each word. An n-best approximation is
somewhat faster and easier to implement and is the
approach employed in our experiments. During the
sentence-level decoding only the n-most-frequent
mixed casings seen during training are considered.
If the true capitalization is not among these n-best
versions, the decoding is not correct. Additional lex-
ical and morphological features might be needed if
identifying MC instances is critical.
2.5 First Word in the Sentence
The first word in a sentence is generally under the
UC form. This sentence-begin indicator is some-
times ambiguous even when paired with sentence-
end indicators such as the period. While sentence
splitting is not within the scope of this paper, we
want to emphasize the fact that many NLP tasks
would benefit from knowing the true case of the first
word in the sentence, thus avoiding having to learn
the fact that beginning of sentences are artificially
important. Since it is uneventful to convert the first
letter of a sentence to uppercase, a more interest-
ing problem from a truecasing perspective is to learn
how to predict the correct case of the first word in a
sentence (i.e. not always UC).
If the language model is built on clean sentences
accounting for sentence boundaries, the decoding
will most likely uppercase the first letter of any sen-
tence. On the other hand, if the language model
is trained on clean sentences disregarding sentence
boundaries, the model will be less accurate since dif-
ferent casings will be presented for the same context
and artificial n-grams will be seen when transition-
ing between sentences. One way to obtain the de-
sired effect is to discard the first n tokens in the train-
ing sentences in order to escape the sentence-begin
effect. The language model is then built on smoother
context. A similar effect can be obtained by initial-
izing the decoding with n-gram state probabilities so
that the boundary information is masked.
3 Evaluation
Both the unigram model and the language model
based truecaser were trained on the AQUAINT
(ARDA) and TREC (NIST) corpora, each consist-
ing of 500M token news stories from various news
agencies. The truecaser was built using IBM?s
ViaVoiceTMlanguage modeling tools. These tools
implement trigram language models using deleted
interpolation for backing off if the trigram is not
found in the training data. The resulting model?s
perplexity is 108.
Since there is no absolute truth when truecasing a
sentence, the experiments need to be built with some
reference in mind. Our assumption is that profes-
sionally written news articles are very close to an
intangible absolute truth in terms of casing. Fur-
thermore, we ignore the impact of diverging stylistic
forms, assuming the differences are minor.
Based on the above assumptions we judge the
truecasing methods on four different test sets. The
first test set (APR) consists of the August 25,
2002 ? top 20 news stories from Associated Press
and Reuters excluding titles, headlines, and sec-
tion headers which together form the second test set
(APR+). The third test set (ACE) consists of ear-
?Randomly chosen test date
Figure 3: LM truecaser vs. unigram baseline.
lier news stories from AP and New York Times be-
longing to the ACE dataset. The last test set (MT)
includes a set of machine translation references (i.e.
human translations) of news articles from the Xin-
hua agency. The sizes of the data sets are as follows:
APR - 12k tokens, ACE - 90k tokens, and MT - 63k
tokens. For both truecasing methods, we computed
the agreement with the original news story consid-
ered to be the ground truth.
3.1 Results
The language model based truecaser consistently
displayed a significant error reduction in case
restoration over the unigram model (figure 3). On
current news stories, the truecaser agreement with
the original articles is ? 98%.
Titles and headlines usually have a higher con-
centration of named entities than normal text. This
also means that they need a more complex model to
assign case information more accurately. The LM
based truecaser performs better in this environment
while the unigram model misses named entity com-
ponents which happen to have a less frequent surface
form.
3.2 Qualitative Analysis
The original reference articles are assumed to have
the absolute true form. However, differences from
these original articles and the truecased articles are
not always casing errors. The truecaser tends to
modify the first word in a quotation if it is not
proper name: ?There has been? becomes ?there has
been?. It also makes changes which could be con-
sidered a correction of the original article: ?Xinhua
BLEU Breakdown
System BLEU 1gr Precision 2gr Precision 3gr Precision 4gr Precision
all lowercase 0.1306 0.6016 0.2294 0.1040 0.0528
rule based 0.1466 0.6176 0.2479 0.1169 0.0627
1gr truecasing 0.2206 0.6948 0.3328 0.1722 0.0988
1gr truecasing+ 0.2261 0.6963 0.3372 0.1734 0.0997
lm truecasing 0.2596 0.7102 0.3635 0.2066 0.1303
lm truecasing+ 0.2642 0.7107 0.3667 0.2066 0.1302
Table 1: BLEU score for several truecasing strategies. (truecasing+ methods additionally employ the ?first
sentence letter uppercased? rule adjustment).
Baseline With Truecasing
Class Recall Precision F Recall Precision F
ENAMEX 48.46 36.04 41.34 59.02 52.65 55.66 (+34.64%)
NUMEX 64.61 72.02 68.11 70.37 79.51 74.66 (+9.62%)
TIMEX 47.68 52.26 49.87 61.98 75.99 68.27 (+36.90%)
Overall 52.50 44.84 48.37 62.01 60.42 61.20 (+26.52%)
Table 2: Named Entity Recognition performance with truecasing and without (baseline).
news agency? becomes ?Xinhua News Agency? and
?northern alliance? is truecased as ?Northern Al-
liance?. In more ambiguous cases both the original
version and the truecased fragment represent differ-
ent stylistic forms: ?prime minister Hekmatyar? be-
comes ?Prime Minister Hekmatyar?.
There are also cases where the truecaser described
in this paper makes errors. New movie names are
sometimes miss-cased: ?my big fat greek wedding?
or ?signs?. In conducive contexts, person names
are correctly cased: ?DeLay said in?. However, in
ambiguous, adverse contexts they are considered to
be common nouns: ?pond? or ?to delay that?. Un-
seen organization names which make perfectly nor-
mal phrases are erroneously cased as well: ?interna-
tional security assistance force?.
3.3 Application: Machine Translation
Post-Processing
We have applied truecasing as a post-processing step
to a state of the art machine translation system in or-
der to improve readability. For translation between
Chinese and English, or Japanese and English, there
is no transfer of case information. In these situations
the translation output has no case information and it
is beneficial to apply truecasing as a post-processing
step. This makes the output more legible and the
system performance increases if case information is
required.
We have applied truecasing to Chinese-to-English
translation output. The data source consists of news
stories (2500 sentences) from the Xinhua News
Agency. The news stories are first translated, then
subjected to truecasing. The translation output is
evaluated with BLEU (Papineni et al, 2001), which
is a robust, language independent automatic ma-
chine translation evaluation method. BLEU scores
are highly correlated to human judges scores, pro-
viding a way to perform frequent and accurate au-
tomated evaluations. BLEU uses a modified n-gram
precision metric and a weighting scheme that places
more emphasis on longer n-grams.
In table 1, both truecasing methods are applied to
machine translation output with and without upper-
casing the first letter in each sentence. The truecas-
ing methods are compared against the all letters low-
ercased version of the articles as well as against an
existing rule-based system which is aware of a lim-
ited number of entity casings such as dates, cities,
and countries. The LM based truecaser is very ef-
fective in increasing the readability of articles and
captures an important aspect that the BLEU score is
sensitive to. Truecasig the translation output yields
Baseline With Truecasing
Source Recall Precision F Recall Precision F
BNEWS ASR 23 3 5 56 39 46 (+820.00%)
BNEWS HUMAN 77 66 71 77 68 72 (+1.41%)
XINHUA 76 71 73 79 72 75 (+2.74%)
Table 3: Results of ACE mention detection with and without truecasing.
an improvement ? of 80.2% in BLEU score over the
existing rule base system.
3.4 Task Based Evaluation
Case restoration and normalization can be employed
for more complex tasks. We have successfully lever-
aged truecasing in improving named entity recogni-
tion and automatic content extraction.
3.4.1 Named Entity Tagging
In order to evaluate the effect of truecasing on ex-
tracting named entity labels, we tested an existing
named entity system on a test set that has signif-
icant case mismatch to the training of the system.
The base system is an HMM based tagger, similar
to (Bikel et al, 1997). The system has 31 semantic
categories which are extensions on the MUC cate-
gories. The tagger creates a lattice of decisions cor-
responding to tokenized words in the input stream.
When tagging a word wi in a sentence of words
w0...wN , two possibilities. If a tag begins:
p(tN1 |wN1 )i = p(ti|ti?1, wi?1)p?(wi|ti, wi?1)
If a tag continues:
p(tN1 |wN1 )i = p(wi|ti, wi?1)
The ? indicates that the distribution is formed from
words that are the first words of entities. The p? dis-
tribution predicts the probability of seeing that word
given the tag and the previous word instead of the
tag and previous tag. Each word has a set of fea-
tures, some of which indicate the casing and embed-
ded punctuation. These models have several levels
of back-off when the exact trigram has not been seen
in training. A trellis spanning the 31 futures is built
for each word in a sentence and the best path is de-
rived using the Viterbi algorithm.
?Truecasing improves legibility, not the translation itself
The performance of the system shown in table 2
indicate an overall 26.52% F-measure improvement
when using truecasing. The alternative to truecas-
ing text is to destroy case information in the train-
ing material 	 SNORIFY procedure in (Bikel et al,
1997). Case is an important feature in detecting
most named entities but particularly so for the title
of a work, an organization, or an ambiguous word
with two frequent cases. Truecasing the sentence is
essential in detecting that ?To Kill a Mockingbird? is
the name of a book, especially if the quotation marks
are left off.
3.4.2 Automatic Content Extraction
Automatic Content Extraction (ACE) is task fo-
cusing on the extraction of mentions of entities and
relations between them from textual data. The tex-
tual documents are from newswire, broadcast news
with text derived from automatic speech recognition
(ASR), and newspaper with text derived from optical
character recognition (OCR) sources. The mention
detection task (ace, 2001) comprises the extraction
of named (e.g. ?Mr. Isaac Asimov?), nominal (e.g.
?the complete author?), and pronominal (e.g. ?him?)
mentions of Persons, Organizations, Locations, Fa-
cilities, and Geo-Political Entities.
The automatically transcribed (using ASR) broad-
cast news documents and the translated Xinhua
News Agency (XINHUA) documents in the ACE
corpus do not contain any case information, while
human transcribed broadcast news documents con-
tain casing errors (e.g. ?George bush?). This prob-
lem occurs especially when the data source is noisy
or the articles are poorly written.
For all documents from broadcast news (human
transcribed and automatically transcribed) and XIN-
HUA sources, we extracted mentions before and af-
ter applying truecasing. The ASR transcribed broad-
cast news data comprised 86 documents containing
a total of 15,535 words, the human transcribed ver-
sion contained 15,131 words. There were only two
XINHUA documents in the ACE test set containing
a total of 601 words. None of this data or any ACE
data was used for training the truecasing models.
Table 3 shows the result of running our ACE par-
ticipating maximum entropy mention detection sys-
tem on the raw text, as well as on truecased text. For
ASR transcribed documents, we obtained an eight
fold improvement in mention detection from 5% F-
measure to 46% F-measure. The low baseline score
is mostly due to the fact that our system has been
trained on newswire stories available from previous
ACE evaluations, while the latest test data included
ASR output. It is very likely that the improvement
due to truecasing will be more modest for the next
ACE evaluation when our system will be trained on
ASR output as well.
4 Possible Improvements & Future Work
Although the statistical model we have considered
performs very well, further improvements must go
beyond language modeling, enhancing how expres-
sive the model is. Additional features are needed
during decoding to capture context outside of the
current lexical item, medium range context, as well
as discontinuous context. Another potentially help-
ful feature to consider would provide a distribu-
tion over similar lexical items, perhaps using an
edit/phonetic distance.
Truecasing can be extended to cover a more gen-
eral notion surface form to include accents. De-
pending on the context, words might take different
surface forms. Since punctuation is a notion exten-
sion to surface form, shallow punctuation restora-
tion (e.g. word followed by comma) can also be ad-
dressed through truecasing.
5 Conclusions
We have discussed truecasing, the process of restor-
ing case information to badly-cased or non-cased
text, and we have proposed a statistical, language
modeling based truecaser which has an agreement
of ?98% with professionally written news articles.
Although its most direct impact is improving legibil-
ity, truecasing is useful in case normalization across
styles, genres, and sources. Truecasing is a valu-
able component in further natural language process-
ing. Task based evaluation shows a 26% F-measure
improvement in named entity recognition when us-
ing truecasing. In the context of automatic content
extraction, mention detection on automatic speech
recognition text is improved by a factor of 8. True-
casing also enhances machine translation output leg-
ibility and yields a BLEU score improvement of
80.2% over the original system.
References
2001. Entity detection and tracking. ACE Pilot Study
Task Definition.
D. Bikel, S. Miller, R. Schwartz, and R. Weischedel.
1997. Nymble: A high-performance learning name
finder. pages 194?201.
E. Brill and R. C. Moore. 2000. An improved error
model for noisy channel spelling correction. ACL.
H.L. Chieu and H.T. Ng. 2002. Teaching a weaker clas-
sifier: Named entity recognition on upper case text.
William A. Gale, Kenneth W. Church, and David
Yarowsky. 1994. Discrimination decisions for
100,000-dimensional spaces. Current Issues in Com-
putational Linguistics, pages 429?450.
Andrew R. Golding and Dan Roth. 1996. Applying win-
now to context-sensitive spelling correction. ICML.
M. P. Jones and J. H. Martin. 1997. Contextual spelling
correction using latent semantic analysis. ANLP.
A. Mikheev. 1999. A knowledge-free method for capi-
talized word disambiguation.
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei Jing Zhu. 2001. Bleu: a method for automatic
evaluation of machine translation. IBM Research Re-
port.
L. R. Rabiner. 1989. A tutorial on hidden markov models
and selected applications in speech recognition. Read-
ings in Speech Recognition, pages 267?295.
David Yarowsky. 1994. Decision lists for ambiguity res-
olution: Application to accent restoration in spanish
and french. ACL, pages 88?95.
Resource Analysis for Question Answering
Lucian Vlad Lita
Carnegie Mellon University
llita@cs.cmu.edu
Warren A. Hunt
Carnegie Mellon University
whunt@andrew.cmu.edu
Eric Nyberg
Carnegie Mellon University
ehn@cs.cmu.edu
Abstract
This paper attempts to analyze and bound the utility
of various structured and unstructured resources in
Question Answering, independent of a specific sys-
tem or component. We quantify the degree to which
gazetteers, web resources, encyclopedia, web doc-
uments and web-based query expansion can help
Question Answering in general and specific ques-
tion types in particular. Depending on which re-
sources are used, the QA task may shift from com-
plex answer-finding mechanisms to simpler data ex-
traction methods followed by answer re-mapping in
local documents.
1 Introduction
During recent years the Question Answering (QA)
field has undergone considerable changes: question
types have diversified, question complexity has in-
creased, and evaluations have become more stan-
dardized - as reflected by the TREC QA track
(Voorhees, 2003). Some recent approaches have
tapped into external data sources such as the Web,
encyclopedias, databases in order to find answer
candidates, which may then be located in the spe-
cific corpus being searched (Dumais et al, 2002; Xu
et al, 2003). As systems improve, the availability
of rich resources will be increasingly critical to QA
performance. While on-line resources such as the
Web, WordNet, gazetteers, and encyclopedias are
becoming more prevalent, no system-independent
study has quantified their impact on the QA task.
This paper focuses on several resources and their
inherent potential to provide answers, without con-
centrating on a particular QA system or component.
The goal is to quantify and bound the potential im-
pact of these resources on the QA process.
2 Related Work
More and more QA systems are using the Web as
a resource. Since the Web is orders of magni-
tude larger than local corpora, redundancy of an-
swers and supporting passages allows systems to
produce more correct, confident answers (Clarke et
al., 2001; Dumais et al, 2002). (Lin, 2002) presents
two different approaches to using the Web: access-
ing the structured data and mining the unstructured
data. Due to their complementary nature of these
approaches, hybrid systems are likely to perform
better (Lin and Katz, 2003).
Definitional questions (?What is X??, ?Who
is X??) are especially compatible with structured
resources such as gazetteers and encyclopedias.
The top performing definitional systems (Xu et
al., 2003) at TREC extract kernel facts similar to
a question profile built using structured and semi-
structured resources: WordNet (Miller et al, 1990),
Merriam-Webster dictionary www.m-w.com), the
Columbia Encyclopedia (www.bartleby.com),
Wikipedia (www.wikipedia.com), a biog-
raphy dictionary (www.s9.com) and Google
(www.google.com).
3 Approach
For the purpose of this paper, resources consist of
structured and semi-structured knowledge, such as
the Web, web search engines, gazetteers, and ency-
clopedias. Although many QA systems incorporate
or access such resources, few systems quantify in-
dividual resource impact on their performance and
little work has been done to estimate bounds on re-
source impact to Question Answering. Independent
of a specific QA system, we quantify the degree to
which these resources are able to directly provide
answers to questions.
Experiments are performed on the 2,393 ques-
tions and the corresponding answer keys provided
through NIST (Voorhees, 2003) as part of the TREC
8 through TREC 12 evaluations.
4 Gazetteers
Although the Web consists of mostly unstructured
and loosely structured information, the available
structured data is a valuable resource for question
answering. Gazetteers in particular cover several
frequently-asked factoid question types, such as
?What is the population of X?? or ?What is the cap-
ital of Y??. The CIA World Factbook is a database
containing geographical, political, and economi-
cal profiles of all the countries in the world. We
also analyzed two additional data sources contain-
ing astronomy information (www.astronomy.com)
and detailed information about the fifty US states
(www.50states.com).
Since gazetteers provide up-to-date information,
some answers will differ from answers in local
corpora or the Web. Moreover, questions requir-
ing interval-type answers (e.g. ?How close is the
sun??) may not match answers from different
sources which are also correct. Gazetteers offer
high precision answers, but have limited recall since
they only cover a limited number of questions (See
Table 1).
CIA All
Q-Set #qtions R P R P
TREC8 200 4 100% 6 100%
TREC9 693 8 100% 22 79%
TREC10 500 14 100% 23 96%
TREC11 500 8 100% 20 100%
TREC12 500 2 100% 11 92%
Overall 2393 36 100% 82 91%
Table 1: Recall (R): TREC questions can be directly
answered directly by gazetteers - shown are results
for CIA Factbook and All gazetteers combined. Our
extractor precision is Precision (P).
5 WordNet
Wordnets and ontologies are very common re-
sources and are employed in a wide variety of di-
rect and indirect QA tasks, such as reasoning based
on axioms extracted from WordNet (Moldovan et
al., 2003), probabilistic inference using lexical rela-
tions for passage scoring (Paranjpe et al, 2003), and
answer filtering via WordNet constraints (Leidner et
al., 2003).
Q-Set #qtions All Gloss Syns Hyper
TREC 8 200 32 22 7 13
TREC 9 693 197 140 73 75
TREC 10 500 206 148 82 88
TREC 11 500 112 80 29 46
TREC 12 500 93 56 10 52
Overall 2393 641 446 201 268
Table 2: Number of questions answerable using
WordNet glosses (Gloss), synonyms (Syns), hyper-
nyms and hyponyms (Hyper), and all of them com-
bined All.
Table 2 shows an upper bound on how many
TREC questions could be answered directly using
WordNet as an answer source. Question terms and
phrases were extracted and looked up in WordNet
glosses, synonyms, hypernyms, and hyponyms. If
the answer key matched the relevant WordNet data,
then an answer was considered to be found. Since
some answers might occur coincidentally, we these
results to represent upper bounds on possible utility.
6 Structured Data Sources
Encyclopedias, dictionaries, and other web
databases are structured data sources that are often
employed in answering definitional questions (e.g.,
?What is X??, ?Who is X??). The top-performing
definitional systems at TREC (Xu et al, 2003)
extract kernel facts similar question profiles built
using structured and semi-structured resources:
WordNet (Miller et al, 1990), the Merriam-
Webster dictionary www.m-w.com), the Columbia
Encyclopedia (www.bartleby.com), Wikipedia
(www.wikipedia.com), a biography dictionary
(www.s9.com) and Google (www.google.com).
Table 3 shows a number of data sources and
their impact on answering TREC questions. N-
grams were extracted from each question and run
through Wikipedia and Google?s define operator
(which searches specialized dictionaries, definition
lists, glossaries, abbreviation lists etc). Table 3
show that TREC 10 and 11 questions benefit the
most from the use of an encyclopedia, since they
include many definitional questions. On the other
hand, since TREC 12 has fewer definitional ques-
tions and more procedural questions, it does not
benefit as much from Wikipedia or Google?s define
operator.
Q-Set #qtions WikiAll Wiki1st DefOp
TREC 8 200 56 5 30
TREC 9 693 297 49 71
TREC 10 500 225 45 34
TREC 11 500 155 19 23
TREC 12 500 124 12 27
Overall 2393 857 130 185
Table 3: The answer is found in a definition ex-
tracted from Wikipedia WikiAll, in the first defi-
nition extracted from Wikipedia Wiki1st, through
Google?s define operator DefOp.
7 Answer Type Coverage
To test coverage of different answer types, we em-
ployed the top level of the answer type hierarchy
used by the JAVELIN system (Nyberg et al, 2003).
The most frequent types are: definition (e.g. ?What
is viscosity??), person-bio (e.g. ?Who was La-
can??), object(e.g. ?Name the highest mountain.?),
process (e.g. ?How did Cleopatra die??), lexicon
(?What does CBS stand for??)temporal(e.g. ?When
is the first day of summer??), numeric (e.g. ?How
tall is Mount Everest??), location (e.g. ?Where is
Tokyo??), and proper-name (e.g. ?Who owns the
Raiders??).
AType #qtions WikiAll DefOp Gaz WN
object 1003 426 92 58 309
lexicon 50 25 3 0 26
defn 178 105 9 11 112
pers-bio 39 15 11 0 17
process 138 23 6 9 16
temporal 194 63 14 0 50
numeric 121 27 13 10 18
location 151 69 21 2 47
proper 231 76 10 0 32
Table 4: Coverage of TREC questions divided by
most common answer types.
Table 4 shows TREC question coverage broken
down by answer type. Due to temporal consistency,
numeric questions are not covered very well. Al-
though the process and object types are broad an-
swer types, the coverage is still reasonably good.
As expected, the definition and person-bio answer
types are covered well by these resources.
8 The Web as a Resource
An increasing number of QA systems are using the
web as a resource. Since the Web is orders of mag-
nitude larger than local corpora, answers occur fre-
quently in simple contexts, which is more conducive
to retrieval and extraction of correct, confident an-
swers (Clarke et al, 2001; Dumais et al, 2002;
Lin and Katz, 2003). The web has been employed
for pattern acquisition (Ravichandran et al, 2003),
document retrieval (Dumais et al, 2002), query ex-
pansion (Yang et al, 2003), structured information
extraction, and answer validation (Magnini et al,
2002) . Some of these approaches enhance exist-
ing QA systems, while others simplify the question
answering task, allowing a less complex approach
to find correct answers.
8.1 Web Documents
Instead of searching a local corpus, some QA sys-
tems retrieve relevant documents from the web (Xu
et al, 2003). Since the density of relevant web doc-
uments can be higher than the density of relevant
local documents, answer extraction may be more
successful from the web. For a TREC evaluation,
answers found on the web must also be mapped to
relevant documents in the local corpus.
0 10 20 30 40 50 60 70 80 90 100
0
100
200
300
400
500
600
700
800
900
1000
Web Retrieval Performance For QA
document rank
# 
qu
es
tio
ns
Correct Doc Density
First Correct Doc
Figure 1: Web retrieval: relevant document density
and rank of first relevant document.
In order to evaluate the impact of web docu-
ments on TREC questions, we performed an ex-
periment where simple queries were submitted to
a web search engine. The questions were to-
kenized and filtered using a standard stop word
list. The resulting keyword queries were used to
retrieve 100 documents through the Google API
(www.google.com/api). Documents containing the
full question, question number, references to TREC,
NIST, AQUAINT, Question Answering and similar
content were filtered out.
Figure 1 shows the density of documents contain-
ing a correct answer, as well as the rank of the first
document containing a correct answer. The sim-
ple word query retrieves a relevant document for
almost half of the questions. Note that for most
systems, the retrieval performance should be supe-
rior since queries are usually more refined and addi-
tional query expansion is performed. However, this
experiment provides an intuition and a very good
lower bound on the precision and density of current
web documents for the TREC QA task.
8.2 Web-Based Query Expansion
Several QA systems participating at TREC have
used search engines for query expansion (Yang et
al., 2003). The basic query expansion method
utilizes pseudo-relevance feedback (PRF) (Xu and
Croft, 1996). Content words are selected from ques-
tions and submitted as queries to a search engine.
The top n retrieved documents are selected, and k
terms or phrases are extracted according to an op-
timization criterion (e.g. term frequency, n-gram
frequency, average mutual information using cor-
pus statistics, etc). These k items are used in the
expanded query.
We experimented by using the top 5, 10, 15, 20,
0 5 10 15 20 25 30 35 40 45 50
100
200
300
400
500
600
700
800
900
1000
1100
Answer frequency using PRF
# PRF terms
# 
qu
es
tio
ns
Top 5 documents
Top 10 documents
Top 15 documents
Top 20 documents
Top 50 documents
Top 100 documents
Figure 2: Finding a correct answer in PRF expan-
sion terms - applied to 2183 questions for witch an-
swer keys exist.
50, and 100 documents retrieved via the Google API
for each question, and extracted the most frequent
fifty n-grams (up to trigrams). The goal was to de-
termine the quality of query expansion as measured
by the density of correct answers already present
in the expansion terms. Even without filtering n-
grams matching the expected answer type, simple
PRF produces the correct answer in the top n-grams
for more than half the questions. The best correct
answer density is achieved using PRF with only 20
web documents.
8.3 Conclusions
This paper quantifies the utility of well-known and
widely-used resources such as WordNet, encyclope-
dias, gazetteers and the Web on question answering.
The experiments presented in this paper represent
loose bounds on the direct use of these resources in
answering TREC questions. We reported the perfor-
mance of these resources on different TREC collec-
tions and on different question types. We also quan-
tified web retrieval performance, and confirmed that
the web contains a consistently high density of rel-
evant documents containing correct answers even
when simple queries are used. The paper also
shows that pseudo-relevance feedback alone using
web documents for query expansions can produce
a correct answer for fifty percent of the questions
examined.
9 Acknowledgements
This work was supported in part by the Advanced
Research and Development Activity (ARDA)?s
Advanced Question Answering for Intelligence
(AQUAINT) Program.
References
C.L.A. Clarke, G.V. Cormack, and T.R. Lynam.
2001. Exploiting redundancy in question answer-
ing. SIGIR.
S. Dumais, M. Banko, E. Brill, J. Lin, and A. Ng.
2002. Web question answering: Is more always
better? SIGIR.
J. Leidner, J. Bos, T. Dalmas, J. Curran, S. Clark,
C. Bannard, B. Webber, and M. Steedman. 2003.
Qed: The edinburgh trec-2003 question answer-
ing system. TREC.
J. Lin and B. Katz. 2003. Question answering from
the web using knowledge annotation and knowl-
edge mining techniques. CIKM.
J. Lin. 2002. The web as a resource for question
answering: Perspectives and challenges. LREC.
B. Magnini, M. Negri, R. Pervete, and H. Tanev.
2002. Is it the right answer? exploiting web re-
dundancy for answer validation. ACL.
G.A. Miller, R. Beckwith, C. Fellbaum, D. Gross,
and K. Miller. 1990. Five papers on wordnet. In-
ternational Journal of Lexicography.
D. Moldovan, D. Clark, S. Harabagiu, and S. Maio-
rano. 2003. Cogex: A logic prover for question
answering. ACL.
E. Nyberg, T. Mitamura, J. Callan, J. Carbonell,
R. Frederking, K. Collins-Thompson, L. Hiyaku-
moto, Y. Huang, C. Huttenhower, S. Judy, J. Ko,
A. Kupsc, L.V. Lita, V. Pedro, D. Svoboda, and
B. Vand Durme. 2003. A multi strategy approach
with dynamic planning. TREC.
D. Paranjpe, G. Ramakrishnan, and S. Srinivasan.
2003. Passage scoring for question answering via
bayesian inference on lexical relations. TREC.
D. Ravichandran, A. Ittycheriah, and S. Roukos.
2003. Automatic derivation of surface text pat-
terns for a maximum entropy based question an-
swering system. HLT-NAACL.
E.M. Voorhees. 2003. Overview of the trec 2003
question answering track. TREC.
J. Xu and W.B. Croft. 1996. Query expansion using
local and global analysis. SIGIR.
J. Xu, A. Licuanan, and R. Weischedel. 2003. Trec
2003 qa at bbn: Answering definitional ques-
tions. TREC.
H. Yang, T.S. Chua, S. Wang, and C.K. Koh. 2003.
Structured use of external knowledge for event-
based open domain question answering. SIGIR.
 
	 	
	Instance-Based Question Answering:
A Data-Driven Approach
Lucian Vlad Lita
Carnegie Mellon University
llita@cs.cmu.edu
Jaime Carbonell
Carnegie Mellon University
jgc@cs.cmu.edu
Abstract
Anticipating the availability of large question-
answer datasets, we propose a principled, data-
driven Instance-Based approach to Question An-
swering. Most question answering systems incor-
porate three major steps: classify questions accord-
ing to answer types, formulate queries for document
retrieval, and extract actual answers. Under our ap-
proach, strategies for answering new questions are
directly learned from training data. We learn mod-
els of answer type, query content, and answer ex-
traction from clusters of similar questions. We view
the answer type as a distribution, rather than a class
in an ontology. In addition to query expansion, we
learn general content features from training data and
use them to enhance the queries. Finally, we treat
answer extraction as a binary classification problem
in which text snippets are labeled as correct or in-
correct answers. We present a basic implementation
of these concepts that achieves a good performance
on TREC test data.
1 Introduction
Ever since Question Answering (QA) emerged as
an active research field, the community has slowly
diversified question types, increased question com-
plexity, and refined evaluation metrics - as reflected
by the TREC QA track (Voorhees, 2003). Starting
from successful pipeline architectures (Moldovan et
al., 2000; Hovy et al, 2000), QA systems have re-
sponded to changes in the nature of the QA task by
incorporating knowledge resources (Hermjakob et
al., 2000; Hovy et al, 2002), handling additional
types of questions, employing complex reasoning
mechanisms (Moldovan et al, 2003; Nyberg et al,
2003), tapping into external data sources such as
the Web, encyclopedias, databases (Dumais et al,
2002; Xu et al, 2003), and merging multiple agents
and strategies into meta-systems (Chu-Carroll et al,
2003; Burger et al, 2002).
In recent years, learning components have started
to permeate Question Answering (Clarke et al,
2003; Ravichandran et al, 2003; Echihabi and
Marcu, 2003). Although the field is still domi-
nated by knowledge-intensive approaches, compo-
nents such as question classification, answer extrac-
tion, and answer verification are beginning to be ad-
dressed through statistical methods. At the same
time, research efforts in data acquisition promise to
deliver increasingly larger question-answer datasets
(Girju et al, 2003; Fleischman et al, 2003). More-
over, Question Answering is expanding to different
languages (Magnini et al, 2003) and domains other
than news stories (Zweigenbaum, 2003). These
trends suggest the need for principled, statisti-
cally based, easily re-trainable, language indepen-
dent QA systems that take full advantage of large
amounts of training data.
We propose an instance-based, data-driven ap-
proach to Question Answering. Instead of classify-
ing questions according to limited, predefined on-
tologies, we allow training data to shape the strate-
gies for answering new questions. Answer mod-
els, query content models, and extraction models are
also learned directly from training data. We present
a basic implementation of these concepts and eval-
uate the performance.
2 Motivation
Most existing Question Answering systems classify
new questions according to static ontologies. These
ontologies incorporate human knowledge about the
expected answer (e.g. date, location, person), an-
swer type granularity (e.g. date, year, century), and
very often semantic information about the question
type (e.g. birth date, discovery date, death date).
While effective to some degree, these ontologies
are still very small, and inconsistent. Considerable
manual effort is invested into building and maintain-
ing accurate ontologies even though answer types
are arguably not always disjoint and hierarchical in
nature (e.g. ?Where is the corpus callosum?? ex-
pects an answer that is both location and body part).
The most significant drawback is that ontologies
are not standard among systems, making individual
component evaluation very difficult and re-training
for new domains time-consuming.
2.1 Answer Modeling
The task of determining the answer type of a ques-
tion is usually considered a hard 1 decision prob-
lem: questions are classified according to an an-
swer ontology. The classification (location, per-
son?s name, etc) is usually made in the beginning
of the QA process and all subsequent efforts are
focused on finding answers of that particular type.
Several existing QA systems implement feedback
loops (Harabagiu et al, 2000) or full-fledged plan-
ning (Nyberg et al, 2003) to allow for potential an-
swer type re-classification.
However, most questions can have multiple an-
swer types as well as specific answer type distribu-
tions. The following questions can accommodate
answers of types: full date, year, and decade.
Question Answer
When did Glen lift off in Friendship7? Feb. 20, 1962
When did Glen join NASA? 1959
When did Glen have long hair? the fifties
However, it can be argued that date is the most
likely answer type to be observed for the first ques-
tion, year the most likely type for the second ques-
tion, and decade most likely for the third ques-
tion. In fact, although the three questions can be
answered by various temporal expressions, the dis-
tributions over these expressions are quite different.
Existing answer models do not usually account for
these distributions, even though there is a clear po-
tential for better answer extraction and more refined
answer scoring.
2.2 Document Retrieval
When faced with a new question, QA systems usu-
ally generate few, carefully expanded queries which
produce ranked lists of documents. The retrieval
step, which is very critical in the QA process,
does not take full advantage of context information.
However, similar questions with known answers do
share context information in the form of lexical and
structural features present in relevant documents.
For example all questions of the type ?When was
X born?? find their answers in documents which
often contain words such as ?native? or ?record?,
phrases such as ?gave birth to X?, and sometimes
even specific parse trees.
Most IR research in Question Answering is fo-
cused on improving query expansion and structur-
1the answer is classified into a single class instead of gener-
ating a probability distribution over answers
ing queries in order to take advantage of specific
document pre-processing. In addition to automatic
query expansion for QA (Yang et al, 2003), queries
are optimized to take advantage of expansion re-
sources and document sources. Very often, these
optimizations are performed offline, based on the
type of question being asked.
Several QA systems associate this type of infor-
mation with question ontologies: upon observing
questions of a certain type, specific lexical features
are sought in the retrieved documents. These fea-
tures are not always automatically learned in order
to be used in query generation. Moreover, systems
are highly dependent on specific ontologies and be-
come harder to re-train.
2.3 Answer Extraction
Given a set of relevant documents, the answer ex-
traction step consists of identifying snippets of text
or exact phrases that answer the question. Manual
approaches to answer extraction have been mod-
erately successful in the news domain. Regular
expressions, rule and pattern-based extraction are
among the most efficient techniques for information
extraction. However, because of the difficulty in ex-
tending them to additional types of questions, learn-
ing methods are becoming more prevalent.
Current systems (Ravichandran et al, 2003) al-
ready employ traditional information extraction and
machine learning for extracting answers from rel-
evant documents. Boundary detection techniques,
finite state transducers, and text passage classifica-
tion are a few methods that are usually applied to
this task.
The drawback shared by most statistical answer
extractors is their reliance on predefined ontologies.
They are often tailored to expected answer types and
require type-specific resources. Gazetteers, ency-
clopedias, and other resources are used to generate
type specific features.
3 Related Work
Current efforts in data acquisition for Question An-
swering are becoming more and more common.
(Girju et al, 2003) propose a supervised algorithm
for part-whole relations extraction. (Fleischman et
al., 2003) also propose a supervised algorithm that
uses part of speech patterns and a large corpus to ex-
tract semantic relations for Who-is type questions.
Such efforts promise to provide large and dense
datasets required by instance based approaches.
Several statistical approaches have proven to be
successful in answer extraction. The statistical
agent presented in (Chu-Carroll et al, 2003) uses
TestQuestion: WhendidJohnGlenstartworkingatNASA?
WhendidJayLenogetajobattheNBC?
WhendidColumbusarriveathisdestination?
?
Whendid<NNP+><VB>?at??
Whendid?SonybeginitsV AIOcampaign?
Whendid?T omRidgeinitiatetheterr oralertsystem?
?
Whendid<NNP+><SYNSETto_initiate>??
WhendidBeethovendie?
WhendidMuhammadlive?
?
Whendid<NNP><VB>?
WhendidtheRaiderswintheirlastgame?
WhendidEMNLP celebrateits5 th anniversary?
?
Whendid<NNP+>??
Whendiddinosaurswalktheearth?
Whendidpeoplediscoverfir e?
?
Whendid<NN><VB><NP>?
Whendid<NP>??
Figure 1: Neighboring questions are clustered according to features they share.
maximum entropy and models answer correctness
by introducing a hidden variable representing the
expected answer type. Large corpora such as the
Web can be mined for simple patterns (Ravichan-
dran et al, 2003) corresponding to individual ques-
tion types. These patterns are then applied to test
questions in order to extract answers. Other meth-
ods rely solely on answer redundancy (Dumais et
al., 2002): high performance retrieval engines and
large corpora contribute to the fact that the most re-
dundant entity is very often the correct answer.
Predictive annotation (Prager et al, 1999) is one
of the techniques that bring together corpus process-
ing and smarter queries. Twenty classes of objects
are identified and annotated in the corpus, and cor-
responding labels are used to enhance IR queries.
Along the same lines, (Agichtein et al, 2001) pro-
pose a method for learning query transformations
in order to improve answer retrieval. The method
involves learning phrase features for question clas-
sification. (Wen and Zhang, 2003) address the prob-
lem of query clustering based on semantic similar-
ity and analyze several applications such as query
re-formulation and index-term selection.
4 An Instance-Based Approach
This paper presents a data driven, instance-based
approach for Question Answering. We adopt the
view that strategies required in answering new ques-
tions can be directly learned from similar train-
ing examples (question-answer pairs). Consider
a multi-dimensional space, determined by features
extracted from training data. Each training question
is represented as a data point in this space. Features
can range from lexical n-grams to parse trees ele-
ments, depending on available processing.
Each test question is also projected onto the fea-
ture space. Its neighborhood consists of training
instances that share a number of features with the
new data point. Intuitively, each neighbor is similar
in some fashion to the new question. The obvious
next step would be to learn from the entire neigh-
borhood - similar to KNN classification. However,
due to the sparsity of the data and because different
groups of neighbors capture different aspects of the
test question, we choose to cluster the neighborhood
instead. Inside the neighborhood, we build individ-
ual clusters based on internal similarity. Figure 1
shows an example of neighborhood clustering. No-
tice that clusters may also have different granularity
- i.e. can share more or less features with the new
question.
Cluster1
Models
AnswerSet 1
Cluster2
Models
AnswerSet 2
Cluster3
Models
AnswerSet 3
Clusterk
Models
AnswerSet k
Neighborhood
Cluster2
Cluster3
Clusterk
NewQuestion
NET agging
POS
Parsing
Cluster1
Figure 2: The new question is projected onto the multi-
dimensional feature space. A set of neighborhood clus-
ters are identified and a model is dynamically built for
each of them. Each model is applied to the test question
in order to produce its own set of candidate answers.
By clustering the neighborhood, we set the stage
for supervised methods, provided the clusters are
sufficiently dense. The goal is to learn models that
explain individual clusters. A model explains the
data if it successfully answers questions from its
corresponding cluster. For each cluster, a mod-
els is constructed and tailored to the local data.
Models generating high confidence answers are ap-
plied to the new question to produce answer candi-
dates (Figure 2) Since the test question belongs to
multiple clusters, it benefits from different answer-
seeking strategies and different granularities.
Answering clusters of similar questions involves
several steps: learning the distribution of the
expected answer type, learning the structure and
content of queries, and learning how to extract the
answer. Although present in most systems, these
steps are often static, manually defined, or based on
limited resources (section 2). This paper proposes a
set of trainable, cluster-specific models:
1. the Answer Model Ai learns the cluster-specific
distribution of answer types.
2. the Query Content Model Ui is trained to enhance
the keyword-based queries with cluster-specific
content conducive to better document retrieval.
This model is orthogonal to query expansion.
3. the Extraction Model Ei is dynamically built
for answer candidate extraction, by classifying
snippets of text whether they contain a correct
answer or not.
AnswerModel
QueryContentModel
ExtractionModel
ClusterModels
Training
Samples
(Q, A)
Figure 3: Three cluster-specific components are learned
in order to better retrieve relevant documents, model the
expected answer, and then extract it from raw text. Local
question-answer pairs (Q,A) are used as training data.
These models are derived directly from cluster
data and collectively define a focused strategy for
finding answers to similar questions (Figure 3).
4.1 The Answer Model
Learning cluster-specific answer type distributions
is useful not only in terms of identifying answers
in running text but also in answer ranking. A prob-
abilistic approach has the advantage of postponing
answer type decisions from early in the QA process
until answer extraction or answer ranking. It also
has the advantage of allowing training data to shape
the expected structure of answers.
The answer modeling task consists of learning
specific answer type distributions for each cluster of
questions. Provided enough data, simple techniques
such as constructing finite state machines or learn-
ing regular expressions are sufficient. The principle
can also be applied to current answer ontologies by
replacing the hard classification with a distribution
over answer types.
For high-density clusters, the problem of learn-
ing the expected answer type is reduced to learn-
ing possible answer types and performing a reliable
frequency count. However, very often clusters are
sparse (e.g. are based on rare features) and a more
reliable method is required. k-nearest training data
points Q1..Qk can be used in order to estimate the
probability that the test question q will observe an
answer type ?j :
P (?j , q) = ? ?
k
?
i=0
P (?j |Qi) ? ?(q,Qi) (1)
where P (?j , Qi) is the probability of observing
an answer of type ?j when asking question Qi.
?(q,Qi) represents a distance function between q
and Qi, and ? is a normalizing factor over the set
of all viable answer types in the neighborhood of q.
4.2 The Query Content Model
Current Question Answering systems use IR in a
straight-forward fashion. Query terms are extracted
and then expanded using statistical and semantic
similarity measures. Documents are retrieved and
the top K are further processed. This approach de-
scribes the traditional IR task and does not take ad-
vantage of specific constraints, requirements, and
rich context available in the QA process.
The data-driven framework we propose takes ad-
vantage of knowledge available at retrieval time
and incorporates it to create better cluster-specific
queries. In addition to query expansion, the goal is
to learn content features: n-grams and paraphrases
(Hermjakob et al, 2002) which yield better queries
when added to simple keyword-based queries. The
Query Content Model is a cluster-specific collec-
tion of content features that generate the best docu-
ment set (Table 1).
Cluster: When did X start working for Y?
Simple Queries Query Content Model
X, Y ?X joined Y in?
X, Y start working ?X started working for Y?
X, Y ?start working? ?X was hired by Y?
... ?Y hired X?
X, Y ?job interview?
...
Table 1: Queries based only on X and Y question
terms may not be appropriate if the two entities share
a long history. A focused, cluster-specific content model
is likely to generate more precise queries.
For training, simple keyword-based queries are
run through a retrieval engine in order to produce
a set of potentially relevant documents. Features
(n-grams and paraphrases) are extracted and scored
based on their co-occurrence with the correct an-
swer. More specifically, consider a positive class:
documents which contain the correct answer, and a
negative class: documents which do not contain the
answer. We compute the average mutual informa-
tion I(C;Fi) between a class of a document, and
the absence or presence of a feature fi in the doc-
ument (McCallum and Nigam, 1998). We let C be
the class variable and Fi the feature variable:
I(C;Fi) = H(C) ? H(C|Fi)
=
?
c?C
?
fi?0,1
P (c, fi) log
P (c, fi)
P (c)P (fi)
where H(C) is the entropy of the class variable and
H(C|Fi) is the entropy of the class variable condi-
tioned on the feature variable. Features that best dis-
criminate passages containing correct answers from
those that do not, are selected as potential candi-
dates for enhancing keyword-based queries.
For each question-answer pair, we generate can-
didate queries by individually adding selected fea-
tures (e.g. table 1) to the expanded word-based
query. The resulting candidate queries are subse-
quently run through a retrieval engine and scored
based on the number of passages containing cor-
rect answers (precision). The content features found
in the top u candidate queries are included in the
Query Content Model.
The Content Model is cluster specific and not in-
stance specific. It does not replace traditional query
expansion - both methods can be applied simulta-
neously to the test questions: specific keywords are
the basis for traditional query expansion and clus-
ters of similar questions are the basis for learning
additional content conducive to better document re-
trieval. Through the Query Content Model we al-
low shared context to play a more significant role in
query generation.
4.3 The Extraction Model
During training, documents are retrieved for each
question cluster and a set of one-sentence passages
containing a minimum number of query terms is
selected. The passages are then transformed into
feature vectors to be used for classification. The
features consist of n-grams, paraphrases, distances
between keywords and potential answers, simple
statistics such as document and sentence length, part
of speech features such as required verbs etc. More
extensive sets of features can be found in informa-
tion extraction literature (Bikel et al, 1999).
Under our data-driven approach, answer extrac-
tion consists of deciding the correctness of candi-
date passages. The task is to build a model that
accepts snippets of text and decides whether they
contain a correct answer.
A classifier is trained for each question cluster.
When new question instances arrive, the already
trained cluster-specific models are applied to new,
relevant text snippets in order to test for correctness.
We will refer to the resulting classifier scores as an-
swer confidence scores.
5 Experiments
We present a basic implementation of the instance-
based approach. The resulting QA system is fully
automatically trained, without human intervention.
Instance-based approaches are known to require
large, dense training datasets which are currently
under development. Although still sparse, the
subset of all temporal questions from the TREC
9-12 (Voorhees, 2003) datasets is relatively dense
compared to the rest of the question space. This
makes it a good candidate for evaluating our
instance-based QA approach until larger and denser
datasets become available. It is also broad enough
to include different question structures and varying
degrees of difficulty and complexity such as:
? ?When did Beethoven die??
? ?How long is a quarter in an NBA game??
? ?What year did General Montgomery lead the Allies
to a victory over the Axis troops in North Africa??
The 296 temporal questions and their correspond-
ing answer patterns provided by NIST were used
in our experiments. The questions were processed
with a part of speech tagger (Brill, 1994) and a
parser (Collins, 1999).
The questions were clustered using template-
style frames that incorporate lexical items, parser
labels, and surface form flags (Figure 1). Consider
the following question and several of its corre-
sponding frames:
?When did Beethoven die??
when did <NNP> die
when did <NNP> <VB>
when did <NNP> <Q>
when did <NP> <Q>
when did <Q>
where <NNP>,<NP>,<VB>,<Q> denote:
proper noun, noun phrase, verb, and generic ques-
tion term sequence, respectively. Initially, frames
are generated exhaustively for each question. Each
frame that applies to more than three questions is
then selected to represent a specific cluster.
One hundred documents were retrieved
for each query through the Google API
(www.google.com/api). Documents containing
the full question, question number, references to
TREC, NIST, AQUAINT, Question Answering and
other similar problematic content were filtered out.
When building the Query Content Model
keyword-based queries were initially formulated
and expanded. From the retrieved documents a set
of content features (n-grams and paraphrases) were
selected through average mutual information. The
features were added to the simple queries and a
new set of documents was retrieved. The enhanced
queries were scored and the corresponding top 10 n-
grams/paraphrases were included in the Query Con-
tent Model. The maximum n-gram and paraphrase
size for these features was set to 6 words.
The Extraction Model uses a support vector ma-
chine (SVM) classifier (Joachims, 2002) with a lin-
ear kernel. The task of the classifier is to decide if
text snippets contain a correct answer. The SVM
was trained on features extracted from one-sentence
passages containing at least one keyword from the
original question. The features consist of: distance
between keywords and potential answers, keyword
density in a passage, simple statistics such as doc-
ument and sentence length, query type, lexical n-
grams (up to 6-grams), and paraphrases.
We performed experiments using leave-one-out
cross validation. The system was trained and tested
without any question filtering or manual input. Each
cluster produced an answer set with correspond-
ing scores. Top 5 answers for each instance were
considered by a mean reciprocal rank (MRR) met-
ric over all N questions: MRRN =
?N
i=0
1
ranki ,
where ranki refers to the first correct occurrence in
the top 5 answers for question i. While not the fo-
cus of this paper, answer clustering algorithms are
likely to further improve performance.
6 Results
The most important step in our instance-based ap-
proach is identifying clusters of questions. Figure
4 shows the question distribution in terms of num-
ber of clusters. For example: 30 questions belong
to exactly 3 clusters. The number of clusters cor-
responding to a question can be seen as a measure
of how common the question is - the more clusters
a question has, the more likely it is to have a dense
neighborhood.
The resulting MRR is 0.447 and 61.5% ques-
tions have correct answers among the first five pro-
posed answers. This translates into results consis-
tently above the sixth highest score at each TREC
9-12. Our results were compared directly to the top
performing systems? results on the same temporal
2 3 4 5 6 7 8 9 larger
0
10
20
30
40
50
60
70
80
 Question Distribution With Number of Clusters
# clusters
# 
qu
es
tio
ns
(avg) 
Figure 4: Question distribution - each bar shows the
number of questions that belong to exactly c clusters.
1 2 3 4 5 6 7 8
0
10
20
30
40
50
60
70
80
Cluster Contribution to Top 10 Answers
# clusters
# 
qu
es
tio
ns
Figure 5: Number of clusters that contribute with cor-
rect answers to the final answer set - only the top 10 an-
swers were considered for each question.
question test set.
Figure 5 shows the degree to which clusters pro-
duce correct answers to test questions. Very often,
more than one cluster contributes to the final answer
set, which suggests that there is a benefit in cluster-
ing the neighborhood according to different similar-
ity features and granularity.
It is not surprising that cluster size is not cor-
related with performance (Figure 6). The overall
strategy learned from the cluster ?When did <NP>
die?? corresponds to an MRR of 0.79, while the
strategy learned from cluster ?How <Q>?? corre-
sponds to an MRR of 0.13. Even if the two clusters
generate strategies with radically different perfor-
mance, they have the same size - 10 questions are
covered by each cluster.
Figure 7 shows that performance is correlated
with answer confidence scores. The higher the con-
fidence threshold the higher the precision (MRR)
of the predicted answers. When small, unstable
clusters are ignored, the predicted MRR improves
considerably. Small clusters tend to produce unsta-
0 20 40 60 80 100 120
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Performance And Cluster Size
cluster size
M
R
R
Figure 6: Since training data is not uniformly distributed
in the feature space, cluster size is not well correlated
with performance. A specific cardinality may represent a
small and dense part cluster, or a large and sparse cluster.
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Performance And Confidence Thresholds
confidence threshold
M
R
R
Cardinality 2+
Cardinality 3+
Cardinality 4+
Cardinality 5+
Figure 7: MRR of predicted answers varies with answer
confidence thresholds. There is a tradeoff between confi-
dence threshold and MRR . The curves represent differ-
ent thresholds for minimum cluster size.
ble strategies and have extremely low performance.
Often times structurally different but semantically
equivalent clusters have a higher cardinality and
much better performance. For example, the cluster
?What year did <NP> die?? has cardinality 2 and
a corresponding MRR of zero. However, as seen
previously, the cluster ?When did <NP> die?? has
cardinality 10 and a corresponding MRR of 0.79.
Table 2 presents an intuitive cluster and the top n-
grams and paraphrases with most information con-
tent. Each feature has also a corresponding average
mutual information score. These particular content
features are intuitive and highly indicative of a cor-
rect answer. However, in sparse clusters, the con-
tent features have less information content and are
more vague. For example, the very sparse cluster
?When was <Q>?? yields content features such as
?April?, ?May?, ?in the spring of?, ?back in? which
only suggest broad temporal expressions.
Cluster: When did <QTERM> die?
N-grams Paraphrases
0.81 his death in 0.80 <Q> died in
0.78 died on 0.78 <Q> died
0.77 died in 0.68 <Q> died on
0.75 death in 0.58 <Q> died at
0.73 of death 0.38 <Q> , who died
0.69 to his death 0.38 <Q> dies
0.66 died 0.38 <Q> died at the age of
0.63 , born on 0.38 <Q> , born
0.63 date of death 0.35 <Q> ?s death on
Table 2: Query Content Model: learning n-grams and
paraphrases for class ?When did <NP> die??, where
<Q> refers to a phrase in the original question.
7 Conclusions
This paper presents an principled, statistically
based, instance-based approach to Question An-
swering. Strategies and models required for answer-
ing new questions are directly learned from training
data. Since training requires very little human ef-
fort, relevant context, high information query con-
tent, and extraction are constantly improved with
the addition of more question-answer pairs.
Training data is a critical resource for this ap-
proach - clusters with very few data points are not
likely to generate accurate models. However, re-
search efforts involving data acquisition are promis-
ing to deliver larger datasets in the near future and
solve this problem. We present an implementation
of the instance-based QA approach and we eval-
uate it on temporal questions. The dataset is of
reasonable size and complexity, and is sufficiently
dense for applying instance-based methods. We per-
formed leave-one-out cross validation experiments
and obtained an overall mean reciprocal rank of
0.447. 61.5% of questions obtained correct answers
among the top five which is equivalent to a score in
the top six TREC systems on the same test set.
The experiments show that strategies derived
from very small clusters are noisy and unstable.
When larger clusters are involved, answer confi-
dence becomes correlated with higher predictive
performance. Moreover, when ignoring sparse data,
answering strategies tend to be more stable. This
supports the need for more training data as means to
improve the overall performance of the data driven,
instance based approach to question answering.
8 Current & Future Work
Data is the single most important resource for
instance-based approaches. Currently we are ex-
ploring large-scale data acquisition methods that
can provide the necessary training data density for
most question types, as well as the use of trivia
questions in the training process.
Our data-driven approach to Question Answering
has the advantage of incorporating learning com-
ponents. It is very easy to train and makes use of
very few resources. This property suggests that lit-
tle effort is required to re-train the system for dif-
ferent domains as well as other languages. We plan
to apply instance-based QA to European languages
and test this hypothesis using training data acquired
through unsupervised means.
More effort is required in order to better integrate
the cluster-specific models. Strategy overlap analy-
sis and refinement of local optimization criteria has
the potential to improve overall performance under
time constraints.
References
E. Agichtein, S. Lawrence, and L. Gravano. 2001.
Learning search engine specific query transfor-
mations for question answering. WWW.
D. Bikel, R. Schwartz, and R. Weischedel. 1999.
An algorithm that learns what?s in a name. Ma-
chine Learning.
E. Brill. 1994. Some advances in rule-based part of
speech tagging. AAAI.
J. Burger, L. Ferro, W. Greiff, J. Henderson,
M. Light, and S. Mardis. 2002. Mitre?s qanda
at trec-11. TREC.
J. Chu-Carroll, K. Czuba, J. Prager, and A. Itty-
cheriah. 2003. In question answering, two heads
are better than one. HLT-NAACL.
C. Clarke, G. Cormack, G. Kemkes, M. Laszlo,
T. Lynam, E. Terra, and P. Tilker. 2003. Statis-
tical selection of exact answers. TREC.
M. Collins. 1999. Head-driven statistical models
for natural language parsing. Ph.D. Disertation.
S. Dumais, M. Banko, E. Brill, J. Lin, and A. Ng.
2002. Web question answering: Is more always
better? SIGIR.
A. Echihabi and D. Marcu. 2003. A noisy channel
approach to question answering. ACL.
M. Fleischman, E. Hovy, and A. Echihabi. 2003.
Offline strategies for online question answering:
Answering questions before they are asked. ACL.
R. Girju, D. Moldovan, and A. Badulescu. 2003.
Learning semantic constraints for the automatic
discovery of part-whole relations. HLT-NAACL.
S. Harabagiu, D. Moldovan, M. Pasca, R. Mihalcea,
R. Bunescu, R. Girju, V. Rus, and P. Morarescu.
2000. Falcon: Boosting knowledge for answer
engines. TREC.
U. Hermjakob, E. Hovy, and C. Lin. 2000.
Knowledge-based question answering. TREC.
Ulf Hermjakob, Abdessamad Echihabi, and Daniel
Marcu. 2002. Natural language based reformu-
lation resource and web exploitation for question
answering. TREC.
E. Hovy, L. Gerber, U. Hermjakob, M. Junk, and
C.Y. Lin. 2000. Question answering in webclo-
pedia. TREC.
E. Hovy, U. Hermjakob, C. Lin, and D. Ravichan-
dran. 2002. Using knowledge to facilitate factoid
answer pinpointing. COLING.
T. Joachims. 2002. Learning to classify text using
support vector machines. Disertation.
B. Magnini, S. Romagnoli, A. Vallin, J. Herrera,
A. Penas, V. Peiado, F. Verdejo, and M. de Rijke.
2003. The multiple language question answering
track at clef 2003. CLEF.
A. McCallum and K. Nigam. 1998. A comparison
of event models for naive bayes text classifica-
tion. AAAI, Workshop on Learning for Text Cate-
gorization.
D. Moldovan, S. Harabagiu, M. Pasca, R. Mihalcea,
R. Girju, R. Goodrum, and V. Rus. 2000. The
structure and performance of an open-domain
question answering system. ACL.
D. Moldovan, D. Clark, S. Harabagiu, and S. Maio-
rano. 2003. Cogex: A logic prover for question
answering. ACL.
E. Nyberg, T. Mitamura, J. Callan, J. Carbonell,
R. Frederking, K. Collins-Thompson, L. Hiyaku-
moto, Y. Huang, C. Huttenhower, S. Judy, J. Ko,
A. Kupsc, L.V. Lita, V. Pedro, D. Svoboda, and
B. Vand Durme. 2003. A multi strategy approach
with dynamic planning. TREC.
J. Prager, D. Radev, E. Brown, A. Coden, and
V. Samn. 1999. The use of predictive annotation
for question answering in trec8. TREC.
D. Ravichandran, A. Ittycheriah, and S. Roukos.
2003. Automatic derivation of surface text pat-
terns for a maximum entropy based question an-
swering system. HLT-NAACL.
E.M. Voorhees. 2003. Overview of the trec 2003
question answering track. TREC.
J.R. Wen and H.J. Zhang. 2003. Query clustering
in the web context. IR and Clustering.
J. Xu, A. Licuanan, and R. Weischedel. 2003. Trec
2003 qa at bbn: Answering definitional ques-
tions. TREC.
H. Yang, T.S. Chua, S. Wang, and C.K. Koh. 2003.
Structured use of external knowledge for event-
based open domain question answering. SIGIR.
P. Zweigenbaum. 2003. Question answering in
biomedicine. EACL.



Cluster-Based Query Expansion for Statistical Question Answering
Lucian Vlad Lita ?
Siemens Medical Solutions
lucian.lita@siemens.com
Jaime Carbonell
Carnegie Mellon University
jgc@cs.cmu.edu
Abstract
Document retrieval is a critical component
of question answering (QA), yet little work
has been done towards statistical modeling
of queries and towards automatic generation
of high quality query content for QA. This
paper introduces a new, cluster-based query
expansion method that learns queries known
to be successful when applied to similar
questions. We show that cluster-based ex-
pansion improves the retrieval performance
of a statistical question answering system
when used in addition to existing query ex-
pansion methods. This paper presents exper-
iments with several feature selection meth-
ods used individually and in combination.
We show that documents retrieved using the
cluster-based approach are inherently differ-
ent than documents retrieved using existing
methods and provide a higher data diversity
to answers extractors.
1 Introduction
Information retrieval has received sporadic exam-
ination in the context of question answering (QA).
Over the past several years, research efforts have in-
vestigated retrieval quality in very controlled scenar-
ios under the question answering task. At a first
glance, document and passage retrieval is reason-
able when considering the fact that its performance
is often above 80% for this stage in the question
answering process. However, most often, perfor-
mance is measured in terms of the presence of at
? work done at Carnegie Mellon
least one relevant document in the retrieved docu-
ment set, regardless of relevant document density ?
where a document is relevant if it contains at least
one correct answer. More specifically, the retrieval
stage is considered successful even if there is a sin-
gle document retrieved that mentions a correct an-
swer, regardless of context. This performance mea-
sure is usually not realistic and revealing in question
answering.
In typical scenarios, information extraction is not
always able to identify correct answers in free text.
When successfully found, correct answers are not
always assigned sufficiently high confidence scores
to ensure their high ranks in the final answer set.
As a result, overall question answering scores are
still suffering and considerable effort is being di-
rected towards improving answer extraction and an-
swer merging, yet little attention is being directed
towards retrieval.
A closer look at retrieval in QA shows that the
types of documents retrieved are not always con-
ducive to correct answers given existing extraction
methods. It is not sufficient to retrieve a relevant
document if the answer is difficult to extract from its
context. Moreover, the retrieval techniques are often
very simple, consisting of extracting keywords from
questions, expanding them using conventional meth-
ods such as synonym expansion and inflectional ex-
pansion, and then running the queries through a re-
trieval engine.
In order to improve overall question answering
performance, additional documents and better doc-
uments need to be retrieved. More explicitly, infor-
mation retrieval needs to: a) generate query types
and query content that is designed to be successful
(high precision) for individual questions and b) en-
426
sure that the documents retrieved by the new queries
are different than the documents retrieved using con-
ventional methods. By improving retrieval along
these dimensions, we provide QA systems with ad-
ditional new documents, increasing the diversity and
the likelihood of extracting correct answers. In this
paper, we present a cluster-based method for ex-
panding queries with new content learned from the
process of answering similar questions. The new
queries are very different from existing content since
they are not based on the question being answered,
but on content learned from other questions.
1.1 Related Work
Experiments using the CMU Javelin (Collins-
Thompson et al, 2004) and Waterloo?s MultiText
(Clarke et al, 2002) question answering systems
corroborate the expected direct correlation between
improved document retrieval performance and QA
accuracy across systems. Effectiveness of the re-
trieval component was measured using question cov-
erage ? number of questions with at least one rele-
vant document retrieved ? and mean average preci-
sion. Results suggest that retrieval methods adapted
for question answering which include question anal-
ysis performed better than ad-hoc IR methods which
supports previous findings (Monz, 2003).
In question answering, queries are often ambigu-
ous since they are directly derived from the ques-
tion keywords. Such query ambiguity has been ad-
dressed in previous research (Raghavan and Allan,
2002) by extracting part of speech patterns and con-
structing clarification queries. Patterns are mapped
into manually generated clarification questions and
presented to the user. The results using the clarity
(Croft et al, 2001) statistical measure suggest that
query ambiguity is often reduced by using clarifica-
tion queries which produce a focused set of docu-
ments.
Another research direction that tailors the IR com-
ponent to question answering systems focuses on
query formulation and query expansion (Woods et
al., 2001). Taxonomic conceptual indexing system
based on morphological, syntactic, and semantic
features can be used to expand queries with inflected
forms, hypernyms, and semantically related terms.
In subsequent research (Bilotti et al, 2004), stem-
ming is compared to query expansion using inflec-
tional variants. On a particular question answering
controlled dataset, results show that expansion us-
ing inflectional variants produces higher recall than
stemming.
Recently (Riezler et al, 2007) used statistical ma-
chine translation for query expansion and took a step
towards bridging the lexical gap between questions
and answers. In (Terra et al, 2005) query expansion
is studied using lexical affinities with different query
formulation strategies for passage retrieval. When
evaluated on TREC datasets, the affinity replace-
ment method obtained significant improvements in
precision, but did not outperform other methods in
terms of recall.
2 Cluster-Based Retrieval for QA
In order to explore retrieval under question answer-
ing, we employ a statistical system (SQA) that
achieves good factoid performance on the TREC
QA task: for ? 50% of the questions a correct an-
swer is in the top highest confidence answer. Rather
than manually defining a complete answering strat-
egy ? the type of question, the queries to be run, the
answer extraction, and the answer merging meth-
ods ? for each type of question, SQA learns dif-
ferent strategies for different types of similar ques-
tions SQA takes advantage of similarity in training
data (questions and answers from past TREC evalua-
tions), and performs question clustering. Two meth-
ods are employed constraint-based clustering and
EM with similar performance. The features used
by SQA clustering are surface-form n-grams as well
as part of speech n-grams extracted from questions.
However, any clustering method can be employed in
conjunction with the methods presented in this pa-
per.
The questions in each cluster are similar in some
respect (i.e. surface form and syntax), SQA uses
them to learn a complete answering strategy. For
each cluster of training questions, SQA learns an an-
swering strategy. New questions may fall in more
than one cluster, so multiple answering strategies at-
tempt simultaneously to answer it.
In this paper we do not cover a particular ques-
tion answering system such as SQA and we do not
examine the whole QA process. We instead focus
on improving retrieval performance using a set of
427
similar questions. The methods presented here can
generalize when similar training questions are avail-
able. Since in our experiments we employ a cluster-
based QA system, we use individual clusters of simi-
lar questions as local training data for learning better
queries.
2.1 Expansion Using Individual Questions
Most existing question answering systems use IR in
a simple, straight-forward fashion: query terms are
extracted online from the test question and used to
construct basic queries. These queries are then ex-
panded from the original keyword set using statisti-
cal methods, semantic, and morphological process-
ing. Using these enhanced queries, documents (or
passages) are retrieved and the top K are further
processed. This approach describes the traditional
IR task and does not take advantage of specific con-
straints, requirements, and rich context available in
the QA process. Pseudo-relevance feedback is often
used in question answering in order to improve the
chances of retrieving relevant documents. In web-
based QA, often systems rely on retrieval engines
to perform the keyword expansion. Some question
answering systems associate additional predefined
structure or content based on the question classifi-
cation. However, there this query enhancement pro-
cess is static and does not use the training data and
the question answering context differently for indi-
vidual questions.
Typical question answering queries used in docu-
ment or passage retrieval are constructed using mor-
phological and semantic variations of the content
words in the question. However, these expanded
queries do not benefit from the underlying structure
of the question, nor do they benefit from available
training data, which provides similar questions that
we already know how to answer.
2.2 Expansion Based on Similar Questions
We introduce cluster-based query expansion
(CBQE), a new task-oriented method for query ex-
pansion that is complementary to existing strategies
and that leads to different documents which contain
correct answers. Our approach goes beyond single
question-based methods and takes advantage of
high-level correlations that appear in the retrieval
process for similar questions.
The central idea is to cluster available training
questions and their known correct answers in or-
der to exploit the commonalities in the retrieval pro-
cess. From each cluster of similar questions we
learn a different, shared query content that is used
in retrieving relevant documents - documents that
contain correct answers. This method leverages
the fact that answers to similar questions tend to
share contextual features that can be used to enhance
keyword-based queries. Experiments with question
answering data show that our expanded queries in-
clude a different type of content compared to and
in addition to existing methods. These queries have
training question clusters as a source for expansion
rather than an individual test question. We show that
CBQE is conducive to the retrieval of relevant doc-
uments, different than the documents that can be re-
trieved using existing methods.
We take advantage of the fact that for similar
training questions, good IR queries are likely to
share structure and content features. Such features
can be learned from training data and can then be
applied to new similar questions. Note that some of
these features cannot be generated through simple
query expansion, which does not takes advantage of
successful queries for training questions. Features
that generate the best performing queries across an
entire cluster are then included in a cluster-specific
feature set, which we will refer to as the query con-
tent model.
While pseudo-relevance feedback is performed
on-line for each test question, cluster-based rel-
evance feedback is performed across all training
questions in each individual cluster. Relevance feed-
back is possible for training data, since correct an-
swers are already known and therefore document
relevance can be automatically and accurately as-
sessed.
Algorithm 1 shows how to learn a query content
model for each individual cluster, in particular: how
to generate queries enhanced with cluster-specific
content, how to select the best performing queries,
and how to construct the query content model to be
used on-line.
Initially, simple keyword-based queries are for-
mulated using words and phrases extracted directly
from the free question keywords that do not appear
in the cluster definition. The keyword queries are
428
Algorithm 1 Cluster-based relevance feedback algorithm for
retrieval in question answering
1: extract keywords from training questions in a cluster and
build keyword-based queries; apply traditional query ex-
pansion methods
2: for all keyword-based query do
3: retrieve an initial set of documents
4: end for
5: classify documents into relevant and non-relevant
6: select top k most discriminative features (e.g. n-grams,
paraphrases) from retrieved documents (across all training
questions).
7: use the top k selected features to enhance keyword-based
queries ? adding one feature at a time (k new queries)
8: for all enhanced queries do
9: retrieve a second set of documents
10: end for
11: classify documents into relevant and non-relevant based
12: score enhanced queries according to relevant document
density
13: include in the query content model the top h features whose
corresponding enhanced queries performed best across all
training questions in the cluster ? up to 20 queries in our
implementation
then subjected to frequently used forms of query ex-
pansion such as inflectional variant expansion and
semantic expansion (table ??). Further process-
ing depends on the available and desired process-
ing tools and may generate variations of the origi-
nal queries: morphological analysis, part of speech
tagging, syntactic parsing. Synonym and hypernym
expansion and corpus-based techniques can be em-
ployed as part of the query expansion process, which
has been extensively studied (Bilotti et al, 2004).
The cluster-based query expansion has the advan-
tage of being orthogonal to traditional query expan-
sion and can be used in addition to pseudo-relevance
feedback. CBQE is based on context shared by sim-
ilar training questions in each cluster, rather than on
individual question keywords. Since cluster-based
expansion relies on different features compared to
traditional expansion, it leads to new relevant doc-
uments, different from the ones retrieved using the
existing expansion techniques.
3 The Query Content Model
Simple queries are run through a retrieval engine in
order to produce a set of potentially relevant docu-
ments. While this step may produce relevant doc-
uments, we would like to construct more focused
queries, likely to retrieve documents with correct an-
swers and appropriate contexts. The goal is to add
query content that increases retrieval performance
on training questions. Towards this end, we evaluate
the discriminative power of features (n-grams and
paraphrases), and select the ones positively corre-
lated with relevant documents and negatively corre-
lated with non-relevant documents. The goal of this
approach is to retrieve documents containing simple,
high precision answer extraction patterns. Features
Cluster: When did X start working for Y?
Simple Queries Query Content Model
X, Y ?X joined Y in?
X, Y, start, working ?X started working for Y?
X, Y, ?start working? ?X was hired by Y?
X, Y, working ?Y hired X?
. . . X, Y, ?job interview?
. . .
Table 1: Sample cluster-based expansion features
that best discriminate passages containing correct
answers from those that do not, are selected as
potential candidates for enhancing keyword-based
queries. For each question-answer pair, we gener-
ate enhanced queries by individually adding selected
features (e.g. Table 1) to simple queries. The result-
ing queries are subsequently run through a retrieval
engine and scored using the measure of choice (e.g.
average precision). The content features used to
construct the top h features and corresponding en-
hanced queries are included in the query content
model.
The query content model is a collection of fea-
tures used to enhance the content of queries which
are successful across a range of similar questions
(Table 1). The collection is cluster specific and not
question specific - i.e. features are derived from
training data and enhanced queries are scored us-
ing training question answer pairs. Building a query
content model does not preclude traditional query
expansion. Through the query content model we al-
low shared context to play a more significant role in
query generation.
4 Experiments With Cluster-Based
Retrieval
We tested the performance of cluster-based con-
tent enhanced queries and compared it to the per-
429
formance of simple keyword-based queries and to
the performance of queries expanded through syn-
onyms and inflectional variants. We also experiment
with several feature selection methods for identify-
ing content features conducive to successful queries.
These experiments were performed with a web-
based QA system which uses the Google API for
document retrieval and a constraint-based approach
for question clustering. Using this system we
retrieved ?300, 000 and built a document set of
?10GB. For each new question, we identify train-
ing questions that share a minimum surface struc-
ture (e.g. a size 3 skip-ngram in common) which
we consider to be the prototype of a loose cluster.
Each cluster represents a different, implicit notion of
question similarity based on the set of training ques-
tions it covers. Therefore different clusters lead to
different retrieval strategies. These retrieval experi-
ments are restricted to using only clusters of size 4 or
higher to ensure sufficient training data for learning
queries from individual clusters. All experiments
were performed using leave-one-out cross valida-
tion.
For evaluating the entire statistical question an-
swering system, we used all questions from TREC8-
12. One of the well-known problems in QA consists
of questions having several unknown correct an-
swers with multiple answer forms ? different ways
of expressing the same answer. Since we are lim-
ited to a set of answer keys, we avoid the this prob-
lem by using all temporal questions from this dataset
for evaluating individual stages in the QA process
(i.e. retrieval) and for comparing different expan-
sion methods. These questions have the advantage
of having a more restrictive set of possible answer
surface forms, which lead to a more accurate mea-
sure of retrieval performance. At the same time they
cover both more difficult questions such as ?When
was General Manuel Noriega ousted as the leader
of Panama and turned over to U.S. authorities??
as well as simpler questions such as ?What year
did Montana become a state??. We employed this
dataset for an in-depth analysis of retrieval perfor-
mance.
We generated four sets of queries and we tested
their performance. We are interested in observ-
ing to what extent different methods produce addi-
tional relevant documents. The initial set of queries
are constructed by simply using a bag-of-words ap-
proach on the question keywords. These queries
are run through the retrieval engine, each generating
100 documents. The second set of queries builds on
the first set, expanding them using synonyms. Each
word and potential phrase is expanded using syn-
onyms extracted from WordNet synsets. For each
enhanced query generated, 100 documents are re-
trieved. To construct the third set of queries, we ex-
pand the queries in the first two sets using inflec-
tional variants of all the content words (e.g. verb
conjugations and noun pluralization (Bilotti et al,
2004)). For each of these queries we also retrieve
100 documents.
When text corpora are indexed without using
stemming, simple queries are expanded to include
morphological variations of keywords to improve re-
trieval and extraction performance. Inflectional vari-
ants include different pluralizations for nouns (e.g.
report, reports) and different conjugations for verbs
(e.g. imagine, imagines, imagined, imagining). Un-
der local corpus retrieval inflectional expansion by-
passes the unrelated term conflation problem that
stemmers tend to have, but at the same time, recall
might be lowered if not all related words with the
same root are considered. For a web-based question
answering system, the type of retrieval depends on
the search-engine assumptions, permissible query
structure, query size limitation, and search engine
bandwidth (allowable volume of queries per time).
By using inflectional expansion with queries that tar-
get web search engines, the redundancy for support-
ing different word variants is higher, and has the
potential to increase answer extraction performance.
Finally, in addition to the previous expansion meth-
ods, we employ our cluster-based query expansion
method. These queries incorporate the top most
discriminative ngrams and paraphrases (section 4.1)
learned from the training questions covered by the
same cluster. Instead of further building an expan-
sion using the original question keywords, we ex-
pand using contextual features that co-occur with
answers in free text. For all the training ques-
tions in a cluster, we gather statistics about the co-
occurrence of answers and potentially beneficial fea-
tures. These statistics are then used to select the best
features and apply them to new questions whose an-
swers are unknown. Figure 1 shows that approx-
430
Figure 1: Cumulative effect of expansion methods
imately 90% of the questions consistently benefit
from cluster-based query expansion when compared
to approximately 75% of the questions when em-
ploying the other methods combined. Each question
can be found in multiple clusters of different reso-
lution. Since different clusters may lead to differ-
ent selected features, questions benefit from multi-
ple strategies and even though one cluster-specific
strategy cannot produce relevant documents, other
cluster-specific strategies may be able to.
The cluster-based expansion method can generate
a large number of contextual features. When com-
paring feature selection methods, we only select the
top 10 features from each method and use them to
enhance existing question-based queries. Further-
more, in order to retrieve, process, extract, and score
a manageable number of documents, we limited the
retrieval to 10 documents for each query. In Fig-
ure 1 we observe that even as the other methods
retrieve more documents, ? 90% of the questions
still benefit from the cluster-based method. In other
words, the cluster-based method generates queries
using a different type of content and in turn, these
queries retrieve a different set documents than the
other methods. This observation is true even if we
continue to retrieve up to 100 documents for sim-
ple queries, synonym-expanded queries, and inflec-
tional variants-expanded queries.
This result is very encouraging since it suggests
that the answer extraction components of ques-
tion answering systems are exposed to a different
type of relevant documents, previously inaccessible
to them. Through these new relevant documents,
cluster-based query expansion has the potential to
provide answer extraction with richer and more var-
ied sources of correct answers for 90% of the ques-
tions.
new relevant documents
simple 4.43 100%
synonyms 1.48 33.4%
inflect 2.37 53.43%
cluster 1.05 23.65%
all 9.33 210.45%
all - synonyms 7.88 177.69%
all - inflect 6.99 157.69%
all - cluster 8.28 186.80%
Table 2: Keyword-based (?simple?), synonym, inflectional
variant, and cluster-based expansion. Average number of new
relevant documents across instances at 20 documents retrieved.
Although expansion methods generate additional
relevant documents that simpler methods cannot ob-
tain, an important metric to consider is the den-
sity of these new relevant documents. We are in-
terested in the number/percentage of new relevant
documents that expansion methods contribute with.
Table 2 shows at retrieval level of twenty docu-
ments how different query generation methods per-
form. We consider keyword based methods to be the
baseline and add synonym expanded queries (?syn-
onym?), inflectional variants expanded queries (?in-
flect?) which build upon the previous two types of
queries, and finally the cluster enhanced queries
(?cluster?) which contain features learned from train-
ing data. We see that inflectional variants have
the most impact on the number of new documents
added, although synonym expansion and cluster-
based expansion also contribute significantly.
4.1 Feature Selection for CBQE
Content features are learned from the training data
based on observing their co-occurrences with cor-
rect answers. In order to find the most appropri-
ate content features to enhance our cluster-specific
queries, we have experimented with several feature
selection methods (Yang and Pederson, 1997): in-
formation gain, chi-square, and scaled chi-square
(phi). Information gain (IG) measures the reduction
in entropy for the pre presence/absence of an answer
in relevant passages, given an n-gram feature. Chi-
square (?2) is a non-parametric measure of associa-
431
tion that quantifies the passage-level association be-
tween n-gram features and correct answers.
Given any of the above methods, individual n-
gram scores are combined at the cluster level by av-
eraging over individual questions in the cluster. In
figure 2 we compare these feature selection meth-
ods on our dataset. The selected features are used to
enhance queries and retrieve additional documents.
We measure the fraction of question instances for
which enhanced queries obtain at least one new rel-
evant document. The comparison is made with the
document set generated by keyword-based queries,
synonym expansion, and inflectional variant expan-
sion. We also include in our comparison the com-
bination of all feature selection methods (?All?). In
0 20 40 60 80 100
0.5
0.55
0.6
0.65
0.7
0.75
Instances With Additional Relevant Documents
#docs retrieved
fra
ct
io
n 
of
 in
st
an
ce
s
 
 
All
Prec
IGain
Phi
Chi2
Figure 2: Selection methods for cluster-based expansion
this experiment, average precision on training data
proves to be the best predictor of additional relevant
documents: ?71% of the test questions benefit from
queries based on average precision feature selection.
However, the other feature selection methods also
obtain a high performance, benefiting ?68% of the
test question instances.
Since these feature selection methods have differ-
ent biases, we expect to observe a boost in perfor-
mance (73%) from merging their feature sets (Fig-
ure 2). In this case there is a trade-off between
a 2% boost in performance and an almost double
set of features and enhanced queries. This trans-
lates into more queries and more documents to be
processed. Although it is not the focus of this re-
search, we note that a clever implementation could
incrementally add features from the next best selec-
tion method only after the existing queries and doc-
uments have been processed. This approach lends
itself to be a good basis for utility-based models
and planning (Hiyakumoto et al, 2005). We in-
0.3 0.4 0.5 0.6 0.7
0.4
0.5
0.6
0.7
Cluster Enhanced Queries
feature selection score (train)
a
ve
ra
ge
 p
re
cis
io
n 
(re
trie
va
l)
 
 
Precision at   1
Precision at   5
Precision at 10
Figure 3: Average precision of cluster enhanced queries
vestigate to what extent the scores of the selected
features are meaningful and correlate with actual re-
trieval performance on test data by measuring the
average precision of these queries at different num-
ber of documents retrieved. Figure 3 shows preci-
sion at one, five, and ten documents retrieved. We
observe that feature scores correlate well with ac-
tual retrieval performance, a result confirmed by all
three retrieval levels, suggesting that useful features
learned. The average precision also increases with
more documents retrieved, which is a desirable qual-
ity in question answering.
4.2 Qualitative Results
The cluster-based relevance feedback process can be
used to discover several artifacts useful in question
answering. For several of the clusters, we observe
that the feature selection process consistently and
with high confidence selected features such as ?noun
NP1 has one meaning? where NP1 is the first noun
phrase in the question. The goal is to add such fea-
tures to the keyword-based queries to retrieve high
precision documents. Note that our example, NP1
would be different for different test questions.
The indirect reason for selecting such features is
in fact the discovery of authorities: websites that fol-
low a particular format and which have a particular
type of information, relevant to a cluster. In the ex-
ample above, the websites answers.com and word-
net.princeton.edu consistently included answers to
clusters relevant to a person?s biography. Simi-
larly, wikipedia.org often provides answers to def-
initional questions (e.g. ?what is uzo??). By includ-
432
ing non-intuitive phrases, the expansion ensures that
the query will retrieve documents from a particular
authoritative source ? during feature selection, these
authorities supplied high precision documents for all
training questions in a particular cluster, hence fea-
tures specific to these sources were identified.
Q: When did Bob Marley die? [A: answers.com]
The noun Bob Marley has one meaning:
Jamaican singer who popularized reggae (1945-81)
Born: 6 February 1945
Birthplace: St. Ann?s Parish, Jamaica
Died: 11 May 1981 (cancer)
Songs: Get Up, Stand Up, Redemption Song . . .
In this example, profiles for many entities men-
tioned in a question cluster were found on several
authority websites. Due to unlikely expansions such
as ?noun Bob Marley has one meaning? the entity
?Bob Marley?, the answer to the question ?When
did Bob Marley die?? can easily be found. In fact,
this observation has the potential to lead to a cluster-
based authority discovery method, in which certain
sources are given more credibility and are used more
frequently than others. For example, by observing
that for most questions in a cluster, the wikipedia site
covers at least one correct answer (ideally that can
actually be extracted), then it should be considered
(accessed) for test questions before other sources of
documents. Through this process, given a set of
questions processed using the IBQA approach, a set
of authority answer sources can be identified.
5 Conclusions & Future Work
We presented a new, cluster-based query expansion
method that learns query content which is success-
fully used in answering other similar questions. Tra-
ditional QA query expansion is based only on the
individual keywords in a question. In contrast, the
cluster-based expansion learns features from context
shared by similar training questions from a cluster.
Since the features of cluster-based expansion are
different from the features used in traditional query
expansion, they lead to new relevant documents that
are different from documents retrieved using exist-
ing expansion techniques. Our experiments show
that more than 90% of the questions benefit from
our cluster-based method when used in addition to
traditional expansion methods.
Retrieval in local corpora offers more flexibility
in terms of query structure and expressivity. The
cluster-based method can be extended to take advan-
tage of structure in addition to content. More specif-
ically, different query structures could benefit differ-
ent types of questions. However, learning structure
might require more training questions for each clus-
ter. Further research can also be done to improve
the methods of combining learned content into more
robust and generalizable queries. Finally we are in-
terested modifying our cluster-based expansion for
the purpose of automatically identifying authority
sources for different types of questions.
References
M. W. Bilotti, B. Katz, and J. Lin. 2004. What works
better for question answering: Stemming or morpho-
logical query expansion? In IR4QA, SIGIR Workshop.
C. Clarke, G. Cormack, G. Kemkes, M. Laszlo, T. Ly-
nam, E. Terra, and P. Tilker. 2002. Statistical selection
of exact answers.
K. Collins-Thompson, E. Terra, J. Callan, and C. Clarke.
2004. The effect of document retrieval quality on fac-
toid question-answering performance.
W.B. Croft, S. Cronen-Townsend, and V. Lavrenko.
2001. Relevance feedback and personalization: A lan-
guage modeling perspective. In DELOS-NSF Work-
shop on Personalization and Recommender Systems in
Digital Libraries.
L. Hiyakumoto, L.V. Lita, and E. Nyberg. 2005. Multi-
strategy information extraction for question answer-
ing.
C. Monz. 2003. From document retrieval to question
answering. In Ph. D. Dissertation, Universiteit Van
Amsterdam.
H. Raghavan and J. Allan. 2002. Using part-of-speech
patterns to reduce query ambiguity.
S. Riezler, A. Vasserman, I. Tsochantaridis, V. Mittal, and
Y. Liu. 2007. Statistical machine translation for query
expansion in answer retrieval. In ACL.
E. Terra, C.L., and A. Clarke. 2005. Comparing query
formulation and lexical affinity replacements in pas-
sage retrieval. In ELECTRA, SIGIR Workshop.
W.A. Woods, S.J. Green, P. Martin, and A. Houston.
2001. Aggressive morphology and lexical relations for
query expansion.
Y. Yang and J. Pederson. 1997. Feature selection in sta-
tistical learning of text categorizatio n.
433
Large Scale Diagnostic Code Classification for Medical Patient Records
Lucian Vlad Lita and Shipeng Yu and Stefan Niculescu and Jinbo Bi
Siemens Medical Solutions
firstname.lastname@siemens.com
Abstract
A critical, yet not very well studied problem
in medical applications is the issue of accu-
rately labeling patient records according to
diagnoses and procedures that patients have
undergone. This labeling problem, known as
coding, consists of assigning standard medi-
cal codes (ICD9 and CPT) to patient records.
Each patient record can have several corre-
sponding labels/codes, many of which are
correlated to specific diseases. The cur-
rent, most frequent coding approach involves
manual labeling, which requires considerable
human effort and is cumbersome for large
patient databases. In this paper we view
medical coding as a multi-label classification
problem, where we treat each code as a label
for patient records. Due to government regu-
lations concerning patient medical data, pre-
vious studies in automatic coding have been
quite limited. In this paper, we compare two
efficient algorithms for diagnosis coding on a
large patient dataset.
1 Introduction
In order to be reimbursed for services provided to pa-
tients, hospitals need to provide proof of the proce-
dures that they performed. Currently, this is achieved
by assigning a set of CPT (Current Procedural Ter-
minology) codes to each patient visit to the hospi-
tal. Providing these codes is not enough for receiv-
ing reimbursement: in addition, hospitals need to jus-
tify why the corresponding procedures have been per-
formed. In order to do that, each patient visit needs to
be coded with the appropriate diagnosis that require
the above procedures. There are several standardized
systems for patient diagnosis coding, with ICD9 (In-
ternational Classification of Diseases, (Organization,
1997)) being the official version. Usually a CPT code
is represented by a five digit integer whereas an ICD9
code is a real number consisting of a 2-3 digit dis-
ease category followed by 1-2 decimal subcategory.
For example, a CPT code of 93307 is used for an
Echo Exam. An ICD9 code of 428 represents Heart
Failure (HF) with subcategories 428.0 (Congestive
HF, Unspecified), 428.1 (Left HF), 428.2 (Systolic
HF), 428.3 (Diastolic HF), 428.4(Combined HF) and
428.9 (HF, Unspecified).
The coding approach currently used in hospi-
tals relies heavily on manual labeling performed by
skilled and/or not so skilled personnel. This is a
very time consuming process, where the person in-
volved reads the patient chart and assigns the appro-
priate codes. Moreover, this approach is very er-
ror prone given the huge number of CPT and ICD9
codes. A recent study (Benesch et al, 1997) suggests
that only 60%-80% of the assigned ICD9 codes re-
flect the exact patient medical diagnosis. This can
be partly explained by the fact that coding is done
by medical abstractors who often lack the medical
expertise to properly reach a diagnosis. Two situa-
tions are prevalent: ?over-coding? (assigning a code
for a more serious condition than it is justified) and
?under-coding? (missing codes for existing proce-
dures/diagnoses). Both situations translate into sig-
nificant financial loses: for insurance companies in
the first case and for hospitals in the second case.
Additionally, accurate coding is extremely important
because ICD9 codes are widely used in determining
patient eligibility for clinical trials as well as in quan-
tifying hospital compliance with quality initiatives.
Another recent study (Sonel et al, 2006) stresses
the importance of developing automated methods for
patient record information extraction by demonstrat-
ing how an automated system performed with 8%
better accuracy than a human abstractor on a task of
identifying guideline compliance for unstable angina
patients. In the study, differences between the auto-
mated system and the human abstractor were adjudi-
877
cated by an expert based on the evidence provided.
In this paper we compare several data mining tech-
niques for automated ICD9 diagnosis coding. Our
methods are able to predict ICD9 codes by model-
ing this task as a classification problem in the natural
language processing framework. We demonstrate our
algorithms in section 4 on a task of ICD9 coding of a
large population of patients seen at a cardiac hospital.
2 Related Work
Classification under supervised learning setting has
been a standard problem in machine learning or
data mining area, which learns to construct inference
models from data with known assignments, and then
the models can be generalized to unseen data for code
prediction. However, it has been rarely employed
in the domain for automatic assignment of medi-
cal codes such as ICD9 codes to medical records.
Part of the reason is that the data and labels are dif-
ficult to obtain. Hospitals are usually reluctant to
share their patient data with research communities,
and sensitive information (e.g. patient name, date of
birth, home address, social security number) has to
by anonymized to meet HIPAA (Health Insurance
Portability and Accountability Act) (hip, ) standards.
Another reason is that the code classification task is
itself very challenging. The patient records contain
a lot of noise (misspellings, abbreviations, etc), and
understanding the records correctly is very important
to make correct code predictions.
Most of the ICD9 code assignment systems
work with a rule-based engine as, for in-
stance, the one available online from the site
http://www.icd9coding.com/, or the one described in
(reb, ), which displays different ICD9 codes for a
trained medical abstractor to look at and manually
assign proper codes to patient records.
A health care organization can significantly im-
prove its performance by implementing an automated
system that integrates patients documents, tests with
standard medical coding system and billing systems.
Such a system offers large health care organizations
a means to eliminate costly and inefficient man-
ual processing of code assignments, thereby improv-
ing productivity and accuracy. Early efforts dedi-
cated to automatic or semi-automatic assignments of
ICD9 codes (Larkey and Croft, 1995; Lovis et al,
1995) demonstrate that simple machine learning ap-
proaches such as k-nearest neighbor, relevance feed-
back, or Bayesian independence classifiers can be
used to acquire knowledge from already-coded train-
ing documents. The identified knowledge is then em-
ployed to optimize the means of selecting and rank-
ing candidate codes for the test document. Often a
combination of different classifiers produce better re-
sults than any single type of classifier. Occasionally,
human interaction is still needed to enhance the code
assignment accuracy (Lovis et al, 1995).
Similar work was performed to automatically cat-
egorize patients documents according to meaningful
groups and not necessarily in terms of medical codes
(de Lima et al, 1998; Ruch, 2003; Freitas-Junior et
al., 2006; Ribeiro-Neto et al, 2001). For instance, in
(de Lima et al, 1998), classifiers were designed and
evaluated using a hierarchical learning approach. Re-
cent works (Halasz et al, 2006) also utilize NGram
techniques to automatically create Chief Complaints
classifiers based on ICD9 groupings.
In (Rao et al, ), the authors present a small scale
approach to assigning ICD9 codes of Diabetes and
Acute Myocardial Infarction (AMI) on a small popu-
lation of patients. Their approach is semi-automatic,
consisting of association rules implemented by an ex-
pert, which are further combined in a probabilistic
fashion. However, given the high degree of human
interaction involved, their method will not be scal-
able to a large number of medical conditions. More-
over, the authors do not further classify the subtypes
within Diabetes or AMI.
Very recently, the Computation Medicine Center
was sponsoring an international challenge task on
this type of text classification problem.1 About 2, 216
documents are carefully extracted (including training
and testing), and 45 ICD9 labels (with 94 distinct
combinations) are used for these documents. More
than 40 groups submitted their results, and the best
macro and micro F1 measures are 0.89 and 0.77, re-
spectively. The competition is a worthy effort in the
sense that it provides a test bed to compare different
algorithms. Unfortunately, public datasets are to date
much smaller than the patient records in even a small
hospital. Moreover, many of the documents are very
simple (one or two sentences). It is difficult to train
1http://www.computationalmedicine.org/challenge/index.php
878
good classifiers based on such a small data set (even
the most common label 786.2 (for ?Cough?) has only
155 reports to train on), and the generalizability of
the obtained classifiers is also problematic.
3 Approach
This section describes the two data mining algo-
rithms used in section 4 for assigning ICD9 codes to
patient visits as well as the real world dataset used in
our experiments.
3.1 Data: ICD-9 Codes & Patient Records
We built a 1.3GB corpus using medical patient
records extracted from a real single-institution pa-
tient database. This is important since most pub-
lished previous work was performed on very small
datasets. Due to privacy concerns, since the database
contains identified patient information, it cannot be
made publicly available. Each document consists of
a full hospital visit record for a particular patient.
Each patient may have several hospital visits, some of
which may not be documented if they choose to visit
multiple hospitals2 . Our dataset consists of 96557
patient visits, each of them being labeled with a one
or more ICD9 codes. We have encountered 2618 dis-
tinct ICD9 codes associated with these visits, with the
top five most frequent summarized in table 1. Given
sufficient patient records supporting a code, this pa-
per investigates the performance of statistical classifi-
cation techniques. This paper focuses on correct clas-
sification of high-frequency diagnosis codes.
Automatic prediction of the ICD9 codes is a chal-
lenging problem. During each hospital visit, a patient
might be subjected to several tests, have different lab
results and undergo various treatments. For the ma-
jority of these events, physicians and nurses generate
free text data either by typing the information them-
selves or by using a local or remote speech-to-text
engine. The input method also affects text quality
and therefore could impact the performance of clas-
sifiers based on this data. In addition to these obsta-
cles for the ICD9 classification task, patient records
often include medical history (i.e. past medical con-
ditions, medications etc) and family history (i.e. par-
ents? chronic diseases). By embedding unstructured
2Currently, there is a movement to more portable electronic
health records
medical information that does not directly describe a
patient?s state, the data becomes noisier.
A significant difference between medical patient
record classification and general text classification
(e.g. news domain) is word distribution. Depend-
ing on the type of institution, department profile, and
patient cohort, phrases such as ?discharge summary?,
?chest pain?, and ?ECG? may be ubiquitous in cor-
pus and thus not carry a great deal of information
for a classification task. Consider the phrase ?chest
pain?: intuitively, it should correlate well with the
ICD-9 code 786.50, which corresponds to the con-
dition chest pain. However, through the nature of
the corpus, this phrase appears in well over half of
the documents, many of which do not belong to the
786.50 category.
3.2 Support Vector Machines
The first classification method consists of support
vector machines (SVM), proven to perform well
on textual data (Rogati and Yang, 2002). The
experiments presented use the SVM Light toolkit
(Joachims, 2002) with a linear kernel and a tar-
get positive-to-negative example ratio defined by the
training data. We experiment with a cost function
that assigns equal value to all classes, as well as with
a target class cost equal to the ratio of negative to pos-
itive examples. The results shown in this paper corre-
spond to SVM classifiers trained using the latter cost
function. Note that better results may be obtained by
tuning such parameters on a validation set.
3.3 Bayesian Ridge Regression
The second method we have tried on this problem is a
probabilistic approach based on Gaussian processes.
A Gaussian process (GP) is a stochastic process
that defines a nonparametric prior over functions in
Bayesian statistics (Rasmussen and Williams, 2006).
In the linear case, i.e. the function has linear form
f(x) = w>x, the GP prior on f is equivalent to
a Gaussian prior on w, which takes the form w ?
N (?w,?w), with mean ?w and covariance ?w.
Then the likelihood of labels y = [y1, . . . , yn]> is
P (y) =
? n
?
i=1
P (yi|w>xi)P (w|?w,?w) dw (1)
with P (yi|w>xi) the probability that document xi
takes label yi.
879
ICD9 Freq Coverage Description
786.50 59957 0.621 Chest pain, unspecified
401.9 28232 0.292 Essential hypertension, unspecified
414.00 27872 0.289 Unspecified type of vessel, native or graft
427.31 15269 0.158 Atrial fibrillation
414.01 13107 0.136 Coronary atherosclerosis of native coronary artery
Table 1: Statistics of the top five ICD-9 codes most frequent in the patient record database. Frequency of ICD-9 code in corpus and
the corresponding coverage (i.e. fraction of documents in the corpus that were coded with the particular ICD-9 code).
In general we fix ?w = 0, and ?w = I with
I the identity matrix. Based on past experience we
simply choose P (yi|w>xi) to be a Gaussian, yi ?
N (w>xi, ?2), with ?2 a model parameter. Since
everything is Gaussian here, the a posteriori dis-
tribution of w conditioned on the observed labels,
P (w|y, ?2), is also a Gaussian, with mean
??w =
(
X>X + ?2I
)?1
X>y, (2)
where X = [x1, . . . ,xn]> is a n?d matrix. The only
model parameter ?2 can also be optimized by maxi-
mizing the likelihood (1) with respect to ?2. Finally
for a test document x?, we predict its label as ??>wx?
with the optimal ?2. We can also estimate the vari-
ance of this prediction, but describing this is beyond
the scope of this paper.
This model is sometimes called the Bayesian ridge
regression, since the log-likelihood (i.e., the loga-
rithm of (1)) is the negation of the ridge regression
cost up to a constant factor (see, e.g., (Tikhonov and
Arsenin, 1977; Bishop, 1995)):
`(y,w,X) = ?y ?Xw?2 + ??w?2,
with ? = ?2. One advantage of Bayesian ridge re-
gression is that there is a systematic way of optimiz-
ing ? from the data. Feature selection is done prior to
calculation (2) to ensure the matrix inverse is feasi-
ble. Cholesky factorization is used to speed up calcu-
lation. Though the task here is classification, we treat
the classification labels as regression labels and nor-
malize them before learning (i.e., subtract the mean
such that
?
i yi = 0).
4 Experiments
In this section we describe our experimental setup
and results using the previously mentioned dataset
and approaches. Each document in the patient
database represents an event in the patient?s hospi-
tal stay: e.g. radiology note, personal physician note,
lab tests etc. These documents are combined to cre-
ate a hospital visit profile and are subsequently pre-
processed for the classification task. No stemming is
performed for the experiments in this paper.
We limit our experiments on hospital visits with
less than 200 doctor?s notes. As a first pre-processing
step, we eliminate redundancy at a paragraph level
and we perform tokenization and sentence splitting.
In addition, tokens go through a number and pro-
noun classing smoothing process, in which all num-
bers are replaced with the same token, and all person
pronouns are replaced with a similar token. Further
classing could be performed: e.g. dates, entity class-
ing etc, but were not considered in these experiments.
As a shared pre-processing for all classifiers, viable
features are considered all unigrams with a frequency
of occurrence greater or equal to 10 that do not appear
in a standard lists of function words.
After removing consolidating patient visits from
multiple documents, our corpus consists of near
100, 000 data points. We then randomly split the
visits into training, validation, and test sets which
contain 70%, 15%, and 15% of the corpus respec-
tively. The classifiers were tested on an 15% unseen
test set. Thus, the training set consists of approxi-
mately 57, 000 data points (patient visits), which is
a more realistic dataset compared to the previously
used datasets ? e.g. the medical text dataset used in
the Computation Medicine Center competition.
This paper presents experiments with the five most
frequent ICD9 codes. This allows for more in-depth
experiments with only a few labels and also ensures
sufficient training and testing data for our experi-
ments. From a machine learning perspective, most of
the ICD9 codes are unbalanced: i.e. much less than
half of the documents in the corpus actually have a
given label. From a text processing perspective, this
880
Average F1 Measure
Micro Macro
SVM 0.683 0.652
BRR 0.688 0.667
Table 3: F1 measure for the ICD-9 classification experiments
is a normal multi-class classification setting.
Prior to training the classifiers on our dataset, we
performed feature selection using ?2. The top 1, 500
features with the highest ?2 values were selected to
make up the feature vector. The previous step in
which the vocabulary was drastically reduced was
necessary, since the ?2 measure is unstable (Yang and
Pedersen, 1997) when infrequent features are used.
To generate the feature vectors, the ?2 values were
normalized into the ? coefficient and then each vec-
tor was normalized to an Euclidean norm of 1.
In these experiments, we have employed two
classification approaches: support vector machine
(SVM) and Bayesian ridge regression (BRR), for
each of the ICD9 codes. We used the validation set to
tune the specific parameters parameters for these ap-
proaches ? all the final results are reported using the
unseen test set. For the Bayesian ridge regression, the
validation set is used to determine the ? parameter as
well as the best cutting point for positive versus nega-
tive predictions in order to optimize the F1 measure.
Training is very fast for both methods when 1, 500
features are selected using ?2.
We evaluate our models using Precision, Recall,
AUC (Area under the Curve) and F1 measure. The
results on the top five codes for both classification ap-
proaches are shown in Table 2. For the same exper-
iments, the receiver operating characteristic (ROC)
curves of prediction are shown in Figure 1 and in
Figure 2. The support vector machine and Bayesian
ridge regression methods obtain comparable results
on these independent ICD9 classification problems.
The Bayesian ridge regression method obtains a
slightly better performance.
It is important to note that the results presented in
this section may considerably underestimate the true
performance of our classifiers. Our classifiers are
tested on ICD9 codes labeled by the medical abstrac-
tors, who, according to (Benesch et al, 1997), only
have a 60%-80% accuracy. A better performance es-
timation can be obtained by adjudicating the differ-
0 0.2 0.4 0.6 0.8 1
0
0.2
0.4
0.6
0.8
1
 
 
414.00
786.50
414.01
427.31
401.9
Figure 1: ROC curve for the SVM ICD9 classification
0 0.2 0.4 0.6 0.8 1
0
0.2
0.4
0.6
0.8
1
 
 
414.00
786.50
414.01
427.31
401.9
Figure 2: ROC curve for the BRR ICD9 classification
ences using a medical expert (as the small scale ap-
proach presented in (Sonel et al, 2006)), but we did
not have access to such a resource.
5 Conclusions & Future Work
Code classification for medical patient records is be-
coming a critical task in the healthcare industry. This
paper presents two automatic code classification ap-
proaches and applies them on a real, large hospi-
tal dataset. We view this problem as a multi-label
classification problem and seek automatic solutions,
specifically targeting ICD9 code classification. We
have tested two state-of-the-art classification algo-
rithms: support vector machines and Bayesian ridge
regression) with promising performance.
The data set in our study contains more than
90,000 patient visits, and is by far the largest corpus
for research purpose to the best of our knowledge.
The features extracted from patient visits were se-
lected for individual ICD9 codes based on ?2 score.
Low and high-frequency features were filtered out.
Several other feature selection methods were consid-
ered (including information gain), yielding compara-
tively moderate performance levels.
881
ICD9 Support Vector Machine Bayesian Ridge Regression
Prec Rec F1 AUC Prec Rec F1 AUC
786.50 0.620 0.885 0.729 0.925 0.657 0.832 0.734 0.921
401.9 0.447 0.885 0.594 0.910 0.512 0.752 0.609 0.908
414.00 0.749 0.814 0.784 0.826 0.784 0.763 0.772 0.827
427.31 0.444 0.852 0.584 0.936 0.620 0.625 0.623 0.931
414.01 0.414 0.906 0.568 0.829 0.575 0.742 0.648 0.836
Table 2: Top five ICD-9 codes most frequent in the patient record database showing the performance of support vector machine-
based method (SVM) and of bayesian ridge regression-based method (BRR).
Both Support Vector Machines and Bayesian ridge
regression methods are fast to train and achieve com-
parable results. The F1 measure performance on the
unseen test data is between 0.6 to 0.75 for the tested
ICD9 codes, and the AUC scores are between 0.8 to
0.95. These results support the conclusion that au-
tomatic code classification is a promising research
direction and offers the potential to change clinical
coding dramatically.
Current approaches are still an incipient step to-
wards more complex, flexible and robust coding
models for classifying medical patient records. In
current and future work we plan to employ more
powerful models, extract more complex features, and
explore inter-code correlations.
Patient record data exhibits strong correlations
among certain ICD9 codes. For instance the code for
fever 780.6 is very likely to co-occur with the code
for cough 786.2. Currently we do not consider inter-
code correlations and train separate classifier for in-
dividual codes. We are currently exploring methods
that can take advantage of inter-code correlations and
obtain a better, joint model for all ICD9 codes.
References
C. Benesch, D.M. Witter Jr, A.L. Wilder, P.W. Duncan, G.P.
Samsa, and D.B. Matchar. 1997. Inaccuracy of the interna-
tional classification of diseases (icd-9-cm) in identifying the
diagnosis of ischemic cerebrovascular disease. Neurology.
C. M. Bishop. 1995. Neural Networks for Pattern Recognition.
Oxford University Press.
Luciano R. S. de Lima, Alberto H. F. Laender, and Berthier A.
Ribeiro-Neto. 1998. A hierarchical approach to the auto-
matic categorization of medical documents. In CIKM.
H. R. Freitas-Junior, B. A. Ribeiro-Neto, R. De Freitas-
Vale, A. H. F. Laender, and L. R. S. De Lima. 2006.
Categorization-driven cross-language retrieval of medical in-
formation. JASIST.
S. Halasz, P. Brown, C. Goodall, D. G. Cochrane, and J. R. Al-
legra. 2006. The NGram cc classifier: A novel method of au-
tomatically creating cc classifiers based on ICD9 groupings.
Advances in Disease Surveillance, 1(30).
Health insurance portability and accountability act. 2003.
http://www.hhs.gov/ocr/hipaa.
T. Joachims. 2002. Learning to Classify Text Using Support
Vector Machines. Dissertation. Kluwer.
L. Larkey and W. Croft. 1995. Automatic assignment of icd9
codes to discharge summaries.
Christian Lovis, P. A. Michel, Robert H. Baud, and Jean-Raoul
Scherrer. 1995. Use of a conceptual semi-automatic icd-9
encoding system in a hospital environment. In AIME, pages
331?339.
World Health Organization. 1997. Manual of the international
statistical classification or diseases, injuries, and causes of
death. World Health Organization, Geneva.
R.B. Rao, S. Sandilya, R.S. Niculescu, C. Germond, and H. Rao.
Clinical and financial outcomes analysis with existing hospi-
tal patient records. SIGKDD.
C. E. Rasmussen and C. K. I. Williams. 2006. Gaussian Pro-
cesses for Machine Learning. MIT Press.
PhyCor of Corsicana. In Book Chapter of Information Technol-
ogy for the Practicing Physician. Springer London.
B. A. Ribeiro-Neto, A. H. F. Laender, and L. R. S. De-Lima.
2001. An experimental study in automatically categorizing
medical documents. JASIST.
Monica Rogati and Yiming Yang. 2002. High-performing fea-
ture selection for text classification. CIKM.
P. Ruch. 2003. Applying natural language processing to
information retrieval in clinical records and biomedical
texts. Ph.D. thesis, Department of Informatics, Universite De
Gene?ve.
A.F. Sonel, C.B. Good, H. Rao, A. Macioce, L.J. Wall, R.S.
Niculescu, S. Sandilya, P. Giang, S. Krishnan, P. Aloni, and
R.B. Rao. 2006. Use of remind artificial intelligence software
for rapid assessment of adherence to disease specific manage-
ment guidelines in acute coronary syndromes. AHRQ.
A. N. Tikhonov and V. Y. Arsenin. 1977. Solutions of Ill-Posed
Problems. Wiley, New York.
Yiming Yang and Jan O. Pedersen. 1997. A comparative study
on feature selection in text categorization. ICML.
882
