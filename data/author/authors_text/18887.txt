Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: System Demonstrations,
pages 71?75, Dublin, Ireland, August 23-29 2014.
CLAM: Quickly deploy NLP command-line tools on the web
Maarten van Gompel
Centre for Language Studies (CLS)
Radboud University Nijmegen
proycon@anaproy.nl
Martin Reynaert
CLS, Radboud University Nijmegen
TiCC, Tilburg University
reynaert@uvt.nl
http://proycon.github.io/clam
Abstract
In this paper we present the software CLAM; the Computational Linguistics Application Medi-
ator. CLAM is a tool that allows you to quickly and transparently transform command-line NLP
tools into fully-fledged RESTful webservices with which automated clients can communicate, as
well as a generic webapplication interface for human end-users.
1 Introduction
In the field of Natural Language Processing, tools often come in the form of command-line tools aimed at
UNIX-derived systems. We consider this good practice in line with the UNIX philosophy (McIlroy et al.,
1978) which states, amongst others, that programs should 1) do one thing and do it well, and 2) expect
the output of one program to be the input of another. This can be rephrased as the Rule of Modularity:
write programs consisting of simple parts, connected by well-defined interfaces (Raymond, 2004).
Programs operating at the command-line offer such modularity, making them ideally suitable for in-
tegration in a wide variety of workflows. However, the command-line may not be the most suitable
interface for non-specialised human end-users. Neither does it by itself facilitate usage over network un-
less explicit server functionality has been programmed into the application. Human end-users often want
a Graphical User Interface (GUI), a special instance of which is a Web User Interface. Yet for automated
clients operating over a network, such an interface is a cumbersome barrier, and these instead prefer a
properly formalised webservice interface. CLAM offers a solution to this problem, when all there is is a
simple NLP command-line tool.
CLAM finds application in areas where people want to make their software available to a larger public,
but a command-line interface is not sufficient. Setting up your tool may be complicated, especially if
there are many dependencies or the target audience does not use Linux machines. CLAM is ideally suited
for quick demo purposes, or for integration into larger workflow systems. It removes the burden from
the software developer (you) to have to implement a server mode and build a GUI or web-interface, thus
saving precious time.
2 System architecture
The Computational Linguistics Application Mediator (CLAM) is a tool that wraps around your
command-line interface and allows you to very quickly and transparently turn your program into 1) a
RESTful (Fielding, 2000) webservice with which automated clients can communicate, as well as 2) a
generic web user interface for human end-users. Just like an actual clam is a shell around the animal that
inhabits it, which most onlookers never see directly, CLAM wraps around your sofware, providing extra
functionality and hardening it through its built-in security mechanism. You do not need to modify your
original software in any way, it is always taken as a given, you merely need to describe it.
This work is licensed under a Creative Commons Attribution 4.0 International Licence: http://creativecommons.
org/licenses/by/4.0/
71
An NLP command-line tool can usually be described in terms of input files, output files and parameters
influencing its run. Parameters may either be global parameters, pertaining to the system as a whole, or
local parameters which act as metadata for specific input files. File formats are never dictated by CLAM
itself, but are up to the service provider to define.
Figure 1: Schematic overview of the CLAM architecture
CLAM discerns three states, which also reflect the stages in which the end-user or automated client
interacts with the system
1. The system is ready to accept files for input and input parameters
2. The system is running
3. The system is done and the output files are offered for presentation/download.
Any tool that can be described in these terms can be used with CLAM. The system has been designed
specifically to work with software that may take quite some time to process or runs large batches. Stage
two therefore is not confined to lasting mere seconds as is custom in web-based applications, but may
last as long as hours, days, or any duration that the end-user is willing to wait. Also, end-users need not
maintain a connection to the server. Human end-users may close their browser and return at will, and
automated clients simply poll the system?s status with a certain interval.
You are not limited to just a single run of your system; you may set it up to allow upload and processing
of multiple files and run them in batch fashion. This approach is common in processing text files for
purposes such as tokenisation or any form of tagging.
In order for CLAM to turn a command-line tool into a webservice, developers are expected to provide
two things in addition to the actual tool:
1. Service configuration - This specifies everything there is to know about your application, it defines
what the input will be, what the output will be, and what parameters the system may take. Input
and output are always in the form of files, adhering to whatever format you desire. The web user
interface, however, also optionally offers a text field for users to create files on the fly.
72
2. System wrapper script - This is a small script that CLAM will invoke to start your system. It acts
as the glue between CLAM and your actual application and may do some necessary interpretation
and transformation of parameters to suit the command-line interface of your application.
A generic client for communicating with the webservice is already provided, more specific clients can
be written using the CLAM API (Python) to greatly facilitate development. The architecture of CLAM
is schematically visualised in Figure 1.
CLAM is a multi-user system, although out-of-the-box it simply uses an ?anonymous? user and re-
quires no authentication. Each user can create an arbitrary number of projects. One project corresponds
to one run of the system, which may be one large batch depending on how you configure your service.
Users can always return to earlier projects and inspect input files and output files, until they explicitly
delete the project.
2.1 Service Configuration
In the service configuration file, you specify precisely what kind of input goes into the system, and what
kind of output goes out: this results in a deterministic and thus predictable webservice. With any input
and output files, arbitrary metadata can be associated. For input files, metadata is created from parameters
that can be set by users, these are rendered as input fields in the web interface. You can specify how this
metadata is carried over to output files. Additionally, as part of the metadata, provenance data is generated
for all output files. These are both stored in a simple and straightforward XML format.
All these definitions are specified in so-called profiles. A profile defines input templates and output
templates. These can be seen as ?slots? for certain filetypes and their metadata. A small excerpt of
a profile for a simple translation system with some associated metadata is shown in Figure 2. A full
discussion of its syntax goes beyond the scope of this paper, but is explained at length in the manual.
Profile(InputTemplate(?maininput?, PlainTextFormat,
"Translator input: Plain-text document",
StaticParameter(
id=?encoding?,name=?Encoding?,description=?The character encoding of the file?,
value=?utf-8?
),
ChoiceParameter(
id=?language?,name=?Language?,description=?The language the text is in?,
choices=[(?en?,?English?),(?nl?,?Dutch?),(?fr?,?French?)]),
),
extension=?.txt?,
multi=True
), OutputTemplate(?translationoutput?, PlainTextFormat,
"Translator output: Plain-text document",
CopyMetaField(?encoding?,?maininput.encoding?)
SetMetaField(?language?,?de?),
removeextension=?.txt?,
extension=?.translation?,
multi=True
))
Figure 2: An excerpt of a fictitious profile for a simple translation system from English, Dutch or French
to German. The attribute multi=True states that multiple files of this type may be submitted during a
single run
Global parameters to the system are specified independently of any profiles. Consider a global pa-
rameter that would indicate whether or not want the fictitious translation system seen in Figure 2 to be
case-sensitive, and take a look at the following example
1
:
PARAMETERS = [
(?Translation parameters?, [
BooleanParameter(id=?casesensitive?,name=?Case Sensitivity?,
description=?Enable case sensitive behaviour??)
])]
1
Parameters are always grouped into named groups, ?Translation parameters? is just the label of the group here
73
2.2 System Wrapper
Communication between CLAM and your command-line tool proceeds through a system wrapper script.
The service configuration file defines what script to call and what variables, pre-defined by CLAM, to
pass to it:
COMMAND = "mywrapper.py $DATAFILE $OUTPUTDIRECTORY"
This is then executed whenever a user runs a project. It is the job of the system wrapper script to
invoke your actual application.
There are two main means of communicating the parameters to the system wrapper: one is to make
use of the data file ($DATAFILE), which is an XML file that contains all input parameters. It can be
parsed and queried effortlessly using the CLAM API, provided you write your wrapper script in Python.
The other way, more limited, is to specify parameter flags for your global parameters
2
in the service
configuration, and simply let CLAM pass all global parameters as arguments on the command line:
COMMAND = "mywrapper.pl $INPUTDIRECTORY $OUTPUTDIRECTORY $PARAMETERS"
By passing the input directory, the system wrapper script can simply look for its input files there.
3 Extensions
CLAM can be extended by developers in several ways. One is to write viewers, which take care of the
visualisation of output files for a specific file format, and are used by the web user interface. Viewers
may be implemented as internal Python modules, or you can link to any external URL which takes care
of the visualisation. Another extension is converters, these allow users to upload an input file in one
file type and have it automatically converted to another. Converters for PDF and Word to plain text are
already provided through third party tools.
4 Technical Details
CLAM is written in Python (2.6 or 2.7), (van Rossum, 2007). It comes with a built-in HTTP server for
development purposes, allowing you to quickly test and adjust your service. Final deployment can be
made on common webservers such as Apache, Nginx or lighthttpd through the WSGI mechanism. The
service configuration file itself is by definition a Python file calling specific configuration directives in
the CLAM API. The system wrapper script may be written in any language, but Python users benefit as
they can use the CLAM API which makes the job easier. Projects and input files are stored in a simple
directory structure on disk, allowing your tool easy access. No database server is required.
The webservice offers a RESTful interface (Fielding, 2000), meaning that the HTTP verbs GET, POST,
PUT and DELETE are used on URLs that represent resources such as projects, input files, output files.
The web application is implemented as a client-side layer on the webservice. It is presented through XSL
transformation (Clark, 1999) of the webservice XML output.
User authentication is implemented in the form of HTTP Digest Authentication, which ensures that
the password is sent in encrypted form over the network even with servers where HTTPS is not used.
HTTPS support is not present in CLAM itself but can be configured in the encompassing webserver. The
underlying user database can be specified either directly in the service configuration file or in a table in
a Mysql database, but it is fairly easy to replace this and communicate with another external database of
your choice instead. There is also support for propagating credentials from another authentication source
such as Shibboleth
3
, allowing for integrating with single-sign-on scenarios. Implementation of OAuth2
4
will follow in a later version.
CLAM is open-source software licensed under the GNU Public License v3. Both the software as
well as the documentation can be obtained through the CLAM website at github: http://proycon.
github.io/clam .
2
caveat: this does not work for local parameters, i.e. parameters pertaining to files
3
http://shibboleth.net
4
http://oauth.net/2/
74
5 Related Work
As far as we know, the only tool comparable to CLAM is Weblicht (Hinrichs et al., 2010). Both tools are
specifically designed for an NLP context. CLAM, however, is of a more generic and flexible nature and
may also find easier adoption in other fields.
When it comes to data formats, Weblicht commits to a specific file format for corpus data. CLAM
leaves file formats completely up to the service providers, although it does come, as a bonus, with a
viewer for users of FoLiA (van Gompel and Reynaert, 2013).
Weblicht is Java-based whereas CLAM is Python-based, which tends to be less verbose and more
easily accessible. System wrapper scripts can be written in any language, and service configuration files
simply consist of directives that require virtually no Python knowledge.
All in all CLAM offers a more lightweight solution than Weblicht, allowing webservices to be set
up more easily and quicker. Nevertheless, CLAM offers more power and flexibility in doing what it
does: wrapping around command-line tools, its webservice specification is more elaborate than that of
Weblicht. On the other hand, CLAM deliberately does not go as far as Weblicht and does not offer a
complete chaining environment, which is what Weblicht is. In this we follow the aforementioned UNIX
philosophy of doing one thing well and one thing only. Service chaining certainly remains possible and
CLAM provides all the information to facilitate it, but it is left to other tools designed for the task. CLAM
has been successfully used with Taverna (Hull et al., 2006) in the scope of the CLARIN-NL project ?TST
Tools for Dutch as Webservices in a Workflow? (Kemps-Snijders et al., 2012).
Acknowledgements
CLAM support and development is generously funded by CLARIN-NL (Odijk, 2010), and is being
used by various projects in the Dutch & Flemish NLP communities, whose feedback and support have
contributed to its success.
References
J. Clark. 1999. XSL transformations (XSLT) version 1.0. Technical report, 11.
R. T. Fielding. 2000. Architectural Styles and the Design of Network-based Software Architectures. Doctoral
dissertation. University of California, Irvine.
M. Hinrichs, T. Zastrow, and E. W. Hinrichs. 2010. Weblicht: Web-based LRT services in a Distributed eScience
Infrastructure. In Nicoletta Calzolari, Khalid Choukri, Bente Maegaard, Joseph Mariani, Jan Odijk, Stelios
Piperidis, Mike Rosner, and Daniel Tapias, editors, LREC. European Language Resources Association.
D. Hull, K. Wolstencroft, R. Stevens, C. Goble, M. R. Pocock, P. Li, and T. Oinn. 2006. Taverna: a tool for
building and running workflows of services. Nucleic Acids Res, 34( Web Server issue):729?732, July.
M. Kemps-Snijders, M. Brouwer, J.P. Kunst, and T. Visser. 2012. Dynamic web service deployment in a cloud
environment. In Nicoletta Calzolari, Khalid Choukri, Thierry Declerck, Mehmet Ugur Dogan, Bente Mae-
gaard, Joseph Mariani, Jan Odijk, and Stelios Piperidis, editors, LREC, pages 2941?2944. European Language
Resources Association (ELRA).
M. D. McIlroy, E. N. Pinson, and B. A. Tague. 1978. Unix time-sharing system forward. The Bell System
Technical Journal, 57(6, part 2):p.1902.
J. Odijk. 2010. The CLARIN-NL project. In Proceedings of the Seventh International Conference on Language
Resources and Evaluation, LREC-2010, pages 48?53, Valletta, Malta.
E. S. Raymond. 2004. The Art of Unix Programming.
M. van Gompel and M. Reynaert. 2013. FoLiA: A practical XML Format for Linguistic Annotation - a descriptive
and comparative study. Computational Linguistics in the Netherlands Journal, 3.
G. van Rossum. 2007. Python programming language. In USENIX Annual Technical Conference. USENIX.
75
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 871?880,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Translation Assistance by Translation of L1 Fragments in an L2 Context
Maarten van Gompel & Antal van den Bosch
Centre for Language Studies
Radboud University Nijmegen
proycon@anaproy.nl
Abstract
In this paper we present new research in
translation assistance. We describe a sys-
tem capable of translating native language
(L1) fragments to foreign language (L2)
fragments in an L2 context. Practical ap-
plications of this research can be framed in
the context of second language learning.
The type of translation assistance system
under investigation here encourages lan-
guage learners to write in their target lan-
guage while allowing them to fall back to
their native language in case the correct
word or expression is not known. These
code switches are subsequently translated
to L2 given the L2 context. We study
the feasibility of exploiting cross-lingual
context to obtain high-quality translation
suggestions that improve over statistical
language modelling and word-sense dis-
ambiguation baselines. A classification-
based approach is presented that is in-
deed found to improve significantly over
these baselines by making use of a contex-
tual window spanning a small number of
neighbouring words.
1 Introduction
Whereas machine translation generally concerns
the translation of whole sentences or texts from
one language to the other, this study focusses on
the translation of native language (henceforth L1)
words and phrases, i.e. smaller fragments, in a
foreign language (L2) context. Despite the ma-
jor efforts and improvements, automatic transla-
tion does not yet rival human-level quality. Vex-
ing issues are morphology, word-order change and
long-distance dependencies. Although there is a
morpho-syntactic component in this research, our
scope is more constrained; its focus is on the faith-
ful preservation of meaning from L1 to L2, akin to
the role of the translation model in Statistical Ma-
chine Translation (SMT).
The cross-lingual context in our research ques-
tion may at first seem artificial, but its design ex-
plicitly aims at applications related to computer-
aided language learning (Laghos and Panayiotis,
2005; Levy, 1997) and computer-aided transla-
tion (Barrachina et al, 2009). Currently, lan-
guage learners need to refer to a bilingual dictio-
nary when in doubt about a translation of a word or
phrase. Yet, this problem arises in a context, not
in isolation; the learner may have already trans-
lated successfully a part of the text into L2 leading
up to the problematic word or phrase. Dictionar-
ies are not the best source to look up context; they
may contain example usages, but remain biased to-
wards single words or short expressions.
The proposed application allows code switch-
ing and produces context-sensitive suggestions as
writing progresses. In this research we test the
feasibility of the foundation of this idea.The fol-
lowing examples serve to illustrate the idea and
demonstrate what output the proposed translation
assistance system would ideally produce. The
parts in bold correspond to respectively the in-
serted fragment and the system translation.
? Input (L1=English,L2=Spanish): ?Hoy va-
mos a the swimming pool.?
Desired output: ?Hoy vamos a la piscina.?
? Input (L1-English, L2=German): ?Das wet-
ter ist wirklich abominable.?
Desired output: ?Das wetter ist wirklich
ekelhaft.?
? Input (L1=French,L2=English): ?I rentre `a
la maison because I am tired.?
Desired output: ?I return home because I am
tired.?
? Input (L1=Dutch, L2=English): ?Workers
are facing a massive aanval op their employ-
871
ment and social rights.?
Desired output: ?Workers are facing a mas-
sive attack on their employment and social
rights.?
The main research question in this research is
how to disambiguate an L1 word or phrase to
its L2 translation based on an L2 context, and
whether such cross-lingual contextual approaches
provide added value compared to baseline models
that are not context informed or compared to stan-
dard language models.
2 Data preparation
Preparing the data to build training and test data
for our intended translation assistance system is
not trivial, as the type of interactive translation as-
sistant we aim to develop does not exist yet. We
need to generate training and test data that real-
istically emulates the task. We start with a par-
allel corpus that is tokenised for both L1 and L2.
No further linguistic processing such as part-of-
speech tagging or lemmatisation takes place in our
experiments; adding this remains open for future
research.
The parallel corpus is randomly sampled into
two large and equally-sized parts. One is the basis
for the training set, and the other is the basis for
the test set. The reason for such a large test split
shall become apparent soon.
From each of the splits (S), a phrase-translation
table is constructed automatically in an unsuper-
vised fashion. This is done using the scripts
provided by the Statistical Machine Translation
system Moses (Koehn et al, 2007). It invokes
GIZA++ (Och and Ney, 2000) to establish sta-
tistical word alignments based on the IBM Mod-
els and subsequently extracts phrases using the
grow-diag-final algorithm (Och and Ney,
2003). The result, independent for each set, will
be a phrase-translation table (T ) that maps phrases
in L1 to L2. For each phrase-pair (f
s
, f
t
) this
phrase-translation table holds the computed trans-
lation probabilities P (f
s
|f
t
) and P (f
t
|f
s
).
Given these phrase-translation tables, we can
now extract both training data and test data using
the algorithm in Figure 1. In our discourse, the
source language (s) corresponds to L1, the fall-
back language used for by the end-user for insert-
ing fragments, whilst the target language (t) is L2.
Step 4 is effectively a filter: two thresholds
can be configured to discard weak alignments,
1. using phrase-translation table T and par-
allel corpus split S
2. for each aligned sentence pair
(sentence
s
? S
s
, sentence
t
? S
t
)
in the parallel corpus split (S
s
,S
t
):
3. for each fragment (f
s
?
sentence
s
, f
t
? sentence
t
) where
(f
s
, f
t
) ? T :
4. if P (f
s
|f
t
) ? P (f
t
|f
s
) ? ?
1
and P (f
s
|f
t
) ? P (f
t
|f
s
) ? ?
2
?
P (f
s
|f
strongest t
) ? P (f
strongest t
|f
s
):
5. Output a pair
(sentence
?
t
, sentence
t
) where
sentence
?
t
is a copy of t but with
fragment f
t
substituted by f
s
, i.e. the
introduction of an L1 word or phrase in
an L2 sentence.
Figure 1: Algorithm for extracting training and
test data on the basis of a phrase-translation ta-
ble (T ) and subset/split from a parallel corpus (S).
The indentation indicates the nesting.
i.e. those with low probabilities, from the phrase-
translation table so that only strong couplings
make it into the generated set. The parameter
?
1
adds a constraint based on the product of the
two conditional probabilities (P (f
t
|f
s
)?P (f
s
|f
t
)),
and sets a threshold that has to be surpassed.
A second parameter ?
2
further limits the con-
sidered phrase pairs (f
s
, f
t
) to have the prod-
uct of their conditional probabilities not not devi-
ate more than a fraction ?
2
from the joint prob-
ability for the strongest possible pairing for f
s
,
the source fragment. f
strongest t
in Figure 1
corresponds to the best scoring translation for a
given source fragment f
s
. This metric thus effec-
tively prunes weaker alternative translations in the
phrase-translation table from being considered if
there is a much stronger candidate. Nevertheless,
it has to be noted that even with ?
1
and ?
2
, the test
set will include a certain amount of errors. This is
due to the nature of the unsupervised method with
which the phrase-translation table is constructed.
For our purposes however, the test set suffices to
test our hypothesis.
872
In our experiments, we choose fixed values for
these parameters, by manual inspection and judge-
ment of the output. The ?
1
parameter was set to
0.01 and ?
2
to 0.8. Whilst other thresholds may
possibly produce cleaner sets, this is hard to eval-
uate as finding optimal values causes a prohibitive
increase in complexity of the search space, and
again this is not necessary to test our hypothesis.
The output of the algorithm in Fig-
ure 1 is a modified set of sentence pairs
(sentence
?
t
, sentence
t
), in which the same
sentence pair may be used multiple times with
different L1 substitutions for different fragments.
The final test set is created by randomly sampling
the desired number of test instances.
Note that the training set and test set are con-
structed on their own respective and indepen-
dently generated phrase-translation tables. This
ensures complete independence of training and
test data. Generating test data using the same
phrase-translation table as the training data would
introduce a bias. The fact that a phrase-translation
table needs to be constructed for the test data is
also the reason that the parallel corpus split from
which the test data is derived has to be large
enough, ensuring better quality.
We concede that our current way of testing is
a mere approximation of the real-world scenario.
An ideal test corpus would consist of L2 sentences
with L1 fallback as crafted by L2 language learn-
ers with an L1 background. However, such cor-
pora do not exist as yet. Nevertheless, we hope to
show that our automated way of test set genera-
tion is sufficient to test the feasibility of our core
hypothesis that L1 fragments can be translated to
L2 using L2 context information.
3 System
We develop a classifier-based system composed of
so-called ?classifier experts?. Numerous classi-
fiers are trained and each is an expert in translating
a single word or phrase. In other words, for each
word type or phrase type that occurs as a fragment
in the training set, and which does not map to just a
single translation, a classifier is trained. The clas-
sifier maps the L1 word or phrase in its L2 context
to its L2 translation. Words or phrases that always
map to a single translation are stored in a sim-
ple mapping table, as a classifier would have no
added value in such cases. The classifiers use the
IB1 algorithm (Aha et al, 1991) as implemented
in TiMBL (Daelemans et al, 2009).
1
IB1 im-
plements k-nearest neighbour classification. The
choice for this algorithm is motivated by the fact
that it handles multiple classes with ease, but first
and foremost because it has been successfully em-
ployed for word sense disambiguation in other
studies (Hoste et al, 2002; Decadt et al, 2004),
in particular in cross-lingual word sense disam-
biguation, a task closely resembling our current
task (van Gompel and van den Bosch, 2013). It
has also been used in machine translation stud-
ies in which local source context is used to clas-
sify source phrases into target phrases, rather than
looking them up in a phrase table (Stroppa et al,
2007; Haque et al, 2011). The idea of local phrase
selection with a discriminative machine learning
classifier using additional local (source-language)
context was introduced in parallel to Stroppa et al
(2007) by Carpuat and Wu (2007) and Gim?enez
and M?arquez (2007); cf. Haque et al (2011) for
an overview of more recent methods.
The feature vector for the classifiers represents
a local context of neighbouring words, and op-
tionally also global context keywords in a binary-
valued bag-of-words configuration. The local con-
text consists of an X number of L2 words to the
left of the L1 fragment, and Y words to the right.
When presented with test data, in which the
L1 fragment is explicitly marked, we first check
whether there is ambiguity for this L1 fragment
and if a direct translation is available in our sim-
ple mapping table. If so, we are done quickly and
need not rely on context information. If not, we
check for the presence of a classifier expert for the
offered L1 fragment; only then we can proceed by
extracting the desired number of L2 local context
words to the immediate left and right of this frag-
ment and adding those to the feature vector. The
classifier will return a probability distribution of
the most likely translations given the context and
we can replace the L1 fragment with the highest
scoring L2 translation and present it back to the
user.
In addition to local context features, we also ex-
perimented with global context features. These
are a set of L2 contextual keywords for each L1
word/phrase and its L2 translation occurring in the
same sentence, not necessarily in the immediate
neighbourhood of the L1 word/phrase. The key-
words are selected to be indicative for a specific
1
http://ilk.uvt.nl/timbl
873
translation. We used the method of extraction by
Ng and Lee (1996) and encoded all keywords in
a binary bag of words model. The experiments
however showed that inclusion of such keywords
did not make any noticeable impact on any of the
results, so we restrict ourselves to mentioning this
negative result.
Our full system, including the scripts for
data preparation, training, and evaluation, is
implemented in Python and freely available
as open-source from http://github.com/
proycon/colibrita/ . Version tag v0.2.1
is representative for the version used in this re-
search.
3.1 Language Model
We also implement a statistical language model as
an optional component of our classifier-based sys-
tem and also as a baseline to compare our system
to. The language model is a trigram-based back-
off language model with Kneser-Ney smooth-
ing, computed using SRILM (Stolcke, 2002) and
trained on the same training data as the translation
model. No additional external data was brought
in, to keep the comparison fair.
For any given hypothesisH , results from the L1
to L2 classifier are combined with results from the
L2 language model. We do so by normalising the
class probability from the classifier (score
T
(H)),
which is our translation model, and the language
model (score
lm
(H)), in such a way that the high-
est classifier score for the alternatives under con-
sideration is always 1.0, and the highest language
model score of the sentence is always 1.0. Take
score
T
(H) and score
lm
(H) to be log probabili-
ties, the search for the best (most probable) trans-
lation hypothesis
?
H can then be expressed as:
?
H = argmax
H
(score
T
(H) + score
lm
(H)) (1)
If desired, the search can be parametrised with
variables ?
3
and ?
4
, representing the weights we
want to attach to the classifier-based translation
model and the language model, respectively. In
the current study we simply left both weights set to
one, thereby assigning equal importance to trans-
lation model and language model.
4 Evaluation
Several automated metrics exist for the evaluation
of L2 system output against the L2 reference out-
put in the test set. We first measure absolute accu-
racy by simply counting all output fragments that
exactly match the reference fragments, as a frac-
tion of the total amount of fragments. This mea-
sure may be too strict, so we add a more flexible
word accuracy measure which takes into account
partial matches at the word level. If output o is
a subset of reference r then a score of
|o|
|r|
is as-
signed for that sentence pair. If instead, r is a sub-
set of o, then a score of
|r|
|o|
will be assigned. A
perfect match will result in a score of 1 whereas
a complete lack of overlap will be scored 0. The
word accuracy for the entire set is then computed
by taking the sum of the word accuracies per sen-
tence pair, divided by the total number of sentence
pairs.
We also compute a recall metric that measures
the number of fragments that the system provided
a translation for as a fraction of the total number
of fragments in the input, regardless of whether
the fragment is translated correctly or not. The
system may skip fragments for which it can find
no solution at all.
In addition to these, the system?s output can be
compared against the L2 reference translation(s)
using established Machine Translation evaluation
metrics. We report on BLEU, NIST, METEOR,
and word error rate metrics WER and PER. These
scores should generally be much better than the
typical MT system performances as only local
changes are made to otherwise ?perfect? L2 sen-
tences.
5 Baselines
A context-insensitive yet informed baseline was
constructed to assess the impact of L2 context in-
formation in translating L1 fragments. The base-
line selects the most probable L1 fragment per L2
fragment according to the phrase-translation ta-
ble. This baseline, henceforth referred to as the
?most likely fragment? baseline (MLF) is analo-
gous to the ?most frequent sense?-baseline com-
mon in evaluating WSD systems.
A second baseline was constructed by weigh-
ing the probabilities from the translation table di-
rectly with the L2 language model described ear-
lier. It adds a LM component to the MLF base-
line. This LM baseline allows the comparison of
classification through L1 fragments in an L2 con-
text, with a more traditional L2 context modelling
(i.e. target language modelling) which is also cus-
874
tomary in MT decoders. Computing this base-
line is done in the same fashion as previously il-
lustrated in Equation 1, where score
T
then repre-
sents the normalised p(t|s) score from the phrase-
translation table rather than the class probability
from the classifier.
6 Experiments & Results
The data for our experiments were drawn from
the Europarl parallel corpus (Koehn, 2005) from
which we extracted two sets of 200, 000 sentence
pairs each for several language pairs. These were
used to form the training and test sets. The final
test sets are a randomly sampled 5, 000 sentence
pairs from the 200, 000-sentence test split for each
language pair.
All input data for the experiments in this section
are publicly available
2
.
Let us first zoom in to convey a sense of scale
on a specific language pair. The actual Europarl
training set we generate for English (L1) to Span-
ish (L2), i.e. English fallback in a Spanish con-
text, consists of 5, 608, 015 sentence pairs. This
number is much larger than the 200, 000 we men-
tioned before because single sentence pairs may be
reused multiple times with different marked frag-
ments. From this training set of sentence pairs
over 100, 000 classifier experts are derived. The
eleven largest classifiers are shown in Table 1,
along with the number of training instances per
classifier. The full table would reveal a Zipfian
distribution.
Fragment Training instances Translations
the 256,772 la, el, los, las
of 139,273 de, del
and 128,074 y, de, e
to 66,565 a, para, que, de
a 54,306 un, una
is 40,511 es, est?a, se
for 34,054 para, de, por
this 29,691 este, esta, esto
European 26,543
Europea, Europeo
Europeas, Europeos
on 23,147 sobre, en
of the 22,361 de la, de los
Table 1: The top eleven classifier experts for En-
glish to Spanish. The eleventh entry is included as
an example of a common phrasal fragment
Among the classifier experts are only words and
phrases that are ambiguous and may thus map to
2
Download and unpack http://lst.science.ru.
nl/
?
proycon/colibrita-acl2014-data.zip
Figure 2: Accuracy for different local context
sizes, Europarl English to Spanish
multiple translations. This implies that such words
and phrases must have occurred at least twice in
the corpus, though this threshold is made config-
urable and could have been set higher to limit the
number of classifiers. The remaining 246, 380 un-
ambiguous mappings are stored in a separate map-
ping table.
For the classifier-based system, we tested var-
ious different feature vector configurations. The
first experiment, of which the results are shown in
Figure 2, sets a fixed and symmetric local context
size across all classifiers, and tests three context
widths. Here we observe that a context width of
one yields the best results. The BLEU scores, not
included in the figure but shown in Table 2, show
a similar trend. This trend holds for all the MT
metrics.
Table 2 shows the results for English to Span-
ish in more detail and adds a comparison with the
two baseline systems. The various lXrY config-
urations use the same feature vector setup for all
classifier experts. HereX indicates the left context
size and Y the right context size. The auto con-
figuration does not uniformly apply the same fea-
ture vector setup to all classifier experts but instead
seeks to find the optimal setup per classifier expert.
This shall be further discussed in Section 6.1.
As expected, the LM baseline substantially out-
performs the context-insensitive MLF baseline.
Second, our classifier approach attains a sub-
stantially higher accuracy than the LM baseline.
Third, we observe that adding the language model
to our classifier leads to another significant gain
875
Configuration Accuracy Word Accuracy BLEU METEOR NIST WER PER
MLF baseline 0.6164 0.6662 0.972 0.9705 17.0784 1.4465 1.4209
LM baseline 0.7158 0.7434 0.9785 0.9739 17.1573 1.1735 1.1574
l1r1 0.7588 0.7824 0.9801 0.9747 17.1550 1.1625 1.1444
l2r2 0.7574 0.7801 0.9800 0.9746 17.1550 1.1750 1.1569
l3r3 0.7514 0.7742 0.9796 0.9744 17.1445 1.1946 1.1780
l1r1+LM 0.7810 0.7973 0.9816 0.9754 17.1685 1.0946 1.077
auto 0.7626 0.7850 0.9803 0.9748 17.1544 1.1594 1.1424
auto+LM 0.7796 0.7966 0.9815 0.9754 17.1664 1.1021 1.0845
l1r0 0.6924 0.7223 0.9757 0.9723 17.1087 1.3415 1.3249
l2r0 0.6960 0.7245 0.9759 0.9724 17.1091 1.3364 1.3193
l2r1 0.7624 0.7849 0.9803 0.9748 17.1558 1.1554 1.1378
Table 2: Europarl results for English to Spanish (i.e English fallback in Spanish context). Recall =
0.9422
(configuration l1r1+LM in the results in Ta-
ble 2). It appears that the classifier approach and
the L2 language model are able to complement
each other.
Statistical significance on the BLEU scores was
tested using pairwise bootstrap sampling (Koehn,
2004). All significance tests were performed
with 5, 000 iterations. We compared the out-
comes of several key configurations. We first
tested l1r1 against both baselines; both differ-
ences are significant at p < 0.01 for both. The
same significance level was found when compar-
ing l1r1+LM against l1r1, auto+LM against
auto, as well as the LM baseline against the MLF
baseline. Automatic feature selection auto was
found to perform statistically better than l1r1,
but only at p < 0.05. Conclusions with regard to
context width may have to be tempered somewhat,
as the performance of the l1r1 configuration was
found to not be significantly better than that of the
l2r2 configuration. However, l1r1 performs
significantly better than l3r3 at p < 0.01, and
l2r2 performs significantly better than l3r3 at
p < 0.01.
In Table 3 we present some illustrative exam-
ples from the English?Spanish Europarl data.
We show the difference between the most-likely-
fragment baseline and our system.
Likewise, Table 4 exemplifies small fragments
from the l1r1 configuration compared to the
same configuration enriched with a language
model. We observe in this data that the language
model often has the added power to choose a cor-
rect translation that is not the first prediction of
the classifier, but one of the weaker alternatives
that nevertheless fits better. Though the classifier
generally works best in the l1r1 configuration,
i.e. with context size one, the trigram-based lan-
guage model allows further left-context informa-
tion to be incorporated that influences the weights
of the classifier output, successfully forcing the
system to select alternatives. This combination
of a classifier with context size one and trigram-
based language model proves to be most effective
and reaches the best results so far. We have not
conducted experiments with language models of
other orders.
6.1 Context optimisation
It has been argued that classifier experts in a word
sense disambiguation ensemble should be individ-
ually optimised (Decadt et al, 2004; van Gompel
and van den Bosch, 2013). The latter study on
cross-lingual WSD finds a positive impact when
conducting feature selection per classifier. This in-
tuitively makes sense; a context of one may seem
to be better than any other when uniformly applied
to all classifier experts, but it may well be that cer-
tain classifiers benefit from different feature selec-
tions. We therefore proceed with this line of inves-
tigation as well.
Automatic configuration selection was done by
performing leave-one-out testing (for small num-
ber of instances) or 10-fold-cross validation (for
larger number of instances, n ? 20) on the train-
ing data per classifier expert. Various configura-
tions were tested. Per classifier expert, the best
scoring configuration was selected, referred to as
the auto configuration in Table 2. The auto
configuration improves results over the uniformly
876
Input: Mientras no haya prueba en contrario , la financiaci?on de partidos pol??ticos European s?olo se justifica , incluso
despu?es del tratado de Niza , desde el momento en que concurra a la expresi?on del sufragio universal , que es la ?unica
definici?on aceptable de un partido pol??tico .
MLF baseline: Mientras no haya prueba en contrario , la financiaci?on de partidos pol??ticos Europea s?olo se justifica ,
incluso despu?es del tratado de Niza , desde el momento en que concurra a la expresi?on del sufragio universal , que es la
?unica definici?on aceptable de un partido pol??tico .
l1r1: Mientras no haya prueba en contrario , la financiaci?on de partidos pol??ticos europeos s?olo se justifica , incluso
despu?es del tratado de Niza , desde el momento en que concurra a la expresi?on del sufragio universal , que es la ?unica
definici?on aceptable de un partido pol??tico .
Input: Esta Directiva es nuestra oportunidad to marcar una verdadera diferencia , reduciendo la tr?agica p?erdida de vidas
en nuestras carreteras .
MLF baseline: Esta Directiva es nuestra oportunidad a marcar una verdadera diferencia , reduciendo la tr?agica p?erdida
de vidas en nuestras carreteras .
l1r1: Esta Directiva es nuestra oportunidad para marcar una verdadera diferencia , reduciendo la tr?agica p?erdida de vidas
en nuestras carreteras .
Input: Es la last vez que me dirijo a esta C?amara .
MLF baseline: Es la pasado vez que me dirijo a esta C?amara .
l1r1: Es la ?ultima vez que me dirijo a esta C?amara .
Input: Pero el enfoque actual de la Comisi?on no puede conducir a una buena pol??tica ya que es tributario del fun-
cionamiento del mercado y de las normas establecidas por la OMC , el FMI y el Banco Mundial , normas que siguen
siendo desfavorables para los developing countries .
MLF baseline: Pero el enfoque actual de la Comisi?on no puede conducir a una buena pol??tica ya que es tributario del
funcionamiento del mercado y de las normas establecidas por la OMC , el FMI y el Banco Mundial , normas que siguen
siendo desfavorables para los los pa??ses en desarrollo .
l1r1: Pero el enfoque actual de la Comisi?on no puede conducir a una buena pol??tica ya que es tributario del funcionamiento
del mercado y de las normas establecidas por la OMC , el FMI y el Banco Mundial , normas que siguen siendo desfavor-
ables para los pa??ses en desarrollo .
Table 3: Some illustrative examples of MLF-baseline output versus system output, in which system
output matches the correct human reference output. The actual fragments concerned are highlighted in
bold. The first example shows our system correcting for number agreement, the second a correction
in selecting the right preposition, and the third shows that the English word last can be translated in
different ways, only one of which is correct in this context. The last example shows a phrasal translation,
in which the determiner was duplicated in the baseline
applied feature selection. However, if we enable
the language model as we do in the auto+LM
configuration we do not notice an improvement
over l1r1+LM, surprisingly. We suspect the lack
of impact here can be explained by the trigram-
based Language Model having less added value
when the (left) context size of the classifier is two
or three; they are now less complementary.
Table 5 lists what context sizes have been cho-
sen in the automatic feature selection. A context
size of one prevails in the vast majority of cases,
which is not surprising considering the good re-
sults we have already seen with this configuration.
In this study we did not yet conduct optimisa-
tion of the classifier parameters. We used the IB1
algorithm with k = 1 and the default values of
the TiMBL implementation. In earlier work van
Gompel and van den Bosch (2013), we reported
a decrease in performance due to overfitting when
66.5% l1r1
19.9% l2r2
7.7% l3r3
3.5% l4r4
2.4% l5r5
Table 5: Frequency of automatically selected con-
figurations on English to Spanish Europarl dataset
this is done, so we do not expect it to make a pos-
itive impact. The second reason for omitting this
is more practical in nature; to do this in combina-
tion with feature selection would add substantial
search complexity, making experiments far more
time consuming, even prohibitively so.
The bottom lines in Table 2 represent results
when all right-context is omitted, emulating a real-
time prediction when no right context is available
yet. This has a substantial negative impact on re-
877
Input: Sin ese tipo de protecci?on la gente no aprovechar?a la oportunidad to vivir , viajar y trabajar donde les parezca en
la Uni?on Europea .
l1r1: Sin ese tipo de protecci?on la gente no aprovechar?a la oportunidad para vivir , viajar y trabajar donde les parezca en
la Uni?on Europea .
l1r1+LM: Sin ese tipo de protecci?on la gente no aprovechar?a la oportunidad de vivir , viajar y trabajar donde les parezca
en la Uni?on Europea .
Input: La Comisi?on tambi?en est?a acometiendo medidas en el ?ambito social y educational con vistas a mejorar la
situaci?on de los ni?nos .
l1r1: La Comisi?on tambi?en est?a acometiendo medidas en el ?ambito social y educativas con vistas a mejorar la situaci?on
de los ni?nos .
l1r1+LM: La Comisi?on tambi?en est?a acometiendo medidas en el ?ambito social y educativo con vistas a mejorar la
situaci?on de los ni?nos .
Table 4: Some examples of l1r1 versus the same configuration enriched with a language model.
sults. We experimented with several asymmetric
configurations and found that taking two words to
the left and one to the right yields even better re-
sults than symmetric configurations for this data
set. This result is in line with the positive effect of
adding the LM to the l1r1.
In order to draw accurate conclusions, experi-
ments on a single data set and language pair are not
sufficient. We therefore conducted a number of ex-
periments with other language pairs, and present
the abridged results in Table 6.
There are some noticeable discrepancies for
some experiments in Table 6 when compared to
our earlier results in Table 2. We see that the lan-
guage model baseline for English?French shows
the same substantial improvement over the base-
line as our English?Spanish results. The same
holds for the Chinese?English experiment. How-
ever, for English?Dutch and English?Chinese
we find that the LM baseline actually performs
slightly worse than baseline. Nevertheless, in all
these cases, the positive effect of including a Lan-
guage Model to our classifier-based system again
shows. Also, we note that in all cases our system
performs better than the two baselines.
Another discrepancy is found in the BLEU
scores of the English?Chinese experiments,
where we measure an unexpected drop in BLEU
score under baseline. However, all other scores do
show the expected improvement. The error rate
metrics show improvement as well. We therefore
attach low importance to this deviation in BLEU
here.
In all of the aforementioned experiments, the
system produced a single solution for each of the
fragments, the one it deemed best, or no solution
at all if it could not find any. Alternative evaluation
metrics could allow the system to output multiple
alternatives. Omission of a solution by definition
causes a decrease in recall. In all of our experi-
ments recall is high (well above 90%), mostly be-
cause train and test data lie in the same domain and
have been generated in the same fashion, lower re-
call is expected with more real-world data.
7 Discussion and conclusion
In this study we have shown the feasibility of
a classifier-based translation assistance system in
which L1 fragments are translated in an L2 con-
text, in which the classifier experts are built indi-
vidually per word or phrase. We have shown that
such a translation assistance system scores both
above a context-insensitive baseline, as well as an
L2 language model baseline.
Furthermore, we found that combining this
cross-language context-sensitive technique with
an L2 language model boosts results further.
The presence of a one-word right-hand side
context proves crucial for good results, which has
implications for practical translation assistance ap-
plication that translate as soon as the user finishes
an L1 fragment. Revisiting the translation when
right context becomes available would be advis-
able.
We tested various configurations and conclude
that small context sizes work better than larger
ones. Automated configuration selection had pos-
itive results, yet the system with context size one
and an L2 language model component often pro-
duces the best results. In static configurations, the
failure of a wider context window to be more suc-
878
Dataset L1 L2 Configuration Accuracy Word Accuracy BLEU
europarl200k en nl baseline 0.7026 0.7283 0.9771
europarl200k en nl LM baseline 0.6958 0.7195 0.9773
europarl200k en nl l1r1 0.7790 0.7941 0.9814
europarl200k en nl l1r1+LM 0.7838 0.7973 0.9818
europarl200k en nl auto 0.7796 0.7947 0.9815
europarl200k en nl auto+LM 0.7812 0.7954 0.9816
europarl200k en fr baseline 0.5874 0.6403 0.9709
europarl200k en fr LM baseline 0.7054 0.7319 0.9787
europarl200k en fr l1r1 0.7416 0.7698 0.9797
europarl200k en fr l1r1+LM 0.7680 0.7885 0.9815
europarl200k en fr auto 0.7484 0.7737 0.9801
europarl200k en fr auto+LM 0.7654 0.7860 0.9813
iwslt12ted en zh baseline 0.6622 0.7122 0.6421
iwslt12ted en zh LM baseline 0.6550 0.6982 0.6416
iwslt12ted en zh l1r1 0.7150 0.7531 0.5736
iwslt12ted en zh l1r1+LM 0.7296 0.7619 0.5826
iwslt12ted en zh auto 0.7150 0.7519 0.5746
iwslt12ted en zh auto+LM 0.7280 0.7605 0.5833
iwslt12ted zh en baseline 0.5784 0.6167 0.9634
iwslt12ted zh en LM baseline 0.6148 0.6463 0.9656
iwslt12ted zh en l1r1 0.7104 0.7338 0.9709
iwslt12ted zh en l1r1+LM 0.7270 0.7460 0.9721
iwslt12ted zh en auto 0.7078 0.7319 0.9709
iwslt12ted zh en auto+LM 0.7230 0.7428 0.9719
Table 6: Results on different datasets and language pairs. The iwslt12ted set is the dataset used in the
IWSLT 2012 Evaluation Campaign (Federico et al, 2012), and is formed by a collection of transcriptions
of TED talks. Here we used of just over 70, 000 sentences for training. Recall for each of the four datasets
is 0.9498 (en-nl), 0.9494 (en-fr), 0.9386 (en-zh), and 0.9366 (zh-en)
cesful may be attributed to the increased sparsity
that comes from such an expansion.
The idea of a comprehensive translation assis-
tance system may extend beyond the translation of
L1 fragments in an L2 context. There are more
NLP components that might play a role if such a
system were to find practical application. Word
completion or predictive editing (in combination
with error correction) would for instance seem an
indispensable part of such a system, and can be
implemented alongside the technique proposed in
this study. A point of more practically-oriented
future research is to see how feasible such combi-
nations are and what techniques can be used.
An application of our idea outside the area of
translation assistance is post-correction of the out-
put of some MT systems that, as a last-resort
heuristic, copy source words or phrases into their
output, producing precisely the kind of input our
system is trained on. Our classification-based ap-
proach may be able to resolve some of these cases
operating as an add-on to a regular MT system ?
or as a independent post-correction system.
Our system allows L1 fragments to be of arbi-
trary length. If a fragment was not seen during
training stage, and is therefore not covered by a
classifier expert, then the system will be unable
to translate it. Nevertheless, if a longer L1 frag-
ment can be decomposed into subfragments that
are known, then some recombination of the trans-
lations of said sub-fragments may be a good trans-
lation for the whole. We are currently exploring
this line of investigation, in which the gap with
MT narrows further.
Finally, an important line of future research
is the creation of a more representative test set.
Lacking an interactive system that actually does
what we emulate, we hypothesise that good ap-
proximations would be to use gap exercises, or
cloze tests, that test specific aspects difficulties
in language learning. Similarly, we may use
L2 learner corpora with annotations of code-
switching points or errors. Here we then assume
that places where L2 errors occur may be indica-
tive of places where L2 learners are in some trou-
ble, and might want to fall back to generating L1.
By then manually translating gaps or such prob-
lematic fragments into L1 we hope to establish a
more realistic test set.
References
D. W. Aha, D. Kibler, and M. K. Albert. 1991.
Instance-based learning algorithms. Machine
879
Learning, 06(1):37?66, January.
S. Barrachina, O. Bender, F. Casacuberta, J. Civera,
E. Cubel, S. Khadivi, A. L. Lagarda, H. Ney,
J. Tom?as, E. Vidal, and J.M. Vilar. 2009. Statistical
approaches to computer-assisted translation. Com-
putational Linguistics, 35(1):3?28.
M. Carpuat and D. Wu. 2007. Improving statis-
tical machine translation using word sense disam-
biguation. In Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL), pages 61?72.
W. Daelemans, J. Zavrel, K. van der Sloot, and A. van
den Bosch. 2009. TiMBL: Tilburg memory based
learner, version 6.2, reference guide. Technical Re-
port ILK 09-01, ILK Research Group, Tilburg Uni-
versity.
B. Decadt, V. Hoste, W. Daelemans, and A. van den
Bosch. 2004. GAMBL, genetic algorithm optimiza-
tion of memory-based WSD. In R. Mihalcea and
P. Edmonds, editors, Proceedings of the Third In-
ternational Workshop on the Evaluation of Systems
for the Semantic Analysis of Text (Senseval-3), pages
108?112, New Brunswick, NJ. ACL.
M. Federico, M. Cettolo, L. Bentivogli, M. Paul, and
S. St?uker. 2012. Overview of the IWSLT 2012 eval-
uation campaign. In Proceedings of the seventh In-
ternational Workshop on Spoken Language Transla-
tion (IWSLT), pages 12?33.
J. Gim?enez and L. M`arquez. 2007. Context-aware dis-
criminative phrase selection for statistical machine
translation. In Proceedings of the Second Workshop
on Statistical Machine Translation, pages 159?166,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
R. Haque, S. Kumar Naskar, A. van den Bosch, and
A. Way. 2011. Integrating source-language con-
text into phrase-based statistical machine transla-
tion. Machine Translation, 25(3):239?285, Septem-
ber.
V. Hoste, I. Hendrickx, W. Daelemans, and A. van
den Bosch. 2002. Parameter optimization for ma-
chine learning of word sense disambiguation. Natu-
ral Language Engineering, 8(4):311?325.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit
for statistical machine translation. In Proceedings of
the 45th Annual Meeting of the Association for Com-
putational Linguistics Companion Volume Proceed-
ings of the Demo and Poster Sessions, pages 177?
180, Prague, Czech Republic, June. Association for
Computational Linguistics.
P. Koehn. 2004. Statistical significance tests for ma-
chine translation evaluation. In Dekang Lin and
Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 388?395, Barcelona, Spain, July. Association
for Computational Linguistics.
P. Koehn. 2005. Europarl: A parallel corpus for statis-
tical machine translation. In In Proceedings of the
Machine Translation Summit X ([MT]?05)., pages
79?86.
A. Laghos and Z. Panayiotis. 2005. Computer assist-
ed/aided language learning. pages 331?336.
M. Levy. 1997. Computer-assisted language learning:
Context and conceptualization. Oxford: Clarendon
Press.
H. Tou Ng and H. Beng Lee. 1996. Integrating multi-
ple knowledge sources to disambiguate word sense:
An exemplar-based approach. In ACL, pages 40?47.
F.J. Och and H. Ney. 2000. Giza++: Training of sta-
tistical translation models. Technical report, RWTH
Aachen, University of Technology.
F. J. Och and H. Ney. 2003. A systematic compari-
son of various statistical alignment models. Comput.
Linguist., 29(1):19?51, March.
A. Stolcke. 2002. Srilm - an extensible language mod-
eling toolkit. In John H. L. Hansen and Bryan L.
Pellom, editors, 7th International Conference on
Spoken Language Processing, ICSLP2002 - INTER-
SPEECH 2002, Denver, Colorado, USA, September
16-20, 2002. ISCA.
N. Stroppa, A. van den Bosch, and A. Way. 2007.
Exploiting source similarity for SMT using context-
informed features. In A. Way and B. Gawronska,
editors, Proceedings of the 11th International Con-
ference on Theoretical Issues in Machine Transla-
tion (TMI 2007), pages 231?240, Sk?ovde, Sweden.
M. van Gompel and A. van den Bosch. 2013. WSD2:
Parameter optimisation for memory-based cross-
lingual word-sense disambiguation. In Proceedings
of the 7th International Workshop on Semantic Eval-
uation (SemEval 2013), in conjunction with the Sec-
ond Joint Conference on Lexical and Computational
Semantics.
880
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 238?241,
Uppsala, Sweden, 15-16 July 2010.
c?2010 Association for Computational Linguistics
UvT-WSD1: a Cross-Lingual Word Sense Disambiguation system
Maarten van Gompel
Tilburg centre for Cognition and Communication
Tilburg University
proycon@anaproy.nl
Abstract
This paper describes the Cross-Lingual
Word Sense Disambiguation system UvT-
WSD1, developed at Tilburg University,
for participation in two SemEval-2 tasks:
the Cross-Lingual Word Sense Disam-
biguation task and the Cross-Lingual Lex-
ical Substitution task. The UvT-WSD1
system makes use of k-nearest neighbour
classifiers, in the form of single-word ex-
perts for each target word to be disam-
biguated. These classifiers can be con-
structed using a variety of local and global
context features, and these are mapped
onto the translations, i.e. the senses,
of the words. The system works for a
given language-pair, either English-Dutch
or English-Spanish in the current imple-
mentation, and takes a word-aligned par-
allel corpus as its input.
1 Introduction
The UvT-WSD1 system described in this paper
took part in two similar SemEval-2 tasks: Cross-
Lingual Word Sense Disambiguation (Lefever and
Hoste, 2010) and Cross-Lingual Lexical Substitu-
tion (Mihalcea et al, 2010). In each task, a num-
ber of words is selected for which the senses are to
be determined for a number of instances of these
words. For each word, a number of samples in
context is provided, where each sample consists of
one sentence, with the word to be disambiguated
marked.
Because of the cross-lingual nature of the tasks,
a word sense corresponds to a translation in an-
other language, rather than a sense description in
the same language. In the Cross-lingual Lexical
Substitution task, the target language is Spanish.
The task is to find Spanish substitutes for the En-
glish words marked in the test samples. In the
Cross-Lingual Word Sense Disambiguation task,
we participate for English-Dutch and English-
Spanish. The Word Sense Disambiguation task
provides training data for all five languages, in the
form of the sentence-aligned EuroParl parallel cor-
pus (Koehn, 2005). This is the source of training
data the UvT-WSD1 system uses for both tasks.
The system may output several senses per in-
stance, rather than producing just one sense pre-
diction. These are evaluated in two different ways.
The scoring type ?best? expects that the system
outputs the best senses, in the order of its con-
fidence. The scoring type ?out of five/ten? ex-
pects five or ten guesses, and each answer weighs
the same. These metrics are more extensively
described in (Mihalcea et al, 2010). The UvT-
WSD1 system participates in both scoring types,
for both tasks. The system put forth in this paper
follows a similar approach as described in earlier
research by (Hoste et al, 2002).
2 System Description
The UvT-WSD1 system uses machine learning
techniques to learn what senses/translations are as-
sociated with any of the target words. It does
so on the basis of a variety of local and global
context features, discussed in Section 2.2. At the
core of the system are the classifiers, or so called
?word experts?, one per target word. These are
built using the Tilburg Memory Based Learner
(TiMBL) (Daelemans et al, 2009), making use
of the IB1 algorithm, an implementation of the k-
nearest neighbour classifier.
The core of the system can be subdivided into
roughly three stages. In the first stage, the word-
aligned parallel corpus is read and for each found
instance of one of the target words, features are ex-
tracted to be used in the classifier. The class con-
sists of the word aligned to the found instance of
the target word, i.e. the translation/sense. In this
way a word expert is built for each of the target
238
words in the task, yielding a total amount of clas-
sifiers equal to the total amount of target words.
The test data is processed in a similar way, for
each marked occurrence of any of the target words,
features are extracted and test instances are cre-
ated. Subsequently, the word experts are trained
and tested, and on the basis of the training data, a
parameter search algorithm (Van den Bosch, 2004)
determines the optimal set of classifier parameters
for each word expert, including for example the
value of k and the distance weighting metric used.
In the last phase, the classifier output of each
word expert is parsed. The classifiers yield a dis-
tribution of classes per test instance, and these are
converted to the appropriate formats for ?best? and
?out of five/ten? evaluation. For the latter scor-
ing type, the five/ten highest scoring senses are
selected, for the former scoring type, all classes
scoring above a certain threshold are considered
?best?. The threshold is set at 90% of the score of
the highest scoring class.
2.1 Word-Alignment, Tokenisation,
Lemmatisation and
Part-of-Speech-tagging
The Europarl parallel corpus, English-Spanish and
English-Dutch, is delivered as a sentence-aligned
parallel corpus. We subsequently run GIZA++
(Och and Ney, 2000) to compute a word-aligned
parallel corpus.
This, however, is not the sole input. The tar-
get words in both tasks are actually specified as
a lemma and part-of-speech tag pair, rather than
words. In the Word Sense Disambiguation task, all
target lemmas are simply nouns, but in the Cross-
Lingual Lexical Substitution task, they can also be
verbs, adjectives or adverbs. Likewise, both tasks
expect the sense/translation output to also be in the
form of lemmas. Therefore the system internally
has to be aware of the lemma and part-of-speech
tag of each word in the parallel corpus and test
data, only then can it successfully find all occur-
rences of the target words. In order to get this
information, both sides of the word-aligned paral-
lel corpus are run through tokenisers, lemmatisers
and Part-of-Speech taggers, and the tokenised out-
put is realigned with the untokenised input so the
word alignments are retained. The test data is also
processed this way. For English and Spanish, the
software suite Freeling (Atserias et al, 2006) per-
formed all these tasks, and for Dutch it was done
by Tadpole (Van den Bosch et al, 2007).
2.2 Feature Extraction
The system can extract a variety of features to be
used in training and testing. A distinction can be
made between local context features and global
context features. Local context features are ex-
tracted from the immediate neighbours of the oc-
currence of the target word. One or more of the
following local context features are extractable by
the UvT-WSD1 system: word features, lemma
features, and part-of-speech tag features. In each
case, n features both to the right and left of the
focus word are selected. Moreover, the system
also supports the extraction of bigram features, but
these did not perform well in the experiments.
The global context features are made up of a
bag-of-words representation of keywords that may
be indicative for a given word to sense/translation
mapping. The idea is that words are collected
which have a certain power of discrimination for
the specific target word with a specific sense,
and all such words are then put in a bag-of-word
representation, yielding as many features as the
amount of keywords found. A global count over
the full corpus is needed to find these keywords.
Each keyword acts as a binary feature, indicating
whether or not that particular keyword is found in
the context of the occurrence of the target word.
The context in which these keywords are searched
for is exactly one sentence, i.e. the sentence in
which the target word occurs. This is due to the
test data simply not supplying a wider context.
The method used to extract these keywords (k)
is proposed by (Ng and Lee, 1996) and used also
in the research of (Hoste et al, 2002). Assume we
have a focus word f , more precisely, a lemma and
part-of-speech tag pair of one of the target words.
We also have one of its aligned translations/senses
s, which in this implementation is also a lemma.
We can now estimate P (s|k), the probability of
sense s, given a keyword k, by dividing N
s,k
local
.
(the number of occurrences of a possible local
context word k with particular focus word lemma-
PoS combination and with a particular sense s) by
N
k
local
(the number of occurrences of a possible
local context keyword k
loc
with a particular focus
word-PoS combination regardless of its sense). If
we also take into account the frequency of a pos-
sible keyword k in the complete training corpus
(N
k
corpus
), we get:
239
P (s|k) =
N
s,k
local
N
k
local
(
1
N
k
corpus
) (1)
(Hoste et al, 2002) select a keyword k for in-
clusion in the bag-of-words representation if that
keyword occurs more than T
1
times in that sense
s, and if P (s|k) ? T
2
. Both T
1
and T
2
are pre-
defined thresholds, which by default were set to 3
and 0.001 respectively. In addition, UvT-WSD1
contains an extra parameter which can be enabled
to automatically adjust the T
1
threshold when it
yields too many or too few keywords. The selec-
tion of bag-of-word features is computed prior to
the extraction of the training instances, as this in-
formation is a prerequisite for the successful gen-
eration of both training and test instances.
2.3 Voting system
The local and global context features, and the var-
ious parameters that can be configured for extrac-
tion, yield a lot of possible classifier combinations.
Rather than merging all local context and global
context features together in a single classifier, they
can also be split over several classifiers and have
an arbiter voting system do the final classification
step. UvT-WSD1 also supports this approach. A
voter is constructed by taking as features the class
output of up to three different classifiers, trained
and tested on the training data, and mapping these
features onto the actual correct sense in the train-
ing data. For testing, the same approach is taken:
up to three classifiers run on the test data; their out-
put is taken as feature vector, and the voting sys-
tem predicts a sense. This approach may be useful
in boosting results and smoothing out errors. In
our experiments we see that a voter combination
often performs better than taking all features to-
gether in one single classifier. Finally, also in the
voter system there is a stage of automatic parame-
ter optimisation for TiMBL.
3 Experiments and Results
Both SemEval-2 tasks have provided trial data
upon which the system could be tested during the
development stage. Considering the high config-
urability of the various parameters for feature ex-
traction, the search space in possible configura-
tions and classifier parameters is vast, also due
to fact that the TiMBL classifier used may take a
wealth of possible parameters. As already men-
tioned, for the latter an automatic algorithm of pa-
BEST UvT-WSD1-v UvT-WSD1-g
Precision & Recall 21.09 19.59
Mode Prec. & Rec. 43.76 41.02
Ranking (out of 14) 6 9
OUT OF TEN UvT-WSD1-v UvT-WSD1-g
Precision & Recall 58.91 55.29
Mode Prec. & Rec. 62.96 73.94
Ranking 3 4
Table 1: UvT-WSD1 results in the Cross-Lingual Lexical
Substitution task
rameter optimisation was used (Van den Bosch,
2004), but optimisation of the feature extraction
parameters has not been automated. Rather, a se-
lection of configurations has been manually cho-
sen and tested during the development stage.
The following two configurations of features
were found to perform amongst the best on the
trial data. Therefore they have been selected and
submitted for the contest:
1. UvT-WSD1-v (aka UvT-v) ? An arbiter-
voting system over three classifiers: 1) Word
experts with two word features and lemma
features on both sides of the focus word.
2)Word experts with global features
1
. 3)
Word experts with two word features, two
lemma features and two part-of-speech tag
features.
2. UvT-WSD1-g (aka UvT-g) ? Word experts
with global features only.
Table 1 shows a condensed view of the results
for the Cross-Lingual Lexical Substitution task.
Table 2 shows the final results for the Word-Sense
Disambiguation task. Note that UvT-WSD1-v and
UvT-WSD1-g are two different configurations of
the UvT-WSD1 system, and to conserve space
these are abbreviated as UvT-v and UvT-g respec-
tively. These are also the names used in both tasks
(Lefever and Hoste, 2010; Mihalcea et al, 2010)
to refer to our system.
4 Discussion and Conclusion
Cross-Lingual Word Sense Disambiguation and
Cross-Lingual Lexical Substitution have proven to
be hard tasks, with scores that are relatively close
to baseline. This can be attributed to a noticeable
trait in the system output to be inclined to assign
the same majority sense to all instances.
1
For the Cross-Lingual Lexical Substitution task only, the
parameter to recompute the T
1
threshold automatically was
enabled.
240
Dutch BEST UvT-v UvT-g T3-COLEUR
Precision & Recall 17.7 15.93 10.72 & 10.56
Mode Prec. & Rec. 12.06 10.54 6.18 & 6.16
Dutch OUT OF FIVE UvT-v UvT-g T3-COLEUR
Precision & Recall 34.95 34.92 21.54 & 21.22
Mode Prec. & Rec. 24.62 19.72 12.05 & 12.03
Spanish BEST UvT-v UHD-1 UvT-g T3-COLEUR FCC-WSD1
Precision & Recall 23.42 20.48 & 16.33 19.92 19.78 & 19.59 15.09
Mode Prec. & Rec. 24.98 28.48 & 22.19 24.17 24.59 14.31
Spanish OUT OF FIVE UvT-g UvT-v FCC-WSD2 UHD-1 T3-COLEUR
Precision & Recall 43.12 42.17 40.76 38.78 & 31.81 35.84 & 35.46
Mode Prec. & Rec. 43.94 40.62 44.84 40.68 & 32.38 39.01 & 38.78
Table 2: UvT-WSD1 results in comparison to other participants in the Word-Sense Disambiguation task
In our system, we used the same configuration
of feature extraction, or a voter over a set of con-
figurations, for all word experts. The actual classi-
fier parameters however, do differ per word expert,
as they are the result of the automatic parameter
optimisation algorithm. Selecting different feature
extraction configurations per word expert would
be a logical next step to attempt to boost results
even further, as been done in (Decadt et al, 2004).
Keeping in mind the fact that different word ex-
perts may perform differently, some general con-
clusions can be drawn from the experiments on
the trial data. It appears to be beneficial to in-
clude lemma features, rather than just word fea-
tures. However, adding Part-of-speech features
tends to have a negative impact. For these lo-
cal context features, the optimum context size is
often two features to the left and two features to
the right of the focus word, cf. (Hendrickx et al,
2002). The global keyword features perform well,
but best results are achieved if they are not mixed
with the local context features in one classifier.
An arbiter voting approach over multiple clas-
sifiers helps to smooth out errors and yields the
highest scores (see Tables 1 and 2). When com-
pared to the other participants, the UvT-WSD1
system, in the voting configuration, ranks first in
the Word Sense Disambiguation task, for the two
language pairs in which we participated.
References
Jordi Atserias, Bernardino Casas, Elisabet Comelles, Mer-
itxell Gonzlez, Llu??s Padr?o, and Muntsa Padr?o. 2006.
FreeLing 1.3: Syntactic and semantic services in an open-
source NLP library . In Proceedings of the Fifth Interna-
tional Conference on Language Resources and Evaluation
(LREC 2006), Genoa, Italy. ELRA.
W. Daelemans, J. Zavrel, K. Van der Sloot, and A. Van den
Bosch. 2009. TiMBL: Tilburg memory based learner, ver-
sion 6.2, reference guide. Technical Report ILK 09-01,
ILK Research Group, Tilburg University.
B. Decadt, V. Hoste, W. Daelemans, and A. Van den
Bosch. 2004. GAMBL, genetic algorithm optimization
of memory-based WSD. In R. Mihalcea and P. Edmonds,
editors, Proceedings of the Third International Workshop
on the Evaluation of Systems for the Semantic Analysis of
Text (Senseval-3), pages 108?112, New Brunswick, NJ.
ACL.
I. Hendrickx, A. Van den Bosch, V. Hoste, and W. Daele-
mans. 2002. Dutch word sense disambiguation: Optimiz-
ing the localness of context. In Proceedings of the Work-
shop on word sense disambiguation: Recent successes and
future directions, pages 61?65, Philadelphia, PA.
V. Hoste, I. Hendrickx, W. Daelemans, and A. Van den
Bosch. 2002. Parameter optimization for machine learn-
ing of word sense disambiguation. Natural Language En-
gineering, 8(4):311?325.
Philipp Koehn. 2005. Europarl: A parallel corpus for statisti-
cal machine translation. In In Proceedings of the Machine
Translation Summit X ([MT]?05)., pages 79?86.
Els Lefever and Veronique Hoste. 2010. Semeval 2010 task
3: Cross-lingual word sense disambiguation. In Proceed-
ings of the 5th International Workshop on Semantic Eval-
uations (SemEval-2010), Uppsala, Sweden.
Rada Mihalcea, Ravi Sinha, and Diana McCarthy. 2010. Se-
meval 2010 task 2: Cross-lingual lexical substitution. In
Proceedings of the 5th International Workshop on Seman-
tic Evaluations (SemEval-2010), Uppsala, Sweden.
Hwee Tou Ng and Hian Beng Lee. 1996. Integrating mul-
tiple knowledge sources to disambiguate word sense: An
exemplar-based approach. In ACL, pages 40?47.
F.J. Och and H. Ney. 2000. Giza++: Training of statisti-
cal translation models. Technical report, RWTH Aachen,
University of Technology.
A. Van den Bosch, G.J. Busser, S. Canisius, and W. Daele-
mans. 2007. An efficient memory-based morpho-
syntactic tagger and parser for Dutch. In P. Dirix, I. Schu-
urman, V. Vandeghinste, , and F. Van Eynde, editors, Com-
putational Linguistics in the Netherlands: Selected Papers
from the Seventeenth CLIN Meeting, pages 99?114, Leu-
ven, Belgium.
A. Van den Bosch. 2004. Wrapped progressive sampling
search for optimizing learning algorithm parameters. In
R. Verbrugge, N. Taatgen, and L. Schomaker, editors,
Proceedings of the Sixteenth Belgian-Dutch Conference
on Artificial Intelligence, pages 219?226, Groningen, The
Netherlands.
241
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 36?44,
Dublin, Ireland, August 23-24, 2014.
SemEval-2014 Task 5: L2 Writing Assistant
Maarten van Gompel, Iris Hendrickx,
Antal van den Bosch
Centre for Language Studies,
Radboud University Nijmegen,
The Netherlands
proycon@anaproy.nl,
i.hendrickx@let.ru.nl,
a.vandenbosch@let.ru.nl
Els Lefever and V
?
eronique Hoste
LT3,
Language and Translation Technology Team,
Ghent University,
Belgium
els.lefever@ugent.be,
veronique.hoste@ugent.be
Abstract
We present a new cross-lingual task for
SemEval concerning the translation of
L1 fragments in an L2 context. The
task is at the boundary of Cross-Lingual
Word Sense Disambiguation and Machine
Translation. It finds its application in the
field of computer-assisted translation, par-
ticularly in the context of second language
learning. Translating L1 fragments in an
L2 context allows language learners when
writing in a target language (L2) to fall
back to their native language (L1) when-
ever they are uncertain of the right word
or phrase.
1 Introduction
We present a new cross-lingual and application-
oriented task for SemEval that is situated in the
area where Word Sense Disambiguation and Ma-
chine Translation meet. Finding the proper trans-
lation of a word or phrase in a given context is
much like the problem of disambiguating between
multiple senses.
In this task participants are asked to build a
translation/writing assistance system that trans-
lates specifically marked L1 fragments in an L2
context to their proper L2 translation. This type
of translation can be applied in writing assistance
systems for language learners in which users write
in a target language, but are allowed to occasion-
ally back off to their native L1 when they are un-
certain of the proper lexical or grammatical form
in L2. The task concerns the NLP back-end rather
than any user interface.
Full-on machine translation typically concerns
the translation of complete sentences or texts from
This work is licensed under a Creative
Commons Attribution 4.0 International Licence:
http://creativecommons.org/licenses/by/4.0/
L1 to L2. This task, in contrast, focuses on smaller
fragments, side-tracking the problem of full word
reordering.
We focus on the following language combi-
nations of L1 and L2 pairs: English-German,
English-Spanish, French-English and Dutch-
English. Task participants could participate for all
language pairs or any subset thereof.
2 Task Description
We frame the task in the context of second lan-
guage learning, yielding a specific practical appli-
cation.
Participants build a translation assistance sys-
tem rather than a full machine translation system.
The L1 expression, a word or phrase, is translated
by the system to L2, given the L2 context already
present, including right-side context if available.
The aim here, as in all translation, is to carry the
semantics of the L1 fragment over to L2 and find
the most suitable L2 expression given the already
present L2 context.
Other than a limit on length (6 words), we do
not pose explicit constraints on the kinds of L1
fragments allowed. The number of L1 fragments
is limited to one fragment per sentence.
The task addresses both a core problem of
WSD, with cross-lingual context, and a sub-
problem of Phrase-based Statistical Machine
Translation; that of finding the most suitable trans-
lation of a word or phrase. In MT this would be
modelled by the translation model. In our task
the full complexity of full-sentential translation
is bypassed, putting the emphasis on the seman-
tic aspect of translation. Our task has specific
practical applications and a specific intended au-
dience, namely intermediate and advanced second
language learners, whom one generally wants to
encourage to use their target language as much as
possible, but who may often feel the need to fall
back to their native language.
36
Currently, language learners are forced to fall
back to a bilingual dictionary when in doubt. Such
dictionaries do not take the L2 context into ac-
count and are generally more constrained to single
words or short expressions. The proposed applica-
tion would allow more flexible context-dependent
lookups as writing progresses. The task tests how
effectively participating systems accomplish this.
The following examples illustrate the task for
the four language pairs we offer:
? Input (L1=English,L2=Spanish): ?Todo ello,
in accordance con los principios que siempre
hemos apoyado.?
Desired output: ?Todo ello, de conformidad
con los principios que siempre hemos apoy-
ado.?
? Input (L1-English, L2=German): ?Das,
was wir heute machen, is essentially ein
?
Argernis.?
Desired output: ?Das, was wir heute machen,
ist im Grunde genommen ein
?
Argernis.?
? Input (L1=French,L2=English): ?I rentre `a
la maison because I am tired.?
Desired output: ?I return home because I am
tired.?
? Input (L1=Dutch, L2=English): ?Workers
are facing a massive aanval op their employ-
ment and social rights.?
Desired output: ?Workers are facing a mas-
sive attack on their employment and social
rights.?
The task can be related to two tasks that were
offered in previous years of SemEval: Lexical
Substitution (Mihalcea et al., 2010) and most no-
tably Cross-lingual Word Sense Disambiguation
(Lefever and Hoste, 2013).
When comparing our task to the Cross-Lingual
Word-Sense Disambiguation task, one notable dif-
ference is the fact that our task concerns not just
words, but also phrases. Another essential differ-
ence is the nature of the context; our context is in
L2 instead of L1. Unlike the Cross-Lingual Word
Sense Disambiguation task, we do not constrain
the L1 words or phrases that may be used for trans-
lation, except for a maximum length which we set
to 6 tokens, whereas Lefever and Hoste (2013)
only tested a select number of nouns. Our task
emphasizes a correct meaning-preserving choice
of words in which translations have to fit in the
L2 context. There is thus a clear morphosyntactic
aspect to the task, although less prominent than
in full machine translation, as the remainder of
the sentence, already in L2, does not need to be
changed. In the Cross-Lingual Word Sense Dis-
ambiguation tasks, the translations/senses were
lemmatised. We deliberately chose a different path
that allows for the envisioned application to func-
tion directly as a translation assistance system.
A pilot study was conducted to test the feasibil-
ity of the proposed translation system (van Gom-
pel and van den Bosch, 2014). It shows that L2
context information can be a useful cue in transla-
tion of L1 fragments to L2, improving over a non-
context-informed baseline.
3 Data
We did not provide training data for this task, as
we did not want to bias participating systems by
favouring a particular sort of material and method-
ology. Moreover, it would be a prohibitively large
task to manually collect enough training data of
the task itself. Participants were therefore free to
use any suitable training material such as parallel
corpora, wordnets, or bilingual lexica.
Trial and test data has been collected for the
task, both delivered in a simple XML format that
explicitly marks the fragments. System output of
participants adheres to the same format. The trial
set, released early on in the task, was used by par-
ticipants to develop and tune their systems on. The
test set corresponds to the final data released for
the evaluation period; the final evaluation was con-
ducted on this data.
The trial data was constructed in an automated
fashion in the way described in our pilot study
(van Gompel and van den Bosch, 2014). First a
phrase-translation table is constructed from a par-
allel corpus. We used the Europarl parallel corpus
(Koehn, 2005) and the Moses tools (Koehn et al.,
2007), which in turn makes use of GIZA++ (Och
and Ney, 2000). Only strong phrase pairs (ex-
ceeding a set threshold) were retained and weaker
ones were pruned. This phrase-translation table
was then used to create input sentences in which
the L2 fragments are swapped for their L1 coun-
terparts, effectively mimicking a fall-back to L1 in
an L2 context. The full L2 sentence acts as refer-
ence sentence. Finally, to ensure all fragments are
correct and sensible, a manual selection from this
37
automatically generated corpus constituted the fi-
nal trial set.
In our pilot study, such a data set, even with-
out the manual selection stage, proved adequate to
demonstrate the feasibility of translating L1 frag-
ments in an L2 context (van Gompel and van den
Bosch, 2014). One can, however, rightfully argue
whether such data is sufficiently representative for
the task and whether it would adequately cover in-
stances where L2 language learners might experi-
ence difficulties and be inclined to fall back to L1.
We therefore created a more representative test set
for the task.
The actual test set conforms to much more
stringent constraints and was composed entirely
by hand from a wide variety of written sources.
Amongst these sources are study books and gram-
mar books for language learners, short bilingual
on-line stories aimed at language learners, gap-
exercises and cloze tests, and contemporary writ-
ten resources such as newspapers, novels, and
Wikipedia. We aimed for actual learner corpora,
but finding suitable learner corpora with sufficient
data proved hard. For German we could use the
the Merlin corpus (Abel et al., 2013). In example
(a) we see a real example of a fragment in a fall-
back language in an L2 context from the Merlin
corpus.
(a) Input: Das Klima hier ist Tropical und wir haben fast
keinen Winter
Reference: Das Klima hier ist tropisch und wir haben
fast keinen Winter.
For various sources bilingual data was avail-
able. For the ones that were monolingual (L2)
we resorted to manual translation. To ensure our
translations were correct, these were later indepen-
dently verified, and where necessary corrected by
native speakers.
A large portion of the test set comes from off-
line resources because we wanted to make sure
that a substantial portion of the test set could not
be found verbatim on-line. This was done to pre-
vent systems from solving the actual problem by
just attempting to just look up the sources through
the available context information.
Note that in general we aimed for the European
varieties of the different languages. However, for
English we did add the US spelling variants as al-
ternatives. A complete list of all sources used in
establishing the test set is available on our web-
site
1
.
We created a trial set and test set/gold standard
of 500 sentence pairs per language pair. Due to
the detection of some errors at a later stage, some
of which were caused by the tokenisation pro-
cess, we were forced to remove some sentences
from the test set and found ourselves slightly be-
low our aim for some of the language pairs. The
test set was delivered in both tokenised
2
and unto-
kenised form. The trial set was delivered only in
tokenised form. Evaluation was conducted against
the tokenised version, but our evaluation script
was designed to be as lenient as possible regard-
ing differences in tokenisation. We explicitly took
cases into account where participant?s tokenisers
split contractions (such as Spanish ?del? to ?de?
+ ?el?), whereas our tokeniser did not.
For a given input fragment, it may well be possi-
ble that there are multiple correct translations pos-
sible. In establishing our test set, we therefore paid
special attention to adding alternatives. To ensure
no alternatives were missed, all participant output
was aggregated in one set, effectively anonymis-
ing the systems, and valid but previously missed
alternatives were added to the gold standard.
4 Evaluation
Several metrics are available for automatic eval-
uation. First, we measure the absolute accuracy
a = c/n, where c is the number of fragment
translations from the system output that precisely
match the corresponding fragments in the refer-
ence translation, and n is the total number of trans-
latable fragments, including those for which no
translation was found. We also introduce a word-
based accuracy, which unlike the absolute accu-
racy gives some credits to mismatches that show
partial overlap with the reference translation. It as-
signs a score according to the longest consecutive
matching substring between output fragment and
reference fragment and is computed as follows:
wac =
|longestsubmatch(output, reference)|
max(|output|, |reference|)
(1)
The system with the highest word-based accu-
racy wins the competition. All matching is case-
sensitive.
1
https://github.com/proycon/semeval2014task5
2
Using ucto, available at https://github.com/proycon/ucto
38
Systems may decide not to translate fragments
if they cannot find a suitable translation. A recall
metric simply measures the number of fragments
for which the system generated a translation, re-
gardless of whether that translation is correct or
not, as a proportion of the total number of frag-
ments.
In addition to these task-specific metrics, stan-
dard MT metrics such as BLEU, NIST, METEOR
and error rates such as WER, PER and TER, are
included in the evaluation script as well. Scores
such as BLEU will generally be high (> 0.95)
when computed on the full sentence, as a large
portion of the sentence is already translated and
only a specific fragment remains to be evaluated.
Nevertheless, these generic metrics are proven in
our pilot study to follow the same trend as the
more task-specific evaluation metrics, and will be
omitted in the result section for brevity.
It regularly occurs that multiple translations are
possible. As stated, in the creation of the test set
we have taken this into account by explicitly en-
coding valid alternatives. A match with any alter-
native in the reference counts as a valid match. For
word accuracy, the highest word accuracy amongst
all possible alternatives in the reference is taken.
Likewise, participant system output may contain
multiple alternatives as well, as we allowed two
different types of runs, following the example of
the Cross-Lingual Lexical Substitution and Cross-
Lingual Word Sense Disambiguation tasks:
? Best - The system may only output one, its
best, translation;
? Out of Five - The system may output up
to five alternatives, effectively allowing 5
guesses. Only the best match is counted. This
metric does not count how many of the five
are valid.
Participants could submit up to three runs per
language pair and evaluation type.
5 Participants
Six teams submitted systems, three of which par-
ticipated for all language pairs. In alphabetic or-
der, these are:
1. CNRC - Cyril Goutte, Michel Simard, Ma-
rine Carpuat - National Research Council
Canada ? All language pairs
2. IUCL - Alex Rudnick, Liu Can, Levi King,
Sandra K?ubler, Markus Dickinson - Indiana
University (US) ? all language pairs
3. UEdin - Eva Hasler - University of Ed-
inburgh (UK) ? all language pairs except
English-German
4. UNAL - Sergio Jim?enez, Emilio Silva - Uni-
versidad Nacional de Colombia ? English-
Spanish
5. Sensible - Liling Tan - Universit?at des Saar-
landes (Germany) and Nanyang Technolog-
ical University (Singapore) ? all language
pairs
6. TeamZ - Anubhav Gupta - Universit?e de
Franche-Comt?e (France) ? English-Spanish,
English-German
Participants implemented distinct methodolo-
gies and implementations. One obvious avenue of
tackling the problem is through standard Statisti-
cal Machine Translation (SMT). The CNRC team
takes a pure SMT approach with few modifica-
tions. They employ their own Portage decoder and
directly send an L1 fragment in an L2 context, cor-
responding to a partial translation hypothesis with
only one fragment left to decode, to their decoder
(Goutte et al., 2014). The UEdin team applies a
similar method using the Moses decoder, marking
the L2 context so that the decoder leaves this con-
text as is. In addition they add a context similarity
feature for every phrase pair in the phrase transla-
tion table, which expresses topical similarity with
the test context. In order to properly decode, the
phrase table is filtered per test sentence (Hasler,
2014). The IUCL and UNAL teams do make use
of the information from word alignments or phrase
translation tables, but do not use a standard SMT
decoder. The IUCL system combines various in-
formation sources in a log-linear model: phrase
table, L2 Language Model, Multilingual Dictio-
nary, and a dependency-based collocation model,
although this latter source was not finished in time
for the system submission (Rudnick et al., 2014).
The UNAL system extracts syntactic features as a
means to relate L1 fragments with L2 context to
their L2 fragment translations, and uses memory-
based classifiers to achieve this (Silva-Schlenker
et al., 2014). The two systems on the lower end of
the result spectrum use different techniques alto-
gether. The Sensible team approaches the problem
39
by attempting to emulate the manual post-editing
process human translators employ to correct MT
output (Tan et al., 2014), whereas TeamZ relies on
Wiktionary as the sole source (Gupta, 2014).
6 Results
The results of the six participating teams can be
viewed in consensed form in Table 1. This table
shows the highest word accuracy achieved by the
participants, in which multiple system runs have
been aggregated. A ranking can quickly be dis-
tilled from this, as the best score is marked in
bold. The system by the University of Edinburgh
emerges as the clear winner of the task. The full
results of the various system runs by the six par-
ticipants are shown in Tables 2 and 3, two pages
down, all three aforementioned evaluation metrics
are reported there and the systems are sorted by
word accuracy per language pair and evaluation
type.
Team en-es oof en-de oof
CNRC 0.745 0.887 0.717 0.868
IUCL 0.720 0.847 0.722 0.857
UEdin 0.827 0.949 - -
UNAL 0.809 0.880 - -
Sensible 0.351 0.231 0.233 0.306
TeamZ 0.333 0.386 0.293 0.385
fr-en oof nl-en oof
CNRC 0.694 0.839 0.610 0.723
IUCL 0.682 0.800 0.679 0.753
UEdin 0.824 0.939 0.692 0.811
UNAL - - - -
Sensible 0.116 0.14 0.152 0.171
TeamZ - - - -
Table 1: Highest word accuracy per team, per lan-
guage pair, and per evaluation type (out-of-five is
include in the ?oof? column). The best score in
each column is marked in bold.
For the lowest-ranking participants, the score is
negatively impacted by the low recall; their sys-
tems could not find translations for a large number
of fragments.
Figures 1 (next page) and 2 (last page) show the
results for the best evaluation type for each sys-
tem run. Three bars are shown; from left to right
these represent accuracy (blue), word-accuracy
(green) and recall (red). Graphs for out-of-five
evaluation were omitted for brevity, but tend to fol-
low the same trend with scores that are somewhat
higher. These scores can be viewed on the result
website at http://github.com/proycon/
semeval2014task5/. The result website also
holds the system output and evaluation scripts with
which all graphs and tables can be reproduced.
We observe that the best scoring team in the
task (UEdin), as well as the CNRC team, both em-
ploy standard Statistical Machine Translation and
achieve high results. From this we can conclude
that standard SMT techniques are suitable for this
task. Teams IUCL and UNAL achieve similarly
good results, building on word and phrase align-
ment data as does SMT, yet not using a traditional
SMT decoder. TeamZ and Sensible, the two sys-
tems ranked lowest do not rely on any techniques
from SMT. To what extent the context-informed
measures of the various participants are effective
can not be judged from this comparison, but can
only be assessed in comparison to their own base-
lines. For this we refer to the system papers of the
participants.
7 Discussion
We did not specify any training data for the task.
The advantage of this is that participants were free
to build a wider variety of systems from various
sources, rather than introducing a bias towards for
instances statistical systems. The disadvantage,
however, is that a comparison of the various sys-
tems does not yield conclusive results regarding
the merit of their methodologies. Discrepancies
might at least be partly due to differences in train-
ing data, as it is generally well understood in MT
that more training data improves results. The base-
lines various participants describe in their system
papers provide more insight to the merit of their
approaches than a comparison between them.
In the creation of the test set, we aimed to mimic
intermediate to high-level language learners. We
also aimed at a fair distribution of different part-
of-speech categories and phrasal length. The dif-
ficulty of the task differs between language pairs,
though not intentionally so. We observe that the
Dutch-English set is the hardest and the Spanish-
English is the easiest in the task. One of the par-
ticipants implicitly observes this through measure-
ment of the number of Out-of-Vocabulary words
(Goutte et al., 2014). This implies that when com-
paring system performance between different lan-
guage pairs, one can not simply ascribe a lower
result to a system having more difficulty with said
40
Figure 1: English to Spanish (top), English to German (middle) and French to English (bottom). The
three bars, left-to-right, represent Accuracy (blue), Word Accuracy (green) and Recall (red).
41
System Acc W.Acc. Recall
English-Spanish (best)
UEdin-run2 0.755 0.827 1.0
UEdin-run1 0.753 0.827 1.0
UEdin-run3 0.745 0.82 1.0
UNAL-run2 0.733 0.809 0.994
UNAL-run1 0.721 0.794 0.994
CNRC-run1 0.667 0.745 1.0
CNRC-run2 0.651 0.735 1.0
IUCL-run1 0.633 0.72 1.0
IUCL-run2 0.633 0.72 1.0
Sensible-wtmxlingyu 0.239 0.351 0.819
TeamZ-run1 0.223 0.333 0.751
Sensible-wtm 0.145 0.175 0.470
Sensible-wtmxling 0.141 0.171 0.470
English-Spanish (out-of-five)
UEdin-run3 0.928 0.949 1.0
UEdin-run1 0.924 0.946 1.0
UEdin-run2 0.92 0.944 1.0
CNRC-run1 0.843 0.887 1.0
CNRC-run2 0.837 0.884 1.0
UNAL-run1 0.823 0.88 0.994
IUCL-run1 0.781 0.847 1.0
IUCL-run2 0.781 0.847 1.0
Sensible-wtmxlingyu 0.263 0.416 0.819
TeamZ-run1 0.277 0.386 0.751
Sensible-wtm 0.173 0.231 0.470
Sensible-wtmxling 0.169 0.228 0.470
English-German (best)
IUCL-run2 0.665 0.722 1.0
CNRC-run1 0.657 0.717 1.0
CNRC-run2 0.645 0.702 1.0
TeamZ-run1 0.218 0.293 0.852
IUCL-run1 0.198 0.252 1.0
Sensible-wtmxlingyu 0.162 0.233 0.878
Sensible-wtm 0.16 0.184 0.647
Sensible-wtmxling 0.152 0.178 0.647
English-German (out-of-five)
CNRC-run1 0.834 0.868 1.0
CNRC-run2 0.828 0.865 1.0
IUCL-run2 0.806 0.857 1.0
TeamZ-run1 0.307 0.385 0.852
IUCL-run1 0.228 0.317 1.0
Sensible-wtmxlingyu 0.18 0.306 0.878
Sensible-wtm 0.182 0.256 0.647
Sensible-wtmxling 0.174 0.25 0.647
Table 2: Full results for English-Spanish and
English-German.
language pair. This could rather be an intrinsic
property of the test set or the distance between the
languages.
Distance in syntactic structure between lan-
guages also defines the limits of this task. Dur-
ing composition of the test set it became clear that
backing off to L1 was not always possible when
syntax diverged to much. An example of this is
separable verbs in Dutch and German. Consider
the German sentence ?Er ruft seine Mutter an?
(translation: ?He calls his mother?). Imagine
System Acc W.Acc. Recall
French-English (best)
UEdin-run1 0.733 0.824 1.0
UEdin-run2 0.731 0.821 1.0
UEdin-run3 0.723 0.816 1.0
CNRC-run1 0.556 0.694 1.0
CNRC-run2 0.533 0.686 1.0
IUCL-run1 0.545 0.682 1.0
IUCL-run2 0.545 0.682 1.0
Sensible-wtmxlingyu 0.081 0.116 0.321
Sensible-wtm 0.055 0.067 0.210
Sensible-wtmxling 0.055 0.067 0.210
French-English (out-of-five)
UEdin-run2 0.909 0.939 1.0
UEdin-run1 0.905 0.938 1.0
UEdin-run3 0.907 0.937 1.0
CNRC-run1 0.739 0.839 1.0
CNRC-run2 0.731 0.834 1.0
IUCL-run1 0.691 0.8 1.0
IUCL-run2 0.691 0.8 1.0
Sensible-wtmxlingyu 0.085 0.14 0.321
Sensible-wtmxling 0.061 0.09 0.210
Sensible-wtm 0.061 0.089 0.210
Dutch-English (best)
UEdin-run1 0.575 0.692 1.0
UEdin-run2 0.567 0.688 1.0
UEdin-run3 0.565 0.688 1.0
IUCL-run1 0.544 0.679 1.0
IUCL-run2 0.544 0.679 1.0
CNRC-run1 0.45 0.61 1.0
CNRC-run2 0.444 0.609 1.0
Sensible-wtmxlingyu 0.115 0.152 0.335
Sensible-wtm 0.092 0.099 0.214
Sensible-wtmxling 0.088 0.095 0.214
Dutch-English (out-of-five)
UEdin-run1 0.733 0.811 1.0
UEdin-run3 0.727 0.808 1.0
UEdin-run2 0.725 0.808 1.0
IUCL-run1 0.634 0.753 1.0
IUCL-run2 0.634 0.753 1.0
CNRC-run1 0.606 0.723 1.0
CNRC-run2 0.602 0.721 1.0
Sensible-wtmxlingyu 0.123 0.171 0.335
Sensible-wtm 0.099 0.115 0.214
Sensible-wtmxling 0.096 0.112 0.214
Table 3: Full results for French-English and
Dutch-English.
a German language learner wanting to compose
such a sentence but wanting to fall back to En-
glish for the verb ?to call?, which would translate
to German as ?anrufen?. The possible input sen-
tence may still be easy to construe: ?Er calls seine
Mutter?, but the solution to this problem would
require insertion at two different points, whereas
the task currently only deals with a substitution of
a single fragment. The reverse is arguably even
more complex and may stray too far from what
a language learner may do. Consider an English
language learner wanting to fall back to her na-
42
tive German, struggling with the English transla-
tion for ?anrufen?. She may compose a sentence
such as ?He ruft his mother an?, which would
require translating two dependent fragments into
one.
We already have interesting examples in the
gold standard, such as example (b), showing syn-
tactic word-order changes confined to a single
fragment.
(b) Input: I always wanted iemand te zijn , but now I
realize I should have been more specific.
Reference: I always wanted to be somebody , but
now I realize I should have been more specific.
Participant output (aggregated): to be a person; it to
be; someone to his; to be somebody; person to be;
someone to; someone to be; to be anybody; to anyone;
to be someone; a person to have any; to be someone
else
Another question we can ask, but have not in-
vestigated, is whether a language learner would
insert the proper morphosyntactic form of an L1
word given the L2 context, or whether she may
be inclined to fall back to a normal form such
as an infinitive. Especially in the above case of
separable verbs someone may be more inclined to
circumvent the double fragments and provide the
input: ?He anrufen his mother?, but in simpler
cases the same issue arises as well. Consider an
English learner falling back to her native Croatian,
a Slavic language which heavily declines nouns.
If she did not know the English word ?book? and
wanted to write ?He gave the book to him?, she
could use either the Croatian word ?knjigu? in its
accusative declension or fall back to the normal
form ?knjiga?. A proper writing assistant system
would have to account for both options.
We can analyse which of the sentences in the
test data participants struggled with most. First
we look at the number of sentences that produce
an average word accuracy of zero, measured per
sentence over all systems and runs in the out-of-
five metric. This means no participant was close
to the correct output. There were 6 such sentences
in English-Spanish, 17 in English-German, 6 in
French-English, and 32 in Dutch-English.
A particularly difficult context from the Span-
ish set is when a subjunctive verb form was re-
quired, but an indicative verb form was submit-
ted by the systems, such as in the sentence: ?Es-
pero que los frenos del coche funcionen bien.?.
Though this may be deduced from context (the
word ?Espero?, expressing hope yet doubt, be-
ing key here), it is often subtle and hard to cap-
ture. Another problematic case that recurs in the
German and Dutch data sets is compound nouns.
The English fragment ?work motivation? should
translate into the German compound ?Arbeitsmo-
tivation? or ?Arbeitsmoral?, yet participants were
not able to find the actual compound noun. Beside
compound nouns, other less frequent multi-word
expressions are also amongst the difficult cases.
Sparsity or complete absence in training data of
these expressions is why systems struggle here.
Another point of discussion is the fact that we
enriched the test set by adding previously unavail-
able alternative translations from an aggregated
pool of system output. This might draw criticism
for possibly introducing a bias, also considering
the fact that the decision to include a particular al-
ternative for a given context is not always straight-
forward and at times subjective. We, however,
contend that this is the best way to ensure that
valid system output is not discarded and reduce the
number of false negatives. The effect of this mea-
sure has been an increase in (word) accuracy for
all systems, without significant impact on ranking.
8 Conclusion
In this SemEval task we showed that systems can
translate L1 fragments in an L2 context, a task
that finds application in computer-assisted trans-
lation and computer-assisted language learning.
The localised translation of a fragment in a cross-
lingual context makes it a novel task in the field.
Though the task has its limits, we argue for its
practical application in a language-learning set-
ting: as a writing assistant and dictionary replace-
ment. Six contestants participated in the task,
and used an ensemble of techniques from Statis-
tical Machine Translation and Word Sense Disam-
biguation. Most of the task organizers? time went
into manually establishing a gold standard based
on a wide variety of sources, most aimed at lan-
guage learners, for each of the four language pairs
in the task. We have been positively surprised by
the good results of the highest ranking systems.
9 Acknowledgements
We would like to thank Andreu van Hooft and
Sarah Schulz for their manual correction work,
and Sean Banville, Geert Joris, Bernard De Clerck,
Rogier Crijns, Adriane Boyd, Detmar Meurers,
Guillermo Sanz Gallego and Nils Smeuninx for
helping us with the data collection.
43
Figure 2: Dutch to English.
References
Andrea Abel, Lionel Nicolas, Jirka Hana, Barbora
?
Stindlov?a, Katrin Wisniewski, Claudia Woldt, Det-
mar Meurers, and Serhiy Bykh. 2013. A trilingual
learner corpus illustrating european reference lev-
els. In Proceedings of the Learner Corpus Research
Conference, Bergen, Norway, 27-29 September.
Cyril Goutte, Michel Simard, and Marine Carpuat.
2014. CNRC-TMT: Second language writing as-
sistant system description. In Proceedings of the
8th International Workshop on Semantic Evaluation
(SemEval-2014), Dublin, Ireland.
Anubhav Gupta. 2014. Team Z: Wiktionary as L2
writing assistant. In Proceedings of the 8th Interna-
tional Workshop on Semantic Evaluation (SemEval-
2014), Dublin, Ireland.
Eva Hasler. 2014. UEdin: Translating L1 phrases in
L2 context using context-sensitive smt. In Proceed-
ings of the 8th International Workshop on Semantic
Evaluation (SemEval-2014), Dublin, Ireland.
Philipp Koehn, Hieu Hoang, Alexandra Birch,
Chris Callison-Burch, Marcello Federico,
Nicola Bertoldi, Brooke Cowan, Wade Shen,
Christine Moran, Richard Zens, Chris Dyer,
Ohttp://www.aclweb.org/anthology/P/P07/P07
2045ndrej Bojar, Alexandra Constantin, and Evan
Herbst. 2007. Moses: Open source toolkit for
statistical machine translation. In Proceedings of
the 45th Annual Meeting of the Association for
Computational Linguistics Companion Volume
Proceedings of the Demo and Poster Sessions,
pages 177?180, Prague, Czech Republic, June.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In In Proceedings
of the Machine Translation Summit X ([MT]?05).,
pages 79?86.
Els Lefever and Veronique Hoste. 2013. SemEval-
2013 Task 10: Cross-Lingual Word Sense Disam-
biguation. In Proceedings of the 7th International
Workshop on Semantic Evaluation (SemEval 2013),
in conjunction with the Second Joint Conference on
Lexical and Computational Semantics.
Rada Mihalcea, Ravi Sinha, and Diana McCarthy.
2010. Semeval 2010 task 2: Cross-lingual lex-
ical substitution. In Proceedings of the 5th
International Workshop on Semantic Evaluations
(SemEval-2010), Uppsala, Sweden.
Franz Josef Och and Hermann Ney. 2000. Giza++:
Training of statistical translation models. Technical
report, RWTH Aachen, University of Technology.
Alex Rudnick, Levi King, Can Liu, Markus Dickinson,
and Sandra K?ubler. 2014. IUCL: Combining infor-
mation sources for semeval task 5. In Proceedings
of the 8th International Workshop on Semantic Eval-
uation (SemEval-2014), Dublin, Ireland.
Emilio Silva-Schlenker, Sergio Jimenez, and Julia Ba-
quero. 2014. UNAL-NLP: Cross-lingual phrase
sense disambiguation with syntactic dependency
trees. In Proceedings of the 8th International
Workshop on Semantic Evaluation (SemEval-2014),
Dublin, Ireland.
Liling Tan, Anne Schumann, Jos?e Martinez, and Fran-
cis Bond. 2014. Sensible: L2 translation assistance
by emulating the manual post-editing process. In
Proceedings of the 8th International Workshop on
Semantic Evaluation (SemEval-2014), Dublin, Ire-
land.
Maarten van Gompel and Antal van den Bosch. 2014.
Translation assistance by translation of L1 frag-
ments in an L2 context. In To appear in Proceedings
of ACL 2014.
44
