Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 662?670,
Beijing, August 2010
Adaptive Development Data Selection for Log-linear Model
in Statistical Machine Translation
Mu Li
Microsoft Research Asia
muli@microsoft.com
Yinggong Zhao?
Nanjing University
zhaoyg@nlp.nju.edu.cn
Dongdong Zhang
Microsoft Research Asia
dozhang@microsoft.com
Ming Zhou
Microsoft Research Asia
mingzhou@microsoft.com
Abstract
This paper addresses the problem of dy-
namic model parameter selection for log-
linear model based statistical machine
translation (SMT) systems. In this work,
we propose a principled method for this
task by transforming it to a test data de-
pendent development set selection prob-
lem. We present two algorithms for au-
tomatic development set construction, and
evaluated our method on several NIST
data sets for the Chinese-English trans-
lation task. Experimental results show
that our method can effectively adapt
log-linear model parameters to different
test data, and consistently achieves good
translation performance compared with
conventional methods that use a fixed
model parameter setting across different
data sets.
1 Introduction
In recent years, log-linear model (Och and Ney,
2002) has been a mainstream method to formu-
late statistical models for machine translation. Us-
ing this formulation, various kinds of relevant
properties and data statistics used in the transla-
tion process, either on the monolingual-side or on
the bilingual-side, are encoded and used as real-
valued feature functions, thus it provides an ef-
fective mathematical framework to accommodate
a large variety of SMT formalisms with different
computational linguistic motivations.
?This work was done while the author was visiting Mi-
crosoft Research Asia.
Formally, in a log-linear SMT model, given a
source sentence f , we are to find a translation e?
with largest posterior probability among all possi-
ble translations:
e? = argmax
e
Pr(e|f)
and the posterior probability distribution Pr(e|f)
is directly approximated by a log-linear formula-
tion:
Pr(e|f) = p?(e|f)
= exp(
?M
m=1 ?mhm(e, f))?
e? exp(
?M
m=1 ?mhm(e?, f))
(1)
in which hm?s are feature functions and ? =
(?1, . . . , ?M ) are model parameters (feature
weights).
For a successful practical log-linear SMT
model, it is usually a combined result of the sev-
eral efforts:
? Construction of well-motivated SMT models
? Accurate estimation of feature functions
? Appropriate scaling of log-linear model fea-
tures (feature weight tuning).
In this paper, we focus on the last mentioned
issue ? parameter tuning for log-linear model.
In general, log-linear model parameters are opti-
mized on a held-out development data set. Us-
ing this method, similarly to many machine learn-
ing tasks, the model parameters are solely tuned
based on the development data, and the optimal-
ity of obtained model on unseen test data relies
on the assumption that both development and test
data observe identical probabilistic distribution,
662
which often does not hold for real-world data. The
goal of this paper is to investigate novel meth-
ods for test data dependent model parameter se-
lection. We begin with discussing the principle
of parameter learning for log-linear SMT models,
and explain the rationale of task transformation
from parameter selection to development data se-
lection. We describe two algorithms for automatic
development set construction, and evaluated our
method on several NIST MT evaluation data sets.
Experimental results show that our method can ef-
fectively adapt log-linear model parameters to dif-
ferent test data and achieves consistent good trans-
lation performance compared with conventional
methods that use a group of fixed model param-
eters across different data sets.
2 Model Learning for SMT with
Log-linear Models
Model learning refers to the task to estimate a
group of suitable log-linear model parameters
? = (?1, . . . , ?M ) for use in Equation 1, which is
often formulated as an optimization problem that
finds the parameters maximizing certain goodness
of the translations generated by the learnt model
on a development corpus D. The goodness can be
measured with either the translations? likelihood
or specific machine translation evaluation metrics
such as TER or BLEU.
More specifically, let e? be the most probable
translation of D with respect to model parameters
?, and E(e?,?, D) be a score function indicating
the goodness of translation e?, then a parameter
estimation algorithm will try to find the ? which
satisfies:
?? = argmax
?
E(e?,?, D) (2)
Note when the goodness scoring function E(?)
is specified, the parameter learning criterion in
Equation 2 indicates that the derivation of model
parameters ?? only depends on development data
D, and does not require any knowledge of test
data T . The underlying rationale for this rule is
that if the test data T observes the same distribu-
tion as D, ?? will be optimal for both of them.
On the other side, however, when there are mis-
matches between development and test data, the
translation performance on test data will be sub-
optimal, which is very common for real-world
data. Due to the difference between data sets, gen-
erally there is no such ?? that is optimal for multi-
ple data sets at the same time. Table 1 shows some
empirical evidences when two data sets are mutu-
ally used as development and test data. In this set-
ting, we used a hierarchical phrase based decoder
and 2 years? evaluation data of NIST Chinese-
to-English machine translation task (for the year
2008 only the newswire subset was used because
we want to limit both data sets within the same do-
main to show that data mismatch also exists even
if there is no domain difference), and report re-
sults using BLEU scores. Model parameters were
tuned using the MERT algorithm (Och, 2003) op-
timized for BLEU metric.
Dev data MT05 MT08-nw
MT05 0.402 0.306
MT08-nw 0.372 0.343
Table 1: Translation performance of cross devel-
opment/test on two NIST evaluation data sets.
In our work, we present a solution to this prob-
lem by using test data dependent model parame-
ters for test data translation. As discussed above,
since model parameters are solely determined by
development dataD, selection of log-linear model
parameters is basically equivalent to selecting a
set of development data D.
However, automatic development data selection
in current SMT research remains a relatively open
issue. Manual selection based on human experi-
ence and observation is still a common practice.
3 Adaptive Model Parameter Selection
An important heuristic behind manual develop-
ment data selection is to use the dataset which is
as similar to test set as possible in order to work
around the data mismatch problem to maximal
extent. There are also empirical evidences sup-
porting this heuristics. For instance, it is gener-
ally perceived that data set MT03 is more similar
to MT05, while MT06-nw is closer to MT08-nw.
Table 2 shows experimental results using model
parameters induced from MT03 and MT06-nw as
663
development sets with the same settings as in Ta-
ble 1. As expected, MT06-nw is far more suitable
than MT03 as the development data for MT08-
nw; yet for test set MT05, the situation is just the
opposite.
Dev data MT05 MT08-nw
MT03 0.397 0.306
MT06-nw 0.381 0.337
Table 2: Translation performance on different test
sets of using different development sets.
In this work, this heuristic is further exploited
for automatic development data selection when
there is no prior knowledge of the test data avail-
able. In the following discussion, we assume the
availability of a set of candidate source sentences
together with translation references that are qual-
ified for the log-linear model parameter learning
task. Let DF be the full candidate set, given a test
set T , the task of selecting a set of development
data which can optimize the translation quality on
T can be transformed to searching for a suitable
subset of DF which is most similar to T :
D? = argmax
D?DF
Sim(D,T )
To achieve this goal, we need to address the fol-
lowing key issues:
? How to define and compute Sim(D,T ), the
similarity between different data sets;
? How to extract development data sets from a
full candidate set for unseen test data.
3.1 Dataset Similarity
Computing document similarity is a classical task
in many research areas such as information re-
trieval and document classification. However, typ-
ical methods for computing document similarity
may not be suitable for our purpose. The reasons
are two-fold:
1. The sizes of both development and test data
are small in usual circumstances, and using
similarity measures such as cosine or dice
coefficient based on term vectors will suffer
from severe data sparseness problems. As a
result, the obtained similarity measure will
not be statistically reliable.
2. More importantly, what we care about here
is not the surface string similarity. Instead,
we need a method to measure how similar
two data sets are from the view of a log-linear
SMT model.
Next we start with discussing the similarity
between sentences. Given a source sentence
f , we denote its possible translation space with
H(f). In a log-linear SMT model, every trans-
lation e ? H(f) is essentially a feature vector
h(e) = (h1, . . . , hM ). Accordingly, the similar-
ity between two sentences f1 and f2 should be de-
fined on the feature space of the model in use. Let
V (f) = {h(e) : e ? H(f)} be the set of feature
vectors for all translations inH(f), we have
Sim(f1, f2) = Sim
(
V (f1),V (f2)
)
(3)
Because it is not practical to compute Equation
3 directly by enumerating all translations inH(f1)
and H(f2) due to the huge search space in SMT
tasks, we need to resort to some approximations.
A viable solution to this is that if we can use a
single feature vector h?(f) to represent V (f), then
Equation 3 can be simply computed using existing
vector similarity measures.
One reasonable method to derive h?(f) is to use
a feature vector based on the average principle ?
each dimension of the vector is set to the expec-
tation of its corresponding feature value over all
translations:
h?(f) =
?
e?H(f)
P (e|f)h(e) (4)
An alternative and much simpler way to com-
pute h?(f) is to employ the max principle in which
we just use the feature vector of the best transla-
tion inH(f):
h?(f) = h(e?) (5)
where e? = argmaxe P (e|f).
Note that in both Equation 4 and Equation 5
we make use of e?s posterior probability P (e|f).
664
Since the true distribution is unknown, a pre-
learnt modelM has to be used to assign approxi-
mate probabilities to translations, which indicates
that the obtained similarity depends on a specific
model. As a convention, we use SimM(f1, f2) to
denote the similarity between f1 and f2 based on
M, and callM the reference model of the com-
puted similarity. To avoid unexpected bias caused
by a single reference model, multiple reference
models can be simultaneously used, and the simi-
larity is defined to be the maximum of all model-
dependent similarity values:
Sim(f1, f2) = maxM SimM(f1, f2) (6)
where M belongs to {M1, . . . ,Mn}, which is
the set of reference models under consideration.
To generalize this method to data set level, we
compute the vector h?(S) for a data set S =
(f1, . . . , f|S|) as follows:
h?(S) =
|S|?
i=1
h?(fi) (7)
3.2 Development Sets Pre-construction
In the following, we sketch a method for automat-
ically building a set of development data based on
the full candidate set DF before seeing any test
data.
Theoretically, a subset of DF containing ran-
domly sampled sentences from DF will not meet
our requirement well because it is very probable
that it will observe a distribution similar to DF .
What we expect is that the pre-built development
sets can approximate as many as possible typi-
cal data distributions that can be estimated from
subsets of DF . Our solution is based on the as-
sumption that DF can be depicted by some mix-
ture models, hence we can use classical cluster-
ing methods such as k-means to partition DF into
subsets with different distributions.
Let SF be the set of extracted development data
from DF . The construction of SDF proceeds as
following:
1. Train a log-linear model MF using DF as
development data;
2. Compute a feature vector h?(d)1 for each sen-
tence d ? DF usingMF as reference model;
3. Cluster sentences in DF using h?(d)/|d| as
feature vectors;
4. Add obtained sentence clusters to SDF as
candidate development sets.
In the third step, since the feature vector h?(d)
is defined at sentence level, it is averaged by the
number of words in d so that it is irrelevant to the
length of a sentence. Considering the outputs of
unsupervised data clustering methods are usually
sensitive to initial conditions, we include in SDF
sentence clusters based on different initialization
configurations to remove related random effects.
An initialization configuration for sentence clus-
tering in our work includes starting point for each
cluster and total number of clusters. In fact, the
inclusion of more sentence clusters increases the
diversity of the resulted SDF as well.
At decoding time, when a test set T is pre-
sented, we compute the similarity between T and
each development set D ? SDF , and choose the
one with largest similarity score as the develop-
ment set for T :
D? = argmax
D?SDF
Sim(T,D) (8)
When a single reference model is used to com-
pute Sim(T,D), MF is a natural choice. In the
multi-model setting as shown in Equation 6, mod-
els learnt from the development sets in SDF can
serve this purpose.
Note in this method model learning is not re-
quired for every new test set because the model
parameters for each development set in SDF can
also be pre-learnt and ready to be used for decod-
ing.
3.3 Dynamic Development Set Construction
In the previous method, test data T is only in-
volved in the process of choosing a development
set from a list of candidates but not in process of
development set construction. Next we present a
1Throughout this paper, a development sentence d gener-
ally refers to the source part of it if there is no extra explana-
tion.
665
method for building a development set on demand
based on test data T .
Let DF = (d1, . . . , dn) be the data set con-
taining all candidate sentences for development
data selection. The method is iterative process in
which development data and learnt model are al-
ternatively updated. Detailed steps are illustrated
as follows:
1. Let i = 0, D0 = DF ;
2. Train a modelMi based on Di;
3. For each dk ? DF , compute the similarity
score SimMi(T, dk) between T and dk based
on modelMi;
4. Select top n candidate sentences with highest
similarity scores from DF to form Di+1;
5. Repeat step 2 to step 4 until the similarity be-
tween T and latest selected development data
converges (the increase in similarity measure
is less than a specified threshold compared to
last round) or the specified iteration limit is
reached.
In step 4, Di+1 is greedily extracted from DF ,
and there is no guarantee that SimMi(T,Di+1)
will increase or decrease after a new sentence is
added to Di+1. Thereby the number of selected
sentences n needs to be empirically determined.
If n is too small, neither the selected data nor the
learnt model parameters will be statistically reli-
able; while if n is too large, we may have to in-
clude some sentences that are not suitable for test
data in the development data, and miss the oppor-
tunity to extract the most desirable development
set.
One drawback of this method is the relatively
high computational cost because it requires multi-
ple parameter training passes when any test set is
presented to the system for translation.
4 Experiments
4.1 Data
Experiments were conducted on the data sets
used for NIST Chinese-English machine transla-
tion evaluation tasks. MT03 and MT06 data sets,
which contain 919 and 1,664 sentences respec-
tively, were used for development data in vari-
ous settings. MT04, MT05 and MT08 data sets
were used for test purpose. In some settings, we
also used a test set MT0x, which containing 1,000
sentences randomly sampled from the above 3
data sets. All the translation performance results
were measured in terms of case-insensitive BLEU
scores.
For all experiments, all parallel corpora avail-
able to the constrained track of NIST 2008
Chinese-English MT evaluation task were used
for translation model training, which consist of
around 5.1M bilingual sentence pairs. GIZA++
was used for word alignment in both directions,
which was further refined with the intersec-diag-
grow heuristics.
We used a 5-gram language model which was
trained from the Xinhua portion of English Giga-
word corpus version 3.0 from LDC and the En-
glish part of parallel corpora.
4.2 Machine Translation System
We used an in-house implementation of the hierar-
chical phrase-based decoder as described in Chi-
ang (2005). In addtion to the standard features
used in Chiang (2005), we also used a lexicon fea-
ture indicating how many word paris in the trans-
lation found in a conventional Chinese-English
lexicon. Phrasal rules were extracted from all the
parallel data, but hierarchical rules were only ex-
tracted from the FBIS part of the parallel data
which contains around 128,000 sentence pairs.
For all the development data, feature weights of
the decoder were tuned using the MERT algorithm
(Och, 2003).
4.3 Results of Development Data
Pre-construction
In the following we first present some overall re-
sults using the method of development data pre-
construction, then dive into more detailed settings
of the experiments.
Table 3 shows the results using 3 different data
sets for log-linear model parameter tuning. El-
ements in the first column indicate the data sets
used for parameter tuning, and other columns con-
tain evaluation results on different test sets. In the
666
Tuning set MT04 MT05 MT08 MT0x
MT03 0.399 / 0.392 0.395 / 0.390 0.241 / 0.258 0.319 / 0.322
MT06 0.381 / 0.388 0.382 / 0.391 0.275 / 0.283 0.343 / 0.342
MT03+MT06 0.391 / 0.401 0.392 / 0.397 0.265 / 0.281 0.336 / 0.345
Oracle cluster 0.401 0.398 0.293 0.345
Self-training 0.406 0.402 0.298 0.351
Table 3: Translation performance using different methods and data sets for parameter tuning.
third row of the table, MT03+MT06 means com-
bining the data sets of MT03 and MT06 together
to form a larger tuning set. The first number in
each cell denotes the BLEU score using the tuning
set as standard development setD, and the second
for using the tuning set as a candidate set DF .
For all experiment settings in the table, we
used cosine value between feature vectors to mea-
sure similarity between data sets, and feature vec-
tors were computed according to Equation 5 and
Equation 7 using a reference model which is
trained on the corresponding candidate set DF as
development set.2 We adopted the k-means algo-
rithm for data clustering with the number of clus-
ters iterating from 2 to 5. In each iteration, we ran
4 passes of clustering using different initial values.
Therefore, in total there are 56 sentence clusters
generated in each SDF .3
From the table it can be seen that given
the same set of sentences (MT03, MT06 and
MT03+MT06), when they are used as the can-
didate set DF for the development set pre-
construction method, the translation performance
is generally better than when they are just used as
development sets as a whole. Using MT03 data
set as DF is an exception: there is slight perfor-
mance drop on test sets MT04 and MT05, but it
also helps reduce the performance see-saw prob-
lem on different test sets as shown in Table 1.
Meanwhile, in the other two settings of DF , we
observed significant BLEU score increase on all
test sets but MT0x (on which the performance al-
most kept unchanged). In addition, the fact that
using MT03+MT06 as DF achieves best (or al-
2For example, in all the experiments in the row of MT03
as DF , we use the same reference model trained with MT03
as development set.
3Sometimes some clusters are empty or contain too few
sentences, so the actual number may be smaller.
most best) performance on all test sets implies that
it should be a better choice to include as diverse
data as possible in DF .
We also appended two oracle BLEU numbers
for each test set in Table 3 for reference. One is
denoted with oracle cluster, which is the high-
est possible BLEU that can be achieved on the
test set when the development set must be cho-
sen from the sentence clusters in SMT03+MT06.
The other is labeled as self-training, which is the
BLEU score that can be obtained when the test
data itself is used as development data. This num-
ber can serve as actual performance upper bound
on the test set.
Next we investigated the impact of using dif-
ferent ways to compute feature vectors presented
in Section 3.1. We re-ran some previous exper-
iments on test sets MT04, MT05 and MT08 us-
ing MT03+MT06 as DF . Most settings were kept
unchanged except that the feature vector of each
sentence was computed according to Equation 4.
A 20-best translation list was used to approximate
H(f). The results are shown in Table 4.
Test set average max
MT04 0.397 0.401
MT05 0.393 0.397
MT08 0.286 0.281
Table 4: Translation performance when using av-
eraged feature values for similarity computation.
The numbers in the second column are based
on Equation 4. Numbers based on Equation 5 are
also listed in the third column for comparison. In
all the experiment settings we did not observe con-
sistent or significant advantage when using Equa-
tion 4 over using Equation 5. Since Equation 5
667
is much simpler, it is a good decision to use it in
practice. So did we conduct all following experi-
ments based on Equation 5.
We are also interested in the correlation be-
tween two measures: the similarity between de-
velopment and test data and the actual translation
performance on test data.
First we would like to echo the motivating ex-
periment presented in Section 3. Table 5 shows
the similarity between the data sets used in the ex-
periment withMMT03+MT06 as reference model.
Obviously the results in Table 2 and Table 5 fit
each other very well.
Dev data MT05 MT08-nw
MT03 0.99988 0.99012
MT06-nw 0.99004 0.99728
Table 5: Similarity between NIST data sets.
Figure 1 shows the results of a set of more com-
prehensive experiments on MT05 data set con-
cerning the similarity between development and
test sets.
 0.28
 0.3
 0.32
 0.34
 0.36
 0.38
 0.4
 10  20  30  40  50  60
BL
EU
Rank of development set
Multiple
MT03+MT06
MT06
Figure 1: Correlation between similarity and
BLEU on MT05 data set
In the figure, every data line shows how BLEU
score changes when different pre-built develop-
ment set in SMT03+MT06 is used for model learn-
ing. The data points in each line are sorted by the
rank of similarity between the development set in
use and the MT05 data set. We also compared re-
sults based on 3 reference model settings. In the
first one (multiple), the similarity was computed
using Equation 6, and the reference model set con-
tains all models learnt from the development sets
in SMT03+MT06. The other two settings use refer-
ence models learnt from MT06 and MT03+MT06
data sets respectively.
We can observe from the figure that the corre-
lation between BLEU scores and data set similar-
ity can only be identified on macro scales for all
the three similarity settings. Although using data
similarity may not be able to select the perfect de-
velopment data set from SDF , by picking a devel-
opment set with highest similarity score, we can
usually (almost always) get good enough BLEU
scores in our experiments.
4.4 Results of Development Data Dynamic
Generation
We ran two sets of experiments for the method of
development data dynamic construction.
The first one was designed to investigate how
the size of extracted development data affects the
translation performance. Using MT05 and MT08
as test sets and MT03+MT06 as DF , we ran ex-
periments for the algorithm presented in Section
3.3 with n = 200 to n = 1, 000. In this ex-
periment we did not observe significant enough
changes in BLEU scores ? the difference between
the highest and lowest numbers is generally less
than 0.005.
The second one aimed at examining how BLEU
numbers changes when the extracted development
data were iteratively updated. Figure 2 shows one
set of results on test sets MT05 and MT08 using
MT03+MT06 data set as DF and n set to 400.
 0.365
 0.37
 0.375
 0.38
 0.385
 0.39
 0.395
 0.4
 0.405
 1  2  3  4  5  6  7
 0.265
 0.27
 0.275
 0.28
 0.285
 0.29
 0.295
 0.3
 0.305
M
T0
5 
BL
EU
M
T0
8 
BL
EU
Iteration
MT05
MT08
Figure 2: BLEU score as function of iteration in
dynamic development data extraction.
The similarity usually converged after 2 to 3 it-
668
erations, which is consistent with trend of BLEU
scores on test sets. However, in all our experimen-
tal settings, we did not observe any results signif-
icantly better than using the development set pre-
construction method.
5 Discussions
Some of the previous work related to building
adaptive SMT systems were discussed in the do-
main adaptation context, in which one fundamen-
tal idea is to estimate a more suitable domain-
specific translation model or language model.
When the target domain is already known, adding
a small amount of domain data (both monolingual
and bilingual) to the existing training corpora has
been shown to be very effective in practice. But
model adaptation is required in more scenarios
other than explicitly defined domains. As shown
by the results in Table 2, even for the data from
the same domain, distribution mismatch can also
be a problem.
There are also considerable efforts made to deal
with the unknown distribution of text to be trans-
lated, and the research topics were still focused on
translation and language model adaptation. Typ-
ical methods used in this direction include dy-
namic data selection (Lu? et al, 2007; Zhao et al,
2004; Hildebrand et al, 1995) and data weighting
(Foster and Kuhn, 2007; Matsoukas et al, 2009).
All the mentioned methods use information re-
trieval techniques to identify relevant training data
from the entire training corpora.
Our work presented here also makes no as-
sumption about the distribution of test data, but
it differs from the previous methods significantly
from a log-linear model?s perspective. Adjust-
ing translation and language models based on test
data can be viewed as adaptation of feature val-
ues, while our method is essentially adaptation of
feature weights. This difference makes these two
kinds of methods complementary to each other ?
it is possible to make further improvement by us-
ing both of them in one task.
To our knowledge, there is no dedicated discus-
sion on principled methods to perform develop-
ment data selection in previous research. In Lu?
et al (2007), log-linear model parameters can
also be adjusted at decoding time. But in their
approach, the adjustment was based on heuristic
rules and re-weighted training data distribution.
In addition, compared with training data selection,
the computational cost of development data selec-
tion is much smaller.
From machine learning perspective, both pro-
posed methods can be viewed as certain form
of transductive learning applied to the SMT task
(Ueffing et al, 2007). But our methods do
not rely on surface similarities between training
and training/development sentences, and develop-
ment/test sentences are not used to re-train SMT
sub-models.
6 Conclusions and Future Work
In this paper, we addressed the data mismatch is-
sue between training and decoding time of log-
linear SMT models, and presented principled
methods for dynamically inferring test data de-
pendent model parameters with development set
selection. We describe two algorithms for this
task, development set pre-construction and dy-
namic construction, and evaluated our method
on the NIST data sets for the Chinese-English
translation task. Experimental results show that
our methods are capable of consistently achiev-
ing good translation performance on multiple
test sets with different data distributions without
manual tweaking of log-linear model parameters.
Though theoretically using the dynamic construc-
tion method could bring better results, the pre-
construction method performs comparably well in
our experimental settings. Considering the fact
that the pre-consruction method is computation-
ally cheaper, it should be a better choice in prac-
tice.
In the future, we are interested in two direc-
tions. One is to explore the possibility to perform
data clustering on test set as well and choosing
suitable model parameters for each cluster sepa-
rately. The other involves dynamic SMT model
selection ? for example, some parts of the test
data fit the phrase-based model better while other
parts can be better translated using a syntax-based
model.
669
References
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Proc.
of the 43th Annual Meeting of the Association
for Computational Linguistic (ACL). Ann Arbor,
Michigan.
George Foster and Roland Kuhn. 2007. Mixture-
Model Adaptation for SMT. In Proc. of the Second
ACL Workshop on Statistical Machine Translation..
Prague, Czech Republic.
Michel Galley, Mark Hopkins, Kevin Knight and
Daniel Marcu. 2004. What?s in a translation rule?
In Proc. of the Human Language Technology Conf.
(HLT-NAACL). Boston, Massachusetts.
Almut Hildebrand, Matthias Eck, Stephan Vogel, and
Alex Waibel. 1995. Adaptation of the Transla-
tion Model for Statistical Machine translation Based
on Information Retrieval. In Proc. of EAMT. Bu-
dapest, Hungary.
Philipp Koehn, Franz Och and Daniel Marcu. 2003.
Statistical Phrase-Based Translation. In Proc. of the
Human Language Technology Conf. (HLT-NAACL).
Edmonton, Canada.
Yang Liu, Qun Liu, and Shouxun Lin. 2007. Tree-
to-string alignment template for statistical machine
translation. In Proc. of the 45th Annual Meet-
ing of the Association for Computational Linguistic
(ACL). Prague, Czech Republic.
Yajuan Lu?, Jin Huang and Qun Liu. 2007. Improv-
ing Statistical Machine Translation Performance by
Training Data Selection and Optimization. In Proc.
of the Conference on Empirical Methods in Natural
Language Processing. Prague, Czech Republic.
Spyros Matsoukas, Antti-Veikko I. Rosti and Bing
Zhang. 2009. Discriminative Corpus Weight Es-
timation for Machine Translation. In Proc. of the
Conference on Empirical Methods in Natural Lan-
guage Processing. Singapore.
Franz Och and Hermann Ney. 2002. Discriminative
Training and Maximum Entropy Models for Statis-
tical Machine Translation. In Proc. of the 40th An-
nual Meeting of the Association for Computational
Linguistic (ACL). Philadelphia, PA.
Franz Och. 2003. Minimum Error Rate Training in
Statistical Machine Translation. In Proc. of the 41th
Annual Meeting of the Association for Computa-
tional Linguistic (ACL). Sapporo, Japan.
Nicola Ueffing, Gholamreza Haffari and Anoop
Sarkar. 2007. Transductive learning for statistical
machine translation. In Proc. of the Annual Meet-
ing of the Association for Computational Linguis-
tics. Prague, Czech Republic.
Bing Zhao, Matthias Eck, and Stephan Vogel. 2004.
Language Model Adaptation for Statistical Machine
Translation with Structured Query Models. In Proc.
of COLING. Geneva, Switzerland.
670
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1387?1392,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Decoding with Large-Scale Neural Language Models
Improves Translation
Ashish Vaswani
University of Southern California
Department of Computer Science
avaswani@isi.edu
Yinggong Zhao
Nanjing University, State Key Laboratory
for Novel Software Technology
zhaoyg@nlp.nju.edu.cn
Victoria Fossum and David Chiang
University of Southern California
Information Sciences Institute
{vfossum,chiang}@isi.edu
Abstract
We explore the application of neural language
models to machine translation. We develop a
new model that combines the neural proba-
bilistic language model of Bengio et al, rec-
tified linear units, and noise-contrastive esti-
mation, and we incorporate it into a machine
translation system both by reranking k-best
lists and by direct integration into the decoder.
Our large-scale, large-vocabulary experiments
across four language pairs show that our neu-
ral language model improves translation qual-
ity by up to 1.1 Bleu.
1 Introduction
Machine translation (MT) systems rely upon lan-
guage models (LMs) during decoding to ensure flu-
ent output in the target language. Typically, these
LMs are n-gram models over discrete representa-
tions of words. Such models are susceptible to data
sparsity?that is, the probability of an n-gram ob-
served only few times is difficult to estimate reli-
ably, because these models do not use any informa-
tion about similarities between words.
To address this issue, Bengio et al (2003) pro-
pose distributed word representations, in which each
word is represented as a real-valued vector in a
high-dimensional feature space. Bengio et al (2003)
introduce a feed-forward neural probabilistic LM
(NPLM) that operates over these distributed repre-
sentations. During training, the NPLM learns both a
distributed representation for each word in the vo-
cabulary and an n-gram probability distribution over
words in terms of these distributed representations.
Although neural LMs have begun to rival or even
surpass traditional n-gram LMs (Mnih and Hin-
ton, 2009; Mikolov et al, 2011), they have not yet
been widely adopted in large-vocabulary applica-
tions such as MT, because standard maximum like-
lihood estimation (MLE) requires repeated summa-
tions over all words in the vocabulary. A variety of
strategies have been proposed to combat this issue,
many of which require severe restrictions on the size
of the network or the size of the data.
In this work, we extend the NPLM of Bengio et
al. (2003) in two ways. First, we use rectified lin-
ear units (Nair and Hinton, 2010), whose activa-
tions are cheaper to compute than sigmoid or tanh
units. There is also evidence that deep neural net-
works with rectified linear units can be trained suc-
cessfully without pre-training (Zeiler et al, 2013).
Second, we train using noise-contrastive estimation
or NCE (Gutmann and Hyva?rinen, 2010; Mnih and
Teh, 2012), which does not require repeated summa-
tions over the whole vocabulary. This enables us to
efficiently build NPLMs on a larger scale than would
be possible otherwise.
We then apply this LM to MT in two ways. First,
we use it to rerank the k-best output of a hierarchi-
cal phrase-based decoder (Chiang, 2007). Second,
we integrate it directly into the decoder, allowing the
neural LM to more strongly influence the model. We
achieve gains of up to 0.6 Bleu translating French,
German, and Spanish to English, and up to 1.1 Bleu
on Chinese-English translation.
1387
u1 u2
input
words
input
embeddings
hidden
h1
hidden
h2
output
P(w | u)
D?
M
C1 C2
D
Figure 1: Neural probabilistic language model (Bengio et
al., 2003).
2 Neural Language Models
Let V be the vocabulary, and n be the order of
the language model; let u range over contexts, i.e.,
strings of length (n?1), and w range over words. For
simplicity, we assume that the training data is a sin-
gle very long string, w1 ? ? ?wN , where wN is a special
stop symbol, </s>. We write ui for wi?n+1 ? ? ?wi?1,
where, for i ? 0, wi is a special start symbol, <s>.
2.1 Model
We use a feedforward neural network as shown in
Figure 1, following Bengio et al (2003). The input
to the network is a sequence of one-hot represen-
tations of the words in context u, which we write
u j (1 ? j ? n ? 1). The output is the probability
P(w | u) for each word w, which the network com-
putes as follows.
The hidden layers consist of rec-
tified linear units (Nair and Hinton,
2010), which use the activation func-
tion ?(x) = max(0, x) (see graph at
right).
The output of the first hidden layer h1 is
h1 = ?
?
????????
n?1?
j=1
C jDu j
?
????????
(1)
where D is a matrix of input word embeddings
which is shared across all positions, the C j are the
context matrices for each word in u, and ? is applied
elementwise. The output of the second layer h2 is
h2 = ? (Mh1) ,
where M is the matrix of connection weights be-
tween h1 and h2. Finally, the output layer is a soft-
max layer,
P(w | u) ? exp
(
D?h2 + b
)
(2)
where D? is the output word embedding matrix and b
is a vector of biases for every word in the vocabulary.
2.2 Training
The typical way to train neural LMs is to maximize
the likelihood of the training data by gradient ascent.
But the softmax layer requires, at each iteration, a
summation over all the units in the output layer, that
is, all words in the whole vocabulary. If the vocabu-
lary is large, this can be prohibitively expensive.
Noise-contrastive estimation or NCE (Gutmann
and Hyva?rinen, 2010) is an alternative estimation
principle that allows one to avoid these repeated
summations. It has been applied previously to log-
bilinear LMs (Mnih and Teh, 2012), and we apply it
here to the NPLM described above.
We can write the probability of a word w given a
context u under the NPLM as
P(w | u) =
1
Z(u)
p(w | u)
p(w | u) = exp
(
D?h2 + b
)
Z(u) =
?
w?
p(w? | u) (3)
where p(w | u) is the unnormalized output of the unit
corresponding to w, and Z(u) is the normalization
factor. Let ? stand for the parameters of the model.
One possibility would be to treat Z(u), instead of
being defined by (3), as an additional set of model
parameters which are learned along with ?. But it is
easy to see that we can make the likelihood arbitrar-
ily large by making the Z(u) arbitrarily small.
In NCE, we create a noise distribution q(w).
For each example uiwi, we add k noise samples
w?i1, . . . , w?ik into the data, and extend the model to
account for noise samples by introducing a random
1388
variable C which is 1 for training examples and 0 for
noise samples:
P(C = 1,w | u) =
1
1 + k
?
1
Z(u)
p(w | u)
P(C = 0,w | u) =
k
1 + k
? q(w).
We then train the model to classify examples as
training data or noise, that is, to maximize the con-
ditional likelihood,
L =
N?
i=1
(
log P(C = 1 | uiwi) +
k?
j=1
log P(C = 0 | uiw?i j)
)
with respect to both ? and Z(u).
We do this by stochastic gradient ascent. The gra-
dient with respect to ? turns out to be
?L
??
=
N?
i=1
(
P(C = 0 | uiwi)
?
??
log p(wi | ui) ?
k?
j=1
P(C = 1 | uiw?i j)
?
??
log p(w?i j | ui)
)
and similarly for the gradient with respect to Z(u).
These can be computed by backpropagation. Unlike
before, the Z(u) will converge to a value that normal-
izes the model, satisfying (3), and, under appropriate
conditions, the parameters will converge to a value
that maximizes the likelihood of the data.
3 Implementation
Both training and scoring of neural LMs are compu-
tationally expensive at the scale needed for machine
translation. In this section, we describe some of the
techniques used to make them practical for transla-
tion.
3.1 Training
During training, we compute gradients on an en-
tire minibatch at a time, allowing the use of matrix-
matrix multiplications instead of matrix-vector mul-
tiplications (Bengio, 2012). We represent the inputs
as a sparse matrix, allowing the computation of the
input layer (1) to use sparse matrix-matrix multi-
plications. The output activations (2) are computed
only for the word types that occur as the positive ex-
ample or one of the noise samples, yielding a sparse
matrix of outputs. Similarly, during backpropaga-
tion, sparse matrix multiplications are used at both
the output and input layer.
In most of these operations, the examples in a
minibatch can be processed in parallel. However, in
the sparse-dense products used when updating the
parameters D and D?, we found it was best to di-
vide the vocabulary into blocks (16 per thread) and
to process the blocks in parallel.
3.2 Translation
To incorporate this neural LM into a MT system, we
can use the LM to rerank k-best lists, as has been
done in previous work. But since the NPLM scores
n-grams, it can also be integrated into a phrase-based
or hierarchical phrase-based decoder just as a con-
ventional n-gram model can, unlike a RNN.
The most time-consuming step in computing n-
gram probabilities is the computation of the nor-
malization constants Z(u). Following Mnih and Teh
(2012), we set al the normalization constants to one
during training, so that the model learns to produce
approximately normalized probabilities. Then, when
applying the LM, we can simply ignore normaliza-
tion. A similar strategy was taken by Niehues and
Waibel (2012). We find that a single n-gram lookup
takes about 40 ?s.
The technique, described above, of grouping ex-
amples into minibatches works for scoring of k-best
lists, but not while decoding. But caching n-gram
probabilities helps to reduce the cost of the many
lookups required during decoding.
A final issue when decoding with a neural LM
is that, in order to estimate future costs, we need
to be able to estimate probabilities of n?-grams for
n? < n. In conventional LMs, this information is
readily available,1 but not in NPLMs. Therefore, we
defined a special word <null> whose embedding is
the weighted average of the (input) embeddings of
all the other words in the vocabulary. Then, to esti-
mate the probability of an n?-gram u?w, we used the
probability of P(w | <null>n?n
?
u?).
1However, in Kneser-Ney smoothed LMs, this information
is also incorrect (Heafield et al, 2012).
1389
setting dev 2004 2005 2006
baseline 38.2 38.4 37.7 34.3
reranking 38.5 38.6 37.8 34.7
decoding 39.1 39.5 38.8 34.9
Table 1: Results for Chinese-English experiments, with-
out neural LM (baseline) and with neural LM for rerank-
ing and integrated decoding. Reranking with the neural
LM improves translation quality, while integrating it into
the decoder improves even more.
4 Experiments
We ran experiments on four language pairs ? Chi-
nese to English and French, German, and Spanish
to English ? using a hierarchical phrase-based MT
system (Chiang, 2007) and GIZA++ (Och and Ney,
2003) for word alignments.
For all experiments, we used four LMs. The base-
lines used conventional 5-gram LMs, estimated with
modified Kneser-Ney smoothing (Chen and Good-
man, 1998) on the English side of the bitext and the
329M-word Xinhua portion of English Gigaword
(LDC2011T07). Against these baselines, we tested
systems that included the two conventional LMs as
well as two 5-gram NPLMs trained on the same
datasets. The Europarl bitext NPLMs had a vocab-
ulary size of 50k, while the other NPLMs had a vo-
cabulary size of 100k. We used 150 dimensions for
word embeddings, 750 units in hidden layer h1, and
150 units in hidden layer h2. We initialized the net-
work parameters uniformly from (?0.01, 0.01) and
the output biases to ? log |V |, and optimized them by
10 epochs of stochastic gradient ascent, using mini-
batches of size 1000 and a learning rate of 1. We
drew 100 noise samples per training example from
the unigram distribution, using the alias method for
efficiency (Kronmal and Peterson, 1979).
We trained the discriminative models with MERT
(Och, 2003) and the discriminative rerankers on
1000-best lists with MERT. Except where noted, we
ran MERT three times and report the average score.
We evaluated using case-insensitive NIST Bleu.
4.1 NIST Chinese-English
For the Chinese-English task (Table 1), the training
data came from the NIST 2012 constrained track,
excluding sentences longer than 60 words. Rules
Fr-En De-En Es-En
setting dev test dev test dev test
baseline 33.5 25.5 28.8 21.5 33.5 32.0
reranking 33.9 26.0 29.1 21.5 34.1 32.2
decoding 34.12 26.12 29.3 21.9 34.22 32.12
Table 2: Results for Europarl MT experiments, without
neural LM (baseline) and with neural LM for reranking
and integrated decoding. The neural LM gives improve-
ments across three different language pairs. Superscript 2
indicates a score averaged between two runs; all other
scores were averaged over three runs.
without nonterminals were extracted from all train-
ing data, while rules with nonterminals were ex-
tracted from the FBIS corpus (LDC2003E14). We
ran MERT on the development data, which was the
NIST 2003 test data, and tested on the NIST 2004?
2006 test data.
Reranking using the neural LM yielded improve-
ments of 0.2?0.4 Bleu, while integrating the neural
LM yielded larger improvements, between 0.6 and
1.1 Bleu.
4.2 Europarl
For French, German, and Spanish translation, we
used a parallel text of about 50M words from Eu-
roparl v7. Rules without nonterminals were ex-
tracted from all the data, while rules with nonter-
minals were extracted from the first 200k words. We
ran MERT on the development data, which was the
WMT 2005 test data, and tested on the WMT 2006
news commentary test data (nc-test2006).
The improvements, shown in Table 2, were more
modest than on Chinese-English. Reranking with
the neural LM yielded improvements of up to 0.5
Bleu, and integrating the neural LM into the decoder
yielded improvements of up to 0.6 Bleu. In one
case (Spanish-English), integrated decoding scored
higher than reranking on the development data but
lower on the test data ? perhaps due to the differ-
ence in domain between the two. On the other tasks,
integrated decoding outperformed reranking.
4.3 Speed comparison
We measured the speed of training a NPLM by NCE,
compared with MLE as implemented by the CSLM
toolkit (Schwenk, 2013). We used the first 200k
1390
10 20 30 40 50 60 70
0
1,
00
0
2,
00
0
3,
00
0
4,
00
0
Vocabulary size (?1000)
T
ra
in
in
g
tim
e
(s
)
CSLM
NCE k = 1000
NCE k = 100
NCE k = 10
Figure 2: Noise contrastive estimation (NCE) is much
faster, and much less dependent on vocabulary size, than
MLE as implemented by the CSLM toolkit (Schwenk,
2013).
lines (5.2M words) of the Xinhua portion of Giga-
word and timed one epoch of training, for various
values of k and |V |, on a dual hex-core 2.67 GHz
Xeon X5650 machine. For these experiments, we
used minibatches of 128 examples. The timings are
plotted in Figure 2. We see that NCE is considerably
faster than MLE; moreover, as expected, the MLE
training time is roughly linear in |V |, whereas the
NCE training time is basically constant.
5 Related Work
The problem of training with large vocabularies in
NPLMs has received much attention. One strategy
has been to restructure the network to be more hi-
erarchical (Morin and Bengio, 2005; Mnih and Hin-
ton, 2009) or to group words into classes (Le et al,
2011). Other strategies include restricting the vocab-
ulary of the NPLM to a shortlist and reverting to a
traditional n-gram LM for other words (Schwenk,
2004), and limiting the number of training examples
using resampling (Schwenk and Gauvain, 2005) or
selecting a subset of the training data (Schwenk et
al., 2012). Our approach can be efficiently applied
to large-scale tasks without limiting either the model
or the data.
NPLMs have previously been applied to MT, most
notably feed-forward NPLMs (Schwenk, 2007;
Schwenk, 2010) and RNN-LMs (Mikolov, 2012).
However, their use in MT has largely been limited
to reranking k-best lists for MT tasks with restricted
vocabularies. Niehues and Waibel (2012) integrate a
RBM-based language model directly into a decoder,
but they only train the RBM LM on a small amount
of data. To our knowledge, our approach is the first
to integrate a large-vocabulary NPLM directly into a
decoder for a large-scale MT task.
6 Conclusion
We introduced a new variant of NPLMs that com-
bines the network architecture of Bengio et al
(2003), rectified linear units (Nair and Hinton,
2010), and noise-contrastive estimation (Gutmann
and Hyva?rinen, 2010). This model is dramatically
faster to train than previous neural LMs, and can be
trained on a large corpus with a large vocabulary and
directly integrated into the decoder of a MT system.
Our experiments across four language pairs demon-
strated improvements of up to 1.1 Bleu. Code for
training and using our NPLMs is available for down-
load.2
Acknowledgements
We would like to thank the anonymous reviewers for
their very helpful comments. This research was sup-
ported in part by DOI IBC grant D12AP00225. This
work was done while the second author was visit-
ing USC/ISI supported by China Scholarship Coun-
cil. He was also supported by the Research Fund for
the Doctoral Program of Higher Education of China
(No. 20110091110003) and the National Fundamen-
tal Research Program of China (2010CB327903).
References
Yoshua Bengio, Re?jean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Research.
Yoshua Bengio. 2012. Practical recommendations for
gradient-based training of deep architectures. CoRR,
abs/1206.5533.
Stanley F. Chen and Joshua Goodman. 1998. An empir-
ical study of smoothing techniques for language mod-
2http://nlg.isi.edu/software/nplm
1391
eling. Technical Report TR-10-98, Harvard University
Center for Research in Computing Technology.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Michael Gutmann and Aapo Hyva?rinen. 2010. Noise-
contrastive estimation: A new estimation principle for
unnormalized statistical models. In Proceedings of
AISTATS.
Kenneth Heafield, Philipp Koehn, and Alon Lavie. 2012.
Language model rest costs and space-efficient storage.
In Proceedings of EMNLP-CoNLL, pages 1169?1178.
Richard Kronmal and Arthur Peterson. 1979. On the
alias method for generating random variables from
a discrete distribution. The American Statistician,
33(4):214?218.
Hai-Son Le, Ilya Oparin, Alexandre Allauzen, Jean-Luc
Gauvain, and Franc?ois Yvon. 2011. Structured output
layer neural network language model. In Proceedings
of ICASSP, pages 5524?5527.
Toma?s? Mikolov, Anoop Deoras, Stefan Kombrink, Luka?s?
Burget, and Jan ?Honza? C?ernocky?. 2011. Em-
pirical evaluation and combination of advanced lan-
guage modeling techniques. In Proceedings of IN-
TERSPEECH, pages 605?608.
Toma?s? Mikolov. 2012. Statistical Language Models
Based on Neural Networks. Ph.D. thesis, Brno Uni-
versity of Technology.
Andriy Mnih and Geoffrey Hinton. 2009. A scalable
hierarchical distributed language model. In Advances
in Neural Information Processing Systems.
Andriy Mnih and Yee Whye Teh. 2012. A fast and sim-
ple algorithm for training neural probabilistic language
models. In Proceedings of ICML.
Frederic Morin and Yoshua Bengio. 2005. Hierarchical
probabilistic neural network language model. In Pro-
ceedings of AISTATS, pages 246?252.
Vinod Nair and Geoffrey E. Hinton. 2010. Rectified lin-
ear units improve restricted Boltzmann machines. In
Proceedings of ICML, pages 807?814.
Jan Niehues and Alex Waibel. 2012. Continuous
space language models using Restricted Boltzmann
Machines. In Proceedings of IWSLT.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of ACL,
pages 160?167.
Holger Schwenk and Jean-Luc Gauvain. 2005. Training
neural network language models on very large corpora.
In Proceedings of EMNLP.
Holger Schwenk, Anthony Rousseau, and Mohammed
Attik. 2012. Large, pruned or continuous space lan-
guage models on a GPU for statistical machine trans-
lation. In Proceedings of the NAACL-HLT 2012 Work-
shop: Will We Ever Really Replace the N-gramModel?
On the Future of Language Modeling for HLT, pages
11?19.
Holger Schwenk. 2004. Efficient training of large neural
networks for language modeling. In Proceedings of
IJCNN, pages 3059?3062.
Holger Schwenk. 2007. Continuous space language
models. Computer Speech and Language, 21:492?
518.
Holger Schwenk. 2010. Continuous-space language
models for statistical machine translation. Prague Bul-
letin of Mathematical Linguistics, 93:137?146.
Holger Schwenk. 2013. CSLM - a modular open-source
continuous space language modeling toolkit. In Pro-
ceedings of Interspeech.
M.D. Zeiler, M. Ranzato, R. Monga, M. Mao, K. Yang,
Q.V. Le, P. Nguyen, A. Senior, V. Vanhoucke, J. Dean,
and G.E. Hinton. 2013. On rectified linear units for
speech processing. In Proceedings of ICASSP.
1392
Proceedings of the ACL-HLT 2011 Student Session, pages 1?5,
Portland, OR, USA 19-24 June 2011. c?2011 Association for Computational Linguistics
Word Alignment Combination over Multiple Word Segmentation 
 
 
Ning Xi, Guangchao Tang, Boyuan Li, Yinggong Zhao 
State Key Laboratory for Novel Software Technology, 
Department of Computer Science and Technology, 
Nanjing University, Nanjing, 210093, China 
 {xin,tanggc,liby,zhaoyg}@nlp.nju.edu.cn 
 
 
 
 
 
 
Abstract 
In this paper, we present a new word alignment 
combination approach on language pairs where 
one language has no explicit word boundaries. 
Instead of combining word alignments of dif-
ferent models (Xiang et al, 2010), we try to 
combine word alignments over multiple mono-
lingually motivated word segmentation. Our 
approach is based on link confidence score de-
fined over multiple segmentations, thus the 
combined alignment is more robust to inappro-
priate word segmentation. Our combination al-
gorithm is simple, efficient, and easy to 
implement. In the Chinese-English experiment, 
our approach effectively improved word align-
ment quality as well as translation performance 
on all segmentations simultaneously, which 
showed that word alignment can benefit from 
complementary knowledge due to the diversity 
of multiple and monolingually motivated seg-
mentations. 
1 Introduction 
Word segmentation is the first step prior to word 
alignment for building statistical machine transla-
tions (SMT) on language pairs without explicit 
word boundaries such as Chinese-English.  Many 
works have focused on the improvement of word 
alignment models. (Brown et al, 1993; Haghighi et 
al., 2009; Liu et al, 2010). Most of the word 
alignment models take single word segmentation 
as input. However, for languages such as Chinese, 
it is necessary to segment sentences into appropri-
ate words for word alignment. 
A large amount of works have stressed the im-
pact of word segmentation on word alignment. Xu 
et al (2004), Ma et al (2007), Chang et al (2008), 
and Chung et al (2009) try to learn word segmen-
tation from bilingually motivated point of view; 
they use an initial alignment to learn word segmen-
tation appropriate for SMT. However, their per-
formance is limited by the quality of the initial 
alignments, and the processes are time-consuming. 
Some other methods try to combine multiple word 
segmentation at SMT decoding step (Xu et al, 
2005; Dyer et al, 2008; Zhang et al, 2008; Dyer et 
al., 2009; Xiao et al, 2010). Different segmenta-
tions are yet independently used for word align-
ment. 
Instead of time-consuming segmentation optimi-
zation based on alignment or postponing segmenta-
tion combination late till SMT decoding phase, we 
try to combine word alignments over multiple 
monolingually motivated word segmentation on 
Chinese-English pair, in order to improve word 
alignment quality and translation performance for 
all segmentations. We introduce a tabular structure 
called word segmentation network (WSN for short) 
to encode multiple segmentations of a Chinese sen-
tence, and define skeleton links (SL for short) be-
tween spans of WSN and words of English 
sentence. The confidence score of a SL is defined 
over multiple segmentations. Our combination al-
gorithm picks up potential SLs based on their con-
fidence scores similar to Xiang et al (2010), and 
then projects each selected SL to link in all seg-
mentation respectively. Our algorithm is simple, 
efficient, easy to implement, and can effectively 
improve word alignment quality on all segmenta-
tions simultaneously, and alignment errors caused 
1
by inappropriate segmentations from single seg-
menter can be substantially reduced. 
Two questions will be answered in the paper: 1) 
how to define the link confidence over multiple 
segmentations in combination algorithm? 2) Ac-
cording to Xiang et al (2010), the success of their 
word alignment combination of different models 
lies in the complementary information that the 
candidate alignments contain. In our work, are 
multiple monolingually motivated segmentations 
complementary enough to improve the alignments? 
The rest of this paper is structured as follows: 
WSN will be introduced in section 2. Combination 
algorithm will be presented in section 3. Experi-
ments of word alignment and SMT will be reported 
in section 4. 
2  Word Segmentation Network 
We propose a new structure called word segmenta-
tion network (WSN) to encode multiple segmenta-
tions. Due to space limitation, all definitions are 
presented by illustration of a running example of a 
sentence pair: 
 
???? (xia-yu-lu-hua)  
Road is slippery when raining 
 
We first introduce skeleton segmentation. Given 
two segmentation S1 and S2 in Table 1, the word 
boundaries of their skeleton segmentation is the 
union of word boundaries (marked by ?/?) in S1 
and S2. 
 
 Segmentation 
S1 ? / ? / ?? 
S2 ?? / ? / ? 
skeleton ? / ? / ? / ? 
 
Table 1: The skeleton segmentation of two seg-
mentations S1 and S2. 
 
The WSN of S1 and S2 is shown in Table 2.  As 
is depicted, line 1 and 2 represent words in S1 and 
S2 respectively, line 3 represents skeleton words. 
Each column, or span, comprises a skeleton word 
and words of S1 and S2 with the skeleton word as 
their morphemes at that position. The number of 
columns of a WSN is equal to the number of skele-
ton words. It should be noted that there may be 
words covering two or more spans, such as ???? 
in S1, because the word ???? in S1 is split into 
two words ??? and ??? in S2.  
S1 ? 1 ? 2 ?? 3 
S2 ?? 1 ? 2 ? 3 
skeleton ? 1 ? 2 ? 3 ? 4 
 
Table 2:  The WSN of Table 1. Subscripts 
indicate indexes of words. 
 
The skeleton word can be projected onto words 
in the same span in S1 and S2. For clarity, words in 
each segmentation are indexed (1-based), for ex-
ample, ???? in S1 is indexed by 3. We use a pro-
jection function       to denote the index of the 
word onto which the j-th skeleton word is project-
ed in the k-th segmentation, for example,       
  and        . 
In the next, we define the links between spans of 
the WSN and English words as skeleton links (SL), 
the subset of all SLs comprise the skeleton align-
ment (SA). Figure 1 shows an SA of the example. 
 
Figure 1: An example alignment between WSN in 
Table 2 and English sentence ?Road is slippery 
when raining?. (a) skeleton link; (b) skeleton 
alignment. 
 
Each span of the WSN comprises words from 
different segmentations (Figure 1a), which indi-
cates that the confidence score of a SL can be de-
fined over words in the same span. By projection 
function, a SL can be projected onto the link for 
each segmentation. Therefore, the problem of 
combining word alignment over different segmen-
tations can be transformed into the problem of se-
lecting SLs for SA first, and then project the 
selected SLs onto links for each segmentation re-
spectively. 
3  Combination Algorithm 
Given k alignments    over segmentations    
respectively         ), and       is the pair 
Road  
  
? 1 ? 2 ?? 3 
?? 1 ? 2 ? 3 
? 1 ? 2 ? 3 ? 4 
 
(a) 
  
(b) 
 
?? 3 
? 2 
? 3 
 
Road is slippery when raining  
2
of the Chinese WSN and its parallel English sen-
tence. Suppose     is the SL between the j-th span 
   and i-th English word   ,    
   is the link between 
the j-th Chinese word   
  in    and   . Inspired by 
Huang (2009), we define the confidence score of 
each SL as follows 
 (   |   )  ?             
           (1) 
 
where          
       is the confidence score of the 
link        
 , defined as 
 (       
 |   )
 ?    (       
 |   )              
       
(2) 
where c-to-e link posterior probability is defined as 
    (       
 |   )  
            
  
?               
   
    
  
 (3) 
and I is the length of  . E-to-c link posterior prob-
ability     (       
 |   )  can be defined similarly,  
Our alignment combination algorithm is as fol-
lows.  
1. Build WSN for Chinese sentence. 
2. Compute the confidence score for each SL 
based on Eq. (1). A SL     gets a vote from    
if        
  appears in             . Denote 
the set of all SLs getting at least one vote by 
  . 
3. All SLs in    are sorted in descending order 
and evaluated sequentially. A SL     is includ-
ed if its confidence score is higher than a tuna-
ble threshold  , and one of the following is 
true1: 
? Neither    nor    is aligned so far; 
?    is not aligned and its left or right neigh-
boring word is aligned to    so far; 
?    is not aligned and its left or right 
neighboring word is aligned to    so far. 
4. Repeat 3 until no more SLs can be included. 
All included SLs comprise   . 
5. Map SLs in    on each    to get k new align-
ments   
  respectively, i.e.   
          
      
   2         . For each  , we sort all 
                                                          
1 SLs getting   votes are forced to be included without further 
examination. 
2 Two or more SLs in    may be projected onto one links in 
  
 , in this case, we keep only one in   
 . 
links in   
  in ascending order and evaluated 
them sequentially  Compare   
  and   , A link 
    
  is removed from   
  if it is not appeared in 
  , and one of the following is true: 
? both   
 and    are aligned in   
 ; 
? There is a word which is neither left nor 
right neighboring word of    but aligned 
to   
  in   
 ; 
? There is a word which is neither left nor 
right neighboring word of   
  but aligned 
to    in   
 . 
The heuristic in step 3 is similar to Xiang et al 
(2010), which avoids adding error-prone links. We 
apply the similar heuristic again in step 5 in each 
  
            to delete error-prone links. The 
weights in Eq. (1) and   can be tuned in a hand-
aligned dataset to maximize word alignment F-
score on any   
  with hill climbing algorithm. 
Probabilities in Eq. (2) and Eq. (3) can be estimat-
ed using GIZA. 
4 Experiment 
4.1   Data 
Our training set contains about 190K Chinese-
English sentence pairs from LDC2003E14 corpus. 
The NIST?06 test set is used as our development 
set and the NIST?08 test set is used as our test set. 
The Chinese portions of all the data are prepro-
cessed by three monolingually motived segmenters 
respectively. These segmenters differ in either 
training method or specification, including 
ICTCLAS (I)3, Stanford segmenters with CTB (C) 
and PKU (P) specifications4 respectively. We used 
a phrase-based MT system similar to (Koehn et al, 
2003), and generated two baseline alignments us-
ing GIZA++ enhanced by gdf heuristics (Koehn et 
al., 2003) and a linear discriminative word align-
ment model (DIWA) (Liu et al, 2010) on training 
set with the three segmentations respectively. A 5-
gram language model trained from the Xinhua por-
tion of Gigaword corpus was used.  The decoding 
weights were optimized with Minimum Error Rate 
Training (MERT) (Och, 2003). We used the hand-
aligned set of 491 sentence pairs in Haghighi et al 
(2009), the first 250 sentence pairs were used to 
tune the weights in Eq. (1), and the other 241 were 
                                                          
3 http://www.ictclas.org/ 
4 http://nlp.stanford.edu/software/segmenter.shtml 
3
[???] [?] [380] [?] [??] [???] 
relief funds worth 3.8 million us dollars from the national foodstuff department 
[??] [??] [???] [??] [??] 
chief executive in the hksar  
[???] [?] [380] [?] [??] [???] [??] [??] [???] [??] [??] 
Figure 2: Two examples (left and right respectively) of word alignment on segmentation C. Baselines 
(DIWA) are in the top half, combined alignments are in the bottom half. The solid line represents the cor-
rect link while the dashed line represents the bad link. Each word is enclosed in square brackets. 
used to measure the word alignment quality. Note 
that we adapted the Chinese portion of this hand-
aligned set to segmentation C. 
4.2 Improvement of Word Alignment 
We first evaluate our combination approach on the 
hand-aligned set (on segmentation C). Table 3 
shows the precision, recall and F-score of baseline 
alignments and combined alignments. 
As shown in Table 3, the combination align-
ments outperformed the baselines (setting C) in all 
settings in both GIZA and DIWA. We notice that 
the higher F-score is mainly due to the higher pre-
cision in GIZA but higher recall in DIWA. In 
GIZA, the result of C+I and C+P achieve 8.4% and 
9.5% higher F-score respectively, and both of them 
outperformed C+P+I, we speculate it is because 
GIZA favors recall rather than DIWA, i.e. GIZA 
may contain more bad links than DIWA, which 
would lead to more unstable F-score if more 
alignments produced by GIZA are combined, just 
as the poor precision (69.68%) indicated. However, 
DIWA favors precision than recall (this observa-
tion is consistent with Liu et al (2010)), which 
may explain that the more diversified segmenta-
tions lead to better results in DIWA. 
 
 GIZA DIWA 
setting P R F P R F 
C 61.84 84.99 71.59 83.12 78.88 80.94 
C+P 80.16 79.80 79.98 84.15 79.41 81.57 
C+I 82.96 79.28 81.08 84.41 81.69 83.03 
C+I+P 69.68 85.17 77.81 83.38 82.98 83.18 
 
Table 3: Alignment precision, recall and F-score.  
C: baseline, C+I: Combination of C and I. 
 
Figure 2 gives baseline alignments and com-
bined alignments on two sentence pairs in the 
training data. As can be seen, alignment errors 
caused by inappropriate segmentations by single 
segmenter were substantially reduced.  For exam-
ple, in the second example, the word ??????
?? hksar? appears in segmentation I of the Chi-
nese sentence, which benefits the generation of the 
three correct links connecting for words ??
?? ,????, ????? respectively in the com-
bined alignment. 
4.3   Improvement in MT performance 
We then evaluate our combination approach on the 
SMT training data on all segmentations. For effi-
ciency, we just used the first 50k sentence pairs of 
the aligned training corpus with the three segmen-
tations to build three SMT systems respectively. 
Table 4 shows the BLEU scores of baselines and 
combined alignment (C+P+I, and then projected 
onto C, P, I respectively). Our approach achieves 
improvement over baseline alignments on all seg-
mentations consistently, without using any lattice 
decoding techniques as Dyer et al (2009).  The 
gain of translation performance purely comes from 
improvements of word alignment on all segmenta-
tions by our proposed word alignment combination. 
 
 GIZA DIWA 
Segmentation B Comb B Comb 
C 19.77 20.9 20.18 20.71 
P 20.5 21.16 20.41 21.14 
I 20.11 21.14 20.46 21.30 
 
Table 4: Improvement in BLEU scores. B:Baseline 
alignment, Comb: Combined alignment. 
4
5 Conclusion 
We evaluated our word alignment combination 
over three monolingually motivated segmentations 
on Chinese-English pair. We showed that the com-
bined alignment significantly outperforms the 
baseline alignment with both higher F-score and 
higher BLEU score on all segmentations. Our work 
also proved the effectiveness of link confidence 
score in combining different word alignment mod-
els (Xiang et al, 2010), and extend it to combine 
word alignments over different segmentations. 
Xu et al (2005) and Dyer et al (2009) combine 
different segmentations for SMT. They aim to 
achieve better translation but not higher alignment 
quality of all segmentations. They combine multi-
ple segmentations at SMT decoding step, while we 
combine segmentation alternatives at word align-
ment step. We believe that we can further improve 
the performance by combining these two kinds of 
works. We also believe that combining word 
alignments over both monolingually motivated and 
bilingually motivated segmentations (Ma et al, 
2009) can achieve higher performance. 
In the future, we will investigate combining 
word alignments on language pairs where both 
languages have no explicit word boundaries such 
as Chinese-Japanese. 
Acknowledgments 
This work was supported by the National Natural 
Science Foundation of China under Grant No. 
61003112, and the National Fundamental Research 
Program of China (2010CB327903). We would 
like to thank Xiuyi Jia and Shujie Liu for useful 
discussions and the anonymous reviewers for their 
constructive comments. 
 
References  
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Del-
la Peitra, Robert L. Mercer. 1993. The Mathematics 
of statistical machine translation: parameter estima-
tion. Computational Linguistics, 19(2):263-311. 
Pi-Chuan Chang, Michel Galley, and Christopher D. 
Manning. 2008. Optimizing Chinese word segmenta-
tion for machine translation performance.  In Pro-
ceedings of third workshop on SMT, Pages:224-232. 
Tagyoung Chung and Daniel Gildea. 2009. Unsuper-
vised tokenization for machine translation. In Pro-
ceedings of EMNLP, Pages:718-726. 
Christopher Dyer, Smaranda Muresan, and Philip Res-
nik. 2008. Generalizing word lattice translation. In 
Proceedings of ACL, Pages:1012-1020. 
Christopher Dyer. 2009. Using a maximum entropy 
model to build segmentation lattices for mt. In Pro-
ceedings of NAACL, Pages:406-414. 
Franz Josef Och. 2003. Minimum error rate training in 
statistical machine translation. In Proceedings of 
ACL, Pages:440-447. 
Aria Haghighi, John Blitzer, John DeNero, and Dan 
Klein. 2009. Better word alignments with supervised 
ITG models. In Proceedings of ACL, Pages: 923-931. 
Fei Huang. 2009. Confidence measure for word align-
ment. In Proceedings of ACL, Pages:932-940. 
Philipp Koehn, Franz Josef Och and Daniel Marcu. 
2003. Statistical phrase-based translation. In Pro-
ceedings of HLT-NAACL, Pages:48-54. 
Yang Liu, Qun Liu, Shouxun Lin. 2010. Discriminative 
word alignment by linear modeling. Computational 
Linguistics, 36(3):303-339. 
Yanjun Ma, Nicolas Stroppa, and Andy Way. 2007. 
Bootstrapping word alignment via word packing. In 
Proceedings of ACL, Pages:304-311. 
Yanjun Ma and Andy Way. 2009. Bilingually motivated 
domain-adapted word segmentation for statistical 
machine translation. In Proceedings of EACL, Pag-
es:549-557. 
Bing Xiang, Yonggang Deng, and Bowen Zhou. 2010. 
Diversify and combine: improving word alignment 
for machine translation on low-resource languages. 
In Proceedings of ACL, Pages:932-940. 
Xinyan Xiao, Yang Liu, Young-Sook Hwang, Qun Liu, 
Shouxun Lin. 2010.  Joint tokenization and transla-
tion. In Proceedings of COLING, Pages:1200-1208. 
Jia Xu, Richard Zens, and Hermann Ney. 2004. Do we 
need Chinese word segmentation for statistical ma-
chine translation?  In Proceedings of the ACL 
SIGHAN Workshop, Pages: 122-128. 
Jia Xu, Evgeny Matusov, Richard Zens, and Hermann 
Ney. 2005. Integrated Chinese word segmentation in 
statistical machine translation. In Proceedings of 
IWSLT. 
Ruiqiang Zhang, Keiji Yasuda, and Eiichiro Sumita. 
2008. Improved statistical machine translation by 
multiple Chinese word segmentation. In Proceedings 
of the Third Workshop on SMT, Pages:216-223. 
5
