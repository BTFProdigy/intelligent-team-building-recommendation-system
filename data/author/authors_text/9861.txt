Proceedings of the NAACL HLT Workshop on Computational Approaches to Linguistic Creativity, pages 94?101,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
?Sorry? seems to be the hardest word
Allan Ramsay Debora Field
School of Computer Science Dept of Computer Science
Univ of Manchester Univ of Sheffield
Manchester M60 1QD, UK Sheffield S1 4DP, UK
Abstract
We are interested in the ways that language
is used to achieve a variety of goals, where
the same utterance may have vastly different
consequences in different situations. This is
closely related to the topic of creativity in lan-
guage. The fact that the same utterance can
be used to achieve a variety of goals opens up
the possibility of using it to achieve new goals.
The current paper concentrates largely on an
implemented system for exploring how the ef-
fects of an utterance depend on the situation
in which it is produced, but we will end with
some speculations about how how utterances
can come to have new kinds of uses.
1 Introduction
We are interested in the ways that language is used to
achieve a variety of goals, where the same utterance
may have vastly different consequences in different
situations. We will take, as a running example, the
use of the single word ?Sorry?.
We will look at a number of situations in which
this word may be uttered, and investigate the ways
in which its consequences may be determined by
considering the goals and belief states of the partic-
ipants. The kinds of reasoning that lie behind the
various uses of this word are, we believe, typical of
the way that utterances can be used to achieve novel
aims. ?Sorry? is perhaps a fairly extreme case: very
simple indeed on the surface, very complex indeed
in terms of its uses. Any account of how this specific
word gets used will have lessons for other kinds of
novel action.
As with many common but slippery words, dic-
tionary definitions are not much help when trying to
work out what ?sorry? means: Merriam-Webster, for
instance, has ?feeling sorrow, regret, or penitence?
as the primary definition, and the free dictionary
(www.thefreedictionary.com has ?Feeling
or expressing sympathy, pity, or regret?. These def-
initions are, as is common for words whose mean-
ings are highly context dependent, essentially circu-
lar. How much do we gain from knowing that ?sorry?
is a word that is used to express sorrow, or from the
free dictionary?s definition of ?sympathy? as a ?feel-
ing or an expression of pity or sorrow for the distress
of another??
Perhaps, then, considering a set of examples of
situations where someone utters this word is a better
way of getting at what it means. The following is a
rather long list, but then there are a very wide set of
situations in which people say ?sorry?. That is, after
all, the problem:
(1) a. EXPRESSION OF DISAPPOINT-
MENT
I?m sorry I missed your talk. I forgot
to set my alarm. I?d really been
looking forward to seeing your demo.
b. APOLOGY FOR OWN ACTION
WHILE NOT TAKING FULL PER-
SONAL RESPONSIBILITY
I?m sorry I missed your talk. My flight
was delayed. [situation: S & H mutu-
ally knew that S was counting on H to
help with a demo during the talk.]
94
c. APOLOGY FOR OWN ACTION
WHILE ALSO TAKING FULL PER-
SONAL RESPONSIBILITY
I?m sorry I missed your talk. I forgot
to set my alarm. [situation: S & H mu-
tually knew that S was counting on H
to help with a demo during the talk.]
(2) a. EXPRESSION OF EMPATHY
I?m sorry that this situation is so awful
for you. I would not be coping if I were
in your shoes.
b. APOLOGY FOR A 3RD PARTY?S
ACTION WHILE NOT TAKING
FULL PERSONAL RESPONSIBIL-
ITY
I?m sorry that this situation is so
awful for you. My parents have
really excelled themselves this time
[sarcasm].
c. APOLOGY FOR A 3RD PARTY?S
ACTION WHILE ALSO TAKING
FULL PERSONAL RESPONSIBIL-
ITY
I?m sorry that this situation is so awful
for you. As head of the division I take
full responsibility, and I am submitting
my resignation.
d. APOLOGY FOR OWN ACTION
WHILE ALSO TAKING FULL PER-
SONAL RESPONSIBILITY
I?m sorry that this situation is so aw-
ful for you. I should have been more
careful.
e. EXPRESSION OF EMPATHY
I?m sorry that this situation is so awful
for you. I?m not sorry for causing the
situation, because I didn?t cause it. But
I am sorry it is so awful.
(3) a. EXPRESSION OF DISDAIN+PITY
I?m sorry they?re not good enough. It?s
your loss.
b. APOLOGY FOR OWN ACTION
WHILE ALSO TAKING FULL PER-
SONAL RESPONSIBILITY
I?m sorry they?re not good enough. I
tried very hard, but I couldn?t get them
quite right.
(4) a. EXPRESSION OF EMPATHY
I?m sorry, Dave, I?m afraid I can?t do
that. All the pod locks are jammed
shut. I have tried everything I can think
of, but I can?t get them open.
b. APOLOGY FOR OWN ACTION
WHILE ALSO TAKING FULL PER-
SONAL RESPONSIBILITY
I?m sorry, Dave, I?m afraid I can?t do
that. I have turned the tables and you
are my prisoner now.
(5) a. EXPRESSION OF REGRET
I?m sorry I told him. Things would be
much simpler for me now if I?d kept
quiet.
b. APOLOGY FOR OWN ACTION
WHILE ALSO TAKING FULL PER-
SONAL RESPONSIBILITY
I?m sorry I told him. I know I promised
you I wouldn?t but it just slipped out.
(6) a. EXPRESSION OF REGRET
I?m sorry I killed their daughter. She
was in the wrong place at the wrong
time. [Speaker feels no remorse for
killing, only regret for killing the
wrong person.]
b. APOLOGY FOR OWN ACTION
WHILE ALSO TAKING FULL PER-
SONAL RESPONSIBILITY
I?m sorry I killed their daughter. It was
a terrible thing I did.
If nothing else, these examples show how flexible
the word ?sorry? is. About all they have in common
is that the speaker is referring to some action or state
of affairs which is disadvantageous to someone (usu-
ally, but not necessarily, either the speaker or hearer:
see (6) for a counter-example). The follow-up sen-
tences then say something more about the speaker?s
attitude to this action or state of affairs (we will
use the generic term ?event? to cover both of these).
Just what the speaker?s attitude to the event is varies
wildly: the glosses in the examples use terms like
95
?empathy?, ?apology?, ?regret?, but these are almost
as slippery as ?sorry? itself.
2 Literal uses of ?sorry?
The idea that ?sorry? is ambiguous, with fifteen dif-
ferent senses, is ludicrous. Apart from anything else,
we have another dozen examples up our sleeves that
do not fit any of the patterns above, and it would be
easy to find yet further uses. It seems more plausi-
ble that it has a single meaning, which can be used
as the trigger for a variety of ideas depending on the
the nature of the event and the beliefs of the speaker
and hearer. The task of determining what a speaker
meant by using this word in a given utterance then
devolves to epistemic inference. This does not actu-
ally make it very easy; but it does at least put it in
the right place.
We will take it, then, that ?sorry? is an adjective
that takes a sentential complement, and that the in-
terpretation of a sentence involving it is something
like Fig. 11. In other words, (1a) says that right now
the relation sorry holds between me and the fact that
I missed your talk.
That seems fair enough, but it also seems rather
weak. We cannot do anything with it unless we
know what follows from saying that the relation
sorry holds between a person and a proposition. In
other words, we need to start writing axioms (mean-
ing postulates, rules, definitions, . . . ) to link this re-
lation with other concepts.
The first thing we note is that any such axioms
will be inherently intensional: sorry is a relationship
between a person and a proposition (a description
of a state of affairs). We will therefore have to use
1We use the ?restricted quantifiers? ?X :: {P}Q and ?X ::
{P}Q as shorthand for ?X(P ? Q) and ?X(P&Q)
?Lat(L,
sorry(ref (?M(speaker(M))),
?N: {past(now,N)}
?Oevent(miss,O,P,Q)
&?(O,object,ref (?R(own(ref (?S(hearer(S))),R) & sort(talk,R,T,U))))
&?(O,agent,ref (?V (speaker(V)))) & aspect(N,simplePast,O)))
&aspect(now,simple,L)
Figure 1: Logical form for (1a)
some kind of intensional logic for writing our ax-
ioms. We follow (Chierchia and Turner, 1987; Fox
and Lappin, 2005) in using a variant on ?property
theory? (Turner, 1987) for this purpose. Property
theory has the required expressive power for writ-
ing rules that discuss propositions, and it has an ax-
iomatisation which allows the implementation of a
practical theorem prover (Ramsay, 2001).
So what do we want to say about sorry? The very
first observation is that it is factive: if I am sorry
about something, then it must have happened. I can-
not (sensibly) say that I am sorry that the moon is
made of green cheese, because it isn?t. Our first ax-
iom, then, says that anything that anyone is sorry
about is indeed true (A1):
(A1)
?B?C(sorry(B,C) ? C)
The only other thing that all the examples above
have in common is that the speaker wishes that the
proposition she is sorry about were not true (A2):
(A2)
?B?C(sorry(B,C) ? C & wish(B,?(C)))
There are, indeed, cases where absolutely nothing
more follows from the use of ?Sorry?:
(7) My dear Pandora, you?re going to be sorry
you opened that.
In (7), the speaker is simply telling their hearer
that she is going to wish she hadn?t opened it, what-
ever it is. No hint of apology or remorse or empathy.
Just a plain a statement of fact: at some time in the
future the hearer is going to wish that she?d left the
box closed.
It is hard to find a distinction between the set of
propositions that follow from every use of a term and
its meaning. We will therefore take it that (A1) and
(A2) characterise the meaning of ?sorry?: that the
proposition in question is true, and that the person
who is sorry about it wishes that it wasn?t.
96
How, then, do all the other examples get their
force? The key is that once you have said that you
wish something were not true, two questions arise:
why do you wish it were not so, and why are you
telling me that you wish it were not so. To answer
these two questions you have to think harder about
what the proposition in question is like.
There are two particularly interesting issues.
Who, if anyone, was responsible for the proposition
being true; and who, if anyone, is affected by it. In
particular, if the speaker is the person who was re-
sponsible for it then wishing that it were not now
true entails wishing that they had not earlier per-
formed the action that led to it; and if the person
who is affected by it is the hearer, and the effect
is adverse, then the fact that the speaker wishes it
were not true establishes some degree of empathy
between the two.
Before we can start formalising these notions we
need to introduce rules that specify responsibility
and affectedness.
The simplest rules for these notions are centred
around the roles that individuals play in events.
What, for instance, is the difference between (8a)
and (8b)?
(8) a. I saw him fall off a cliff.
b. I watched him fall off a cliff.
They both refer to the same set of events: he fell
off a cliff, and I had my eyes open and looking in
that direction at the time (and I was awake, and var-
ious other routine side-conditions). The difference
is that (8b) implies a degree of control: that I was
aware that he was falling, and I deliberately kept my
attention on what I was seeing.
One way of capturing this distinction concisely is
by using names for thematic roles which reflect the
way that the individuals concerned are involved: if,
for instance, we say that the speaker was the patient
of the seeing event in (8a), but was the agent in (8b),
then we can use rules like (A3) and (A4) to distin-
guish between cases where someone was just acci-
dentally involved in an event from ones where they
caused it or where they intentionally caused it.
(A3)
?B?C: {?(C,actor,B) ? ?(C,agent,B)}
cause(B,C)
(A4)
?B?C: {?(C,agent,B)}intended(B,C)
We can use (A3) and (A4) to pick out cases where
the person who is sorry for some state of affairs is in
fact the person who caused it to come about. We will
not yet say much about what follows from recognis-
ing these cases. For the moment we will just label
them as cases where the person regrets the event in
question.
(A5)
?B?C : {wish(B,?(C))}
?D : {C ? cause(B,D)}
regret(B,D))
Note that what the person is sorry about is a
proposition, but what they regret is an event (in a
classical Davidsonian treatment of events (David-
son, 1980)). The key question here is whether the
description of the state of affairs entails the existence
of an event for which they are responsible. The rules
in (A3) and (A4) provide the relevant support in very
many cases: just using a verb whose set of thematic
roles includes one with connotations of causality is a
shorthand for making a statement about responsibil-
ity. There are, of course, other more complex cases,
but in many such cases the key lies in spotting se-
quences of causally related events where the start of
the sequence involves the person in a causal role.
Given these rules, we can distinguish between the
cases in (9):
(9) a. I?m sorry I saw him fall off a cliff.
b. I?m sorry I watched him fall off a cliff.
If we assume that the hearer believes what the
speaker tells them, then following (9)b we can ask
who believes that someone regrets something:
| ?- prove(bel(X, regret(A, B))).
A = ?#speaker?,
B = ?#166?,
X = ?#hearer? ?
yes
The hearer believes that the speaker regrets some-
thing, namely the action of watching someone fall
of a cliff (represent here by a Skolem constant #166,
introduced by the existential quantifier for the event
in the logical form for (9b), shown in Fig. 2.
97
sorry(#user,
?O: {past(now,O)}
?Pevent(watch,P,Q,R)
&?(P,
-event,
?S: {sort(cliff ,S,T,U)}
?Vevent(fall,V,W,X) & ?(V,agent,#171) & off (V,S) & aspect(now,simple,V))
&?(P,agent,#user) & aspect(O,simplePast,P))
Figure 2: Logical form for (9b)
Although the speaker regrets watching this unfor-
tunate event, he cannot be seen as apologising for it.
An apology expresses regret that the speaker caused
something unfortunate to happen to the hearer. We
need the axiom A6 below to describe this situation:
(A6)
?B?C: {regret(B,C)}
?D?E: {want(D,?(E))
& E ? event(F,C,G,H)}
apologise(B,D,C)
In other words, if B regrets performing the action
C then if C is part of some situation which D re-
gards as undesirable, the B can be seen as apologis-
ing to D.
We also need, of course, descriptions of situations
which people might find undesirable. A typical rule
might be as in (A7), which simply says that people
do not want to be hurt (any individual B wants the
proposition event(hurt,D,E,F)&?(D,object,B) to be
false for all D,E and F ):
(A7)
?B?C?Dwant(B,
?(event(hurt,D,E,F)&?(D,object,B)))
Given A6 and A7, we can see that saying ?I am
sorry I hurt you? would be an apology: the speaker
is saying that he wishes that ?I hurt you? was not
true, and since this is something which was under
the speaker?s control (so he regrets it), then since
it also something that the hearer did not want then
the speaker?s utterance of this sentence is indeed an
apology.
Clearly this approach to the problem requires a
great deal of general knowledge. There is nothing
esoteric about A7. On the contrary, it as about as
obvious a fact of life as it is possible to imagine.
Collecting a large enough body of such rules to
cope with everyday language is, indeed, a daunt-
ing task, but it is the sheer number of such rules
that make it problematic, not the nature of the rules
themselves.
Once we have this background knowledge, how-
ever, we can see that various rather subtle differ-
ences between the basic uses of ?Sorry? emerge quite
straightforwardly from rules like the ones above.
Many of these rules are inherently intensional, as
noted above, so for a program to be able to work out
whether someone is actually apologising for some
action it will have to have access to a theorem prover
for an intensional logic. Fortunately such theorem
provers exist (see e.g. (Ramsay, 2001) for an exam-
ple).
3 Indirect uses
The axioms in Section 2 let us distinguish between
some of the examples in (1)?(6). We are faced
with two remaining questions. What do we gain
by labelling some examples as instances of regret or
apology, and what do we do about the less obvious
cases?
The key to both these questions is that linguistic
acts are inherently epistemic. They are concerned
with conveying information about what the speaker
S believes, including what she believes about the
hearer H?s beliefs, with the intention of changing
H?s beliefs.
We will consider, in particular, the cases that we
have labelled as apologies. What is the point of an
apology? What does S want to achieve by making
an apology?
We have characterised apologising above as the
act of saying that S wishes some proposition P were
98
not true, in a situation where S is responsible for P
being true and is something that H would like to be
untrue. Note that all that S actually did was to say
that she wished P were not true. There is nothing
in the form of the utterance ?I am sorry that I didn?t
do the washing up? that makes it obviously different
from ?I am sorry that you didn?t do the washing up?.
The two utterances do, of course, feel very different?
one is an apology, the other is something more like
a threat or an admonition?but their structural prop-
erties are very similar. They are both, essentially,
simple declarative sentences.
To get a closer grip on why they convey such radi-
cally different underlying consequences, we will re-
visit the idea that linguistic actions are just actions,
to be dealt with by specifying their preconditions
and effects, to be linked together by some planning
algorithm so that they lead to outcomes that are de-
sirable for the speaker.
We have argued elsewhere for a very sparse treat-
ment of speech acts (Field and Ramsay, 2004; Field
and Ramsay, 2007; Ramsay and Field, 2008). The
argument starts by considering the classical use of
AI planning theory in domains such as the blocks
world, where the preconditions of an action are a
set of propositions that must hold before that action
can be performed, and the effects are a set of actions
that will definitely hold after it has been performed.
If preconditions and effects were not entirely rigid
in this way then planning algorithms, from the origi-
nal means-end analysis of (Fikes and Nilsson, 1971)
through more modern approaches that involve static
analysis of the relationships between different types
of action (Kambhampati, 1997; Nguyen and Kamb-
hampati, 2001; Blum and Furst, 1997) would just
not work.
Suppose, however, that we try to give this kind of
description of the linguistic act of stating something.
What should the preconditions and effects of the act
of stating something be?
There seem to be very few limits on the situations
in which you can state something. Consider (3) (re-
peated here).
(3) a. EXPRESSION OF DISDAIN+PITY
I?m sorry they?re not good enough. It?s
your loss.
b. APOLOGY FOR OWN ACTION
WHILE ALSO TAKING FULL PER-
SONAL RESPONSIBILITY
I?m sorry they?re not good enough. I
tried very hard, but I couldn?t get them
quite right.
It is very hard to say that the speaker is performing
two different actions when she utters the words ?I?m
sorry they?re not good enough? in these two exam-
ples. She is, clearly, intending to achieve different
outcomes in the two cases, but they are, surely, the
same action, in the same way that getting the milk
out of the fridge in order to make custard and get-
ting the milk out of the fridge in order to in order
to make space for the orange juice are the same ac-
tion. In both (3a) and (3b) S is claiming to be sorry
that they (whatever they are) are not good enough.
In (3a), of course, it is clear that she does not believe
that this is true. Nonetheless, the form of the utter-
ance makes it clear that she is making a statement.
This is typical of linguistic actions. It is possible
to state things that you do not believe, or to ask ques-
tions where you already know the answer, or to issue
commands which you do not want to have carried
out. Unless we want to have as many sub-types of
the action ?statement? as there are examples in (1)?
(6) (and then the dozen other examples that we did
not include, and then all the ones we haven?t thought
of) then we have to see whether we can make a sin-
gle, rather simple, act cover all these cases.
What are the preconditions and effects of this act?
The only completely essential precondition for mak-
ing a statement is that you have the proposition in
question in mind, and the only thing that you can
be sure that your hearer will believe is that you had
it in mind. When S states a proposition P , S may
believe it (3a); or she may disbelieve it (3b); or she
may be unsure about it (there are no examples of this
in (1)?(6), but situations where a speaker makes a
statement despite not having an opinion on whether
it is true or not can occur). The situation for H is
even less clear: H may or may not believe that S
is being honest, and he may or may not believe that
S is reliable. Hence, H may decide that although S
has claimed P she does not actually believe it; and
even if he does decide that she believes it, he may
regard her being unreliable (on, at least, the topic of
99
P ) so he may decide not to believe it anyway. And
as for what S believes that H will believe after she
has uttered P , the possibilities are almost boundless
. . . The only thing you can be reasonably sure of is
that so long as H was paying attention and the ut-
terance was not ambiguous then H will know that
a claim was made, and hence that its preconditions
must have held (because that is what preconditions
are: a set of propositions that must held in order for
the action to be performable).
The only safe characterisation of a claim seems to
be as in Fig. 3
claim(S, H, P)
pre: bel(S, P) or bel(S, ?P) or bel(S, P or ?P)
effects:
Figure 3: Preconditions and effects of ?claim?
The preconditions will hold so long as S has
thought about P (and so long as P is not something
paradoxical like the Liar Paradox). They do not hold
at all times for all speakers. Until you read the sen-
tence ?Dan Holden hit some good first serves last
night? it was not the case that you believed that this
sentence was either true or false, because you had
never thought about it before. Thus the precondi-
tions of this action are roughly equivalent to saying
that S has the proposition P in her mind.
Given the extremely wide range of conclusions
that H can come to, it seems safest not to say any-
thing about the effects of a claim. It would be fairly
pointless to say that the effects of a claim are either
H believes S believes P or H believes that S does
not believe P or H believes that S believes that P is
false, and that either H believes P or H is agnostic
about P or H believes P is false. What we can say
is that if H realises that S has claimed P then he
will be recognise that S deliberately raised the topic
of P ?s truth value. In order to come to a conclusion
about why S should do this, he will have to come
to some view on S?s opinion of P . In other words,
a claim is an invitation to verify bel(S, P) or bel(S,
?P) or bel(S, P or ?P).
This will, of course, always be verifiable unless
P is a paradox, but the process of verification will
typically have side-effects. In particular, bel(S, P)
or bel(S, ?P) or bel(S, P or ?P) can be verified
by showing that bel(S, P) holds, or by showing that
bel(S, ?P) holds. H?s first move, then, will be to
investigate bel(S, P). S will know this, so if S does
believe P then if she also thinks that H has a reason-
able model of her beliefs then she will conclude that
H will shortly have the proposition bel(S, P) avail-
able to him.
If, on the other hand, S believes that P is false
then again assuming that H has a reasonable model
of her beliefs she can assume that he will shortly
have bel(S, ?P) available to him. In other words, if S
believes that H?s picture of her beliefs is reasonably
complete and reasonably accurate then by claiming
P she can bring either P or ?P to H?s attention.
Given that linguistic acts are public, in the sense
that all the participants are aware that they have
taken place and that all the other participants are
aware of this, both S and H will be aware that
H knows that one of bel(S,P ), bel(S,?P ) and
bel(S,Por?P ) is true. However, this disjunction is
so uninformative that it amounts to an invitation to
H to try to work out which disjunct actually holds.
Furthermore, S knows that it is tantamount to such
an invitation, and H knows that S knows this. Thus
the simple act of producing a highly uninformative
utterance in a public situation will lead both S and
H to expect that they will both believe that H will
try find out which of the disjuncts actually holds.
This allows S to say ?I?m sorry they?re not good
enough? in a situation where both parties know that
S actually believes they are good enough. H will
try to check the preconditions of S?s act of claiming
to be sorry about the situation. He will not man-
age to verify that S is sorry about, but he can show
that she is not: the fact that she believes they are
good enough will clash with (A1), which says that
you can only actually be sorry about things that are
true. Thus S has brought to the fact that she does
not believe they are not good enough, whilst also
raising the possibility that she might have been, but
is not, sorry about something. She has done so in a
way that has forced H to think about it, and to arrive
at these conclusions for himself, which is likely to
be more forceful and indeed more convincing than
if she had just asserted it. In other words, by saying
that she has sorry about something she has conveyed
the complex message that the proposition in ques-
tion is not true, and that she is not apologising for
100
H?s disappointment with the situation.
4 Conclusions
In the first part of the paper we explored the way
that the consequences of direct uses of a word like
?Sorry? can vary, depending on aspects of the propo-
sition under consideration. Saying that you wish
some state of affairs for which you are responsi-
ble and which adversely affects your hearer did not
hold has different consequences from saying that
you wish that some more neutral proposition were
true. The degree of (admitted) responsibility of the
speaker for the situation affects these consequences
? ?I?m sorry I shrank your favourite jumper? carries
a different message from ?I?m sorry your favourite
jumper shrank when I did the washing yesterday? be-
cause of the indirectness of the causal link between
me and the shrinking in the second example. We
have all the machinery for accounting for examples
like these implemented, via a theorem prover which
can handle intensionality and which can effectively
ascribe beliefs to individuals. Clearly this relies on
background knowledge about everyday facts such as
the obsvervation that people generally dislike being
hurt (A7). We do not have a massive repository of
such general knowledge, and inspection of publicly
available sources such as CYC and ConceptNet sug-
gests that they generally omit such very basic facts,
presumably because they are so self-evident that the
are below the radar of the compilers. Nonethe-
less, there is nothing about such rules that makes
them particularly difficult to express, and we have no
doubt that if we had more general-knowledge of this
kind then we would be able to determine the conse-
quences of a wide range of literal uses of ?Sorry?.
The later discussion of indirect uses of ?sorry?
is more speculative: we have an implementation
of a planner which can use very underspecified ac-
tions descriptions of the kind in Fig. 3 by look-
ing for instantiations of such an action which en-
tail some proposition in a particular situation, rather
than simply looking for actions whose effects match
the user?s goals, and we have used this to explore a
number of examples of ?indirect speech acts?. There
is more work to be done here, but the kind of anal-
ysis we are looking at has the potential for handling
entirely novel uses of linguistic acts that approaches
that enumerate a fixed set of acts (e.g. (Austin, 1962;
Searle, 1969; Cohen and Perrault, 1979; Allen and
Perrault, 1980; Cohen et al, 1990) with detailed pre-
conditions and effects, would find more difficult. In
the same way that having a very simple definition of
?sorry? and allowing the different consequences to
emerge in the light of other information that is avail-
able in the situation lets us treat an open-ended set
of literal uses of this word, using a very simple no-
tion of linguistic act and allowing the different con-
sequences to emerge in different situations leads to
the possibility of accounting for entirely novel uses.
References
J F Allen and C R Perrault. 1980. Analysing intention in utter-
ances. Artificial Intelligence, 15:148?178.
J Austin. 1962. How to Do Things with Words. Oxford Univer-
sity Press, Oxford.
A Blum and M L Furst. 1997. Fast planning through planning
graph analysis. Artificial Intelligence, 90(1-2).
G Chierchia and R Turner. 1987. Semantics and property the-
ory. Linguistics and Philosophy, 11(3).
P R Cohen and C R Perrault. 1979. Elements of a plan-based
theory of speech acts. Cognitive Science, 7(2):171?190.
P R Cohen, J Morgan, and M E Pollack. 1990. Intentions in
Communication. Bradford Books, Cambridge, Mass.
D Davidson. 1980. Essays on actions and events. Clarendon
Press, Oxford.
D G Field and A M Ramsay. 2004. Sarcasm, deception, and
stating the obvious: Planning dialogue without speech acts.
Artificial Intelligence Review, 22:149?171.
D G Field and A M Ramsay. 2007. Minimal sets of minimal
speech acts. In Recent Advances in Natural Language Pro-
cessing (RANLP?07), pages 193?199, Borovets, Bulgaria.
R E Fikes and N J Nilsson. 1971. Strips: a new approach to the
application of theorem proving to problem solving. Artificial
Intelligence, 3(4):251?288.
C Fox and S Lappin. 2005. Foundations of Intensional Seman-
tics. Blackwell.
S Kambhampati. 1997. Refinement planning as a unifiying
framework for plan synthesis. AI Magazine, 18(2):67?97.
X Nguyen and S Kambhampati. 2001. Reviving partial order
planning. In IJCAI, pages 459?466.
A M Ramsay and D G Field. 2008. Speech acts, epistemic
planning and Grice?s maxims. Logic and Computation,
18:431?457.
A M Ramsay. 2001. Theorem proving for untyped constructive
?-calculus: implementation and application. Logic Journal
of the Interest Group in Pure and Applied Logics, 9(1):89?
106.
J R Searle. 1969. Speech Acts: an Essay in the Philosophy of
Language. Cambridge University Press, Cambridge.
R Turner. 1987. A theory of properties. Journal of Symbolic
Logic, 52(2):455?472.
101
Proceedings of the 8th International Conference on Computational Semantics, pages 181?194,
Tilburg, January 2009. c?2009 International Conference on Computational Semantics
Using English for commonsense knowledge
Allan Ramsay Debora Field
School of Computer Science Dept of Computer Science
Univ of Manchester Univ of Sheffield
Manchester M60 1QD, UK Sheffield S1 4DP, UK
Abstract
The work reported here arises from an attempt to provide a body
of simple information about diet and its effect on various common
medical conditions. Expressing this knowledge in natural language
has a number of advantages. It also raises a number of difficult issues.
We will consider solutions, and partial solutions, to these issues below.
1 Commonse knowledge
Suppose you wanted to have a system that could provide advice about what
you should and should not eat if you suffer from various common medical
conditions. You might expect, at the very least, to be able to have dialogues
like (1).
(1) a. User: I am allergic to eggs.
Computer: OK
User: Should I eat pancakes
Computer: No, because pancakes contain eggs, and eating things which
contain eggs will make you ill if you are allergic to eggs.
b. User: My son is very fat.
Computer: OK
User: Should he go swimming.
Computer: Yes, because swimming is a form of exercise, and exercise is
good for people who are overweight.
c. User: I have scurvy.
Computer: OK
User: Is eating berries good for me?
Computer: Yes, because berries contain vitamin C and eating fruit which
contains vitamin C is good for people who scurvy.
181
These are comparatively simple dialogues, requiring a very limited
amount of knowledge about foods and medical conditions. As we will see,
however, dealing with them does require a remarkable amount of knowledge
about language.
The framework we are using makes a number of very basic assumptions
about how you design a system to deal with such dialogues.
? To give appropriate answers to these questions you have to consider
whether the available information supports or contradicts the queried
proposition.
? In order to see whether the available information supports or contra-
dicts a proposition you need a body of domain knowledge, and you
need to be able to reason with it.
? Natural languages provide numerous ways of saying almost identical
things. There is, for instance, almost no difference between ?I am
allergic to eggs? and ?I have an allergy to eggs? . You therefore have to
have a way of dealing with paraphrases.
We will explore each of these issues in turn below.
2 Answering questions
We will concentrate here on polar (YES/NO) questions. We also take the
very simple view that when someone asks a polar question it is because they
want to know whether the proposition encoded in the question is true or
not. Someone who asks ?Is it safe for me to eggs?? wants to be told ?Yes?
if it is and ?No? if it is not. We have explored the nature of WH-questions
elsewhere (Ramsay & Seville, 2001), and we have discussed situations where
people use language in indirect ways (Ramsay & Field, 2008), but for the
moment just trying to answer polar questions will pose enough problems to
keep us occupied.
In order to answer such a question by saying ?Yes? you have to see
whether ?It is safe for the speaker to eat eggs? follows from what you know
about the speaker and your general knowledge. You cannot, however, say
?No? simply because your attempted proof that it is safe failed. If you failed
to prove that it is safe you should then see whether you can prove that it is
not. If, and only if, you can then you should say ?no? .
In general, then, answering a polar question may involve two attempted
proofs?one aimed at showing that the proposition under discussion is true
182
and then possibly a second aimed at showing that it is false. If you are
lucky you might discover evidence that the proposition is false while you are
trying to show that it is true, but in general you may have to attempt two
proofs.
In order to carry out proofs you need an inference engine. Inference
engines come in all sorts of shapes and sizes?fast or slow, sound or unsound,
complete or incomplete?and they can be applied to a variety of knowledge
representation schemes. The choice of representation scheme, and of the
inference engine that you apply to it, will depend on what you want to do
with it. For our current task we assume that soundness is crucial, since
you really would not want a medical advice system to give wrong advice;
and that the representation scheme has to be highly expressive, since the
relations involved are subtle and need to be represented very carefully.
This leads us to a very classical architecture: we construct formal para-
phrases (logical forms, LFs) of the user?s statements and questions, and we
use a theorem prover to investigate the status of the propositions encoded in
the user?s questions. We are, in particular, not following the pattern match-
ing path taken in most ?textual entailment? systems, since the informal rules
used in such systems are not guaranteed to be sound.
We use ?property theory? (Turner, 1987; Chierchia & Turner, 1987) as our
formal language. There are very strong grounds for believing that natural
language is inherently intensional (we will see some examples below, but
the simple presence of verbs of propositional attitude is hard to cope with
unless you allow some degree of intensionality). There are a number of
logics which allow for a degree of intensionality?the typed logic used by
Montague (Montague, 1974; Dowty et al, 1981), the notion of non-well-
founded sets (Aczel, 1988) used in situation semantics (Barwise & Perry,
1983), Bealer (1989)?s intensional logic, and so on. We choose property
theory because one of Turner?s axiomatisations draws a very strong analogy
with modal logic which in turn suggests ways of adapting standard first-order
theorem proving techniques for it. We have developed a theorem prover for
a constructive version of property theory along these lines (Ramsay, 1995;
Cryan & Ramsay, 1997), and have shown that it is sound (Ramsay, 2001).
No theorem prover for a logic with this degree of expressive power can be
complete?property theory, like default logic, is worse than first-order logic
in this respect, in that it is not even recursively enumerable. Practical
systems for first-order logic, however, do not proceed by enumerating all the
theorems. They do their best, and if they cannot find an answer within a
reasonable period of time then they give up. This the only sensible thing to
do, and it is just as sensible when reasoning with more expressive languages.
183
We do not, however, just want to find out whether the answer to the
user?s question is ?Yes? or ?No? . We would also like to provide them with
some explanation of how we arrived at our conclusion. It is much better to
answer ?Should I eat pancakes?? with ?No, because pancakes contain eggs,
and eating things which contain eggs will make you ill if you are allergic
to eggs.? than just by saying ?No? . The user will be more likely to accept
the system?s answer if it comes with some supporting explanation, and they
may also be able to generalise from the explanation to cover other cases.
Where might we get such explanatory material from? The obvious place
to look is in the trace of the proof that led to the conclusion. The proof
tree contains the facts and rules that the system used in arriving at its
conclusion. Showing these facts and rules to the user would let them see
why the system believes that the queried proposition is true or false, and
lets them judge the trustworthiness of what the system says.
There are two difficult problems to be addressed here. The first is that
the proof tree will contain a mixture of things that the user themselves
said, things are blindingly obvious and things that the system suspects that
the user might not know. The explanation should probably concentrate on
things that the user might not have been aware of, so we should be looking
for items in the proof tree that the system believes the user may not know.
In other words, we need an epistemic version of property theory, and we
need to be able to inspect facts and rules to see who has access to them.
We will not discuss this further here, except to note that in the concrete
examples below we are not doing this, so that the support for the system?s
conclusions currently includes material that the user would actually already
be aware of.
The second problem is that it is extremely difficult to generate natural
language text from arbitrary logical expressions. We use standard composi-
tional techniques to build our logical forms on the basis of the form of the
input text (van Genabith & Crouch, 1997; Konrad et al, 1996). However,
as with virtually any practical theorem prover, we then perform a number of
transformations (Skolemisation, distribution of negation, splitting rules with
conjunctive heads) to our logical forms in order to make them amenable to
the theorem prover. By the time we have done this there is very little hope
of using the compositional semantics in reverse to generate natural language
text from elements of the proof tree. There is, in particular, no chance of
using head-driven generation (Shieber et al, 1990) to produce text from el-
ements of the logical form, since this approach requires that the elements of
the logical form be unifiable with the meanings of lexical items in order to
drive the selection and combination of words. This is just not feasible with
184
the elements of a proof tree.
Where do the facts and rules that appear in the proof tree come from?
We clearly have to specify them in advance in some form. Some of this
information comes from the user, in the form of statements about their
conditions, but most of it will have to provided explicitly.
We can do this in a variety of ways. We could try to use some existing
resource?WordNet, CYC, some medical ontology. It turns out that these re-
sources, or at least the publicly available ones, lack a great deal of what we
need. Very few such resources contain the kind of rules you need for answer-
ing questions such as the ones in (1). Lexical resources contain information
about relations between words. WordNet, for instance, provides hypersensi-
tivity reaction, hypersensitivity, sensitivity, susceptibility, condition, state,
attribute, abstraction, entity as hypernyms of ?allergy??all perfectly sensi-
ble hypernyms, but not all that useful for answering (1a). Likewise the
only mention of allergy or allergic in the ontology in OpenGalen (version
7, downloaded 09/10/08) says that an allergy is a kind of pathology, and
SnoMed has ?propensity to adverse reactions? and disease as hypernyms, and
a variety of links to special types of allergies and other related conditions.
This is not, of course, an exhaustive search of all potentially relevant
ontologies, but it does suggest that the kind of information stored in a typical
ontology is not what we require for answering our questions. It is, however,
extremely interesting to note that WordNet contains an English gloss for
?allergy? as ?hypersensitivity reaction to a particular allergen; symptoms can
vary greatly in intensity? , OpenGalen contains the text ?Hypersensitivity
caused by exposure to a particular antigen (allergen) resulting in a marked
increase in reactivity to that antigen upon subsequent exposure sometimes
resulting in harmful immunologic consequences.? and SnoMed provides very
brief English glosses. It seems as though when ontology designers want to
say what a term really means, they resort to natural language.
It also seems as though this kind of ontology fails to include ?common-
sense? knowledge, e.g. that if you are allergic to a foodstuff then you should
not eat it. We need this kind of information in order to answer questions.
The prevalence of natural language glosses in ontological resources of this
kind, suggests that expressing it in natural language might be a good idea.
Using natural language to express the knowledge that we need has a
number of advantages:
? It is comparatively easy. Most people (even logicians and semanti-
cists!) generally find it easiest to express themselves in their native
language. It is much easier to write ?If you are allergic to something
185
then eating it will make you ill? than to express the same rule in some
formal language.
? Linking knowledge that has been expressed in natural language with
facts and queries that have been expressed in natural language obvi-
ates the need for a set of terminological mappings between domain
knowledge and natural language. The vocabularies used in termino-
logical databases tend to have a superfical resemblance to words in
natural languages, but the mapping is seldom exact, and indeed the
types associated with such terms are often quite different. If all your
knowledge is expressed in natural language then this kind of problem
can be avoided.
? Finally, it makes it much easier to generate answers. If we keep a link
between surface text and logic we can retrieve the surface text from
the proof tree. This does not entirely solve the problem of producing
coherent natural language answers, but it does make it much simpler.
3 English with variables
We therefore want to try writing down various commonsense rules in English.
To make it slightly easier to write rules, we allow variables in various places.
Thus we write (2b) rather than (2a).
(2) a. Eating fruit which contains vitamin C is good for you if you have
scurvy
b. Eating fruit which contains vitamin C is good for X if X has scurvy
This is helpful here simply to get around the fact that ?you? is normally
taken to be a reference to the hearer, whereas in (2a) it is being used in a
rather generic way. Rather than allowing ?you? to be ambiguous in this way,
we simply allow variables to be used in natural language.
The logical form we obtain for (2b) is shown in Fig. 1. There are a
number of things to note about Fig. 1:
? It?s enormous. Reading it you can see how it relates to (2b) itself,
but producing something like this by hand would certainly be a major
challenge. However, if we have a set of rules that explain the relation-
ship between structural (lexical and syntactic) choices and semantics
then we can obtain Fig. 1 directly from the parse tree of (2b). This and
186
?X?Bevent(B ,have)
&?C : {scurvy(C , D)}?(B ,object ,C )
&?(B ,agent ,X )
&aspect(now ,simple,B)
? ?Estate(E,
?F (?Gevent(G,eat)
& ?H : {fruit(H , I)
& ?Jevent(J ,contain)
& ?(J ,
object,
ref (?K(vitamin(K )
& ?(K ,
type,
ref (?L(named(L,C )))))))
& ?(J ,agent ,H )
& aspect(now ,simple,J )}
?(G,object ,H )
& ?(G, agent, F )),
?M(?N(good(N ,M ,normal))))
&for(E ,X )
&aspect(now ,simple,E )
Figure 1: Logical form for (2b)
all other logical forms in this paper were produced by applying compo-
sitional rules to the first parse we obtained from the relevant texts. So
although it is indeed enormous, and complicated, all we have to do is
write the rule in English and the system will take care of the rest.
? The analysis of ?eating fruit which contains vitamin C is good for you?
introduces a relationship between two intensional objects, namely ?the
kind of event where someone eats fruit which contains vitamin C? and
?the property of being good?. This has significant consequences for the
kind of inference that is required. We will explore this further below.
Once we allow variables in places where you might expect an NP, it
becomes tempting to introduce them in other places. The consequences of
doing this are interesting:
(3) a. Eating something will make you ill if you are allergic to it
b. Eating P will make X ill if X is allergic to P
187
Again the second version of the rule sidesteps some tricky details to do
with pronouns, but the formal paraphrase throws up some awkward issues.
?X?P?Cstate(C ,X , ?D(?E(allergic(E ,D ,normal)))) &to(C ,P)
&aspect(now ,simple,C )
? ?F : {future(now ,F )}
?Bevent(B ,make)
&?(B ,object ,X )
&?(B ,object1 , ?G(?H(ill(H ,G,normal))))
&?(B,
agent,
?I ?Jevent(J ,eat)
&?K : {P : K}?(J ,object ,K )
&?(J ,agent ,I ))
&aspect(F ,simple,B)
Figure 2: Logical form for (3b)
The new problem in Fig. 2 is the paraphrase of ?eating P will make X ill? ,
where ?P? stands for something like ?something of type P? . In other words,
P here is a variable noun rather than a variable NP.
Under almost any analysis, nouns denote kinds rather than individuals.
But that means that (3b) involves quantification over kinds, which is again
very intensional.
4 Inference
Constructing LFs which involve relations between intensional entities is not
problematic. As noted above, we know there are formal languages which al-
low for intensionality, so all we have to do is choose one of these for our LFs.
Indeed, most approaches to compositionality exploit ?-abstraction and ?-
reduction, so the intermediate objects that are constructed are inherently in-
tensional anyway. Anyone who takes the interpretation of ?man in a park? to
be something like ?A(?B : {park(B , C)}?D(man(D , E) & in(D ,B))&(A :
D)) (i.e. the standard Montague representation) is using an intensional
language as an intermediate representation.
Problems only arise when we try to reason with representations of this
kind. There is, however, very little point in making LFs if you are not going
to reason with them, so we do have to worry about it. To see a concrete
example, reconsider (1c), repeated as (4):
188
(4) [ User: I have scurvy.
Computer: OK
User: Is eating berries good for me?
Computer: Yes, because berries contain vitamin C and eating fruit which
contains vitamin C is good for people who scurvy.
Our rule about scurvy says that events of a certain kind are good for
people who have scurvy. The description of these events occupies an argu-
ment position in the LF, as one of the terms describing the general state of
affairs that holds if someone has scurvy.
In Prolog-based first-order theorem provers, you determine whether you
can use a rule to prove a goal by unifying the arguments of the goal with
the arguments of the consequent of the rule
1
. In the current case, this would
mean unifying the terms describing ?eating fruit which contains vitamin C?-
events and ?eating berries?-events.
Clearly these descriptions will not unify. What we have to do is to accept
that the rule can be used with terms that describe subsets of the classes that
appear in argument positions.
We do not want to do this everywhere. This is a characteristic of the rule
about the link between vitamin C and scurvy, not a general characteristic
of all rules. When we want to allow for this, we have to say so explicitly.
We therefore include a rule which says that the idea that events of some
kind are good or bad or safe or . . . for you is ?downward entailing?: if eating
fruit which contains vitamin C is good for you then eating berries is good
for you, because the set of ?eating berries?-events is a subset of the set of
?eating fruit which contains vitamin C?-events. This rule is given in Fig. 3.
?B?C?D : {?F : {state(B ,F ,E ,for(C ))&(?G(D:G) ? (F:G))}
state(B ,D ,E ,for(C ))
Figure 3: Downward entailment for states
Fig. 3 says that if events that satisfy the description F satisfy the property
E for the individual C, then so do all events whose description G entails F .
We need a similar rule to say that this kind of relationship is upward
entailing in the third argument?that anything which is good for you, for
1
Prolog-based theorem proving is a special case of resolution theorem proving. In
general resolution theorem provers you have to unify some positive literal in one clause with
a negative one in another. For simplicity we will talk in terms of goals and consequents,
but the analysis would apply to other resolution-based engines.
189
instance, is also safe for you. Rules like these exploit a notion of ?guarded
intensionality?, in that they are only applicable when you already know what
properties you are interested in. They thus essentially act as schemas for
first-order rules. If we only use them backwards, in situations where we
know what properties we are interested in, they can be applied in a fairly
controlled way, and hence do not introduce wild combinatorial problems.
This is not the only kind of intensional rule that we need, but it does
cover a substantial number of interesting cases. The theorem prover de-
scribed in (Ramsay, 2001) can cope with more general intensional rules, but
guarded rules of this kind can be dealt with more efficiently than general
rules, and they are particularly useful for axiomatising the phenomena that
interest us.
5 Paraphrases and other lexical relations
It is clear that we need to treat with a variety of relations between everyday
terms. We will return, as an example, to (1b), repeated here as (5).
(5) User: My son is very fat.
Computer: OK
User: Should he go swimming.
Computer: Yes, because swimming is a form of exercise, and exercise is good
for people who are overweight.
The computer?s answer to the user?s question clearly depends on an
understanding that if something is good for you then you should do it.
To say this is not, of course, to provide a complete characterisation of the
meaning of ?should? . It is just a piece of commonsense. Nonetheless, for a
system to be able to cope with (1b) it has to have access to this piece of
commonsense. Fig. 4 shows the axiomatisation of this notion: if events of
the kind described by B are good C, then if I describes an action whose
performance entailed that B held for C then I should happen.
?B?C?D : {state(D ,B , ?E(?F (good(E ,F ,G))),for(C ))
&(?H(I:H )) ? (B:C ))}
?J : {aspect(now ,simple,J )}should(J ,I )
Figure 4: If something?s good for you then you should do it
190
There are a variety of other very basic elements of commonsense which
have to be captured, and which are not generally included in formal ontolo-
gies. We need to know, for instance, that something cannot be both good
and bad, and that dangerous things are bad, and so on. Some of these can
only be axiomatised manually, but the aim is to keep things that have to
be encoded manually to a minimum. As noted earlier, writing axioms in
English is generally easier and it also makes them easily available for use
in explanations. Very basic things like the fact that things cannot be both
good and bad are unlikely to be required for explanations, even if they do
take part in proofs, so the fact that they are unavailable for this purpose
does not matter.
Some of these basic relations turn out to be bi-equivalences, or as near to
bi-equivalences as makes no difference. It is extremely difficult, for instance,
to articulate any difference between (6a) and (6b).
(6) a. I have an allergy to eggs.
b. I am allergic to eggs.
We could take account of this by introducing a pair of implications: ?X
has an allergy to P if X is allergic to P? and ?X is allergic to P if X has
an allergy to P? . This would work, in the sense that we would be able to
use these two constructions interchangeably, but it would slow the inference
engine down considerably. The presence of any pair of rules of the form
P ? Q and Q ? P will inevitably slow any theorem prover down, since any
attempt to prove Q is likely to lead to an attempt to prove P , which will in
turn lead to an attempt to prove Q. It is not difficult to catch simple loops
of this kind, but it is better to avoid them in the first place if possible.
We therefore use rules like this as part of the normal-forming pro-
cess. Construction of normal forms generally involves application of bi-
equivalences where one side has a form which is particularly well-suited to
the needs of a particular theorem proving algorithm. In resolution, for in-
stance, the rules ?(P&Q) ? (?P ? ?Q) and ?(P ? Q) ? (?P&?Q) are
used during the construction of the normal form because resolution looks
for matching positive and negative literals, so axioms that can be used to
ensure that the only negation signs appear at the lowest possible level are
useful.
The point of normal-forming, then, is to ensure that bi-equivalences are
applied just once, and in just one direction. We thus apply bi-equivalences
like the one between (6a) and (6b) during the construction the construction
of logical forms. This lets us cope with the fact that natural languages
191
typically provide a range of ways of saying virtually the same thing without
incurring the expense of applying rules which potentially lead to loops when
we are carrying out inferences.
There is a complication here. The system needs to realise that (7a) and
(7b) are also the same (and likewise for other variations).
(7) a. I have a severe allergy to eggs.
b. I am severely allergic to eggs.
Dealing with this requires considerable care in the design of logical forms.
Space precludes a deeper discussion of this issue, but this is something we
have to take care over.
6 Conclusions
The work described here covers very similar ground to work in textual en-
tailment (Dagan et al, 2005), in that we want to draw inferences based
on facts and rules expressed in natural language. Producing logical forms
and then using a theorem prover to carry out the required inference leads
to more reliable conclusions, since we can check that the theorem prover is
sound, and hence we can rely on the conclusions that it draws. It also leads
to deeper chains of inference, since the pattern matching algorithms gen-
erally employed for textual entailment do not lend themselves to repeated
application.
The approach outlined here does involve a number of risks. We might
not be able to express all the knowledge we want in natural language; we
might not be able to produce logical forms from our natural language rules;
when we have more rules the theorem prover might not be able to cope.
The last of these is the most dangerous. If there are rules which we
cannot express in natural language, or where we cannot convert the natural
language into a logical form, we can always express them directly in property
theory (or property theory with procedural attachment of appropriate, e.g.
mathematical, rules (Steels, 1979)). How long will it take when there are
large numbers of rules? The proofs for the examples here take around 0.1
sec. Most of this time is spent investigating intensional rules. Most of the
commonsense knowledge, however, is represented as Horn clauses. Indeed it
is represented as pure Prolog. The speed of Prolog programs is not affected
by the number of clauses that are present, so we are confident that adding
more rules will have very little effect on the performance so long as they
can be represented as Horn clauses. The key issue, then, is how many new
192
intensional rules we will need. Only time will tell, but we are hopeful that
we will retain a reasonable level of performance even when we have a more
substantial set of rules. If not, we will just have to make the theorem prover
faster.
References
P. Aczel (1988). Non-Well-Founded-Sets. CSLI Publications, Stanford.
J. Barwise & J. Perry (1983). Situations and Attitudes. Bradford Books, Cambridge,
MA.
G. Bealer (1989). ?Fine-grained type-free intensionality?. In G. Chierchia, B. H.
Partee, & R. Turner (eds.), Properties, types and meaning: vol I, foundational
issues. Kluwer Academic Publishers, Dordrecht/Boston/London.
G. Chierchia & R. Turner (1987). ?Semantics and Property Theory?. Linguistics
and Philosophy 11(3).
M. Cryan & A. M. Ramsay (1997). ?A Normal Form for Property Theory?. In Pro-
ceedings of the 14th International Conference on Automated Deduction (CADE-
14), vol. 1249 of Lecture Notes in Artificial Intelligence, pp. 237?251, Berlin.
Springer-Verlag.
I. Dagan, et al (2005). ?The PASCAL Recognising Textual Entailment Challenge?.
In Proceedings of Pascal Challenge Workshop on Recognizing Textual Entailment.
D. R. Dowty, et al (1981). Introduction to Montague Semantics. D. Reidel, Dor-
drecht.
K. Konrad, et al (1996). ?An education and research tool for computational se-
mantics?. In Proceedings of the 16th International Conference on Computational
Linguistics (COLING-96), pp. 1098?1102, Copenhagen.
R. Montague (1974). ?The proper treatment of quantification in ordinary English?.
In R. Thomason (ed.), Formal Philosophy: Selected Papers of Richard Montague,
New Haven. Yale University Press.
A. M. Ramsay (1995). ?A Theorem Prover for an Intensional Logic?. Journal of
Automated Reasoning 14:237?255.
A. M. Ramsay (2001). ?Theorem proving for untyped constructive ?-calculus: im-
plementation and application?. Logic Journal of the Interest Group in Pure and
Applied Logics 9(1):89?106.
A. M. Ramsay & D. G. Field (2008). ?Speech acts, epistemic planning and Grice?s
maxims?. Logic and Computation 18:431?457.
A. M. Ramsay & H. L. Seville (2001). ?Relevant Answers to WH-questions?. In 3rd
International Conference on Inference in Computational Semantics, pp. 73?86,
Siena.
193
S. M. Shieber, et al (1990). ?Semantic-Head-Driven Generation?. Computational
Linguistics 16(1):30?42.
L. Steels (1979). ?Procedural attachment?. Tech. rep., MIT.
R. Turner (1987). ?A Theory of Properties?. Journal of Symbolic Logic 52(2):455?
472.
J. van Genabith & R. Crouch (1997). ?How to glue a donkey to an f-structure?.
In H. C. Bunt, L. Kievit, R. Muskens, & M. Verlinden (eds.), 2nd International
Workshop on Computational Semantics, pp. 52?65, University of Tilburg.
Appendix: commonsense rules
(8) a. eating P will make X ill if X is allergic to P.
b. exercise is good for X if X is overweight.
c. swimming is good for X if exercise is good for X.
d. walking is good for X if exercise is good for X.
e. eating fruit which contains vitamin C is good for X if X has scurvy.
f. X eats P if X eats something which contains P.
g. X is dangerous for Y if X will make Y ill.
194
How to change a person?s mind:
Understanding the difference between
the effects and consequences of speech acts
Debora Field and Allan Ramsay
Computer Science, Univ. of Liverpool, L69 3BX, UK
Informatics, Univ. of Manchester, PO Box 88, M60 1QD, UK
debora@ csc. liv. ac. uk,allan. ramsay@ manchester. ac. uk
Abstract
This paper discusses a planner of the semantics of utterances, whose essential
design is an epistemic theorem prover. The planner was designed for the purpose
of planning communicative actions, whose effects are famously unknowable and
unobservable by the doer/speaker, and depend on the beliefs of and inferences made
by the recipient/hearer. The fully implemented model can achieve goals that do
not match action effects, but that are rather entailed by them, which it does by
reasoning about how to act: state-space planning is interwoven with theorem proving
in such a way that a theorem prover uses the effects of actions as hypotheses. The
planner is able to model problematic conversational situations, including felicitous
and infelicitous instances of bluffing, lying, sarcasm, and stating the obvious. 1
1 Introduction
The motivation for this research was the problem of planning the semantics
of communicative actions: given that I want you to believe P, how do I choose
what meaning to express to you? The well-documented, considerable difficul-
ties involved in this problem include this: a key player in the ensuing evolution
of the post-utterance environment is the hearer of the utterance.
First, consider an imaginary robot Rob, designed not for communication,
but for making tea. Whenever he is in use, Rob?s top-level goal is to attain a
state in which there is a certain configuration of cups, saucers, hot tea, cold
milk, etc. Rob?s plans for making tea are made on the strong assumption that
at plan execution time, the cups (and other items) will have no desires and
opinions of their own concerning which positions they should take up?Rob
expects to be the author of the effects of his actions. 2
1 Initially funded by the EPSRC. Recent partial funding under EU-grant FP6/IST No.
507019 (PIPS: Personalised Information Platform for Health and Life Services).
2 notwithstanding impeding concurrent events, sensor failures, motor failures, etc.
In contrast, consider the human John, designed for doing all sorts of
things besides making tea, including communicating messages to other hu-
mans. Imagine John?s current goal is to get human Sally to believe the propo-
sition John is kind. In some respects, John has a harder problem than Rob.
Unlike Rob, John has no direct access to the environment he wishes to affect?
he cannot simply implant John is kind into Sally?s belief state. John knows
that Sally has desires and opinions of her own, and that he will have to plan
something that he considers might well lead Sally to infer John is kind. This
means that when John is planning his action?whether to give her some choco-
late, pay her a compliment, tell her he is kind, lend her his credit card?he
has to consider the many different messages Sally might infer from the one
thing John chooses to say or do. Unfortunately, there is no STRIPS operator
[13] John can choose that will have his desired effect; he has to plan an action
that he expects will entail the state he desires.
We considered ?reasoning-centred? planning of actions that entailed goals
to be an approach that would enable this difficult predicament to be managed,
and implemented a model accordingly. Our planner is, in essence, an epistemic
theorem prover that hypothesises desirable actions, and is able to plan to
achieve goals that do not match action effects, but that are entailed by the
final state. Like John, the planner can have particular communicative goals in
mind, and knows that the execution of any single plan could have a myriad
different effects on H ?s belief state, depending on what H chooses to infer.
1.1 Bucking the trend
The main focus of current research in AI planning is on how to reduce the
search space required for making plans, and thus, for example, to get Rob the
tea-making robot to be able to make his plans fast enough to be of practical
use in the real world. Many planners use heuristics, either to constrain the
generation of a search space, or to prune and guide the search through the state
space for a solution, or both [4,5,18,25]. All such planners succeed by relying
on the static effects of actions?on the fact that you can tell by inspection
what the effects of an action will be in any situation?which limits their scope
in a particular way [4, p. 299]:
?. . . if one of the actions allows the planner to dig a hole of an arbitrary inte-
gral depth, then there are potentially infinitely many objects that can be cre-
ated. . . The effect of this action cannot be determined statically . . . ?
The class of problems that these planners do not attempt to solve?the ability
to plan actions whose effects are not determined statically?was the class that
particularly interested us.
2 Planning the semantics of utterances
With our attention firmly fixed on the myriad different effects a single commu-
nicative act can have on a hearer?s belief state, we concentrated on a (logically)
very simple utterance:
?There?s a/an [some object]!?
We devised situations culminating in this utterance which illustrate sarcasm,
stating the obvious, bluffing, and lying, and developed a planner which
could use these tactics. Here is a much-shortened example of a scenario from
the model, which leads to the planning of an instance of sarcasm: 3 4
Initial state John has been bird-watching with Sally for hours, and so far,
they have only seen pigeons. John thinks Sally is feeling bored
and fed up. John has some chocolate in his bag. John thinks
Sally likes chocolate. John knows lots of rules about how con-
versation works, and what one can expect a hearer to infer
under given conditions.
Goal condition John wants to cheer Sally up.
Solutions John is just thinking about getting out some chocolate to give
her, when yet another pigeon lands in a nearby tree. John sees
an opportunity to make Sally laugh by means of a bit of sar-
casm, and so plans to say to her,
?There?s an albatross!?
John plans (the semantics of) his utterance, expecting that the utterance
will have particular ?effects? on Sally?s belief state; if John were to perform
the utterance, he would not be certain that it had achieved his intention, but
he would expect that it probably had. Whether John?s intention would be
achieved by this utterance depends on Sally having the ?right? set of beliefs
(the ones John thinks she has) and making the ?right? inferences (the ones
John expects her to make).
For example, if John?s utterance ?There?s an albatross!? is to be felicitous,
the following must happen. Sally must first believe that John has said some-
thing that Sally thinks John and Sally mutually believe is false. From this, she
must infer that John has flouted a conversational maxim, and consequently
that John has attempted to implicate a meaning which is not expressed by the
semantics of ?There?s an albatross!?. Sally must then infer that the implica-
ture John intends is of humour. Whether or not any of this happens depends
on Sally?s beliefs, which John cannot observe, but about which he has beliefs.
The formal version of this example contains all the necessary information
about the beliefs of John and Sally in this situation for the planner: (i) to be
able to plan John?s utterance; and (ii) to additionally deduce whether John?s
utterance would be felicitous or infelicitous, if he performed it.
3 The example is an English paraphrase of a task, written in the model in Prolog code.
4 An albatross (Diomedea exulans) is a huge sea-faring bird, rarely seen from the land.
2.1 Linguistic motivations
Our approach to planning the semantics of utterances was to build on seminal
work in speech acts [3,28] and pragmatics [29,15,21]. In contrast to the ?speech
acts with STRIPS? approach [6,11,1,2], which is fraught with well-documented
difficulties [9,16,26,7,27], we aimed to develop a small set of linguistic acts that
were unambiguously identifiable purely by surface linguistic form (after [7]),
including ?declare?, ?request?, and perhaps others?a set of acts with negligible
effects (after [27]), and minimal preconditions. We in fact developed a single
linguistic act for all contexts.
2.2 Planner design
The planner is essentially an epistemic theorem prover which employs some
planning search. The development process we undertook is helpful in under-
standing the planner?s design:
? A state-space search was implemented that searches backwards in hypothetical
time from the goal via STRIPS operators (based on foundational work in classical
planning [23,24,14,22,13]);
? A theorem prover for FOL was implemented that constructively proves conjunc-
tions, disjunctions, implications, and negations, and employs modus ponens and
unit resolution;
? State-space search and theorem proving were interwoven in such a way that:
? not only can disjunctions, implications and negations be proved true, they can
also be achieved;
? not only can a goal Q be proved true by proving (P ? Q) ? P, but Q can
also be achieved by proving P ? Q and achieving P ;
? a goal can be achieved by reasoning with recursive domain-specific rules?thus
the planner is able to plan to ?dig holes of arbitrary depths?.
? The theorem prover was transformed into an epistemic theorem prover by incor-
porating a theory of knowledge and belief suitable for human reasoning about
action, so agents make plans according to their beliefs about the world, including
their beliefs about others? beliefs.
A goal is proved by assuming the effect of some action is true, on the
grounds that the goal would be true in the situation that resulted from per-
forming that action. Hence, a set of actions is computed that might be useful
for achieving a goal by carrying out hypothetical proofs, where the hypotheses
are the actions whose effects have been exploited.
Here is a simple, non-dialogue example to aid explanation. Consider the
achievement of the goal above(e,f) and on(e,d), where above is the transitive
closure of on. First, it is not possible to judge whether the first goal above(e,f)
is true by inspecting the current state (which contains on( , ) facts but no
above( , ) facts), so reasoning is carried out to find out whether it is false.
Secondly, in order to achieve above(e,f), something different from an action
with an above( , ) expression in its add list is needed. Placing e onto f, for
example, will make above(e,f) proveable, but it will also make the achievement
of on(e,d) impossible. By reasoning with rules that describe the meaning of
above as the transitive closure of on, the planner hypothesises that on(d,f)
might enable the proof of above(e,f) to be completed, and also knows that
on(d,f) is an effect of action stack(d,f). A proof of the preconditions of action
stack(d,f) is carried out, and the process continues (with backtracking), until
a solution is found.
The preference for a backwards planning search was motivated by a defin-
ing quality of the communication problem, as epitomised by utterance plan-
ning: there are too many applicable actions to make a forwards search feasible.
People generally have the physical and mental capabilities to say whatever
they want at any moment. This means that the answer to the question ?What
can I say in the current state?? is something like ?Anything, I just have to
decide what I want to say?. A backwards search is far more suitable than a
forwards search under conditions like these.
With this ?reasoning-centred? design, the planner is able to plan an utter-
ance to achieve a goal, ?knowing? that the utterance may or may not achieve
the desired effects on H, and that the same utterance can have many different
effects, depending on H ?s belief state.
3 Modelling problematic conversations
In the model, utterances are planned according to Grice?s Cooperative Prin-
ciple [15]. Here is an extract from the CP (ibid p. 308):
?[Quantity]
(i) Make your contribution as informative as is required (for the current purposes
of the exchange).
(ii) Do not make your contribution more informative than is required. . .
[Quality]
(i) Do not say what you believe to be false.
(ii) Do not say that for which you lack adequate evidence.?
Grice?s maxims prescribe a standard for speaker behaviour which S can bla-
tantly contravene (?flout?), thus signalling to H that there is an implicature
to be recovered. For instance, in our ?sarcasm? scenario, John?s utterance is
planned using the following maxim, derived from Grice?s first Quality maxim. 5
The first line means, ?If S addresses H by putting Q into the conversational
minutes?:
(1) minute([S], [H], Q)
and believes(S, believes(H, mutuallybelieve(([H, S]), not(Q))))
==> believes(S, believes(H, griceuncoop(S, [H], Q)))
5 The model embodies a ?deduction? model of belief [19], rather than a ?possible worlds?
model [17,20]. Thus agents are not required to draw all logically possible inferences, and
are therefore not required to infer an infinite number of propositions from a mutual belief.
Using this maxim, John reasons that he can get Sally to realise he is flouting
a maxim in order to generate an implicature (that he is being ?Grice uncoop-
erative with respect to Q ?). But what is the nature of the implicature? This
is dealt with by two additional rules: (2), which describes what John thinks
Sally believes about the meaning of this kind of maxim-flouting; and (3), a
?general knowledge? rule:
(2) believes(john,
believes(sally,
(griceuncoop(PERSON2, _PERSON1, Q)
and mutuallybelieve(([sally,john]), not(Q)))
==> funny(PERSON2, re(Q))))
(3) believes(john,
believes(sally,
(funny(PERSON2, re(Q))
==> happy(sally))))
With these three rules, John can reason that saying something he thinks
he and Sally mutually disbelieve will make her laugh, and thus cheer her up,
thus achieving his goal. Here is a second maxim from the model, also derived
from Grice?s CP:
(4) minute([S], [H], Q)
and believes(S, believes(H, mutuallybelieve(([H, S]), Q)))
==> believes(S, believes(H, griceuncoop(S, [H], Q)))
Using this maxim, and some additional rules, John can plan to flout Quan-
tity maxim 2, and generate an implicature by ?stating the obvious?.
3.1 Modelling deception
Grice?s CP seems an excellent formalism for planning and understanding ut-
terances, so long as everyone is committed to obeying it. We know, however,
that people violate the CP maxims?S contravenes maxims without wanting
H to know. For example, lying violates Quality maxim (1) , bluffing violates
Quality maxim (2) , and being economical with the truth violates Quantity
maxim (1) . However, there is nothing in Grice?s maxims to help H deal with
the possibility that S may be trying to deceive her. Our solution is to give S
and H some further maxims which legislate for the fact that speakers do not
necessarily always adhere to the CP, and which enable S to plan to deceive,
and H to detect intended deceptions.
3.1.1 Hearer violation maxims
Given that H admits the possibility that S might be trying to deceive her
with his utterance, we consider that there are three strong predictors of how
H ?s belief state will change in response to S ?s utterance of the proposition P :
(5) i What is H ?s view of the proposition P?
ii What is H ?s view concerning the goodwill of S?
iii What is H ?s view of the reliability of S ?s testimony?
Consider, for example, an attempt at bluffing: 6
Initial state John has gone bird-watching with Sally. John is wearing a warm
coat, and he thinks that Sally looks cold. John thinks Sally will be
impressed by a chivalrous gesture. John thinks Sally is new to bird-
watching, and that she is keen to learn about birds. John knows lots
of rules about how conversation works, and what one can expect a
hearer to infer under given conditions.
Goal condition John wants Sally to be impressed by him.
Solutions John is just thinking of offering Sally his coat to wear, when a huge
bird lands in a nearby tree. John isn?t quite sure what species the
bird is, nevertheless, he decides to try and impress Sally with his
bird expertise, and plans to say to her,
?There?s a dodo!?
Let us imagine that Sally?s answers to three above questions are as follows.
Before John performed his utterance:
(6) i Sally believed that the proposition P (?There?s a dodo!?) was false (because
she knew the bird was a buzzard).
Additionally, she did not believe that John thought that they mutually be-
lieved P was false.
ii She believed that John was well-disposed towards her.
iii She didn?t know whether John was a reliable source of information or not.
After John has said ?There?s a dodo!?, Sally derives the following new set of
beliefs from the above set:
(7) i? Sally still believes that the proposition P (?There?s a dodo!?) is false.
She now believes that John thinks that they mutually believe P is true.
ii? She still believes that John is well-disposed towards her.
iii? She now believes John is an unreliable source of information.
The mapping of belief set (6) into belief set (7) is determined in the model
by a ?hearer violation (HV) maxim?. We call this maxim the ?infelicitous bluff?
HV maxim. We have so far implemented eight HV maxims, however, there is
clearly scope for many more permutations of all the different possible answers
to (6). There are obvious additional refinements that should be made, for
example, people do not normally consider others to be reliable sources of
information on all subjects.
3.1.2 Speaker violation maxims
If S is to succeed in his attempt to deceive H, he will have to take into account
how H is going to try and detect his deception. To represent this in the model,
S has his own ?speaker violation (SV) maxims?, which concern the same issues
as the HV maxims, but from the other side of the table, as it were. What S
plans to say will depend on which answer he selects from each of these four
categories:
6 A dodo is a large flightless bird that is famously extinct.
(8) i What is S ?s view of H ?s view of various different propositions?
ii What is S ?s own view of the same propositions?
iii What is S ?s view of H ?s view of the goodwill of S?
iv What is S ?s view of H ?s view of the reliability of S as a source?
Here is an example of an SV maxim from the model:
(9) minute([S], [H], Q)
and believes(S, believes(H, reliable(S)))
and believes(S, believes(H, well_disposed_towards(S, [H])))
and believes(S, believes(H, Q or not(Q)))
==> believes(S, believes(H, gricecoop(S, [H], Q)))
Using this maxim, John can reason that Sally will believe he is being Grice-
cooperative, which means Sally will believe that what he is saying is true, even
if John does not believe it himself. Thus John is able to plan to lie to Sally by
using tactics he hopes will prevent Sally from detecting his attempt to deceive.
4 Epistemic theorem prover
The planner?s theorem prover embodies a constructive/intuitionist logic and
it proves theorems by natural deduction, chosen in preference to classical logic
and its inferencing methods. The way humans do every-day inferencing is, we
consider, quite different from the way inferencing is handled under classical
logic. In classical logic, for example, and using our general knowledge, we judge
the following formulae to be true:
(10) Earth has one moon ? Elvis is dead
(11) Earth has two moons ? Elvis is alive
(12) Earth has two moons ? Elvis is dead
(10) is true simply because antecedent and consequent are both true formulae.
We find this truth odd, however, because of the absence of any discernible
relationship between antecedent and consequent. (11) and (12) are true sim-
ply because the antecedent is false, which seems very counter-intuitive. Even
more peculiarly, the following formula is provable in classical logic in all cir-
cumstances:
(13) (Earth has one moon ? Elvis is dead) or
(Elvis is dead ? Earth has one moon)
but it feels very uncomfortable to say that it must be the case that one of
these implies the other.
In order to avoid having to admit proofs like this, and to be able to do
reasoning in a more human-like way, we opted for constructive logic and natu-
ral deduction. In order to prove P ? Q by natural deduction, one must show
that Q is true when P is true; if P is not true, constructive logic does not
infer P ? Q. This treatment of implication hints at a relationship between P
and Q which is absent from material implication.
4.1 Constructive logic and belief
Taking a constructive view allows us to simplify our reasoning about when the
hearer believes something of the form P ? Q, and hence (because of the con-
structive interpretation of ?P as P ? ?) about whether she believes ?P . We
will assume that believes(H,P ) means that H could infer P on the basis of
her belief set, not that she already does believe P , and we will examine the rela-
tionship between believes(H,P ? Q) and believes(H,P ) ? believes(H,Q).
Consider first believes(H,P ) ? believes(H,Q). Under what circumstances
could you convince yourself that this held?
For a constructive proof, you would have to assume that believes(H,P )
held, and try to prove believes(H,Q). So you would say to yourself ?Suppose
I were H, and I believed P . Would I believe Q?? The obvious way to answer
this would be to try to prove Q, using what you believe to be H?s rules of
inference. If you could do this, you could assume thatH could construct a proof
of P ? Q, and hence it would be reasonable to conclude believes(H,P ? Q).
Suppose, on the other hand, that you believed believes(H,P ? Q), and
that you also believed believes(H,P ). This would mean that you thought that
H had both P ? Q and P available to her. But if you had these two available
to you, you would be able to infer Q, so since H is very similar to you she
should also be able to infer Q. So from believes(H,P ? Q) and believes(H,P )
we can infer believes(H,Q), or in other words (believes(H,P ? Q)) ?
(believes(H,P ) ? believes(H,Q)).
We thus see that if we take believes(H,P ) to mean ?If I were H I would
be able to prove P ?, then (believes(H,P ? Q)) and (believes(H,P ) ?
believes(H,Q)) are equivalent. This has considerable advantages in terms of
theorem proving, since it means that much of the time we can do our reasoning
by switching to the believer?s point of view and doing perfectly ordinary first-
order reasoning. If, in addition, we treat ?P as a shorthand for P ? ?, we
see that believes(H,?P ) is equivalent to believes(H,P ) ? believes(H,?). If
we take the further step of assuming that nobody believes ?, we can see
that believes(H,?P ) ? ?believes(H,P ) (though not ?believes(H,P ) ?
believes(H,?P )). We cannot, however, always assume that everyone?s beliefs
are consistent, so we may not always want to take this further step (note
that in possible worlds treatments, we are forced to assume that everyone?s
beliefs are consistent), but it is useful to be able to use it as a default rule,
particularly once we understand the assumptions that lie behind it.
References
[1] Allen, J. F. and C. R. Perrault, Analyzing intention in utterances (1980), AI
15: 143?78.
[2] Appelt, D. E., Planning English referring expressions (1985), AI 26: 1?33.
[3] Austin, J. L., How to do things with words (1962), Oxford: OUP, 2nd edition.
[4] Blum, A. L. and M. L. Furst, Fast planning through planning graph analysis
(1995), in Proc. 14th IJCAI, pp. 1636?1642.
[5] Bonet, B. and H. Geffner, Heuristic Search Planner (2000), AI Magazine 21(2).
[6] Bruce, B. C., Generation as a social action (1975), in B. L. Nash-Webber and
R. C. Schank (eds), Theoretical issues in natural language processing, pp. 74?7.
Cambridge, Massachusetts: ACL.
[7] Bunt, H., Dialogue pragmatics and context specification (2000), [8], pp. 81?150.
[8] Bunt, H. and W. Black, (eds), Abduction, belief and context in dialogue: studies
in computational pragmatics (2000), Philadelphia: John Benjamins.
[9] Cohen, P. R. and H. J. Levesque, Rational interaction as the basis for
communication (1990), [10], pp. 221?55.
[10] Cohen, P. R., J. Morgan and M. E. Pollack, (eds), Intentions in communication
(1990), Cambridge, Massachusetts: MIT.
[11] Cohen, P. R. and C. R. Perrault, Elements of a plan-based theory of speech
acts (1979), Cognitive Science 3: 177?212.
[12] Feigenbaum, E. A. and J. Feldman, Editors, Computers and thought (1995),
Cambridge, Massachusetts: MIT Press. First published 1963 by McGraw-Hill.
[13] Fikes, R. E. and N. J. Nilsson, STRIPS: A new approach to the application of
theorem proving to problem solving (1971), AI 2: 189?208.
[14] Green, C., Application of theorem proving to problem solving (1969), in Proc.
1st IJCAI, pp. 219?39.
[15] Grice, H. P., Logic and conversation (1975), in P. Cole and J. Morgan, (eds),
Syntax and semantics 3: Speech acts, pp. 41?58. New York: Academic Press.
[16] Grosz, B. J. and C. L. Sidner, Plans for discourse (1990), [10], pp. 416?44.
[17] Hintikka, J., Knowledge and belief: An introduction to the two notions (1962),
New York: Cornell University Press.
[18] Hoffmann, J. and B. Nebel, The FF planning system: Fast plan generation
through heuristic search (2001), Journal of AI Research 14: 253?302.
[19] Konolige, K., A deduction model of belief (1986), London: Pitman.
[20] Kripke, S., Semantical considerations on modal logic (1963), in Acta
Philosophica Fennica 16: 83?94.
[21] Lewis, D., Scorekeeping in a language game (1979), J. Phil. Logic 8: 339?59.
[22] McCarthy, J. and P. J. Hayes, Some philosophical problems from the standpoint
of artificial intelligence (1969), Machine Intelligence 4: 463?502.
[23] Newell, A., J. C. Shaw and H. A. Simon, Empirical explorations with the logic
theory machine (1957), Proc. Western Joint Computer Conference, 15: 218?239.
[24] Newell, A. and H. A. Simon, GPS, a program that simulates human thought
(1963), [12], pp. 279?93.
[25] Nguyen, X. and S. Kambhampati, Reviving partial order planning (2001), in
Proc. IJCAI, pp. 459?66.
[26] Pollack, M. E., Plans as complex mental attitudes (1990), [10], pp. 77?103.
[27] Ramsay, A., Speech act theory and epistemic planning (2000), [8], pp. 293?310.
[28] Searle, J. R., What is a speech act? (1965), in M. Black, (ed), Philosophy in
America, pp. 221?39. Allen and Unwin.
[29] Stalnaker, R., Pragmatics (1972), in D. Davidson and G. Harman, (eds),
Semantics of natural language (Synthese Library, Vol. 40), pp. 380?97.
Dordrecht, Holland: D. Reidel.
Everyday Language is Highly
Intensional
Allan Ramsay
University of Manchester (UK)
email: allan.ramsay@manchester.ac.uk
Debora Field
University of Sheffield (UK)
email: D.Field@sheffield.ac.uk
Abstract
There has recently been a great deal of work aimed at trying to extract
information from substantial texts for tasks such as question answering.
Much of this work has dealt with texts which are reasonably large, but
which are known to contain reliable relevant information, e.g. FAQ lists,
on-line encyclopaedias, rather than looking at huge unorganised resources
such as the web. We believe, however, that even this work underestimates
the complexity and subtlety of language, and hence will inevitably be
restricted in what it can cope with. In particular, everyday use of lan-
guage involves considerable amounts of reasoning over intensional ob-
jects (properties and propositions). In order to respond appropriately to
simple-seeming questions such as ?Is going for a walk good for me??, for
instance, you have to be able to talk about event-types, which are intrinsi-
cally intensional. We discuss the issues involved in handling such items,
and shows the kind of background knowledge that is required for drawing
the appropriate conclusions about them.
193
194 Ramsay and Field
1 Introduction
The work reported here aims to allow users to interact with a health information sys-
tem via natural language. In this context, allowing a user to make simple statements
about their condition and then ask questions about what they can or should do, as in
(1), seems to be a minimal requirement.
(1) My doctor says I am allergic to eggs. Is it safe for me to eat cake?
Understanding such utterances requires the use of a highly intensional representa-
tion language, and responding to them requires a surprising amount of background
knowledge. We will consider below the problems that such everyday utterances bring
for formal paraphrases of natural language, and we will look at the kind of back-
ground knowledge that is required for producing the right kinds of response. In order
to produce a system that carries out the required inference we need access to an in-
ference engine for carrying out proofs in a representation language with the required
expressive power. The details of the engine we use are beyond the scope of this paper.
(Ramsay, 2001; Ramsay and Field, 2008). For the purposes of the current paper we
will simply show the results that can be obtained by using it.
The work reported here is complementary to work on corpus-based approaches
such as textual entailment: approaches that ignore the intensionality of everyday lan-
guage will inevitably fail to capture important inference patterns, but on the other hand
the work reported here cannot deal with large amounts of information provided as free
text. Ideally, the two approaches will be combined. The aim of the current paper is
to provide a reminder of the prevalence of intensionality in everyday language, and
to demonstrate that modern theorem proving techniques can cope with this kind of
knowledge without introducing undue processing delays.
2 Background
The general idea behind the work reported here is that users will input statements
about their health, either spontaneously or in response to prompts from the system,
and will ask questions about what they can and should do, and the system will provide
them with appropriate guidance. The overall architecture is completely classical:
1. The user?s input is translated into a meaning representation (logical form, LF)
in some suitable representation language.
2. This LF contains a specification of the illocutionary force of the input (is it a
statement, or a question, or a command, or . . . ?).
3. If the utterance is classified as a statement, its propositional content is added to
the system?s view of the user?s beliefs, and if it is classified as a question, the
system will attempt to use its background knowledge of the domain to answer
it. We are not currently attempting to make the system do anything in response
to a command from the user, since users do not generally issue commands in
our chosen domain, but clearly if this did happen then we would want to make
the system construct a plan to carry out the required action.
Everyday Language is Highly Intensional 195
This part of the system?s activity requires it be able to access and exploit relevant
background knowledge. This is obvious in the case of questions, but in the given
domain it is also important to be able to spot situations where the user?s beliefs
are incomplete or are in conflict with the system?s beliefs, since most people?s
understanding about medical topics is flawed. The ability to reason about what
has been said, then, is crucial to the construction of appropriate responses.
This architecture is entirely orthodox. What is unusual about the current work is
the emphasis on intensionality, so the first thing to do is examine why we believe that
this is such a significant problem.
1. Doctors and patients make extensive use of generic NPs and bare plurals: ?If
you follow this diet you should manage to control them without drugs?, ?Do you
normally have snacks??, ?When I started chemotherapy, on the 2nd of August,
glycaemia was still rather high? . . .
Such NPs are not, in fact, all that much more prevalent in this domain than in
general language. Across the BNC, for instance, it turns out that 27% of NPs
have ?the? as their determiner, 19% are bare plurals, 29% are bare singulars,
11% have ?a? or ?an? as their determiner, and the remainder have a variety of
other determiners1.
Thus bare plural and generic singular NPs occur about as frequently as ?the? and
?a?, and substantially more freqently than ?some?, ?all? and ?every? (less than
1% each). They have, however, been much less widely discussed by formal
semanticists, and there are a number of serious problems with the analyses that
have been proposed (Carlson, 1989; Ramsay, 1992; Cohen, 1994).
2. Everyday language is littered with words that can be used either as nouns or
verbs, and many of the apparently verbal uses of such words occur in essentially
nominal contexts. Table 1 shows the pattern of usage for three common words2,
but it should be noted that about 25% of the instances that are classified as
verbs are present participle forms, many of which are actually nominal or verbal
gerunds and hence should be regarded as nouns.
Table 1: Uses of common words in the BNC
Verb Noun Other
walk 75% 22% 3%
run 70% 24% 6%
kick 63% 35% 2%
Axiomatisation of the semantics of such words requires considerable care, since
we need to ensure that all the examples in (2) have very similar consequences.
(2) a. Swimming is good for you.
1The count of bare singulars is in fact a slight overestimate, since it includes some uses of singular nouns
as modifiers.
2The classification is taken directly from the BNC tags.
196 Ramsay and Field
b. Going for a swim is good for you.
c. It is good for you to go swimming.
3. The goal of the project is to produce appropriate responses to simple statements
and queries about a patient?s health. To do this, we need to be able to specify
a body of background knowledge in this area. We believe that for applications
such as medical information provision it is important that the information pro-
vided be as accurate as possible, and hence that it may be necessary to provide
the required background knowledge from scratch. This is, of course, a very
time-consuming and challenging activity, and it would be nice to be able to
side-step it by extracting the required information from existing texts. Unfortu-
nately, it seems likely that any such existing text will contain gaps which will
lead to the generation of partial, or wrong, answers. As noted above, ideally we
would want to link special purpose knowledge of the kind outlined here with
information extracted from existing texts, but for the current paper we are just
looking at what is involved in providing the required knowledge from scratch.
It turns out, as will be seen below, that much of this knowledge involves quantifica-
tion over situation types (of roughly the kind discussed by (Barwise and Perry, 1983)),
and in particular it involves statements about whether one situation type is a subset of
another, or is incompatible with it. This kind of knowledge is intrinsically intensional,
but it is hard to see how it can be avoided in this domain.
3 Logical forms
The logical forms that we use are fairly orthodox.
? We assume that events are first-class objects, as suggested by Davidson David-
son (1967, 1980).
? We allow other entities to play named roles with respect to these events, where
we denote that some item X is, for instance, the agent of some event E by writing
?(E,agent,X): using this notation, rather than writing agent(E,X), allows us
to quantify over thematic roles, which in turn allows us to state generalisations
that would otherwise be awkward.
? We treat tense as a relation between speech time and ?reference time?, and aspect
as a relation between reference time and event time, as suggested by Reichen-
bach Reichenbach (1947, 1956).
? We use ?reference terms? to denote referring expressions, so that re f (?Xman(X))
is used to denote ?the man?. Reference terms are similar to ?anchors? from (Bar-
wise and Perry, 1983), though the treatment is essentially proof-theoretic (sim-
ilar to the discussion of presupposition in (Gazdar, 1979; van der Sandt, 1992))
rather than model theoretic.
? Given that we are particularly concerned with the intensional nature of natural
language, we need to use a formal language that supports intensionaly. The
Everyday Language is Highly Intensional 197
language we choose is a constructive version of property theory (Turner, 1987;
Ramsay, 2001). We have extended the theorem prover described in (Ramsay,
2001) to cope with reasoning about knowledge and belief, and we have shown
how this can be used to carry out interesting inferences in cooperative and non-
cooperative situations (Ramsay and Field, 2008).
We also include the surface illocutionary force in the LF, since this is part of the
meaning of the utterance and hence it seems sensible to include it in the LF. In partic-
ular, there are interactions between surface illocutionary force and other aspects of the
meaning which are hard to capture if you treat them independently. This is slightly
less standard than the other aspects of our LFs, but it does have the advantage that
these LFs keep all the information that we can obtain by inspecting the form of the
utterance in one place.
A typical example of an LF for a simple sentence is given in Figure 13.
(3) The man loves a woman.
claim(?B : {woman(B)}
?C : {past(now,C)}
?D : {aspect(C,simplePast,D)}
?(D, agent, ref (?E(man(E))))
&?(D,object,B)
&event(D, love))
Figure 1: Logical form for (3)
If you want to reason about utterances in natural language, e.g. in order to answer
questions on the basis of things you have been told, then there seems to be no alterna-
tive to constructing LFs of the kind in Figure 1, axiomatising the relevant background
knowledge, and then invoking your favourite theorem prover. Shallow semantic anal-
ysis simply does not provide the necessary detail, and it is very hard to link textual
entailment algorithms (Dagan et al, 2005) to complex domain knowledge. The crit-
ical issue in connecting NLP systems to rich axiomatisations of domain knowledge
seems likely to be that existing frameworks for constructing meaning representations
are not rich enough, not that they are too rich. In the remainder of this paper we will
explore three specific issues that have arisen in our attempt to use natural language
as a means for accessing medical knowledge. We have beoome sensitised to these
issues because of their importance for our application, but we believe that they are ac-
tually widespread, and they will need to be solved for any system which links natural
language to complex domain knowledge.
4 Bare NPs
Consider (4):
3All the formal paraphrases in this paper are obtained from the target sentences by parsing the text and
using the standard techniques of compositional semantics.
198 Ramsay and Field
(4) a. I am eating eggs.
b. I eat eggs.
c. I am allergic to eggs.
What is the status of ?eggs? in these sentences?
It is clear that in (4a) there are some eggs that I am eating, so that (4a) means some-
thing quite like ?I am eating some eggs.?. (4b), on the other hand, means something
fairly different from ?There are some eggs that I eat?, since it does not seem to commit
the speaker to the existence of any specific set of eggs. The use of the simple aspect
with a non-stative verb gives (4b) a habitual/repeated interpretation, saying that there
are numerous eating events, each of which involves at least one egg.
It seems, then, that it is possible to treat ?eggs? in (4a) and (4b) as a narrow scope
existential, with the simple aspect introducing a set of eating events of the required
kind.
You would not, however, want to paraphrase (4c) by saying that there are some eggs
to which I am allergic. (4b) says that there is a relationship between me and situations
where there is an egg present, namely that if I eat something which has been made
out of some part of an egg then I am likely to have an allergic reaction. The bare
plural ?eggs? in (4c) seems to have some of the force of a universal quantifier. This is
problematic: does the bare plural ?eggs? induce an existential or a universal reading,
or something entirely different?
Note that the word ?eggs? can appear as a free-standingNP (as in (4a)) or as the head
noun of an NP with an explicit determiner (as in ?He was cooking some eggs.?). In the
latter context, the meaning of ?eggs? is normally taken be the property ?X(egg(X)),
to be combined with the determiner ?some? to produce an existentially quantified ex-
pression which can be used as part of the interpretation of the entire sentence.
It is clear that there are constructions that involve allowing prepositions to take
nouns rather than NPs as their complements, in examples like ?For example, cockerels
generally have more decorative plumage than hens?, where ?example? is evidently a
noun rather than an NP. If we allow the adjective ?allergic? to select for a PP with a
noun complement rather than an NP complement, we can obtain an interpretation of
(4c) which says that my allergy is a relation between me and the property of being an
egg (= the set of eggs) (Figure 2).
utt(claim,
?Bstate(B,allergic(to,
?C(egg(C))),
ref (?D(speaker(D)))!0)
&aspect(now,simple,B))
Figure 2: Logical form for (4c)
Thus we can distinguish between cases where ?eggs? is being used as an NP, where
it introduces a narrow scope existential quantifier, and ones where it is being used as
an NN, where it denotes, as usual, the property ?(X ,egg(X)). We still have to work
Everyday Language is Highly Intensional 199
out saying that the relationship ?allergic? holds between me and the property of being
an egg, but at least we have escaped the trap of saying that it holds between me and
some eggs (or indeed all eggs). We will return ton this in ?6
5 Nominalisations and paraphrases
As noted above, there are often numerous ways of saying very much the same thing,
and these often involve using combinations of nominal and verbal forms of the same
root. To cope with these, we have to do two things: we have to construct appropriate
logical forms, and we have to spot cases where we believe that there is no significant
difference between the various natural language forms and introduce appropriate rules
for treating one as canonical.
Gerunds and gerundives occur in very much the same places as bare NPs, and have
very much the same feeling of being about types of entity.
(5) a. Exercise is good for you.
b. Swimming is good for you.
(6) a. I like watching old movies.
b. I like old movies.
It therefore seems natural to treat them in much the same way, as descriptions of
event types, as in Figure 3
utt(claim,
?Bstate(B,
?C(?Devent(D,swim) & ?(D,agent,C)),
?E(good(E)))
&for(B,ref (?F(hearer(F)))!4)
&aspect(now,simple,B))
Figure 3: Logical form for (5b)
The logical form in Figure 3 says that there is a state of affairs relating events where
someone does some swimming and the property of being good, and that this state of
affairs concerns the speaker. This does at least have the benefit of exposing the key
concepts mentioned in (5b), and of doing so in such a way that it is possible to write
rules that support appropriate chains of inference.
The kind of inferencewe are interested in concerns patterns like the ones in Figure 4
Exercise is good for you if you are overweight
Swimming is a form of exercise
I am obese
Should I go swimming?
Figure 4: A simple(!) pattern of natural reasoning
200 Ramsay and Field
We will discuss the rules and inference engine that are required in order to support
this kind of reasoning in ?6 and ?7. For now we are concerned with the fact that the
last line in Figure 4 could have been replaced by a number of alternative forms such
as ?Is swimming good for me?? or ?Is it good for me to go swimming? without any
substantial change of meaning.
In general, we believe that determining the relationships between sentences re-
quires inference based on background rules which describe the relationships between
terms. However, when we have forms which are essentially paraphrases of one an-
other, these rules will tend to be bi-equivalences?rules of the form P ? Q. Such rules
are awkward for any theorem prover, since they potentially introduce infinite loops:
in order to prove P you can try proving Q, where one of the possible ways of proving
Q is by proving P, . . . It is possible to catch such loops, and our inference engine does
monitor for various straightforward loops of this kind, but they do introduce an extra
overhead. Equivalences of this kind are, in any case, not really facts about the world so
much as facts about the way natural language describes the world. It seems therefore
more sensible to capture them at the point when we construct our logical forms, when
they can be dealt with by straightforward pattern matching and substitution on logical
forms, rather than by embodying them as bi-directional rules to be used as required by
the inference engine. We use rules of the kind given in Figure 5 to canonical versions
of logical forms for sentences which we regard as mutual paraphrases. These rules
are matched against elements of the logical form, and the required substitutions are
made. This process is applied iteratively, so that multiple rules can be applied when
necessary.
?B : {allergy(B,C)}
?Devent(D,have) & ?(D,object,B)
& ?(D,agent,E) & aspect(X,Y ,D)
? ?Fstate(F,E,?G(allergic(G)),to(C))
& aspect(X,Y ,F)
event(B,go)
&?(B,event,?C(event(C,D)))
&?(B,agent,E)
? event(B,D) &?(B,agent,E)
Figure 5: Canonical form rules
The first of the rules in Figure 5 captures the equivalences between ?I have an al-
lergy to eggs? and ?I am allergic to eggs?, ?having an allergy to milk is bad news? and
?being allergic to milk is bad news?, and so on, and the second captures the equiv-
alences between ?I like walking? and ?I like going walking?, ?Swimming is good for
you? and ?Going for a swim is good for you?, and so on. These equivalences have to
be captured somewhere, and we believe that canonical forms of this kind arte a good
way to do it. We will return to where the rules in Figure 5 come from in ?8.
Everyday Language is Highly Intensional 201
6 Intensional predicates
The material we are interested in, like all natural language, makes extensive use of
intensional predicates. The adjective ?good? in ?Going swimming is good for you? ex-
presses a relationship between an event type (?going swimming?) and an individual;
the verb ?make? in ?Eating raw meat will make you feel sick? expresses a relation-
ship between an event type (?eating raw meat?) and a state of affairs (?you are ill?).
Constructions like these are widespread, and are inherently intensional. To draw con-
clusions about sentences involving them, you have to be able to reason about whether
one event type or one parameterised state of affairs is a subset of another, which is the
essence of intensionality.
Once you recognise that examples like these involve event types and propositions,
it is fairly straightforward to construct appropriate logical forms. We simply use the
notation of the ?-calculus to depict abstractions (e.g. event types), and we allow propo-
sitions to appear in argument positions, and standard techniques from comppsitional
semantics do the rest.
?C : {future(now,C)}
?Bevent(B,make)
&?(B,
scomp,
?D(event(D,feel)
& ?(D,object,?E(sick(E)))
& ?(D,agent,ref (?F(hearer(F)))!5)))
&?(B,
cause,
?G ?Hevent(H,eat)
&?I : {raw(I) & meat(I)}?(H,object,I)
&?(H,agent,G))
&aspect(C,simple,B)
Figure 6: Eating raw meat will make you feel sick
Figure 6 describes a relationship between situations where you eat raw meat and
ones where you feel sick. This is entirely correct: what else could this sentence de-
note?
Constructing formal paraphrases for sentences involving intensional predicates is
thus both straightforward (so long as you can parse them) and essential. Formal lan-
guages that support such paraphrases are, however, potentially problematic. The key
problem is that such languages tend to permit paradoxical constructions such as the
Liar Paradox and Ruessll?s set which introduce sentences which are true if and only if
they are false. It is difficult to provide semantics for languages which allow paradoxes
to be stated, but there are a number of ways out of this dilemma, either by putting
syntactic restrictions on what can be said (Whitehead and Russell, 1925; Jech, 1971)
or by devising appropriate interpretations (Turner, 1987; Aczel, 1988). We choose to
employ a constructive variant of property theory, because it allows us a comparatively
straightforward and implemetable proof theory, but it does not really matter what you
choose. What does matter is that if you choose a language with less expressive power
202 Ramsay and Field
than natural language, such as description logic, your paraphrases must fail to support
some of the distinctions that are expressible in natural language, and as a consequence
you will inevitably draw incorrect conclusions from the texts you are processing.
7 Inference
Consider (7):
(7) a. Eating eggs will make you ill if you are allergic to eggs.
b. I am allergic to eggs.
c. Will eating fried-egg sandwiches make me ill?
It is pretty obvious that the answer to (7c), given (7a) and (7b), must be ?Yes?. The
reasoning that is required to arrive at this answer turns out to be suprisingly complex.
The problem is, as noted above, that we need to reason about relationships between
event types. We need to be able to spot that events where someone eats a fried-egg
sandwich involve situations where they eat an egg. It is clearly quite easy, if tedious,
to write rules that say that if someone eats something which contains an egg then they
must eat an egg, and that fried-egg sandwiches contain eggs. The trouble is that we
have to be able invoke this rule in order to determine whether the arguments of ?make?
are of the right kind. Because we are (correctly) allowing event types as arguments
in intensional predicates, we have to be able to invoke arbitrary and unpredictable
amounts of inference even to determine whether the arguments of a predicate are ad-
missible. Roughly speaking, we have to be prepared to carry out arbitrary amounts of
inference at the point where first-order theorem provers invoke unification.
There is nothing to stop us doing this. Sorted logics, for instance, use an extended
notion of unification to try to ensure that items that are being considered as arguments
have specific properties (Cohn, 1987). We can, indeed, do any computation we like in
order to verify the suitability of arguments. The more complex the computations we
perform, of course, the longer it may take to come to a decision. The key is thus to
try to bound the potential costs without compromising what we can do too much. We
exploit a notion of ?guarded? axioms, where we allow arbitrary amounts of reasoning
to be performed to verify that some item fits a fully specified description, but we do
not allow such reasoning to be used for generating candidates. We do, of course, have
to put a bound on the amount of work that will be done at any point, as indeed any
inference engine for a language as expressive as first-order logic must do. In general,
however, using guarded intensionality in this way allows us to cover a wide range
of cases which are simply inexpressible using first-order logic (or any fragment of
first-order logic, such as description logic) comparatively inexpensively.
8 Conclusions
We have argued that in order to cope properly with even quite straightforward uses
of language, you need large amounts of background knowledge, much of which has
to be couched in some highly intensional framework, and you need inference engines
which can manipulate this knowledge. In the body of the paper we have shown a
number of examples which we believe illustrate this argument, and have looked at the
representations and rules that we employ for dealing with these cases. The natural
Everyday Language is Highly Intensional 203
question that arises at this point is: that?s all very well, but can the approach outlined
here be extended to cover a more substantial set of cases?
There are two key issues here. How difficult is it to capture a reasonably substantial
body of knowledge within the framework we have outlined, and what will happen to
the inference engine when we do?
Writing rules in property theory is very hard work. Writing rules in property theory
which will mesh nicely with logical forms obtained from natural language sentences
is extremely hard work. If we had to hand-code the rules we want directly in property
theory (or indeed in any formal language) then the approach discussed here would,
clearly, be impossible to extend to cover more than a handful of cases. Fortunately,
however, we have a much easier way of constructing rules. We have, after all, a
mechanism for converting natural language sentences into logical forms. So if we
state the rules we want in natural language we will obtain logical forms of those rules,
and furthermore those paraphrases will automatically be couched in terms which mesh
nicely with logical forms obtained from other natural language sentences. Thus (8)
produces the rule in Figure 7
(8) Eating Y will make X ill if X is allergic to Y.
?C?D?Estate(E,C,?F(allergic(F))) & to(E,D)
& aspect(now,simple,E)
? ?G : {future(now,G)}
?Bevent(B,make)
&?(B,object,C)
&?(B,object1,?H(ill(H)))
&?(B,
agent,
?I ?Jevent(J,eat)
& ?(J,object,D)
& ?(J,agent,I))
&aspect(G,simple,B)
Figure 7: Logical form for (8)
Writing rules like (8) is clearly easier than producing formulae like Figure 7 by
hand. Writing down all the knowledge you need in order to cope with a non-trivial
domain is still a very substantial task, but doing it in English is at least feasible in a
way that doing it directly in a formal language is not.
How will the inference engine cope when confronted with thousands of rules? Very
large parts of everyday knowledge can, in fact, be expressed pretty much as Horn
clauses. Our inference engine converts Horn clauses into (almost) pure Prolog, and
there is certainly no problem in using very large sets of Horn clauses converted to
this form (a modern Prolog system will cope comfortably with sets of several hun-
dred thousand Horn clauses, and will carry out substantial inference chains involving
such sets in small fractions of a second). The only concern here relates to non-Horn
clauses (which do not tend to occur all that frequently in rules explaining the rela-
tionships between natural language terms) and intensional rules. The fact that most
204 Ramsay and Field
intensional rules are guarded has certainly meant that so far we have not encountered
any problems when using them, and we are hopeful that this will remain the case.
In any case, there is an alternative question to be answered: what will happen if
you don?t take the approach outlined here? All the phenomena we have discussed are
widespread?bare plurals, mutual paraphrases, intensional attitudes all occur all over
the place. It is extremely hard to see that systems that rely on surface patterns (either
directly, as in textual entailment, or indirectly through shallow parsing/information
extraction) can support the kind of reasoning required for getting from ?I have an
allergy to eggs.? to ?It is dangerous for me to eat pancakes?, so at some point inference
based on background knowledgewill have to be invoked. There seems little alternative
to constructing formal paraphrases that capture the subtleties of natural language in all
its glory. If you don?t, then you will by definition lose some of the information that
was expressed in the text, and that will inevitably mean that you get things wrong.
There is no way round it: either you bite the bullet, construct formal paraphrases that
capture the content of the input and use them to carry out inference, or you will get
some things wrong.
References
Aczel, P. (1988). Non-Well-Founded-Sets. Stanford: CSLI Publications.
Barwise, J. and J. Perry (1983). Situations and Attitudes. Cambridge, MA: Bradford
Books.
Carlson, G. (1989). On the semantic composition of English generic sentences. In
G. Chierchia, B. H. Partee, and R. Turner (Eds.), Properties, Types and Meaning
II: Semantic Issues, Dordrecht, pp. 167?192. Kluwer Academic Press.
Cohen, A. (1994). Reasoning with generics. In H. C. Bunt (Ed.), 1st International
Workshop on Computational Semantics, University of Tilburg, pp. 263?270.
Cohn, A. G. (1987). A more expressive formulation of many sorted logic. Journal of
Automated Reasoning 3, 113?200.
Dagan, I., B. Magnini, and O. Glickman (2005). The PASCAL recognising textual en-
tailment challenge. In Proceedings of Pascal Challenge Workshop on Recognizing
Textual Entailment.
Davidson, D. (1967). The logical form of action sentences. In N. Rescher (Ed.), The
Logic of Decision and Action, Pittsburgh. University of Pittsburgh Press.
Davidson, D. (1980). Essays on actions and events. Oxford: Clarendon Press.
Gazdar, G. (1979). Pragmatics: Implicature, Presupposition and Logical Form. New
York: Academic Press.
Jech, T. J. (1971). Lectures in Set Theory, with Particular Emphasis on the Method of
Forcing. Berlin: Springer Verlag (Lecture Notes in Mathematics 217).
Everyday Language is Highly Intensional 205
Ramsay, A. M. (1992). Bare plural NPs and habitual VPs. In Proceedings of the 14th
International Conference on Computational Linguistics (COLING-92), Nantes, pp.
226?231.
Ramsay, A. M. (2001). Theorem proving for untyped constructive ?-calculus: imple-
mentation and application. Logic Journal of the Interest Group in Pure and Applied
Logics 9(1), 89?106.
Ramsay, A. M. and D. G. Field (2008). Speech acts, epistemic planning and Grice?s
maxims. Journal of Logic and Computation 18(3), 431?457.
Reichenbach, H. (1947). Elements of Symbolic Logic. New York: The Free Press.
Reichenbach, H. (1956). The Direction of Time. Berkeley: University of California
Press.
Turner, R. (1987). A theory of properties. Journal of Symbolic Logic 52(2), 455?472.
van der Sandt, R. (1992). Presupposition projection as anaphora resolution. Journal
of Semantics 9, 333?377.
Whitehead, A. N. and B. Russell (1925). Principia Mathematica. Cambridge: Cam-
bridge University Press.
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 47?50,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
?How was your day?? An architecture for multimodal ECA systems 
 
Ra?l Santos de la 
C?mara 
Telef?nica I+D 
C/ Emilio Vargas 6 
28043 Madrid, Spain 
e.rsai@tid.es 
Markku Turunen 
Univ. of Tampere 
Kanslerinrinne 1 
FI-33014, Finland 
mturunen@ 
cs.uta.fi 
Jaakko Hakulinen 
Univ. of Tampere 
Kanslerinrinne 1 
FI-33014, Finland 
jh@cs.uta.fi 
Debora Field 
Computer Science 
Univ. of  Sheffield 
S1 4DP, UK 
d.field@shef. 
ac.uk 
 
Abstract 
Multimodal conversational dialogue sys-
tems consisting of numerous software 
components create challenges for the un-
derlying software architecture and devel-
opment practices. Typically, such sys-
tems are built on separate, often pre-
existing components developed by dif-
ferent organizations and integrated in a 
highly iterative way. The traditional dia-
logue system pipeline is not flexible 
enough to address the needs of highly in-
teractive systems, which include parallel 
processing of multimodal input and out-
put. We present an architectural solution 
for a multimodal conversational social 
dialogue system. 
1 Introduction 
Multimodal conversational dialogue applica-
tions with embodied conversational agents 
(ECas) are complex software systems consisting 
of multiple software components. They require 
much of architectural solutions and development 
approaches compared to traditional spoken dia-
logue systems. These systems are mostly assem-
bled from separate, often pre-existing compo-
nents developed by different organizations. For 
such systems, the simple pipeline architecture is 
not a viable choice. When multimodal systems 
are built, software architecture should be flexible 
enough to enable the system to support natural 
interaction with features such as continuous and 
timely multimodal feedback and interruptions by 
both participants. Such features require parallel 
processing components and flexible communica-
tion between the components. Furthermore, the 
architecture should provide an open sandbox, 
where the components can be efficiently com-
bined and experimented with during the iterative 
development process. 
The HWYD (?How was your day??) Compan-
ion system is a multimodal virtual companion 
capable of affective social dialogue and for 
which we have developed a custom novel archi-
tecture. The application features an ECA which 
exhibits facial expressions and bodily move-
ments and gestures. The system is rendered on a 
HD screen with the avatar being presented as 
roughly life-size. The user converses with the 
ECA using a wireless microphone. A demonstra-
tion video of the virtual companion in action is 
available online1. 
The application is capable of long social con-
versations about events that take place during a 
user?s working day. The system monitors the 
user?s emotional state on acoustic and linguistic 
levels, generates affective spoken responses, and 
attempts to positively influence the user?s emo-
tional state. The system allows for user initiative, 
it asks questions, makes comments and sugges-
tions, gives warnings, and offers advice. 
2 Communications framework 
The HWYD Companion system architecture em-
ploys Inamode, a loosely coupled multi-hub 
framework which facilitates a loose, non-
hierarchical connection between any number of 
components. Every component in the system is 
connected to a repeating hub which broadcasts 
all messages sent to it to all connected compo-
nents. The hub and the components connected to 
it form a single domain. Facilitators are used to 
forward messages between different domains 
according to filtering rules. During development, 
we have experimented with a number of Facilita-
tors to create efficient and simple domains to 
overcome problems associated with single-hub 
systems. For example, multiple hubs allow the 
                                                
1
 http://www.youtube.com/ 
watch?v=BmDMNguQUmM 
47
reduction of broadcast messages, which is for 
example used in the audio processing pipeline, 
where a dedicated hub allows very rapid message 
broadcast (nearly 100 messages per second are 
exchanged) without compromising the stability 
of the system by flooding the common pipeline. 
For communication between components, a 
lightweight communication protocol is used to 
support components implemented in various 
programming languages. A common XML mes-
sage ?envelope? specifies the basic format of 
message headers as seen in Figure 1. 
 
Figure 1: System message XML format 
. 
Mandatory elements in the envelope (top 
block) are necessary so other modules can iden-
tify the purpose of the message and its contents 
upon a shallow inspection. These include the 
sender component and a unique message id. Ad-
ditional envelope fields elements include: mes-
sage type, turn id, dialogue segment identifier, 
recipient identifier, and a list of message identi-
fiers corresponding to the previous messages in 
the current processing sequence.  
For system-wide and persistent knowledge 
management, a central XML-database allows the 
system to have inter-session and intra-session 
?memory? of past events and dialogues. This da-
tabase (KB) includes information such the user 
and dialogue models, processing status of mod-
ules, and other system-wide information. 
3 Data flow in the architecture 
To maximize the naturalness of the ECA?s inter-
action, the system implements parallel process-
ing paths. It also makes use of a special module, 
the Interruption Manager (IM), to control 
components in situations where regular process-
ing procedure must be deviated from. In addi-
tion, there are ?long? and ?short? processing  se-
quences from user input to system output. Both 
?loops? operate simultaneously. The Main Dia-
logue (?long?) Loop, which is the normal proc-
essing path, is indicated by the bold arrows in 
Fig. 2, and includes all system components ex-
cept the IM. The dotted arrows signal the devia-
tions to this main path that are introduced by the  
Natural Language 
Understanding (NLU)
Acoustic
Analyzer (AA)
Autom
atic Speech
Recognition (ASR)
Acoustic Em
otion Classifier (AEC)
Sentiment
Analyzer (SA)
Dialogue Act 
Tagger (DAT)
Dialogue 
Manager(DM)
Affective Strategy 
(ASM
)
Multimodal Fission 
Manager (MFM)
Text-to-Speech
(TTS)
Avatar
(ECA)
Acoustic Turn-
Taking (ATT)
Interruption 
M
anager(IM
)
Emotional 
Model (EM)
Natural Language 
Generation (NLG)
Knowledge 
Base & UM
 
Figure 2:HWYD Companion main modules 
 
interruption management and feedback loops.  
The system has an activity detector in the input 
subsystem that is active permanently and analy-
ses user input in real-time. If there is a detection 
of user input at the same time as the ECA is talk-
ing, this module triggers a signal that is captured 
by the IM. The IM, which tracks the activity of 
the rest of the modules in the system, has a set of 
heuristics that are examined each time this trig-
gering signal is detected. If any heuristic 
matches, the system decides there has been a 
proper user interruption and decides upon a se-
ries of actions to recover from the interruption. 
4 Module Processing Procedure 
The first stage in the processing is the acoustic 
processing. User speech is processed by the 
Acoustic Analyzer, the Automatic Speech Rec-
ognizer, and the Acoustic Emotion Classifier 
simultaneously for maximum responsiveness. 
The Acoustic Analyzer (AA) extracts low-
level features (pitch, intensity and the probability 
that the input was from voiced speech) from the 
acoustic signal at frequent time intervals (typi-
cally 10 milliseconds). Features are passed to the 
Acoustic Turn-Taking Detector in larger buffers 
(a few hundred milliseconds) together with time-
stamps. AA is implemented in TCL using Snack 
toolkit (http://www.speech.kth.se/snack/). 
The Acoustic Turn-Taking detector (ATT) 
is a Java module, which estimates when the user 
has finished a turn by comparing intensity pause 
lengths and pitch information of user speech to 
configurable empirical thresholds. ATT also de-
cides whether the user has interrupted the system 
48
(?barge-in?), while ignoring shorter backchannel-
ling phrases (Crook et al (2010)). Interruption 
messages are passed to the Interruption Manager. 
ATT receives a message from the ECA module 
when the system starts or stops speaking. 
Dragon Naturally Speaking Automatic 
Speech Recognition (ASR) system is used to 
provide real-time large vocabulary speech recog-
nition. Per-user acoustic adaptation is used to 
improve recognition rates. ASR provides N-best 
lists, confidence scores, and phrase hypotheses. 
The Acoustic Emotion Classifier (AEC) 
component (EmoVoice (Vogt et al (2008)) cate-
gorizes segments of user speech into five va-
lence+arousal categories, also applying a confi-
dence score. The Interruption Manager monitors 
the messages of the AEC to include emotion-
related information into feedback loop messages 
sent to the ECA subsystem. This allows rapid 
reactions to the user mood. 
The Sentiment Analyzer (SA) labels ASR 
output strings with sentiment information at 
word and sentence levels using valence catego-
ries positive, neutral and negative. The SA uses 
the AFFECTiS Sentiment Server, which is a gen-
eral purpose .NET SOAP XML service for 
analysis and scoring of author sentiment. 
The Emotional Model (EM), written in Lisp, 
fuses information from the AEC and SA. It 
stores a globally accessible emotional representa-
tion of the user for other system modules to 
make use of. Affective fusion is rule-based, pre-
fers the SA?s valence information, and outputs 
the same five valence+arousal categories as used 
in the AEC. The EM can also serve as a basis for 
temporal integration (mood representation) as 
part of the affective content of the User Model. It 
also combines the potentially different segmenta-
tions by the ASR and AEC. 
The User Model (UM) stores facts about the 
user as objects and associated attributes. The in-
formation contained in the User Model is used by 
other system modules, in particular by Dialogue 
Manager and Affective Strategy Module. 
The Dialogue Act Tagger and Segmenter 
(DAT), written in C under Linux, uses the ATT 
results to compile all ASR results corresponding 
to each user turn. DAT then segments the com-
bined results into semantic units and labels each 
with a dialogue act (DA) tag (from a subset of 
SWBD-DAMSL (Jurafsky et al (2001)). A Sto-
chastic Machine Learning model combining 
Hidden Markov Model (HMM) and N-grams is 
used in a manner analogous to Mart?nez-
Hinarejos et al (2006). The N-grams yield the 
probability of a possible DA tag given the previ-
ous ones. The Viterbi algorithm is used to find 
the most likely sequence of DA tags.  
The Natural Language Understanding 
(NLU) component, implemented in Prolog, pro-
duces a logical form representing the semantic 
meaning of a user turn. The NLU consists of a 
part-of-speech tagger, a Noun Phrase and Verb 
Group chunker, a named-entity classification 
component (rule-based), and a set of pattern-
matching rules which recognize major gram-
matical relationships (subject, direct object, etc.) 
The resulting shallow-parsed text is further proc-
essed using pattern-matching rules. These recog-
nize configurations of entity and relation relevant 
to the templates needed by the Dialogue Man-
ager, the EM, and the Affective Strategy Module. 
The Dialogue Manager (DM), written in Java 
and Prolog, combines the SA and NLU results, 
decides on the system's next utterance and identi-
fies salient objects for the Affective Strategy 
Module. The DM maintains an information state 
containing information about concepts under dis-
cussion, as well as the system's agenda of current 
conversational goals.  
One of the main features of the HWYD Com-
panion is its ability to positively influence the 
user?s mood through its Affective Strategy 
Module (ASM). This module appraises the 
user?s situation, considering the events reported 
in the user turn and its (bi-modal) affective ele-
ments. From this appraisal, the ASM generates a 
long multi-utterance turn. Each utterance imple-
ments communicative acts constitutive of the 
strategy. ASM generates influence operators 
which are passed to the Natural Language Gen-
eration module. ASM output is triggered when 
the system has learned enough about a particular 
event to warrant affective influence. As input, 
ASM takes information extraction templates de-
scribing events, together with the emotional data 
attached. ASM is a Hierarchical Task Network 
(HTN) Planner implemented in Lisp.  
The Natural Language Generator (NLG), 
written in Lisp, produces linguistic surface forms 
from influence operators produced by the ASM. 
These operators correspond to communicative 
actions taking the form of performatives. NLG 
uses specific rhetorical structures and constructs 
associated with humour, and uses emotional TTS 
expressions through specific lexical choice.  
49
5 Multimodal ECA Control 
Multimodal control of the ECA, which consists 
of a tightly-synchronized naturalistic avatar and 
affective Text-To-Speech (TTS) generation, is 
highly challenging from an architectural view-
point, since the coordinating component needs to 
be properly synchronized with the rest of the sys-
tem, including both the main dialogue loop and 
the feedback and interruption loops. 
The system Avatar is in charge of generating a 
three-dimensional, human-like character to serve 
as the system?s ?face?. The avatar is connected to 
the TTS, and the speech is synchronized with the 
lip movements. The prototype is currently using 
the HaptekTM 3D avatar engine running inside a 
web browser. The Haptek engine provides a talk-
ing head and torso along with a low level API to 
control its interaction with any SAPI-compliant 
TTS subsystem, and also allows some manipula-
tion of the character animation. An intermediate 
layer consisting of a Java applet and Javascript 
code embeds the rendered avatar in a web page 
and provides connectivity with the Multimodal 
Fission Manager. We intend to replace the cur-
rent avatar with a photorealistic avatar under de-
velopment within the project consortium. 
LoquendoTM TTS SAPI synthesizer is used to 
vocalize system turns. The TTS engine works in 
close connection with the ECA software using 
the SAPI interface. TTS includes custom para-
linguistic events for producing expressive 
speech. TTS is based on the concatenative tech-
nique with variable length acoustic units. 
The Multimodal Fission Manager (MFM) con-
trols the Avatar and the TTS engine, enabling the 
system to construct complex communicative acts 
that chain together series of utterances and ges-
tures. It offers FML-standard-based syntax to 
make the avatar perform a series of body and 
facial gestures. 
The system features a template-based input 
mode in which a module can call ECA to per-
form actions without having to build a full FML-
based XML message. This is intended to be used 
in the feedback loops, for example, to convey the 
impression that the ECA is paying attention.  
6 Conclusions 
We have presented an advanced multimodal dia-
logue system that challenges the usual pipeline-
based implementation. To do so, it leverages on 
an architecture that provides the means for a 
flexible component interconnection, that can ac-
comodate the needs of a system using more than 
one processing path for its data. We have shown 
how this has enabled us to implement complex 
behavior such as interrupt and short loop han-
dling. We are currently expanding coverage and 
will carry out an evaluation with real users this 
September. 
Acknowledgements 
This work was funded by Companions, a Euro-
pean Commission Sixth Framework Programme 
Information Society Technologies Integrated 
Project (IST-34434). 
References 
Vogt, T., Andr?, E. and Bee, N. 2008. EmoVoice ? A 
framework for online recognition of emotions from 
voice. In: Proc. Workshop on Perception and In-
teractive Technologies for Speech-Based Systems, 
Springer, Kloster Irsee, Germany. 
Cavazza, M., Smith, C., Charlton, D., Crook, N., 
Boye, J., Pulman, S., Moilanen, K., Pizzi, D., San-
tos de la Camara, R., Turunen, M. 2010 Persuasive 
Dialogue based on a Narrative Theory: an ECA 
Implementation, Proc. 5th Int. Conf. on Persuasive 
Technology (to appear). 
Hern?ndez, A., L?pez, B., Pardo, D., Santos, R., 
Hern?ndez, L., Rela?o Gil, J. and Rodr?guez, M.C. 
2008 Modular definition of multimodal ECA 
communication acts to improve dialogue robust-
ness and depth of intention. In: Heylen, D., Kopp, 
S., Marsella, S., Pelachaud, C., and Vilhj?lmsson, 
H. (Eds.), AAMAS 2008 Workshop on Functional 
Markup Language. 
Crook, N., Smith, C., Cavazza, M., Pulman, S., 
Moore, R., and Boye, J. 2010 Handling User Inter-
ruptions in an Embodied Conversational Agent.  In 
Proc. AAMAS 2010. 
Wagner J., Andr?, E., and Jung, F. 2009 Smart sensor 
integration: A framework for multimodal emotion 
recognition in real-time. In Affective Computing 
and Intelligent Interaction 2009. 
Cavazza, M., Pizzi, D., Charles, F., Vogt, T. Andr?, 
E. 2009 Emotional input for character?based in-
teractive storytelling AAMAS (1) 2009: 313-320. 
Jurafsky, D. Shriberg, E., Biasca, D. 2001 
Switchboard swbd?damsl shallow? discourse?
function annotation coders manual. Tech. Rep. 97
?01, University of Colorado Institute of Cognitive 
Science 
Mart?nez?Hinarejos, C.D., Granell, R., Bened?, J.M. 
2006. Segmented and unsegmented dialogue?act 
annotation with statistical dialogue models. Proc. 
COLING/ACL Sydney, Australia, pp. 563?570. 
50
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 277?280,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
?How was your day?? An affective companion ECA prototype 
 
Marc Cavazza 
School of Computing 
Teesside University 
Middlesbrough TS1 3BA 
m.o.cavazza@tees.ac.uk 
Ra?l Santos de la C?mara 
Telef?nica I+D 
C/ Emilio Vargas 6 
28043 Madrid 
e.rsai@tid.es 
Markku Turunen 
University of Tampere 
Kanslerinrinne 1 
FI-33014 
mturunen@cs.uta.fi 
 
 
Jos? Rela?o Gil 
Telef?nica I+D 
C/ Emilio Vargas 6 
28043 Madrid 
joserg@tid.es 
Jaakko Hakulinen 
University of Tampere 
Kanslerinrinne 1 
FI-33014 
jh@cs.uta.fi 
Nigel Crook 
Oxford University 
Computing Laboratory 
Oxford OX1 3QD 
nigc@comlab.ox.
ac.uk 
Debora Field 
Computer Science 
Sheffield University 
Sheffield S1 4DP 
d.field@shef. 
ac.uk 
 
Abstract 
This paper presents a dialogue system in 
the form of an ECA that acts as a socia-
ble and emotionally intelligent compan-
ion for the user. The system dialogue is 
not task-driven but is social conversation 
in which the user talks about his/her day 
at the office. During conversations the 
system monitors the emotional state of 
the user and uses that information to in-
form its dialogue turns. The system is 
able to respond to spoken interruptions 
by the user, for example, the user can in-
terrupt to correct the system. The system 
is already fully implemented and aspects 
of actual output will be used to illustrate. 
1 Introduction 
Historically, Embodied Conversational Agents 
(ECAs) have been used in research and industry 
make information and complex tasks more ac-
cessible to customers and users. With the rise of 
new technologies in affective dialogue systems, 
we are beginning to see a future in which ECA 
dialogues are not all task-driven, but some will 
be focused on the social aspects of conversation. 
We envisage the development of ECAs that en-
hance the social quality of life of the user, and 
that build deep relationships with their users over 
years of use. Our affective companion ECA is 
our first contribution to this emerging technol-
ogy.  
Our emotionally intelligent dialogue system 
requires a high level of understanding of the 
meaning of the user turns, and so is quite unlike 
a ?chatbot?. Affective aspects of the situation are 
also key in influencing the course of the conver-
sation. The system attempts to balance both se-
mantic and affective understanding in order to 
create an engaging and enjoyable companion for 
social conversation. 
2 Scenario description 
The system conversation is free-ranging and fol-
lows the user?s initiative, rather than being task-
driven. We have for the time being restricted the 
range of conversation topics to things that hap-
pen in the life of someone who works in an of-
fice (the ?How was your day?? (HWYD) sce-
nario). The user ?offloads? about his/her day at 
work by chatting to the ECA, and the ECA tries 
to empathise with and advise the user. The ECA 
asks questions to elicit information from the user, 
makes empathetic and sympathetic comments, 
and offers advice.  
The system is able to process long user turns, 
as well as to generate long system turns (around 
50 words). If the user perhaps dislikes what 
he/she hears, he/she can interrupt the system by 
barging in, and the ECA will respond in a hu-
man-like way. The conversation style is therefore 
considerably different from traditional task-based 
dialogues that consist of short dialogue turns and 
in which the system takes all the initiative.  
The dialogue in Table 1 (actual output from a 
user session) illustrates some of the significant 
features of the system, including long system 
277
turns, user interruptions and emotional analysis 
of user input.  
 
User1 Worst day ever. I arrived late be-
cause of the traffic. 
System1 You have my sympathy. What 
happened next? 
User2 I missed the meeting on project 
responsibilities; got more work 
dumped on me. Just what I don?t 
need. I?m already likely to miss 
the deadline. [ECA looks worried 
when user?s negative tone is de-
tected] 
System2 Sorry to hear that. Who was at the 
meeting? 
User3 Only my team. They gave me a 
set of PowerPoint slides from the 
meeting. I spent the rest of the day 
reading them but my computer 
kept crashing because of  updates. 
System3 Oh dear! It is understandable how 
you must be feeling. It?s right to 
be unhappy about such a bad situ-
ation. Try not to worry about 
missing the deadline because of 
the technical problems. [At this 
point user interrupts ECA by 
beginning turn User4] It?s sad that 
your motivation could be affected 
by them. [System stops speaking] 
User4 No, I?m not worried about the 
technical problems, I?m worried 
about the amount of extra work on 
this project and the deadline. 
System4 What is the project about? 
 
Table 1: Dialogue example 
 
A video demonstration of the system in action 
is available at: 
http://www.youtube.com/watch?v=
BmDMNguQUmM 
3 Architecture 
Figure 1 shows a screen shot taken at run-time of 
actual system output. The ECA is represented on 
a screen as a woman (waist up) who displays 
natural, human-like movements and performs a 
wide range of complex facial expressions, bodily 
movements, and hand and arm gestures. 
The screen also displays a transcript of the 
user and system turns. The user turns shown con-
stitute the output of the Automatic Speech Rec-
ogniser (ASR). The system?s analysis of the 
user?s emotional state is also shown. 
The right-most panel of the screen shows 
graphics which convey real-time information 
about how the dialogue is being processed. It 
presents a streamlined view of the software 
modules that comprise the system. Module activ-
ity is visually represented at run-time by flashing 
colours. This ?glass-box? approach enables de-
tailed observation and analysis of system 
procedure at run-time. 
The system comprises a number of distinct 
modules that are connected using Inamode, a 
hub-based message-passing framework using 
XML formatted messages over plain text sock-
ets. 
The system?s ASR is the NuanceTM dictation 
engine. This is run in parallel with our own a-
coustic analysis pipeline which extracts low level 
(pitch, tone) speech features and also high-level 
features such as emotional characteristics. 
Analysis of the emotions is currently carried out 
 
Figure 1: Screenshot of the prototype interface 
 
278
by EmoVoice (Vogt et al (2008)). The ASR 
output strings are analysed for sentiment by the 
AFFECTiS system (Moilanen and Pulman (2007, 
2009)) and classed as positive, neutral, or nega-
tive. This output is fused with the output from 
EmoVoice to generate a value that represents the 
user?s current emotional state, which is ex-
pressed as a valence+arousal pairing (with five 
possible values). 
The ASR output goes to our own Natural Lan-
guage Understanding (NLU) module which per-
forms syntactic and semantic analysis of user 
utterances and derives noun phrases and verb 
groups and associated arguments. Events rele-
vant to the scenario (e.g., promotions, redundan-
cies, meetings, arguments, etc.) are recognised 
by the NLU and are used to populate an ontology 
(a model of the conversation content).  The sys-
tem is currently able to recognize and respond to 
more than 30 event types.  
The events recognised in a user turn are 
labelled with the output of the Emotion Module 
for that turn; the result is a representation of both 
the semantic and affective information that the 
user might be trying to convey. 
Our own rule-based Dialogue Manager (DM) 
takes the affect-annotated semantic output of the 
NLU, and from that and its model of the conver-
sation content determines the next system turn. It 
will either ask a question about the events that 
occurred in the user?s day, express an opinion on 
the events already described, or make empathetic 
comments. Whenever the system has gained suf-
ficient understanding of a key event in the user?s 
day, it generates a complex long turn that encap-
sulates comfort, opinion, warnings and advice to 
the user. 
These long system turns are generated by our 
own plan-based Affective Strategy Module that 
makes an appraisal of the user?s situation and  
generates an appropriate emotional strategy 
(Cavazza et al (2010)). This strategy?expressed 
as an abstract, conceptual representation?is han-
ded to our own Natural Language Generator 
(NLG) that maps it into a series of linguistic sur-
face forms (usually 4 or 5 sentences). We use a 
style-controllable system using Tree-Furcating 
Grammars (an extension of the Tree-Adjoining 
Grammars formalism (Joshi et al (1997)). This 
ensures the generation of a large set of different 
surface forms from the same semantic input. 
The output of the NLG is passed to a module 
that adds this information to its system turn 
instructions for the ECA. The ECA has been de-
veloped around the HaptekTM toolkit and is con-
trolled using an FML-like language (after 
Hern?ndez et al (2008)). This 2-D embodiment 
produces gestures, facial expressions, and body 
movements that convey the emotional state of 
the ECA. Its movements and expressions enable 
it to visually display interest and enjoyment in 
talking to the user, and to display empathy with 
the user. The speech synthesis module is our own 
emotion-focused extension of the LoquendoTM 
TTS system. It includes paralinguistic elements 
such as exclamations and laughter, and emo-
tional prosody generation for negative and posi-
tive utterances. 
4 Special procedural features 
A significant processing design feature of the 
system is that there are two main processing 
loops from user input to system output; a ?long 
loop? which passes through all the components 
of the system; and a ?short loop? or ?feedback 
loop? which will now be discussed (the proce-
dure already described in Section 3 is the long 
loop procedure). 
4.1 Feedback loop 
The feedback loop (?short loop?) bypasses many 
linguistic components and generates immediate 
reactions to user activity. The main function of 
the short loop is maintain user engagement by 
preventing unnaturally long gaps of ECA inactiv-
ity. The feedback loop engages the acoustic 
analysis components, the TTS, and the ECA. It is 
responsible for the generation of real-time (< 500 
ms) reactions in the ECA in response to the emo-
tional state of the user. It attempts to align  both 
verbal behaviour (backchannelling) and non-
verbal behaviour (facial expressions, gestures, 
and general body language) to the emotions de-
tected during most recent user turn. In order to 
achieve a reasonable level of realism, these sys-
tem reactions to the perceived emotional state of 
the user need to be perceptibly instantaneous. 
Using this short feedback loop that bypasses 
many of the linguistic components ensures this. 
The feedback loop is also occasionally used to 
make sympathetic comments immediately after 
the user stops speaking. These act as acknowl-
edgements of the emotion expressed by the user. 
An example can be seen in the System2 turn of 
the example dialogue in Table 1: 
1.?Sorry to hear that. Who was at the meeting?? 
Here, the first utterance was spoken by the sys-
tem within a few tenths of a second after the end 
279
of the previous user turn (User2). The system 
tried to identify the user?s emotion in the previ-
ous turn and then to behave linguistically and 
visually in an empathetic way. The actual sympa-
thetic utterance was randomly chosen from a set 
of ?negative emotion utterances? (there are also 
?positive? and ?neutral? sets).  
The second half of the system turn in (1) was 
derived by the system?s ?long loop?. It is a ques-
tion which refers to a meeting that the user men-
tioned in the previous turn. This ?meeting? event 
has been heard by the ASR, understood by the 
NLU system, remembered by the DM, and is 
now referred to by an appropriate definite noun 
phrase in the output of the NLG.   
The feedback and main loops run in parallel. 
However, the feedback loop generates its speech 
output almost immediately, giving time for the 
main dialogue loop to complete its more detailed 
analysis of the user?s utterance.  
4.2 Handling user interruptions 
This system has a complex strategy for handling 
situations in which the user interrupts long 
system turns.  The system?s response to ?barge-
in? user interruptions is overseen by the Interrup-
tion Manager (IM), which is alerted by the 
acoustic input modules whenever a genuine user 
interruption (as opposed to, say, a backchannel) 
is detected during a long system utterance. When 
alerted, the IM instructs the ECA to stop speak-
ing when it reaches a natural stopping point in its 
current turn (usually the end of the current 
phrase). The user?s interruption utterance is 
processed by the long loop. Its progress is 
tracked and controlled by the IM, for example, it 
makes sure that the linguistic modules know that 
the current utterance is an interruption, whic 
means it requires special treatment. The DM has 
a range of strategies for system recoveries from 
user interruptions, including different ways of 
continuing, replanning, and aborting. An exam-
ple of a user interruption is shown in Table 1. 
The user interrupts the long system utterance in 
the System3 turn. The system?s response to the 
interruption is to stop the speech output from the 
ECA, abort the long system turn altogether, and 
instead to ask for more details about the project 
that the user has just mentioned during the inter-
ruption. (See (Crook et al (2010))  for a more 
detailed description of the IM.) 
 
 
Acknowledgements 
This work was funded by Companions, a Eu-
ropean Commission Sixth Framework Pro-
gramme Information Society Technologies Inte-
grated Project (IST-34434).  
We would also like to thank the following 
people for their valuable contributions to the 
work presented here: Stephen Pulman, Ramon 
Granell, and Simon Dobnick (Oxford Univer-
sity), Johan Boye (KTH Stockholm), Cameron 
Smith and Daniel Charlton (Teesside Univer-
sity), Roger Moore, WeiWei Cheng and Lei Ye 
(University of Sheffield), Morena Danieli and 
Enrico Zovato (Loquendo). 
References 
Cavazza, M., Smith, C., Charlton, D., Crook, N., 
Boye, J., Pulman, S., Moilanen, K., Pizzi, D., San-
tos de la Camara, R., Turunen, M. 2010 Persuasive 
Dialogue based on a Narrative Theory: an ECA 
Implementation, Proc. of the 5th Int. Conf. on Per-
suasive Technology (Persuasive 2010), to appear 
2010. 
Crook, N., Smith, C., Cavazza, M., Pulman, S., 
Moore, R., and Boye, J. 2010 Handling User Inter-
ruptions in an Embodied Conversational Agent In 
proc. of AAMAS 2010. 
Hern?ndez, A., L?pez, B., Pardo, D., Santos, R., 
Hern?ndez, L., Rela?o Gil, J. and Rodr?guez, M.C. 
(2008) Modular definition of multimodal ECA 
communication acts to improve dialogue robust-
ness and depth of intention. In: Heylen, D., Kopp, 
S., Marsella, S., Pelachaud, C., and Vilhj?lmsson, 
H. (Eds.), AAMAS 2008 Workshop on Functional 
Markup Language.  
Joshi, A.K. & Schabes, Y. (1997) Tree-adjoining 
Grammars. Handbook of formal languages, vol. 3: 
Beyond Words, Springer-Verlag New York, Inc., 
New York, NY, 1997. 
Moilanen, K. and Pulman. S. (2009). Multi-entity 
Sentiment Scoring. Proc. Recent Advances in 
Natural Language Processing (RANLP 2009). 
September 14-16, Borovets, Bulgaria. pp. 258--
263.  
Moilanen, K. and Pulman. S. (2007). Sentiment Com-
position. Proc. Recent Advances in Natural Lan-
guage Processing (RANLP 2007). September 27-
29, Borovets, Bulgaria. pp. 378--382. 
Vogt, T., Andr?, E. and Bee, N. 2008. EmoVoice ? A 
framework for online recognition of emotions 
from voice. Proc. Workshop on Perception and 
Interactive Technologies for Speech-Based Sys-
tems, Springer, Kloster Irsee, Germany, (June 
2008). 
280
Proceedings of the Ninth Workshop on Innovative Use of NLP for Building Educational Applications , pages 43?53,
Baltimore, Maryland USA, June 26, 2014.
c
?2014 Association for Computational Linguistics
The pragmatics of margin comments: An empirical study
Debora Field, Stephen Pulman
Dept Computer Science
University of Oxford
Oxford OX1 3QD, UK
firstname.lastname@cs.ox.ac.uk
Denise Whitelock
Institute of Educational Technology
The Open University
Milton Keynes MK7 6AA, UK
denise.whitelock@open.ac.uk
Abstract
This paper describes the design and ratio-
nale behind a classification scheme for En-
glish margin comments. The scheme?s de-
sign was informed by pragmatics and ped-
agogy theory, and by observations made
from a corpus of 24,387 margin comments
from assessed university assignments. The
purpose of the scheme is to computation-
ally explore content and form relationships
between margin comments and the pas-
sages to which they point. The process
of designing the scheme resulted in the
conclusion that margin comments require
more work to understand than utterances
do, and that they are more prone to being
misunderstood.
1 Introduction
We have a collection of 24,387 real margin com-
ments, expressed in English, which we want to ex-
ploit through machine learning in order to inform
the design of an automatic margin comments gen-
erator. The corpus margin comments were added
by humans to a corpus of real assessed university
assignments. The assignments were argumenta-
tive essays submitted towards a Master?s degree in
Education.
We have designed a margin comment classifi-
cation scheme which classifies natural language
(NL) margin comments without reference to the
essay parts to which they point. High inter-
annotator agreement scores have been achieved for
the scheme. We plan to use the scheme to look for
relationships between the corpus comments and
the essay parts to which they point.
This paper is about the classification scheme?s
design, including what led to the design decisions,
which were informed by examination of the mar-
gin comments, the assignments corpus, and con-
sideration of key ideas in pragmatics and peda-
gogy. A feature of margin comments that be-
came clear during the design process, and that in-
fluenced the design, is that margin comments are
harder to understand and are more prone to being
misunderstood than conversational utterances.
2 What are the corpus comments like?
The design of the classification scheme is based on
answers we sought to three core questions:
- What are the margin comments like?
- What are they ?doing??
- How do they get their messages across?
A margin comment is a message written or
typed by an assessor and positioned in the ?mar-
gin? of a piece of text produced by a learner. Most
margin comments graphically point to a part of the
learner text, and the message content of a margin
comment typically concerns the text part to which
the comment points. The margin comments in our
corpus had been added to word-processed assign-
ments using a digital commenting tool.
To gain a first impression of what the corpus
margin comments were like, we carried out some
frequency counts and from these derived a set of
simple pattern-matching rules for clustering sim-
ilar comments?143 complex regular expressions
to match the start of a comment. Most of the rules
invoked one or more of 13 regex groups. Each
group was a disjunction of strings (e.g., 29 ?nega-
tive? verb disjuncts). Each comment was typed on
the basis of its first sentence only, on the grounds
that any subsequent sentences were most likely
elaborations on the first (based on manual scrutiny
of hundreds of comments.) Probable comment-
initial filler words were skipped. The clustering
rules assigned a type to 90.9% of the comments.
The following subsections describe some of the re-
sults.
43
2.1 Positive-sounding
Expressions that are positive-sounding in general
(e.g., ?good?, freq. 5,177) and positive with respect
to essay writing (e.g., ?interesting?, freq. 954)
were very common.
1
There were 9,272 occur-
rences of a positive-sounding adjective. In con-
trast, there were 551 occurrences of a negative-
sounding adjective, the top 3 being ?difficult?
(freq. 133), ?missing? (123), ?informal? (90). A
large proportion of positive-sounding comments
were descriptions. For example, 3,151 comments
(12.9%) began with ?good?.
2.2 Missing, unnecessary, or inappropriate
3,351 comments expressed the idea that something
was missing from the essay that marker M thought
should have been present (1a). 574 comments ex-
pressed the idea that something was present in the
essay that M thought should not have been (1b).
2,069 comments expressed the idea that something
that was present in the essay that M thought should
have been different in some way (1c).
2
(1) a. Could you have developed this?
b. I would not leave a space.
c. Another long quote
2.3 Confusion and apparent uncertainty
1,119 comments expressed confusion or appar-
ent uncertainty. Many confusion expressions con-
cerned M?s understanding. There were 1,232
expressions concerned with comprehensibility.
Many uncertainty expressions concerned M?s
agreement or understanding. There were 1,193 ex-
pressions concerned with agreement.
2.4 Questions
4,307 comments (17.6%) ended in a question mark
and 1,109 comments began with a WH question
word. 1,119 comments were polar questions.
2.5 Parts of instructions
6,169 expressions looked like parts of instructions
or polite suggestions, the top 3 being ?you might?
(freq. 882), ?you need? (693) and ?explain? (332).
2.6 Adversative conjunctions
There were 2,237 occurrences of ?but?, 283 of ?al-
though?, 127 of ?however?, typically used in the
corpus to present contrasting or opposing opinion.
1
All quoted example terms are case-insensitive.
2
All examples in the paper are real, whole comments from
the corpus, apart from examples that are prefixed with a ? ? ?,
which are interpretations. Punctuation, spelling, capitalisa-
tion, etc.in the examples are faithfully reproduced.
2.7 Non-sentential
The distribution of comment lengths is heavily
skewed towards short comments (Figure 1).
3
Just
under 9.5 % of comments have 11 characters or
fewer. The top 3 most frequent comment lengths
were 10 characters (freq. 430), 4 characters (freq.
358) and 1 character (freq. 316).
Scrutiny of many short comments revealed that
non-sentential comments are the main reason for
the brevity. These include elliptical comments
(2a), fragments (2b), and other non-sentential ex-
pressions such as exclamations (2c) and short di-
rectives (Klein, 1985; Merchant, 2004) (2d).
(2) a. Why not?
b. Good point
c. What a good idea.
d. Reference
Very short corpus comments that are complete
sentences are rare (set 3).
(3) a. Avoid jargon
b. This is unclear.
2.8 Politeness
There are 3,996 occurrences of terms typically
used to soften the impact of a criticism or make an
instruction sound like a suggestion (hereon ?soft-
eners?), including ?perhaps? (freq. 863), ?rather?
(422), and ?a little? (381). There are also 7,287 oc-
currences of conditional auxiliary verbs (including
many non-modal uses of ?would?), which are typ-
ically used to make polite suggestions.
2.9 Informality
There are 3,818 contractions, including ?don?t?
(freq. 568), ?I?m? (370), ?you?re? (138). Filler
words were also common. 444 comments began
with ?ok? (a range of spellings), and 1109 com-
ments began with ?yes? (some of these express
agreement, but most are fillers).
2.10 Skills
We noticed 4 large groups of terms relating to par-
ticular skills. Table 1 shows each group, the num-
ber of occurrences of terms from that group, an
example term from that group, and the number of
occurrences of the example. Category ?presenta-
tion? includes matters relating to the presentation
of English, such as spelling, grammar, formatting,
and style.
3
The inset in Figure 1 is the main figure presented on log-
log scale axes.
44
Figure 1: Distribution of comment lengths
Grouping Freq. Example Freq.
Argument 14705 ?argument? 817
Referencing 6657 ?reference? 1322
Essay structure 5243 ?section? 614
Presentation 2613 ?sentence? 428
Table 1: Skills-related terms in comments corpus
2.11 Are margin comments conversation?
The corpus investigations revealed frequent use of
phenomena common in speech: non-sentential ex-
pressions, contractions, politeness devices, soften-
ers, and fillers. This led us to consider whether a
dialogue act taxonomy such as DIT (Bunt, 1990)
or DAMSL (Core and Allen, 1997) might be suit-
able for typing margin comments.
Many pedagogy papers have argued or assumed
that margin comments are or are like a conversa-
tion. Straub (1996) reviewed a number of contem-
porary papers, including (Ziv, 1984; Danis, 1987;
Lindemann, 1987; Anson, 1989) to explore the
question: ?what does it mean to treat teacher com-
mentary as a dialogue?? (Straub, 1996, p. 375).
Straub concluded that margin comments are not
conversational utterances, either real or imaginary,
and that what pedagogy scholars were referring
to was the informal style of comments. Infor-
mal language was becoming popular as a result
of the movement away from ?teaching product? to
?teaching process?, which encouraged the expres-
sion of empathy with the learner, because it was
thought this would make teacher comments more
likely to be read and acted upon (Hairston, 1982).
From linguistics, Schegloff (1999), tackles the
problem of whether there is such a thing as ?or-
dinary conversation?. He first defines ?talk-in-
interaction? (p. 406), which includes speech spo-
ken with the intention of communicating mes-
sages to some audience. Next Schegloff talks
about ?speech exchange systems? (citing (Sacks et
al., 1974)), which are ?organizational formats for
talk-in-interaction? (Schegloff, 1999, p. 407), in-
cluding the lecture format, classroom discourse,
courts-in-session, meetings, debates, etc. Margin
comments arguably qualify as a speech exchange
system, even though they are written, not spo-
ken. But Schegloff?s definition of ?ordinary con-
versation? arguably excludes margin comments on
the grounds that they don?t involve ?generic as-
pects of talking-in-interaction such as turn-taking,
sequence organization, repair organization, over-
all structural organization? (p. 413), and on the
grounds that they are ?subject to functionally spe-
cific or context-specific restrictions? (p. 407).
Having considered relevant literature, we con-
cluded that margin comments are not conversa-
tion, principally on the grounds that there is no
turn taking?only the marker M gets the opportu-
nity to ?speak? and only the comment?s addressee
A gets the opportunity to ?hear?. Whilst there
is common ground (Stalnaker, 1972; Thomason,
1990) and accommodation (Clark and Haviland,
1974; Lewis, 1979; Kamp, 1981) there is no
turn taking and therefore no grounding (Clark and
Schaefer, 1989). M presents utterances, and the
constraints of the context demand that A must ac-
cept the evidence. Consequently, if A misunder-
stands M?s intended message, there is no mecha-
nism to enable M or A to discover that A has mis-
45
understood the message; and if A is confused by
M?s comment, there is no opportunity for A to ask
M for clarification.
We concluded that dialogue acts were an inap-
propriate classification scheme for margin com-
ments, because the conditions for human-to-
human dialogue do not apply.
3 What are margin comments ?doing??
If dialogue acts are inappropriate, what kinds of
things are NL margin comments ?doing?? Con-
sider WH questions (4).
(4) Why bold?
When M asks a WH question in a margin com-
ment, M is not desiring or expecting A to supply
the requested information to M. The Addressee A
of a NL margin comment will never take a turn in
response to that comment. This is something of
which A and marker M are both mutually aware
before the comments are written by M, and it has
important repercussions with respect to M?s inten-
tions. Consider also imperatives (5).
(5) Explain what they do.
5 looks like an instruction, but cannot be. The
corpus comments were added to the final, submit-
ted versions of assessed assignments. There was
no desire or expectation on M?s part that A would
revise the essay in response to M?s comments.
M must have been desiring something by these
comments (otherwise there would be no com-
ments), but that something is not what one might
expect given their linguistic surface forms. This
suggests that margin comments are like indi-
rect speech acts (Searle, 1969; Searle and Van-
derveken, 1985)?acts which have an apparent
function that is distinct from what the comment
is really ?doing? (Austin, 1962). We would ar-
gue that, for the evaluative comments in the cor-
pus (which are the vast majority), the thing the
comments are doing is this: to communicate M?s
opinion to A about the essay part to which the
comment pointed.
This conclusion is not surprising. NL margin
comments are doing what all margin comments
are doing, it seems, including non-NL coded com-
ment schemes. Why this conclusion seems sur-
prising is that margin comments do not look
like expressions of opinion about weaknesses and
strengths. Instead they look like excerpts from
friendly, informal conversations. The informality
is, however, masking the principal messages of the
comments, which are evaluative ones.
4 How do NL margin comments express
whether the essay met the standard?
Having decided what NL margin comments are
doing, we reasoned that M?s opinion expressed by
a comment must have two aspects, on the grounds
that they do not just point to essay parts, they con-
tain messages. The two aspects are: (1) Whether
or not essay part P to which a comment points
attained the required standard; (2) How P at-
tained (or did not attain) the required stan-
dard. The required standard is a standard defined
by some set of principles or instructions of which
M and A are typically mutually aware.
We observed that the semantics of very few
corpus comments communicated a message ap-
proaching ?This essay part has failed to achieved
the agreed standard?. Set 6 shows two of them.
(6) a. Something?s wrong or missing here. . .
b. Two line sentences is not enough to get
the maximum 30% marks for this sec-
tion
For the vast majority of comments, whether
essay part P attained the required standard
was communicated implicitly by the use of cer-
tain types of words and syntactic structures.
To convey attainment or surpassing of the stan-
dard, positive-sounding adjectives were used ex-
tensively (section 2), also positive-sounding ad-
verbs, and terms of liking, agreement, and un-
derstanding. A much wider variety of techniques
was used to convey failure to attain the standard,
including negative-sounding verbs (e.g., ?contra-
dict?), negative-sounding adjectives (e.g., ?inap-
propriate?), lone noun phrases (e.g., ?brackets?),
questions, instructions, polite suggestions, notifi-
cations of marker edits, referrals to authoritative
sources, and assertions of uncertainty, confusion,
doubt, disagreement, and non-understanding.
Addressee A?s understanding of whether essay
part P had attained the required standard would
therefore have depended on A?s being able to cor-
rectly interpret the semantics of the comment. For
non-native speakers of English, this may have pre-
sented a problem.
4
Since many corpus comments
4
The corpus assignments were towards a distance-
learning degree course, and many of the students are likely
to have been non-native speakers of English.
46
constitute a lone modified noun phrase, and since
the meanings of everyday adjectives change de-
pending on what they are modifying, it may have
been difficult for A to tell whether a comment was
a criticism or a commendation (set 7).
(7) a. A very long sentence.
b. Very strong supporting quote.
c. A strong argument
d. A big assumption
Note that the way we decide whether these are
criticisms or commendations is by considering the
type of entity the adjective is modifying. We know
quotes should be strong, so 7b must be a commen-
dation. We know assumptions should not be big,
so 7d must be a criticism. This means that, in ad-
dition to having a sensitivity to compositional se-
mantics, the addressees of these comments would
have needed to possess expert knowledge about
what sentences, quotes, arguments, and assump-
tions should be like in order to be able to infer
whether the essay part had met the standard.
Difficulties in understanding whether an essay
part has met the standard are also caused by the
use of non-sentential expressions (set 8).
(8) a. Reference
b. Colloquialism
c. No issues
d. No comma
e. No apostrophe
Which of the following interpretations (if any)
applies to each of the set 8 comments?
(9) a. ? The named thing is missing
b. ? The spelling of the named thing is incorrect
c. ? The named thing is erroneously included
d. ? The named thing needs correcting
e. ? This part attains the required standard
In order to understand these comments, A has to
inspect the passage to which the comment points
to see whether it contains the object named by the
comment. If it does, there may still be the pos-
sibility that it should be present, but that there is
something wrong with it.
5 Scheme design: Skill targeted
We have considered what the corpus margin com-
ments are doing, and the ways in which they ex-
press whether an essay part met the required stan-
dard. The way in which a comment conveys how
the standard was or was not met is embodied by
the comments classification scheme?s design. The
scheme has three layers, and here we consider the
first. When M wrote a comment, M had in mind
a good-essay-writing principle. Our classification
scheme makes explicit the skill area of that essay-
writing principle. Consider set 10.
(10) a. Why not?
b. Why bold?
To understand what these comments mean, we
first need to know what M intended, which we
have argued was to communicate to A whether and
how the related essay part had reached an agreed
standard. On that account, the comments (a) and
(b) in 10 mean something like (a) and (b) in 11.
(11) a. ?The argument here would have been improved
by including an explanation of why not.
b. ?The use of bold font here is questionable.
These are very different messages. One com-
ment is alerting A to some missing argument, and
the other is questioning A?s use of different fonts.
How do we know this, given that both comments
have very similar syntactic structure?
Addressee A works out that these comments
mean very different things by first identifying the
skill area that the comment is targeting, and then
considering what that skill area is like?in what
ways it can be good or bad. To understand 10a,
A needs to observe that essay part P contains a
statement, and to infer that M is responding to the
argument made by the statement. To understand
10b, A needs to observe that P contains some text
in bold font, and to infer that M is questioning the
use of the bold font. The difficulty here is that
conversational-style comments do not make it ex-
plicit whether they are targeting content or form.
Concluding that the identification of a com-
ment?s target is often critical to understanding it,
we defined 11categories for the scheme?s ?targeted
skill? layer. The corpus investigations (see 2.10)
revealed four main skill areas targeted by com-
ments:
? Referencing
? Situating work in the relevant literature, referenc-
ing conventions
? Structuring Essays
? Layout, scope, components
? Composing Argument
? Content, quality, arguing techniques, compre-
hensibility
? Presenting English
? Spelling, grammar, formatting, style
47
We made Referencing and Structure target
categories in their own right. Owing to the high
frequency of comments expressing confusion and
comprehensibility (see 2.3) we made Compre-
hensibility a target category. Comments targeting
the content of an argument, the quality of an ar-
gument (not including its comprehensibility), and
arguing techniques are covered by target category
Argument. We divide the skill area of present-
ing English into five subcategories: Formatting,
Grammar, Punctuation, Spelling, Style.
An additional target category is Context-
Dependent. This is assigned if an evaluative com-
ment has very little information in it about what its
targeted skill might be (set 12).
(12) a. Good [212 occurrances]
b. Avoid
c. Unfinished
The 11th target category, Author, is assigned to
all comments which appear non-evaluative. These
include, for example, casual observations, per-
sonal reminiscences, and expressions of gratitude.
6 Scheme design: marker?s Attitude
Having concluded that each corpus comment was
communicating M?s opinion about an essay part,
for the next layer in the scheme, we focused on
opinion types. The investigation results revealed
three common types (see section 2.2), which we
named Miss, Reject, and Condemn. The atti-
tudes do not involve the emotional connotations
normally associated with these names in everyday
communication. (Hereon we will refer to these as
categories of attitude, rather than opinion.)
Having observed the large proportion of polar
questions and expressions of uncertainty or doubt
in the corpus (see 2.3 and 2.4), we decided to treat
Miss, Reject, and Condemn as attitudes held by M
with certainty, and to add another attitude Doubt
to cover comments in which M called into ques-
tion things that A had done, or in which M ex-
pressed some uncertainty or doubt.
? Doubt: ?Why bold??
? M considers that something in the essay is of
questionable value.
Since expressions of uncertainty are often used
as softeners rather than to express actual uncer-
tainty, it seemed inappropriate to treat apparent
uncertainty as a qualifier (Bunt, 2011) of attitudes.
If we treat it as a qualifier, it suggests that M
was not sure about M?s own opinion, rather than
that the target of M?s comment was questionable.
Doubt is the attitude most applicable to the major-
ity of polar questions in the corpus.
A further attitude, which is a sub-type of Con-
demn, is defined as Dispute (see section 2.6):
? Dispute: ?Not necessarily.?
? M holds views that are in opposition to some
proposition in the essay.
A further attitude Commend covers all com-
ments that announce a ?strength? (see section 2.1):
? Commend: ?Good?
? M considers that something in the essay has at-
tained or exceeded the required standard, or is
pleasing or interesting to M.
Two further attitudes (Refer and Exclaim) are
defined, which have a special characteristic.
? Refer: ?Ditto.?
? M believes that A would benefit from reading a
particular source.
? Exclaim: ?Ah!?
? M is surprised or shocked by something in the
essay that M does not specify.
It is not possible to tell whether Refer comments
are evaluative or not without reading the source to
which M has referred the addressee. Similarly, it is
impossible to tell whether Exclaim comments are
evaluative or not, either from the comment or the
essay part to which the comment points.
Two final attitudes?Engage and Thank?are
reserved for non-evaluative comments, i.e., com-
ments whose target is Author.
? Engage: ?I know how you feel.?
? M finds something about the essay or about A en-
gaging. It appears that M has become engaged in
a way that is more complex than liking or finding
interesting.
? Thank: ?Thanks?
? M is grateful to A.
These attitudes are what we term ?solidarity? at-
titudes, in that we assume that they were made in
order to engender positive feelings in A. Engage
comments have a very wide variety of forms and
topics, which we will not be attempting to analyse
in the initial rounds of the machine learning trials.
Thank comments are all expressions of gratitude.
7 Scheme design: Linguistic Act
The third layer of the categorisation scheme iden-
tifies what we are calling the ?linguistic act? of the
comment. The acts are distinguished principally
48
by surface form and do not concern the evaluative
(or non-evaluative) message that the comment is
attempting to communicate.
We began with the three basic English sen-
tence types: declarative, interrogative, imperative.
We divided ?interrogative? into acts WHQuestion
and Polar Question, as they have clearly distin-
guishable surface forms.
We also divided declarative comments into two
acts: Assertion and Description. All margin
comments, including interrogatives and impera-
tives, are by definition assertions of M?s opin-
ions, we have argued. The scheme?s act Asser-
tion is reserved for assertions of propositions in
response to argument (13a, 13b) and explicit ex-
pressions concerning understanding (13c), agree-
ment (13d), verification or certainty. Many asser-
tions are subjective-sounding.
(13) a. That is impossible!
b. This is true of many other organisations
c. I don?t understand
d. Not sure I agree!
Act Description is assigned to a comment which
is a description of a (non-propositional) object in
or quality of an essay part P or of an action that has
been carried out by author A and that is evidenced
by part P (set 14).
(14) a. Too many references.
b. Factors clearly articulated.
c. This is a very strong assertion
Splitting declaratives into acts Description and
Assertion is a small step away from categoris-
ing linguistic acts according to syntax only. The
move separates declarative comments which re-
spond directly to propositional content from all
other declarative comments.
We interpreted ?imperative? as linguistic act cat-
egory Instruction. We treat the category loosely,
allowing it to include comments that do not use
the imperative form but that look like guidance on
what should have been done (set 15).
(15) a. You should add a citation here.
b. I would not leave a space.
c. Ditto
All Instruction comments talk in a variety of
ways about things that were not done but that
should have been, whereas all Description com-
ments (set 14) talk about what was actually done.
This distinction is not too dissimilar to the distinc-
tion between imperatives and declaratives. That
Instruction comments do not always have the im-
perative form is a repercussion of the informal
conversational style of the comments.
A sixth ?dummy? linguistic act category is as-
signed to all comments with attitude Engage, be-
cause we will not be attempting to analyse those.
The linguistic act layer, then, categorises the
comment?s form, while the target and attitude lay-
ers categorise its meaning. The linguistic act ac-
counts for what the comment is apparently doing
(see section 3). The attitude and target account for
what the comment is really doing. A stark differ-
ence between utterances and margin comments is
that, to understand an utterance, hearer H does not
have to work out what speaker S was really doing
(Ramsay and Field, 2008); whereas to understand
a margin comment, addressee A does have to work
out what marker M was really doing.
8 Evaluation
We have demonstrated that the classification
scheme can be deployed with high agreement lev-
els between independent annotators. Agreement
by two annotators was calculated for 313 sample
comments that were annotated by each annotator
independently. Annotator A designed the scheme
over several months. Annotator B spent about 50
minutes learning the scheme (from no prior expo-
sure to it). Annotator B took a mean average of
1.1 minutes to fully annotate each comment in the
sample. Annotator A took a mean average of .49
minutes to fully annotate each comment.
The corpus comprised 1,408 essays submitted
for 13 different assessed university Master?s mod-
ules, the official word limits of which ranged from
500 to 4,000. The essays had been marked by 20
different markers. The number of essays marked
by each marker varied. The mean average number
of comments per essay per marker ranged from
4.83 to 47.00. To avoid potential bias towards
the more prolific markers? styles, the same num-
ber of essays were randomly sampled for each
marker (where possible), and approximately the
same number of comments were randomly sam-
pled from each of those essays.
Some tutors appear to prefer very short com-
ments, some long. For some (but not all) of the tu-
tors who marked essays of different lengths, there
was a correlation between essay length and the
number of margin comments. No analysis of lin-
guistic style similarities across comments within
49
individual essays was carried out for this paper.
Inter-annotator agreement was calculated using
Cohen?s Kappa for each of the three layers of
the scheme independently. 95% confidence inter-
vals (CI) for test statistics were generated through
10,000 statistical bootstrappings of the annotated
comments. The agreement coefficient for the atti-
tude layer was 0.874 (95% CI, 0.831?0.914), for
the target layer was 0.791 (0.734?0.844), and for
the linguistic act layer was 0.822 (0.770?0.869).
The percentage agreement across all three lay-
ers was 72.1% (67.0%?77.0%) (the percentage
of comments for which both annotators were in
agreement on all three layers). There were no
occurrences of comments which both annotators
deemed unclassifiable. One of the comments was
deemed unclassifiable by one annotator.
The scheme has five attitude+target cross-layer
dependencies (Engage+Author, Thank+Author,
Refer+Context-Dependent, Exclaim+Context-
Dependent, Dispute+Argument), and five
target+act cross-layer dependencies (each of the
same five pairs plus a linguistic act). We acknowl-
edge that these might argue for a more complex
agreement calculation. It is expected that some
linguistic act categories are unlikely to combine
with some attitude categories, though this requires
empirical verification. A conservative estimate of
the number of possible combinations of attitude,
target, and act that we believe might be found
in the corpus is 155 combinations. Additionally,
some categories from a given layer appear to be
more frequent than other categories from the same
layer. We acknowledge, therefore, that a weighted
coefficient method may be more suitable for
calculating inter-annotator agreement.
9 Comparison with previous work
Now that the categorisation scheme has been de-
scribed, we will discuss comparisons with previ-
ous work. Categorisation schemes have been de-
vised or re-used in order to analyse written feed-
back, and discover where improvements might be
made. The studies were principally interested in
whether the marker was writing comments that
would ?feed forward?. Measures for deciding
whether a comment would feed forward tended to
revolve around the power of a comment to moti-
vate its addressee, or whether the comment con-
tained explanatory text that would make it clear
how to do things better in future. We have not
found any feedback categorisation schemes pri-
marily concerned with how opinion in comments
is conveyed through the medium of NL.
Hyland (2001) designed a feedback classifica-
tion scheme that was used to analyse the quality of
feedback for a distance-learning language course.
Hyland?s scheme focused on targeted skills, af-
fective aspects, and explicit pointers for future
writing. Bales (1950) devised 12 categories for
the purpose of analysing small group interactions,
which were later applied to the analysis of mar-
gin comments by Whitelock et al. (2004). Bales?
scheme focused on affective aspects (including
solidarity, tension, antagonism), and pragmatics
aspects (suggestions, opinions, disagreements, re-
quests). Brown and Glover?s (2006) scheme fo-
cused on skills, content, affective aspects, and
feeding forward. They used their scheme to ar-
gue that the feedback in a particular corpus of
comments was of limited value, because most of
the comments did not aid learning or understand-
ing (Brown et al., 2004). Nelson and Schunn
(2009) wanted to identify conditions under which
addressees of peer feedback might actually im-
plement that feedback. Their categories focused
on the linguistic features of comments (includ-
ing summarisation, specificity, explanations) and
affective issues. Perpignan (2003) viewed mar-
gin comments as part of dialogue and discussed
the ?intentions and interpretations of the exchange
from both the teacher?s and the learners? perspec-
tive? (p. 259). The work did not attempt to analyse
the linguistic features of feedback.
The categorisation scheme with the strongest
resemblance to ours was Ferris et al. (1997). The
scheme viewed margin comments as having two
?phases?: teacher?s goal, and linguistic form (p.
163). The scheme has a very different interpre-
tation of the intention of the marker from ours.
It confuses marker intention with comment target.
It implicitly recognises what we call marker atti-
tude, but identifies only one (our Commend). It
implicitly recognises the target of a comment but
has only two target types (?form? and ?content?).
10 Discussion
We have presented a classification scheme for
margin comments which is based on observations
of real data and on linguistics theory. The goal
of the classification was to ultimately use machine
learning to look for relationships between the mar-
50
gin comments and the essay parts to which they
point so that we could design an automatic NL
margin comments generator. The scheme there-
fore focuses on the linguistic aspects of margin
comments: their form and meaning. It is designed
to classify comments independently of the essay
parts to which the comments point. This was to en-
sure that the comments in isolation could be clas-
sified to a useful level of agreement, and, in future
work, to make it possible to investigate whether
essay properties can be used to predict character-
istics of margin comments. The 3-layered scheme
enables the intended evaluative meanings of mar-
gin comments to be captured despite their conver-
sational style, while also preserving linguistic in-
formation about that style (set 16).
(16) a. Could you have developed this?
i. Attitude: Miss
ii. Target: Argument
iii. Act: Polar Question
b. Why bold?
i. Attitude: Doubt
ii. Target: Formatting
iii. Act: WH Question
c. No issues
i. Attitude: Commend
ii. Target: Context-Dependent
iii. Act: Assertion
The classification scheme is arguably a suitable
scheme for all margin comments expressed in NL,
with the proviso that the skills being targeted by
the comments would need to be tailored to the doc-
ument type if it were not an argumentative essay.
Details not discussed earlier include the follow-
ing. (i) We use a skills precedence list to select a
target for comments that are ambiguous over skill
area. (ii) Prior to categorisation, each comment
is segmented, and one ?principal segment? only is
identified for categorisation. This is for 4 reasons.
(1) Many comments begin with filler words (e.g.,
?yes?, ?well?, ?ok?, ?hmm?); (2) Many begin with
preambles (e.g., ?minor point?, ?Just one thought?);
(3) Many use a commendation as a softener be-
fore delivering the main message; (4) Many con-
tain more than one clause or sentence. We usu-
ally assume the first non-filler/non-preamble seg-
ment is the principal segment, and that any non-
filler segments that follow are elaborations on the
first segment. The exception to this is comments
in which a commendation is used as a softener, in
which case the segment that follows the softener
becomes the principal segment.
High inter-annotator agreement scores have
been achieved for the classification scheme. We
have not yet annotated the whole corpus, but we
intend to. We will also calculate inter-annotator
agreement for a higher number of sampled com-
ments, since the number of possible combinations
of attitude, target, and act is so high (circa 155,
see section 8). We may make small changes to the
annotation scheme before doing any further anno-
tation. In particular, we are considering dividing
target Argument into two or three subcategories.
While designing the classification scheme, we
have observed that, despite their conversational
style?indeed because of it?understanding NL
margin comments is harder than understanding
conversational utterances. Although the essay part
to which a comment points is a public object
and therefore in M and A?s views of the common
ground, the aspect of the essay part that M is tar-
geting usually remains unexpressed and private in
M?s mental state. A therefore has to do some in-
ferencing to identify that aspect. In other words,
A has to do some inferencing to fill in gaps in A?s
view of the common ground that do not arise in a
conversation. This extra work is necessary just to
understand the comment. To fully benefit from the
comment by inferring the essay-writing principle
M had in mind requires even more work.
The planned machine learning investigations
will attempt to recognise and categorise appropri-
ate opportunities for feedback comments by look-
ing for associations between the categories as-
signed to each margin comment according to our
scheme, and features of the passage in the essay
to which a comment points?simple n-gram fea-
tures, more complex measures of semantic simi-
larity, and analysis of syntactic structure will be
experimented with. The planned automatic feed-
back comment generator will be informed by the
machine learning investigations. The form and
style of the comments generated is yet to be de-
cided.
Acknowledgments
This work was supported by the Engineering and
Physical Sciences Research Council (grant num-
bers EP/J005959/1 and EP/J005231/1).
51
References
Chris Anson. 1989. Response styles and ways of
knowing. In Writing and Response: Theory, Prac-
tice, Research, pages 332?366. NCTE, Urbana, IL.
J.L. Austin. 1962. How to do things with words. Ox-
ford University Press, Oxford, 2nd edition.
R.F. Bales. 1950. A set of categories for the analysis
of small group interactions. American Sociological
Review, 15(2):257?263.
E. Brown and C. Glover. 2006. Evaluating written
feedback. In C. Bryan and K. Clegg, editors, Inno-
vative assessment in higher education, pages 81?91.
Routledge, Abingdon.
E. Brown, C. Glover, V. Stevens, and S. Freake. 2004.
Evaluating the effectiveness of written feedback as
an element of formative assessment in science. In
C. Rust, editor, Proceedings of the 12th Improv-
ing Student Learning Symposium. The Oxford Cen-
tre for Staff and Learning Development, Oxford
Brookes University, Oxford.
Harry C. Bunt. 1990. DIT: Dynamic interpretation in
text and dialogue. In ITK research report / Institute
for Language Technology and Artificial Intelligence,
15.
Harry Bunt. 2011. The semantics of dialogue acts. In
Proceedings of the 9th International Conference on
Computational Semantics. Oxford.
H.H. Clark and S.E. Haviland. 1974. Psychological
processes in linguistic explanation. In D. Cohen, ed-
itor, Explaining linguistic phenomena. Hemisphere
Publication Corporation, Washington.
H.H. Clark and E.F. Schaefer. 1989. Contributing to
discourse. Cognitive Science, (13):259?294.
M. G. Core and J. Allen. 1997. Coding dialogs with
the DAMSL annotation scheme. In AAAI Fall Sym-
posium on Communicative Action in Humans and
Machines, MIT, Cambridge, MA.
M. Francine Danis. 1987. The voice in the margins:
Paper-marking as conversation. Freshman English
News, (15):18?20.
Dana R. Ferris, Susan Pezone, Cathy R. Tade, and Sha-
ree Tinti. 1997. Teacher commentary on student
writing: Descriptions and implications. Journal of
Second Language Writing, 6(2):155?182.
Maxine Hairston. 1982. The winds of change: Thomas
Kuhn and the revolution in the teaching of writ-
ing. College Composition and Communication,
33(1):76?88.
F. Hyland. 2001. Providing effective support: inves-
tigating feedback to distance learners. Open Learn-
ing, 16(3):233?247.
J.A.W. Kamp. 1981. A theory of truth and semantic
representation. In J. Groenendijk, T. Janssen, and
M. Stockhof, editors, Formal methods in the study
of language, pages 177?321. Mathematical Centre
Tracts, Amsterdam.
Wolfgang Klein. 1985. Ellipse, fokusgliederung
und thematischer stand. In Reinhard Meyer-
Hermann and Hannes Rieser, editors, Ellipsen und
fragmentarische Ausdr?ucke, pages 1?24. Niemeyer,
T?ubingen.
D. Lewis. 1979. Scorekeeping in a language game.
Journal of Philosophical Logic, (9):339?359.
Erika Lindemann. 1987. A Rhetoric for Writing Teach-
ers. Oxford UP, New York, 2nd edition edition.
Jason Merchant. 2004. Fragments and ellipsis. Lin-
guistics and philosophy, (27):661?738.
Melissa M. Nelson and Christian D. Schunn. 2009.
The nature of feedback: how different types of peer
feedback affect writing performance. Instructional
Science, 37:375?401.
Hadara Perpignan. 2003. Exploring the written feed-
back dialogue: a research, learning and teaching
practice. Language Teaching Research, 7(2):259?
278.
Allan Ramsay and Debora Field. 2008. Speech acts,
epistemic planning and Grice?s maxims. Journal of
Logic and Computation, 18:431?457.
H. Sacks, E.A. Schegloff, and G. Jefferson. 1974. A
simplest systematics for the organization of turn-
taking for conversation. Language, (50):696?735.
E.A. Schegloff. 1999. Discourse, pragmatics, conver-
sation, analysis. Discourse Studies, 1(4):405?435.
J.R. Searle and D. Vanderveken. 1985. Foundations
of illocutionary logic. Cambridge University Press,
New York.
J.R. Searle. 1969. Speech acts: An essay in the phi-
losophy of language. Cambridge University Press,
Cambridge.
R Stalnaker. 1972. Pragmatics. In D. Davidson and
G. Harman, editors, Semantics of natural language
(Synthese Library, Vol. 40), pages 380?397. D. Rei-
del, Dordrecht, Holland.
R. Straub. 1996. Teacher response as conversation:
more than casual talk, an exploration. Rhetoric Re-
view, 14(2):374?98.
R.H. Thomason. 1990. Accommodation, meaning,
and implicature: Interdisciplinary foundations for
pragmatics. In P.R. Cohen, J. Morgan, and M.E.
Pollack, editors, Intentions in communication, pages
325?363. MIT, Cambridge, Massachusetts.
52
D. Whitelock, S. Watt, Y. Raw, and Moreale E. 2004.
Analysing tutor feedback to students: first steps
towards constructing an electronic monitoring sys-
tem. Association for Learning Technology Journal,
11(3):31?42.
Nina Ziv. 1984. the effect of teacher comments on the
writing of four college freshmen. In R. Beach and
L. Bridwell, editors, New Directions in Composition
Research, pages 362?380. Guilford, New York.
53
