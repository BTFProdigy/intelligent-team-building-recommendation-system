Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 166?170,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
An Improved MDL-Based Compression Algorithm for
Unsupervised Word Segmentation
Ruey-Cheng Chen
National Taiwan University
1 Roosevelt Rd. Sec. 4
Taipei 106, Taiwan
rueycheng@turing.csie.ntu.edu.tw
Abstract
We study the mathematical properties of
a recently proposed MDL-based unsuper-
vised word segmentation algorithm, called
regularized compression. Our analysis
shows that its objective function can be
efficiently approximated using the nega-
tive empirical pointwise mutual informa-
tion. The proposed extension improves the
baseline performance in both efficiency
and accuracy on a standard benchmark.
1 Introduction
Hierarchical Bayes methods have been main-
stream in unsupervised word segmentation since
the dawn of hierarchical Dirichlet process (Gold-
water et al, 2009) and adaptors grammar (Johnson
and Goldwater, 2009). Despite this wide recog-
nition, they are also notoriously computational
prohibitive and have limited adoption on larger
corpora. While much effort has been directed
to mitigating this issue within the Bayes frame-
work (Borschinger and Johnson, 2011), many
have found minimum description length (MDL)
based methods more promising in addressing the
scalability problem.
MDL-based methods (Rissanen, 1978) rely on
underlying search algorithms to segment the text
in as many possible ways and use description
length to decide which to output. As differ-
ent algorithms explore different trajectories in
the search space, segmentation accuracy depends
largely on the search coverage. Early work in this
line focused more on existing segmentation algo-
rithm, such as branching entropy (Tanaka-Ishii,
2005; Zhikov et al, 2010) and bootstrap voting ex-
perts (Hewlett and Cohen, 2009; Hewlett and Co-
hen, 2011). A recent study (Chen et al, 2012) on
a compression-based algorithm, regularized com-
pression, has achieved comparable performance
result to hierarchical Bayes methods.
Along this line, in this paper we present a novel
extension to the regularized compressor algorithm.
We propose a lower-bound approximate to the
original objective and show that, through analy-
sis and experimentation, this amendment improves
segmentation performance and runtime efficiency.
2 Regularized Compression
The dynamics behind regularized compression is
similar to digram coding (Witten et al, 1999). One
first breaks the text down to a sequence of char-
acters (W0) and then works from that represen-
tation up in an agglomerative fashion, iteratively
removing word boundaries between the two se-
lected word types. Hence, a new sequence Wi
is created in the i-th iteration by merging all the
occurrences of some selected bigram (x, y) in the
original sequence Wi?1. Unlike in digram cod-
ing, where the most frequent pair of word types is
always selected, in regularized compression a spe-
cialized decision criterion is used to balance com-
pression rate and vocabulary complexity:
min. ??f(x, y) + |Wi?1|?H?(Wi?1,Wi)
s.t. either x or y is a character
f(x, y) > nms.
Here, the criterion is written slightly differ-
ently. Note that f(x, y) is the bigram fre-
quency, |Wi?1| the sequence length of Wi?1, and
?H?(Wi?1,Wi) = H?(Wi)? H?(Wi?1) is the dif-
ference between the empirical Shannon entropy
measured on Wi and Wi?1, using maximum like-
lihood estimates. Specifically, this empirical esti-
mate H?(W ) for a sequence W corresponds to:
log |W | ? 1|W |
?
x:types
f(x) log f(x).
For this equation to work, one needs to estimate
other model parameters. See Chen et al (2012)
for a comprehensive treatment.
166
f(x) f(y) f(z) |W |
Wi?1 k l 0 N
Wi k ?m l ?m m N ?m
Table 1: The change between iterations in word
frequency and sequence length in regularized
compression. In the new sequence Wi, each oc-
currence of the x-y bigram is replaced with a new
(conceptually unseen) word z. This has an effect
of reducing the number of words in the sequence.
3 Change in Description Length
The second term of the aforementioned objective
is in fact an approximate to the change in descrip-
tion length. This is made obvious by coding up a
sequence W using the Shannon code, with which
the description length ofW is equal to |W |H?(W ).
Here, the change in description length between se-
quences Wi?1 and Wi is written as:
?L = |Wi|H?(W )? |Wi?1|H?(Wi?1). (1)
Let us focus on this equation. Suppose that the
original sequence Wi?1 is N -word long, the se-
lected word type pair x and y each occurs k and l
times, respectively, and altogether x-y bigram oc-
curs m times in Wi?1. In the new sequence Wi,
each of the m bigrams is replaced with an un-
seen word z = xy. These altogether have reduced
the sequence length by m. The end result is that
compression moves probability masses from one
place to the other, causing a change in descrip-
tion length. See Table 1 for a summary to this
exchange.
Now, as we expand Equation (1) and reorganize
the remaining, we find that:
?L = (N ?m) log(N ?m)?N logN
+ k log k ? (k ?m) log(k ?m)
+ l log l ? (l ?m) log(l ?m)
+ 0 log 0?m logm
(2)
Note that each line in Equation (2) is of the form
x1 log x1 ? x2 log x2 for some x1, x2 ? 0. We
exploit this pattern and derive a bound for ?L
through analysis. Consider g(x) = x log x. Since
g??(x) > 0 for x ? 0, by the Taylor series we have
the following relations for any x1, x2 ? 0:
g(x1)? g(x2) ? (x1 ? x2)g?(x1),
g(x1)? g(x2) ? (x1 ? x2)g?(x2).
Plugging these into Equation (2), we have:
m log (k ?m)(l ?m)Nm ? ?L ? ?. (3)
The lower bound1 at the left-hand side is a best-
case estimate. As our aim is to minimize ?L, we
use this quantity to serve as an approximate.
4 Proposed Method
Based on this finding, we propose the following
two variations (see Figure 1) for the regularized
compression framework:
? G1: Replacing the second term in the origi-
nal objective with the lower bound in Equa-
tion (3). The new objective function is writ-
ten out as Equation (4).
? G2: Same as G1 except that the lower bound
is divided by f(x, y) beforehand. The nor-
malized lower bound approximates the per-
word change in description length, as shown
in Equation (5). With this variation, the func-
tion remains in a scalarized form as the orig-
inal does.
We use the following procedure to compute de-
scription length. Given a word sequence W , we
write out all the induced word types (say, M types
in total) entry by entry as a character sequence, de-
noted as C. Then the overall description length is:
|W |H?(W ) + |C|H?(C) + M ? 12 log |W |. (6)
Three free parameters, ?, ?, and nms remain to
be estimated. A detailed treatment on parameter
estimation is given in the following paragraphs.
Trade-off ? This parameter controls the bal-
ance between compression rate and vocabulary
complexity. Throughout this experiment, we es-
timated this parameter using MDL-based grid
search. Multiple search runs at different granular-
ity levels were employed as necessary.
Compression rate ? This is the minimum
threshold value for compression rate. The com-
pressor algorithm would go on as many iteration
as possible until the overall compression rate (i.e.,
1Sharp-eyed readers may have noticed the similarity be-
tween the lower bound and the negative (empirical) point-
wise mutual information. In fact, when f(z) > 0 in Wi?1, it
can be shown that limm?0 ?L/m converges to the empirical
pointwise mutual information (proof omitted here).
167
G1 ? f(x, y)
(
log (f(x)? f(x, y))(f(y)? f(x, y))|Wi?1|f(x, y)
? ?
)
(4)
G2 ? ??f(x, y) + log
(f(x)? f(x, y))(f(y)? f(x, y))
|Wi?1|f(x, y)
(5)
Figure 1: The two newly-proposed objective functions.
word/character ratio) is lower than ?. Setting this
value to 0 forces the compressor to go on until
no more can be done. In this paper, we exper-
imented with predetermined ? values as well as
those learned from MDL-based grid search.
Minimum support nms We simply followed the
suggested setting nms = 3 (Chen et al, 2012).
5 Evaluation
5.1 Setup
In the experiment, we tested our methods on
Brent?s derivation of the Bernstein-Ratner cor-
pus (Brent and Cartwright, 1996; Bernstein-
Ratner, 1987). This dataset is distributed via the
CHILDES project (MacWhinney and Snow, 1990)
and has been commonly used as a standard bench-
mark for phonetic segmentation. Our baseline
method is the original regularized compressor al-
gorithm (Chen et al, 2012). In our experiment, we
considered the following three search settings for
finding the model parameters:
(a) Fix ? to 0 and vary ? to find the best value (in
the sense of description length);
(b) Fix ? to the best value found in setting (a)
and vary ?;
(c) Set ? to a heuristic value 0.37 (Chen et al,
2012) and vary ?.
Settings (a) and (b) can be seen as running a
stochastic grid searcher one round for each param-
eter2. Note that we tested (c) here only to compare
with the best baseline setting.
5.2 Result
Table 2 summarizes the result for each objective
and each search setting. The best (?, ?) pair for
2A more formal way to estimate both ? and ? is to run
a stochastic searcher that varies between settings (a) and (b),
fixing the best value found in the previous run. Here, for
simplicity, we leave this to future work.
Run P R F
Baseline 76.9 81.6 79.2
G1 (a) ? : 0.030 76.4 79.9 78.1
G1 (b) ? : 0.38 73.4 80.2 76.8
G1 (c) ? : 0.010 75.7 80.4 78.0
G2 (a) ? : 0.002 82.1 80.0 81.0
G2 (b) ? : 0.36 79.1 81.7 80.4
G2 (c) ? : 0.004 79.3 84.2 81.7
Table 2: The performance result on the Bernstein-
Ratner corpus. Segmentation performance is mea-
sured using word-level precision (P), recall (R),
and F-measure (F).
G1 is (0.03, 0.38) and the best for G2 is (0.002,
0.36). On one hand, the performance ofG1 is con-
sistently inferior to the baseline across all settings.
Although approximation error was one possible
cause, we noticed that the compression process
was no longer properly regularized, since f(x, y)
and the ?L estimate in the objective are intermin-
gled. In this case, adjusting ? has little effect in
balancing compression rate and complexity.
The second objective G2, on the other hand,
did not suffer as much from the aforementioned
lack of regularization. We found that, in all three
settings, G2 outperforms the baseline by 1 to 2
percentage points in F-measure. The best perfor-
mance result achieved by G2 in our experiment is
81.7 in word-level F-measure, although this was
obtained from search setting (c), using a heuristic
? value 0.37. It is interesting to note that G1 (b)
and G2 (b) also gave very close estimates to this
heuristic value. Nevertheless, it remains an open
issue whether there is a connection between the
optimal ? value and the true word/token ratio (?
0.35 for Bernstein-Ratner corpus).
The result has led us to conclude that MDL-
based grid search is efficient in optimizing seg-
mentation accuracy. Minimization of descrip-
tion length is in general aligned with perfor-
mance improvement, although under finer gran-
ularity MDL-based search may not be as effec-
168
Method P R F
Adaptors grammar, colloc3-syllable Johnson and Goldwater (2009) 86.1 88.4 87.2
Regularized compression + MDL, G2 (b) ? 79.1 81.7 80.4
Regularized compression + MDL Chen et al (2012) 76.9 81.6 79.2
Adaptors grammar, colloc Johnson and Goldwater (2009) 78.4 75.7 77.1
Particle filter, unigram Bo?rschinger and Johnson (2012) ? ? 77.1
Regularized compression + MDL, G1 (b) ? 73.4 80.2 76.8
Bootstrap voting experts + MDL Hewlett and Cohen (2011) 79.3 73.4 76.2
Nested Pitman-Yor process, bigram Mochihashi et al (2009) 74.8 76.7 75.7
Branching entropy + MDL Zhikov et al (2010) 76.3 74.5 75.4
Particle filter, bigram Bo?rschinger and Johnson (2012) ? ? 74.5
Hierarchical Dirichlet process Goldwater et al (2009) 75.2 69.6 72.3
Table 3: The performance chart on the Bernstein-Ratner corpus, in descending order of word-level F-
measure. We deliberately reproduced the results for adaptors grammar and regularized compression. The
other measurements came directly from the literature.
tive. In our experiment, search setting (b) won
out on description length for both objectives, while
the best performance was in fact achieved by the
others. It would be interesting to confirm this
by studying the correlation between description
length and word-level F-measure.
In Table 3, we summarize many published re-
sults for segmentation methods ever tested on the
Bernstein-Ratner corpus. Of the proposed meth-
ods, we include only setting (b) since it is more
general than the others. From Table 3, we find that
the performance of G2 (b) is competitive to other
state-of-the-art hierarchical Bayesian models and
MDL methods, though it still lags 7 percentage
points behind the best result achieved by adap-
tors grammar with colloc3-syllable. We also com-
pare adaptors grammar to regularized compressor
on average running time, which is shown in Ta-
ble 4. On our test machine, it took roughly 15
hours for one instance of adaptors grammar with
colloc3-syllable to run to the finish. Yet an im-
proved regularized compressor could deliver the
result in merely 1.25 second. In other words, even
in an 100 ? 100 grid search, the regularized com-
pressor algorithm can still finish 4 to 5 times ear-
lier than one single adaptors grammar instance.
6 Concluding Remarks
In this paper, we derive a new lower-bound ap-
proximate to the objective function used in the
regularized compression algorithm. As computing
the approximate no longer relies on the change in
lexicon entropy, the new compressor algorithm is
made more efficient than the original. Besides run-
Method Time (s)
Adaptors grammar, colloc3-syllable 53826
Adaptors grammar, colloc 10498
Regularized compressor 1.51
Regularized compressor, G1 (b) 0.60
Regularized compressor, G2 (b) 1.25
Table 4: The average running time in seconds on
the Bernstein-Ratner corpus for adaptors grammar
(per fold, based on trace output) and regularized
compressors, tested on an Intel Xeon 2.5GHz 8-
core machine with 8GB RAM.
time efficiency, our experiment result also shows
improved performance. Using MDL alone, one
proposed method outperforms the original regu-
larized compressor (Chen et al, 2012) in preci-
sion by 2 percentage points and in F-measure by 1.
Its performance is only second to the state of the
art, achieved by adaptors grammar with colloc3-
syllable (Johnson and Goldwater, 2009).
A natural extension of this work is to repro-
duce this result on some other word segmenta-
tion benchmarks, specifically those in other Asian
languages (Emerson, 2005; Zhikov et al, 2010).
Furthermore, it would be interesting to investigate
stochastic optimization techniques for regularized
compression that simultaneously fit both ? and ?.
We believe this would be the key to adapt the al-
gorithm to larger datasets.
Acknowledgments
We thank the anonymous reviewers for their valu-
able feedback.
169
References
Nan Bernstein-Ratner. 1987. The phonology of parent
child speech. Children?s language, 6:159?174.
Benjamin Borschinger and Mark Johnson. 2011. A
particle filter algorithm for bayesian word segmen-
tation. In Proceedings of the Australasian Language
Technology Association Workshop 2011, pages 10?
18, Canberra, Australia, December.
Benjamin Bo?rschinger and Mark Johnson. 2012. Us-
ing rejuvenation to improve particle filtering for
bayesian word segmentation. In Proceedings of the
50th Annual Meeting of the Association for Com-
putational Linguistics (Volume 2: Short Papers),
pages 85?89, Jeju Island, Korea, July. Association
for Computational Linguistics.
Michael R. Brent and Timothy A. Cartwright. 1996.
Distributional regularity and phonotactic constraints
are useful for segmentation. In Cognition, pages 93?
125.
Ruey-Cheng Chen, Chiung-Min Tsai, and Jieh Hsiang.
2012. A regularized compression method to unsu-
pervised word segmentation. In Proceedings of the
Twelfth Meeting of the Special Interest Group on
Computational Morphology and Phonology, SIG-
MORPHON ?12, pages 26?34, Montreal, Canada.
Association for Computational Linguistics.
Thomas Emerson. 2005. The second international chi-
nese word segmentation bakeoff. In Proceedings of
the Fourth SIGHAN Workshop on Chinese Language
Processing, volume 133. Jeju Island, Korea.
Sharon Goldwater, Thomas L. Griffiths, and Mark
Johnson. 2009. A bayesian framework for word
segmentation: Exploring the effects of context.
Cognition, 112(1):21?54, July.
Daniel Hewlett and Paul Cohen. 2009. Bootstrap vot-
ing experts. In Proceedings of the 21st international
jont conference on Artifical intelligence, IJCAI?09,
pages 1071?1076, San Francisco, CA, USA. Mor-
gan Kaufmann Publishers Inc.
Daniel Hewlett and Paul Cohen. 2011. Fully unsuper-
vised word segmentation with BVE and MDL. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies: short papers - Volume 2, HLT
?11, pages 540?545, Portland, Oregon. Association
for Computational Linguistics.
Mark Johnson and Sharon Goldwater. 2009. Im-
proving nonparameteric bayesian inference: exper-
iments on unsupervised word segmentation with
adaptor grammars. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, NAACL ?09, pages
317?325, Boulder, Colorado. Association for Com-
putational Linguistics.
Brian MacWhinney and Catherine Snow. 1990. The
child language data exchange system: an update.
Journal of child language, 17(2):457?472, June.
Daichi Mochihashi, Takeshi Yamada, and Naonori
Ueda. 2009. Bayesian unsupervised word segmen-
tation with nested Pitman-Yor language modeling.
In Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing
of the AFNLP: Volume 1 - Volume 1, ACL ?09, pages
100?108, Suntec, Singapore. Association for Com-
putational Linguistics.
Jorma Rissanen. 1978. Modeling by shortest data de-
scription. Automatica, 14(5):465?471, September.
Kumiko Tanaka-Ishii. 2005. Entropy as an indica-
tor of context boundaries: An experiment using a
web search engine. In Robert Dale, Kam-Fai Wong,
Jian Su, and Oi Kwong, editors, Natural Language
Processing IJCNLP 2005, volume 3651 of Lecture
Notes in Computer Science, chapter 9, pages 93?
105. Springer Berlin / Heidelberg, Berlin, Heidel-
berg.
Ian H. Witten, Alistair Moffat, and Timothy C. Bell.
1999. Managing gigabytes (2nd ed.): compressing
and indexing documents and images. Morgan Kauf-
mann Publishers Inc., San Francisco, CA, USA.
Valentin Zhikov, Hiroya Takamura, and Manabu Oku-
mura. 2010. An efficient algorithm for unsuper-
vised word segmentation with branching entropy
and MDL. In Proceedings of the 2010 Conference
on Empirical Methods in Natural Language Pro-
cessing, EMNLP ?10, pages 832?842, Cambridge,
Massachusetts. Association for Computational Lin-
guistics.
170
Proceedings of the Twelfth Meeting of the Special Interest Group on Computational Morphology and Phonology (SIGMORPHON2012), pages 26?34,
Montre?al, Canada, June 7, 2012. c?2012 Association for Computational Linguistics
A Regularized Compression Method To Unsupervised Word Segmentation
Ruey-Cheng Chen, Chiung-Min Tsai and Jieh Hsiang
National Taiwan University
1 Roosevelt Rd. Sec. 4
Taipei 106, Taiwan
rueycheng@turing.csie.ntu.edu.tw
cmtsai@mail.lis.ntu.edu.tw
jhsiang@ntu.edu.tw
Abstract
Languages are constantly evolving through
their users due to the need to communicate
more efficiently. Under this hypothesis, we
formulate unsupervised word segmentation as
a regularized compression process. We re-
duce this process to an optimization problem,
and propose a greedy inclusion solution. Pre-
liminary test results on the Bernstein-Ratner
corpus and Bakeoff-2005 show that the our
method is comparable to the state-of-the-art in
terms of effectiveness and efficiency.
1 Introduction
Unsupervised word segmentation has been a popular
research subject due to its close connection to lan-
guage acquisition. It has attracted researchers from
different communities, including linguistics, cogni-
tive science, and machine learning, to investigate
how human beings develop and harness their lan-
guages, and, more importantly, how knowledge is
acquired.
In this paper we propose a new formulation to the
unsupervised word segmentation problem. Our idea
is based on the observation that language evolves be-
cause of the need to reduce communication efforts.
For instance, new terminologies, abbreviations, and
slang that carry complex semantics which cannot be
efficiently expressed in the original languages are in-
vented so that concepts can be conveyed. Such an
evolution, we hypothesize, is limited to the extent
where the evolved vocabulary exhibits similar com-
plexity as the original one, in light of reducing the
extra cost to pick up the new language. This process
is realized as an optimization problem called regu-
larized compression, which gets this name from its
analogy to text compression.
The rest of the paper is organized as follows.
We briefly summarize related work on unsupervised
word segmentation in Section 2. In Section 3, we in-
troduce the proposed formulation. The iterative al-
gorithm and other technical details for solving the
optimization problem are covered in Section 4. In
Section 5, we describe the evaluation procedure and
discuss the experimental results. Finally, we present
concluding remarks in Section 6.
2 Related Work
The past few years have seen many nonparametric
Bayesian methods developed to model natural lan-
guages. Many such applications were applied to
word segmentation and have collectively reshaped
the entire research field. Two most notable exam-
ples are hierarchical Bayesian models and the min-
imum description length principle. Our method fits
in the latter category since we use this principle to
optimize model parameters.
Hierarchical Bayesian methods were first in-
troduced to complement conventional probabilistic
methods to facilitate context-aware word generation.
Goldwater et al (2006) used hierarchical Dirichlet
processes (HDP) to induce contextual word mod-
els. Their approach was a significant improvement
over conventional probabilistic methods, and has in-
spired further explorations into more advanced hi-
erarchical modeling techniques. Such examples in-
clude the nested Pitman-Yor process (Mochihashi et
al., 2009), a sophisticated installment for hierarchi-
26
cal modeling at both word and character levels, and
adaptor grammars (Johnson and Goldwater, 2009), a
framework that aligns HDP to probabilistic context-
free grammars.
The minimum description length (MDL) princi-
ple, originally developed in the context of infor-
mation theory, was adopted in Bayesian statistics
as a principled model selection method (Rissanen,
1978). Its connection to lexical acquisition was
first uncovered in behavioral studies, and early ap-
plications focused mostly on applying MDL to in-
duce word segmentation that results in compact lex-
icons (Kit and Wilks, 1999; Yu, 2000; Argamon et
al., 2004). More recent approaches (Zhikov et al,
2010; Hewlett and Cohen, 2011) used MDL in com-
bination with existing algorithms, such as branch-
ing entropy (Tanaka-Ishii, 2005; Jin and Ishii, 2006)
and bootstrap voting experts (Hewlett and Cohen,
2009), to determine the best segmentation parame-
ters. On various benchmarks, MDL-powered algo-
rithms have achieved state-of-the-art performance,
sometimes even surpassing that of the most sophis-
ticated hierarchical modeling methods.
3 Regularized Compression
3.1 Preliminaries
Consider that the unsegmented text consists ofK ut-
terances and totally of N characters. We denote the
text as a sequence of characters c = ?c1, . . . , cN ?,
as if conceptually concatenating all the K utter-
ances into one string. The positions of all the ut-
terance boundaries in c are represented as a set
U = {u0 = 0, u1, . . . , uK}. In other words, the
k-th utterance (k = 1, . . . ,K) is stored as the sub-
sequence ?cuk?1+1, . . . , cuk? in c.
A segmented text is denoted as a sequence of
words w = ?w1, w2, . . . , wM ? for some M < N . It
represents the same piece of text as c does. The word
sequence w is said to respect the utterance bound-
aries U if any word in the sequence does not span
over two utterances. Unique elements in a charac-
ter or word sequence implicitly define an alphabet
set (or lexicon). Hereafter, we denote such alphabet
sets for c and w as Ac and Aw, respectively.
3.2 Effects of Compression
Word segmentation results from compressing a se-
quence of characters. By compression, we mean to
replace the occurrences for some k-characters sub-
sequence ?c1, c2, . . . , ck? in the text with those for a
new string w = c1c2 . . . ck (word). This procedure
can be generalized to include more subsequences to
be replaced, each with a different length. The result-
ing sequence is a mixture of characters and words
introduced during compression. For clarity, we use
the term token sequence to refer to such a mixed se-
quence of characters or words.
Compression has a few effects to the token se-
quence: (i) it increases the total number of tokens,
(ii) it expands the alphabet set to include newly pro-
duced tokens, (iii) it affects the entropy rate esti-
mates. Note that, by seeing a token sequence as
a series of outcomes drawn from some underlying
stochastic process, we can estimate the entropy rate
empirically.
Items (i) and (ii) are natural consequences of com-
pression. The effort to describe the same piece
of information gets reduced at the expense of ex-
panding the vocabulary, and sometimes even chang-
ing the usage. A real-life example for this is that
language users invent new terminologies for effi-
ciently conveying complex information. Item (iii)
describes something more subtle. Observe that,
when some n occurrences of a k-character subse-
quence ?c1, c2, . . . , ck? get compressed, each char-
acter ci loses n occurrences, and totally nk occur-
rences move away from the subsequence; as a result,
the newly created word w receives n occurrences. It
is clear that compression has this side effect of re-
distributing probability masses among the observa-
tions (i.e., characters), thereby causing deviation to
entropy rate estimates.
3.3 Formulation
The choice of subsequences to be compressed is es-
sential in the aforementioned process. We hypothe-
size that a good choice has the following two prop-
erties: (i) higher frequency, and (ii) low deviation in
entropy rate.
We motivate these two properties as follows.
First, high frequency subsequences are favorable
here since they are more likely to be character-level
27
collocations; compressing these subsequences re-
sults in better compression rate. Second, deviation
in entropy rate is reflected in vocabulary complex-
ity, and we believe that it directly translates to efforts
that language users pay to adapt to the new language.
In this case, there seems no reason to believe that ei-
ther increasing or decreasing vocabulary complexity
is beneficial, since in two trivial ?bad choices? that
one can easily imagine, i.e., the text being fully seg-
mented or unsegmented, the entropy rates reach both
extremes.
Motivated by these observations, we expect that
the best word segmentation (i) achieves some prede-
fined compression rate, and (ii) minimizes deviation
in entropy rate. This idea is realized as an optimiza-
tion problem, called regularized compression. Con-
ceptually, this problem is defined as:
minimize DV(c,w)
subject to w respects U
| |w||c| ? ?| ? 
(1)
where ? denotes some expected compression ratio
and  denotes the tolerance. Note that DV(c,w) =
|H?(C) ? H?(W )| represents the deviation in en-
tropy rate with respect to sequences c and w. In
this definition, H?(C) and H?(W ) denote the em-
pirical entropy rates for random variables C ? Ac
and W ? Aw, estimated on the corresponding se-
quences c and w, respectively.
4 Iterative Algorithm
4.1 Ordered Ruleset
Acknowledging that exponentially many feasible
word sequences need to be checked, we propose an
alternative formulation in a restricted solution space.
The idea is, instead of optimizing for segmentations,
we search for segmentation generators, i.e., a set of
functions that generate segmentations from the in-
put. The generators we consider here is the ordered
rulesets.
An ordered ruleset R = ?r1, r2, . . . , rk? is a se-
quence of translation rules, each of which takes the
following form:
w ? c1c2 . . . cn,
where the right-hand side (c1c2 . . . cn) denotes the
n-token subsequence to be replaced, and the left-
hand side (w) denotes the new token to be intro-
duced. Applying a translation rule r to a token se-
quence has an effect of replacing all the occurrences
for subsequence c1c2 . . . cn with those for token w.
Applying an ordered ruleset R to a token se-
quence is equivalent to iteratively applying the trans-
lation rules r1, r2, . . . , rk in strict order. Specifi-
cally, consider that the initial token sequence is de-
noted as c(0) and let the final result be denoted as
c(k). By iterative application, we mean to repeat the
following step for i = 1 . . . k:
Apply rule ri to c(i?1) and save the result
as c(i).
4.2 Alternative Formulation
This notion of ordered rulesets allows one to explore
the search space efficiently using a greedy inclusion
algorithm. The idea is to maintain a globally best
ruleset B that covers the best translation rules we
have discovered so far, and then iteratively expand
B by discovering new best rule and adding it to rule-
set. The procedure repeats several times until the
compression rate reaches some predefined ratio ?. In
each iteration, the best translation rule is determined
by solving a modified version of Equation (1), which
is written as follows:
(In iteration i)
minimize ? |c
(i)|
|c(i?1)| + DV(c
(i?1), c(i))
subject to r is a rule
r(c(i?1)) = c(i)
c(i) respects U
(2)
Note that the alternative formulation is largely a
greedy version of Equation (1) except a few minor
changes. First, the compression rate constraint be-
comes the termination condition in the greedy in-
clusion algorithm. Second, we add an extra term
|c(i)|/|c(i?1)| to the objective to encourage early in-
clusion of frequent collocations. The trade-off pa-
rameter ? is introduced in Equation (2) to scalarize
both terms in the objective.
A brief sketch of the algorithm is given in the fol-
lowing paragraphs.
1. Let B be an empty ordered ruleset, and let c(0)
be the original sequence of tokens.
28
2. Repeat the following steps for each i ? N ,
starting from i = 1, until the compression rate
reaches some predefined threshold.
(a) Find a rule r that maximizes Equation (2)
(b) Apply the rule r to form a new sequence
c(i) from c(i?1).
(c) Add r to the end of B.
3. Output B and the final sequence.
4.3 Implementation
Additional care needs to be taken in implementing
Steps 2a and 2b. The simplest way to collect n-gram
counts for computing the objective in Equation (2) is
to run multiple scans over the entire sequence. Our
experience suggests that using an indexing structure
that keeps track of token positions can be more ef-
ficient. This is especially important when updating
the affected n-gram counts in each iteration. Since
replacing one occurrence for any subsequence af-
fects only its surrounding n-grams, the total num-
ber of such affected n-gram occurrences in one it-
eration is linear in the number of occurrences for
the replaced subsequence. Using an indexing struc-
ture in this case has the advantage to reduce seek
time. Note that, however, the overall running time
remains in the same complexity class regardless of
the deployment of an indexing structure. The time
complexity for this algorithm isO(TN), where T is
the number of iterations and N is the length of the
input sequence.
Although it is theoretically appealing to create an
n-gram search algorithm, in this preliminary study
we used a simple bigram-based implementation for
efficiency. We considered only bigrams in creat-
ing translation rules, expecting that the discovered
bigrams can grow into trigrams or higher-order n-
grams in the subsequent iterations. To allow un-
merged tokens (i.e., characters that was supposed
to be in one n-gram but eventually left out due to
bigram implementation) being merged into the dis-
covered bigram, we also required that that one of
the two participating tokens at the right-hand side
of any translation rule has to be an unmerged to-
ken. This has a side effect to exclude generation of
collocation-based words1. It can be an issue in cer-
1Fictional examples include ?homework? or ?cellphone?.
tain standards; on the test corpora we used, this kind
of problems is not obvious.
Another constraint that we added to the imple-
mentation is to limit the choice of bigrams to those
has more frequency counts. Generally, the number
of occurrence for any candidate bigram being con-
sidered in the search space has to be greater or equal
to some predefined threshold. In practice, we found
little difference in performance for specifying any
integer between 3 and 7 as the threshold; in this pa-
per, we stick to 3.
5 Evaluation
5.1 Setup
We conducted a series of experiments to investi-
gate the effectiveness of the proposed segmentation
method under different language settings and seg-
mentation standards. In the first and the second
experiments, we focus on drawing comparison be-
tween our method and state-of-the-art approaches.
The third experiment focuses on the influence of
data size to segmentation accuracy.
Segmentation performance is assessed using stan-
dard metrics, such as precision, recall, and F-
measure. Generally, these measures are reported
only at word level; in some cases where further anal-
ysis is called for, we report boundary-level and type-
level measures as well. We used the evaluation script
in the official HDP package to calculate these num-
bers.
The reference methods we considered in the com-
parative study include the following:
? Hierarchical Dirichlet process, denoted as HDP
(Goldwater et al, 2009);
? Nested Pitman-Yor process, denoted as NPY
(Mochihashi et al, 2009);
? Adaptor grammars, denoted as AG (Johnson
and Goldwater, 2009);
? Branching entropy + MDL, denoted as Ent-
MDL (Zhikov et al, 2010);
? Bootstrap voting experts + MDL, denoted as
BVE-MDL (Hewlett and Cohen, 2011);
? Description length gain, denoted as DLG (Zhao
and Kit, 2008).
29
The proposed method is denoted as RC; it is also
denoted as RC-MDL in a few cases where MDL is
used for parameter estimation.
5.2 Parameter Estimation
There are two free parameters ? and ? in our model.
The parameter ? specifies the degree to which we
favors high-frequency collocations when solving
Equation (2). Experimentation suggests that ? can
be sensitive when set too low2. Practically, we rec-
ommend optimizing ? based on grid search on de-
velopment data, or the MDL principle. The formula
for calculating description length is not shown here;
see Zhikov et al (2010), Hewlett and Cohen (2011),
and Rissanen (1978) for details.
The expected compression rate ? determines
when to stop the segmentor. It is related to the
expected word length: When the compression rate
|c|/|w| reaches ? and the segmentor is about to stop,
1/? is the average word length in the segmentation.
In this sense, it seems ? is somehow connected to the
language of concern. We expect that optimal values
learned on one data set may thus generalize on the
other sets of the same language. Throughout the ex-
periments, we estimated this value based on devel-
opment data.
5.3 Evaluation on Bernstein-Ratner Corpus
We conducted the first experiment on the Bernstein-
Ratner corpus (Bernstein-Ratner, 1987), a standard
benchmark for English phonetic segmentation. We
used the version derived by Michael Brent, which
is made available in the CHILDES database (Brent
and Cartwright, 1996; MacWhinney and Snow,
1990). The corpus comprises 9,790 utterances,
which amount to 95,809 words in total. Its rel-
atively small size allows experimentation with the
most computational-intensive Bayesian models.
Parameter estimation for the proposed method has
been a challenge due to the lack of appropriate de-
velopment data. We first obtained a rough estimate
for the compression rate ? via human inspection into
the first 10 lines of the corpus (these 10 lines were
later excluded in evaluation) and used that estimate
to set up the termination condition. Since the first
2Informally speaking, when ? < H?(c). The analysis is not
covered in this preliminary study.
P R F Time
HDP 0.752 0.696 0.723 ?
NPY, bigram 0.748 0.767 0.757 17 min.
AG ? ? 0.890 ?
Ent-MDL 0.763 0.745 0.754 2.6 sec.
BVE-MDL 0.793 0.734 0.762 2.6 sec.
RC-MDL 0.771 0.819 0.794 0.9 sec.
Table 2: Performance evaluation on the Bernstein-Ratner
corpus. The reported values for each method indicate
word precision, recall, F-measure and running time, re-
spectively. The boldface value for each column indicates
the top performer under the corresponding metric.
10 lines are too small to reveal any useful segmenta-
tion cues other than the word/token ration of interest,
we considered this setting (?almost unsupervised?)
a reasonable compromise. In this experiment, ? is
set to 0.37; the trade-off parameter ? is set to 8.3,
optimized using MDL principle in a two-pass grid
search (the first pass over {1, 2, . . . , 20} and the sec-
ond over {8.0, 8.1, . . . , 10.0}).
A detailed performance result for the proposed
method is described in Table 1. A reference run
for HDP is included for comparison. The pro-
posed method achieved satisfactory result at word
and boundary levels. Nevertheless, low type-level
numbers (in contrast to those for HDP) together with
high boundary recall suggested that we might have
experienced over-segmentation.
Table 2 covers the same result with less details
in order to compare with other reference methods.
All the reported measures for reference methods
are directly taken from the literature. The result
shows that AG achieved the best performance in F-
measure (other metrics are not reported), surpass-
ing all the other methods by a large margin (10 per-
cent). Among the other methods, our method paired
with MDL achieved comparable performance as the
others in precision; it does slightly better than the
others in recall (5 percent) and F-measure (2.5 per-
cent). Furthermore, our algorithm also seems to be
competitive in terms of computational efficiency. On
this benchmark it demanded only minimal memory
low as 4MB and finished the segmentation run in 0.9
second, even less than the reported running time for
both MDL-based algorithms.
30
P R F BP BR BF TP TR TF
HDP, Bernstein-Ratner 0.75 0.70 0.72 0.90 0.81 0.85 0.64 0.55 0.59
RC-MDL, Bernstein-Ratner 0.77 0.82 0.79 0.85 0.92 0.89 0.57 0.48 0.50
RC, CityU training 0.75 0.79 0.77 0.89 0.93 0.91 0.63 0.35 0.45
RC, MSR training 0.73 0.82 0.77 0.86 0.96 0.91 0.70 0.26 0.38
Table 1: Performance evaluation for the proposed method across different test corpora. The first row indicates a
reference HDP run (Goldwater et al, 2009); the other rows represent the proposed method tested on different test cor-
pora. Columns indicates performance metrics, which correspond to precision, recall, and F-measure at word (P/R/F),
boundary (BP/BR/BF), and type (TP/TR/TF) levels.
Corpus Training (W/T) Test (W/T)
AS 5.45M / 141K 122K / 19K
PKU 1.1M / 55K 104K / 13K
CityU 1.46M / 69K 41K / 9K
MSR 2.37M / 88K 107K / 13K
Table 3: A short summary about the subsets in the
Bakeoff-2005 dataset. The size of each subset is given in
number of words (W) and number of unique word types
(T).
5.4 Evaluation on Bakeoff-2005 Corpus
The second benchmark that we adopted is the
SIGHAN Bakeoff-2005 dataset (Emerson, 2005)
for Chinese word segmentation. The corpus has
four separates subsets prepared by different research
groups; it is among the largest word segmentation
benchmarks available. Table 3 briefly summarizes
the statistics regarding this dataset.
We decided to compare our algorithm with de-
scription length gain (DLG), for that it seems to de-
liver best segmentation accuracy among other un-
supervised approaches ever reported on this bench-
mark (Zhao and Kit, 2008). Since the reported
values for DLG were obtained on another closed
dataset Bakeoff-2006 (Levow, 2006), we followed a
similar experimental setup as suggested in the liter-
ature (Mochihashi et al, 2009): We compared both
methods only on the training sets for the common
subsets CityU and MSR. Note that this experimental
setup departed slightly from that of Mochihashi et al
in that all the comparisons were strictly made on the
training sets. The approach is more straightforward
than the suggested sampling-based method.
Other baseline methods that we considered in-
clude HDP, Ent-MDL, and BVE-MDL, for their
representativeness in segmentation performance and
CityU MSR
RC, r = 0.65 0.770 0.774
DLG, ensemble 0.684 0.665
Ent-MDL, nmax = 3 0.798 0.795
Table 4: Performance evaluation on the common training
subsets in the Bakeoff-2005 and Bakeoff-2006 datasets.
The reported values are token F-measure. The boldface
value in each column indicates the top performer for the
corresponding set.
ease of implementation. The HDP implementation
we used is a modified version of the offical HDP
package3; we patched the package to make it work
with Unicode-encoded Chinese characters. For Ent-
MDL and BVE-MDL, we used the software pack-
age4 distributed by Hewlett and Cohen (2011). We
estimated the parameters using the AS training set
as the development data. We set ? to 6 based on a
grid search. The expected compression rate ? that
we learned from the development data is 0.65.
In Table 1, we give a detailed listing of vari-
ous performance measures for the proposed method.
Segmentation performance seems moderate at both
word and boundary levels. Nevertheless, high type
precision and low type recall on both CityU and
MSR training corpora signaled that our algorithm
failed to discover most word types. This issue, we
suspect, was caused by exclusion of low-frequency
candidate bigrams, as discussed in Section 4.3.
Table 4 summarizes the result for word segmen-
tation conducted on the CityU and MSR subsets of
Bakeoff-2005. Due to practical computational lim-
its, we were not able to run HDP and BVE-MDL
on any complete subset. The result shows that our
3http://homepages.inf.ed.ac.uk/sgwater/
4http://code.google.com/p/voting-experts
31
0.0 0.2 0.4 0.6 0.8 1.0
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
Data size in percentage
F?
m
ea
su
re
F
BF
TF
Figure 1: Performance evaluation for the proposed
method on the CityU training set.
CityU-1k MSR-1k
RC, r = 0.65 0.505 0.492
HDP, 10 sample average 0.591 0.623
RC, r = 0.65/punc. 0.599 0.591
Table 5: Performance evaluation on two random samples
from the common sets (CityU and MSR subsets) in the
Bakeoff-2005 and Bakeoff-2006 datasets.
algorithm outperforms DLG by 8 to 10 percents in
F-measure, while Ent-MDL still performs slightly
better, achieving the top performance among all the
experimental runs on both subsets.
To compare with HDP, we conducted another test
run on top of a random sample of 1,000 lines from
each subset. We chose 1,000 lines because HDP can
easily consume more than 4GB of main memory on
any larger sample. We adopted standard settings for
HDP: ?0 = 3, 000, ?1 = 300, and pb = 0.2. In
each trial run, we ran the Gibbs sampler for 20,000
iterations using simulated annealing (Goldwater et
al., 2009). We obtained 10 samples from the Gibbs
sampler and used the average performance in com-
parison. It took slightly more than 50 hours to col-
lect one trial run on one subset.
The evaluation result is summarized in Table 5.
We ran our algorithm to the desired compression
ratio r = 0.65 on this small sample. The result
0.0 0.2 0.4 0.6 0.8 1.0
0.
0
0.
2
0.
4
0.
6
0.
8
1.
0
Data size in percentage
F?
m
ea
su
re
F
BF
TF
Figure 2: Performance evaluation for the proposed
method on the MSR training set.
shows that the performance of regularized compres-
sion is inferior to that of HDP by 9 to 13 percents
in F-measure for both sets. To investigate why, we
looked into the segmentation output. We observed
that, in the regularized compression output, most of
the punctuation marks were incorrectly aligned to
their neighboring words, owing to the short of fre-
quency counts in this small sample. The HDP, how-
ever, does not seem to suffer from this issue.
We devised a simple post-processing step, in
which each punctuation mark was forced segmented
from the surrounding text. Another outside test was
conducted to see how well the algorithm works us-
ing heuristics derived from minimal domain knowl-
edge. The additional run is denoted as RC/punc.
The result is shown in Table 5. From the result,
we found that the combined approach works slightly
better than HDP in one corpus, but not in the other.
5.5 Effects of Data Size
We employed the third experiment to study the influ-
ence of corpora size to segmentation accuracy. Since
the proposed method relies on empirical estimates
for entropy rate to decide the word boundaries, we
were interested in learning about how it responds to
relatively low and high volume input.
This experiment was conducted on CityU and
32
MSR training sets. On each corpus, we took the first
k% of data (in terms of utterances) and tested the
proposed method against that subset; this test was
repeated several times with different values for k. In
this experiment, we chose the value for k from the
set {2, 4, 6, 8, 10, 20, 30, . . . , 90, 100}. The perfor-
mance is evaluated using word, boundary, and type
F-measures.
Figures 1 and 2 show the experiment results. Both
figures revealed similar patterns for segmentation
performance at different volume levels. Word F-
measures for both corpora begin at roughly 0.52,
climb up rapidly to 0.73 as the volume grows from
2% to 20%, and finally settle on some value around
0.77. Boundary F-measures for both corpora show a
similar trend?a less steep increase before 20% from
0.80 to 0.89 followed by a plateau at around 0.93.
Here, the result seems to suggest that estimating to-
ken entropy rate using less than 20% of data might
be insufficient for this type of text corpora. Further-
more, since performance is saturated at such an early
stage, it seems feasible to split the entire dataset into
a number of folds (e.g., 5, in this case) and solve
each fold individually in parallel. This technique
may greatly enhance the run-time efficiency of the
segmentor.
The patterns we observed for type F-measure tells
another story. On both corpora, type F-measures do
not seem to improve as data volume increases. On
CityU corpora, type F-measure gradually increased
from 0.42 to 0.48 and then slowly falling back to
0.45. On MSR corpora, type F-measure peaked at
0.45 when receiving 10% of data; after that it started
decreasing, going all the way down to 0.37, even
lower than the number 0.43 it received at the begin-
ning. Our guess is that, at some early point (20%),
the proposed method started to under-segment the
text. We suspect that there is some deep con-
nection between performance saturation and under-
segmentation, since from the result they both begin
at roughly the same level. Further investigation in
this respect is needed to give out definitive explana-
tions.
6 Concluding Remarks
Preliminary experimental results suggest that the
regularized compression method, even only with
partial evidence, seems as effective as the state-of-
the-art methods in different language settings. When
paired with MDL criteria, regularized compression
is comparable to hierarchical Bayesian methods and
MDL-based algorithms in terms of segmentation ac-
curacy and computational efficiency. Furthermore,
regularized compression is less memory-demanding
than the other approaches; thus, it scales more easily
to large corpora for carrying out certain tasks such
as segmenting historical texts written in ancient lan-
guages, or preprocessing a large dataset for subse-
quent manual annotation.
We have identified a number of limitations of reg-
ular compression. First, the choice of candidate n-
grams does not cover hapax legomena, i.e., words
that occur only once in the corpus. At present, pre-
cluding these low-frequency n-grams seems to be
a necessary compromise due to our limited under-
standing about the dynamics behind regular com-
pression. Second, regularized compression does not
work well with low volume data, since on smaller
dataset the distribution of frequency counts is less
precise. Third, the algorithm may stop identifying
new word types at some point. We suspect that this
is related to the choice of n-gram, since in our im-
plementation no two existing ?words? can be aggre-
gated into one. These issues shall be addressed in
our future work.
Acknowledgments
We thank the anonymous reviewers for their valu-
able comments. The research efforts described in
this paper are supported under the National Tai-
wan University Digital Archives Project (Project
No. NSC-98-2631-H-002-005), which is sponsored
by National Science Council, Taiwan.
References
Shlomo Argamon, Navot Akiva, Amihood Amir, and
Oren Kapah. 2004. Efficient unsupervised recursive
word segmentation using minimum description length.
In Proceedings of the 20th international conference
on Computational Linguistics, COLING ?04, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Nan Bernstein-Ratner. 1987. The phonology of parent
child speech. Children?s language, 6:159?174.
33
Michael R. Brent and Timothy A. Cartwright. 1996. Dis-
tributional regularity and phonotactic constraints are
useful for segmentation. In Cognition, pages 93?125.
Thomas Emerson. 2005. The second international chi-
nese word segmentation bakeoff. In Proceedings of
the Fourth SIGHAN Workshop on Chinese Language
Processing, volume 133. Jeju Island, Korea.
Sharon Goldwater, Thomas L. Griffiths, and Mark John-
son. 2006. Contextual dependencies in unsupervised
word segmentation. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and the 44th annual meeting of the Association for
Computational Linguistics, ACL-44, pages 673?680,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Sharon Goldwater, Thomas L. Griffiths, and Mark John-
son. 2009. A bayesian framework for word segmen-
tation: Exploring the effects of context. Cognition,
112(1):21?54, July.
Daniel Hewlett and Paul Cohen. 2009. Bootstrap voting
experts. In Proceedings of the 21st international jont
conference on Artifical intelligence, IJCAI?09, pages
1071?1076, San Francisco, CA, USA. Morgan Kauf-
mann Publishers Inc.
Daniel Hewlett and Paul Cohen. 2011. Fully unsuper-
vised word segmentation with BVE and MDL. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies: short papers - Volume 2, HLT
?11, pages 540?545, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Zhihui Jin and Kumiko T. Ishii. 2006. Unsupervised seg-
mentation of chinese text by use of branching entropy.
In Proceedings of the COLING/ACL on Main confer-
ence poster sessions, COLING-ACL ?06, pages 428?
435, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Mark Johnson and Sharon Goldwater. 2009. Improving
nonparameteric bayesian inference: experiments on
unsupervised word segmentation with adaptor gram-
mars. In Proceedings of Human Language Technolo-
gies: The 2009 Annual Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics, NAACL ?09, pages 317?325, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Chunyu Kit and Yorick Wilks. 1999. Unsupervised
learning of word boundary with description length
gain. In CoNLL-99, pages 1?6, Bergen, Norway.
Gina-Anne Levow. 2006. The third international chinese
language processing bakeoff: Word segmentation and
named entity recognition. In Proceedings of the Fifth
SIGHAN Workshop on Chinese Language Processing,
volume 117. Sydney: July.
Brian MacWhinney and Catherine Snow. 1990. The
child language data exchange system: an update.
Journal of child language, 17(2):457?472, June.
Daichi Mochihashi, Takeshi Yamada, and Naonori Ueda.
2009. Bayesian unsupervised word segmentation with
nested Pitman-Yor language modeling. In Proceed-
ings of the Joint Conference of the 47th Annual Meet-
ing of the ACL and the 4th International Joint Confer-
ence on Natural Language Processing of the AFNLP:
Volume 1 - Volume 1, ACL ?09, pages 100?108,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Jorma Rissanen. 1978. Modeling by shortest data de-
scription. Automatica, 14(5):465?471, September.
Kumiko Tanaka-Ishii. 2005. Entropy as an indicator of
context boundaries: An experiment using a web search
engine. In Robert Dale, Kam-Fai Wong, Jian Su,
and Oi Kwong, editors, Natural Language Process-
ing IJCNLP 2005, volume 3651 of Lecture Notes in
Computer Science, chapter 9, pages 93?105. Springer
Berlin / Heidelberg, Berlin, Heidelberg.
Hua Yu. 2000. Unsupervised word induction using MDL
criterion. In Proceedings of the International Sympo-
sium of Chinese Spoken Language Processing, Beijin,
China.
Hai Zhao and Chunyu Kit. 2008. An empirical compar-
ison of goodness measures for unsupervised chinese
word segmentation with a unified framework. In The
Third International Joint Conference on Natural Lan-
guage Processing (IJCNLP-2008).
Valentin Zhikov, Hiroya Takamura, and Manabu Oku-
mura. 2010. An efficient algorithm for unsupervised
word segmentation with branching entropy and MDL.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, EMNLP
?10, pages 832?842, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
34
