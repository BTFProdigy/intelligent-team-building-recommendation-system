Chinese Syntactic Parsing Based on Extended GLR Parsing 
Algorithm with PCFG* 
 
Yan Zhang, Bo Xu and Chengqing Zong 
National Laboratory of Pattern Recognition, Institute of Automation 
Chinese Academy of sciences, Beijing 100080, P. R. China 
E-mail: {yzhang, xubo, cqzong}@nlpr.ia.ac.cn 
 
Abstract  
This paper presents an extended GLR 
parsing algorithm with grammar PCFG* that 
is based on Tomita?s GLR parsing algorithm 
and extends it further. We also define a new 
grammar?PCFG* that is based on PCFG 
and assigns not only probability but also 
frequency associated with each rule. So our 
syntactic parsing system is implemented 
based on rule-based approach and statistics 
approach. Furthermore our experiments are 
executed in two fields: Chinese base noun 
phrase identification and full syntactic 
parsing. And the results of these two fields 
are compared from three ways. The 
experiments prove that the extended GLR 
parsing algorithm with PCFG* is an 
efficient parsing method and a 
straightforward way to combine statistical 
property with rules. The experiment results 
of these two fields are presented in this 
paper. 
1. Introduction 
Recently the syntactic parsing system is one of 
significant components in natural language 
processing. Many parsing methods have been 
developed as the development of corpus 
linguistics and applications of linguistics. 
Tomita? GLR parsing (Tomita M., 1986, 1987) 
is the most general shift-reduce method of 
bottom-up parsing and widely used in syntactic 
parsing. Several methods are based on it. Lavie 
(Lavie A., 1996) used the GLR* parsing 
algorithm for spoken language system. It uses a 
finite-state probabilistic model to compute the 
action probabilities. Inui (Inui K. et al, 1997, 
1998) presented a formalization of probabilistic 
GLR (PGLR) parsing model which assigns a 
probability to each LR parsing action. To 
shallow parsing, many researchers have made 
experiments with identification of noun phrases. 
Abney (Abney S., 1991) used two level 
grammar rules to implement the noun phrase 
parsing through pure LR parsing algorithm.  
Some new methods based on GLR algorithm 
aim to capture action probabilities by statistics 
distribution and context relations. This paper 
combines rule approach and statistics approach 
simultaneously. Furthermore, based on GLR and 
PCFG, we present an extended GLR parsing and 
a new grammar PCFG* that provides the action 
probabilities to prune the meaningless branches 
in the parsing table. Our experiments are also 
made in two parts: Chinese base noun phrase 
parsing and Chinese full parsing. The former is a 
simplified formalization of full parsing and is 
relatively simpler than the latter. 
This paper includes four sections. Section 2 
presents a brief description of rule structure 
system-PCFG*. Section 3 gives our extended 
GLR parsing algorithm and the parsing 
processing. Section 4 shows the experiment 
results of our parser including Chinese base 
noun phrases (baseNP) identification and 
Chinese full syntactic parser. The conclusions 
are drawn in section 5. 
2. A New Grammar (PCFG*) and the 
Rule Structure 
Grammar system is one of the important pars of 
a parsing system. We explain it in detail in the 
following section.  
2.1 Structure of Rules 
The definition of symbols in our system inherits 
the classifications of Penn Chinese tree-bank 
(Xia F., 2000). There are totally 33 
part-of-speech tags, 23 syntactic tags and 26 
functional tags in the Chinese tree-bank tag set. 
The POS tags belong to terminal symbols, while 
others belong to non-terminal symbols.  
In the final rule base there are about 2000 rules 
and 400 rules learned from corpus for full 
parsing and base noun phrases identification 
respectively. The rules have the following 
format showed in table 1. 
num rule probability frequency
1 VCD?VV  
+VV 
0.754491 126 
2 VCP?
VV+VC 
0.545455 6 
3 VCP?
VV+VV 
0.454545 5 
    Table 1: the format of grammar rules 
In order to denote each rule explicitly, the mark 
?+? is used as the junction mark. In above 
examples, symbols VP, VCD and VCP are verb 
phrase and verb compounds. Symbols VV and 
VC stand for common verbs and copula ??? 
respectively. 
2.2  A New Grammar (PCFG*) 
Context-free grammars (CFGs) are widely used 
to describe the grammar structures in natural 
language processing. And probabilistic 
context-free grammars (PCFGs) directly add the 
probabilities to the rules. But it is sometimes not 
sufficient to only associate probability with each 
rule. So we define a new grammar 
system-PCFG*: each rule is assigned probability 
distribution and frequency distribution 
simultaneously. The probability number is the 
relative value since it is the percentage value in 
the rule group that have the same left sides. 
While the frequency number is the absolute 
value because it is the total numbers occurred in 
whole corpus. The probability property is the 
key value to full parsing. The probability 
attribute is superior to frequency attribute. 
A sample is presented to show how to use 
probability and frequency of a rule.  
Suppose there are three rules showed in table 2 
and the relations is displayed in figure 1. 
Rule F(r) P(r) 
X?A+C f1 p1=f1/(f1+f2)
X?A+B+C f2 >f1 p2=f2/(f1+f2)
Y?A+C f3 <f1 p3 =1>p1 
Table 2: the examples of rule 
A B
X Y
C  
Figure 1: structure of rules 
Suppose the input symbols contain A, B and C. 
When rule 1 and rule 3 simultaneously satisfy 
the reduce condition, rule 3 is executed and the 
left side item ?Y? is pushed to the stack because 
p3 is bigger than p1. To complete parsing, 
probability always has the priority to frequency. 
But to baseNP parsing, frequency is superior to 
probability attribution. Since f1>f3, rule1 is 
executed first. If f1 is equal to f3, then go on to 
compare probability.  
3. Parsing Algorithm 
The parsing algorithm is very significant as well 
as the grammar rules to the parsing system. We 
produce an extended GLR parsing algorithm 
based on the Tomita?s GLR parsing algorithm in 
our system.  
3.1 the Extended GLR Parsing Algorithm 
The GLR method augments the LR parser and 
overcomes the drawback of the LR parser. In 
fact, from the point of parsing algorithm, there 
are no clear differences between LR and GLR 
algorithm. In parsing processing, there are also 
four actions in GLR algorithm that are similar to 
the LR parsing. But GLR parsing algorithm 
admits multiple entries in the parsing table. Our 
extended GLR algorithm also permits that 
several shift and reduce actions exist in one 
branch in the parsing table simultaneously. So 
there are mainly two types of conflicts: 
shift-reduce conflict and reduce-reduce conflict. 
These conflicts are the most difficult problems 
of GLR algorithm. In the parsing process, when 
the conflicts between shift and reduce occur, the 
principle of our parsing method is that the 
reduce action is superior to the shift action.  
If only grammar rules are used to describe the 
context relations, they may produce many 
conflicts when several rules satisfy the 
conditions. So we use the grammar 
system--PCFG* to add statistical information. 
The probabilities distributions are associated 
with the rules to each parsing action and decide 
which step to continue.  
Therefore the extended GLR algorithm handles 
the conflicts with two steps: (1). The reduce 
action is always executed first, then the shift 
action. (2). When more than one reduce actions 
satisfy the conditions, probability and frequency 
decide the order of these reduce actions.  
 
3.2 Parsing Actions and Parsing Process 
3.2.1 Parsing Table and Actions 
The parsing table consists of two sub-tables: 
ACTION table and GOTO table that are 
constructed by the grammar rules. The GOTO 
table is not different from GLR table. Just 
ACTION table is modified a little. Figure 2 
shows the structure of the parsing table. 
ACTION GOTO State 
X1, X2, ?, Xi ,       # Y1, ?, 
Yj  
S0 Sh1  
S1  Re1  
?  Re-Sh  
Sn  Accept 
 
Figure 2: the parsing table 
 
The ACTION table contains four action 
sub-tables: Sh1, Re1, Re-Sh and Accept. They 
stand for shift part, reduce part, reduce-shift part 
and accept part respectively. Because the error 
action is similar to accept action, it is not 
explained here. The Re-Sh part is the key part in 
the table. It contains multiple entries while the 
others have no conflicts. In the Re-Sh part, the 
rules are firstly arranged according to the 
probabilities and then compared based on the 
frequencies. The maximum probability is put on 
the top. This sequence continues until the last 
rule with minimum probability. According to the 
order of Re-Sh sub-table, the parsing program is 
transformed to the corresponding state of the 
stack. This order suits for the full parsing. But to 
the base noun phrases identification, frequency 
is firstly compared. 
Since the ambiguities and conflicts existed in the 
Re-Sh sub-table, we give a limit that no more 
than 20 entries in the Re-Sh part. From the 
experiment results, it is better to select 20 rules 
as the branch limit in the parsing process 
because it not only permits the multiple entries, 
but also fits for the performance efficiency of 
our program.  
Since the parser uses PCFG*, it has strong 
control to handle action conflicts and rule 
ambiguities. The parsing process need to prune 
the meaningless parsing branches. Excessive 
pruning may cause the loss of some grammar 
rules and add the error opportunities. Reasonable 
pruning can improve efficiency. 
 
3.2.2 the Parsing Process 
We give the following the symbols definition 
and interpretation to explain the parsing process.  
Let ?#? denotes the start and the end of the input 
Chinese sentence. The system contains a list of 
stacks simultaneously. The parsing table 
contains two elements: state nodes and symbol 
nodes. The parsing stack includes state stack 
(StateStack, name in the program), symbol state 
(SymbolStack) and input stack (InputStack) 
whose pointers are ps, pb and pi respectively.  
Following algorithm is established for the 
shift-reduce parsing process. 
Input:  
An input Chinese words sequence W in which 
each word has its part-of-speech and a parsing 
table produced by grammar rules; 
Output:  
If the input word sequence W satisfies the 
grammar rules and is accepted according to the 
parsing table, then output the parsing result of W, 
otherwise give error result; 
 
Main Loop:  
It mainly consists of four parts: shift, reduce, 
accept and error in the parsing process.  
Repeat  
Begin 
s := *ps++;  //s is current state 
b := *pb++; //to the next symbol 
c := *pi++; //to the next input word 
if Action[reduce rule 
VtVnVnAA ?????? ,, ] = reduce(), 
then begin 
1) Pop |?| symbols from top of the symbol 
stack, and push the left side symbol A to 
the symbol state; 
2) Pop |?| symbols from top of the state 
stack, and push s* 
3) ps -= |?|; *ps := s*;  
 end reduce(); //reduce part 
  
else if Action[] = shift(input s*),  
then begin 
 pi++; *pi := s*; pb++; *pb := s*; 
end shift(); //shift part 
 
else if Action[] = accept() 
then Success and Output; //the parsing 
succeeds 
else 
     error(); // parsing is error here 
End 
Until: The input symbol is the end of the 
sentence. Or accept function occurs or error 
function occurs. 
 
(1) Reduce Action 
When the reduce action is performed, the rule 
candidates are selected in the list from the first 
rule to the last one that are arranged according to 
the probabilities and frequencies. If one of these 
rules satisfies the condition, then the flag of this 
rule is changed from FALSE to TRUE and stop 
here, and continue to read input word. Otherwise 
trace back.  
(2) Shift Action 
Shift action is executed under two conditions. 
One is based on the action table. The other is 
that when error action occurs, the base noun 
phrase identification continues to perform shift 
action while the full parsing enters trace part. 
(3) Error Action 
When error action occurs, trace back to the 
previous branch and perform another rule 
candidate listed in the entry. If there is no path 
can be searched in the current branch point or all 
routes are not passed through, the parsing fails 
and output the final error symbol. This situation 
is only used to the full parsing.   
3.2.3 the Comparison with GLR 
In order to explain explicitly our extended GLR 
parsing algorithm, we compare it with GLR 
algorithm. Table 3 gives the comparison results. 
    methods 
aspects 
GLR algorithm Our 
algorithm 
Grammar 
System 
CFG PCFG* 
Statistical 
Information 
no Probability, 
Frequency 
Data Structure Graph-Structured 
Stack 
Stack List 
Parsing 
Process 
Not simplified Pruning  
Other 
Attributes 
Augmentation to 
each rule 
no 
Table 3: Comparison with GLR  
4. Experiment and Results 
Our experiments include two parts: Chinese base 
noun phrase parsing and Chinese full syntactic 
parsing. 
The obvious difference of Chinese baseNP 
parsing and full parsing is that the former must 
give the parsing results while the latter 
sometimes need to trace back and output the 
error symbols. Because baseNP identification 
belongs to the shallow parsing, it only need to 
gives the recognized noun phrase structures. If 
there are no phrases found, then output the 
original sentence. Obviously Chinese baseNP 
parsing is much simpler and more efficient than 
the full parsing from the point of the method and 
the runtime.  
Our experiments are performed based on 
Chinese tree-bank corpus. There are totally 
10,000 Chinese sentences whose grammar 
structures are described by brackets. Table 4 
shows the characteristic of the corpus in the 
parsing process. 
Corpus 
 
Style 
Of Parsing 
Number of the 
Sentences. 
Average 
length of 
each 
sentence 
Training: 97% 22 words BaseNP 
Identification Testing: 3% 15 words 
Training: 98% 22 words Full Parsing 
Test: 2% 15 words 
Table 4: characteristic of corpus 
 
To two styles of parsing presented above, we 
give two types of results respectively.  
(1). Chinese BaseNP identification 
In our system, base noun phrases are defined to 
include not only pure noun phrase (NP) but also 
quantifier phrase (QP), such as QP ( ???/CD 
?/M ). 
To each Chinese sentence, baseNP identification 
always gives the final parsing results in which 
the base noun phrases are distinguished by 
brackets. Some samples are listed. 
1. ??/VV ?/AS  NP (??/NR ??/NN) 
?/DEG  NP(??/JJ ??/NN)  
2. (?/DT ?/M ??/NN ) ??/VV ?/AS 
(???/NN ???/NN) ?/DEG (?/JJ 
?/NN) 
There are two and three base noun phrases in 
sentence 1 and sentence 2 respectively.  
 
(2). Chinese full parsing 
Following sentences are the results of Chinese 
full parsing.  
1. VP (VP (??/VV ?/AS)  NP ( NP ( ??
/NR ??/NN )  ?/DEG  NP ( ??/JJ ?
?/NN ) ) ) 
2.IP ( NP (?/DT ?/M ??/NN )  VP( ??
/VV ?/AS)   NP ( NP (???/NN ???
/NN) ?/DEG  NP (?/JJ ?/NN)))) 
In order to display the parsing result clearly, 
sentence 2 is showed in the tree bank format. 
IP (NP ( DT   ? 
M   ? 
NN  ??) 
VP (VV ?? 
       AS  ?? 
NP ( NP ( NN ??? 
NN ???) 
        DEG ? 
NP ( JJ ? 
NN ?))) 
Type Precision 
(%) 
Recall  
(%) 
Num 
of 
Rules 
BaseNP 87.42 81.4 400 
Full 
parsing 
70.56 67.77 2000 
Table 5 is the results of these types of 
parsing. 
The experimental results show that our parsing 
algorithm, extended GLR parsing algorithm, is 
efficient to both Chinese baseNP parsing and 
full parsing. 
5. Conclusions 
In our system, we present the extended GLR 
parsing algorithm that is based on the Tomita?s 
GLR algorithm. A new grammar system PCFG* 
based on PCFG is proposed to describe the 
grammatical rules that are added probability and 
frequency attributes. So our parsing system 
combines Chinese grammar phenomena with 
statistics distribution. This is feasible and 
efficient to implement Chinese shallow parsing 
and full parsing. In the future task, we further 
improve the efficiency and robust of our parsing 
algorithm and expand Chinese grammatical rules 
with both statistical attributions and language 
information. It is important to utilize the results 
of base noun phrases identification and to 
improve the precision of Chinese full parsing.  
Acknowledgements 
The research work described in this paper is 
supported by the National Nature Science 
Foundation of China under grant number 
9835003 and the National Science Foundation of 
China under grand number 60175012 and the 
National Key Basic Research Program of China 
under grand number G1998030504. 
References  
Masaru Tomita, Efficient Parsing for Natural 
Language ? A Fast Algorithm for Practical 
Systems, Kluwer Academic Publishers, 1986 
Tomita M., an Efficient Augmented-Context-Free 
Algorithm, Computational Linguistics, Volume 13, 
Numbers 1-2, 1987 
Inui K., Sornlertlamvanich V., Tanaka H. and 
Tokunaga T., Probabilistic GLR Parsing: a New 
Formalization and Its Impact on parsing 
Performance, Journal of Natural Language 
Processing, Vol.5, No.3, pp.33-52, 1998 
Sornlertlamvanich V., Inui K., Tanaka H. and 
Tokunaga, T., A New Probabilistic LR Parsing, 
Proceedings of Annual Meeting of the Japan 
Association for Natural Language Processing,  
1997 
Lavie A., GLR*: A Robust Grammar-Focus Parser 
for Spontaneously Spoken Language, Ph.D. thesis, 
Carnegie Mellon University, USA, 1996 
Abney S., Parsing by Chunks, Kluwer Academic 
Publishers, 1991 
Xia F., the Segmentation Guidelines for the Penn 
Chinese Treebank (3.0), 2000 
 
Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pp. 940?946,
Prague, June 2007. c?2007 Association for Computational Linguistics
Probabilistic Parsing Action Models for Multi-lingual Dependency 
Parsing 
Xiangyu Duan 
Institute of Automation, Chi-
nese Academy of Sciences 
xyduan@nlpr.ia.ac.cn 
Jun Zhao 
Institute of Automation, Chi-
nese Academy of Sciences 
jzhao@nlpr.ia.ac.cn 
Bo Xu 
Institute of Automation, Chi-
nese Academy of Sciences 
xubo@hitic.ia.ac.cn 
 
 
Abstract 
Deterministic dependency parsers use pars-
ing actions to construct dependencies. 
These parsers do not compute the probabil-
ity of the whole dependency tree. They 
only determine parsing actions stepwisely 
by a trained classifier. To globally model 
parsing actions of all steps that are taken on 
the input sentence, we propose two kinds 
of probabilistic parsing action models that 
can compute the probability of the whole 
dependency tree. The tree with the maxi-
mal probability is outputted. The experi-
ments are carried on 10 languages, and the 
results show that our probabilistic parsing 
action models outperform the original de-
terministic dependency parser. 
1 Introduction 
The target of CoNLL 2007 shared task (Nivre et al, 
2007) is to parse texts in multiple languages by 
using a single dependency parser that has the ca-
pacity to learn from treebank data. Among parsers 
participating in CoNLL 2006 shared task 
(Buchholz et al, 2006), deterministic dependency 
parser shows great efficiency in time and compa-
rable performances for multi-lingual dependency 
parsing (Nivre et al, 2006). Deterministic parser 
regards parsing as a sequence of parsing actions 
that are taken step by step on the input sentence. 
Parsing actions construct dependency relations be-
tween words. 
Deterministic dependency parser does not score 
the entire dependency tree as most of state-of-the-
art parsers. They only stepwisely choose the most 
probable parsing action. In this paper, to globally 
model parsing actions of all steps that are taken on 
the input sentence, we propose two kinds of prob-
abilistic parsing action models that can compute 
the entire dependency tree?s probability. Experi-
ments are evaluated on diverse data set of 10 lan-
guages provided by CoNLL 2007 shared-task 
(Nivre et al, 2007). Results show that our prob-
abilistic parsing action models outperform the 
original deterministic dependency parser. We also 
present a general error analysis across a wide set of 
languages plus a detailed error analysis of Chinese. 
Next we briefly introduce the original determi-
nistic dependency parsing algorithm that is a basic 
component of our models. 
2 Introduction of Deterministic Depend-
ency Parsing 
There are mainly two representative deterministic 
dependency parsing algorithms proposed respec-
tively by Nivre (2003), Yamada and Matsumoto 
(2003). Here we briefly introduce Yamada and 
Matsumoto?s algorithm, which is adopted by our 
models, to illustrate deterministic dependency 
parsing. The other representative method of Nivre 
also parses sentences in a similar deterministic 
manner except different data structure and parsing 
actions. 
Yamada?s method originally focuses on unla-
beled dependency parsing. Three kinds of parsing 
actions are applied to construct the dependency 
between two focus words. The two focus words are 
the current sub tree?s root and the succeeding (right) 
sub tree?s root given the current parsing state. 
Every parsing step results in a new parsing state, 
which includes all elements of the current partially 
built tree. Features are extracted about these two 
focus words. In the training phase, features and the 
corresponding parsing action compose the training
940
 
 
 
 
 
 
 
 
 
 
 
 
 
He provides confirming evidence RIGHT
He
provides confirming evidence
SHIFT
LEFT
RIGHTconfirming 
He 
provides evidence provides evidence 
He confirming 
provides 
He evidence 
confirming 
Figure 1. The example of the parsing process of Yamada and Matsumoto?s method. The input sentence 
is ?He provides confirming evidence.? 
 
data. In the testing phase, the classifier determines 
which parsing action should be taken based on the 
features. The parsing algorithm ends when there is 
no further dependency relation can be made on the 
whole sentence. The details of the three parsing 
actions are as follows: 
LEFT: it constructs the dependency that the 
right focus word depends on the left focus word. 
RIGHT: it constructs the dependency that the 
left focus word depends on the right focus word. 
SHIFT: it does not construct dependency, just 
moves the parsing focus. That is, the new left focus 
word is the previous right focus word, whose suc-
ceeding sub tree?s root is the new right focus word. 
The illustration of these three actions and the 
parsing process is presented in figure 1. Note that 
the focus words are shown as bold black box. 
We extend the set of parsing actions to do la-
beled dependency parsing. LEFT and RIGHT are 
concatenated by dependency labels, while SHIFT 
remains the same. For example in figure 1, the 
original action sequence ?RIGHT -> SHIFT -> 
RIGHT -> LEFT? becomes ?RIGHT-SBJ -> 
SHIFT -> RIGHT-NMOD -> LEFT-OBJ?. 
3 Probabilistic Parsing Action Models 
Deterministic dependency parsing algorithms are 
greedy. They choose the most probable parsing 
action at every parsing step given the current pars-
ing state, and do not score the entire dependency 
tree. To compute the probability of whole depend-
ency tree, we propose two kinds of probabilistic 
models that are defined on parsing actins: parsing 
action chain model (PACM) and parsing action 
phrase model (PAPM). 
3.1 Parsing Action Chain Model (PACM) 
The parsing process can be viewed as a Markov 
Chain. At every parsing step, there are several can-
didate parsing actions. The objective of this model 
is to find the most probable sequence of parsing 
actions by taking the Markov assumption. As 
shown in figure 1, the action sequence ?RIGHT-
SBJ -> SHIFT -> RIGHT-NMOD -> LEFT-
OBJ? constructs the right dependency tree of the 
example sentence. Choosing this action sequence 
among all candidate sequences is the objective of 
this model.  
Firstly, we should define the probability of the 
dependency tree conditioned on the input sentence. 
)1(),...|()|(
...1
10?
=
?=
ni
ii SdddPSTP  
Where T denotes the dependency tree, S denotes 
the original input sentence,  denotes the parsing 
action at time step i. We add an artificial parsing 
action  as initial action. 
id
0d
We introduce a variable  to denote the 
resulting parsing state when the action  is taken 
on .  is the original input sen-
tence. 
id
context
id
1?idcontext 0dcontext
Suppose  are taken sequentially on the 
input sentence S, and result in a sequence of pars-
ing states , then P(T|S) de-
fined in equation (1) becomes as below: 
ndd ...0
ndd
contextcontext ...
0
941
)4()|(
)3()|(
)2(),...,|(
...1
...1
...1
1
1
10
?
?
?
=
=
=
?
?
?
=
?
ni
di
ni
dd
ni
ddd
i
ii
ii
contextdP
contextcontextP
contextcontextcontextP
 
Formula (3) comes from formula (2) by obeying 
the Markov assumption. Note that formula (4) is 
about the classifier of parsing actions. It denotes 
the probability of the parsing action given the 
parsing state . If we train a classifier 
that can predict with probability output, then we 
can compute P(T|S) by computing the product of 
the probabilities of parsing actions. The classifier 
we use throughout this paper is SVM (Vapnik, 
1995). We adopt Libsvm (Chang and Lin, 2005), 
which can train multi-class classifier and support 
training and predicting with probability output 
(Chang and Lin, 2005). 
id
1?idcontext
For this model, the objective is to choose the 
parsing action sequence that constructs the de-
pendency tree with the maximal probability. 
)5()|(max)|(max
...1
... 11
?
= ?
=
ni
didd in
contextdPSTP  
Because this model chooses the most probable 
sequence, not the most probable parsing action at 
only one step, it avoids the greedy property of the 
original deterministic parsers. 
We use beam search for the decoding of this 
model. We use m to denote the beam size. Then 
beam search is carried out as follows. At every 
parsing step, all parsing states are ordered (or par-
tially m ordered) according to their probabilities. 
Probability of a parsing state is determined by 
multiplying the probabilities of actions that gener-
ate that state. Then we choose m best parsing 
states for this step, and next parsing step only con-
sider these m best parsing states. Parsing termi-
nates when the first entire dependency tree is con-
structed. To obtain a list of n-best parses, we sim-
ply continue parsing until either n trees are found, 
or no further parsing can be fulfilled. 
3.2 Parsing Action Phrase Model (PAPM) 
In the Parsing Action Chain Model (PACM), ac-
tions are competing at every parsing step. Only m 
best parsing states resulted by the corresponding 
actions are kept for every step. But for the parsing 
problem, it is reasonable that actions are competing 
for which phrase should be built. For dependency 
syntax, one phrase consists of the head word and 
all its children. Based on this motivation, we pro-
pose Parsing Action Phrase Model (PAPM), which 
divides parsing actions into two classes: construct-
ing action and shifting action. 
If a phrase is built after an action is performed, 
the action is called constructing action. In original 
Yamada?s algorithm, constructing actions are 
LEFT and RIGHT. For example, if LEFT is taken, 
it indicates that the right focus word has found all 
its children and becomes the head of this new 
phrase. Note that one word with no children can 
also be viewed as a phrase if its dependency on 
other word is constructed. In the extended set of 
parsing actions for labeled parsing, compound ac-
tions, which consist of LEFT and RIGHT con-
catenated by dependency labels, are constructing 
actions. 
If no phrase is built after an action is performed, 
the action is called shifting action. Such action is 
SHIFT. 
We denote  as constructing action and  as 
shifting action. j indexes the time step. Then we 
introduce a new concept: parsing action phrase. 
We use  to denote the ith parsing action phrase. 
It can be expanded as . That is, 
parsing action phrase  is a sequence of parsing 
actions that constructs the next syntactic phrase. 
ja jb
iA
jjkji abbA 1... ???
iA
For example, consider the parsing process in 
figure 1,  is ?RIGHT-SBJ?,  is ?SHIFT, 
RIGHT-NMOD?,  is ?LEFT-OBJ?. Note that 
 consists of a constructing action,  consists 
of a shifting action and a constructing action,  
consists of a constructing action. 
1A 2A
3A
1A 2A
3A
The indexes are different for both sides of the 
expansion ,  is the ith parsing 
action phrase corresponding to both constructing 
action  at time step j and all its preceding shift-
ing actions. Note that on the right side of the ex-
pansion, only one constructing action is allowed 
and is always at the last position, while shifting 
action can occur several times or does not occur at 
all. It is parsing action phrases, i.e. sequences of 
parsing actions, that are competing for which next 
phrase should be built. 
jjkji abbA 1... ??? iA
ja
942
The probability of the dependency tree given the 
input sentence is redefined as: 
)|())|((
)|(
)|...(
)|(
)|(
)...|(
)6(),...|()|(
1
1
1
1
1
11
...2
1
...1
...1
1
...1
...1
...1
...1
11
??
?
?
?
?
?
?
?=
=
=
?
=
=
?
?
?
?
?
?
?
=
+?
=
?
=
??
=
=
=
=
?
jtj
i
i
i
ii
ii
bj
kt
btj
ni
Akj
ni
Ajjkj
ni
Ai
ni
AA
ni
AAA
ni
ii
contextaPcontextbP
contextbP
contextabbP
contextAP
contextcontextP
contextcontextcontextP
SAAAPSTP
 
Where k represents the number of steps that shift-
ing action can be taken.  is the parsing 
state resulting from a sequence of actions 
 taken on . 
iA
context
jjkj abb 1... ?? 1?iAcontext
The objective in this model is to find the most 
probable sequence of parsing action phrases. 
)7()|(max)|(max
...1
... 11
?
= ?
=
ni
AiAA in
contextAPSTP  
Similar with parsing action chain model 
(PACM), we use beam search for the decoding of 
parsing action phrase model (PAPM). The differ-
ence is that PAPM do not keep m best parsing 
states at every parsing step. Instead, PAPM keep m 
best states which are corresponding to m best cur-
rent parsing action phrases (several steps of 
SHIFT and the last step of a constructing action). 
4 Experiments and Results 
Experiments are carried on 10 languages provided 
by CoNLL 2007 shared-task organizers (Nivre et 
al., 2007). Among these languages, Chinese (Chen 
et al, 2003), Catalan (Mart? et al, 2007) and Eng-
lish (Johansson and Nugues, 2007) have low per-
centage of non-projective relations, which are 
0.0%, 0.1% and 0.3% respectively. Except these 
three languages, we use software of projectiviza-
tion/deprojectivization provided by Nivre and 
Nilsson (2005) for other languages. Because our 
algorithm only deals with projective parsing, we 
should projectivize training data at first to prepare 
for the following training of our algorithm. During 
testing, deprojectivization is applied to the output 
of the parser. 
Considering the classifier of Libsvm (Chang and 
Lin, 2005), the features are extracted from the fol-
lowing fields of the data representation: FORM, 
LEMMA, CPOSTAG, POSTAG, FEATS and DE-
PREL. We split values of FEATS field into its 
atomic components. We only use available features 
of DEPREL field during deterministic parsing. We 
use similar feature context window as used in Ya-
mada?s algorithm (Yamada and Matsumoto, 2003). 
In detail, the size of feature context window is six, 
which consists of left two sub trees, two focus 
words related sub trees and right two sub trees. 
This feature template is used for all 10 languages. 
4.1 Results of PACM and Yamada?s Method 
After submitting the testing results of Parsing Ac-
tion Chain Model (PACM), we also perform origi-
nal deterministic parsing proposed by Yamada and 
Matsumoto (2003). The total results are shown in 
table 1. The experimental results are mainly evalu-
ated by labeled attachment score (LAS), unlabeled 
attachment score (UAS) and labeled accuracy (LA). 
Table 1 shows that Parsing Action Chain Model 
(PACM) outperform original Yamada?s parsing 
method for all languages. The LAS improvements 
range from 0.60 percentage points to 1.71 percent-
age points. Note that the original Yamada?s 
method still gives testing results above the official 
reported average performance of all languages. 
 Ara Bas Cat Chi Cze Eng Gre Hun Ita Tur 
YamLAS  69.31 69.67 83.26 81.88 74.63 84.81 72.75 76.24 80.08 73.94
YamUAS  78.93 75.86 88.53 86.17 80.11 85.83 79.45 79.97 83.69 79.79
YamLA  81.13 75.71 88.36 84.56 82.10 89.71 82.58 88.37 86.93 80.81
PACMLAS  69.91 71.26 84.95 82.58 75.34 85.83 74.29 77.06 80.75 75.03
PACMUAS  79.04 77.57 89.71 86.88 80.82 86.97 80.77 80.66 84.20 81.03
PACMLA  81.40 77.35 89.55 85.35 83.17 90.57 83.87 88.92 87.32 81.17
Table 1. The performances of Yamada?s method (Yam) and Parsing Action Chain Model (PACM). 
 
943
4.2 Results of PAPM 
Not all languages have only one root node of a 
sentence. Since Parsing Action Phrase Model 
(PAPM) only builds dependencies, and shifting 
action is not the ending action of a parsing action 
phrase, PAPM always ends with one root word. 
This property makes PAPM only suitable for 
Catalan, Chinese, English and Hungarian, which 
are unary root languages. PAPM result of Catalan 
was not submitted before deadline due to the    
shortage of time and computing resources. We 
report Catalan?s PAPM result together with that of 
other three languages in table 2.  
 
 Cat Chi Eng Hun 
PAPMLAS  87.26 82.64 86.69 76.89 
PAPMUAS  92.07 86.94 87.87 80.53 
PAPMLA  91.89 85.41 92.04 89.73 
Table 2. The performance of Parsing Action 
Phrase Model (PAPM) for Catalan, Chinese, Eng-
lish and Hungarian. 
 
Compared with the results of PACM shown in 
table 1, the performance of PAPM differs among 
different languages. Catalan and English show 
that PAPM improves 2.31% and 0.86% respec-
tively over PACM, while the improvement of Chi-
nese is marginal, and there is a little decrease of 
Hungarian. Hungarian has relatively high percent-
age of non-projective relations. If phrase consists 
of head word and its non-projective children, the 
constructing actions that are main actions in 
PAPM will be very difficult to be learned because 
some non-projective children together with their 
heads have no chance to be simultaneously as fo-
cus words. Although projectivization is also per-
formed for Hungarian, the built-in non-projective 
property still has negative influence on the per-
formance. 
5 Error Analysis 
In the following we provide a general error analy-
sis across a wide set of languages plus a detailed 
analysis of Chinese. 
5.1 General Error Analysis 
One of the main difficulties in dependency parsing 
is the determination of long distance dependencies. 
Although all kinds of evaluation scores differ 
dramatically among different languages, 69.91% 
to 85.83% regarding LAS, there are some general 
observations reflecting the difficulty of long dis-
tance dependency parsing. We study this difficulty 
from two aspects about our full submission of 
PACM: precision of dependencies of different arc 
lengths and precision of root nodes. 
For arcs of length 1, all languages give high 
performances with lowest 91.62% of Czech 
(B?hmova et al, 2003) to highest 96.8% of Cata-
lan (Mart? et al, 2007). As arcs lengths grow 
longer, various degradations are caused. For Cata-
lan, score of arc length 2 is similar with that of arc 
length 1, but there are dramatic degradations for 
longer arc lengths, from 94.94% of arc length 2 to 
85.22% of length 3-6. For English (Johansson and 
Nugues, 2007) and Italian (Montemagni et al, 
2003), there are graceful degradation for arcs of 
length 1,2 and 3-6, with 96-91-85 of English and 
95-85-75 of Italian. For other languages, long arcs 
also give remarkable degradations that pull down 
the performance. 
Precision of root nodes also reflects the per-
formance of long arc dependencies because the 
arc between the root and its children are often 
long arcs. In fact, it is the precision of roots and 
arcs longer than 7 that mainly pull down the over-
all performance. Yamada?s method is a bottom-up 
parsing algorithm that builds short distance de-
pendencies at first. The difficulty of building long 
arc dependencies may partially be resulted from 
the errors of short distance dependencies. The de-
terministic manner causes error propagation, and 
it indirectly indicates that the errors of roots are 
the final results of error propagation of short dis-
tance dependencies. But there is an exception oc-
curred in Chinese. The root precision is 90.48%, 
only below the precision of arcs of length 1. This 
phenomenon exists because the sentences in Chi-
nese data set (Chen et al, 2003) are in fact clauses 
with average length of 5.9 rather than entire sen-
tences. The root words are heads of clauses. 
Both Parsing Action Chain Model (PACM) and 
Parsing Action Phrase Model (PAPM) avoid 
greedy property of original Yamada?s method. It 
can be expected that there will be a precision im-
provement of long distance dependencies over 
original Yamada?s method. For PACM, the results 
of Basque (Aduriz et al, 2003), Catalan (Mart? et 
al., 2007), Chinese (Chen et al, 2003), English 
(Johansson and Nugues, 2007) and Greek (Pro-
944
kopidis et al, 2005) show that the root precision 
improvement over Yamada?s method is more con-
spicuous than that of other long distance depend-
encies. The largest improvement of roots precision 
is 10.7% of Greek. While for Arabic (Hajic et al, 
2004), Czech (B?hmova et al, 2003), Hungarian 
(Csendes et al, 2005), Italian (Montemagni et al, 
2003) and Turkish (Oflazer et al, 2003), the im-
provement of root precision is small, but depend-
encies of arcs longer than 1 give better scores. For 
PAPM, good performances of Catalan and English 
also give significant improvements of root preci-
sion over PACM. For Catalan, the root precision 
improvement is from 63.86% to 95.21%; for Eng-
lish, the root precision improvement is from 
62.03% to 89.25%. 
5.2 Error Analysis of Chinese 
There are mainly two sources of errors regarding 
LAS in Chinese dependency parsing. 
One is from conjunction words (C) that have a 
relatively high percentage of wrong heads (about 
20%), and therefore 19% wrong dependency la-
bels. In Chinese, conjunction words often con-
catenate clauses. Long distance dependencies be-
tween clauses are bridged by conjunction words. 
It is difficult for conjunction words to find their 
heads. 
The other source of errors comes from auxiliary 
words (DE) and preposition words (P). Unlike 
conjunction words, auxiliary words and preposi-
tion words have high performance of finding right 
head, but label accuracy (LA) decrease signifi-
cantly. The reason may lie in the large depend-
ency label set consisting of 57 kinds of depend-
ency labels in Chinese. Moreover, auxiliary words 
(DE) and preposition words (P) have more possi-
ble dependency labels than other coarse POS have. 
This introduces ambiguity for parsers. 
Most common POS including noun and verb 
contribute much to the overall performance of 
83% Labeled Attachment Scores (LAS). Adverbs 
obtain top score while adjectives give the worst. 
6 Conclusion 
We propose two kinds of probabilistic models 
defined on parsing actions to compute the prob-
ability of entire sentence. Compared with original 
Yamada and Matsumoto?s deterministic depend-
ency method which stepwisely chooses most 
probable parsing action, the two probabilistic 
models improve the performance regarding all 10 
languages in CoNLL 2007 shared task. Through 
the study of parsing results, we find that long dis-
tance dependencies are hard to be determined for 
all 10 languages. Further analysis about this diffi-
culty is needed to guide the research direction. 
Feature exploration is also necessary to provide 
more informative features for hard problems. 
Ackowledgements 
This work was supported by Hi-tech Research and 
Development Program of China under grant No. 
2006AA01Z144, the Natural Sciences Foundation 
of China under grant No. 60673042, and the Natu-
ral Science Foundation of Beijing under grant No. 
4052027, 4073043. 
References 
S. Buchholz, E. Marsi, A. Dubey, and Y. Krymolowski. 
2006. CoNLL-X shared task on multilingual de-
pendency parsing. SIGNLL. 
Chih-Chung Chang and Chih-Jen Lin. 2005. LIBSVM: 
A library for support vector machines. 
J. Nivre. 2003. An efficient algorithm for projective 
dependency parsing. In Proceedings of the 8th In-
ternational Workshop on Parsing Technologies 
(IWPT). 
J. Nivre and J. Nilsson. 2005. Pseudo-projective de-
pendency parsing. In Proc. of ACL-2005, pages 99?
106. 
J. Nivre, J. Hall, J. Nilsson, G. Eryigit, S. Marinov. 
2006. Labeled Pseudo-Projective Dependency 
Parsing with Support Vector Machines. In Proc. of 
the Tenth Conference on Computational Natural 
Language Learning (CoNLL). 
J. Nivre, J. Hall, S. K?bler, R. McDonald, J. Nilsson, S. 
Riedel, and D. Yuret. 2007. The CoNLL 2007 
shared task on dependency parsing. In Proc. of the 
Joint Conf. on Empirical Methods in Natural 
Language Processing and Computational Natural 
Language Learning (EMNLP-CoNLL). 
H. Yamada and Y. Matsumoto. 2003. Statistical de-
pendency analysis with support vector machines. In 
Proceedings of the 8th International Workshop on 
Parsing Technologies (IWPT). 
V. Vapnik. 1995. The Nature of StatisticalLearning 
Theory. Springer. 
945
A. Abeill?, editor. 2003. Treebanks: Building and 
Using Parsed Corpora. Kluwer.  
I. Aduriz, M. J. Aranzabe, J. M. Arriola, A. Atutxa, A. 
Diaz de Ilarraza, A. Garmendia and M. Oronoz. 
2003. Construction of a Basque Dependency Tree-
bank. In Proc. of the 2nd Workshop on Treebanks 
and Linguistic Theories (TLT), pages 201?204. 
A. B?hmov?, J. Hajic, E. Hajicov? and B. Hladk?. 
2003. The PDT: a 3-level annotation scenario. In 
Abeill? (2003), chapter 7, 103?127. 
K. Chen, C. Luo, M. Chang, F. Chen, C. Chen, C. 
Huang and Z. Gao. 2003. Sinica Treebank: Design 
Criteria, Representational Issues and Implementa-
tion. In Abeill? (2003), chapter 13, pages 231?248. 
D. Csendes, J. Csirik, T. Gyim?thy, and A. Kocsor. 
2005. The Szeged Treebank. Springer.  
J. Hajic, O. Smrz, P. Zem?nek, J. Snaidauf and E. 
Beska. 2004. Prague Arabic Dependency Treebank: 
Development in Data and Tools. In Proc. of the 
NEMLAR Intern. Conf. on Arabic Language Re-
sources and Tools, pages 110?117. 
R. Johansson and P. Nugues. 2007. Extended 
constituent-to-dependency conversion for English. 
In Proc. of the 16th Nordic Conference on 
Computational Linguistics (NODALIDA).  
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993. 
Building a large annotated corpus of English: the 
Penn Treebank. Computational Linguistics, 
19(2):313?330. 
M. A. Mart?, M. Taul?, L. M?rquez and M. Bertran. 
2007. CESS-ECE: A Multilingual and Multilevel 
Annotated Corpus. Available for download from: 
http://www.lsi.upc.edu/~mbertran/cess-ece/. 
S. Montemagni, F. Barsotti, M. Battista, N. Calzolari, 
O. Corazzari, A. Lenci, A. Zampolli, F. Fanciulli, M. 
Massetani, R. Raffaelli, R. Basili, M. T. Pazienza, D. 
Saracino, F. Zanzotto, N. Nana, F. Pianesi, and R. 
Delmonte. 2003. Building the Italian Syntactic-
Semantic Treebank. In Abeill? (2003), chapter 11, 
pages 189?210.  
J. Nivre, J. Hall, S. K?bler, R. McDonald, J. Nilsson, S. 
Riedel, and D. Yuret. 2007. The CoNLL 2007 
shared task on dependency parsing. In Proc. of the 
CoNLL 2007 Shared Task. Joint Conf. on Empirical 
Methods in Natural Language Processing and 
Computational Natural Language Learning 
(EMNLP-CoNLL). 
K. Oflazer, B. Say, D. Zeynep Hakkani-T?r, and G. 
T?r. 2003. Building a Turkish treebank. In Abeill? 
(2003), chapter 15, pages 261?277.  
P. Prokopidis, E. Desypri, M. Koutsombogera, H. 
Papageorgiou, and S. Piperidis. 2005. Theoretical 
and practical issues in the construction of a Greek 
depen- dency treebank. In Proc. of the 4th 
Workshop on Treebanks and Linguistic Theories 
(TLT), pages 149?160. 
946
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 427?434, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Chinese Named Entity Recognition Based on Multiple Features 
 
 
Youzheng Wu, Jun Zhao, Bo Xu Hao Yu 
National Laboratory of Pattern Recognition Fujitsu R&D Center Co., Ltd 
Institute of Automation, CAS Beijing 100016, China 
Beijing, 100080, China yu@frdc.fujitsu.com 
(yzwu,jzhao,bxu)@nlpr.ia.ac.cn  
 
 
 
 
Abstract 
This paper proposes a hybrid Chinese 
named entity recognition model based on 
multiple features. It differentiates from 
most of the previous approaches mainly 
as follows. Firstly, the proposed Hybrid 
Model integrates coarse particle feature 
(POS Model) with fine particle feature 
(Word Model), so that it can overcome 
the disadvantages of each other. Secondly, 
in order to reduce the searching space and 
improve the efficiency, we introduce heu-
ristic human knowledge into statistical 
model, which could increase the perform-
ance of NER significantly. Thirdly, we 
use three sub-models to respectively de-
scribe three kinds of transliterated person 
name, that is, Japanese, Russian and 
Euramerican person name, which can im-
prove the performance of PN recognition. 
From the experimental results on People's 
Daily testing data, we can conclude that 
our Hybrid Model is better than the mod-
els which only use one kind of features. 
And the experiments on MET-2 testing 
data also confirm the above conclusion, 
which show that our algorithm has consis-
tence on different testing data. 
1 Introduction 
Named Entity Recognition (NER) is one of the key 
techniques in the fields of Information Extraction, 
Question Answering, Parsing, Metadata Tagging in 
Semantic Web, etc. In MET-2 held in conjunction 
with the Seventh Message Understanding Confer-
ence (MUC-7), the task of NER is defined as rec-
ognizing seven sub-categories entities: person (PN), 
location (LN), organization (ON), time, date, cur-
rency and percentage. As for Chinese NEs, we fur-
ther divide PN into five sub-classes, that is, 
Chinese PN (CPN), Japanese PN (JPN), Russian 
PN (RPN), Euramerican PN (EPN) and abbrevi-
ated PN (APN) like "???/Mr. Wu". Similarly, 
LN is split into common LN (LN) like "???
/Zhongguancun" and abbreviated LN (ALN) such 
as "?/Beijing", "?/Shanghai". The recognition of 
time (TM) and numbers (NM) is comparatively 
simpler and can be implemented via finite state 
automata. Therefore, our research focuses on the 
recognition of CPN, JPN, RPN, EPN, APN, LN, 
ALN and ON. 
Compared to English NER, Chinese NER is 
more difficult. We think that the main differences 
between Chinese NER and English NER lie in: (1) 
Unlike English, Chinese lacks the capitalization 
information which can play very important roles in 
identifying named entities. (2) There is no space 
between words in Chinese, so we have to segment 
the text before NER. Consequently, the errors in 
word segmentation will affect the result of NER. 
In this paper, we proposes a hybrid Chinese 
NER model based on multiple features which em-
phasizes on (1) combining fine particle features 
(Word Model) with coarse particle features (POS 
Model); (2) integrating human knowledge into sta-
tistical model; (3) and using diverse sub-models 
for different kinds of entities. Especially, we divide 
transliterated person name into three sub-classes 
according to their characters set, that is, JPN, RPN 
and EPN. In order to deduce the complexity of the 
model and the searching space, we divide the rec-
427
ognition process into two steps: (1) word segmen-
tation and POS tagging; (2) named entity recogni-
tion based on the first step. 
Trained on the NEs labeled corpus of five-
month People's Daily corpus and tested on one-
month People's Daily corpus, the Hybrid Model 
achieves the following performance. The precision 
and the recall of PN (including CPN, JPN, RPN, 
EPN, AP N), LN (including ALN) and ON are re-
spectively (94.06%, 95.21%), (93.98%, 93.48%), 
and (84.69%, 86.86%). From the experimental re-
sults on People's Daily testing data, we can con-
clude that our Hybrid Model is better than other 
models which only use one kind of features. And 
the experiments on MET-2 testing data also con-
firm the above conclusion, which show that our 
algorithm has consistence on different testing data. 
2 Related Work 
On the impelling of international evaluations like 
MUC, CoNLL, IEER and ACE, the researches on 
English NER have achieved impressive results. For 
example, the best English NER system[Chinchor. 
1998] in MUC7 achieved 95% precision and 92% 
recall. However, Chinese NER is far from mature. 
For example, the performance (precision, recall) of 
the best Chinese NER system in MET-2 is (66%, 
92%), (89%, 91%), (89%, 88%) for PN, LN and 
ON respectively.  
Recently, approaches for NER are a shift away 
from handcrafted rules[Grishman, et al 1995] 
[Krupka, et al 1998][Black et al 1998] towards 
machine learning algorithms, i.e. unsupervised 
model like DL-CoTrain, CoBoost[Collins, 1999, 
2002], supervised learning like Error-driven [Ab-
erdeen, et al 1995], Decision Tree [Sekine, et al 
1998], HMM[Bikel, et al 1997] and Maximum 
Entropy[Borthwick, et al 1999][Mikheev, et 
al.1998].  
Similarly, the models for Chinese NER can also 
be divided into two categories: Individual Model 
and Integrated Model.  
Individual Model[Chen, et al 1998][Sun, et al 
1994][Zheng, et al 2000] consists of several sub-
models, each of them deals with a kind of entities. 
For example, the recognition of PN may be statis-
tical-based model, while LN and ON may be rule-
based model like [Chen, et al 1998]. Integrated 
Model[Sun, et al 2002] [Zhang, et al 2003][Yu, et 
al. 1998][Chua, et al 2002] deals with all kinds of 
entities in a unified statistical framework. Most of 
these integrated models can be viewed as a HMM 
model. The differences among them are the defini-
tion of state and the features used in entity model 
and context model.  
In fact, a NER model recognizes named entities 
through mining the intrinsic features in the entities 
and the contextual features around the entities. 
Most of existing approaches employ either coarse 
particle features, like POS and ROLE[Zhang, et al 
2003], or fine particle features like word. The data 
sparseness problem is serious if only using fine 
particle features, and coarse particle features will 
lose much important information though without 
serious data sparseness problem. Our idea is that 
coarse particle features should be integrated into 
fine particle features to overcome the disadvan-
tages of them. However, most systems do not com-
bine them and especially ignore the impact of POS. 
Inspired by the algorithms of identifying 
BaseNP and Chunk[Xun, et al 2000], we propose 
a hybrid NER model which emphasizes on com-
bining coarse particle features (POS Model) with 
fine particle features (Word Model). Though the 
Hybrid Model can overcome the disadvantages of 
the Word Model and the POS Model, there are still 
some problems in such a framework. Data sparse-
ness still exists and very large searching space in 
decoding will influence efficiency. Our idea is that 
heuristic human knowledge can not only improve 
the time efficiency, but also solve the data sparse-
ness problem to some extent by restricting the gen-
eration of entity candidates. So we intend to 
incorporate human knowledge into the statistical 
model to improve efficiency and effectivity of the 
Hybrid Model.  
Similarly, for capturing intrinsic features in dif-
ferent types of entities, we design several sub-
models for each kind of entities. For example, we 
divide transliterated person name into three sub-
classes according to their characters sets, that is, 
JPN, RPN and EPN. 
3 Chinese NER with Multiple Features 
Chinese NEs have very distinct word features in 
their composition and contextual information. For 
example, about 365 highest frequently used sur-
names cover 99% Chinese surnames[Sun, et al 
1994]. Similarly the characters used for transliter-
ated names are also limited. LNs and ONs often 
428
end with the specific words like "?/province" and 
"??/company". However, data sparseness is very 
serious when using word features. So we try to 
introduce coarse particle feature to overcome the 
data sparseness problem. POS features are simplest 
and easy to obtain. Therefore, our hybrid model 
combines word feature with POS feature to recog-
nize Chinese NEs. 
Given a word/pos sequence as equation (1): 
nnii twtwtwTW //// 11 LL=                    (1) 
where n is the number of words and ti is the POS 
of word wi. The task of Chinese NE identification 
is to find the optimal sequence WC*/ TC* by split-
ting, combining and classifying the sequence of (1). 
mmii21 tc/wctc/wctc/wc*TC/*WC LL=     (2) 
where [ ]ljji wwwc += L , [ ]ljji tttc += L , nm ? . 
Note that the definition of words in {wi} set is 
that each kind of NEs (including PN, APN, LN, 
ALN, ON, TM, NM) is defined as a word and all 
the other words in the vocabulary are also defined 
as individual words. Consequently, {wi} set has 
|V|+7 words, where |V| is the size of vocabulary. 
The size of {ti} set is 48 which include PKU POS 
tagging set1 and each kind of NEs. 
Obviously, we could obtain the optimal se-
quence WC*/TC* through the following three 
models: the Word Model, the POS Model and the 
Hybrid Model.  
The Word Model employs word features for 
NER, which is introduced by [Sun, et al 2002]. 
The POS Model employs POS features for NER. 
This paper proposes a Hybrid Model which com-
bines word features with POS features.  
We will describe these models in detail in fol-
lowing section. 
3.1 The Hybrid Model 
For the convenience of description, we take apart 
equation (1) into two components: word sequence 
as equation (3) and POS sequence as (4).  
ni21 wwwwW LL=                                     (3) 
ni21 ttttT LL=                                          (4) 
The Word Model estimates the probability of 
generating a NE from the viewpoint of word se-
quence, which can be expressed in equation (5).  
                                                          
1 http://icl.pku.edu.cn/nlp-tools/catetkset.html 
( ) ( )WC|WPWCPargmax*WC wc=                  (5) 
The POS Model estimates the probability of 
generating a NE from the viewpoint of POS se-
quence, which can be expressed in equation (6). 
( ) ( )TC|TPTCPargmax*TC TC=                      (6) 
Our proposed Hybrid Model combines the Word 
Model with the POS Model, which can be ex-
pressed in the equation (7). 
( )
( ) ( )
( ) ( ) ( )
( ) ( )
( ) ( ) ( ) ( ) ( ) ?]TCPTC|T[PWCPWC|WPargmax
W,TWC,TC,Pargmax
T,WPW,TWC,TC,Pargmax
W,T|WC,TCPargmax
*TC*,WC
TCWC,
TCWC,
TCWC,
TCWC,
?
=
=
=
  (7) 
where factor ? > 0 is to balance the Word Model 
and the POS Model. 
Therefore, the Hybrid Model consists of four 
sub-models: word context model P(WC), POS con-
text model P(TC), word entity model P(W|WC) 
and POS entity model P(T|TC). 
3.2 Context Model 
The word context model and the POS context 
model estimate the probability of generating a 
word or a POS given previous context. P(WC) and 
P(TC) can be estimated according to (8) and (9) 
respectively.  
( ) ( )?
=
=
m
1i
1i2ii wcwc|wcPWCP                       (8) 
( ) ( )?
=
=
m
1i
1i2ii tctc|tcPTCP                             (9) 
3.3 Word Entity Model  
Different types of NEs have different structures 
and intrinsic characteristics. Therefore, a single 
model can't capture all types of entities. Typical, 
character-based model is more appropriate for PNs, 
whereas, word-based model is more competent for 
LNs and ONs. Especially, we divided transliterated 
PN into three categories such as JPN, RPN and 
EPN.  
For the sake of estimating the probability of 
generating a NE, we define 19 sub-classes shown 
as Table 1 according to their position in NEs. 
 
429
Tag Description 
Sur Surname of CPN 
Dgb First character of Given Name of CPN 
Dge Last character of Give Name of CPN 
Bfn First character of EPN 
Mfn Middle character of EPN 
Efn Last character of EPN 
RBfn First character of RPN 
RMfn Middle character of RPN 
REfn Last character of RPN 
JBfn surname of JPN 
JMfn Middle character of JPN 
JEfn Last character of JPN 
Bol First word of LN 
Mol Middle word of LN 
Eol Last word of LN 
Aloc Single character LN 
Boo First word of ON 
Moo Middle word of ON 
Eoo Last word of ON 
Table 1 Sub-classes in Entity Model 
3.3.1 Word Entity Model for PN 
For the class of PN (including CPN, APN, JPN, 
RPN and EPN), the word entity model is a charac-
ter-based trigram model which can be expressed in 
equation (10). ( )
( ) ( )( )
( )( )1kiik
1liil1i
ik1i
ik1i
wcwc
1k
2l
wcwcwc
2k
wcwc
iwcwc
w,ENe|wP
w,MNe|wPBNe|wP
ENeMNeMNeBNe|wwP
wc|wwP
?
?
?
??
???
?
???
?=
??
=
? 448476
LL
L
       (10) 
where, BNe, MNe and ENe denotes the first, mid-
dle and last characters respectively. 
The word entity models for PN are estimated 
with Chinese, Japanese, Russian and Euramerican 
names lists which contain 15.6 million, 0.15 mil-
lion, 0.44 million, 0.4 million entities respectively. 
3.3.2 Word Entity Model for LN and ON 
For the class of LN and ON, the word entity model 
is a word-based trigram model. The model can be 
expressed by (11). 
( )
( ) ( )
( )( ) ( )
( )( ) ( )ikwcwcwcwc
wcwcwc
1k
2l
1liil
wcendwcstartwcwc
2k
wcwcwc
iendwcstartwc
wc|wwPwc,ENe|wcP
wc|wwPwc,MNe|wcP
wc|w..wPBNe|wcP
ENeMNeMNeBNe|wcwcwcP
wc|wwP
denikstartik1kiik
ilendilstartil
1i1i1i1i
ikil1i
ii
?
L
L
448476
LLL
L
?
?
=
???
?
???
?=
=
 (11) 
The word entity models and the POS entity 
model for LN and ON are estimated with LN and 
ON names lists which respectively contain 0.44 
mil-lion and 3.2 million entities. 
3.3.3 Word Entity Model for ALN 
For the class of ALN, we use word-based bi-gram 
model. The entity model for ALN can be expressed 
by equation (12). 
( ) ( )
)LocA(C
ocAL,wC
ocAL|wP ii =                           (12) 
where wi is the ALN which includes single and 
multiple characters ALN. 
3.4 POS Entity Model 
But for the class of PN, it's very difficult to obtain 
the corpus to train POS Entity Model. For the sake 
of simplification, we use word entity model shown 
in equation (10) to replace the POS entity model. 
For the class of LN and ON, POS entity model 
can be expressed by equation (13). ( )
( ) ( )
( )( ) ( )
( )( ) ( )iktctctctc
tctctc
1k
2l
1liil
tcendwcstarttctc
2k
tctctc
iendtcstarttc
tc|ttPtc,ENe|tcP
tc|ttPtc,MNe|tcP
tc|t..tPBNe|tcP
ENeMNeMNeBNe|tctctcP
tc|ttP
denikstartik1kiik
ilendilstartil
1i1i1i1i
ikil1i
ii
?
L
L
448476
LLL
L
?
?
=
???
?
???
?=
=
    (13) 
While for the class of ALN, POS entity model is 
shown as equation (14). 
( ) ( )
)ocAL(C
ocAL,tiCocAL|tP i =                               (14) 
4 Heuristic Human Knowledge 
In this section, we will introduce heuristic human 
knowledge that is used for Chinese NER and the 
430
method of how to incorporate them into statistical 
model which are shown as follows. 
1. CPN surname list (including 476 items) and 
JPN surnames list (including 9189 items): Only 
those characters in the surname list can trigger per-
son name recognition. 
2. RPN and EPN characters lists: Only those 
consecutive characters in the transliterated charac-
ter list form a candidate transliterated name. 
3. Entity Length Restriction: Person name can-
not span any punctuation and the length of CN 
cannot exceed 8 characters while the length of TN 
is unrestrained. 
4. Location keyword list (including 607 items):  
If the word belongs to the list, 2~6 words before 
the salient word are accepted as candidate LNs. 
5. General word list (such as verbs and preposi-
tions): Words in the list usually is followed by a 
location name, such as "?/at", "?/go". If the cur-
rent word is in the list, 2~6 words following it are 
accepted as candidate LNs. 
6. ALN name list (including 407 items): If the 
current word belongs to the list, we accept it as a 
candidate ALN. 
7. Organization keyword list (including 3129 
items): If the current word is in organization key-
word list, 2~6 words before keywords are accepted 
as the candidate ONs. 
8. An organization name template list: We 
mainly use organization name templates to recog-
nize the missed nested ONs in the statistical model. 
Some of these templates are as follows: 
ON-->LN D* OrgKeyWord 
ON-->PN D* OrgKeyWord 
ON-->ON OrgKeyWord 
D and OrgKeyWord denote words in the middle 
of ONs and ONs keywords. D* means repeating 
zero or more times. 
5 Back-off Model to Smooth 
Data sparseness problem still exists. As some pa-
rameters were never observed in training corpus, 
the model will back off to a less powerful model. 
The escape probability[Black, et al 1998] was ad-
opted to smooth the statistical model shown as (15). 
00N11N2N1N
1N1NN1N1N
^
p)W(p)WWW(p
)WWW(p)WWW(p
???
?
+++
+=
LL
LL  (15) 
where NN e1? = , Ni0,e)e1(?
N
1ik
kii <<=
+=
? , and ei 
is the escape probability which can be estimated by 
equation (16). 
)WWW(f
)WWW(q
e
1N21
1N21
N L
L=                           (16) 
q(w1w2?wN-1) in (16) denotes the number of dif-
ferent symbol wN that have directly followed the 
word sequence w1w2?wN-1. 
6 Experiments 
In this chapter, we will conduct experiments to 
answer the following questions.  
Will the Hybrid Model be more effective than 
the Word Model and the POS Model? To answer 
this question, we will compare the performances of 
models with different parameter ? and find the best 
value of ? in equation (7). 
Will the conclusion from different testing sets be 
consistent? To answer this question, we evaluate 
models on the MET-2 test data and compare the 
performances of the Word Model, the POS Model 
and the Hybrid Model. 
Will the performance be improved significantly 
after combining human knowledge? To answer this 
question, we compare two models with and with-
out human knowledge.  
In our evaluation, only NEs with correct 
boundaries and correct categories are considered as 
the correct recognition. We conduct evaluations in 
terms of precision, recall and F-Measure. Note that 
PNs in experiments includes all kinds of PNs and 
LNs include ALNs. 
6.1 Will the Hybrid Model be More Effective 
Than the Word Model and POS Model? 
The parameter ? in equation (7) denotes the balanc-
ing factor of the Word Model and the POS Model. 
The larger ?, the larger contribution of the POS 
Model. The smaller ?, the larger contribution of the 
Word Model. So the task of this experiment is to 
find the best value of ?. In this experiment, the 
training corpus is from five-month's People's Daily 
tagged with NER tags and the testing set is from 
one-month's People's Daily. 
With the change of ?, the performances of rec-
ognizing PNs are shown in Fig.1.  
Note that the left, middle and right point in ab-
scissa respectively denote the performance of the 
431
Word Model, the Hybrid Model and the POS 
Model. 
0 1.6 3.2 4.8 6.4 8 9.6
0.88
0.89
0.9
0.91
0.92
0.93
0.94
0.95
0.96
Lamda
%
Precision
Recall
F?Measure
 
Fig.1 Performance of Recognizing LNs Impacted 
by ? 
From Fig.1, we can find that the performances 
of recognizing PNs are improved with the increas-
ing of ? in the beginning stage but decline in the 
ending. This experiment shows that the Word 
Model and the POS Model can overcome their dis-
advantages, and it is a feasible approach to inte-
grate the Word Model and the POS Model in order 
to improve the performance PNs recognition.  
With the change of ?, the performances of rec-
ognizing LNs are shown in Fig.2. 
0 1.6 3.2 4.8 6.4 8 9.6
0.9
0.91
0.92
0.93
0.94
Lamda
%
Precision
Recall
F?Measure
 
Fig.2 Performance of Recognizing LNs Impacted 
by ? 
As the Fig.2 shows, the precision and recall of 
LNs are improved with the increasing of ? and de-
creased in the later stage. This phenomenon also 
proves that the Hybrid Model is better for recog-
nizing LN than either the Word Model or the POS 
Model. 
Similarly, with the change of ?, the perform-
ances of recognizing ONs are shown in Fig.3. 
 
0 1.6 3.2 4.8 6.4 8 9.6
0.7
0.75
0.8
0.85
Lamda
%
Precision
Recall
F?Measure
 
Fig.3 Performance of Recognizing LNs Impacted 
by ? 
Comparing Fig.3 with Fig.1 and Fig.2, we find 
that the POS Model has different impact on recog-
nizing ONs from that on recognizing PNs and LNs. 
Especially, the POS Model has obvious side-effect 
on the recall. We speculate that the reasons may be 
that the probability of generating POS sequence by 
POS entity model is lower than that by POS con-
text model. 
According to Fig.1~Fig.3, we choose the best 
value ? = 2.8. And the performances of different 
models are shown in Table 2 in detail. 
 P(%) R(%) F(%) 
PN 94.06 95.21 94.63 
LN 93.98 93.48 93.73 
Hybrid 
Model 
(?= 2.8) 
ON 84.69 86.86 85.76 
 
PN 88.24 90.11 89.16 
LN 91.50 93.17 92.32 
Word 
Model 
ON 78.85 88.77 83.52 
    
PN 93.44 95.11 94.27 
LN 89.97 92.20 91.07 
POS 
Model 
ON 80.90 69.29  74.65 
Table 2 Performance of the Hybrid Model, the 
Word Model and the POS Model 
From Table 2, we find that the F-Measures of 
the Hybrid Model for PN, LN, ON are improved 
by 5.4%, 1.4%, 2.2% respectively in comparison 
with the Word Model, and these F-Measures are 
improved by 0.4%, 2.7%, 11.1% respectively in 
comparison with the POS Model. 
432
Conclusion 1: The experimental results validate 
our idea that the Hybrid Model can improve the 
performance of both the Word Model and the POS 
Model. However, the improvements for PN, LN 
and ON are different. That is, the POS Model has 
obvious side-effect on the recall of ON recognition 
at all times, while the recalls for PN and ON rec-
ognition are improved in the beginning but de-
creased in the ending with the increasing of ?. 
6.2 Will the Conclusion from Different Test-
ing Sets be Consistent? 
We also conduct experiments on the MET-2 test-
ing corpus to validate our conclusion from Exp.1, 
that is, the Hybrid Model could achieve better per-
formance than either the Word Model or the POS 
Model alone. The experimental results (F-Measure) 
on MET-2 are shown in Table 3. 
Model Word Model 
Hybrid 
Model 
POS 
Model 
PN 75.21% 80.77% 76.61% 
LN 89.78% 90.95% 89.81% 
ON 76.30% 80.21% 76.83% 
Table 3 F-Measure on MET-2 test corpus  
Comparing Table 3 with Table 2, we find that 
the performances of models on MET-2 are not as 
good as that on People Daily's testing data. The 
main reason lies in that the NE definitions in Peo-
ple Daily's corpus are different from that in MET-2. 
However, Table 3 can still validate our conclude 1, 
that is, the Hybrid Model is better than both the 
Word Model and the POS Model. For example, the 
F-Measures of the Hybrid Model for PN, LN and 
ON are improved by 5.6%, 1.2% and 3.9% respec-
tively in comparison with the Word Model, and 
these F-Measures are improved by 4.2%, 3.1% and 
3.4% respectively in comparison with the POS 
Model. 
Conclusion 2: Though the performances of the 
Hybrid Model on MET-2 are not as good as that 
on People's Daily corpus, the experimental results 
also support conclusion 1, i.e. the Hybrid Model 
which combining the Word Model with the POS 
Model can achieve better performance than either 
the Word Model or the POS Model. 
6.3 Will the Performance be Improved Sig-
nificantly after Incorporating Human 
Knowledge?  
One of our ideas in this paper is that human 
knowledge can not only reduce the search space, 
but also improve the performance through avoiding 
generating the noise NEs. This experiment will be 
conducted to validate this idea. Table 4 shows the 
performances of models with and without human 
knowledge.  
 P(%) R(%) F(%) 
PN 91.81 70.65 79.85 
LN 79.47 88.83 83.89 Model I 
ON 64.95 80.63 71.95 
 
PN 94.06 95.21 94.63 
LN 93.98 93.48 93.73 Model II 
ON 84.69 86.86 85.76 
Table 4 Performances Impacted by Human Know-
ledge 
From Table 4, we find that F-Measure of model 
with human knowledge (Model II) is improved by 
14.8%?9.8%?13.8% for PN, LN and ON respec-
tively compared with that of the model without 
human knowledge (Model I). 
Conclusion 3: From this experiment, we learn 
that human knowledge can not only reduce the 
search space, but also significantly improve the 
performance of pure statistical model. 
7 Conclusion 
In this paper, we propose a hybrid Chinese NER 
model which combines multiple features. The main 
contributions are as follows: ? The proposed Hy-
brid Model emphasizes on integrating coarse parti-
cle feature (POS Model) with fine particle feature 
(Word Model), so that it can overcome the disad-
vantages of each other; ? In order to reduce the 
search space and improve the efficiency of model, 
we incorporate heuristic human knowledge into 
statistical model, which could increase the per-
formance of NER significantly; ? For capturing 
intrinsic features in different types of entities, we 
design several sub-models for different entities. 
Especially, we divide transliterated person name 
into three sub-classes according to their characters 
set, that is, CPN JPN, RPN and EPN. 
There is a lack of effective recognition strategy 
for abbreviated ONs such as ????(Kunming 
Machine Tool Co.,Ltd), ? ? ? ? (Phoenix 
Photonics Ltd) in this paper. And most of mis-
433
recognized ONs in current system belong to them. 
So in the future work, we will be focusing more on 
recognizing abbreviated ONs. 
8 Acknowledgements 
This research is carried out as part of the coopera-
tive project with Fujitsu R&D Center Co., Ltd. We 
would like to thank Yingju Xia, Fumihito Nisino 
for helpful feedback in the process of developing 
and implementing. This work was supported by the 
Natural Sciences Foundation of China under grant 
No. 60372016 and 60272041, the Natural Science 
Foundation of Beijing under grant No. 4052027. 
References  
N.A. Chinchor: Overview of MUC-7/MET-2. In: Pro-
ceedings of the Seventh Message Understanding 
Conference (MUC-7), April. (1998). 
Youzheng Wu, Jun Zhao, Bo Xu: Chinese Named En-
tity Recognition Combining Statistical Model with 
Human Knowledge. In: The Workshop attached with 
41st ACL for Multilingual and Mix-language Named 
Entity Recognition, Sappora, Japan. (2003) 65-72. 
Endong Xun, Changning Huang, Ming Zhou: A Unified 
Statistical Model for the Identification of English 
BaseNP. In: Proceedings of ACL-2000, Hong Kong. 
(2000). 
Jian Sun, Jianfeng Gao, Lei Zhang, Ming Zhou, 
Changning Huang: Chinese Named Entity Identifica-
tion Using Class-based Language Model. In: 
COLING 2002. Taipei, August 24-25. (2002). 
Huaping Zhang, Qun Liu, Hongkui Yu, Xueqi Cheng, 
Shuo Bai: Chinese Named Entity Recognition Using 
Role Model. In: the International Journal of Compu-
tational Linguistics and Chinese Language Process-
ing, vol.8, No.2. (2003) 29-60. 
D.M. Bikel, Scott Miller, Richard Schwartz, Ralph 
Weischedel: Nymble: a High-Performance Learning 
Name-finder. In: Fifth Conference on Applied Natu-
ral Language Processing, (published by ACL). (1997) 
194-201. 
Borthwick .A: A Maximum Entropy Approach to 
Named Entity Recognition. PhD Dissertation. (1999). 
Mikheev A., Grover C. and Moens M: Description of 
the LTG System Used for MUC-7. In: Proceedings of 
7th Message Understanding Conference (MUC-7), 
1998. 
Sekine S., Grishman R. and Shinou H: A decision tree 
method for finding and classifying names in Japanese 
texts. In: Proceedings of the Sixth Workshop on Very 
Large Corpora, Canada, 1998. 
Aberdeen, John, et al MITRE: Description of the 
ALEMBIC System Used for MUC-6. In: Proceedings 
of the Sixth Message Understanding Conference 
(MUC-6), November. (1995) 141-155. 
Ralph Grishman and Beth Sundheim: Design of the 
MUC-6 evaluation. In: 6th Message Understanding 
Conference, Columbia, MD. (1995) 
Krupka, G. R. and Hausman, K. IsoQuest: Inc.: Descrip-
tion of the NetOwl TM Extractor System as Used for 
MUC-7. In Proceedings of the MUC-7, 1998. 
Black, W.J.; Rinaldi, F, Mowart, D: FACILE: Descrip-
tion of the NE System Used for MUC-7. In Proceed-
ings of the MUC-7, 1998. 
Michael Collins, Yoram Singer: Unsupervised models 
for named entity classification. In Proceedings of 
EMNLP. (1999) 
Michael Collins: Ranking Algorithms for Named Entity 
Extraction: Boosting and the Voted Perceptron. In: 
Proceeding of ACL-2002. (2002) 489-496. 
S.Y.Yu, et al Description of the Kent Ridge Digital 
Labs System Used for MUC-7. In: Proceedings of the 
Seventh Message Understanding Conference, 1998. 
H.H. Chen, et al Description of the NTU System Used 
for MET2. In: Proceedings of the Seventh Message 
Understanding Conference. 
Tat-Seng Chua, et al Learning Pattern Rules for Chi-
nese Named Entity Extraction. In: Proceedings of 
AAAI'02. (2002) 
Maosong Sun, et al Identifying Chinese Names in Un-
restricted Texts. Journal of Chinese Information 
Processing. (1994). 
Jiahen Zheng, Xin Li, Hongye Tan: The Research of 
Chinese Names Recognition Methods Based on Cor-
pus. In: Journal of Chinese Information Processing. 
Vol.14 No.1. (2000). 
CoNLL. http://cnts.uia.ac.be/conll2004/ 
IEER. http://www.nist.gov/speech/tests/ie-er/er99/er99. 
htm 
ACE. http://www.itl.nist.gov/iad/894.01/tests/ace/ 
 
434
Product Named Entity Recognition Based on Hierarchical Hidden
Markov Model?
Feifan Liu, Jun Zhao, Bibo Lv, Bo Xu
National Laboratory of Pattern Recognition
Institute of Automation Chinese Academy of Sciences
Beijing P.O. Box 2728, 100080
{ffliu,jzhao,bblv,xubo}@nlpr.ia.ac.cn
Hao Yu
FUJITSU R&D
Xiao Yun Road No.26
Chao Yang District, Beijing, 100016
yu@frdc.fujitsu.com
Abstract
A hierarchical hidden Markov model
(HHMM) based approach of product
named entity recognition (NER) from
Chinese free text is presented in this pa-
per. Characteristics and challenges in
product NER is also investigated and
analyzed deliberately compared with
general NER. Within a unified statis-
tical framework, the approach we pro-
posed is able to make probabilistically
reasonable decisions to a global opti-
mization by leveraging diverse range
of linguistic features and knowledge
sources. Experimental results show that
our approach performs quite well in two
different domains.
1 Introduction
Named entity recognition(NER) plays a sig-
nificantly important role in information extrac-
tion(IE) and many other applications. Previous
study on NER is mainly focused either on the
proper name identification of person(PER), lo-
cation(LOC), organization(ORG), time(TIM) and
numeral(NUM) expressions almost in news do-
main, which can be viewed as general NER, or
other named entity (NE) recognition in specific
domain such as biology.
As far as we know, however, there is little prior
research work conducted by far on product named
0This work was supported by the Natural Sciences Foun-
dation of China(60372016,60272041) and the Natural Sci-
ence Foundation of Beijing(4052027).
entity recognition which can be crucial and valu-
able in many business IE applications, especially
with the increasing research interest in Market
Intelligence Management(MIM), Enterprise Con-
tent Management (ECM) [Pierre 2002] and etc.
This paper describes a prototype system for
product named entity recognition, ProNER, in
which a HHMM-based approach is employed.
Within a unified statistical framework, the ap-
proach based on a mixture model is able to make
probabilistically reasonable decisions to a global
optimization by exploiting diverse range of lin-
guistic features and knowledge sources. Experi-
mental results show that ProNER performs quite
well in two different domains.
2 Related Work
Up to now not much work has been done on
product named entity recognition, nor systematic
analysis of characteristics for this task. [Pierre
2002] developed an English NER system capable
of identifying product names in product views. It
employed a simple Boolean classifier for identi-
fying product name, which was constructed from
the list of product names. The method is sim-
ilar to token matching and has a limitation for
product NER applications. [Bick et al 2004] rec-
ognized named entities including product names
based on constraint grammar based parser for
Danish. This rule-based approach is highly de-
pendent on the performance of Danish parser and
suffers from its weakness in system portability.
[C. Niu et al 2003] presented a bootstrapping ap-
proach for English named entity recognition us-
ing successive learners of parsing-based decision
40
System Statistical Model Linguistic Feature Combinative Points
[Zhang et al 2003] HMM semantic role, tokens pattern rules
[Sun et al 2002] class-based LM word form, NE category cue words list
[Tsai et al 2004] ME model tokens knowledge representation
Table 1: Comparison between several Chinese NER systems1
list and HMM, and promising experiment results
(F-measure: 69.8%) on product NE (correspond-
ing to our PRO) were obtained. Its main advan-
tage lies in that manual annotation of a sizable
training corpus can be avoided, but it suffers from
two problems, one is that it is difficult to find suf-
ficient concept-based seeds needed in bootstrap-
ping for the coverage of the variations of PRO
subcategories, another it is highly dependent on
parser performance as well.
Research on product NER is still at its early
stage, especially in Chinese free text collec-
tions. However, considerable amount of work
has been done in the last decade on the gen-
eral NER task and biological NER task. The
typical machine learning approaches for English
NE are transformation-based learning[Aberdeen
et al 1995], hidden Markov model[Bikel et
al. 1997], maximum entropy model[Borthwick,
1999], support vector machine learning[Eunji Yi
et al 2004], unsupervised model[Collins et al
1999]and etc.
For Chinese NER, the prevailing methodology
applied recently also lie in machine learning com-
bining other knowledge base or heuristic rules,
which can be compared on the whole in three as-
pects showed in Table 1.
In short, the trend in NER is to adopt a statis-
tical framework which try to exploit some knowl-
edge base as well as different level of text features
within and outside NEs. Further those ideas, we
present a hybrid approach based on HHMM [S.
Fine et al 1998] which will be described in de-
tail.
3 Problem Statements and Analysis
3.1 Task Definition
3.1.1 Definition of Product Named Entity
In our study, only three kinds of prod-
uct named entities are considered, namely
1Note: LM(language model); ME(maximum entropy).
Brand Name(BRA), Product Type(TYP), Product
Name(PRO), and BRA and TYP are often embed-
ded in PRO. In the following two examples, there
are two BRA NEs, one TYP NE and one PRO
NE all of which belong to the family of product
named entities.
Exam 1: ??(Benq)/BRA ??(brand)?
? ? ? ?(market shares)? ?(steadily)?
?(ascend)b
Exam 2: ? ?(corporation)? ?(will)?
?(deliver) [Canon/BRA 334?(ten thou-
sand)? ?(pixels)? ?(digital)? ?(camera)
Pro90IS/TYP]/PROb
Brand Name refer to proper name of product
trademark such as ???(Benq)? in Exam 1.
Product Type is a kind of product named en-
tities indicating version or series information of
product, which can consist of numbers, English
characters, or other symbols such as ?+? and ?-
? etc.In our study, two principles should be fol-
lowed.
(1) Chinese characters are not considered to
be TYP, nor subpart of TYP although some of
them can contain version or series information.
For instance, in ?2005????(happy new
year)?(version)??(cell phone)?, here ???
??(happy new year)?(version)?should not be
considered as a TYP.
(2) Numbers are essential elements in prod-
uct type entity. For instance, in ?PowerShot
??(series)??(digital)??(camera)?, ?Pow-
erShot? is not considered as a TYP, however,
in ?PowerShot S10 ??(digital)??(camera)?,
?PowerShot S10? can make up of a TYP.
Product Name, as showed above in Exam 2, is
a kind of product named entities expressing self-
contained proper name for some specified product
in real world compared to BRA and TYP which
only express one attribute of product. i.e. a PRO
NE must be assigned with distinctly discrimina-
tive information which can not shared with other
general product-related expressions.
41
(1) Product-related expressions which are em-
bedded with either BRA or TYP can be qual-
ified to be a PRO entity. e.g. ?BenQ?
?(flash)?(disk)? is a PRO entity, but the gen-
eral product-related expression ???(flash)?
?(market)??(investigation)? cannot make up
of a PRO entity.
(2) Product-related expressions indicating
some specific version or series information which
is unique for a BRA can also be considered as a
PRO entity. e.g. ?DIGITAL IXUS??(series)?
?(digital)? ?(camera)? is a PRO because
?DIGITAL IXUS? series is unique for Canon
product, but ?? ?(intelligent)?(version)?
?(cell phone)? is not a PRO because the at-
tribute of ?intelligent version? can be assigned to
any cell phone product.
3.1.2 Product Named Entity Recognition
Product named entity recognition involves the
identification of product-related proper names
in free text and their classification into differ-
ent kinds of product named entities, referring to
PRO, TYP and BRA in this paper.In comparison
with general NER, nested product NEs should be
tagged separately rather than being tagged just as
a single item, shown as Figure 1.
3.2 Challenges for Product Named Entity
Recognition
?For general named entities, there are some
cues which are very useful for entity recogni-
tion, such as ???(city), ????(Inc.), and etc. In
comparison, product named entities have no such
named conventions and cues, resulting in higher
boundary ambiguities and more complex NE can-
didate triggering difficulties.
?In comparison with general NER, more chal-
lenges in product NER result from miscellaneous
classification ambiguities. Many entities with
identical form can be a kind of general named en-
tity, a kind of product named entity, or just com-
mon words.
?In comparison with general named entities,
product named entities show more flexible vari-
ant forms. The same entity can be expressed in
several different forms due to spelling variation,
word permutation and etc. This also compounds
the difficulties in product named entity recogni-
tion.
?In comparison with general named entities,
it is more frequent that product named entities are
nested as Figure 1 illustrates. More efforts have
to be made to identify such named entities sepa-
rately.
3.3 Our Solutions
We adopt the following strategies in triggering
and disambiguating process respectively.
(1) As to product NER, it?s pivotal to control
the triggering candidates efficiently for the bal-
ance between precision and recall. Here we use
the knowledge base such as brand word list, and
other heuristic information which can be easily
acquired.
(2)After triggering candidates, we try to em-
ploy a statistical model to make the most of
multi-level context information mentioned above
in disambiguation. We choose hierarchical hid-
den Markov model (HHMM) [S. Fine et al 1998]
for its more powerful ability to model the multi-
plicity of length scales and recursive nature of se-
quences.
42
4 Hybrid Approach for Product NE
Recognition
4.1 Overall Workflow of ProNER
?Preprocessing: Segment, POS tagging and
general NER is primarily conducted using our off-
shelf SegNer2.0 toolkit on input text.
?Generating Product NE Candidates: First,
BRA or ORG and TYP are triggered by brand
word list and some word features respectively.
Here we categorize the triggering word features
into six classes: alphabet string, alphanumeric
string, digits, alphabet string with fullwidth, dig-
its with fullwidth and other symbols except Chi-
nese characters. Then PRO are triggered by BRA
and TYP candidates as well as some clue words
indicating type information to some extent such
as ???(version), ????(series). In this step the
model structure(topology) of HHMM[S. Fine et
al. 1998] is dynamically constructed, and some
conjunction words or punctuations and specified
maximum length of product NE are used to con-
trol it.
?Disambiguating Candidates: In this mod-
ule, boundary and classification ambiguities be-
tween candidates are resolved simultaneously.
And Viterbi algorithm is applied for most-likely
state sequences based on the HHMM topology.
4.2 Integration with Heuristic Information
To get more efficient control in triggering process
above, we try to integrate some heuristic informa-
tion. The heuristic rules we used are as domain-
independent as possible in order that they can
be integrated with statistical model systematically
rather than just some tricks on it.
(1) Stop Word List:
Common English words, English brand word,
and some punctuations are extracted automati-
cally from training set to make up of stop word
list for TYP; by co-occurrence statistics between
ORG and its contexts, some words are extracted
from the contexts to make up of stop word list
for PRO in order to overcome the case that brand
word is prone to bind its surroundings to be a
PRO.
(2) Constrain Rules:
Rule 1: For the highly frequent pattern ??
?+?????(number + English quantifier
ES PS5IS2IS1
IS0
ES PS1 PS2 PS4PS3ES
0.2 0.5
0.3
0.7  0.3 0.5 0.30.7
0.2
0.3
Figure 2 Structure of Hierarchical Hidden
Markov Model (HHMM)
word), all the corresponding TYP candidates trig-
gered by categorized word features(CWF) should
be removed.
Rule 2: Product NE candidates in which some
binate symbols don?t match each other should be
removed.
Rule 3: Unreasonable symbols such as ?-? or
?:? should not occur in the beginning or end of
product NE candidates.
4.3 HHMM for product NER application
By HHMM [S. Fine et al 1998] the product
NER can be formulated as a tagging problem us-
ing Viterbi algorithm. Unlike traditional HMM
in POS tagging, here the topology of HHMM is
not fixed and internal states can be also a similar
stochastic model on themselves, called internal
states compared to production states which will
emit only observations.
Our HHMM structure actually consists of three
level approximately illustrated as figure 2 in
which IS denotes internal state, PS denotes pro-
duction state and ES denote end state at ev-
ery level. For our application, an input se-
quence from our SegNer2.0 toolkit can be formal-
ized as w1/t1w2/t2 . . . wi/ti . . . wn/tn, among
which wi and ti is the ith word and its part-of-
speech, n is the number of words. The POS
tag set here is the combination of tag set from
Peking University(PKU-POS) and our general
NE categories(GNEC) including PER(person),
LOC(location), ORG(organization), TIM(time ex-
pression), NUM(numeric expression). Therefore
we can construct our HHMM model by the state
set {S} consisting of {GNEC}, {BRA, PRO,
TYP}, and {V} as well as the observation set {O}
consisting of {V} which is the word set from
training data. That is to say, the word forms
43
in {V} which are not included in NEs are also
viewed as production states.
In our model, only PRO are internal state which
may activate other production states such as BRA
and TYP resulting in recursive HMM. In consis-
tence with S. Fine?s work, qdi (1? d ? D) is used
to indicate the ith state in the dth level of hierar-
chy. So, the product NER problem is to find the
most-likely state activation sequence Q*, a multi-
scale list of states, based on the dynamic topol-
ogy of HHMM given a observation sequence W
= w1w2 . . . wi . . . wn, formulated as follows based
on Bayes rule (P (W )=1).
Q?= argmax
Q
P (Q|W )= argmax
Q
P (Q)P (W |Q)
(1)
From the root node of HHMM, activity flows
to all other nodes at different levels according to
their transition probability. For description conve-
nience, we take the kth level as example(activated
by the mth state at the k-1th level).
P (Q) ?= p(qk1 |qk?1m )
? ?? ?
vertical transition
horizontal transition
? ?? ?
p(qk2 |qk1 )
|qk|
?
j=3
p(qkj |qkj?1, qkj?2)
(2)
P (W |Q)=
?
???????
???????
?=
|qkPS |?
j=1
p([wqkj ?begin...wqkj ?end]|q
k
j )
if qkj /? {IS}
activate other states recursively
if qkj ? {IS}
(3)
Where |qk| is the number of all states and |qkPS |
is the number of production states in the kth level;
wqkj ?begin...wqkj ?end indicates the word sequence
corresponding to the state qkj .
(1) In equation (3), if qkj ? {{GNEC},{V}},
p([wqkj ?begin...wqkj ?end]|q
k
j )=1, because we as-
sume that the general NER results from the pre-
ceding toolkit are correct;
(2) If qkj = PRO, production states in the
(k+1)th level will be activated by this internal
state through equation (2),(3) and go back when
arriving at an end state, thus hierarchical compu-
tation is implemented;
(3) If qkj =BRA, we assign equation (3) a con-
stant value in that BRA candidates consist of only
a single brand word in our method. In addition
brand word can also generate ORG candidates,
thus we can assign equation (3) as follows.
p([wqkj ?begin...wqkj ?end]|q
k
j = BRA) = 0.5 (4)
(4) If qkj = TY P , categorized word fea-
tures(CWFs) defined in section 4.1 are applied,
i.e. the words associated with the current state are
replaced with their CWFs (WC) acting as obser-
vations. Then we can compute the emission prob-
ability of this TYP production state as the follow-
ing equation, among which |qkj | is the length of
observation sequence associated with the current
state.
p([wqkj ?begin...wqkj ?end]|q
k
j = TY P )
?=p(wc1|begin)p(end|wc|qkj |)
|qkj |?
m=2
p(wcm|wcm?1)
All the parameters in every level of HHMM can
be acquired using maximum likelihood method
with smoothing from training data.
4.4 Mixture of Two Hierarchical Hidden
Markov Models
Now we have implemented a simple HHMM
for product NER. Note that in the above
model(HHMM-1), we exploit both internal and
external features of product NEs only at lev-
els of simply semantic classification and just
word form. To achieve our motivation in sec-
tion 3.3, we construct another HHMM(HHMM-
2) for exploiting multi-level contexts by mixing
with HHMM-1.
In HHMM-2, the difference from HHMM-1
lies in the state set SII and observation set OII .
Because the input text will be processed by seg-
ment, POS tagging and general NER, as a alterna-
tive, we can also take T=t1t2 . . . ti . . . tn as obser-
vation sequence, i.e. OII={PKU-POS}. Accord-
ingly, SII= {{PKU-POS}, {GNEC}, BRA, TYP,
44
Data Sets PRO BRA TYP PER LOC ORG
DataSetPRO1.2 12,432 5,047 10,606 424 1,733 4,798
OpenTestSet 1800 803 1364 39 207 614
CloseTestSet 1553 513 1296 55 248 619
Table 2: Overview of Data Sets
PRO}, among which PRO is internal state. Sim-
ilarly, the problem is formulated as follows with
HHMM-2.
Q?II = argmax
QII
P (QII |T )
= argmax
QII
P (QII)P (T |QII) (5)
The description and computation of HHMM-2
is similar to HHMM-1 and is omitted here.
We can see that besides making use of semantic
classification of NEs in common, HHMM-1 and
HHMM-2 exploit word form and part-of-speech
(POS) features respectively. Word form features
make the model more discriminative, while POS
features result in robustness. Intuitively, the mix-
ture of these two models is desirable for higher
performance in product NER by balancing the ro-
bustness and discrimination which can be formu-
lated in logarithmic form as follows.
(Q?, Q?II)
= argmax
Q,QII
{log(P (Q)) + log(P (W |Q))
+ ?[log(P (QII)) + log(P (T |QII))]} (6)
Where ? is a tuning parameter for adjusting the
weight of two models.
5 Experiments and analysis
5.1 Data Set Preparation
A large number of web pages in mobile phone
and digital domain are compiled into text collec-
tions, DataSetPRO, on which multi-level process-
ing were performed. Our final version, DataSet-
PRO1.2, consists of 1500 web pages, roughly
1,000,000 Chinese characters. Randomly se-
lected 140 texts (digital 70, mobile phone 70) are
separated from DataSetPRO1.2 as our OpenTest-
Set, the rest as TrainingSet, from which 160 texts
are extracted as CloseTestSet. Table 2 illustrates
the overview of them.
5.2 Experiments
Due to various and flexible forms of product NEs,
though some boundaries of recognized NEs are
inconsistent with manual annotation, they are also
reasonable. So soft evaluation is also applied
in our experiments to make the evaluation more
reasonable. The main idea is that a discount
score will be given to recognized NEs with wrong
boundary but correct detection and classification.
However, strict evaluation only score completely
correct ones.
All the results is conducted on OpenTestSet un-
less it is particularly specified. Also, the evalu-
ation scores used below are obtained mainly by
45
Digital Domain (??8)
Product NEs Close Test Open TestPrecision Recall F-measure Precision Recall F-measure
PRO 0.864 0.799 0.830 0.762 0.744 0.753
TYP 0.903 0.906 0.905 0.828 0.944 0.882
BRA 0.824 0.702 0.758 0.723 0.705 0.714
Mobile Phone Domain (??8)
Product NEs Close Test Open TestPrecision Recall F-measure Precision Recall F-measure
PRO 0.917 0.935 0.926 0.799 0.856 0.827
TYP 0.959 0.976 0.967 0.842 0.886 0.864
BRA 0.911 0.741 0.818 0.893 0.701 0.785
Table 3: Experimental Results in Digital and Mobile Phone Domain
soft metrics, and strict scores are also given for
comparison in experiment 3.
1. Evaluation on the Influence of ? in the Mix-
ture Model.
In the mixture model denoted as equation (6),
the ? value reflects the different contribution of
two individual models to the overall system per-
formance. The larger ?, the more contribution
made by HHMM-2. Figure 3, 4, 5 illustrate the
varying curves of recognition performance with
the ? value on PRO, TYP, BRA respectively.
Note that, if ? equal to 1 then two models
are mixed with equivalent weight. We can see
that, as ? goes up, the F-measures of PRO and
TYP increase obviously firstly, and begin to go
down slightly after a period of growing flat. It
can be explained that HHMM-2 mainly exploits
part-of-speech and general NER features which
can relieve the sparseness problem to some ex-
tent, which is more serious in HHMM-1 due to
using lower level of contextual information such
as word form. However, as ? becomes larger,
the problem of imprecise modeling in HHMM-
2 will be more salient and begin to illustrate a
side-effect in the mixture model. Whereas, the
influence of ? on BRA is negligible because its
candidates are triggered by the relatively reliable
knowledge base and its sub-model in HHMM is
assigned a constant as shown in equation(4).
Summings-up:
(1) Mixture with HHMM-2 can make up the
weakness of HHMM-1.
(2) HHMM-2 can make more contributions
to the mixture model under the conditions that
limited annotated data is available at present. In
our system, ? is assigned to 8 based on above ex-
perimental results.
2. Evaluation on the portability of ProNER in
two domains.
First, we can see from Table 3 that ProNER
have achieved fairly high performance in both
digital and mobile phone domain. This can val-
idate to some extent the portability of our sys-
tem?which is consistent with our initial motiva-
tion.
Second, the results also show that our system
performs slightly better in mobile phone domain
for both close test and open test. This can be ex-
plained that there are more challenging ambigui-
ties in digital domain due to more complex prod-
uct taxonomy and more flexible variants of prod-
uct NEs.
Summings-up: The results provide promising
evidence on the portability of our system to dif-
ferent domains though there are some differences
between them.
3. Evaluation on the efficiency of the mixture
model and the improvement of the triggering
control with heuristics.
In table 4, ?1? denotes HHMM-1; ?2? denotes
HHMM-2; ?+? means the mixture model; ?*?
means integrating with heuristics mentioned in
section 4.2.
The results reveal that the mixture model out-
performs each individual model with both soft
and strict metrics. Also, the results show that
heuristic information can increase the F-measure
of PRO and TYP by 10 points or so for both indi-
46
HHMM
BRA TYP PRO
strict
score
soft
score
strict
score
soft
score
strict
score
soft
score
1 0.68 0.72 0.57 0.66 0.52 0.61
1* 0.70 0.74 0.70 0.80 0.63 0.72
2 0.67 0.73 0.66 0.74 0.61 0.68
2* 0.70 0.74 0.76 0.85 0.70 0.76
1+2 0.70 0.75 0.67 0.77 0.67 0.72
1+2* 0.72 0.76 0.76 0.87 0.75 0.80
Table 4: Improvement results (F-measure) with
heuristics and model mixture
vidual model and the mixture model. Addition-
ally we can see that HHMM-2 performs better
on the whole than HHMM-1, which is consistent
with experiment 1 that heavier weights should be
assigned to HHMM-2 in the mixture model.
Summings-up:
(1) Either HHMM-1 or HHMM-2 can not
perform quite well independently, but systemat-
ical integration of them can achieve obvious per-
formance improvement due to the leverage of di-
verse levels of linguistic features by their efficient
interaction.
(2) Heuristic information can highly enhance
the performance for both individual model and the
mixture model.
6 Conclusions and Future Work
This paper presented a hierarchical HMM (hidden
Markov model) based approach of product named
entity recognition from Chinese free text. By uni-
fying some heuristic rules into a statistical frame-
work based on a mixture model of HHMM, the
approach we proposed can leverage diverse range
of linguistic features and knowledge sources to
make probabilistically reasonable decisions for a
global optimization. The prototype system we
built achieved the overall F-measure of 79.7%,
86.9%, 75.8% corresponding to PRO, TYP, BRA
respectively, which also provide experimental ev-
idence to some extent on its portability to differ-
ent domains.
Our future work will focus on the following:
(1) Using long dependency information;
(2) Integrating segment, POS tagging, general
NER and product NER to avoid error spread.
References
John M. Pierre. (2002) Mining Knowledge from Text
Collections Using Automatically Generated Meta-
data. In: Procs of Fourth International Conference
on Practical Aspects of Knowledge Management.
Michael Collins and Yoram Singer. (1999) Unsuper-
vised Models for Named Entity Classification. In:
Proc. of EMNLP/VLC-99.
Eunji Yi, Gary Geunbae Lee, and Soo-Jun Park.
(2004) SVM-based Biological Named Entity
Recognition using Minimum Edit-Distance Feature
Boosted by Virtual Examples. In: Proceedings of
the First International Joint Conference on Natural
Language Processing (IJCNLP-04).
Bick, Eckhard (2004) A Named Entity Recognizer for
Danish. In: Proc. of 4th International Conf. on Lan-
guage Resources and Evaluation,pp:305-308.
Jian Sun, Jianfeng Gao, Lei Zhang, Ming Zhou,
Changning Huang. (2002) Chinese Named Entity
Identification Using Class-based Language Model.
In: COLING 2002. Taipei, Taiwan.
Huaping Zhang, Qun Liu, Hongkui Yu, Xueqi Cheng,
Shuo Bai. Chinese Named Entity Recognition Us-
ing Role Model. Special Iissue ?Word Formation
and Chinese Language processing? of the Inter-
national Journal of Computational Linguistics and
Chinese Language Processing, 8(2),2003, pp:29-60
Aberdeen, John et al (1995)MITRE: Description of
the ALEMBIC System Used for MUC-6. Proc. of
MUC-6, pp. 141-155
D.M. Bikel, S. Miller, R. Schwartz, R. Weischedel.
(1997) Nymble: a High-Performance Learning
Name-finder. In: Fifth Conference on Applied Nat-
ural Language Processing, pp 194-201.
Borthwick. A. (1999) A Maximum Entropy Approach
to Named Entity Recognition. PhD Dissertation.
Tzong-Han Tsai, S.H. Wu, C.W. Lee, Cheng-Wei
Shih, and Wen-Lian Hsu. (2004) Mencius: A Chi-
nese Named Entity Recognizer Using the Maxi-
mum Entropy-based Hybrid Model. International
Journal of Computational Linguistics and Chinese
Language Processing, Vol. 9, No 1.
Cheng Niu, W. Li, J.h. Ding and R.K. Srihari. (2003) A
Bootstrapping Approach to Named Entity Classifi-
cation Using Successive Learners. In: Proceedings
of the 41st ACL, Sapporo, Japan, pp:335-342.
S. Fine, Y. Singer, N. Tishby. (1998) The Hierarchical
Hidden Markov Model: Analysis and Applications.
Machine Learning. 32(1), pp:41-62
47
Bridging the Gap
Between Dialogue Management and Dialogue Models
Weiqun Xu and Bo Xu and Taiyi Huang and Hairong Xia
National Laboratory of Pattern Recognition
Institute of Automation, Chinese Academy of Sciences
Beijing, 100080, P. R. China
 wqxu, xubo, huang, hrxia@nlpr.ia.ac.cn
Abstract
Why do few working spoken dialogue sys-
tems make use of dialogue models in their
dialogue management? We find out the
causes and propose a generic dialogue
model. It promises to bridge the gap be-
tween practical dialogue management and
(pattern-based) dialogue model through
integrating interaction patterns with the
underling tasks and modeling interaction
patterns via utterance groups using a high
level construct different from dialogue act.
1 Introduction
Due to the rapid progress of speech and language
processing technologies (Cole et al, 1998; Juang
and Furui, 2000), ever-increasing computing power,
and vast quantity of social requirements, spoken di-
alogue systems (SDSs), which promise to provide
natural and ubiquitous access to online information
and service, have become the focus of many research
groups (both academic and industrial) with many
projects sponsored by EU, US (D)ARPA and others
in the past few years (Zue and Glass, 2000; McTear,
2002; Xu, 2001). The last decade saw the emergence
of a great deal of SDSs.
Despite so much progress, some problems still
remain, prominent among which are usability and
reusability (or portability across domains and lan-
guages). Through a survey of typical working spo-
ken (or natural language) dialogue systems in the
nineties (Xu, 2001), we find their central control-
ling component ? dialogue management ? is rela-
tively less well-established than other components.
In most working SDSs, the design of dialogue
management is usually guided by some principles
(den Os et al, 1999), strategies (Souvignier et al,
2000), or objectives (Lamel et al, 2000). In some
even these guidelines are implicit. The problem is
more outstanding in those SDSs developed by the
speech recognition community, in which most work-
ing SDSs come into being. Among many causes,
we think, the most important is that dialogue man-
agement is short of solid theoretical support from
dialogue models (the distinction between dialogue
management and dialogue model will be explicated
in section 2), in addition to the design of SDSs being
a real world problem.
The approach we adopt in building dialogue man-
agement model for SDSs is to study human-human
dialogues solving the same or similar problem.
Though human-computer dialogues may be differ-
ent in some aspects from human-human dialogues,
the design of human-computer dialogue will bene-
fit a lot from the study of human-human dialogues.
It will not be clear whether those that characterize
human-human dialogues are applicable to human-
computer dialogues until they are well studied. Ap-
plicable or not, they are sure to contribute some in-
sights to the design of dialogue management.
In what follows we first inspect main approaches
to dialogue modeling and dialogue management and
find two deep causes behind the gap between them
(section 2). Against the causes we propose a generic
dialogue model which distinguishes five ranks of
discourse units and three levels of dialogue dynam-
     Philadelphia, July 2002, pp. 201-210.  Association for Computational Linguistics.
                  Proceedings of the Third SIGdial Workshop on Discourse and Dialogue,
ics (section 3). Then we apply it to information-
seeking (one of the most common tasks adopted in
the study of SDSs) dialogues and elaborate interac-
tion patterns as utterance groups, which are classi-
fied along two dimensions (initiative and direction
of information flow) into four basic types with some
variations (section 4). We also experiment on seg-
menting utterance groups in our corpus with a sub-
ject and three algorithms.
2 The Gap
Why do most working SDSs make little use of di-
alogue models in their dialogue management? Or,
why is there a gap between dialogue management
and dialogue models?
To make it clear, we first distinguish between di-
alogue models and dialogue management models1,
or equivalently, between dialogue modeling and di-
alogue management modeling.The goal of dialogue
modeling is to develop general theories of (coopera-
tive task-oriented) dialogues and to uncover the uni-
versals in dialogues and, if appropriate, to provide
dialogue management with theoretical support. It
takes an analyzer?s point of view. While the goal
of dialogue management modeling is to integrate di-
alogue model with task model in some specific do-
main to ?develop algorithms and procedures to sup-
port a computer?s participation in a cooperative dia-
logue? (Cohen, 1998, p.204). It takes the viewpoint
of a dialogue system designer.
Next, we briefly overview main approaches to di-
alogue modeling and dialogue management, then
point out the causes behind the gap.
2.1 Dialogue Models
There are mainly two approaches to dialogue mod-
eling: pattern-based and plan-based.2
1The distinction between dialogue models and dialogue
management models is close to what Cohen (1998) makes in
dialogue modeling. He distinguishes ?two related, but at times
conflicting, research goals ... often adopted by researchers of
dialogue?. Roughly speaking, one is theoretical and the other is
practical.
2Cohen (1998) gives a more detailed discussion on dialogue
modeling. Below we draw a lot from there. He mentions ?three
approaches to modeling dialogue ? dialogue grammars, plan-
based models of dialogue, and joint action theories of dialogue?.
We treat joint action theories as further development of original
plan-based approach. So his latter two correspond to our plan-
based approach in general.
Patten-based approach models recurrent interac-
tion patterns or regularities in dialogues at the illo-
cutionary force level of speech acts (Austin, 1962;
Searle, 1969) in terms of dialogue grammar (Sin-
clair and Coulthard, 1975), dialogue/conversational
game (Carlson, 1983; Kowtko et al, 1992; Mann,
2001), or adjacency pairs (Sacks et al, 1974). It
benefits a lot from the insights of discourse analy-
sis (Sinclair and Coulthard, 1975; Coulthard, 1992;
Brown and Yule, 1983) and conversation analysis
(Levinson, 1983).
Plan-based approach relates speech acts per-
formed in utterances to plans and complex mental
states (Cohen and Perrault, 1979; Allen and Perrault,
1980; Lochbaum et al, 2000) and uses AI plan-
ning techniques (Fikes and Nilsson, 1971). Later de-
velopments of plan-based dialogue models include
multilevel plan extension (Litman and Allen, 1987;
Litman and Allen, 1990; Carberry, 1990; Lambert
and Carberry, 1991), theories of joint action (Co-
hen and Levesque, 1991) and SharedPlan (Grosz and
Sidner, 1990; Grosz and Kraus, 1996).
Pattern-based dialogue model describes what hap-
pens in dialogues at the speech act level and cares
little about why. Plan-based dialogue model ex-
plains why agents act in dialogues, but at the ex-
pense of complex representation and reasoning. In
other words, the former is shallow and descrip-
tive and the latter is deep and explanatory. Hul-
stijn (2000) argues for the complementary aspects of
the two approaches and claims that ?dialogue games
are recipes for joint action?.
Since, on the one hand, our target tasks belong to
the class of simple service, like information-seeking
and simple transactions, which are relatively well-
structured and well-defined and not too complex for
pattern-based dialogue models, on the other hand,
there are some significant problems in using plan-
based models in practical SDSs ? those of ?knowl-
edge representation, knowledge engineering, com-
putational complexity, and noisy input? (Allen et
al., 2000), we will choose pattern-based instead of
plan-based dialogue model as our theoretical basis
for practical dialogue management at present.
2.2 Dialogue Management Models
We view dialogue management as an organic com-
bination of dialogue model with task model in some
specific domain. Its basic functionalities include in-
terpretation in context, generation in context, task
management, interaction management, choice of di-
alogue strategies, and context management. All
of them require contextual (linguistic and/or world)
knowledge.
According to how task model and dialogue model
are used, approaches to dialogue management can
be classified into four categories3 in Table 1.
Table 1: Classifying dialogue management models
Task Model
implicit explicit
Dialogue implicit DITI DITE
Model explicit DETI DETE
DITI or graph-based, both dialogue model and task
model are implicit. Dialogue is controlled via
finite state transitions (McTear, 1998). Topic
flow is predetermined. It is neither flexible nor
natural, but simple and efficient. It?s suitable
for simple and well-structured tasks similar to
automated services over ATMs or telephones
with DTMF input.
DITE or frame-based, with no explicit dialogue
model, but task is explicitly represented as a
frame or a form (Goddeau et al, 1996), a task
description table (Lin et al, 1998), a topic for-
est (Wu et al, 2000), or an agenda (Xu and
Rudnicky, 2000), etc. Both system and user
may take the initiative. Topic flow is not prede-
termined. It?s more flexible than that of DITI,
but still far from naturalness and friendliness,
since it makes no explicit use of dialogue mod-
els. Most working SDSs adopt this way of dia-
logue management.
DETI there is no practical dialogue management
3For a more comprehensive discussion on dialogue man-
agement (and SDSs), see (McTear, 2002). He identifies two
aspects of dialogue control (i.e., dialogue management) ? ini-
tiative and flow of dialogue, and three strategies for dialogue
control ? finite-state-based, frame-based, and agent-based. The
first two are similar to DITI and DITE respectively and the third
is a collection of some other approaches which are now hardly
applicable for practical dialogue management, among which is
plan-based. Our classification below is more clear.
using such a combination of task model and di-
alogue model.
DETE both dialogue model and task model are
explicit. This type of dialogue management
shares the advantages of frame-based one. At
the same time it is potential to allow of more
natural interactions according to the dialogue
model used. This is what we are after here.
2.3 The Causes behind the Gap
From the analysis above we can see the surface
gap between (DITE) dialogue management in most
working SDSs and (pattern-based) dialogue models
is mainly due to a deep one, i.e., the one between
dialogue models and the underlying tasks.
There is another important cause ? the interaction
patterns are described at the level of speech act or
dialogue act.4 To link dialogue acts to utterances,
three problems5 must be addressed at the same time:
  Dialogue act classification scheme and its reli-
ability in coding corpus, (Carletta et al, 1997;
Allen and Core, 1997; Traum, 1999);
  Choice of features/cues that can support auto-
matic dialogue act identification, including lex-
ical, syntactic, prosodic, collocational, and dis-
course cues;
  A model that correlates dialogue acts with
those features.
Some of the problems are discussed in (Jurafsky et
al., 1998; Stolcke et al, 2000; Jurafsky and Martin,
2000; Jurafsky, 2002). The empirical work on dia-
logue act classification and recognition did not begin
until some dialogue corpora (like Map Task, Verb-
mobil, TRAINS, and our NLPR-TI) were available.
But how could dialogue act recognition be suc-
cessfully applied to practical dialogue management
remains to be seen. So we choose a higher level
4Following Jurafsky (2002), we will adopt the term dia-
logue act, which captures the illocutionary force or commuca-
tive function of speech act. Though there are some arguments
in (Levinson, 1983) and others against using dialogue act to
model dialogues, and there are indeed some unresolved prob-
lems in linking dialogue acts to utterances, it will be our choice
for the time being.
5We extend Webber?s (2001) idea by splitting feature choice
out.
construct (UT-3, see section 3.1.3) to describe inter-
action patterns instead. We are by no means deny-
ing the important role dialogue act plays in dialogue
modeling, but try to incorporate high level knowl-
edge into dialogue modeling.
3 The Bridge ? GDM
Against the above gap and its causes we propose a
generic dialogue model (GDM) for task-oriented di-
alogues, which consists of five ranks of discourse
units and three levels of dialogue dynamics. It cap-
tures two important aspects of task-oriented dia-
logue ? interaction patterns at the low level and un-
derlying task at the high level.
3.1 Discourse Units
We distinguish five ranks of discourse units in de-
scribing task-oriented dialogues: dialogue, phase,
transaction, utterance group, and utterance.
3.1.1 Dialogue, Phase, and Transaction
The overall organization of a typical task-oriented
dialogue can be divided into three phases, namely,
an opening phase, a closing phase, and between
them a problem-solving phase, which can be subdi-
vided into transactions depending on how the under-
lying task is divided into subtasks. Each subtask cor-
responds to a transaction. If a task is atomic, there
will be only one transaction in the problem-solving
phase, just like the task of tourism information-
seeking.
3.1.2 Utterance Group
In performing a subtask (or task, if atomic), some
interaction patterns will recur. We name the interac-
tion patterns utterance groups (or groups, for short).
It?s also called exchanges or conversational games
(see section 2.1). The unit at this level involves com-
plex grounding process towards common ground or
mutual knowledge (Clark and Schaefer, 1989; Clark,
1996; Traum, 1994).
3.1.3 Utterance
The elementary unit in our model is utterance.
Every utterance either initiates a new group, contin-
ues, or ends an old one. Usually it is what a speaker
utters in his/her turn (for simplification, overlaps
will not be considered here). But there are some
turns with two or more utterances. These multi-
utterance turns usually end an old group with their
first utterance and begin a new one with their last ut-
terance. Similar observations are found in Verbmo-
bil corpus (Alexandersson and Reithinger, 1997).
Each utterance can be analyzed at three levels
and assigned a type correspondingly (utterance type,
UT):
UT-1 sentence type or mood, i.e., declarative, im-
perative, and interrogative (including yes-no
question (ynq), wh-question (whq), alterna-
tive question (atq), disjunctive question (djq),
which can be identified using surface lexical
and prosodic features).
UT-2 dialogue act, see section 2.3.
UT-3 a more general communicative function, rel-
ative to a group, of a small number, including
initiative (I), response/reply (R), feedback (F),
acknowledgement (A) (typical in information-
seeking dialogues), and others. It can be iden-
tified using UT-1 and semantic content (or ut-
terance topic) and preceding UT-3s, It is at this
level that interaction patterns are more obvi-
ous. What?s more, it can be recognized without
UT-2 (dialogue act) but contribute to dialogue
act recognition.
3.2 Dialogue Dynamics
By dialogue dynamics, we mean the dynamic pro-
cess within dialogues, i.e., how dialogues flow from
one partner?s utterance to another?s all the way till
the closing. The dynamic process includes that of
intra-utterance (micro-dynamics) and that of inter-
utterance. Inter-utterance dynamics is further di-
vided into intra-group dynamics (meso-dynamics)
and inter-group dynamics (macro-dynamics).
3.2.1 Micro-dynamics
Micro-dynamics deals with how discourse phe-
nomena (like anaphora, ellipsis, etc.) within one
utterance are decoded (interpretation) or encoded
(generation) in discourse context and how utterance
level intention (dialogue act) is recognized using
lexical, prosodic, and other cues and discourse struc-
ture (see section 2.3). Discourse phenomena contain
much discourse-level context information. It is those
that contribute partly to the naturalness and coher-
ence in human-human dialogues. But it?s very dif-
ficult for computers to make full use of them, either
in interpretation or in generation. They are imple-
mented in few of present SDSs, though much effort
has been put on the study of computational models
of discourse phenomena (see (Webber, 2001) for an
overview and references therein for further details).
3.2.2 Meso-dynamics
Meso-dynamics explains utterance-to-utterance
moves within one group which present recurrent
interaction patterns. Our corpus study shows that
those patterns in information-seeking dialogues are
closely related to two factors ? initiative and direc-
tion of information flow between user and server
(see section 4.1).
3.2.3 Macro-dynamics
Macro-dynamics describes inter-group moves,
which may take place intra-transactionally within
one subtask or inter-transactionally between sub-
tasks. Inter-group moves are subject to the under-
lying task. It?s difficult to give an account like intra-
group moves, because they reflect the process how
a problem is solved.The account depends on how
tasks are represented and reasoned. We may gain
some hints from the study of general problem solv-
ing in AI (Bonet and Geffner, 2001).
3.3 Discussion
GDM as we propose above is a DETE dialogue man-
agement framework with fine-grained patterns. We
discuss related work and its implication for dialogue
management below.
3.3.1 Discourse Unit
Different discourse units are used by different re-
searchers in studying discourse. In (Sinclair and
Coulthard, 1975), five ranks of units are used to ana-
lyze classroom interactions: lesson, transaction, ex-
change, move, and act. The first four roughly cor-
respond to our dialogue, transaction, group, utter-
ance. We add the unit phase and omit act, which
is a sub-utterance unit. In (Alexandersson and Re-
ithinger, 1997), four ranks of units are used to ana-
lyze meeting scheduling dialogues: dialogue, phase,
turn, and dialogue act. Turn is a natural unit that
appears in dialogues, but is it an basic unit? Four
units with conversation acts (Traum and Hinkelman,
1992; Traum, 1994), are used to analyze TRAINS
(freight scheduling) dialogues: multiple discourse
unit (argumentation act), discourse unit (core speech
act), utterance unit (grounding act), sub-utterance
unit (turn-taking act). Theirs differ a lot from ours
partly because they pay more attention to grounding.
3.3.2 Discourse Structure
In GDM the structure of discourse6 is accounted
for from two aspects: local structure is reflected
in utterance groups and shaped by meso-dynamics;
global structure is determined by the underlying task
and shaped by macro-dynamics. This is obvious to
task-oriented dialogues in view of GDM.
3.3.3 Dialogue Strategies
In most working SDSs dialogue strategies are
handcrafted by system developers. Recently there
are some efforts in applying machine learning ap-
proaches to the acquisition of dialogue strategies
(Walker, 2000; Levin et al, 2000). We hope to
find out what strategies are used in human-human
dialogue and how they could be applied to human-
computer dialogue. We first refine the concept of
dialogue strategies. From the view of GDM, the
strategies a dialogue agent may choose can also be
classified into three levels, i.e.,
Micro-level strategies how to realize information
structure, anaphora, ellipsis, and others, in ut-
terances,
Meso-level strategies what to say regarding current
group status, so as to complete ongoing group
more friendly,
Macro-level strategies how to choose discourse
topic regarding current task status, so as to
complete the underlying task more efficiently.
6Grosz and Sidner (1986) proposed a tripartite discourse
model consisting of attentional state, intentional structure, and
linguistic structure. It is influential and covers both dialogue
and text. But their intentional structure fails to capture the dis-
tinction between global level and local level structure. Their
discourse unit ? discourse segment ? is used without noticing
that there are different ranks of discourse unit in dialogues. This
is partly due to that they looked more at the similarities between
dialogue and text and less at the differences between them. Di-
alogue and text, as two types of discourse, share something in
common, but there is also something that makes them different.
3.3.4 The Complexity of Dialogue Management
Since dialogue management is closely related to
dialogue model and underlying task and domain,
the complexity of dialogue management can be de-
composed into three parts, i.e., the complexity of
dialogue model, the complexity of task, and the
complexity of domain. The complexity of dialogue
model is affected by what kind of initiative and dia-
logue phenomena are allowed. The task complexity
is affected by the number of its possible actions and
by whether it is well-structured and well-defined.
The domain complexity is affected by domain en-
tities and their relations and by the volume of in-
formation. The three are not independent but inter-
twined.
4 Utterance Groups in GDM-IS
We now apply GDM to information-seeking dia-
logues (GDM-IS) and search for interaction patterns
in the NLPR-TI corpus. We first try to classify and
segment utterance groups. This is a preliminary step
toward group pattern recognition. Details of the
recognition process and results will be given in (Xu,
2002).
4.1 Group Classification
Group patterns are recurrent, but how many? Or,
is there a limited number? In our NLPR-TI corpus
information-seeking dialogues (see section 4.2.1),
we find four basic groups with some variations.
4.1.1 Basic Groups
The recurrent patterns, according to our observa-
tion, can be classified into one of the four types in
Table 2 along two dimensions ? initiative and the di-
rection of information flow (determined using world
knowledge in the domain).
Table 2: Basic utterance groups
Information Flow
S  U U  S
Group User UISU UIUS
Initiative Server SISU SIUS
Direction of information flow In the dialogues
of information-seeking, there are two directions of
information flow: one from user to server (U  S)
and the other from server to user (S  U). In the
tourism domain, the former includes intended route
(or sight-spot, or a rough area, obligatory), intended
start time, number of tourists (optional); the latter in-
cludes start time, duration, vehicle, price, accommo-
dation, meal, schedule, and more. Server must know
the information about user?s intended route before
providing user with other information.
Initiative7 In GDM, initiative always starts a new
utterance group. It is one of utterance?s general com-
municative functions relative to a group, together
with reply, feedback, acknowledgement, as we men-
tion in section 3.1.3. Regarding one group topic
there are user initiatives (UI) and there are server ini-
tiatives (SI). Group patterns depend heavily on who
initiates the group regarding some specific topic.
This is due to the role asymmetry of the dialogue
partners.
4.1.2 Complex Groups
Though most groups can be covered by the above
basic patterns, there are some exceptions which are
more complex. They are usually embedded ones.
When one partner signals non-understanding or non-
hearing, or a normal group is suspended, one or two
more utterances will be inserted, either to repeat pre-
vious utterance or resume suspended group. The
embedded groups may also be precondition groups.
Precondition groups occur when some obligatory in-
formation is missing before the salient issue could
be addressed. Once the missing is provided, the
outer group will continue. Complex groups can also
occur when one partner lists more than one items or
does some repairing.
4.2 Group Segmentation
Given the above group classification, how to rec-
ognize them? We have to segment and classify
groups, and determine UT-3 of every utterance
within groups. This is a big problem. Only the ex-
periment on group segmentation is reported in this
paper.
7We note that there are task initiative and dialogue initia-
tive (Chu-Carroll and Brown, 1998) and there are local initiative
and global initiative (Rich and Sidner, 1998). Our initiative-in-
group is more task-related and global. For a comprehensive dis-
cussion on mixed initiative interaction, see (Haller and McRoy,
1998, 1999).
To segment a dialogue into groups is first to deter-
mine the beginning of a group, i.e., to determine if
an utterance is an initiative or not. (Multi-utterance
turns are manually segmented beforehand for sim-
plification.)
4.2.1 NLPR-TI Corpus
We use NLPR-TI corpus (Xu et al, 1999) in the
experiment. It consists of 60 spontaneous human-
human dialogues (about 5.5 hours) over telephones
on tourism information-seeking. There are total
2716 turns (1346 by the user and 1370 by the
server). The average length of user?s turns is about
7 Chinese characters and server?s about 9. The first
20 dialogues (transcript) are used for current group
segmentation.
4.2.2 Manual Segmentation
A subject was given the basic ideas about GDM
and utterance groups in GDM-IS and segmented two
dialogues with an expert?s guide before starting the
work.
To test the reliability of group segmentation
within GDM-IS, we calculate the kappa coefficient
()8 (Carletta, 1996; Carletta et al, 1997; Flam-
mia, 1998) to measure pairwise agreement between
the subject and the expert. Two coders segmented
the first 20 dialogues (totally 845 utterances). They
reached       , which shows a high reliabil-
ity. Using the expert?s segmentation as reference, we
also measure the subject?s segmentation using infor-
mation retrieval metrics ? precision (P), recall (R),
and F-measure9 (see Table 3 for the result).
4.2.3 Automated Segmentation
Three simple algorithms in Figure 1 are used to
perform the same task on the 20 dialogues. The in-
put is a semantic tag sequence produced by a statis-
tical parser (Deng et al, 2000)10.
8
             , where   is the
proportion of times that the coders agree and   is the pro-
portion of times that one would expect them to agree by chance.
? From (Carletta, 1996)
9Combined metric          , from
(Jurafsky and Martin, 2000, p.578),    .
10That we adopt such deep features in discourse segmenta-
tion is mainly due to our target application ? dialogue manage-
ment. This makes it different from others using surface features
like (Passonneau and Litman, 1997).
I. Using topic only for segmentation
if topic is new
then UT-3 = initiative
else UT-3 = non-initiative
II. Using UT-1 only for segmentation
if UT-1  interrogatives
then UT-3 = initiative
else UT-3 = non-initiative
III. Using both for segmentation
if topic is new  UT-1  interrogatives
then UT-3 = initiative
else UT-3 = non-initiative
Figure 1: Group segmentation algorithms
Given the semantic tag sequence of an utterance,
we determine its topic11 and UT-1 (what we are most
interested in is interrogatives (ynq, whq, atq, and
djq)). Since the parser performs with an error rate
of , there will be some wrong semantic tags,
which lead to errors in assigning UT-1 and topic.
Then we use the three simple algorithms to seg-
ment groups in the 20 dialogues. Their performance
(also using the expert?s segmentation as reference)
is given in Table 3.
Table 3: Group segmentation results
subject I II III
Precision .88 .59 .67 .83
Recall .92 .82 .62 .56
F-measure .90 .69 .64 .67
4.3 Discussion
Table 3 shows the results of group segmentation,
both manual and automated. Though none of the
three algorithms outperforms the subject, they all
show that topic change and UT-1 as interrogative
are acceptable and also good indicators of utterance
group beginning, esp. when topic and UT-1 are the
11We presume that the topic of an utterance is the last one in
the candidate tags. This seems problematic but is true to most of
the utterances according to our observation. How to determine
the topic of an utterance needs further study.
only information sources and when discourse mark-
ers (Schiffrin, 1987) in spontaneous speech are un-
available in current deep analysis.
There is no obvious performance difference in
segmenting dialogue into groups with the three al-
gorithms. The performance of algorithm I may be
improved if the noises brought by the parser and
our simple topic identification algorithm are cleared.
This implies that topic change is a potentially bet-
ter indicator of the beginning of new groups. The
result using UT-1 only is the worst. This is partly
because not all groups begin with interrogatives and
that interrogatives do not always occur at the begin-
ning of a group. When using both topic and UT-1,
the performance changes little, though seemly more
constraints are used. This possibly is because topic
change and UT-1 as interrogative overlap a lot.
5 Conclusions
After a survey of the main approaches to dialogue
modeling and dialogue management in working
SDSs, we find the causes behind the gap between
practical dialogue management and dialogue models
and propose GDM, which consists of five ranks of
discourse units and three levels of dialogue dynam-
ics. It promises to bridge the gap through integrat-
ing meso-dynamics at the group level with macro-
dynamics at the task level, and modeling interaction
patterns via utterance groups using UT-3.
Then we apply it to information-seeking dia-
logues and elaborate utterance groups (or interaction
patterns) in the model. We also classify and seg-
ment utterance groups in our information-seeking
corpus, which takes a preliminary step toward bet-
ter dialogue modeling for practical dialogue man-
agement with empirical justification. A more chal-
lenging task ? group pattern recognition ? is under
way (Xu, 2002). After that we will investigate how
local discourse structure in terms of utterance group
structure could contribute to the recognition of dia-
logue act (UT-2).
GDM takes a step further toward better dialogue
modeling for practical dialogue management with
empirical justification. It is expected to be used in
practical dialogue management in the near future for
better usability and portability.
Acknowledgments
The work described in this paper was partly sup-
ported by the National Key Fundamental Research
Program (the 973 Program) of China under the grant
G19980300504 and the National Natural Science
Foundation of China under the grant 69835003.
References
Jan Alexandersson and Norbert Reithinger. 1997. Learn-
ing dialogue structures from a corpus. In Proceedings
of the 5th European Conference on Speech Communi-
cation and Technology, volume 4, pages 2231?2234.
James Allen and Mark Core. 1997. Draft of damsl:
Dialog act markup in several layers. Available from
http://www.cs.rochester.edu/research/
cisd/resources/damsl/.
James F. Allen and C. Raymond Perrault. 1980. Ana-
lyzing intention in utterances. Artificial Intelligence,
15(3):143?178.
James Allen, George Ferguson, Bradford W. Miller,
Eric K. Ringger, and Teresa Sikorski Zollo, 2000. Di-
alogue Systems: From Theory to Practice in TRAINS-
96, chapter 14, pages 347?376. In Dale et al (Dale et
al., 2000).
J. L. Austin. 1962. How to do Things with Words.
Clarendon Press, Oxford.
Blai Bonet and He?ctor Geffner. 2001. Planning and Con-
trol in Artificial Intelligence: A Unifying Perspective.
Applied Intelligence, 14(3):237?252.
Gillian Brown and George Yule. 1983. Discourse Anal-
ysis. Cambridge University Press.
Sandra Carberry. 1990. Plan Recognition in Natural
Language Dialogue. ACL-MIT Press Series in Nat-
ural Language Processing. A Bradford book, MIT
Press, Cambridge, Massachusetts.
J. Carletta, A. Isard, S. Isard, J. C. Kowtko, G. Doherty-
Sneddon, and A. H. Anderson. 1997. The reliability
of a dialogue structure coding scheme. Computational
Linguistics, 23(1):13?31.
Jean Carletta. 1996. Assessing agreement on classifica-
tion tasks: The Kappa statistic. Computational Lin-
guistics, 22(2):249?254.
Lari Carlson. 1983. Dialogue Games: An Approach to
Discourse Analysis. D. Reidel, Dordrecht, Holland.
Jennifer Chu-Carroll and Michael K. Brown. 1998. An
evidential model for tracking initiative in collabora-
tive dialogue interactions. User Modeling and User-
Adapted Interaction, 8(3-4):215?253.
Herbert H. Clark and Edward F. Schaefer. 1989. Con-
tributing to discourse. Cognitive Science, 13:259?294.
Herbert H. Clark. 1996. Using Language. Cambridge
University Press.
Philip Cohen, Jerry Morgan, and Martha Pollack , editors.
1990. Intentions in Communication. MIT Press.
P. R. Cohen and H. J. Levesque. 1991. Teamwork. Nou?s,
25(4):487?512.
P R. Cohen and C. R. Perrault 1979. Elements of a
plan-based theory of speech acts. Cognitive Science,
3(3):177?212.
Phil Cohen, 1998. Dialogue Modeling, chapter 6.3. In
Cole et al (Cole et al, 1998).
Ronald Cole, Joseph Mariani, Hans Uszkoreit, Giovanni
Varile, Annie Zaenen, Antonio Zampolli, and Victor
Zue, editors. 1998. Survey of the State of the Art in
Human Language Technology. Cambridge University
Press, Cambridge.
Malcolm Coulthard, editor. 1992. Advances in Spoken
Discourse Analysis. Routledge. London.
Robert Dale, Hermann Moisl, and Harold Somers, edi-
tors. 2000. Handbook of Natural Language Process-
ing. Marcel Dekker. New York.
Yunbin Deng, Bo Xu, and Taiyi Huang. 2000. Chi-
nese spoken language understanding across domain.
In Proceedings of the 6th International Conference on
Spoken Language Processing, volume 1, pages 230?
233.
Richard Fikes and Nils J. Nilsson. 1971. STRIPS: A
new approach to the application of theorem proving to
problem solving. Artificial Intelligence, 2(3-4):189?
208.
Giovanni Flammia. 1998. Discourse segmentation of
spoken dialogue: an empirical approach. Ph.D. the-
sis, MIT.
D. Goddeau, H. Meng, J. Polifroni, S. Seneff, and
S. Busayapongchai. 1996. A form-based dialogue
manager for spoken language applications. In Pro-
ceedings of the 4th International Conference on Spo-
ken Language Processing, volume 2, pages 701?704.
Barbara J. Grosz and Sarit Kraus. 1996. Collaborative
plans for complex group action. Artificial Intelligence,
86(2):269?357.
Barbara J. Grosz and Candace L. Sidner. 1986. Atten-
tion, intention, and the structure of discourse. Compu-
tational Linguistics, 12(3):175?204.
Barbara J. Grosz and Candace L. Sidner. 1990. Plans for
Discourse. In Cohen et al (Cohen et al, 1990).
Susan Haller and Susan McRoy, editors. 1998, 1999.
User Modeling and User-Adapted Interaction, Special
Issue on Computational Models for Mixed Initiative
Interaction, 8(3-4),9(1-2).
Joris Hulstijn. 2000. Dialogue games are recipes for joint
action. In Proceedings of the Forth Workshop on the
Semantics and Pragmatics of Dialogue (Gotalog?00).
Biing-Hwang Juang and Sadaoki Furui, editors. 2000.
Proceedings of the IEEE , Special Issue on Spoken
Language Processing, 88(8).
Daniel Jurafsky, Rebecca Bates, Noah Coccaro, Rachel
Martin, Marie Meteer, Klaus Ries, Elizabeth Shriberg,
Andreas Stolcke, Paul Taylor, and Carol Van Ess-
Dykema. 1998. Switchboard discourse language
modeling project report. Technical Report Research
Note No. 30, Center for Speech and Language Pro-
cessing, Johns Hopkins University, Baltimore, MD.
Daniel Jurafsky and James H. Martin. 2000. Speech
and Language Processing: An Introduction to Natural
Language Processing, Speech Recognition, and Com-
putational Linguistics. Prentice-Hall.
Daniel Jurafsky, 2002. Pragmatics and Computational
Linguistics. To appear in Laurence R. Horn and Gre-
gory Ward, editors. Handbook of Pragmatics. Black-
well, Oxford.
J. Kowtko, S. Isard, and G. M. Doherty. 1992. Conver-
sational games within dialogue. Research Paper 31,
Human Communication Research Centre, Edinburgh
University, Edinburgh.
Lynn Lambert and Sandra Carberry. 1991. A tripartite
plan-based model of dialogue. In Proceedings of the
29th Annual Meeting of the Association for Computa-
tional Linguistics, pages 47?54, Berkeley, CA.
L. Lamel, S. Rosset, J.L. Gauvain, S. Bennacef,
M. Garnier-Rizet, and B. Prouts. 2000. The LIMSI
ARISE system. Speech Communication, 31(4):339?
354.
Esther Levin, Roberto Pieraccini, and Wieland Eckert.
2000. A stochastic model of human-machine interac-
tion for learning dialog strategies. IEEE Transactions
on Speech and Audio Processing, 8(1):11?24.
Stephen C. Levinson. 1983. Pragmatics. Cambridge
University Press.
Y. Lin, T. Chiang, H. Wang, C. Peng, and C. Chang.
1998. The design of a multi-domain mandarin Chinese
spoken dialogue system. In Proceedings of the 5th In-
ternational Conference on Spoken Language Process-
ing, volume 1, pages 230?233.
Diane J. Litman and James F. Allen. 1987. A plan recog-
nition model for subdialogues in conversation. Cogni-
tive Science, 11(2):163?200.
Diane J. Litman and James F. Allen. 1990. Discourse
Processing and Commonsense Plans. In Cohen et al
(Cohen et al, 1990).
Karen E. Lochbaum, Barbara J. Grosz, and Candace L.
Sidner, 2000. Discourse Structure and Intention
Recognition, chapter 6, pages 123?146. In Dale et al
(Dale et al, 2000).
William C. Mann. 2001. The genre diversity of dia-
logue game theory. Available from http://www-
rcf.usc.edu/ billmann/memos.htm.
Michael F McTear. 1998. Modelling spoken dialogues
with state transition diagrams: experiences with the
CSLU toolkit. In Proceedings of the 5th Interna-
tional Conference on Spoken Language Processing,
volume 2, pages 1223?1226.
Michael F. McTear. 2002. Spoken dialogue technology:
Enabling the conversational user interface. ACM Com-
puting Surveys,34(1):90?169.
Els den Os, Lou Boves, Lori Lamel, and Paolo Baggia.
1999. Overview of the ARISE project. In Proceedings
of the 6th European Conference on Speech Communi-
cation and Technology, volume 4, pages 1527?1530.
Rebecca Passonneau and Diane Litman. 1997. Discourse
segmentation by human and automated means. Com-
putational Linguistics, 23(1):103?140.
Charles Rich and Candace L. Sidner. 1998. Colla-
gen: A collaboration manager for software interface
agents. User Modeling and User-Adapted Interaction,
8(3-4):315?350.
H. Sacks, E. A. Schegloff, and G. Jefferson. 1974.
A simplest systematics for the organization of turn-
taking for conversation. Language, 50(4):696?735.
Deborah Schiffrin. 1987. Discourse Markers. Cam-
bridge University Press.
J. R. Searle. 1969. Speech Acts. Cambridge University
Press.
John M. Sinclair and Malcolm Coulthard. 1975. Towards
an Analysis of Discourse: The English Used by Teach-
ers and Pupils. Oxford University Press.
Bernd Souvignier, Andreas Kellner, Bernhard Rueber,
Hauke Schramm, and Frank Seide. 2000. The
thoughtful elephant: Strategies for spoken dialog sys-
tems. IEEE Transactions on Speech and Audio Pro-
cessing, 8(1):51?62.
Andreas Stolcke, Klaus Ries, Noah Coccaro, Eliza-
beth Shriberg, Rebecca Bates, Daniel Jurafsky, Paul
Taylor, Rachel Martin, Carol Van Ess-Dykema, and
Marie Meteer. 2000. Dialogue act modeling for
automatic tagging and recognition of conversational
speech. Computational Linguistics, 26(3):339?371.
David R. Traum and Elizabeth A. Hinkelman. 1992.
Conversation acts in task-oriented spoken dialogue.
Computational Intelligence, 8(3):575?599.
David R. Traum. 1994. A Computational Theory of
Grounding in Natural Language Conversation. Ph.D.
thesis, University of Rochester.
David R. Traum. 1999. 20 questions for dialogue act tax-
onomies. In Proceedings of the Third Workshop on the
Semantics and Pragmatics of Dialogue (Amstelog?99).
Marilyn A. Walker. 2000. An application of reinforce-
ment learning to dialogue strategy selection in a spo-
ken dialogue system for email. Journal of Artificial
Intelligence Research, 12:387?416.
Bonnie Webber. 2001. Computational Perspectives on
Discourse and Dialogue. In Deborah Schiffrin, Deb-
orah Tannen, and Heidi Hamilton, editors. Handbook
of Discourse Analysis. Blackwell, Oxford.
Xiaojun Wu, Fang Zheng, and Mingxing Xu. 2000.
Topic forest: A plan-based dialog management struc-
ture. In Proceedings of ICASSP, volume 1, pages 617?
620.
Wei Xu and Alexander I. Rudnicky. 2000. Task-based di-
alog management using an agenda. In Proceedings of
ANLP/NAACL 2000 Workshop on Conversational Sys-
tems, pages 42?47.
B. Xu, T.Y. Huang, X. Zhang, and C. Huang. 1999. A
Chinese spoken dialogue database and its application
for travel routine information retrieval. In Proceed-
ings of the Second International Workshop on East-
Asia Language Resources and Evaluation, Taipei.
Weiqun Xu. 2001. Survey of the state of the art in spoken
dialogue systems. Manuscript.
Weiqun Xu. 2002. Grouping utterances in information-
seeking dialogues. In preparation.
Victor Zue and Jim Glass. 2000. Conversational inter-
faces: Advances and challenges. Proceedings of the
IEEE, 88(8):1166?1180.
Interactive Chinese-to-English Speech Translation Based on  
Dialogue Management 
 Chengqing Zong, Bo Xu, and Taiyi Huang 
National Laboratory of Pattern Recognition 
Institute of Automation, Chinese Academy of Sciences 
P. O. Box 2728, Beijing 100080, China 
{cqzong, xubo, huang}@nlpr.ia.ac.cn 
 
 
 
Abstract 
In this paper, we propose a novel 
paradigm for the Chinese-to-English 
speech-to-speech (S2S) translation, which 
is interactive under the guidance of 
dialogue management. In this approach, 
the input utterance is first pre-processed 
and then serially translated by the 
template-based translator and the inter-
lingua based translator. The dialogue 
management mechanism (DMM) is 
employed to supervise the interactive 
analysis for disambiguation of the input. 
The interaction is led by the system, so 
the system always acts on its own 
initiative in the interactive procedure. In 
this approach, the complicated semantic 
analysis is not involved. 
1 Introduction 
2 
Over the past decade, many approaches to S2S 
translation have been proposed. Unfortunately, the 
S2S translation systems still suffer from the poor 
performance, even though the application domains 
are restricted. The common questions are: what 
translation strategies are necessary? What do the 
problems exist in the current S2S systems? And 
what performance of a system is acceptable? 
Based on the questions, we have analyzed the 
current approaches to machine translation (MT) 
and investigated some experimental systems and 
the user?s requirements. A novel paradigm for the 
Chinese-to-English S2S translation has been 
proposed, which is interactive under the guidance 
of DMM. In this approach, the input utterance is 
first pre-processed and serially translated by the 
template-based translator and the inter-lingua 
based translator. If the two translators are failed to 
translate the input, the dialogue management 
mechanism is brought into play to supervise the 
interactive analysis for disambiguation of the input. 
The interaction is led by the system, so the system 
always acts on its own initiative in the interactive 
procedure. In this approach, the complicated 
semantic analysis is not involved.  
Remainder of the paper presents our 
motivations and the proposal scheme in detail. 
Section 2 gives analysis on the current MT 
approaches and the user?s requirements. Section 3 
describes in detail our approach to Chinese-to-
English S2S translation. Section 4 draws 
conclusions and presents the future work. 
Analysis on MT approaches and S2S 
translation systems 
2.1 Analysis on MT approaches 
In the past decades, many MT approaches have 
been proposed. We roughly divided the current 
approaches into two types, which are respectively 
named as the mainstream approaches and the non-
mainstream approaches. The mainstream 
approaches include four basic methods: the 
analysis-based method, the example-based method, 
the template-based method and also the statistical 
method as well. The analysis-based method here 
includes the rule-based method, the inter-lingual 
method, or even the knowledge-based method. In 
the recent years, the approach based on multi-
engine has been practiced in many systems (Lavie, 
                                            Association for Computational Linguistics.
                           Algorithms and Systems, Philadelphia, July 2002, pp. 61-68.
                          Proceedings of the Workshop on Speech-to-Speech Translation:
1999; Wahlster, 2000; Zong, 2000a). However, the 
engines employed in these experimental systems 
are mainly based on the four mainstream methods. 
The strong points and the weak points of the four 
methods have been analyzed in many works (Zong, 
1999; Ren, 1999; Zhao, 2000). 
The non-mainstream approach here refers to 
any other methods exclusive of the four methods 
mentioned above. To improve the performance of 
MT systems, especially to cope with the specific 
problems in S2S translation, many schemes have 
been proposed. Ren (1999) proposed a super-
function based MT method, which tries to address 
the MT users? requests and translates the input 
without thorough syntactic and semantic analysis. 
The super-function based MT system is fast, 
inexpensive, easy to control and easy to update. 
However, the fluency and the correctness of the 
translation results are usual not high. Moreover, to 
extract the practical super-functions from the 
corpus is also a hard work. Yamamoto et al (2001) 
proposed a paradigm named Sandglass. In the 
sandglass system, the input utterances from a 
speech recognizer are paraphrased firstly, and the 
paraphrased text is passed to the transfer controller. 
The task of the paraphrasing module for the source 
language is to deal with noisy inputs from the 
speech recognizer and provides different 
expressions of the input. An obvious question 
about the Sandglass is why the system would 
rather rewrite the input than to translate it directly?  
Zong et al (2000b) proposed an MT method based 
on the simple expression. In the method the 
keywords in an input utterances are spotted out 
firstly and the dependence relation among the 
keywords are analyzed. Then, the translation 
module searches the examples in the knowledge 
base according to the keywords and their 
dependence relation. If an example is matched with 
the conditions, the target language expression of 
the example is sent out as the translation result of 
the input. When the input is not very long, and the 
domain and the type of the input are restricted, the 
method is very practical. However, to develop the 
knowledge base with dependence relation of 
keywords and to match an input with all examples 
in the knowledge base are sometimes difficult. 
Wakita et al (1997) proposed a robust translation 
method which locally extracts only reliable parts, 
i.e., those within the semantic distance threshold 
and over some word length. This technique, 
however, does not split input into units globally, or 
sometimes does not output any translation result 
(Furuse et al 1998). In addition, the method 
closely lies on the semantic computation, and 
sometimes it is hard to compute the semantic 
distance for the spoken utterances. 
In summary, both mainstream MT methods and 
non-mainstream methods have been practiced in 
many experimental S2S translation systems. 
However, all methods mentioned above are 
unilateral and based on user's own wishful thinking. 
The system is passive and blind in some extent. 
The task that machine translates is imposed by 
human, and some problems are also brought by the 
speaker, e.g., the topics are changed casually, or 
the ill-formed expressions are uttered. In these 
cases, it is unreasonable to expect the system to get 
the correct translation results, but not to give the 
system any rights to ask the speaker about his or 
her intention or some ambiguous words. In fact, if 
we examine the procedures that human interpreters 
use, we can see that the translation is usually 
interactive. When an interpreter is unable to 
directly translate an utterance due to an ill-formed 
expression or something even worse, the 
interpreter may have to ask the speaker to repeat or 
explain his / her words. Based on the ideas, the 
interactive paradigms for S2S translation have 
been proposed (Blanchon, 1996; Waibel, 1996; 
Seligman, 1997; Seligman, 2000; Ren, 2000). 
Seligman (2000) proposed a ? quick and dirty? or 
?low road? scheme, in which he suggested that, by 
stressing interactive disambiguation, practically 
usable speech translation systems may be 
constructable in the near term. In addition, two 
interactive MT demos were shown respectively in 
1997 and 1998 (Seligman, 2000). However, all the 
proposed interactive schemes and the demos put 
the emphasis on the interface between speech 
recognition (SR) and analysis. The interface can be 
supplied entirely by the user, who can correct SR 
results before passing them to translation 
components. That means the translation system is 
still passive. Actually, as we know that the parsing 
results and the translation results are not certainly 
correct even though the input is completely correct, 
but some noisy words usually have not any 
influence whether they are correct or not. In this 
sense, the user should know what the system needs? 
And what brought the system ambiguity? This 
means, the system has rights and obligations to tell 
the user what the system want to know. In another 
words, the system necessitates a DMM to guide the 
interaction between the system and user, and 
sometimes the system should play the leading role.  
2.2 Analysis on user?s requirements 
Although much progress in SR and spoken 
language parsing has been made, there is still a 
long way to reach the final and ideal goal that the 
translation results are complete correct. In this 
situation, let?s think does a user always need the 
complete correct translation results? Please see the 
following three examples: 
(1) Input: ?????? ????????
?????????????? (Oh, 
that ? well, please reserve a single room 
for me, sure, a single room.) 
In the input, there are many redundant words, 
such as, ?(Oh)???(that), ???(well) and so 
on. If all words in the input are translated, the 
translation result is verbose and wordy. In fact, in 
the input only three keywords are useful, which are: 
??(reserve), ??(one), and ???(single room) 
as well. The preposition phrase ???(for me)? is 
not  obligatory. Even the word ???? is also not 
obligatory.  
(2) Input: ? ? ? ? ? ?? ? ?(Is this ? 
Xiang Ge Li La? Hotel?) 
In the example, the four characters with 
underline are originally a hotel name ????
??(Shangri-la), but they are wrong transliterated 
and separated due to the absence of the word in the 
SR dictionary. In this case, it is impossible to 
correctly parse the input without user?s help.  
(3) Input: ??? ? ?? ? ?? ?? ???(Is 
there any ? ask ... have? route to 
Huangshan mountain?) 
The input is a result of the SR component. 
Obviously, in the input two characters with 
stressing dots are wrong recognized from the 
original word ???  (tour)?. In this case, if all 
words are translated, the results will be 
inconceivable. On the contrary, the result is quite 
understandable if the two characters with stressing 
dots are omitted or ignored.  
The example (1) shows that if the input is 
recognized completely correct, the parsing result is 
still probably wrong due to the ill-formed 
expression of the input. The example (2) means 
that it is impossible to correctly parse the input due 
to the unknown word and its incorrect recognition. 
The example (3) shows that even though the 
expression is formal and there is not any unknown 
word in the input, the result of SR is still probably 
wrong. The parser is impossible to correctly 
analyze the wrong SR result.  
From the three examples we can easily get the 
following standpoints: a) the user expects his or 
her intentions to be translated rather than his (her) 
all words. The keywords and their dependence 
relations are the main objects to hold the user?s 
intentions. b) For the translation component, it is 
not indispensable to correct all mistakes in the 
input from the SR component. c) If the parser is 
failed to parse the input, and the system only 
translates the keywords, the translation results may 
be still understandable and acceptable. 
3 Interactive translation based on 
dialogue management 
3.1 Overview of the paradigm 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Utterances
SR N-best Speaker 
Pre-processor M
achine learning
Interactive
interface BP identifier
Uttr. segment.DMM 
n partsInputF
Template-based 
translator FParser &Evaluation 
ResultsS Inter-lingua S
Language 
generator 
TTS 
Target speech Slot-based translator
 
 
Figure 1. The paradigm of interactive translation 
Based on the analysis on MT approaches and the 
user?s requirements, we propose an interactive 
paradigm for the S2S translation, which is based 
on the template-based translation, inter-lingual 
translation and the DMM based translation as well. 
The paradigm is shown as Figure 1. 
Where, the letter S beside the line with arrow 
means that the results of the former module are 
successful, and the letter F means the results are 
failure. 
According to the paradigm, an input from the SR 
component is probably processed and translated by 
the following four steps. First, the input is pre-
processed. Some noisy words are recognized, some 
repeated words are deleted, and the numbers are 
processed (Zong, 2000a). Then the base phrases 
(BP) in the input are identified, which include 
noun phrase (NP) and verb phrase (VP) mainly. 
And also, if the input is a long utterance containing 
several simple sentences or some fixed expressions, 
the input is possibly segmented into n parts. n is an 
integer, and n ? 1. Second, each part of the input is 
passed to the template-based translator. If the input 
part is matched with a translation template, the 
translation result is sent to the text-to-speech (TTS) 
synthesizer directly. Otherwise, the input part will 
be passed to the inter-lingual translator. Third, in 
the inter-lingual translator, the input is parsed and 
the parsing results are evaluated. If the evaluation 
score is bigger than the given threshold value, the 
parsing results will be mapped into the inter-lingua, 
and the translation result will be generated by the 
inter-lingua based target language generator. 
Otherwise, the system performs the fourth step. 
Fourth, DMM works to supervise the interaction 
for disambiguation of the input. In the interaction, 
the user is asked to answer some questions 
regarding to the input part. The system will fill the 
slots according to the question-answers. The slots 
are designed to express the user?s intentions in the 
input. The system directly generates the translation 
result according to the slots. So, the translation in 
the fourth step is named as slot-based translation. 
Where, the template-based translator employs 
the forward maximum match algorithm (Zong, 
2000c). The inter-lingua uses the interchangeable 
format (IF) developed by C-STAR (Consortium for 
Speech Translation Advanced Research). The 
parser oriented to IF is realized on the basis of 
HMM spoken language understanding model. In 
the experimental system we use the tri-gram to 
compute the probability of the sequence of 
semantic units (Xie, 2002). The IF-based language 
generator employs a task-oriented micro-planner 
and a general surface realizer. The target language 
is generated by the combination of template 
method and generation technology (Wu, 2000). 
The generic DMM has been proposed by (Xu, 
2001), which combines both interaction patterns 
and task structure. The machine learning module is 
taking charge of recording the dialogue patterns, 
topics and modifying the dialogue history, and so 
on. This module is still under construction. 
3.2 Utterance segmentation 
In an S2S translation system, how to split the long 
input utterances is one of the key problems, 
because an input is often uttered by the 
spontaneous speech, and there is not any special 
mark to indicate which word is the beginning or 
the end of each simple sentence inside the 
utterance. In our system an input Chinese utterance 
is first split by the SR component according to the 
acoustic features, including the prosodic cues and 
pause etc. Suppose an input utterance has been 
transcribed by SR and separated into k parts P1, 
P2, ? Pk (k is an integer, and k ? 1.). Each part Pi 
(i?[1 .. k]) is possibly further segmented into m 
(m is an integer and m?1) units U1, U2, ?, Um by 
the segmentation module based on the linguistic 
analysis (SBLA). Where, all Pi (i?[1 .. k]) and Uj 
(j?[1 .. m]) are called as the split units in our 
system. A split unit is one of the following 
expressions: 
z A single word. 
z A fixed expression, such as a greeting 
phrase in Chinese. 
z A simple sentence. 
z A clause indicated by some special 
conjunctions. For example, an input similar 
with the pattern ???(because) ? , ??
(therefore) ? ? will be separated into two 
parts ? ? ? (because)?? and ? ? ? 
(therefore) ? ?. 
Each Pi (i?[1 .. n]) is analyzed and segmented 
by SBLA through the following three steps: 
splitting on the shallow level, splitting on the 
middle level, and splitting on the deep level. This 
means if a string S is separated into n parts by 
using the method on the shallow level, each part 
will possibly be further segmented by the method 
on the middle level, and so on.  
3.3 Slot-based translation with DMM 
The slot-based translation with DMM is built on 
the following viewpoints and hypothesis: 1) there 
are some noisy words or ambiguous words in the 
results from SR component, but the keywords are 
recognized correctly; 2) the user?s intentions lie on 
the keywords and their dependence relations; and 3) 
the translation results based on the keywords are 
understandable and reflect the main intentions of 
the user. The slot-based translation under the 
guidance of DMM is performed as the following 
steps:  
i) Re-analyze the original input string, spot out 
the keywords, and also do the analysis on the 
dependence relation of the keywords.  
ii) Interact with the user, make decision about 
the keywords and their dependence relation, 
and fill the slots for the translation.  
iii) Generate the translation results according to 
the slots.  
iv) DMM writes down the keywords and their 
dependence relations and modifies the 
dialogue history. 
3.3.1 Keywords spotting and dependence 
analysis 
According to the evaluation score, if the parsing 
result of an input part is too worse, the parsing is 
treated as failure, and all analysis results, including 
base phrases, are ignored. The system will spot out 
the keywords from the original input and analyze 
the dependence relation among the keywords. 
Please note that the dependence relation of the 
keywords in this component is used for seizing the 
user?s intentions and generating the translation 
results. It is different with the function in the 
simple expression based translation (Zong, 2000b). 
In a specific domain, it is easy to define some 
keywords according to the statistical results of the 
collected corpus. In our system, a word is treated 
as the keyword if the following two conditions are 
met: 
? The part-of-speech (POS) of the word is 
one of the following three POSs: noun (N), 
verb (V), and adjective (A), and the word 
occurs with high probability in the specific 
domain. 
? The word is a number or a time word. 
In our method, the verb keyword is always treated 
as the center when the dependence relations are 
analyzed. The dependence relations between the 
verb keyword and the noun keywords are defined 
as four types: (1) agent, (2) direct object, (3) 
indirect object, and (4) the pivot word as well. The 
agent is usually located at the left of the verb 
keyword. In general, the direct object, indirect 
object, and the pivot word all occur at the right of 
the verb keyword. The pronoun is treated as the 
noun. Other content words are treated as the 
modification words of the keywords. The search 
direction and the position relation may be shown as 
the following Figure 2. Where, Wi means a 
common word, and KWi means a keyword.. 
 
W1  ?  KW1(verb) ?  Wi  ?  KW2(noun)?  
 modifications 
 agent object / pivot word 
 
Figure 2. Keywor lations 
According to the chara
verbs, there are five case
? There is no object af
? There is one object o
? There are two objec
and another one is t
? The object is a claus
? After the verb keyw
object (pivot word
agent of another fol
In the keyword dictionar
its all possible POSs a
DMM asks the user q
features of a specific ver
3.3.2 Interaction and s
In the DMM module, 
express the user?s inten
series of slots as follows
 ds and their recteristics of the Chinese 
s respectively: 
ter the verb; 
nly; 
ts. One is the direct object 
he indirect object. 
e. 
ord, the first noun is the 
) and acts as the role of 
lowed verb. 
y, each verb is tagged with 
nd relative features. The 
uestions according to the 
b, its context, and the slots. 
lot filling 
a frame is designed to 
tions, which consists of a 
.  
Frame: ACTION: Keywords (verb)  
 TENSE: {Present/Past/?} 
EXP. TYPE: {Interrogative/?} 
AGENT: noun; 
OBJECT1: noun; 
OBJECT2: noun; 
QUANTITY1: numeral; 
UNIT1: classifier; 
QUANTITY2: numeral; 
UNIT2: classifier; 
TIME: numeral & classifier; 
 HOW: adjective; 
 
 Figure 3. Frame of slots 
 
Where, QUANTITY1 and UNIT1 modify the 
agent, QUANTITY2 and UNIT2 modify the 
OBJECT1 or OBJECT2. Because the keywords 
have been spotted out and their dependence 
relations have been analyzed, the DMM asks the 
user according to the analysis results and the 
concrete context. Please see the following example. 
Input: ? ? ? ? ? ?? ? ?(Is this ? 
Xiang Ge Li La ? Hotel?) 
Two keywords, ??(be)? and ???(hotel)?, are 
spotted, and the word ???? is recognized as the 
object of the verb ?? ?. i.e.,  ACTION=? ; 
TENSE=Present; EXP. TYPE= Interrogative; 
OBJECT1=?? ; and other slots are empty. 
However, there are four noisy characters between 
the two keywords. The DMM will ask the user by 
using the question pattern: ???(what) X ??. The 
variable X is just replaced with the keyword ????. 
The user needs to answer the hotel name. Because 
the SR module still does not recognize the speech 
of the word ?????(Shangri-la)?, the DMM is 
unable to parse the user?s answer. The following 
dialogue will be done: 
System: ?????????????????
(Is the word ?????? an adjective or a noun?) 
User:   (?)??(It is a noun.) 
System: ??????? ? (Is it the hotel 
name?) 
User:  ??(yes). 
System: ??????????? (Please 
input the English name of the word ??????). 
The DMM will append the word ?????? 
both into the SR dictionary and translation 
dictionary and treat it as the attribute of the 
keyword ????. The input is finally translated by 
using the template ?Is this the X ??.  
3.3.3 Generation based on slots 
After the interaction, the translation result will be 
generated based on the templates that are consisted 
of the slots. For example, if AGENT and ACTION 
are filled, the EXP. TYPE = Statement, and other 
slots are empty. The generation template 
is: !AGENT !ACTION. Where, !AGENT means 
the English word corresponding to the Chinese 
word in the AGENT slot. !ACTION is the English 
word corresponding to the Chinese word in the 
ACTION slot. However, the morphology of the 
verb will be changed according to the agent. 
From the frame of slots we can see that the 
frame can only express the analysis results of 
simple sentence. So, the translation result is always 
expressed by the simple sentence. If the subject or 
the object of a Chinese input is a clause, the input 
will be translated into two or more simple English 
sentences. For instance,  
Input: ??????????????
(How much does it cost if I reserve two single 
rooms?) 
The input will be mapped into two frames. In 
the first frame, AGENT=?; ACTION=??; EXP. 
TYPE=Statement; QUANTITY2=?; UNIT2=?; 
OBJECT1= ? ? ? . In the second frame, 
ACTION= ? ? ; EXP. TYPE= Interrogative; 
QUANTITY1=?? ; OBJECT1=? . Therefore, 
the input is separately translated into two simple 
English sentences: ?I reserve two single rooms.?, 
and ?How much does it cost??. Obviously, in the 
specific context, the results are completely 
understandable and acceptable. 
4 Conclusion 
This paper describes a new paradigm for S2S 
translation system, which is based on DMM. 
According to the description we can see that the 
paradigm is of the following features: 
(1) The S2S translation is realized in the 
combination of direct translation 
engines and the interaction led by DMM. 
The interaction is not always brought 
into the role, and it only works when the 
former translation engines work failed.  
(2) The interaction is impersonative, target- 
oriented, and led by the system, not 
blind. The user does not need to correct 
all of the errors in the results of SR. He 
or she only needs to concern what the 
system asks. 
(3) The system can always give the results 
for an input speech despite of the ill-
formed expressions and the worse 
recognition results. 
Although the whole experimental system is under 
construction, some preliminary results have been 
gained. Zong (2000c) reported the performance of 
the template-based translator; Xie (2002) reported 
the results of the robust parser for the Chinese 
spoken language; Xu (2001) presented the results 
of dialogue model; and so on. The results have 
made us confident to develop the practical S2S 
translation system based on the dialogue 
management. However, we are facing much hard 
work that involve the following aspects at least: 
? Develop the reasonable strategies and 
standards to evaluate the parsing results; 
? Design the effective templates to ask the 
user questions according the keywords and 
the concrete context; 
? Define the practical templates to generate 
the translation results; 
? Build the machine learning mechanism to 
enrich the knowledge base of the system. 
References 
Blanchon, H. 1996. A Customizable Interactive 
Disambiguation Methodology and Two 
Implementations to Disambiguate French and 
English Input. In Proceedings of MIDDIM-96 
(International Seminar on Multimodal Interactive 
Disambiguation), Col de Porte, Fance. 
Furuse, O., Satsuo Yamada and Kazuhide Yamamoto. 
1998. Splitting Long or Ill-formed Input for Robust 
Spoken-language Translation. In Proceeding of 
COLING-ACL, Canada. Vol. I, pp. 421-427. 
Lavie, A., Lori Levin et al 1999. The JANUS-III 
Translation System: Speech-to- Speech Translation 
in Multiple Domains. In Proceedings of C-STAR II 
Workshop, Schwetzingen of Germany, 24 Sept., 
1999. 
Ren, F., Shigang Li. 2000. Dialogue Machine 
Translation Based upon Parallel Translation Engines 
and Face Image Processing. In Journal of 
INFORMATION?Vol.3, No.4, pp.521-531. 
Ren, F. 1999. Super-function Based Machine 
Translation, in Communications of COLIPS, 9(1): 
83-100. 
Seligman, M. 1997. Interactive Real-time Translation 
via the Internet. In Working Notes, Natural 
Language Processing for the World Wide Web. 
AAAI-97 Spring Symposium, Stanford University. 
March 24-26, 1997. 
Seligman, M. 2000. Nine Issues in Speech Translation. 
In Machine Translation. 15: 149-185. 
Waibel, A. 1996. Interactive Translation of 
Conversational Speech. In Proceedings of ATR 
International Workshop on Speech Translation. pp. 
1~17. 
Wahlster, W. 2000. Mobile Speech-to-Speech 
Translation of Spontaneous Dialogs: An Overview 
of the Final Verbmobil System. In Verbmobil: 
Foundations of Speech-to-Speech Translation. 
Springer Press. pp. 3-21. 
Wakita, Y., Jun Kawai, Hitoshi Iida. 1997. Correct Parts 
Extraction from Speech Recognition Results Using 
Semantic Distance Calculation, and Its Application 
to Speech Translation. In Proceedings of a 
Workshop Sponsored by the ACL and by the 
European Network in Language and Speech 
(ELSNET). pp. 24-29. 
Wu, H., Taiyi Huang, Chengqing Zong, and Bo Xu. 
2000. Chinese Generation in a Spoken Dialogue 
Translation System. In Proceedings of COLING. pp. 
1141-1145. 
Xie, G., Chengqing Zong, and Bo, Xu. 2002. Chinese 
Spoken Language Analyzing Based on Combination 
of Statistical and Rule Methods. Submitted to the 
International Conference on Spoken Language 
Processing (ICSLP-2002). 
Xu, W., Taiyi Huang, and Bo Xu. Towards a Generic 
Dialogue Model for Information-seeking Dialogues. 
In Proceedings of the National Conference on Man-
Machine Speech Communications (NCMMSC6). 
Shenzhen,  China. pp. 125-130. 
Yamamoto, K., Satoshi Shirai, Masashi Sakamoto, and 
Yujie Zhang. 2001. Sandglass: Twin Paraphrasing 
Spoken Language Translation. In Proceedings of the 
19th International Conference on Computer 
Processing of Oriental Languages (ICCPOL- 2001). 
pp. 154-159. 
Zhao, T. et al 2000. The Principle of Machine 
Translation (in Chinese). Press of Harbin Institute of 
Technology.  
Zong, C., Taiyi Huang and Bo XU. 1999. Technical 
Analysis on Automatic Spoken Language 
Translation Systems (in Chinese). In Journal of 
Chinese Information Processing, 13(2):55-65. 
Zong, C., Taiyi Huang and Bo Xu. 2000a. Design and 
Implementation of a Chinese-to-English Spoken 
Language Translation System. In Proceedings of the 
International Symposium of Chinese Spoken 
Language Processing (ISCSLP-2000), Beijing, 
China. pp. 367-370. 
Zong, C., Yumi Wakita, Bo Xu, Kenji Matsui and 
Zhenbiao Chen. 2000b. Japanese-to-Chinese Spoken 
Language Translation Based on the Simple 
Expression. In Proceedings of International 
Conference on Spoken Language Processing 
(ICSLP-2000). Beijing, China. pp. 418-421. 
Zong, C., Taiyi Huang and Bo Xu. 2000c. An Improved 
Template-based Approach to Spoken Language 
Translation. In Proceedings of International 
Conference on Spoken Language Processing 
(ICSLP-2000). Beijing, China. pp. 440-443. 
 
Chinese Named Entity Recognition Combining a Statistical Model with 
Human Knowledge 
Youzheng WU Jun ZHAO Bo XU 
National Laboratory of Pattern Recognition 
Institute of Automation Chinese Academy of Sciences 
No.95 Zhongguancun East Road, 100080, Beijing, China 
(yzwu, jzhao,boxu)@nlpr.ia.ac.cn 
 
 
Abstract 
Named Entity Recognition is one of the 
key techniques in the fields of natural 
language processing, information retrieval, 
question answering and so on. 
Unfortunately, Chinese Named Entity 
Recognition (NER) is more difficult for 
the lack of capitalization information and 
the uncertainty in word segmentation. In 
this paper, we present a hybrid algorithm 
which can combine a class-based 
statistical model with various types of 
human knowledge very well. In order to 
avoid data sparseness problem, we 
employ a back-off model and?????
?/TONG YI CI CI LIN? , a Chinese 
thesaurus, to smooth the parameters in the 
model. The F-measure of person names, 
location names, and organization names 
on the newswire test data for the 1999 
IEER evaluation in Mandarin is 86.84%, 
84.40% and 76.22% respectively. 
1 Introduction 
The NER task was first introduced as Message 
Understanding Conference (MUC) subtask in 1995 
(MUC-6). Named Entities were defined as entity 
names (organizations, persons and locations), 
temporal expressions (dates and times) and number 
expressions (monetary values and percentages). 
Compared with the entity name recognition, the 
recognition of temporal and number expressions is 
simpler. So, our research focuses on the 
recognition of person, location and organization 
names. 
The Multilingual NE task first started in 
1995(MET-1), including Chinese, Japanese, and 
Spanish in that year, and continued for Chinese, 
Japanese in 1998(MET-2). Compared with English 
NER, Chinese NER is more difficult. We think the 
main differences between Chinese NER and 
English NER lie in:  
First, unlike English, Chinese lacks the 
capitalization information that plays an important 
role in signaling named entities.  
Second, there is no space between words in 
Chinese, and we have to segment the text before 
NER. However, the errors in word segmentation 
will affect the result of NER. 
Third, Different types of named entities have 
different structures, especially for abbreviative 
entities. Therefore, a single unified model can?t 
capture all the types of entities. Typical structures 
of Chinese person name (CN), location name (LN) 
and organization name (ON) are as follows: 
CN--><surname> <given name> 
LN--><name part>* <a salient word> 
ON-->{[person name] [organization name] [place 
name] [kernel name] }*  [organization type] <a 
salient word> 
Here <>* means repeating one or several times. 
{}* means selecting at least one of items. 
Fourth, there are few openly available resources 
for Chinese NER. Thus we have to resort to the 
algorithm that doesn?t rely on large NER-tagged 
text corpus. 
Based on the above analysis, we present a 
hybrid algorithm that incorporating various types 
of human knowledge into a statistical model. The 
innovative points of our paper are as follows. 
First, the hybrid algorithm can make the best 
use of existing limited resources to develop an 
effective NER system. These resources include 
one-month?s Chinese People?s Daily tagged with 
NER tags by Peking University (which contains 
about two-million Chinese characters) and various 
types of human knowledge. 
Second, in order to compensate for the lack of 
labeled corpus, we use several types of human 
knowledge, such as??????/TONG YI CI 
CI LIN? [Mei.J.J, et al 1983], a general location 
names list, the list of the salient words in location 
name, the list of the salient words in organization 
names, a Chinese surnames list, the list of Chinese 
characters that could be included in transliterated 
person names, and so on. 
Third, we emphasize that human knowledge 
and statistical information should be combined 
very well. For example, a general LN list and a 
general famous ON list are used in our system. 
However, we only accept words in the lists as 
entity candidates with a probability. Whether it is 
a LN or ON depends on the context. This is 
different from other systems which accept them as 
a LN or ON once the system meets them. More 
details refer to section 4. 
This paper will be organized as follows. Section 
2 is the background of NER. Section 3 describes 
the class-based statistical baseline Chinese NER 
model. Section 4 describes different types of 
human knowledge for different named entities 
recognitions and how to combine them with a 
statistical model organically in details. Section 5 is 
the evaluation and section 6 is the conclusion. 
2 Backgroud 
The researches on English NER have made 
impressive achievement. The best NER system 
[Mikheev, et al 1999] in MUC7 achieved 95% 
precision and 92% recall. Recent methods for 
English NER focus on machine-learning 
algorithms such as DL-CoTrain, CoBoost [Collins 
and Singer 1999], HMM [Daniel M. Bikel 1997], 
maximum entropy model [Borthwick, et al 1999] 
and so on. 
However, Chinese NER is still at its immature 
phase. Typical Chinese NER systems are as 
follows. 
NTU system [Hsin-His Chen, et al 1997] relied 
on a statistical model when recognizing person 
names, but rules when recognizing location and 
organization names. In the formal run of MET-2, 
the total F-measure is 79.61%. As a result, they 
may miss the person names whose probability is 
lower than the threshold, the location and 
organization names may also be missed for those 
which don?t accord with the rules. 
[Yu et al 1998] uses both a contextual model 
and a morphological model. However, their system 
requires information of POS tags, semantic tags 
and NE lists. The system obtains 86.38% F-
measure. 
[CHUA et al 2000] employs a combination of 
template-based rules supplemented by the default-
exception trees and decision tree that obtains over 
91% F-measure on MET-2 test data. It also uses 
HowNet [Dong & Dong 2000] to cluster 
semantically related words. 
[Jian Sun, 2002] presents a class-based 
language model for Chinese NER which achieves 
81.79% F-measure on MET-2 test set and 78.75% 
F-measure on IEER test data. However, the model 
heavily depends on statistical information, and 
must be trained on large labeled corpus. 
For Chinese NER, we can?t achieve satisfactory 
performance if we use only a statistical model or 
handcrafted heuristic rules. Therefore, we have to 
resort to the algorithm that can incorporate human 
knowledge into a statistical model. 
In the following sections, we will introduce a 
statistical Chinese NER model first, and then 
incorporate various types of human knowledge into 
the statistical model in order to show the power of 
human knowledge for Chinese NER. 
3 The Baseline Class-based Statistical 
Model 
We regard NER as a tagging problem. Given a 
sequence of Chinese string nwwwW L21= , the task 
of NER is to find the most likely sequence of class 
sequence ( )nmcccC m <== L21*  that maximizes 
the probability ( )WCP | . We use Bayes? Rule to 
rewrite ( )WCP |  as equation (3.1): 
( ) ( )( )
( )
( )WP
CPCWP
WP
WCPWCP ?== )|(,|             (3.1) 
So, the class-based baseline model can be 
expressed as equation (3.2). 
( ) ( )( )
( ) ( )( )
( ) ( )???
????
? ??
?=
?=
?
=
?
m
i
iiiiji
C
mmn
C
C
ccPcwwP
cccPcccwwwP
CPCWPC
1
11
212121
||maxarg
|maxarg
|maxarg*
L
LLL (3.2) 
We call ( )CP  as the contextual model and 
( )CWP |  as the morphological model. Formally, we 
can regard such a class-based statistical model as 
HMM. The classes used in our model are shown in 
Table 1, where |V| means the size of vocabulary 
used for word segmentation. 
Class Description 
PN Person Name 
LN Location Name 
ON Organization Name 
TM Time Name 
NM Number Name 
Other One word is on Class 
Total |V| + 5 
Table 1 Classes used in our model 
3.1 Contextual Model 
Due to our small-sized labeled corpus, we use a 
statistical bi-gram language model as the 
contextual model. This model can be described as 
equation (3.3). 
( ) ( )?=
=
??
mi
i
ii ccPCP
1
1|                                       (3.3) 
Theoretically, trigram is more powerful for 
NER than bi-gram, however when training corpus 
is small, trigram can?t work effectively. Using bi-
gram model, we still need ( )25+V  transmission 
probabilities, some of which can?t be observed in 
our small-sized labeled corpus and some of which 
are unauthentic. That is, data sparseness is still 
serious. We will explain how to resolve data 
sparseness problem in details in section 3 and 4. 
3.2 Morphological Model 
Recognition of Person Names 
The model of person names recognition 
(including Chinese person names abbreviated to 
CN and Transliterated person names abbreviated to 
TN) is a character-based tri-states unigram model. 
In principle, Chinese person name is composed 
of a surname (including single-character surname 
like "?/wu" and double-character surname like"?
? /Ouyang") and a given name (one or two 
characters like "?/peng" or "??/youzheng"). So 
we divide Chinese name words into three parts as 
the surname (surCN), the middle name (midCN) 
and the end name (endCN), which means the 
probability of a specific character used in different 
position in person names isn?t equal. For example, ( ) ( )
( )endCNc|?/wu
CNsc|?/wusurCNc|?/wu
j
jj
=?
=?=
P
ecPP         (3.4) 
The model for three-character-CN recognition 
is described as equation (3.5). ( )
( ) ( )
( )endCNcwP
midCNcwPsurCNcwP
CNcwwwP
jj
jjjj
jjjj
=?
=?=?
=
|
||
|
3
21
321
        (3.5) 
The model for two-character-CN recognition is 
described as equation (3.6). ( )
( ) ( )endCNwPsurCNwP
CNcwwP
jj
jjj
||
|
21
21
??
=        (3.6) 
where ( )CNcwwwP jjjj =|321  means the probability 
of emitting the candidate person name 321 jjj www  
under the state of CN. 
For TN, we don?t divide transliterated name 
words into several different parts. That is, the 
probability of a word used in different position in 
TN is same. The model is as follows. 
( ) ( )?=
=
=?=
ki
i
jjijjkjj TNcwPTNcwwwP
1
21 ||L (3.7) 
Must be mentioned is that all these probabilities 
are estimated from labeled corpus using maximum 
likelihood estimation. 
Recognition of Location Names 
For location names recognition, we use a word-
based bi-state unigram model, and divide words 
used in the location name into two parts: location-
end-words (LE) and non-location-end words 
(NLE). That means the probability of the word 
used in the end position of location name is 
different from that of in other position. 
The model for location name recognition is 
shown in equation (3.8). ( )
( ) ( )LEcwPNLEcwP
LNcwwwP
jjk
ki
i
jji
jjkjj
=?=
?=
??=
=
||
|
1
1
21 L
           (3.8) 
The parameters in equation (3.8) are also 
estimated from labeled training corpus. 
Recognition of Organization Names 
For the model of organization names 
recognition, we use bi-state unigram that is similar 
to the location morphological model shown as 
equation (3.9): ( )
( ) ( )NOEcwPOEcwP
ONcwwwP
jjk
ki
i
jji
jjkjj
=?=
==
??=
=
||
|
1
1
21 L
           (3.9) 
where OE means the word used in the end position 
of organization name, while NOE is not. 
The parameters in equation (3.9) are also 
estimated from the labeled training corpus. 
Back-off Models to Smooth 
Data sparseness problem still exists. As some 
parameters were never observed in trained corpus, 
the model will back off to a less-powerful model. 
We employ escape probability to smooth the 
statistical model [Teahan, et al 1999]. 
An escape probability is the probability that a 
previously unseen character will occur. There is no 
theoretical basis for choosing the escape 
probability optimally. Here we estimate the escape 
probability in a particular context as: 
n
d5.0=?                                                      (3.10) 
The probability of a word ci that has occurred c 
times in that context ci-1 is: 
( )
n
cccP ii
5.0| 1
?=?                                        (3.11) 
While the probability of a word that has never 
occurred in that context is: 
( ) ( )iii cPccP ?=? ?1|                                     (3.12) 
where n is the number of times that context has 
appeared and d is the number of different symbols 
that have directly followed it. 
As a example, if we observe the bi-gram "A B" 
once in training corpus and ?A C" three times, and 
nowhere else did we see the word "A", then 
( )
31
5.03| +
?=ACP , while the escape probability 
31
25.0
+
?=?  and unseen transition probability of 
( ) ( )DPADP ?= ?| . 
The Evaluation for the Baseline 
The baseline model was evaluated in terms of 
precision (P), recall (R) and F-measure (F) metrics.  
responsesofnumber
responsescorrectofnumberP =  
NEallofnumber
responsescorrectofnumberR =  
( )
( ) RP
RPF +?
??+= ?
? 12                                       (3.13) 
where ?  is a weighted constant often set to 1. 
We test the baseline system on the newswire 
test data for the 1999 IEER evaluation in Mandarin 
(http://www.nist.gov/speech/tests/ie-r/er_99/er_ 99. 
htm). Table 2 in section 4 summarizes the result of 
baseline model. 
 Precision Recall F-measure
PN 80.23% 89.55% 84.63% 
LN 45.05% 66.96% 53.86% 
ON 42.98% 61.45% 50.58% 
Total 52.61% 71.53% 60.63% 
Table 2 The Performance of The Baseline 
4 The Hybrid Model Incorporating 
Human Knowledge into the Baseline 
From table 1, we find that the performance of 
the above statistical baseline model isn?t 
satisfactory. The problems mainly lie in: 
? Data sparseness is still serious though we 
only use bi-gram contextual model, unigram 
morphological model and smooth the parameters 
with a back-off model.  
? In order to recognize the named entities, 
we have to estimate the probability of every word 
in text as named entities. Thus redundant 
candidates not only enlarge search space but also 
result in many unpredictable errors.  
? Abbreviative named entities especially 
organization abbreviation can?t be resolved by the 
baseline model. Because abbreviations have weak 
statistical regularities, so can?t be captured by such 
a baseline model. 
We try to resolve these problems by 
incorporating human knowledge. In fact, human 
being usually uses prior knowledge when 
recognizing named entities. In this section, we 
introduce the human knowledge that is used for 
NER and the method of how to incorporate them 
into the baseline model.  
Given a sequence of Chinese characters, the 
recognition process after combined with human 
knowledge consists of the five steps shown in Figure1. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1 Recognition Process of the Hybrid Model 
 
4.1 Incorporate Knowledge for Person Name 
Recognition 
Chinese person names are composed of a 
surname and a given name. Usually the characters 
used for Chinese person names are limited. 
[Maosong Sun, Changning Huang, 1994] presents 
365 most high frequently used surnames cover 
99% Chinese surnames. 1141 most high frequently 
used characters cover 99% Chinese given names. 
Similarly the characters used for transliterated 
names are also limited. We extract about 476 
transliterated characters from the training corpus. 
The following is the human knowledge used for 
person name recognition and the method of how to 
incorporate them into the baseline. 
? A Chinese single and plural surname list: 
Only those characters in the surname list can 
trigger person name recognition. 
? A list of person title list: Only when the 
current character belongs to the surname list and 
the next word is in the title list, candidates are 
accepted. 
? A transliterated character list: Only 
those consecutive characters in the transliterated 
character list form a candidate transliterated name. 
? Person name can?t span any punctuation 
and the length of CN can?t exceed 8 characters 
while the length of TN is unrestrained. 
All these knowledge are used for restricting 
search space. 
4.2 Incorporate Knowledge for Location 
Name Recognition 
A complete location name is composed of the 
name part and a salient word. For the location 
name "???/Beijing City", the name part is "?
? /Beijing" and the salient word is "? /city". 
Unfortunately, the salient word is omitted in many 
occasions. So it is unfeasible to trigger LN 
recognition only depending on the salient words in 
location name. In order to improve the precision 
and recall of LN recognition, we use the following 
human knowledge. The method of incorporating 
them is also explained. 
? A general location name list: The list 
includes the names of Chinese provinces and 
counties, foreign country and its capitals, some 
famous geographical names and foreign cities. If 
the current word is in the list, we accept it as a 
candidate LN. 
? A location salient word list: If the word 
wi belongs to the list, 2~6 words before the salient 
word are accepted as candidate LNs. 
? A general word list (such as verbs and 
prepositions) which usually is followed by a 
location name, such as "? /at", "? /go". If the 
word wi is in the list, 2~6 words following it are 
accepted as candidate LNs. 
? An abbreviative location name list: If the 
current word is in the list, we accept it as a 
candidate LN such as "?/China", "?/America". 
PN and LN 
Generate 
NE Candidates
Recognition Nested 
Organization Names
Named Entities
Human Knowledge 
TONG YI CI CI LIN 
Extract 
Organization Kernel 
Word Segmentation
Nested Organization 
Name Templates
NE Pools
Text
Search the Max. 
P(C|W)
? Coordinate LN recognition: If wi-2 is a 
candidate LN and wi-1 is "? "(a punctuation 
denoting coordinate relation), LN recognition is 
triggered at the position of word wi. 
? Location name can?t span punctuations and 
its length couldn?t exceed 6 words. 
Knowledge ?, ?, ?, ?, ? can restrict 
search space while knowledge ? deals with 
abbreviative location name. 
4.3 Incorporate Knowledge for Organization 
Name Recognition 
The organization names recognition is the most 
difficult task. The reasons lie in nested ONs and 
abbreviative ONs especially. 
Nested ON means there are one or more 
location names, person names and/or organization 
names embedded in organization name. Typical 
structure of ON has been given in section 1. We 
can capture most of the nested organization names 
by several ON templates mentioned in the 
following section. 
Abbreviative ONs include continuous and 
discrete abbreviation which omits some words in 
the full name. Take "????????????
" as example, abbreviative ON of it may omit LN "
?? /Shanghai", organization types like"??
/supermarket", "??/stock", "??/limited", and 
salient word like "??/company" from full names 
but usually remains organization kernel "??
/Hualian". Table 3 lists some examples of 
abbreviative ONs. 
?????? 
?????? 
Shanghai Hualian 
Co.,Ltd 
????
Shanghai
Hualian Continuous Abbreviation 
???? 
Tsinghua niversity 
?? 
Tsinghua
??????? 
Shanghai Stock 
Exchange 
?? 
Shanghai 
Stock Discrete Abbreviation ???? 
Peking University 
?? 
Bei Da 
Table 3 Nest Organization Full Names and Its 
Abbreviative Names 
So it is important to extract organization kernel 
from the full name in order to recognize 
abbreviative ON like "????". Moreover, an 
organization's abbreviative names usually occur 
after its' full name, unless it is a well-known 
organization. So this strategy for abbreviation 
organization name recognition is effective. 
The following is the human knowledge used for 
ON recognition and the method of how to 
incorporate them. 
? An organization salient word (OrgSws) 
list: If the current word wi is in OrgSws list, 2~6 
words before OrgSw are accepted as the candidate 
ONs. 
? A general famous organization name list: 
If the current word is in the list, we accept it as a 
candidate ON such as "???/ State Department", 
"???/ U.N. ". 
? An organization names template list: We 
mainly use organization name templates to 
recognize the nested ONs. Some of these templates 
are as follows: 
ON-->LN D* OrgSw 
ON-->PN D* OrgSw 
ON-->ON OrgSw 
D means words used in the middle of organization 
names. D* means repeating zero or more times. 
This component runs in the end stage of 
recognition process shown in Figure 1. 
? An organization type list: The list is used 
to extract organization kernels from recognized 
ONs. We have a pool which memorizes ONs 
recognized in current paragraph and its kernel. If 
the current word belongs to organization kernel in 
pool, we accept it as a candidate ON. The idea is 
effective especially in financial domain which 
contains many stocks such as"????/Shanghai 
Hualian", "????/Changjiang Technology". 
Knowledge ?, ?, ? restrict search space 
while knowledge ? deals with abbreviative 
organization name. 
4.4 Semantic Similarity Computation for 
Data Sparseness 
??????/TONG YI CI CI LIN?classifies 
the words in terms of semantic similarity. Here we 
use it to resolve data sparseness problem. If current 
transmission probability doesn?t exist, we resort to 
its synonym transmission. In statistical sense, 
synonym transmissions are approximate. Take an 
example, the probability of P(A|B) doesn?t exist, 
but there has P(C|B), meanwhile, the word A and 
C are thesaurus according to ??????/TONG 
YI CI CI LIN?, then we use P(C|B) to replace 
P(A|B). 
5 Results of Evaluation 
We also test our hybrid model on IEER-99 neswire 
test data. The performance is shown in Table 4. 
 Precision Recall F-measure
PN 83.30% 92.28% 87.56% 
LN 88.31% 84.69% 86.47% 
ON 84.49% 71.08% 77.21% 
Total 86.09% 83.18% 84.61% 
Table 4 The Performance of the Hybrid Model 
Comparing Table 1 with 4, we find that the 
performance of the hybrid model increases 
remarkably. More specifically, the precision and 
the recall of PNs increase from 80.23% to 83.30% 
and from 89.55% to 92.28% respectively. The 
precision and recall of LNs increase from 45.05% 
to 82.18% and from 66.96% to 86.74% 
respectively. The precision and recall of ONs 
increase from 42.98% to 80.86% and from 61.45% 
to 72.09% respectively. The reason that the 
improvement of PNs is slighter than that of ONs 
and LNs is that the statistical information 
estimated from labeled corpus for PNs is good 
enough but not for LNs and ONs. 
Must be mentioned is that, in our evaluation, 
only NEs with both correct boundary and correct 
type label are considered as the correct 
recognitions, which is a little different from other 
evaluation systems. 
We also test our system on data set of sport, 
finance, news and entertainment domains. These 
test data are downloaded from Internet shown in 
Table 4.  
Number of NE 
Domain 
PN LN ON
File 
size 
Sport(S) 954 510 609 91K
Finance(F) 212 406 461 80K
News(N) 526 961 437 76K
Entertainment(E) 1016 511 133 100K
Total 2708 2388 1640 247K
Table 4 Statistic of Multi-field Test Data 
The results are shown in Table 5. 
 Precision Recall F-measure
S 80.17% 91.10% 85.28% 
F 61.35% 94.34% 74.35% 
N 88.66% 83.27% 85.88% 
PN 
E 82.20% 82.28% 82.24% 
S 82.90% 81.76% 82.33% 
F 83.72% 81.03% 82.35% 
N 91.95% 91.56% 91.75% 
LN
E 81.64% 87.87% 84.64% 
S 73.43% 67.16% 70.15% 
F 65.88% 60.30% 62.97% 
N 92.52% 84.70% 88.44% 
ON
E 78.30% 62.41% 69.46% 
Total 81.01% 81.24% 81.12% 
Table 5 Results on different domain 
Table 5 shows that the performance on financial 
domain is much lower. The reason is that, in 
financial domain, there are many stock names 
which are the abbreviation of organization names. 
Moreover, organization full name never appear in 
the text. So the system can?t recognize them as an 
organization name. However, on many occasions, 
they are recognized as person names. As a result, 
the precision of PNs declines, meanwhile, the 
precision and recall of ONs can?t be high. 
Based on the above analysis, we find that the 
main sources of errors in our system are as follows. 
First, we still have not found a good strategy for 
the abbreviation location names and organization 
names. Because abbreviative LNs and ONs 
sometimes appear before full LN, sometimes not, 
so the pool strategy can?t work well. 
Second, some famous organization names that 
always appear in the shape of abbreviation can?t be 
recognized as ON because the full name never 
appear such as ?? /GaoTong, ?? /Xinlang. 
However, these ONs are often recognized as PNs. 
Such errors are especially serious in finance 
domain shown Table 5. 
Third, many words can?t be found in ????
??/TONG YI CI CI LIN?. 
6 Conclusions 
Chinese NER is a more difficult task than English 
NER. Though many approaches have been tried, 
the result is still not satisfactory. In this paper, we 
present a hybrid algorithm of incorporating human 
knowledge into statistical model. Thus we only 
need a relative small-sized labeled corpus (one-
month?s Chinese People?s Daily tagged with NER 
tags at Peking University) and human knowledge, 
but can achieve better performance. The main 
contribution of this paper is putting forward an 
approach which can make up for the limitation of 
using the statistical model or human knowledge 
purely by combining them organically. 
Our lab was mainly devoted to cross-language 
information processing and its application. So in 
the future we will shift our algorithm to other 
languages. And fine-tune to a specific domain such 
as sports. 
ACKNOWLEDGEMENT 
This paper is supported by the National ?973? 
project G1998030501A-06 and the Natural Science 
Foundation of China 60272041. 
References 
Jian Sun, et al 2002. Chinese Named Entity 
Identification Using Class-based Language Model. 
Proceedings of the 19th International Conference on 
Computational Linguistics 
Hsin-His Chen, et al 1997. Description of the NTU 
System Used for MET2. Proceedings of the Seventh 
Message Understanding Conference 
Tat-Seng Chua, et al 2002. Learning Pattern Rules for 
Chinese Named Entity Extraction. Proceedings of 
AAAI?02 
W.J.Teahan, et al 1999. A Compression-based 
Algorithm for Chinese Word Segmentation. 
Computational Linguistic 26(2000) 375-393 
Maosong Sun, et al 1994. Identifying Chinese Names in 
Unrestricted Texts. Journal of Chinese Information 
Processing. 1994,8(2) 
Collins, Singer. 1999. Unsupervised Models for Named 
Entity Classification. Proceedings of 1999 Joint 
SIGDAT Conference on Empirical Methods in NLP 
and Very Large Corpora 
Daniel M. Bikel, et al 1997. Nymble: a High-
Performance Learning Name-finder. Proceedings of 
ANLP-97, page 194-201, 1997 
Yu et al 1998. Description of the Kent Ridge Digital 
Labs System Used for MUC-7. Proceedings of the 
Seventh Message Understanding Conference 
Silviu Cucerzan, David Yarowsky. 1999. Language 
Independent Named Entity Recognition Combining 
Morphological and Contextual Evidence. 
Proceedings 1999 Joint SIGDAT Conference on 
EMNLP and VLC 
Peter F.Brown, et al 1992. Class-Based n-gram Model 
of Natural Language. 1992 Association for 
Computational Linguistics 
A.Mikheev, M.Moens, and C.Grover. 1999. Named 
entity recognition without gazetteers. Proceedings of 
the Ninth Conference of the European Chapter of the 
Association for Computational Linguistics. Bergen, 
Norway 
Borthwich. A. 1999. A Maximum Entropy Approach to 
Named Entity Recognition. PhD Dissertation 
Dong & Dong. 2000. Hownet. At: http://www.keenage. 
com 
Yu.S.W. 1999. The Specification and Manual of 
Chinese Word Segmentation and Part of Speech 
Tagging. At: http://www.icl.pku.edu.cn/Introduction/ 
corpustagging. htm 
Mei.J.J, et al 1983. ??????/TONG YI CI CI 
LIN?. Shanghai CISHU Press 
Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 56?63,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Cluster-based Language Model for Sentence Retrieval in Chinese 
Question Answering 
 
 
Youzheng Wu                               Jun Zhao                               Bo Xu 
National Laboratory of Pattern Recognition 
Institute of Automation Chinese Academy of Sciences 
No.95 Zhongguancun East Road, 100080, Beijing, China 
(yzwu, jzhao,boxu)@nlpr.ia.ac.cn 
 
  
 
Abstract 
Sentence retrieval plays a very important 
role in question answering system. In this 
paper, we present a novel cluster-based 
language model for sentence retrieval in 
Chinese question answering which is mo-
tivated in part by sentence clustering and 
language model. Sentence clustering is 
used to group sentences into clusters. 
Language model is used to properly rep-
resent sentences, which is combined with 
sentences model, cluster/topic model and 
collection model. For sentence clustering, 
we propose two approaches that are One-
Sentence-Multi-Topics and One-
Sentence-One-Topic respectively. From 
the experimental results on 807 Chinese 
testing questions, we can conclude that 
the proposed cluster-based language 
model outperforms over the standard lan-
guage model for sentence retrieval in 
Chinese question answering. 
1 Introduction 
To facilitate the answer extraction of question 
answering, the task of retrieval module is to find 
the most relevant passages or sentences to the 
question. So, the retrieval module plays a very 
important role in question answering system, 
which influences both the performance and the 
speed of question answering. In this paper, we 
mainly focus on the research of improving the 
performance of sentence retrieval in Chinese 
question answering. 
Many retrieval approaches have been pro-
posed for sentence retrieval in English question 
answering. For example, Ittycheriach [Ittycheriah, 
et al 2002] and H. Yang [Hui Yang, et al 2002] 
proposed vector space model. Andres [Andres, et 
al. 2004] and Vanessa [Vanessa, et al 2004] pro-
posed language model and translation model re-
spectively. Compared to vector space model, 
language model is theoretically attractive and a 
potentially very effective probabilistic frame-
work for researching information retrieval prob-
lems [Jian-Yun Nie. 2005]. 
However, language model for sentence re-
trieval is not mature yet, which has a lot of diffi-
cult problems that cannot be solved at present. 
For example, how to incorporate the structural 
information, how to resolve data sparseness 
problem. In this paper, we mainly focus on the 
research of the smoothing approach of language 
model because sparseness problem is more seri-
ous for sentence retrieval than for document re-
trieval. 
At present, the most popular smoothing ap-
proaches for language model are Jelinek-Mercer 
method, Bayesian smoothing using Dirichlet pri-
ors, absolute discounting and so on [C. Zhai, et al 
2001]. The main disadvantages of all these 
smoothing approaches are that each document 
model (which is estimated from each document) 
is interpolated with the same collection model 
(which is estimated from the whole collection) 
through a unified parameter. Therefore, it does 
not make any one particular document more 
probable than any other, on the condition that 
neither the documents originally contains the 
query term. In other word, if a document is rele-
vant, but does not contain the query term, it is 
still no more probable, even though it may be 
topically related. 
As we know, most smoothing approaches of 
sentence retrieval in question answering are 
learned from document retrieval without many 
adaptations. In fact, question answering has some 
56
characteristics that are different from traditional 
document retrieval, which could be used to im-
prove the performance of sentence retrieval. 
These characteristics lie in: 
1. The input of question answering is natural 
language question which is more unambiguous 
than query in traditional document retrieval. 
For traditional document retrieval, it?s difficult 
to identify which kind of information the users 
want to know. For example, if the user submit 
the query {??/invent, ??/telephone}, search 
engine does not know what information is 
needed, who invented telephone, when telephone 
was invented, or other information. On the other 
hand, for question answering system, if the user 
submit the question {???????/who in-
vented the telephone?}, it?s easy to know that the 
user want to know the person who invented the 
telephone, but not other information. 
2. Candidate answers extracted according to 
the semantic category of the question?s answer 
could be used for sentence clustering of question 
answering. 
Although the first retrieved sentences are re-
lated to the question, they usually deal with one 
or more topics. That is, relevant sentences for a 
question may be distributed over several topics. 
Therefore, treating the question?s words in re-
trieved sentences with different topics equally is 
unreasonable. One of the solutions is to organize 
the related sentences into several clusters, where 
a sentence can belong to about one or more clus-
ters, each cluster is regarded as a topic. This is 
sentence clustering. Obviously, cluster and topic 
have the same meaning and can be replaced each 
other. In the other word, a particular entity type 
was expected for each question, and every spe-
cial entity of that type found in a retrieved sen-
tence was regarded as a cluster/topic.  
In this paper, we propose two novel ap-
proaches for sentence clustering. The main idea 
of the approaches is to conduct sentence cluster-
ing according to the candidate answers which are 
also considered as the names of the clusters.  
For example, given the question {?????
??/who invented telephone?}, the top ten re-
trieved sentences and the corresponding candi-
date answers are shown as Table 1. Thus, we can 
conduct sentence clustering according to the 
candidate answers, that are, {??/Bell, ???
/Siemens, ???/Edison,??/Cooper, ???
/Stephen}.
 
ID Top 10 Sentences Candidate Answer 
S1 1876? 3? 10???????/Bell invented telephone on Oct. 3th, 1876. ??/Bell 
S2 
????????????????????????
/ Bell, Siemens and Edison invented telephone, electromo-
tor and electric light respectively. 
???/ Siemens 
??/Bell 
???/ Edison 
S3 
??? ??????? ????????????
/Recently, the public paid a great deal of attention to Cooper 
who is Father of Mobile Phone. 
??/Cooper 
S4 1876 ????????????? /In 1876, Bell in-vented telephone. ??/Bell 
S5 
???1876 ???????????????1879 ?
??????????????/Subsequently, American 
scientist Bell invented the phone in 1876; Edison invented 
the electric light in 1879. 
??/Bell 
???/Edison 
S6 1876 ? 3 ? 7 ???????????????/On March 7th, 1876, Bell became the patentee of telephone. ??/Bell 
S7 
????????????????????????
???/Bell not only invented telephone, but also estab-
lished his own company for spreading his invention. 
??/Bell 
S8 
??????????? 30 ???????????
????????????????/Thirty years after 
the invention of first mobile phone, Cooper still anticipated 
the date of the realization of future phone?s technology. 
??/Cooper 
57
S9 
????????????????????????
????????????????????????
???/Cooper said, he was surprised at the speed that the 
consumers switched to mobile phones; but the populariza-
tion of mobile phone isn?t omnipresent, which made him a 
little bit disappointed. 
??/Cooper 
S10 
????????????????????????
???????????/England inventor Stephen de-
signed the paper-clicked CMOS chip which included all 
electronic components. 
???/Stephen 
Table 1 The Top 10 Retrieved Sentences and its Candidate Answers 
Based on the above analysis, this paper pre-
sents cluster-based language model for sentence 
retrieval of Chinese question answering. It dif-
fers from most of the previous approaches 
mainly as follows. 1. Sentence Clustering is con-
ducted according to the candidate answers ex-
tracted from the top 1000 sentences. 2. The in-
formation of the cluster of the sentence, which is 
also called as topic, is incorporated into language 
model through aspect model. For sentence clus-
tering, we propose two novel approaches that are 
One-Sentence-Multi-Topics and One-Sentence-
One-Topic respectively. The experimental results 
show that the performances of cluster-based lan-
guage model for sentence retrieval are improved 
significantly. 
The framework of cluster-based language 
model for sentence retrieval is shown as Figure 1. 
 
Figure 1 The Framework of Cluster-based Language Model for Sentence Retrieval 
2 Language Model for Information Re-
trieval 
Language model for information retrieval is pre-
sented by Ponte & Croft in 1998[J. Ponte, et al 
1998] which has more advantages than vector 
space model. After that, many improved models 
are proposed like J.F. Gao [J.F Gao, et al 2004], 
C. Zhai [C. Zhai, et al 2001], and so on. In 1999, 
Berger & Lafferty [A. Berger, et al 1999] pre-
sented statistical translation model for informa-
tion retrieval. 
The basic approach of language model for in-
formation retrieval is to model the process of 
generating query Q. The approach has two steps. 
1. Constructing document model for each docu-
ment in the collection; 2. Ranking the documents 
according to the probabilities p(Q|D). A classical 
unigram language model for IR could be ex-
pressed in equation (1). 
( ) ( )?
Qw
i
i
D|wpD|Qp
?
=                                   (1) 
where, wi is a query term, p(wi|D) is document 
model which represents terms distribution over 
document. Obviously, estimating the probability 
p(wi|D) is the key of document model. To solve 
the sparseness problem, Jelinek-Mercer is com-
monly used which could be expressed by equa-
tion (2). 
( ) ( ) ( ) ( )C|wp?1D|wp?D|wp MLML ?+?= -   (2) 
where, pML(w|D) and pML(w|C) are document 
model and collection model respectively esti-
mated via maximum likelihood. 
Question 
Document 
Retrieval 
Sentence 
Splitter 
Candidate An-
swer Extraction
Language Model 
for Sentence Re-
trieval
Sentence Clus-
tering 
Results 
Cluster-based Lan-
guage Model for 
Sentence Retrieval
Question 
Analyzer 
58
As described above, the disadvantages of 
standard language model is that it does not make 
any one particular document any more probable 
than any other, on the condition that neither the 
documents originally contain the query term. In 
the other word, if a document is relevant, but 
does not contain the query term, it is still no 
more probable, even though it may be topically 
related. Thus, the smoothing approaches based 
on standard language model are improper. In this 
paper, we propose a novel cluster-based lan-
guage model to overcome it. 
3 Cluster-based Language Model for 
Sentence Retrieval 
Note that document model p(w|D) in document 
retrieval is replace by p(w|S) called sentence 
model in sentence retrieval. 
The assumption of cluster-based language 
model for retrieval is that topic-related sentences 
tend to be relevant to the same query. So, incor-
porating the topic of sentences into language 
model can improve the performance of sentence 
retrieval based on standard language model. 
The proposed cluster-based language model is 
a mixture model of three components, that are 
sentence model pML(w|S), cluster/topic model 
p_topicML(w|T) and collection model pML(w|C). 
We can formulate our model as equation (3). 
( ) ( ) ( )
( ) ( ) ( )( )Cwp?1Twp_topic?
?1Swp?S|wp
MLML
ML
|?+|?
?+|?=
-
-          (3) 
In fact, the cluster-based language model can 
also be viewed as a two-stage smoothing ap-
proach. The cluster model is first smoothed using 
the collection model, and the sentence model is 
then smoothed with the smoothed cluster model. 
In this paper, the cluster model is in the form 
of term distribution over cluster/topic, associated 
with the distribution of clusters/topics over sen-
tence, which can be expressed by equation (4).  
( ) ( ) ( )?
?Tt
StptwpTwp_topic ||=|                     (4) 
where, T is the set of clusters/topics. p_topic(w|T) 
is cluster model. p(t|S) is topic sentence distribu-
tion which means the distribution of topic over 
sentence. And p(w|t) is term topic distribution 
which means the term distribution over topics. 
Before estimating the sentence model p(w|S), 
topic-related sentences should be organized into 
clusters/topics to estimate p(t|S) and p(w|t) prob-
abilities. For sentence clustering, this paper pre-
sents two novel approaches that are One-
Sentence-Multi-Topics and One-Sentence-One-
Topic respectively. 
3.1 One-Sentence-Multi-Topics 
The main idea of One-Sentence-Multi-Topics 
can be summarized as follows. 
1. If a sentence includes M different candidate 
answers, then the sentence consists of M different 
topics. 
For example, the sentence S5 in Table 1 includes 
two topics which are ???????/Bell in-
vented telephone? and ????????/Edison 
invented electric light? respectively. 
2. Different sentences have the same topic if two 
candidate answers are same. 
For example, the sentence S4 and S5 in Table 1 
have the same topic ??????? /Bell in-
vented telephone? because both of sentences 
have the same candidate answer ???/Bell?. 
Based on the above ideas, the result of sen-
tence clustering based on One-Sentence-Multi-
Topics is shown in Table 2. 
Name of Clusters Sentences 
??/Bell S1 S2 S4 S5 S6 S7 S8 
???/Siemens S2 
???/Edison S2 S5 
??/Cooper S3 S8 S9 
???/Stephen S10 
Table 2 The Result of One-Sentence-Multi-
Topics Sentence Clustering 
So, we could estimate term topic distribution 
using equation (5). 
( ) ( )( )?
w'
t,w'n
twn
twp
,=|                                         (5) 
Topic sentence distribution can be estimated 
using equation (6) and (7). 
( )
? /
/=|
t
st
st
kl1
kl1
Stp                                            (6) 
( ) ( ) ( )( )?w ML
ML
MLst t|wp
swp
logs|wptsKLkl
|?=||=    (7) 
where, klst means the Kullback-Leibler diver-
gence between the sentence with the cluster/topic. 
k denotes the number of cluster/topic. The main 
idea of equation (6) is that the closer the Kull-
back-Leibler divergence, the larger the topic sen-
tence probability p(t|S). 
3.2 One-Sentence-One-Topic 
The main idea of One-Sentence-One-Topic also 
could be summarized as follows. 
59
1. A sentence only has one kernel candidate an-
swer which represents the kernel topic no matter 
how many candidate answers is included. 
For example, the kernel topic of sentence S5 in 
Table 1 is ???????/Bell invented tele-
phone? though it includes three different candi-
date answers. 
2. Different sentences have the same topic if two 
kernel candidate answers are same. 
For example, the sentence S4 and S5 in Table 1 
have the same topic ??????? /Bell in-
vented telephone?. 
3. The kernel candidate answer has shortest av-
erage distance to all query terms. 
Based on the above ideas, the result of sen-
tence clustering based on One-Sentence-One-
Topic is shown in Table 3. 
Name of Clusters Sentences 
??/Bell S1 S2 S4 S5 S6 S7
??/Cooper S3 S8 S9 
???/Stephen S10 
Table 3 The Result of One-Sentence-One-Topic 
Sentence Clustering 
Equation (8) and (9) can be used to estimate 
the kernel candidate answer and the distances of 
candidate answers respectively. Term topic dis-
tribution in One-Sentence-One-Topic can be es-
timated via equation (5). And topic sentence dis-
tribution is equal to 1 because a sentence only 
belongs to one cluster/topic. { }
i
i
a
a
*
i SemDis  a argmin=                               (8) 
( )
N
q,aSemDis
SemDis j
ji
ai
?
=
                         (9) 
( )
ji qaji PositionPositionqaSemDis -=,           (10) 
where, ai* is the kernel candidate answer. ai is 
the i-th candidate answer, 
iaSemDis is the average 
distance of i-th candidate answer. qj is the j-th 
query term, N is the number of all query terms. 
jqPosition and iaPosition  mean the position of 
query term qj and candidate answer ai. 
4 Experiments and Analysis 
Research on Chinese question answering, is still 
at its early stage. And there is no public evalua-
tion platform for Chinese question answering. So 
in this paper, we use the evaluation environment 
presented by [Youzheng Wu, et al 2004] which 
is similar to TREC question answering track 
[Ellen. M. Voorhees. 2004]. The documents col-
lection is downloaded from Internet which size is 
1.8GB. The testing questions are collected via 
four different approaches which has 7050 Chi-
nese questions currently. 
In this section, we randomly select 807 testing 
questions which are fact-based short-answer 
questions. Moreover, the answers of all testing 
questions are named entities identified by 
[Youzheng Wu, et al 2005]. Figure 2 gives the 
details. Note that, LOC, ORG, PER, NUM and 
TIM denote the questions which answer types 
are location, organization, person, number and 
time respectively, SUM means all question types. 
165
311
28
168
135
0
100
200
300
400
PER LOC ORG TIM NUM
 
Figure 2 The Distribution of Various Question 
Types over Testing Questions 
Chinese question answering system is to re-
turn a ranked list of five answer sentences per 
question and will be strictly evaluated (unsup-
ported answers counted as wrong) using mean 
reciprocal rank (MRR). 
4.1 Baseline: Standard Language Model for 
Sentence Retrieval 
Based on the standard language model for infor-
mation retrieval, we can get the baseline per-
formance, as is shown in Table 4, where ? is the 
weight of document model. 
? 0.6 0.7 0.8 0.9 
LOC 49.95 51.50 52.63 54.54
ORG 53.69 51.01 50.12 51.01
PER 63.10 64.42 65.94 65.69
NUM 48.43 49.86 51.78 53.26
TIM 56.97 58.38 58.77 61.49
SUM 53.98 55.28 56.40 57.93
Table 4 The Baseline MRR5 Performance 
60
In the following chapter, we conduct experi-
ments to answer two questions.  
1. Whether cluster-based language model for 
sentence retrieval could improve the perform-
ance of standard language model for sentence 
retrieval? 
2. What are the performances of sentence clus-
tering for various question types? 
4.2 Cluster-based Language Model for Sen-
tence Retrieval 
In this part, we will conduct experiments to vali-
date the performances of cluster-based language 
models which are based on One-Sentence-Multi-
Topics and One-Sentence-One-Topic sentence 
clustering respectively. In the following experi-
ments, ? = 0.9. 
4.2.1 Cluster-based Language Model Based 
on One-Sentence-Multi-Topics 
The experimental results of cluster-based lan-
guage model based on One-Sentence-Multi-
Topics sentence clustering are shown in Table 5. 
The relative improvements are listed in the 
bracket. 
? 0.6 0.7 0.8 0.9 
LOC 55.57 (+11.2) 
55.61 
(+7.98) 
56.59 
(+7.52) 
57.70 
(+5.79)
ORG 59.05 (+9.98) 
59.46 
(+16.6) 
59.46 
(+18.6) 
59.76 
(+17.2)
PER 67.73 (+7.34) 
68.03 
(+5.60) 
67.71 
(+2.68) 
67.45 
(+2.68)
NUM 52.79 (+9.00) 
53.90 
(+8.10) 
54.45 
(+5.16) 
55.51 
(+4.22)
TIM 60.17 (+5.62) 
60.63 
(+3.85) 
62.33 
(+6.06) 
61.68 
(+0.31)
SUM 58.14 (+7.71) 
58.63 
(+6.06) 
59.30 
(+5.14) 
59.54 
(+2.78)
Table 5 MRR5 Performance of Cluster-based 
Language Model Based on One-Sentence-Multi-
Topics 
From the experimental results, we can find 
that by integrating the clusters/topics of the sen-
tence into language model, we can achieve much 
improvement at each stage of ?. For example, the 
largest and smallest improvements for all types 
of questions are about 7.7% and 2.8% respec-
tively. This experiment shows that the proposed 
cluster-based language model based on One-
Sentence-Multi-Topics is effective for sentence 
retrieval in Chinese question answering.  
4.2.2 Cluster-based Language Model Based 
on One-Sentence-One-Topic 
The performance of cluster-based language 
model based on One-Sentence-One-Topic sen-
tence clustering is shown in Table 6. The relative 
improvements are listed in the bracket. 
? 0.6 0.7 0.8 0.9 
LOC 53.02 (+6.15)
54.27 
(+5.38) 
56.14 
(+6.67) 
56.28 
(+3.19)
ORG 58.75 (+9.42)
58.75 
(+17.2) 
59.46 
(+18.6) 
59.46 
(+16.6)
PER 66.57 (+5.50)
67.07 
(+4.11) 
67.44 
(+2.27) 
67.29 
(+2.44)
NUM 49.95 (+3.14)
50.87 
(+2.02) 
52.15 
(+0.71) 
53.51 
(+0.47)
TIM 59.75 (+4.88)
60.65 
(+3.89) 
62.71 
(+6.70) 
62.20 
(+1.15)
SUM 56.48 (+4.63)
57.65 
(+4.29) 
58.82 
(+4.29) 
59.22 
(+2.23)
Table 6 MRR5 Performance of Cluster-based 
Language Model Based on One-Sentence-One-
Topic 
In Comparison with Table 5, we can find that 
the improvement of cluster-based language 
model based on One-Sentence-One-Topic is 
slightly lower than that of cluster-based language 
model based on One-Sentence-Multi-Topics. The 
reasons lie in that Clusters based on One-
Sentence-One-Topic approach are very coarse 
and much information is lost. But the improve-
ments over baseline system are obvious. 
Table 7 shows that MRR1 and MRR20 scores 
of cluster-based language models for all question 
types. The relative improvements over the base-
line are listed in the bracket. This experiment is 
to validate whether the conclusion based on dif-
ferent measurements is consistent or not. 
 One-Sentence-Multi-Topics 
One-Sentence-
One-Topic 
? MRR1 MRR20 MRR1 MRR20
0.6 50.00 (+14.97)
59.60 
(+7.66) 
48.33 
(+10.37) 
57.70 
(+4.23)
0.7 50.99 (+13.36)
60.03 
(+6.12) 
49.44 
(+9.92) 
58.62 
(+3.62)
0.8 51.05 (+8.99) 
60.68 
(+5.06) 
51.05 
(+8.99) 
60.01 
(+3.90)
0.9 51.92 (+5.81) 
61.05 
(+2.97) 
51.30 
(+4.54) 
60.25 
(+1.62)
Table 7 MRR1 and MRR20 Performances of 
Two Cluster-based Language Models 
61
Table 7 also shows that the performances of 
two cluster-based language models are higher 
than that of the baseline system under different 
measurements. For MRR1 scores, the largest 
improvements of cluster-based language models 
based on One-Sentence-Multi-Topics and One-
Sentence-One-Topic are about 15% and 10% 
respectively. For MRR20, the largest improve-
ments are about 7% and 4% respectively. 
Conclusion 1: The experiments show that the 
proposed cluster-based language model can im-
prove the performance of sentence retrieval in 
Chinese question answering under the various 
measurements. Moreover, the performance of 
clustering-based language model based on One-
Sentence-Multi-Topics is better than that based 
on One-Sentence-One-Topic. 
4.3 The Analysis of Sentence Clustering for 
Various Question Types 
The parameter ? in equation (3) denotes the bal-
ancing factor of the cluster model and the collec-
tion model. The larger ?, the larger contribution 
of the cluster model. The small ?, the larger con-
tribution of the collection model. If the perform-
ance of sentence retrieval decreased with the in-
creasing of ?, it means that there are many noises 
in sentence clustering. Otherwise, sentence clus-
tering is satisfactory for cluster-based language 
model. So the task of this experiment is to find 
the performances of sentence clustering for vari-
ous question types, which is helpful to select the 
most proper ? to obtain the best performance of 
sentence retrieval. 
With the change of ? and the fixed ? (? = 0.9), 
the performances of cluster-based language 
model based on One-Sentence-Multi-Topics are 
shown in Figure 3. 
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
0.5
0.52
0.54
0.56
0.58
0.6
0.62
0.64
0.66
0.68
SUM
LOC
ORG
PER
NUM
TIM
 
Figure 3 MRR5 Performances of Cluster-based 
Language Model Based on One-Sentence-Multi-
Topics with the Change of ? 
In Figure 3, the performances of TIM and 
NUM type questions decreased with the increas-
ing of the parameter ? (from 0.6 to 0.9), while 
the performances of LOC, PER and ORG type 
questions increased. This phenomenon showed 
that the performance of sentence clustering based 
on One-Sentence-Multi-Topics for TIM and 
NUM type questions is not as good as that for 
LOC, PER and ORG type questions. This is in 
fact reasonable. The number and time words fre-
quently appeared in the sentence, which does not 
represent a cluster/topic when they appear. While 
PER, LOC and ORG entities can represent a 
topic when they appeared in the sentence. 
Similarly, with the change of ? and the fixed ? 
(?=0.9), the performances of cluster-based lan-
guage model based on One-Sentence-One-Topic 
are shown in Figure 4. 
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
0.5
0.52
0.54
0.56
0.58
0.6
0.62
0.64
0.66
0.68
SUM
LOC
ORG
PER
NUM
TIM
 
Figure 4 MRR5 Performance of Cluster-based 
Language Model Based on One-Sentence-One-
Topic with the Change of ? 
In Figure 4, the performances of TIM, NUM, 
LOC and SUM type questions decreased with the 
increasing of ? (from 0.6 to 0.9). This phenome-
non shows that the performances of sentence 
clustering based on One-Sentence-One-Topic are 
not satisfactory for most of question types. But, 
compared to the baseline system, the cluster-
based language model based on this kind of sen-
tence clustering can still improve the perform-
ances of sentence retrieval in Chinese question 
answering. 
Conclusion 2: The performance of the pro-
posed sentence clustering based on One-
Sentence-Multi-Topics for PER, LOC and ORG 
type questions is higher than that for TIM and 
NUM type questions. Thus, for PER, LOC and 
ORG questions, we should choose the larger ? 
value (about 0.9) in cluster-based language 
model based on One-Sentence-Multi-Topics. 
While for TIM and NUM type questions, the 
62
value of ? should be smaller (about 0.5). But, the 
performance of sentence clustering based on 
One-Sentence-One-Topic for all questions is not 
ideal, so the value for cluster-based language 
model based on One-Sentence-One-Topic should 
be smaller (about 0.5) for all questions. 
5 Conclusion and Future Work 
The input of a question answering system is 
natural language question which contains richer 
information than the query in traditional docu-
ment retrieval. Such richer information can be 
used in each module of question answering sys-
tem. In this paper, we presented a novel cluster-
based language model for sentence retrieval in 
Chinese question answering which combines the 
sentence model, the cluster/topic model and the 
collection model. 
For sentence clustering, we presented two ap-
proaches that are One-Sentence-Multi-Topics 
and One-Sentence-One-Topic respectively. The 
experimental results showed that the proposed 
cluster-based language model could improve the 
performances of sentence retrieval in Chinese 
question answering significantly. 
However, we only conduct sentence clustering 
for questions, which have the property that their 
answers are named entities in this paper. In the 
future work, we will focus on all other type ques-
tions and improve the performance of the sen-
tence retrieval by introducing the structural, syn-
tactic and semantic information into language 
model. 
Reference 
J. Ponte, W. Bruce Croft. A Language Modeling Ap-
proach to Information Retrieval. In the Proceedings 
of ACM SIGIR 1998, pp 275-281, 1998. 
C. Zhai, J. Lafferty. A Study of Smoothing Tech-
niques for Language Modeling Applied to ad hoc 
Information Retrieval. In the Proceedings of the 
ACM SIGIR Conference on Research and Devel-
opment in Information Retrieval, 2001. 
Ittycheriah, S. Roukos. IBM's Statistical Question 
Answering System-TREC 11. In the Eleventh Text 
Retrieval Conference (TREC 2002), Gaithersburg, 
Maryland, November 2002. 
Hui Yang, Tat-Seng Chua. The Integration of Lexical 
Knowledge and External Resources for Question 
Answering. In the Proceedings of the Eleventh 
Text REtrieval Conference (TREC?2002), Mary-
land, USA, 2002, page 155-161. 
Andres Corrada-Emmanuel, W.Bruce Croft, Vanessa 
Murdock. Answer Passage Retrieval for Question 
Answering. In the Proceedings of the 27th Annual 
International Conference on Research and Devel-
opment in Information Retrieval, pp. 516 ? 517, 
2004. 
Ellen M. Voorhees. Overview of the TREC 2004 
Question Answering Track. In Proceedings of the 
Twelfth Text REtrieval Conference (TREC 2004), 
2004. 
Vanessa Murdock, W. Bruce Croft. Simple Transla-
tion Models for Sentence Retrieval in Factoid 
Question Answering. In the Proceedings of the 
SIGIR 2004 Workshop on Information Retrieval 
for Question Answering, pp.31-35, 2004. 
Thomas Hofmann. Probabilistic Latent Semantic In-
dexing. In the Proceedings of the Twenty-Second 
Annual International SIGIR Conference on Re-
search and Development in Information Retrieval, 
1999. 
A. Berger and J. Lafferty. Information Retrieval as 
Statistical Translation. In the Proceedings of ACM 
SIGIR-1999, pp. 222?229, Berkeley, CA, August 
1999. 
A. Echihabi and D.Marcu. A noisy-channel approach 
to question answering. In the Proceeding of the 
41st Annual Meeting of the Association for Com-
putational Linguistics, Sappora, Japan, 2003. 
Leif Azzopardi, Mark Girolami and Keith van 
Rijsbergen. Topic Based Language Models for ad 
hoc Information Retrieval. In the Proceeding of 
IJCNN 2004 & FUZZ-IEEE 2004, July 25-29, 
2004, Budapest, Hungary. 
Jian-Yun Nie. Integrating Term Relationships into 
Language Models for Information Retrieval. Re-
port at ICT-CAS. 
Jianfeng Gao, Jian-Yun Nie, Guangyuan Wu and 
Guihong Cao. 2004b. Dependence language model 
for information retrieval. In SIGIR-2004. Sheffield, 
UK, July 25-29. 
Youzheng Wu, Jun Zhao, Bo Xu. Chinese Named 
Entity Recognition Model Based on Multiple Fea-
tures. In the Proceeding of HLT/EMNLP 2005, 
Vancouver, B.C., Canada, pp.427-434, 2005. 
Youzheng Wu, Jun Zhao, Xiangyu Duan  and Bo Xu. 
Building an Evaluation Platform for Chinese Ques-
tion Answering Systems. In Proceeding of the First 
National Conference on Information Retrieval and 
Content Security. Shanghai, China, December, 
2004.(In Chinese) 
 
63
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 512?522, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Translation Model Based Cross-Lingual Language Model Adaptation: from
Word Models to Phrase Models
Shixiang Lu, Wei Wei, Xiaoyin Fu, and Bo Xu
Interactive Digital Media Technology Research Center
Institute of Automation, Chinese Academy of Sciences
95 Zhongguancun East Road, Haidian District, Beijing 100190, China
{shixiang.lu,wei.wei.media,xiaoyin.fu,xubo}@ia.ac.cn
Abstract
In this paper, we propose a novel translation
model (TM) based cross-lingual data selec-
tion model for language model (LM) adapta-
tion in statistical machine translation (SMT),
from word models to phrase models. Given
a source sentence in the translation task, this
model directly estimates the probability that
a sentence in the target LM training corpus
is similar. Compared with the traditional ap-
proaches which utilize the first pass translation
hypotheses, cross-lingual data selection mod-
el avoids the problem of noisy proliferation.
Furthermore, phrase TM based cross-lingual
data selection model is more effective than
the traditional approaches based on bag-of-
words models and word-based TM, because
it captures contextual information in model-
ing the selection of phrase as a whole. Ex-
periments conducted on large-scale data set-
s demonstrate that our approach significantly
outperforms the state-of-the-art approaches on
both LM perplexity and SMT performance.
1 Introduction
Language model (LM) plays a critical role in sta-
tistical machine translation (SMT). It seems to be
a universal truth that LM performance can always
be improved by using more training data (Brants et
al., 2007), but only if the training data is reason-
ably well-matched with the desired output (Moore
and Lewis, 2010). It is also obvious that among the
large training data the topics or domains of discus-
sion will change (Eck et al2004), which causes the
mismatch problems with the translation task. For
this reason, most researchers preferred to select sim-
ilar training data from the large training corpus in the
past few years (Eck et al2004; Zhao et al2004;
Kim, 2005; Masskey and Sethy, 2010; Axelrod et
al., 2011). This would empirically provide more ac-
curate lexical probabilities, and thus better match the
translation task at hand (Axelrod et al2011).
Many previous data selection approaches for LM
adaptation in SMT depend on the first pass transla-
tion hypotheses (Eck et al2004; Zhao et al2004;
Kim, 2005; Masskey and Sethy, 2010), they selec-
t the sentences which are similar to the translation
hypotheses. These schemes are overall limited by
the quality of the translation hypotheses (Tam et al
2007 and 2008), and better initial translation hy-
potheses lead to better selected sentences (Zhao et
al., 2004). However, while SMT has achieved a
great deal of development in recent years, the trans-
lation hypotheses are still far from perfect (Wei and
Pal, 2010), which have many noisy data. The noisy
translation hypotheses mislead data selection pro-
cess (Xu et al2001; Tam et al2006 and 2007;
Wei and Pal, 2010), and thus take noisy data into the
selected training data, which causes noisy prolifera-
tion and degrades the performance of adapted LM.
Furthermore, traditional approaches for LM adap-
tation are based on bag-of-words models and con-
sidered to be context independent, despite of their
state-of-the-art performance, such as TF-IDF (Eck et
al., 2004; Zhao et al2004; Hildebrand et al2005;
Kim, 2005; Foster and Kuhn, 2007), centroid simi-
larity (Masskey and Sethy, 2010), and cross-lingual
similarity (CLS) (Ananthakrishnan et al2011a).
They all perform at the word level, exact only ter-
512
m matching schemes, and do not take into account
any contextual information when modeling the se-
lection by single words in isolation, which degrade
the quality of selected sentences.
In this paper, we argue that it is beneficial to mod-
el the data selection based on the source transla-
tion task directly and capture the contextual infor-
mation for LM adaptation. To this end, we propose
a more principled translation model (TM) based
cross-lingual data selection model for LM adapta-
tion, from word models to phrase models. We as-
sume that the data selection should be performed
by the cross-lingual model and at the phrase lev-
el. Given a source sentence in the translation task,
this model directly estimates the probability before
translation that a sentence in the target LM train-
ing corpus is similar. Therefore, it does not require
the translation task to be pre-translation as in mono-
lingual adaptation, and can address the problem of
noisy proliferation.
To the best of our knowledge, this is the first
extensive and empirical study of using phrase T-
M based cross-lingual data selection for LM adap-
tation. This model learns the transform probabili-
ty of a multi-term phrase in a source sentence giv-
en a phrase in the target sentence of LM training
corpus. Compared with bag-of-words models and
word-based TM that account for selecting single
words in isolation, this model performs at the phrase
level and captures some contextual information in
modeling the selection of phrase as a whole, thus it
is potentially more effective. More precise data se-
lection can be determined for phrases than for word-
s. In this model, we propose a linear ranking model
framework to further improve the performance, re-
ferred to the linear discriminant function (Duda et
al., 2001; Collins, 2002; Gao et al2005) in pattern
classification and information retrieval (IR), where
different models are incorporated as features, as we
will show in our experiments.
Unlike the general TM in SMT, we explore the
use of TextRank algorithm (Mihalcea et al2004)
to identify and eliminate unimportant words (e.g.,
non-topical words, common words) for corpus pre-
processing, and construct TM by important words.
This reduces the average number of words in cross-
lingual data selection model, thus improving the ef-
ficiency. Moreover, TextRank utilizes the contex-
t information of words to assign term weights (Lee
et al2008), which makes phrase TM based cross-
lingual data selection model play its advantage of
capturing the contextual information, thus further
improving the performance.
The remainder of this paper is organized as fol-
lows. Section 2 introduces the related work of
LM adaptation. Section 3 presents the framework
of cross-lingual data selection for LM adaptation.
Section 4 describes our proposed TM based cross-
lingual data selection model: from word models to
phrase models. In section 5 we present large-scale
experiments and analyses, and followed by conclu-
sions and future work in section 6.
2 Related Work
TF-IDF and cosine similarity have been widely used
for LM adaptation (Eck et al2004; Zhao et al
2004; Hildebrand et al2005; Kim, 2005; Foster
and Kuhn, 2007). Masskey and Sethy (2010) se-
lected the auxiliary data by computing centroid sim-
ilarity score to the centroid of the in-domain data.
The main idea of these methods is to select the sen-
tences which are similar to the first pass translation
hypotheses or in-domain corpus from the large LM
training corpus, and estimate the bias LM for SMT
system to improve the translation quality.
Tam et al2007 and 2008) proposed a bilingual-
LSA model for LM adaptation. They integrated
the LSA marginal into the target generic LM using
marginal adaptation which minimizes the Kullback-
Leibler divergence between the adapted LM and the
generic LM. Ananthakrishnan et al2011a) pro-
posed CLS to bias the count and probability of cor-
responding n-gram through weighting the LM train-
ing corpus. However, these two cross-lingual ap-
proaches focus on modify LM itself, which are d-
ifferent from data selection method for LM adap-
tation. In our comparable experiments, we apply
CLS for the first time to the task of cross-lingual
data selection for LM adaptation. Due to lack of
smoothing measure for sparse vector representation
in CLS, the similarity computation is not accurate
which degrades the performance of adapted LM. To
avoid this, we add smoothing measure like TF-IDF,
called CLSs, as we will discuss in the experiments.
Snover et al2008) used a word TM based CLIR
513
system (Xu et al2001) to select a subset of tar-
get documents comparable to the source document
for adapting LM. Because of the data sparseness in
the document state and it operated at the document
level, this model selected large quantities of irrele-
vant text, which may degrade the adapted LM (Eck
et al2004; Ananthakrishnan et al2011b). In our
word TM based cross-lingual data selection model,
we operate at the sentence level and add the smooth-
ing mechanism by integrating with the background
word frequency model, and these can significantly
improve the performance. Axelrod et al2011)
proposed a bilingual cross-entropy difference to se-
lect data from parallel corpus for domain adaptation
which captures the contextual information slightly,
and outperformed monolingual cross-entropy differ-
ence (Moore and Lewis, 2010), which first shows the
advantage of bilingual data selection. However, its
performance depends on the parallel in-domain cor-
pus which is usually hard to find, and its application
is assumed to be limited.
3 Cross-Lingual Data Selection for
Language Model Adaptation
Our LM adaptation is an unsupervised similar train-
ing data selection guided by TM based cross-lingual
data selection model. For the source sentences in
the translation task, we estimate a new LM, the bias
LM, from the corresponding target LM training sen-
tences which are selected as the similar sentences.
Since the size of the selected sentences is small, the
corresponding bias LM is specific and more effec-
tive, giving high probabilities to those phrases that
occur in the desired output translations.
Following the work of (Zhao et al2004; Snover
et al2008), the generic LM Pg(wi|h) and the bias
LM Pb(wi|h) are combined using linear interpola-
tion as the adapted LM Pa(wi|h), which is shown to
improve the performance over individual model,
Pa(wi|h) = ?Pg(wi|h) + (1? ?)Pb(wi|h) (1)
where the interpolation factor ? can be simply esti-
mated using the Powell Search algorithm (Press et
al., 1992) via cross-validation.
Our work focuses on TM based cross-lingual data
selection model, from word model to phrase models,
and the quality of this model is crucial to the perfor-
mance of adapted LM.
4 Translation Model for Cross-Lingual
Data Selection (CLTM)
Let Q = q1, . . . ,qj be a source sentence in the
translation task and S = w1, . . . ,wi be a sentence
in the general target LM training corpus, thus cross-
lingual data selection model can be framed proba-
bilistically as maximizing the P (S|Q) . By Bayes?
rule,
P (S|Q) =
P (S)P (Q|S)
P (Q)
(2)
where the prior probability P (S) can be viewed as
uniform, and the P (Q) is constant across all sen-
tences. Therefore, selecting a sentence to maximize
P (S|Q) is equivalent to selecting a sentence that
maximizes P (Q|S).
4.1 Word-Based Translation Model for
Cross-Lingual Data Selection (CLWTM)
4.1.1 Cross-Lingual Sentence Selection Model
Following the work of (Xu et al2001; Snover et al
2008), CLWTM can be described as
P (Q|S) =
?
q?Q
P (q|S) (3)
P (q|S) = ?P (q|Cq)+ (1??)
?
w?S
P (q|w)P (w|S)
(4)
where ? is the interpolation weight empirically set
as a constant1, P (q|w) is the word-based TM which
is estimated by IBM Model 1 (Brown et al1993)
from the parallel corpus, P (q|Cq) and P (w|S) are
the un-smoothed background and sentence model,
respectively, estimated using maximum likelihood
estimation (MLE) as
P (q|Cq) =
freq(q, Cq)
|Cq|
(5)
P (w|S) =
freq(w, S)
|S|
(6)
where Cq refers to the translation task, freq(q, Cq)
refers to the number of times q occurs in Cq,
freq(w, S) refers to the number of times w occurs
in S, and |Cq| and |S| are the sizes of the translation
task and the current target sentence, respectively.
1As in Xu et al2001), a value of 0.3 was used for ?.
514
4.1.2 Ranking Candidate Sentences
Because of the data sparseness in the sentence state
which degrades the model, Equation (6) does not
perform well in our data selection experiments. In-
spired by the work of (Berger et al1999) in IR, we
make the following smoothing mechanism:
P (q|S) = ?P (q|Cq)+(1??)
?
w?S
P (q|w)Ps(w|S)
(7)
Ps(w|S) = ?P (w|Cs) + (1? ?)P (w|S) (8)
P (w|Cs) =
freq(w,Cs)
|Cs|
(9)
where P (w|Cs) is the un-smoothed background
model, estimated using MLE as Equation (5), Cs
refers to the LM training corpus and |Cs| refers to
its size. Here, ? is interpolation weight; notice that
letting ? = 0 in Equation (8) reduces the model to
the un-smoothed model in Equation (4).
4.2 Phrase-Based Translation Model for
Cross-Lingual Data Selection (CLPTM)
4.2.1 Cross-Lingual Sentence Selection Model
The phrase-based TM (Koehn et al2003; Och and
Ney, 2004) has shown superior performance com-
pared to the word-based TM. In this paper, the
goal of phrase-based TM is to transfer S into Q.
Rather than transferring single words in isolation,
the phrase model transfers one sequence of word-
s into another sequence of words, thus incorporat-
ing contextual information. Inspired by the work
of web search (Gao et al2010) and question re-
trieval in community question answer (Q&A) (Zhou
et al2011), we assume the following generative
process: first the sentence S is broken into K non-
empty word sequences w1, . . . ,wk, then each is
transferred into a new non-empty word sequences
q1, . . . ,qk, and finally these phrases are permutat-
ed and concatenated to form the sentence Q, where
q and w denote the phrases or consecutive sequence
of words.
To formulate this generative process, let U denote
the segmentation of S into K phrases w1, . . . ,wk,
and let V denote the K phrases q1, . . . ,qk, we refer
to these (wi,qi) pairs as bi-phrases. Finally, let M
denote a permutation ofK elements representing the
final ranking step.
Next we place a probability distribution over
rewrite pairs. Let B(S,Q) denote the set of U ,
V , M triples that transfer S into Q. Here we as-
sume a uniform probability over segmentations, so
the phrase-based selection probability can be formu-
lated as
P (Q|S) ?
?
(U,V,M)?
B(S,Q)
P (V |S,U) ? P (M |S,U, V )
(10)
Then, we use the maximum approximation to the
sum:
P (Q|S) ? max
(U,V,M)?
B(S,Q)
P (V |S,U) ? P (M |S,U, V )
(11)
Although we have defined a generative model for
transferring S into Q, our goal is to calculate the
ranking score function over existing Q and S. How-
ever, this model can not be used directly for sen-
tence ranking becauseQ and S are often of different
lengths, the length of S is almost 1.5 times to that of
Q in our corpus, leaving many words in S unaligned
to any word in Q. This is another key difference be-
tween our task and SMT. As pointed out by the pre-
vious work (Berger and Lafferty, 1999; Gao et al
2010; Zhou et al2011), sentence-query selection
requires a distillation of the sentence, while selec-
tion of natural language tolerates little being thrown
away. Thus we restrict our attention to those key sen-
tence words that form the distillation of S, do not
consider the unaligned words in S, and assume that
Q is transfered only from the key sentence words.
In this paper, the key sentence words are identi-
fied via word alignment. Let A = a1 . . . aJ be the
?hidden? word alignment, which describes a map-
ping from a term position j in Q to a word position
aj in S. We assume that the positions of the key
sentence words are determined by the Viterbi align-
ment A?, which can be obtained using IBM Model 1
(Brown et al1993) as follows:
A? = argmax
A
P (Q,A|S)
= argmax
A
{
P (J |I)
J?
j=1
P (qj |waj )
}
=
[
argmax
aj
P (qj |waj )
]J
j=1
(12)
515
Given A?, when scoring a given Q/S pair, we re-
strict our attention to those U , V , M triples that are
consistent with A?, which we denote as B(S,Q, A?).
Here, consistency requires that if two words are
aligned in A?, then they must appear in the same bi-
phrase (wi,qi). Once the word alignment is fixed,
the final permutation is uniquely determined, so we
can safely discard that factor. Then Equation (11)
can be written as
P (Q|S) ? max
(U,V,M)?
B(S,Q,A?)
P (V |S,U) (13)
For the sole remaining factor P (V |S,U), we
assume that a segmented queried question V =
q1, . . . ,qk is generated from left to right by transfer-
ring each phrase w1, . . . ,wk independently, as fol-
lows:
P (V |S,U) =
K?
k=1
P (qk|wk) (14)
where P (qk|wk) is a phrase translation probability
computed from the parallel corpus, which can be es-
timated in two ways (Koehn et al2003; Och and
Ney, 2004): relative frequency and lexical weight-
ing, and has two format: phrase translation proba-
bility and lexical weight probability.
In order to find the maximum probability assign-
ment P (Q|S) efficiently, we use a dynamic pro-
gramming approach, somewhat similar to the mono-
tone decoding algorithm described in the work (Och,
2002). We consider quantity aj as the maximal
probability of the most likely sequence of phrases
in S covering the first j words in Q, therefore the
probability can be calculated using the following re-
cursion:
step (1). Initialization:
?0 = 1 (15)
step (2). Induction:
?j =
?
j?<j,q=qj?+1...qj
{
?j?P (q|wq)
}
(16)
step (3). Total:
P (Q|S) = ?J (17)
4.2.2 Ranking Candidate Sentences
However, directly using the phrase-based TM, com-
puted in Equations (15) to (17), to rank the candi-
date sentences does not perform well. Inspired by
the linear discriminant function (Duda et al2001;
Collins, 2002; Gao et al2005) in pattern classifi-
cation and IR, we therefore propose a linear rank-
ing model framework for cross-lingual data selec-
tion model in which different models are incorporat-
ed as features.
We consider the linear ranking model as follows:
Score(Q,S) = ?T ?H(Q,S)
=
N?
n=1
?nhn(Q,S) (18)
where the model has a set of N features, and each
feature is an arbitrary function that maps (Q|S) to a
real value, i.e., H(Q,S) ? R. ?n for n = 1 . . . N
is the corresponding parameters of each feature,
and we optimize these parameters using the Pow-
ell Search algorithm (Press et al1992) via cross-
validation.
The used features in the linear ranking model are
as follows:
? Phrase translation feature (PT):
hPT (Q,S,A) = logP (Q|S), where P (Q|S)
is computed using Equations (15) to (17), and
P (qk|wk) is phrase translation probability.
? Inverted phrase translation feature (IPT):
hIPT (S,Q,A) = logP (S|Q), where P (S|Q)
is computed using Equations (15) to (17), and
P (wk|qk) is inverted phrase translation proba-
bility.
? Lexical weight feature (LW): hLW (Q,S,A) =
logP (Q|S), where P (Q|S) is computed using
Equations (15) to (17), and P (qk|wk) is lexical
weight probability.
? Inverted lexical weight feature (ILW):
hILW (S,Q,A) = logP (S|Q), where
P (S|Q) is computed using Equations (15) to
(17), and P (wk|qk) is inverted lexical weight
probability.
? Unaligned word penalty feature (UWP):
hUWP (Q,S,A), which is defined as the ratio
between the number of unaligned terms and
the total number of terms in Q.
516
? Word-based translation feature (WT):
hWT (Q,S,A) = logP (Q|S), where P (Q|S)
is the word-based TM defined by Equations (3)
and (7).
4.3 Eliminating Unimportant Words (EUW)
To improve the efficiency of cross-lingual data se-
lection process, we consider the translation task, the
LM training corpus and the parallel corpus in our
task are constructed by the key words or importan-
t words, and thus construct TM by the key words
or important words, which is another key difference
between our task and SMT. We identify and elimi-
nate unimportant words, somewhat similar to Q&A
retrieval (Lee et al2008; Zhou et al2011). Thus,
the average number of words (the total word number
inQ and S) in cross-lingual sentence selection mod-
el would be minimized naturally, and the efficiency
of cross-lingual data selection would be improved.
In this paper, we adopt a variant of TextRank
algorithm (Mihalcea and Tarau, 2004), a graph-
based ranking model for key word extraction which
achieves state-of-the-art accuracy. It identifies and
eliminates unimportant words from the corpus, and
assumes that a word is unimportant if it holds a rela-
tively low significance in the corpus. Compared with
the traditional approaches, such as TF-IDF, Tex-
tRank utilizes the context information of words to
assign term weights (Lee et al2008), so it further
improves the performance of CLPTM, as we will
show in the experiments.
Following the work of (Lee et al2008), the rank-
ing algorithm proceeds as follows. First, all the
words in a given document are added as vertices in
a graph. Then edges are added between words (ver-
tices) if the words co-occur in a fixed-sized window.
The number of co-occurrences becomes the weight
of an edge. When the graph is constructed, the score
of each vertex is initialized as 1, and the PageRank
based ranking algorithm is run on the graph itera-
tively until convergence. The TextRank score Rkwi,D
of a word wi in document D at kth iteration is de-
fined as follows:
Rkwi,D = (1?d)+d?
?
?j:(i,j)?G
ei,j
?
?l:(j,l)?G ej,l
Rk?1wj ,D
(19)
where d is a damping factor usually set as a constan-
t2, and ei,j is an edge weight between wi and wj .
In our experiments, we manually set the propor-
tion to be removed as 25%, that is to say, 75% of
total words in the documents would be remained as
the important words.
5 Experiments
We measure the utility of our proposed LM adap-
tation approach in two ways: (a) comparing refer-
ence translations based perplexity of adapted LMs
with the generic LM, and (b) comparing SMT per-
formance of adapted LMs with the generic LM.
5.1 Corpus and Tasks
We conduct experiments on two Chinese-to-English
translation tasks: IWSLT-07 (dialogue domain) and
NIST-06 (news domain).
IWSLT-07. The bilingual training corpus comes
from BTEC3 and CJK4 corpus, which contain-
s 3.82K sentence pairs with 3.0M/3.1M Chi-
nese/English words. The LM training corpus is from
the English side of the parallel data (BTEC, CJK,
and CWMT20085), which consists of 1.34M sen-
tences and 15.2M English words. The test set is
IWSLT-07 test set which consists of 489 sentences,
and the development set is IWSLT-05 test set which
consists of 506 sentences.
NIST-06. The bilingual training corpus comes
from Linguistic Data Consortium (LDC)6, which
consists of 3.4M sentence pairs with 64M/70M Chi-
nese/English words. The LM training corpus is from
the English side of the parallel data as well as the
English Gigaword corpus7, which consists of 11.3M
sentences. The test set is 2006 NIST MT Evaluation
test set which consists of 1664 sentences, and the de-
velopment set is 2005 NIST MT Evaluation test set
which consists of 1084 sentences.
2As in Lee et al2008), a value of 0.85 was used for d.
3Basic Traveling Expression Corpus
4China-Japan-Korea
5The 4th China Workshop on Machine Translation
6LDC2002E18, LDC2002T01, LDC2003E07, LD-
C2003E14, LDC2003T17, LDC2004T07, LDC2004T08,
LDC2005T06, LDC2005T10, LDC2005T34, LDC2006T04,
LDC2007T09
7LDC2007T07
517
(a)IWSLT-07 (b)NIST-06
Figure 1: English reference translations based perplexity of adapted LMs vs. the size of selected training data with
different approaches on two development sets.
5.2 Perplexity Analysis
We randomly divide the development set into five
subsets and conduct 5-fold cross-validation experi-
ments. In each trial, we tune the parameter ? in E-
quation (1) and parameter ? in Equation (18) with
four of five subsets and then apply it to one re-
maining subset. The experiments reported below are
those averaged over the five trials.
We estimate the generic 4-gram LM with the en-
tire LM training corpus as the baseline. Then, we se-
lect the top-N sentences which are similar to the de-
velopment set, estimate the bias 4-gram LMs (with
n-gram cutoffs tuned as above) with these selected
sentences, and interpolate with the generic 4-gram
LM as the adapted LMs. All the LMs are estimated
by the SRILM toolkit (Stolcke, 2002). Perplexity is
a metric of LM performance, and the lower perplexi-
ty value indicates the better performance. Therefore,
we estimate the perplexity of adapted LMs accord-
ing to English reference translations.
Figure 1 shows the perplexity of adapted LMs vs.
the size of selected data. In this paper, we choose
TF-IDF as the foundation of our solution since TF-
IDF has gained the state-of-the-art performance for
LM adaptation (Eck et al2004; Hildebrand et al
2005; Kim, 2005; Foster and Kuhn, 2007). CLS
refers to the cross-lingual similarity of (Ananthakr-
ishnan et al2011a), and CLSs is our proposed im-
proved algorithm on CLS with optimization mea-
sure like TF-IDF. CLWTM(? = 0) refers to S-
nover et al2008), which is the un-smooth ver-
Task Method Perplexity Reduction
IWSLT-07
Baseline 524.1 ?
TF-IDF 471.4 10.06%
CLS 475.7 9.23%
CLSs 468.9 10.53%
CLWTM(? = 0) 463.5 11.56%
CLWTM 451.5 13.85%
CLPTM(l = 4) 435.3 16.94%
NIST-06
Baseline 398.3 ?
TF-IDF 346.2 13.08%
CLS 351.6 11.72%
CLSs 340.9 14.41%
CLWTM(? = 0) 341.1 14.36%
CLWTM 332.7 16.47%
CLPTM(l = 4) 319.2 19.86%
Table 1: English reference translations based perplexi-
ty of adapted LMs with different approaches on two test
sets, with the top 8K sentences on IWSLT-07 and top 16K
sentences on NIST-06, respectively.
sion of our proposed CLWTM in the document s-
tate. CLPTM(l = 4) is our proposed CLPTM with
a maximum phrase length of four, and we score the
target sentences by the highest scoring Q/S pair.
The results in Figure 1 indicate that English ref-
erence translations based perplexity of adapted LMs
decreases consistently with increase of the size of
selected top-N sentences, and increases consistent-
ly after a certain size in all approaches. Therefore,
proper size of similar sentences with the transla-
tion task makes the adapted LM perform well, but
if too many noisy data are taken into the selected
sentences, the performance becomes worse. Similar
observations have been done by (Eck et al2004;
518
Task # Method BLEU
IWSLT-07
1 Baseline 33.60
2 TF-IDF 34.14
3 CLS 34.08
4 CLSs 34.18
5 CLWTM(? = 0) 34.22
6 CLWTM 34.30
7 CLPTM(l = 4) 34.49
NIST-06
8 Baseline 29.15
9 TF-IDF 29.78
10 CLS 29.73
11 CLSs 29.84
12 CLWTM(? = 0) 29.87
13 CLWTM 29.93
14 CLPTM(l = 4) 30.17
Table 2: Comparison of SMT performance (p < 0.05)
with different approaches for LM adaptation on two test
sets.
Axelrod et al2011). Furthermore, it is comforting
that our approaches (CLWTM and CLPTM(l = 4))
performs better and are more stable than other ap-
proaches.
According to the perplexity results in Figure 1,
we select the top 8K sentences on IWSLT-07 and
top 16K sentences on NIST-06 which are similar to
the test set for adapting LM, respectively. Table 1
shows English reference translations based perplex-
ity of adapted LMs on two test sets. Our approach-
es have significantly reduction in perplexity com-
pared with other approaches, and the results indicate
that adapted LMs are significantly better predictors
of the corresponding translation task at hand than
the generic LM. We use these adapted LMs for next
translation experiments to show the detailed perfor-
mance of selected training data for LM adaptation.
5.3 Translation Experiments
We carry out translation experiments on the test set
by hierarchical phrase-based (HPB) SMT (Chiang,
2005 and 2007) system to demonstrate the utility of
LM adaptation on improving SMT performance by
BLEU score (Papineni et al2002). The generic LM
and adapted LMs are estimated as above in perplexi-
ty analysis experiments. We use minimum error rate
training (Och, 2003) to tune the feature weights of
HPB for maximum BLEU score on the development
set with serval groups of different start weights.
Table 2 shows the main translation results on two
Task Translation Hypotheses BLEU
IWSLT-07
First Pass 34.14
Second Pass 34.31
NIST-06
First Pass 29.78
Second Pass 29.91
Table 3: The impact of noisy data in the translation hy-
potheses on the performance of LM adaptation.
test sets, and the improvements are statistically sig-
nificant at the 95% confidence interval with respect
to the baseline. From the comparison results, we get
some clear trends:
(1) Cross-lingual data selection model outper-
forms the traditional approaches which utilize the
first pass translation hypotheses (row 4 vs. row2;
row 11 vs. row 9), but the detailed impact of noisy
data in the translation hypotheses on data selection
will be shown in the next section (section 5.4).
(2) CLWTM significantly outperforms CLSs (row
6 vs. row 4; row 13 vs. row 11), we suspect that
word-based TM makes more accurate cross-lingual
data selection model than single cross-lingual pro-
jection (Ananthakrishnan et al2011a).
(3) Compared with (Snover et al2008), adding
the smoothing mechanism in the sentence state for
CLWTM significantly improves the performance
(row 6 vs. row 5; row 13 vs. row 12).
(4) Phrase-based TM (CLPTM) significantly out-
performs the state-of-the-art approaches based on
bag-of-words models and word-based TM (row 7 vs.
row 2, row 4, row 5 and row 6; row 14 vs. row 9,
row 11, row 12 and row 13).
5.4 Impact of Noisy Data in the Translation
Hypotheses
The experiment results in Table 2 indicate the sec-
ond pass translation hypotheses (row 2 and row 9)
made by TF-IDF are better than the first pass trans-
lation hypotheses (row 1 and row 8), so we consid-
er that these translations have less noisy data. Thus,
they were considered as the new translation hypothe-
ses (the second pass) to select the similar sentences
for LM adaptation by TF-IDF.
Table 3 shows the impact of noisy data in the
translation hypotheses on the performance of adapt-
ed LMs. The observed improvement suggests that
better initial translations which have less noisy data
519
Task Phrase Length BLEU
IWSLT-07
l = 1 34.33
l = 2 34.44
l = 3 34.49
l = 4 34.49
NIST-06
l = 1 29.97
l = 2 30.07
l = 3 30.14
l = 4 30.17
Table 4: The impact of phrase length in CLPTM on the
performance of LM adaptation, and the maximum phrase
length is four.
lead to better adapted LMs, and thereby better sec-
ond iteration translations. Therefore, it is advisable
to use cross-lingual data selection for LM adaptation
in SMT, which can address the problem of noisy pro-
liferation.
5.5 Impact of Phrase Length
The results in Table 4 show that longer phrases do
yield some visible improvement up to the maximum
length of four. This may suggest that some proper-
ties captured by longer phrases are also captured by
other features. The performances when the phrase
length is 1 are better than that of single word-based
TM (row 6 and row 13 in Table 2), this suspec-
t that the features in our linear ranking model are
useful. However, it will be instructive to explore the
methods of preserving the improvement generated
by longer phrase when more features are incorporat-
ed in the future work.
5.6 Impact of Eliminating Unimportant Words
Table 5 shows the results of EUW by TextRank al-
gorithm on the performance of CLTM for LM adap-
tation. Initial represents that we do not eliminate
unimportant words. Average number represents the
average number of words (the total word number in
Q and S) in cross-lingual data selection model. The
average number is reduced when unimportant words
are eliminated, from 19 to 12 on IWSLT-07 and from
37 to 24 on NIST-06, respectively. This makes the
cross-lingual data selection process become more
efficient. In CLWTM, the performance with EUW
is basically the same with that of the initial state; but
in CLPTM, EUW outperforms the initial state be-
cause TextRank algorithm utilizes the context infor-
Task Method
Average
BLEU
Number CLWTM
CLPTM
(l = 4)
IWSLT-07
Initial 19 34.31 34.47
EUW 12 34.30 34.49
NIST-06
Initial 37 29.91 30.12
EUW 24 29.93 30.17
Table 5: The impact of eliminating unimportant words
by TextRank algorithm on the performance of CLTM for
LM adaptation.
mation of words when assigning term weights, thus
makeing CLPTM play its advantage of capturing the
contextual information.
6 Conclusions and Future Work
In this paper, we propose a novel TM based cross-
lingual data selection model for LM adaptation in
SMT, from word models to phrase models, and aims
to find the LM training corpus which are similar to
the translation task at hand. Unlike the general TM
in SMT, we explore the use of TextRank algorithm
to identify and eliminate unimportant words for cor-
pus preprocessing, and construct TM by importan-
t words. Compared with the traditional approach-
es which utilize the first pass translation hypothe-
ses, cross-lingual data selection avoids the prob-
lem of noisy proliferation. Furthermore, phrase T-
M based cross-lingual data selection is more effec-
tive than the traditional approaches based on bag-
of-words models and word-based TM, because it
captures contextual information in modeling the s-
election of phrase as a whole. Large-scale exper-
iments are conducted on LM perplexity and SMT
performance, and the results demonstrate that our
approach solves the two aforementioned disadvan-
tages and significantly outperforms the state-of-the-
art methods for LM adaptation.
There are some ways in which this research could
be continued in the future. First, we will utilize our
approach to mine large-scale corpora by distributed
infrastructure system, and investigate the use of our
approach for other domains, such as speech transla-
tion system. Second, the significant improvement of
LM adaptation based on cross-lingual data selection
is exciting, so it will be instructive to explore oth-
er knowledge based cross-lingual data selection for
LM adaptation, such as latent semantic model.
520
Acknowledgments
This work was supported by 863 program in China
(No. 2011AA01A207). We thank Guangyou Zhou
for his helpful discussions and suggestions. We also
thank the anonymous reviewers for their insightful
comments.
References
Sankaranarayanan Ananthakrishnan, Rohit Prasad, and
Prem Natarajan. 2011a. On-line language model bias-
ing for statistical machine translation. In Proceedings
of ACL, pages 445-449.
Sankaranarayanan Ananthakrishnan, Stavros Tsakalidis,
Rohit Prasad, and Prem Natarajan. 2011b. On-
line language model biasing for multi-pass automat-
ic speech recognition. In Proceedings of INTER-
SPEECH, pages 621-624.
Amittai Axelrod, Xiaodong He, and Jianfeng Gao. 2011.
Domain adaptation via pseudo in-domain data selec-
tion. In Proceedings of EMNLP, pages 355-362.
Adam Berger and John Lafferty. 1999. Information re-
trieval as statistical translation. In Proceedings of SI-
GIR, pages 222-229.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och,
and Jeffrey Dean. 2007. Large language models in
machine translation. In Proceedings of EMNLP, pages
858-867.
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The mathematic-
s of statistical machine translation: parameter estima-
tion. Computational Linguistics, 19(2):263-311.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
ACL, pages 263-270.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201-228.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: theory and experi-
ments with the perceptron algorithm. In Proceedings
of EMNLP, pages 1-8.
Richard O. Duda, Peter E. Hart, and David G. Stork.
2001. Pattern classification. John Wiley & Sons, Inc.
Matthias Eck, Stephan Vogel, and Alex Waibel. 2004.
Language model adaptation for statistical machine
translation based on information retrieval. In Proceed-
ings of LREC, pages 327-330.
George Foster and Roland Kuhn. 2007. Mixture-model
adaptation for SMT. In Proceedings of ACL, pages
128-135.
Jianfeng Gao, Haoliang Qi, Xinsong Xia, and Jian-Yun
Nie. 2005. Linear discriminative model for informa-
tion retrieval. In Proceedings of SIGIR, pages 290-
297.
Jianfeng Gao, Xiaodong He, and Jian-Yun Nie. 2010.
Clickthrough-based translation models for web search:
from word models to phrase models. In Proceedings
of CIKM, pages 1139-1148.
Almut Silja Hildebrand, Matthias Eck, Stephan Vogel,
and Alex Waibel. 2005. Adaptation of the transla-
tion model for statistical machine translation based in-
formation retrieval. In Proceedings of EAMT, pages
133-142.
Woosung Kim. 2005. Language model adaptation for
automatic speech recognition and statistical machine
translation. Ph.D. thesis, The Johns Hopkins Univer-
sity.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of NAACL, pages 48-54.
Jung-Tae Lee, Sang-Bum Kim, Young-In Song, and Hae-
Chang Rim. 2008. Bridging lexical gaps between
queries and questions on large online Q&A collection-
s with compact translation models. In Proceedings of
EMNLP, pages 410-418.
Sameer Masskey and Abhinav Sethy. 2010. Resampling
auxiliary data for language model adaptation in ma-
chine translation for speech. In Proceedings of ICAS-
SP, pages 4817-4820.
Rada Mihalcea and Paul Tarau. 2004. TextRank: Bring-
ing order into text. In Proceedings of EMNLP, pages
404-411.
Robert C. Moore and William Lewis. 2010. Intelligent
selection of language model training data. In Proceed-
ings of ACL, pages 220-224.
Franz Josef Och. 2002. Statistical mahcine transla-
tion: from single word models to alignment templates.
Ph.D thesis, RWTH Aachen.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of ACL,
pages 160-167.
Franz Josef Och and Hermann Ney. 2004. The alignmen-
t template approach to statistical machine translation.
Computational Linguistics, 30(4):417-449.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
jing Zhu. 2002. BLEU: A method for automatic eval-
uation of machine translation. In Proceedings of ACL,
pages 311-318.
William H. Press, Saul A. Teukolsky, William T. Vetter-
ling, and Brian P. Flannery. 1992. Numerical Recipes
in C. Cambridge University Press.
521
Matthew Snover, Bonnie Dorr, and Richard Marcu.
2008. Language and translation model adaptation us-
ing comparable corpora. In Proceedings of EMNLP,
pages 857-866.
Andreas Stolcke. 2002. SRILM - An extensible lan-
guage modeling toolkit. In Proceedings of ICSLP,
pages 901-904.
Yik-Cheung Tam and Tanja Schultz. 2006. Unsuper-
vised language model adaptation using latent seman-
tic marginals. In Proceedings of ICSLP, pages 2206-
2209.
Yik-Cheung Tam, Ian Lane, and Tanja Schultz. 2007.
Bilingual-LSA based LM adaptation for spoken lan-
guage translation. In Proceedings of ACL, pages 520-
527.
Yik-Cheung Tam, Ian Lane, and Tanja Schultz. 2008.
Bilingual-LSA based adaptation for statistical machine
translation. Machine Translation, 21:187-207.
Bin Wei and Christopher Pal. 2010. Cross lingual adap-
tation: an experiment on sentiment classifications. In
Proceedings of ACL, pages 258-262.
Jinxi Xu, Ralpha Weischedel, and Chanh Nguyen. 2001.
Evaluating a probabilistic model for cross-lingual in-
formation retrieval. In Proceedings of SIGIR, pages
105-110.
Xiaobing Xue, Jiwoon Jeon, and W. Bruce Croft. 2008.
Retrieval models for question and answer archives. In
Proceedings of SIGIR, pages 475-482.
Bing Zhao, Matthias Eck, and Stephan Vogel. 2004.
Language model adaptation for statistical machine
translation with structured query models. In Proceed-
ings of COLING, pages 411-417.
Guangyou Zhou, Li Cai, Jun Zhao, and Kang Liu. 2011.
Phrase-based translation model for question retrieval
in community question answer archives. In Proceed-
ings of ACL, pages 653-662.
522
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 50?59,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Automated Essay Scoring Based on Finite State Transducer: towards ASR
Transcription of Oral English Speech
Xingyuan Peng?, Dengfeng Ke?, Bo Xu??
?Digital Content Technology and Services Research Center
?National Lab of Pattern Recognition
Institute of Automation, Chinese Academy of Sciences
No.95 Zhongguancun East Road, Haidian district, Beijing 100190, China
{xingyuan.peng,dengfeng.ke,xubo}@ia.ac.cn
Abstract
Conventional Automated Essay Scoring
(AES) measures may cause severe problems
when directly applied in scoring Automatic
Speech Recognition (ASR) transcription
as they are error sensitive and unsuitable
for the characteristic of ASR transcription.
Therefore, we introduce a framework of
Finite State Transducer (FST) to avoid the
shortcomings. Compared with the Latent
Semantic Analysis with Support Vector
Regression (LSA-SVR) method (stands for
the conventional measures), our FST method
shows better performance especially towards
the ASR transcription. In addition, we apply
the synonyms similarity to expand the FST
model. The final scoring performance reaches
an acceptable level of 0.80 which is only 0.07
lower than the correlation (0.87) between
human raters.
1 Introduction
The assessment of learners? language abilities is a
significant part in language learning. In conven-
tional assessment, the problem of limited teach-
er availability has become increasingly serious
with the population increase of language learn-
ers. Fortunately, with the development of com-
puter techniques and machine learning techniques
(natural language processing and automatic speech
recognition), Computer-Assisted Language Learn-
ing (CALL) systems help people to learn language
by themselves.
One form of CALL is evaluating the speech of
the learner. Efforts in speech assessment usually fo-
cus on the integrality, fluency, pronunciation, and
prosody (Cucchiarini et al, 2000; Neumeyer et al,
2000; Maier et al, 2009; Huang et al, 2010) of the
speech, which are highly predictable like the exam
form of the read-aloud text passage. Another form
of CALL is textual assessment. This work is also
named AES. Efforts in this area usually focus on the
content, arrangement and language usage (Landauer
et al, 2003; Ishioka and Kameda, 2004; Kakkonen
et al, 2005; Attali and Burstein, 2006; Burstein et
al., 2010; Persing et al, 2010; Peng et al, 2010; At-
tali, 2011; Yannakoudakis et al, 2011) of the text
written by the learner under a certain form of exam-
ination.
In this paper, our evaluation objects are the oral
English picture compositions in English as a Sec-
ond Language (ESL) examination. This examina-
tion requires students to talk about four successive
pictures with at least five sentences in one minute,
and the beginning sentence is given. This examina-
tion form combines both of the two forms described
above. Therefore, we need two steps in the scoring
task. The first step is Automatic Speech Recognition
(ASR), in which we get the speech scoring features
as well as the textual transcriptions of the speech-
es. Then, the second step could grade the text-free
transcription in an (conventional) AES system. The
present work is mainly about the AES system un-
der the certain situation as the examination grading
criterion is more concerned about the integrated con-
tent of the speech (the reason will be given in sub-
section 3.1).
There are many features and techniques which
are very powerful in conventional AES systems, but
50
applying them in this task will cause two differen-
t problems as the scoring objects are the ASR out-
put results. The first problem is that the inevitable
recognition errors of the ASR will affect the perfor-
mance of the feature extractions and scoring system.
The second problem is caused by the special charac-
teristic of the ASR result. As all these methods are
designed under the normal AES situation that they
are not suitable for the characteristic.
The impact of the first problem can be reduced
by either perfecting the results of the ASR system or
building the AES system which is not sensitive to the
ASR errors. Improving the performance of the ASR
is not what we concern about, so building an error
insensitive AES system is what we care about in this
paper. This makes many conventional features no
longer useful in the AES system, such as spelling
errors, punctuation errors and even grammar errors.
The second problem is caused by applying the
bag-of-words (BOW) techniques to score the ASR
transcription. The BOW are very useful in measur-
ing the content features and are usually robust even
if there are some errors in the scoring transcription.
However, the robustness would not exist anymore
because of the characteristic of the ASR result. It is
known that better performance of ASR (reduce the
word error rate in ASR) usually requires a strong
constrain Language Model (LM). It means that more
meaningless parts of the oral speeches would be rec-
ognized as the words quite related to the topic con-
tent. These words will usually be the key words in
the BOW methods, which will lead to a great distur-
brance for the methods. Therefore, the conventional
BOW methods are no longer appropriate because of
the characteristic of the ASR result.
To tackle the two problems described above, we
apply the FST (Mohri, 2004). As the evaluating ob-
jects are from an oral English picture composition
examination, it has two important features that make
the FST algorithm quite suitable.
? Picture composition examinations require stu-
dents to speak according to the sequence of the
pictures, so there is strong sequentiality in the
speech.
? The sentences for describing the same picture
are very identical in expression, so there is a
hierarchy between the word sequences in the
sentences (the expression) and the sense for the
same picture.
FST is designed to describe a structure mapping
two different types of information sequences. It is
very useful in expressing the sequences and the hi-
erarchy in picture composition. Therefore, we build
a FST-based model to extract features related to the
transcription assessment in this paper. As the FST-
based model is similar to the BOW metrics, it is also
an error insensitive model. In this way, the impact of
the first problem could be reduced. The FST model
is very powerful in delivering the sequence informa-
tion that a meaningless sequence of words related to
the topic content will get low score under the mod-
el. Therefore, it works well concerning the second
problem. In a word, the FST model can not only be
insensitive to the recognition error in the ASR sys-
tem, but also remedy the weakness of BOW methods
in ASR result scoring.
In the remainder of the paper, the related work of
conventional AES methods is addressed in section 2.
The details of the speech corpus and the examination
grading criterion are introduced in section 3. The
FST model and its improved method are proposed
in section 4. The experiments and the results are
presented in section 5. The final section presents the
conclusion and future work.
2 Related Work
Conventional AES systems usually exploit textual
features to assess the quality of writing mainly in
three different facets: the content facet, the arrange-
ment facet and the language usage facet. In the con-
tent facet, many existing BOW techniques have been
applied, such as the content vector analysis (Attal-
i and Burstein, 2006; Attali, 2011) and the LSA to
reduce the dimension of content vector (Landauer et
al., 2003; Ishioka and Kameda, 2004; Kakkonen et
al., 2005; Peng et al, 2010). In arrangement facet,
Burstein et al (2010) modeled the coherence in s-
tudent essays, while Persing et al (2010) modeled
the organization. In language usage facet, grammar,
spelling and punctuation are common features in as-
sessment of the writing competence (Landauer et al,
2003; Attali and Burstein, 2006), and so does the di-
versity of words and clauses (Lonsdale and Strong-
Krause, 2003; Ishioka and Kameda, 2004). Besides
51
Grading levels Content Integrity Acoustic
(18-20)
passed
Describe the information in the four pictures with proper elaboration Perfect
(15-17) Describe all the information in all of the four pictures Good
(12-14) Describe most of the information in all of the four pictures Allow errors
(9-11)
failed
Describe most of the information in the pictures, but lose about 1 or 2 pictures
(6-8) Describe some of the information in the pictures, but lose about 2 or 3 pictures
(3-5) Describe little information in the four pictures
(0-2) Describe some words related to the four pictures
Table 1: Criterion of Grading
the textual features, many methods are also proposed
to evaluate the quality. The cosine similarity is one
of the most common used similarity measures (Lan-
dauer et al, 2003; Ishioka and Kameda, 2004; Attali
and Burstein, 2006; Attali, 2011). Also, the regres-
sion or the classification method is a good choice for
scoring (Rudner and Liang, 2002; Peng et al, 2010).
The rank preference techniques show excellent per-
formance in grading essays (Yannakoudakis et al,
2011). Chen et al (2010) proposed an unsupervised
approach to AES.
As our work concerns more about the content in-
tegrity, we applied the LSA-SVR approach (Peng et
al., 2010) as the contrast experiment, which is very
effective and robust. In the LSA-SVR method, each
essay transcription is represented by a latent seman-
tic space vector, which is regarded as the features in
the SVR model. The LSA (Deerwester et al, 1990)
considers the relations between the dimensions in
conventional vector space model (VSM) (Salton et
al., 1975), and it can order the importance of each di-
mension in the Latent Semantic Space (LSS). There-
fore, it is useful in reducing the dimensions of the
vector by truncate the high dimensions. The sup-
port vector machine can be performed for the func-
tion estimation (Smola and Scho?lkopf, 2004). The
LSA-SVR method takes the LSS vector as the fea-
ture vector, and applies the SVR for the training da-
ta to obtain the SVR model. Each test transcription
represented by the LSS vector can be scored by the
model.
3 Data
As characteristics of the data determine the effec-
tiveness of our methods, the details of it will be in-
troduced first. Our experimental data is acquired in
an oral English examination for ESL students. Three
score > 0 > 12 > 15 > 18
WER(%) 58.86 50.58 45.56 36.36
MR(%) 72.88 74.03 75.70 78.45
Table 2: WER and MR of ASR result
classes of students participated in the exam and 417
valid speeches are obtained in the examination. As
the paper mainly focuses on scoring the text tran-
scriptions, we have two ways to obtain them. One
is manually typing the text transcriptions which we
regarded as the Correct Recognition Result (CRR)
transcription, and another is the ASR result which
we named ASR transcription. We use the HTK (Y-
oung et al, 2006), which stands for the state of art
in speech recognition, to build the ASR system.
To better reveal the differences of the methods?
performance, all the experiments will be done in
both transcriptions. A better understanding of the
difference in the CRR transcription and the ASR
transcription from the low score to the high score
is shown in Table 2, where WER is the word error
rate and MR is the match rate which is the words?
correct rate.
3.1 Criterion of Grading
According to the Grading Criterion of the exami-
nation, the score of the examination ranges from 0
to 20, and the grading score is divided into 7 levels
with 3 points? interval for each level. The criterion
mainly concerns about two facets of the speech: the
acoustic level and the content integrity. The details
of the criterion are shown in Table 1. The criterion
indicates that the integrity is the most important part
in rating the speech. The acoustic level only work-
s well in excellent speeches (Huang et al, 2010).
Therefore, this paper mainly focuses on the integrity
52
Correlation R1 R2 R3 ES OC
R1 - 0.8966 0.8557 0.9620 0.9116
R2 - - 0.8461 0.9569 0.9048
R3 - - - 0.9441 0.8739
Average 0.8661 0.9543 0.8968
Table 3: Correlations of Human Scores
Figure 1: Distribution of Final Expert Scores
of content. The acoustic level as well as other levels
such as grammar errors is ignored. Because the cri-
terion is almost based on the content, our methods
obtain good performance although we ignore some
features.
3.2 Human Score Correlation and Distribution
Each speech in our experiments was scored by three
raters. Therefore, we have three scores for each
speech. The final expert score is the average of these
three scores. The correlations between human s-
cores are shown in Table 3.
R1, R2, and R3 stand for the three raters, and ES
is the final expert score. The Open Correlation (OC)
is the correlation between human rater scores and
the final scores, which are not related to the human
scores themselves (average of the other two scores).
As most students are supposed to pass the ex-
amination, the expert scores are mostly distributed
above 12 points, as shown in Figure 1. In the range
of the pass score, the distribution is close to normal
distribution, while in the range of failed score except
0, the distribution is close to uniform distribution.
4 Approach
The approach used in this paper is to build a standard
FST for the current examination topic. However,
the annotation of the corpus is necessary before the
Figure 2: Distribution of Sentence Labels
building. After the annotation and the building, the
features are extracted based on the FST. The auto-
mated machine score is computed from the features
at last. Therefore, subsection 4.1 will show the cor-
pus annotation, subsection 4.2 will introduce how to
build the standard FST of the current topic, and sub-
sections 4.3 and 4.4 will discuss how to extract the
features, at last, an improved method is proposed in
subsection 4.5.
4.1 Corpus Annotation
The definitions of the sequences and hierarchy in
the corpus will be given before we apply the FST
algorithm. According to the characteristics of the
picture composition examination, each composition
can be held as an orderly combination of the senses
of pictures. The senses of pictures are called sense-
groups here. We define a sense-group as one sen-
tence either describing the same one or two pictures
or elaborating on the same pictures. The descrip-
tion sentence is labeled with a tag ?m?(main sense of
the picture) and the elaboration one is labeled with
?s?(subordinate sense of the picture). The first giv-
en sentence in the examination is labeled with 0m
and the other describing sentences for the 1 to 4 pic-
tures are labeled with 1m to 4m, while the elabo-
ration ones for the 4 pictures are labeled with 1s to
4s. Therefore, each sentence in the composition is
labeled as a sense-group. For the entire 417 CRR
transcriptions, we manually labeled 274 transcrip-
tions whose scores are higher than 15 points. We
gained 8 types of labels from the manually labeled
results. They are 0m, 1m, 2m, 3m, 34m (one sen-
tence describes both of the third and the fourth pic-
tures), 4m, 2s and 4s. Other labels were discarded
for the number of their appearance is very low. The
distribution of sentences with each label is shown in
Figure 2. There are 1679 sentences in the 274 CRR
53
Figure 3: FST Building
transcriptions and 1667 are labeled in the eight sym-
bols.
4.2 FST Building
In this paper, we build three types of FST to extract
scoring features with the help of openFST tool (Al-
lauzen et al, 2007). The first is the sense-group F-
ST, the second is the words to each sense-group FST
and the last is the words to all the sense-groups FST.
They are shown in Figure 3.
The definition of the sense-group has been giv-
en in subsection 4.1. The sense-group FST can de-
scribe all the possible proper sense-group sequences
of the current picture composition topic. It is also
an acceptor trained from the labeled corpus. We use
manually labeled corpus, which are the sequences
of sense-groups of the CRR transcriptions with ex-
pert scores higher than 15 points, to build the sense-
group FST. In the process, each CRR transcription
sense-group sequence is a simple sense-group FST.
Later, we unite these sense-group FSTs to get the
final FST which considers every situation of sense-
group sequences in the train corpus. Also, we use
the operation of ?determinize? and ?minimize? in
openFST to optimize the final sense-group FST that
its states have no same input label and is a smallest
FST.
The second type is the words to sense-group F-
ST. It determines what word sequence input will re-
sult in what sense-group output. With the help of
these FSTs, we can find out how students use lan-
guage to describe a certain sense-group, or in other
words, a certain sense-group is usually constructed
with what kind of word sequence. All the differ-
ent sentences with their sense-group labels are tak-
en from the train corpus. We regard each sentence
as a simple words to sense-group FST, and then u-
nite these FSTs which have the same sense-group la-
bel. The final union FSTs can transform proper word
sequence into the right sense-group. Like building
the sense-group FST, the optimization operations of
?determinize? and ?minimize? are also done for the
FSTs.
The last type of FST is a words to sense-groups
FST. We can also treat it as a words FSA, because
any word sequence accepted by the words to sense-
groups FST is considered to be an integrated com-
position. Meanwhile, it can transform the word se-
quence into the sense-group label sequence which
is very useful in extracting the scoring features (de-
tails will be presented in subsection 4.4). The F-
ST is built from the other two types of FST that we
made before. We compute the composition of all the
words to each sense-group FSTs (the second type)
and the sense-group FST (the first type) with the op-
erations of ?compose? in openFST. Then, the com-
position result is the words to sense-groups FST, the
third type of FST in this paper.
4.3 Search for the Best Path in FST
Now we have successfully built the words to sense-
groups FST, the third type described above. Just like
the similarity methods mentioned in section 2 can
score essays from a have-been-scored similar essay,
we need to find the best path, which is closest to
the to-be-scored transcription, in the FST. Here, we
apply the edit distance to measure how best the path
is. This means the best path is the word sequence
path in the FST which has the smallest edit distance
compared with the to-be-scored transcription?s word
sequences .
Here, we modify the Wagner-Fischer algorithm
(Wagner and Fischer, 1974), which is a Dynamic
Programming (DP) algorithm, to quest the best path
in the FST. A simple example is illustrated in Figure
4. The best path can be described as
path = argmin
path?
allpath
EDcost(path, transcription) (1)
EDcost = ins+ del + sub (2)
EDcost is the edit distance from the transcription to
the paths which start at state 0 and end at the end
54
Figure 4: Search the Best Path in the FST by DP
state. The DP process can be described by equation
(3):
minEDcost(i) = argmin
j?
X1,...,Xp?1
(minEDcost(j) + cost(j, i))
(3)
The minEDcost(j) is the accumulated minimum ed-
it distance from state 0 to state j, and the cost(i,j) is
the cost of insertion, deletion or substitution from s-
tate j to state i. The equation means the minED of
state i can be computed by the accumulated minED-
cost of state j in the phase p. The state j belongs to
the have-been-calculated state set {X0,. . . ,Xp?1} in
phase p. In phrase p, we compute the best path and
its edit distance from the transcription for all the to-
be-calculated states which is the Xp shown in Fig-
ure 4. After computing all the phrases, the best path
and its edit distances of the end states are obtained.
Then the final best path is the one with the smallest
edit distance.
4.4 Feature Extraction
After building the FST and finding the best path
for the to-be-scored transcription, we can extrac-
t some effective features from the path information
and the transcription. Inspired by the similarity s-
coring measures, our proposed features represent the
similarity between the best path?s word sequence
and the to-be-scored transcription.
The features used for the scoring model are as fol-
lows:
? The Edit Distance (ED):
The edit distance is the linear combination of
the weights of insertion, deletion and substi-
tution. The relation is shown in equation (2),
where ins, del and sub are the appearance times
of insertions, deletions and substitutions, re-
spectively. Normally, we set the cost of each
to be 1.
? The Normalized Edit Distance(NED):
The NED is the ED normalized with the tran-
scription?s length.
NEDcost = EDcost/length (4)
? The Match Number(MN):
The match number is the number of words
matched between the best path and the tran-
scription.
? The Match Rate(MR):
The match rate is the match number normalized
with the transcription?s length.
MR = MN/length (5)
? The Continuous Match Value(CMV):
Continuous match should be better than the
fragmentary match, so a higher value is given
for the continuous situation.
CMV =
?
OM + 2
?
SM + 3
?
LM (6)
where OM (One Match) is the fragmentary
match number, SM (Short Match) is the con-
tinuous match number which is no more than 4,
and LM (Long Match) is the continuous match
number which is more than 4.
? The Length(L):
The length of transcription. Length is always
a very effective feature in essay scoring (Attali
and Burstein, 2006).
? The Sense-group Scoring Feature(SSF):
For each best path, we can transform the tran-
scription?s word sequence into the sense-group
label sequence with the FST. Then, the words
match rate of each sense-group can be comput-
ed. The match rate of each sense-group can be
regarded as one feature so that all the sense-
group match rate in the transcription will be
combined to a feature vector (called the Sense-
group Match Rate vector (SMRv)), which is
an 8-dimensional vector in the present experi-
ments. After that, we applied the SVR algorith-
m to train a sense-group scoring model with the
vectors and scores, and the transcription gets its
SSF from the model.
55
4.5 Extend the FST model with the similarity
of synonym
Because the FST is trained from the limited corpus,
it does not contain all the possible situations prop-
er for the current composition topic. To complete
the current FST model, we add the similarity of syn-
onym to extend the FST model so that it can handle
more situations.
The extension of the FST model is mainly reflect-
ed in calculation of the edit distance of the best path.
The previous edit distance, in equation (2), refers
to the Levenshtein distance in which the insertion-
s, deletions and substitutions have equal cost, but in
the edit distance in this section, the cost of substi-
tutions is less than that of insertions and deletion-
s. Here, we assume that the cost of substitutions is
based on the similarity of the two words. Then with
the help of different cost of substitutions, each word
edge is extended to some of its synonym word edges
under the cost of similarity. The new edit distance is
calculated by equation (7) as follows:
EDcost = ins+ del + sub? (1? sim) (7)
where, sim is the similarity of two words.
We used the Wordnet::Similarity software pack-
age (Pedersen et al, 2004) to calculate the similarity
between every two words at first. However, the per-
formance?s reduction of the AES system indicates
that the similarity is not good enough to extend the
FST model. Therefore, we seek for human help
to accurate the similarity calculation. We manual-
ly checked the similarity, and deleted some improp-
er similarity. Thus the final similarity applied in our
experiment is the Wordnet::Similarity software com-
puting result after the manual check.
5 Experiments
In this section, the proposed features and our FST
methods will be evaluated on the corpus we men-
tioned above. The contrasting approach, the LSA-
SVR approach, will also be presented.
5.1 Data Setup
The experiment corpus consists of 417 speeches.
With the help of manual typing and the ASR system,
417 CRR transcriptions and 417 ASR transcriptions
are obtained from the speeches after preprocessing
FST SVR SVR CRR ASR
build train test transcription transcription
Set2 Set3
Set1
0.7999 0.7505
Set3 Set2 0.8185 0.7401
Set1 Set3
Set2
0.8557 0.7372
Set3 Set1 0.8111 0.7257
Set1 Set2
Set3
0.9085 0.8086
Set2 Set1 0.8860 0.8086
Table 4: Correlation Between the SSF and the Expert S-
cores
which includes the capitalization processing and the
stemming processing. We divide them into 3 sets
by the same distribution of their scores. Therefore,
there are totally 6 sets, and each of them has 139 of
the transcriptions. The FST building only uses the
CRR transcriptions whose expert scores are higher
than 15 points. While treating one set (one CRR set)
as the FST building train set, we get the ED, NED,
MN, MR, CMV features and the SMR vectors for
the other two sets(could be either CRR sets or ASR
sets). Then, the SSF is obtained by another set as
the SVR train set and the last set as the test set. The
parameters of the SVR are trained through the grid
search from the whole data sets (ASR or CRR set-
s) by cross-validation. Therefore, except the length
feature, the other six features of each set can be ex-
tracted from the FST model.
Also, we presented the result of using LSA-SVR
approach as a contrast experiment to show the im-
provement of our FST model in scoring oral English
picture composition.
To quantitatively assess the effectiveness of the
methods, the Pearson correlation between the expert
scores and the automated results is adopted as the
performance measure.
5.2 Correlation of Features
The correlations between the seven features and the
final expert scores are shown in Tables 4 and 5 on
the three sets.
The MN and CMV are very good features, while
the NED is not. This is mainly due to the nature of
the examination. When scoring the speech, human
raters concern more about how much valid informa-
tion it contains and irrelevant contents are not taken
for penalty. Therefore, the match features are more
reasonable than the edit distance features. This im-
56
Script Train Test L ED NED MN MR CMV
CRR
Set2
Set1 0.7404
0.2410 -0.6690 0.8136 0.1544 0.7417
Set3 0.3900 -0.4379 0.8316 0.1386 0.7792
Set1
Set2 0.7819
0.4029 -0.7667 0.8205 0.4904 0.7333
Set3 0.4299 -0.5672 0.8370 0.5090 0.7872
Set1
Set3 0.8645
0.4983 -0.7634 0.8867 0.2718 0.8162
Set2 0.3639 -0.6616 0.8857 0.3305 0.8035
Average 0.7956 0.3877 -0.6443 0.8459 0.3158 0.7769
ASR
Set2
Set1 0.1341
-0.2281 -0.6375 0.7306 0.6497 0.7012
Set3 -0.1633 -0.5110 0.7240 0.6071 0.6856
Set1
Set2 0.2624
-0.0075 -0.4640 0.6717 0.5929 0.6255
Set3 0.0294 -0.4389 0.6860 0.6259 0.6255
Set1
Set3 0.1643
-0.1871 -0.5391 0.7419 0.6213 0.7001
Set2 -0.1742 -0.4721 0.7714 0.6199 0.7329
Average 0.1869 -0.1218 -0.5104 0.7209 0.6195 0.6785
Table 5: Correlations Between the Six Features and the Expert Scores
Script Method Set1 Set2 Set3 Average
CRR
Length 0.7404 0.7819 0.8645 0.7956
LSA-SVR 0.7476 0.8024 0.8663 0.8054
FST 0.8702 0.8852 0.9386 0.8980
ASR
Length 0.1341 0.2624 0.1643 0.1869
LSA-SVR 0.5975 0.5643 0.5907 0.5842
FST 0.7992 0.7678 0.8452 0.8041
Table 6: Performance of the FST Method, the LSA-SVR
Approach and the Length Feature
pact is similar to the result displayed by the ASR
output performance in Table 2 in section 3, where
the WER has significant difference from the low s-
core speeches to the high score ones while the MR
does not, and the MR is much better than the WER.
As the length feature is a strong correlation fea-
ture in CRR transcription, the MR feature, which is
normalized by the length, is strongly affected. How-
ever, with the impact declining in the ASR transcrip-
tion, the MR feature performs very well. This also
explains the reason of different correlations of ED
and NED in CRR transcription.
The SSF is entirely based on the FST model, so
the impact of the length feature is very low. The
decline of it in different transcriptions is mainly be-
cause of the ASR error.
5.3 Performance of the FST Model
For each test transcription, it has 12 dimensions of F-
ST features. The ED, NED, MN, MR and CMV fea-
tures have two dimensions of each as trained from
two different FST building sets. The SSF needs t-
wo train sets as there are two train models: one is
for the FST building model and another is for the
SVR model. As different sets for different models,
it also has two dimension features. We use the linear
regression to combine these 12 features to the final
automated score. The linear regression parameter-
s were trained from all the data by cross-validation.
After the weight of each feature and the linear bias
are gained, we calculate the automated score of each
transcription by the FST features. The performance
of our FST model is shown in Table 6. Compared
with it, the performance of the LSA-SVR algorithm,
the baseline in our paper, is also shown. As a usual
best feature for AES, the length shows its outstand-
ing performance in CRR transcription. However, it
fails in the ASR transcription.
As we have predicted above, the BOW algorith-
m (the LSA-SVR) performance declines drastically
in the ASR transcription, which also happens to the
length feature. By contrast, the decline of the per-
formance of our FST method is acceptable consid-
ering the impact of recognition errors in the ASR
system. This means the FST model is an error in-
sensitive model that is very appropriate for the task.
5.4 Improvement of FST by Adding the
Similarity
The improved FST extends the original FST model
by considering the word similarity in substitution-
s. In the extension, the similarities of the synonyms
57
Script Method Set1 Set2 Set3 Average
CRR
FST 0.8702 0.8852 0.9386 0.8980
IFST 0.8788 0.8983 0.9418 0.9063
ASR
FST 0.7992 0.7678 0.8452 0.8041
IFST 0.8351 0.7617 0.8168 0.8045
Table 7: Performance of the FST Method and the Im-
proved FST Method
describe the invisible (extended) part of the FST, so
it should be very accurate for the substitutions cost.
Therefore, we added manual intervention to the sim-
ilarity result calculated by the wordnet::similarity
software packet.
After we added the similarity of synonym to ex-
tend the FST model, the performance of the new
model increased stably in the CRR transcription.
However, the increase is not significant in the AS-
R transcription (shown in Table 7). We believe it is
because the superiority of the improved model is dis-
guised by the ASR error. In other words, the impact
of ASR error under the FST model is more signifi-
cant than the improvement of the FST model. The
performance correlation of our FST model in the
CRR transcription is about 0.9 which is very close to
the human raters? (shown in Table 3). Even though
the performance correlation in the ASR transcription
declines compared with that in the CRR transcrip-
tion, the FST methods still perform very well under
the current recognition errors of the ARS system.
6 Conclusion and Future work
The aforementioned experiments indicate three
points. First, the BOW algorithm has its own weak-
ness. In regular text essay scoring, the BOW algo-
rithm can have excellent performance. However, in
certain situations, such as towards ASR transcription
of oral English speech, its weakness of sequence ne-
glect will be magnified, leading to drastic decline of
performance. Second, the introduced FST model is
suitable in our task. It is an error insensitive mod-
el under the task of automated oral English picture
composition scoring. Also, it considers the sequence
and the hierarchy information. As we expected, the
performance of the FST model is more outstanding
than that of the BOW metrics in CRR transcription,
and the decline of performance is acceptable in AS-
R transcription scoring. Third, adding the similarity
of synonyms to extend the FST model improves the
system performance. The extension can complete
the FST model, and achieve better performance in
the CRR transcription.
The future work may focus on three facets. First,
as the extension of the FST model is a preliminary
study, there is much work that can be done, such
as calculating the similarity more accurately without
manual intervention, or finding a balance between
the original FST model and the extended one to im-
prove the performance in ASR transcription. Sec-
ond, as the task is speech evaluation, considering the
acoustic features may give more information to the
automated scoring system. Therefore, the features
at the acoustic level could be introduced to com-
plete the scoring model. Third, the decline of the
performance in ASR transcription is derived from
the recognition error of ASR system. Therefore, im-
proving the performance of the ASR system or mak-
ing full use of the N-best lists may give more accu-
rate transcription for the AES system.
Acknowledgments
This work was supported by the National Natural
Science Foundation of China (No. 90820303 and
No. 61103152). We thank the anonymous reviewers
for their insightful comments.
References
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Woj-
ciech Skut and Mehryar Mohri. 2007. OpenFst: a
general and efficient weighted finite-state transducer
library. In Proceedings of International Conference on
Implementation and Application of Automata, 4783:
11-23.
Yigal Attali. 2011. A differential word use measure for
content analysis in automated essay scoring. ETS re-
search report, ETS RR-11-36.
Yigal Attali and Jill Burstein. 2006. Automated essay
scoring with e-rater R?V.2. The Journal of Technology,
Learning, and Assessment, 4(3), 1-34.
Jill Burstein, Joel Tetreault, and Slava Andreyev. 2010.
Using entity-based features to model coherence in stu-
dent essays. In Human Language Technologies: The
Annual Conference of the North American Chapter of
the ACL, 681-684.
Chih-Chung Chang, Chih-Jen Lin. 2011. LIBSVM: a li-
brary for support vector machines. ACM Transactions
on Intelligent Systems and Technology, Vol. 2.
58
Yen-Yu Chen, Chien-Liang Liu, Chia-Hoang Lee, and
Tao-Hsing Chang. 2010. An unsupervised automated
essay scoring system. IEEE Intelligent Systems, 61-
67.
Catia Cucchiarini, Helmer Strik, and Lou Boves. 2000.
Quantitative assessment of second language learner-
s? fluency by means of automatic speech recognition
technology. Acoustical Society of America, 107(2):
989-999.
Scott Deerwester, Susan T. Dumais, George W. Furnas,
Thomas K. Landauer and Richard Harshman. 1990.
Indexing by latent semantic analysis. Journal of the
American Society for Information Science. 41(6): 391-
407.
Shen Huang, Hongyan Li, Shijin Wang, Jiaen Liang and
Bo Xu. 2010. Automatic reference independent e-
valuation of prosody quality using multiple knowledge
fusions. In INTERSPEECH, 610-613.
Tsunenori Ishioka and Masayuki Kameda. 2004. Auto-
mated Japanese essay scoring system: jess. In Pro-
ceedings of the International Workshop on database
and Expert Systems applications.
Tuomo Kakkonen, Niko Myller, Jari Timonen, and Erkki
Sutinen. 2005. Automatic essay grading with prob-
abilistic latent semantic analysis. In Proceedings of
Workshop on Building Educational Applications Us-
ing NLP, 29-36.
Thomas K. Landauer, Darrell Laham and Peter Foltz.
2003. Automatic essay assessment. Assessment in E-
ducation: Principles, Policy and Practice (10:3), 295-
309.
Deryle Lonsdale and Diane Strong-Krause. 2003. Au-
tomated rating of ESL essays. In Proceedings of the
HLT-NAACL Workshop: Building Educational Appli-
cations Using NLP.
Andreas Maier, F. Ho?nig, V. Zeissler, Anton Batliner,
E. Ko?rner, N. Yamanaka, P. Ackermann, Elmar No?th
2009. A language-independent feature set for the auto-
matic evaluation of prosody. In INTERSPEECH, 600-
603.
Mehryar Mohri. 2004. Weighted finite-state transducer
algorithms: an overview. Formal Languages and Ap-
plications, 148 (620): 551-564.
Leonardo Neumeyer, Horacio Franco, Vassilios Di-
galakis, Mitchel Weintraub. 2000. Automatic scor-
ing of pronunciation quality. Speech Communication,
30(2-3): 83-94.
Ted Pedersen, Siddharth Patwardhan and Jason Miche-
lizzi. 2004. WordNet::Similarity - measuring the re-
latedness of concepts. In Proceedings of the National
Conference on Artificial Intelligence, 144-152.
Xingyuan Peng, Dengfeng Ke, Zhenbiao Chen and Bo
Xu. 2010. Automated Chinese essay scoring using
vector space models. In Proceedings of IUCS, 149-
153.
Isaac Persing, Alan Davis and Vincent Ng. 2010. Mod-
eling organization in student essays. In Proceedings of
EMNLP, 229-239.
Lawrence M. Rudner and Tahung Liang. 2002. Auto-
mated essay scoring using Bayes? theorem. The Jour-
nal of Technology, Learning and Assessment, 1(2):3-
21.
G. Salton, C. Yang, A. Wong. 1975. A vector space
model for automatic indexing. Communications of the
ACM, 18(11): 613-620.
Alex J. Smola and Bernhard Scho?lkopf. 2004. A tutorial
on support vector regression. Statistics and Comput-
ing 14(3): 199-222.
Robert A. Wagner and Michael J. Fischer. 1974. The
string-to-string correction problem. Journal of the
ACM, 21(1):168-173.
Helen Yannakoudakis, Ted Briscoe and Ben Medlock.
2011. A new dataset and method for automatically
grading ESOL texts. In Proceedings of ACL, 180-189.
Steve Young, Gunnar Evermann, Mark Gales, Thomas
Hain, Dan Kershaw, Xunying Liu, Gareth Moore,
Julian Odell, Dave Ollason, Dan Povey, Valtcho
Valtchev, Phil Woodland. 2006. The HTK book (for
HTK version 3.4). Cambridge University Engineering
Department.
59
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 122?132,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Learning New Semi-Supervised Deep Auto-encoder Features
for Statistical Machine Translation
Shixiang Lu, Zhenbiao Chen, Bo Xu
Interactive Digital Media Technology Research Center (IDMTech)
Institute of Automation, Chinese Academy of Sciences, Beijing, China
{shixiang.lu,zhenbiao.chen,xubo}@ia.ac.cn
Abstract
In this paper, instead of designing new fea-
tures based on intuition, linguistic knowl-
edge and domain, we learn some new
and effective features using the deep auto-
encoder (DAE) paradigm for phrase-based
translation model. Using the unsupervised
pre-trained deep belief net (DBN) to ini-
tialize DAE?s parameters and using the in-
put original phrase features as a teacher for
semi-supervised fine-tuning, we learn new
semi-supervised DAE features, which are
more effective and stable than the unsuper-
vised DBN features. Moreover, to learn
high dimensional feature representation,
we introduce a natural horizontal compo-
sition of more DAEs for large hidden lay-
ers feature learning. On two Chinese-
English tasks, our semi-supervised DAE
features obtain statistically significant im-
provements of 1.34/2.45 (IWSLT) and
0.82/1.52 (NIST) BLEU points over the
unsupervised DBN features and the base-
line features, respectively.
1 Introduction
Recently, many new features have been explored
for SMT and significant performance have been
obtained in terms of translation quality, such as
syntactic features, sparse features, and reordering
features. However, most of these features are man-
ually designed on linguistic phenomena that are
related to bilingual language pairs, thus they are
very difficult to devise and estimate.
Instead of designing new features based on in-
tuition, linguistic knowledge and domain, for the
first time, Maskey and Zhou (2012) explored the
possibility of inducing new features in an unsuper-
vised fashion using deep belief net (DBN) (Hinton
et al, 2006) for hierarchical phrase-based trans-
lation model. Using the 4 original phrase fea-
tures in the phrase table as the input features, they
pre-trained the DBN by contrastive divergence
(Hinton, 2002), and generated new unsupervised
DBN features using forward computation. These
new features are appended as extra features to the
phrase table for the translation decoder.
However, the above approach has two major
shortcomings. First, the input original features
for the DBN feature learning are too simple, the
limited 4 phrase features of each phrase pair,
such as bidirectional phrase translation probabil-
ity and bidirectional lexical weighting (Koehn et
al., 2003), which are a bottleneck for learning ef-
fective feature representation. Second, it only uses
the unsupervised layer-wise pre-training of DBN
built with stacked sets of Restricted Boltzmann
Machines (RBM) (Hinton, 2002), does not have a
training objective, so its performance relies on the
empirical parameters. Thus, this approach is un-
stable and the improvement is limited. In this pa-
per, we strive to effectively address the above two
shortcomings, and systematically explore the pos-
sibility of learning new features using deep (multi-
layer) neural networks (DNN, which is usually re-
ferred under the name Deep Learning) for SMT.
To address the first shortcoming, we adapt and
extend some simple but effective phrase features
as the input features for new DNN feature learn-
ing, and these features have been shown sig-
nificant improvement for SMT, such as, phrase
pair similarity (Zhao et al, 2004), phrase fre-
quency, phrase length (Hopkins and May, 2011),
and phrase generative probability (Foster et al,
2010), which also show further improvement for
new phrase feature learning in our experiments.
To address the second shortcoming, inspired
by the successful use of DAEs for handwrit-
ten digits recognition (Hinton and Salakhutdinov,
2006; Hinton et al, 2006), information retrieval
(Salakhutdinov and Hinton, 2009; Mirowski et
122
al., 2010), and speech spectrograms (Deng et al,
2010), we propose new feature learning using
semi-supervised DAE for phrase-based translation
model. By using the input data as the teacher, the
?semi-supervised? fine-tuning process of DAE ad-
dresses the problem of ?back-propagation without
a teacher? (Rumelhart et al, 1986), which makes
the DAE learn more powerful and abstract features
(Hinton and Salakhutdinov, 2006). For our semi-
supervised DAE feature learning task, we use the
unsupervised pre-trained DBN to initialize DAE?s
parameters and use the input original phrase fea-
tures as the ?teacher? for semi-supervised back-
propagation. Compared with the unsupervised
DBN features, our semi-supervised DAE features
are more effective and stable.
Moreover, to learn high dimensional feature
representation, we introduce a natural horizontal
composition for DAEs (HCDAE) that can be used
to create large hidden layer representations simply
by horizontally combining two (or more) DAEs
(Baldi, 2012), which shows further improvement
compared with single DAE in our experiments.
It is encouraging that, non-parametric feature
expansion using gaussian mixture model (GMM)
(Nguyen et al, 2007), which guarantees invari-
ance to the specific embodiment of the original
features, has been proved as a feasible feature gen-
eration approach for SMT. Deep models such as
DNN have the potential to be much more represen-
tationally efficient for feature learning than shal-
low models like GMM. Thus, instead of GMM,
we use DNN (DBN, DAE and HCDAE) to learn
new non-parametric features, which has the sim-
ilar evolution in speech recognition (Dahl et al,
2012; Hinton et al, 2012). DNN features are
learned from the non-linear combination of the
input original features, they strong capture high-
order correlations between the activities of the
original features, and we believe this deep learn-
ing paradigm induces the original features to fur-
ther reach their potential for SMT.
Finally, we conduct large-scale experiments
on IWSLT and NIST Chinese-English translation
tasks, respectively, and the results demonstrate
that our solutions solve the two aforementioned
shortcomings successfully. Our semi-supervised
DAE features significantly outperform the unsu-
pervised DBN features and the baseline features,
and our introduced input phrase features signifi-
cantly improve the performance of DAE feature
learning.
The remainder of this paper is organized as fol-
lows. Section 2 briefly summarizes the recent re-
lated work about the applications of DNN for SMT
tasks. Section 3 presents our introduced input fea-
tures for DNN feature learning. Section 4 de-
scribes how to learn our semi-supervised DAE fea-
tures for SMT. Section 5 describes and discusses
the large-scale experimental results. Finally, we
end with conclusions in section 6.
2 Related Work
Recently, there has been growing interest in use of
DNN for SMT tasks. Le et al (2012) improved
translation quality of n-gram translation model
by using a bilingual neural LM, where transla-
tion probabilities are estimated using a continu-
ous representation of translation units in lieu of
standard discrete representations. Kalchbrenner
and Blunsom (2013) introduced recurrent contin-
uous translation models that comprise a class for
purely continuous sentence-level translation mod-
els. Auli et al (2013) presented a joint lan-
guage and translation model based on a recur-
rent neural network which predicts target words
based on an unbounded history of both source
and target words. Liu et al (2013) went be-
yond the log-linear model for SMT and proposed
a novel additive neural networks based translation
model, which overcome some of the shortcom-
ings suffered by the log-linear model: linearity
and the lack of deep interpretation and represen-
tation in features. Li et al (2013) presented an
ITG reordering classifier based on recursive auto-
encoders, and generated vector space representa-
tions for variable-sized phrases, which enable pre-
dicting orders to exploit syntactic and semantic
information. Lu et al (2014) adapted and ex-
tended the max-margin based RNN (Socher et al,
2011) into HPB translation with force decoding
and converting tree, and proposed a RNN based
word topology model for HPB translation, which
successfully capture the topological structure of
the words on the source side in a syntactically and
semantically meaningful order.
However, none of these above works have fo-
cused on learning new features automatically with
input data, and while learning suitable features
(representations) is the superiority of DNN since
it has been proposed. In this paper, we systemat-
ically explore the possibility of learning new fea-
123
tures using DNN for SMT.
3 Input Features for DNN Feature
Learning
The phrase-based translation model (Koehn et al,
2003; Och and Ney, 2004) has demonstrated supe-
rior performance and been widely used in current
SMT systems, and we employ our implementation
on this translation model. Next, we adapt and ex-
tend some original phrase features as the input fea-
tures for DAE feature learning.
3.1 Baseline phrase features
We assume that source phrase f = f
1
, ? ? ? , f
l
f
and target phrase e = e
1
, ? ? ? , e
l
e
include l
f
and
l
e
words, respectively. Following (Maskey and
Zhou, 2012), we use the following 4 phrase fea-
tures of each phrase pair (Koehn et al, 2003)
in the phrase table as the first type of input fea-
tures, bidirectional phrase translation probability
(P (e|f) and P (f |e)), bidirectional lexical weight-
ing (Lex(e|f) and Lex(f |e)),
X
1
? P (f |e), Lex(f |e), P (e|f), Lex(e|f)
3.2 Phrase pair similarity
Zhao et al (2004) proposed a way of using term
weight based models in a vector space as addi-
tional evidences for phrase pair translation quality.
This model employ phrase pair similarity to en-
code the weights of content and non-content words
in phrase translation pairs. Following (Zhao et al,
2004), we calculate bidirectional phrase pair simi-
larity using cosine distance and BM25 distance as,
S
cos
i
(e, f) =
?
l
e
j=1
?
l
f
i=1
w
e
j
p(e
j
|f
i
)w
f
i
sqrt(
?
l
e
j=1
w
2
e
j
)sqrt(
?
l
e
j=1
w
e
j
a
2
)
S
cos
d
(f, e) =
?
l
f
i=1
?
l
e
j=1
w
f
i
p(f
i
|e
j
)w
e
j
sqrt(
?
l
f
i=1
w
2
f
i
)sqrt(
?
l
f
i=1
w
f
i
a
2
)
where, p(e
j
|f
i
) and p(f
i
|e
j
) represents bidirec-
tional word translation probability. w
f
i
and w
e
j
are term weights for source and target words, w
e
j
a
and w
f
i
a
are the transformed weights mapped from
all source/target words to the target/source dimen-
sion at word e
j
and f
i
, respectively.
S
bm25
i
(e, f) =
l
f
?
i=1
idf
f
i
(k
1
+ 1)w
f
i
(k
3
+ 1)w
f
i
a
(K + w
f
i
)(k
3
+ w
f
i
a
)
S
bm25
d
(f, e) =
l
e
?
j=1
idf
e
j
(k
1
+ 1)w
e
j
(k
3
+ 1)w
e
j
a
(K + w
e
j
)(k
3
+ w
e
j
a
)
where, k
1
, b, k
3
are set to be 1, 1 and 1000, re-
spectively. K = k
1
((1? b) + J/avg(l)), and J is
the phrase length (l
e
or l
f
), avg(l) is the average
phrase length. Thus, we have the second type of
input features
X
2
? S
cos
i
(f, e), S
bm25
i
(f, e), S
cos
d
(e, f), S
bm25
d
(e, f)
3.3 Phrase generative probability
We adapt and extend bidirectional phrase genera-
tive probabilities as the input features, which have
been used for domain adaptation (Foster et al,
2010). According to the background LMs, we esti-
mate the bidirectional (source/target side) forward
and backward phrase generative probabilities as
P
f
(f) = P (f
1
)P (f
2
|f
1
) ? ? ?P (f
l
f
|f
l
f
?n+1
, ? ? ? , f
l
f
?1
)
P
f
(e) = P (e
1
)P (e
2
|e
1
) ? ? ?P (e
l
e
|e
l
e
?n+1
, ? ? ? , e
l
e
?1
)
P
b
(f) = P (f
l
f
)P (f
l
f
?1
|f
l
f
) ? ? ?P (f
1
|f
n
, ? ? ? , f
2
)
P
b
(e) = P (e
l
e
)P (e
l
e
?1
|e
l
e
) ? ? ?P (e
1
|e
n
, ? ? ? , e
2
)
where, the bidirectional forward and backward
1
background 4-gram LMs are trained by the corre-
sponding side of bilingual corpus
2
. Then, we have
the third type of input features
X
3
? P
f
(e), P
b
(e), P
f
(f), P
b
(f)
3.4 Phrase frequency
We consider bidirectional phrase frequency as the
input features, and estimate them as
P (f) =
count(f)
?
|f
i
|=|f |
count(f
i
)
P (e) =
count(e)
?
|e
j
|=|e|
count(e
j
)
where, the count(f)/count(e) are the total num-
ber of phrase f/e appearing in the source/target side
of the bilingual corpus, and the denominator are
the total number of the phrases whose length are
equal to |f |/|e|, respectively. Then, we have the
forth type of input features
X
4
? P (f), P (e)
1
Backward LM has been introduced by Xiong et al
(2011), which successfully capture both the preceding and
succeeding contexts of the current word, and we estimate the
backward LM by inverting the order in each sentence in the
training data from the original order to the reverse order.
2
This corpus is used to train the translation model in our
experiments, and we will describe it in detail in section 5.1.
124
3.5 Phrase length
Phrase length plays an important role in the trans-
lation process (Koehn, 2010; Hopkins and May,
2011). We normalize bidirectional phrase length
by the maximum phrase length, and introduce
them as the last type of input features
X
5
? l
n
e
, l
n
f
In summary, except for the first type of phrase
feature X
1
which is used by (Maskey and Zhou,
2012), we introduce another four types of effec-
tive phrase features X
2
, X
3
, X
4
and X
5
. Now, the
input original phrase features X includes 16 fea-
tures in our experiments, as follows,
X ? X
1
, X
2
, X
3
, X
4
, X
5
We build the DAE network where the first layer
with visible nodes equaling to 16, and each visible
node v
i
corresponds to the above original features
X in each phrase pair.
4 Semi-Supervised Deep Auto-encoder
Features Learning for SMT
Each translation rule in the phrase-based transla-
tion model has a set number of features that are
combined in the log-linear model (Och and Ney,
2002), and our semi-supervised DAE features can
also be combined in this model. In this section,
we design our DAE network with various network
structures for new feature learning.
4.1 Learning a Deep Belief Net
Inspired by (Maskey and Zhou, 2012), we first
learn a deep generative model for feature learning
using DBN. DBN is composed of multiple layers
of latent variables with the first layer represent-
ing the visible feature vectors, which is built with
stacked sets of RBMs (Hinton, 2002).
For a RBM, there is full connectivity between
layers, but no connections within either layer. The
connection weight W , hidden layer biases c and
visible layer biases b can be learned efficiently
using the contrastive divergence (Hinton, 2002;
Carreira-Perpinan and Hinton, 2005). When given
a hidden layer h, factorial conditional distribution
of visible layer v can be estimated by
P (v = 1|h) = ?(b+ h
T
W
T
)
where ? denotes the logistic sigmoid. Given v, the
element-wise conditional distribution of h is
P (h = 1|v) = ?(c+ v
T
W )
Figure 1: Pre-training consists of learning a stack
of RBMs, and these RBMs create an unsupervised
DBN.
The two conditional distributions can be shown
to correspond to the generative model,
P (v, h) =
1
Z
exp(?E(v, h))
where,
Z =
?
v,h
e
?E(v,h)
E(v, h) = ?b
T
v ? c
T
h? v
T
Wh
After learning the first RBM, we treat the acti-
vation probabilities of its hidden units, when they
are being driven by data, as the data for training
a second RBM. Similarly, a n
th
RBM is built on
the output of the n ? 1
th
one and so on until a
sufficiently deep architecture is created. These n
RBMs can then be composed to form a DBN in
which it is easy to infer the states of the n
th
layer
of hidden units from the input in a single forward
pass (Hinton et al, 2006), as shown in Figure 1.
This greedy, layer-by-layer pre-training can be re-
peated several times to learn a deep, hierarchical
model (DBN) in which each layer of features cap-
tures strong high-order correlations between the
activities of features in the layer below.
To deal with real-valued input features X in our
task, we use an RBM with Gaussian visible units
(GRBM) (Dahl et al, 2012) with a variance of 1
on each dimension. Hence, P (v|h) and E(v, h) in
the first RBM of DBN need to be modified as
P (v|h) = N (v; b+ h
T
W
T
, I)
E(v, h) =
1
2
(v ? b)
T
(v ? b)? c
T
h? v
T
Wh
where I is the appropriate identity matrix.
125
Figure 2: After the unsupervised pre-training,
the DBNs are ?unrolled? to create a semi-
supervised DAE, which is then fine-tuned using
back-propagation of error derivatives.
To speed-up the pre-training, we subdivide the
entire phrase pairs (with features X) in the phrase
table into small mini-batches, each containing 100
cases, and update the weights after each mini-
batch. Each layer is greedily pre-trained for
50 epochs through the entire phrase pairs. The
weights are updated using a learning rate of 0.1,
momentum of 0.9, and a weight decay of 0.0002
? weight ? learning rate. The weight matrix W
are initialized with small random values sampled
from a zero-mean normal distribution with vari-
ance 0.01.
After the pre-training, for each phrase pair in
the phrase table, we generate the DBN features
(Maskey and Zhou, 2012) by passing the original
phrase featuresX through the DBN using forward
computation.
4.2 From DBN to Deep Auto-encoder
To learn a semi-supervised DAE, we first ?unroll?
the above n layer DBN by using its weight ma-
trices to create a deep, 2n-1 layer network whose
lower layers use the matrices to ?encode? the in-
put and whose upper layers use the matrices in
reverse order to ?decode? the input (Hinton and
Salakhutdinov, 2006; Salakhutdinov and Hinton,
2009; Deng et al, 2010), as shown in Figure 2.
The layer-wise learning of DBN as above must be
treated as a pre-training stage that finds a good
region of the parameter space, which is used to
initialize our DAE?s parameters. Starting in this
region, the DAE is then fine-tuned using average
squared error (between the output and input) back-
propagation to minimize reconstruction error, as to
make its output as equal as possible to its input.
For the fine-tuning of DAE, we use the method
of conjugate gradients on larger mini-batches of
1000 cases, with three line searches performed
for each mini-batch in each epoch. To determine
an adequate number of epochs and to avoid over-
fitting, we fine-tune on a fraction phrase table
and test performance on the remaining validation
phrase table, and then repeat fine-tuning on the en-
tire phrase table for 100 epochs.
We experiment with various values for the noise
variance and the threshold, as well as the learn-
ing rate, momentum, and weight-decay parame-
ters used in the pre-training, the batch size and
epochs in the fine-tuning. Our results are fairly ro-
bust to variations in these parameters. The precise
weights found by the pre-training do not matter
as long as it finds a good region of the parameter
space from which to start the fine-tuning.
The fine-tuning makes the feature representa-
tion in the central layer of the DAE work much
better (Salakhutdinov and Hinton, 2009). After
the fine-tuning, for each phrase pair in the phrase
table, we estimate our DAE features by passing the
original phrase features X through the ?encoder?
part of the DAE using forward computation.
To combine these learned features (DBN and
DAE feature) into the log-linear model, we need
to eliminate the impact of the non-linear learning
mechanism. Following (Maskey and Zhou, 2012),
these learned features are normalized by the av-
erage of each dimensional respective feature set.
Then, we append these features for each phrase
pair to the phrase table as extra features.
4.3 Horizontal Composition of Deep
Auto-encoders (HCDAE)
Although DAE can learn more powerful and ab-
stract feature representation, the learned features
usually have smaller dimensionality compared
with the dimensionality of the input features, such
as the successful use for handwritten digits recog-
nition (Hinton and Salakhutdinov, 2006; Hinton
et al, 2006), information retrieval (Salakhutdinov
and Hinton, 2009; Mirowski et al, 2010), and
126
Figure 3: Horizontal composition of DAEs to ex-
pand high-dimensional features learning.
speech spectrograms (Deng et al, 2010). More-
over, although we have introduced another four
types of phrase features (X
2
, X
3
, X
4
and X
5
), the
only 16 features in X are a bottleneck for learning
large hidden layers feature representation, because
it has limited information, the performance of the
high-dimensional DAE features which are directly
learned from single DAE is not very satisfactory.
To learn high-dimensional feature representa-
tion and to further improve the performance, we
introduce a natural horizontal composition for
DAEs that can be used to create large hidden layer
representations simply by horizontally combining
two (or more) DAEs (Baldi, 2012), as shown in
Figure 3. Two single DAEs with architectures
16/m
1
/16 and 16/m
2
/16 can be trained and the
hidden layers can be combined to yield an ex-
panded hidden feature representation of sizem
1
+
m
2
, which can then be fed to the subsequent lay-
ers of the overall architecture. Thus, these new
m
1
+m
2
-dimensional DAE features are added as
extra features to the phrase table.
Differences in m
1
- and m
2
-dimensional hidden
representations could be introduced by many dif-
ferent mechanisms (e.g., learning algorithms, ini-
tializations, training samples, learning rates, or
distortion measures) (Baldi, 2012). In our task,
we introduce differences by using different initial-
izations and different fractions of the phrase table.
4-16-8-2 4-16-8-4 4-16-16-8
4-16-8-4-2 4-16-16-8-4 4-16-16-8-8
4-16-16-8-4-2 4-16-16-8-8-4 4-16-16-16-8-8
4-16-16-8-8-4-2 4-16-16-16-8-8-4 4-16-16-16-16-8-8
6-16-8-2 6-16-8-4 6-16-16-8
6-16-8-4-2 6-16-16-8-4 6-16-16-8-8
6-16-16-8-4-2 6-16-16-8-8-4 6-16-16-16-8-8
6-16-16-16-8-4-2 6-16-16-16-8-8-4 6-16-16-16-16-8-8
8-16-8-2 8-16-8-4 8-16-16-8
8-16-8-4-2 8-16-16-8-4 8-16-16-8-8
8-16-16-8-4-2 8-16-16-8-8-4 8-16-16-16-8-8
8-16-16-16-8-4-2 8-16-16-16-8-8-4 8-16-16-16-16-8-8
16-32-16-2 16-32-16-4 16-32-16-8
16-32-16-8-2 16-32-16-8-4 16-32-32-16-8
16-32-16-8-4-2 16-32-32-16-8-4 16-32-32-16-16-8
16-32-32-16-8-4-2 16-32-32-16-16-8-4 16-32-32-32-16-16-8
Table 1: Details of the used network structure.
For example, the architecture 16-32-16-2 (4 lay-
ers? network depth) corresponds to the DAE with
16-dimensional input features (X) (input layer),
32/16 hidden units (first/second hidden layer), and
2-dimensional output features (new DAE features)
(output layer). During the fine-tuning, the DAE?s
network structure becomes 16-32-16-2-16-32-16.
Correspondingly, 4-16-8-2 and 6(8)-16-8-2 repre-
sent the input features are X
1
and X
1
+X
i
.
5 Experiments and Results
5.1 Experimental Setup
We now test our DAE features on the following
two Chinese-English translation tasks.
IWSLT. The bilingual corpus is the Chinese-
English part of Basic Traveling Expression cor-
pus (BTEC) and China-Japan-Korea (CJK) cor-
pus (0.38M sentence pairs with 3.5/3.8M Chi-
nese/English words). The LM corpus is the En-
glish side of the parallel data (BTEC, CJK and
CWMT08
3
) (1.34M sentences). Our development
set is IWSLT 2005 test set (506 sentences), and our
test set is IWSLT 2007 test set (489 sentences).
NIST. The bilingual corpus is LDC
4
(3.4M sen-
tence pairs with 64/70M Chinese/English words).
The LM corpus is the English side of the paral-
lel data as well as the English Gigaword corpus
(LDC2007T07) (11.3M sentences). Our develop-
ment set is NIST 2005 MT evaluation set (1084
sentences), and our test set is NIST 2006 MT eval-
uation set (1664 sentences).
We choose the Moses (Koehn et al, 2007)
framework to implement our phrase-based ma-
chine system. The 4-gram LMs are estimated
by the SRILM toolkit with modified Kneser-Ney
3
the 4th China Workshop on Machine Translation
4
LDC2002E18, LDC2002T01, LDC2003E07,
LDC2003E14, LDC2003T17, LDC2004T07, LDC2004T08,
LDC2005T06, LDC2005T10, LDC2005T34, LDC2006T04,
LDC2007T09
127
# Features
IWSLT NIST
Dev Test Dev Test
1 Baseline 50.81 41.13 36.12 32.59
2
X
1
+DBN X
1
2f 51.92 42.07
?
36.33 33.11
?
3 +DAE X
1
2f 52.49 43.22
??
36.92 33.44
??
4 +DBN X
1
4f 51.45 41.78
?
36.45 33.12
?
5 +DAE X
1
4f 52.45 43.06
??
36.88 33.47
??
6 +HCDAE X
1
2+2f 53.69 43.23
???
37.06 33.68
???
7 +DBN X
1
8f 51.74 41.85
?
36.61 33.24
?
8 +DAE X
1
8f 52.33 42.98
??
36.81 33.36
??
9 +HCDAE X
1
4+4f 52.52 43.26
???
37.01 33.63
???
10
X
+DBN X 2f 52.21 42.24
?
36.72 33.21
?
11 +DAE X 2f 52.86 43.45
??
37.39 33.83
??
12 +DBN X 4f 51.83 42.08
?
34.45 33.07
?
13 +DAE X 4f 52.81 43.47
??
37.48 33.92
??
14 +HCDAE X 2+2f 53.05 43.58
???
37.59 34.11
???
15 +DBN X 8f 51.93 42.01
?
36.74 33.29
?
16 +DAE X 8f 52.69 43.26
??
37.36 33.75
??
17 +HCDAE X 4+4f 52.93 43.49
???
37.53 34.02
???
18 +(X
2
+X
3
+X
4
+X
5
) 52.23 42.91
?
36.96 33.65
?
19 +(X
2
+X
3
+X
4
+X
5
)+DAE X 2f 53.55 44.17
+???
38.23 34.50
+???
20 +(X
2
+X
3
+X
4
+X
5
)+DAE X 4f 53.61 44.22
+???
38.28 34.47
+???
21 +(X
2
+X
3
+X
4
+X
5
)+HCDAE X 2+2f 53.75 44.28
+????
38.35 34.65
+????
22 +(X
2
+X
3
+X
4
+X
5
)+DAE X 8f 53.47 44.19
+???
38.26 34.46
+???
23 +(X
2
+X
3
+X
4
+X
5
)+HCDAE X 4+4f 53.62 44.29
+????
38.39 34.57
+????
Table 2: The translation results by adding new DNN features (DBN feature (Maskey and Zhou, 2012),
our proposed DAE and HCDAE feature) as extra features to the phrase table on two tasks. ?DBN X
1
xf?,
?DBN X xf?, ?DAE X
1
xf? and ?DAE X xf? represent that we use DBN and DAE, input features
X
1
and X , to learn x-dimensional features, respectively. ?HCDAE X x+xf? represents horizontally
combining two DAEs and each DAE has the same x-dimensional learned features. All improvements on
two test sets are statistically significant by the bootstrap resampling (Koehn, 2004). *: significantly better
than the baseline (p < 0.05), **: significantly better than ?DBN X
1
xf? or ?DBN X xf? (p < 0.01),
***: significantly better than ?DAE X
1
xf? or ?DAE X xf? (p < 0.01), ****: significantly better than
?HCDAE X x+xf? (p < 0.01), +: significantly better than ?X
2
+X
3
+X
4
+X
5
? (p < 0.01).
discounting. We perform pairwise ranking opti-
mization (Hopkins and May, 2011) to tune feature
weights. The translation quality is evaluated by
case-insensitive IBM BLEU-4 metric.
The baseline translation models are generated
by Moses with default parameter settings. In the
contrast experiments, our DAE and HCDAE fea-
tures are appended as extra features to the phrase
table. The details of the used network structure in
our experiments are shown in Table 1.
5.2 Results
Table 2 presents the main translation results. We
use DBN, DAE and HCDAE (with 6 layers? net-
work depth), input features X
1
and X , to learn 2-,
4- and 8-dimensional features, respectively. From
the results, we can get some clear trends:
1. Adding new DNN features as extra features
significantly improves translation accuracy (row
2-17 vs. 1), with the highest increase of 2.45
(IWSLT) and 1.52 (NIST) (row 14 vs. 1) BLEU
points over the baseline features.
2. Compared with the unsupervised DBN fea-
tures, our semi-supervised DAE features are more
effective for translation decoder (row 3 vs. 2; row
5 vs. 4; row 8 vs. 7; row 11 vs. 10; row 13 vs.
12; row 16 vs. 15). Specially, Table 3 shows the
variance distributions of the learned each dimen-
sional DBN and DAE feature, our DAE features
have bigger variance distributions which means
128
Features
IWSLT NIST
?
1
?
2
?
3
?
4
?
1
?
2
?
3
?
4
DBN X
1
4f 0.1678 0.2873 0.2037 0.1622 0.0691 0.1813 0.0828 0.1637
DBN X 4f 0.2010 0.1590 0.2793 0.1692 0.1267 0.1146 0.2147 0.1051
DAE X
1
4f 0.5072 0.4486 0.1309 0.6012 0.2136 0.2168 0.2047 0.2526
DAE X 4f 0.5215 0.4594 0.2371 0.6903 0.2421 0.2694 0.3034 0.2642
Table 3: The variance distributions of each dimensional learned DBN feature and DAE feature on the
two tasks.
Figure 4: The compared results of feature learning with different network structures on two development
sets.
Features
IWSLT NIST
Dev Test Dev Test
+DAE X
1
4f 52.45 43.06 36.88 33.47
+DAE X
1
+X
2
4f 52.76 43.38
?
37.28 33.80
?
+DAE X
1
+X
3
4f 52.61 43.27
?
37.13 33.66
?
+DAE X
1
+X
4
4f 52.52 43.24
?
36.96 33.58
?
+DAE X
1
+X
5
4f 52.49 43.13
?
36.96 33.56
?
+DAE X 4f 52.81 43.47
?
37.48 33.92
?
Table 4: The effectiveness of our introduced in-
put features. ?DAE X
1
+X
i
4f? represents that
we use DAE, input features X
1
+ X
i
, to learn 4-
dimensional features. *: significantly better than
?DAE X
1
4f? (p < 0.05).
our DAE features have more discriminative power,
and also their variance distributions are more sta-
ble.
3. HCDAE outperforms single DAE for high
dimensional feature learning (row 6 vs. 5; row 9
vs. 8; row 14 vs. 13; row 17 vs. 16), and further
improve the performance of DAE feature learning,
which can also somewhat address the bring short-
coming of the limited input features.
4. Except for the phrase feature X
1
(Maskey
and Zhou, 2012), our introduced input features
X significantly improve the DAE feature learn-
ing (row 11 vs. 3; row 13 vs. 5; row 16 vs. 8).
Specially, Table 4 shows the detailed effectiveness
of our introduced input features for DAE feature
learning, and the results show that each type of
features are very effective for DAE feature learn-
ing.
5. Adding the original features (X
2
,X
3
,X
4
and
X
5
) and DAE/HCDAE features together can fur-
ther improve translation performance (row 19-23
vs. 18), with the highest increase of 3.16 (IWSLT)
and 2.06 (NIST) (row 21 vs. 1) BLEU points over
the baseline features. DAE and HCDAE features
are learned from the non-linear combination of the
original features, they strong capture high-order
correlations between the activities of the original
features, as to be further interpreted to reach their
potential for SMT. We suspect these learned fea-
129
tures are complementary to the original features.
5.3 Analysis
Figure 5: The compared results of using single
DAE and the HCDAE for feature learning on two
development sets.
Figure 4 shows our DAE features are not only
more effective but also more stable than DBN
features with various network structures. Also,
adding more input features (X vs. X
1
) not only
significantly improves the performance of DAE
feature learning, but also slightly improves the
performance of DBN feature learning.
Figure 5 shows there is little change in the per-
formance of using single DAE to learn different
dimensional DAE features, but the 4-dimensional
features work more better and more stable. HC-
DAE outperforms the single DAE and learns high-
dimensional representation more effectively, espe-
cially for the peak point in each condition.
Figures 5 also shows the best network depth for
DAE feature learning is 6 layers. When the net-
work depth of DBN is 7 layers, the network depth
of corresponding DAE during the fine-tuning is 13
layers. Although we have pre-trained the corre-
sponding DBN, this DAE network is so deep, the
fine-tuning does not work very well and typically
finds poor local minima. We suspect this leads to
the decreased performance.
6 Conclusions
In this paper, instead of designing new features
based on intuition, linguistic knowledge and do-
main, we have learned new features using the DAE
for the phrase-based translation model. Using the
unsupervised pre-trained DBN to initialize DAE?s
parameters and using the input original phrase fea-
tures as the ?teacher? for semi-supervised back-
propagation, our semi-supervised DAE features
are more effective and stable than the unsuper-
vised DBN features (Maskey and Zhou, 2012).
Moreover, to further improve the performance, we
introduce some simple but effective features as
the input features for feature learning. Lastly, to
learn high dimensional feature representation, we
introduce a natural horizontal composition of two
DAEs for large hidden layers feature learning.
On two Chinese-English translation tasks, the
results demonstrate that our solutions solve the
two aforementioned shortcomings successfully.
Firstly, our DAE features obtain statistically sig-
nificant improvements of 1.34/2.45 (IWSLT) and
0.82/1.52 (NIST) BLEU points over the DBN fea-
tures and the baseline features, respectively. Sec-
ondly, compared with the baseline phrase features
X
1
, our introduced input original phrase features
X significantly improve the performance of not
only our DAE features but also the DBN features.
The results also demonstrate that DNN (DAE
and HCDAE) features are complementary to the
original features for SMT, and adding them to-
gether obtain statistically significant improve-
ments of 3.16 (IWSLT) and 2.06 (NIST) BLEU
points over the baseline features. Compared with
the original features, DNN (DAE and HCDAE)
features are learned from the non-linear combi-
nation of the original features, they strong cap-
ture high-order correlations between the activities
of the original features, and we believe this deep
learning paradigm induces the original features to
further reach their potential for SMT.
Acknowledgments
This work was supported by 863 program in
China (No. 2011AA01A207). We would like to
thank Xingyuan Peng, Lichun Fan and Hongyan
Li for their helpful discussions. We also thank
the anonymous reviewers for their insightful com-
ments.
130
References
Michael Auli, Michel Galley, Chris Quirk and Geoffrey
Zweig. 2013. Joint language and translation model-
ing with recurrent neural networks. In Proceedings
of EMNLP, pages 1044-1054.
Pierre Baldi. 2012. Autoencoders, unsupervised learn-
ing, and deep architectures. JMLR: workshop on un-
supervised and transfer learning, 27:37-50.
Miguel A. Carreira-Perpinan and Geoffrey E. Hinton.
2005. On contrastive divergence learning. In Pro-
ceedings of AI and Statistics.
George Dahl, Dong Yu, Li Deng, and Alex Acero.
2012. Context-dependent pre-trained deep neural
networks for large vocabulary speech recognition.
IEEE Transactions on Audio, Speech, and Language
Processing, 20(1):30-42.
Li Deng, Mike Seltzer, Dong Yu, Alex Acero, Abdel-
rahman Mohamed, and Geoffrey E. Hinton. 2010.
Binary coding of speech spectrograms using a deep
auto-encoder. In Proceedings of INTERSPEECH,
pages 1692-1695.
George Foster, Cyril Goutte, and Roland Kuhn. 2010.
Discriminative instance weighting for domain adap-
tation in statistical machine translation. In Proceed-
ings of EMNLP, pages 451-459.
Geoffrey E. Hinton. 2002. Training products of ex-
perts by minimizing contrastive divergence. Neural
Computation, 14(8):1771-1800.
Geoffrey E. Hinton, Li Deng, Dong Yu, George Dahl,
Abdel-rahman Mohamed, Navdeep Jaitly, Andrew
Senior, Vincent Vanhoucke, Patrick Nguyen, Tara
Sainath, and Brian Kingsbury. 2012. Deep neural
networks for acoustic modeling in speech tecogni-
tion. IEEE Signal Processing Magazine, 29(6):82-
97.
Geoffrey E. Hinton, Alex Krizhevsky, and Sida D.
Wang. 2001. Transforming auto-encoders. In Pro-
ceedings of ANN.
Geoffrey E. Hinton and Ruslan R. Salakhutdinov.
2006. Reducing the dimensionality of data with
neural networks. Science, 313:504-507.
Geoffrey E. Hinton, Simon Osindero, and Yee-Whye
Teh. 2006. A fast learning algorithm for deep belief
nets. Neural Computation, 18:1527-1544.
Mark Hopkins and Jonathan May 2011. Tuning as
ranking. In Proceedings of EMNLP, pages 1352-
1362.
Nal Kalchbrenner and Phil Blunsom. 2013. Recur-
rent continuous translation models. In Proceedings
of EMNLP, pages 1700-1709.
Philipp Koehn. 2004. Statistical significance tests
from achine translation evaluation. In Proceedings
of ACL, pages 388-395.
Philipp Koehn. 2010. Statistical machine translation.
Cambridge University Press.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of ACL, Demonstration Session, pages
177-180.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings
of NAACL, pages 48-54.
Hai-Son Le, Alexandre Allauzen, and Franc?ois Yvon.
2012. Continuous space translation models with
neural networks. In Proceedings of NAACL, pages
39-48.
Peng Li, Yang Liu, Maosong Sun. 2013. Recursive
autoencoders for ITG-based translation. In Proceed-
ings of EMNLP, pages 567-577.
Lemao Liu, Taro Watanabe, Eiichiro Sumita, and
Tiejun Zhao. 2013. Additive neural networks for
statistical machine translation. In Proceedings of
ACL, pages 791-801.
Shixiang Lu, Wei Wei, Xiaoyin Fu and Bo Xu. 2014.
Recursive neural network based word topology
model for hierarchical phrase-based speech transla-
tion. In Proceedings of ICASSP.
Yuval Marton and Philip Resnik. 2008. Soft syntactic
constraints for hierarchical phrase-based translation.
In Proceedings of ACL, pages 1003-1011.
Sameer Maskey and Bowen Zhou. 2012. Unsuper-
vised deep belief features for speech translation. In
Proceedings of INTERSPEECH.
Piotr Mirowski, MarcAurelio Ranzato, and Yann Le-
Cun. 2010. Dynamic auto-encoders for semantic
indexing. In Proceedings of NIPS-2010 Workshop
on Deep Learning.
Patrick Nguyen, Milind Mahajan, and Xiaodong He.
2007. Training non-parametric features for statis-
tical machine translation. In Proceedings of WMT,
pages 72-79.
Franz J. Och and Hermann Ney. 2000. Improved sta-
tistical alignment models. In Proceedings of ACL,
pages 440-447.
Franz J. Och and Hermann Ney. 2002. Discriminative
training and maximum entropy models for statistical
machine translation. In Proceedings of ACL, pages
295-302.
Franz J. Och and Hermann Ney. 2004. The alignment
template approach to statistical machine translation.
Computational Linguistics, 30(4):417-449.
131
David Rumelhart, Geoffrey E. Hinton, and Ronale
Williams. 1986. Learning internal representations
by back-propagation errors. Parallel Distributed
Processing, Vol 1: Foundations, MIT Press.
Ruslan R. Salakhutdinov and Geoffrey E. Hinton.
2009. Semantic hashing. International Journal of
Approximate Reasoning, 50(7):969-978.
Richard Socher, Cliff C. Lin, Andrew Y. Ng, and
Christopher D. Manning. 2011. Parsing natural
scenes and natural language with recursive neural
networks. In Proceedings of ICML.
Deyi Xiong, Min Zhang, and Haizhou Li. 2011.
Enhancing language models in statistical machine
translation with backward n-grams and mutual in-
formation triggers. In Proceedings of ACL, pages
1288-1297.
Bing Zhao, Stephan Vogel, and Alex Waibel. 2004.
Phrase pair rescoring with term weightings for sta-
tistical machine translation. In Proceedings of
EMNLP.
132
