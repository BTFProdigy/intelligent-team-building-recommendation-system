Monolingual and Bilingual Concept Visualization from Corpora
Dominic Widdows Scott Cederberg
Center for the Study of Language and Information, Stanford University
{dwiddows,cederber}@csli.stanford.edu
As well as identifying relevant information, a suc-
cessful information management system must be able to
present its findings in terms which are familiar to the user,
which is especially challenging when the incoming in-
formation is in a foreign language (Levow et al, 2001).
We demonstrate techniques which attempt to address this
challenge by placing terms in an abstract ?information
space? based on their occurrences in text corpora, and
then allowing a user to visualize local regions of this in-
formation space. Words are plotted in a 2-dimensional
picture so that related words are close together and whole
classes of similar words occur in recognizable clusters
which sometimes clearly signify a particular meaning. As
well as giving a clear view of which concepts are related
in a particular document collection, this technique also
helps a user to interpret unknown words.
The main technique we will demonstrate is planar pro-
jection of word-vectors from a vector space built using
Latent Semantic Analysis (LSA) (Landauer and Dumais,
1997; Schu?tze, 1998), a method which can be applied
multilingually if translated corpora are available for train-
ing. Following the method of Schu?tze (1998), we assign
each word 1000 coordinates based on the number of times
that word occurs in a 15 word window with one of 1000
?content-bearing words?, chosen by frequency, and the
number of coordinates is reduced to 100 ?latent dimen-
sions? using LSA.
This is still far too many words and too many dimen-
sions to be visualized at once. To produce a meaningful
diagram of results related to a particular word or query,
we perform two extra steps. Firstly, we restrict atten-
tion to a given number of closely related words (deter-
mined by cosine similarity of word vectors), selecting a
local group of up to 100 words and their word vectors
for deeper analysis. A second round of Latent Seman-
tic Analysis is then performed on this restricted set, giv-
ing the most significant directions to describe this local
information. The 2 most significant axes determine the
plane which best represents the data. (This process can
be regarded as a higher-dimensional analogue of finding
the line of best-fit for a normal 2-dimensional graph.) The
resulting diagrams give an summary of the areas of mean-
ing in which a word is actually used in a particular docu-
ment collection.
This is particularly effective for visualizing words in
more than one language. This can be achieved by build-
ing a single latent semantic vector space incorporat-
ing words from two languages using a parallel corpus
(Littman et al, 1998; Widdows et al, 2002b). We will
demonstrate a system which does this for English and
German terms in the medical domain. The system is
trained on a corpus of 10,000 abstracts from German
medical documents available with their English transla-
tions 1. In the demonstration, users submit a query state-
ment consisting of any combination of words in English
or German, and are then able to visualize the words most
closely related to this query in a 2-dimensional plot of the
latent semantic space.
An example output for the English query word drug is
shown in Figure below. 2. Such words are of special
interest because the English word drug has two mean-
ings which are represented by different words in German
(medikament = prescription drug and drogen = narcotic).
The 2-dimensional plot clearly distinguishes these two
areas of meaning, with the English word drug being in
between. Such techniques can enable users to recognize
and understand translational ambiguities.
As well as the Springer abstracts corpus, the system
has been trained to work with the parallel English/French
Canadian Hansard corpus and several large monolingual
corpora. Other functionalities of this system include au-
tomatic thesaurus generation, clustering of terms to deter-
mine different context areas, query refinement and docu-
ment retrieval.
As well as LSA, which only uses broad ?bag of words?
1Available from the Springer Link website,
http://link.springer.de/
2In the actual demonstration, English results appear in red
and German results in blue: for the description here we have
used different fonts instead.
                                                               Edmonton, May-June 2003
                                                            Demonstrations , pp. 31-32
                                                         Proceedings of HLT-NAACL 2003
DRUG
DRUGS
FATALITIES
FORENSIC
COCAINE
ABUSE
METHADONE
OPIATES
ANTIEPILEPTIC
THC
URINE
ANTICONVULSANT
NEUROTRANSMISSIONCANNABINOIDS
DOSAGE
DEPENDENCEPIGMENTATION
HAIR
ANTIARRHYTHMIC
SEROTONERGIC
HEROIN
drogentodesfa?lle
kokain
drogen
substanzen
antiepileptika
medikamento?se
antiarrhythmika
opiate
drogenabha?ngigen medikamente
medikamenten
pharmaka
methadon
gc
heroin
pigmentierung
arzneimittel
substanz
wirksame
wirksamer
beta?ubungsmittel
Figure 1: ENGLISH and German terms related to the English word drug in the Springer medical abstracts.
coocurrence to define similarities, mathematical models
can be built using local coordination of terms based on
syntactic properties. For example, list of nouns such as
?apples, pears and oranges? can be used as information
that these words are all linked, and these links can be
recorded in a database which can also be analyzed using
visualization techniques (Widdows et al, 2002a) and will
be included in the demonstration.
Demonstration website
Versions of these demonstrations are publicly avail-
able through the CSLI Infomap project website,
(http://infomap.stanford.edu/).
Acknowledgments
This research was supported in part by the Research
Collaboration between the NTT Communication Science
Laboratories, Nippon Telegraph and Telephone Corpora-
tion and CSLI, Stanford University, and by EC/NSF grant
IST-1999-11438 for the MUCHMORE project.
References
T. Landauer and S. Dumais. 1997. A solution to plato?s
problem: The latent semantic analysis theory of acqui-
sition. Psychological Review, 104(2):211?240.
Gina-Anne Levow, Douglas W. Oard, and Philip Resnik.
2001. Rapidly retargetable interactive translingual re-
trieval. In Human Language Technology Conference
(HLT 2001), San Diego, CA.
Michael L. Littman, Susan T. Dumais, and Thomas K.
Landauer. 1998. Automatic cross-language informa-
tion retrieval using latent semantic indexing. In Gre-
gory Grefenstette, editor, Cross-language information
retrieval, chapter 4. Kluwer, Boston.
Hinrich Schu?tze. 1998. Automatic word sense discrimi-
nation. Computational Linguistics, 24(1):97?124.
Dominic Widdows, Scott Cederberg, and Beate Dorow.
2002a. Visualisation techniques for analysing mean-
ing. In Fifth International Conference on Text, Speech
and Dialogue, Lecture Notes in Artificial Intelligence
2448, pages 107?115, Brno, Czech Republic, Septem-
ber. Springer.
Dominic Widdows, Beate Dorow, and Chiu-Ki Chan.
2002b. Using parallel corpora to enrich multilingual
lexical resources. In Third International Conference
on Language Resources and Evaluation, pages 240?
245, Las Palmas, Spain, May.
Using LSA and Noun Coordination Information to Improve the Precision
and Recall of Automatic Hyponymy Extraction
Scott Cederberg Dominic Widdows
Center for the Study of Language and Information
210 Panama Street
Stanford University
Stanford CA 94305
{cederber,dwiddows}@csli.stanford.edu
Abstract
In this paper we demonstrate methods of im-
proving both the recall and the precision of au-
tomatic methods for extraction of hyponymy
(IS A) relations from free text. By applying la-
tent semantic analysis (LSA) to filter extracted
hyponymy relations we reduce the rate of er-
ror of our initial pattern-based hyponymy ex-
traction by 30%, achieving precision of 58%.
Applying a graph-based model of noun-noun
similarity learned automatically from coordi-
nation patterns to previously extracted correct
hyponymy relations, we achieve roughly a five-
fold increase in the number of correct hy-
ponymy relations extracted.
1 Introduction
This paper demonstrates that mathematical models for
measuring semantic similarity between concepts can be
used to improve the learning of hyponymy relationships
between concepts from free text. In particular, we show
that latent semantic analysis can be used to filter results,
giving an increase in precision, and that neighbors in a
graph built from coordination information can be used to
improve recall.
The goal of extracting semantic information from text
is well-established, and has encouraged work on lexical
acquisition (Roark and Charniak, 1998), information ex-
traction (Cardie, 1997), and ontology engineering (Hahn
and Schnattinger, 1998). The purpose of this kind of
work is to collect information about the meanings of lexi-
cal items or phrases, and the relationships between them,
so that the process of building semantic resources (such
as ontologies and dictionaries) by hand can be automated
or at least helped.
One of the standard ways of arranging concepts is in a
concept hierarchy or taxonomy such as the WordNet noun
taxonomy (Fellbaum, 1998). The fundamental relation-
ship between objects in a taxonomy is called hyponymy,
where y is a hyponym of x if every y is also an x. For
example, every trout is also a fish, so we say that trout
is a hyponym (?below name?) of fish and conversely, fish
is a hypernym (?above name?) of trout. Other names ex-
ist for variants of the hyponymy relationship, such as an
IS A relationship, a parent-node / child-node relationship,
and a broader term / narrower term relationship. It is also
noted that the genus of an object, in traditional lexico-
graphic terms, is often a hypernym of that object (Guthrie
et al, 1996). Throughout this paper we will write y < x
for the relationship ?y is a hyponym of x?. In this paper,
we use the hyponymy relationship to describe subset re-
lationships, so we regard y < x to be true if the set of y?s
can reasonably be said to be a subset of the set of x?s.1
Because hyponymy relationships are so central to
knowledge engineering, there have been numerous at-
tempts to learn them from text, beginning with those
of Hearst (1992). We review this work in Section 2,
where we reproduce similar experiments as a baseline
from which to expand. The rest of the paper demon-
strates ways in which other mathematical models built
from text corpora can be used to improve hyponymy ex-
traction. In Section 3, we show how latent semantic anal-
ysis can be used to filter potential relationships accord-
ing to their ?semantic plausibility?. In Section 4, we
show how correctly extracted relationships can be used
as ?seed-cases? to extract several more relationships, thus
improving recall; this work shares some similarities with
that of Caraballo (1999). In Section 5 we show that com-
bining the techniques of Section 3 and Section 4 improves
both precision and recall. Section 6 demonstrates that
1Another possible view is that ?hyponymy? should only re-
fer to core relationships, not contingent ones (so pheasant <
bird might be accepted but pheasant < food might not be, be-
cause it depends on context and culture). We use the broader
?subset? definition because contingent relationships are an im-
portant part of world-knowledge (and are therefore worth learn-
ing), and because in practice we found the distinction difficult to
enforce. Another definition is given by Caraballo (1999): ?. . . a
word A is said to be a hypernym of a word B if native speakers
of English accept the sentence ?B is a (kind of) A.? ?
linguistic tools such as lemmatization can be used to re-
liably put the extracted relationships into a normalized or
?canonical? form for addition to a semantic resource.
2 Pattern-Based Hyponymy Extraction
The first major attempt to extract hyponyms from text
was that of Hearst (1992), described in more detail in
(Hearst, 1998), who extracted relationships from the text
of Grolier?s Encyclopedia. The method is illustrated by
the following example. The sentence excerpt
Even then, we would trail behind other Euro-
pean Community members, such as Germany,
France and Italy. . . (BNC)2
indicates that Germany, France, and Italy are all Euro-
pean Community members. More generally, phrases of
the form
x such as y1 (y2, . . . , and/or yn)
frequently indicate that the yi are all hyponyms of
the hypernym x. Hearst identifies several other con-
structions that have a tendency to indicate hyponymy,
calling these constructions lexicosyntactic patterns, and
analyses the results. She reports that 52% of the re-
lations extracted by the ?or other? pattern (see Ta-
ble 1) were judged to be ?pretty good relations?. A
more recent variant of this technique was implemented
by Alfonseca and Manandhar (2001), who compare the
collocational patterns of words from The Lord of the
Rings with those of words in the WordNet taxonomy,
adding new nouns to WordNet with an accuracy of
28%. Using a much more knowledge-intensive approach,
Hahn and Schnattinger (1998) improve ?learning accu-
racy? from around 50% to over 80% by forming a number
of hypotheses and accepting only those which are most
consistent with their current ontology. Their methods are
like ours in that the ?concept learning? combines infor-
mation from several occurrences, but differ in that they
rely on a detailed existing ontology into which to fit the
new relationships between concepts.
Our initial experiment was to construct a hyponymy
extraction system based on the six lexicosyntactic pat-
terns identified in (Hearst, 1998), which are listed in Ta-
ble 1. We first used a chunker to mark noun groups, and
then recognized and extracted noun groups occurring as
part of one of the extraction patterns.3
We applied these extraction patterns to an approxi-
mately 430,000-word extract from the beginning of the
2This excerpt and others in this paper are from the British
National Corpus.
3The chunker used was LT CHUNK, from
the University of Edinburgh?s Language Tech-
nology Group. It can be downloaded from
http://www.ltg.ed.ac.uk/software/chunk/.
x such as y1 (, y2, . . . , and/or yn)
such x as y1 (, y2, . . . , and/or yn)
y1 (, y2, . . . , yn,) or other x
y1 (, y2, . . . , yn,) and other x
x, including y1 (, y2, . . . , and/or yn)
x, especially y1 (, y2, . . . , and/or yn)
Table 1: The lexicosyntactic patterns described by
Hearst (1998), which we used in the work described in
this paper. Each of these patterns is taken to indicate the
hyponymy relation(s) yi < x.
British National Corpus (BNC). The patterns extracted
513 relations. We selected 100 of the extracted relations
at random and each author evaluated them by hand, scor-
ing each relation on a scale from 4 (correct) to 0 (incor-
rect), defined as follows:
4. Extracted hypernym and hyponym exactly correct as
extracted.
3. Extracted hypernym and hyponym are correct after
a slight modification, such as depluralization or the
removal of an article (e.g. a, the) or other preceding
word.
2. Extracted hypernym and hyponym have something
correct, e.g. a correct noun without a necessary
prepositional phrase, a correct noun with a superflu-
ous prepositional phrase, or a noun + prepositional
phrase where the object of the preposition is correct
but the preposition itself and the noun to which it
attaches are superfluous. Thus these hyponymy re-
lations are potentially correct but will require poten-
tially difficult processing to extract an exactly cor-
rect relation. Some of the errors which would need
to be corrected were in preprocessing (e.g. on the
part of the noun-group chunker) and others were er-
rors caused by our hyponymy extractor (e.g. tacking
on too many or too few prepositional phrases).
1. The relation extracted is correct in some sense, but
is too general or too context specific to be useful.
This category includes relations that could be made
useful by anaphora resolution (e.g. replacing ?this?
with its referent).
0. The relation extracted is incorrect. This results when
the constructions we recognize are used for a pur-
pose other than indicating the hyponymy relation.
The results of each of the authors? evaluations of the
100-relation random sample are show in Table 2.4 For
4Table 2 suggests that although there is significant disagree-
ment about how to assign scores of 1 and 0, inter-annotator
score Author 1 Author 2
4 4 2
3 34 35
2 14 13
1 35 22
0 13 28
Table 2: Number of the 100 randomly selected hyponymy
relations (of 513 extracted) to which each of the authors
assigned the five available scores.
purposes of calculating precision, we consider those rela-
tions with a score of 4 or 3 to be correct and those with a
lower score to be incorrect. After discussion between the
authors on disputed annotations to create ?gold standard?
annotations, we found that 40 of the 100 relations in our
random sample were correct according to this criterion.
In other words, 40% of the relations extracted were ex-
actly correct or would be correct with the use of minor
post-processing consisting of lemmatization and removal
of common types of qualifying words. (We describe our
application of such post-processing in Section 6.)
Thus our initial implementation of Hearst-style hy-
ponymy extraction achieved 40% precision. This is less
than the 52% precision reported in (Hearst, 1998). We
believe this discrepancy to be mainly due to the dif-
ference between working with the BNC and Grolier?s
encyclopedia?as noted by Hearst, the encyclopedia is
designed to be especially rich in conceptual relationships
presented in an accessible format.
Various problems with the pattern-based extraction
method explain the 60% of extracted relations that were
incorrect and/or useless. One problem is that the con-
structions that we assume to indicate hyponymy are often
used for other purposes. For instance, the pattern
x including y1, y2, . . . , and yn
which indicates hyponymy in sentences such as
Illnesses, including chronic muscle debility,
herpes, tremors and eye infections, have come
and gone. (BNC)
and is a quite productive source of hyponymy relations,
can be used instead to indicate group membership:
agreement regarding the assignment of scores of 4, 3, and 2 is
quite high. Indeed, considering the rougher distinction we use
for reporting precision, in which scores of 4 and 3 are deemed
correct and scores of 2, 1, and 0 are deemed incorrect, we found
that inter-annotator agreement across all relations annotated (in-
cluding those from this random sample and those from the sam-
ple described in Section 3) was 86%. We discussed each of
the relations in the 14% of cases where we disagreed until we
reached agreement; this produced the ?gold standard? annota-
tions to which we refer.
Often entire families including young children
need practical home care . . . (BNC)
While all children are members of families, the hy-
ponymy relationship child < family does not hold, since
it is not true that all children are families.
Another source of errors in lexicosyntactic hyponymy
extraction is illustrated by the sentence
A kit such as Edme Best Bitter, Tom Caxton
Best Bitter, or John Bull Best Bitter will be a
good starting kit. (BNC)
which indicates the (potentially useful) relations Edme
Best Bitter < beer-brewing kit, Tom Caxton Best Bitter
< beer-brewing kit, and John Bull Best Bitter < beer-
brewing kit, but only when we use the context to infer
that the type of ?kit? referred to is a beer-brewing kit, a
process that is difficult by automatic means. Without this
inference, the extracted relations Edme Best Bitter < kit,
etc., while correct in a certain sense, are not helpful. One
frequent source of such problems is anaphora that require
resolution.
There are also problems related to prepositional phrase
attachment.
3 Improving Precision Using Latent
Semantic Analysis
Solving all of the problems with pattern-based hyponymy
extraction that we describe above would require near-
human-level language understanding, but we have ap-
plied a far simpler technique for filtering out many of the
incorrect and spurious extracted relations with good re-
sults, using a variant of latent semantic analysis (LSA)
(Deerwester et al, 1990; Baeza-Yates and Ribiero-Neto,
1999, p. 44). LSA is a method for representing words
as points in a vector space, whereby words which are re-
lated in meaning should be represented by points which
are near to one another. The LSA model we built is sim-
ilar to that described in (Schu?tze, 1998). First 1000 fre-
quent content words (i.e. not on the stoplist)5 were chosen
as ?content-bearing words?. Using these content-bearing
words as column labels, the other words in the corpus
were assigned row vectors by counting the number of
times they occured within a 15-word context window of
a content-bearing word. Singular-value decomposition
(Deerwester et al, 1990) was then used to reduce the
number of dimensions from 1000 to 100. Similarity be-
tween two vectors (points) was measured using the cosine
of the angle between them, in the same way as the simi-
larity between a query and a document is often measured
5A ?stoplist? is a list of frequent words which have little
semantic content in themselves, such as prepositions and pro-
nouns (Baeza-Yates and Ribiero-Neto, 1999, p. 167).
score Author 1 Author 2
4 4 5
3 57 52
2 18 14
1 12 19
0 9 10
Table 3: Number of the 100 top-ranked hyponymy re-
lations (of 513 extracted) to which each of the authors
assigned the five available scores.
in information retrieval (Baeza-Yates and Ribiero-Neto,
1999, p. 28). Effectively, we could use LSA to measure
the extent to which two words x and y usually occur in
similar contexts. This LSA similarity score will be called
sim(x, y).
Since we expect a hyponym and its hypernym to be
semantically similar, we can use the LSA similarity be-
tween two terms as a test of the plausibility of a putative
hyponymy relation between those terms. If their similar-
ity is low, it is likely that they do not have a true and use-
ful hyponymy relationship; the relation was probably ex-
tracted erroneously for one or more of the reasons listed
above. If the similarity between two terms is high, we
have increased confidence that a hyponymy relationship
exists between them, because we know that they are at
least in similar ?semantic regions?.
We ranked the 513 putative hyponym/hypernym pairs
that we extracted from our trial excerpt of the BNC ac-
cording to the similarity between the putative hypernym
and the putative hyponym in each pair; i.e. for each pair
x and y where the relationship y < x had been suggested,
we calculated the cosine similarity sim(x, y), then we
ranked the extracted relations from highest to lowest sim-
ilarity. We then manually evaluated the accuracy of the
top 100 extracted relations according to this ranking us-
ing the 5-point scale described in Section 2. We found
that 58 of these 100 top-ranked relations received scores
of 4 or 3 according to our ?gold standard? annotations.
Comparing this 58% precision with the 40% precision
obtained on a random sample in Section 2, we determine
that LSA achieved a 30% reduction in error (see Table 3
for a breakdown of annotation results by author).6
Thus LSA proved quite an effective filter. LSA pro-
vides broad-based semantic information learned statis-
tically over many occurences of words; lexicosyntactic
hyponymy extraction learns semantic information from
specific phrases within a corpus. Thus we have bene-
fitted from combining local patterns with statistical in-
6It should be noted that 24 of the top 100 hyponymy rela-
tions evaluated in this section were also in the randomly-chosen
sample of 100 relations described in Section 2. Thus there were
a total of 176 distinct hyponymy relations across both test sets.
formation. Considered in analogy with the process by
which humans learn from reading, we might think of
the semantic information learned by LSA as background
knowledge that is applied by the reader when determining
what can accurately be gleaned from a particular sentence
when it is read.
4 Improving Recall Using Coordination
Information
One of the main challenges facing hyponymy extraction
is that comparatively few of the correct relations that
might be found in text are expressed overtly by the simple
lexicosyntactic patterns used in Section 2, as was appar-
ent in the results presented in that section.
This problem has been addressed by Caraballo (1999),
who describes a system that first builds an unlabelled hi-
erarchy of noun clusters using agglomerative bottom-up
clustering of vectors of noun coordination information.
The leaves of this hierarchy (corresponding to nouns)
are assigned hypernyms using Hearst-style lexicosyntac-
tic patterns. Internal nodes in the hierarchy are then la-
belled with hypernyms of the leaves they subsume ac-
cording to a vote of these subsumed leaves.
We proceed along similar lines, using noun coordi-
nation information and an alternative graph-based clus-
tering method. We do not build a complete hierarchy,
but our method nonetheless obtains additional hypernym-
hyponym pairs not extracted by lexicosyntactic patterns.
Our method is based on the following sort of inference.
Consider the sentence
This is not the case with sugar, honey, grape
must, cloves and other spices which increase
its merit. (BNC)
which provides evidence that clove is a kind of spice.
Given this, the sentence
Ships laden with nutmeg or cinnamon, cloves
or coriander once battled the Seven Seas to
bring home their precious cargo. (BNC)
might suggest that nutmeg, cinnamon, and coriander are
also spices, because they appear to be similar to cloves.
Thus we can learn the hyponymy relations nutmeg <
spice, cinnamon < spice, and coriander < spice that
are not directly attested by lexicosyntactic patterns in our
training corpus.
This kind of information from coordination patterns
has been used for work in automatic lexical acquisition
(Riloff and Shepherd, 1997; Roark and Charniak, 1998;
Widdows and Dorow, 2002). The basic rationale behind
these methods is that words that occur together in lists
are usually semantically similar in some way: for exam-
ple, the phrase
y1, y2, and y3
suggests that there is some link between y1 and y2, etc.
Performing this analysis on a whole corpus results in a
data structure which holds a collection of nouns and ob-
served noun-noun relationships. If we think of the nouns
as nodes and the noun-noun relationships as edges, this
data structure is a graph (Bolloba?s, 1998), and combina-
toric methods can be used to analyze its structure.
Work using such techniques for lexical acquisition has
proceeded by building classes of related words from a
single ?seed-word? with some desired property (such as
being a representative of a paticular semantic class). For
example, in order to extract a class of words referring to
kinds of disease from a corpus, you start with a single
seed-word such as typhoid, and then find other nouns that
occur in lists with typhoid. Using the graph model de-
scribed above, Widdows and Dorow (2002) developed a
combinatoric algorithm for growing clusters from a sin-
gle seed-word, and used these methods to find correct
new members for chosen categories with an accuracy of
over 80%.
The idea that certain patterns can be identified using
finite-state techniques and used as evidence for seman-
tic relationships is the same as Hearst?s (1992), but ap-
pears to be more effective for finding just similar words
rather than hypernyms because there are many more in-
stances of simple coordination patterns than of hyper-
nymy patterns?in the lists we used to extract these re-
lationships, we see much more cooccurence of words on
the same ontological level than between words from dif-
ferent ontological levels. For example, in the BNC there
are 211 instances of the phrase ?fruit and vegetables? and
9 instances of ?carrots and potatoes?, but no instances of
?fruit and potatoes?, only 1 instance of ?apples and veg-
etables?, and so on.
This sort of approach should be ideal for improving
the recall of automatic hyponymy extraction, by using the
hyponym from each of the correct hypernym/hyponym
pairs as a seed-word for the category represented by the
hypernym?for example, from the relationship clove <
spice, the word clove could be taken as a seed-word, with
the assumption that words which frequently occur in co-
ordination with clove are also names of spices.
We used the algorithm of (Widdows and Dorow, 2002)
on the British National Corpus to see if many more hy-
ponymy relations would be extracted in this way. For
each correct pair y < x where y was a single-word hy-
ponym of x discovered by the lexicosyntactic patterns of
Section 2, we collected the 10 words most similar to y ac-
cording to this algorithm and tested to see if these neigh-
bors were also hyponyms of x.
Of the 176 extracted hyponyms that we evaluated by
hand in the overlapping test sets described in Section 2
and Section 3, 95 were rated 4 or 3 on our 5-point scor-
ing system (Section 2) by at least one of the authors. Con-
sidering these correct or nearly-correct relations in their
hand-corrected form, we found that 45 of these 95 rela-
tions involved single-word hyponyms. (We restricted our
attention to these 45 relations because the graph model
was built using only single words as nodes in the graph.)
This set of 45 correct hypernym ?seed-pairs? was ex-
tended by another potential 459 pairs (slightly more than
10 for each seed-pair because if there was a tie for 10th
place both neighbors were used). Of these, 211 (46%)
were judged to be correct hypernym pairs and 248 (54%)
were not.7 This accuracy compares favorably with the ac-
curacy of 40% obtained for the raw hyponymy extraction
experiments in Section 2, suggesting that inferring new
relations by using corpus-based similarities to previously
known relations is more reliable than trying to learn com-
pletely new relations even if they are directly attested in
the corpus. However, our accuracy falls way short of the
figure of 82% reported by Widdows and Dorow (2002).
We believe this is because the classes in (Widdows and
Dorow, 2002) are built from carefully selected seed-
examples: ours are built from an uncontrolled sample
of seed-examples extracted automatically from a corpus.
We outline three cases where this causes a critical differ-
ence.
The ambiguity of ?mass?
One of the correct hyponymy relations extracted in our
experiments in Section 2 was mass < religious service.
Using mass as a seed suggested the following candidates
as potential hyponyms of religious service:
Seed Semantically Similar Words
mass length weight angle shape depth
height range charge size momentum
All these neighbors are related to the ?measurement of
physical property? sense of the word mass rather than the
?religious service? sense. The inferred hyponymy rela-
tions are all incorrect because of this mismatch.
The specific properties of ?nitrogen?
Another true relation we extracted was nitrogen < nu-
trient. Using the same process as above gave the follow-
ing neighbors of nitrogen:
Seed Semantically Similar Words
nitrogen methane dioxide carbon hydrogen methanol
vapour ammonia oxide oxygen monoxide water
These neighboring terms are not in general nutrients,
and the attempt to infer new hyponymy relations is a fail-
7As before, we consider scores of 4 and 3 on our 5-point
scale to be correct and lower scores to be incorrect. The pre-
cision of graph-model results (reported in this section and in
Section 5), unlike those reported elsewhere, are based on the
annotations of a single author.
ure in this case. While the relationship nitrogen < nu-
trient is one of the many facts which go to make up the
vast store of world-knowledge that an educated adult uses
for reasoning, it is not a necessary property of nitrogen
itself, and one could arguably ?know? the meaning of
nitrogen without being aware of this fact. In traditional
lexicographic terms, the fact that nitrogen is a nutrient
might be regarded as part of the differentiae rather than
the genus of nitrogen. Had our seed-pair instead been
nitrogen < gas or nitrogen < chemical element, many
correct hyponymy relations would have been inferred by
our method, and both of these classifications are central
to the meaning of nitrogen.
Accurate levels of abstraction for ?dill?
Finally, even when the hyponymy relationship y < x
used as a seed-case was central to the meaning of y and
all of the neighbors of y were related to this meaning,
they were still not always hyponyms of x but sometimes
members of a more general category. For example, using
the correct seed-pair dill < herb we retrieved the follow-
ing suggested hyponyms for herb:
Seed Semantically Similar Words
dill rind fennel seasoning juice sauce
pepper parsley vinegar oil pur
All of these items are related to dill, but only some of
them are herbs. The other items should also be placed
in the same general area of a taxonomy as dill, but as
cooking ingredients rather than specifically herbs.
In spite of these problems, the algorithm for improv-
ing recall by adding neighbors of the correct hyponyms
worked reasonably well, obtaining 211 correct relation-
ships from 45 seeds, an almost fivefold increase in recall,
with an accuracy of 46%, which is better than that of our
baseline pattern-matching hyponymy extractor.
It is possible that using coordination (such as co-
occurence in lists) as a measure of noun-noun similarity
is well-adapted for this sort of work, because it mainly
extracts ?horizontal? relationships between items of sim-
ilar specificity or similar generality. Continuing the ge-
ometric analogy, these mainly ?horizontal? relationships
might be expected to combine particularly well with seed
examples of ?vertical? relationships, i.e. hyponymy rela-
tionships.
5 Combining LSA and Coordination to
Improve Precision and Recall
Having used two separate techniques to improve preci-
sion and recall in isolation, it made sense to combine
our methods to improve performance overall. This was
accomplished by applying LSA filtering as described in
Section 3 to the results obtained by extending our initial
hypernym pairs with coordination patterns in Section 4.
LSA filtering of extended results: phase I
The first application of filtering to the additional hy-
ponymy relations obtained using noun-cooccurrence was
straightforward. We took the 459 potential hyponymy
relationships obtained in Section 4. For each of the
prospective hyponyms y of a given hypernym x, we com-
puted the LSA similarity sim(x, y). We then considered
only those potential hyponyms whose LSA similarity to
the hypernym surpassed a certain threshhold. Using this
technique with an experimentally determined threshhold
of 0.15, we obtained a set of 260 hyponymy relations of
which 166 were correct (64%, as opposed to the 46%
correct in the unfiltered results). The LSA filtering had
removed 154 incorrect relationships and only 45 correct
ones, reducing the overall error rate by 33%.
In particular, this technique removed all but one of
the spurious religious service hyponyms which were ob-
tained through inappropriate similarities with mass in the
example in Section 4, though it was much less effective
in filtering the neighbors of nitrogen and dill, as might be
expected.
LSA filtering of extended results: phase II
For some of the hyponymy relations to which we ap-
plied our extension technique, the hypernym had multiple
words.8 In some of these cases, it was clear that one of
the words in the hypernym had a meaning more closely
related to the original (correct) hyponym. For instance, in
the mass < religious service relation, the word religious
tells us more about the appropriate meaning of mass than
does the word service. It thus seemed that, at least in cer-
tain cases, we might be able to get more traction in LSA
filtering of potential additional hyponyms by first select-
ing a particular word from the hypernym as the ?most
important? and using that word rather than the entire hy-
pernym for filtering.9
We thus applied a simple two-step algorithm to refine
the filtering technique presented above:
1. The LSA similarity between the original (correct)
hyponym and each word in the hypernym is com-
puted. The words of the hypernym are ranked ac-
cording to these similarities.
2. The word in the hypernym that has the highest LSA
similarity to the original (correct) hyponym is used
instead of the entire hypernym for phase-I-style fil-
tering.
8The graph model used to obtain new candidate hyponyms
was built using single words, which is why our extended results
include some multiword expressions among the hypernyms but
only single word hyponyms.
9When using an entire multiword hypernym for filtering, a
term-vector was produced for the multiword hypernym by aver-
aging the LSA vectors for the constituent words.
This filtering technique, with an LSA-similarity thresh-
hold of 0.15, resulted in the extraction of 35 correct and
25 incorrect relationships. In contrast, using LSA simi-
larity with the whole expression rather than the most im-
portant word resulted in the extraction of 32 correct and
30 incorrect relationships for those hypernyms with mul-
tiple words. On the face of it, selecting only the most
important part of the hypernym for comparison enabled
us to obtain more correct and fewer incorrect relations,
but it is also clear that by this stage in our experiments
our sample of seed-relationships had become too small
for these results to be statistically significant.
However, the examples we considered did demonstrate
another point?that LSA could help to determine which
parts of a multiword expression were semantically rel-
evant. For example, one of the seed-relationships was
France < European Community member. Finding that
sim(france, european) > sim(france, community),
we could infer that the adjective European was central to
the meaning of the hyponym, whereas for the example
wallflowers < hardy biennials the opposite conclusion,
that hardy is an adjectival modifier which isn?t central to
the relationship, could be drawn. However, these conclu-
sions could also be drawn by using established colloca-
tion extraction techniques (Manning and Schu?tze, 1999,
Ch. 5) to find semantically significant multiword expres-
sions.
6 Obtaining Canonical Forms for
Relations
An important part of extracting semantic relations like
those discussed in this paper is converting the terms in
the extracted relations to a canonical form. In the case
of our extracted hyponymy relations, such normalization
consists of two steps:
1. Removing extraneous articles and qualifiers. Our
extracted hyponyms and hypernyms were often in
the form ?another x?, ?some x?, and so forth, where
x is the hypernym or hyponym that we actually want
to consider.
2. Converting nouns to their singular form. This is el-
ementary morphological analysis, or a limited form
of lemmatization.
We performed the second of these steps using the
morph morphological analysis software (Minnen et al,
2001).10 To perform the first step of removing modifiers,
we implemented a Perl script to do the following:
10This software is freely available from
http://www.cogs.susx.ac.uk/lab/nlp/carroll/morph.html.
? Remove leading determiners from the beginning of
the hypernym and from the beginning of the hy-
ponym.
? Remove leading prepositions from the beginning of
the hypernym. Doing this after removing leading
determiners eliminates the common ?those of? con-
struction.
? Remove cardinal numbers from the hypernym and
the hyponym.
? Remove possessive prefixes from the hypernym and
the hyponym.
? Remove ?set of? and ?number of? from the hy-
pernym and the hyponym. This ad hoc but rea-
sonable procedure eliminates common troublesome
constructions not covered by the above rules.
? Remove leading adjectives from hypernyms, but not
from hyponyms. In addition to removing ?other?,
this amounts to playing it safe. By removing leading
adjectives we make potential hypernyms more gen-
eral, and thus more likely to be a superset of their
potential hyponym. While this removal sometimes
makes the learned relationship less useful, it sel-
dom makes it incorrect. We leave adjectives on hy-
ponyms to make them more specific, and thus more
likely to be a subset of their purported hypernym.
Using these simple rules, we were able to convert 73
of the 78 relations orginally scored as 3 (see Section 2)
to relations receiving a score of 4. This demonstrates as
a ?proof of concept? that comparatively simple language
processing techniques can be used to map relationships
from the surface forms in which they were observed in
text to a canonical form which could be included in a se-
mantic resource.
7 Conclusion and Further Work
The results presented in this paper demonstrate that the
application of linguistic information from automatically-
learned mathematical models can significantly enhance
both the precision and the recall of pattern-based hy-
ponymy extraction techniques. Using a graph model of
noun similarity we were able to obtain an almost five-
fold improvement in recall, though the precision of this
technique is clearly affected by the correctness of the
?seed-relationships? used. Using LSA filtering we elimi-
nated spurious relations extracted by the original pattern
method, reducing errors by 30%. Such filtering also elim-
inated spurious relations learned using the graph model
that were the result of lexical ambiguity and of seed hy-
ponymy relations inappropriate for the technique, reduc-
ing errors by 33%.
This paper suggests many possibilities for future work.
First of all, it would be interesting to apply LSA to a sys-
tem for building an entire hypernym-labelled ontology in
roughly the way described in (Caraballo, 1999), perhaps
by using an LSA-weighted voting method to determine
which hypernym would be used to label each node. We
are considering how to extend our techniques to such a
task.
Also, systematic comparison of the lexicosyntactic
patterns used for extraction to determine the relative pro-
ductiveness and accuracy of each pattern might prove
illuminating, as would comparison across different cor-
pora to determine the impact of the topic area and
medium/format of documents on the effectiveness of hy-
ponymy extraction. Ultimately, the ability to predict a
priori how well a knowledge-extraction system will work
on a previously unseen corpus will be crucial to its use-
fulness.
Applying the techniques of this paper to a system that
used mutual bootstrapping (Riloff and Jones, 1999) to
find additional extraction patterns would also be interest-
ing (such an approach is suggested in (Hearst, 1998)).
And of course, further refinement of the mathematical
models we use and our methods of learning them, includ-
ing more sophisticated use of available tools for linguistic
pre-processing, such as the identification and indexing of
multiword expressions, could further improve the preci-
sion and recall of hyponymy extraction techniques.
Acknowledgements
This research was supported in part by the Research
Collaboration between the NTT Communication Science
Laboratories, Nippon Telegraph and Telephone Corpora-
tion and CSLI, Stanford University, and by EC/NSF grant
IST-1999-11438 for the MUCHMORE project. Thanks
also to Stanley Peters for his helpful comments on an ear-
lier draft.
References
Enrique Alfonseca and Suresh Manandhar. 2001. Im-
proving an ontology refinement method with hy-
ponymy patterns. In Third International Conference
on Language Resources and Evaluation, pages 235?
239, Las Palmas, Spain.
Ricardo Baeza-Yates and Berthier Ribiero-Neto. 1999.
Modern Information Retrieval. Addison Wesley /
ACM Press.
Be?la Bolloba?s. 1998. Modern Graph Theory. Num-
ber 184 in Graduate Texts in Mathematics. Springer-
Verlag.
Sharon Caraballo. 1999. Automatic construction of a
hypernym-labeled noun hierarchy from text. In 37th
Annual Meeting of the Association for Computational
Linguistics: Proceedings of the Conference, pages
120?126.
Claire Cardie. 1997. Empirical methods in information
extraction. AI Magazine, 18:65?79.
Scott Deerwester, Susan Dumais, George Furnas,
Thomas Landauer, and Richard Harshman. 1990. In-
dexing by latent semantic analysis. Journal of the
American Society for Information Science, 41(6):391?
407.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge MA.
L Guthrie, J Pustejovsky, Y Wilks, and B Slator. 1996.
The role of lexicons in natural language processing.
Communications of the ACM, 39(1):63?72.
Udo Hahn and Klemens Schnattinger. 1998. Towards
text knowledge engineering. In AAAI/IAAI, pages
524?531.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In COLING, Nantes,
France.
Marti A. Hearst, 1998. WordNet: An Electronic Lexical
Database, chapter 5, Automated discovery of WordNet
relations, pages 131?152. MIT Press, Cambridge MA.
Christopher D. Manning and Hinrich Schu?tze. 1999.
Foundations of Statistical Natural Language Process-
ing. The MIT Press, Cambridge, Massachusetts.
Guido Minnen, John Carroll, and Darren Pearce. 2001.
Applied morphological processing of english. Natural
Language Engineering, 7(3):207?223.
Ellen Riloff and Rosie Jones. 1999. Learning dictionar-
ies for infomation extraction by multi-level bootstrap-
ping. In Proceedings of the Sixteenth National Confer-
ence on Artificial Intelligence, pages 472?479. AAAI.
Ellen Riloff and Jessica Shepherd. 1997. A corpus-based
approach for building semantic lexicons. In Claire
Cardie and Ralph Weischedel, editors, Proceedings of
the Second Conference on Empirical Methods in Natu-
ral Language Processing, pages 117?124. Association
for Computational Linguistics, Somerset, New Jersey.
Brian Roark and Eugene Charniak. 1998. Noun-phrase
co-occurence statistics for semi-automatic semantic
lexicon construction. In COLING-ACL, pages 1110?
1116.
Hinrich Schu?tze. 1998. Automatic word sense discrimi-
nation. Computational Linguistics, 24(1):97?124.
Dominic Widdows and Beate Dorow. 2002. A graph
model for unsupervised lexical acquisition. In 19th In-
ternational Conference on Computational Linguistics,
pages 1093?1099, Taipei, Taiwan, August.
Unsupervised Monolingual and Bilingual Word-Sense
Disambiguation of Medical Documents using UMLS
Dominic Widdows, Stanley Peters, Scott Cederberg, Chiu-Ki Chan
Stanford University, California
{dwiddows,peters,cederber,ckchan}@csli.stanford.edu
Diana Steffen
Consultants for Language Technology,
Saarbru?cken, Germany
steffen@clt-st.de
Paul Buitelaar
DFKI, Saarbru?cken, Germany
paulb@dfki.de
Abstract
This paper describes techniques for unsu-
pervised word sense disambiguation of En-
glish and German medical documents us-
ing UMLS. We present both monolingual
techniques which rely only on the structure
of UMLS, and bilingual techniques which
also rely on the availability of parallel cor-
pora. The best results are obtained using
relations between terms given by UMLS,
a method which achieves 74% precision,
66% coverage for English and 79% preci-
sion, 73% coverage for German on evalua-
tion corpora and over 83% coverage over the
whole corpus. The success of this technique
for German shows that a lexical resource
giving relations between concepts used to
index an English document collection can
be used for high quality disambiguation in
another language.
1 Introduction
This paper reports on experiments in monolingual
and multilingual word sense disambiguation (WSD)
in the medical domain using the Unified Medical
Language System (UMLS). The work described was
carried out as part of the MUCHMORE project 1 for
multilingual organisation and retrieval of medical in-
formation, for which WSD is particularly important.
The importance of WSD to multilingual applica-
tions stems from the simple fact that meanings repre-
sented by a single word in one language may be rep-
resented by multiple words in other languages. The
English word drug when referring to medically ther-
apeutic drugs would be translated as medikamente,
1http://muchmore.dfki.de
while it would be rendered as drogen when referring
to a recreationally taken narcotic substance of the
kind that many governments prohibit by law.
The ability to disambiguate is therefore essential
to the task of machine translation ? when translat-
ing from English to Spanish or from English to Ger-
man we would need to make the distinctions men-
tioned above and other similar ones. Even short of
the task of full translation, WSD is crucial to ap-
plications such as cross-lingual information retrieval
(CLIR), since search terms entered in the language
used for querying must be appropriately rendered in
the language used for retrieval. WSD has become a
well-established subfield of natural language process-
ing with its own evaluation standards and SENSE-
VAL competitions (Kilgarriff and Rosenzweig, 2000).
Methods for WSD can effectively be divided into
those that require manually annotated training data
(supervised methods) and those that do not (unsu-
pervised methods) (Ide and Ve?ronis, 1998). In gen-
eral, supervised methods are less scalable than unsu-
pervised methods because they rely on training data
which may be costly and unrealistic to produce, and
even then might be available for only a few ambigu-
ous terms. The goal of our work on disambiguation
in the MUCHMORE project is to enable the correct
semantic annotation of entire document collections
with all terms which are potentially relevant for or-
ganisation, retrieval and summarisation of informa-
tion. Therefore a decision was taken early on in the
project that we should focus on unsupervised meth-
ods, which have the potential to be scaled up enough
to meet our needs.
This paper is arranged as follows. In Section 2 we
describe the lexical resource (UMLS) and the cor-
pora we used for our experiments. We then describe
and evaluate three different methods for disambigua-
tion. The bilingual method (Section 3) takes ad-
vantage of our having a translated corpus, because
knowing the translation of an ambiguous word can
be enough to determine its sense. The collocational
method (Section 4) uses the occurence of a term in a
recognised fixed expression to determine its meaning.
UMLS relation based methods (Section 5) use rela-
tions between terms in UMLS to determine which
sense is being used in a particular instance. Other
techniques used in the MUCHMORE project in-
clude domain-specific sense selection (Buitelaar and
Sacaleanu, 2001), used to select senses appropri-
ate to the medical domain from a general lexical
resource, and instance-based learning, a machine-
learning technique that has been adapted for word-
sense disambiguation (Widdows et al, 2003).
2 Language resources used in these
experiments
2.1 Lexical Resource ? UMLS
The Unified Medical Language System (UMLS) is
a resource that contains linguistic, terminological
and semantic information in the medical domain.2
It is organised in three parts: Specialist Lexi-
con, MetaThesaurus and Semantic Network. The
MetaThesaurus contains concepts from more than
60 standardised medical thesauri, of which for our
purposes we only use the concepts from MeSH (the
Medical Subject Headings thesaurus). This decision
is based on the fact that MeSH is also available in
German. The semantic information that we use in
annotation is the so-called Concept Unique Identifier
(CUI), a code that represents a concept in the UMLS
MetaThesaurus. We consider the possible ?senses? of
a term to be the set of CUI?s which list this term
as a possible realisation. For example, UMLS con-
tains the term trauma as a possible realisation of the
following two concepts:
C0043251 Injuries and Wounds: Wounds
and Injuries: trauma: traumatic disorders:
Traumatic injury:
C0021501 Physical Trauma: Trauma
(Physical): trauma:
Each of these CUI?s is a possible sense of the term
trauma. The term trauma is therefore noted as am-
biguous, since it can be used to express more than
one UMLS concept. The purpose of disambiguation
is to find out which of these possible senses is ac-
tually being used in each particular context where
there term trauma is used.
2UMLS is freely available under license from
the United States National Library of Medicine,
http://www.nlm.nih.gov/research/umls/
CUI?s in UMLS are also interlinked to each other
by a number of relations. These include:
? ?Broader term? which is similar to the hyper-
nymy relation in WordNet (Fellbaum, 1998). In
general, x is a ?broader term? for y if every y is
also a (kind of) x.
? More generally, ?related terms? are listed, where
possible relationships include ?is like?, ?is clini-
cally associated with?.
? Cooccurring concepts, which are pairs of con-
cepts which are linked in some information
source. In particular, two concepts are regarded
as cooccurring if they have both been used to
manually index the same document in MED-
LINE. We will refer to such pairs of concepts
as coindexing concepts.
? Collocations and multiword expressions. For ex-
ample, the term liver transplant is included sep-
arately in UMLS, as well as both the terms liver
and transplant. This information can sometimes
be used for disambiguation.
2.2 The Springer Corpus of Medical
Abstracts
The experiments and implementations of WSD de-
scribed in this paper were all carried out on a par-
allel corpus of English-German medical scientific ab-
stracts obtained from the Springer Link web site.3
The corpus consists approximately of 1 million to-
kens for each language. Abstracts are from 41 medi-
cal journals, each of which constitutes a relatively ho-
mogeneous medical sub-domain (e.g. Neurology, Ra-
diology, etc.). The corpus was automatically marked
up with morphosyntactic and semantic information,
as described by S?pela Vintar et al (2002). In brief,
whenever a token is encountered in the corpus that is
listed as a term in UMLS, the document is annotated
with the CUI under which that term is listed. Ambi-
guity is introduced by this markup process because
the lexical resources often list a particular term as a
possible realisation of more than one concept or CUI,
as with the trauma example above, in which case
the document is annotated with all of these possible
CUI?s.
The number of tokens of UMLS terms included by
this annotation process is given in Table 1. The table
shows how many tokens were found by the annota-
tion process, listed according to how many possible
senses each of these tokens was assigned in UMLS (so
that the number of ambiguous tokens is the number
3http://link.springer.de/
Number of Senses 1 2 3 4
Before Disambiguation
English 223441 31940 3079 56
German 124369 7996 0 0
After Disambiguation
English 252668 5299 568 5
German 131302 1065 0 0
Table 1: The number of tokens of terms that have 1,
2, 3 and 4 possible senses in the Springer corpus
of tokens with more than one possible sense). The
greater number of concepts found in the English cor-
pus reflects the fact that UMLS has greater cover-
age for English than for German, and secondly that
there are many small terms in English which are ex-
pressed by single words which would be expressed
by larger compound terms in German (for exam-
ple knee + joint = kniegelenk). Table 1 also shows
how many tokens of UMLS concepts were in the an-
notated corpus after we applied the disambiguation
process described in Section 5, which proved to be
our most successful method. As can be seen, our
disambiguation methods resolved some 83% of the
ambiguities in the English corpus and 87% of the
ambiguities in the German corpus (we refer to this
proportion as the ?Coverage? of the method). How-
ever, this only measures the number of disambigua-
tion decisions that were made: in order to determine
how many of these decisions were correct, evaluation
corpora were needed.
2.3 Evaluation Corpora
An important aspect of word sense disambiguation is
the evaluation of different methods and parameters.
Unfortunately, there is a lack of test sets for evalu-
ation, specifically for languages other than English
and even more so for specific domains like medicine.
Given that our work focuses on German as well as
English text in the medical domain, we had to de-
velop our own evaluation corpora in order to test our
disambiguation methods.
Because in the MUCHMORE project we devel-
oped an extensive format for linguistic and semantic
annotation (S?pela Vintar et al, 2002) that includes
annotation with UMLS concepts, we could automat-
ically generate lists of all ambiguous UMLS types
(English and German) along with their token fre-
quencies in the corpus. Using these lists we selected a
set of 70 frequent types for English (token frequencies
at least 28, 41 types having token frequencies over
100). For German, we only selected 24 ambiguous
types (token frequencies at least 11, 7 types having
token frequencies over 100) because there are fewer
ambiguous terms in the German annotation (see Ta-
ble 1). We automatically selected instances to be
annotated using a random selection of occurrences if
the token frequency was higher than 100, and using
all occurrences if the token frequency was lower than
100. The level of ambiguity for these UMLS terms is
mostly limited to only 2 senses; only 7 English terms
have 3 senses.
Correct senses of the English tokens in context
were chosen by three medical experts, two native
speakers of German and one of English. The Ger-
man evaluation corpus was annotated by the two
German speakers. Interannotator agreement for in-
dividual terms ranged from very low to very high,
with an average of 65% for German and 51% for En-
glish (where all three annotators agreed). The rea-
sons for this low score are still under investigation.
In some cases, the UMLS definitions were insufficient
to give a clear distinction between concepts, espe-
cially when the concepts came from different origi-
nal thesauri. This allowed the decision of whether
a particular definition gave a meaningful ?sense? to
be more or less subjective. Approximately half of
the disagreements between annotators occured with
terms where interannotator agreement was less than
10%, which is evidence that a significant amount of
the disagreement between annotators was on the type
level rather than the token level. In other cases, it
is possible that there was insufficient contextual in-
formation provided for annotators to agree. If one of
the annotators was unable to choose any of the senses
and declared an instance to be ?unspecified?, this also
counted against interannotator agreement. What-
ever is responsible, our interannotator agreement fell
far short of the 88%-100% achieved in SENSEVAL
(Kilgarriff and Rosenzweig, 2000, ?7), and until this
problem is solved or better datasets are found, this
poor agreement casts doubt on the generality of the
results obtained in this paper.
A ?gold standard? was produced for the German
UMLS evaluation corpus and used to evaluate the
disambiguation of German UMLS concepts. The En-
glish experiments were evaluated on those tokens for
which the annotators agreed. More details and dis-
cussion of the annotation process is available in the
project report (Widdows et al, 2003).
In the rest of this paper we describe the techniques
that used these resources to build systems for word
sense disambiguation, and evaluate their level of suc-
cess.
3 Bilingual Disambiguation
The mapping between word-forms and senses differs
across languages, and for this reason the importance
of word-sense disambiguation has long been recog-
nised for machine translation. By the same token,
pairs of translated documents naturally contain in-
formation for disambiguation. For example, if in a
particular context the English word drugs is trans-
lated into French as drogues rather than medica-
ments, then the English word drug is being used
to mean narcotics rather than medicines. This ob-
servation has been used for some years on varying
scales. Brown et al (1991) pioneered the use of sta-
tistical WSD for translation, building a translation
model from one million sentences in English and
French. Using this model to help with translation
decisions (such as whether prendre should be trans-
lated as take or make), the number of acceptable
translations produced by their system increased by
8%. Gale et al (1992) use parallel translations to
obtain training and testing data for word-sense dis-
ambiguation. Ide (1999) investigates the information
made available by a translation of George Orwell?s
Nineteen Eighty-four into six languages, using this
to analyse the related senses of nine ambiguous En-
glish words into hierarchical clusters.
These applications have all been case studies of a
handful of particularly interesting words. The large
scale of the semantic annotation carried out by the
MUCHMORE project has made it possible to extend
the bilingual disambiguation technique to entire dic-
tionaries and corpora.
To disambiguate an instance of an ambiguous
term, we consulted the translation of the abstract
in which it appeared. We regarded the translated
abstract as disambiguating the ambiguous term if it
met the following two criteria:
? Only one of the CUI?s was assigned to any term
in the translated abstract.
? At least one of the terms to which this CUI
was assigned in the translated abstract was un-
ambiguous (i.e. was not also assigned another
CUI).
3.1 Results for Bilingual Disambiguation
We attempted both to disambiguate terms in the
German abstracts using the corresponding English
abstracts, and to disambiguate terms in the English
abstracts using the corresponding German ones. In
this collection of documents, we were able to disam-
biguate 1802 occurrences of 63 English terms and
1500 occurrences of 43 German terms. Comparing
this with the evaluation corpora gave the results in
Table 2.4
4In all of the results presented in this paper, Precision
is the proportion of decisions made which were correct
Precision Recall Coverage
English 81% 18% 22%
German 66% 22% 33%
Table 2: Results for bilingual disambiguation
As can be seen, the recall and coverage of this
method is not especially good but the precision (at
least for English) is very high. The German results
contain roughly the same proportion of correct deci-
sions as the English, but many more incorrect ones
as well.
Our disambiguation results break down into three
cases:
1. Terms ambiguous in one language that translate
as multiple unambiguous terms in the other lan-
guage; one of the meanings is medical and the
other is not.
2. Terms ambiguous in one language that trans-
late as multiple unambiguous terms in the other
language; both of the terms are medical.
3. Terms that are ambiguous between two mean-
ings that are difficult to distinguish.
One striking aspect of the results was that rel-
atively few terms were disambiguated to different
senses in different occurrences. This phenomenon
was particularly extreme in disambiguating the Ger-
man terms; of the 43 German terms disambiguated,
42 were assigned the same sense every time we were
able to disambiguate them. Only one term, Metas-
tase, was assigned difference senses; 88 times it was
assigned CUI C0027627 (?The spread of cancer from
one part of the body to another ...?, associated with
the English term Metastasis and 6 times it was as-
signed CUI C0036525 ?Used with neoplasms to in-
dicate the secondary location to which the neoplas-
tic process has metastasized?, corresponding to the
English terms metastastic and secondary). Metas-
tase therefore falls into category 2 from above, al-
though the distinction between the two meanings is
relatively subtle.
The first and third categories above account for
the vast majority of cases, in which only one mean-
ing is ever selected. It is easy to see why this would
according to the evaluation corpora, Recall is the pro-
portion of instances in the evaluation corpora for which
a correct decision was made, and Coverage is the propor-
tion of instances in the evaluation corpora for which any
decision was made. It follows that
Recall = Precision ? Coverage.
happen in the first category, and it is what we want
to happen. For instance, the German term Krebse
can refer either to crabs (Crustaceans) or to cancer-
ous growths; it is not surprising that only the latter
meaning turns up in the corpus under consideration
and that we can determine this from the unambigu-
ous English translation cancers.
In English somewhat more terms were disam-
biguated multiple ways: eight terms were assigned
two different senses across their occurrences. All
three types of ambiguity were apparent. For in-
stance, the second type (medical/medical ambiguity)
appeared for the term Aging, which can refer either
to aging people (Alte Menschen) or to the process of
aging itself (Altern); both meanings appeared in our
corpus.
In general, the bilingual method correctly find the
meanings of approximately one fifth of the ambigu-
ous terms, and makes only a few mistakes for English
but many more for German.
4 Collocational disambiguation
By a ?collocation? we mean a fixed expression formed
by a group of words occuring together, such as
blood vessel or New York. (For the purposes of
this paper we only consider contiguous multiword
expressions which are listed in UMLS.) There is a
strong and well-known tendency for words to ex-
press only one sense in a given collocation. This
property of words was first described and quantified
by Yarowsky (1993), and has become known gen-
erally as the ?One Sense Per Collocation? property.
Yarowsky (1995) used the one sense per collocation
property as an essential ingredient for an unsuper-
vised Word-Sense Disambiguation algorithm. For ex-
ample, the collocations plant life and manufacturing
plant are used as ?seed-examples? for the living thing
and building senses of plant, and these examples can
then be used as high-precision training data to per-
form more general high-recall disambiguation.
While Yarowsky?s algorithm is unsupervised (the
algorithm does not need a large collection of anno-
tated training examples), it still needs direct human
intervention to recognise which ambiguous terms are
amenable to this technique, and to choose appropri-
ate ?seed-collocations? for each sense. Thus the algo-
rithm still requires expert human judgments, which
leads to a bottleneck when trying to scale such meth-
ods to provide Word-Sense Disambiguation for a
whole document collection.
A possible method for widening this bottleneck is
to use existing lexical resources to provide seed collo-
cations. The texts of dictionary definitions have been
used as a traditional source of information for disam-
biguation (Lesk, 1986). The richly detailed structure
of UMLS provides a special opportunity to combine
both of these approaches, because many multiword
expressions and collocations are included in UMLS
as separate concepts.
For example, the term pressure has the following
three senses in UMLS, each of which is assigned to a
different semantic type (TUI):
Sense of pressure Semantic Type
Physical pressure Quantitative Concept
(C0033095)
Pressure - action Therapeutic or
(C0460139) Preventive Procedure
Baresthesia, sensation
of pressure (C0234222)
Organ or Tissue Func-
tion
Many other collocations and compounds which in-
clude the word pressure are also of these semantic
types, as summarised in the following table:
Quantitative
Concept
mean pressure, bar pressure,
population pressure
Therapeutic
Procedure
orthostatic pressure, acupres-
sure
Organ or Tissue
Function
arterial pressure, lung pres-
sure, intraocular pressure
This leads to the hypothesis that the term pres-
sure, when used in any of these collocations, is used
with the meaning corresponding to the same seman-
tic type. This allows deductions of the following
form:
Collocation bar pressure, mean pressure
Semantic type Quantitative Concept
Sense of pressure C0033095, physical pressure
Since nearly all English and German multiword
technical medical terms are head-final, it follows that
the a multiword term is usually of the same seman-
tic type as its head, the final word. (So for example,
lung cancer is a kind of cancer, not a kind of lung.)
For English, UMLS 2001 contains over 800,000 multi-
word expressions the last word in which is also a term
in UMLS. Over 350,000 of these expressions have a
last word which on its own, with no other context,
would be regarded as ambiguous (has more that one
CUI in UMLS). Over 50,000 of these multiword ex-
pressions are unambiguous, with a unique semantic
type which is shared by only one of the meanings of
the potentially ambiguous final word. The ambigu-
ity of the final word in such multiword expressions
is thus resolved, providing over 50,000 ?seed colloca-
tions? for use in semantically annotating documents
with disambiguated word senses.
4.1 Results for collocational disambiguation
Unfortunately, results for collocational disambigua-
tion (Table 3) were disappointing compared with the
promising number of seed collocations we expected
to find. Precision was high, but comparatively few
of the collocations suggested by UMLS were found
in the Springer corpus.
Precision Recall Coverage
English 79% 3% 4%
German 82% 1% 1.2%
Table 3: Results for collocational disambiguation
In retrospect, this may not be surprising given that
many of the ?collocations? in UMLS are rather col-
lections of words such as
C0374270 intracoronary percutaneous
placement s single stent transcatheter vessel
which would almost never occur in natural text.
Thus very few of the potential collocations we ex-
tracted from UMLS actually occurred in the Springer
corpus. This scarcity was especially pronounced for
German, because so many terms which are several
words in English are compounded into a single word
in German. For example, the term
C0035330 retinal vessel
does occur in the (English) Springer corpus and con-
tains the ambiguous word vessel, whose ambiguity is
successfully resolved using the collocational method.
However, in German this concept is represented by
the single word
C0035330 Retinagefaesse
and so this ambiguity never arises in the first place.
It should still be remarked that the few decisions
that were made by the collocational method were
very accurate, demonstrating that we can get some
high precision results using this method. It is pos-
sible that recall could be improved by relaxing the
conditions which a multiword expression in UMLS
must satisfy to be used as a seed-collocation.
5 Disambiguation using related
UMLS terms found in the same
context
While the collocational method turned out to give
disappointing recall, it showed that accurate infor-
mation could be extracted directly from the existing
UMLS and used for disambiguation, without extra
human intervention or supervision. What we needed
was advice on how to get more of this high-quality
information out of UMLS, which we still believed to
be a very rich source of information which we were
not yet exploiting fully. Fortunately, no less than 3
additional sources of information for disambiguation
using related terms from UMLS were suggested by a
medical expert.5 The suggestion was that we should
consider terms that were linked by conceptual rela-
tions (as given by the MRREL and MRCXT files
in the UMLS source) and which were noted as coin-
dexing concepts in the same MEDLINE abstract (as
given by the MRCOC file in the UMLS source). For
each separate sense of an ambiguous word, this would
give a set of related concepts, and if examples of any
of these related concepts were found in the corpus
near to one of the ambiguous words, it might indi-
cate that the correct sense of the ambiguous word
was the one related to this particular concept.
This method is effectively one of the many variants
of Lesk?s (1986) original dictionary-based method for
disambiguation, where the words appearing in the
definitions of different senses of ambiguous words are
used to indicate that those senses are being used if
they are observed near the ambiguous word. How-
ever, we gain over purely dictionary-based methods
because the words that occur in dictionary defini-
tions rarely correspond well with those that occur
in text. The information we collected from UMLS
did not suffer from this drawback: the pairs of coin-
dexing concepts from MRCOC were derived precisely
from human judgements that these two concepts
both occured in the same text in MEDLINE.
The disambiguation method proceeds as follows.
For each ambiguous word w, we find its possible
senses {sj(w)}. For each sense sj , find all CUI?s
in MRREL, MRCXT or MRCOC files that are re-
lated to this sense, and call this set {crel(sj)}. Then
for each occurrence of the ambiguous word w in the
corpus we examine the local context to see if a term
t occurs whose sense6 (CUI) is one of the concepts
in {crel(sj)}, and if so take this as positive evidence
that the sense sj is the appropriate one for this con-
text, by increasing the score of sj by 1. In this way,
each sense sj in context gets assigned a score which
measures the number of terms in this context which
are related to this sense. Finally, choose the sense
5Personal communication from Stuart Nelson (instru-
mental in the design of UMLS), at the MUCHMORE
workshop in Croatia, September 2002.
6This fails to take into account that the term t might
itself be ambiguous ? it is possible that results could be
improved still further by allowing for mutual disambigua-
tion of more than one term at once.
with the highest score.
One open question for this algorithm is what re-
gion of text to use as a context-window. We experi-
mented with using sentences, documents and whole
subdomains, where a ?subdomain? was considered to
be all of the abstracts appearing in one of the jour-
nals in the Springer corpus, such as Arthroskopie
or Der Chirurg. Thus our results (for each lan-
guage) vary according to which knowledge sources
were used (Conceptually Related Terms from MR-
REL and MRCXT or coindexing terms from MR-
COC, or a combination), and according to whether
the context-window for recording cooccurence was a
sentence, a document or a subdomain.
5.1 Results for disambiguation based on
related UMLS concepts
The results obtained using this method (Tables 5.1
and 5.1) were excellent, preserving (and in some
cases improving) the high precision of the bilingual
and collocational methods while greatly extending
coverage and recall. The results obtained by using
the coindexing terms for disambiguation were partic-
ularly impressive, which coincides with a long-held
view in the field that terms which are topically re-
lated to a target word can be much richer clues for
disambiguation that terms which are (say) hierarchi-
cally related. We are very fortunate to have such
a wealth of information about the cooccurence of
pairs of concepts through UMLS, which appears to
have provided the benefits of cooccurence data from
a manually annotated training sample without hav-
ing to perform the costly manual annotation.
In particular, for English (Table 5.1), results were
actually better using only coindexing terms rather
than combining this information with hierarchically
related terms: both precision and recall are best
when using only the MRCOC knowledge source. As
we had expected, recall and coverage increased but
precision decreased slightly when using larger con-
texts.
The German results (Table 5.1) were slightly dif-
ferent, and even more successful, with nearly 60% of
the evaluation corpus being correctly disambiguated,
nearly 80% of the decisions being correct. Here, there
was some small gain when combining the knowledge
sources, though the results using only coindexing
terms were almost as good. For the German experi-
ments, using larger contexts resulted in greater recall
and greater precision. This was unexpected ? one
hypothesis is that the sparser coverage of the German
UMLS contributed to less predictable results on the
sentence level.
These results are comparable with some of the bet-
ter SENSEVAL results (Kilgarriff and Rosenzweig,
2000) which used fully supervised methods, though
the comparison may not be accurate because we are
choosing between fewer senses than on avarage in
SENSEVAL, and because of the doubts over our in-
terannotator agreement.
Comparing these results with the number of words
disambiguated in the whole corpus (Table 1), it is
apparent that the average coverage of this method is
actually higher for the whole corpus (over 80%) than
for the words in the evaluation corpus. It is possible
that this reflects the fact the the evaluation corpus
was specifically chosen to include words with ?inter-
esting? ambiguities, which might include words which
are more difficult than average to disambiguate. It is
possible that over the whole corpus, the method ac-
tually works even better than on just the evaluation
corpus.
This technique is quite groundbreaking, because it
shows that a lexical resource derived almost entirely
from English data (MEDLINE indexing terms) could
successfully be used for automatic disambiguation in
a German corpus. (The alignment of documents and
their translations was not even considered for these
experiments so the results do not depend at all on
our having access to a parallel corpus.) This is be-
cause the UMLS relations are defined between con-
cepts rather than between words. Thus if we know
that there is a relationship between two concepts, we
can use that relationship for disambiguation, even if
the original evidence for this relationship was derived
from information in a different language from the
language of the document we are seeking to disam-
biguate. We are assigning the correct senses based
not upon how terms are related in language, but how
medical concepts are related to one another.
It follows that this technique for disambiguation
should be applicable to any language which UMLS
covers, and applicable at very little cost. This pro-
posal should stimulate further research, and not too
far behind, successful practical implementation.
6 Summary and Conclusion
We have described three implementations of unsu-
pervised word-sense disambiguation techniques for
medical documents. The bilingual method relies on
the availability of a translated parallel corpus: the
collocational and relational methods rely solely on
the structure of UMLS, and could therefore be ap-
plied to new collections of medical documents with-
out requiring any new resources. The method of
disambiguation using relations between terms given
by UMLS was by far the most successful method,
achieving 74% precision, 66% coverage for English
ENGLISH Related terms Related terms Coindexing terms Combined
RESULTS (MRREL) (MRCXT) (MRCOC) (majority voting)
Prec. Rec. Cov. Prec. Rec. Cov. Prec. Rec. Cov. Prec. Rec. Cov.
Sentence 50 14 28 60 9 15 78 32 41 74 32 43
Document 48 24 50 63 22 35 74 46 62 72 45 63
Subdomain 51 33 65 64 38 59 74 49 66 71 49 69
Table 4: Results for disambiguation based on UMLS relations (English)
GERMAN Related terms Related terms Coindexing terms Combined
RESULTS (MRREL) (MRCXT) (MRCOC) (majority voting)
Prec. Rec. Cov. Prec. Rec. Cov. Prec. Rec. Cov. Prec. Rec. Cov.
Sentence 64 24 38 75 11 15 76 29 38 77 31 40
Document 68 43 63 75 27 36 79 52 66 79 53 67
Subdomain 70 51 73 74 52 70 79 58 73 79 58 73
Table 5: Results for disambiguation based on UMLS relations (German)
and 79% precision, 73% coverage for German on the
evaluation corpora, and achieving over 80% coverage
overall. This result for German is particularly en-
couraging, because is shows that a lexical resource
giving relations between concepts in one language
can be used for high quality disambiguation in an-
other language.
Acknowledgments
This research was supported in part by the Re-
search Collaboration between the NTT Communi-
cation Science Laboratories, Nippon Telegraph and
Telephone Corporation and CSLI, Stanford Univer-
sity, and by EC/NSF grant IST-1999-11438 for the
MUCHMORE project.
We would like to thank the National Library of
Medicine for providing the UMLS, and in particular
Stuart Nelson for his advice and guidance.
References
P. Brown, S. de la Pietra, V. de la Pietra, and R Mer-
cer. 1991. Word sense disambiguation using sta-
tistical methods. In ACL 29, pages 264?270.
Paul Buitelaar and Bogdan Sacaleanu. 2001. Rank-
ing and selecting synsets by domain relevance. In
Proceedings of WordNet and Other Lexical Re-
sources, NAACL 2001 Workshop, Pittsburgh, PA,
June.
Christiane Fellbaum, editor. 1998. WordNet: An
Electronic Lexical Database. MIT Press, Cam-
bridge MA.
W. Gale, K. Church, and D. Yarowsky. 1992. A
method for disambiguating word senses in a large
corpus. Computers and the Humanities, 26:415?
439.
Nancy Ide and Jean Ve?ronis. 1998. Introduction
to the special issue on word sense disambiguation:
The state of the art. Computational Linguistics,
24(1):1?40, March.
Nancy Ide. 1999. Parallel translations and
sense discriminators. In Proceedings of the ACL
SIGLEX workshop on Standardizing Lexical Re-
sources, pages 52?61.
A. Kilgarriff and J. Rosenzweig. 2000. Framework
and results for english senseval. Computers and
the Humanities, 34(1-2):15?48, April.
M. E. Lesk. 1986. Automated sense disambiguation
using machine-readable dictionaries: How to tell a
pine cone from an ice cream cone. In Proceedings
of the SIGDOC conference. ACM.
S?pela Vintar, Paul Buitelaar, Ba?rbel Ripplinger,
Bogdan Sacaleanu, Diana Raileanu, and Detlef
Prescher. 2002. An efficient and flexible format
for linguistic and semantic annotation. In Third
International Language Resources and Evaluation
Conference, Las Palmas, Spain.
Dominic Widdows, Diana Steffen, Scott Ceder-
berg, Chiu-Ki Chan, Paul Buitelaar, and Bog-
dan Sacaleanu. 2003. Methods for word-sense
disambiguation. Technical report, MUCHMORE
project report.
David Yarowsky. 1993. One sense per collocation.
In ARPA Human Language Technology Workshop,
pages 266?271, Princeton, NJ.
David Yarowsky. 1995. Unsupervised word sense
disambiguation rivaling supervised methods. In
Proceedings of the 33rd Annual Meeting of the
Association for Computational Linguistics, pages
189?196.
