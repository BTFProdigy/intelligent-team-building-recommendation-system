Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 137?140,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Mining Wikipedia Revision Histories for Improving Sentence Compression
Elif Yamangil Rani Nelken
School of Engineering and Applied Sciences
Harvard University
Cambridge, MA 02138, USA
{elif,nelken}@eecs.harvard.edu
Abstract
A well-recognized limitation of research on
supervised sentence compression is the dearth
of available training data. We propose a new
and bountiful resource for such training data,
which we obtain by mining the revision his-
tory of Wikipedia for sentence compressions
and expansions. Using only a fraction of the
available Wikipedia data, we have collected
a training corpus of over 380,000 sentence
pairs, two orders of magnitude larger than the
standardly used Ziff-Davis corpus. Using this
newfound data, we propose a novel lexical-
ized noisy channel model for sentence com-
pression, achieving improved results in gram-
maticality and compression rate criteria with a
slight decrease in importance.
1 Introduction
With the increasing success of machine translation
(MT) in recent years, several researchers have sug-
gested transferring similar methods for monolingual
text rewriting tasks. In particular, Knight and Marcu
(2000) (KM) applied a channel model to the task of
sentence compression ? dropping words from an in-
dividual sentence while retaining its important in-
formation, and without sacrificing its grammatical-
ity. Compressed sentences can be useful either on
their own, e.g., for subtitles, or as part of a larger
summarization or MT system. A well-recognized
problem of this approach, however, is data spar-
sity. While bilingual parallel corpora are abundantly
available, monolingual parallel corpora, and espe-
cially collections of sentence compressions are van-
ishingly rare. Indeed, most work on sentence com-
pression has used the Ziff-Davis corpus (Knight and
Marcu, 2000), which consists of a mere 1067 sen-
tence pairs. While data sparsity is a common prob-
lem of many NLP tasks, it is much more severe for
sentence compression, leading Turner and Charniak
(2005) to question the applicability of the channel
model for this task altogether.
Our contribution in this paper is twofold. First,
we solve the data sparsity issue by showing that
abundant sentence compressions can be extracted
from Wikipedia?s revision history. Second, we use
this data to validate the channel model approach
for text compression, and improve upon it by cre-
ating a novel fully lexicalized compression model.
Our model improves grammaticality and compres-
sion rate with only a slight decrease in importance.
2 Data: Wikipedia revision histories as a
source of sentence compressions
Many researchers are increasingly turning to
Wikipedia as a large-scale data source for training
NLP systems. The vast majority of this work uses
only the most recent version of the articles. In fact,
Wikipedia conveniently provides not only the lat-
est version, but the entire revision history of each
of its articles, as dramatically visualized by Vie?gas
et al (2004). Through Wikipedia?s collaborative
editing process, articles are iteratively amended and
refined by multiple Web users. Users can usually
change any aspect of the document?s structure and
content, but for our purposes here, we focus only on
sentence-level edits that add or drop words.
We have downloaded the July snapshot of the
137
English Wikipedia, consisting of 1.4 million arti-
cles, and mined a subset of them for such compres-
sions/expansions. We make the simplifying assump-
tion that all such edits also retain the core mean-
ing of the sentence, and are therefore valid training
data for our purposes. This assumption is of course
patently na??ve, as there are many cases in which such
revisions reverse sentence meaning, add or drop es-
sential information, are part of a flame war, etc.
Classifying these edits is an interesting task which
we relegate to future work.1
From about one-third of the snapshot, we ex-
tracted over 380,000 sentence pairs, which is 2 or-
ders of magnitude more than the Ziff-Davis corpus.2
Wikipedia currently has 2.3 million articles and is
constantly expanding. We can therefore expect an
increase of another order of magnitude. We thus can
afford to be extremely selective of the sentence pairs
we use. To handle a dataset of such size (hundreds of
GBs), we split it into smaller chunks, and distribute
all the processing.
More technically, for each article, we first extract
all revisions, and split each revision into a list of its
sentences. We run an edit-distance comparison be-
tween each such pair, treating each sentence as an
atomic ?letter?. We look for all replacements of one
sentence by another and check whether one sentence
is a compression of the other.3 We then run Collins?
parser (1997), using just the sentence pairs where
parsing succeeds with a negative log likelihood be-
low 200.
3 Noisy channel model
We follow KM in modeling the problem using a gen-
erative noisy channel model, but use the new-found
training data to lexicalize the model. Sentences start
their life in short form, s, are ranked by a source
language model, p(s), and then probabilistically ex-
panded to form the long sentence, p(l|s). During
decoding, given a long sentence, we seek the most
likely short sentence that could have generated it.
1For instance, compressions are more likely to signal op-
tional information than expansions; the lexical items added are
likely to be indicative of the type of edit, etc.
2The sentence pair corpus is available by contacting the
authors.
3We ignore word reorderings or replacements that are be-
yond word addition or deletion.
Using Bayes? rule, this is equivalent to seeking the
short sentence s that maximizes p(s) ? p(l|s).
3.1 Lexicalized channel model
KM?s original model was purely syntax-based.
Daume et al (2002) used a lexicalized PCFG to
rerank the compressions, showing that the addition
of lexical information helps eliminate improbable
compressions. Here, we propose to enhance lexical-
ization by including lexical information within the
channel model, allowing us to better model which
compressions are likely and which are not. A min-
imal example pair illustrating the utility of lexical-
ization is the following.
(1) Hillary barely won the primaries.
(2) Hillary almost won the primaries.
The validity of dropping the adverbial here clearly
depends on the lexical value of the adverb. It is more
acceptable to drop the adverb in Sentence 1, since
dropping it in Sentence 2 reverses the meaning. We
learn probabilities of the form:
p( S[won]
NP[Hillary] ADVP[almost] VP[won]
| S[won]
NP[Hillary] VP[won]
)
Our model has the power of making compression de-
cisions based on lexical dependencies between the
compressed and retained parts of the parse tree.
Note that Daume et al?s reranking model cannot
achieve this type of distinction, since it is based on
reranking the compressed version, at which point the
adverb is no longer available.
Since we are interested not only in learning how
to compress, but also when to compress, we also in-
clude in this procedure unchanged CFG rule pairs
that are attested in the corpus. Thus, different ways
of expanding a CFG rule compete with each other as
well as the possibility of not doing any expansion.
3.2 Smoothing
In order to smooth our estimates we use Witten-Bell
discounting (1991) with 6 levels of back-off. This
method enables us to tune the confidence parameter
associated with an estimate inversely proportionally
with the diversity of the context of the estimate. The
different levels are illustrated in Table 1. Level 1,
138
the most specific level, is fully lexicalized. Transi-
tioning to levels 2 to 4, we lose the lexical informa-
tion about the subtrees that are not dropped, the head
child bearing subtree, and the dropped subtrees, re-
spectively. At level 4, we end up with the non-
lexicalized estimates that are equivalent to KM?s
model. In subsequent back off levels, we abstract
away from the CFG rules. In particular, level 5 es-
timates the probability of dropping subtrees in the
context of a certain parent and head child, and level
6 estimates the probability of the same outcome in
the coarser context of a parent only.
3.3 Source model
In addition to the lexicalized channel model, we also
use a lexicalized probabilistic syntax-based source
model, which we train from the parser?s output on
the short sentences of each pair.
3.4 Decoding
We implemented the forest-based statistical sen-
tence generation method of Langkilde (2000). KM
tailored this method to sentence compression, com-
pactly encoding all compressions of a sentence in
a forest structure. The forest ranking algorithm
which extracts compressed parse trees, optimized
the model scores as well as an additional bigram
score. Since our model is lexicalized, the bigram
scores become less relevant, which was confirmed
by experimentation during development. Therefore
in our implementation we exclude the bigram scores
and other related aspects of the algorithm such as
pruning of bigram-suboptimal phrases.
4 Evaluation
We evaluated our system using the same method as
KM, using the same 32 sentences taken from the
Ziff-Davis corpus. We solicited judgments of im-
portance (the value of the retained information), and
grammaticality for our compression, the KM results,
and human compressions from 8 judges, on a scale
of 1 (worst) to 5 (best). Mean and standard deviation
are shown in Table 2. Our model improves gram-
maticality and compression rate criteria with only a
slight decrease in importance. Here are some illus-
trative examples, with the deleted material shown in
brackets:
(3) The chemical etching process [used for glare
protection] is effective and will help if your
office has the fluorescent-light overkill [that
?s typical in offices].
(4) Prices range from $5,000 [for a microvax
2000] to $179,000 [for the vax 8000 or
higher series].
We suspect that the decrease in importance stems
from our indiscriminative usage of compressions
and expansions to train our system. We hypothesize
that in Wikipedia, expansions often add more useful
information, as opposed to compressions which are
more likely to drop superfluous or erroneous infor-
mation.4 Further work is required to classify sen-
tence modifications.
Since one of our model?s back-off levels simulates
KM?s model, we plan to perform an additional com-
parative evaluation of both models trained on the
same data.
5 Discussion and future work
Turner and Charniak (2005) question the viability
of a noisy channel model for the sentence compres-
sion task. Briefly put, in the typically sparse data
setting, there is no way to distinguish between the
probability of a sentence as a short sentence and its
probability as a regular sentence of English. Fur-
thermore, the channel model is likely to prefer to
leave sentences intact, since that is the most preva-
lent pattern in the training data. Thus, they argue,
the channel model is not really compressing, and it
is only by virtue of the length penalty that anything
gets shortened at all. Our hope here is that by using
a far richer source of short sentences, as well as a
huge source of compressions, we can overcome this
problem. The noisy channel model posits a virtual
competition on each word of coming either from the
source model (in which case it is retained in the com-
pression) or from the channel model (in which case
it is dropped). By having access to a large data set
for the first time, we hope to be able to learn which
parts of the sentence are more likely to come from
4For instance, here is an expansion seen in the data, where
the added information (italicized) is important: ?In 1952 and
1953 he was stationed in Sendai, Japan during the Korean War
and was shot.? It would be undesirable to drop this added
phrase.
139
Back-off level expanded short
1 S[won]? NP[Hillary] ADVP[almost] VP[won] S[won]? NP[Hillary] VP[won]
2 S[won]? NP ADVP[almost] VP[won] S[won]? NP VP[won]
3 S? NP ADVP[almost] VP S? NP VP
4 S? NP ADVP VP S? NP VP
5 parent = S, head-child = VP, child = ADVP parent = S, head-child = VP
6 parent = S, child = ADVP parent = S
Table 1: Back off levels
KM Our model Humans
Compression 72.91% 67.38% 53.33%
Grammaticality 4.02?1.03 4.31?0.78 4.78?0.17
Importance 3.86?1.09 3.65?1.07 3.90?0.58
Table 2: Evaluation results
which of the two parts of the model. Further work is
required in order to clarify this point.
Naturally, discriminative models such as McDon-
ald (2006) are also likely to improve by using the
added data. We leave the exploration of this topic
for future work.
Finally, we believe that the Wikipedia revision
history offers a wonderful resource for many addi-
tional NLP tasks, which we have begun exploring.
Acknowledgments
This work was partially supported by a Google re-
search award, ?Mining Wikipedia?s Revision His-
tory?. We thank Stuart Shieber for his comments on
an early draft of this paper, Kevin Knight and Daniel
Marcu for sharing the Ziff-Davis dataset with us, and
the volunteers for rating sentences. Yamangil thanks
Michael Collins for his feedback on the project idea.
References
Michael Collins. 1997. Three generative, lexicalized
models for statistical parsing. In Philip R. Cohen and
WolfgangWahlster, editors, Proceedings of the Thirty-
Fifth Annual Meeting of the Association for Computa-
tional Linguistics and Eighth Conference of the Eu-
ropean Chapter of the Association for Computational
Linguistics, pages 16?23, Somerset, New Jersey. As-
sociation for Computational Linguistics.
H. Daume, Kevin Knight, I Langkilde-Geary, Daniel
Marcu, and K Yamada. 2002. The importance of lexi-
calized syntax models for natural language generation
tasks. Proceedings of the Second International Confer-
ence on Natural Language Generation. Arden House,
NJ, July 1-3.
Kevin Knight and Daniel Marcu. 2000. Statistics-
based summarization - step one: Sentence compres-
sion. In Proceedings of the Seventeenth National Con-
ference on Artificial Intelligence and Twelfth Confer-
ence on Innovative Applications of Artificial Intelli-
gence, pages 703?710. AAAI Press / The MIT Press.
Irene Langkilde. 2000. Forest-based statistical sentence
generation. In Proceedings of the first conference on
North American chapter of the Association for Com-
putational Linguistics, pages 170?177, San Francisco,
CA, USA. Morgan Kaufmann Publishers Inc.
Ryan T. McDonald. 2006. Discriminative sentence com-
pression with soft syntactic evidence. In Proceedings
of EACL 2006, 11st Conference of the EuropeanChap-
ter of the Association for Computational Linguistics,
April 3-7, 2006, Trento, Italy, pages 297?304.
Jenine Turner and Eugene Charniak. 2005. Supervised
and unsupervised learning for sentence compression.
In ACL ?05: Proceedings of the 43rd Annual Meeting
on Association for Computational Linguistics, pages
290?297,Morristown, NJ, USA. Association for Com-
putational Linguistics.
Fernanda B. Vie?gas, Martin Wattenberg, and Kushal
Dave. 2004. Studying cooperation and conflict be-
tween authors with istory flow visualizations. In Eliza-
beth Dykstra-Erickson andManfred Tscheligi, editors,
CHI, pages 575?582. ACM.
I.Witten and T. Bell. 1991. The zero-frequencyproblem:
Estimating the probabilities of novel events in adaptive
text compression. IEEE Transactions on Information
Theory, 37(4).
140
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 357?361,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Correction Detection and Error Type Selection as an ESL Educational Aid
Ben Swanson
Brown University
chonger@cs.brown.edu
Elif Yamangil
Harvard University
elif@eecs.harvard.edu
Abstract
We present a classifier that discriminates be-
tween types of corrections made by teachers
of English in student essays. We define a set
of linguistically motivated feature templates
for a log-linear classification model, train this
classifier on sentence pairs extracted from
the Cambridge Learner Corpus, and achieve
89% accuracy improving upon a 33% base-
line. Furthermore, we incorporate our classi-
fier into a novel application that takes as input
a set of corrected essays that have been sen-
tence aligned with their originals and outputs
the individual corrections classified by error
type. We report the F-Score of our implemen-
tation on this task.
1 Introduction
In a typical foreign language education classroom
setting, teachers are presented with student essays
that are often fraught with errors. These errors can
be grammatical, semantic, stylistic, simple spelling
errors, etc. One task of the teacher is to isolate these
errors and provide feedback to the student with cor-
rections. In this body of work, we address the pos-
sibility of augmenting this process with NLP tools
and techniques, in the spirit of Computer Assisted
Language Learning (CALL).
We propose a step-wise approach in which a
teacher first corrects an essay and then a computer
program aligns their output with the original text and
separates and classifies independent edits. With the
program?s analysis the teacher would be provided
accurate information that could be used in effective
lesson planning tailored to the students? strengths
and weaknesses.
This suggests a novel NLP task with two compo-
nents: The first isolates individual corrections made
by the teacher, and the second classifies these cor-
rections into error types that the teacher would find
useful. A suitable corpus for developing this pro-
gram is the Cambridge Learner Corpus (CLC) (Yan-
nakoudakis et al, 2011). The CLC contains approxi-
mately 1200 essays with error corrections annotated
in XML within sentences. Furthermore, these cor-
rections are tagged with linguistically motivated er-
ror type codes.
To the best of our knowledge our proposed task
is unexplored in previous work. However, there is
a significant amount of related work in automated
grammatical error correction (Fitzgerald et al, 2009;
Gamon, 2011; West et al, 2011). The Helping
Our Own (HOO) shared task (Dale and Kilgarriff,
2010) also explores this issue, with Rozovskaya et
al. (2011) as the best performing system to date.
While often addressing the problem of error type
selection directly, previous work has dealt with the
more obviously useful task of end to end error detec-
tion and correction. As such, their classification sys-
tems are crippled by poor recall of errors as well as
the lack of information from the corrected sentence
and yield very low accuracies for error detection and
type selection, e.g. Gamon (2011).
Our task is fundamentally different as we assume
the presence of both the original and corrected text.
While the utility of such a system is not as obvi-
ous as full error correction, we note two possible
applications of our technique. The first, mentioned
357
above, is as an analytical tool for language teach-
ers. The second is as a complementary tool for au-
tomated error correction systems themselves. Just
as tools such as BLAST (Stymne, 2011) are useful
in the development of machine translation systems,
our system can produce accurate summaries of the
corrections made by automated systems even if the
systems themselves do not involve such fine grained
error type analysis.
In the following, we describe our experimental
methodology (Section 2) and then discuss the fea-
ture set we employ for classification (Section 3) and
its performance. Next, we outline our application
(Section 4), its heuristic correction detection strat-
egy and empirical evaluation. We finish by dis-
cussing the implications for real world systems (Sec-
tion 5) and avenues for improvement.
2 Methodology
Sentences in the CLC contain one or more error cor-
rections, each of which is labeled with one of 75
error types (Nicholls, 2003). Error types include
countability errors, verb tense errors, word order er-
rors, etc. and are often predicated on the part of
speech involved. For example, the category AG
(agreement) is augmented to form AGN (agreement
of a noun) to tag an error such as ?here are some
of my opinion?. For ease of analysis and due to
the high accuracy of state-of-the-art POS tagging,
in addition to the full 75 class problem we also
perform experiments using a compressed set of 15
classes. This compressed set removes the part of
speech components of the error types as shown in
Figure 1.
We create a dataset of corrections from the CLC
by extracting sentence pairs (x, y) where x is the
original (student?s) sentence and y is its corrected
form by the teacher. We create multiple instances
out of sentence pairs that contain multiple correc-
tions. For example, consider the sentence ?With this
letter I would ask you if you wuld change it?. This
consists of two errors: ?ask? should be replaced with
?like to ask? and ?wuld? is misspelled. These are
marked separately in the CLC, and imply the cor-
rected sentence ?With this letter I would like to ask
you if you would change it?. Here we extract two
instances consisting of ?With this letter I would ask
you if you would change it? and ?With this letter I
would like to ask if you wuld change it?, each paired
with the fully corrected sentence. As each correc-
tion in the CLC is tagged with an error type t, we
then form a dataset of triples (x, y, t). This yields
45080 such instances. We use these data in cross-
validation experiments with the feature based Max-
Ent classifier in the Mallet (McCallum, 2002) soft-
ware package.
3 Feature Set
We use the minimum unweighted edit distance path
between x and y as a source of features. The edit dis-
tance operations that compose the path are Delete,
Insert, Substitute, and Equal. To illustrate, the op-
erations we would get from the sentences above
would be (Insert, ?like?), (Insert, ?to?), (Substitute,
?wuld?, ?would?), and (Equal, w, w) for all other
words w.
Our feature set consists of three main categories
and a global category (See Figure 2). For each edit
distance operation other than Equal we use an indi-
cator feature, as well as word+operation indicators,
for example ?the word w was inserted? or ?the word
w1 was substituted with w2?. The POS Context fea-
tures encode the part of speech context of the edit,
recording the parts of speech immediately preced-
ing and following the edit in the corrected sentence.
For all POS based features we use only tags from the
corrected sentence y, as our tags are obtained auto-
matically.
For a substitution of w2 for w1 we use several
targeted features. Many of these are self explana-
tory and can be calculated easily without outside li-
braries. The In Dictionary? feature is indexed by
two binary values corresponding to the presence of
the words in the WordNet dictionary. For the Same
Stem? feature we use the stemmer provided in the
freely downloadable JWI (Java Wordnet Interface)
library. If the two words have the same stem then
we also trigger the Suffixes feature, which is in-
dexed by the two suffix strings after the stem has
been removed. For global features, we record the
total number of non-Equal edits as well as a feature
which fires if one sentence is a word-reordering of
the other.
358
Description (Code) Sample and Correction Total # % Accuracy
Unnecessary (U)
July is the period of time that suits me best
5237 94.0
July is the time that suits me best
Incorrect verb tense (TV)
She gave me autographs and talk really nicely.
2752 85.2
She gave me autographs and talked really nicely.
Countability error (C)
Please help them put away their stuffs.
273 65.2
Please help them put away their stuff.
Incorrect word order (W)
I would like to know what kind of clothes should I bring.
1410 76.0
I would like to know what kind of clothes I should bring.
Incorrect negative (X)
We recommend you not to go with your friends.
124 18.5
We recommend you don?t go with your friends.
Spelling error (S)
Our music lessons are speccial.
4429 90.0
Our music lessons are special.
Wrong form used (F)
In spite of think I did well, I had to reapply.
2480 82.0
In spite of thinking I did well, I had to reapply.
Agreement error (AG)
I would like to take some picture of beautiful scenery.
1743 77.9
I would like to take some pictures of beautiful scenery.
Replace (R)
The idea about going to Maine is common.
14290 94.6
The idea of going to Maine is common.
Missing (M)
Sometimes you surprised when you check the balance.
9470 97.6
Sometimes you are surprised when you check the balance.
Incorrect argument structure (AS)
How much do I have to bring the money?
191 19.4
How much money do I have to bring?
Wrong Derivation (D)
The arrive of every student is a new chance.
1643 58.6
The arrival of every student is a new chance.
Wrong inflection (I)
I enjoyded it a lot.
590 58.6
I enjoyed it a lot.
Inappropriate register (L)
The girls?d rather play table tennis or badminton.
135 23.0
The girls would rather play table tennis or badminton.
Idiomatic error (ID)
The level of life in the USA is similar to the UK.
313 15.7
The cost of living in the USA is similar to the UK.
Figure 1: Error types in the collapsed 15 class set.
3.1 Evaluation
We perform five-fold cross-validation and achieve
a classification accuracy of 88.9% for the 15 class
problem and 83.8% for the full 75 class problem.
The accuracies of the most common class base-
lines are 33.3% and 7.8% respectively. The most
common confusion in the 15 class case is between
D (Derivation), R (Replacement) and S (Spelling).
These are mainly due to context-sensitive spelling
corrections falling into the Replace category or noise
in the mark-up of derivation errors. For the 75 class
case the most common confusion is between agree-
ment of noun (AGN) and form of noun (FN). This is
unsurprising as we do not incorporate long distance
features which would encode agreement.
To check against over-fitting we performed an ex-
periment where we take away the strongly lexical-
ized features (such as ?word w is inserted?) and
observed a reduction from 88.9% to 82.4% for 15
class classification accuracy. The lack of a dramatic
reduction demonstrates the generalization power of
our feature templates.
4 An Educational Application
As mentioned earlier, we incorporate our classifier
in an educational software tool. The input to this
tool is a group of aligned sentence pairs from orig-
inal and teacher edited versions of a set of essays.
This tool has two components devoted to (1) isola-
tion of individual corrections in a sentence pair, and
(2) classification of these corrections. This software
could be easily integrated in real world curriculum
as it is natural for the teacher to produce corrected
versions of student essays without stopping to label
and analyze distribution of correction types.
We devise a family of heuristic strategies to
separate independent corrections from one another.
Heuristic hi allows at most i consecutive Equal edit
distance operations in a single correction. This im-
plies that hn+1 would tend to merge more non-
Equal edits than hn. We experimented with i ?
{0, 1, 2, 3, 4}. For comparison we also implemented
359
? Insert
? Insert
? Insert(w)
? POS Context
? Delete
? Delete
? Delete(w)
? POS Context
? Substitution
? Substitution
? Substitution(w1,w2)
? Character Edit Distance
? Common Prefix Length
? In Dictionary?
? Previous Word
? POS of Substitution
? Same Stem?
? Suffixes
? Global
? Same Words?
? Number Of Edits
Figure 2: List of features used in our classifier.
a heuristic h? that treats every non-Equal edit as
an individual correction. This is different than h0,
which would merge edits that do not have an in-
tervening Equal operation. F-scores (using 5 fold
cross-validation) obtained by different heuristics are
reported in Figure 3 for the 15 and 75 class prob-
lems. For these F-scores we attempt to predict both
the boundaries and the labels of the corrections. The
unlabeled F-score (shown as a line) evaluates the
heuristic itself and provides an upper bound for the
labeled F-score of the overall application. We see
that the best upper bound and F-scores are achieved
with heuristic h0 which merges consecutive non-
Equal edits.
5 Future Work
There are several directions in which this work could
be extended. The most obvious is to replace the
correction detection heuristic with a more robust al-
gorithm. Our log-linear classifier is perhaps better
suited for this task than other discriminative clas-
sifiers as it can be extended in a larger framework
which maximizes the joint probability of all correc-
tions. Our work shows that h0 will provide a strong
baseline for such experiments.

	

     









	
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 937?947,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Bayesian Synchronous Tree-Substitution Grammar Induction
and its Application to Sentence Compression
Elif Yamangil and Stuart M. Shieber
Harvard University
Cambridge, Massachusetts, USA
{elif, shieber}@seas.harvard.edu
Abstract
We describe our experiments with training
algorithms for tree-to-tree synchronous
tree-substitution grammar (STSG) for
monolingual translation tasks such as
sentence compression and paraphrasing.
These translation tasks are characterized
by the relative ability to commit to parallel
parse trees and availability of word align-
ments, yet the unavailability of large-scale
data, calling for a Bayesian tree-to-tree
formalism. We formalize nonparametric
Bayesian STSG with epsilon alignment in
full generality, and provide a Gibbs sam-
pling algorithm for posterior inference tai-
lored to the task of extractive sentence
compression. We achieve improvements
against a number of baselines, including
expectation maximization and variational
Bayes training, illustrating the merits of
nonparametric inference over the space of
grammars as opposed to sparse parametric
inference with a fixed grammar.
1 Introduction
Given an aligned corpus of tree pairs, we might
want to learn a mapping between the paired trees.
Such induction of tree mappings has application
in a variety of natural-language-processing tasks
including machine translation, paraphrase, and
sentence compression. The induced tree map-
pings can be expressed by synchronous grammars.
Where the tree pairs are isomorphic, synchronous
context-free grammars (SCFG) may suffice, but in
general, non-isomorphism can make the problem
of rule extraction difficult (Galley and McKeown,
2007). More expressive formalisms such as syn-
chronous tree-substitution (Eisner, 2003) or tree-
adjoining grammars may better capture the pair-
ings.
In this work, we explore techniques for inducing
synchronous tree-substitution grammars (STSG)
using as a testbed application extractive sentence
compression. Learning an STSG from aligned
trees is tantamount to determining a segmentation
of the trees into elementary trees of the grammar
along with an alignment of the elementary trees
(see Figure 1 for an example of such a segmenta-
tion), followed by estimation of the weights for the
extracted tree pairs.1 These elementary tree pairs
serve as the rules of the extracted grammar. For
SCFG, segmentation is trivial ? each parent with
its immediate children is an elementary tree ? but
the formalism then restricts us to deriving isomor-
phic tree pairs. STSG is much more expressive,
especially if we allow some elementary trees on
the source or target side to be unsynchronized, so
that insertions and deletions can be modeled, but
the segmentation and alignment problems become
nontrivial.
Previous approaches to this problem have
treated the two steps ? grammar extraction and
weight estimation ? with a variety of methods.
One approach is to use word alignments (where
these can be reliably estimated, as in our testbed
application) to align subtrees and extract rules
(Och and Ney, 2004; Galley et al, 2004) but
this leaves open the question of finding the right
level of generality of the rules ? how deep the
rules should be and how much lexicalization they
should involve ? necessitating resorting to heuris-
tics such as minimality of rules, and leading to
1Throughout the paper we will use the word STSG to re-
fer to the tree-to-tree version of the formalism, although the
string-to-tree version is also commonly used.
937
large grammars. Once a given set of rules is ex-
tracted, weights can be imputed using a discrimi-
native approach to maximize the (joint or condi-
tional) likelihood or the classification margin in
the training data (taking or not taking into account
the derivational ambiguity). This option leverages
a large amount of manual domain knowledge en-
gineering and is not in general amenable to latent
variable problems.
A simpler alternative to this two step approach
is to use a generative model of synchronous
derivation and simultaneously segment and weight
the elementary tree pairs to maximize the prob-
ability of the training data under that model; the
simplest exemplar of this approach uses expecta-
tion maximization (EM) (Dempster et al, 1977).
This approach has two frailties. First, EM search
over the space of all possible rules is computation-
ally impractical. Second, even if such a search
were practical, the method is degenerate, pushing
the probability mass towards larger rules in order
to better approximate the empirical distribution of
the data (Goldwater et al, 2006; DeNero et al,
2006). Indeed, the optimal grammar would be one
in which each tree pair in the training data is its
own rule. Therefore, proposals for using EM for
this task start with a precomputed subset of rules,
and with EM used just to assign weights within
this grammar. In summary, previous methods suf-
fer from problems of narrowness of search, having
to restrict the space of possible rules, and overfit-
ting in preferring overly specific grammars.
We pursue the use of hierarchical probabilistic
models incorporating sparse priors to simultane-
ously solve both the narrowness and overfitting
problems. Such models have been used as gener-
ative solutions to several other segmentation prob-
lems, ranging from word segmentation (Goldwa-
ter et al, 2006), to parsing (Cohn et al, 2009; Post
and Gildea, 2009) and machine translation (DeN-
ero et al, 2008; Cohn and Blunsom, 2009; Liu
and Gildea, 2009). Segmentation is achieved by
introducing a prior bias towards grammars that are
compact representations of the data, namely by en-
forcing simplicity and sparsity: preferring simple
rules (smaller segments) unless the use of a com-
plex rule is evidenced by the data (through repeti-
tion), and thus mitigating the overfitting problem.
A Dirichlet process (DP) prior is typically used
to achieve this interplay. Interestingly, sampling-
based nonparametric inference further allows the
possibility of searching over the infinite space of
grammars (and, in machine translation, possible
word alignments), thus side-stepping the narrow-
ness problem outlined above as well.
In this work, we use an extension of the afore-
mentioned models of generative segmentation for
STSG induction, and describe an algorithm for
posterior inference under this model that is tai-
lored to the task of extractive sentence compres-
sion. This task is characterized by the availabil-
ity of word alignments, providing a clean testbed
for investigating the effects of grammar extraction.
We achieve substantial improvements against a
number of baselines including EM, support vector
machine (SVM) based discriminative training, and
variational Bayes (VB). By comparing our method
to a range of other methods that are subject dif-
ferentially to the two problems, we can show that
both play an important role in performance limi-
tations, and that our method helps address both as
well. Our results are thus not only encouraging for
grammar estimation using sparse priors but also il-
lustrate the merits of nonparametric inference over
the space of grammars as opposed to sparse para-
metric inference with a fixed grammar.
In the following, we define the task of extrac-
tive sentence compression and the Bayesian STSG
model, and algorithms we used for inference and
prediction. We then describe the experiments in
extractive sentence compression and present our
results in contrast with alternative algorithms. We
conclude by giving examples of compression pat-
terns learned by the Bayesian method.
2 Sentence compression
Sentence compression is the task of summarizing a
sentence while retaining most of the informational
content and remaining grammatical (Jing, 2000).
In extractive sentence compression, which we fo-
cus on in this paper, an order-preserving subset of
the words in the sentence are selected to form the
summary, that is, we summarize by deleting words
(Knight and Marcu, 2002). An example sentence
pair, which we use as a running example, is the
following:
? Like FaceLift, much of ATM?s screen perfor-
mance depends on the underlying applica-
tion.
? ATM?s screen performance depends on the
underlying application.
938
Figure 1: A portion of an STSG derivation of the example sentence and its extractive compression.
where the underlined words were deleted. In su-
pervised sentence compression, the goal is to gen-
eralize from a parallel training corpus of sentences
(source) and their compressions (target) to unseen
sentences in a test set to predict their compres-
sions. An unsupervised setup also exists; meth-
ods for the unsupervised problem typically rely
on language models and linguistic/discourse con-
straints (Clarke and Lapata, 2006a; Turner and
Charniak, 2005). Because these methods rely on
dynamic programming to efficiently consider hy-
potheses over the space of all possible compres-
sions of a sentence, they may be harder to extend
to general paraphrasing.
3 The STSG Model
Synchronous tree-substitution grammar is a for-
malism for synchronously generating a pair of
non-isomorphic source and target trees (Eisner,
2003). Every grammar rule is a pair of elemen-
tary trees aligned at the leaf level at their frontier
nodes, which we will denote using the form
cs/ct ? es/et, ?
(indices s for source, t for target) where cs, ct are
root nonterminals of the elementary trees es, et re-
spectively and ? is a 1-to-1 correspondence be-
tween the frontier nodes in es and et. For example,
the rule
S / S? (S (PP (IN Like) NP[]) NP[1] VP[2]) /
(S NP[1] VP[2])
can be used to delete a subtree rooted at PP. We
use square bracketed indices to represent the align-
ment ? of frontier nodes ? NP[1] aligns with
NP[1], VP[2] aligns with VP[2], NP[] aligns with
the special symbol  denoting a deletion from the
source tree. Symmetrically -aligned target nodes
are used to represent insertions into the target tree.
Similarly, the rule
NP / ? (NP (NN FaceLift)) / 
can be used to continue deriving the deleted sub-
tree. See Figure 1 for an example of how an STSG
with these rules would operate in synchronously
generating our example sentence pair.
STSG is a convenient choice of formalism for
a number of reasons. First, it eliminates the iso-
morphism and strong independence assumptions
of SCFGs. Second, the ability to have rules deeper
than one level provides a principled way of model-
ing lexicalization, whose importance has been em-
phasized (Galley and McKeown, 2007; Yamangil
and Nelken, 2008). Third, we may have our STSG
operate on trees instead of sentences, which allows
for efficient parsing algorithms, as well as provid-
ing syntactic analyses for our predictions, which is
desirable for automatic evaluation purposes.
A straightforward extension of the popular EM
algorithm for probabilistic context free grammars
(PCFG), the inside-outside algorithm (Lari and
Young, 1990), can be used to estimate the rule
weights of a given unweighted STSG based on a
corpus of parallel parse trees t = t1, . . . , tN where
tn = tn,s/tn,t for n = 1, . . . , N . Similarly, an
939
Figure 2: Gibbs sampling updates. We illustrate a sampler move to align/unalign a source node with a
target node (top row in blue), and split/merge a deletion rule via aligning with  (bottom row in red).
extension of the Viterbi algorithm is available for
finding the maximum probability derivation, use-
ful for predicting the target analysis tN+1,t for a
test instance tN+1,s. (Eisner, 2003) However, as
noted earlier, EM is subject to the narrowness and
overfitting problems.
3.1 The Bayesian generative process
Both of these issues can be addressed by taking
a nonparametric Bayesian approach, namely, as-
suming that the elementary tree pairs are sampled
from an independent collection of Dirichlet pro-
cess (DP) priors. We describe such a process for
sampling a corpus of tree pairs t.
For all pairs of root labels c = cs/ct that we
consider, where up to one of cs or ct can be  (e.g.,
S / S, NP / ), we sample a sparse discrete distribu-
tion Gc over infinitely many elementary tree pairs
e = es/et sharing the common root c from a DP
Gc ? DP(?c, P0(? | c)) (1)
where the DP has the concentration parameter ?c
controlling the sparsity of Gc, and the base dis-
tribution P0(? | c) is a distribution over novel el-
ementary tree pairs that we describe more fully
shortly.
We then sample a sequence of elementary tree
pairs to serve as a derivation for each observed de-
rived tree pair. For each n = 1, . . . , N , we sam-
ple elementary tree pairs en = en,1, . . . , en,dn in
a derivation sequence (where dn is the number of
rules used in the derivation), consulting Gc when-
ever an elementary tree pair with root c is to be
sampled.
e
iid
? Gc, for all e whose root label is c
Given the derivation sequence en, a tree pair tn is
determined, that is,
p(tn | en) =
{
1 en,1, . . . , en,dn derives tn
0 otherwise.
(2)
The hyperparameters ?c can be incorporated
into the generative model as random variables;
however, we opt to fix these at various constants
to investigate different levels of sparsity.
For the base distribution P0(? | c) there are a
variety of choices; we used the following simple
scenario. (We take c = cs/ct.)
Synchronous rules For the case where neither cs
nor ct are the special symbol , the base dis-
tribution first generates es and et indepen-
dently, and then samples an alignment be-
tween the frontier nodes. Given a nontermi-
nal, an elementary tree is generated by first
making a decision to expand the nontermi-
nal (with probability ?c) or to leave it as a
frontier node (1 ? ?c). If the decision to ex-
pand was made, we sample an appropriate
rule from a PCFG which we estimate ahead
940
of time from the training corpus. We expand
the nonterminal using this rule, and then re-
peat the same procedure for every child gen-
erated that is a nonterminal until there are no
generated nonterminal children left. This is
done independently for both es and et. Fi-
nally, we sample an alignment between the
frontier nodes uniformly at random out of all
possible alingments.
Deletion/insertion rules If ct = , that is, we
have a deletion rule, we need to generate
e = es/. (The insertion rule case is symmet-
ric.) The base distribution generates es using
the same process described for synchronous
rules above. Then with probability 1 we align
all frontier nodes in es with . In essence,
this process generates TSG rules, rather than
STSG rules, which are used to cover deleted
(or inserted) subtrees.
This simple base distribution does nothing to
enforce an alignment between the internal nodes
of es and et. One may come up with more sophis-
ticated base distributions. However the main point
of the base distribution is to encode a control-
lable preference towards simpler rules; we there-
fore make the simplest possible assumption.
3.2 Posterior inference via Gibbs sampling
Assuming fixed hyperparameters ? = {?c} and
? = {?c}, our inference problem is to find the
posterior distribution of the derivation sequences
e = e1, . . . , eN given the observations t =
t1, . . . , tN . Applying Bayes? rule, we have
p(e | t) ? p(t | e)p(e) (3)
where p(t | e) is a 0/1 distribution (2) which does
not depend on Gc, and p(e) can be obtained by
collapsing Gc for all c.
Consider repeatedly generating elementary tree
pairs e1, . . . , ei, all with the same root c, iid from
Gc. Integrating over Gc, the ei become depen-
dent. The conditional prior of the i-th elementary
tree pair given previously generated ones e<i =
e1, . . . , ei?1 is given by
p(ei | e<i) =
nei + ?cP0(ei | c)
i? 1 + ?c
(4)
where nei denotes the number of times ei occurs
in e<i. Since the collapsed model is exchangeable
in the ei, this formula forms the backbone of the
inference procedure that we describe next. It also
makes clear DP?s inductive bias to reuse elemen-
tary tree pairs.
We use Gibbs sampling (Geman and Geman,
1984), a Markov chain Monte Carlo (MCMC)
method, to sample from the posterior (3). A
derivation e of the corpus t is completely specified
by an alignment between the source nodes and the
corresponding target nodes (as well as  on either
side), which we take to be the state of the sampler.
We start at a random derivation of the corpus, and
at every iteration resample a derivation by amend-
ing the current one through local changes made
at the node level, in the style of Goldwater et al
(2006).
Our sampling updates are extensions of those
used by Cohn and Blunsom (2009) in MT, but are
tailored to our task of extractive sentence compres-
sion. In our task, no target node can align with
 (which would indicate a subtree insertion), and
barring unary branches no source node i can align
with two different target nodes j and j? at the same
time (indicating a tree expansion). Rather, the
configurations of interest are those in which only
source nodes i can align with , and two source
nodes i and i? can align with the same target node
j. Thus, the alignments of interest are not arbitrary
relations, but (partial) functions from nodes in es
to nodes in et or . We therefore sample in the
direction from source to target. In particular, we
visit every tree pair and each of its source nodes i,
and update its alignment by selecting between and
within two choices: (a) unaligned, (b) aligned with
some target node j or . The number of possibil-
ities j in (b) is significantly limited, firstly by the
word alignment (for instance, a source node dom-
inating a deleted subspan cannot be aligned with
a target node), and secondly by the current align-
ment of other nearby aligned source nodes. (See
Cohn and Blunsom (2009) for details of matching
spans under tree constraints.)2
2One reviewer was concerned that since we explicitly dis-
allow insertion rules in our sampling procedure, our model
that generates such rules wastes probability mass and is there-
fore ?deficient?. However, we regard sampling as a separate
step from the data generation process, in which we can for-
mulate more effective algorithms by using our domain knowl-
edge that our data set was created by annotators who were
instructed to delete words only. Also, disallowing insertion
rules in the base distribution unnecessarily complicates the
definition of the model, whereas it is straightforward to de-
fine the joint distribution of all (potentially useful) rules and
then use domain knowledge to constrain the support of that
distribution during inference, as we do here. In fact, it is pos-
941
More formally, let eM be the elementary tree
pair rooted at the closest aligned ancestor i? of
node i when it is unaligned; and let eA and eB
be the elementary tree pairs rooted at i? and i re-
spectively when i is aligned with some target node
j or . Then, by exchangeability of the elementary
trees sharing the same root label, and using (4), we
have
p(unalign) =
neM + ?cMP0(eM | cM )
ncM + ?cM
(5)
p(align with j) =
neA + ?cAP0(eA | cA)
ncA + ?cA
(6)
?
neB + ?cBP0(eB | cB)
ncB + ?cB
(7)
where the counts ne? , nc? are with respect to the
current derivation of the rest of the corpus; except
for neB , ncB we also make sure to account for hav-
ing generated eA. See Figure 2 for an illustration
of the sampling updates.
It is important to note that the sampler described
can move from any derivation to any other deriva-
tion with positive probability (if only, for example,
by virtue of fully merging and then resegment-
ing), which guarantees convergence to the poste-
rior (3). However some of these transition prob-
abilities can be extremely small due to passing
through low probability states with large elemen-
tary trees; in turn, the sampling procedure is prone
to local modes. In order to counteract this and to
improve mixing we used simulated annealing. The
probability mass function (5-7) was raised to the
power 1/T with T dropping linearly from T = 5
to T = 0. Furthermore, using a final tempera-
ture of zero, we recover a maximum a posteriori
(MAP) estimate which we denote eMAP.
3.3 Prediction
We discuss the problem of predicting a target tree
tN+1,t that corresponds to a source tree tN+1,s
unseen in the observed corpus t. The maximum
probability tree (MPT) can be found by consid-
ering all possible ways to derive it. However a
much simpler alternative is to choose the target
tree implied by the maximum probability deriva-
sible to prove that our approach is equivalent up to a rescaling
of the concentration parameters. Since we fit these parame-
ters to the data, our approach is equivalent.
tion (MPD), which we define as
e? = argmax
e
p(e | ts, t)
= argmax
e
?
e
p(e | ts, e)p(e | t)
where e denotes a derivation for t = ts/tt. (We
suppress the N + 1 subscripts for brevity.) We
approximate this objective first by substituting
?eMAP(e) for p(e | t) and secondly using a finite
STSG model for the infinite p(e | ts, eMAP), which
we obtain simply by normalizing the rule counts in
eMAP. We use dynamic programming for parsing
under this finite model (Eisner, 2003).3
Unfortunately, this approach does not ensure
that the test instances are parsable, since ts may
include unseen structure or novel words. A work-
around is to include all zero-count context free
copy rules such as
NP / NP? (NP NP[1] PP[2]) / (NP NP[1] PP[2])
NP / ? (NP NP[] PP[]) / 
in order to smooth our finite model. We used
Laplace smoothing (adding 1 to all counts) as it
gave us interpretable results.
4 Evaluation
We compared the Gibbs sampling compressor
(GS) against a version of maximum a posteriori
EM (with Dirichlet parameter greater than 1) and
a discriminative STSG based on SVM training
(Cohn and Lapata, 2008) (SVM). EM is a natural
benchmark, while SVM is also appropriate since
it can be taken as the state of the art for our task.4
We used a publicly available extractive sen-
tence compression corpus: the Broadcast News
compressions corpus (BNC) of Clarke and Lap-
ata (2006a). This corpus consists of 1370 sentence
pairs that were manually created from transcribed
Broadcast News stories. We split the pairs into
training, development, and testing sets of 1000,
3We experimented with MPT using Monte Carlo integra-
tion over possible derivations; the results were not signifi-
cantly different from those using MPD.
4The comparison system described by Cohn and Lapata
(2008) attempts to solve a more general problem than ours,
abstractive sentence compression. However, given the nature
of the data that we provided, it can only learn to compress
by deleting words. Since the system is less specialized to the
task, their model requires additional heuristics in decoding
not needed for extractive compression, which might cause a
reduction in performance. Nonetheless, because the compar-
ison system is a generalization of the extractive SVM com-
pressor of Cohn and Lapata (2007), we do not expect that the
results would differ qualitatively.
942
SVM EM GS
Precision 55.60 58.80 58.94
Recall 53.37 56.58 64.59
Relational F1 54.46 57.67 61.64
Compression rate 59.72 64.11 65.52
Table 1: Precision, recall, relational F1 and com-
pression rate (%) for various systems on the 200-
sentence BNC test set. The compression rate for
the gold standard was 65.67%.
SVM EM GS Gold
Grammar 2.75? 2.85? 3.69 4.25
Importance 2.85 2.67? 3.41 3.82
Comp. rate 68.18 64.07 67.97 62.34
Table 2: Average grammar and importance scores
for various systems on the 20-sentence subsam-
ple. Scores marked with ? are significantly dif-
ferent than the corresponding GS score at ? < .05
and with ? at ? < .01 according to post-hoc Tukey
tests. ANOVA was significant at p < .01 both for
grammar and importance.
170, and 200 pairs, respectively. The corpus was
parsed using the Stanford parser (Klein and Man-
ning, 2003).
In our experiments with the publicly available
SVM system we used all except paraphrasal rules
extracted from bilingual corpora (Cohn and Lap-
ata, 2008). The model chosen for testing had pa-
rameter for trade-off between training error and
margin set to C = 0.001, used margin rescaling,
and Hamming distance over bags of tokens with
brevity penalty for loss function. EM used a sub-
set of the rules extracted by SVM, namely all rules
except non-head deleting compression rules, and
was initialized uniformly. Each EM instance was
characterized by two parameters: ?, the smooth-
ing parameter for MAP-EM, and ?, the smooth-
ing parameter for augmenting the learned gram-
mar with rules extracted from unseen data (add-
(? ? 1) smoothing was used), both of which were
fit to the development set using grid-search over
(1, 2]. The model chosen for testing was (?, ?) =
(1.0001, 1.01).
GS was initialized at a random derivation. We
sampled the alignments of the source nodes in ran-
dom order. The sampler was run for 5000 itera-
tions with annealing. All hyperparameters ?c, ?c
were held constant at ?, ? for simplicity and were
fit using grid-search over ? ? [10?6, 106], ? ?
[10?3, 0.5]. The model chosen for testing was
(?, ?) = (100, 0.1).
As an automated metric of quality, we compute
F-score based on grammatical relations (relational
F1, or RelF1) (Riezler et al, 2003), by which the
consistency between the set of predicted grammat-
ical relations and those from the gold standard is
measured, which has been shown by Clarke and
Lapata (2006b) to correlate reliably with human
judgments. We also conducted a small human sub-
jective evaluation of the grammaticality and infor-
mativeness of the compressions generated by the
various methods.
4.1 Automated evaluation
For all three systems we obtained predictions for
the test set and used the Stanford parser to extract
grammatical relations from predicted trees and the
gold standard. We computed precision, recall,
RelF1 (all based on grammatical relations), and
compression rate (percentage of the words that are
retained), which we report in Table 1. The results
for GS are averages over five independent runs.
EM gives a strong baseline since it already uses
rules that are limited in depth and number of fron-
tier nodes by stipulation, helping with the overfit-
ting we have mentioned, surprisingly outperform-
ing its discriminative counterpart in both precision
and recall (and consequently RelF1). GS however
maintains the same level of precision as EM while
improving recall, bringing an overall improvement
in RelF1.
4.2 Human evaluation
We randomly subsampled our 200-sentence test
set for 20 sentences to be evaluated by human
judges through Amazon Mechanical Turk. We
asked 15 self-reported native English speakers for
their judgments of GS, EM, and SVM output sen-
tences and the gold standard in terms of grammat-
icality (how fluent the compression is) and impor-
tance (how much of the meaning of and impor-
tant information from the original sentence is re-
tained) on a scale of 1 (worst) to 5 (best). We re-
port in Table 2 the average scores. EM and SVM
perform at very similar levels, which we attribute
to using the same set of rules, while GS performs
at a level substantially better than both, and much
closer to human performance in both criteria. The
943
Figure 3: RelF1, precision, recall plotted against
compression rate for GS, EM, VB.
human evaluation indicates that the superiority of
the Bayesian nonparametric method is underap-
preciated by the automated evaluation metric.
4.3 Discussion
The fact that GS performs better than EM can be
attributed to two reasons: (1) GS uses a sparse
prior and selects a compact representation of the
data (grammar sizes ranged from 4K-7K for GS
compared to a grammar of about 35K rules for
EM). (2) GS does not commit to a precomputed
grammar and searches over the space of all gram-
mars to find one that bests represents the corpus.
It is possible to introduce DP-like sparsity in EM
using variational Bayes (VB) training. We exper-
iment with this next in order to understand how
dominant the two factors are. The VB algorithm
requires a simple update to the M-step formulas
for EM where the expected rule counts are normal-
ized, such that instead of updating the rule weight
in the t-th iteration as in the following
?t+1c,e =
nc,e + ?? 1
nc,. +K??K
where nc,e represents the expected count of rule
c ? e, and K is the total number of ways
to rewrite c, we now take into account our
DP(?c, P0(? | c)) prior in (1), which, when
truncated to a finite grammar, reduces to a
K-dimensional Dirichlet prior with parameter
?cP0(? | c). Thus in VB we perform a variational
E-step with the subprobabilities given by
?t+1c,e =
exp (?(nc,e + ?cP0(e | c)))
exp (?(nc,. + ?c))
where ? denotes the digamma function. (Liu and
Gildea, 2009) (See MacKay (1997) for details.)
Hyperparameters were handled the same way as
for GS.
Instead of selecting a single model on the devel-
opment set, here we provide the whole spectrum of
models and their performances in order to better
understand their comparative behavior. In Figure
3 we plot RelF1 on the test set versus compres-
sion rate and compare GS, EM, and VB (? = 0.1
fixed, (?, ?) ranging in [10?6, 106]?(1, 2]). Over-
all, we see that GS maintains roughly the same
level of precision as EM (despite its larger com-
pression rates) while achieving an improvement in
recall, consequently performing at a higher RelF1
level. We note that VB somewhat bridges the gap
between GS and EM, without quite reaching GS
performance. We conclude that the mitigation of
the two factors (narrowness and overfitting) both
contribute to the performance gain of GS.5
4.4 Example rules learned
In order to provide some insight into the grammar
extracted by GS, we list in Tables (3) and (4) high
5We have also experimented with VB with parametric in-
dependent symmetric Dirichlet priors. The results were sim-
ilar to EM with the exception of sparse priors resulting in
smaller grammars and slightly improving performance.
944
(ROOT (S CC[] NP[1] VP[2] .[3])) / (ROOT (S NP[1] VP[2] .[3]))
(ROOT (S NP[1] ADVP[] VP[2] (. .))) / (ROOT (S NP[1] VP[2] (. .)))
(ROOT (S ADVP[] (, ,) NP[1] VP[2] (. .))) / (ROOT (S NP[1] VP[2] (. .)))
(ROOT (S PP[] (, ,) NP[1] VP[2] (. .))) / (ROOT (S NP[1] VP[2] (. .)))
(ROOT (S PP[] ,[] NP[1] VP[2] .[3])) / (ROOT (S NP[1] VP[2] .[3]))
(ROOT (S NP[] (VP VBP[] (SBAR (S NP[1] VP[2]))) .[3])) / (ROOT (S NP[1] VP[2] .[3]))
(ROOT (S ADVP[] NP[1] (VP MD[2] VP[3]) .[4])) / (ROOT (S NP[1] (VP MD[2] VP[3]) .[4]))
(ROOT (S (SBAR (IN as) S[]) ,[] NP[1] VP[2] .[3])) / (ROOT (S NP[1] VP[2] .[3]))
(ROOT (S S[] (, ,) CC[] (S NP[1] VP[2]) .[3])) / (ROOT (S NP[1] VP[2] .[3]))
(ROOT (S PP[] NP[1] VP[2] .[3])) / (ROOT (S NP[1] VP[2] .[3]))
(ROOT (S S[1] (, ,) CC[] S[2] (. .))) / (ROOT (S NP[1] VP[2] (. .)))
(ROOT (S S[] ,[] NP[1] ADVP[2] VP[3] .[4])) / (ROOT (S NP[1] ADVP[2] VP[3] .[4]))
(ROOT (S (NP (NP NNP[] (POS ?s)) NNP[1] NNP[2]) / (ROOT (S (NP NNP[1] NNP[2])
(VP (VBZ reports)) .[3])) (VP (VBZ reports)) .[3]))
Table 3: High probability ROOT / ROOT compression rules from the final state of the sampler.
(S NP[1] ADVP[] VP[2]) / (S NP[1] VP[2])
(S INTJ[] (, ,) NP[1] VP[2] (. .)) / (S NP[1] VP[2] (. .))
(S (INTJ (UH Well)) ,[] NP[1] VP[2] .[3]) / (S NP[1] VP[2] .[3])
(S PP[] (, ,) NP[1] VP[2]) / (S NP[1] VP[2])
(S ADVP[] (, ,) S[1] (, ,) (CC but) S[2] .[3]) / (S S[1] (, ,) (CC but) S[2] .[3])
(S ADVP[] NP[1] VP[2]) / (S NP[1] VP[2])
(S NP[] (VP VBP[] (SBAR (IN that) (S NP[1] VP[2]))) (. .)) / (S NP[1] VP[2] (. .))
(S NP[] (VP VBZ[] ADJP[] SBAR[1])) / S[1]
(S CC[] PP[] (, ,) NP[1] VP[2] (. .)) / (S NP[1] VP[2] (. .))
(S NP[] (, ,) NP[1] VP[2] .[3]) / (S NP[1] VP[2] .[3])
(S NP[1] (, ,) ADVP[] (, ,) VP[2]) / (S NP[1] VP[2])
(S CC[] (NP PRP[1]) VP[2]) / (S (NP PRP[1]) VP[2])
(S ADVP[] ,[] PP[] ,[] NP[1] VP[2] .[3]) / (S NP[1] VP[2] .[3])
(S ADVP[] (, ,) NP[1] VP[2]) / (S NP[1] VP[2])
Table 4: High probability S / S compression rules from the final state of the sampler.
probability subtree-deletion rules expanding cate-
gories ROOT / ROOT and S / S, respectively. Of
especial interest are deep lexicalized rules such as
a pattern of compression used many times in the
BNC in sentence pairs such as ?NPR?s Anne Gar-
rels reports? / ?Anne Garrels reports?. Such an
informative rule with nontrivial collocation (be-
tween the possessive marker and the word ?re-
ports?) would be hard to extract heuristically and
can only be extracted by reasoning across the
training examples.
5 Conclusion
We explored nonparametric Bayesian learning
of non-isomorphic tree mappings using Dirich-
let process priors. We used the task of extrac-
tive sentence compression as a testbed to investi-
gate the effects of sparse priors and nonparamet-
ric inference over the space of grammars. We
showed that, despite its degeneracy, expectation
maximization is a strong baseline when given a
reasonable grammar. However, Gibbs-sampling?
based nonparametric inference achieves improve-
ments against this baseline. Our investigation with
variational Bayes showed that the improvement is
due both to finding sparse grammars (mitigating
overfitting) and to searching over the space of all
grammars (mitigating narrowness). Overall, we
take these results as being encouraging for STSG
induction via Bayesian nonparametrics for mono-
lingual translation tasks. The future for this work
would involve natural extensions such as mixing
over the space of word alignments; this would al-
low application to MT-like tasks where flexible
word reordering is allowed, such as abstractive
sentence compression and paraphrasing.
References
James Clarke and Mirella Lapata. 2006a. Constraint-
based sentence compression: An integer program-
ming approach. In Proceedings of the 21st Interna-
945
tional Conference on Computational Linguistics and
44th Annual Meeting of the Association for Compu-
tational Linguistics, pages 144?151, Sydney, Aus-
tralia, July. Association for Computational Linguis-
tics.
James Clarke and Mirella Lapata. 2006b. Models
for sentence compression: A comparison across do-
mains, training requirements and evaluation mea-
sures. In Proceedings of the 21st International Con-
ference on Computational Linguistics and 44th An-
nual Meeting of the Association for Computational
Linguistics, pages 377?384, Sydney, Australia, July.
Association for Computational Linguistics.
Trevor Cohn and Phil Blunsom. 2009. A Bayesian
model of syntax-directed tree to string grammar in-
duction. In EMNLP ?09: Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 352?361, Morristown, NJ,
USA. Association for Computational Linguistics.
Trevor Cohn and Mirella Lapata. 2007. Large mar-
gin synchronous generation and its application to
sentence compression. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing and on Computational Natural Lan-
guage Learning, pages 73?82, Prague. Association
for Computational Linguistics.
Trevor Cohn and Mirella Lapata. 2008. Sentence
compression beyond word deletion. In COLING
?08: Proceedings of the 22nd International Confer-
ence on Computational Linguistics, pages 137?144,
Manchester, United Kingdom. Association for Com-
putational Linguistics.
Trevor Cohn, Sharon Goldwater, and Phil Blun-
som. 2009. Inducing compact but accurate tree-
substitution grammars. In NAACL ?09: Proceed-
ings of Human Language Technologies: The 2009
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 548?556, Morristown, NJ, USA. Association
for Computational Linguistics.
A. Dempster, N. Laird, and D. Rubin. 1977. Max-
imum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society,
39 (Series B):1?38.
John DeNero, Dan Gillick, James Zhang, and Dan
Klein. 2006. Why generative phrase models under-
perform surface heuristics. In StatMT ?06: Proceed-
ings of the Workshop on Statistical Machine Trans-
lation, pages 31?38, Morristown, NJ, USA. Associ-
ation for Computational Linguistics.
John DeNero, Alexandre Bouchard-Co?te?, and Dan
Klein. 2008. Sampling alignment structure under
a Bayesian translation model. In EMNLP ?08: Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing, pages 314?323, Mor-
ristown, NJ, USA. Association for Computational
Linguistics.
Jason Eisner. 2003. Learning non-isomorphic tree
mappings for machine translation. In ACL ?03: Pro-
ceedings of the 41st Annual Meeting on Associa-
tion for Computational Linguistics, pages 205?208,
Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Michel Galley and Kathleen McKeown. 2007. Lex-
icalized Markov grammars for sentence compres-
sion. In Human Language Technologies 2007:
The Conference of the North American Chapter of
the Association for Computational Linguistics; Pro-
ceedings of the Main Conference, pages 180?187,
Rochester, New York, April. Association for Com-
putational Linguistics.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation
rule? In Daniel Marcu Susan Dumais and Salim
Roukos, editors, HLT-NAACL 2004: Main Proceed-
ings, pages 273?280, Boston, Massachusetts, USA,
May 2 - May 7. Association for Computational Lin-
guistics.
S. Geman and D. Geman. 1984. Stochastic Relaxation,
Gibbs Distributions and the Bayesian Restoration of
Images. pages 6:721?741.
Sharon Goldwater, Thomas L. Griffiths, and Mark
Johnson. 2006. Contextual dependencies in un-
supervised word segmentation. In Proceedings of
the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 673?680,
Sydney, Australia, July. Association for Computa-
tional Linguistics.
Hongyan Jing. 2000. Sentence reduction for auto-
matic text summarization. In Proceedings of the
sixth conference on Applied natural language pro-
cessing, pages 310?315, Morristown, NJ, USA. As-
sociation for Computational Linguistics.
Dan Klein and Christopher D. Manning. 2003. Fast
exact inference with a factored model for natural
language parsing. In Advances in Neural Informa-
tion Processing Systems 15 (NIPS, pages 3?10. MIT
Press.
Kevin Knight and Daniel Marcu. 2002. Summa-
rization beyond sentence extraction: a probabilis-
tic approach to sentence compression. Artif. Intell.,
139(1):91?107.
K. Lari and S. J. Young. 1990. The estimation of
stochastic context-free grammars using the Inside-
Outside algorithm. Computer Speech and Lan-
guage, 4:35?56.
Ding Liu and Daniel Gildea. 2009. Bayesian learn-
ing of phrasal tree-to-string templates. In Proceed-
ings of the 2009 Conference on Empirical Methods
in Natural Language Processing, pages 1308?1317,
Singapore, August. Association for Computational
Linguistics.
946
David J.C. MacKay. 1997. Ensemble learning for hid-
den markov models. Technical report, Cavendish
Laboratory, Cambridge, UK.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. Comput. Linguist., 30(4):417?449.
Matt Post and Daniel Gildea. 2009. Bayesian learning
of a tree substitution grammar. In Proceedings of the
ACL-IJCNLP 2009 Conference Short Papers, pages
45?48, Suntec, Singapore, August. Association for
Computational Linguistics.
Stefan Riezler, Tracy H. King, Richard Crouch, and
Annie Zaenen. 2003. Statistical sentence condensa-
tion using ambiguity packing and stochastic disam-
biguation methods for lexical-functional grammar.
In NAACL ?03: Proceedings of the 2003 Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology, pages 118?125, Morristown, NJ, USA.
Association for Computational Linguistics.
Jenine Turner and Eugene Charniak. 2005. Super-
vised and unsupervised learning for sentence com-
pression. In ACL ?05: Proceedings of the 43rd An-
nual Meeting on Association for Computational Lin-
guistics, pages 290?297, Morristown, NJ, USA. As-
sociation for Computational Linguistics.
Elif Yamangil and Rani Nelken. 2008. Mining
wikipedia revision histories for improving sentence
compression. In Proceedings of ACL-08: HLT,
Short Papers, pages 137?140, Columbus, Ohio,
June. Association for Computational Linguistics.
947
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 110?114,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Estimating Compact Yet Rich Tree Insertion Grammars
Elif Yamangil and Stuart M. Shieber
Harvard University
Cambridge, Massachusetts, USA
{elif, shieber}@seas.harvard.edu
Abstract
We present a Bayesian nonparametric model
for estimating tree insertion grammars (TIG),
building upon recent work in Bayesian in-
ference of tree substitution grammars (TSG)
via Dirichlet processes. Under our general
variant of TIG, grammars are estimated via
the Metropolis-Hastings algorithm that uses
a context free grammar transformation as a
proposal, which allows for cubic-time string
parsing as well as tree-wide joint sampling of
derivations in the spirit of Cohn and Blun-
som (2010). We use the Penn treebank for
our experiments and find that our proposal
Bayesian TIG model not only has competitive
parsing performance but also finds compact
yet linguistically rich TIG representations of
the data.
1 Introduction
There is a deep tension in statistical modeling of
grammatical structure between providing good ex-
pressivity ? to allow accurate modeling of the data
with sparse grammars ? and low complexity ?
making induction of the grammars and parsing of
novel sentences computationally practical. Recent
work that incorporated Dirichlet process (DP) non-
parametric models into TSGs has provided an effi-
cient solution to the problem of segmenting train-
ing data trees into elementary parse tree fragments
to form the grammar (Cohn et al, 2009; Cohn and
Blunsom, 2010; Post and Gildea, 2009). DP infer-
ence tackles this problem by exploring the space of
all possible segmentations of the data, in search for
fragments that are on the one hand large enough so
that they incorporate the useful dependencies, and
on the other small enough so that they recur and have
a chance to be useful in analyzing unseen data.
The elementary trees combined in a TSG are, in-
tuitively, primitives of the language, yet certain lin-
guistic phenomena (notably various forms of modifi-
cation) ?split them up?, preventing their reuse, lead-
ing to less sparse grammars than might be ideal.
For instance, imagine modeling the following set of
structures:
? [NP the [NN [NN [NN president] of the university] who
resigned yesterday]]
? [NP the [NN former [NN [NN president] of the univer-
sity]]]
? [NP the [NN [NN president] who resigned yesterday]]
A natural recurring structure here would be the
structure ?[NP the [NN president]]?, yet it occurs
not at all in the data.
TSGs are a special case of the more flexible gram-
mar formalism of tree adjoining grammar (TAG)
(Joshi et al, 1975). TAG augments TSG with an
adjunction operator and a set of auxiliary trees in
addition to the substitution operator and initial trees
of TSG, allowing for ?splicing in? of syntactic frag-
ments within trees. In the example, by augmenting a
TSG with an operation of adjunction, a grammar that
hypothesizes auxiliary trees corresponding to ad-
joining ?[NN former NN ]?, ?[NN NN of the uni-
versity]?, and ?[NN NN who resigned yesterday]?
would be able to reuse the basic structure ?[NP the
[NN president]]?.
Unfortunately, TAG?s expressivity comes at the
cost of greatly increased complexity. Parsing com-
plexity for unconstrained TAG scales as O(n6), im-
110
NP
DT NN
the president
NP
NP* SBAR
WHNP
S
who
NP
NP* PP
IN
NP
of
NN
JJ
NN*
former
Figure 1: Example TIG derivation of an NP constituent:
One left insertion (at NN) and two simultaneous right in-
sertions (at NP).
practical as compared to CFG and TSG?s O(n3). In
addition, the model selection problem for TAG is
significantly more complicated than for TSG since
one must reason about many more combinatorial op-
tions with two types of derivation operators.1 This
has led researchers to resort to heuristic grammar ex-
traction techniques (Chiang, 2000; Carreras et al,
2008) or using a very small number of grammar cat-
egories (Hwa, 1998).
Hwa (1998) first proposed to use tree-insertion
grammars (TIG), a kind of expressive compromise
between TSG and TAG, as a substrate on which to
build grammatical inference. TIG constrains the ad-
junction operation so that spliced-in material falls
completely to the left or completely to the right of
the splice point. By restricting the form of possible
auxiliary trees to only left or right auxiliary trees in
this way, TIG remains within the realm of context-
free formalisms (with cubic complexity) while still
modeling rich linguistic phenomena (Schabes and
Waters, 1995). Figure 1 depicts some examples of
TIG derivations.
Sharing the same intuitions, Shindo et al (2011)
have provided a previous attempt at combining TIG
and Bayesian nonparametric principles, albeit with
severe limitations. Their TIG variant (which we will
refer to as TIG0) is highly constrained in the follow-
ing ways.
1. The foot node in an auxiliary tree must be the immediate
child of the root node.
2. Only one adjunction can occur at a given node.
1This can be seen by the fact that tree-path languages under
TAG are context free, whereas they are regular for TSG. (Sch-
abes and Waters, 1995)
(a)
(b)
NP
NP
R
NP
L
NP
DT NN
the
NP
L
?
NN
president
NN
L
NN
R
NN
R
?
NP
NP*
PP
IN NP
of
?
NP
SBAR
WHNP
S
who
NP
R
NP
NP* PP
IN NP
of
NP
NP
R
?
NP
R
NP
NP* SBAR
WHNP
S
who
?
NP
R
NN
JJ
NN*
former ?
NN
L
Figure 2: TIG-to-TSG transform: (a) and (b) illus-
trate transformed TSG derivations for two different TIG
derivations of the same parse tree structure. The TIG
nodes where we illustrate the transformation are in bold.
(We suppress the rest of the transformational nodes.)
3. Even modeling multiple adjunction with root adjunction
is disallowed. There is thus no recursion possibility with
adjunction, no stacking of auxiliary trees.
4. As a consequence of the prior two constraints, no adjunc-
tion along the spines of auxiliary trees is allowed.
5. As a consequence of the first constraint, all nonterminals
along the spine of an auxiliary tree are identical.
In this paper we explore a Bayesian nonparamet-
ric model for estimating a far more expressive ver-
sion of TIG, and compare its performance against
TSG and the restricted TIG0 variant. Our more gen-
eral formulation avoids these limitations by support-
ing the following features and thus relaxing four of
the five restrictions of TIG0.
1. Auxiliary trees may have the foot node at depth greater
than one.2
2. Both left and right adjunctions may occur at the same
node.
3. Simultanous adjunction (that is, more than one left or
right adjunction per node) is allowed via root adjunction.
4. Adjunctions may occur along the spines of auxiliary trees.
The increased expressivity of our TIG variant is
motivated both linguistically and practically. From
a linguistic point of view: Deeper auxiliary trees can
help model large patterns of insertion and potential
correlations between lexical items that extend over
multiple levels of tree. Combining left and right
auxiliary trees can help model modifiers of the same
node from left and right (combination of adjectives
2Throughout the paper, we will refer to the depth of an aux-
iliary tree to indicate the length of its spine.
111
and relative clauses for instance). Simultaneous in-
sertion allows us to deal with multiple independent
modifiers for the same constituent (for example, a
series of adjectives). From a practical point of view,
we show that an induced TIG provides modeling
performance superior to TSG and comparable with
TIG0. However we show that the grammars we in-
duce are compact yet rich, in that they succinctly
represent complex linguistic structures.
2 Probabilistic Model
In the basic nonparametric TSG model, there is an
independent DP for every grammar category (such
as c = NP ), each of which uses a base distribution
P0 that generates an initial tree by making stepwise
decisions.
Ginitc ? DP(?
init
c , P
init
0 (? | c))
The canonical P0 uses a probabilistic CFG P? that
is fixed a priori to sample CFG rules top-down and
Bernoulli variables for determining where substitu-
tions should occur (Cohn et al, 2009; Cohn and
Blunsom, 2010).
We extend this model by adding specialized DPs
for left and right auxiliary trees.3
Grightc ? DP(?
right
c , P
right
0 (? | c))
Therefore, we have an exchangeable process for
generating right auxiliary trees
p(aj | a<j) =
naj + ?
right
c P
right
0 (aj | c)
j ? 1 + ?rightc
(1)
as for initial trees in TSG.
We must define three distinct base distributions
for initial trees, left auxiliary trees, and right aux-
iliary trees. P init0 generates an initial tree with root
label c by sampling CFG rules from P? and making
a binary decision at every node generated whether
to leave it as a frontier node or further expand (with
probability ?c) (Cohn et al, 2009). Similarly, our
P right0 generates a right auxiliary tree with root la-
bel c by first making a binary decision whether to
generate an immediate foot or not (with probability
?rightc ), and then sampling an appropriate CFG rule
3We use right insertions for illustration; the symmetric ana-
log applies to left insertions.
(VP (, ,) (VP PP (VP (, ,) VP*)))
(VP (SBAR (WHADVP (WRB (WRB When) ) ) S) (VP (, ,) VP*))
(VP (PP (IN For) (NP NN )) (VP (, ,) VP*))
(VP (CC But) (VP PP (VP (, ,) VP*)))
(VP ADVP (VP (, ,) VP*))
(IN (ADVP (RB (RB particularly) ) ) IN*)
(NP PP (NP (CC and) (NP PP NP*)))
Figure 3: Example left auxiliary trees that occur in the
top derivations for Section 23. Simultaneous insertions
occur most frequently for the labels VP (85 times), NNS
(21 times), NNP (14 times).
from P? . For the right child, we sample an initial tree
from P init0 . For the left child, if decision to gener-
ate an immediate foot was made, we generate a foot
node, and stop. Otherwise we recur into P right0 which
generates a right auxiliary tree that becomes the left
child.
We bring together these three sets of processes
via a set of insertion parameters ?leftc , ?
right
c . In any
derivation, for every initial tree node labelled c (ex-
cept for frontier nodes) we determine whether or
not there are insertions at this node by sampling a
Bernoulli(?leftc ) distributed left insertion variable and
a Bernoulli(?rightc ) distributed right insertion vari-
able. For left auxiliary trees, we treat the nodes that
are not along the spine of the auxiliary tree the same
way we treat initial tree nodes, however for nodes
that are along the spine (including root nodes, ex-
cluding foot nodes) we consider only left insertions
by sampling the left insertion variable (symmetri-
cally for right insertions).
3 Inference
Given this model, our inference task is to explore
optimal derivations underlying the data. Since TIG
derivations are highly structured objects, a basic
sampling strategy based on local node-level moves
such as Gibbs sampling (Geman and Geman, 1984)
would not hold much promise. Following previ-
ous work, we design a blocked Metropolis-Hastings
sampler that samples derivations per entire parse
trees all at once in a joint fashion (Cohn and Blun-
som, 2010; Shindo et al, 2011). This is achieved by
proposing derivations from an approximating distri-
bution and stochastically correcting via accept/reject
to achieve convergence into the correct posterior
(Johnson et al, 2007).
Since our base distributions factorize over levels
of tree, CFG is the most convenient choice for a
112
CFG rule CFG probability
Base distribution: P init0
NP? NPinit ?initc /(n
init
NP + ?
init
c )
NPinit? NPL NP
init NPR 1.0
NPinit? DT NN P? (NP? DT NN)? (1? ?DT)? (1? ?NN)
NPinit? DT NNinit P? (NP? DT NN)? (1? ?DT)? ?NN
NPinit? DTinit NN P? (NP? DT NN)? ?DT ? (1? ?NN)
NPinit? DTinit NNinit P? (NP? DT NN)? ?DT ? ?NN
Base distribution: P right0
NPR? NP
right ?rightNP ?
(
?rightc /(n
right
NP + ?
right
c )
)
NPR?  1? ?
right
NP
NPright? NPright NPR 1.0
NPright? NP* SBAR
init P? (NP? NP SBAR | NP? NP )
?(1? ?rightNP )? (1? ?SBAR)
NPright? NP* SBAR P? (NP? NP SBAR | NP? NP )
?(1? ?rightNP )? ?SBAR
NPright? NPright SBARinit P? (NP? NP SBAR | NP? NP )
??rightNP ? (1? ?SBAR)
NPright? NPright SBAR P? (NP? NP SBAR | NP? NP )
??rightNP ? ?SBAR
Figure 4: Transformation CFG rules that represent infi-
nite base distributions. P init0 is taken from Cohn and Blun-
som (2010). Underscored labels (such as NPright as op-
posed to NPright) are used to differentiate the pre-insertion
nodes in Figure 2 from the post-insertion ones. P left0 rules
are omitted for brevity and mirror the P right0 rules above.
Model FMeasure # Initial Trees # Auxiliary Trees (# Left)
TSG 77.51 6.2K -
TIG0 78.46 6.0K 251 (137)
TIG 78.62 5.6K 604 (334)
Figure 5: EVALB results after training on Section 2 and
testing on Section 23. Note that TIG finds a compact yet
rich representation. Elementary tree counts are based on
ones with count > 1.
proposal distribution. Fortunately, Schabes and Wa-
ters (1995) provide an (exact) transformation from a
fully general TIG into a TSG that generates the same
string languages. It is then straightforward to repre-
sent this TSG as a CFG using the Goodman trans-
form (Goodman, 2002; Cohn and Blunsom, 2010).
Figure 4 lists the additional CFG productions we
have designed, as well as the rules used that trigger
them.
4 Evaluation Results
We use the standard Penn treebank methodology of
training on sections 2?21 and testing on section 23.
All our data is head-binarized and words occurring
only once are mapped into unknown categories of
the Berkeley parser. As has become standard, we
carried out a small treebank experiment where we
train on Section 2, and a large one where we train
on the full training set. All hyperparameters are re-
sampled under appropriate vague gamma and beta
priors. All reported numbers are averages over three
runs. Parsing results are based on the maximum
probability parse which was obtained by sampling
derivations under the transform CFG.
We compare our system (referred to as TIG) to
our implementation of the TSG system of (Cohn
and Blunsom, 2010) (referred to as TSG) and the
constrained TIG variant of (Shindo et al, 2011) (re-
ferred to as TIG0). The upshot of our experiments is
that, while on the large training set al models have
similar performance (85.6, 85.3, 85.4 for TSG, TIG0
and TIG respectively), on the small dataset inser-
tion helps nonparametric model to find more com-
pact and generalizable representations for the data,
which affects parsing performance (Figure 4). Al-
though TIG0 has performance close to TIG, note that
TIG achieves this performance using a more suc-
cinct representation and extracting a rich set of aux-
iliary trees. As a result, TIG finds many chances to
apply insertions to test sentences, whereas TIG0 de-
pends mostly on TSG rules. If we look at the most
likely derivations for the test data, TIG0 assigns 663
insertions (351 left insertions) in the parsing of en-
tire Section 23, meanwhile TIG assigns 3924 (2100
left insertions). Some of these linguistically sophis-
ticated auxiliary trees that apply to test data are listed
in Figure 3.
5 Conclusion
We described a nonparametric Bayesian inference
scheme for estimating TIG grammars and showed
the power of TIG formalism over TSG for returning
rich, generalizable, yet compact representations of
data. The nonparametric inference scheme presents
a principled way of addressing the difficult model
selection problem with TIG which has been pro-
hibitive in this area of research. TIG still remains
within context free and both our sampling and pars-
ing techniques are highly scalable.
Acknowledgements
The first author was supported in part by a Google
PhD Fellowship in Natural Language Processing.
113
References
Xavier Carreras, Michael Collins, and Terry Koo. 2008.
TAG, dynamic programming, and the perceptron for
efficient, feature-rich parsing. In Proceedings of the
Twelfth Conference on Computational Natural Lan-
guage Learning, CoNLL ?08, pages 9?16, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
David Chiang. 2000. Statistical parsing with an
automatically-extracted tree adjoining grammar. In
Proceedings of the 38th Annual Meeting on Associa-
tion for Computational Linguistics, ACL ?00, pages
456?463, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Trevor Cohn and Phil Blunsom. 2010. Blocked in-
ference in Bayesian tree substitution grammars. In
Proceedings of the ACL 2010 Conference Short Pa-
pers, ACLShort ?10, pages 225?230, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Trevor Cohn, Sharon Goldwater, and Phil Blunsom.
2009. Inducing compact but accurate tree-substitution
grammars. In NAACL ?09: Proceedings of Human
Language Technologies: The 2009 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics, pages 548?556, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
S. Geman and D. Geman. 1984. Stochastic Relaxation,
Gibbs Distributions and the Bayesian Restoration of
Images. pages 6:721?741.
J. Goodman. 2002. Efficient parsing of DOP with
PCFG-reductions. Bod et al 2003.
Rebecca Hwa. 1998. An empirical evaluation of prob-
abilistic lexicalized tree insertion grammars. In Pro-
ceedings of the 17th international conference on Com-
putational linguistics - Volume 1, pages 557?563, Mor-
ristown, NJ, USA. Association for Computational Lin-
guistics.
Mark Johnson, Thomas Griffiths, and Sharon Goldwa-
ter. 2007. Bayesian inference for PCFGs via Markov
chain Monte Carlo. In Human Language Technologies
2007: The Conference of the North American Chap-
ter of the Association for Computational Linguistics;
Proceedings of the Main Conference, pages 139?146,
Rochester, New York, April. Association for Compu-
tational Linguistics.
Aravind K. Joshi, Leon S. Levy, and Masako Takahashi.
1975. Tree adjunct grammars. Journal of Computer
and System Sciences, 10(1):136?163.
Matt Post and Daniel Gildea. 2009. Bayesian learn-
ing of a tree substitution grammar. In Proceedings
of the ACL-IJCNLP 2009 Conference Short Papers,
pages 45?48, Suntec, Singapore, August. Association
for Computational Linguistics.
Remko Scha and Rens Bod. 2003. Efficient parsing of
DOP with PCFG-reductions, October.
Yves Schabes and Richard C. Waters. 1995. Tree in-
sertion grammar: a cubic-time parsable formalism that
lexicalizes context-free grammar without changing the
trees produced. Comput. Linguist., 21:479?513, De-
cember.
Hiroyuki Shindo, Akinori Fujino, and Masaaki Nagata.
2011. Insertion operator for Bayesian tree substitution
grammars. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics: Hu-
man Language Technologies: short papers - Volume
2, HLT ?11, pages 206?211, Stroudsburg, PA, USA.
Association for Computational Linguistics.
114
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 302?310,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
A Context Free TAG Variant
Ben Swanson
Brown University
Providence, RI
chonger@cs.brown.edu
Eugene Charniak
Brown University
Providence, RI
ec@cs.brown.edu
Elif Yamangil
Harvard University
Cambridge, MA
elif@eecs.harvard.edu
Stuart Shieber
Harvard University
Cambridge, MA
shieber@eecs.harvard.edu
Abstract
We propose a new variant of Tree-
Adjoining Grammar that allows adjunc-
tion of full wrapping trees but still bears
only context-free expressivity. We provide
a transformation to context-free form, and
a further reduction in probabilistic model
size through factorization and pooling of
parameters. This collapsed context-free
form is used to implement efficient gram-
mar estimation and parsing algorithms.
We perform parsing experiments the Penn
Treebank and draw comparisons to Tree-
Substitution Grammars and between dif-
ferent variations in probabilistic model de-
sign. Examination of the most probable
derivations reveals examples of the lin-
guistically relevant structure that our vari-
ant makes possible.
1 Introduction
While it is widely accepted that natural language
is not context-free, practical limitations of ex-
isting algorithms motivate Context-Free Gram-
mars (CFGs) as a good balance between model-
ing power and asymptotic performance (Charniak,
1996). In constituent-based parsing work, the pre-
vailing technique to combat this divide between
efficient models and real world data has been to
selectively strengthen the dependencies in a CFG
by increasing the grammar size through methods
such as symbol refinement (Petrov et al, 2006).
Another approach is to employ a more power-
ful grammatical formalism and devise constraints
and transformations that allow use of essential ef-
ficient algorithms such as the Inside-Outside al-
gorithm (Lari and Young, 1990) and CYK pars-
ing. Tree-Adjoining Grammar (TAG) is a natural
starting point for such methods as it is the canoni-
cal member of the mildly context-sensitive family,
falling just above CFGs in the hierarchy of for-
mal grammars. TAG has a crucial advantage over
CFGs in its ability to represent long distance in-
teractions in the face of the interposing variations
that commonly manifest in natural language (Joshi
and Schabes, 1997). Consider, for example, the
sentences
These pretzels are making me thirsty.
These pretzels are not making me thirsty.
These pretzels that I ate are making me thirsty.
Using a context-free language model with
proper phrase bracketing, the connection between
the words pretzels and thirsty must be recorded
with three separate patterns, which can lead to
poor generalizability and unreliable sparse fre-
quency estimates in probabilistic models. While
these problems can be overcome to some extent
with large amounts of data, redundant representa-
tion of patterns is particularly undesirable for sys-
tems that seek to extract coherent and concise in-
formation from text.
TAG allows a linguistically motivated treatment
of the example sentences above by generating the
last two sentences through modification of the
first, applying operations corresponding to nega-
tion and the use of a subordinate clause. Un-
fortunately, the added expressive power of TAG
comes with O(n6) time complexity for essential
algorithms on sentences of length n, as opposed to
O(n3) for the CFG (Schabes, 1990). This makes
TAG infeasible to analyze real world data in a rea-
sonable time frame.
In this paper, we define OSTAG, a new way to
constrain TAG in a conceptually simple way so
302
SNP VP NP
NP
DT
the
NN
lack
NP
NNS
computers
VP
VBP
do
RB
not
VP
NP
NP PP
NP
PRP
I
PP
IN
of
PRP
them
VP
VB
fear
Figure 1: A simple Tree-Substitution Grammar using S as its start symbol. This grammar derives the
sentences from a quote of Isaac Asimov?s - ?I do not fear computers. I fear the lack of them.?
that it can be reduced to a CFG, allowing the use of
traditional cubic-time algorithms. The reduction is
reversible, so that the original TAG derivation can
be recovered exactly from the CFG parse. We pro-
vide this reduction in detail below and highlight
the compression afforded by this TAG variant on
synthetic formal languages.
We evaluate OSTAG on the familiar task of
parsing the Penn Treebank. Using an automati-
cally induced Tree-Substitution Grammar (TSG),
we heuristically extract an OSTAG and estimate
its parameters from data using models with var-
ious reduced probabilistic models of adjunction.
We contrast these models and investigate the use
of adjunction in the most probable derivations of
the test corpus, demonstating the superior model-
ing performance of OSTAG over TSG.
2 TAG and Variants
Here we provide a short history of the relevant
work in related grammar formalisms, leading up
to a definition of OSTAG. We start with context-
free grammars, the components of which are
?N,T,R, S?, where N and T are the sets of non-
terminal and terminal symbols respectively, and S
is a distinguished nonterminal, the start symbol.
The rules R can be thought of as elementary trees
of depth 1, which are combined by substituting a
derived tree rooted at a nonterminalX at some leaf
node in an elementary tree with a frontier node
labeled with that same nonterminal. The derived
trees rooted at the start symbol S are taken to be
the trees generated by the grammar.
2.1 Tree-Substitution Grammar
By generalizing CFG to allow elementary trees in
R to be of depth greater than or equal to 1, we
get the Tree-Substitution Grammar. TSG remains
in the family of context-free grammars, as can be
easily seen by the removal of the internal nodes
in all elementary trees; what is left is a CFG that
generates the same language. As a reversible al-
ternative that preserves the internal structure, an-
notation of each internal node with a unique index
creates a large number of deterministic CFG rules
that record the structure of the original elementary
trees. A more compact CFG representation can be
obtained by marking each node in each elemen-
tary tree with a signature of its subtree. This trans-
form, presented by Goodman (2003), can rein in
the grammar constant G, as the crucial CFG algo-
rithms for a sentence of length n have complexity
O(Gn3).
A simple probabilistic model for a TSG is a set
of multinomials, one for each nonterminal in N
corresponding to its possible substitutions in R. A
more flexible model allows a potentially infinite
number of substitution rules using a Dirichlet Pro-
cess (Cohn et al, 2009; Cohn and Blunsom, 2010).
This model has proven effective for grammar in-
duction via Markov Chain Monte Carlo (MCMC),
in which TSG derivations of the training set are re-
peatedly sampled to find frequently occurring el-
ementary trees. A straightforward technique for
induction of a finite TSG is to perform this non-
parametric induction and select the set of rules that
appear in at least one sampled derivation at one or
several of the final iterations.
2.2 Tree-Adjoining Grammar
Tree-adjoining grammar (TAG) (Joshi, 1985;
Joshi, 1987; Joshi and Schabes, 1997) is an exten-
sion of TSG defined by a tuple ?N,T,R,A, S?,
and differs from TSG only in the addition of a
303
VP
always VP
VP* quickly
+ S
NP VP
runs
? S
NP VP
always VP
VP
runs
quickly
Figure 2: The adjunction operation combines the auxiliary tree (left) with the elementary tree (middle)
to form a new derivation (right). The adjunction site is circled, and the foot node of the auxiliary tree is
denoted with an asterisk. The OSTAG constraint would disallow further adjunction at the bold VP node
only, as it is along the spine of the auxiliary tree.
set of auxiliary trees A and the adjunction oper-
ation that governs their use. An auxiliary tree ?
is an elementary tree containing a single distin-
guished nonterminal leaf, the foot node, with the
same symbol as the root of ?. An auxiliary tree
with root and foot node X can be adjoined into an
internal node of an elementary tree labeled with
X by splicing the auxiliary tree in at that internal
node, as pictured in Figure 2. We refer to the path
between the root and foot nodes in an auxiliary
tree as the spine of the tree.
As mentioned above, the added power afforded
by adjunction comes at a serious price in time
complexity. As such, probabilistic modeling for
TAG in its original form is uncommon. However,
a large effort in non-probabilistic grammar induc-
tion has been performed through manual annota-
tion with the XTAG project(Doran et al, 1994).
2.3 Tree Insertion Grammar
Tree Insertion Grammars (TIGs) are a longstand-
ing compromise between the intuitive expressivity
of TAG and the algorithmic simplicity of CFGs.
Schabes and Waters (1995) showed that by re-
stricting the form of the auxiliary trees in A and
the set of auxiliary trees that may adjoin at par-
ticular nodes, a TAG generates only context-free
languages. The TIG restriction on auxiliary trees
states that the foot node must occur as either the
leftmost or rightmost leaf node. This introduces
an important distinction between left, right, and
wrapping auxiliary trees, of which only the first
two are allowed in TIG. Furthermore, TIG disal-
lows adjunction of left auxiliary trees on the spines
of right auxiliary trees, and vice versa. This is
to prevent the construction of wrapping auxiliary
trees, whose removal is essential for the simplified
complexity of TIG.
Several probabilistic models have been pro-
posed for TIG. While earlier approaches such as
Hwa (1998) and Chiang (2000) relied on hueristic
induction methods, they were nevertheless sucess-
ful at parsing. Later approaches (Shindo et al,
2011; Yamangil and Shieber, 2012) were able to
extend the non-parametric modeling of TSGs to
TIG, providing methods for both modeling and
grammar induction.
2.4 OSTAG
Our new TAG variant is extremely simple. We al-
low arbitrary initial and auxiliary trees, and place
only one restriction on adjunction: we disallow
adjunction at any node on the spine of an aux-
iliary tree below the root (though we discuss re-
laxing that constraint in Section 4.2). We refer to
this variant as Off Spine TAG (OSTAG) and note
that it allows the use of full wrapping rules, which
are forbidden in TIG. This targeted blocking of
recursion has similar motivations and benefits to
the approximation of CFGs with regular languages
(Mohri and jan Nederhof, 2000).
The following sections discuss in detail the
context-free nature of OSTAG and alternative
probabilistic models for its equivalent CFG form.
We propose a simple but empirically effective
heuristic for grammar induction for our experi-
ments on Penn Treebank data.
3 Transformation to CFG
To demonstrate that OSTAG has only context-
free power, we provide a reduction to context-free
grammar. Given an OSTAG ?N,T,R,A, S?, we
define the set N of nodes of the corresponding
CFG to be pairs of a tree inR orA together with an
304
?: S
T
x
T
y
?: T
a T* a
?: T
b T* b
S ? X Y S ? X Y
X ? x X ? x
Y ? y Y ? y
X ? A
X ? B
Y ? A?
Y ? B?
A ? a X ? a X ? a X a
A? ? a Y ? a Y ? a Y a
X ? ? X
Y ? ? Y
B ? b X ?? b X ? b X b
B? ? b Y ?? b Y ? b Y b
X ?? ? X
Y ?? ? Y
(a) (b) (c)
Figure 3: (a) OSTAG for the language wxwRvyvR where w, v ? {a|b}+ and R reverses a string. (b) A
CFG for the same language, which of necessity must distinguish between nonterminalsX and Y playing
the role of T in the OSTAG. (c) Simplified CFG, conflating nonterminals, but which must still distinguish
between X and Y .
address (Gorn number (Gorn, 1965)) in that tree.
We take the nonterminals of the target CFG gram-
mar to be nodes or pairs of nodes, elements of the
setN +N ?N . We notate the pairs of nodes with
a kind of ?applicative? notation. Given two nodes
? and ??, we notate a target nonterminal as ?(??).
Now for each tree ? and each interior node ?
in ? that is not on the spine of ? , with children
?1, . . . , ?k, we add a context-free rule to the gram-
mar
? ? ?1 ? ? ? ?k (1)
and if interior node ? is on the spine of ? with
?s the child node also on the spine of ? (that is,
dominating the foot node of ? ) and ?? is a node (in
any tree) where ? is adjoinable, we add a rule
?(??)? ?1 ? ? ? ?s(??) ? ? ? ?k . (2)
Rules of type (1) handle the expansion of a node
not on the spine of an auxiliary tree and rules of
type (2) a spinal node.
In addition, to initiate adjunction at any node ??
where a tree ? with root ? is adjoinable, we use a
rule
?? ? ?(??) (3)
and for the foot node ?f of ? , we use a rule
?f (?)? ? (4)
The OSTAG constraint follows immediately
from the structure of the rules of type (2). Any
child spine node ?s manifests as a CFG nonter-
minal ?s(??). If child spine nodes themselves al-
lowed adjunction, we would need a type (3) rule
of the form ?s(??) ? ?s(??)(???). This rule itself
would feed adjunction, requiring further stacking
of nodes, and an infinite set of CFG nonterminals
and rules. This echoes exactly the stacking found
in the LIG reduction of TAG .
To handle substitution, any frontier node ? that
allows substitution of a tree rooted with node ??
engenders a rule
? ? ?? (5)
This transformation is reversible, which is to
say that each parse tree derived with this CFG im-
plies exactly one OSTAG derivation, with substi-
tutions and adjunctions coded by rules of type (5)
and (3) respectively. Depending on the definition
of a TAG derivation, however, the converse is not
necessarily true. This arises from the spurious am-
biguity between adjunction at a substitution site
(before applying a type (5) rule) versus the same
adjunction at the root of the substituted initial tree
(after applying a type (5) rule). These choices
lead to different derivations in CFG form, but their
TAG derivations can be considered conceptually
305
identical. To avoid double-counting derivations,
which can adversely effect probabilistic modeling,
type (3) and type (4) rules in which the side with
the unapplied symbol is a nonterminal leaf can be
omitted.
3.1 Example
The grammar of Figure 3(a) can be converted to
a CFG by this method. We indicate for each CFG
rule its type as defined above the production arrow.
All types are used save type (5), as substitution
is not employed in this example. For the initial
tree ?, we have the following generated rules (with
nodes notated by the tree name and a Gorn number
subscript):
? 1?? ?1 ?2 ?1 3?? ?(?1)
?1 1?? x ?1 3?? ?(?1)
?2 1?? y ?2 3?? ?(?2)
?2 3?? ?(?2)
For the auxiliary trees ? and ? we have:
?(?1) 2?? a ?1(?1) a
?(?2) 2?? a ?1(?2) a
?1(?1) 4?? ?1
?1(?2) 4?? ?2
?(?1) 2?? b ?1(?1) b
?(?2) 2?? b ?1(?2) b
?1(?1) 4?? ?1
?1(?2) 4?? ?2
The grammar of Figure 3(b) is simply a renaming
of this grammar.
4 Applications
4.1 Compact grammars
The OSTAG framework provides some leverage in
expressing particular context-free languages more
compactly than a CFG or even a TSG can. As
an example, consider the language of bracketed
palindromes
Pal = aiw aiwR ai
1 ? i ? k
w ? {bj | 1 ? j ? m}?
containing strings like a2 b1b3 a2 b3b1 a2. Any
TSG for this language must include as substrings
some subpalindrome constituents for long enough
strings. Whatever nonterminal covers such a
string, it must be specific to the a index within
it, and must introduce at least one pair of bs as
well. Thus, there are at least m such nontermi-
nals, each introducing at least k rules, requiring at
least km rules overall. The simplest such gram-
mar, expressed as a CFG, is in Figure 4(a). The
ability to use adjunction allows expression of the
same language as an OSTAG with k +m elemen-
tary trees (Figure 4(b)). This example shows that
an OSTAG can be quadratically smaller than the
corresponding TSG or CFG.
4.2 Extensions
The technique in OSTAG can be extended to ex-
pand its expressiveness without increasing gener-
ative capacity.
First, OSTAG allows zero adjunctions on each
node on the spine below the root of an auxiliary
tree, but any non-zero finite bound on the num-
ber of adjunctions allowed on-spine would simi-
larly limit generative capacity. The tradeoff is in
the grammar constant of the effective probabilis-
tic CFG; an extension that allows k levels of on
spine adjunction has a grammar constant that is
O(|N |(k+2)).
Second, the OSTAG form of adjunction is con-
sistent with the TIG form. That is, we can extend
OSTAG by allowing on-spine adjunction of left- or
right-auxiliary trees in keeping with the TIG con-
straints without increasing generative capacity.
4.3 Probabilistic OSTAG
One major motivation for adherence to a context-
free grammar formalism is the ability to employ
algorithms designed for probabilistic CFGs such
as the CYK algorithm for parsing or the Inside-
Outside algorithm for grammar estimation. In this
section we present a probabilistic model for an OS-
TAG grammar in PCFG form that can be used in
such algorithms, and show that many parameters
of this PCFG can be pooled or set equal to one and
ignored. References to rules of types (1-5) below
refer to the CFG transformation rules defined in
Section 3. While in the preceeding discussion we
used Gorn numbers for clarity, our discussion ap-
plies equally well for the Goodman transform dis-
cussed above, in which each node is labeled with a
signature of its subtree. This simply redefines ? in
the CFG reduction described in Section 3 to be a
subtree indicator, and dramatically reduces redun-
dancy in the generated grammar.
306
S ? ai Ti ai
Ti ? bj Ti bj
Ti ? ai
?i | 1 ? i ? k: S
ai T
ai
ai
?j | 1 ? j ? m: T
bj T* bj
(a) (b)
Figure 4: A CFG (a) and more compact OSTAG (b) for the language Pal
The first practical consideration is that CFG
rules of type (2) are deterministic, and as such
we need only record the rule itself and no asso-
ciated parameter. Furthermore, these rules employ
a template in which the stored symbol appears in
the left-hand side and in exactly one symbol on
the right-hand side where the spine of the auxil-
iary tree proceeds. One deterministic rule exists
for this template applied to each ?, and so we may
record only the template. In order to perform CYK
or IO, it is not even necessary to record the index
in the right-hand side where the spine continues;
these algorithms fill a chart bottom up and we can
simply propagate the stored nonterminal up in the
chart.
CFG rules of type (4) are also deterministic and
do not require parameters. In these cases it is not
necessary to record the rules, as they all have ex-
actly the same form. All that is required is a check
that a given symbol is adjoinable, which is true for
all symbols except nonterminal leaves and applied
symbols. Rules of type (5) are necessary to cap-
ture the probability of substitution and so we will
require a parameter for each.
At first glance, it would seem that due to the
identical domain of the left-hand sides of rules of
types (1) and (3) a parameter is required for each
such rule. To avoid this we propose the follow-
ing factorization for the probabilistic expansion of
an off spine node. First, a decision is made as to
whether a type (1) or (3) rule will be used; this cor-
responds to deciding if adjunction will or will not
take place at the node. If adjunction is rejected,
then there is only one type (1) rule available, and
so parameterization of type (1) rules is unneces-
sary. If we decide on adjunction, one of the avail-
able type (3) rules is chosen from a multinomial.
By conditioning the probability of adjunction on
varying amounts of information about the node,
alternative models can easily be defined.
5 Experiments
As a proof of concept, we investigate OSTAG in
the context of the classic Penn Treebank statistical
parsing setup; training on section 2-21 and testing
on section 23. For preprocessing, words that oc-
cur only once in the training data are mapped to
the unknown categories employed in the parser of
Petrov et al (2006). We also applied the annota-
tion from Klein and Manning (2003) that appends
?-U? to each nonterminal node with a single child,
drastically reducing the presence of looping unary
chains. This allows the use of a coarse to fine
parsing strategy (Charniak et al, 2006) in which
a sentence is first parsed with the Maximum Like-
lihood PCFG and only constituents whose prob-
ability exceeds a cutoff of 10?4 are allowed in
the OSTAG chart. Designed to facilitate sister ad-
junction, we define our binarization scheme by ex-
ample in which the added nodes, indicated by @,
record both the parent and head child of the rule.
NP
@NN-NP
@NN-NP
DT @NN-NP
JJ NN
SBAR
A compact TSG can be obtained automatically
using the MCMC grammar induction technique of
Cohn and Blunsom (2010), retaining all TSG rules
that appear in at least one derivation in after 1000
iterations of sampling. We use EM to estimate the
parameters of this grammar on sections 2-21, and
use this as our baseline.
To generate a set of TAG rules, we consider
each rule in our baseline TSG and find all possi-
307
All 40 #Adj #Wrap
TSG 85.00 86.08 ? ?
TSG? 85.12 86.21 ? ?
OSTAG1 85.42 86.43 1336 52
OSTAG2 85.54 86.56 1952 44
OSTAG3 85.86 86.84 3585 41
Figure 5: Parsing F-Score for the models under
comparison for both the full test set and sentences
of length 40 or less. For the OSTAG models, we
list the number of adjunctions in the MPD of the
full test set, as well as the number of wrapping
adjunctions.
ble auxiliary root and foot node pairs it contains.
For each such root/foot pair, we include the TAG
rule implied by removal of the structure above the
root and below the foot. We also include the TSG
rule left behind when the adjunction of this auxil-
iary tree is removed. To be sure that experimental
gains are not due to this increased number of TSG
initial trees, we calculate parameters using EM for
this expanded TSG and use it as a second base-
line (TSG?). With our full set of initial and aux-
iliary trees, we use EM and the PCFG reduction
described above to estimate the parameters of an
OSTAG.
We investigate three models for the probabil-
ity of adjunction at a node. The first uses a con-
servative number of parameters, with a Bernoulli
variable for each symbol (OSTAG1). The second
employs more parameters, conditioning on both
the node?s symbol and the symbol of its leftmost
child (OSTAG2).The third is highly parameterized
but most prone to data sparsity, with a separate
Bernoulli distribution for each Goodman index ?
(OSTAG3). We report results for Most Probable
Derivation (MPD) parses of section 23 in Figure
5.
Our results show that OSTAG outperforms both
baselines. Furthermore, the various parameteri-
zations of adjunction with OSTAG indicate that,
at least in the case of the Penn Treebank, the
finer grained modeling of a full table of adjunction
probabilities for each Goodman index OSTAG3
overcomes the danger of sparse data estimates.
Not only does such a model lead to better parsing
performance, but it uses adjunction more exten-
sively than its more lightly parameterized alterna-
tives. While different representations make direct
comparison inappropriate, the OSTAG results lie
in the same range as previous work with statistical
TIG on this task, such as Chiang (2000) (86.00)
and Shindo et al (2011) (85.03).
The OSTAG constraint can be relaxed as de-
scribed in Section 4.2 to allow any finite number of
on-spine adjunctions without sacrificing context-
free form. However, the increase to the grammar
constant quickly makes parsing with such models
an arduous task. To determine the effect of such a
relaxation, we allow a single level of on-spine ad-
junction using the adjunction model of OSTAG1,
and estimate this model with EM on the training
data. We parse sentences of length 40 or less in
section 23 and observe that on-spine adjunction is
never used in the MPD parses. This suggests that
the OSTAG constraint is reasonable, at least for
the domain of English news text.
We performed further examination of the MPD
using OSTAG for each of the sentences in the test
corpus. As an artifact of the English language, the
majority have their foot node on the left spine and
would also be usable by TIG, and so we discuss
the instances of wrapping auxiliary trees in these
derivations that are uniquely available to OSTAG.
We remove binarization for clarity and denote the
foot node with an asterisk.
A frequent use of wrapping adjunction is to co-
ordinate symbols such as quotes, parentheses, and
dashes on both sides of a noun phrase. One com-
mon wrapping auxiliary tree in our experiments is
NP
? NP* ? PP
This is used frequently in the news text of
the Wall Street Journal for reported speech when
avoiding a full quotation. This sentence is an ex-
ample of the way the rule is employed, using what
Joshi and Schabes (1997) referred to as ?factoring
recursion from linguistic constraints? with TAG.
Note that replacing the quoted noun phrase and
its following prepositional phrase with the noun
phrase itself yields a valid sentence, in line with
the linguistic theory underlying TAG.
Another frequent wrapping rule, shown below,
allows direct coordination between the contents of
an appositive with the rest of the sentence.
308
NP
NP , CC
or
NP* ,
This is a valuable ability, as it is common to
use an appositive to provide context or explanation
for a proper noun. As our information on proper
nouns will most likely be very sparse, the apposi-
tive may be more reliably connected to the rest of
the sentence. An example of this from one of the
sentences in which this rule appears in the MPD is
the phrase ?since the market fell 156.83, or 8 %,
a week after Black Monday?. The wrapping rule
allows us to coordinate the verb ?fell? with the pat-
tern ?X %? instead of 156.83, which is mapped to
an unknown word category.
These rules highlight the linguistic intuitions
that back TAG; if their adjunction were undone,
the remaining derivation would be a valid sen-
tence that simply lacks the modifying structure of
the auxiliary tree. However, the MPD parses re-
veal that not all useful adjunctions conform to this
paradigm, and that left-auxiliary trees that are not
used for sister adjunction are susceptible to this
behavior. The most common such tree is used to
create noun phrases such as
P&G?s share of [the Japanese market]
the House?s repeal of [a law]
Apple?s family of [Macintosh Computers]
Canada?s output of [crude oil]
by adjoining the shared unbracketed syntax onto
the NP dominating the bracketed text. If adjunc-
tion is taken to model modification, this rule dras-
tically changes the semantics of the unmodified
sentence. Furthermore, in some cases removing
the adjunction can leave a grammatically incorrect
sentence, as in the third example where the noun
phrase changes plurality.
While our grammar induction method is a crude
(but effective) heuristic, we can still highlight the
qualities of the more important auxiliary trees
by examining aggregate statistics over the MPD
parses, shown in Figure 6. The use of left-
auxiliary trees for sister adjunction is a clear trend,
as is the predominant use of right-auxiliary trees
for the complementary set of ?regular? adjunc-
tions, which is to be expected in a right branch-
ing language such as English. The statistics also
All Wrap Right Left
Total 3585 (1374) 41 (26) 1698 (518) 1846 (830)
Sister 2851 (1180) 17 (11) 1109 (400) 1725 (769)
Lex 2244 (990) 28 (19) 894 (299) 1322 (672)
FLex 1028 (558) 7 (2) 835 (472) 186 (84)
Figure 6: Statistics for MPD auxiliary trees us-
ing OSTAG3. The columns indicate type of aux-
iliary tree and the rows correspond respectively to
the full set found in the MPD, those that perform
sister adjunction, those that are lexicalized, and
those that are fully lexicalized. Each cell shows
the number of tokens followed by the number of
types of auxiliary tree that fit its conditions.
reflect the importance of substitution in right-
auxiliary trees, as they must capture the wide va-
riety of right branching modifiers of the English
language.
6 Conclusion
The OSTAG variant of Tree-Adjoining Grammar
is a simple weakly context-free formalism that
still provides for all types of adjunction and is
a bit more concise than TSG (quadratically so).
OSTAG can be reversibly transformed into CFG
form, allowing the use of a wide range of well
studied techniques in statistical parsing.
OSTAG provides an alternative to TIG as a
context-free TAG variant that offers wrapping ad-
junction in exchange for recursive left/right spine
adjunction. It would be interesting to apply both
OSTAG and TIG to different languages to deter-
mine where the constraints of one or the other are
more or less appropriate. Another possibility is the
combination of OSTAG with TIG, which would
strictly expand the abilities of both approaches.
The most important direction of future work for
OSTAG is the development of a principled gram-
mar induction model, perhaps using the same tech-
niques that have been successfully applied to TSG
and TIG. In order to motivate this and other re-
lated research, we release our implementation of
EM and CYK parsing for OSTAG1. Our system
performs the CFG transform described above and
optionally employs coarse to fine pruning and re-
laxed (finite) limits on the number of spine adjunc-
tions. As a TSG is simply a TAG without adjunc-
tion rules, our parser can easily be used as a TSG
estimator and parser as well.
1bllip.cs.brown.edu/download/bucketparser.tar
309
References
Eugene Charniak, Mark Johnson, Micha Elsner,
Joseph L. Austerweil, David Ellis, Isaac Hax-
ton, Catherine Hill, R. Shrivaths, Jeremy Moore,
Michael Pozar, and Theresa Vu. 2006. Multilevel
coarse-to-fine PCFG parsing. In North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies.
Eugene Charniak. 1996. Tree-bank grammars. In As-
sociation for the Advancement of Artificial Intelli-
gence, pages 1031?1036.
David Chiang. 2000. Statistical parsing with
an automatically-extracted tree adjoining grammar.
Association for Computational Linguistics.
Trevor Cohn and Phil Blunsom. 2010. Blocked infer-
ence in bayesian tree substitution grammars. pages
225?230. Association for Computational Linguis-
tics.
Trevor Cohn, Sharon Goldwater, and Phil Blun-
som. 2009. Inducing compact but accurate tree-
substitution grammars. In Proceedings of Human
Language Technologies: The 2009 Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 548?556.
Association for Computational Linguistics.
Christy Doran, Dania Egedi, Beth Ann Hockey, Banga-
lore Srinivas, and Martin Zaidel. 1994. XTAG sys-
tem: a wide coverage grammar for English. pages
922?928. Association for Computational Linguis-
tics.
J. Goodman. 2003. Efficient parsing of DOP with
PCFG-reductions. Bod et al 2003.
Saul Gorn. 1965. Explicit definitions and linguistic
dominoes. In Systems and Computer Science, pages
77?115.
Rebecca Hwa. 1998. An empirical evaluation of prob-
abilistic lexicalized tree insertion grammars. In Pro-
ceedings of the 36th Annual Meeting of the Associ-
ation for Computational Linguistics and 17th Inter-
national Conference on Computational Linguistics,
pages 557?563. Association for Computational Lin-
guistics.
Aravind K. Joshi and Yves Schabes. 1997. Tree-
adjoining grammars. In G. Rozenberg and A. Salo-
maa, editors, Handbook of Formal Languages, vol-
ume 3, pages 69?124. Springer.
Aravind K Joshi. 1985. Tree adjoining grammars:
How much context-sensitivity is required to provide
reasonable structural descriptions? University of
Pennsylvania.
Aravind K Joshi. 1987. An introduction to tree ad-
joining grammars. Mathematics of Language, pages
87?115.
Dan Klein and Christopher D Manning. 2003. Accu-
rate unlexicalized parsing. pages 423?430. Associ-
ation for Computational Linguistics.
K. Lari and S. J. Young. 1990. The estimation of
stochastic context-free grammars using the inside-
outside algorithm. Computer Speech and Language,
pages 35?56.
Mehryar Mohri and Mark jan Nederhof. 2000. Regu-
lar approximation of context-free grammars through
transformation. In Robustness in language and
speech technology.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and
interpretable tree annotation. In Proceedings of
the 21st International Conference on Computational
Linguistics and the 44th annual meeting of the Asso-
ciation for Computational Linguistics, pages 433?
440. Association for Computational Linguistics.
Yves Schabes and Richard C. Waters. 1995. Tree
insertion grammar: a cubic-time, parsable formal-
ism that lexicalizes context-free grammar without
changing the trees produced. Computational Lin-
guistics, (4):479?513.
Yves Schabes. 1990. Mathematical and computa-
tional aspects of lexicalized grammars. Ph.D. thesis,
University of Pennsylvania, Philadelphia, PA, USA.
Hiroyuki Shindo, Akinori Fujino, and Masaaki Nagata.
2011. Insertion operator for bayesian tree substi-
tution grammars. pages 206?211. Association for
Computational Linguistics.
Elif Yamangil and Stuart M. Shieber. 2012. Estimat-
ing compact yet rich tree insertion grammars. pages
110?114. Association for Computational Linguis-
tics.
310
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 597?603,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Nonparametric Bayesian Inference and Efficient Parsing for
Tree-adjoining Grammars
Elif Yamangil and Stuart M. Shieber
Harvard University
Cambridge, Massachusetts, USA
{elif, shieber}@seas.harvard.edu
Abstract
In the line of research extending statis-
tical parsing to more expressive gram-
mar formalisms, we demonstrate for the
first time the use of tree-adjoining gram-
mars (TAG). We present a Bayesian non-
parametric model for estimating a proba-
bilistic TAG from a parsed corpus, along
with novel block sampling methods and
approximation transformations for TAG
that allow efficient parsing. Our work
shows performance improvements on the
Penn Treebank and finds more compact
yet linguistically rich representations of
the data, but more importantly provides
techniques in grammar transformation and
statistical inference that make practical
the use of these more expressive systems,
thereby enabling further experimentation
along these lines.
1 Introduction
There is a deep tension in statistical modeling of
grammatical structure between providing good ex-
pressivity ? to allow accurate modeling of the
data with sparse grammars ? and low complexity
? making induction of the grammars (say, from
a treebank) and parsing of novel sentences com-
putationally practical. Tree-substitution grammars
(TSG), by expanding the domain of locality of
context-free grammars (CFG), can achieve better
expressivity, and the ability to model more con-
textual dependencies; the payoff would be better
modeling of the data or smaller (sparser) models
or both. For instance, constructions that go across
levels, like the predicate-argument structure of a
verb and its arguments can be modeled by TSGs
(Goodman, 2003).
Recent work that incorporated Dirichlet pro-
cess (DP) nonparametric models into TSGs has
provided an efficient solution to the daunting
model selection problem of segmenting training
data trees into appropriate elementary fragments
to form the grammar (Cohn et al, 2009; Post and
Gildea, 2009). The elementary trees combined in
a TSG are, intuitively, primitives of the language,
yet certain linguistic phenomena (notably various
forms of modification) ?split them up?, preventing
their reuse, leading to less sparse grammars than
might be ideal (Yamangil and Shieber, 2012; Chi-
ang, 2000; Resnik, 1992).
TSGs are a special case of the more flexible
grammar formalism of tree adjoining grammar
(TAG) (Joshi et al, 1975). TAG augments TSG
with an adjunction operator and a set of auxil-
iary trees in addition to the substitution operator
and initial trees of TSG, allowing for ?splicing in?
of syntactic fragments within trees. This func-
tionality allows for better modeling of linguistic
phenomena such as the distinction between modi-
fiers and arguments (Joshi et al, 1975; XTAG Re-
search Group, 2001). Unfortunately, TAG?s ex-
pressivity comes at the cost of greatly increased
complexity. Parsing complexity for unconstrained
TAG scales as O(n6), impractical as compared to
CFG and TSG?s O(n3). In addition, the model
selection problem for TAG is significantly more
complicated than for TSG since one must reason
about many more combinatorial options with two
types of derivation operators. This has led re-
searchers to resort to manual (Doran et al, 1997)
or heuristic techniques. For example, one can con-
sider ?outsourcing? the auxiliary trees (Shieber,
2007), use template rules and a very small num-
ber of grammar categories (Hwa, 1998), or rely
on head-words and force lexicalization in order to
constrain the problem (Xia et al, 2001; Chiang,
597
2000; Carreras et al, 2008). However a solution
has not been put forward by which a model that
maximizes a principled probabilistic objective is
sought after.
Recent work by Cohn and Blunsom (2010) ar-
gued that under highly expressive grammars such
as TSGs where exponentially many derivations
may be hypothesized of the data, local Gibbs sam-
pling is insufficient for effective inference and
global blocked sampling strategies will be nec-
essary. For TAG, this problem is only more se-
vere due to its mild context-sensitivity and even
richer combinatorial nature. Therefore in previ-
ous work, Shindo et al (2011) and Yamangil and
Shieber (2012) used tree-insertion grammar (TIG)
as a kind of expressive compromise between TSG
and TAG, as a substrate on which to build nonpara-
metric inference. However TIG has the constraint
of disallowing wrapping adjunction (coordination
between material that falls to the left and right
of the point of adjunction, such as parentheticals
and quotations) as well as left adjunction along the
spine of a right auxiliary tree and vice versa.
In this work we formulate a blocked sampling
strategy for TAG that is effective and efficient, and
prove its superiority against the local Gibbs sam-
pling approach. We show via nonparametric in-
ference that TAG, which contains TSG as a sub-
set, is a better model for treebank data than TSG
and leads to improved parsing performance. TAG
achieves this by using more compact grammars
than TSG and by providing the ability to make
finer-grained linguistic distinctions. We explain
how our parameter refinement scheme for TAG
allows for cubic-time CFG parsing, which is just
as efficient as TSG parsing. Our presentation as-
sumes familiarity with prior work on block sam-
pling of TSG and TIG (Cohn and Blunsom, 2010;
Shindo et al, 2011; Yamangil and Shieber, 2012).
2 Probabilistic Model
In the basic nonparametric TSG model, there is
an independent DP for every grammar category
(such as c = NP), each of which uses a base dis-
tribution P0 that generates an initial tree by mak-
ing stepwise decisions and concentration parame-
ter ?c that controls the level of sparsity (size) of
the generated grammars: Gc ? DP(?c, P0(? | c))
We extend this model by adding specialized DPs
for auxiliary trees Gauxc ? DP(?auxc , P aux0 (? | c))
Therefore, we have an exchangeable process for
generating auxiliary tree aj given j ? 1 auxiliary
trees previously generated
p(aj | a<j) =
nc,aj + ?auxc P aux0 (aj | c)
j ? 1 + ?auxc
(1)
as for initial trees in TSG (Cohn et al, 2009).
We must define base distributions for initial
trees and auxiliary trees. P0 generates an initial
tree with root label c by sampling rules from a
CFG P? and making a binary decision at every
node generated whether to leave it as a frontier
node or further expand (with probability ?c) (Cohn
et al, 2009). Similarly, our P aux0 generates an aux-
iliary tree with root label c by sampling a CFG rule
from P? , flipping an unbiased coin to decide the di-
rection of the spine (if more than a unique child
was generated), making a binary decision at the
spine whether to leave it as a foot node or further
expand (with probability ?c), and recurring into P0
or P aux0 appropriately for the off-spine and spinal
children respectively.
We glue these two processes together via a set
of adjunction parameters ?c. In any derivation for
every node labeled c that is not a frontier node
or the root or foot node of an auxiliary tree, we
determine the number (perhaps zero) of simulta-
neous adjunctions (Schabes and Shieber, 1994)
by sampling a Geometric(?c) variable; thus k si-
multaneous adjunctions would have probability
(?c)k(1 ? ?c). Since we already provide simul-
taneous adjunction we disallow adjunction at the
root of auxiliary trees.
3 Inference
Given this model, our inference task is to ex-
plore posterior derivations underlying the data.
Since TAG derivations are highly structured ob-
jects, we design a blocked Metropolis-Hastings
sampler that samples derivations per entire parse
trees all at once in a joint fashion (Cohn and Blun-
som, 2010; Shindo et al, 2011; Yamangil and
Shieber, 2012). As in previous work, we use a
Goodman-transformed TAG as our proposal dis-
tribution (Goodman, 2003) that incorporates ad-
ditional CFG rules to account for the possibil-
ity of backing off to the infinite base distribution
P aux0 , and use the parsing algorithm described by
Shieber et al (1995) for computing inside proba-
bilities under this TAG model.
The algorithm is illustrated in Table 1 along
with Figure 1. Inside probabilities are computed
in a bottom-up fashion and a TAG derivation is
sampled top-down (Johnson et al, 2007). The
598
N? N
? N
N
Ni
. . .
?
?N0
? N1
? N2
N3
N4
. . .
?
?
Nj
? Nk
N? ?
Nl
? Nm
N? ?
Figure 1: Example used for illustrating blocked
sampling with TAG. On the left hand side we have
a partial training tree where we highlight the par-
ticular nodes (with node labels 0, 1, 2, 3, 4) that the
sampling algorithm traverses in post-order. On the
right hand side is the TAG grammar fragment that
is used to parse these particular nodes: one initial
tree and two wrapping auxiliary trees where one
adjoins into the spine of the other for full general-
ity of our illustration. Grammar nodes are labeled
with their Goodman indices (letters i, j, k, l,m).
Greek letters ?, ?, ?, ? denote entire subtrees. We
assume that a subtree in an auxiliary tree (e.g., ?)
parses the same subtree in a training tree.
sampler visits every node of the tree in post-order
(O(n) operations, n being the number of nodes),
visits every node below it as a potential foot (an-
other O(n) operations), visits every mid-node in
the path between the original node and the poten-
tial foot (if spine-adjunction is allowed) (O(log n)
operations), and forms the appropriate chart items.
The complexity is O(n2 log n) if spine-adjunction
is allowed, O(n2) otherwise.
4 Parameter Refinement
During inference, adjunction probabilities are
treated simplistically to facilitate convergence.
Only two parameters guide adjunction: ?c, the
probability of adjunction; and p(aj | a<j , c) (see
Equation 1), the probability of the particular aux-
iliary tree being adjoined given that there is an
adjunction. In all of this treatment, c, the con-
text of an adjunction, is the grammar category la-
bel such as S or NP, instead of a unique identi-
fier for the node at which the adjunction occurs as
was originally the case in probabilistic TAG liter-
ature. However it is possible to experiment with
further refinement schemes at parsing time. Once
the sampler converges on a grammar, we can re-
estimate its adjunction probabilities. Using the
O(n6) parsing algorithm (Shieber et al, 1995) we
experimented with various refinements schemes
? ranging from full node identifiers, to Goodman
Chart item Why made? Inside probability
Ni[4] By assumption. ?
Nk[3-4] N?[4] and ? (1 ? ?c) ? pi(?)
Nm[2-3] N?[3] and ? (1 ? ?c) ? pi(?)
Nl[1-3] ? and Nm[2-3] (1 ? ?c) ? pi(?)
?pi(Nm[2-3])
Naux[1-3] Nl[1-3] nc,al/(nc + ?auxc )
?pi(Nl[1-3])
Nk[1-4] Naux[1-3] and Nk[3-4] ?c ? pi(Naux[1-3])
?pi(Nk[3-4])
Nj [0-4] ? and Nk[1-4] (1 ? ?c) ? pi(?)
?pi(Nk[1-4])
Naux[0-4] Nj [0-4] nc,aj /(nc + ?auxc )
?pi(Nj [0-4])
Ni[0] Naux[0-4] and Ni[4] ?c ? pi(Naux[0-4])
?pi(Ni[4])
Table 1: Computation of inside probabilities for
TAG sampling. We create two types of chart
items: (1) per-node, e.g., Ni[?] denoting the
probability of starting at an initial subtree that
has Goodman index i and generating the subtree
rooted at node ?, and (2) per-path, e.g., Nj[?-?]
denoting the probability of starting at an auxiliary
subtree that has Goodman index j and generating
the subtree rooted at ? minus the subtree rooted
at ?. Above, c denotes the context of adjunction,
which is the nonterminal label of the node of ad-
junction (here, N), ?c is the probability of adjunc-
tion, nc,a is the count of the auxiliary tree a, and
nc =
?
a nc,a is total number of adjunctions atcontext c. The function pi(?) retrieves the inside
probability corresponding to an item.
index identifiers of the subtree below the adjunc-
tion (Hwa, 1998), to simple grammar category la-
bels ? and find that using Goodman index identi-
fiers as c is the best performing option.
Interestingly, this particular refinement scheme
also allows for fast cubic-time parsing, which we
achieve by approximating the TAG by a TSG with
little loss of coverage (no loss of coverage under
special conditions which we find that are often sat-
isfied) and negligible increase in grammar size, as
discussed in the next section.
5 Cubic-time parsing
MCMC training results in a list of sufficient statis-
tics of the final derivation that the TAG sampler
converges upon after a number of iterations. Basi-
cally, these are the list of initial and auxiliary trees,
their cumulative counts over the training data, and
their adjunction statistics. An adjunction statistic
is listed as follows. If ? is any elementary tree, and
? is an auxiliary tree that adjoins n times at node ?
of ? that is uniquely reachable at path p, we write
? p? ? (n times). We denote ? alternatively as
?[p].
599
*q
!
p
"
n
m
k
# *
p
"
i
i
i
q
!
i k
# *
m
i
"
i
#
i
i
i
#
j
j
j
q
!
i
j
i
j
!
ij
i
(1) (2) (3)
Figure 2: TAG to TSG transformation algorithm. By removing adjunctions in the correct order we end
up with a larger yet adjunction-free TSG.
Now imagine that we end up with a small gram-
mar that consists of one initial tree ? and two aux-
iliary trees ? and ?, and the following adjunctions
occurring between them
? p? ? (n times)
? p? ? (m times)
? q? ? (k times)
as shown in Figure 2. Assume that ? itself occurs
l > n +m times in total so that there is nonzero
probability of no adjunction anywhere within ?.
Also assume that the node uniquely identified by
?[p] has Goodman index i, which we denote as
i = G(?[p]).
The general idea of this TAG-TSG approxima-
tion is that, for any auxiliary tree that adjoins at a
node ? with Goodman index i, we create an ini-
tial tree out of it where the root and foot nodes of
the auxiliary tree are both replaced by i. Further,
we split the subtree rooted at ? from its parent and
rename the substitution site that is newly created
at ? as i as well. (See Figure 2.) We can sep-
arate the foot subtree from the rest of the initial
tree since it is completely remembered by any ad-
joined auxiliary trees due to the nature of our re-
finement scheme. However this method fails for
adjunctions that occur at spinal nodes of auxiliary
trees that have foot nodes below them since we
would not know in which order to do the initial
tree creation. However when the spine-adjunction
relation is amenable to a topological sort (as is the
case in Figure 2), we can apply the method by go-
ing in this order and doing some extra bookkeep-
ing: updating the list of Goodman indices and re-
directing adjunctions as we go along. When there
is no such topological sort, we can approximate
the TAG by heuristically dropping low-frequency
adjunctions that introduce cycles.1
The algorithm is illustrated in Figure 2. In (1)
we see the original TAG grammar and its adjunc-
tions (n,m, k are adjunction counts). Note that
the adjunction relation has a topological sort of
?, ?, ?. We process auxiliary trees in this order
and iteratively remove their adjunctions by creat-
ing specialized initial tree duplicates. In (2) we
first visit ?, which has adjunctions into ? at the
node denoted ?[p] where p is the unique path from
the root to this node. We retrieve the Goodman in-
dex of this node i = G(?[p]), split the subtree
rooted at this node as a new initial tree ?i, relabel
its root as i, and rename the newly-created sub-
stitution site at ?[p] as i. Since ? has only this
adjunction, we replace it with initial tree version
?i where root/foot labels of ? are replaced with
i, and update all adjunctions into ? as being into
?i. In (3) we visit ? which now has adjunctions
into ? and ?i. For the ?[p] adjunction we create ?i
the same way we created ?i but this time we can-
not remove ? as it still has an adjunction into ?i.
We retrieve the Goodman index of the node of ad-
junction j = G(?i[q]), split the subtree rooted at
this node as new initial tree ?ij , relabel its root
as j, and rename the newly-created substitution
site at ?i[q] as j. Since ? now has only this ad-
junction left, we remove it by also creating initial
tree version ?j where root/foot labels of ? are re-
placed with j. At this point we have an adjunction-
free TSG with elementary trees (and counts)
?(l), ?i(l), ?i(n), ?ij(n), ?i(m), ?j(k) where l is
the count of initial tree ?. These counts, when they
are normalized, lead to the appropriate adjunc-
1We found that, on average, about half of our grammars
have a topological sort of their spine-adjunctions. (On aver-
age fewer than 100 spine adjunctions even exist.) When no
such sort exists, only a few low-frequency adjunctions have
to be removed to eliminate cycles.
600
 0
 5
 10
 15
 20
 25
 30
 35
 40
 0  10  20  30  40  50  60
P
a
r
s
i
n
g
 
t
i
m
e
 
(
s
e
c
o
n
d
s
)
Sentence length (#tokens)
Figure 3: Nonparametric TAG (blue) parsing is ef-
ficient and incurs only a small increase in parsing
time compared to nonparametric TSG (red).
tion probability refinement scheme of ?c ? p(aj |
a<j , c) where c is the Goodman index.
Although this algorithm increases grammar
size, the sparsity of the nonparametric solution
ensures that the increase is almost negligible: on
average the final Goodman-transformed CFG has
173.9K rules for TSG, 189.2K for TAG. Figure 3
demonstrates the comparable Viterbi parsing times
for TSG and TAG.
6 Evaluation
We use the standard Penn treebank methodology
of training on sections 2?21 and testing on section
23. All our data is head-binarized, all hyperpa-
rameters are resampled under appropriate vague
gamma and beta priors. Samplers are run 1000
iterations each; all reported numbers are aver-
ages over 5 runs. For simplicity, parsing results
are based on the maximum probability derivation
(Viterbi algorithm).
In Table 4, we compare TAG inference
schemes and TSG. TAGGibbs operates by locally
adding/removing potential adjunctions, similar to
Cohn et al (2009). TAG? is the O(n2) algorithm
that disallows spine adjunction. We see that TAG?
has the best parsing performance, while TAG pro-
vides the most compact representation.
model F measure # initial trees # auxiliary trees
TSG 84.15 69.5K -
TAGGibbs 82.47 69.9K 1.7K
TAG? 84.87 66.4K 1.5K
TAG 84.82 66.4K 1.4K
Figure 4: EVALB results. Note that the Gibbs
sampler for TAG has poor performance and pro-
vides no grammar compaction due to its lack of
convergence.
label #adj ave. #lex. #left #right #wrap
(spine adj) depth trees trees trees trees
VP 4532 (23) 1.06 45 22 65 0
NP 2891 (46) 1.71 68 94 13 1
NN 2160 (3) 1.08 85 16 110 0
NNP 1478 (2) 1.12 90 19 90 0
NNS 1217 (1) 1.10 43 9 60 0
VBN 1121 (1) 1.05 6 18 0 0
VBD 976 (0) 1.0 16 25 0 0
NP 937 (0) 3.0 1 5 0 0
VB 870 (0) 1.02 14 31 4 0
S 823 (11) 1.48 42 36 35 3
total 23320 (118) 1.25 824 743 683 9
Table 2: Grammar analysis for an estimated TAG,
categorized by label. Only the most common top
10 are shown, binarization variables are denoted
with overline. A total number of 98 wrapping
adjunctions (9 unique wrapping trees) and 118
spine adjunctions occur.
ADJP
?
?
ADJP
ADJP* ?
?
NP
-LRB- NP
NP* -RRB-
S
-LRB-
-LRB-
S
S* -RRB-
-RRB-
S
?
?
S
S* ?
?NP
-LRB-
-LRB-
NP
NP* -RRB-
-RRB-
NNP
,
,
NNP
NNP
NNP* CC
&
NNP
NP
?
?
NP
NP* ?
?
NP
NP
NP :
NP
NP* PP
Figure 5: Example wrapping trees from estimated
TAGs.
7 Conclusion
We described a nonparametric Bayesian inference
scheme for estimating TAG grammars and showed
the power of TAG formalism over TSG for return-
ing rich, generalizable, yet compact representa-
tions of data. The nonparametric inference scheme
presents a principled way of addressing the diffi-
cult model selection problem with TAG. Our sam-
pler has near quadratic-time efficiency, and our
parsing approach remains context-free allowing
for fast cubic-time parsing, so that our overall
parsing framework is highly scalable.2
There are a number of extensions of this
work: Experimenting with automatically in-
duced adjunction refinements as well as in-
corporating substitution refinements can benefit
Bayesian TAG (Shindo et al, 2012; Petrov et al,
2006). We are also planning to investigate TAG
for more context-sensitive languages, and syn-
chronous TAG for machine translation.
2An extensive report of our algorithms and experiments
will be provided in the PhD thesis of the first author (Ya-
mangil, 2013). Our code will be made publicly available at
code.seas.harvard.edu/?elif.
601
References
Xavier Carreras, Michael Collins, and Terry Koo.
2008. TAG, dynamic programming, and the percep-
tron for efficient, feature-rich parsing. In Proceed-
ings of the Twelfth Conference on Computational
Natural Language Learning, CoNLL ?08, pages 9?
16, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
David Chiang. 2000. Statistical parsing with an
automatically-extracted tree adjoining grammar. In
Proceedings of the 38th Annual Meeting on Associa-
tion for Computational Linguistics, ACL ?00, pages
456?463, Morristown, NJ, USA. Association for
Computational Linguistics.
Trevor Cohn and Phil Blunsom. 2010. Blocked in-
ference in Bayesian tree substitution grammars. In
Proceedings of the ACL 2010 Conference Short Pa-
pers, ACLShort ?10, pages 225?230, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Trevor Cohn, Sharon Goldwater, and Phil Blun-
som. 2009. Inducing compact but accurate tree-
substitution grammars. In NAACL ?09: Proceed-
ings of Human Language Technologies: The 2009
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 548?556, Morristown, NJ, USA. Association
for Computational Linguistics.
Christine Doran, Beth Hockey, Philip Hopely, Joseph
Rosenzweig, Anoop Sarkar, B. Srinivas, Fei Xia,
Alexis Nasr, and Owen Rambow. 1997. Maintain-
ing the forest and burning out the underbrush in xtag.
In Proceedings of the ENVGRAM Workshop.
Joshua Goodman. 2003. Efficient parsing of DOP
with PCFG-reductions. In Rens Bod, Remko Scha,
and Khalil Sima?an, editors, Data-Oriented Parsing.
CSLI Publications, Stanford, CA.
Rebecca Hwa. 1998. An empirical evaluation of
probabilistic lexicalized tree insertion grammars. In
Proceedings of the 17th international conference on
Computational linguistics - Volume 1, pages 557?
563, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
Mark Johnson, Thomas Griffiths, and Sharon Gold-
water. 2007. Bayesian inference for PCFGs via
Markov chain Monte Carlo. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics; Proceedings of the Main Confer-
ence, pages 139?146, Rochester, New York, April.
Association for Computational Linguistics.
Aravind K. Joshi, Leon S. Levy, and Masako Taka-
hashi. 1975. Tree adjunct grammars. Journal of
Computer and System Sciences, 10(1):136?163.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and
interpretable tree annotation. In Proceedings of
the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 433?440,
Sydney, Australia, July. Association for Computa-
tional Linguistics.
Matt Post and Daniel Gildea. 2009. Bayesian learning
of a tree substitution grammar. In Proceedings of the
ACL-IJCNLP 2009 Conference Short Papers, pages
45?48, Suntec, Singapore, August. Association for
Computational Linguistics.
Philip Resnik. 1992. Probabilistic tree-adjoining
grammar as a framework for statistical natural lan-
guage processing. In Proceedings of the 14th con-
ference on Computational linguistics - Volume 2,
COLING ?92, pages 418?424, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Yves Schabes and Stuart M. Shieber. 1994. An
alternative conception of tree-adjoining derivation.
Computational Linguistics, 20(1):91?124. Also
available as cmp-lg/9404001.
Stuart M. Shieber, Yves Schabes, and Fernando C. N.
Pereira. 1995. Principles and implementation of de-
ductive parsing. J. Log. Program., 24(1&2):3?36.
Stuart M. Shieber. 2007. Probabilistic synchronous
tree-adjoining grammars for machine translation:
The argument from bilingual dictionaries. In Dekai
Wu and David Chiang, editors, Proceedings of the
Workshop on Syntax and Structure in Statistical
Translation, Rochester, New York, 26 April.
Hiroyuki Shindo, Akinori Fujino, and Masaaki Nagata.
2011. Insertion operator for Bayesian tree substitu-
tion grammars. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies: short pa-
pers - Volume 2, HLT ?11, pages 206?211, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Hiroyuki Shindo, Yusuke Miyao, Akinori Fujino, and
Masaaki Nagata. 2012. Bayesian symbol-refined
tree substitution grammars for syntactic parsing. In
Proceedings of the 50th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 440?448, Jeju Island, Korea,
July. Association for Computational Linguistics.
Fei Xia, Chung-hye Han, Martha Palmer, and Aravind
Joshi. 2001. Automatically extracting and compar-
ing lexicalized grammars for different languages. In
Proceedings of the 17th international joint confer-
ence on Artificial intelligence - Volume 2, IJCAI?01,
pages 1321?1326, San Francisco, CA, USA. Mor-
gan Kaufmann Publishers Inc.
XTAG Research Group. 2001. A lexicalized tree
adjoining grammar for English. Technical Report
IRCS-01-03, IRCS, University of Pennsylvania.
602
Elif Yamangil and Stuart Shieber. 2012. Estimating
compact yet rich tree insertion grammars. In Pro-
ceedings of the 50th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 2: Short
Papers), pages 110?114, Jeju Island, Korea, July.
Association for Computational Linguistics.
Elif Yamangil. 2013. Rich Linguistic Structure from
Large-Scale Web Data. Ph.D. thesis, Harvard Uni-
versity. Forthcoming.
603
Proceedings of the Ninth Workshop on Innovative Use of NLP for Building Educational Applications , pages 124?133,
Baltimore, Maryland USA, June 26, 2014.
c?2014 Association for Computational Linguistics
Natural Language Generation with Vocabulary Constraints
Ben Swanson
Brown University
Providence, RI
chonger@cs.brown.edu
Elif Yamangil
Google Inc.
Mountain View, CA
leafer@google.com
Eugene Charniak
Brown University
Providence, RI
ec@cs.brown.edu
Abstract
We investigate data driven natural lan-
guage generation under the constraints
that all words must come from a fixed vo-
cabulary and a specified word must ap-
pear in the generated sentence, motivated
by the possibility for automatic genera-
tion of language education exercises. We
present fast and accurate approximations
to the ideal rejection samplers for these
constraints and compare various sentence
level generative language models. Our
best systems produce output that is with
high frequency both novel and error free,
which we validate with human and auto-
matic evaluations.
1 Introduction
Freeform data driven Natural Language Genera-
tion (NLG) is a topic explored by academics and
artists alike, but motivating its empirical study is a
difficult task. While many language models used
in statistical NLP are generative and can easily
produce sample sentences by running their ?gen-
erative mode?, if all that is required is a plausible
sentence one might as well pick a sentence at ran-
dom from any existing corpus.
NLG becomes useful when constraints exist
such that only certain sentences are valid. The
majority of NLG applies a semantic constraint of
?what to say?, producing sentences with commu-
nicative goals. Other work such as ours investi-
gates constraints in structure; producing sentences
of a certain form without concern for their specific
meaning.
We study two constraints concerning the words
that are allowed in a sentence. The first sets a
fixed vocabulary such that only sentences where
all words are in-vocab are allowed. The second
demands not only that all words are in-vocab,
but also requires the inclusion of a specific word
somewhere in the sentence.
These constraints are natural in the construction
of language education exercises, where students
have small known vocabularies and exercises that
reinforce the knowledge of arbitrary words are re-
quired. To provide an example, consider a Chi-
nese teacher composing a quiz that asks students
to translate sentences from English to Chinese.
The teacher cannot ask students to translate words
that have not been taught in class, and would like
ensure that each vocabulary word from the current
book chapter is included in at least one sentence.
Using a system such as ours, she could easily gen-
erate a number of usable sentences that contain a
given vocab word and select her favorite, repeat-
ing this process for each vocab word until the quiz
is complete.
The construction of such a system presents two
primary technical challenges. First, while highly
parameterized models trained on large corpora are
a good fit for data driven NLG, sparsity is still
an issue when constraints are introduced. Tradi-
tional smoothing techniques used for prediction
based tasks are inappropriate, however, as they lib-
erally assign probability to implausible text. We
investigate smoothing techniques better suited for
NLG that smooth more precisely, sharing proba-
bility only between words that have strong seman-
tic connections.
The second challenge arises from the fact that
both vocabulary and word inclusion constraints
are easily handled with a rejection sampler that re-
peatedly generates sentences until one that obeys
the constraints is produced. Unfortunately, for
124
models with a sufficiently wide range of outputs
the computation wasted by rejection quickly be-
comes prohibitive, especially when the word in-
clusion constraint is applied. We define models
that sample directly from the possible outputs for
each constraint without rejection or backtracking,
and closely approximate the distribution of the
true rejection samplers.
We contrast several generative systems through
both human and automatic evaluation. Our best
system effectively captures the compositional na-
ture of our training data, producing error-free text
with nearly 80 percent accuracy without wasting
computation on backtracking or rejection. When
the word inclusion constraint is introduced, we
show clear empirical advantages over the simple
solution of searching a large corpus for an appro-
priate sentence.
2 Related Work
The majority of NLG focuses on the satisfaction
of a communicative goal, with examples such as
Belz (2008) which produces weather reports from
structured data or Mitchell et al. (2013) which gen-
erates descriptions of objects from images. Our
work is more similar to NLG work that concen-
trates on structural constraints such as generative
poetry (Greene et al., 2010) (Colton et al., 2012)
(Jiang and Zhou, 2008) or song lyrics (Wu et al.,
2013) (Ramakrishnan A et al., 2009), where spec-
ified meter or rhyme schemes are enforced. In
these papers soft semantic goals are sometimes
also introduced that seek responses to previous
lines of poetry or lyric.
Computational creativity is another subfield of
NLG that often does not fix an a priori meaning in
its output. Examples such as
?
Ozbal et al. (2013)
and Valitutti et al. (2013) use template filling tech-
niques guided by quantified notions of humor or
how catchy a phrase is.
Our motivation for generation of material for
language education exists in work such as Sumita
et al. (2005) and Mostow and Jang (2012), which
deal with automatic generation of classic fill in the
blank questions. Our work is naturally comple-
mentary to these efforts, as their methods require a
corpus of in-vocab text to serve as seed sentences.
3 Freeform Generation
For clarity in our discussion, we phrase the sen-
tence generation process in the following general
terms based around two classes of atomic units :
contexts and outcomes. In order to specify a gen-
eration system, we must define
1. the set C of contexts c
2. the set O of outcomes o
3. the ?Imply? function I(c, o)? List[c ? C]
4. M : derivation tree sentence
where I(c, o) defines the further contexts implied
by the choice of outcome o for the context c. Be-
ginning with a unique root context, a derivation
tree is created by repeatedly choosing an outcome
o for a leaf context c and expanding c to the new
leaf contexts specified by I(c, o). M converts be-
tween derivation tree and sentence text form.
This is simply a convenient rephrasing of the
Context Free Grammar formalism, and as such
the systems we describe all have some equivalent
CFG interpretation. Indeed, to describe a tradi-
tional CFG, let C be the set of symbols, O be the
rules of the CFG, and I(c, o) return a list of the
symbols on the right hand side of the rule o. To de-
fine an n-gram model, a context is a list of words,
an outcome a single word, and I(c, o) can be pro-
cedurally defined to drop the first element of c and
append o.
To perform the sampling required for derivation
tree construction we must define P (o|c). Using
M, we begin by converting a large corpus of sen-
tence segmented text into a training set of deriva-
tion trees. Maximum likelihood estimation of
P (o|c) is then as simple as normalizing the counts
of the observed outcomes for each observed con-
text. However, in order to obtain contexts for
which the conditional independence assumption
of P (o|c) is appropriate, it is necessary to con-
dition on a large amount of information. This
leads to sparse estimates even on large amounts of
training data, a problem that can be addressed by
smoothing. We identify two complementary types
of smoothing, and illustrate them with the follow-
ing sentences.
The furry dog bit me.
The cute cat licked me.
An unsmoothed bigram model trained on this
data can only generate the two sentences verba-
tim. If, however, we know that the tokens ?dog?
and ?cat? are semantically similar, we can smooth
by assuming the words that follow ?cat? are also
likely to follow ?dog?. This is easily handled with
125
traditional smoothing techniques that interpolate
between distributions estimated for both coarse,
P (w|w
?1
=[animal]), and fine, P (w|w
?1
=?dog?),
contexts. We refer to this as context smoothing.
However, we would also like to capture the in-
tuition that words which can be followed by ?dog?
can also be followed by ?cat?, which we will call
outcome smoothing. We extend our terminology
to describe a system that performs both types of
smoothing with the following
? the set
?
C of smooth contexts c?
? the set
?
O of smooth outcomes o?
? a smoothing function S
C
: C ?
?
C
? a smoothing function S
O
: O ?
?
O
dog [animal]
bit
[action]
4
2
1
3
5
7
6
2 Related Work
The application of structural constraints appears
in previous work in the form of generative po-
etry (Greene et al., 2010) or lyrics (Wu et al.,
2013), where specified meter or rhyme schemes
are enforced.
?
Ozbal et al. (2013) produces
freeform text by filling templates with respect to
abstract notions such as humor.
3 Freeform Generation
We first address the problem of freeform data
driven language generation directly. We do not
set a semantic goal but instead ask only that the
output be considered a valid sentence, seeking a
model that captures the variability of language.
For clarity in our discussion, we phrase the
generation process in the following general terms
based around two classes of atomic units : Con-
texts and Outcomes. In order to specify a genera-
tion system, we must define
1. the set C of contexts c
2. the set O of outcomes o
3. the ?Imply? function I(c, o) ? List[c ? C]
where I(c, o) defines the further contexts implied
by the choice of outcome o for the context c. This
model can be made probabilistic by the definition
of P (o|c), where each outcome is sampled inde-
pendently given its context. We also require the
existence of a single unique root context, and refer
to the result of repeated sampling of outcomes for
contexts as a derivation tree. Finally, a mapping
from derivation tree to surface form is required to
produce actual text.
This is simply a convenient rephrasing of the
Context Free Grammar formalism, and as such
the systems we describe all have some equivalent
CFG interpretation. Indeed, to describe a tradi-
tional CFG, let C be the set of nonterminals, O be
the rules of the CFG, and I(c, o) returns a list of
the nonterminals on the right hand side of the rule
o and does not depend on c. P (o|c) would enforce
the choice of rules with appropriate lefthand sides.
The Context-Outcome terms can be more natu-
ral when describing other models where we do not
want to explicitly define the space of nonterminals.
A simple example is an n-gram model, for which
a context is an ordered list of words, an outcome
a single word, and I(c, o) can be procedurally de-
fined to produce a list containg a single context
made by dropping the first word of the previous
context and appending the outcome to the end.
This formulation is well suited to data driven
estimation from a corpus of derivation trees.
While our methods are easily extended to mul-
tiple derivations for each single sentence, in this
work we assume access to a single derivation for
each sentence in our data set. Maximum likeli-
hood estimation of P (o|c) is then as simple as nor-
malizing the counts of the observed outcomes for
each observed context. However, in order to ob-
tain contexts for which the conditional indepen-
dence assumption of P (o|c) is appropriate, it is
necessary to condition on a large amount of in-
formation. This leads to sparse estimates even on
large amounts of training data, a problem that can
be addressed by smoothing.
We identify two complementary types of
smoothing, and illustrate them with the following
sentences.
The furry dog bit me.
The cute cat licked me.
Assuming a simple bigram model where con-
text is the previous word and the outcome a sin-
gle word, an unsmoothed model tr ined on this
data can only generate the two sentences verba-
tim. Imagine we have some way of knowing that
the tokens ?dog? and ?cat? are similar and would
like to leverage this fact . In our bigram model,
this amounts to the claim that the words that follow
?cat? are perhaps also likely to follow ?dog?. This
is easily handled with traditional smoothing tech-
niques, which interpolate between distributions
estimated for both coarse, P (w|w
?1
=[is-animal]),
and fine, P (w|w
?1
=?dog?), contexts. We refer to
this as context smoothing.
However, we would also like to capture the in-
tuition that words which can be followed by ?dog?
can also be followed by ?cat?, which we will call
outcome smoothing. We extend our terminology
to describe a system that performs both types of
smoothing with the following
? the set
?
C of smooth contexts c?
? the set
?
O of smooth outcomes o?
? a smoothing function S
C
: C ?
?
C
? a smoothing function S
O
: O ?
?
O
We describe the generative process with the fol-
lowing flowchart
2 Related Work
The application of structural constraints appears
in previous work in the form of generative po-
etry (Greene et al., 2010) or lyrics (Wu et al.,
2013), wher specified meter or rhyme schemes
are enforce .
?
Ozbal et al. (2013) produces
freeform text by filling templates with respect to
abstract notions such as humor.
3 Freeform Generation
We first address the problem of freeform data
driven language generation directly. We do not
set a semantic goal but in tead ask only that the
output b co sidered a valid sentence, seeking
m del that captures the variability of language.
For clarity in our discussion, we phrase the
generation proce s in the following general ter s
based around two classes of atomic unit : Con-
texts and Outcomes. In order to specify a genera-
tion system, we must define
1. the set C of contexts c
2. the set O of outcomes o
3. the ?Imply? function I(c, o) ? List[c ? C]
wher I(c, o) d fines the further contexts implied
by the choice of outcome o for the context c. This
model can be made probabilistic by the definition
of P (o|c), where each outcome is sampled inde-
pendently given its context. W lso require the
exist nce of a single un que r ot context, and refer
to the result of repeated sampling of outcomes f r
contexts as a derivation tree. Fi a ly, a mapping
f om derivation tree to s rface form is r quired to
produce actual text.
Thi is simply a co veni nt rephrasing of the
Context Free Grammar formalism, and as such
the systems w describe all have some equiv lent
CFG nterpretat on. Indeed, to describe a tradi-
tional CFG let C be the set of nonterminals, O be
the r le of the CFG, and I(c, o) returns a list of
the nonterminals on the right hand side of the rul
o and does ot d pend on c. P (o|c) would enforce
the choice of rules with appropriate lefthand sides.
The C ntext-Outcome term can be re natu-
ral wh n describing other models where we do no
want to expl cit y define the space of onterm nals.
A simple example is an n-gram model, for which
a context is an ordered list of words, an outcome
a single word, and I(c, o) can be procedurally d -
fined to pr duce a list containg a single context
made by dropping the first word of the previous
con ext and appending the outcome to the end.
This formulation is well suited to data driven
estim f om a corpus of derivation trees.
While our methods are easily extended to mul-
tiple derivations for each single sentence, in this
work we assume access to a single derivation for
each sentence in our data set. Maximum likeli-
hood estim tion of P (o|c) is then as simple as nor-
malizing th counts of the observed outcomes for
each bs rved context. However, in order to ob-
tain contexts for which the conditional indepen-
dence assump ion of P (o|c) is appropriate, it is
n cessary t condition on a large amount of in-
for ion. This leads to sparse estimates even on
la ge a ounts of training data, a problem that can
be addressed by smoothing.
We identify two complementary types of
smoothing, and illustrate them with the following
sentences.
The furry dog bit me.
The cute cat licked me.
Assuming a simple bigram model where con-
text is the previous word and the outcome a sin-
gle wor , an unsmooth d model trained n this
data can only generate the two sentences verba-
tim. Imagine we have some way of knowing that
the toke s ?d g? and ?cat? are similar and would
like to leverag this fact . In our bigram model,
this amounts to the claim that the words that follow
?cat? are perhaps also likely t follow ?dog?. This
is easily handled with traditional smoothing tech-
niques, which interpolate between distributions
estimated for both coarse, P (w|w
?1
=[is-animal]),
and fine, P (w|w
?1
=?dog?), contexts. We refer to
this as context smoothing.
However, we would also like to capture the in-
tuition that words which can be followed by ?dog?
can also be followed by ?cat?, which we will call
outcome sm thing. We extend our terminology
to describe a system that performs both types of
smoothing with the following
? the set
?
C of smooth contexts c?
? the set
?
O of smooth outcomes o?
? a smoothing function S
C
: C ?
?
C
? a smoothing function S
O
: O ?
?
O
We describe the generative process with the fol-
lowing flowchart
2 Related Work
The application of structural constraints appears
in previous work in the form of generative po-
etry (Greene et al., 2010) or lyrics (Wu et al.,
2013), where specified meter or rhyme schemes
are enforced.
?
Ozbal et al. (2013) produces
freeform text by filling templates with respect to
abstract notions such as humor.
3 Freeform Generation
We first address the problem of freeform data
driven language generation directly. We do not
set a s mantic goal but instead ask only that the
output be onsidered a valid sentence, seeking a
model that captures the variability of language.
For clarity in our discussion, we phrase the
generation process in the following general terms
based around two classes of atomic units : Con-
texts and Outcomes. In order to specify a genera-
tion system, we must define
1. the set C of cont xts c
2. the set O of outco es o
3. the ?Imply? function I(c, o) ? List[c ? C]
where I(c, o) define the fur r contexts impli d
by the choice of outcome o for the context . This
model can be made probabi istic by the definition
of P (o|c), wher each outcome is sampled inde-
pend ntly give its context. We also r quire the
existe ce of a single unique ro t context, and refer
to esult of r peated samp ing of utcomes for
contexts as a derivati n tree. Finally, a mapping
from derivation tree to sur ac form is required to
produce actual text.
T is is simply a convenient rephrasing of the
Co t xt Free Grammar formalism, and as suc
the systems we describe all hav some equivalent
CFG interpret tion. Indeed, to describe a tradi-
tional CFG, let C be the set of onterminals, O be
the rules of the CFG, and I( , o) returns a list of
the nonterminals on the right hand side of the rule
o and does not depend on c. P (o|c) would enforce
the choice of rules with appropriate lefthand sides.
The Context-Outcome terms can be more natu-
ral when describing other models where we do not
want to explicitly define the space of nonterminals.
A simple example is an n-gram model, for which
a context is an ordered list of words, an outcome
a single word, and I(c, o) can be procedurally de-
fined to produce a list containg a single context
made by dropping the first word of the pr ious
c ntext nd appe ding the outcome to the end.
This f rmul tion is w ll suited to data driven
estimation from a corpus f derivation trees.
While our methods are eas ly extended to mul-
tiple derivations for eac single se tence, in this
w rk we assum cces to a single derivation for
each sentence i our data set. Maximum lik li-
hood s imation of P (o|c) is then as simple as nor-
malizing the cou ts of the observed outcomes for
each observed context. However, in order to ob-
tain contexts for which the conditional indepen-
dence assumption of P (o|c) is appropriate, it is
necessary to condition on a large amount of in-
formation. This leads to sparse estimates even on
large amounts of training data, a problem that can
be addressed by smoothing.
We identify two complementary types of
smoothing, and illustrate them with the following
sentences.
The furry d g bit me.
The cut cat licked me.
Assuming a simple bigram model wher con-
text is the previous word and the outcome a sin-
gle word, an u smoothed model trained on this
at can only generate the two sentences verba-
tim. Imagine we have som way of knowing that
the tokens ?dog? and ?cat? a e sim lar and wou d
like to leverage this fact . In our bigram model,
this amounts to the clai that e words that follow
?cat? e perhap al o likely to ollow ?dog?. Thi
is easily handled wit traditional smo thi tech-
niques, which interpolate between distributions
estimated for both coar , P (w|w
?1
=[is-a imal]),
and fine, P (w|w
?1
=?dog?), contexts. We refer to
this as context smoothing.
However, we would also like to apture the in-
tuition that words which can be followed by ?dog?
can also be f llowed by ?cat?, which we will all
out m smoothing. We extend our terminology
to describe a system that performs both types of
smoothing with the following
? the set
?
C of smooth contexts c?
? the set
?
O of smooth outcomes o?
? a smoothing function S
C
: C ?
?
C
? a smoothing function S
O
: O ?
?
O
We describe the generative process with the fol-
lowing flowchart
2 Related Work
The application of structural constraints appears
in previous work in the form of generative po-
etry (Greene et al., 2010) or lyrics (Wu et al.,
2013), where specified meter or rhyme schemes
are enforced.
?
Ozbal et al. (2013) produces
freeform text by filling templates with respect to
abstract notions such as humor.
3 Freeform Gene ation
We first addr ss th problem of freeform data
driven language g neration directly. We do not
set a semantic goal but instead ask only that the
output b considered a valid s ntenc , seeking a
model that captures the vari bility f language.
For clarity in our discussion, we phrase the
generation process in the following general terms
based around two classes of atomic units : Con-
texts and Outcomes. In order to specify a genera-
tion system, we must define
1. the set C of contexts c
2. the set O of outcomes o
3. the ?Imply? function I(c, o) ? List[c ? C]
where I(c, o) defines the further contexts implied
by the choice of outcome o for the context c. This
model can be made probabilistic by the definition
of P (o|c), where each outcome is sampled inde-
pendently given its context. We also require the
existence of a single unique root context, and refer
to the result of repeated sampling of outcomes for
contexts as a derivation tree. Finally, a mapping
from derivation tree to surface form is required to
produce actual text.
This is simply a convenient rephrasing of the
Context Free Grammar formalism, and as such
the systems we describe all have some equivalent
CFG interpretation. Indeed, to describe a tradi-
tional CFG, let C be the set of nonterminals, O be
the rules of the CFG, and I(c, o) returns a list of
the nonterminals on the right hand side of the rule
o and does not depend on c. P (o|c) would enforce
the choice of rules with appropriate lefthand sides.
The Context-Outcome terms can be more natu-
ral when describing other models where we do not
want to explicitly define the space of nonterminals.
A simple example is an n-gram model, for which
a context is an ordered list of words, an outcome
a single word, and I(c, o) can be procedurally de-
fined to produce a list containg a single context
made by dro ping the first word of the previous
context and appending the outcome to the end.
This formulation is well suited to data driven
estimation from a corpus of derivation trees.
While our me hods are easily extended to mul-
tiple derivations for each single sentence, in this
work we assume access to a single derivation for
each sentence in our data set. Maximum likeli-
hood estimation of P (o|c) is then as simple as nor-
malizing the counts of the observed outcomes for
each observed context. However, in order to ob-
tain contexts for which the conditional indepen-
dence assumption of P (o|c) is appropriate, it is
necessary to condition on a large amount of in-
f rmation. This leads to sparse estimates even on
large amounts of training data, a problem that can
be addressed by smoothing.
We identify two complementary types of
smoothing, and illustrate them with the following
sentences.
The furry d g bit me.
The cute cat licked me.
Assuming a simple bigram model where con-
text is the previous word and the outcome a sin-
gle word, an unsmoothed model trained on this
data can only gener e the two senten es verba-
tim. Imagine we have some way of knowing that
the tokens ?dog? and ?cat? are similar and would
like to leverage this fact . In our bigram model,
this amounts to the claim that the words that follow
?cat? are perhaps also likely to follow ?dog?. This
is easily handled with traditional smoothing tech-
niques, which interpolate between distributions
estimated for bot coarse, P (w|w
?1
=[is-animal]),
and fine, P (w|w
?1
=?dog?), contexts. We refer to
this as context smoothing.
However, we would also like to capture the in-
tuition that words which can be followed by ?dog?
can also be followed by ?cat?, which we will call
outcome smoothing. We extend our terminology
to describe a system that performs both types of
smoothing with the following
? the set
?
C of smooth contexts c?
? the set
?
O of smooth outcomes o?
? a smoothing function S
C
: C ?
?
C
? a smoothing function S
O
: O ?
?
O
We describe the generative process with the fol-
lowing flowchart
Figure 1: A flow chart depicting the decisions
made when choosing an outcome for a context.
The large circles show the set of items associated
with each decision, and contain examples items
for a bigram model where S
C
and S
O
map words
(e.g. dog) to semantic classes (e.g. [animal]).
We describe the smoothed generative process
with the flowchart shown in Figure 1. In order to
choose an outcome for a given context, two deci-
sions must be made. First, we must decide which
context we will employ, the true context or the
smooth context, marked by edges 1 or 2 respec-
tively. Next, we choose to generate a true outcome
or a smooth outcome, and if we select the latter
we use edge 6 to choose a true outcome given the
smooth outcome. The decision between edges 1
and 2 can be sampled from a Bernoulli random
variable with parameter ?
c
, with one variable es-
timated for each context c. The decision between
edges 5 and 3 and the one between 4 and 7 can also
be made with Bernoulli random variables, with pa-
rameter sets ?
c
and ?
c?
respectively.
This yields the full form of the unconstrained
probabilistic generative model as follows
P (o|c) = ?
c
P
1
(o|c) + (1? ?
c
)P
2
(o|S
C
(c))
P
1
(o|c) = ?
c
P
5
(o|c)+
(1? ?
c
)P
7
(o|o?)P
3
(o?|c) (1)
P
2
(o|c?) = ?
c?
P
6
(o|c)+
(1? ?
c?
)P
7
(o|o?)P
4
(o?|c?)
requiring estimation of the ? and ? variables as
well as the five multinomial distributions P
3?7
.
This can be done with a straightforward applica-
tion of EM.
4 Limiting Vocabulary
A primary concern in the generation of language
education exercises is the working vocabulary of
the students. If efficiency were not a concern, the
natural solution to the vocabulary constraint would
be rejection sampling: simply generate sentences
until one happens to obey the constraint. In this
section we show how to generate a sentence di-
rectly from this constrained set with a distribution
closely approximating that of the rejection sam-
pler.
4.1 Pruning
The first step is to prune the space of possible sen-
tences to those that obey the vocabulary constraint.
For the models we investigate there is a natural
predicate V (o) that is true if and only if an out-
come introduces a word that is out of vocab, and
so the vocabulary constraint is equivalent to the
requirement that V (o) is false for all possible out-
comes o. Considering transitions along edges in
Figure 1, the removal of all transitions along edges
5,6, and 7 that lead to outcomes where V (o) is true
satisfies this property.
Our remaining concern is that the generation
process does not reach a failure case. Again
considering transitions in Figure 1, failure occurs
when we require P (o|c) for some c and there is no
transition to c on edge 1 or S
C
(c) along edge 2.
We refer to such a context as invalid. Our goal,
which we refer to as consistency, is that for all
126
valid contexts c, all outcomes o that can be reached
in Figure 1 satisfy the property that all members of
I(c, o) are valid contexts.
To see how we might end up in failure, consider
a trigram model on POS/word pairs for which S
C
is the identity function and S
O
backs off to the
POS tag. Given a context c = (
(
t
?2
w
?2
)
,
(
t
?1
w
?1
)
) if
we generate along a path using edge 6 we will
choose a smooth outcome t
0
that we have seen
following c in the data and then independenently
choose a w
0
that has been observed with tag t
0
.
This implies a following context (
(
t
?1
w
?1
)
,
(
t
0
w
0
)
). If
we have estimated our model with observations
from data, there is no guarantee that this context
ever appeared, and if so there will be no available
transition along edges 1 or 2.
Let the list
?
I(c, o) be the result of the mapped
application of S
C
to each element of I(c, o). In
order to define an efficient algorithm, we require
the following property D referring to the amount
of information needed to determine
?
I(c, o). Sim-
ply put, D states if the smoothed context and out-
come are fixed, then the implied smooth contexts
are determined.
D {S
C
(c), S
O
(o)} ?
?
I(c, o)
To highlight the statement D makes, consider the
trigram POS/word model described above, but let
S
C
also map the POS/word pairs in the context
to their POS tags alone. D holds here because
given S
C
(c) = (t
?2
, t
?1
) and S
O
(o) = t
0
from
the outcome, we are able to determine the implied
smooth context (t
?1
, t
0
). If context smoothing in-
stead produced S
C
(c) = (t
?2
),D would not hold.
If D holds then we can show consistency based
on the transitions in Figure 1 alone as any com-
plete path through Figure 1 defines both c? and o?.
By D we can determine
?
I(c, o) for any path and
verify that all its members have possible transi-
tions along edge 2. If the verification passes for
all paths then the model is consistent.
Algorithm 1 produces a consistent model by
verifying each complete path in the manner just
described. One important feature is that it pre-
serves the invariant that if a context c can be
reached on edge 1, then S
C
(c) can be reached on
edge 2. This means that if the verification fails
then the complete path produces an invalid con-
text, even though we have only checked the mem-
bers of
?
I(c, o) against path 2.
If a complete path produces an invalid con-
text, some transition along that path must be re-
Algorithm 1 Pruning Algorithm
Initialize with all observed transitions
for all out of vocab o do
remove ?? o from edges 5,6, and 7
end for
repeat
for all paths in flow chart do
if ?c? ?
?
I(c, o) s.t. c? is invalid then
remove transition from edge 5,7,3 or 4
end if
end for
Run FIXUP
until edge 2 transitions did not change
moved. It is never optimal to remove transitions
from edges 1 or 2 as this unnecessarily removes
all downstream complete paths as well, and so for
invalid complete paths along 1-5 and 2-7 Algo-
rithm 1 removes the transitions along edges 5 and
7. The choice is not so simple for the complete
paths 1-3-6 and 2-4-6, as there are two remaining
choices. Fortunately, D implies that breaking the
connection on edge 3 or 4 is optimal as regardless
of which outcome is chosen on edge 6,
?
I(c, o) will
still produce the same invalid c?.
After removing transitions in this manner, some
transitions on edges 1-4 may no longer have any
outgoing transitions. The subroutine FIXUP re-
moves such transitions, checking edges 3 and 4
before 1 and 2. If FIXUP does not modify edge 2
then the model is consistent and Algorithm 1 ter-
minates.
4.2 Estimation
In order to replicate the behavior of the rejection
sampler, which uses the original probability model
P (o|c) from Equation 1, we must set the probabil-
ities P
V
(o|c) of the pruned model appropriately.
We note that for moderately sized vocabularies it
is feasible to recursively enumerate C
V
, the set of
all reachable contexts in the pruned model. In
further discussion we simplify the representation
of the model to a standard PCFG with C
V
as its
symbol set and its PCFG rules indexed by out-
comes. This also allows us to construct the reach-
ability graph for C
V
, with an edge from c
i
to c
j
for each c
j
? I(c
i
, o). Such an edge is given
weight P (o|c), the probability under the uncon-
strained model, and zero weight edges are not in-
cluded.
Our goal is to retain the form of the stan-
127
dard incremental recursive sampling algorithm for
PCFGs. The correctness of this algorithm comes
from the fact that the probability of a rule R ex-
panding a symbolX is precisely the probability of
all trees rooted atX whose first rule isR. This im-
plies that the correct sampling distribution is sim-
ply the distribution over rules itself. When con-
straints that disallow certain trees are introduced,
the probability of all trees whose first rule is R
only includes the mass from valid trees, and the
correct sampling distribution is the renormaliza-
tion of these values.
Let the goodness of a contextG(c) be the proba-
bility that a full subtree generated from c using the
unconstrained model obeys the vocabulary con-
straint. Knowledge of G(c) for all c ? C
V
al-
lows the calculation of probabilities for the pruned
model with
P
V
(o|c) ? P (o|c)
?
c
?
?I(c,o)
G(c
?
) (2)
While G(c) can be defined recursively as
G(c) =
?
o?O
P (o|c)
?
c
?
?I(c,o)
G(c
?
) (3)
its calculation requires that the reachability graph
be acyclic. We approximate an acyclic graph by
listing all edges in order of decreasing weight and
introducing edges as long as they do not create cy-
cles. This can be done efficiently with a binary
search over the edges by weight. Note that this ap-
proximate graph is used only in recursive estima-
tion of G(c), and the true graph can still be used
in Equation 2.
5 Generating Up
In this section we show how to efficiently gener-
ate sentences that contain an arbitrary word w
?
in
addition to the vocabulary constraint. We assume
the ability to easily find C
w
?
, a subset of C
V
whose
use guarantees that the resulting sentence contains
w
?
. Our goal is once again to efficiently emulate
the rejection sampler, which generates a derivation
tree T and accepts if and only if it contains at least
one member of C
w
?
.
Let T
w
?
be the set of derivation trees that would
be accepted by the rejection sampler. We present
a three stage generative model and its associated
probability distribution P
w
?
(?) over items ? for
which there is a functional mapping into T
w
?
.
In addition to the probabilities P
V
(o|c) from the
previous section, we require an estimate of E(c),
the expected number of times each context c ap-
pears in a single tree. This can be computed effi-
ciently using the mean matrix, described in Miller
and Osullivan (1992). This |C
V
| ? |C
V
| matri x M
has its entries defined as
M(i, j) =
?
o?O
P (o|c
i
)#(c
j
, c
i
, o) (4)
where the operator # returns the number of times
context c
j
appears I(c
i
, o). Defining a 1 ? |C
V
|
start state vector z
0
that is zero everywhere and 1
in the entry corresponding to the root context gives
E(z) =
?
?
i=0
z
0
M
i
which can be iteratively computed with sparse ma-
trix multiplication. Note that the ith term in the
sum corresponds to expected counts at depth i in
the derivation tree. With definitions of context and
outcome for which very deep derivations are im-
probable, it is reasonable to approximate this sum
by truncation.
Our generation model operates in three phases.
1. Chose a start context c
0
? C
w
?
2. Generate a spine S of contexts and outcomes
connecting c
0
to the root context
3. Fill in the full derivation tree T below all re-
maining unexpanded contexts
In the first phase, c
0
is sampled from the multi-
nomial
P
1
(c
0
) =
E(c
0
)
?
c?C
w
?
E(c)
(5)
The second step produces a spine S, which is
formally an ordered list of triples. Each element
of S records a context c
i
, an outcome o
i
, and the
index k in I(c
i
, o
i
) of the child along which the
spine progresses. The members of S are sampled
independantly given the previously sampled con-
text, starting from c
0
and terminating when the
root context is reached. Intuitively this is equiv-
alent to generating the path from the root to c
0
in
a bottom up fashion.
We define the probability P
?
of a triple
(c
i
, o
i
, k) given a previously sampled context c
j
128
as
P
?
({c
i
, o
i
, k}|c
j
) ?
{
E(c
i
)P
V
(o
i
|c
i
) I(c
i
, o
i
)[k] = c
j
0 otherwise
(6)
Let S = (c
1
, o
1
, k
1
) . . . (c
n
, o
n
, k
n
) be the re-
sults of this recursive sampling algorithm, where
c
n
is the root context, and c
1
is the parent context
of c
0
. The total probability of a spine S is then
P
2
(S|c
0
) =
|S|
?
i=1
E(c
i
)P
V
(o
i
|c
i
)
Z
i?1
(7)
Z
i?1
=
?
(c,o)?I
c
i?1
E(c)P
V
(o|c)#(c
i?1
, c, o)
(8)
where I
c?1
is the set of all (c, o) for which
P
?
(c, o, k|c
i?1
) is non-zero for some k. A key
observation is that Z
i?1
= E(c
i?1
), which can-
cels nearly all of the expected counts from the full
product. Along with the fact that the expected
count of the root context is one, the formula sim-
plifies to
P
2
(S|c
0
) =
|S|
?
i=1
P
V
(o
i
|c
i
)
E(c
0
)
(9)
The third step generates a final tree T by fill-
ing in subtrees below unexpanded contexts on the
spine S using the original generation algorithm,
yielding results with probability
P
3
(T |S) =
?
(c,o)?T/S
P
V
(o|c) (10)
where the set T/S includes all contexts that are
not ancestors of c
0
, as their outcomes are already
specified in S.
We validate this algorithm by considering its
distrubution over complete derivation trees T ?
T
w
?
. The algorithm generates ? = (T, S, c
0
) and
has a simple functional mapping into T
w
?
by ex-
tracting the first member of ? .
Combining the probabilities of our three steps
gives
P
w
?
(?) =
E(c
0
)
?
c?C
w
?
E(c)
|S|
?
i=1
P
V
(o
i
|c
i
)
E(c
0
)
?
(c,o)?T/S
P
V
(o|c)
P
w
?
(?) =
P
V
(T )
?
c?C
w
?
E(c)
=
1
?
P
V
(T ) (11)
where ? is a constant and
P
V
(T ) =
?
(c,o)?T
P
V
(o|c)
is the probability of T under the original model.
Note that several ? may map to the same T by
using different spines, and so
P
w
?
(T ) =
?(T )
?
P
V
(T ) (12)
where ?(T ) is the number of possible spines, or
equivalently the number of contexts c ? C
w
?
in T .
Recall that our goal is to efficiently emulate the
output of a rejection sampler. An ideal system P
w
?
would produce the complete set of derivation trees
accepted by the rejection sampler using P
V
, with
probabilities of each derivation tree T satisfying
P
w
?
(T ) ? P
V
(T ) (13)
Consider the implications of the following as-
sumption
A each T ? T
w
?
contains exactly one c ? C
w
?
A ensures that ?(T ) = 1 for all T , unifying Equa-
tions 12 and 13. A does not generally hold in prac-
tice, but its clear exposition allows us to design
models for which it holds most of the time, lead-
ing to a tight approximation.
The most important consideration of this type is
to limit redundancy in C
w
?
. For illustration con-
sider a dependency grammar model with parent
annotation where a context is the current word and
its parent word. When specifying C
w
?
for a partic-
ular w
?
, we might choose all contexts in which w
?
appears as either the current or parent word, but
a better choice that more closely satisfies A is to
choose contexts where w
?
appears as the current
word only.
129
END
END
Freeform Generation from a Fixed Vocabulary
Abstract
We investigate data driven natural lan-
guage generation under the constraint that
all words must come from a fixed arbi-
trary vocabulary. This constraint is then
extended such that a user specified word
must also appear in the sentence. We
present fast approximations to the ideal re-
jection samplers and increase variability in
generated text through controlled smooth-
ing.
1 Introduction
ROOT PRP VBZ JJ NNS
she likes big dogs
ROOT VBZ
likes
PRP NNS
she dogs
ROOT VBZ NNS
likes dogs
ROOT PRP VBZ
she likes
JJ
big
VBZ JJ NNS
big dogs
Data driven Natural Language Generation
(NLG) is a fascinating topic explored by aca-
demics and artists alike, but motivating its empiri-
cal study is a difficult task. While many language
models used in statistical NLP are generative and
can easily produce sample sentences from distri-
butions estimated from data, if all that is required
is a plausible sentence one might as well pick one
at random from any existing corpus.
NLG is useful when constraints are applied such
that only certain plausible sentences are valid. The
majority of NLG applies the semantic constraint of
?what to say?, producing sentences with commu-
nicative goals. Other work such as ours investi-
gates constraints in structure; producing sentences
of a certain form without concern for their mean-
ing.
We motivate two specific constraints concern-
ing the words that are allowed in a sentence. The
first sets a fixed vocabulary such that only sen-
tences where all words are in-vocab are allowed.
The second demands not only that all words are
in-vocab, but specifies the inclusion of a single ar-
bitrary word somewhere in the sentence. These
contraints are most natural in the case of language
education, where students have small known vo-
cabularies and exercises that reinforce the knowl-
edge of arbitrary words are required. This use
Freeform Generation from a Fixed Vocabulary
Abstract
We investigate data driven natural lan-
guage generation under the constraint that
all words must come from a fixed arbi-
trary vocabulary. This constraint is then
extended such that a user specified word
must also appear in the sentence. We
present fast approximations to the ideal re-
jection samplers and increase variability in
generated text through controlled smooth-
ing.
1 Introduction
ROOT PRP VBZ JJ NNS
she likes big dogs
ROOT VBZ
likes
PRP NNS
she dogs
ROOT VBZ NNS
likes dogs
ROOT PRP VBZ
she likes
JJ
big
VBZ JJ NNS
big dogs
Data driven Natural Language Generation
(NLG) is a fascinating topic explored by aca-
demics and artists alike, but motivat ng its empiri-
cal study is a difficult task. While many language
models used in statistical NLP ar generative and
can easily produce sample sentences from distri-
bu ions estim ed rom d ta, if all that is required
is a plausible sentence one might as well pick one
at random from any exi ting corpus.
NLG is useful when constraints are applied such
that only certain plausible sentences are valid. The
majority of NLG applies the semantic constraint of
?what to say?, producing sentences with commu-
nicative goals. Other work such as ours investi-
gates constraints in structure; producing sentences
of a certain form without concern for their mean-
ing.
We motivate two specific constraints concern-
ing the words that are allowed in a sentence. The
first sets a fixed vocabulary such that only sen-
tences where all words are in-vocab are allowed.
The second demands not only that all words are
in-vocab, but specifies the inclusion of a single ar-
bitrary word somewhere in the sentence. These
contraints are most natural in the case of language
education, where students have small known vo-
cabularies and exercises that reinforce the knowl-
edge of arbitrary words are required. This use
Freefor Generation fro a Fixed Vocabulary
bstract
e investigate data driven natural lan-
guage generation under the constraint that
all ords ust co e fro a fixed arbi-
trary vocabulary. This constraint is then
extended such that a user specified ord
ust also appear in the sentence. e
present fast approxi ations to the ideal re-
jection sa plers and increase variability in
generated text through controlled s ooth-
ing.
I tr cti
JJ
s li s i s
li
l
JJ
big
JJ S
big dogs
ata driven atural anguage eneration
( ) is a fascinating topic explored by aca-
de ics and artists alike, but otivating its e piri-
cal st is a iffic lt tas . ile a la a e
els se i statistical are e erati e a
ca easil r ce sa le se te ces fr istri-
ti s sti t r t , if ll t t is r ir
is l si l s t i t s ll i
t r fr i ti r .
i f l tr i t r li
t l rt i l i l t r li .
j it li t ti t i t
t t , i t it
i ti l . t i ti
Freeform Generation from a Fixed Vocabulary
Abstract
We investigate data driven natural lan-
guage generation under the constraint that
all words must come from a fixed arbi-
trary vocabulary. This constraint is then
extended such that a user specified word
must also appear in the sentence. We
present fast approximations t the ideal re-
jection samplers and increase variability in
generated text thro gh controlled smooth-
ing.
1 Introduction
ROOT PRP VBZ JJ NNS
she likes big dogs
ROOT VBZ
likes
PRP NNS
she d gs
ROOT VBZ NNS
lik s dogs
ROOT PRP VBZ
she likes
JJ
big
VBZ JJ NNS
big dogs
Data driven Natural Language Generation
(NLG) is a fascinating topic explored by aca-
demics and artists alike, but motivating its empiri-
cal study is a difficult task. While many language
models used in statistical NLP are generative and
can easily produce sample sentences from distri-
butions estimated from data, if all that is required
is a plausible sentence one might as well pick one
at random from any existing corpus.
NLG is useful when constraints are applied such
that only certain plausible sentences are valid. The
majority of NLG applies the semantic constraint of
?what to say?, producing sentences with commu-
nicative goals. Other work such as ours investi-
gates constraints in structure; producing sentences
of a certain form without concern for their mean-
ing.
We motivate two specific constraints concern-
ing the words that are allowed in a sentence. The
first sets a fixed vocabulary such that only sen-
tences where all words are in-vocab are allowed.
The second demands not only that all words are
in-v cab, but specifies the inclusion of a single ar-
bitrary wo d somewhere in the sentence. These
contraints are most natural in the case of language
education, where students have small known vo-
cabularies and exercises that reinforce the knowl-
edge of arbitrary words are required. This use
Freeform Generation from a Fixed Vocabulary
Abstract
We investigate data driven natural lan-
guage generation under the constraint that
all words must come from a fixed arbi-
trary vocabulary. This constraint is then
extended such that a user specified word
must also appear in the sentence. We
present fast approximations to the ideal re-
jection samplers and increase variability in
generated text through controlled smooth-
ing.
1 Introduction
ROOT PRP VBZ JJ NNS
she likes big dogs
ROOT VBZ
likes
PRP NNS
he dogs
ROOT VBZ NNS
likes dogs
ROOT PRP VBZ
she likes
JJ
big
VBZ JJ NNS
big dogs
Data driven Natural Language Generation
(NLG) is a fascinating topic explored by aca-
demics and artists alike, but motivating its empiri-
cal study is a difficult task. While many language
models used in statistical NLP are generative and
c n easily produce sample sentences from distri-
butions estimated from data, if all that is required
is a plausible sentence one might as well pick one
at random from any existing corpus.
NLG is useful when co straints are applied such
that only certain plausible sentences are valid. The
ajority of NLG applies the semantic constraint of
?what to say?, producing sentences with commu-
nicative goals. Other work such as ours investi-
gates constraints in structure; producing sentences
of a certain form without concern for their mean-
ing.
We motivate two specific constraints concern-
ing the words that are allowed in a sentence. The
first sets a fixed vocabulary such that only sen-
tences where all words are in-vocab are allowed.
The second demands not only that all words are
in-vocab, but specifies the inclusion of a single ar-
bitrary word somewhere in the sentence. These
contraints are most natural in the case of language
education, where students have small known vo-
cabularies and exercises that reinforce the knowl-
edge of arbitrary words are required. This use
Freeform Generation from a Fixed Vocabulary
Abstract
We investigate data driven natural lan-
guage generation under the constraint that
all words must come from a fixed arbi-
trary vocabulary. This constraint is then
extended such that a user specified word
must also appear in the sentence. We
present fast approximatio s to the ideal re-
jection samplers and increase variability in
generated text through controlled smooth-
ing.
1 Introduction
ROOT PRP VBZ JJ NNS
she likes big dogs
ROOT VBZ
likes
PRP NNS
she dogs
ROOT VBZ NNS
likes dogs
ROOT PRP VBZ
she likes
JJ
big
VBZ JJ NNS
big dogs
Data driven Natural Language Generation
(NLG) is a fascinating topic explored by aca-
demics and artists alike, but motivating its empiri-
cal study is a difficult task. While many language
models used in statistical NLP are generative and
can easily produce sample sentences from distri-
butions estimated from data, if all that is required
is a plausible sentence one might as well pick one
at r ndom from any existing corpus.
NLG is useful when constraints are applied such
that only certain plausible sentences are valid. The
majority of NLG applies the semantic constraint of
?what to say?, producing sentences with commu-
nicative goals. Other work such as ours investi-
gates constraints in structure; producing sentences
of a certain form without concern for their mean-
ing.
We motivate two specific constraints concern-
ing the words that are allowed in a sentence. The
first s ts a fixed vocabulary such that only sen-
tences where all words are in-vocab are allowed.
The second demands not only that all words are
in-vocab, but specifies the inclusion of a single ar-
bitrary word somewhere in the sentence. These
contraints are most natural in the case of language
education, w re students have small known vo-
cabularies and exercises that reinforce the knowl-
edge of arbitrary words are required. This use
Freeform Generation from a Fixed Vocabulary
Abstract
We investigate data driven natural lan-
guage generation under the constraint that
all words must come from a fixed arbi-
trary vocabul ry. This constraint is then
extended such that a user specified word
must also appear in the sentence. We
present fast approximations to the ideal re-
jection samplers and increase variability in
generated text through controlled smooth-
ing.
1 Introduction
ROOT PRP VBZ JJ NNS
she likes big dogs
ROOT VBZ
likes
PRP NNS
she dogs
ROOT VBZ NNS
likes dogs
ROOT PRP VBZ
she likes
JJ
big
VBZ JJ NNS
big dogs
Data driven Natural Language Generation
(NLG) is a fascinating topic explored by aca-
demics and artists alike, but motivating its empiri-
cal study is a difficult task. While many language
models used in statistical NLP are generative and
can easily produce sample sentences from distri-
butions estimated from data, if all that is required
is a plausible sentence one might as well pick one
at random from any existing corpus.
NLG is useful when constraints are applied such
that only certain plausible sentences are valid. The
majority of NLG applies the semantic constraint of
?what to say?, producing sentences with commu-
nicative goals. Other work such as ours investi-
gates constraints in structure; producing sentences
of a certain form without concern for their mean-
ing.
We motivate two specific c nstraints concern-
i g the word that are all wed in a s ntence. The
first sets a fixed vocabulary such that only sen-
tences where ll words are in-vocab are allowed.
The second demands not only that all words are
in-vocab, but specifies the inclusion of a single ar-
bitrary word somewhere in the sentence. These
contraints are most natural in the case of language
education, where students have small known vo-
cabularies and exercises that reinforce the knowl-
edge f arbitrary words are required. This use
Freeform Generatio fro a Fixed Vocabulary
Abstract
We inves igate data driven natural lan-
guage g ne ation under the constraint that
all words must come from a fixed arbi-
trary vocabulary. This constraint is then
extended such that a user specifi d word
must also appear in the sentence. We
present fast approximations to the ideal re-
jection samplers and increase variability in
generated text through controlled smooth-
ing.
1 In roduction
ROOT PRP VBZ JJ NNS
she likes big dogs
ROOT VBZ
likes
PRP NNS
she dogs
ROOT VBZ NNS
likes dogs
ROOT PRP VBZ
she likes
JJ
big
VBZ JJ NNS
big dogs
Data driven Natural Language Generation
(NLG) is a fascinating topic explored by aca-
demics and artists alike, but motivating its empiri-
cal study is a difficult task. While many language
models used in statistical NLP are generative and
can easily produce sample sentences from distri-
butions estimated from data, if all that is required
is a plausible sentence one might as well pick one
at random from any existing corpus.
NLG is useful when constraints are applied such
that only certain plausible sentences are valid. The
majority of NLG applies the semantic constraint of
?what to say?, producing sentences with commu-
nicative goals. Other work such as ours investi-
gates constraints in structure; producing sentences
of a certain form without concern for their mean-
ing.
We motivate two specific constraints concern-
ing the words that are allowed in a sentence. The
first sets a fixed vocabulary such that only sen-
tences where all words are in-vocab are allowed.
The second demands not only that all words are
in-vocab, but specifies the inclusion of a single ar-
bitrary word somewhere in the sentence. These
contraints are most natural in the case of language
education, where students have small known vo-
cabularies and exercises that reinforce the knowl-
edge of arbitrary words are required. This use
Freeform Generation from a Fixed Vocabulary
Abstract
We inves gate data driven natural l -
guage generation u der the constraint tha
all words must come from a fixed arbi-
trary vocabulary. This constraint is then
extended such that a user specified word
must also appear in the sentence. We
present fast approximations to the ideal re-
jection samplers and increase variability in
generated text through controlled smooth-
ing.
1 Introduction
ROOT PRP VBZ JJ NNS
she likes big dogs
ROOT VBZ
likes
PRP NNS
she dogs
ROOT VBZ NNS
likes dogs
ROOT PRP VBZ
she likes
JJ
big
VBZ JJ NNS
big dogs
Data driven Natur l Language Generation
(NLG) is a fascinati g topic explored by aca-
demics and artists alike, but motivating its empiri-
cal study is a difficult task. While many language
models used in statistical NLP are generative and
can easily produce sample sentences from distri-
butions estimated from data, if all that is required
is a plausible sentence one might as well pick one
at random from any existing corpus.
NLG is us ful when constraints are applied such
that only certain plausible sentences are valid. The
majority of NLG applies the semantic constraint of
?what to say?, producing sentences with commu-
nicative goals. Other work such as ours investi-
gates constraints in structure; producing sentences
of a certain form without concern for their mean-
ing.
We motivate two specific constraints concern-
ing the words that are allowed in a sentence. The
first sets a fixed vocabulary such that only sen-
tences where all words are in-vocab are allowed.
The seco d demands not only that all words are
in-vocab, but specifies the inclusion of a single ar-
bitrary word somewhere in the sentence. These
contraints are most natural in the case of language
education, where students have small known vo-
cabularies and exercises that reinforce the knowl-
edge of arbitrary words are required. This use
Figure 2: The generation system SPINEDEP draws on ep n ency tree sy t x where we use the term
node to refer to a POS/word pair. Contexts consist of a nod , its parent ode, and grandparent POS tag,
as shown in squares. Outcomes, shown in squares with rounded right s des, are full lists of dependents
or the END symbol. The shaded rectangles contain the results o I(c, o) from the indicated (c, o) pair.
6 Experiments
We train our models on sentences drawn from the
Simple English Wikipedia
1
. We obtained these
sentences from a data dump which we liberally fil-
tered to remove items such as lists and sentences
longer than 15 words or shorter then 3 words. We
parsed this data with the recently updated Stanford
Parser (Socher et al., 2013) to Penn Treebank con-
stituent form, and removed any sentence that did
not parse to a top level S containing at least one
NP and one VP child. Even with such strong fil-
ters, we retained over 140K sentences for use as
training data, and provide this exact set of parse
trees for use in future work.
2
Inspired by the application in language educa-
tion, for our vocabulary list we use the English Vo-
cabulary Profile (Capel, 2012), which predicts stu-
dent vocabulary at different stages of learning En-
glish as a second language. We take the most ba-
sic American English vocabulary (the A1 list), and
retrieve all inflections for each word using Sim-
pleNLG (Gatt and Reiter, 2009), yielding a vocab-
ulary of 1226 simple words and punctuation.
To mitigate noise in the data, we discard any
pair of context and outcome that appears only once
in the training data, and estimate the parameters of
the unconstrained model using EM.
6.1 Model Comparison
We experimented with many generation models
before converging on SPINEDEP, described in
Figure 2, which we use in these experiments.
1
http://simple.wikipedia.org
2
data url anon for review
Corr(%) % uniq
SPINEDEP unsmoothed 87.6 5.0
SPINEDEP WordNe 78.3 32.5
SPINEDEP word2vec 5000 72.6 52.9
SPINEDEP word2vec 500 65.3 60.2
KneserNey-5 64.0 25.8
DMV 33.7 71.2
Figure 3: System comparison based on human
judged correctness and the percentage of unique
sentences in a sample of 100K.
SPINEDEP uses dependency grammar elements,
with parent and grandparent information in the
contexts to capture such distinctions as that be-
tween main and clausal verbs. Its outcomes are
full configurations of dependents, capturing co-
ordinations such as subject-object pairings. This
specificity greatly increases the size of the model
and in turn reduces the speed of the true rejection
sampler, which fails over 90% of the time to pro-
duce an in-vocab sentence.
We found that large amounts of smoothing
quickly diminishes the amount of error free out-
put, and so we smooth very cautiously, map-
ping words in the contexts and outcomes to
fine semantic classes. We compare the use
of human annotated hypernyms from Word-
net (Miller, 1995) with automatic word clusters
from word2vec (Mikolov et al., 2013), based on
vector space word embeddings, evaluating both
500 and 5000 clusters for the latter.
We compare these models against several base-
line alternatives, shown in Figure 3. To determine
130
correctness, used Amazon Mechanical Turk, ask-
ing the question: ?Is this sentence plausible??. We
further clarified this question in the instructions
with alternative definitions of plausibility as well
as both positive and negative examples. Every sen-
tence was rated by five reviewers and its correct-
ness was determined by majority vote, with a .496
Fleiss kappa agreement. To avoid spammers, we
limited our hits to Turkers with an over 95% ap-
proval rating.
Traditional language modeling techniques such
as such as the Dependency Model with Va-
lence (Klein and Manning, 2004) and 5-gram
Kneser Ney (Chen and Goodman, 1996) perform
poorly, which is unsurprising as they are designed
for tasks in recognition rather than generation. For
n-gram models, accuracy can be greatly increased
by decreasing the amount of smoothing, but it be-
comes difficult to find long n-grams that are com-
pletely in-vocab and results become redundant,
parroting the few completely in-vocab sentences
from the training data. The DMV is more flex-
ible, but makes assumptions of conditional inde-
pendence that are far too strong. As a result it
is unable to avoid red flags such as sentences not
ending in punctuation or strange subject-object co-
ordinations. Without smoothing, SPINEDEP suf-
fers from a similar problem as unsmoothed n-gram
models; high accuracy but quickly vanishing pro-
ductivity.
All of the smoothed SPINEDEP systems show
clear advantages over their competitors. The
tradeoff between correctness and generative ca-
pacity is also clear, and our results suggest that the
number of clusters created from the word2vec em-
beddings can be used to trace this curve. As for the
ideal position in this tradeoff, we leave such deci-
sions which are particular to specific application to
future work, arbitrarily using SPINEDEP WordNet
for our following experiments.
6.2 Fixed Vocabulary
To show the tightness of the approximation pre-
sented in Section 4.2, we evaluate three settings
for the probabilities of the pruned model. The first
is a weak baseline that sets all distributions to uni-
form. For the second, we simply renormalize the
true model?s probabilities, which is equivalent to
setting G(c) = 1 for all c in Equation 2. Finally,
we use our proposed method to estimate G(c).
We show in Figure 4 that our estimation method
Corr(%) -LLR
True RS 79.3 ?
Uniform 47.3 96.2
G(c) = 1 77.0 25.0
G(c) estimated 78.3 1.0
Figure 4: A comparison of our system against both
a weak and a strong baseline based on correctness
and the negative log of the likelihood ratio mea-
suring closeness to the true rejection sampler.
more closely approximates the distribution of the
rejection sampler by drawing 500K samples from
each model and comparing them with 500K sam-
ples from the rejection sampler itself. We quantify
this comparison with the likelihood ratio statistic,
evaluating the null hypothesis that the two sam-
ples were drawn from the same distribution. Not
only does our method more closely emulate that of
the rejection sampler, be we see welcome evidence
that closeness to the true distribution is correlated
with correctness.
6.3 Word Inclusion
To explore the word inclusion constraint, for each
word in our vocabulary list we sample 1000 sen-
tences that are constrained to include that word
using both unsmoothed and WordNet smoothed
SPINEDEP. We compare these results to the ?Cor-
pus? model that simply searches the training data
and uniformly samples from the existing sentences
that satisfy the constraints. This corpus search ap-
proach is quite a strong baseline, as it is trivial to
implement and we assume perfect correctness for
its results.
This experiment is especially relevant to our
motivation of language education. The natural
question when proposing any NLG approach is
whether or not the ability to automatically produce
sentences outweighs the requirement of a post-
process to ensure goal-appropriate output. This
is a challenging task in the context of language
education, as most applications such as exam or
homework creation require only a handful of sen-
tences. In order for an NLG solution to be appro-
priate, the constraints must be so strong that a cor-
pus search based method will frequently produce
too few options to be useful. The word inclusion
constraint highlights the strengths of our method
as it is not only highly plausible in a language ed-
131
# < 10 # > 100 Corr(%)
Corpus 987 26 100
Unsmooth 957 56 89.0
Smooth 544 586 79.0
Figure 5: Using systems that implement the word
inclusion constraint, this table shows the number
of words for which the amount of unique sentences
out of 1000 samples was less than 10 or greater
than 100, along with the correctness of each sys-
tem.
ucation setting but difficult to satisfy by chance in
large corpora.
Figure 5 shows that the corpus search approach
fails to find more than ten sentences that obey the
word inclusion constraints for most target words.
Moreover, it is arguably the case that unsmoothed
SPINEDEP is even worse due to its inferior cor-
rectness. With the addition of smoothing, how-
ever, we see a drastic shift in the number of words
for which a large number of sentences can be pro-
duced. For the majority of the vocabulary words
this model generates over 100 sentences that obey
both constraints, of which approximately 80% are
valid English sentences.
7 Conclusion
In this work we address two novel NLG con-
straints, fixed vocabulary and fixed vocabulary
with word inclusion, that are motivated by lan-
guage education scenarios. We showed that un-
der these constraints a highly parameterized model
based on dependency tree syntax can produce a
wide range of accurate sentences, outperforming
the strong baselines of popular generative lan-
guage models. We developed a pruning and es-
timation algorithm for the fixed vocabulary con-
straint and showed that it not only closely approx-
imates the true rejection sampler but also that the
tightness of approximation is correlated with hu-
man judgments of correctness. We showed that
under the word inclusion constraint, precise se-
mantic smoothing produces a system whose abili-
ties exceed the simple but powerful alternative of
looking up sentences in large corpora.
SPINEDEP works surprisingly well given the
widely held stigma that freeform NLG produces
either memorized sentences or gibberish. Still, we
expect that better models exist, especially in terms
of definition of smoothing operators. We have pre-
sented our algorithms in the flexible terms of con-
text and outcome, and clearly stated the properties
that are required for the full use of our methodol-
ogy. We have also implemented our code in these
general terms
3
, which performs EM based param-
eter estimation as well as efficient generation un-
der the constraints discussed above. All systems
used in this work with the exception of 5-gram in-
terpolated Kneser-Ney were implemented in this
way, are included with the code, and can be used
as templates.
We recognize several avenues for continued
work on this topic. The use of form-based con-
straints such as word inclusion has clear applica-
tion in language education, but many other con-
straints are also desirable. The clearest is perhaps
the ability to constrain results based on a ?vocab-
ulary? of syntactic patterns such as ?Not only ...
but also ...?. Another extension would be to incor-
porate the rough communicative goal of response
to a previous sentence as in Wu et al. (2013) and
attempt to produce in-vocab dialogs such as are
ubiquitous in language education textbooks.
Another possible direction is in the improve-
ment of the context-outcome framework itself.
While we have assumed a data set of one deriva-
tion tree per sentence, our current methods eas-
ily extend to sets of weighted derivations for each
sentence. This suggests the use of techinques that
have proved effective in grammar estimation that
reason over large numbers of possible derivations
such as Bayesian tree substitution grammars or un-
supervised symbol refinement.
References
Anja Belz. 2008. Automatic generation of weather
forecast texts using comprehensive probabilistic
generation-space models. Natural Language Engi-
neering, 14(4):431?455.
A. Capel. 2012. The english vocabulary profile.
http://vocabulary.englishprofile.org/.
Stanley F. Chen and Joshua Goodman. 1996. An em-
pirical study of smoothing techniques for language
modeling. In Proceedings of the 34th Annual Meet-
ing on Association for Computational Linguistics,
ACL ?96, pages 310?318, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Simon Colton, Jacob Goodwin, and Tony Veale. 2012.
Full face poetry generation. In Proceedings of the
3
url anon for review
132
Third International Conference on Computational
Creativity, pages 95?102.
Albert Gatt and Ehud Reiter. 2009. Simplenlg: A re-
alisation engine for practical applications. In Pro-
ceedings of the 12th European Workshop on Natu-
ral Language Generation, ENLG ?09, pages 90?93,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Erica Greene, Tugba Bodrumlu, and Kevin Knight.
2010. Automatic analysis of rhythmic poetry with
applications to generation and translation. In Pro-
ceedings of the 2010 Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ?10,
pages 524?533, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Long Jiang and Ming Zhou. 2008. Generating chi-
nese couplets using a statistical mt approach. In
Proceedings of the 22nd International Conference
on Computational Linguistics-Volume 1, pages 377?
384. Association for Computational Linguistics.
Dan Klein and Christopher D. Manning. 2004.
Corpus-based induction of syntactic structure: Mod-
els of dependency and constituency. In Proceed-
ings of the 42Nd Annual Meeting on Association for
Computational Linguistics, ACL ?04, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word represen-
tations in vector space. CoRR, abs/1301.3781.
Michael I. Miller and Joseph A. Osullivan. 1992. En-
tropies and combinatorics of random branching pro-
cesses and context-free languages. IEEE Transac-
tions on Information Theory, 38.
George A. Miller. 1995. Wordnet: A lexical database
for english. COMMUNICATIONS OF THE ACM,
38:39?41.
Margaret Mitchell, Kees van Deemter, and Ehud Reiter.
2013. Generating expressions that refer to visible
objects. In HLT-NAACL, pages 1174?1184.
Jack Mostow and Hyeju Jang. 2012. Generating di-
agnostic multiple choice comprehension cloze ques-
tions. In Proceedings of the Seventh Workshop
on Building Educational Applications Using NLP,
pages 136?146. Association for Computational Lin-
guistics.
G?ozde
?
Ozbal, Daniele Pighin, and Carlo Strapparava.
2013. Brainsup: Brainstorming support for creative
sentence generation. In Proceedings of the 51st An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1446?
1455, Sofia, Bulgaria, August. Association for Com-
putational Linguistics.
Ananth Ramakrishnan A, Sankar Kuppan, and
Sobha Lalitha Devi. 2009. Automatic generation
of tamil lyrics for melodies. In Proceedings of the
Workshop on Computational Approaches to Linguis-
tic Creativity, pages 40?46. Association for Compu-
tational Linguistics.
Richard Socher, John Bauer, Christopher D. Manning,
and Andrew Y. Ng. 2013. Parsing With Composi-
tional Vector Grammars. In ACL.
Eiichiro Sumita, Fumiaki Sugaya, and Seiichi Ya-
mamoto. 2005. Measuring non-native speak-
ers? proficiency of english by using a test with
automatically-generated fill-in-the-blank questions.
In Proceedings of the second workshop on Building
Educational Applications Using NLP, pages 61?68.
Association for Computational Linguistics.
Alessandro Valitutti, Hannu Toivonen, Antoine
Doucet, and Jukka M. Toivanen. 2013. ?let every-
thing turn well in your wife?: Generation of adult
humor using lexical constraints. In ACL (2), pages
243?248.
Dekai Wu, Karteek Addanki, Markus Saers, and
Meriem Beloucif. 2013. Learning to freestyle: Hip
hop challenge-response induction via transduction
rule segmentation. In EMNLP, pages 102?112.
133
