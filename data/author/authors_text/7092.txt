Automatic  Semantic Sequence Extraction 
from Unrestricted Non-Tagged Texts 
Shiho Nobesawa and Hi roak i  Saito mad Masakazu  Nakan ish i  
Dept. of Computer  Science 
Keio University 
3-14-1 Hiyoshi Kohoku, Yokohama 223-8522, Japan 
{shiho, hxs, czl}@nak.ies.keio.ac.jp 
Abst rac t  
Mophological processing, syntactic parsing and 
other useflfl tools have been proposed in the field 
of natural language processing(NLP). Many 
of those NLP tools take dictionary-based ap- 
proaches. Thus these tools are often not very 
efficient with texts written in casual wordings 
or texts which contain maw domain-specific 
terms, because of the lack of vocabulary. 
In this paper we propose a simple method 
to obtain domain-specific sequences from unre- 
stricted texts using statist;ical information only. 
This method is language-independent. 
We had experiments oil sequence xtraction 
on email l;exts in Japanese, and succeeded in 
extracting significant semantic sequences in the 
test corpus. We tried morphological parsing 
on the test corpus with ChaSen, a Japanese 
dictionary-based morphological parser, and ex- 
amined our system's efficiency in extraction of 
semantic sequences which were not recognized 
with ChaSen. Our system detected 69.06% of 
the unknown words correctly. 
1 I n t roduct ion  
I/eeognition of contained words is an impof  
tan| preproecssing for syntactic parsing. Word 
recognition is mostly done based on dictionary 
lookup, and unknown words often cause parse 
errors. Thus most of the researches have been 
done on fixed corpora with special dictionaries 
for the domain. 
Part-of-speech(POS) tags are often used for 
term recognition. This kind of preprocessing 
is often time-consmning and causes anfi)iguity. 
Wtmn it conies to the corpus with high rate of 
unknown words it is not easy to do a fair parsing 
with dictionaries and rules. 
Obtaining the contained terms and phrases 
correctly can be an efficient preprocessing. In 
this paper we propose a method to recognize 
domain-specific sequences with simple and non- 
eosty processing, which enables the use of unre- 
stricted corpora fc)r NLP tools. 
We concentrate on building a tool for extract- 
ing nmaningful sequences automatically with 
less preparation. Our systcnl only necds a fair 
size of non-tagged training corpus of tim tar- 
get language. No restriction is required for the 
training corpus. We do not need any preprocess- 
ing for the training corpus. 
We had experiments on email messages in 
Japanese and our system could recognize 69.06% 
of the undcfined sequences of the test corpus. 
2 Japanese  Characters  and Terms 
Taking a word as a basic semantic unit simplifies 
the conflming tasks of processing real languages. 
However single words are often not a good unit 
regarding the meaning of the context, because 
of the polysemy of the words(Fung, 1998). In- 
stead a phrase or a term can be taken as smallest 
semantic units. 
Most of the phrase/term extraction systems 
are based on recognizing noun phrases, or 
domain-specific terms, fi'om large corpora. Arg- 
anion et a1.(1998) proposed a memory-based ap- 
proach for noun phrase, which was to learn 
patterns with several sub-patterns. Anani- 
adou(1994) proposed a methodology based on 
term recognition using morphological rules. 
2.1 Term Extract ion in Japanese 
Japanese has 11o separator between words. On 
noun phrase extraction many researches have 
been done in Japanese as well, both stochas- 
tic and gramlnatical ways. In stochastic ap- 
proaches ~z-gram is one of the most fascinat- 
ing model. Noun phrase extraction(Nagao nd 
Mori, 1994), word segmentation(Oda and Kita~ 
579 
1999) and diction extraction are the major is- 
sues. There also are many researches on segmen- 
tation according to the entropy. Since Japanese 
has a great number of characters use of the infor- 
mation of letters is also a very interesting issue. 
2.2 Characters in Japanese 
Unlike English, Japanese has great mnount of 
characters for daily use. Japanese is special not 
only for its huge set of characters but its con- 
taining of three character types. Hiragana is a 
set of 71 phonetic haracters, which are mostly 
used for flmction words, inflections and adverbs. 
Katakana is also a set of phonetic characters, 
each related to a hiragana character. The use 
is mainly restricted to the representation f for- 
eign words. It's also used to represent pronun- 
ciations. Kanji is a set of Chinese-origin char- 
acters. There are thousands of kanji characters, 
and each kanji holds its own meaning. They  are 
used to represent content words. We also use 
alphabetical characters and Arabic numerals. 
3 Overv iew 
This system takes Japanese sentences as input. 
It processes sentences one by one, and we obtain 
segments of the sentences which are recognized 
as meaningful sequences as output. The flow of 
this system is as follows(Figure 1): Our system 
TRAINING EXTRACTION 
input (sentences) 
cooccur rence  
information 
- - _4  
input (a sentence) 
linking score 
calculation 
sequence extraction 
output (sequences) 
Figure 1: The Flow of tim System 
takes one sentence as an input at one time, and 
calculates tile scores between two neighboring 
letters according to the statistical data driven 
from the training corpus. After scoring the sys- 
tem decides which sequences to extract. 
3.1 Automatic Sequence Extraction 
Nobesawa et a1.(1996; 1999) proposed a system 
which estimates the likelihood of a string of let- 
ters be a meaningfifl block in a sentence. This 
method oes not need any knowledge of lexicon, 
and they showed that it was possible to segment 
sentences in meaningflfl way only with statisti- 
cal information between letters. Tile experimeN; 
was in Japanese, and they also showed that tile 
cooccurrence information between Japanese let- 
ters had enough information for estimating the 
connection of letters. 
We use this point in this paper and had ex- 
periments on extracting meaningfnl sequences in
email message texts to make up the lack of vo- 
cabulary of dictionaries. 
3.2 Scoring 
Our system introduces the linking score, 
which indicates the likelihood that two let- 
ters are neighboring as a (part of) meaningful 
string(Nobesawa et al, 1996). 
Only with neighboring bigrams it is impossi- 
ble to distinguish the events 'XY' in 'AXYB' 
fi'om 'CXYD'. Thus we introduce d-bigram 
which is a bigram cooccurrence information con- 
cerning the distance(Tsutsumi et al, 1993). 
Expression (1) calculates the score between 
two neighboring letters; 
UK(i) = E E M~(wj,wi+d;d ) x,q(d) (1) 
d=l  j-=i--(d--1) 
where wl as an eveN;, d as the distance between 
two eveN;s, dmax as the maximum distance used 
in the processing (we set drnax -~ 5), and g(d) as 
the weight fimction on distance (for this system 
g(d) = d-2(Sano et al, 1996), to decrease tile in- 
fluence of tile d-bigrams when the distance get 
longer (Church and Hanks, 1989)). When cal- 
culating the linking score between the letters wi 
and Wi+l, tile d-bigram information of the let- 
ter pairs around tim target two (such as (wi-l, 
wi+2; 3)) are added. 
Expression (2) calculates the mutual informa- 
tion between two events with d-bigram data; 
v; d) 
d) - -  (2) 
where x, y as events, d for the distance between 
two events, and P(x) as the probability. 
3.3 Sequence Extraction 
Using the linking score calculated according to 
tile statistical information, our system searches 
for the sequences to extract (thus we call our 
system LSE(linky sequence xtraction) system). 
580 
Figure 2 shows an example graph of the link- 
ing scores for a sentence. Each alphabet letter 
on the x-axis stands for a letter in a sentence. 
Figure 2: The Score Graph 
The linking scores between two neighbor- 
ing letters are dotted on the graph on the 
y-axis. Since the linking score gets higher 
when the pair has stronger connection, the 
mountain-shaped lines may get considered as 
unsegmentable blocks of letters. The linking 
scores of the pairs in longer words/phrases can 
be higher with the influence of the statistical in- 
formation of other letter pairs around them. On 
the other hand, the linking score between two 
letters which are accidentally neighboring ets 
lower, and it makes valley-shaped point on the 
score graph. Our system extracts the mountain- 
shaped parts of the sentence as the qinky se- 
quences', which is considered to be nleaningflfl 
according to the statistical information. In ex- 
ample Figure 2, strings AB, CDEF and HIJK 
might be extracted. 
The height of mountains are not fixed, accord- 
ing to the likelihood of the letters blocked as 
a string. Tiros we need a threshold to decide 
strings to extract according to the required size 
and the strength of connection. With higher 
threshold the strings gets shorter, since the 
higher linking score means that the neighboring 
letters can be connected only wlmn they have 
stronger commotion between them. 
3.4 I-Iow the System Uses the 
Statist ical Information 
Figure 3 shows the example graph on a sentence 
"~ i~"~"~2 \[o-gen-ki-de-su-ka-?\]" (: How are 
you?)(Sano, 1997). Each graph line indicates 
the linking score of the sentence after learning 
some thousamts of sentences of the target do~ 
main (for this graph we used a postcard corpus 
as Lhe target domain, and for the base domain 
we took a newspaper corpus). When the system 
have no information on the postcard domain, 
the system could indicate that only the pair of 
letters "~/z(, (gen-ki)" is connectable (there is a 
mountain-shaped line for this pair). Obtaining 
the information of postcard corpus, the linking 
scores of every pair in this sentence gel; bigger, 
to make higher mountain. And the shape of 
the mountain also changes to a flat one moun- 
tain which contains whole sentence from a steel) 
mountain with deep valleys. 
t0.00 
5.00 
o.0~ 
-5.03 
-t0.00 
-150~ 
-20.~ 
-3o.oo 
a N 
! 
! '>,, 
I 
'131RIiS HOLD 
0 
3011 
6014 
I_~?ql 
t~/( ,  
Ah~ 
? 
Figure 3: Score Graph for "@@23~-~-~-)5~ ? 
(@@-o-gen-ki-de-su-ka-?: How are you?)" 
4 Exper iments  
We had experiments on extracting semantic se- 
quences based only on letter cooccm'rencc infor- 
mation. 
We tried a dictionary-based Jap~mese mor- 
phological parser ChaSen vet. 1.51 (\] 9 9 7) oil the 
test corpus as well to check sequences whid~ a 
dictionary-based parser can not: recognize. 
4.1 Corpus 
We chose email messages as the corpora for ex- 
periments of our system. Email messages are 
mostly written in colloquialism, especially when 
they are written by younger people to send to 
their friends. In Japanese colloquialism has ca- 
sual wording which (lifters from literary style. 
Casual wording contains emphasizing and terms 
not in dictionary such as slangs. In English an 
emphasized word may he written in capital et;- 
ters, such as in "it SURE is not true", which 
is easily connected to the basic word "sure". 
We do the same kind of letter type changes in 
Japanese for emphasizing, however, since the re- 
lationship between letter types is not the same 
as English, it is not easy to connect he empha- 
sized terms and the basic terms. 
581 
4.1.1 T ra in ing  Corpus  
The training corpus we used to extract statisti- 
cal information is a set of email messages ent 
between young female friends during 1998 to 
1999. This corpus does not cover the one used 
as the test corpus. All the messages were sent 
to one receiver, and the number of senders is 
17. The email corpus contains 351 email mes- 
sages, which has 7,865 sentences(176,380 letters, 
i.e. 22.4 letters per sentence on average). 
We did not include quotations of other emails 
in the training corpus to avoid double-counting 
of same sentences, though email messages often 
contain quotations. 
4.1.2 Test  Corpus  
The test corpus is a set of email messages sent 
between young female friends during 1999. This 
corpus is not a part of the training corpus. All 
the messages were sent to one receiver, and the 
number of senders is 3. This corpus contains 
1,118 sentences(24,160 letters, i.e. 21.6 letters 
per sentence on average). 
4.2 P re l iminary  Resu l ts  
Figure 4 shows the distribution of the linking 
scores. The average of the scores is 0.34. The 
pairs of letters with higher linking scores are 
treated as highly 'linkable' pairs, that is, pairs 
with strong connection according to the statis- 
tical inforination of the domain (actually of the 
training corpus). 
.,a .~ .m .,a 
\] 
t 
J { 
I 
m 
Figure 4: Score Distril)ution 
Pairs of letters with high scores are mainly found 
in high-scored sequences (Tahle 1). 
Table 1 shows a part of the sequences ex- 
tracted with our system using letter cooccur- 
rence information. The threshold of extraction 
for Table 1 is 5.0. 
Table 1: Sequences Extracted Based on Letter 
Cooccurrence 
sequence memfing frequency 
(with scores over 5.0) 
... a ...... 72 
~: ~ so 52 
~:~ " ~ but 48 
/J~ ~ '~ ttmrefore 43 
/~ ~J~ mail 39 
(~)  (laugh) 36 
~Y~'5/b I 29 
Jc~h~ it 26 
.... a ...... 25 
I~ 5~ myself 20 
25 ~, b a net/Internet 20 
! !  a ! !  16  
I) >/~ link 15 
2~ fl'iend 13 
casuM wording 
b representation change (written in katat~na) 
These sequences which extracted frequently 
are the ones often use in tile target domain. 
4.3 Undef ined  Words  w i th  ChaSen 
Since ChaSen is a dictionary-based system, it 
outputs unknown strings of letters as they are, 
with a tag 'undefined word'. 
Table 2 shows the number of sequences which 
ChaSen resulted as "undefined words". The row 
'undefined words' indicates the sequences which 
were labeled as 'undefined word' with ChaSen, 
and the row 'parsing errors' stands for the se- 
quences which were not undefined words with 
ChaSen but not segmented correctly 1 . The ex- 
traction threshold is 0.5. 
ChaSen had 627 undefined words as its out- 
put. Since the test corpus contains 1,118 sen- 
tences, 56.08% of the sentences had an unde- 
fined word on average. As it is impossible to di- 
vide an undefined sequence into two undefined 
words, when two or more undefined sequences 
are neighboring they m'e often connected into 
one undefined word s Ttms the real number of 
undefined sequences can be more than counted. 
Table 2 shows that our system on statistical in- 
formation can be a help to recover 69.06% of the 
undefined sequences detected by ChaSen. 
1Since our system is not to put POS tags, we do not 
count agging errors with ChaScn (i.e., we do not contain 
tagging errors in the 'parsing errors'). 
2ChaSen cm, divide two neighboring undifined se- 
quences when the letter types of the sequences differs. 
582 
Table 2: Undefined Words with ChaSen 
undefined words w/LSE  system 
frequency ~/:total suc. ~ part. b failed 
over 10 281 230 7 44 
3 - 9 143 100 13 30 
2 56 43 4 9 
1 147 60 44 43 
total 627 433 68 126 
69.06% 10.85% 20.10% 
a sue.: succeeded to extract 
b pm't.: pm'tiMly extracted 
Table 2 also shows that  this system has bet- 
ter precision with tile sequences with larger fre- 
quency. For the sequences with frequency over 
10 times (in the test corpus), 81.85% of the se- 
quences have extracted correctly. Ignoring se- 
quences which appeared in the test corpus once, 
the rate of correct extract ion rose up to 77.71%. 
Table 3 shows how our system worked with 
the sequences whirl1 are found as undefined 
words with ChaSen parsing system. The  thresh- 
old for extract ion is 0.5. Table 3 shows that tim 
Table 3: Categories of undefined Words 
mldefined words w 7 LSE system 
category #total sue." part) fidled 
proper nouns 60 39 17 4 
new words 70 48 12 l0 
letter additions 119 89 4 26 
changes ~ 276 194 28 54 
term. marks ~z 58 43 0 15 
smileys 15 9 6 0 
et:c. 29 12 1 16 
toted 627 433 68 126 
a sue.: succeeded to extract 
I, part.: partially extracted 
c changes: representation changes 
't tenn. marks: termination marks 
biggest reason for the undefined words are the 
wob lem of the representation. As descril)ed in 
Section 4.3.2, we change the way of descript ion 
wlmn we want to emphasize the sequence. The  
pronunciat ion extension with adding extra vow- 
els or extension marks is also for the same rea- 
son. Adding these two categories, 356 sequences 
out of 627 undefined words(56.78%) are caused 
in this emphasizing. 
Terminat ion marks as undefined words con- 
lain sequences uch as " ...... " and " ! ! " The  
terminat ion marks not in dict ionary often indi- 
cate the impression, sud l  as surprise, hal)piness, 
considering and so on. 
New words including proper  nouns are the ac- 
tual 'undefined words'. ChaSen had 130 of them 
as its output ,  that  is 20.73% of the undefined 
words. 
4.3.1 Let ter  Types  in Undef ined Words  
Table 4 shows the types of letters included in 
the 'undefined words' with ChaSen. Tile figures 
indicate the numbers of letters. 
We had 627 undefined words in the test cor- 
pus with ChaSen (Table 2), which contain 1,493 
letters totally. The average length of the unde- 
fined words is thus 2.38. 70.40% of the letters in 
%~ble 4: Letter  Types  of Undefined Words 
undefined words w/LSE system 
type variety #total sllc. a pal't, b failed 
l~mji I. 19 19 0 0 
hiragmm 12 200 155 7 38 
katal~ma 73 1051 712 188 151 
nmneral 1 1 0 1 0 
alphabet 23 122 43 72 7 
symbol 22 100 39 37 24 
total 1493 968 305 220 
a sue.: succeeded to ex~rac~ 
l, part.: pm-t, ially extracted 
undefined words were katakana letters(Table 4), 
which are phonetic and often used for describing 
new words. Katakana letters are also often used 
for emphasiz ing sequences. 
OI1 the other hand, there was only one letter 
each for kanji and numeral  figure. That  is be- 
cause each kanji letter and numeral  figure has its 
own meaning, and those letters are most ly  found 
in the dictionary, even though tlle tags are not 
semantical ly correct. Or, as for kan.\]i letters, it 
sometimes can be tagged with incorrect segmen- 
tat ion 3. Thus undefined words in kanji letters 
are not counted as 'undefined words' mostly, and 
instead they cause segmentat ion fai lure(Section 
4.4). 
4.3.2 Representat ion  Changes 
Since Japanese have two phonetic haracter sets, 
we have several ways to represent one term; in 
kanii (if thm'e is any for the tin-m), in hiragana, 
in katakana, or sevm'al d laracter  type mixed. It 
is also possible to use Romanizat ion to represent 
a tern1. 
a ,,~_ a )  \[ko-no\] (:this)/t!k'~ \[se-l~d\] (:the world)" is incor- 
rectly segmented as ",~ 0~91~: \[ko-no-yo\](:the pr sent life)/ 
\[kai\](:world)"; "kono yo" is a fixed phrase, and "lmi" 
is a suffix for a nmm to put the meaning of "the worht 
of", e.g. "~7::gl ~(:the literary world)" 
583 
Table 5 shows the numbers of ChaSen errors 
according to the representation change. Most of 
Table 5: Undefined Words because of Represen- 
tation Changes 
undefined words w/LSE  system 
subeategory ~tota l  sue." part. b failed 
term chmlges 40 33 3 4 
lmtal~na 137 102 12 23 
chmlge & katalmala 55 34 10 11 
etc. 44 25 3 16 
total 276 194 28 54 
. sue.: succeeded to extract 
b part.: partiMly extracted 
the dictionaries have only one basic representa- 
tion for one term as its entry 4. However, in ca- 
sual writing we sometimes do not use the basic 
representation, to emphasize the term, or just 
to simplify tile writing. 
4.3.3 Pronunciat ion Extens ion 
In JapalmSe language we have many kinds of 
function words to put at the end of sentences 
(or sometimes 'bunsetsu' blocks). The function 
words for sentence nds are to change the sound 
of the sentences, to represent friendliness, order- 
ing, and other emotions. These function words 
are not basically used in written texts, but in 
colloquial sentences. 
In Japanese language we put extra letters to 
represent he lengthening of a phone,. Since 
almost all Japanese phones have vowels, to 
lengthen a phone for emphasizing we put extra 
vowels or extension marks after tim letter. Table 
TaMe 6: Extra Letters output as Undefined 
Words 
letter ~b ~, -) ~ $~ ~ 'y ~" total  
a i u e o t t n 
sue." 39 2 5 32 7 3 1 0 89 
part.b 0 0 0 0 0 4 0 0 4 
failed 5 1 4 2 1 7 5 1 26 
total 44 3 9 34 8 14 6 1 119 
suc.: succeeded to extract 
b part.: partially extracted. 
6 shows that 74.79% of the small letters which 
resulted as undefined words with ChaSen could 
be salvaged as parts of semantic sequences with 
our system. 
4Dictionaries may have phonetic representations for 
the entries, not as headings. 
These small letters in this table are extra let- 
ters to change the pronunciation; i.e. they ~re 
mostly not included in the dictionary. However 
they are actually a part of the word, since they 
could not be separated from the previous se- 
quences. 
4.4 Segmentation Failure with ChaSen 
Table 7 shows (;he result of the extraction of
sequences which ChaSen made parsing errors. 
It indicates that our system could recognize 
70.88% of the sequences which ChaSen made 
parsing errors. 
Table 7: Segmentation Failure with ChaSen 
undefined words w/LSE system 
category 7~total SilO. a part. b failed 
A 42 41 1 0 
B 60 35 10 15 
C 92 81 5 6 
D 11 2 5 4 
E 8 4 3 1 
F 176 106 37 33 
G 19 10 5 4 
H 257 154 73 30 
I 253 233 6 14 
J 115 82 19 14 
torn 941 667 159 115 
70.88% 16.90% 12.22% 
A: sequences incl. alphabetical characters 
B: sequences incl. numeral figures 
C: proper nouns 
D: new words excl. proper nouns 
E: fixed locutions 
F: sequences with representation changes 
G: sequences in other character types 
II: emph,xsized expressions 
I: termination marks 
J: parsing errors 
a sue.: succeeded to extract 
part.: partially extracted 
Category F is for the sequences which changed 
their representations according to tile terms' 
pronunciation changes for casual use. For ex- 
ample, "~?  \[ya-p-pa\]" is a casual form of "~ 
lTk 9 \[ya-ha-ri\](: as I thought)". In casual talk- 
ing using original terln "yahari" sounds a little 
too polite. Sonm common casual forms are in 
dictionaries, but not all. 
For the category B, our system failed to ex- 
tract 25 sequences. All the sequences in B are 
with counting suffixes. 12 sequences out of the 
584 
25 couhl not l)e connected wil;h the counting suf- 
fixes, e.g. "3 0 H \[3-0-nichi\](: 30 days, or, the 
30th day)" got over-segmented l)etween zero and 
the suffix. We have a big w~riety of counting suf- 
fixes in Japanese and since our system is only 
on letter cooccurrence information we couM not 
avoid tlm over-segmentation. 
Category C indicates the sequences wlfich are 
written in other character types for emphasizing. 
The major changes are: (1) to write in hiragana 
characters instead of kan.ji characters, and (2) to 
write in katakana characters to emphasize the 
term. 
5 Conclus ion 
Dictionary-based NLP tools often have worse 
precision with ~exts written in casual wordings 
and texts which contain many domain-specific 
terms. 'lbrm recognition system available fi)r 
any corpora as a preprocessing enables the use 
of NLP tools on many kinds of texts. 
In this paper we proposed a simple mefllod fi)r 
term recognition based on statistical informa- 
tion. We had experiments on extracting seman- 
tically meaningflfl sequences according to the 
statistical information drawn fi:om the training 
corpus~ and our system recognized 69.06% of the 
sequences whidl were tagged as undefined words 
witll a conventional nmrphologieal parser. 
Our sysi;em was efl3cient in recognizing differ- 
ent representations of terms, proper nouns, and 
other casual wording phrases. This helps to sal- 
vage semantically meaningful sequences not in 
dictionaries and this can be an efficient prepro- 
cessing. 
6 Future Work 
In this paper we proposed a simt)le term recog- 
nition method based only on statistical informa- 
tion. There may be several ways to combine the 
extracted sequences with the dictionaries. We 
may need to put POS tags to the sequences for 
the use with other NLP tools. We ext)ect that 
we can use tagging tools for this. 
This system we propsed is language- 
independent. For example, we Call use this 
system on English to extract English sequences 
which appeared frequently in the training 
corpus, such as proper nouns. 
References 
Sophia Ammiadou. 1994. A Methodology for Auto- 
tactic Term ll,ecoglfit;ion. Colin9-9~, pages 1034- 
1038. 
Shlomo Argmnon, Ido DagmL and Yuwtl Kry- 
molowski. 1998. A M(~mory-Based Approach 
to Learning Shallow Natural Lang~lage, Patterns. 
Col'ing-ACL'98, pages 67-73, August. 
Kenneth W. Church and Patrick IIanks. 1989. Word 
Association Norms, Mutual hfformation, and Lex~ 
icography. The 27th Annual Conference of th, e As- 
sociation of Computational Lin quistics. 
PascMe Fung. 1998. Extracting Key Terms fl'om 
Chinese and .lapmmse texts. Th, e International 
Journal on Computer P~vccssin9 of Oriental Lan- 
9~zagc, Special Issue on Information Retrieval on 
Oriental Languages. 
Yuji Matsumoto, Akira Kitauchi, Tatsuo Ya- 
mashit:a, Yoshital~t tlirano, Osmnu hnaMfi, 
and Tomo~fld hnanmra. 1997. Jat)mlese 
Morpholotical Analysis System ChaSen 1.51 
Manual. '2bchnical report, Nara Institute of 
S(:ience mid Technology. http://cactus.aist- 
nara.a(:.jp/lab/nlt/chasen.litnfl. 
Makoto Nagao mid Shinsuke Mori. 1994. A New 
Method of N-gram Statistics for Large, Number of 
n m~d Automatic Extraction of Words and Phrases 
fi'om Large Text Data of ,lapmlese. Colin9-95, 
pages 611-615, August. 
Shiho Nobesawa, Junya Tsutsumi, Da Jiang Sun, ~\[~)- 
mohisa State, Kengo Sate, mM Masalmzu Nakan- 
ishi. 1996. Segmenting Sentenc(',s into Linky 
Strings Using D-bigram St:atistics. Uolin9-96 , 
t)ages 586 591, August;. 
Shiho Nobesawa, Itiroaki S~fito, and Mas~fl(azu 
Nal(anishi. 1999. String Extraction Based Only 
on SLatisl;ic Lint~tbilil.y. IUCI)OL'99, pages 23- 
28, March. 
Iliroki Oda and Kenji Kita. 1999. A Character- 
Based Japanese Word Segmenter Using a PPM*- 
B~use(t Language Model. ICCPOL'99, pages 527- 
532, Mm'ch. 
Tomohisa Sane, Junya Tsutsmni, Da Jimlg Sun, 
Shiho Nobesawa, Kc, ngo Sate, Kumiko Omori, 
m~d Masal~/zu Nal~mishi. 1996. An Experiment 
on Good Usages of D-bigram Statistics in Natu- 
ral Lmtguage Ev~fluation. End Annual Meeting of 
th, c ANLP (NLP96), pages 185-188. Written ill 
Japmw, se. 
r\]Smohisa Sane. 1997. NaturM Language Processing 
Using Dynmnie StatisticM Information. Master's 
thesis, Keio University. \?ritten in Jal)anese. 
Junya Tsutsumi, Tomoaki Nitta, Notate One, m~d 
Shiho Nobesawa. 1.993. A Multi-Lingual Transla- 
tion System Based on A Statistical Model. JSAI 
~chnical report, S}G-PPAI-9302-g pages 7 12. 
Written in Japanese. 
585 
Extracting Word Sequence Correspondences
with Support Vector Machines
Kengo SATO and Hiroaki SAITO
Department of Information and Computer Science
Keio University
3?14?1, Hiyoshi, Kohoku, Yokohama 223?8522, Japan
{satoken,hxs}@nak.ics.keio.ac.jp
Abstract
This paper proposes a learning and extracting
method of word sequence correspondences from
non-aligned parallel corpora with Support Vector
Machines, which have high ability of the generaliza-
tion, rarely cause over-fit for training samples and
can learn dependencies of features by using a kernel
function. Our method uses features for the trans-
lation model which use the translation dictionary,
the number of words, part-of-speech, constituent
words and neighbor words. Experiment results in
which Japanese and English parallel corpora are
used archived 81.1 % precision rate and 69.0 % re-
call rate of the extracted word sequence correspon-
dences. This demonstrates that our method could
reduce the cost for making translation dictionaries.
1 Introduction
Translation dictionaries used in multilingual natu-
ral language processing such as machine transla-
tion have been made manually, but a great deal of
labor is required for this work and it is difficult
to keep the description of the dictionaries consis-
tent. Therefore, researches of extracting transla-
tion pairs from parallel corpora automatically be-
come active recently (Gale and Church, 1991; Kaji
and Aizono, 1996; Tanaka and Iwasaki, 1996; Kita-
mura and Matsumoto, 1996; Fung, 1997; Melamed,
1997; Sato and Nakanishi, 1998).
This paper proposes a learning and extract-
ing method of bilingual word sequence correspon-
dences from non-aligned parallel corpora with Sup-
port Vector Machines (SVMs) (Vapnik, 1999).
SVMs are ones of large margin classifiers (Smola
et al, 2000) which are based on the strategy where
margins between separating boundary and vectors
of which elements express the features of train-
ing samples is maximized. Therefore, SVMs have
higer ability of the generalization than other learn-
ing models such as the decision trees and rarely
cause over-fit for training samples. In addition, by
using kernel functions, they can learn non-linear
separating boundary and dependencies between the
features. Therefore, SVMs have been recently used
for the natural language processing such as text
categorization (Joachims, 1998; Taira and Haruno,
1999), chunk identification (Kudo and Matsumoto,
2000b), dependency structure analysis (Kudo and
Matsumoto, 2000a).
The method proposed in this paper does not re-
quire aligned parallel corpora which do not exist too
many at present. Therefore, without limiting appli-
cable domains, word sequence correspondences can
been extracted.
2 Support Vector Machines
SVMs are binary classifiers which linearly separate
d dimension vectors to two classes. Each vector rep-
resents the sample which has d features. It is distin-
guished whether given sample ~x = (x1, x2, . . . , xd)
belongs to X1 or X2 by equation (1) :
f (~x) = sign(g(~x)) =
{
1
(~x ? X1
)
?1 (~x ? X2
) (1)
where g(~x) is the hyperplain which separates two
classes in which ~w and b are decided by optimiza-
tion.
g(~x) = ~w ? ~x + b (2)
Let supervise signals for the training samples be
expressed as
yi =
{
1
(~xi ? X1
)
?1 (~xi ? X2
)
where X1 is a set of positive samples and X2 is a set
of negative samples.
If the training samples can be separated linearly,
there could exist two or more pairs of ~w and b that
PSfrag replacements X1
X22/||~w||
~w ? ~x + b = 0
~w ? ~x + b = 1
~w ? ~x + b = ?1
Figure 1: A separating hyperplain
satisfy equation (1). Therefore, give the following
constraints :
?i, yi(~w ? ~xi + b) ? 1 ? 0 (3)
Figure 1 shows that the hyperplain which sepa-
rates the samples. In this figure, solid line shows
separating hyperplain ~w ? ~x + b = 0 and two dotted
lines show hyperplains expressed by ~w ? ~x+ b = ?1.
The constraints (3) mean that any vectors must not
exist inside two dotted lines. The vectors on dotted
lines are called support vectors and the distance be-
tween dotted lines is called a margin, which equals
to 2/||~w||.
The learning algorithm for SVMs could optimize
~w and b which maximize the margin 2/||~w|| or min-
imize ||~w||2/2 subject to constraints (3). According
to Lagrange?s theory, the optimization problem is
transformed to minimizing the Lagrangian L :
L = 1
2
||~w||2 +
n
?
i=1
?i
(
yi(~w ? ~xi + b ? 1)) (4)
where ?i ? 0 (i = 1, . . . , n) are the Lagrange mul-
tipliers. By differentiating with respect to ~w and b,
the following relations are obtained,
?L
?~w = ~w ?
n
?
i=1
?iyi~x = 0 (5)
?L
?b =
n
?
i=1
?iyi = 0 (6)
and substituting equations (5) (6) into equation (4)
to obtain
D = ?1
2
n
?
i=1
n
?
j=1
?i? jyiy j~xi ? ~x j +
n
?
i=1
?i (7)
Consequently, the optimization problem is trans-
formed to maximizing the object function D subject
to
?n
i=1 ?iyi = 0 and ?i ? 0. For the optimal pa-
rameters ?? = arg max? D, each training sample ~xi
where ??i > 0 is corresponding to support vector.
~w can be obtained from equation (5) and b can be
obtained from
b = yi ? ~w ? ~xi
where ~xi is an arbitrary support vector. From equa-
tion (2) (5), the optimal hyperplain can be expressed
as the following equation with optimal parameters
?? :
g(~x) =
n
?
i=1
??i yi~xi ? ~x + b (8)
The training samples could be allowed in some
degree to enter the inside of the margin by changing
equation (3) to :
?i, yi(~w ? ~xi + b) ? 1 + ?i ? 0 (9)
where ?i ? 0 are called slack variables. At this time,
the maximal margin problem is enhanced as mini-
mizing ||~w||2/2 + C?ni=1 ?i, where C expresses the
weight of errors. As a result, the problem is to max-
imize the object function D subject to ?ni=1 ?iyi = 0
and 0 ? ?i ? C.
For the training samples which cannot be sepa-
rated linearly, they might be separated linearly in
higher dimension by mapping them using a non-
linear function:
? : Rd 7? Rd?
A linear separating in Rd? for ?(~x) is same as a non-
linear separating in Rd for ~x. Let ? satisfy
K(~x, ~x?) = ?(~x) ? ?(~x?) (10)
where K(~x, ~x?) is called kernel function. As a result,
the object function is rewritten to
D = ?1
2
n
?
i=1
n
?
j=1
?i? jyiy jK(~xi, ~x j) +
n
?
i=1
?i (11)
and the optimal hyperplain is rewritten to
g(~x) =
n
?
i=1
??i yiK(~xi, ~x) + b (12)
Note that ? does not appear in equation (11) (12).
Therefore, we need not calculate ? in higher dimen-
sion.
The well-known kernel functions are the polyno-
mial kernel function (13) and the Gaussian kernel
function (14).
K(~x, ~x?) = (~x ? ~x? + 1)p (13)
K(~x, ~x?) = exp
?
?
?
?
?
?
?||~x ?
~x?||2
2?2
?
?
?
?
?
?
(14)
A non-linear separating using one of these kernel
functions is corresponding to separating with con-
sideration of the dependencies between the features
in Rd.
3 Extracting Word Sequence
Correspondences with SVMs
3.1 Outline
The method proposed in this paper can obtain word
sequence correspondences (translation pairs) in the
parallel corpora which include Japanese and En-
glish sentences. It consists of the following three
steps:
1. Make training samples which include positive
samples as translation pairs and negative sam-
ples as non-translation pairs from the train-
ing corpora manually, and learn a translation
model from these with SVMs.
2. Make a set of candidates of translation pairs
which are pairs of phrases obtained by pars-
ing both Japanese sentences and English sen-
tences.
3. Extract translation pairs from the candidates by
inputting them to the translation model made in
step 1.
3.2 Features for the Translation Model
To apply SVMs for extracting translation pairs, the
candidates of the translation pairs must be converted
into feature vectors. In our method, they are com-
posed of the following features:
1. Features which use an existing translation dic-
tionary.
(a) Bilingual word pairs in the translation
dictionary which are included in the can-
didates of the translation pairs.
(b) Bilingual word pairs in the translation
dictionary which are co-occurred in the
context in which the candidates appear.
2. Features which use the number of words.
(a) The number of words in Japanese phrases.
(b) The number of words in English phrases.
3. Features which use the part-of-speech.
(a) The ratios of appearance of noun, verb,
adjective and adverb in Japanese phrases.
(b) The ratios of appearance of noun, verb,
adjective and adverb in English phrases.
4. Features which use constituent words.
(a) Constituent words in Japanese phrases.
(b) Constituent words in English phrases.
5. Features which use neighbor words.
(a) Neighbor words which appear in Japanese
phrases just before or after.
(b) Neighbor words which appear in English
phrases just before or after.
Two types of the features which use an existing
translation dictionary are used because the improve-
ment of accuracy can be expected by effectively us-
ing existing knowledge in the features. For features
(1a), words included in a candidate of the trans-
lation pair are looked up with the translation dic-
tionary and the bilingual word pairs in the candi-
date become features. They are based on the idea
that a translation pair would include many bilingual
word pairs. Each bilingual word pair included in
the dictionary is allocated to the dimension of the
feature vectors. If a bilingual word pair appears in
the candidate of translation pair, the value of the
corresponding dimension of the vector is set to 1,
and otherwise it is set to 0. For features (1b), all
pairs of words which co-occurred with a candidate
of the translation pair are looked up with the trans-
lation dictionary and the bilingual word pairs in the
dictionary become features. They are based on the
idea that the context of the words which appear in
neighborhood looks like each other for the trans-
lation pairs although expressed in the two different
languages (Kaji and Aizono, 1996). The candidates
are converted into the feature vectors just like (1a).
Features (2a) (2b) are based on the idea that there
is a correlation in the number of constituent words
of the phrases of both languages in the translation
pair. The number of constituent words of each lan-
guage is used for the feature vector.
Features (3a) (3b) are based on the idea that there
is a correlation in the ratio of content words (noun,
verb, adjective and adverb) which appear in the
phrases of both languages in a translation pair. The
ratios of the numbers of noun, verb, adjective and
adverb to the number of words of the phrases of
each language are used for the feature vector.
For features (4a) (4b), each content word (noun,
verb, adjective and adverb) is allocated to the di-
mension of the feature vectors for each language. If
a word appears in the candidate of translation pair,
the value of the corresponding dimension of the vec-
tor is set to 1, and otherwise it is set to 0.
For features (5a) (5b), each content words (noun,
verb, adjective and adverb) is allocated to the di-
mension of the feature vectors for each language. If
a word appears in the candidate of translation pair
just before or after, the value of the corresponding
dimension of the vector is set to 1, and otherwise it
is set to 0.
3.3 Learning the Translation Model
Training samples which include positive samples as
the translation pairs and negative samples as the
non-translation pairs are made from the training
corpora manually, and are converted into the fea-
ture vectors by the method described in section 3.2.
For supervise signals yi, each positive sample is as-
signed to +1 and each negative sample is assigned
to ?1. The translation model is learned from them
by SVMs described in section 2. As a result, the
optimal parameters ?? for SVMs are obtained.
3.4 Making the Candidate of the Translation
Pairs
A set of candidates of translation pairs is made from
the combinations of phrases which are obtained by
parsing both Japanese and English sentences. How
to make the combinations does not require sen-
tence alignments between both languages. Because
the set grows too big for all the combinations, the
phrases used for the combinations are limited in up-
per bound of the number of constituent words and
only noun phrases and verb phrases.
3.5 Extracting the Translation Pairs
The candidates of the translation pairs are converted
into the feature vectors with the method described
in section 3.2. By inputting them to equation (8)
with the optimal parameters ?? obtained in section
3.3, +1 or ?1 could be obtained as the output for
each vector. If the output is +1, the candidate corre-
sponding to the input vector is the translation pair,
otherwise it is not the translation pair.
4 Experiments
To confirm the effectiveness of the method de-
scribed in section 3, we did the experiments where
the English Business Letter Example Collection
published from Nihon Keizai Shimbun Inc. are used
as parallel corpora, which include Japanese and En-
glish sentences which are examples of business let-
ters, and are marked up at translation pairs.
As both training and test corpora, 1,000 sentences
were used. The translation pairs which are already
marked up in the corpora were corrected to the form
described in section 3.4 to be used as the positive
samples. Japanese sentences were parsed by KNP 1
and English sentences were parsed by Apple Pie
Parser 2. The negative samples of the same number
as the positive samples were randomly chosen from
combinations of phrases which were made by pars-
ing and of which the numbers of constituent words
were below 8 words. As a result, 2,000 samples
(1,000 positives and 1,000 negatives) for both train-
ing and test were prepared.
The obtained samples must be converted into the
feature vectors by the method described in section
3.2. For features (1a) (1b), 94,511 bilingual word
pairs included in EDICT 3 were prepared. For fea-
tures (4a) (4b) (5a) (5b), 1,009 Japanese words and
890 English words which appeared in the training
corpora above 3 times were used. Therefore, the
number of dimensions for the feature vectors was
94, 511?2+1?2+4?2+1, 009+890+1, 009+890 =
192, 830.
S V Mlight 4 was used for the learner and the clas-
sifier of SVMs. For the kernel function, the squared
polynomial kernel (p = 2 in equation (13)) was
used, and the error weight C was set to 0.01.
The translation model was learned by the train-
ing samples and the translation pairs were extracted
from the test samples by the method described in
section 3.
1http://www-lab25.kuee.kyoto-u.ac.jp/
nl-resource/knp.html
2http://www.cs.nyu.edu/cs/projects/proteus/
app/
3http://www.csse.monash.edu.au/?jwb/edict.
html
4http://svmlight.joachims.org/
 0
 20
 40
 60
 80
 100
 0  2  4  6  8  10  12  14  16  18  20
ra
te
 (%
)
the number of the training samples (x1.0e02)
Precision
Recall
Figure 2: Transition in the precision rate and the
recall rate when the number of the training samples
are increased
Table 1 shows the precision rate and the recall
rate of the extracted translation pairs, and table 2
shows examples of the extracted translation pairs.
Table 1: Precision and recall rate
Outputs Corrects Precision Recall
851 690 81.1 % 69.0 %
5 Discussion
Figure 2 shows the transition in the precision rate
and the recall rate when the number of the training
samples are increased from 100 to 2,000 by every
100 samples. The recall rate rose according to the
number of the training samples, and reaching the
level-off in the precision rate since 1,300. There-
fore, it suggests that the recall rate can be improved
without lowering the precision rate too much by in-
creasing the number of the training samples.
Figure 3 shows that the transition in the precision
rate and the recall rate when the number of the bilin-
gual word pairs in the translation dictionary are in-
creased from 0 to 90,000 by every 5,000 pairs. The
precision rate rose almost linearly according to the
number of the pairs, and reaching the level-off in the
recall rate since 30,000. Therefore, it suggests that
the precision rate can be improved without lowering
the recall rate too much by increasing the number of
the bilingual word pairs in the translation dictionary.
Table 3 shows the precision rate and the recall
rate when each kind of features described in section
3.2 was removed. The values in parentheses in the
columns of the precision rate and the recall rate are
 0
 20
 40
 60
 80
 100
 0  10  20  30  40  50  60  70  80  90  100
ra
te
 (%
)
the size of dictionary (x1.0e03)
Precision
Recall
Figure 3: Transition in the precision rate and the
recall rate when the number of the bilingual word
pairs in the translation dictionary are increased
differences with the values when all the features are
used. The fall of the precision rate when the features
which use the translation dictionary (1a) (1b) were
removed and the fall of the recall rate when the fea-
tures which use the number of words (2a) (2b) were
removed were especially large.
It is clear that feature (1a) (1b) could restrict
the translation model most strongly in all features.
Therefore, if feature (1a) (1b) were removed, it
causes a good translation model not to be able to
be learned only by the features of the remainder
because of the weak constraints, wrong outputs in-
creased, and the precision rate has fallen.
Only features (2a) (2b) surely appear in all sam-
ples although some other features appeared in the
training samples may not appear in the test samples.
So, in the test samples, the importance of features
(2a) (2b) are increased on the coverage of the sam-
ples relatively. Therefore, if features (2a) (2b) were
removed, it causes the recall rate to fall because of
the low coverage of the samples.
6 Related Works
With difference from our method, there have been
researches which are based on the assumption of
the sentence alignments for parallel corpora (Gale
and Church, 1991; Kitamura and Matsumoto, 1996;
Melamed, 1997). (Gale and Church, 1991) has used
the ?2 statistics as the correspondence level of the
word pairs and has showed that it was more effective
than the mutual information. (Kitamura and Mat-
sumoto, 1996) has used the Dice coefficient (Kay
and Ro?schesen, 1993) which was weighted by the
logarithm of the frequency of the word pair as the
Table 2: Examples of translation pairs extracted by our method
Japanese English
 	

chairman of a special program committee


officially retired as
 

	Preferential Presentation of Japanese Near-Synonyms
Using Definition Statements
Hiroyuki OKAMOTO Kengo SATO Hiroaki SAITO
Department of Information and Computer Science
Keio University
3?14?1 Hiyoshi, Kouhoku-ku, Yokohama 223?8522, Japan
Tel: (+81?45)563?1151 (ex 43250), Fax: (+81?45)566?1747
{motch, satoken, hxs}@nak.ics.keio.ac.jp
Abstract
This paper proposes a new method of
ranking near-synonyms ordered by their
suitability of nuances in a particular con-
text. Our method distincts near-synonyms
by semantic features extracted from their
definition statements in an ordinary dictio-
nary, and ranks them by the types of fea-
tures and a particular context. Our method
is an initial step to achieve a semantic
paraphrase system for authoring support.
1 Introduction
Most researches on automatic paraphrasing aim ei-
ther at document modification for a wide range
of NLP applications (Shirai et al, 1998; Tomuro
and Lytinen, 2001), at reading comprehension sup-
port (Inui and Yamamoto, 2001), or at transforma-
tion based on external constraints (Dras, 1998). On
the other hand, authoring / revision support is known
as another type of paraphrasing which targets at texts
in preparation. However, there are not so many re-
searches of such paraphrasing.
Paraphrase systems which aim at revising docu-
ments can be classified into three types:
? Syntactic suitability
This type of systems points out spelling or
grammatical mistakes and corrects them, such
as a grammar checker (Heidorn, 2000).
? Readability
Similar to reading comprehension support,
this type of paraphrase systems aims to
simplify difficult / complicated sentences or
phrases (Suganuma et al, 1990; Inui and
Okada, 2000).
? Semantic suitability
To reflect authors? intentions precisely, these
paraphrase systems replace words, which are
semantically ambiguous or inadequate, to ones
which are suitable for their contexts.
Almost all known authoring / revision support sys-
tems aim at syntactic suitability or readability, while
researches of the third type of paraphrasing, which
handle semantics, are very rare.
Let us consider a kind of authoring support sys-
tem, which first presents near-synonyms (words
counted among the same semantic category) of a
target word in an input sentence. Then, based on
user?s choise, the system paraphrases the target word
to the selected one with keeping syntactic and se-
mantic consistency through paraphrasing. Espe-
cially for semantic consistency, it is important to
express semantic differences between paraphrased
word pairs clearly. If fine-grained meanings of all
near-synonyms (not only a paraphrased pair) can be
extracted at a time, the system would be able to
present semantically suitable near-synonyms. Based
on this idea, this paper proposes a new method of
ranking Japanese near-synonyms ordered by their
suitability of nuances in a particular context. First,
this paper describes an overview of the method in
Section 2. Next, Section 3 shows the classification
of fine-grained meanings of a word and a method
of extracting those fine-grained meanings from a
definition statement of the word, to identify se-
mantic differences between near-synonyms. Then,
Section 4 presents our method of ranking near-
synonyms using fine-grained meanings described in
Section 3. Finally, this paper shows conclusion and
further works in Section 5.
2 Overview of our method of preferential
presentation
Though some word processing applications (e.g.
Microsoft Word) have a function of showing near-
synonyms of a word, it is not easy to choose the
most adequate word from the near-synonyms be-
cause they are not ordered by their semantic similar-
ity or suitability. Also, a simple replacement from
a word to one of its near-synonyms is very danger-
ous, because there are some differences between the
words in their modification rules and in their fine-
grained meanings.
Against these semantic problems, we propose a
new method of presenting near-synonyms ordered
by their semantic suitability in a particular context.
When a target word is given from an input sentence,
first our method obtains all near-synonyms of the
target word from an existing thesaurus, and differen-
tiates them semantically by features extracted from
their definition statements. Next, our method ranks
those near-synonyms by relations between the type
of features and the context of the input sentence. Fi-
nally, the ranking of near-synonyms are presented
with information of variation in the original sen-
tence for each near-synonym. This process enables
the user to choose a word suitable for the input con-
text, and helps prevention of semantic variation (or
redundancy / loss) in paraphrasing.
3 Semantic differentiation between
near-synonyms
As the first step to realize the preferential suggestion
of near-synonyms, we identify fine-grained word
senses of near-synonyms in order to differentiate
them semantically, by using sentences written in an
ordinal dictionary (definition statements) and word
co-occurrence information extracted from large cor-
pora.
3.1 Fine-grained word senses
There are some researches which deal with fine-
grained word senses for a lexical choice in language
generation (DiMarco et al, 1993; Edmonds, 1999).
Edmonds roughly classified semantic differences
between near-synonyms into four categories: deno-
tational (difference in nuances of near-synonyms),
expressive (in attitudes or emotions), stylistic (in
formalities or dialects), and collocational (as idioms
or in co-occurrence restrictions). In addition, he
classified them into 35 types and proposed an on-
tology for describing their differences formally.
Edmonds implemented I-Saurus, a prototype im-
plementation of this ontology, to achieve a lexical
choice in machine translation and denoted the effec-
tiveness of differences between near-synonyms for
a lexical choice. Though, there is a crucial prob-
lem that he did not mention how to obtain those dif-
ferences automatically. Against this problem, our
method extracts such differences by using definition
statements for each near-synonym. Although (Fu-
jita and Inui, 2001) has already focused on using
definition statements in order to determine a pair of
near-synonyms whether one can be paraphrased to
the other or not, it was only a kind of matching be-
tween two statements and did not identify individ-
ual features in each statement. Therefore, this paper
defines three types of semantic features as follows,
which can be extracted from definition statements:
? Core meaning indicates the basic sense of a
word. All near-synonyms in a category must
always have the same core meaning, such as
the name of the category which they belong to.
? Denotation, which can be paraphrased to ?nu-
ance?, is defined as ?the thing that is actually
described by a word rather than the feelings or
ideas it suggests? in Longman web dictionary1.
In this paper, this feature is defined as a mean-
ing included in a word, which partially qualify
the core meaning. It is similar to a denotational
constraint in (Edmonds, 1999).
? Lexical restriction of a word is a constraint on
the range of co-occurrence of the word. This
feature is almost the same as a collocational
constraint in (Edmonds, 1999).
An example of these features is shown in Figure 1.
We divide our method into two steps to extract
each feature from a definition statement. First, we
extract a word defined as a core meaning and all
other content words (in Section 3.2). Then, the ex-
tracted words except the core meaning are classified
into denotations or lexical restrictions by using each
co-occurrence information obtained from large cor-
pora (in Section 3.3).
1http://www.longmanwebdict.com/
Word:
????
?? saikon
(rebuilding of shrines / temples)
Definition statement:
???????????????
jinja (shrine) bukkaku (temple) wo
(OBJ) tate (to build) naosu (to repair)
koto (matter)
(To build a shrine or a temple to repair.
Core meaning: ?? tate (build)
Denotation: ?? naosu (repair)
Lexical restriction: ?? jinja (shrine)
?? bukkaku (temple)
Figure 1: Features in a definition statement
3.2 Extraction of fine-grained word senses
In this paper, we assume that a definition statement
of a word (hereafter an entry) in a dictionary con-
sists of four types of materials as follows:
? Core meaning is a word which exactly de-
scribes a particular semantic category which
the entry belongs to.
? Fine-grained meaning semantically differen-
ciates the entry from its near-synonyms. It
is defined as a core meaning of some content
words in the definition statement. Fine-grained
meaning can be divided into ?denotation? or
?lexical restriction?.
? Stop word indicates a content word which
commonly and frequently appears in any def-
inition statement.
? Others include function words and symbols.
According to this assumption, the ?core mean-
ing? and ?fine-grained meanings? of an entry are
extracted from a definition statement, using of
Kadokawa thesaurus (Ohno and Hamanishi, 1981)2.
A procedure of this method is given as follows:
Step 1. For each morpheme in the morpheme dic-
tionary of ChaSen (Matsumoto et al, 2002),
a Japanese morphological analyzer, add a
label of a semantic category in Kadokawa
Thesaurus, which the morpheme belongs to.
Step 2. Assign semantic labels to each morpheme in
a definition statement of an entry e, by ap-
plying ChaSen to the statement.
2Kadokawa thesaurus semantically categorizes 57,130 en-
tries into 2,924 categories and each entry has a definition state-
ment.
Step 3. Give a word c as a ?provisional? core mean-
ing if c is classified into the same semantic
category as e.
Step 4. Extract all semantic labels, which are as-
signed to all content words except c, as fine-
grained meanings.
Step 5. Recursively apply Step 2?4 to the definition
statement of c until no core meaning is ex-
tracted from the definition statement.
Step 6. Define c extracted at last as the ?true? core
meaning of e.
According to this procedure, some fine-grained
meanings could be extracted from stop words. Thus,
we give a semantic weight to each fine-grained
meaning, by the reciprocal of its occurrence prob-
ability in all definition statements. These weights
can distinct true fine-grained meanings from ones
extracted from stop words.
A result of this method is shown in Figure 2,
where the bold numbers show their categories and
the italics show their weights.
Word: [394]
????
?? saikon
(rebuilding of shrines / temples)
Core meaning:
[394]??? tateru (to build)
Fine-grained meaning:
[727a]?? jinja (shrine: 5687)
[940c]?? bukkaku (temple: 6184)
[277b]?? naosu (to alter: 1441)
[277c]?? naosu (to recover: 2359)
[392]?? naosu (to repair: 7494)
[417a]?? naosu (to get right: 3703)
[811]?? koto (matter: 30)
Figure 2: Example of extraction of core-meaning
and fine-grained meanings
3.3 Classification of fine-grained word senses
After obtaining features in Section 3.2, our method
classifies fine-grained meanings into denotations
and lexical restrictions, according to the following
heuristics:
? If a word w includes a denotation d, w seldom
co-occurs with any word whose core meaning
is d. For example, one possible paraphrase of a
sentence
He is extremely angry.
is
He is enraged.
where the word extremely is deleted, because
enraged has a denotation ?extremely? if angry
is defined as the core meaning of enraged.
? If w involves a lexical restriction l, w often co-
occurs with words whose core meaning is l. For
example, ?a rancid butter? is more appropriate
than ?a rotten butter?, because rancid has a lex-
ical restriction ?oily or fatty food?, while rotten
does not.
Based on these heuristics, our method classifies fine-
grained meanings of an entry as follows:
Step 1. Assign semantic labels to all words in cor-
pora (consisting of 1.93 million sentences,
including newspapers 3 and novels 4).
Step 2. Obtain co-occurrence frequencies of all
pairs between a word and a semantic label
of a neighbor word from the corpora.
Step 3. Delete the entry e from the thesaurus if e
does not appear in the corpora at all.
Step 4. For each fine-grained meaning f of e which
belongs to a semantic category C, compute
co-occurrence probabilities
P (f, C) =
?
i nsif?
i Nsi
(1)
P (f, e) = nefNe (2)
where si is a near-synonym of e, nab is the
co-occurrence frequency between a word a
and a label b, and Na is the frequency of a.
Step 5. Remove f if P (f, C) = 0.
Step 6. Define f as a denotation if P (f, e) = 0. The
weight of the denotation is the product of
P (f, C) and the weight of f .
Step 7. Define f as a lexical restriction if P (f, e) 6=
0. The weight of the lexical restriction is the
product of P (f,e)P (f,C) and the weight of f .
Figure 3 shows an example of classification about
the word ?saikon (??)?. In Figure 3, under-lined
features are the results of word sense disambiguation
and elimination of stop words.
3Mainichi Shimbun CD-ROM
http://cl.aist-nara.ac.jp/lab/resource/
cdrom/Mainichi/MS.html
4Aozora Bunko http://www.aozora.gr.jp/
Word: [394]
????
?? saikon
(rebuilding of shrines / temples)
Denotation:
[277b]?? naosu (to alter: 1.45)
[392]?? naosu (to repair: 4.19)
Lexical restriction:
[727a]?? jinja (shrine: 8518)
[940c]?? bukkaku (temple: 5859)
[277c]?? naosu (to recover: 3504)
[417a]?? naosu (to get right: 2135)
[811]?? koto (matter: 15)
Figure 3: Classification example of fine-grained
meanings
3.4 Evaluation and discussions
We applied these procedures to all 57,130 entries in
Kadokawa thesaurus (2,924 categories). As a result,
36,434 entries, which consist of one core meaning
and 0 or more fine-grained meanings, and 1,857 en-
tries, which has no core meaning but is refered as a
core meaning to other entries, were obtained. One
entry has 4.7 denotations and 5.1 lexical restrictions
on average.
To evaluate our methods, we compared the results
of automatic extraction against manually extracted
ones for randomly selected 50 entries. Table 1 shows
the result of extracting core meanings, and the result
of the classification is shown in Table 2.
number of entries
corrects 40
errors 10
(direct) (4)
(indirect) (6)
precision 80 %
Table 1: Result of extracting core meanings
Failure results of extractions of core meanings ap-
peared in the following cases; a core meaning in
a definition statement does not belong to the same
semantic category as the entry; the correct core
meaning involves negative expressions in a defini-
tion statement; or two or more near-synonyms are
appeared in one definition statement. Therefore, the
extraction of core meanings needs to be estimated
without relying on their semantic categories, that
is, with other information such as modification re-
result
recall [%]denotation lexical restriction
answer
denotation 56 13 81.2
lexical restriction 22 20 47.6
precision [%] 71.8 60.6
Table 2: Result of classification
lations of a definition statement.
Table 2 shows that both the precision and the
recall of the classification into lexical restrictions
are worse than the ones of denotations. A sparse
data problems could cause it. In our classification
method, if a feature of an entry does not co-occur
with the entry, the feature is classified into a denota-
tion or deleted, even though it is expected to be de-
fined as a lexical restriction. It would be improved
by increasing domains and the size of corpora, or by
using information of modification relations just as
the extraction of core meanings.
4 Preferential presentation of
near-synonyms
We secondly propose a method of ranking near-
synonyms by using information derived in Sec-
tion 3. Though (Edmonds, 1999) proposed a ranking
method for lexical choice by using information of
fine-grained meanings in I-Saurus, it requires more
detailed information than the one which can be ex-
tracted from a definition statement. Thus, this pa-
per proposes a ranking method as follows: when
a target word in a sentence is given, our method
obtains all near-synonyms5 of the target word and
their semantic features. Then, our method ranks the
near-synonyms with respect to their suitability be-
tween the input context and features of each near-
synonym. Additionally, if a paraphrase to a near-
synonym causes neighbor words in the input sen-
tence to arrange in order to keep semantic consis-
tency, our method adds such information to the near-
synonym when the ranking is presented.
4.1 Comparison between denotations and
contexts
?Denotations? can appear in any word, including
a target word in an input sentence. Therefore, all
5There are sometimes two or more core meanings in one
semantic category. We treat whole core meanings as the exactly
same meaning here.
denotations of each near-synonym have to be com-
pared not only with the input context but with deno-
tations of a target word. Our method determines the
propriety of paraphrasing between a target word w
and its near-synonym si for each denotation dij of
si, with the following cases:
Case 1. No denotation appears in neither w nor si:
? w can be directly paraphrased to si.
Case 2. w has a denotation dw equivalent to dij :
? w can be paraphrased to si on the sense
of dij .
Case 3. dw does not match with any dij :
? w can be paraphrased to si with adding
dw to the input sentence.
Case 4. dij does not match with any dw:
(a) if dij can be covered with a neighbor
word w? of w in the input sentence:
? w can paraphrase to si with deleting
w? from the input sentence.
(b) if dij can not be covered with any words
in the input sentence:
? w can not be paraphrased to si.
In Case 3 and Case 4a, some arrangements (addi-
tion / deletion of words) to the input sentence are
needed. Our method presents these information with
the presentation of near-synonyms rankings (in Sec-
tion 4.3).
According to these cases, the total denotational
score Sd of si is defined by
Sd =
?
j
pWj (3)
where Wj is the weight of dij (one of the denota-
tions of si) and
p =
?
??
??
1 (in Case 1, 2, 4a)
0 (in Case 3)
?1 (in Case 4b)
Note that Case 3 gives no weight, because the case
does not consider any denotation of si but compares
only between dw and its context.
4.2 Comparison between lexical restrictions
and contexts
?Lexical restriction?, the other fine-grained mean-
ing, is the feature which notably often co-occur with
its target word, as described in Section 3.3. In fact,
however, a word which often co-occurs with a target
word does not have to belong exactly to one of the
lexical restrictions of the target word. They could
be the ?similar? words. Therefore, it is necessary to
compute the similarity between a lexical restriction
and a context in order to compare them.
The thesaurus used in our method has a tree struc-
ture and each entry belongs to the node at 4 or 5 in
depth. The similarity can be defined by a heuristic
approach that any two words are semantically inde-
pendent if the depth of their root node is less than
3, such as the categories between [588] ?rebels? and
[506] ?private and public?. Hence, our method de-
fines the similarity between a lexical restriction vi
and a semantic label qi of a word in an input context
as follows:
sim(vi, qi) = log2
(dep (root(vi, qi))? 4
dep(vi) + dep(qi)
)
(4)
where root(a, b) is the root node of the minimum
subtree which includes both a and b, and dep(a) is
the depth of a in the thesaurus.
To determine the score of a lexical restriction,
there is another problem. An input sentence has sev-
eral content words outside of the target word, and
some of them belong to several semantic categories
because of their ambiguities. Also, the target word
often has two or more lexical restrictions. Thus,
each lexical restriction must select a semantic label
which has the highest similarity with the lexical re-
striction from the input sentence. Against the prob-
lem, first, our method computes the similarities of all
possible pairs which consist of a lexical restriction
and a semantic label extracted from the sentence.
Then, our method extracts pairs in descending or-
der of the similarity with no overlap in any category
or any lexical restriction.
Based on this process, we can compute the total
score Sv of each near-synonym si of a target word
w in an input sentence, with all extracted pairs of a
lexical restriction vj and a semantic label qj in the
input sentence by
Sv =
?
j
(Wj ? sim(vj , qj)) (5)
where Wj is the weight of vj .
4.3 Ranking method
This section describes our method of ranking near-
synonyms with respect to the scores defined in Sec-
tion 4.2 and Section 4.1, which is the aim of this
paper. The criterion of ranking is simply the sum of
normalized Sd and Sv6. Our method presents near-
synonyms according to their ranking, and if neces-
sary, information of arrangements to an input sen-
tence (extracted in Section 4.1) are shown with each
near-synonym.
4.4 An example
When an input sentence is
?????????
tera (joss house) wo (OBJ) tate (to build)
naosu (to repair)
(Someone rebuilds a joss house.)
and the word ??? (?) (tate(ru), to build)? is given
as a target, the semantic labels assigned to each con-
tent word in the sentence are
? tera [727b] temple
?? tate [394] to build
?? naosu [277b] to alter [277c] to recover
[392] to repair [417a] to get right
and 24 near-synonyms of tateru are extracted. Then,
our method computes Sd and Sv for each near-
synonym. For example, the scores of a word ?
????
??
(saikon, rebuilding of shrines / temples)?, which in-
cludes features shown in Figure 3, are given as fol-
lows:
? Sd (the denotational score)
For the denotations of saikon, [277b] (to al-
ter: 1.45) and [392] (to repair: 4.19) could be
obtained, where the italic numbers show their
weight. They match to the labels in the word
naosu, thus Sd of saikon is 5.64 and the word
naosu is given as a deletion information.
? Sv (the score in lexical restriction)
For the lexical restrictions of saikon, [277c] (to
recover: 3504), [417a] (to get right: 2135),
[727a] (shrine: 8518), [811] (matter: 15) and
[940c] (temple: 5859) could be obtained, then
the extracted pairs and their similarity are cal-
culated as follows:
6Each score has to be normalized because the place of Sd
far differs from that of Sv .
lexical
context similarityrestriction
[277c] ? [277c] 1.00
[417a] ? [417a] 1.00
[727a] ? [727b] 0.68
[811] ? [392] ?1.00
[940c] ? [277b] ?1.32
Therefore, Sv of saikon is calculated as 3682.
Finally, by computing Sd and Sv of all the
other near-synonyms, our method ranks the near-
synonyms and presents them as shown in Figure 4.
In Figure 4, the first 9 near-synonyms can be para-
phrased from the target word appropriately. How-
ever, saikon is ranked next to fushin contrary to our
expectation that it would be ranked as the first, be-
cause saikon and the fifth word saiken has the same
orthography, and thus the co-occurrence information
of saikon is imprecise by mixture with the informa-
tion of saiken.
4.5 Evaluation and discussions
To evaluate our ranking method, we randomly ex-
tracted 40 sentences from corpora and applied our
method to a certain word in each sentence. Also, for
each case, we manually selected all near-synonyms
which can be paraphrased7. We evaluated the rank-
ing results of our method by the measure of non-
interpolated average precision (NAP):
NAP = 1R
n?
i=1
zi
i
(
1 +
i?1?
k=1
zk
)
(6)
where R is the number of near-synonyms which can
be paraphrased, n is the number of presented near-
synonyms, and
zi =
?
?
?
1 if a near synonym in rank i can be
paraphrased
0 otherwize
Table 3 shows the result.
Table 3 shows that our method is remarkably ef-
fective for the judgement of semantic suitability of
near-synonyms if a target word is not ambiguous.
However, the average precision is worse for ambigu-
ous words, thus it is important to disambiguate those
target words before applying to our method.
7For the criterion if a word can paraphrase to another or not,
we dissemble any addition / deletion informations. That is, we
assume that a word can paraphrase if the paraphrased sentence
has the same meaning as the original with some changes to their
context.
ambiguity of NAP [%]
target word our method non-
(sentences) Sd Sv Sd + Sv ordered
distinct (21) 74.2 63.8 71.2 60.0
vague (19) 48.8 48.3 51.0 42.1
both (40) 62.8 56.9 62.2 52.0
Table 3: Average precision of ranking
Most of failure results are caused by the follow-
ing cases; incorrect core meanings or fine-grained
meanings were extracted in Section 3; adequate re-
lations between a near-synonym and an input con-
text could not be identified because of the ambiguity
of neighbor words in the input sentence; or the se-
mantic range of the label of a denotation or a lexi-
cal restriction is too wide to express the fine-grained
meaning of the near-synonym clearly.
In addition, Table 3 shows that the average preci-
sion by only Sv is worse than the one by only Sd. It
could be caused by the low precision of classifica-
tion into lexical restrictions and by the inadequacy
in the measure of similarity described in Section 4.2.
To improve those problems, another measure such as
semantical similarities without using a structure of a
thesaurus is needed. Also, we would learn from a
method of lexical choice with knowledge about col-
locational behavior (Inkpen and Hirst, 2002).
Though we have not discussed the evaluation of
the propriety of arrangements to an input sentence,
it seems that the information of addition often occurs
imprecisely, against that the information of deletion
appears infrequently but almost correctly, because,
in our method, all denotations of a target word are
given as the information of addition when they do
not match with any denotation of a near-synonym.
Therefore, we must define the importance of each
addition information and to present selected ones.
5 Conclusion and future work
This paper proposed a new method of preferential
presentation of Japanese near-synonyms in order to
treat with semantic suitability against contexts, as a
first step of semantic paraphrase system for elabo-
ration. We achieved the effectiveness of using def-
inition statements for extracting fine-grained mean-
ings, especially for denotations. Also, the experi-
mental results showed that our method could rank
near-synonyms of an unambiguous word for 71%
1. ?? fushin (??: ??) (delete naosu) 6. ?? chikuzo
(Construct or repair a house / a temple / a road) (Build or construct)
2.
????
?? saikon (??: ??) (delete naosu) 7. ??? tateru
(Rebuild a shrine / a temple) (Build)
3. ?? shuchiku (??: ??) (delete naosu) 8. ?? kizuku
(Repair a house etc.) (Build)
4. ?? konryu 9. ?? kenzo
(Build a chapel / a tower of a temple) (Construct a buildiing / a ship)
5.
????
?? saiken 10. ???? tatemashi
(Rebuild or Reconstruction) (Add to a building)
Figure 4: Result of preferential presentation of ?tera wo tate naosu.?
in accuracy by non-interpolated average precision,
about 10 points higher than non-ordered.
We have discussed only the initial step of the elab-
oration system, thus one of our future work is to
handle syntactic and semantic constraints on actual
paraphrasings after applying this method.
Acknowledgements
We would like to thank Mainichi Shinbun-sha and
Aozora Bunko for allowing us to use their corpora,
and Kadokawa Sho-ten for providing us with their
thesaurus. We are also grateful to our colleagues for
helping our experiment.
References
Akira Suganuma, Masanori Kurata and Kazuo Ushijima.
1990. A textual Analysis Method to Extract Negative
Expressions in writing Tools for Japanese Documents.
Journal of Information Processing Society of Japan,
31(6):792?800. (In Japanese)
Atsushi Fujita and Kentaro Inui. 2001. Paraphrase of
Common Nouns to Its Synonyms by Using Definition
Statements. The Seventh Annual Meeting of The As-
sociation for Natural Language Processing, 331?334.
(In Japanese)
Chrysanne DiMarco, Graeme Hirst and Manfred Stede.
1993. The semantic and stylistic differentiation of
synonyms and near-synonyms. AAAI Spring Sympo-
sium on Building Lexicons for Machine Translation,
114?121.
Diana Zaiu Inkpen and Graeme Hirst. 2002. Acquir-
ing Collocations for Lexical Choice between Near-
Synonyms. ACL 2002 Workshop on Unsupervised
Lexical Acquisition, Philadelphia.
George E. Heidorn. 2000. Intelligent Writing Assis-
tance. In Robert Dale, Hermann Moisl and Harold
Somers (eds.), A Handbook of Natural Language Pro-
cessing, Marcel Dekker, New York. Chapter 8.
Hiroko Inui and Naoyuki Okada. 2000. Is a Long Sen-
tence Always Incomprehensible?: A Structural Anal-
ysis of Readability Factors. Information Process-
ing Society of Japan SIGNotes Natural Language,
135(9):63?70. (In Japanese)
Kentaro Inui and Satomi Yamamoto. 2001. Corpus-
Based Acquisition of Sentence Readability Ranking
Models for Deaf People. Proceedings of the sixth
Natural Language Processing Pacific Rim Symposium
(NLPRS), 159?166, Tokyo.
Mark Dras. 1998. Search in Constraint-Based Paraphras-
ing. Proceedings of the second International Confer-
ence on Natural Language Processing and Industrial
Applications, 213?219, Moncton.
Noriko Tomuro and Steven L. Lytinen. 2001. Selecting
Features for Paraphrasing Question Sentences. Pro-
ceedings of the Workshop on Automatic Paraphrasing
at Natural Language Processing Pacific Rim Sympo-
sium (NLPRS), 55?62, Tokyo.
Philip Edmonds. 1999. Semantic Representations of
Near-Synonyms for Automatic Lexical Choice. Ph.D.
thesis, Department of Computer Science, University of
Toronto.
Satoshi Shirai, Satoru Ikehara, Akio Yokoo and Yoshi-
fumi Ooyama. 1998. Automatic Rewriting Method
for Internal Expressions in Japanese to English MT
and Its Effects. Proceedings of the second Interna-
tional Workshop on Controlled Language Applications
(CLAW-98), 62?75.
Shin Ohno and Masato Hamanishi. 1981. New Synonym
Dictionary. Kadokawa Shoten, Tokyo.
Yuji Matsumoto, Akira Kitauchi, Tatsuo Yamashita,
Yoshitaka Hirano, Hiroshi Matsuda, Kazuma Takaoka
and Masayuki Asahara. 2002. Morphological Anal-
ysis System ChaSen 2.2.9 Users Manual. Nara Ad-
vanced Institute of Science and Technology, Nara.
An Annotation Tool for Multimodal Dialogue Corpora  
using Global Document Annotation 
Kazunari ITO and Hiroaki SAITO 
Keio University 
Department of Science for Open and Environmental Systems 
3-14-1 Hiyoshi, Kohoku-ku, Yokohama, Japan, 223-8522 
 {k_ito , hxs}@nak.ics.keio.ac.jp 
 
 
 
 
 
Abstract 
This paper reports a tool which assists the 
user in annotating a video corpus and en-
ables the user to search for a semantic or  
pragmatic structure in a GDA tagged cor-
pus. An XQL format is allowed for search 
patterns as well as a plain phrase. This 
tool is capable of generating a GDA time-
stamped corpus from a video file  manu-
ally. It will be publicly available for aca-
demic purposes. 
1 Introduction 
To achieve a natural communication environment 
between computers and the users, many interactive 
prototype systems that can talk with the user have 
been developed using multimodal information 
(face expressions, voice tones, gestures, etc .). 
Since multimodalness of these systems is manually 
built in, achieving free and effective communica-
tion or enhancing communication ablilities is not 
easy. Thus automatic learning from huge data is 
hoped for.  
Recently such various video data as TV dramas, 
news, and language teaching materials are avail-
able, from which natural interactiveness should be 
extracted. Such interactiveness from an intellectual 
content is also valuable for the fields of machine 
translation, information retrieval, handling ques-
tion responses, and knowledge discovery systems. 
GDA1 (Global Document Annotation), which is 
an XML tag set, adds information on syntax, se-
mantics, and pragmatics to texts (Hashida 1998). 
The texts with GDA organically corresponding to 
voice and a video will contribute to the basic re-
search into these technologies and promote the ap-
plication development. 
2 GDA tagged corpus 
This chapter explains the GDA tag set and a 
method which relates tagged data with the video 
image. 
 
2.1 GDA 
 
The GDA Initiative aims at having Internet authors  
annotate their electronic documents with a com-
mon standard tag set which allows machines to  
automatically recognize the semantic and prag-
matic structures of the documents. A huge amount 
of annotated data is expected to emerge, which 
should serve not just as tagged linguistic corpora 
but also as a worldwide, self-extending knowl-
edge-base mainly consisting of examples of how 
our knowledge manifests. It describes the meaning 
of sentence analysis (semantics and pragmatics) 
basically. It also describes information on the  sub-
ject role, the rhetoric relation, and correspondence. 
Figure 1 shows an example of the text ?????
???? (an ear is covered)? tagged with GDA. 
Note that GDA is totally language independent, 
although all the following examples include Japa-
nese texts. 
 
                                                               
1 http://www.i-content.org/GDA/tagset.html 
<q who="A"> 
<su syn=?fc? id="kakure"> 
<vp syn="f"> 
<n arg="X">??</n> 
<ad sem="obj">?</ad> 
<adp syn="f"> 
<v>???</v> 
<ad>?</ad> 
</adp> 
<v>?</v> 
</vp> 
</su> 
</q> 
 
Figure 1: A fragment of GDA corpus 
 
In Figure 1, <q> represents a word where that 
part is spoken by someone, and the value of who 
attribute shows who utters it. <su> indicates a sen-
tence, having no syntactic relation to other parts of 
the utterance. Attribute ?syn? means it is a syntax 
structure of the sentence, and ?fc? means a forward 
link dependency. <v> and <vp> elements mean a 
verb and a verb phrase, respectively. <n> element 
represents a noun or a noun phrase. <ad> and 
<adp> elements are an adverb, and a postpositional 
phrase. As these examples display, the GDA tag 
set has been determined to show syntactic structure 
effectively where a word is assumed to be a unit. 
 
2.2 Adding Time -information to GDA tagged 
corpora 
 
When you relate the video image file with its text 
file, it is widely used to embed the time  informa-
tion indicating when an utterance is spoken. We 
define the following two kinds of new formats to 
relate a video file with its GDA corpus file . 
 
1. btime and etime attributes: These attributes can 
be added to an arbitrary tag. Attribute btime shows 
the start time of an utterance. Attribute etime 
shows the finished time. The format is described as 
follows. 
<any btime=?utterance start time(sec)? 
etime=?utterance end time(sec)? > 
sentence 
</any> 
 
With these attributes, the voiceless section of an 
utterance can be precisely indicated and the over-
lap event is detected in a multi-speaker environ-
ment. 
2. tst (time stamp) tag: Tag ?tst? is an empty ele-
ment tag and described in the following format. 
 
<tst val = ?utterance start time 
(sec)? /> 
 
A tst tag can be inserted in an arbitrary place. The 
ending time of an utterance can be determined 
from the value of the next btime attribute or val 
attribute of the next tst tag.  Moreover, it is also 
possible that the tst tag has btime and etime attrib-
ute. Figure 2 shows an example when time infor-
mation is added to the GDA tagged corpus in 
Figure 1. You can see a man allocated ?A? speaks 
a word ??? (an ear)? from 60.81 to 61.03 sec, a 
word ?? (is)? from 61.10 to 61.90 sec. 
 
<q who=?A?> 
<su id=?kakure?> 
<vp syn=?f?> 
<n arg="X" btime=?60.815? 
etime="61.034?>?? 
</n> 
<ad sem=?obj?> 
?<tst val=?61.100?/></ad> 
<adp syn=?f?> 
<v>??? 
<tst val="61.907"/></v> 
<ad>?<tst val="62.192"/></ad> 
</adp> 
<v>?<tst val="62.383"/></v> 
</vp> 
</su> 
</q> 
 
Figure 2: An example of GDA corpus with time-
stamp 
 
These time annotation is often inserted after 
GDA tagging, because time information is inde-
pendent from the standard syntactic/pragmatic 
GDA. 
3 An annotation tool for multimodal dia-
logue corpus 
We have developed a multimodal annotation tool 
for a video corpus and its annotated data (JEITA 
2001). This tool runs on any platform that accom-
modates Java2 and Java Media  Framework (JMF2) 
ver.2.0 or higher. It also requires Java-based XML 
parser Xerces-J 3  and Java-based XQL engine 
                                                               
2 http://java.sun.com/products/java-media/jmf/ 
3 http://xml.apache.org/xerces-j/ 
GMD-IPSI XQL4. Moreover, users can easily ex-
tend functions by mounting plug-ins. The proto-
type is equipped with two different types of plug-
ins. One is ?XQL search plug-in? in which the user 
can search for various syntactic and semantic  pat-
terns in a GDA corpus. An XQL format query as 
well as a plain word is allowed for search patterns. 
Another is ?Annotation plug-in? which assists the 
annotator in generating a time-stamped GDA cor-
pus from a video file. 
 
4 Basic Functions 
A screenshot of basic composition is shown in 
Figure 3. The whole window is composed of sev-
eral internal windows.  
 
 
Figure 3: Screenshot of basic  composition 
 
The left internal window is ?time table window? 
which displays each sentence in a table format. 
When the user clicks one of the columns in the 
table, the video corresponding to the sentence is 
played. Rows of the table are highlighted consecu-
tively during that part of the video is being dis-
played. The top right is ?video image window?. It 
displays and plays the video image.  
5 Extended Functions 
This section explains the outline of two plug-ins 
first and usage of  these after that. 
 
5.1 XQL search plug-in 
 
                                                               
4 http://xml.darmstadt.gmd.de/xql/  
Figure 4 shows the screenshot when the XQL 
search plug-in is loaded, you can see new window 
appears at the bottom left of the tool. The user can 
type a query into the text field at the bottom of the 
new window.  
 
 
Figure 4: Screenshot when XQL search plug-in is 
loaded 
 
An acceptable query format is a plain text or a 
query text defined by XQL (XML Query Lan-
guage). XQL is a subset of query language XQuery 
that uses XML as a data  model, a recommendation 
by W3C. XQL has already been mounted on soft-
ware over many fields like Web browsers, docu-
ment archiving systems, XML middleware, Perl 
libraries, and a command line utility. XQL always 
returns a part of the document. In XQL, the hierar-
chy of the node is written by ?/?, an arbitrary hie r-
archy is written by ?//?, the attribute name by 
?@name?, the tag name by ?name? as it is. A regu-
lar expression and a conditional expression are also 
acceptable. For a exmple, ? //q[@who='A']? returns 
?q? node with the value of attribute name ?who? 
equals ?A?.  
Figure 5 shows search results. In Figure 5, five 
words [????(a hair), ????(a forelock), ?
??(a prominent forehead), ??(a face), ??(an 
ear)] are matched on the condition that the part of 
speech is noun and the value of the attribute ?arg? 
equals to ?X? (see the fourth line of Figure 2). 
Moreover, since the label ?X? is attached to the 
utterance "???(this person)" in this GDA file, 
this query becomes a meaning of extracting a noun 
phrase whose subject is ?this person?. The user can 
search for the subject even if an object is omitted 
in suited sentences.  
  
Figure 6:  Screenshot when Annoation plug-in is loaded 
 
A query of extracting a synonym of a certain 
word, only the utterance of a specific speaker to 
another, and a sentence that of the response for a 
certain utterance, etc. are possible. Semantic  or 
discourse structures are also extractable if such 
information is annotated in the file. Clicking any 
column of the table, the corresponding media sec-
tion is played like the ?time table Window?. 
 
 
Figure 5: Result by query [a noun and value of at-
tribute ?arg? equals ?X?]  
 
5.2 Annotation plug-in 
 
Creating a GDA file with timestamps record auto-
matically with less time and less labor is indispen-
sable to make large quantities of them and to 
spread them. From the accuracy of the current 
speech recognition technology, it is difficult to at-
tach timestamps record automatically and accu-
rately by taking synchronization of a video image. 
Annotation plug-in increases the efficiency of 
time-stamping a GDA file by visual operation.  
Figure 6 shows the screenshot when it is loaded. 
The window located on the lower part of Figure 
6 called ?Annotation board? which displays infor-
mation on a GDA file with a time-stamp visually. 
You can also see a horizontal axis which expresses 
the time on media, and two layers in the board. 
Upper layer displays utterances of ?Speaker A?, 
lower layer displays ?Speaker B? in this case. Rec-
tangles on one layer represent each speaker's utter-
ances according to the time series. The utterance 
itseft is displayed in the rectangle, color of which 
is different for each speaker. Length and the pos i-
tion indicate time information of the utterance. A 
person edits the annotation by operating two kinds 
of lines on the board. "Current line" shows a cur-
rent playback position on the media. "Base line" 
indicates the start or the end point of the time-
stamp when the utterance sentence is newly in-
serted. Various functions (change of line?s position, 
and deletion or insertion of an utterance) can be 
executed by mouse operation. When the annotator 
clicks on the board, the ?Current line? moves and a 
frame to which a video image corresponds is dis-
played. Thus, an annotator can attach the informa-
tion of the start time and the end time and utterance 
text itself manually. A prototype system which 
automatically converts the GDA file from raw text 
files with a morphological analysis and a parsing 
tool in addition to the original filter has already 
been proposed (Suzuki 2001). 
6 Current development 
The core functions of the tool are complete and 
stable. Still, there is much room for expansion. The 
following functions are being developed. 
 
6.1 User-friendly GUI-based search interface 
 
Needless to say, there is no guarantee that a clause 
or a sentence which agrees with an XQL query 
exists. Moreover the user has to know the XQL 
expression for search. We believe it necessary that 
a retrieval way by the top-down philosophy which 
narrows the candidates while presenting suited 
clauses sequentially. 
It is very difficult at present for an XQL to ex-
press dependency relations among the search con-
dition. Now, a query language of XML has been 
integrated into Xquery. Hence, we are scheduled to 
bulid an Xquery engine. A user-friendly GUI-
based search window for the retrieval which does 
not require an explicit XQL query is currently be-
ing developed.  
 
6.2 Uniting with other multimodal data 
 
There are many kinds of specifications to describe 
multimodal data. For example, J-ToBI(Venditti, 
1995) which describes prosodic information of 
voice, FACS(Ekman, etc. 1978) which annotates 
person's expression, etc. We are scheduled to de-
sign the specification to integrate these information 
into GDA in the XML format. As a result, the user 
will be able to present a condition, for example, of 
a word ?Truth? of doubt type or ?I see? of  hesita-
tion. 
It is also scheduled to relate visual information 
of video data with GDA. They can contain infor-
mation on motion, glance and the place of the ob-
ject in video image. A reverse-search which 
extracts a corresponding text from visual informa-
tion in a video image becomes available, too. 
 
6.3 Coordinated functions 
 
We intend to enhance a relation with other annota-
tion tools. Concretely, various formats of output 
files can be taken in this tool in XML formats. We 
shall define a DTD (data conversion definition) to 
enable export and import to/from other tools. A 
function of date exchange enables the user to en-
hance flexibility and accessbility of this tool. 
 
6.4 Retrieval for multiple files 
 
The user can retrieve only a single file in a local 
machine at present. This tool will cope with the 
client-server model that it requests retrieval de-
mand to the corpora database servers on a network, 
downloads only necessary files to the local ma-
chine and analyzes them. 
7 Related works 
Most of recent multilmodal annotation tool pro-
jects are almost Java-based, use XML for file  ex-
change and have an object-oriented design:    
MATE (Carletta, etc. 1999) is an annotation 
workbench that allows highly generic  specification 
via stylesheets that determine  shape and 
functionality of the user?s implemation. Speed and 
stability for the tool are both still problematic for 
real annotation. Also, the generic approach 
increases the initial effort to set up the tool since 
you basically have to write your own tool using the 
MATE style language.  
EUDICO (Brugman et al 2000) is an effort to 
allow multi-user annotation of a centrally localted 
corpus via a web-based interface. The tools that are 
available to work with the  multimodal corpus 
make it possible to analyze their content and to add 
new free defined annotations to them. A EUDICO 
client can choose a subset of the corpus data.  
Anvil (Kipp, 2001) is a generic video annota-
tion tool which allows frame-accurate, hierarchical 
multi-layered annotation with objects that contain 
attribute-value pairs. Layers and attributes are all 
user-defineable. A time-aligned view and some 
configuration options make coding work quite in-
tuitive.  
ATLAS project (Steven, etc. 2000) deals with 
all types of annotation and is theoretically based on 
the idea of annotation graphs where nodes repre-
sent time points and edges indicate annotation la-
bels. 
8 Conclusion 
We have reported an annotation tool for multimo-
dal dialogue corpora. This tool enables semantic 
and pragmatic search from a video data with anno-
tated texts in the GDA format. This tool is plat-
form-independent and equipped with easy-to-use 
interface. It will be of use to researchers dealing 
with multimodal dialogue for exploratory studies 
as well as annotation. Core functions are complete 
and various extension facilities are now being de-
veloped.  This prototype will be publicly available 
soon. 
9 Acknowledgements 
We wish to express our gratitute for the members 
of the committee on methods and standards of dia-
logic content in Japan Electronics and Information 
Technology Industries Association (JEITA). Espe-
cially we would like to thank Koichi Hashida for 
insightful advices. 
 
References 
Hasida, K. (1998) Intellectual contents of all-round 
based on GDA meaning modification. The transac-
tion of Japanese Society for Artificial Intelligence., 
Vol. 13, No.4, pp.528-535 (in Japanese). 
JEITA (2001) Servey Report about natural language 
processing system. pp.49-56(in Japanese). 
J. Suzuki and K. Hasida (2001) Proposal of answer ex-
traction system using GDA tag. The 7th annual meet-
ing of Language Processing Society (in Japanese). 
Ekman, P. and Friesen, W.(1978) Facial action coding 
system : a technique for the measurement of facia l-
movement, Consulting Psychologists Press, 1978 
 
J.J.Venditti,(1995) Japanese ToBI Labelling Guide lines. 
Ohio-State University,Columbus,U.S.A.,1995. 
Michael Kipp (2001) Anvil - A Generic Annotation 
Tool for Multimodal Dialogue, Proceedings of Eu-
rospeech 2001, pp.1367-1370. 
Carletta, J. and Isard, A (1999) The MATE Annotation 
Workbench, In Proceedings of the ACL Workshop, 
Towards Standards and Tools for Discourse Tagging., 
pp.11-17. 
H. Brugman, A. Russel, D. Broeder, and P.Wittenburg 
(2000) EUDICO. Annotation and Exploitation of 
Multi Media Corpora, Proceedings of LREC 2000 
Workshop. 
Steven Bird, David Day, John Garofolo, John Hender-
son, Christophe Laprun, and Mark Liberman (2000) 
ATLAS: A Flexible and Extensible Architecture for 
Linguistic Annotation, Proceedings of the Second In-
ternational Conference on Language Resource and 
Evaluation, pp.1699-1706. 
 
A New E-learning Paradigm through Annotating Operations
Hiroaki Saito, Kyoko Ohara, Kengo Sato, Kazunari Ito, Shinsuke Hizuka
Masaya Soga, Tomoya Nishino, Yuji Nomura, Hideaki Shirakawa, Hiroyuki Okamoto
Dept. of Computer Science, Keio University
3-14-1, Hiyoshi, Kohoku-ku, Yokohama, 223-8522, Japan
hxs@ics.keio.ac.jp
Abstract
This paper proposes a new e-learning paradigm
which enables the user to type in arbitrary sentences.
The current NLP technologies, however, are not ma-
tured enough to perform full-automatic semantic or
discourse analysis. Thus, we take a different ap-
proach; an instructor corrects the contents and its
correction is embedded into the contents in an XML
format called KeML. The key/mouse operations of
the user are also embedded as annotations. Thus,
the contents can be incomplete at the initial stage
and become solid gradually as being utilized. We
have implemented an e-learning system of group dis-
cussion for a foreign language class, which is demon-
strated at the workshop.
1 Introduction
Many old e-learning systems asked the user to click
the button or to type a unique answer. The domains
of the e-learning systems were limited by their ap-
proaches and their contents were fixed. This paper
introduces a new approach to expand the domain.
Namely, the user himself annotates the contents in
addition to the instructor.
Annotated contents can be used for further learn-
ing such as example-based learning and group learn-
ing. The burden of building e-learning contents are
so heavy that this ?annotated contents become other
contents? scheme is important for practical applica-
tions. Annotations are attached in an XML format.
This project can be considered as another applica-
tion of XML technologies, like MATE [1] [2] or Anvil
[3] to name a few. The principal difference is that
some annotations are implicitly attached and used
for NLP.
2 System Overview
Here we consider the debate discussion for a for-
eign language class as an example. This course was
originally taught in a regular classroom and through
an electronic chatting board supervised by a human
instructor. One student posts his/her thought in
English and others express positive or negative re-
sponses to it. Since this is a foreign language appre-
hension class, the instructor corrects the students?
English if necessary. Since students express various
opinions, we cannot prepare for them in advance.
The instructor was occupied with correcting syntac-
tic errors, therefore, the instructor could not thor-
oughly pay attention to the flow of the debate or to
whether students had appropriately expressed their
opinions.
In Figure 1 example debate submissions are shown
on the topic ?English should be taught at an elemen-
tary school.? #n indicates the submitted order and
P1, P2, ... stand for the identifier of the debaters.
(We will further explain Figure 1 later.)
Our system is designed for multi-user discussion,
not for self-learning. Thus, we divide the system
into the server and the client machines as shown in
Figure 2 (only one client machine is drawn in the
figure). The server machine manages the contents
and handles computationally heavy NLP, while each
client machine is responsible for user interface.
We have developed an e-learning system which of-
fers the process above. Here we describe five impor-
tant modules:
? Sentence analysis module: In this module the
input sentences are parsed and tagged syntac-
tically and semantically in the GDA (Global
Document Annotation) [4] format. We have
adopted the Charniak parser [5], which is cus-
tomized so that the head word is identified be-
cause the GDA tagging requires the attachment
direction between phrases. The GDA tagger
consults the WordNet [6] to find the root form
of a word. GDA tags can be utilized for such
further NLP as high quality summarization or
information retrieval [7].
? KWIC module: Novice English learners of-
ten make such mistakes in collocations and in
phrasal verbs. These word-usage mistakes can
be effectively resolved by looking at example
sentences rather than by consulting a regular
dictionary. This module presents the corpus
sentences in the KWIC (KeyWord in Context)
format in which the specified words are in-
cluded. Although any corpus will do for KWIC,
we have chosen the EDR English corpus [8]
which consists of 125,820 tagged sentences. Be-
cause the root form of each word is described as
a tag, conjugated forms are also searched.
? Annotation module: The instructor corrects the
wrong usages of the students? English. This op-
PROPOSITION
English should be taught as early as at an elementary school.
2. It is often said that language is most efficiently taught during his/her early age.
against early English education?
What kind of negative effect would the score-ism cause
#6   P3 (pro)
approval
refutation
refutation
supplement
correction
1. English is a global standard language and is indispensable for grownups.
might lower their total achievement.
I disagree with teaching English at an elementary school.
more effectively   tought   in a junior high school also.
Current curriculum of teaching English is not effective enough.
I agree that the method of teaching English should be improved.
However, if the similar improvement is performed against other subjects, early
English education has no bad effect to them.
taught
#1   P1 (pro)
#2   P2 (con)
#3   P3 (pro)
#4   P4 (con)
#5   P1 (pro)
I am for teaching English at an elementary school in the following points.
Teaching English relatively decreases the class hours of other subjects and
current education system of ?  score-ism   ?, not the starting age.
he/she is young, but what we have to do first is   to improve the
It is true that learning a foreign language is best effective when
question
approval
We can expect that children who become familiar with English earlier can be
Figure 1: Submission Statements and their Relations
NL resources
EDRcorpus
dictionary
Parser
Charniak
GDA tagger
KWIC
NLP modules
results
user interface
user
profile
DB
Retriever
Summarizer
with data
commands
DB
contents
document in KeML
client server
user
user operations
info
display
operations
Figure 2: System Architecture
Figure 3: A Snapshot of Interface Window
eration is recorded as annotation, not overwrit-
ing the originals. Preserving the originals is ef-
fective for education; it can prevent other stu-
dents from making the similar mistakes. When
the debater expresses his opinion against/for
someone else?s, that operation is also observed
and attached to the contents, which will be ex-
ploited by NLP.
? Interface module: This module enables the user
to type in sentences, specify what part he is
arguing about, express his/her attitude, etc, ef-
ficiently. This module displays the contents ef-
fectively according to the needs of the user with
the help of annotations. Our current interface
snapshot is shown in Figure 3.
? Debate Flow module
It is important to know the debate flow when
one expresses his/her opinion. Since the rela-
tions among statements are annotated, precise
analysis of the debate flow is possible.
In the following sections, the annotation module
is explained deeply.
3 Annotation by the Instructor and
Students
When a student expresses his/her opinion in re-
sponse to someone else?s, he can specify and denote
what part he is arguing about. This linkage is an-
notated by the user and recorded in the contents.
The corrections/comments by the instructor are also
stored in the learning contents as annotations. Ar-
rows in Figure 1 show the relation of statements,
where a dotted line expresses the linkage denoted by
the instructor, and solid lines mean that the debater
specified those relations.
4 The Tag Set for Debate
We have defined a tag set for annotating debates
in an XML format called KeML (Keio e-learning
Markup Language). Here we describe our tag set
along with how each tag is attached through opera-
tions by the instructor or students.
<debate> encloses the whole debate and is at-
tached when a new debate starts. No at-
tribute is allowed. Possible child-nodes are one
<proposition> and zero or more <statement>s.
<proposition> is attached when a new proposition
is submitted. Its mandatory attribute is ?id?
and whose value is always ?0?. Its child-node
is <su> of GDA. The instructor or students
should remark the proposition as pros or cons.
<statement> This tag is attached when a state-
ment to a proposition or other statements is
submitted by the instructor or students. Its
mandatory attributes are ?attitude? whose value
would be pro or con, ?person? whose value in-
dicates who submitted that statement, ?time?
which indicates when that statement was given,
and ?id? number (an integer). The values of the
first two attributes are given by the user explic-
itly, while those of the last two are filled by the
system. Its optional attributes are ?approval?,
?refutation?, ?supplement?, ?summary?, ?ques-
tion?, and ?answer? (some of those attributes ap-
pear in Figure 1). They are expressed as ?ap-
proval=target id? for example. Its child-node is
<su> of GDA.
Such tags below the <su> level as <np> or <v>
are attached by the parser according to the
GDA specifications. Every tag must have ?id?
attribute and its value is filled automatically by
the server.
Appendix shows the annotated contents of the de-
bate example in Figure 1.
5 Preserving Corrected Contents
In order that a novice student could observe mis-
takes by other students, our system preserves the
original contents and shows them effectively when
needed. While some mistakes are obvious, others
are not. Only the instructor can correct or comment
those errors and KeML offers two levels of correction
preservation. Obvious mistakes are stored as the
value of ?original? attribute; ?<np original=?tought?
..>taught</np>? for instance. Unobvious mistakes
are commented in the value of ?comment? attribute;
?<su comment=?This is a comment for this sen-
tence.? ....</su>? for example. When further cor-
rection is made against already corrected contents,
only the very first version is preserved. Our current
implementation allows the correction/comments un-
der <su> nodes.
6 Conclusions
We have implemented an e-learning system which fa-
cilitates group-discussion for second language learn-
ing. Plain texts become solid as being used because
of the embedded explicit and implicit annotations by
the instructor and students. Accumulated contents
will be a good resource for statistical analysis and
example-based learning.
References
[1] MATE Workbench Homepage:
http://www.cogsci.ed.ac.uk/? dmck/MateCode/
[2] MATE Homepape:
http://mate.nis.sdu.dk
[3] Anvil Homepage:
http://www.dfki.de/? kipp/anvil/
[4] The GDA Tag Set Homepage:
http://www.i-content.org/gda/
[5] Charniak, E. ?A Maximum-Entropy-Inspired
Parser?, NAACL 2000. (For software, see
http://www.cs.brown.edu/people/ec/)
[6] WordNet
http://www.cogsci.princeton.edu/? wn/
[7] Miyata, T. and Hasida, K. ?Information Retrieval
Based on Linguistic Structure? in Proceedings of
the Japanese-German Workshop on Natural Language
Processing, July 2003.
[8] EDR Electric Dictionary, EDR English Corpus
http://www2.crl.go.jp/kk/e416/EDR/
Appendix. Annotated contents (a head por-
tion)
<?xml version=?1.0? encoding=?UTF-8?
standalone=?yes??> <debate> <proposition
id=?0? time=?Sun Jun 13 22:08:30 JST
2004?> <su id=?0.1?> <segp id=?0.1.1?
mph=?ptb;NNP;;English;?>English</segp>
<v id=?0.1.2?> <v id=?0.1.2.1?
mph=?ptb;MD;;should;?>should</v> <vp id=?0.1.2.2?>
<v id=?0.1.2.2.1? mph=?ptb;VBN;;teach;?>taught</v>
<adp id=?0.1.2.2.2?> <ad id=?0.1.2.2.2.1?> <adp
id=?0.1.2.2.2.1.1? mph=?ptb;RB;;as;?>as</adp> <ad
id=?0.1.2.2.2.1.2? mph=?ptb;RB;;early;?>early</ad>
</ad> <adp id=?0.1.2.2.2.2?> <ad id=?0.1.2.2.2.2.1?
mph=?ptb;IN;;as;?>as</ad> <adp id=?0.1.2.2.2.2.2?>
<ad id=?0.1.2.2.2.2.2.1? mph=?ptb;IN;;at;?>at</ad>
<np id=?0.1.2.2.2.2.2.2?> <adp id=?0.1.2.2.2.2.2.2.1?
mph=?ptb;DT;;an;?>an</adp> <ajp id=?0.1.2.2.2.2.2.2.2?
mph=?ptb;JJ;;elementary;?>elementary</ajp> <n
id=?0.1.2.2.2.2.2.2.3? mph=?ptb;NN;;school;?>school</n>
</np> </adp> </adp> </adp> </vp> </v>.</su>
</proposition> <statement id=?1? attitude=?pro?
person=?P1? time=?Sun Jun 13 22:10:41 JST 2004?>
<su id=?1.1?> <np id=?1.1.1? mph=?ptb;PRP;;I;?
eq=?p1?>I</np> <v id=?1.1.2?> <v id=?1.1.2.1?
mph=?ptb;VBP;;be;?>am</v> <adp id=?1.1.2.2?>
<ad id=?1.1.2.2.1? mph=?ptb;IN;;for;?>for</ad>
<vp id=?1.1.2.2.2?> <v id=?1.1.2.2.2.1?> <v
id=?1.1.2.2.2.1.1? mph=?ptb;VBG;;teach;?>teaching</v>
<segp id=?1.1.2.2.2.1.2?
mph=?ptb;NNP;;English;?>English</segp> <adp
id=?1.1.2.2.2.1.3?> <ad id=?1.1.2.2.2.1.3.1?
mph=?ptb;IN;;at;?>at</ad> <np
id=?1.1.2.2.2.1.3.2?> <n id=?1.1.2.2.2.1.3.2.1?> <adp
id=?1.1.2.2.2.1.3.2.1.1? mph=?ptb;DT;;an;?>an</adp>
<ajp id=?1.1.2.2.2.1.3.2.1.2?
mph=?ptb;JJ;;elementary;?>elementary</ajp>
<n id=?1.1.2.2.2.1.3.2.1.3?
mph=?ptb;NN;;school;?>school</n>
</n> <adp id=?1.1.2.2.2.1.3.2.2?> <ad
id=?1.1.2.2.2.1.3.2.2.1? mph=?ptb;IN;;in;?>in</ad> <np
id=?1.1.2.2.2.1.3.2.2.2?> <adp id=?1.1.2.2.2.1.3.2.2.2.1?
mph=?ptb;DT;;the;?>the</adp> <vp
id=?1.1.2.2.2.1.3.2.2.2.2? mph=?ptb;VBG;;follow;?>follo
wing</vp> <n id=?1.1.2.2.2.1.3.2.2.2.3?
mph=?ptb;NNS;;point;?>points</n> </np>
</adp> </np> </adp> </v> </vp> </adp>
</v>.</su> <su id=?1.2?> <np id=?1.2.1?> <num
id=?1.2.1.1? mph=?ptb;CD;;1.;?>1.</num> <segp
id=?1.2.1.2? mph=?ptb;NNP;;English;?>English</segp>
</np> <v id=?1.2.2?> <v id=?1.2.2.1?> <v
id=?1.2.2.1.1? mph=?ptb;VBZ;;be;?>is</v>
<np id=?1.2.2.1.2?> <adp id=?1.2.2.1.2.1?
mph=?ptb;DT;;a;?>a</adp> <ajp id=?1.2.2.1.2.2?
mph=?ptb;JJ;;global;?>global</ajp> <ajp id=?1.2.2.1.2.3?
mph=?ptb;JJ;;standard;?>standard</ajp> <n
id=?1.2.2.1.2.4? mph=?ptb;NN;;language;?>language</n>
</np> </v> <segp id=?1.2.2.2?
mph=?ptb;CC;;and;?>and</segp> <vp id=?1.2.2.3?>
<v id=?1.2.2.3.1? mph=?ptb;VBZ;;be;?>is</v>
<ajp id=?1.2.2.3.2?> <aj id=?1.2.2.3.2.1?
mph=?ptb;JJ;;indispensable;?>indispensable</aj>
<adp id=?1.2.2.3.2.2?> <ad id=?1.2.2.3.2.2.1?
mph=?ptb;IN;;for;?>for</ad> <np id=?1.2.2.3.2.2.2?
mph=?ptb;NNS;;grownup;?>grownups</np> </adp>
</ajp> </vp> </v>.</su>
Proceedings of the Multiword Expressions: From Theory to Applications (MWE 2010), pages 81?84,
Beijing, August 2010
A Hybrid Approach for Functional Expression Identification
in a Japanese Reading Assistant
Gregory Hazelbeck
Graduate School of
Science and Technology
Keio University
gregh@nak.ics.keio.ac.jp
Hiroaki Saito
Graduate School of
Science and Technology
Keio University
hxs@ics.keio.ac.jp
Abstract
In this paper we present a hybrid approach
for identifying Japanese functional ex-
pressions and its application in a Japanese
reading assistant. We combine the re-
sults of machine learning and pattern-
based methods to identify several types
of functional expressions. We show that
by using our approach we can double the
coverage of previous approaches and still
maintain the high level of performance
necessary for our application.
1 Introduction
Functional expressions are one of the most im-
portant elements of Japanese grammar that any-
one studying the language must learn. Despite the
importance of functional expressions, many tools
that assist learners of Japanese with reading texts
only provide dictionary look-up of simple words.
However, the ability to quickly obtain informa-
tion about such grammar could not only improve
the learner?s comprehension of the text, but also
facilitate their learning process of new elements
of Japanese grammar. Thus, we have decided to
develop a Japanese reading assistant that is ca-
pable of providing explanations of functional ex-
pressions in addition to vocabulary.
Functional expressions in Japanese are com-
pound expressions that contain content and func-
tion words, and can have both compositional and
non-compositional meanings. For example, in Ta-
ble 1, sentences 1 and 2 contain the???? (ni-
atari) compound expression. In sentence 1, this
expression has a functional, non-compositional
meaning of ?when.? However, in sentence 2, the
same expression has a compositional meaning that
results simply from using the post-particle? (ni)
and verb ??? (a conjugated form of ???
(ataru), meaning ?to hit?) together. We refer to
this as the content usage of a functional expres-
sion. However, there are also functional expres-
sions where this type of content usage is very rare
(or even nonexistent). Sentence 3 shows an ex-
ample of the????????? (nakerebanari-
masen) functional expression which has a very
common functional meaning of ?must or have to.?
Tsuchiya et al (2006) have proposed a method
based on machine learning to identify functional
expressions. However, this method only covers
functional expressions which have balanced func-
tional vs. content usage ratios. In order to boost
coverage of current methods, we propose a hybrid
approach for functional expression identification
which uses a combination of the machine learning
method proposed by Tsuchiya et al (2006) and
simple patterns. Coverage analysis and empirical
evaluations show that our method doubles the cov-
erage of previous approaches while still maintain-
ing a high level of performance.
2 Related Work
2.1 Functional Expressions
Research on Japanese functional expressions has
included work on identification methods as well
as resources that aid identification. Matsuyoshi
et al (2006) developed a hierarchical dictionary
of functional expressions called Tsutsuji. The top
level of the dictionary?s nine level hierarchy con-
tains the lexical form of 341 expressions. The sec-
ond level categorizes these expressions by mean-
ing. The remaining seven levels contain various
81
???? (niatari)
1. Functional ???????????????????????????????????It is a custom in Japan to greet your neighbors when you move into a new apartment.
2. Content ?????????????????The ball hit me in the face and broke my tooth.
????????? (nakerebanarimasen)
3. Functional ??????????????????I have to go to school tomorrow.
Table 1. Examples of Japanese functional expressions.
surface forms for each expression where inser-
tion/deletion of particles and other conjugations
have been made. While this is the most compre-
hensive dictionary of Japanese functional expres-
sions, it can not be directly used for identifica-
tion because of the functional/content usage prob-
lem described in the previous section. Therefore,
identification methods like Tsuchiya et al (2006)
which uses Support Vector Machines(SVM) have
been proposed to solve this problem. The data set
(Tsuchiya et al, 2005) used to train this method,
called MUST, contains annotated instances of 337
functional expressions. For each expression, a
maximum of 50 instances were collected from the
1995 Mainichi newspaper corpus.
Recent work by the same group of researchers
(Nagasaka et al, 2010) indicates that they have
continued to annotate additional functional ex-
pressions for the MUST data set. During this
process, they have observed that only around one
third of all functional expressions possess a suf-
ficient amount of functional and content usages
to be used with their machine learning method.
However, they have yet to propose any method
to cover the other two-thirds of functional expres-
sions. Our hybrid approach aims to improve cov-
erage by identifying functional expressions that
fall into this group.
3 Identification Method
Our hybrid approach combines the results from
two different methods of functional expression
identification. First, we will describe our imple-
mentation of a method that uses machine learning.
Then, we will describe our method of generating
patterns for functional expressions.
3.1 Machine Learning
Our implementation of the method proposed by
Tsuchiya et al (2006) only deviates slightly from
its original form. We developed our own SVM-
based text chunking system in Python while the
original paper uses a text chunker called Yamcha1.
We also use the MeCab2 morphological analyzer
with a dictionary called UniDic while the original
paper used ChaSen with the default dictionary.
When training the SVMs, the original method
uses three sets of labels: functional, content, and
other. This allows both functional and content us-
ages to be explicitly identified. However, in our
application, we only need to identify functional
usages so that the expressions? correct definitions
can be displayed. Therefore, in our implementa-
tion we only use two sets of labels (functional and
other) and label all content usages as other. We
also decided to build a separate model for each
functional expression because it enables us to add
new training data and functional expressions with-
out having to retrain everything. Although this
does increase time complexity in the worse case,
in practice it does not have a big affect on perfor-
mance because only a small fraction of the total
number of models are being used for a given text.
Identification of functional expressions in a new
text is performed in the following steps:
1. Morphologically analyze the text with
MeCab and extract candidate functional
expressions from the morpheme sequence.
2. Select the model corresponding to each can-
didate functional expression.
1http://chasen.org/~taku/software/yamcha/
2http://mecab.sourceforge.net/
82
GeneratePatterns(C: list of candidates from Tsutsuji)
01 P = {}
02 for each candidate c in C:
03 S = sentences that contain c in the BCCWJ
04 for each sentence s in S:
05 Ms = morpheme sequence of s
06 Mc = ExtractCandMorph(c, Ms)
07 if Mc ?= null ? VerbChk(c, Mc, Ms, P):
08 Add Mc to P
09 break out of loop on line 4
10 end if
11 end for
12 end for
13 return P
Figure 1. The GeneratePatterns algorithm.
3. Use each model to conduct chunking. Label
any functional chunks as the model?s corre-
sponding functional expression.
4. Combine the results from each model. Re-
solve any overlapping chunks by the same
rules3 that Tsuchiya et al (2006) use to re-
solve overlapping candidate functional ex-
pressions during feature generation.
3.2 Patterns
We generate simple patterns to identify functional
expressions with a high ratio of functional usage.
First, surveys are conducted of functional expres-
sions in Tsutsuji using the Balanced Corpus of
Contemporary Written Japanese (BCCWJ)4. As
of writing this paper, we have selected 36 func-
tional expressions from Tsutsuji?s top level as can-
didates for pattern generation. We also included
various surface forms of these expressions from
other levels of Tsutsuji resulting in a total of 1558
candidate functional expressions. The algorithm
used to generate patterns is shown in Figure 1.
The ExtractCandMorph function simply re-
turns the candidate c?s morpheme sequence. If
the candidate?s string does not match the bound-
aries of morphemes in Ms then null is returned.
The VerbChk function returns true if a candidate
is an auxiliary verb from Tsutsuji?s top level and
the morpheme immediately preceding it inMs is a
verb. It returns true for lower level auxiliary verb
3Specifically, select the candidate that starts at the left-
most morpheme. If more than one candidate starts at the
same morpheme then select the longest candidate.
4Balanced Corpus of Contemporary Written Japanese
Monitor Release Data (2009 Version).
candidates if the last morpheme in its morpheme
sequence is also in the morpheme sequence of its
top-level parent candidate from Tsutsuji. For any
candidate that is not an auxiliary verb, the func-
tion always returns true. We force candidates from
lower levels to satisfy an extra condition because
their lower frequency in the BCCWJ increases the
probability that a sentence with the wrong ex-
pression/usage will be selected. This algorithm
produces one pattern per functional expression.
Each pattern is composed of the expression?s mor-
pheme sequence. This is a list where each ele-
ment contains a morpheme?s surface form, part of
speech, and lexical form. Patterns for auxiliary
verbs also check if the previous morpheme is a
verb. Using this algorithm, we were able to gen-
erate 502 patterns with our 1558 candidate func-
tional expressions.
4 Coverage Analysis
To investigate the improvement in coverage
achieved by our hybrid approach, we compared
the coverage of our approach with the coverage
of just the MUST data set. We define coverage
as the ratio of functional expressions contained in
both the Tsutsuji dictionary and BCCWJ that are
supported.
We first collected all of the functional expres-
sion surface forms contained in Tsutsuji. We ex-
cluded all of the single character surface forms
which are mostly simple particles. Next, we
recorded the frequency of each surface form?s
string in the BCCWJ. Overlapping of strings is
allowed as long as a string covers at least one
character that no other string does. Finally, we
recorded which surface forms were supported by
our hybrid approach and the MUST data set. Ta-
ble 3 shows our final results.
Our results show that MUST is only cover-
ing around 12% of Tsutsuji?s functional expres-
sions in the BCCWJ. The additional functional
expressions supported by our hybrid approach
helps boost this coverage to 24%. Improvement
in coverage is observed at every frequency inter-
val. This is especially advantageous for our ap-
plication because it allows us to display informa-
tion about many different common and uncom-
mon functional expressions.
83
Corpus Usage Examples Total Examples Total MorphemesFunctional Content
Training (MUST) 1,767 1,463 3,230 114,699
Testing (1995 Mainichi Newspaper) 5,347 1,418 6,765 244,324
Table 2. Training and testing corpora details.
Frequency
Interval
Tsutsuji MUST Hybrid
>5,000 199 44 (22%) 70 (35%)
5,000-1,001 244 70 (29%) 111 (45%)
1,000-501 134 37 (28%) 54 (40%)
500-101 519 124 (24%) 191 (37%)
100-51 269 53 (20%) 90 (33%)
50-26 327 54 (17%) 97 (30%)
25-11 467 46 (10%) 113 (24%)
10-2 1,180 55 (5%) 188 (16%)
1 723 11 (2%) 82 (11%)
Total 4,062 494 (12%) 996 (24%)
Table 3. Functional expressions covered by each
resource. Percentage of Tsutsuji covered in each
frequency interval is given in parenthesis.
Software (kernel) Precision Recall F? = 1
Yamcha (polynomial) 0.928 0.936 0.932
Our chunker (linear) 0.931 0.935 0.933
Table 4. Experiment 1 results.
5 Evaluation
We evaluated the machine learning method on 54
of the most difficult to identify functional expres-
sions. These are the same expressions that were
used in Tsuchiya et al (2006)?s evaluation. De-
tails of the training and testing data sets are shown
in Table 2. Results (Table 4) show that this method
performs well even on the most difficult functional
expressions. We also found that using a simple
linear kernel gave the best precision.
We evaluated the patterns generated from our
method by using them to identify functional ex-
pressions in randomly selected texts from the BC-
CWJ. After verifying 2000 instances of identified
functional expressions, we only found 6 instances
to be incorrect. However, since these 2000 in-
stances only cover 89 of the 502 expressions that
we support, we randomly selected two instances
of each remaining expression from the BCCWJ
and verified them. In the additional 750 instances
that were verified, only 10 instances were found
to be incorrect. Results of the second experi-
ment show that patterns generated for high fre-
quency functional expressions are providing espe-
cially good performance.
6 Conclusion
In this paper we presented a hybrid approach for
identifying Japanese functional expressions and
its application in a Japanese reading assistant. We
showed that a combination of machine learning
and simple patterns can improve coverage while
still maintaining the high level of performance
necessary for our application.
7 Acknowledgements
We would like to thank Takehito Utsuro for allow-
ing us to use his annotated functional expression
data to evaluate our approach. We would also like
to thank all of the other people involved in cre-
ating the MUST data set. Finally, we would like
to thank the anonymous reviewers for all of their
helpful comments.
References
Matsuyoshi, Suguru, Satoshi Sato, and Takehito Utsuro.
2006. Compilation of a Dictionary of Japanese Func-
tional Expressions with Hierarchical Organization. IC-
CPOL. pp. 395?402.
Nagasaka, Taiji, Takehito Utsuro, Suguru Matsuyoshi,
Masatoshi Tsuchiya. 2010. Analysis and Detection of
Japanese Functional Expressions based on a Hierarchical
Lexicon. Proceedings of the 16th Annual Meeting of the
Association for Natural Language Processing. pp. 970?
973. (in Japanese)
Tsuchiya, Masatoshi, Takao Shime, Toshihiro Takagi, Take-
hito Utsuro, Kiyotaka Uchimoto, Suguru Matsuyoshi,
Satoshi Sato, and Seiichi Nakagawa. 2006. Chunking
Japanese Compound Functional Expressions by Machine
Learning. Proceedings of the 2nd International Workshop
on Web as Corpus (EACL-2006). pp. 11?18.
Tsuchiya, Masatoshi, Takehito Utsuro, Suguru Matsuyoshi,
Satoshi Sato, and Seiichi Nakagawa. 2005. A Corpus
for Classifying Usages of Japanese Compound Functional
Expressions. PACLING. pp. 345?350.
84
