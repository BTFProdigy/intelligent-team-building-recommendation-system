Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 33?40
Manchester, August 2008
Improving Alignments for Better Confusion Networks
for Combining Machine Translation Systems
Necip Fazil Ayan and Jing Zheng and Wen Wang
SRI International
Speech Technology and Research Laboratory (STAR)
333 Ravenswood Avenue
Menlo Park, CA 94025
{nfa,zj,wwang}@speech.sri.com
Abstract
The state-of-the-art system combination
method for machine translation (MT) is
the word-based combination using confu-
sion networks. One of the crucial steps in
confusion network decoding is the align-
ment of different hypotheses to each other
when building a network. In this paper, we
present new methods to improve alignment
of hypotheses using word synonyms and a
two-pass alignment strategy. We demon-
strate that combination with the new align-
ment technique yields up to 2.9 BLEU
point improvement over the best input sys-
tem and up to 1.3 BLEU point improve-
ment over a state-of-the-art combination
method on two different language pairs.
1 Introduction
Combining outputs of multiple systems perform-
ing the same task has been widely explored in
various fields such as speech recognition, word
sense disambiguation, and word alignments, and it
had been shown that the combination approaches
yielded significantly better outputs than the in-
dividual systems. System combination has also
been explored in the MT field, especially with
the emergence of various structurally different MT
systems. Various techniques include hypothesis
selection from different systems using sentence-
level scores, re-decoding source sentences using
phrases that are used by individual systems (Rosti
et al, 2007a; Huang and Papineni, 2007) and
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
word-based combination techniques using confu-
sion networks (Matusov et al, 2006; Sim et al,
2007; Rosti et al, 2007b). Among these, confu-
sion network decoding of the system outputs has
been shown to be more effective than the others in
terms of the overall translation quality.
One of the crucial steps in confusion network
decoding is the alignment of hypotheses to each
other because the same meaning can be expressed
with synonymous words and/or with a different
word ordering in different hypotheses. Unfortu-
nately, all the alignment algorithms used in confu-
sion network decoding are insensitive to synonyms
of words when aligning two hypotheses to each
other. This paper extends the previous alignment
approaches to handle word synonyms more effec-
tively to improve alignment of different hypothe-
ses. We also present a two-pass alignment strategy
for a better alignment of hypotheses with similar
words but with a different word ordering.
We evaluate our system combination approach
using variants of an in-house hierarchical MT sys-
tem as input systems on two different language
pairs: Arabic-English and Chinese-English. Even
with very similar MT systems as inputs, we show
that the improved alignments yield up to an abso-
lute 2.9 BLEU point improvement over the best
input system and up to an absolute 1.3 BLEU
point improvement over the old alignments in a
confusion-network-based combination.
The rest of this paper is organized as follows.
Section 2 presents an overview of previous sys-
tem combination techniques for MT. Section 3 dis-
cusses the confusion-network-based system com-
bination. In Section 4, we present the new hy-
pothesis alignment techniques. Finally, Section 5
presents our experiments and results on two lan-
guage pairs.
33
2 Related Work
System combination for machine translation can
be done at three levels: Sentence-level, phrase-
level or word-level.
Sentence-level combination is done by choosing
one hypothesis amongmultipleMT system outputs
(and possibly among n-best lists). The selection
criterion can be a combination of translation model
and language model scores with multiple compar-
ison tests (Akiba et al, 2002), or statistical confi-
dence models (Nomoto, 2004).
Phrase-level combination systems assume that
the input systems provide some internal informa-
tion about the system, such as phrases used by the
system, and the task is to re-decode the source sen-
tence using this additional information. The first
example of this approach was the multi-engine MT
system (Frederking and Nirenburg, 1994), which
builds a chart using the translation units inside
each input system and then uses a chart walk algo-
rithm to find the best cover of the source sentence.
Rosti et al (2007a) collect source-to-target corre-
spondences from the input systems, create a new
translation option table using only these phrases,
and re-decode the source sentence to generate bet-
ter translations. In a similar work, it has been
demonstrated that pruning the original phrase ta-
ble according to reliable MT hypotheses and en-
forcing the decoder to obey the word orderings in
the original system outputs improves the perfor-
mance of the phrase-based combination systems
(Huang and Papineni, 2007). In the absence of
source-to-target phrase alignments, the sentences
can be split into simple chunks using a recursive
decomposition as input to MT systems (Mellebeek
et al, 2006). With this approach, the final output
is a combination of the best chunk translations that
are selected by majority voting, system confidence
scores and language model scores.
The word-level combination chooses the best
translation units from different translations and
combine them. The most popular method for
word-based combination follows the idea behind
the ROVER approach for combining speech recog-
nition outputs (Fiscus, 1997). After reordering
hypotheses and aligning to each other, the com-
bination system builds a confusion network and
chooses the path with the highest score. The fol-
lowing section describes confusion-network-based
system combination in detail.
? 2005 SRI International
Confusion Network ExampleHypothesis 1: she went homeHypothesis 2: she was at schoolHypothesis 3: at home was sheshe school#eps# homeatwentwasshe homewas school#eps#atwent she#eps#was#eps# wentat#eps# home #eps#school<s><s><s> </s></s></s>
Figure 1: Alignment of three hypotheses to each
other using different hypotheses as skeletons.
3 System Combination with Confusion
Networks
The general architecture of a confusion-network-
based system combination is as follows:
1. Extract n-best lists from MT systems.
2. Pick a skeleton translation for each segment.
3. Reorder all the other hypotheses by aligning
them to the skeleton translation.
4. Build a confusion network from the re-
ordered translations for each segment.
5. Decode the confusion network using vari-
ous arc features and sentence-level scores
such as LM score and word penalty.
6. Optimize feature weights on a held-out test
set and re-decode.
In this framework, the success of confusion net-
work decoding for system combination depends on
two important choices: Selection of the skeleton
hypothesis and alignment of other hypotheses to
the skeleton.
For selecting the best skeleton, two common
methods are choosing the hypothesis with the Min-
imum Bayes Risk with translation error rate (TER)
(Snover et al, 2006) (i.e., the hypothesis with the
minimum TER score when it is used as the ref-
erence against the other hypotheses) (Sim et al,
2007) or choosing the best hypotheses from each
system and using each of those as a skeleton in
multiple confusion networks (Rosti et al, 2007b).
In this paper, we use the latter since it performs
slightly better than the first method in our exper-
iments. An example confusion network on three
translations is presented in Figure 1.
1
The major difficulty when using confusion net-
works for system combination for MT is aligning
different hypotheses to the skeleton since the word
1
In this paper, we use multiple confusion networks that are
attached to the same start and end node. Throughout the rest
of the paper, the term confusion network refers to one network
among multiple networks used for system combination.
34
order might be different in different hypotheses
and it is hard to align words that are shifted from
one hypothesis to another. Four popular methods
to align hypotheses to each other are as follows:
1. Multiple string-matching algorithm based
on Levenshtein edit distance (Bangalore et
al., 2001)
2. A heuristic-based matching algorithm (Ja-
yaraman and Lavie, 2005)
3. Using GIZA++ (Och and Ney, 2000) with
possibly additional training data (Matusov
et al, 2006)
4. Using TER (Snover et al, 2006) between
the skeleton and a given hypothesis (Sim et
al., 2007; Rosti et al, 2007b)
None of these methods takes word synonyms
into account during alignment of hypotheses.
2
In
this work, we extend the TER-based alignment
to use word stems and synonyms using the pub-
licly available WordNet resource (Fellbaum, 1998)
when aligning hypotheses to each other and show
that this additional information improves the align-
ment and the overall translation significantly.
4 Confusion Networks with Word
Synonyms and Two-pass Alignment
When building a confusion network, the goal is to
put the same words on the same arcs as much as
possible. Matching similar words between two hy-
potheses is necessary to achieve this goal.
When we align two different hypotheses using
TER, it is necessary that two words have the iden-
tical spelling to be considered a match. However,
in natural languages, it is possible to represent the
same meaning using synonyms of words in pos-
sibly different positions. For example, in the fol-
lowing sentences, ?at the same time? and ?in the
meantime?, ?waiting for? and ?expect?, and ?set?
and ?established? correspond to each other, re-
spectively:
Skeleton: at the same time expect israel
to abide by the deadlines set by .
Hypothesis: in the meantime , we are
waiting for israel to abide by the
established deadlines .
Using TER, synonymous words might be
aligned to each other if they appear in the same po-
2
Note that the approach by Matusov et al (2006) at-
tempts to align synonyms and different morphological forms
of words to each other but this is done implicitly, relying on
the parallel text to learn word alignments.
sition in two hypotheses but this is less likely when
two words appear in different positions. With-
out knowing that two words are synonyms of each
other, they are considered two separate words dur-
ing TER alignment.
Our goal is to create equivalence classes for
each word in the given translations and modify the
alignment algorithm to give priority to the match-
ing of words that are in the same equivalence class.
In this paper, the equivalence classes are generated
using WordNet by extracting synonyms of each
word in the translations.
To incorporate matching of word synonyms into
the alignment, we followed three steps:
1. Use WordNet to extract synonyms of the
words that appear in all hypotheses.
2. Augment each skeleton word with all syn-
onymous words that appear in all the hy-
potheses.
3. Modify TER script to handle words with
alternatives using an additional synonym
matching operation.
In the following subsections, we describe how
each of these tasks is performed.
4.1 Extracting Synonyms from WordNet
The first step is to use WordNet to extract syn-
onyms of each word that appears in all hypotheses.
This is simply done using the publicly available
WordNet processing tools to extract all synonyms
of the given word. To allow matching words that
have the same stem or variations of the same word
with different part-of-the-speech (POS) tags, we
extract all synonyms of the given word regardless
of their POS tag in the given translation.
3
In the example above, it is clear that the verbs
wait and expect have the same meaning but TER
is unable to align these two words to each other
because of different word positions. Using Word-
Net, we get the following two entries for the words
waiting and expect, indicating that two words can
be considered as synonyms of each other:
expect: Sense 1: expect, anticipate ? evalu-
ate, pass judgment, judge. Sense 2: ask, require,
expect ? demand. Sense 3: expect, look, await,
wait. Sense 4: expect ? see, consider, reckon,
3
Our goal is to add as many synonyms as possible to in-
crease the chances of a word aligning to one of its possible
synonyms rather than to any other word. Therefore, we do
not distinguish between the synonyms of the same word ac-
cording to their confidence value or their POS tag.
35
ONE-PASS ALIGNMENT
khaddam receives italian house speaker
khaddam receives the head of the italian chamber of deputies
khaddam will meet with the president of the italian chamber of deputies
TWO-PASS ALIGNMENT
khaddam receives italian house speaker
khaddam receives of the head italian chamber of the deputies
khaddam will meet with president of the italian chamber of the deputies
Figure 2: Example alignment of three hypotheses with one-pass and two-pass alignment
view, regard. Sense 5: expect ? expect, look,
await, wait. Sense 6: have a bun in the oven, bear,
carry, gestate, expect ? give birth, deliver, bear,
birth, have.
waiting: Sense 1: wait. Sense 2: wait, hold
off, hold back ? act, move. Sense 3: wait, expect,
look, await. Sense 4: wait, waitress ? work.
We should note that WordNet contains only
open-class words, i.e., nouns, verbs, adjectives,
and adverbs. There are no entries for determin-
ers, prepositions, pronouns, conjunctions, and par-
ticles. For better matching of these additional POS
tags, we manually created a different equivalence
class for each POS tag that is not included in the
WordNet so that words with the same POS tag can
be considered synonymous.
4.2 Augmenting References
After extracting the synonyms of each word in the
given translations, the next step is to augment each
reference word with its synonyms. To avoid over-
generation of synonyms, we make the assumption
that words w
i
and w
j
are synonyms of each other
only if w
i
appears in the synonym list of w
j
, and
w
j
appears in the synonym list of w
i
. To make
the alignment task more efficient and faster, we re-
strict the synonym list to only words that appear
in the given translations. In our running exam-
ple, the augmented (extended) skeleton according
to the second hypothesis is as follows:
Extended skeleton: at the same time meantime
expect waiting israel to abide by the
deadlines set established by .
4.3 Modifications to TER Script
The final step is to modify TER script to favor
matching of a word to its synonyms rather than to
any other word. To achieve this goal, we modi-
fied the publicly available TER script, TERCOM
(Snover et al, 2006), to match words in the same
equivalence class at an additional synonym cost.
In its original implementation, TERCOM builds a
hash table for the n-grams that appear in both the
reference and the hypothesis translation to deter-
mine possible shifts of words. To allow synony-
mous words to be shifted and aligned to each other,
we extend the hash table for all possible synonyms
of words in the skeleton. Formally, if the skeleton
includes two consecutive words w
i
s
i
and w
j
s
j
,
where s
i
(s
j
) is a synonym of w
i
(w
j
), we put
all four possible combinations to the hash table:
w
i
w
j
, w
i
s
j
, s
i
w
j
, and s
i
s
j
.
4
To give higher priority to the exact matching
of words (which has zero cost during edit dis-
tance computation), we used a slightly higher cost
for synonym matching, a cost of 0.1.
5
All the
other operations (i.e., insertion, deletion, substitu-
tion and shifting of words) have a cost of 1.0.
4.4 Two-pass Alignment Strategy
When building a confusion network, the usual
strategy is first to align each hypothesis to the
skeleton separately and reorder them so that the
word ordering in the given hypothesis matches the
word ordering in the skeleton translation. Next a
confusion network is built between all these re-
ordered hypotheses.
One of the major problems with this process oc-
curs when the hypotheses include additional words
that do not appear in the skeleton translation, as
depicted in Figure 2. Since the alignments of two
different hypotheses are done independently, two
hypotheses other than the skeleton may not align
perfectly, especially when the additional words ap-
pear in different positions.
To overcome this issue, we employ a two-pass
alignment strategy. In the first pass, we align all
hypotheses to the skeleton independently and build
a confusion network. Next an intermediate refer-
ence sentence is created from the confusion net-
work generated in the first pass. To create this in-
termediate reference, we find the best position for
each word that appears in the confusion network
4
Note that the hash table is built in an iterative fashion.
We consider adding a new n-gram only if the previous n? 1
words appear in the hypothesis as well.
5
Synonym matching cost was determined empirically, try-
ing different costs from 0 to 0.5.
36
WITHOUT SYNONYM MATCHING and ONE-PASS ALIGNMENT:
at the same time expect israel to abide by
at the same time we expect israel to abide by
at the same time , we are waiting for israel to abide by
at the same time we expect israel to abide by
at the same time , we expect israel to abide by
at the same time , waiting for israel to comply with
in the meantime , waiting for israel to abide by
WITH SYNONYM MATCHING and TWO-PASS ALIGNMENT:
at the same time expect israel to abide by
at the same time we expect israel to abide by
at the same time , we are waiting for israel to abide by
at the same time we expect israel to abide by
at the same time , we expect israel to abide by
at the same time , waiting for israel to comply with
in the meantime , waiting for israel to abide by
Figure 3: Example alignment via confusion networks with and without synonym matching and two-pass
alignment (using the first sentence as the skeleton)
using majority voting. The second pass uses this
intermediate reference as the skeleton translation
to generate the final confusion network.
When we create the intermediate reference, the
number of positions for a given word is bounded
by the maximum number of occurrences of the
same word in any hypothesis. It is possible that
two different words are mapped to the same po-
sition in the intermediate reference. If this is the
case, these words are treated as synonyms when
building the second confusion network, and the in-
termediate reference looks like the extended refer-
ence in Section 4.2.
Finally, Figure 3 presents our running example
with the old alignments versus the alignments with
synonym matching and two-pass alignment.
4.5 Features
Each word in the confusion network is represented
by system-specific word scores. For computing
scores, each hypothesis is assigned a score based
on three different methods:
1. Uniform weighting: Each hypothesis in the
n-best list has the same score of 1/n.
2. Rank-based weighting: Each hypothesis is
assigned a score of 1/(1+r), where r is the
rank of the hypothesis.
3. TM-based weighting: Each hypothesis is
weighted by the score that is assigned to the
hypothesis by the translation model.
The total score of an arc with word w for a given
system S is the sum of all the scores of the hy-
potheses in system S that contain the word w in
the given position. The score for a specific arc be-
tween nodes n
i
and n
j
is normalized by the sum of
the scores for all the arcs between n
i
and n
j
.
Our experiments demonstrated that rank-based
weighting performs the best among all three
weighting methods although the differences are
small. In the rest of the paper, we report only the
results with rank-based weighting.
Besides the arc scores, we employ the following
features during decoding:
1. Skeleton selection features for each system,
2. NULL-word (or epsilon) insertion score,
3. Word penalty, and
4. Language model score.
Skeleton selection feature is intended to help
choose the best skeleton among the input systems.
NULL-word feature controls the number of ep-
silon arcs used in the chosen translation during
the decoding and word penalty feature controls
the length of the translation. For language model
scores, we used a 4-gram LM that we used to train
the input systems.
5 Evaluation and Results
In this section, we describe how we train the input
systems and how we evaluate the proposed system
combination method.
5.1 Systems and Data
To evaluate the impact of the new alignments,
we tested our system combination approach using
the old alignments and improved alignments on
two language pairs: Arabic-English and Chinese-
English. We ran the system combination on three
system outputs that were generated by an in-house
hierarchical phrase-based decoder, as in (Chiang,
2007). The major difference between the three sys-
tems is that they were trained on different subsets
of the available training data using different word
alignments.
For generating the system outputs, first a hier-
archical phrase-based decoder was used to gener-
37
Data for Training/Tuning/Testing
Arabic-English Chinese-English
# of segments # of tokens # of segments # of tokens
Training Data (System1) 14.8M 170M 9.1M 207M
Training Data (System2) 618K 8.1M 13.4M 199M
Training Data (System3) 2.4M 27.5M 13.9M 208M
Tuning Set (Input Systems) 1800 51K 1800 51K
Tuning Set (System Combination) 1259 37K 1785 55K
Test Set - NIST MTEval?05 1056 32K 1082 32K
Test Set - NIST MTEval?06 1797 45K 1664 41K
Test Set - NIST MTEval?08 1360 43K 1357 34K
Table 1: Number of segments and source-side words in the training and test data.
ate three sets of unique 3000-best lists. Nine fea-
tures were used in the hierarchical phrase-based
systems under the log-linear model framework: a
4-gram language model (LM) score (trained on
nearly 3.6 billion words using the SRILM toolkit
(Stolcke, 2002)), conditional rule/phrase probabil-
ities and lexical weights (in both directions), rule
penalty, phrase penalty, and word penalty. Rules
and phrases were extracted in a similar manner
as described in (Chiang, 2007) from the training
data with word alignments generated by GIZA++.
The n-best lists were then re-scored with three ad-
ditional LMs: a count-based LM built from the
Google Tera word corpus, an almost parsing LM
based on super-ARV tagging, and an approximated
full-parser LM (Wang et al, 2007).
For Arabic-English, the first system was trained
on all available training data (see Table 1 for de-
tails), with long sentences segmented into multiple
segments based on IBM model 1 probabilities (Xu
et al, 2005). The second system was trained on a
small subset of the training data, which is mostly
newswire. The third system was trained on an au-
tomatically extracted subset of the training data ac-
cording to n-gram overlap in the test sets.
For Chinese-English, the first system used all
the training data without any sentence segmenta-
tion. The second system used all training data af-
ter IBM-1 based sentence segmentation, with dif-
ferent weightings on different corpora. The third
system is the same as the second system except
that it used different word alignment symmetriza-
tion heuristics (grow-diag-final-and vs. grow-diag-
final (Koehn et al, 2003)).
5.2 Empirical Results
All input systems were optimized on a ran-
domly selected subset of the NIST MTEval?02,
MTEval?03, and MTEval?04 test sets using min-
System MT?05 MT?06 MT?08
System 1 53.4 43.8 43.2
System 2 53.9 46.0 42.8
System 3 56.1 45.3 43.3
No Syns, 1-pass 56.7 47.5 44.9
w/Syns, 2-pass 57.9 48.4 46.2
Table 2: Lowercase BLEU scores (in percentages)
on Arabic NIST MTEval test sets.
imum error rate training (MERT) (Och, 2003)
to maximize BLEU score (Papineni et al, 2002).
System combination was optimized on the rest of
this data using MERT to maximize BLEU score.
As inputs to the system combination, we used 10-
best hypotheses from each of the re-ranked n-best
lists. To optimize system combination, we gener-
ated unique 1000-best lists from a lattice we cre-
ated from the input hypotheses, and used MERT in
a similar way to MT system optimization.
We evaluated system combination with im-
proved alignments on three different NIST
MTEval test sets (MTEval?05, MTEval?06 NIST
portion, and MTEval?08). The final MT outputs
were evaluated using lowercased BLEU scores.
6
Tables 2 and 3 present the BLEU scores (in per-
centages) for the input systems and for different
combination strategies on three test sets in Arabic-
English and Chinese-English, respectively.
On Arabic-English, the combination with syn-
onym matching and two-pass alignment yields ab-
solute improvements of 1.8 to 2.9 BLEU point on
three test sets over the best input system. When
compared to the combination algorithm with the
old alignments (i.e., 1-pass alignment with no syn-
onymmatching), the improved alignments yield an
additional improvement of 0.9 to 1.3 BLEU point
6
We used the NIST script (version 11b) for BLEU with its
default settings: case-insensitive matching of up to 4-grams,
and the shortest reference sentence for the brevity penalty.
38
System MT?05 MT?06 MT?08
System 1 35.8 34.3 27.6
System 2 35.9 34.2 27.8
System 3 36.0 34.3 27.8
No Syns, 1-pass 38.1 36.5 27.9
w/Syns, 2-pass 38.6 37.0 28.3
No Syns, 1-pass, tuning set w/webtext 28.4
w/Syns, 2-pass, tuning set w/webtext 29.3
Table 3: Lowercase BLEU scores (in percentages)
on Chinese NIST MTEval test sets.
on the three test sets.
For Chinese-English, the improvements over the
previous combination algorithm are smaller. The
new combination system yields up to an absolute
2.6 BLEU point improvement over the best input
system and up to 0.5 BLEU point improvement
over the previous combination algorithm on three
different test sets. Note that for Arabic-English,
the individual systems show a high variance in
translation quality when compared to Chinese-
English systems. This might explain why the im-
provements on Chinese-English are modest when
compared to Arabic-English results.
We also noticed that system combination
yielded much smaller improvement on Chinese
MTEval?08 data when compared to other test
sets, regardless of the alignment method (only 0.5
BLEU point over the best input system). We
suspected that this might happen because of a
mismatch between the genres of the test set and
the tuning set (the amount of web text data in
MTEval?08 test set is high although the tuning set
does not include any web text data). To test this
hypothesis, we created a new tuning set for system
combination, which consists of 2000 randomly se-
lected sentences from the previous MTEval test
sets and includes web text data. Using this new
tuning set, combination with the improved align-
ments yields a BLEU score of 29.3 on MTEval?08
data (an absolute improvement of 1.5 BLEU point
over the best input system, and 0.9 BLEU point
improvement over the combination with the old
alignments). These new results again validate the
usefulness of the improved alignments when the
tuning set matches the genre of the test set.
5.3 A Comparison of the Impact of Synonym
Matching and Two-pass Alignment
One last evaluation investigated the impact of each
component on the overall improvement. For this
Synon. 2-pass MT?05 MT?06 MT?08
No No 56.7 47.5 44.9
Yes No 57.3 47.8 45.2
No Yes 57.7 48.0 45.9
Yes Yes 57.9 48.4 46.2
Table 4: Comparison of Synonym Matching and
Two-pass Alignment on Arabic-English
purpose, we ran system combination by turning on
and off each component. Table 4 presents the sys-
tem combination results in terms of BLEU scores
on Arabic-English test sets when each component
is used on its own or when they are used together.
The results indicate that synonym matching on
its own yields improvements of 0.3-0.6 BLEU
points over not using synonym matching. Two-
pass alignment turns out to be more useful than
synonym matching, yielding an absolute improve-
ment of up to 1 BLEU point over one-pass align-
ment.
6 Conclusions
We presented an extension to the previous align-
ment approaches to handle word synonyms more
effectively in an attempt to improve the align-
ments between different hypotheses during confu-
sion network decoding. We also presented a two-
pass alignment strategy for a better alignment of
hypotheses with similar words but with a different
word ordering.
We evaluated our system combination ap-
proach on two language pairs: Arabic-English
and Chinese-English. Combination with improved
alignments yielded up to an absolute 2.9 BLEU
point improvement over the best input system and
up to an absolute 1.3 BLEU point improvement
over combination with the old alignments. It is
worth noting that these improvements are obtained
using very similar input systems. We expect that
the improvements will be higher when we use
structurally different MT systems as inputs to the
combiner.
Our future work includes a more effective use
of existing linguistic resources to handle alignment
of one word to multiple words (e.g., al-nahayan
vs. al nahyan, and threaten vs. pose threat)
and matching of similar (but not necessarily syn-
onymous) words (polls vs. elections). We are
also planning to extend word lattices to include
phrases from the individual systems (i.e., not just
the words) for more grammatical outputs.
39
Acknowledgments This material is based upon work
supported by the Defense Advanced Research Projects
Agency (DARPA) under Contract No. HR0011-06-C-0023.
References
Akiba, Yasuhiro, Taro Watanabe, and Eiichiro Sumita.
2002. Using language and translation models to se-
lect the best among outputs from multiple MT sys-
tems. In Proc. of the 19th Intl. Conf. on Computa-
tional Linguistics (COLING?2002).
Bangalore, Srinivas, German Bordel, and Giuseppe
Riccardi. 2001. Computing consensus translation
from multiple machine translation systems. In Proc.
of IEEE Automatic Speech Recognition and Under-
standing Workshop (ASRU?2001).
Chiang, David. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Fellbaum, Christiane. 1998. WordNet: An Electronic
Lexical Database. Bradford Books, March. Avail-
able at http://wordnet.princeton.edu.
Fiscus, Jonathan G. 1997. A post-processing system
to yield reduced word error rates: Recognizer output
voting error reduction (ROVER). In Proc. of IEEE
Automatic Speech Recognition and Understanding
Workshop (ASRU?1997).
Frederking, Robert and Sergei Nirenburg. 1994.
Three heads are better than one. In Proc. of the
4th Conf. on Applied Natural Language Processing
(ANLP?1994).
Huang, Fei and Kishore Papineni. 2007. Hierarchi-
cal system combination for machine translation. In
Proc. of the Conf. on Empirical Methods in Natural
Language Processing (EMNLP?2007).
Jayaraman, Shyamsundar and Alon Lavie. 2005.
Multi-engine machine translation guided by explicit
word matching. In Proc. of the 10th Annual Conf. of
the European Association for Machine Translation
(EAMT?2005).
Koehn, Philipp, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proc. of the
Human Language Technology and the Meeting of the
North American Chapter of the Association for Com-
putational Linguistics Conf. (HLT/NAACL?2003).
Matusov, Evgeny, Nicola Ueffing, and Hermann Ney.
2006. Computing consensus translation from multi-
ple machine translation systems using enhanced hy-
potheses alignment. In Proc. of the 11th Conf. of the
European Chapter of the Association for Computa-
tional Linguistics (EACL?2006).
Mellebeek, Bart, Karolina Owczarzak, Josef Van Gen-
abith, and Andy Way. 2006. Multi-engine machine
translation by recursive sentence decomposition. In
Proc. of the 7th Conf. of the Association for Machine
Translation in the Americas (AMTA?2006).
Nomoto, Tadashi. 2004. Multi-engine machine trans-
lation with voted language model. In Proc. of the
42nd Annual Meeting of the Association for Compu-
tational Linguistics (ACL?04).
Och, Franz J. and Hermann Ney. 2000. Improved sta-
tistical alignment models. In Proc. of the 38th An-
nual Meeting of the Association for Computational
Linguistics (ACL?2000).
Och, Franz J. 2003. Minimum error rate training in sta-
tistical machine translation. In Proc. of the 41st An-
nual Meeting of the Association for Computational
Linguistics (ACL?2003).
Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proc. of the
40th Annual Meeting of the Association for Compu-
tational Linguistics (ACL?2002).
Rosti, Antti-Veikko, Necip Fazil Ayan, Bing Xiang,
Spyros Matsoukas, Richard Schwartz, and Bonnie
Dorr. 2007a. Combining outputs from multiple ma-
chine translation systems. In Proc. of the Human
Language Technology and the Meeting of the North
American Chapter of the Association for Computa-
tional Linguistics Conf. (HLT/NAACL?2007).
Rosti, Antti-Veikko, Spyros Matsoukas, and Richard
Schwartz. 2007b. Improved word-level system com-
bination for machine translation. In Proc. of the
45th Annual Meeting of the Association for Compu-
tational Linguistics (ACL?2007).
Sim, Khe Chai, William J. Byrne, Mark J.F. Gales,
Hichem Sahbi, and Phil C. Woodland. 2007.
Consensus network decoding for statistical machine
translation system combination. In Proc. of the 32nd
Intl. Conf. on Acoustics, Speech, and Signal Process-
ing (ICASSP?2007).
Snover, Matthew, Bonnie Dorr, Rich Schwartz, Linnea
Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proc. of the 7th Conf. of the Association for Ma-
chine Translation in the Americas (AMTA?2006).
Stolcke, Andreas. 2002. SRILM ? an extensible lan-
guage modeling toolkit. In Proc. of the Intl. Conf. on
Spoken Language Processing (ICSLP?2002).
Wang, Wen, Andreas Stolcke, and Jing Zheng. 2007.
Reranking machine translation hypotheses with
structured and web-based language models. In Proc.
of the IEEE Automatic Speech Recognition and Un-
derstanding Workshop (ASRU?2007).
Xu, Jia, Richard Zens, and Hermann Ney. 2005.
Sentence segmentation using IBM word alignment
model 1. In Proc. of the 10th Annual Conf. of
the European Association for Machine Translation
(EAMT?2005).
40
Limited-Domain Speech-to-Speech Translation
between English and Pashto
Kristin Precoda, Horacio Franco, Ascander Dost, Michael Frandsen, John Fry,
Andreas Kathol, Colleen Richey, Susanne Riehemann, Dimitra Vergyri, Jing Zheng
SRI International
333 Ravenswood Ave.
Menlo Park, CA 94025
Christopher Culy
FX Palo Alto Laboratory, Inc.
3400 Hillview Ave., Building 4
Palo Alto, CA 94304
Abstract
This paper describes a prototype system for
near-real-time spontaneous, bidirectional
translation between spoken English and
Pashto, a language presenting many
technological challenges because of its lack of
resources, including both data and expert
knowledge. Development of the prototype is
ongoing, and we propose to demonstrate a
fully functional version which shows the basic
capabilities, though not yet their final depth
and breadth.
1 Introduction
This demonstration will present a prototype system for
bidirectional speech-to-speech translation within a
limited semantic domain, that of first encounters
between a patient and a medical professional. A major
goal of the work is to explore techniques that are
appropriate for languages that are not of great
commercial interest and that consequently are lacking in
data and resources of many kinds. The system has been
developed for American English and Pashto, one of the
major languages of Afghanistan, which presents a
variety of challenges for both data-intensive and
knowledge-based approaches.
It should be noted that the system must be viewed as
one component in a real-time transaction between two
cooperating humans. The ultimate goal of those humans
is to exchange information by whatever means is
effective: not, necessarily, to rely exclusively on the
system?s output, but to use it in combination with other,
non-speech modalities of conveying meaning and with
ordinary world knowledge.
The final system is intended to run on a handheld
device, such as a PDA, with its attendant memory and
speed limitations and restriction to integer-only
computation. Most components of the demonstration
system run in a PocketPC emulation environment on a
Windows laptop, with a few in the full Windows
environment. All aspects of the prototype system are
undergoing active development.
2 Overall Architecture
A simple description of the architecture is as follows.
The system is controlled by the English speaker, who is
expected to have greater technological familiarity and
who has the benefit of visual feedback on the system
performance. Spoken input, in either English or Pashto,
is recognized by SRI?s small-footprint DynaSpeak?
recognizer, and an ordered list of hypotheses is
produced. The most likely hypothesis is input to SRI's
Gemini natural language parser/generator (Dowding et
al. 1993), which attempts to parse the speech
recognition output. Handling of possible errors or
failures will be discussed in Section 3.
When a successful parse is obtained, Gemini creates
a quasi-logical form representing the meaning of the
sentence. In general, multiple quasi-logical forms may
be created, reflecting differing interpretations of the
input sentence. These forms, which are domain
independent and serve here as an interlingua, can be
ordered by heuristically assigning preferences or
dispreferences to the parsing rules applied to create
them. Gemini uses a grammar for the target language to
generate a translation from the most-preferred
interpretation possible, and outputs a textual
representation of the translation.
Theta, a small-footprint concatenative synthesizer
from Cepstral LLC (Cepstral LLC 2004), then produces
synthetic spoken output in the target language from the
textual representation generated by Gemini. The Pashto
voice was created specifically for this project.
An English and a Pashto version of each component
are called by a single application which includes a
graphical user interface. A screen shot of the interface
is shown in Figure 1.
Figure 1. Screen shot of prototype system
interface.
3 Redundancy and Handling Infelicities
As in any complex system, performance can differ from
the ideal in a number of ways, and it is important for the
system to provide alternative ways to support successful
communication. Some kinds of subideal performance
and recovery approaches are described here.
The most likely hypothesis output by the speech
recognizer may not be exactly what was spoken. When
the input speech is English, the English speaker can see
whether the most likely hypothesis (shown in the
"Input" box in Figure 1) is correct or approximately
correct. If the English speaker judges the hypothesis to
not be close enough to the intended text, s/he may either
repeat the utterance or click on the "Guesses?" button
to see an ordered list of the best hypotheses. A sample
list is shown in Figure 2. If the correct text is on this list,
the user may select it to submit for translation. When
the input speech is in Pashto, this functionality is less
useful, as the Pashto speaker is not assumed to be able
to read, even if the hypotheses were displayed in Pashto
script.
Figure 2. Sample ordered list of recognizer
hypotheses.
Once a recognition hypothesis has been submitted
for translation, several possible problems can arise.
Pashto is a moderately inflected, split-ergative Indo-
European language, and for Pashto in particular,
recognition errors may lead to apparent lack of syntactic
agreement between elements of the sentence which
should (and did in fact) agree. As Gemini tries to
generate a full parse of the input, it has the option of
using parse rules that relax agreement requirements.
These rules are dispreferred and a parse built upon them
may be kept only if a full, grammatically correct parse
cannot be completed. Another possible problem is that
unknown words, some grammatical constructions, and
input errors may render any full parse unachievable. In
this case, fallback strategies can be applied to translate
partial parses, fragments, or any known words. Other
strategies are currently in development.
Another class of approaches for assisting
communication allows the English speaker to quickly
perform certain actions or play high-frequency phrases
to the Pashto speaker. If there is background noise or
distractions or the TTS quality is not high enough for
easy comprehension, pressing the "Replay" button will
immediately play back the last translation result. "Ask
for Rep" plays a prerecorded sentence asking the Pashto
speaker to repeat what s/he said. Several other useful
phrases are available by clicking on the "Phrases?"
button and then selecting from the screen shown in
Figure 3.
Figure 3. Prerecorded Pashto phrases which can
be played back with a single click.
4 Sample Interaction
The table below shows an excerpt of a dialog between
an English and a Pashto speaker, both new to this
system, whose interaction was part of an informal trial
run by the MITRE Corporation in February 2004 for the
DARPA CAST program. The English speaker, a doctor,
had just received eighteen minutes of training in how to
use the system and had had no other exposure to it. The
Pashto speaker, playing the role of an injured patient,
had received training in complaints consistent with a
particular injury scenario and had seen others use the
system, but had not interacted with the system himself.
Except as noted, the translations are understandable.
Spoken input System result
I am a doctor, can I help
you?
[failure to translate
because two sentences]
I am a doctor z@ ddAkttar y@m
Can I help you? AyA z@ d@ tA sara
komak kaw@lay S@m
ho, mehrabAni w@krra yes make [partial
translation; full translation
should have been "yes
please do"]
Where are you hurt? [rerecorded after one
misrecognition] t@ cherta
khugx ye
ghwagx aw ugxa [misrecognized
repeatedly; unable to give
meaningful translation;
should have been "ear and
shoulder"]
Can you breathe? AyA t@ sA akhIst@lay
Se
na, mUSkel lar@m no I have the problem
Do you have pain? AyA t@ dard lare
zAyt of much [minor
misrecognition; translation
should have been "much"]
Do you take medications? AyA t@ dawAuna akhle
[incorrect plural form of
dawA, but understood by
Pashto speaker anyway]
na no
Do you want pain
medication?
AyA t@ d@ dard dawA
ghwArre
ho yes
Do you have allergies? AyA t@ hasAsiyatuna lare
ho yes
What are your allergies? stA hasAsiyat ts@ day
antibyutik [misrecognized but
correctly understood by
doctor as "antibiotics"]
5 Challenges
Three main challenges face this project. First,
commercially nonviable languages, such as Pashto,
often offer very few linguistic resources (such as
linguistic descriptions, acoustic data, texts, language
processing tools). The lack of resources makes
development more difficult, and severely constrains the
approaches that are viable: approaches that rely on large
corpora cannot be used. In addition, there is a shortage
of literate speakers who are available to act as
consultants, and a scarcity of basic knowledge about the
language. This impedes progress and renders difficult
approaches that rely on a large body of hand-crafted
translation rules. The challenge of having no single
person who has a deep understanding of both the
language and the technology and who can serve as a
bridge between them is substantial, and causes more
iterative development than is ordinarily the case when
bilingual technologists are available, as newly
discovered phenomena or new understanding cause
revisions of previous work.
A second major challenge is due to the nature of the
domain and the underlying concept of operations. Real
speech occurs in noisy environments, has disfluencies,
and is highly variable (e.g., phrasings, dialect
differences). In addition, the output of a speech
recognizer contains more and qualitatively different
errors than typical text input to automatic translation
systems. While the problems of real speech are not
unique to this project, they are magnified by the fact
that the non-English speakers will largely be
unsophisticated users of technology, who will often be
using the system for the first and only time. The system
must work well from the very first utterance ? there
cannot be much of a learning curve. This applies to the
translation quality and to other aspects of the system,
such as the synthetic speech, as speakers are not familiar
with synthesized Pashto speech. These speakers are also
not expected to be literate, and their understanding will
not be bolstered by the extra redundancy and
capabilities that the display offers to the English
speaker.
A third major challenge is posed by the handheld
device platform with its computational and storage
limitations, and the near-real-time requirement of the
envisioned usage. The restriction to integer-only
computation is most serious for the speech recognition,
as nearly all medium- or large-vocabulary speech
recognizers perform extensive floating-point
computations, and the conversion of a speech recognizer
to use only integer computation required considerable
effort. The severe memory limitations perhaps impact
most the parsing/generation components of the system.
6 Summary
We have described a prototype of a spoken language
translation system for use by English-speaking medical
personnel treating Pashto-speaking patients.  The
system, which is targeted to a handheld device, is being
developed with extremely little Pashto-language data of
any kind.  It builds on medium-vocabulary speaker-
independent HMM-based speech recognition, rule-
based translation and supporting methods, and
concatenative synthesis. While the system is certainly
still under development, it has provided a reasonable
proof of concept in informal trials.
Acknowledgments
This work was supported under SPAWAR contract N66001-
99-D-8504 with funding from DARPA. Many thanks are due
to Mohammad Shahabuddin Khan, David Kale, Robert J.
Podesva, Valerie Wagner, and many Pashto speakers who
worked with us in various capacities.
References
Cepstral, LLC. 2004. Theta: Small footprint text-to-
speech synthesizer, Pittsburgh, PA, http://
www.cepstral.com
J. Dowding, J. M. Gawron, D. E. Appelt, J. Bear, L.
Cherny, R. Moore, and D. B. Moran. 1993. Gemini:
A natural language system for spoken language
understanding.  Proceedings of the Thirty-First
Annual Meeting of the Association for Computational
Linguistics, 54-61.
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 237?240,
Suntec, Singapore, 4 August 2009.
c?2009 ACL and AFNLP
Toward Smaller, Faster, and Better Hierarchical Phrase-based SMT
Mei Yang
Dept. of Electrical Engineering
University of Washington, Seattle, WA, USA
yangmei@u.washington.edu
Jing Zheng
SRI International
Menlo Park, CA, USA
zj@speech.sri.com
Abstract
We investigate the use of Fisher?s exact
significance test for pruning the transla-
tion table of a hierarchical phrase-based
statistical machine translation system. In
addition to the significance values com-
puted by Fisher?s exact test, we introduce
compositional properties to classify phrase
pairs of same significance values. We also
examine the impact of using significance
values as a feature in translation mod-
els. Experimental results show that 1% to
2% BLEU improvements can be achieved
along with substantial model size reduc-
tion in an Iraqi/English two-way transla-
tion task.
1 Introduction
Phrase-based translation (Koehn et al, 2003)
and hierarchical phrase-based translation (Chiang,
2005) are the state of the art in statistical ma-
chine translation (SMT) techniques. Both ap-
proaches typically employ very large translation
tables extracted from word-aligned parallel data,
with many entries in the tables never being used
in decoding. The redundancy of translation ta-
bles is not desirable in real-time applications,
e.g., speech-to-speech translation, where speed
and memory consumption are often critical con-
cerns. In addition, some translation pairs in a table
are generated from training data errors and word
alignment noise. Removing those pairs could lead
to improved translation quality.
(Johnson et al, 2007) has presented a tech-
nique for pruning the phrase table in a phrase-
based SMT system using Fisher?s exact test. They
compute the significance value of each phrase
pair and prune the table by deleting phrase pairs
with significance values smaller than a threshold.
Their experimental results show that the size of the
phrase table can be greatly reduced with no signif-
icant loss in translation quality.
In this paper, we extend the work in (Johnson
et al, 2007) to a hierarchical phrase-based transla-
tion model, which is built on synchronous context-
free grammars (SCFG). We call an SCFG rule a
phrase pair if its right-hand side does not contain a
nonterminal, and otherwise a rewrite rule. Our ap-
proach applies to both the phrase table and the rule
table. To address the problem that many transla-
tion pairs share the same significance value from
Fisher?s exact test, we propose a refined method
that combines significance values and composi-
tional properties of surface strings for pruning the
phrase table. We also examine the effect of using
the significance values as a feature in translation
models.
2 Fisher?s exact test for translation table
pruning
2.1 Significance values by Fisher?s exact test
We briefly review the approach for computing
the significance value of a translation pair using
Fisher?s exact test. In Fisher?s exact test, the sig-
nificance of the association of two items is mea-
sured by the probability of seeing the number of
co-occurrences of the two items being the same
as or higher than the one observed in the sam-
ple. This probability is referred to as the p-value.
Given a parallel corpus consisting of N sentence
pairs, the probability of seeing a pair of phrases
(or rules) (s?,
?
t) with the joint frequency C(s?,
?
t) is
given by the hypergeometric distribution
P
h
(C(s?,
?
t))
=
C(s?)!(N ? C(s?))!C(
?
t)!(N ? C(
?
t))!
N !C(s?,
?
t)!C(s?,?
?
t)!C(?s?,
?
t)!C(?s?,?
?
t)!
where C(s?) and C(
?
t) are the marginal frequencies
of s? and
?
t, respectively. C(s?,?
?
t) is the number
of sentence pairs that contain s? on the source side
237
but do not contain
?
t on the target side, and similar
for the definition of C(?s?,
?
t) and C(?s?,?
?
t). The
p-value is therefore the sum of the probabilities of
seeing the two phrases (or rules) occur as often
as or more often than C(s?,
?
t) but with the same
marginal frequencies
P
v
(C(s?,
?
t)) =
?
?
c=C(s?,
?
t)
P
h
(c)
In practice, p-values can be very small, and thus
negative logarithm p-values are often used instead
as the measure of significance. In the rest of this
paper, the negative logarithm p-value is referred to
as the significance value. Therefore, the larger the
value, the greater the significance.
2.2 Table pruning with significance values
The basic scheme to prune a translation table is
to delete all translation pairs that have significance
values smaller than a given threshold.
However, in practice, this pruning scheme does
not work well with phrase tables, as many phrase
pairs receive the same significance values. In par-
ticular, many phrase pairs in the phrase table have
joint and both marginal frequencies all equal to
1. Such phrase pairs are referred to as triple-1
pairs. It can be shown that the significance value
of triple-1 phrase pairs is log(N). Given a thresh-
old, triple-1 phrase pairs either all remain in the
phrase table or are discarded entirely.
To look closer at the problem, Figure 1 shows
two example tables with their percentages of
phrase pairs that have higher, equal, or lower sig-
nificance values than log(N). When the thresh-
old is smaller than log(N), as many as 35% of
the phrase pairs can be deleted. When the thresh-
old is greater than log(N), at least 90% of the
phrase pairs will be discarded. There is no thresh-
old that prunes the table in the range of 35% to
90%. One may think that it is right to delete all
triple-1 phrase pairs as they occur only once in
the parallel corpus. However, it has been shown
in (Moore, 2004) that when a large number of
singleton-singleton pairs, such as triple-1 phrase
pairs, are observed, most of them are not due to
chance. In other words, most triple-1 phrase pairs
are significant and it is likely that the translation
quality will decline if all of them are discarded.
Therefore, using significance values alone can-
not completely resolve the problem of phrase ta-
ble pruning. To further discriminate phrase pairs
80%90%100% 50%60%70%80%90%100%
>?log
(N)
30%40%50%60%70%80%90%100%
>?log
(N)
=?log
(N)
<?log
(N)
0%10%20%30%40%50%60%70%80%90%100%
>?log
(N)
=?log
(N)
<?log
(N)
0%10%20%30%40%50%60%70%80%90%100%
Table
1
Table
2
>?log
(N)
=?log
(N)
<?log
(N)
0%10%20%30%40%50%60%70%80%90%100%
Table
1
Table
2
>?log
(N)
=?log
(N)
<?log
(N)
Figure 1: Percentages of phrase pairs with higher,
equal, and lower significance values than log(N).
of the same significance values, particularly the
triple-1 phrase pairs, more information is needed.
The Fisher?s exact test does not consider the sur-
face string in phrase pairs. Intuitively, some phrase
pairs are less important if they can be constructed
by other phrase pairs in the decoding phase, while
other phrase pairs that involve complex syntac-
tic structures are usually difficult to construct and
thus become more important. This intuition in-
spires us to explore the compositional property of
a phrase pair as an additional factor. More for-
mally, we define the compositional property of a
phrase pair as the capability of decomposing into
subphrase pairs. If a phrase pair (s?,
?
t) can be de-
composed into K subphrase pairs (s?
k
,
?
t
k
) already
in the phrase table such that
s? = s?
1
s?
2
. . . s?
K
?
t =
?
t
1
?
t
2
. . .
?
t
K
then this phrase pair is compositional; otherwise
it is noncompositional. Our intuition suggests that
noncompositional phrase pairs are more important
as they cannot be generated by concatenating other
phrase pairs in order in the decoding phase. This
leads to a refined scheme for pruning the phrase ta-
ble, in which a phrase pair is discarded when it has
a significance value smaller than the threshold and
it is not a noncompositional triple-1 phrase pair.
The definition of the compositional property does
not allow re-ordering. If re-ordering is allowed,
all phrase pairs will be compositional as they can
always be decomposed into pairs of single words.
In the rule table, however, the percentage of
triple-1 pairs is much smaller, typically less than
10%. This is because rules are less sparse than
phrases in general, as they are extracted with a
shorter length limit, and have nonterminals that
match any span of words. Therefore, the basic
pruning scheme works well with rule tables.
238
3 Experiment
3.1 Hierarchical phrase-based SMT system
Our hierarchical phrase-based SMT system trans-
lates from Iraqi Arabic (IA) to English (EN) and
vice versa. The training corpus consists of 722K
aligned Iraqi and English sentence pairs and has
5.0M and 6.7M words on the Iraqi and English
sides, respectively. A held-out set with 18K Iraqi
and 19K English words is used for parameter tun-
ing and system comparison. The test set is the
TRANSTAC June08 offline evaluation data with
7.4K Iraqi and 10K English words, and the transla-
tion quality is evaluated by case-insensitive BLEU
with four references.
3.2 Results on translation table pruning
For each of the two translation directions IA-to-
EN and EN-to-IA, we pruned the translation ta-
bles as below, where ? represents the significance
value of triple-1 pairs and ? is a small positive
number. Phrase table PTABLE3 is obtained us-
ing the refined pruning scheme, and others are ob-
tained using the basic scheme. Figure 2 shows the
percentages of translation pairs in these tables.
? PTABLE0: phrase table of full size without
pruning.
? PTABLE1: pruned phrase table using the
threshold ? ? ? and thus all triple-1 phrase
pairs remain.
? PTABLE2: pruned phrase table using the
threshold ? + ? and thus all triple-1 phrase
pairs are discarded.
? PTABLE3: pruned phrase table using the
threshold ? + ? and the refined pruning
scheme. All but noncompositional triple-1
phrase pairs are discarded.
? RTABLE0: rule table of full size without
pruning.
? RTABLE1: pruned rule table using the thresh-
old ?+ ?.
Since a hierarchical phrase-based SMT system
requires a phrase table and a rule table at the same
time, performance of different combinations of
phrase and rule tables is evaluated. The baseline
system will be the one using the full-size tables of
PTABLE0 and RTABLE0. Tables 2 and 3 show the
BLEU scores for each combination in each direc-
tion, with the best score in bold.
708090100
PTAB
LE0
5060708090100
PTAB
LE0
PTAB
LE1
30405060708090100
PTAB
LE0
PTAB
LE1
PTAB
LE2
PTAB
LE3
RTAB
LE0
102030405060708090100
PTAB
LE0
PTAB
LE1
PTAB
LE2
PTAB
LE3
RTAB
LE0
RTAB
LE1
0102030405060708090100
IA?to
?EN
EN?to
?IA
PTAB
LE0
PTAB
LE1
PTAB
LE2
PTAB
LE3
RTAB
LE0
RTAB
LE1
0102030405060708090100
IA?to
?EN
EN?to
?IA
PTAB
LE0
PTAB
LE1
PTAB
LE2
PTAB
LE3
RTAB
LE0
RTAB
LE1
Figure 2: The percentages of translation pairs in
phrase and rule tables.
It can be seen that pruning leads to a substan-
tial reduction in the number of translation pairs.
As long phrases are more frequently pruned than
short phrases, the actual memory saving is even
more significant. It is surprising to see that using
pruned tables improves the BLEU scores in many
cases, probably because a smaller translation table
generalizes better on an unseen test set, and some
translation pairs created by erroneous training data
are dropped. Table 1 shows two examples of dis-
carded phrase pairs and their frequencies. Both of
them are incorrect due to human translation errors.
We note that using the pruned rule table
RTABLE1 is very effective and improved BLEU
in most cases except when used with PTABLE0 in
the direction EN-to-IA. Although using the pruned
phrase tables had mixed effect, PTABLE3, which
is obtained through the refined pruning scheme,
outperformed others in all cases. This confirms
the hypothesis that noncompositional phrase pairs
are important and thus suggests that the proposed
compositional property is a useful measure of
phrase pair quality. Overall, the best results are
achieved by using the combination of PTABLE3
and RTABLE1, which gave improvement of 1% to
2% BLEU over the baseline systems. Meanwhile,
this combination is also twice faster than the base-
line system in decoding.
3.3 Results on using significance values as a
feature
The p-value of each translation pair can be used
as a feature in the log-linear translation model,
to penalize those less significant phrase pairs and
rewrite rules. Since component feature values can-
not be zero, a small positive number was added to
p-values to avoid infinite log value. The results
of using p-values as a feature with different com-
binations of phrase and rule tables are shown in
239
Iraqi Arabic phrase English phrase in data Correct English phrase Frequencies
there are four of us there are five of us 1, 29, 1
young men three of four young men three or four 1, 1, 1
Table 1: Examples of pruned phrase pairs and their frequencies C(s?,
?
t), C(s?), and C(
?
t).
RTABLE0 RTABLE1
PTABLE0 47.38 48.40
PTABLE1 47.05 48.45
PTABLE2 47.50 48.70
PTABLE3 47.81 49.43
Table 2: BLEU scores of IA-to-EN systems using
different combinations of phrase and rule tables.
RTABLE0 RTABLE1
PTABLE0 29.92 29.05
PTABLE1 29.62 30.60
PTABLE2 29.87 30.57
PTABLE3 30.62 31.27
Table 3: BLEU scores of EN-to-IA systems using
different combinations of phrase and rule tables.
Tables 4 and 5. We can see that the results ob-
tained by using the full rule table with the fea-
ture of p-values (the columns of RTABLE0 in Ta-
bles 4 and 5) are much worse than those obtained
by using the pruned rule table without the fea-
ture of p-values (the columns of RTABLE1 in Ta-
bles 2 and 3). This suggests that the use of signif-
icance values as a feature in translation models is
not as efficient as the use in translation table prun-
ing. Modest improvement was observed in the di-
rection EN-to-IA when both pruning and the fea-
ture of p-values are used (compare the columns
of RTABLE1 in Tables 3 and 5) but not in the
direction IA-to-EN. Again, the best results are
achieved by using the combination of PTABLE3
and RTABLE1.
4 Conclusion
The translation quality and speed of a hierarchi-
cal phrase-based SMT system can be improved
by aggressive pruning of translation tables. Our
proposed pruning scheme, which exploits both
significance values and compositional properties,
achieved the best translation quality and gave im-
provements of 1% to 2% on BLEU when com-
pared to the baseline system with full-size tables.
The use of significance values in translation table
RTABLE0 RTABLE1
PTABLE0 47.72 47.96
PTABLE1 46.69 48.75
PTABLE2 47.90 48.48
PTABLE3 47.59 49.50
Table 4: BLEU scores of IA-to-EN systems using
the feature of p-values in different combinations.
RTABLE0 RTABLE1
PTABLE0 29.33 30.44
PTABLE1 30.28 30.99
PTABLE2 30.38 31.44
PTABLE3 30.74 31.64
Table 5: BLEU scores of EN-to-IA systems using
the feature of p-values in different combinations.
pruning and in translation models as a feature has
a different effect: the former led to significant im-
provement, while the latter achieved only modest
or no improvement on translation quality.
5 Acknowledgements
Many thanks to Kristin Precoda and Andreas
Kathol for valuable discussion. This work is sup-
ported by DARPA, under subcontract 55-000916
to UW under prime contract NBCHD040058 to
SRI International.
References
Philipp Koehn, Franz J. Och and Daniel Marcu. 2003.
Statistical phrase-based translation. Proceedings of
HLT-NAACL, 48-54, Edmonton, Canada.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. Proceed-
ings of ACL, 263-270, Ann Arbor, Michigan, USA.
J Howard Johnson, Joel Martin, George Foster and
Roland Kuhn. 2007. Improving Translation Quality
by Discarding Most of the Phrasetable. Proceed-
ings of EMNLP-CoNLL, 967-975, Prague, Czech
Republic.
Robert C. Moore. 2004. On Log-Likelihood-Ratios
and the Significance of Rare Events. Proceedings of
EMNLP, 333-340, Barcelona, Spain
240
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 604?614,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Name-aware Machine Translation
Haibo Li? Jing Zheng? Heng Ji? Qi Li? Wen Wang?
? Computer Science Department and Linguistics Department
Queens College and Graduate Center, City University of New York
New York, NY, USA 10016
{lihaibo.c, hengjicuny, liqiearth}@gmail.com
? Speech Technology & Research Laboratory
SRI International
Menlo Park, CA, USA 94025
{zj, wwang}@speech.sri.com
Abstract
We propose a Name-aware Machine
Translation (MT) approach which can
tightly integrate name processing into MT
model, by jointly annotating parallel cor-
pora, extracting name-aware translation
grammar and rules, adding name phrase
table and name translation driven decod-
ing. Additionally, we also propose a new
MT metric to appropriately evaluate the
translation quality of informative words,
by assigning different weights to differ-
ent words according to their importance
values in a document. Experiments on
Chinese-English translation demonstrated
the effectiveness of our approach on en-
hancing the quality of overall translation,
name translation and word alignment over
a high-quality MT baseline1.
1 Introduction
A shrinking fraction of the world?s Web pages are
written in English, therefore the ability to access
pages across a range of languages is becoming in-
creasingly important. This need can be addressed
in part by cross-lingual information access tasks
such as entity linking (McNamee et al, 2011; Cas-
sidy et al, 2012), event extraction (Hakkani-Tur
et al, 2007), slot filling (Snover et al, 2011) and
question answering (Parton et al, 2009; Parton
and McKeown, 2010). A key bottleneck of high-
quality cross-lingual information access lies in the
performance of Machine Translation (MT). Tradi-
tional MT approaches focus on the fluency and
accuracy of the overall translation but fall short
in their ability to translate certain content word-
s including critical information, especially names.
1Some of the resources and open source programs devel-
oped in this work are made freely available for research pur-
pose at http://nlp.cs.qc.cuny.edu/NAMT.tgz
A typical statistical MT system can only trans-
late 60% person names correctly (Ji et al, 2009).
Incorrect segmentation and translation of names
which often carry central meanings of a sentence
can also yield incorrect translation of long con-
texts. Names have been largely neglected in the
prior MT research due to the following reasons:
? The current dominant automatic MT scoring
metrics (such as Bilingual Evaluation Under-
study (BLEU) (Papineni et al, 2002)) treat
all words equally, but names have relative low
frequency in text (about 6% in newswire and
only 3% in web documents) and thus are vast-
ly outnumbered by function words and com-
mon nouns, etc..
? Name translations pose a greater complexity
because the set of names is open and highly
dynamic. It is also important to acknowledge
that there are many fundamental differences
between the translation of names and other
tokens, depending on whether a name is ren-
dered phonetically, semantically, or a mixture
of both (Ji et al, 2009).
? The artificial settings of assigning low
weights to information translation (compared
to overall word translation) in some large-
scale government evaluations have discour-
aged MT developers to spend time and ex-
plore resources to tackle this problem.
We propose a novel Name-aware MT (NAMT)
approach which can tightly integrate name pro-
cessing into the training and decoding processes of
an end-to-end MT pipeline, and a new name-aware
metric to evaluate MT which can assign different
weights to different tokens according to their im-
portance values in a document. Compared to pre-
vious methods, the novel contributions of our ap-
proach are:
1. Tightly integrate joint bilingual name tag-
ging into MT training by coordinating tagged
604
names in parallel corpora, updating word seg-
mentation, word alignment and grammar ex-
traction (Section 3.1).
2. Tightly integrate name tagging and transla-
tion into MT decoding via name-aware gram-
mar (Section 3.2).
3. Optimize name translation and context trans-
lation simultaneously and conduct name
translation driven decoding with language
model (LM) based selection (Section 3.2).
4. Propose a new MT evaluation metric which
can discriminate names and non-informative
words (Section 4).
2 Baseline MT
As our baseline, we apply a high-performing
Chinese-English MT system (Zheng, 2008; Zheng
et al, 2009) based on hierarchical phrase-based
translation framework (Chiang, 2005). It is based
on a weighted synchronous context-free grammar
(SCFG). All SCFG rules are associated with a set
of features that are used to compute derivation
probabilities. The features include:
? Relative frequency in two directions P (?|?)
andP (?|?), estimating the likelihoods of one
side of the rule r: X ?< ?, ? > translating
into the other side, where ? and ? are strings
of terminals and non-terminals in the source
side and target side. Non-terminals in ? and
? are in one-to-one correspondence.
? Lexical weights in two directions: Pw(?|?)
andPw(?|?), estimating likelihoods of word-
s in one side of the rule r: X ?< ?, ? >
translating into the other side (Koehn et al,
2003).
? Phrase penalty: a penalty exp(1) for a rule
with no non-terminal being used in deriva-
tion.
? Rule penalty: a penalty exp(1) for a rule
with at least one non-terminal being used in
derivation.
? Glue rule penalty: a penalty exp(1) if a glue
rule used in derivation.
? Translation length: number of words in trans-
lation output.
Our previous work showed that combining mul-
tiple LMs trained from different sources can lead
to significant improvement. The LM used for de-
coding is a log-linear combination of four word
n-gram LMs which are built on different English
corpora (details described in section 5.1), with
the LM weights optimized on a development set
and determined by minimum error rate training
(MERT), to estimate the probability of a word giv-
en the preceding words. All four LMs were trained
using modified Kneser-Ney smoothing algorithm
(Chen and Goodman, 1996) and converted into
Bloom filter LMs (Talbot and Brants, 2008) sup-
porting memory map.
The scaling factors for all features are optimized
by minimum error rate training algorithm to max-
imize BLEU score (Och, 2003). Given an input
sentence in the source language, translation into
the target language is cast as a search problem,
where the goal is to find the highest-probability
derivation that generates the source-side sentence,
using the rules in our SCFG. The source-side
derivation corresponds to a synchronous target-
side derivation and the terminal yield of this target-
side derivation is the output of the system. We em-
ploy our CKY-style chart decoder, named SRInter-
p, to solve the search problem.
3 Name-aware MT
We tightly integrate name processing into the
above baseline to construct a NAMT model. Fig-
ure 1 depicts the general procedure.
3.1 Training
This basic training process of NAMT requires us
to apply a bilingual name tagger to annotate par-
allel training corpora. Traditional name tagging
approaches for single languages cannot address
this requirement because they were all built on da-
ta and resources which are specific to each lan-
guage without using any cross-lingual features.
In addition, due to separate decoding processes
the results on parallel data may not be consistent
across languages. We developed a bilingual joint
name tagger (Li et al, 2012) based on condition-
al random fields that incorporates both monolin-
gual and cross-lingual features and conducts join-
t inference, so that name tagging from two lan-
guages can mutually enhance each other and there-
fore inconsistent results can be corrected simulta-
neously. This joint name tagger achieved 86.3%
bilingual pair F-measure with manual alignment
and 84.4% bilingual pair F-measure with automat-
ic alignment as reported in (Li et al, 2012). Given
a parallel sentence pair we first apply Giza++ (Och
and Ney, 2003) to align words, and apply this join-
605
Dec
odin
g
Hier
arch
ical 
Phra
sed-
base
d M
T
Tran
slate
d Te
xt
Tran
slate
Bi-te
xt 
Data Sou
rce Text
Join
t
Nam
e Ta
gger
Sou
rce L
angu
age 
Nam
e Ta
gger
Nam
e Tr
ansl
ator
Trai
ning
Nam
e Pa
ir M
iner
Extr
act s
ourc
e lan
guag
e na
mes
 
and
 add
 the
m to
 dict
iona
ries 
for 
sour
ce la
ngu
age 
nam
e ta
gger
 E
xtra
ct n
ame
 pair
s an
d 
add
 the
m to
tran
slati
on 
dict
iona
ry
Extr
act a
nd a
dd 
nam
e pa
irs t
o
phra
se ta
ble
GIZA
++
Rule
 Extr
acto
r
Extr
act S
CFG
 rule
s wi
th 
com
bina
tion
 of n
ame
-rep
lace
d 
data
 and
 orig
inal 
bi-te
xt d
ata
Rep
lace
 nam
es w
ith 
non
-term
inals
 and
 
com
bine
 with
 the
 
orig
inal 
para
llel d
ata
Figure 1: Architecture of Name-aware Machine Translation System.
t bilingual name tagger to extract three types of
names: (Person (PER), Organization (ORG) and
Geo-political entities (GPE)) from both the source
side and the target side. We pair two entities from
two languages, if they have the same entity type
and are mapped together by word alignment. We
ignore two kinds of names: multi-word names
with conflicting boundaries in two languages and
names only identified in one side of a parallel sen-
tence.
We built a NAMT system from such name-
tagged parallel corpora. First, we replace tagged
name pairs with their entity types, and then
use Giza++ and symmetrization heuristics to re-
generate word alignment. Since the name tags ap-
pear very frequently, the existence of such tags
yields improvement in word alignment quality.
The re-aligned parallel corpora are used to train
our NAMT system based on SCFG. Since the joint
name tagger ensures that each tagged source name
has a corresponding translation on the target side
(and vice versa), we can extract SCFG rules by
treating the tagged names as non-terminals.
However, the original parallel corpora contain
many high-frequency names, which can already be
handled well by the baseline MT. Some of these
names carry special meanings that may influence
translations of the neighboring words, and thus re-
placing them with non-terminals can lead to infor-
mation loss and weaken the translation model. To
address this issue, we merged the name-replaced
parallel data with the original parallel data and ex-
tract grammars from the combined corpus. For ex-
ample, given the following sentence pair:
? -???e???e????? .
? China appeals to world for non involvement
in Angola conflict .
after name tagging it becomes
? GPE??e???e GPE?? .
? GPE appeals to world for non involvement in
GPE conflict .
Both sentence pairs are kept in the combined data
to build the translation model.
3.2 Decoding
During decoding phase, we extract names with
the baseline monolingual name tagger described
in (Li et al, 2012) from a source document. It-
s performance is comparable to the best report-
ed results on Chinese name tagging on Automat-
ic Content Extraction (ACE) data (Ji and Grish-
man, 2006; Florian et al, 2006; Zitouni and Flo-
rian, 2008; Nguyen et al, 2010). Then we ap-
ply a state-of-the-art name translation system (Ji
et al, 2009) to translate names into the target lan-
guage. The name translation system is composed
of the following steps: (1) Dictionary matching
based on 150,041 name translation pairs; (2) Sta-
tistical name transliteration based on a structured
perceptron model and a character based MT mod-
el (Dayne and Shahram, 2007); (3) Context infor-
mation extraction based re-ranking.
In our NAMT framework, we add the following
extensions to name translation.
We developed a name origin classifier based on
Chinese last name list (446 name characters) and
name structure parsing features to distinguish Chi-
nese person names and foreign person names (Ji,
2009), so that pinyin conversion is applied for Chi-
nese names while name transliteration is applied
only for foreign names. This classifier works rea-
sonably well in most cases (about 92% classifica-
tion accuracy), except when a common Chinese
last name appears as the first character of a foreign
606
name, such as ?1?? which can be translated ei-
ther as ?Jolie? or ?Zhu Li?.
For those names with fewer than five instances
in the training data, we use the name translation
system to provide translations; for the rest of the
names, we leave them to the baseline MT mod-
el to handle. The joint bilingual name tagger was
also exploited to mine bilingual name translation
pairs from parallel training corpora. The mapping
score between a Chinese name and an English
name was computed by the number of aligned to-
kens. A name pair is extracted if the mapping
score is the highest among all combinations and
the name types on both sides are identical. It is
necessary to incorporate word alignment as addi-
tional constraints because the order of names is of-
ten changed after translation. Finally, the extract-
ed 9,963 unique name translation pairs were also
used to create an additional name phrase table for
NAMT. Manual evaluation on 2,000 name pairs
showed the accuracy is 86%.
The non-terminals in SCFG rules are rewritten
to the extracted names during decoding, therefore
allow unseen names in the test data to be trans-
lated. Finally, based on LMs, our decoder ex-
ploits the dynamically created phrase table from
name translation, competing with originally ex-
tracted rules, to find the best translation for the
input sentence.
4 Name-aware MT Evaluation
Traditional MT evaluation metrics such as
BLEU (Papineni et al, 2002) and Translation Ed-
it Rate (TER) (Snover et al, 2006) assign the
same weights to all tokens equally. For exam-
ple, incorrect translations of ?the? and ?Bush? will
receive the same penalty. However, for cross-
lingual information processing applications, we
should acknowledge that certain informationally
critical words are more important than other com-
mon words. In order to properly evaluate the trans-
lation quality of NAMT methods, we propose to
modify the BLEU metric so that they can dynam-
ically assign more weights to names during evalu-
ation.
BLEU considers the correspondence between a
system translation and a human translation:
BLEU = BP ? exp
( N?
n=1
wn log pn
)
(1)
where BP is brevity penalty defined as follows:
BP =
{
1 if c > r,
e(1?r/c) if c ? r. (2)
where wn is a set of positive weights summing to
one and usually uniformly set as wn = 1/N , c is
the length of the system translation and r is the
length of reference translation, and pn is modified
n-gram precision defined as:
pn =
?
C?Candidates
?
n-gram?C
Countclip(n-gram)
?
C??Candidates
?
n-gram??C?
Countclip(n-gram?)
(3)
where C and C ? are translation candidates in the
candidate sentence set, if a source sentence is
translated to many candidate sentences.
As in BLEU metric, we first count the maxi-
mum number of times an n-gram occurs in any s-
ingle reference translation. The total count of each
candidate n-gram is clipped at sentence level by it-
s maximum reference count. Then we add up the
weights of clipped n-grams and divide them by the
total weight of all n-grams.
Based on BLEU score, we design a name-aware
BLEU metric as follows. Depending on whether a
token t is contained in a name in reference trans-
lation, we assign a weight weightt to t as follows:
weightt ={
1? e?tf(t,d)?idf(t,D), if t never appears in names
1 + PEZ , if t occurs in name(s)
(4)
where PE is the sum of penalties of non-name
tokens and Z is the number of tokens within all
names:
PE =
?
t never appears in names
e?tf(t,d)?idf(t,D) (5)
In this paper, the tf ? idf score is computed at sen-
tence level, therefore, D is the sentence set and
each d ? D is a sentence.
The weight of an n-gram in reference translation
is the sum of weights of all tokens it contains.
weightngram =
?
t?ngram
weightt (6)
Next, we compute the weighted modified n-
gram precision Countweight?clip(n-gram) as fol-
lows:
Countweight?clip(n-gram) =?
if the ngrami is correctly translated
weightngrami (7)
607
The Countclip(n-gram) in the equation 3 is
substituted with aboveCountweight?clip(n-gram).
When we sum up the total weight of all n-grams of
a candidate translation, some n-grams may contain
tokens which do not exist in reference translation.
We assign the lowest weight of tokens in reference
translation to these rare tokens.
We also add an item, name penalty NP , to
penalize the output sentences which contain too
many or too few names:
NP = e?(uv?1)2/2? (8)
where u is the number of name tokens in system
translation and v is the number of name tokens in
reference translation.
Finally the name-aware BLEU score is defined
as:
BLEUNA = BP ?NP ? exp
( N?
n=1
wn logwpn
)
(9)
This new metric can also be applied to evalu-
ate MT approaches which emphasize other types
of facts such as events, by simply replacing name
tokens by other fact tokens.
5 Experiments
In this section we present the experimental results
of NAMT compared to the baseline MT.
5.1 Data Set
We used a large Chinese-English MT training cor-
pus from various sources and genres (including
newswire, web text, broadcast news and broadcast
conversations) for our experiments. We also used
some translation lexicon data and Wikipedia trans-
lations. The majority of the data sets were col-
lected or made available by LDC for U.S. DARPA
Translingual Information Detection, Extraction
and Summarization (TIDES) program, Global Au-
tonomous Language Exploitation (GALE) pro-
gram, Broad Operational Language Translation
(BOLT) program and National Institute of Stan-
dards and Technology (NIST) MT evaluations.
The training corpus includes 1,686,458 sentence
pairs. The joint name tagger extracted 1,890,335
name pairs (295,087 Persons, 1,269,056 Geo-
political entities and 326,192 Organizations).
Four LMs, denoted LM1, LM2, LM3, and
LM4, were trained from different English cor-
pora. LM1 is a 7-gram LM trained on the tar-
get side of Chinese-English and Egyptian Arabic-
English parallel text, English monolingual discus-
sion forums data R1-R4 released in BOLT Phase
1 (LDC2012E04, LDC2012E16, LDC2012E21,
LDC2012E54), and English Gigaword Fifth Edi-
tion (LDC2011T07). LM2 is a 7-gram LM trained
only on the English monolingual discussion fo-
rums data listed above. LM3 is a 4-gram LM
trained on the web genre among the target side
of all parallel text (i.e., web text from pre-BOLT
parallel text and BOLT released discussion fo-
rum parallel text). LM4 is a 4-gram LM trained
on the English broadcast news and conversation
transcripts released under the DARPA GALE pro-
gram. Note that for LM4 training data, some tran-
scripts were quick transcripts and quick rich tran-
scripts released by LDC, and some were generated
by running flexible alignment of closed captions or
speech recognition output from LDC on the audio
data (Venkataraman et al, 2004).
In order to demonstrate the effectiveness and
generality of our approach, we evaluated our ap-
proach on seven test sets from multiple genres and
domains. We asked four annotators to annotate
names in four reference translations of each sen-
tence and an expert annotator to adjudicate result-
s. The detailed statistics and name distribution of
each test data set is shown in Table 1. The per-
centage of names occurred fewer than 5 times in
training data are listed in the brackets in the last
column of the table.
5.2 Overall Performance
Besides the new name-aware MT metric, we also
adopt two traditional metrics, TER to evaluate the
overall translation performance and Named Entity
Weak Accuracy (NEWA) (Hermjakob et al, 2008)
to evaluate the name translation performance.
TER measures the amount of edits required to
change a system output into one of the reference
translations. Specifically:
TER = # of editsaverage # of reference words (10)
Possible edits include insertion, substitution dele-
tion and shifts of words.
The NEWA metric is defined as follows. Us-
ing a manually assembled name variant table, we
also support the matching of name variants (e.g.,
?World Health Organization? and ?WHO?).
NEWA = Count # of correctly translated namesCount # of names in references (11)
608
Corpus Genre Sentence # Word # Token # GPE(%) PER(%) ORG(%) All namesin source in reference (% occurred < 5)
BOLT 1 forum 1,200 20,968 24,193 875(82.9) 90(8.5) 91(8.6) 1,056 (51.4)
BOLT 2 forum 1,283 23,707 25,759 815(73.7) 141(12.8) 149(13.5) 1,105 (65.9)
BOLT 3 forum 2,000 38,595 42,519 1,664(80.4) 204(9.8) 204(9.8) 2,072 (47.4)
BOLT 4 forum 1,918 41,759 47,755 1,852(80.0) 348(25.0) 113(5.0) 2,313 (53.3)
BOLT 5 blog 950 23,930 26,875 352(42.5) 235(28.3) 242(29.2) 829 (55.3)
NIST2006 news&blog 1,664 38,442 45,914 1,660(58.2) 568(19.9) 625(21.9) 2,853 (73.1)
NIST2008 news&blog 1,357 32,646 37,315 700(47.9) 367(25.1) 395(27.0) 1,462 (72.0)
Table 1: Statistics and Name Distribution of Test Data Sets.
Metric System BOLT 1 BOLT 2 BOLT 3 BOLT 4 BOLT 5 NIST2006 NIST2008
BLEU
Baseline 14.2 14.0 17.3 15.6 15.3 35.5 29.3
NPhrase 14.1 14.4 17.1 15.4 15.3 35.4 29.3
NAMT 14.2 14.6 16.9 15.7 15.5 36.3 30.0
Name-aware BLEU
Baseline 18.2 17.9 18.6 17.6 18.3 36.1 31.7
NPhrase 18.1 18.8 18.5 18.1 18.0 35.8 31.8
NAMT 18.4 19.5 19.7 18.2 18.9 39.4 33.1
TER
Baseline 70.6 71.0 69.4 70.3 67.1 58.7 61.0
NPhrase 70.6 70.4 69.4 70.4 67.1 58.7 60.9
NAMT 70.3 70.2 69.2 70.1 66.6 57.7 60.5
NEWA
All
Baseline 69.7 70.1 73.9 72.3 60.6 66.5 60.4
NPhrase 69.8 71.1 73.8 72.5 60.6 68.3 61.9
NAMT 71.4 72.0 77.7 75.1 62.7 72.9 63.2
GPE
Baseline 72.8 78.4 80.0 78.7 81.3 79.2 76.0
NPhrase 73.6 79.3 79.2 78.9 82.3 82.6 79.5
NAMT 74.2 80.2 82.8 80.4 79.3 85.5 79.3
PER
Baseline 53.3 44.7 45.1 49.4 48.9 54.2 51.2
NPhrase 52.2 45.4 48.9 48.5 47.6 55.1 50.9
NAMT 55.6 45.4 58.8 55.2 56.2 60.0 52.3
ORG
Baseline 56.0 49.0 52.9 38.1 41.7 44.0 41.3
NPhrase 50.5 50.3 54.4 40.7 41.3 42.2 40.7
NAMT 60.4 52.3 55.4 41.6 45.0 51.0 44.8
Table 2: Translation Performance (%).
For better comparison with NAMT, besides the
original baseline, we develop the other baseline
system by adding name translation table into the
phrase table (NPhrase).
Table 2 presents the performance of overal-
l translation and name translation. We can see
that except for the BOLT3 data set with BLEU
metric, our NAMT approach consistently outper-
formed the baseline system for all data sets with
all metrics, and provided up to 23.6% relative er-
ror reduction on name translation. According to
Wilcoxon Matched-Pairs Signed-Ranks Test, the
improvement is not significant with BLEU metric,
but is significant at 98% confidence level with all
of the other metrics. The gains are more signifi-
cant for formal genres than informal genres main-
ly because most of the training data for name tag-
ging and name translation were from newswire.
Furthermore, using external name translation table
only did not improve translation quality in most
test sets except for BOLT2. Therefore, it is im-
portant to use name-replaced corpora for rule ex-
traction to fully take advantage of improved word
alignment.
Many errors from the baseline MT approach oc-
curred because some parts of out-of-vocabulary
names were mistakenly segmented into common
words. For example, the baseline MT system mis-
takenly translated a person name ?Y?? (Sun
Honglei)? into ?Sun red thunder?. In informal
genres such as discussion forums and web blogs,
even common names often appear in rare form-
s due to misspelling or morphing. For example,
?e8l (Obama)? was mistakenly translated into
?Ma Olympic?. Such errors can be compounded
when word re-ordering was applied. For example,
the following sentence: ????????/:
'J/iy (Guo Meimei?s strength real-
ly is formidable, I really admire her)? was mis-
takenly translated into ?Guo the strength of the
America and the America also really strong , ah
, really admire her? by the baseline MT system
because the person name ???? (Guomeimei)?
was mistakenly segmented into three words ??
(Guo)?, ?? (the America)? and ?? (the Ameri-
ca)?. But our NAMT approach successfully iden-
tified and translated this name and also generated
better overall translation: ?Guo Meimei ?s power
is also really strong , ah , really admire her?.
609
B L E U N a m e - a w a r eB L E U024681 01 2
1 41 61 82 0Score A u t o m a t i c  M e t r i c s H u m .  1  H u m .  2 H u m .  30 . 00 . 51 . 01 . 52 . 02 . 5
3 . 03 . 54 . 0 b a s e l i n e N A M T Score H u m a n  E v a l u a t i o n
Figure 2: Scores based on Automatic Metrics and Human
Evaluation.
5.3 Name-aware BLEU vs The Human
Evaluation
In order to investigate the correlation between
name-aware BLEU scores and human judgment
results, we asked three bi-lingual speakers to judge
our translation output from the baseline system
and the NAMT system, on a Chinese subset of 250
sentences (each sentence has two corresponding
translations from baseline and NAMT) extracted
randomly from 7 test corpora. The annotators rat-
ed each translation from 1 (very bad) to 5 (very
good) and made their judgments based on whether
the translation is understandable and conveys the
same meaning.
We computed the name-aware BLEU scores on
the subset and also the aggregated average scores
from human judgments. Figure 2 shows that
NAMT consistently achieved higher scores with
both name-aware BLEU metric and human judge-
ment. Furthermore, we calculated three Pearson
product-moment correlation coefficients between
human judgment scores and name-aware BLEU s-
cores of these two MT systems. Give the sample
size and the correlation coefficient value, the high
significance value of 0.99 indicates that name-
aware BLEU tracks human judgment well.
5.4 Word Alignment
It is also important to investigate the impact of our
NAMT approach on improving word alignmen-
t. We conducted the experiment on the Chinese-
English Parallel Treebank (Li et al, 2010) with
ground-truth word alignment. The detailed pro-
cedure following NAMT framework is as follows:
(1) Ran the joint bilingual name tagger; (2) Re-
placed each name string with its name type (PER,
ORG or GPE), and ran Giza++ on the replaced
sentences; (3) Ran Giza++ on the words within
Words Method P R F 
Baseline Giza++ 69.8 47.8 56.7 
Joint Name 
Tagging 
70.4 48.1 57.1 
 
Overall 
Words 
Ground-truth 
Name Tagging 
(Upper-bound) 
71.3 48.9 58.0 
Baseline Giza++ 86.0 31.4 46.0 Words 
Within 
Names 
Joint Name 
Tagging 
77.6 37.2 50.3 
 
 
 
 
 
 
 
 
 
 
Table 3: Impact of Joint Bilingual Name Tagging on Word
Alignment (%).
each name pair. (4) Merged (2) and (3) to pro-
duce the final word alignment results. In order to
compare with the upper-bound gains, we also mea-
sured the performance of applying ground-truth
name tagging with the above procedures.
The experiment results are shown in Table 3.
For the words within names, our approach provid-
ed significant gains by enhancing F-measure from
46.0% to 50.3%. Only 10.6% words are within
names, therefore the upper-bound gains on over-
all word alignment is only 1.3%. Our joint name
tagging approach achieved 0.4% (statistically sig-
nificant) improvement over the baseline. In Fig-
ure 3 we categorized the sentences according to
the percentage of name words in each sentence and
measured the improvement for each category. We
can clearly see that as the sentences include more
names, the gains achieved by our approach tend to
be greater.
5.5 Remaining Error Analysis
Although the proposed model has significantly en-
hanced translation quality, some challenges re-
main. We analyze some major sources of the re-
maining errors as follows.
1. Name Structure Parsing.
We found that the gains of our NAMT approach
were mainly achieved for names with one or two
components. When the name structure becomes
too complicated to parse, name tagging and name
translation are likely to produce errors, especially
for long nested organizations. For example, ??0
???b?@? (Anti-malfeasance Bureau of
Gutian County Procuratorate) consists of a nested
organization name with a GPE as modifier: ??
0???b? (Gutian County Procuratorate) and
an ORG name: ??@? (Anti-malfeasance Bu-
reau).
2. Name abbreviation tagging and translation.
Some organization abbreviations are also dif-
ficult to extract because our name taggers have
610
0~10 10~20 20~30 30~40 >40
-0.5
0.0
0.5
1.0
1.5
2.0
2.5
3.0
3.5
4.0
4.5
5.0
5.5
F-Measure Gains in Overall Word Alignment (%)
#name tokens/#all tokens(%)
 Baseline Giza++
 Joint Name Tagging
 Ground-truth Name Tagging (Upper-bound)
Figure 3: Word alignment gains according to the percentage
of name words in each sentence.
not incorporated any coreference resolution tech-
niques. For example, without knowing that ?FAW?
refers to ?First Automotive Works? in ?FAW has
also utilized the capital market to directly fi-
nance, and now owns three domestic listed compa-
nies?, our system mistakenly labeled it as a GPE.
The same challenge exists in name alignment and
translation (for example, ? i (Min Ge)? refer-
s to ? -??Zi}?X