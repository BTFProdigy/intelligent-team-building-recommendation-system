Proceedings of NAACL HLT 2009: Short Papers, pages 49?52,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Modeling Dialogue Structure with  Adjacency Pair Analysis and Hidden Markov Models  Kristy Elizabeth Boyer*1 Robert Phillips1,2 Eun Young Ha1 Michael D.  Wallis1,2 Mladen A.  Vouk1 James C. Lester1  1Department of Computer Science North Carolina State University Raleigh, NC, USA  2Applied Research Associates Raleigh, NC, USA  *keboyer@ncsu.edu  Abstract 
Automatically detecting dialogue structure within corpora of human-human dialogue is the subject of increasing attention.  In the do-main of tutorial dialogue, automatic discovery of dialogue structure is of particular interest because these structures inherently represent tutorial strategies or modes, the study of which is key to the design of intelligent tutor-ing systems that communicate with learners through natural language.  We propose a methodology in which a corpus of human-human tutorial dialogue is first manually an-notated with dialogue acts.  Dependent adja-cency pairs of these acts are then identified through ?2 analysis, and hidden Markov mod-eling is applied to the observed sequences to induce a descriptive model of the dialogue structure.       
1 Introduction Automatically learning dialogue structure from corpora is an active area of research driven by a recognition of the value offered by data-driven ap-proaches (e.g., Bangalore et al, 2006).  Dialogue structure information is of particular importance when the interaction is centered around a learning task, such as in natural language tutoring, because techniques that support empirical identification of dialogue strategies can inform not only the design of intelligent tutoring systems (Forbes-Riley et al, 2007), but also contribute to our understanding of 
the cognitive and affective processes involved in learning through tutoring (VanLehn et al, 2007).        Although traditional top-down approaches (e.g., Cade et al, 2008) and some empirical work on analyzing the structure of tutorial dialogue (Forbes-Riley et al, 2007) have yielded significant results, the field is limited by the lack of an auto-matic, data-driven approach to identifying dialogue structure.  An empirical approach to identifying tutorial dialogue strategies, or modes, could ad-dress this limitation by providing a mechanism for describing in succinct probabilistic terms the tuto-rial strategies that actually occur in a corpus.      Just as early work on dialogue act interpretation utilized hidden Markov models (HMMs) to capture linguistic structure (Stolcke et al, 2000), we pro-pose a system that uses HMMs to capture the structure of tutorial dialogue implicit within se-quences of already-tagged dialogue acts.  This ap-proach operates on the premise that at any given point in the tutorial dialogue, the collaborative in-teraction is in a dialogue mode that characterizes the nature of the exchanges between tutor and stu-dent.  In our model, a dialogue mode is defined by a probability distribution over the observed sym-bols (e.g., dialogue acts and adjacency pairs).      Our previous work has noted some limitations of first-order HMMs as applied to sequences of individual dialogue acts (Boyer et al, in press).  Chief among these is that HMMs allow arbitrarily frequent transitions between hidden states, which does not conform well to human intuition about how tutoring strategies are applied.  Training an HMM on a sequence of adjacency pairs rather than individual dialogue acts is one way to generate a 
49
more descriptive model without increasing model complexity more than is required to accommodate the expanded set of observation symbols.  To this end, we apply the approach of Midgley et al (2006) for empirically identifying significant adja-cency pairs within dialogue, and proceed by treat-ing adjacency pairs as atomic units for the purposes of training the HMM.   2 Corpus Analysis This analysis uses a corpus of human-human tuto-rial dialogue collected in the domain of introduc-tory computer science.  Forty-three learners interacted remotely with a tutor through a key-board-to-keyboard remote learning environment yielding 4,864 dialogue moves.    The tutoring corpus was manually tagged with dialogue acts designed to capture the salient char-acteristics of the tutoring process (Table 1).  Tag Act Example Q Question Where should I  declare i? EQ Evaluation Question How does that look? S Statement You need a  closing brace. G Grounding Ok.  EX Extra-Domain You may use  your book. PF Positive Feedback Yes, that?s right. LF Lukewarm Feedback Sort of. NF Negative Feedback No, that?s not right. Table 1. Dialogue Act Tags  The correspondence between utterances and dia-logue act tags is one-to-one.  Compound utterances (i.e., a single utterance comprising more than one dialogue act) were split by the primary annotator prior to the inter-rater reliability study.1      The importance of adjacency pairs is well-established in natural language dialogue (e.g., Schlegoff & Sacks, 1973), and adjacency pair analysis has illuminated important phenomena in tutoring as well (Forbes-Riley et al, 2007).  For the current corpus, bigram analysis of dialogue acts yielded a set of commonly-occurring pairs.  How-ever, as noted in (Midgley et al, 2006), in order to                                                            1 Details of the study procedure used to collect the corpus, as well as Kappa statistics for inter-rater reliability, are reported in (Boyer et al, 2008). 
establish that two dialogue acts are truly related as an adjacency pair, it is important to determine whether the presence of the first member of the pair is associated with a significantly higher prob-ability of the second member occurring.  For this analysis we utilize a ?2 test for independence of the categorical variables acti and acti+1 for all two-way combinations of dialogue act tags.  Only pairs in which speaker(acti)?speaker(acti+1) were consid-ered.  Other dialogue acts were treated as atomic elements in subsequent analysis, as discussed in Section 3.  Table 2 displays a list of the dependent pairs sorted by descending (unadjusted) statistical significance; the subscript indicates tutor (t) or stu-dent (s).  acti acti+1 P(acti+1|       acti) P(acti+1|    ?acti) ?2 val p-val EQs PFt 0.48 0.07 654 <0.0001 Gs Gt 0.27 0.03 380 <0.0001 EXs EXt 0.34 0.03 378 <0.0001 EQt PFs 0.18 0.01 322 <0.0001 EQt Ss 0.24 0.03 289 <0.0001 EQs LFt 0.13 0.01 265 <0.0001 Qt Ss 0.65 0.04 235 <0.0001 EQt LFs 0.07 0.00 219 <0.0001 Qs St 0.82 0.38 210 <0.0001 EQs NFt 0.08 0.01 207 <0.0001 EXt EXs 0.19 0.02 177 <0.0001 NFs Gt 0.29 0.03 172 <0.0001 EQt NFs 0.11 0.01 133 <0.0001 Ss Gt 0.16 0.03 95 <0.0001 Ss PFt 0.30 0.10 90 <0.0001 St Gs 0.07 0.04 36 <0.0001 PFs Gt 0.14 0.04 34 <0.0001 LFs Gt 0.22 0.04 30 <0.0001 St EQs 0.11 0.07 29 <0.0001 Gt EXs 0.07 0.03 14 0.002 St Qs 0.07 0.05 14 0.0002 Gt Gs 0.10 0.05 9 0.0027 EQt EQs 0.13 0.08 8 0.0042 Table 2. Dependent Adjacency Pairs 3 HMM on Adjacency Pair Sequences The keyboard-to-keyboard tutorial interaction re-sulted in a sequence of utterances that were anno-tated with dialogue acts.  We have hypothesized that a higher-level dialogue structure, namely the tutorial dialogue mode, overlays the observed dia-logue acts.  To build an HMM model of this struc-
50
ture we treat dialogue mode as a hidden variable and train a hidden Markov model to induce the dialogue modes and their associated dialogue act emission probability distributions.    An adjacency pair joining algorithm (Figure 1) was applied to each sequence of dialogue acts.  This algorithm joins pairs of dialogue acts into atomic units according to a priority determined by the strength of the adjacency pair dependency.  Sort adjacency pair list L by descending statistical significance For each adjacency pair (act1, act2) in L         For each dialogue act sequence (a1, a2, ?, an)          in the corpus                 Replace all pairs (ai=act1, ai+1=act2) with a                 new single act (act1act2) Figure 1.  Adjacency Pair Joining Algorithm     Figure 2 illustrates the application of the adja-cency pair joining algorithm on a sequence of dia-logue acts.  Any dialogue acts that were not grouped into adjacency pairs at the completion of the algorithm are treated as atomic units in the HMMianalysis.   Original Dialogue Act Sequence: Qs - St - LFt - St - St - Gs - EQs - LFt - St - St - Qs - St After Adjacency Pair Joining Algorithm: QsSt - LFt - St - StGs - EQsLFt - St - St - QsSt Figure 2.  DA Sequence Before/After Joining     The final set of observed symbols consists of 39 tags: 23 adjacency pairs (Table 2) plus all individ-ual dialogue acts augmented with a tag for the speaker (Table 1).      It was desirable to learn n, the best number of hidden states, during modeling rather than specify-ing this value a priori.  To this end, we trained and ten-fold cross-validated seven models (each featur-ing randomly-initialized parameters) for each number of hidden states n from 2 to 15, inclusive.2  The average log-likelihood was computed across all seven models for each n, and this average log-                                                           2 n=15 was chosen as an initial maximum number of states because it comfortably exceeded our hypothesized range of 3 to 7 (informed by the tutoring literature).  The Akaike Infor-mation Criterion measure steadily worsened above n = 5, con-firming no need to train models with n > 15. 
likelihood ln was used to compute the Akaike In-formation Criterion, a maximum-penalized likeli-hood estimator that penalizes more complex models (Scott, 2002).  The best fit was obtained with n=4 (Figure 3).  The transition probability distribution among hidden states is depicted in Figure 4, with the size of the nodes indicating rela-tive frequency of each hidden state; specifically, State 0 accounts for 63% of the corpus, States 1 and 3 account for approximately 15% each, and State 2 accounts for 7%.  
  Figure 3.  Dialogue Act Emission Probability  Distribution by Dialogue Mode3 4 Discussion and Future Work This exploratory application of hidden Markov models involves training an HMM on a mixed in-put sequence consisting of both individual dialogue acts and adjacency pairs.  The best-fit HMM con-sists of four hidden states whose emission symbol probability distributions lend themselves to inter-pretation as tutorial dialogue modes.  For example, State 0 consists primarily of tutor statements and positive feedback, two of the most common dia-logue  acts  in our corpus.  The transition probabili- 
51
 Figure 4.  Transition Probability Distribution4  ties also reveal that State 0 is highly stable; a self-transition is most likely with probability 0.835.  State 3 is an interactive state featuring student re-flection in the form of questions, statements, and requests for feedback.  The transition probabilities show that nearly 60% of the time the dialogue transitions from State 3 to State 0; this may indi-cate that after establishing what the student does or does not know in State 3, the tutoring switches to a less collaborative ?teaching? mode represented by State 0.        Future evaluation of the HMM presented here will include comparison with other types of graphical models.  Another important step is to correlate the dialogue profile of each tutoring ses-sion, as revealed by the HMM, to learning and af-fective outcomes of the tutoring session.  This type of inquiry can lead directly to design recommenda-tions for tutorial dialogue systems that aim to maximize particular learner outcomes.  In addition, leveraging knowledge of the task state as well as surface-level utterance content below the dialogue act level are promising directions for refining the descriptive and predictive power of these models.      Acknowledgements  This research was supported by the National Science Foundation under Grants REC-0632450, IIS-0812291, CNS-0540523, and GRFP.  Any opinions, findings, and conclusions or recommendations ex-pressed in this material are those of the 
authors and do not necessarily reflect the views of the National Science Foundation.  References Boyer, K.E., Phillips, R., Wallis, M., Vouk, M., & Lester, J. (2008).  Balancing cognitive and moti-vational scaffolding in tutorial dialogue.  Pro-ceedings of the 9th International Conference on Intelligent Tutoring Systems, Montreal, Canada, 239-249. Boyer, K.E., Ha, E.Y., Wallis, M., Phillips, R., Vouk, M. & Lester, J. (in press).  Discovering tutorial dialogue strategies with hidden Markov models.  To appear in Proceedings of the 14th International Conference on Artificial Intelligence in Educa-tion, Brighton, U.K. Bangalore, S., DiFabbrizio, G., Stent, A. (2006).  Learning the structure of task-driven human-human dialogs.  Proceedings of ACL, Sydney, Australia, 201-208. Cade, W., Copeland, J., Person, N., & D'Mello, S. (2008). Dialog modes in expert tutoring. Proceed-ings of the 9th International Conference on Intel-ligent Tutoring Systems, Montreal, Canada, 470-479.  Forbes-Riley, K., Rotaru, M., Litman, D. J., & Tetreault, J. (2007). Exploring affect-context de-pendencies for adaptive system development. Proceedings of NAACL HLT, Companion Volume, 41-44.  Midgley, T. D., Harrison, S., & MacNish, C. (2007). Empirical verification of adjacency pairs using dialogue segmentation. Proceedings of the 7th SIGdial Workshop on Discourse and Dialogue, 104-108.  Schlegoff, E.A., Sacks, H. (1973).  Opening up clos-ings.  Semiotica, 8(4), 289-327. Scott, S. L. (2002). Bayesian methods for hidden Markov models: Recursive computing in the 21st century. Journal of the American Statistical Asso-ciation, 97(457), 337-351. Stolcke, A., Coccaro, N., Bates, R., Taylor, P., Van Ess-Dykema, C., Ries, K., Shirberg, E., Jurafsky, D., Martin, R., Meteer, M. (2000).  Dialog act modeling for automatic tagging and recognition of conversational speech.  Computational Linguistics 26(3), 339-373. VanLehn, K., Graesser, A., Jackson, G.T., Jordan, P., Olney, A., Rose, C.P. (2007). When are tutorial dialogues more effective than reading? Cognitive Science, 31(1), 3-62.  
52
Proceedings of the Third ACL Workshop on Innovative Use of NLP for Building Educational Applications, pages 53?61,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Learner Characteristics and Feedback in Tutorial Dialogue 
 
 
Kristy Elizabeth 
Boyera 
Robert  
Phillipsab 
Michael D. 
Wallisab 
Mladen A. 
Vouka 
James C. 
Lestera 
 
aDepartment of Computer Science, North Carolina State University 
bApplied Research Associates, Inc. 
Raleigh, North Carolina, USA 
{keboyer, rphilli, mdwallis, vouk, lester}@ncsu.edu 
 
 
 
 
 
Abstract 
Tutorial dialogue has been the subject of in-
creasing attention in recent years, and it has 
become evident that empirical studies of hu-
man-human tutorial dialogue can contribute 
important insights to the design of computa-
tional models of dialogue.  This paper reports 
on a corpus study of human-human tutorial 
dialogue transpiring in the course of problem-
solving in a learning environment for intro-
ductory computer science.  Analyses suggest 
that the choice of corrective tutorial strategy 
makes a significant difference in the outcomes 
of both student learning gains and self-
efficacy gains.  The findings reveal that tuto-
rial strategies intended to maximize student 
motivational outcomes (e.g., self-efficacy 
gain) may not be the same strategies that 
maximize cognitive outcomes (i.e., learning 
gain).  In light of recent findings that learner 
characteristics influence the structure of tuto-
rial dialogue, we explore the importance of 
understanding the interaction between learner 
characteristics and tutorial dialogue strategy 
choice when designing tutorial dialogue sys-
tems.  
1 Introduction 
Providing intelligent tutoring systems (ITSs) with 
the ability to engage learners in rich natural lan-
guage dialogue has been a goal of the ITS commu-
nity since the inception of the field.  Tutorial 
dialogue has been studied in the context of a num-
ber of systems devised to support a broad range of 
conversational phenomena.  Systems such as 
CIRCSIM (Evens and Michael 2006), BEETLE (Zinn 
et al 2002), the Geometry Explanation Tutor 
(Aleven et al 2003), Why2/Atlas (VanLehn et al 
2002), ITSpoke (Litman et al 2006), SCOT (Pon-
Barry et al 2006), ProPL (Lane and VanLehn 
2005) and AutoTutor (Graesser et al 2003) support 
research that has begun to the see the emergence of 
a core set of foundational requirements for mixed-
initiative natural language interaction that occurs in 
the kind of tutorial dialogue investigated here.  
Moreover, recent years have witnessed the appear-
ance of corpus studies empirically investigating 
speech acts in tutorial dialogue (Marineau et al 
2000), dialogues? correlation with learning 
(Forbes-Riley et al 2005, Core et al 2003, Ros? et 
al. 2003, Katz et al 2003), student uncertainty in 
dialogue (Liscombe et al 2005, Forbes-Riley and 
Litman 2005), and comparing text-based and spo-
ken dialogue (Litman et al 2006). 
     Recent years have also seen the emergence of a 
broader view of learning as a complex process in-
volving both cognitive and affective states.  To 
empirically explore these issues, a number of ITSs 
such as AutoTutor (Jackson et al 2007), Betty?s 
Brain (Tan and Biswas 2006), ITSpoke (Forbes-
Riley et al 2005), M-Ecolab (Rebolledo-Mendez 
et al 2006), and MORE (del Soldato and Boulay 
1995) are being used as platforms to investigate the 
impact of tutorial interactions on affective and mo-
tivational outcomes (e.g., self-efficacy) along with 
purely cognitive measures (i.e., learning gains).  A 
central problem in this line of investigation is iden-
53
tifying tutorial strategies (e.g., Graesser et al 
1995) that can appropriately balance the tradeoffs 
between cognitive and affective student outcomes 
(Lepper et al 1993).  While a rich set of cognitive 
and affective tutorial strategies is emerging (e.g., 
Porayska-Pomsta et al 2004), the precise nature of 
the interdependence between these types of strate-
gies is not well understood.  In addition, it may be 
the case that different populations of learners en-
gage in qualitatively different forms of dialogue.  
Students with particular characteristics may have 
specific dialogue profiles, and knowledge of such 
profiles could inform the design of tutorial systems 
whose strategies leverage the characteristics of the 
target population.  The extent to which different 
tutorial strategies, and specific instances of them in 
certain contexts, may be used to enhance tutorial 
effectiveness is an important question to designers 
of ITSs.    
     Given that human-human tutorial dialogue of-
fers a promising model for effective communica-
tion (Chi et al 2001), our methodology is to study 
naturally occurring tutorial dialogues in a task-
oriented learning environment to investigate the 
relationship between the structure of tutorial dia-
logue, the characteristics of learners, and the im-
pact of cognitive and motivational corrective 
tutorial strategies on learning and self-efficacy 
(Boyer et al in press).  A text-based dialogue inter-
face was incorporated into a learning environment 
for introductory computer science.  In the envi-
ronment, students undertook a programming task 
and conversed with human tutors while designing, 
implementing, and testing Java programs.    
     The results of the study suggest that the choice 
of corrective tutorial strategy has a significant im-
pact on the learning gains and self-efficacy of stu-
dents.  These findings reinforce those of other 
studies (e.g., Lepper et al 1993, Person et al 1995, 
Keller et al 1983) that indicate that some cognitive 
and motivational goals may be at odds with one 
other because a tutorial strategy designed to maxi-
mize one set of goals (e.g., cognitive goals) can 
negatively impact the other.  We contextualize our 
findings in light of recent results that learner char-
acteristics such as self-efficacy influence the struc-
ture of task-oriented tutorial dialogue (Boyer et al 
2007), and may therefore produce important inter-
action effects when considered alongside tutorial 
strategy.    
     This paper is organized as follows.  Section 2 
describes the corpus study, including experimental 
design and tagging of dialogue and student prob-
lem-solving actions.  Section 3 presents analysis 
and results.  Discussion and design implications 
are considered in Section 4, and concluding re-
marks follow in Section 5.  
 
2 Corpus Study 
The corpus was gathered by logging text-based 
dialogues between tutors and novice computer sci-
ence students.  The learning task was to complete a 
Java programming problem that required students 
to apply fundamental concepts such as iteration, 
modularization, and sequential-access data struc-
tures.  This study was conducted to compare the 
impact of certain corrective cognitive and motiva-
tional tutorial strategies on student learning and 
self-efficacy in human-human tutoring.  Specifi-
cally, the study considered the motivational strate-
gies of praise and reassurance (Lepper et al 1993) 
and the category of informational tutorial utter-
ances termed cognitive feedback (Porayska-Pomsta 
et al 2004, Tan and Biswas 2006) that followed 
questionable student problem-solving action.  Fol-
lowing the approach of Forbes-Riley (2005) and 
others (Marineau et al 2000), utterances from a 
corpus of human-human tutorial dialogues were 
annotated with dialogue acts.  Then, adopting the 
approach proposed by Ohlsson et al (2007), statis-
tical modeling techniques were employed to quan-
tify the relative impact of these different tutorial 
strategies on the outcomes of interest (in this case, 
learning and self-efficacy gains).     
 
2.1 Experimental Design 
Subjects were students enrolled in an introductory 
computer science course and were primarily 
freshman or sophomore engineering majors in dis-
ciplines such as mechanical, electrical, and com-
puter engineering. 
     The corpus was gathered from tutor-student 
interactions between 43 students and 14 tutors dur-
ing a two-week study.  Tutors and students were 
completely blind to each other?s characteristics as 
they worked together remotely from separate labs.  
Tutors observed student problem-solving actions 
54
(e.g., programming, scrolling, executing programs) 
in real time.  Tutors had varying levels of tutoring 
experience, and were not instructed about specific 
tutorial strategies. 
     Subjects first completed a pre-survey including 
items about self-efficacy, attitude toward computer 
science, and attitude toward collaboration.  Sub-
jects then completed a ten item pre-test over spe-
cific topic content.  The tutorial session was 
controlled at 55 minutes for all subjects, after 
which subjects completed a post-survey and post-
test containing variants of the items on the pre- 
versions.   
 
2.2 Problem-Solving Tagging 
The raw corpus contains 4,864 dialogue moves:  
1,528 student utterances and 3,336 tutor utterances.  
As a chronology of tutorial dialogue interleaved 
with student problem-solving (programming) ac-
tions that took place during the tutoring sessions, 
the corpus contains 29,996 programming key-
strokes and 1,277 periods of scrolling ? all per-
formed by students.  Other problem-solving 
actions, such as opening and closing files or run-
ning the program, were sparse and were therefore 
eliminated from the analyses.  Of the 3,336 tutor 
utterances, 1,243 occur directly after ?question-
able? student problem-solving action.  (The notion 
of ?questionable? is defined below.)  This subset of 
tutorial utterances serves as the basis for the tuto-
rial strategy comparison. 
     Student problem-solving actions were logged 
throughout tutoring sessions.  Two actions were 
under consideration for the analysis:  typing in the 
programming interface and scrolling in the pro-
gram editor window.  To interpret the raw logged 
student problem-solving actions, these events were 
automatically tagged using a heuristic measure for 
correctness: if a problem-solving action was a pro-
gramming keystroke (character) that survived until 
the end of the session, this event was tagged prom-
ising, to indicate it was probably correct.  If a prob-
lem-solving act was a programming keystroke 
(character) that did not survive until the end of the 
session, the problem-solving act was tagged ques-
tionable.  Both these heuristics are based on the 
observation that in this tutoring context, students 
solved the problem in a linear fashion and tutors 
did not allow students to proceed past a step that 
had incorrect code in place.  Finally, periods of 
consecutive scrolling were also marked question-
able because in a problem whose entire solution 
fits on one printed page, scrolling was almost uni-
formly undertaken by a student who was confused 
and looking for answers in irrelevant skeleton code 
provided to support the programming task.   
 
2.3 Dialogue Act Tagging 
Because utterances communicate through two 
channels, a cognitive channel and a motiva-
tional/affective channel, each utterance was 
annotated with both a required cognitive dialogue 
tag (Table 1) and an optional motiva-
tional/affective dialogue tag (Table 2).  While no 
single standardized dialogue act tag set has been 
identified for tutorial dialogue, the tags applied 
here were drawn from several schemes in the tuto-
rial dialogue and broader dialogue literature.  A 
coding scheme for tutorial dialogue in the domain 
of qualitative physics influenced the creation of the 
tag set (Forbes-Riley et al 2005), as did the four-
category scheme (Marineau et al 2000).  A more 
expansive general dialogue act tag set alo contrib-
uted commonly occurring acts (Stolcke et al 
2000).  The motivational tags were drawn from 
work by Lepper (1993) on motivational strategies 
of human tutors.   
     Table 1 displays the cognitive subset of this 
dialogue act tag set, while Table 2 displays the mo-
tivational/affective tags.  It should be noted that a 
cognitive tag was required for each utterance, 
while a motivational/affective tag was applied only 
to the subset of utterances that communicated in 
that channel.  If an utterance constituted a strictly 
motivational/affective act, its cognitive channel 
was tagged with EX (EXtra-domain) indicating 
there was no relevant cognitive content.  On the 
other hand, some utterances had both a cognitive 
component and a motivational/affective compo-
nent.  For example, a tutorial utterance of, ?That 
looks great!? would have been tagged as positive 
feedback (PF) in the cognitive channel, and as 
praise (P) in the motivational/affective channel.  In 
contrast, the tutorial move ?That?s right,? would be 
tagged as positive feedback (PF) in the cognitive 
channel and would not be annotated with a motiva-
tional/affective tag.  Table 3 shows an excerpt 
from the corpus with dialogue act tags applied. 
55
     The entire corpus was tagged by a single human 
annotator, with a second tagger marking 1,418 of 
the original 4,864 utterances.  The resulting kappa 
statistics were 0.76 in the cognitive channel and 
0.64 in the motivation channel.   
3 Analysis and Results 
Overall, these tutoring sessions were effective: 
they yielded learning gains (difference between 
posttest and pretest) with mean 5.9% and median 
7.9%, which were statistically significant 
(p=0.038), and they produced self-efficacy gains
Table 1:  Cognitive Channel Dialogue Acts 
56
(difference between pre-survey and post-survey 
scores) with mean 12.1% and median 12.5%, 
which were also statistically significant 
(p<0.0001).  Analyses revealed that statistically 
significant relationships hold between tutorial 
strategy and learning, as well as between tutorial 
strategy and self-efficacy gains.   
 
3.1 Analysis 
First, the values of learning gain and self-efficacy 
gain were grouped into binary categories (?Low?, 
?High?) based on the median value.  We then ap-
plied multiple logistic regression with the gain 
category as the predicted value.  Tutorial strategy, 
incoming self-efficacy rating, and pre-test score 
were predictors in the model.  The binarization 
approach followed by multiple logistic regression 
was chosen over multiple linear regression on a 
continuous response variable because the learning 
instruments (10 items each) and self-efficacy ques-
tionnaires (5 items each) yielded few distinct val-
ues of learning gain, meaning the response variable 
(learning gain and self-efficacy gain, respectively) 
would not have been truly continuous in nature.  
Logistic regression is used for binary response 
variables; it computes the odds of a particular out-
come over another (e.g., ?Having high learning 
gain versus low learning gain?) given one value of 
the predictor variable over another (e.g., ?The cor-
rective tutorial strategy chosen was positive cogni-
tive feedback instead of praise?). 
 
Table 2:  Motivational/Affective Channel Dialogue Acts 
57
3.2 Results 
After accounting for the effects of pre-test score 
and incoming self-efficacy rating (both of which 
were significant in the model with p<0.001), ob-
servations containing tutorial encouragement were 
56% less likely to result in high learning gain than 
observations without explicit tutorial encourage-
ment (p=0.001).  On the other hand, an analogous 
model of self-efficacy gain revealed that tutorial 
encouragement was 57% more likely to result in 
high self-efficacy gain compared to tutorial re-
sponses that had no explicit praise or reassurance 
(p=0.054).  These models suggested that the pres-
ence of tutorial encouragement in response to 
questionable student problem-solving action may 
enhance self-efficacy gain but detract from learn-
ing gain. 
    Another significant finding was that observa-
tions in which the tutor used cognitive feedback 
plus praise were associated with 40% lower likeli-
hood of high learning gain than observations in 
which the tutor used purely cognitive feedback.  
No impact was observed on self-efficacy gain.  
These results suggest that in response to question-
able student problem-solving action, to achieve 
learning gains, purely cognitive feedback is pre-
ferred over cognitive feedback plus praise, while 
self-efficacy gain does not appear to be impacted 
either way. 
     Among students with low incoming self-
efficacy, observations in which the tutor employed 
a standalone motivational act were 300% as likely 
to be in the high self-efficacy gain group as obser-
vations in which the tutor employed a purely cog-
nitive statement or a cognitive statement combined 
with encouragement (p=0.039).  In contrast, among 
students with high initial self-efficacy, a purely 
motivational tactic resulted in 90% lower odds of 
being in the high self-efficacy gain group.  These 
results suggest that standalone praise or reassur-
ance may be useful for increasing self-efficacy 
gain among low initial self-efficacy students, but 
may decrease self-efficacy gain in high initial self-
efficacy students.   
     Considering strictly cognitive feedback, posi-
tive feedback resulted in 190% increased odds of 
high student self-efficacy gain compared to the 
other cognitive strategies (p=0.0057).  Positive 
cognitive feedback did not differ significantly from 
other types of cognitive strategies in a Chi-square 
comparison with respect to learning gains 
(p=0.390).  The models thus suggest when dealing 
with questionable student problem-solving action, 
positive cognitive feedback is preferable to other 
types of cognitive feedback for eliciting self-
efficacy gains, but this type of feedback is not 
Table 3:  Dialogue Excerpts 
58
found to be better or worse than other cognitive 
feedback for effecting learning gains. 
 
4 Discussion 
The study found that the presence of direct tutorial 
praise or encouragement in response to question-
able student problem-solving action increased the 
odds that the student reported high self-efficacy 
gain while lowering the odds of high learning gain.  
The study also found that, with regard to learning 
gains, purely cognitive feedback was preferable to 
cognitive feedback with an explicitly motivational 
component.  These empirical findings are consis-
tent with theories of Lepper et al (1993) who 
found that some cognitive and affective goals in 
tutoring are ?at odds.?  As would be predicted, the 
results also echo recent quantitative results from 
other tutoring domains such as qualitative physics 
(Jackson et al 2007) and river ecosystems (Tan 
and Biswas 2006) that, in general, overt motiva-
tional feedback contributes to motivation but cog-
nitive feedback matters more for learning.   
      Of the corrective tutorial strategies that were 
exhibited in the corpus, positive cognitive feed-
back emerged as an attractive approach for re-
sponding to plausibly incorrect student problem-
solving actions.  Responding positively (e.g., 
?Right?) to questionable student actions is an ex-
ample of indirect correction, which is recognized 
as a polite strategy (e.g., Porayska-Pomsta et al 
2004).  A qualitative investigation of this phe-
nomenon revealed that in the corpus, tutors gener-
ally followed positive feedback in this context with 
more substantive cognitive feedback to address the 
nature of the student?s error.  As such, the positive 
feedback approach seems to have an implicit, yet 
perceptible, motivational component while retain-
ing its usefulness as cognitive feedback. 
    This study found that explicit motivational acts, 
when applied as corrective tutorial approaches, had 
different impacts on different student subgroups.  
Students with low initial self-efficacy appeared to 
benefit more from praise and reassurance than stu-
dents with high initial self-efficacy.  In a prior cor-
pus study to investigate the impact of learner 
characteristics on tutorial dialogue (Boyer et al 
2007), we also found that learners from different 
populations exhibited significantly different dia-
logue profiles.  For instance, high self-efficacy 
students made more declarative statements, or as-
sertions, than low self-efficacy students.  In addi-
tion, tutors paired with high self-efficacy students 
gave more conversational acknowledgments than 
tutors paired with low self-efficacy students, de-
spite the fact that tutors were not made aware of 
any learner characteristics before the tutoring ses-
sion.  Additional dialogue profile differences 
emerged between high and low-performing stu-
dents, as well as between males and females.  To-
gether these two studies suggest that learner 
characteristics influence the structure of tutorial 
dialogue, and that the choice of tutorial strategy 
may impact student subgroups in different ways.             
 
5 Conclusion 
The work reported here represents a first step to-
ward understanding the effects of learner charac-
teristics on task-oriented tutorial dialogue and the 
use of feedback.  Results suggest that positive cog-
nitive feedback may prove to be an appropriate 
strategy for responding to questionable student 
problem-solving actions in task-oriented tutorial 
situations because of its potential for addressing 
the sometimes competing cognitive and affective 
needs of students.  For low self-efficacy students, it 
was found that direct standalone encouragement 
can be used to bolster self-efficacy, but care must 
be used in correctly diagnosing student self-
efficacy because the same standalone encourage-
ment does not appear helpful for high self-efficacy 
students.  These preliminary findings highlight the 
importance of understanding the interaction be-
tween learner characteristics and tutorial strategy 
as it relates to the design of tutorial dialogue sys-
tems. 
     Several directions for future work appear prom-
ising.  First, it will be important to explore the in-
fluence of learner characteristics on tutorial 
dialogue in the presence of surface level informa-
tion about students? utterances.  This line of inves-
tigation is of particular interest given recent results 
indicating that lexical cohesion in tutorial dialogue 
with low-performing students is found to be highly 
correlated with learning (Ward and Litman 2006).   
Second, while the work reported here has consid-
ered a limited set of motivational dialogue acts, 
namely praise and reassurance, future work should 
target an expanded set of affective dialogue acts to 
59
facilitate continued exploration of motivational and 
affective phenomena in this context.  Finally, the 
current results reflect human-human tutoring 
strategies that proved to be effective; however, it 
remains to be seen whether these same strategies 
can be successfully employed in tutorial dialogue 
systems.  Continuing to identify and empirically 
compare the effectiveness of alternative tutorial 
strategies will build a solid foundation for choos-
ing and implementing strategies that consider 
learner characteristics and successfully balance the 
cognitive and affective concerns surrounding the 
complex processes of teaching and learning 
through tutoring. 
Acknowledgments 
The authors wish to thank Scott McQuiggan and 
the members of the Intellimedia Center for Intelli-
gent Systems for their ongoing intellectual contri-
butions, and the Realsearch Group at NC State 
University for extensive project development sup-
port.  This work was supported in part by the Na-
tional Science Foundation through Grant REC-
0632450, an NSF Graduate Research Fellowship, 
and the STARS Alliance Grant CNS-0540523.  
Any opinions, findings, conclusions or recommen-
dations expressed in this material are those of the 
author(s) and do not necessarily reflect the views 
of the National Science Foundation.  Support was 
also provided by North Carolina State University 
through the Department of Computer Science and 
the Office of the Dean of the College of Engineer-
ing.   
 
References  
Kristy Elizabeth Boyer, Robert Phillips, Michael Wallis, 
Mladen Vouk, and James Lester.  In press.  Balanc-
ing cognitive and motivational scaffolding in tutorial 
dialogue.  To appear in Proceedings of the 9th Inter-
national Conference on Intelligent Tutoring Systems. 
Kristy Elizabeth Boyer, Mladen Vouk, and James Les-
ter.  2007.  The influence of learner characteristics on 
task-oriented tutorial dialogue.  Proceedings of 
AIED, pp. 127-134.  IOS Press. 
Vincent Aleven, Kenneth R. Koedinger, and Octav 
Popescu.  2003.  A tutorial dialog system to support 
self-explanation: Evaluation and open questions.  
Proceedings of the 11th International Conference on 
Artificial Intelligence in Education, pp. 39-46.  Am-
sterdam.  IOS Press. 
Vincent Aleven, Bruce McLaren, Ido Roll, and Kenneth 
Koedinger.  2004.  Toward tutoring help seeking: 
Applying cognitive modeling to meta-cognitive 
skills.  J. C. Lester, R. M. Vicari, and F. Paragua?u 
(Eds.), Proceedings of the 7th International Confer-
ence on Intelligent Tutoring Systems, pp. 227-239.  
Berlin: Springer Verlag. 
Albert Bandura.  2006.  Guide for constructing self-
efficacy scales.  T. Urdan and F. Pajares (Eds.): Self-
Efficacy Beliefs of Adolescents, pp. 307-337.  Infor-
mation Age Publishing, Greenwich, Connecticut. 
Michelene T. H. Chi, Nicholas De Leeuw, Mei-Hung 
Chiu, and Christian LaVancher.  1994.  Eliciting self-
explanations improves understanding.  Cognitive Sci-
ence, 18:439-477. 
Michelene T. H. Chi, Stephanie A. Siler, Heisawn 
Jeong, Takashi Yamauchi, and Robert G. Hausmann.  
2001.  Learning from human tutoring.  Cognitive Sci-
ence, 25(4):471-533. 
Mark G. Core, Johanna D. Moore, and Claus Zinn.  
2003.  The role of initiative in tutorial dialogue.  Pro-
ceedings of the Tenth Conference on European 
Chapter of the Association for Computational Lin-
guistics, pp. 67-74. 
Teresa del Soldato and Benedict du Boulay.  1995.  Im-
plementation of motivational tactics in tutoring sys-
tems.  Journal of Artificial Intelligence in Education, 
6(4):337-378.  Association for the Advancement of 
Computing in Education, USA. 
Martha Evens and Joel Michael.  2006.  One-on-One 
Tutoring by Humans and Computers.  Mahwah, New 
Jersey: Lawrence Erlbaum Associates. 
Kate Forbes-Riley and Diane Litman.  2005.  Using 
bigrams to identify relationships between student cer-
tainness states and tutor responses in a spoken dia-
logue corpus.  Proceedings of the 6th SIGdial 
Workshop on Discourse and Dialogue.  Lisbon, Por-
tugal. 
Kate Forbes-Riley, Diane Litman, Alison Huettner, and 
Arthur Ward.  2005.  Dialogue-learning correlations 
in spoken dialogue tutoring.  Looi, C-k., Mccalla, G., 
Bredeweg, B., Breuker, J. (Eds.): Proceedings of 
AIED, pp. 225-232.  IOS Press. 
Arthur C. Graesser, George T. Jackson, Eric Mathews, 
Heather H. Mitchell, Andrew Olney, Mathew Ven-
tura, Patrick Chipman, Donald R. Franceschetti, 
Xiangen Hu, Max M. Louwerse, Natalie K. Person, 
and the Tutoring Research Group.  2003.  
Why/AutoTutor: A test of learning gains from a 
physics tutor with natural language dialog.  Proceed-
ings of the Twenty-Fifth Annual Conference of the 
Cognitive Science Society, pp. 474-479. 
Arthur C. Graesser, Natalie K. Person, and Joseph P. 
Magliano.  1995.  Collaborative dialogue patterns in 
naturalistic one-to-One tutoring.  Applied Cognitive 
Psychology, 9(6):495-522.  John Wiley & Sons, Ltd. 
60
G. Tanner Jackson and Art Graesser.  2007.  Content 
matters: An investigation of feedback categories 
within an ITS.  Luckin, R., Koedinger, K. R., Greer, 
J. (Eds.): Proceedings of AIED 2007, 158:127-134.  
IOS Press. 
Sandra Katz, David Allbritton, and John Connelly.  
2003.  Going beyond the problem given: How human 
tutors use post-solution discussions to support trans-
fer.  International Journal of Artificial Intelligence in 
Education, 13:79-116. 
John M. Keller.  1983.  Motivational design of instruc-
tion.  Reigeluth, C.M. (Ed.): Instructional-Design 
Theories and Models: An Overview of Their Current 
Status, pp. 383-429.  Lawrence Erlbaum Associates, 
Inc., Hillsdale, NJ. 
H. Chad Lane and Kurt VanLehn.  2005.  Teaching the 
tacit knowledge of programming to novices with 
natural language tutoring.  Computer Science Educa-
tion, 15:183-201. 
Mark R. Lepper, Maria Woolverton, Donna L. Mumme, 
and Jean-Luc Gurtner.  1993.  Motivational tech-
niques of expert human tutors: Lessons for the design 
of computer-based tutors.  Lajoie, S.P., Derry, S. J. 
(Eds.): Computers as Cognitive Tools, pp. 75-105. 
Lawrence Erlbaum Associates, Inc., Hillsdale NJ. 
Jackson Liscombe, Julia Hirschberg, and Jennifer J. 
Venditti.  2005.  Detecting certainness in spoken tu-
torial dialogues.  Proceedings of Interspeech, 2005. 
Diane J. Litman, Carolyn P. Ros?, Kate Forbes-Riley, 
Kurt VanLehn, Dumisizwe Bhembe, and Scott Silli-
man.  2006.  Spoken versus typed human and com-
puter dialogue tutoring.  International Journal of 
Artificial Intelligence in Education, 16:145-170. 
Johanna Marineau, Peter Wiemer-Hastings, Derek 
Harter, Brent Olde, Patrick Chipman, Ashish Kar-
navat, Victoria Pomeroy, Sonya Rajan, Art Graesser, 
and the Tutoring Research Group.  2000.  Classifica-
tion of speech acts in tutorial dialog.  Proceedings of 
the Workshop on Modeling Human Teaching Tactics 
and Strategies of ITS 2000, pp. 65-71.  Montreal, 
Canada. 
Stellan Ohlsson, Barbara Di Eugenio, Bettina Chow, 
Davide Fossati, Xin Lu, and Trina C. Kershaw.  
2007.  Beyond the code-and-count analysis of tutor-
ing dialogues.  Luckin, R., Koedinger, K. R., Greer, 
J. (Eds.): Proceedings of AIED 2007, 158:349-356.  
IOS Press. 
Natalie K. Person, Roger J. Kreuz, Rolf A. Zwaan, and 
Arthur C. Graesser.  1995.  Pragmatics and peda-
gogy: Conversational rules and politeness strategies 
may inhibit effective tutoring.  Cognition and In-
struction, 13(2):161-188.  Lawrence Erlbaum Asso-
ciates, Inc., Hillsdale, NJ. 
Heather Pon-Barry, Karl Schultz, Elizabeth Owen Bratt, 
Brady Clark, and Stanley Peters.  2006.  Responding 
to student uncertainty in spoken tutorial dialogue sys-
tems.  International Journal of Artificial Intelligence 
in Education, 16:171-194. 
Ka?ka Porayska-Pomsta and Helen Pain.  2004.  Provid-
ing cognitive and affective scaffolding through teach-
ing strategies:  Applying linguistic politeness to the 
educational context.  J.C. Lester, Vicari, R. M., Para-
gua?u, F. (Eds.): Proceedings of ITS 2004, LNCS 
3220:77-86.  Springer-Verlag Berlin / Heidelberg. 
Genaro Rebolledo-Mendez, Benedict du Boulay, and 
Rosemary Luckin.  2006.  Motivating the learner: an 
empirical evaluation. Ikeda, M., Ashlay, K. D., Chan, 
T.-W. (Eds.): Proceedings of ITS 2006,  LNCS 
4053:545-554.  Springer Verlag Berlin / Heidelberg. 
Carolyn P. Ros?, Dumisizwe Bhembe, Stephanie Siler, 
Ramesh Srivastava, and Kurt VanLehn.  2003.  The 
role of why questions in effective human tutoring.  
Hoppe, U., Verdejo, F., Kay, J. (Eds.): Proceedings 
of AIED 2003, pp. 55-62.  IOS Press. 
Andreas Stolcke, Klaus Ries, Noah Coccaro, Elizabeth 
Shriberg, Rebecca Bates, Daniel Jurafsky, Paul Tay-
lor, Rachel Martin, Carol Van Ess-Dykema, and 
Marie Meteer.  Dialogue act modeling for automatic 
tagging and recognition of conversational speech.  
2000.  Computational Linguistics, 26:339-373. 
Jason Tan and Gautam Biswas.  2006.  The role of feed-
back in preparation for future learning:  A case study 
in learning by teaching environments.  Ikeda, M., 
Ashley, K., Chan, T.-W. (Eds.): Proceedings of ITS 
2006, LNCS 4053:370-381. Springer-Verlag Berlin / 
Heidelberg. 
Kurt VanLehn, Pamela W. Jordan, Carolyn P. Ros?, 
Dumisizwe Bhembe, Michael Bottner, Andy Gaydos, 
Maxim Makatchev, Umarani Pappuswamy, Michael 
Ringenberg, Antonio Roque, Stephanie Siler, and 
Ramesh Srivastava.  2002.  The architecture of 
Why2-Atlas: A coach for qualitative physics essay 
writing.  Proceedings of the 6th International Con-
ference on Intelligent Tutoring Systems, LNCS 
2363:158-167. 
Arthur Ward and Diane Litman.  2006.  Cohesion and 
learning in a tutorial spoken dialog system.  Proceed-
ings of the 19th International FLAIRS (Florida Artifi-
cial Intelligence Research Society) Conference.  
Melbourne Beach, FL. 
Claus Zinn, Johanna D. Moore, and Mark G. Core.  
2002.  A 3-tier planning architecture for managing 
tutorial dialogue.  Intelligent Tutoring Systems, Sixth 
International Conference.  LNCS 2363:574-584.  
Springer-Verlag, London, UK. 
 
 
61
Proceedings of the NAACL HLT Workshop on Innovative Use of NLP for Building Educational Applications, pages 19?26,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Inferring Tutorial Dialogue Structure with Hidden Markov Modeling 
 
 
Kristy 
Elizabeth  
  Boyera 
Eun Young  
 Haa 
     Robert  
Phillipsab 
 Michael     
     D.  
  Wallisab 
Mladen A.  
 Vouka 
James C.  
 Lestera 
 
 
aDepartment of Computer Science, North Carolina State University 
bApplied Research Associates 
Raleigh, NC, USA 
 
{keboyer, eha, rphilli, mdwallis, vouk, lester}@ncsu.edu 
 
 
 
Abstract 
The field of intelligent tutoring systems has 
seen many successes in recent years.  A 
significant remaining challenge is the 
automatic creation of corpus-based tutorial 
dialogue management models.  This paper 
reports on early work toward this goal.  We 
identify tutorial dialogue modes in an 
unsupervised fashion using hidden Markov 
models (HMMs) trained on input 
sequences of manually-labeled dialogue 
acts and adjacency pairs.  The two best-fit 
HMMs are presented and compared with 
respect to the dialogue structure they 
suggest; we also discuss potential uses of 
the methodology for future work. 
1 Introduction 
 
The field of intelligent tutoring systems has made 
great strides toward bringing the benefits of one-
on-one tutoring to a wider population of learners.  
Some intelligent tutoring systems, called tutorial 
dialogue systems, support learners by engaging in 
rich natural language dialogue, e.g., (Graesser et 
al. 2003; Zinn, Moore & Core 2002; Evens & 
Michael 2006; Aleven, Koedinger & Popescu 
2003; Litman et al 2006; Arnott, Hastings & 
Allbritton 2008; VanLehn et al 2002).  However, 
creating these systems comes at a high cost: it 
entails handcrafting each pedagogical strategy the 
tutor might use and then realizing these strategies 
in a dialogue management framework that is also 
custom-engineered for the application.  It is hoped 
that the next generation of these systems can 
leverage corpora of tutorial dialogue in order to 
provide more robust dialogue management models 
that capture the discourse phenomena present in 
effective natural language tutoring.   
The structure of tutorial dialogue has 
traditionally been studied by manually examining 
corpora and focusing on cognitive and 
motivational aspects of tutorial strategies (e.g., 
Lepper et al 1993; Graesser, Person & Magliano 
1995).  While these approaches yielded 
foundational results for the field, such analyses 
suffer from two serious limitations:  manual 
approaches are not easily scalable to different or 
larger corpora, and the rigidity of handcrafted 
dialogue structure tagging schemes may not 
capture all the phenomena that occur in practice.   
In contrast, the stochastic nature of dialogue 
lends itself to description through probabilistic 
models.  In tutorial dialogue, some early work has 
adapted language processing techniques, namely n-
gram analyses, to examine human tutors? responses 
to student uncertainty (Forbes-Riley & Litman 
2005), as well as to find correlations between local 
tutoring strategies and student outcomes (Boyer et 
al. 2008).  However, this work is limited by its 
consideration of small dialogue windows. 
Looking at a broader window of turns is often 
accomplished by modeling the dialogue as a 
Markov decision process.  With this approach, 
19
techniques such as reinforcement learning can be 
used to compare potential policies in terms of 
effectiveness for student learning.  Determining 
relevant feature sets (Tetreault & Litman 2008) 
and conducting focussed experiments for localized 
strategy effectiveness (Chi et al 2008) are active 
areas of research in this line of investigation.  
These approches often fix the dialogue structures 
under consideration in order to compare the 
outcomes associated with those structures or the 
features that influence policy choice.    
    In contrast to treating dialogue structure as a 
fixed entity, one approach for modeling the 
progression of complete dialogues involves 
learning the higher-level structure in order to infer 
succinct probabilistic models of the interaction.  
For example, data-driven approaches for 
discovering dialogue structure have been applied to 
corpora of human-human task-oriented dialogue 
using general models of task structure (Bangalore, 
Di Fabbrizio & Stent 2006).  Encouraging results 
have emerged from using a general model of the 
task structure to inform automatic dialogue act 
tagging as well as subtask segmentation.  
    Our current work examines a modeling 
technique that does not require a priori knowledge 
of the task structure:  specifically, we propose to 
use hidden Markov models (HMMs) (Rabiner 
1989) to capture the structure of tutorial dialogue 
implicit within sequences of tagged dialogue acts.  
Such probablistic inference of discourse structure 
has been used in recent work with HMMs for topic 
identification (Barzilay & Lee 2004) and related 
graphical models for segmenting multi-party 
spoken discourse (Purver et al 2006).  
Analogously, our current work focuses on 
identifying dialogic structures that emerge during 
tutorial dialogue.  Our approach is based on the 
premise that at any given point in the tutorial 
dialogue, the collaborative interaction is ?in? a 
dialogue mode (Cade et al 2008) that characterizes 
the nature of the exchanges between tutor and 
student; these modes correspond to the hidden 
states in the HMM.  Results to date suggest that 
meaningful descriptive models of tutorial dialogue 
can be generated by this simple stochastic 
modeling technique.  This paper focuses on the 
comparison of two first-order HMMs:  one trained 
on sequences of dialogue acts, and the second 
trained on sequences of adjacency pairs.   
 
2 Corpus Analysis 
The HMMs were trained on a corpus of human-
human tutorial dialogue collected in the domain of 
introductory computer science.  Forty-three 
learners interacted remotely with one of fourteen 
tutors through a keyboard-to-keyboard remote 
learning environment yielding 4,864 dialogue 
moves. 
2.1 Dialogue Act Tagging 
The tutoring corpus was manually tagged with 
dialogue acts designed to capture the salient 
characteristics of the tutoring process (Table 1). 
 
Tag Act Example 
Q Question Where should I  
Declare i? 
EQ Evaluation Question How does that look? 
S Statement You need a  
closing brace. 
G Grounding Ok.  
EX Extra-Domain You may use  
your book. 
PF Positive Feedback Yes, that?s right. 
LF Lukewarm Feedback Sort of. 
NF Negative Feedback No, that?s not right. 
Table 1. Dialogue Act Tags 
 
    The correspondence between utterances and 
dialogue act tags is one-to-one; compound 
utterances were split by the primary annotator prior 
to the inter-rater reliability study.1  This dialogue 
act tagging effort produced sequences of dialogue 
acts that have been used in their un-altered forms 
to train one of the two HMMs presented here 
(Section 3).      
2.2 Adjacency Pair Identification 
In addition to the HMM trained on sequences of 
individual dialogue acts, another HMM was 
trained on sequences of dialogue act adjacency 
pairs.  The importance of adjacency pairs is well-
established in natural language dialogue (e.g., 
Schlegoff & Sacks 1973), and adjacency pair 
analysis has illuminated important phenomena in 
tutoring as well (Forbes-Riley et al 2007).  The 
                                                           
1 Details of the study procedure used to collect the corpus, as 
well as Kappa statistics for inter-rater reliability, are reported 
in (Boyer et al 2008). 
20
intuition behind adjacency pairs is that certain 
dialogue acts naturally occur together, and by 
grouping these acts we capture an exchange 
between two conversants in a single structure.  
This formulation is of interest for our purposes 
because when treating sequences of dialogue acts 
as a Markov process, with or without hidden states, 
the addition of adjacency pairs may offer a 
semantically richer observation alphabet.   
    To find adjacency pairs we utilize a ?2 test for 
independence of the categorical variables acti and 
acti+1 for all sequential pairs of dialogue acts that 
occur in the corpus.  Only pairs in which 
speaker(acti) ? speaker(acti+1) were considered.  
Table 2 displays a list of all dependent adjacency 
pairs sorted by descending (unadjusted) statistical 
significance; the subscript on each dialogue act tag 
indicates tutor (t) or student (s). 
    An adjacency pair joining algorithm was applied 
to join statistically significant pairs of dialogue 
acts (p<0.01) into atomic units according to a 
priority determined by the strength of the statistical 
significance.  Dialogue acts that were ?left out? of 
adjacency pair groupings were treated as atomic 
elements in subsequent analysis.  Figure 1 
illustrates the application of the adjacency pair 
joining algorithm on a sequence of dialogue acts 
from the corpus. 
 
 
Figure 1.  DA Sequence Before/After Joining 
3 HMM of Dialogue Structure 
A hidden Markov model is defined by three 
constituents:  1) the set of hidden states (dialogue 
modes), each characterized by its emission 
probability distribution over the possible 
observations (dialogue acts and/or adjacency 
pairs), 2) the transition probability matrix among 
observations (dialogue acts and/or adjacency 
pairs), 2) the transition probability matrix among 
 
acti acti+1 
P(acti+1|   
    acti) 
P(acti+1| 
   ?acti) 
?2 
val p-val 
EQs PFt 0.48 0.07 654 <0.0001 
Gs Gt 0.27 0.03 380 <0.0001 
EXs EXt 0.34 0.03 378 <0.0001 
EQt PFs 0.18 0.01 322 <0.0001 
EQt Ss 0.24 0.03 289 <0.0001 
EQs LFt 0.13 0.01 265 <0.0001 
Qt Ss 0.65 0.04 235 <0.0001 
EQt LFs 0.07 0.00 219 <0.0001 
Qs St 0.82 0.38 210 <0.0001 
EQs NFt 0.08 0.01 207 <0.0001 
EXt EXs 0.19 0.02 177 <0.0001 
NFs Gt 0.29 0.03 172 <0.0001 
EQt NFs 0.11 0.01 133 <0.0001 
Ss Gt 0.16 0.03 95 <0.0001 
Ss PFt 0.30 0.10 90 <0.0001 
St Gs 0.07 0.04 36 <0.0001 
PFs Gt 0.14 0.04 34 <0.0001 
LFs Gt 0.22 0.04 30 <0.0001 
St EQs 0.11 0.07 29 <0.0001 
Gt EXs 0.07 0.03 14 0.002 
St Qs 0.07 0.05 14 0.0002 
Gt Gs 0.10 0.05 9 0.0027 
EQt EQs 0.13 0.08 8 0.0042 
Table 2. All Dependent Adjacency Pairs 
 
hidden states, and 3) the initial hidden state 
(dialogue mode) probability distribution.   
3.1  Discovering Number of Dialogue Modes 
In keeping with the goal of automatically 
discovering dialogue structure, it was desirable to 
learn n, the best number of hidden states for the 
HMM, during modeling.  To this end, we trained 
and ten-fold cross-validated seven models, each 
featuring randomly-initialized parameters, for each 
number of hidden states n from 2 to 15, inclusive.2  
The average log-likelihood fit from ten-fold cross-
                                                           
2 n=15 was chosen as an initial maximum number of states 
because it comfortably exceeded our hypothesized range of 3 
to 7 (informed by the tutoring literature).  The Akaike 
Information Criterion measure steadily worsened above n = 5, 
confirming no need to train models with n > 15. 
21
validation was computed across all seven models 
for each n, and this average log-likelihood ln was 
used to compute the Akaike Information Criterion, 
a maximum-penalized likelihood estimator that 
prefers simpler models (Scott 2002).  This 
modeling approach was used to train HMMs on 
both the dialogue act and the adjacency pair input 
sequences. 
3.2  Best-Fit Models 
The input sequences of individual dialogue acts 
contain 16 unique symbols because each of the 8 
dialogue act tags (Table 1) was augmented with a 
label of the speaker, either tutor or student.  The 
best-fit HMM for this input sequence contains 
nDA=5 hidden states.  The adjacency pair input 
sequences contain 39 unique symbols, including all 
dependent adjacency pairs (Table 2) along with all 
individual dialogue acts because each dialogue act 
occurs at some point outside an adjacency pair.  
The best-fit HMM for this input sequence contains 
nAP=4 hidden states.  In both cases, the best-fit 
number of dialogue modes implied by the hidden 
states is within the range of what is often 
considered in traditional tutorial dialogue analysis 
(Cade et al 2008; Graesser, Person & Magliano 
1995).   
4 Analysis 
Evaluating the impact of grouping the dialogue 
acts into adjacency pairs requires a fine-grained 
examination of the generated HMMs to gain 
insight into how each model interprets the student 
sessions.     
4.1 Dialogue Act HMM 
Figure 2 displays the emission probability 
distributions for the dialogue act HMM.  State 0DA, 
Tutor Lecture,3 is strongly dominated by tutor 
statements with some student questions and 
positive tutor feedback.  State 1DA constitutes 
Grounding/Extra-Domain, a conversational state 
consisting of acknowledgments, backchannels, and 
discussions that do not relate to the computer 
science task.  State 2DA, Student Reflection, 
                                                           
3 For simplicity, the states of each HMM have been named 
according to an intuitive interpretation of the emission 
probability distribution.   
generates student evaluation questions, statements, 
and positive and negative feedback.  State 3DA is 
comprised of tutor utterances, with positive 
feedback occurring most commonly followed by 
statements, grounding, lukewarm feedback, and 
negative feedback.  This state is interpreted as a 
Tutor Feedback mode.  Finally, State 4DA, Tutor 
Lecture/Probing, is characterized by tutor 
statements and evaluative questions with some 
student grounding statements.   
 
 
Figure 2.  Emission Probability Distributions for 
Dialogue Act HMM 
 
    The state transition diagram (Figure 3) illustrates 
that Tutor Lecture (0DA) and Grounding/Extra-
Domain (1DA) are stable states whose probability of 
self-transition is high:  0.75 and 0.79, respectively.  
Perhaps not surprisingly, Student Reflection (2DA) 
is most likely to transition to Tutor Feedback (3DA) 
with probability 0.77.  Tutor Feedback (3DA) 
transitions to Tutor Lecture (0DA) with probability 
0.60, Tutor Lecture/Probing (4DA) with probability 
0.26, and Student Reflection (2DA) with probability 
0.09.  Finally, Tutor Lecture/Probing (4DA) very 
often transitions to Student Reflection (2DA) with 
probability 0.82. 
 
22
 
Figure 3. Transition diagram for dialogue act HMM 
4.2 Adjacency Pair HMM 
Figure 4 displays the emission probability 
distributions for the HMM that was trained on the 
input sequences of adjacency pairs.  State 0AP, 
Tutor Lecture, consists of tutorial statements, 
positive feedback, and dialogue turns initiated by 
student questions.  In this state, student evaluation 
questions occur in adjacency pairs with positive 
tutor feedback, and other student questions are 
answered by tutorial statements.  State 1AP, Tutor 
Evaluation, generates primarily tutor evaluation 
questions, along with the adjacency pair of tutorial 
statements followed by student acknowledgements.  
State 2AP generates conversational grounding and 
extra-domain talk; this Grounding/Extra-Domain 
state is dominated by the adjacency pair of student 
grounding followed by tutor grounding.  State 3AP 
is comprised of several adjacency pairs:  student 
questions followed by tutor answers, student 
statements with positive tutor feedback, and 
student evaluation questions followed by positive 
feedback.  This Question/Answer state also 
generates some tutor grounding and student 
evaluation questions outside of adjacency pairs.   
 
 
Figure 4.  Emission Probability Distributions for 
Adjacency Pair HMM 
 
 
Figure 5. Transition diagram for adjacency pair HMM 
0DA 
3DA 
2DA 
1DA 
4DA 
p > 0.5 
0.1 ? p ? 0.50 
0.05 ? p < 0.1 
0AP 
3AP 
2AP 
1AP 
p > 0.5 
0.1 ? p ? 0.50 
0.05 ? p < 0.1 
23
 4.3 Dialogue Mode Sequences 
In order to illustrate how the above models fit the 
data, Figure 6 depicts the progression of dialogue 
modes that generate an excerpt from the corpus. 
 
 
Figure 6.  Best-fit sequences of hidden states 
In both models, the most commonly-occurring 
dialogue mode is Tutor Lecture, which generates 
45% of observations in the dialogue act model and 
around 60% in the adjacency pair model.  
Approximately 15% of the dialogue act HMM 
observations are fit to each of states Student 
Reflection, Tutor Feedback, and Tutor 
Lecture/Probing.  This model spends the least 
time, around 8%, in Grounding/Extra Domain.  
The adjacency pair model fits approximately 15% 
of its observations to each of Tutor Evaluation and 
Question/Answer, with around 8% in 
Grounding/Extra-Domain.   
4.4 Model Comparison 
While the two models presented here describe the 
same corpus, it is important to exercise caution 
when making direct structural comparisons.  The 
models contain neither the same number of hidden 
states nor the same emission symbol alphabet; 
therefore, our comparison will be primarily 
qualitative.  It is meaningful to note, however, that 
the adjacency pair model with nAP=4 achieved an 
average log-likelihood fit on the training data that 
was 5.8% better than the same measure achieved 
by the dialogue act model with nDA=5, despite the 
adjacency pair input sequences containing greater 
than twice the number of unique symbols.4   
                                                           
4 This comparison is meaningful because the models depicted 
here provided the best fit among all sizes of models trained for 
the same input scenario. 
    Our qualitative comparison begins by examining 
the modes that are highly similar in the two 
models.  State 2AP generates grounding and extra-
domain statements, as does State 1DA.  These two 
states both constitute a Grounding/Extra-Domain 
dialogue mode.  One artifact of the tutoring study 
design is that all sessions begin in this state due to 
a compulsory greeting that signaled the start of 
each session.  More precisely, the initial state 
probability distribution for each HMM assigns 
probability 1 to this state and probability 0 to all 
other states.     
    Another dialogue mode that is structurally 
similar in the two models is Tutor Lecture, in 
which the majority of utterances are tutor 
statements.  This mode is captured in State 0 in 
both models, with State 0AP implying more detail 
than State 0DA because it is certain in the former 
that some of the tutor statements and positive 
feedback occurred in response to student questions.  
While student questions are present in State 0DA, no 
such precise ordering of the acts can be inferred, as 
discussed in Section 1.    
    Other states do not have one-to-one 
correspondence between the two models.  State 
2DA, Student Reflection, generates only student 
utterances and the self-transition probability for the 
state is very low; the dialogue usually visits State 
2DA for one turn and then transitions immediately 
to another state.  Although this aspect of the model 
reflects the fact that students rarely keep the floor 
for more than one utterance at a time in the corpus, 
such quick dialogue mode transitions are 
inconsistent with an intuitive understanding of 
tutorial dialogue modes as meta-structures that 
usually encompass more than one dialogue turn.  
This phenomenon is perhaps more accurately 
captured in the adjacency pair model.  For 
example, the dominant dialogue act of State 2DA is 
a student evaluation question (EQs).  In contrast, 
these dialogue acts are generated as part of an 
adjacency pair by State 3AP; this model joins the 
student questions with subsequent positive 
feedback from the tutor rather than generating the 
question and then transitioning to a new dialogue 
mode.  Further addressing the issue of frequent 
state transitions is discussed as future work in 
Section 6. 
     
24
5 Discussion and Limitations 
Overall, the adjacency pair model is preferable for 
our purposes because its structure lends itself more 
readily to interpretation as a set of dialogue modes 
each of which encompasses more than one 
dialogue move.  This structural property is 
guaranteed by the inclusion of adjacency pairs as 
atomic elements.  In addition, although the set of 
emission symbols increased to include significant 
adjacency pairs along with all dialogue acts, the 
log-likelihood fit of this model was slightly higher 
than the same measure for the HMM trained on the 
sequences of dialogue acts alone.  The remainder 
of this section focuses on properties of the 
adjacency pair model. 
    One promising result of this early work emerges 
from the fact that by applying hidden Markov 
modeling to sequences of adjacency pairs, 
meaningful dialogue modes have emerged that are 
empirically justified.  The number of these 
dialogue modes is consistent with what researchers 
have traditionally used as a set of hypothesized 
tutorial dialogue modes.  Moreover, the 
composition of the dialogue modes reflects some 
recognizable aspects of tutoring sessions:  tutors 
teach through the Tutor Lecture mode and give 
feedback on student knowledge in a Tutor 
Evaluation mode.  Students ask questions and state 
their own perception of their knowledge in a 
Question/Answer mode.  Both parties engage in 
?housekeeping? talk containing such things as 
greetings and acknowledgements, and sometimes, 
even in a controlled environment, extra-domain 
conversation occurs between the conversants in the 
Grounding/Extra-Domain mode.   
    Although the tutorial modes discovered may not 
map perfectly to sets of handcrafted tutorial 
dialogue modes from the literature (e.g., Cade et 
al. 2008), it is rare for such a perfect mapping to 
exist even between those sets of handcrafted 
modes.  In addition, the HMM framework allows 
for succinct probabilistic description of the 
phenomena at work during the tutoring session:  
through the state transition matrix, we can see the 
back-and-forth flow of the dialogue among its 
modes. 
6 Conclusions and Future Work 
Automatically learning dialogue structure is an 
important step toward creating more robust tutorial 
dialogue management systems.  We have presented 
two hidden Markov models in which the hidden 
states are interpreted as dialogue modes for task-
oriented tutorial dialogue.  These models were 
learned in an unsupervised fashion from manually-
labeled dialogue acts.  HMMs offer concise 
stochastic models of the complex interaction 
patterns occurring in natural language tutorial 
dialogue.  The evidence suggests this 
methodology, which as presented requires only a 
sequence of dialogue acts as input, holds promise 
for automatically discovering the structure of 
tutorial dialogue.   
    Future work will involve conducting evaluations 
to determine the benefits gained by using HMMs 
compared to simpler statistical models.  In 
addition, it is possible that more general types of 
graphical models will prove useful in overcoming 
some limitations of HMMs, such as their arbitrarily 
frequent state transitions, to more readily capture 
the phenomena of interest.  The descriptive insight 
offered by these exploratory models may also be 
increased by future work in which the input 
sequences are enhanced with information about the 
surface-level content of the utterance.  In addition, 
knowledge of the task state within the tutoring 
session can be used to segment the dialogue in 
meaningful ways to further refine model structure.   
    It is also hoped that these models can identify 
empirically-derived tutorial dialogue structures that 
can be associated with measures of effectiveness 
such as student learning (Soller & Stevens 2007).  
These lines of investigation could inform the 
development of next-generation natural language 
tutorial dialogue systems.   
Acknowledgments 
Thanks to Marilyn Walker and Dennis Bahler for 
insightful early discussions on the dialogue and machine 
learning aspects of this work, respectively.  This 
research was supported by the National Science 
Foundation under Grants REC-0632450, IIS-0812291, 
CNS-0540523, and GRFP.  Any opinions, findings, and 
conclusions or recommendations expressed in this 
material are those of the authors and do not necessarily 
reflect the views of the National Science Foundation. 
25
 
References 
Aleven, V., K. Koedinger, and O. Popescu. 2003. A 
tutorial dialog system to support self-explanation: 
Evaluation and open questions. Proceedings of the 
11th International Conference on Artificial 
Intelligence in Education: 39-46. 
Arnott, E., P. Hastings, and D. Allbritton. 2008. 
Research methods tutor: Evaluation of a dialogue-
based tutoring system in the classroom. Behavioral 
Research Methods 40(3): 694-698. 
Bangalore, S., Di Fabbrizio, G., and Stent, A. 2006. 
Learning the structure of task-driven human-human 
dialogs.  Proceedings of the 21st International 
Conference on Computational Linguistics and 44th 
Annual Meeting of the ACL: 201-208. 
Barzilay, R., and Lee, L. 2004. Catching the drift: 
Probabilistic content models, with applications to 
generation and summarization.  Proceedings of 
NAACL HLT: 113?120. 
Boyer, K. E., Phillips, R., Wallis, M., Vouk, M., and 
Lester, J. 2008. Balancing cognitive and 
motivational scaffolding in tutorial dialogue.  
Proceedings of the 9th International Conference on 
Intelligent Tutoring Systems: 239-249. 
Cade, W., Copeland, J., Person, N., and D'Mello, S. 
2008. Dialog modes in expert tutoring.  Proceedings 
of the 9th International Conference on Intelligent 
Tutoring Systems: 470-479. 
Chi, M., Jordan, P., VanLehn, K., and Hall, M. 2008. 
Reinforcement learning-based feature selection for 
developing pedagogically effective tutorial dialogue 
tactics.  Proceedings of the 1st International 
Conference  on Educational Data Mining: 258-265. 
Evens, M., and J. Michael. 2006. One-on-one tutoring 
by humans and computers. Lawrence Erlbaum 
Associates, Mahwah, New Jersey. 
Forbes-Riley, K., and Litman, D. J. 2005. Using 
bigrams to identify relationships between student 
certainness states and tutor responses in a spoken 
dialogue corpus. Proceedings of the 6th SIGdial 
Workshop on Discourse and Dialogue: 87-96. 
Forbes-Riley, K., Rotaru, M., Litman, D. J., and 
Tetreault, J. 2007. Exploring affect-context 
dependencies for adaptive system development. 
Proceedings of NAACL HLT: 41-44. 
Graesser, A., G. Jackson, E. Mathews, H. Mitchell, A. 
Olney, M. Ventura, and P. Chipman. 2003. 
Why/AutoTutor: A test of learning gains from a 
physics tutor with natural language dialog. 
Proceedings of the Twenty-Fifth Annual Conference 
of the Cognitive Science Society: 1-6. 
Graesser, A. C., N. K. Person, and J. P. Magliano. 1995. 
Collaborative dialogue patterns in naturalistic one-
to-one tutoring. Applied Cognitive Psychology 9(6): 
495?522. 
Lepper, M. R., M. Woolverton, D. L. Mumme, and J. L. 
Gurtner. 1993. Motivational techniques of expert 
human tutors: Lessons for the design of computer-
based tutors. Pages 75-105 in S. P. Lajoie, and S. J. 
Derry, editors. Computers as cognitive tools. 
Lawrence Erlbaum Associates, Hillsdale, New 
Jersey. 
Litman, D. J., C. P. Ros?, K. Forbes-Riley, K. VanLehn, 
D. Bhembe, and S. Silliman. 2006. Spoken versus 
typed human and computer dialogue tutoring. 
International Journal of Artificial Intelligence in 
Education 16(2): 145-170. 
Purver, M., Kording, K. P., Griffiths, T. L., and 
Tenenbaum, J. B. 2006. Unsupervised topic 
modelling for multi-party spoken discourse.  
Proceedings of the 21st International Conference on 
Computational Linguistics and 44th Annual Meeting 
of the ACL: 17-24. 
Rabiner, L. R. 1989. A tutorial on hidden Markov 
models and selected applications in speech 
recognition. Proceedings of the IEEE 77(2): 257-
286. 
Schlegoff, E., and H. Sacks. 1973. Opening up closings. 
Semiotica 7(4): 289-327. 
Scott, S. L. 2002. Bayesian methods for hidden Markov 
models: Recursive computing in the 21st century. 
Journal of the American Statistical Association 
97(457): 337-352. 
Soller, A., and R. Stevens. 2007. Applications of 
stochastic  analyses for collaborative learning and 
cognitive assessment. Pages 217-253 in G. R. 
Hancock, and K. M. Samuelsen, editors. Advances 
in latent variable mixture models. Information Age 
Publishing. 
Tetreault, J. R., and D. J. Litman. 2008. A 
reinforcement learning approach to evaluating state 
representations in spoken dialogue systems. Speech 
Communication 50(8-9): 683-696. 
VanLehn, K., P. W. Jordan, C. P. Rose, D. Bhembe, M. 
Bottner, A. Gaydos, M. Makatchev, U. 
Pappuswamy, M. Ringenberg, and A. Roque. 2002. 
The architecture of Why2-atlas: A coach for 
qualitative physics essay writing. Proceedings of 
Intelligent Tutoring Systems Conference: 158?167. 
Zinn, C., Moore, J. D., and Core, M. G. 2002. A 3-tier 
planning architecture for managing tutorial dialogue.  
Proceedings of the 6th International Conference on 
Intelligent Tutoring Systems: 574-584. 
26
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1190?1199,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
An Affect-Enriched Dialogue Act Classification Model  for Task-Oriented Dialogue 
Kristy  Elizabeth  Boyer Joseph F. Grafsgaard Eun Young  Ha Robert  Phillips* James C.  Lester  Department of Computer Science North Carolina State University Raleigh, NC, USA  * Dual Affiliation with Applied Research Associates, Inc. Raleigh, NC, USA  {keboyer, jfgrafsg, eha, rphilli, lester}@ncsu.edu 
 
 
Abstract 
Dialogue act classification is a central chal-lenge for dialogue systems. Although the im-portance of emotion in human dialogue is widely recognized, most dialogue act classifi-cation models make limited or no use of affec-tive channels in dialogue act classification. This paper presents a novel affect-enriched dialogue act classifier for task-oriented dia-logue that models facial expressions of users, in particular, facial expressions related to con-fusion. The findings indicate that the affect-enriched classifiers perform significantly bet-ter for distinguishing user requests for feed-back and grounding dialogue acts within textual dialogue. The results point to ways in which dialogue systems can effectively lever-age affective channels to improve dialogue act classification.  1 Introduction Dialogue systems aim to engage users in rich, adaptive natural language conversation. For these systems, understanding the role of a user?s utter-ance in the broader context of the dialogue is a key challenge (Sridhar, Bangalore, & Narayanan, 2009). Central to this endeavor is dialogue act classification, which categorizes the intention be-hind the user?s move (e.g., asking a question, providing declarative information). Automatic dia-logue act classification has been the focus of a 
large body of research, and a variety of approach-es, including sequential models (Stolcke et al, 2000), vector-based models (Sridhar, Bangalore, & Narayanan, 2009), and most recently, feature-enhanced latent semantic analysis (Di Eugenio, Xie, & Serafin, 2010), have shown promise. These models may be further improved by leveraging regularities of the dialogue from both linguistic and extra-linguistic sources. Users? expressions of emotion are one such source. Human interaction has long been understood to include rich phenomena consisting of verbal and nonverbal cues, with facial expressions playing a vital role (Knapp & Hall, 2006; McNeill, 1992; Mehrabian, 2007; Russell, Bachorowski, & Fernandez-Dols, 2003; Schmidt & Cohn, 2001). While the importance of emotional expressions in dialogue is widely recognized, the majority of dia-logue act classification projects have focused either peripherally (or not at all) on emotion, such as by leveraging acoustic and prosodic features of spo-ken utterances to aid in online dialogue act classi-fication (Sridhar, Bangalore, & Narayanan, 2009). Other research on emotion in dialogue has in-volved detecting affect and adapting to it within a dialogue system (Forbes-Riley, Rotaru, Litman, & Tetreault, 2009; L?pez-C?zar, Silovsky, & Griol, 2010), but this work has not explored leveraging affect information for automatic user dialogue act classification. Outside of dialogue, sentiment anal-ysis within discourse is an active area of research (L?pez-C?zar et al, 2010), but it is generally lim-
1190
ited to modeling textual features and not multi-modal expressions of emotion such as facial ac-tions. Such multimodal expressions have only just begun to be explored within corpus-based dialogue research (Calvo & D'Mello, 2010; Cavicchio, 2009).   This paper presents a novel affect-enriched dia-logue act classification approach that leverages knowledge of users? facial expressions during computer-mediated textual human-human dia-logue. Intuitively, the user?s affective state is a promising source of information that may help to distinguish between particular dialogue acts (e.g., a confused user may be more likely to ask a ques-tion). We focus specifically on occurrences of stu-dents? confusion-related facial actions during task-oriented tutorial dialogue.  Confusion was selected as the focus of this work for several reasons. First, confusion is known to be prevalent within tutoring, and its implications for student learning are thought to run deep (Graesser, Lu, Olde, Cooper-Pye, & Whitten, 2005). Second, while identifying the ?ground truth? of emotion based on any external display by a user presents challenges, prior research has demonstrated a correlation between particular faci-al action units and confusion during learning (Craig, D'Mello, Witherspoon, Sullins, & Graesser, 2004; D'Mello, Craig, Sullins, & Graesser, 2006; McDaniel et al, 2007). Finally, automatic facial action recognition technologies are developing rap-idly, and confusion-related facial action events are among those that can be reliably recognized auto-matically (Bartlett et al, 2006; Cohn, Reed, Ambadar, Xiao, & Moriyama, 2004; Pantic & Bartlett, 2007; Zeng, Pantic, Roisman, & Huang, 2009). This promising development bodes well for the feasibility of automatic real-time confusion detection within dialogue systems.  2 Background and Related Work 2.1 Dialogue Act Classification Because of the importance of dialogue act classifi-cation within dialogue systems, it has been an ac-tive area of research for some time. Early work on automatic dialogue act classification modeled dis-course structure with hidden Markov models, ex-perimenting with lexical and prosodic features, and applying the dialogue act model as a constraint to 
aid in automatic speech recognition (Stolcke et al, 2000). In contrast to this sequential modeling ap-proach, which is best suited to offline processing, recent work has explored how lexical, syntactic, and prosodic features perform for online dialogue act tagging (when only partial dialogue sequences are available) within a maximum entropy frame-work (Sridhar, Bangalore, & Narayanan, 2009). A recently proposed alternative approach involves treating dialogue utterances as documents within a latent semantic analysis framework, and applying feature enhancements that incorporate such infor-mation as speaker and utterance duration (Di Eugenio et al, 2010). Of the approaches noted above, the modeling framework presented in this paper is most similar to the vector-based maximum entropy approach of Sridhar et al (2009). Howev-er, it takes a step beyond the previous work by in-cluding multimodal affective displays, specifically facial expressions, as features available to an af-fect-enriched dialogue act classification model. 2.2 Detecting Emotions in Dialogue Detecting emotional states during spoken dialogue is an active area of research, much of which focus-es on detecting frustration so that a user can be automatically transferred to a human dialogue agent (L?pez-C?zar et al, 2010). Research on spo-ken dialogue has leveraged lexical features along with discourse cues and acoustic information to classify user emotion, sometimes at a coarse grain along a positive/negative axis (Lee & Narayanan, 2005). Recent work on an affective companion agent has examined user emotion classification within conversational speech (Cavazza et al, 2010). In contrast to that spoken dialogue research, the work in this paper is situated within textual dialogue, a widely used modality of communica-tion for which a deeper understanding of user af-fect may substantially improve system performance. While many projects have focused on linguistic cues, recent work has begun to explore numerous channels for affect detection including facial ac-tions, electrocardiograms, skin conductance, and posture sensors (Calvo & D'Mello, 2010). A recent project in a map task domain investigates some of these sources of affect data within task-oriented dialogue (Cavicchio, 2009). Like that work, the current project utilizes facial action tagging, for 
1191
which promising automatic technologies exist (Bartlett et al, 2006; Pantic & Bartlett, 2007; Zeng, Pantic, Roisman, & Huang, 2009). However, we leverage the recognized expressions of emotion for the task of dialogue act classification.  2.3 Categorizing Emotions within Dialogue and Discourse Sets of emotion taxonomies for discourse and dia-logue are often application-specific, for example, focusing on the frustration of users who are inter-acting with a spoken dialogue system (L?pez-C?zar et al, 2010), or on uncertainty expressed by students while interacting with a tutor (Forbes-Riley, Rotaru, Litman, & Tetreault, 2007). In con-trast, the most widely utilized emotion frameworks are not application-specific; for example, Ekman?s Facial Action Coding System (FACS) has been widely used as a rigorous technique for coding fa-cial movements based on human facial anatomy (Ekman & Friesen, 1978).  Within this framework, facial movements are categorized into facial action units, which represent discrete movements of mus-cle groups. Additionally, facial action descriptors (for movements not derived from facial muscles) and movement and visibility codes are included. Ekman?s basic emotions (Ekman, 1999) have been used in recent work on classifying emotion ex-pressed within blog text (Das & Bandyopadhyay, 2009), while other recent work (Nguyen, 2010) utilizes Russell?s core affect model (Russell, 2003) for a similar task. During tutorial dialogue, students may not fre-quently experience Ekman?s basic emotions of happiness, sadness, anger, fear, surprise, and dis-gust. Instead, students appear to more frequently experience cognitive-affective states such as flow and confusion (Calvo & D'Mello, 2010). Our work leverages Ekman?s facial tagging scheme to identi-fy a particular facial action unit, Action Unit 4 (AU4), that has been observed to correlate with confusion (Craig, D'Mello, Witherspoon, Sullins, & Graesser, 2004; D'Mello, Craig, Sullins, & Graesser, 2006; McDaniel et al, 2007).   2.4 Importance of Confusion in Tutorial Dia-logue Among the affective states that students experience during tutorial dialogue, confusion is prevalent, and its implications for student learning are signif-
icant. Confusion is associated with cognitive dise-quilibrium, a state in which students? existing knowledge is inconsistent with a novel learning experience (Graesser, Lu, Olde, Cooper-Pye, & Whitten, 2005). Students may express such confu-sion within dialogue as uncertainty, to which hu-man tutors often adapt in a context-dependent fashion (Forbes-Riley et al, 2007). Moreover, im-plementing adaptations to student uncertainty with-in a dialogue system can improve the effectiveness of the system (Forbes-Riley et al, 2009).  For tutorial dialogue, the importance of under-standing student utterances is paramount for a sys-tem to positively impact student learning (Dzikovska, Moore, Steinhauser, & Campbell, 2010). The importance of frustration as a cogni-tive-affective state during learning suggests that the presence of student confusion may serve as a useful constraining feature for dialogue act classi-fication of student utterances. This paper explores the use of facial expression features in this way.  3 Task-Oriented Dialogue Corpus The corpus was collected during a textual human-human tutorial dialogue study in the domain of introductory computer science (Boyer, Phillips, et al, 2010). Students solved an introductory com-puter programming problem and carried on textual dialogue with tutors, who viewed a synchronized version of the students? problem-solving work-space. The original corpus consists of 48 dia-logues, one per student. Each student interacted with one of two tutors. Facial videos of students were collected using built-in webcams, but were not shown to the tutors. Video quality was ranked based on factors such as obscured foreheads due to hats or hair, and improper camera position result-ing in students? faces not being fully captured on the video. The highest-quality set contained 14 videos, and these videos were used in this analysis. They have a total running time of 11 hours and 55 minutes, and include dialogues with three female subjects and eleven male subjects.  3.1 Dialogue act annotation The dialogue act annotation scheme (Table 1) was applied manually. The kappa statistic for inter-annotator agreement on a 10% subset of the corpus was ?=0.80, indicating good reliability.   
1192
Table 1. Dialogue act tags and relative frequencies across fourteen dialogues in video corpus Student Dialogue Act Example Rel. Freq. EXTRA-DOMAIN (EX) Little sleep deprived today .08 GROUNDING (G) Ok or Thanks .21 NEGATIVE FEEDBACK WITH ELABORATION (NE) I?m still confused on what this next for loop is doing. .02 NEGATIVE FEEDBACK (N) I don?t see the diff. .04 POSITIVE FEEDBACK WITH ELABORATION (PE) 
It makes sense now that you explained it, but I never used an else if in any of my other programs .04 POSITIVE FEEDBACK (P) Second part complete. .11 QUESTION (Q) Why couldn?t I have said if (i<5) .11 STATEMENT (S) i is my only index .07 
REQUEST FOR FEEDBACK (RF) So I need to create a new method that sees how many elements are in my array? .16 RESPONSE (RSP) You mean not the length but the contents .14 UNCERTAIN FEEDBACK WITH ELABORATION (UE) I?m trying to remember how to copy arrays .008 UNCERTAIN FEEDBACK (U) Not quite yet .008  3.2 Task action annotation The tutoring sessions were task-oriented, focusing on a computer programming exercise. The task had several subtasks consisting of programming mod-ules to be implemented by the student. Each of those subtasks also had numerous fine-grained goals, and student task actions either contributed or did not contribute to the goals. Therefore, to obtain a rich representation of the task, a manual annota-tion along two dimensions was conducted (Boyer, Phillips, et al, 2010). First, the subtask structure was annotated hierarchically, and then each task action was labeled for correctness according to the requirements of the assignment. Inter-annotator agreement was computed on 20% of the corpus at the leaves of the subtask tagging scheme, and re-
sulted in a simple kappa of ?=.56. However, the leaves of the annotation scheme feature an implicit ordering (subtasks were completed in order, and adjacent subtasks are semantically more similar than subtasks at a greater distance); therefore, a weighted kappa is also meaningful to consider for this annotation. The weighted kappa is ?weighted=.80. An annotated excerpt of the corpus is displayed in Table 2.   Table 2. Excerpt from corpus illustrating annota-tions and interplay between dialogue and task 13:38:09 Student: How do I know where to end? [RF] 13:38:26 Tutor: Well you told me how to get how many elements in an array by using .length right? 13:38:26 Student: [Task action:  Subtask 1-a-iv, Buggy] 13:38:56 Tutor: Great 13:38:56 Student: [Task action: Subtask 1-a-v, Correct] 13:39:35 Student: Well is it "array.length"? [RF]  **Facial Expression: AU4 13:39:46 Tutor: You just need to use the correct array name 13:39:46 Student: [Task action:  Subtask 1-a-iv, Buggy] 3.3 Lexical and Syntactic Features In addition to the manually annotated dialogue and task features described above, syntactic features of each utterance were automatically extracted using the Stanford Parser (De Marneffe et al, 2006). From the phrase structure trees, we extracted the top-most syntactic node and its first two children. In the case where an utterance consisted of more than one sentence, only the phrase structure tree of the first sentence was considered. Individual word tokens in the utterances were further processed with the Porter Stemmer (Porter, 1980) in the NLTK package (Loper & Bird, 2004). Our prior work has shown that these lexical and syntactic features are highly predictive of dialogue acts dur-ing task-oriented tutorial dialogue (Boyer, Ha et al 2010).  
1193
4 Facial Action Tagging An annotator who was certified in the Facial Ac-tion Coding System (FACS) (Ekman, Friesen, & Hager, 2002) tagged the video corpus consisting of fourteen dialogues. The FACS certification process requires annotators to pass a test designed to ana-lyze their agreement with reference coders on a set of spontaneous facial expressions (Ekman & Rosenberg, 2005). This annotator viewed the vide-os continuously and paused the playback whenever notable facial displays of Action Unit 4 (AU4: Brow Lowerer) were seen. This action unit was chosen for this study based on its correlations with confusion in prior research (Craig, D'Mello, Witherspoon, Sullins, & Graesser, 2004; D'Mello, Craig, Sullins, & Graesser, 2006; McDaniel et al, 2007). To establish reliability of the annotation, a se-cond FACS-certified annotator independently an-notated 36% of the video corpus (5 of 14 dialogues), chosen randomly after stratification by gender and tutor. This annotator followed the same method as the first annotator, pausing the video at any point to tag facial action events. At any given time in the video, the coder was first identifying whether an action unit event existed, and then de-scribing the facial movements that were present. The annotators also specified the beginning and ending time of each event. In this way, the action unit event tags spanned discrete durations of vary-ing length, as specified by the coders. Because the two coders were not required to tag at the same point in time, but rather were permitted the free-dom to stop the video at any point where they felt a notable facial action event occurred, calculating agreement between annotators required discretiz-ing the continuous facial action time windows across the tutoring sessions. This discretization was performed at granularities of 1/4, 1/2, 3/4, and 1 second, and inter-rater reliability was calculated at each level of granularity (Table 3). Windows in which both annotators agreed that no facial action event was present were tagged by default as neu-tral. Figure 1 illustrates facial expressions that dis-play facial Action Unit 4. 
  Table 3. Kappa values for inter-annotator agree-ment on facial action events  Granularity  ? sec ? sec ? sec 1 sec Presence of AU4 (Brow Lowerer)  .84 .87 .86 .86   
  
  Figure 1. Facial expressions displaying AU4 (Brow Lowerer)  Despite the fact that promising automatic ap-proaches exist to identifying many facial action units (Bartlett et al, 2006; Cohn, Reed, Ambadar, Xiao, & Moriyama, 2004; Pantic & Bartlett, 2007; Zeng, Pantic, Roisman, & Huang, 2009), manual annotation was selected for this project for two reasons. First, manual annotation is more robust than automatic recognition of facial action units, and manual annotation facilitated an exploratory, comprehensive view of student facial expressions during learning through task-oriented dialogue. Although a detailed discussion of the other emo-tions present in the corpus is beyond the scope of this paper, Figure 2 illustrates some other sponta-neous student facial expressions that differ from those associated with confusion.    
1194
   
  Figure 2. Other facial expressions from the corpus 5 Models The goal of the modeling experiment was to de-termine whether the addition of confusion-related facial expression features significantly boosts dia-logue act classification accuracy for student utter-ances.  5.1 Features We take a vector-based approach, in which the fea-tures consist of the following:  Utterance Features ? Dialogue act features: Manually annotated dialogue act for the past three utterances. These features include tutor dialogue acts, annotated with a scheme analogous to that used to annotate student utterances (Boyer et al, 2009). ? Speaker: Speaker for past three utterances ? Lexical features: Word unigrams ? Syntactic features: Top-most syntactic node and its first two children  Task-based Features ? Subtask: Hierarchical subtask structure for past three task actions (semantic pro-gramming actions taken by student) ? Correctness: Correctness of past three task actions taken by student ? Preceded by task: Indicator for whether the most recent task action immediately pre-ceded the target utterance, or whether it 
was immediately preceded by the last dia-logue move  Facial Expression Features ? AU4_1sec: Indicator for the display of the brow lowerer within 1 second prior to this utterance being sent, for the most recent three utterances ?  AU4_5sec: Indicator for the display of the brow lowerer within 5 seconds prior to this utterance being sent, for the most recent three utterances ? AU4_10sec: Indicator for the display of the brow lowerer within 10 seconds prior to this utterance being sent, for the most recent three utterances  5.2 Modeling Approach A logistic regression approach was used to classify the dialogue acts based on the above feature vec-tors. The Weka machine learning toolkit (Hall et al, 2009) was used to learn the models and to first perform feature selection in a best-first search. Lo-gistic regression is a generalized maximum likeli-hood model that discriminates between pairs of output values by calculating a feature weight vec-tor over the predictors.  The goal of this work is to explore the utility of confusion-related facial features in the context of particular dialogue act types. For this reason, a specialized classifier was learned by dialogue act. 5.3 Classification Results The classification accuracy and kappa for each specialized classifier is displayed in Table 4. Note that kappa statistics adjust for the accuracy that would be expected by majority-baseline chance; a kappa statistic of zero indicates that the classifier performed equal to chance, and a positive kappa statistic indicates that the classifier performed bet-ter than chance. A kappa of 1 constitutes perfect agreement. As the table illustrates, the feature se-lection chose to utilize the AU4 feature for every dialogue act except STATEMENT (S). When consid-ering the accuracy of the model across the ten folds, two of the affect-enriched classifiers exhibit-ed statistically significantly better performance. For GROUNDING (G) and REQUEST FOR FEEDBACK (RF), the facial expression features significantly 
1195
improved the classification accuracy compared to a model that was learned without affective features.  6 Discussion Dialogue act classification is an essential task for dialogue systems, and it has been addressed with a variety of modeling approaches and feature sets. We have presented a novel approach that treats facial expressions of students as constraining fea-tures for an affect-enriched dialogue act classifica-tion model in task-oriented tutorial dialogue. The results suggest that knowledge of the student?s confusion-related facial expressions can signifi-cantly enhance dialogue act classification for two types of dialogue acts, GROUNDING and REQUEST FOR FEEDBACK.   Table 4. Classification accuracy and kappa for spe-cialized DA classifiers. Statistically significant differences (across ten folds, one-tailed t-test) are shown in bold.    Classifier with AU4 Classifier without AU4  Dialogue Act % acc ? % acc ? p-value EX 90.7 .62 89.0 .28 >.05 G 92.6 .76 91 .71 .018 P 93 .49 92.2 .40 >.05 Q 94.6 .72 94.2 .72 >.05 S Not chosen in feat. sel. 93 .22 n/a RF 90.7 .62 88.3 .53 .003 
RSP 93 .68 95 .75 >.05 NE * *  N * * PE * * U * * UE * * *Too few instances for ten-fold cross-validation. 
6.1 Features Selected for Classification Out of more than 1500 features available during feature selection, each of the specialized dialogue act classifiers selected between 30 and 50 features in each condition (with and without affect fea-tures). To gain insight into the specific features that were useful for classifying these dialogue acts, it is useful to examine which of the AU4 history features were chosen during feature selection.  For GROUNDING, features that indicated the presence of absence of AU4 in the immediately preceding utterance, either at the 1 second or 5 se-cond granularity, were selected. Absence of this confusion-related facial action unit was associated with a higher probability of a grounding act, such as an acknowledgement. This finding is consistent with our understanding of how students and tutors interacted in this corpus; when a student experi-enced confusion, she would be unlikely to then make a simple grounding dialogue move, but in-stead would tend to inspect her computer program, ask a question, or wait for the tutor to explain more. For REQUEST FOR FEEDBACK, the predictive features were presence or absence of AU4 within ten seconds of the longest available history (three turns in the past), as well as the presence of AU4 within five seconds of the current utterance (the utterance whose dialogue act is being classified). This finding suggests that there may be some lag between the student experiencing confusion and then choosing to make a request for feedback, and that the confusion-related facial expressions may re-emerge as the student is making a request for feedback, since the five-second window prior to the student sending the textual dialogue message would overlap with the student?s construction of the message itself.    Although the improvements seen with AU4 fea-tures for QUESTION, POSITIVE FEEDBACK, and EXTRA-DOMAIN acts were not statistically reliable, examining the AU4 features that were selected for classifying these moves points toward ways in which facial expressions may influence classifica-tion of these acts (Table 5).      
1196
Table 5. Number of features, and AU4 features selected, for specialized DA classifiers  Dialogue Act # fea-tures selected AU4 features selected G 43 One utterance ago: AU4_1sec, AU4_5sec 
RF 37 Three utterances ago: AU4_10sec Target utterance: AU4_5sec EX 50 Three utterances ago: AU4_1sec P 36 Current utterance: AU4_10sec Q 30 One utterance ago: AU4_5sec  6.2 Implications The results presented here demonstrate that lever-aging knowledge of user affect, in particular of spontaneous facial expressions, may improve the performance of dialogue act classification models. Perhaps most interestingly, displays of confusion-related facial actions prior to a student dialogue move enabled an affect-enriched classifier to rec-ognize requests for feedback with significantly greater accuracy than a classifier that did not have access to the facial action features. Feedback is known to be a key component of effective tutorial dialogue, through which tutors provide adaptive help (Shute, 2008). Requesting feedback also seems to be an important behavior of students, characteristically engaged in more frequently by women than men, and more frequently by students with lower incoming knowledge than by students with higher incoming knowledge (Boyer, Vouk, & Lester, 2007). 6.3 Limitations The experiments reported here have several nota-ble limitations. First, the time-consuming nature of manual facial action tagging restricted the number of dialogues that could be tagged. Although the highest quality videos were selected for annotation, other medium quality videos would have been suf-ficiently clear to permit tagging, which would have increased the sample size and likely revealed sta-tistically significant trends. For example, the per-
formance of the affect-enriched classifier was bet-ter for dialogue acts of interest such as positive feedback and questions, but this difference was not statistically reliable.  An additional limitation stems from the more fundamental question of which affective states are indicated by particular external displays. The field is only just beginning to understand facial expres-sions during learning and to correlate these facial actions with emotions. Additional research into the ?ground truth? of emotion expression will shed additional light on this area. Finally, the results of manual facial action annotation may constitute up-per-bound findings for applying automatic facial expression analysis to dialogue act classification. 7 Conclusions and Future Work Emotion plays a vital role in human interactions. In particular, the role of facial expressions in human-human dialogue is widely recognized. Facial ex-pressions offer a promising channel for under-standing the emotions experienced by users of dialogue systems, particularly given the ubiquity of webcam technologies and the increasing number of dialogue systems that are deployed on webcam-enabled devices. This paper has reported on a first step toward using knowledge of user facial expres-sions to improve a dialogue act classification mod-el for tutorial dialogue, and the results demonstrate that facial expressions hold great promise for dis-tinguishing the pedagogically relevant dialogue act REQUEST FOR FEEDBACK, and the conversational moves of GROUNDING. These early findings highlight the importance of future work in this area. Dialogue act classifica-tion models have not fully leveraged some of the techniques emerging from work on sentiment anal-ysis. These approaches may prove particularly use-ful for identifying emotions in dialogue utterances. Another important direction for future work in-volves more fully exploring the ways in which af-fect expression differs between textual and spoken dialogue. Finally, as automatic facial tagging tech-nologies mature, they may prove powerful enough to enable broadly deployed dialogue systems to feasibly leverage facial expression data in the near future.    
1197
Acknowledgments This work is supported in part by the North Caroli-na State University Department of Computer Sci-ence and by the National Science Foundation through Grants REC-0632450, IIS-0812291, DRL-1007962 and the STARS Alliance Grant CNS-0739216. Any opinions, findings, conclusions, or recommendations expressed in this report are those of the participants, and do not necessarily represent the official views, opinions, or policy of the Na-tional Science Foundation.  References  A. Andreevskaia and S. Bergler. 2008. When specialists and generalists work together: Overcoming do-main dependence in sentiment tagging. Proceed-ings of the Annual Meeting of the Association for Computational Linguistics and Human Language Technologies (ACL HLT), 290-298.  M.S. Bartlett, G. Littlewort, M. Frank, C. Lainscsek, I. Fasel, and J. Movellan. 2006. Fully Automatic Facial Action Recognition in Spontaneous Behav-ior. 7th International Conference on Automatic Face and Gesture Recognition (FGR06), 223-230.  K.E. Boyer, M. Vouk, and J.C. Lester. 2007. The influ-ence of learner characteristics on task-oriented tu-torial dialogue. Proceedings of the International Conference on Artificial Intelligence in Educa-tion, 365?372.  K.E. Boyer, E.Y. Ha, R. Phillips, M.D. Wallis, M. Vouk, and J.C. Lester. 2010. Dialogue act model-ing in a complex task-oriented domain. Proceed-ings of the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), 297-305.  K.E. Boyer, R. Phillips, E.Y. Ha, M.D. Wallis, M.A. Vouk, and J.C. Lester. 2009. Modeling dialogue structure with adjacency pair analysis and hidden Markov models. Proceedings of the Annual Con-ference of the North American Chapter of the As-sociation for Computational Linguistics: Short Papers, 49-52.  K.E. Boyer, R. Phillips, E.Y. Ha, M.D. Wallis, M.A. Vouk, and J.C. Lester. 2010. Leveraging hidden dialogue state to select tutorial moves. Proceed-ings of the NAACL HLT 2010 Fifth Workshop on Innovative Use of NLP for Building Educational Applications, 66-73. R.A. Calvo and S. D?Mello. 2010. Affect Detection: An Interdisciplinary Review of Models, Methods, and Their Applications. IEEE Transactions on Affec-tive Computing, 1(1): 18-37. 
M. Cavazza, R.S.D.L. C?mara, M. Turunen, J. Gil, J. Hakulinen, N. Crook, et al 2010. How was your day? An affective companion ECA prototype. Proceedings of the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), 277-280.  F. Cavicchio. 2009. The modulation of cooperation and emotion in dialogue: the REC Corpus. Proceed-ings of the ACL-IJCNLP 2009 Student Research Workshop, 43-48.  J.F. Cohn, L.I. Reed, Z. Ambadar, J. Xiao, and T. Mori-yama. 2004. Automatic Analysis and Recognition of Brow Actions and Head Motion in Spontaneous Facial Behavior. IEEE International Conference on Systems, Man and Cybernetics, 610-616. S.D. Craig, S. D?Mello, A. Witherspoon, J. Sullins, and A.C. Graesser. 2004. Emotions during learning: The first steps toward an affect sensitive intelli-gent tutoring system. In J. Nall and R. Robson (Eds.), E-learn 2004: World conference on E-learning in Corporate, Government, Healthcare, & Higher Education, 241-250.  D. Das and S. Bandyopadhyay. 2009. Word to sentence level emotion tagging for Bengali blogs. Proceed-ings of the ACL-IJCNLP Conference, Short Pa-pers, 149-152.  S. Dasgupta and V. Ng. 2009. Mine the easy, classify the hard: a semi-supervised approach to automatic sentiment classification. Proceedings of the 46th Annual Meeting of the ACL and the 4th IJCNLP, 701-709.  B. Di Eugenio, Z. Xie, and R. Serafin. 2010. Dialogue Act Classification, Higher Order Dialogue Struc-ture, and Instance-Based Learning. Dialogue & Discourse, 1(2): 1-24.  M. Dzikovska, J.D. Moore, N. Steinhauser, and G. Campbell. 2010. The impact of interpretation problems on tutorial dialogue. Proceedings of the 48th Annual Meeting of the Association for Com-putational Linguistics, Short Papers, 43-48.  S. D?Mello, S.D. Craig, J. Sullins, and A.C. Graesser. 2006. Predicting Affective States expressed through an Emote-Aloud Procedure from AutoTu-tor?s Mixed- Initiative Dialogue. International Journal of Artificial Intelligence in Education, 16(1): 3-28. P. Ekman. 1999. Basic Emotions. In T. Dalgleish and M. J. Power (Eds.), Handbook of Cognition and Emotion. New York: Wiley. P. Ekman, W.V. Friesen. 1978. Facial Action Coding System. Palo Alto, CA: Consulting Psychologists Press. P. Ekman, W.V. Friesen, and J.C. Hager. 2002. Facial Action Coding System: Investigator?s Guide. Salt Lake City, USA: A Human Face. 
1198
P. Ekman and E.L. Rosenberg (Eds.). 2005. What the Face Reveals: Basic and Applied Studies of Spon-taneous Expression Using the Facial Action Cod-ing System (FACS) (2nd ed.). New York: Oxford University Press. K. Forbes-Riley, M. Rotaru, D.J. Litman, and J. Tetreault. 2007. Exploring affect-context depend-encies for adaptive system development. The Con-ference of the North American Chapter of the Association for Computational Linguistics and Human Language Technologies (NAACL HLT), Short Papers, 41-44.  K. Forbes-Riley, M. Rotaru, D.J. Litman, and J. Tetreault. 2009. Adapting to student uncertainty improves tutoring dialogues. Proceedings of the 14th International Conference on Artificial Intelli-gence in Education (AIED), 33-40.  A.C. Graesser, S. Lu, B. Olde, E. Cooper-Pye, and S. Whitten. 2005. Question asking and eye tracking during cognitive disequilibrium: comprehending illustrated texts on devices when the devices break down. Memory & Cognition, 33(7): 1235-1247.  S. Greene and P. Resnik. 2009. More than words: Syn-tactic packaging and implicit sentiment. Proceed-ings of the 2009 Annual Conference of the North American Chapter of the ACL and Human Lan-guage Technologies (NAACL HLT), 503-511.  M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute-mann, and I.H. Witten. 2009. The WEKA data mining software: An update. SIGKDD Explora-tions, 11(1): 10?18.  R. Iida, S. Kobayashi, and T. Tokunaga. 2010. Incorpo-rating extra-linguistic information into reference resolution in collaborative task dialogue. Proceed-ings of the 48th Annual Meeting of the Associa-tion for Computational Linguistics, 1259-1267.  M.L. Knapp and J.A. Hall. 2006. Nonverbal Communi-cation in Human Interaction (6th ed.). Belmont, CA: Wadsworth/Thomson Learning. C.M. Lee, S.S. Narayanan. 2005. Toward detecting emotions in spoken dialogs. IEEE Transactions on Speech and Audio Processing, 13(2): 293-303.  R. L?pez-C?zar, J. Silovsky, and D. Griol. 2010. F2?New Technique for Recognition of User Emotion-al States in Spoken Dialogue Systems. Proceed-ings of the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), 281-288.  B.T. McDaniel, S. D?Mello, B.G. King, P. Chipman, K. Tapp, and A.C. Graesser. 2007. Facial Features for Affective State Detection in Learning Envi-ronments. Proceedings of the 29th Annual Cogni-tive Science Society, 467-472. D. McNeill. 1992. Hand and mind: What gestures reveal about thought. Chicago: University of Chicago Press. 
A. Mehrabian. 2007. Nonverbal Communication. New Brunswick, NJ: Aldine Transaction. T. Nguyen. 2010. Mood patterns and affective lexicon access in weblogs. Proceedings of the ACL 2010 Student Research Workshop, 43-48.  M. Pantic and M.S. Bartlett. 2007. Machine Analysis of Facial Expressions. In K. Delac and M. Grgic (Eds.), Face Recognition, 377-416. Vienna, Aus-tria: I-Tech Education and Publishing. J.A. Russell. 2003. Core affect and the psychological construction of emotion. Psychological Review, 110(1): 145-172. J.A. Russell, J.A. Bachorowski, and J.M. Fernandez-Dols. 2003. Facial and vocal expressions of emo-tion. Annual Review of Psychology, 54, 329-49. K.L. Schmidt and J.F. Cohn. 2001. Human Facial Ex-pressions as Adaptations: Evolutionary Questions in Facial Expression Research. Am J Phys An-thropol, 33: 3-24. V.J. Shute. 2008. Focus on Formative Feedback. Re-view of Educational Research, 78(1): 153-189.  V.K.R Sridar, S. Bangalore, and S.S. Narayanan. 2009. Combining lexical, syntactic and prosodic cues for improved online dialog act tagging. Computer Speech & Language, 23(4): 407-422. Elsevier Ltd.  A. Stolcke, K. Ries, N. Coccaro, E. Shriberg, R. Bates, D. Jurafsky, et al 2000. Dialogue Act Modeling for Automatic Tagging and Recognition of Con-versational Speech. Computational Linguistics, 26(3): 339-373.  C. Toprak, N. Jakob, and I. Gurevych. 2010. Sentence and expression level annotation of opinions in us-er-generated discourse. Proceedings of the 48th Annual Meeting of the Association for Computa-tional Linguistics, 575-584.  T. Wilson, J. Wiebe, and P. Hoffmann. 2009. Recogniz-ing Contextual Polarity: An Exploration of Fea-tures for Phrase-Level Sentiment Analysis. Computational Linguistics, 35(3): 399-433.  Z. Zeng, M. Pantic, G.I. Roisman, and T.S. Huang. 2009. A Survey of Affect Recognition Methods: Audio, Visual, and Spontaneous Expressions. IEEE Transactions on Pattern Analysis and Ma-chine Intelligence, 31(1): 39-58. 
1199
Proceedings of the NAACL HLT 2010 Fifth Workshop on Innovative Use of NLP for Building Educational Applications, pages 66?73,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Leveraging Hidden Dialogue State to Select Tutorial Moves  Kristy Elizabeth    Boyera Robert    Phillipsab Eun Young Haa Michael D.    Wallisab Mladen A.   Vouka James C. Lestera   aDepartment of Computer Science, North Carolina State University bApplied Research Associates Raleigh, NC, USA  {keboyer, rphilli, eha, mdwallis, vouk, lester}@ncsu.edu    Abstract 
A central challenge for tutorial dialogue systems is selecting an appropriate move given the dialogue context. Corpus-based approaches to creating tutorial dialogue management models may facilitate more flexible and rapid development of tutorial dialogue systems and may increase the effectiveness of these systems by allowing data-driven adaptation to learning contexts and to individual learners. This paper presents a family of models, including first-order Markov, hidden Markov, and hierarchical hidden Markov models, for predicting tutor dialogue acts within a corpus. This work takes a step toward fully data-driven tutorial dialogue management models, and the results highlight important directions for future work in unsupervised dialogue modeling. 1 Introduction A central challenge for dialogue systems is selecting appropriate system dialogue moves (Bangalore, Di Fabbrizio, & Stent, 2008; Frampton & Lemon, 2009; Young et al, 2009). For tutorial dialogue systems, which aim to support learners during conceptual or applied learning tasks, selecting an appropriate dialogue move is particularly important because the tutorial approach could significantly influence cognitive and affective outcomes for the learner (Chi, Jordan, VanLehn, & Litman, 2009). The strategies implemented in tutorial dialogue systems have historically been based on handcrafted rules 
derived from observing human tutors (e.g., Aleven, McLaren, Roll, & Koedinger, 2004; Evens & Michael, 2006; Graesser, Chipman, Haynes, & Olney, 2005; Jordan, Makatchev, Pappuswamy, VanLehn, & Albacete, 2006). While these systems can achieve results on par with unskilled human tutors, tutorial dialogue systems have not yet matched the effectiveness of expert human tutors (VanLehn et al, 2007). A more flexible model of strategy selection may enable tutorial dialogue systems to increase their effectiveness by responding adaptively to a broader range of contexts. A promising method for deriving such a model is to learn it directly from corpora of effective human tutoring. Data-driven approaches have shown promise in task-oriented domains outside of tutoring (Bangalore et al, 2008; Hardy et al, 2006; Young et al, 2009), and automatic dialogue policy creation for tutoring has been explored recently (Chi, Jordan, VanLehn, & Hall, 2008; Tetreault & Litman, 2008). Ultimately, devising data-driven approaches for developing tutorial dialogue systems may constitute a key step towards achieving the high learning gains that have been observed with expert human tutors.  The work presented in this paper focuses on learning a model of tutorial moves within a corpus of human-human dialogue in the task-oriented domain of introductory computer science. Unlike the majority of task-oriented domains that have been studied to date, our domain involves the separate creation of a persistent artifact by the user (the student). The modification of this artifact, in our case a computer program, is the focus of the dialogues. Our corpus consists of textual dialogue utterances and a separate synchronous stream of 
66
task actions. Our goal is to extract a data-driven dialogue management model from the corpus, as evidenced by predicting system (tutor) dialogue acts.  In this paper, we present an annotation approach that addresses dialogue utterances and task actions, and we propose a unified sequential representation for these separate synchronous streams of events. We explore the predictive power of three stochastic models ? first-order Markov models, hidden Markov models, and hierarchical hidden Markov models ? for predicting tutor dialogue acts in the unified sequences. By leveraging these models to capture effective tutorial dialogue strategies, this work takes a step toward creating data-driven tutorial dialogue management models. 2 Related Work Much of the research on selecting system dialogue acts relies on a Markov assumption (Levin, Pieraccini, & Eckert, 2000). This formulation is often used in conjunction with reinforcement learning (RL) to derive optimal dialogue policies (Frampton & Lemon, 2009). Sparse data and large state spaces can pose serious obstacles to RL, and recent work aims to address these issues (Ai, Tetreault, & Litman, 2007; Henderson, Lemon, & Georgila, 2008; Heeman, 2007; Young et al, 2009). For tutorial dialogue, RL has been applied to selecting a state space representation that best facilitates learning an optimal dialogue policy (Tetreault & Litman, 2008). RL has also been used to compare specific tutorial dialogue tactic choices (Chi et al, 2008).  While RL learns a dialogue policy through exploration, our work assumes that a flexible, good (though possibly not optimal) dialogue policy is realized in successful human-human dialogues. We extract this dialogue policy by predicting tutor (system) actions within a corpus. Using human dialogues directly in this way has been the focus of work in other task-oriented domains such as finance (Hardy et al, 2006) and catalogue ordering (Bangalore et al, 2008). Like the parse-based models of Bangalore et al, our hierarchical hidden Markov models (HHMM) explicitly capture the hierarchical nesting of tasks and subtasks in our domain. In other work, this level of structure has been studied from a slightly different perspective as conversational game (Poesio & Mikheev, 1998).  
For tutorial dialogue, there is compelling evidence that human tutoring is a valuable model for extracting dialogue system behaviors. The CIRCSIM-TUTOR (Evens & Michael, 2006), ITSPOKE (Forbes-Riley, Rotaru, Litman, & Tetreault, 2007; Forbes-Riley & Litman, 2009), and KSC-PAL (Kersey, Di Eugenio, Jordan, & Katz, 2009) projects have made extensive use of data-driven techniques based on human corpora. Perhaps most directly comparable to the current work are the bigram models of Forbes-Riley et al; we explore first-order Markov models, which are equivalent to bigram models, for predicting tutor dialogue acts.  In addition, we present HMMs and HHMMs trained on our corpus. We found that both of these models outperformed the bigram model for predicting tutor moves. 3 Corpus and Annotation The corpus was collected during a human-human tutoring study in which tutors and students worked to solve an introductory computer programming problem (Boyer et al, in press). The dialogues were effective: on average, students exhibited a 7% absolute gain from pretest to posttest (N=48, paired t-test p<0.0001).  The corpus contains 48 textual dialogues with a separate, synchronous task event stream. Tutors and students collaborated to solve an introductory computer programming problem using an online tutorial environment with shared workspace viewing and textual dialogue. Each student participated in exactly one tutoring session. The corpus contains 1,468 student utterances, 3,338 tutor utterances, and 3,793 student task actions. In order to build the dialogue model, we annotated the corpus with dialogue act tags and task annotation labels. 3.1 Dialogue Act Annotation  We have developed a dialogue act tagset inspired by schemes for conversational speech (Stolcke et al, 2000), task-oriented dialogue (Core & Allen, 1997), and tutoring (Litman & Forbes-Riley, 2006). The dialogue act tags are displayed in Table 1. Overall reliability on 10% of the corpus for two annotators was ?=0.80.   
67
 Table 1. Dialogue act tags 
DA? Description?
Stu.?Rel.?Freq.?
Tut.?Rel.?Freq.? ??ASSESSING?QUESTION?(AQ)? Request?for?feedback?on?task?or?conceptual?utterance.? .20? .11? .91?EXTRA?DOMAIN?(EX)? Asides?not?relevant?to?the?tutoring?task.? .08? .04? .79?GROUNDING?(G)? Acknowledgement/thanks? .26? .06? .92?LUKEWARM?CONTENT?FEEDBACK?(LCF)? Negative?assessment?with?explanation.? .01? .03? .53?LUKEWARM?FEEDBACK?(LF)? Lukewarm?assessment?of?task?action?or?conceptual?utterance.? .02? .03? .49?NEGATIVE?CONTENT?FEEDBACK?(NCF)?
Negative?assessment?with?explanation.? .01? .10? .61?
NEGATIVE?FEEDBACK?(NF)? Negative?assessment?of?task?action?or?conceptual?utterance.? .05? .02? .76?POSITIVE?CONTENT?FEEDBACK?(PCF)? Positive?assessment?with?explanation.? .02? .03? .43?POSITIVE?FEEDBACK?(PF)? Positive?assessment?of?task?action?or?conceptual?utterance.? .09? .16? .81?QUESTION?(Q)? Task?or?conceptual?question.? .09? .03? .85?STATEMENT?(S)? Task?or?conceptual?assertion.? .16? .41? .82?
3.2 Task Annotation The dialogues focused on the task of solving an introductory computer programming problem. The task actions were recorded as a separate but synchronous event stream. This stream included 97,509 keystroke-level user task events. These events were manually aggregated and annotated for subtask structure and then for correctness. The task annotation scheme was hierarchical, reflecting the nested nature of the subtasks. An excerpt from the task annotation scheme is depicted in Figure 1; the full scheme contains 66 leaves. The task annotation scheme was designed to reflect the different depth of possible subtasks nested within the overall task. Each labeled task action was also judged for correctness according to the requirements of the task, with categories CORRECT, BUGGY, INCOMPLETE, and DISPREFERRED (technically 
correct but not accomplishing the pedagogical goals of the task). Each group of task keystrokes that occurred between dialogue utterances was tagged, possibly with many subtask labels, by a human judge. A second judge tagged 20% of the corpus in a reliability study for which one-to-one subtask identification was not enforced (giving judges maximum flexibility to apply the tags). To ensure a conservative reliability statistic, all unmatched subtask tags were treated as disagreements. The resulting unweighted kappa statistic was ?simple= 0.58, but the weighted Kappa ?weighted=0.86 is more meaningful because it takes into account the ordinal nature of the labels that result from sequential subtasks. On task actions for which the two judges agreed on subtask tag, the agreement statistic for correctness was ?simple=0.80. 
 Figure 1. Portion of task annotation scheme 3.3 Adjacency Pair Joining Some dialogue acts establish an expectation for another dialogue act to occur next (Schegloff & Sacks, 1973). Our previous work has found that identifying the statistically significant adjacency pairs in a corpus and joining them as atomic observations prior to model building produces more interpretable descriptive models. The models reported here were trained on hybrid sequences of dialogue acts and adjacency pairs. A full description of the adjacency pair identification methodology and joining algorithm is reported in (Boyer et al, 2009). A partial list of the most highly statistically significant adjacency pairs, 
68
which for this work include task actions, is displayed in Table 2.   Table 2. Subset of significant adjacency pairs CORRECTTASKACTION?CORRECTTASKACTION;??EXTRADOMAINS?EXTRADOMAINT;?GROUNDINGS?GROUNDINGT;?ASSESSINGQUESTIONT?POSITIVEFEEDBACKS;??ASSESSINGQUESTIONS?POSITIVEFEEDBACKT;?QUESTIONT?STATEMENTS;?ASSESSINGQUESTIONT?STATEMENTS;?EXTRADOMAINT?EXTRADOMAINS;?QUESTIONS?STATEMENTT;?NEGATIVEFEEDBACKS?GROUNDINGT;?INCOMPLETETASKACTION?INCOMPLETETASKACTION;?POSITIVEFEEDBACKS?GROUNDINGT;??BUGGYTASKACTION?BUGGYTASKACTION 4 Models We learned three types of models using cross-validation with systematic sampling of training and testing sets. 
4.1 First-Order Markov Model The simplest model we discuss is the first-order Markov model (MM), or bigram model (Figure 2). A MM that generates observation (state) sequence o1o2?ot is defined in the following way. The observation symbols are drawn from the alphabet ?={?1, ?2, ?, ?M}, and the initial probability distribution is ?=[?i] where ?i is the probability of a sequence beginning with observation symbol ?i. The transition probability distribution is A=[aij], where aij is the probability of observation j occurring immediately after observation i. 
 Figure 2. Time-slice topology of MM  We trained MMs on our corpus of dialogue acts and task events using ten-fold cross-validation to produce a model that could be queried for the next predicted tutorial dialogue act given the history.  
4.2 Hidden Markov Model A hidden Markov model (HMM) augments the MM framework, resulting in a doubly stochastic structure (Rabiner, 1989). For a first-order HMM, the observation symbol alphabet is defined as above, along with a set of hidden states S={s1,s2,?,sN}. The transition and initial probability distributions are defined analogously to MMs, except that they operate on hidden states 
rather than on observation symbols (Figure 3). That is, ?=[?i] where ?i is the probability of a sequence beginning in hidden state si. The transition matrix is A=[aij], where aij is the probability of the model transitioning from hidden state i to hidden state j. This framework constitutes the first stochastic layer of the model, which can be thought of as modeling hidden, or unobservable, structure. The second stochastic layer of the model governs the production of observation symbols: the emission probability distribution is B=[bik] where bik is the probability of state i emitting observation symbol k. 
 Figure 3. Time-slice topology of HMM  The notion that dialogue has an overarching unobservable structure that influences the observations is widely accepted. In tutoring, this overarching structure may correspond to tutorial strategies. We have explored HMMs? descriptive power for extracting these strategies (Boyer et al, 2009), and this paper explores the hypothesis that HMMs provide better predictive power than MMs on our dialogue sequences. We trained HMMs on the corpus using the standard Baum-Welch expectation maximization algorithm and applied state labels that reflect post-hoc interpretation (Figure 4).  
 Figure 4. Portion of learned HMM 
69
4.3 Hierarchical Hidden Markov Model Hierarchical hidden Markov models (HHMMs) allow for explicit representation of multilevel stochastic structure. A complete formal definition of HHMMs can be found in (Fine, Singer, & Tishby, 1998), but here we present an informal description.  HHMMs include two types of hidden states: internal nodes, which do not produce observation symbols, and production nodes, which do produce observations. An internal node includes a set of substates that correspond to its potential children, S={s1, s2, ?, sN}, each of which is itself the root of an HHMM. The initial probability distribution ?=[?i] for each internal node governs the probability that the model will make a vertical transition to substate si from this internal node; that is, that this internal node will produce substate si as its leftmost child. Horizontal transitions are governed by a transition probability distribution similar to that described above for flat HMMs. Production nodes are defined by their observation symbol alphabet and an emission probability distribution over the symbols; HHMMs do not require a global observation symbol alphabet. The generative topology of our HHMMs is illustrated in Figure 5. 
 Figure 5. Generative topology of HHMM  HHMMs of arbitrary topology can be trained using a generalized version of the Baum-Welch algorithm (Fine et al, 1998). Our HHMMs featured a pre-specified model topology based on known task/subtask structure. A Bayesian view of a portion of the best-fit HHMM is depicted in Figure 6.  This model was trained using five-fold cross-validation to address the absence of symbols from the training set that were present in the testing set, a sparsity problem that arose from splitting the data hierarchically. 
Figure 6. Portion of learned HHMM 
70
5 Results We trained and tested MMs, HMMs, and HHMMs on the corpus and compared prediction accuracy for tutorial dialogue acts by providing the model with partial sequences from the test set and querying for the next tutorial move. The baseline prediction accuracy for this task is 41.1%, corresponding to the most frequent tutorial dialogue act (STATEMENT). As depicted in Figure 7, a first-order MM performed worse than baseline (p<0.001)1 at 27% average prediction accuracy (
? 
? ? MM=6%). HMMs performed better than baseline (p<0.0001), with an average accuracy of 48% (
? 
? ? HMM=3%). HHMMs averaged 57% accuracy, significantly higher than baseline (p=0.002) but weakly significantly higher than HMMs (p=0.04), and with high variation (
? 
? ? HHMM=23%). 
 Figure 7. Average prediction accuracies of three model types on tutor dialogue acts  To further explore the performance of the HHMMs, Figure 8 displays their prediction accuracy on each of six labeled subtasks. These subtasks correspond to the top level of the hierarchical task/subtask annotation scheme. The UNDERSTAND THE PROBLEM subtask corresponds to the initial phase of most tutoring sessions, in which the student and tutor agree to some extent on a problem-solving plan. Subtasks 1, 2, and 3 account for the implementation and debugging of three distinct modules within the learning task, and Subtask 4 involves testing and assessing the student?s finalized program. The EXTRA-DOMAIN subtask involves side conversations whose topics are outside of the domain.  The HHMM performed as well as or better (p<0.01) than baseline on the first three in-domain subtasks. The performance on SUBTASK 4 was not distinguishable from baseline (p=0.06); relatively few students reached this subtask. The model did                                                 1 All p-values in this section were produced by two-sample one-tailed t-tests with unequal sample variances. 
not outperform baseline (p=0.40) for the UNDERSTAND THE PROBLEM subtask, and qualitative inspection of the corpus reveals that the dialogue during this phase of tutoring exhibits limited regularities between students.  
 Figure 8. Average prediction accuracies of HHMMs by subtask 6 Discussion The results support our hypothesis that HMMs, because of their capacity for explicitly representing dialogue structure at an abstract level, perform better than MMs for predicting tutor moves. The results also suggest that explicitly modeling hierarchical task structure can further improve prediction accuracy of the model. The below-baseline performance of the bigram model illustrates that in our complex task-oriented domain, an immediately preceding event is not highly predictive of the next move. While this finding may not hold for conversational dialogue or some task-oriented dialogue with a more balanced distribution of utterances between speakers, the unbalanced nature of our tutoring sessions may not be as easily captured.  In our corpus, tutor utterances outnumber student utterances by more than two to one. This large difference is due to the fact that tutors frequently guided students and provided multi-turn explanations, the impetus for which are not captured in the corpus, but rather, involve external pedagogical goals. The MM, or bigram model, has no mechanism for capturing this layer of stochastic behavior. On the other hand, the HMM can account for unobserved influential variables, and the HHMM can do so to an even greater extent by explicitly modeling task/subtask structure. Considering the performance of the HHMM on individual subtasks reveals interesting properties of our dialogues. First, the HHMM is unable to outperform baseline on the UNDERSTAND THE PROBLEM subtask. To address this issue, our ongoing work investigates taking into account 
71
student characteristics such as incoming knowledge level and self-confidence. On all four in-domain subtasks, the HHMM achieved a 30% to 50% increase over baseline. For extra-domain dialogues, which involve side conversations that are not task-related, the HHMM achieved 86% prediction accuracy on tutor moves, which constitutes a 115% improvement over baseline. This high accuracy may be due in part to the fact that out-of-domain asides were almost exclusively initiated by the student, and tutors rarely engaged in such exchanges beyond providing a single response. This regularity likely facilitated prediction of the tutor?s dialogue moves during out-of-domain talk. We are aware of only one recent project that reports extensively on predicting system actions from a corpus of human-human dialogue. Bangalore et al?s (2008) flat task/dialogue model in a catalogue-ordering domain achieved a prediction accuracy of 55% for system dialogue acts, a 175% improvement over baseline. When explicitly modeling the hierarchical task/subtask dialogue structure, they report a prediction accuracy of 35.6% for system moves, approximately 75% above baseline (Bangalore & Stent, 2009). These findings were obtained by utilizing a variety of lexical and syntactic features along with manually annotated dialogue acts and task/subtask labels. In comparison, our HHMM achieved an average 42% improvement over baseline using only annotated dialogue acts and task/subtask labels. In ongoing work we are exploring the utility of additional features for this prediction task. Our best model performed better than baseline by a significant margin. The absolute prediction accuracy achieved by the HHMM was 57% across the corpus, which at first blush may appear too low to be of practical use. However, the choice of tutorial move involves some measure of subjectivity, and in many contexts there may be no uniquely appropriate dialogue act. Work in other domains has dealt with this uncertainty by maintaining multiple hypotheses (Wright Hastie, Poesio, & Isard, 2002) and by mapping to clustered sets of moves rather than maintaining policies for each possible system selection (Young et al, 2009). Such approaches may prove useful in our domain as well, and may help to more fully realize 
the potential of a learned dialogue management model.  7 Conclusion and Future Work Learning models that predict system moves within a corpus is a first step toward building fully data-driven dialogue management models. We have presented Markov models, hidden Markov models, and hierarchical hidden Markov models trained on sequences of manually annotated dialogue acts and task events. Of the three models, the hierarchical models appear to perform best in our domain, which involves an intrinsically hierarchical task/subtask structure.  The models? performance points to promising future work that includes utilizing additional lexical and syntactic features along with fixed user (student) characteristics within a hierarchical hidden Markov modeling framework. More broadly, the results point to the importance of considering task structure when modeling a complex domain such as those that often accompany task-oriented tutoring. Finally, a key direction for data-driven dialogue management models involves learning unsupervised dialogue act and task classification models.   Acknowledgements.  This work is supported in part by the North Carolina State University Department of Computer Science and the National Science Foundation through a Graduate Research Fellowship and Grants CNS-0540523, REC-0632450 and IIS-0812291. Any opinions, findings, conclusions, or recommendations expressed in this report are those of the participants, and do not necessarily represent the official views, opinions, or policy of the National Science Foundation. References  Ai, H., Tetreault, J. R., & Litman, D. J. (2007). Comparing user simulation models for dialog strategy learning. Proceedings of NAACL HLT, Companion Volume, Rochester, New York. 1-4.  Aleven, V., McLaren, B., Roll, I., & Koedinger, K. (2004). Toward tutoring help seeking: Applying cognitive modeling to meta-cognitive skills. Proceedings of ITS, 227-239.  Bangalore, S., Di Fabbrizio, G., & Stent, A. (2008). Learning the structure of task-driven human-human dialogs. IEEE Transactions on Audio, Speech, and Language Processing, 16(7), 1249-1259.  
72
Bangalore, S., & Stent, A. J. (2009). Incremental parsing models for dialog task structure. Proceedings of the EACL, 94-102.  Boyer, K. E., Phillips, R., Ha, E. Y., Wallis, M. D., Vouk, M. A., & Lester, J. C. (2009). Modeling dialogue structure with adjacency pair analysis and hidden Markov models. Proceedings of NAACL HLT (Short Papers), 19-26. Boyer, K. E., Phillips, R., Ingram, A., Ha, E. Y., Wallis, M. D., Vouk, M. A., & Lester, J. C. (In press). Characterizing the effectiveness of tutorial dialogue with hidden Markov models. Proceedings of ITS, Pittsburgh, Pennsylvania.  Chi, M., Jordan, P., VanLehn, K., & Hall, M. (2008). Reinforcement learning-based feature selection for developing pedagogically effective tutorial dialogue tactics. Proceedings of EDM, Montreal, Canada. 258-265.  Chi, M., Jordan, P., VanLehn, K., & Litman, D. (2009). To elicit or to tell: Does it matter? Proceedings of AIED, 197-204.  Core, M., & Allen, J. (1997). Coding dialogs with the DAMSL annotation scheme. AAAI Fall Symposium on Communicative Action in Humans and Machines, 28?35.   Evens, M., & Michael, J. (2006). One-on-one tutoring by humans and computers. Mahwah, New Jersey: Lawrence Erlbaum Associates. Fine, S., Singer, Y., & Tishby, N. (1998). The hierarchical hidden Markov model: Analysis and applications. Machine Learning, 32(1), 41-62.  Forbes-Riley, K., Rotaru, M., Litman, D. J., & Tetreault, J. (2007). Exploring affect-context dependencies for adaptive system development. Proceedings of NAACL HLT (Short Papers), 41-44.  Forbes-Riley, K., & Litman, D. (2009). Adapting to student uncertainty improves tutoring dialogues. Proceedings of AIED, 33-40.  Frampton, M., & Lemon, O. (2009). Recent research advances in reinforcement learning in spoken dialogue systems. The Knowledge Engineering Review, 24(4), 375-408.  Graesser, A. C., Chipman, P., Haynes, B. C., & Olney, A. (2005). AutoTutor: An intelligent tutoring system with mixed-initiative dialogue. IEEE Transactions on Education, 48(4), 612-618.  Hardy, H., Biermann, A., Inouye, R. B., McKenzie, A., Strzalkowski, T., Ursu, C., Webb, N., & Wu, M. (2006). The Amiti?s system: Data-driven techniques for automated dialogue. Speech Communication, 48(3-4), 354-373.  Heeman, P. A. (2007). Combining reinforcement learning with information-state update rules. Proceedings of NAACL HLT, 268-275.  
Henderson, J., Lemon, O., & Georgila, K. (2008). Hybrid reinforcement/supervised learning of dialogue policies from fixed data sets. Computational Linguistics, 34(4), 487-511.  Jordan, P., Makatchev, M., Pappuswamy, U., VanLehn, K., & Albacete, P. (2006). A natural language tutorial dialogue system for physics. Proceedings of FLAIRS, 521-526.  Kersey, C., Di Eugenio, B., Jordan, P., & Katz, S. (2009). KSC-PaL: A peer learning agent that encourages students to take the initiative. Proceedings of the NAACL HLT Workshop on Innovative use of NLP for Building Educational Applications, Boulder, Colorado. 55-63.  Levin, E., Pieraccini, R., & Eckert, W. (2000). A stochastic model of human-machine interaction for learning dialog strategies. IEEE Transactions on Speech and Audio Processing, 8(1), 11-23.  Litman, D., & Forbes-Riley, K. (2006). Correlations between dialogue acts and learning in spoken tutoring dialogues. Natural Language Engineering, 12(2), 161-176.  Poesio, M., & Mikheev, A. (1998). The predictive power of game structure in dialogue act recognition: Experimental results using maximum entropy estimation. Proceedings of ICSLP, 90-97.  Rabiner, L. R. (1989). A tutorial on hidden Markov models and selected applications in speech recognition. Proceedings of the IEEE, 77(2), 257-286.  Schegloff, E., & Sacks, H. (1973). Opening up closings. Semiotica, 7(4), 289-327.  Stolcke, A., Ries, K., Coccaro, N., Shriberg, E., Bates, R., Jurafsky, D., Taylor, P., Martin, R., Van Ess-Dykema, C., & Meteer, M. (2000). Dialogue act modeling for automatic tagging and recognition of conversational speech. Computational Linguistics, 26(3), 339-373.  Tetreault, J. R., & Litman, D. J. (2008). A reinforcement learning approach to evaluating state representations in spoken dialogue systems. Speech Communication, 50(8-9), 683-696.  VanLehn, K., Graesser, A. C., Jackson, G. T., Jordan, P., Olney, A., & Rose, C. P. (2007). When are tutorial dialogues more effective than reading? Cognitive Science, 31(1), 3-62.  Wright Hastie, H., Poesio, M., & Isard, S. (2002). Automatically predicting dialogue structure using prosodic features. Speech Communication, 36(1-2), 63-79.  Young, S., Gasic, M., Keizer, S., Mairesse, F., Schatzmann, J., Thomson, B., & Yu, K. (2009). The hidden information state model: A practical framework for POMDP-based spoken dialogue management. Computer Speech and Language, 24(2), 150-174.  
73
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 297?305,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
Dialogue Act Modeling in a Complex Task-Oriented Domain 
  Kristy Elizabeth Boyer Eun Young Ha Robert Phillips* Michael D. Wallis* Mladen A. Vouk James C. Lester  Department of Computer Science, North Carolina State University Raleigh, North Carolina, USA  *Dual affiliation with Applied Research Associates, Inc. Raleigh, North Carolina, USA  {keboyer,?eha,?rphilli,?mdwallis,?vouk,?lester}@ncsu.edu? Abstract 
Classifying the dialogue act of a user utterance is a key functionality of a dialogue management system. This paper presents a data-driven dialogue act classifier that is learned from a corpus of human textual dialogue. The task-oriented domain involves tutoring in computer programming exercises. While engaging in the task, students generate a task event stream that is separate from and in parallel with the dialogue. To deal with this complex task-oriented dialogue, we propose a vector-based representation that encodes features from both the dialogue and the hierarchically structured task for training a maximum likelihood classifier. This classifier also leverages knowledge of the hidden dialogue state as learned separately by an HMM, which in previous work has increased the accuracy of models for predicting tutorial moves and is hypothesized to improve the accuracy for classifying student utterances. This work constitutes a step toward learning a fully data-driven dialogue management model that leverages knowledge of the user-generated task event stream. 1 Introduction Two central challenges for dialogue systems are interpreting user utterances and selecting system dialogue moves. Recent years have seen an increased focus on data-driven techniques for addressing these challenging tasks (Bangalore et al, 2008; Frampton & Lemon, 2009; Hardy et al, 2006; Sridar et al, 2009; Young et al, 2009). Much of this work utilizes dialogue acts, built on the notion of speech acts (Austin, 1962), which 
provide a valuable intermediate representation that can be used for dialogue management. Data-driven approaches to dialogue act interpretation have included models that take into account a variety of lexical, syntactic, acoustic, and prosodic features for dialogue act tagging (Sridhar et al, 2009; Stolcke et al, 2000). In task-oriented domains, recent work has approached dialogue act classification by learning dialogue management models entirely from human-human corpora (Bangalore et al, 2008; Chotimongkol, 2008; Hardy et al, 2006). Our work adopts this approach for a corpus of human-human dialogue in a task-oriented tutoring domain. Unlike the majority of task-oriented domains that have been studied to date, our domain involves the separate creation of a persistent artifact, in our case a computer program, by the user during the course of the dialogue. Our corpus consists of human-human textual dialogue utterances and a separate, parallel stream of user-generated task actions. We utilize structural features including task/subtask, speaker, and hidden dialogue state along with lexical and syntactic features to interpret user (student) utterances.  This paper makes three contributions. First, it addresses representational issues in creating a dialogue model that integrates task actions with hierarchical task/subtask structure. The task is captured within a separate synchronous event stream that exists in parallel with the dialogue. Second, this paper explores the performance of dialogue act classifiers using different lexical/syntactic and structural feature sets. This comparison includes one model trained entirely on lexical/syntactic features, an important step toward robust unsupervised dialogue act tagging 
297
(Sridhar et al, 2009). Finally, it investigates whether the addition of HMM and task/subtask features improves the performance of the dialogue act classifiers. The findings support this hypothesis for three student dialogue moves, each with important implications for tutorial dialogue.  2 Related Work A variety of modeling approaches have been investigated for statistical dialogue act classification, including sequential approaches and vector-based classifiers. Sequential approaches typically formulate dialogue as a Markov chain in which an observation depends on a finite number of preceding observations. HMM-based approaches make use of the Markov assumption in a doubly stochastic framework that allows fitting optimal dialogue act sequences using the Viterbi algorithm (Rabiner, 1989; Stolcke et al, 2000). Like this work, the approach reported here adopts a first-order Markov formulation to train an HMM on sequences of dialogue acts, but the prediction of this HMM is subsequently encoded in a feature vector for training a vector-based classifier. Vector-based approaches, such as maximum entropy modeling, also frequently take into account both lexical/syntactic and structural features. Lexical and syntactic cues are extracted from local utterance context, while structural features involve longer dialogue act sequences and, in task-oriented domains, task/subtask history. Work by Bangalore et al (2008) on learning the structure of human-human dialogue in a catalogue-ordering domain (also extended to the Maptask and Switchboard corpora) utilizes features including words, part of speech tags, supertags, and named entities, and structural features including dialogue acts and task/subtask labels. In order to perform incremental decoding of dialogue acts and task/subtask structure, they take a greedy approach that does not require the search of complete dialogue sequences. Our work also accomplishes left-to-right incremental interpretation with a greedy approach. Our feature vectors differ from the aforementioned work slightly with respect to lexical/syntactic features and notably in the addition of a set of structural features generated by a separately trained HMM, as described in Section 4.2.  Recent work has explored the use of lexical, syntactic, and prosodic features for online dialogue act tagging (Sridhar et al, 2009); that 
work explores the notion that structural (history) features could be omitted altogether from incremental left-to-right decoding, resulting in computationally inexpensive and robust dialogue act classification. Although our textual dialogue does not feature prosodic cues, we report on the use of lexical/syntactic features alone to perform dialogue act classification, a step toward a fully unsupervised approach.   Like Bangalore et al (2008), we treat task structure as an integral part of the dialogue model. Other work that has taken this approach includes the Amiti?s project, in which a dialogue manager for a financial domain was derived entirely from a human-human corpus (Hardy et al, 2006). The TRIPS dialogue system also closely integrated task and dialogue models, for example, by utilizing the task model to facilitate indirect speech act interpretation (Allen et al, 2001). Work on the Maptask corpus has modeled task structure in the form of conversational games (Wright Hastie et al, 2002). Recent work in task-oriented domains has focused on learning task structure with unsupervised approaches (Chotimongkol, 2008). Emerging unsupervised methods, such as for detecting actions in multi-party discourse, also implicitly capture a task structure (Purver et al, 2006).  Our domain differs from the task-oriented domains described above in that our dialogues center on the user creating a persistent artifact of intrinsic value through a separate, synchronous stream of task actions. To illustrate, consider a catalogue-ordering task in which one subtask is to obtain the customer?s name. The fulfillment of this subtask occurs entirely through the dialogue, and the resulting artifact (a completed order) is produced by the system. In contrast, our task involves the user constructing a solution to a computer programming problem. The fulfillment of this task occurs partially in the dialogue through tutoring, and partially in a separate synchronous stream of user-driven task actions about which the tutor must reason. The stream of user-driven task actions produces an artifact of value in itself (a functioning computer program), and that artifact is the subject of much of the dialogue. We propose a representation that integrates task actions and dialogue acts from these streams into a shared vector-based representation, and we investigate the use of the resulting structural, lexical, and syntactic features for dialogue act classification.  
298
3 Corpus and Annotation The corpus was collected during a controlled human-human tutoring study in which tutors and students worked through textual dialogue to solve an introductory computer programming problem. The dialogues were effective: on average, students exhibited significant learning and self-confidence gains (Boyer et al, 2009).   The corpus contains 48 dialogues each with a separate, synchronous task event stream as depicted in Excerpt 1 of the appendix. There is exactly one dialogue (tutoring session) per student. The corpus captures approximately 48 hours of dialogue and contains 1,468 student utterances and 3,338 tutor utterances. Because the dialogue was textual, utterance segmentation consisted of splitting at existing sentence boundaries when more than one dialogue act was present in the utterance. This segmentation was conducted manually by the principal dialogue act annotator.1  The corpus was manually annotated with dialogue act labels and task/subtask features. Lexical and syntactic features were extracted automatically. The remainder of this section describes the manual annotation. 3.1 Dialogue Act Annotation The dialogue act annotation scheme was inspired by schemes for conversational speech (Stolcke et al, 2000) and task-oriented dialogue (Core & Allen, 1997). It was also influenced by tutoring-specific tagsets (Litman & Forbes-Riley, 2006). Inter-rater reliability for the dialogue act tagging on 10% of the corpus selected via stratified (by tutor) random sampling was ?=0.80. The dialogue act tags, their relative frequencies, and their individual kappa scores from manual annotation are displayed in Table 1.  3.2 Task Annotation All task actions were generated by the student while implementing the solution to an introductory computer programming problem in Java. These task actions were recorded as a separate event stream in parallel with the dialogue corpus. This stream included 97,509 keystroke-level user task events, which were manually aggregated into task/subtask event clusters and annotated for subtask structure and then for correctness. A total of 3,793 aggregated                                                 1 Automatic segmentation is a challenging problem in itself and is left to future work. 
student subtask actions were identified through manual annotation. The task annotation scheme is hierarchical, reflecting the nested nature of the subtasks. A subset of this task annotation scheme is depicted in Figure 1. In the models reported in this paper, the 66 leaves of the task/subtask hierarchy were encoded in the input feature vectors.   Table 1. Student dialogue acts Student?Dialogue?Act? Rel.?Freq.? Human???ACKNOWLEDGMENT?(ACK)? .17? .90?REQUEST?FOR?FEEDBACK?(RF)? .20? .91?EXTRA?DOMAIN?(EX)? .08? .79?GREETING?(GR)? .04? .92?UNCERTAIN?FEEDBACK?WITH?ELABORATION?(UE)? .01? .53?UNCERTAIN?FEEDBACK?(U)? .02? .49?NEGATIVE?FEEDBACK?WITH?ELABORATION?(NE)? .01? .61?NEGATIVE?FEEDBACK?(N)? .05? .76?POSITIVE?FEEDBACK?WITH?ELABORATION?(PE)? .02? .43?POSITIVE?FEEDBACK?(P)? .09? .81?QUESTION?(Q)? .09? .85?STATEMENT?(S)? .16? .82?THANKS?(T)? .05? 1?
 Each group of task events that occurred between dialogue utterances was tagged, possibly with many subtask labels, by a human judge. The judge aggregated the raw task keystrokes and tagged the task/subtask hierarchy for each cluster. (Please see Excerpt 1 in the appendix.) A second judge tagged 20% of the corpus in a reliability study for which one-to-one subtask identification was not enforced, an approach that was intended to give judges maximum flexibility to cluster task actions and subsequently apply the tags. All unmatched subtask tags were treated as disagreements. The resulting kappa statistic at the leaves was ?= 0.58. However, we also observe that the sequential nature of the subtasks within the larger task produces an ordinal relationship between subtasks. For example, in Figure 1, the ?distance? between subtasks 1-a and 1-b can be thought of as ?less than? the distance between subtasks 1-a vs. 3-d because those subtasks are farther from each other within the larger task. The weighted Kappa statistic (Artstein & Poesio, 2008) takes into account such an ordinal relationship and its implicit distance function. The weighted Kappa is 
299
?weighted=0.86, which indicates acceptable inter-rater reliability on the task/subtask annotation. 
 Figure 1. Portion of task annotation scheme  Along with its tag for hierarchical subtask structure, each task event was also judged for correctness according to the requirements of the task as depicted in Table 2. The agreement statistic for correctness was calculated for task events on which the two judges agreed on subtask tag. The resulting unweighted agreement statistic for correctness was ?=0.80.  Table 2. Task correctness labels  Label? Description?CORRECT? Fully? satisfying? the? requirements? of?the? learning? task.? Does? not? require?tutorial?remediation.?BUGGY? Violating? the? requirements? of? the?learning?task.?Often?requires?tutorial?remediation.?INCOMPLETE? Not? violating,? but? not? yet? fully?satisfying,? the? requirements? of? the?learning? task.? May? require? tutorial?remediation.?DISPREFERRED? Technically? satisfying? the?requirements? of? the? learning? task,?but? not? adhering? to? its? pedagogical?intentions.? Usually? requires? tutorial?remediation.?4 Features The vector-based representation for training the dialogue act classifiers integrates several sources of features: lexical and syntactic features, and structural features that include dialogue act labels, task/subtask labels, and set of hidden dialogue state prediction features.   
4.1 Lexical and Syntactic Features Lexical and syntactic features were automatically extracted from the utterances using the Stanford Parser default tokenizer and part of speech (pos) tagger (De Marneffe et al, 2006). The parser created both phrase structure trees and typed dependencies for individual sentences. From the phrase structure trees, we extracted the top-most syntactic node and its first two children. In the case where an utterance consisted of more than one sentence, only the phrase structure tree of the first sentence was considered. Typed dependencies between pairs of words were extracted from each sentence. Individual word tokens in the utterances were further processed with the Porter Stemmer (Porter, 1980) in the NLTK package (Loper & Bird, 2004). The pos features were extracted in a similar way. Unigram and bigram word and pos tags were included for feature selection in the classifiers.   4.2 Structural Features Structural features include the annotated dialogue acts, the annotated task/subtask labels, and attributes that represent the hidden dialogue state. Our previous work has found that a set of hidden dialogue states, which correspond to widely accepted notions of dialogue modes in tutoring, can be identified in an unsupervised fashion (without hand labeling of the modes) by HMMs trained on manually labeled dialogue acts and task/subtask features (Boyer et al, 2009). These HMMs performed significantly better than bigram models for predicting human tutor moves (Boyer et al, 2010), which indicates that the hidden dialogue state leveraged by the HMMs has predictive value even in the presence of ?true? (manually annotated) dialogue act labels. Therefore, we hypothesized that an HMM could also improve the performance of models to classify student dialogue acts. To explore this hypothesis, we trained an HMM utilizing the methodology described in (Boyer et al, 2009) and used it to generate hidden dialogue state predictions in the form of a probability distribution over possible user utterances at each step in the dialogue. This set of stochastic features was subsequently passed to the classifier as part of the input vector (Figure 2).  4.3 Input Vectors The features were combined into a shared vector-based representation for training the classifier. As depicted in Table 3, the components of the 
300
feature vector include binary existence vectors for lexical and syntactic features for the current (target) utterance as well as for three utterances of left context (this left context may include both tutor and student utterances, which are distinguished by a separate indicator for the speaker). The task/subtask and correctness history features encode the separate stream of task events. There is no one-to-one correspondence between these history features and the left-hand dialogue context, because several task events could have occurred between a pair of dialogue events (or vice versa). This distinction is indicated in the table by the representation of dialogue time steps as [t, t-1,?] and task history steps as [task(t), task(t-1),?]. In total, the feature vectors included 11,432 attributes that were made available for feature selection.  
 Figure 2. Generation of hidden dialogue state prediction features 5 Experiments This section describes the learning of maximum likelihood vector-based models for classification of user dialogue acts. In addition to investigating the accuracy of the overall model, we also performed experiments regarding the utility of feature types for discriminating between particular dialogue acts of interest.    The classifiers are based on logistic regression, which learns a discriminant for each pair of dialogue acts by assigning weights in a maximum likelihood fashion. 2  The logistic regression models were learned using the Weka machine learning toolkit (Hall et al, 2009). For                                                 2 In general, the model that maximizes likelihood also maximizes entropy under the same constraints (Berger et al, 1996).  
feature selection, we performed attribute subset evaluation with a best-first approach that greedily searched the space of possible features using a hill climbing approach with backtracking. The prediction accuracy of the classifiers was determined through ten-fold cross-validation on the corpus, and the results below are presented in terms of prediction accuracy (number of correct classifications divided by total number of classifications) as well as by the kappa statistic, which adjusts for expected agreement by chance.    Table 3. Feature vectors 
Feature?vector?f? Description?[wt,1,?wt,|w|, pt,1,?,pt,|p|, dt,1,?,dt,|d|, st,1,?,st,|s|] 
Binary?existence?vector?for?word?unigrams?&?bigrams,?pos?unigrams?&?bigrams,?dependency?types,?and?syntactic?nodes?for?current?target?utterance?t??[wt-k,1,?wt-k,|w|, pt-k,1,?,pt-k,|p|, dt-k,1,?,dt-k,|d|, st-k,1,?,st-k,|s|]  where k=1,?,3 
Binary?existence?vector?for?word?unigrams?&?bigrams,?pos?unigrams?&?bigrams,?dependency?types,?and?syntactic?nodes?for?three?utterances?of?left?context?
[p(o1),?,p(o|S|)] Probability?distribution?for?emission?symbols?in?predicted?next?hidden?state?as?generated?by?HMM??[dat-1, dat-2, dat-3] Dialogue?act?left?context??[spt-1,spt-2, spt-3]? Speaker?label?left?context?[tktask(t-1), tktask(t-2), tktask(t-3)] Three?steps?of?subtask?history?(each?level?of?hierarchy?represented?as?a?separate?feature)??[ctask(t-1), ctask(t-2), ctask(t-3)]  
Three?steps?of?task?correctness?history?
pt Indicator?for?whether?the?target?utterance?was?immediately?preceded?by?a?task?event? 5.1 Overall Classification Task The overall dialogue act classification model was trained to classify each utterance with respect to the thirteen dialogue acts (Table 1). For this task, the feature selection algorithm selected 63 attributes including some syntax, dependency, pos, and word attributes as well as dialogue act, speaker, and task/subtask features. No hidden dialogue state features or task correctness attributes were selected. The overall classification accuracy was 62.8%. This accuracy constitutes a 369% improvement over baseline chance of 17% (the relative frequency of the most frequently occurring dialogue act, ACK). An alternate nontrivial baseline is a bigram model on true dialogue acts (including speaker tags); this model?s accuracy was 36.8%. The 
301
overall kappa for the full classifier was ?=.57. The confusion matrix for this model is depicted in Figure 3.        In addition to the classifier described above, we experimented with classifiers that used only the lexical and syntactic features of each utterance. This approach is of interest in part because it avoids the error propagation that can happen when a model relies on a series of its own previous classifications as features. The classifier that used only the set of lexical and syntactic features achieved a prediction accuracy of 60.2% and ?=.53 using 85 attributes.   
 5.2 Binary Dialogue Act Classification In tutoring, some student dialogue acts are particularly important to identify because of their implications for the tutor?s response or for the student model. For example, a student?s REQUEST FOR FEEDBACK requires the tutor to assess the condition of the task, rather than to query the in-domain factual knowledge base. UNCERTAIN FEEDBACK is another dialogue act of high importance because identifying it allows the tutor to respond in an affectively advantageous way (Forbes-Riley & Litman, 2009).  To explore which features are useful for classifying particular dialogue acts, we constructed binary dialogue act classifiers, one for each dialogue act, by preprocessing the dialogue act labels from the set of thirteen down to TRUE or FALSE depending on whether the label of the utterance matched the target dialogue act for that specialized classifier. Table 4 displays the features that were selected for each binary classifier, along with the percent accuracy and kappa for each model. Note that for some dialogue acts the chance baseline is very high, and therefore even a model with high prediction accuracy achieves a low kappa.         As depicted in Table 4, for several dialogue act models, the feature selection algorithm retained subtask and HMM features.   
Table 4. Binary DA classifiers  
DA? #?Features?Selected? %?Correct? Model???
ACK? 51? Lexical/syntax,?HMM,?DA?history?(preceding=S),?speaker?history?(preceding=Tutor)?? .933? .75?RF? 42? Lexical/syntax,?DA?history,?preceded?by?subtask? .905? .72?
EX? 57? Dependency,?pos,?word,?HMM,?DA?history?(preceding=EX),?subtask? .939? .45?
GR? 11? Syntax,?pos,?word,?DA?(previous=EMPTY),?speaker,?subtask?? .998? .97?UE? 21? Dependency,?pos,?word,?subtask? .991? .33?U? 63? Syntax,?dependency,?pos,?word,?HMM,?subtask? .979? .21?
NE? 44? Dependency,?pos,?word,?HMM,?DA?history?(2?ago=UNCERTAIN),?subtask? .987? 0?N? 83? Lexical/syntax,?DA?history,?subtask? .966? .76?PE? 90? Dependency,?pos,?word,?HMM,?subtask? .976? .10?
P? 110? Dependency,?pos,?word,?HMM,?DA?history?(previous=REQUEST?FEEDBACK)? .945? .58?Q? 43? Syntax,?dep,?pos,?word,?HMM,?subtask? .940? .60?S? 92? Syntax,?pos,?word,?HMM,?DA?history?(previous=EMPTY?or?Q)? .901? .57?
T? 29? Syntax,?pos,?word,?DA?history?(previous=POSITIVE)?(3?ago=POSITIVE)? .992? .92?    In an experiment to quantify the utility of these features, it was found that for many dialogue acts, a binary dialogue act classifier that was trained using only lexical and syntactic features achieved the same or better classification accuracy than the model that was given all features (Figure 4). For comparison, the modified baseline model used the last three true dialogue acts (with speaker tags); this model achieved better than chance for four dialogue acts and achieved nearly as well as the full model for GREETING (GR). The models that were given all possible features for selection outperformed the lexical/syntax-only model for seven of the thirteen dialogue acts (GREETING (GR), REQUEST FOR FEEDBACK (RF), POSITIVE FEEDBACK (P), POSITIVE ELABORATED FEEDBACK (PE), UNCERTAIN ELABORATED FEEDBACK (UE), NEGATIVE FEEDBACK (N), and EXTRA-DOMAIN (EX)); however, it should be noted that none of these differences in performance is statistically reliable at the p=0.05 level.   
Figure 3. Confusion matrix 
302
 Figure 4. Kappa for binary DA classifiers by features available for selection 6 Discussion We have presented a maximum likelihood classifier that assigns dialogue act labels to user utterances from a corpus of human-human tutorial dialogue given a set of lexical, syntactic, and structural features. Overall, this classifier achieved 62.8% accuracy in ten-fold cross-validation on the corpus. This performance is on par with other automatic dialogue act tagging models, both sequential and vector-based, in task-oriented domains that do not feature complex, user-driven parallel tasks. In a catalogue ordering domain with an integrated task and dialogue model, Bangalore et al (2009) report 75% classification accuracy for user utterances using a maximum entropy classifier, a 275% improvement over baseline. Poesio & Mikheev (1998) report 54% classification accuracy by utilizing conversational game structure and speaker changes in the Maptask corpus, an improvement of 170% over baseline. Recent work on Maptask reports a classification accuracy of 65.7% using local utterance (such as lexical/syntactic) features alone, with prosodic cues yielding further slight improvement (Sridhar et al, 2009). This classifier is analogous to our lexical/syntactic feature model, which achieved 60.2% accuracy. The results of these models demonstrate that, consistent with the findings in other task-oriented domains, lexical/syntactic features are highly useful for classifying student dialogue moves in this complex task-oriented domain. Models trained using those lexical/syntactic features 
performed almost universally better (with the exception of the binary classifier for GREETING) than models that were given the same left context of true dialogue act tags.  It was hypothesized that leveraging both the hidden dialogue state and hierarchical subtask features would improve the performance of the classifiers. There is some evidence that the subtask structure was helpful for the overall classifier; however, no HMM features were kept during feature selection for the overall model. Of the binary models, approximately half performed better than the overall model in terms of true positive rate; of those, three did so by including HMM or task/subtask features among the selected attributes to differentiate different tones of student feedback. However, this difference in performance was not statistically reliable. This finding suggests that, given lexical and syntactic features which are strong predictors of dialogue acts, the hidden dialogue state as captured by an an HMM may not contribute significantly to the dialogue act classification task. 7 Conclusion and Future Work Dialogue modeling for complex task-oriented domains poses significant challenges. An effective dialogue model allows systems to detect user dialogue acts so that they can respond in a manner that maximizes the chance of success. Experiments with the data-driven classifiers presented in this paper demonstrate that lexical/syntactic features can effectively classify student dialogue acts in the task-oriented tutoring domain. For POSITIVE, NEGATIVE, and UNCERTAIN ELABORATED student feedback acts, which play a key role in tutorial dialogue system, the addition of hidden dialogue state features (as learned by an HMM) and task/subtask features (annotated manually) improve classification accuracy, but not statistically reliably.    The overarching goal of this work is to create a data-driven tutorial dialogue system that learns its behavior from corpora of effective human tutoring. The dialogue act classification models reported here constitute an important step toward that goal, by integrating the dialogue stream with a parallel user-driven task event stream. The next generation of data-driven systems should leverage models that capture the rich interplay between dialogue and task. Future work will focus on data-driven approaches to task recognition and tutorial planning. Additionally, as dialogue system research addresses 
303
increasingly complex task-oriented domains, it becomes increasingly important to investigate unsupervised approaches for dialogue act classification and task recognition.   Acknowledgements.  This work is supported in part by the North Carolina State University Department of Computer Science and the National Science Foundation through a Graduate Research Fellowship and Grants CNS-0540523, REC-0632450 and IIS-0812291. Any opinions, findings, conclusions, or recommendations expressed in this report are those of the participants, and do not necessarily represent the official views, opinions, or policy of the National Science Foundation. References  Allen, J., Ferguson, G., & Stent, A. (2001). An architecture for more realistic conversational systems. Proceedings of the IUI, 1-8.  Artstein, R., & Poesio, M. (2008). Inter-coder agreement for computational linguistics. Computational Linguistics, 34(4), 555-596.  Austin, J. L. (1962). How to do things with words. Oxford: Oxford University Press. Bangalore, S., Di Fabbrizio, G., & Stent, A. (2008). Learning the structure of task-driven human-human dialogs. IEEE Transactions on Audio, Speech, and Language Processing, 16(7), 1249-1259.  Berger, A. L., Pietra, V. J. D., & Pietra, S. A. D. (1996). A maximum entropy approach to natural language processing. Comp. Ling., 22(1), 71.  Boyer, K. E., Phillips, R., Ha, E. Y., Wallis, M. D., Vouk, M. A., & Lester, J. C. (2009). Modeling dialogue structure with adjacency pair analysis and hidden markov models. Proceedings of NAACL-HLT, Short Papers, 49-52.  Boyer, K. E., Phillips, R., Ha, E. Y., Wallis, M. D., Vouk, M. A., & Lester, J. C. (2010). Leveraging hidden dialogue state to select tutorial moves. Proceedings of the 5th NAACL HLT Workshop on Innovative use of NLP for Building Educational Applications, Los Angeles, California.  Chotimongkol, A. (2008). Learning the structure of task-oriented conversations from the corpus of in-domain dialogs. (Unpublished Ph.D. Dissertation). Carnegie Mellon University School of Computer Science. Core, M., & Allen, J. (1997). Coding dialogs with the DAMSL annotation scheme. AAAI Fall Symposium on Communicative Action in Humans and Machines, 28?35.  De Marneffe, M. C., MacCartney, B., & Manning, C. D. (2006). Generating typed dependency parses 
from phrase structure parses. Proceedings of LREC, Genoa, Italy.   Forbes-Riley, K., & Litman, D. (2009). Adapting to student uncertainty improves tutoring dialogues. Proceedings of AIED, 33-40.  Frampton, M., & Lemon, O. (2009). Recent research advances in reinforcement learning in spoken dialogue systems. The Knowledge Engineering Review, 24(4), 375-408.  Hall, M., Frank, E., Holmes, G., Pfahringer, B., Reutemann, P., & Witten, I. (2009). The WEKA data mining software: An update. SIGKDD Explorations, 11(1) Hardy, H., Biermann, A., Inouye, R. B., McKenzie, A., Strzalkowski, T., Ursu, C., Webb, N., & Wu, M. (2006). The Amiti?s system: Data-driven techniques for automated dialogue. Speech Comm., 48(3-4), 354-373.  Litman, D., & Forbes-Riley, K. (2006). Correlations between dialogue acts and learning in spoken tutoring dialogues. Natural Language Engineering, 12(2), 161-176.  Loper, E., & Bird, S. (2004). NLTK: The natural language toolkit. Proceedings of the ACL Demonstration Session, Barcelona, Spain. 214-217.  Porter, M. F. (1980). An algorithm for suffix stripping. Program, 14(3), 130-137.  Purver, M., Kording, K. P., Griffiths, T. L., & Tenenbaum, J. B. (2006). Unsupervised topic modelling for multi-party spoken discourse. Proceedings of the ACL, Sydney, Australia. , 44(1) 17.  Rabiner, L. R. (1989). A tutorial on hidden markov models and selected applications in speech recognition. Proceedings of the IEEE, 77(2), 257-286.  Sridhar, V. K. R., Bangalore, S., & Narayanan, S. (2009). Combining lexical, syntactic and prosodic cues for improved online dialog act tagging. Computer Speech & Language, 23(4), 407-422.  Stolcke, A., Ries, K., Coccaro, N., Shriberg, E., Bates, R., Jurafsky, D., Taylor, P., Martin, R., Van Ess-Dykema, C., & Meteer, M. (2000). Dialogue act modeling for automatic tagging and recognition of conversational speech. Comp. Ling., 26(3), 339-373.  Wright Hastie, H., Poesio, M., & Isard, S. (2002). Automatically predicting dialogue structure using prosodic features. Speech Communication, 36(1-2), 63-79.  Young, S., Gasic, M., Keizer, S., Mairesse, F., Schatzmann, J., Thomson, B., & Yu, K. (2009). The hidden information state model: A practical framework for POMDP-based spoken dialogue management. Computer Speech and Language, 24(2), 150-174.   
304
    
Time Stamp Dialogue Stream Task Stream  2008-04-11 18:23:45 Student:  so do i have to manipulate the array this time? [Q]   2008-04-11 18:23:53 Tutor:  this time, we need to do two things [S]    2008-04-11 18:24:02 Tutor:  first, we need to create a new array to hold the changed values [S]    2008-04-11 18:24:28     i 2008-04-11 18:24:28     n 2008-04-11 18:24:28     t 2008-04-11 18:24:28     \sp 1-a-i BUGGY 2008-04-11 18:24:35     \del  2008-04-11 18:24:36     \sp  2008-04-11 18:24:36     d 2008-04-11 18:24:36     o 2008-04-11 18:24:36     u 2008-04-11 18:24:36     b 2008-04-11 18:24:37     l 2008-04-11 18:24:37     e 2008-04-11 18:24:37     \sp 2008-04-11 18:24:39     [] 
1-a-i CORRECT 
2008-04-11 18:24:40     \sp  2008-04-11 18:24:42     n 2008-04-11 18:24:42     e 2008-04-11 18:24:42     w 2008-04-11 18:24:43     \sp 2008-04-11 18:24:44     \del 2008-04-11 18:24:45     T 2008-04-11 18:24:46     \del 2008-04-11 18:24:54     T 2008-04-11 18:24:54     i 2008-04-11 18:24:54     m 2008-04-11 18:24:54     e 2008-04-11 18:24:54     s 2008-04-11 18:24:55     3 2008-04-11 18:24:57     ; 
1-a-ii CORRECT 
2008-04-11 18:25:11 Student:  good? [RF]    2008-04-11 18:25:14 Tutor:  good so far, yes [PF]    2008-04-11 18:25:29 Student:  so now i have to change parts of the times array right? [Q]    2008-04-11 18:25:34 Tutor:  not quite [LF]    2008-04-11 18:25:57 Tutor:  So, when you create a new object, like a String for example, you'd say something like  String s = new String() [S]    2008-04-11 18:25:59 Tutor:  right? [AQ]    2008-04-11 18:26:06 Student:  right [P]    2008-04-11 18:26:14 Tutor:  arrays are similar [S]         
    
Appendix 
Excerpt 1. Parallel synchronous dialogue and task event streams with annotations. (Note tutor dialogue acts: AQ=ASSESSING QUESTION, LF=LUKEWARM FEEDBACK, PF=POSITIVE FEEDBACK) 
305
Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 49?58,
Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational Linguistics
The Impact of Task-Oriented Feature Sets on  HMMs for Dialogue Modeling   Kristy Elizabeth Boyer  Eun Young Ha   Robert Phillips*   James Lester   Department of Computer Science North Carolina State University *Dual affiliation with Applied Research Associates, Inc. Raleigh, North Carolina, USA {keboyer, eha, rphilli, lester}@ncsu.edu 
  
Abstract 
Human dialogue serves as a valuable model for learning the behavior of dialogue systems. Hidden Markov models? sequential structure is well suited to modeling human dialogue, and their theoretical underpinnings are consistent with the conception of dialogue as a stochastic process with a layer of implicit, highly influen-tial structure. HMMs have been shown to be effective for a variety of descriptive and pre-dictive dialogue tasks. For task-oriented dia-logue, understanding the learning behavior of HMMs is an important step toward building unsupervised models of human dialogue. This paper examines the behavior of HMMs under six experimental conditions including different task-oriented feature sets and preprocessing approaches. The findings highlight the im-portance of providing HMM learning algo-rithms with rich task-based information. Additionally, the results suggest how specific metrics should be used depending on whether the models will be employed primarily in a de-scriptive or predictive manner.  1 Introduction Human dialogue serves as a valuable model for learning the behavior of dialogue systems. For this reason, corpus-based approaches to dialogue man-agement tasks have been an increasingly active area of research (Bangalore, Di Fabbrizio, & Stent, 2006; Di Eugenio, Xie, & Serafin, 2010; Georgila, Lemon, Henderson, & Moore, 2009; Rotaru & Litman, 2009). Modeling the dialogue policies that 
humans employ permits us to directly extract con-versational and task-based expertise. These tech-niques hold great promise for scaling gracefully to large corpora, and for transferring well across do-mains.    The richness and flexibility of human dialogue introduce nondeterministic and complex patterns that present challenges for machine learning ap-proaches. One approach that has been successfully employed in dialogue modeling is the hidden Mar-kov model (HMM) (Rabiner, 1989). These models are well suited to the sequential nature of dialogue (Stolcke et al, 2000). Moreover, their theoretical underpinnings are consistent with the conception of dialogue as a stochastic process whose observations are influenced by a layer of implicit, yet highly rel-evant, structure (Boyer et al, 2009; Woszczyna & Waibel, 1994).  HMMs have been shown to perform well on important dialogue management tasks such as au-tomatic dialogue act classification (Stolcke et al, 2000). Our work has employed HMMs for a differ-ent goal: learning dialogue policies, or strategies, from corpora (Boyer, Phillips, et al, 2010; Boyer, Phillips, Ingram, et al, in press). This work can be viewed from two perspectives. First, a descriptive goal of the work is to learn models that describe the nature of human dialogues in succinct probabilistic terms, in a way that facilitates important qualitative investigations. The second and complementary goal is predictive: learning models that accurately pre-dict the dialogue moves of humans, in order to cap-ture a dialogue policy that can be used within a system.   
49
Both of these goals are of paramount im-portance in tutorial dialogue, in which tutors and students engage in dialogue in support of a learning task (Boyer, Ha, et al, 2010; VanLehn et al, 2007). Descriptive modeling represents a critical step to-ward more fully understanding the phenomena that contribute to the high effectiveness of human tutor-ing, which has to date been unmatched by tutorial dialogue systems. Predictive models, on the other hand, may be used directly as dialogue policies within systems.  The HMMs considered here were learned from an annotated corpus of textual human-human tuto-rial dialogue. In this domain, HMMs have been shown to correspond qualitatively to widely held conceptions of tutorial dialogue strategies, and ad-jacency pair analysis before model learning has been shown to enhance this qualitative correspond-ence (Boyer et al, 2009). Moreover, HMMs can identify in an unsupervised fashion structural com-ponents that correlate with student knowledge gain (Boyer, Phillips, Ingram, et al, in press).  However, to date, several important questions have not been explored. The answers to these ques-tions have implications for learning HMMs for task-oriented dialogues. The questions include the following: 1) How reliably does the HMM learning framework converge to the hyperparameter N, the best-fit number of hidden states? 2) What are the effects of preprocessing approaches, specifically, adjacency pair analysis, on the resulting HMMs? 3) How do different feature sets for task-oriented dialogue impact the descriptive fit and predictive power of learned HMMs? This paper addresses these questions. The findings suggest that model stability and predictive power benefit from the richest possible input sequences, which include not only dialogue acts but also information about the task state and the absence of particular tutor dia-logue moves. Additionally, we find that traditional measures of HMM goodness-of-fit may not identify the most highly predictive models under some con-ditions. 2 Background HMMs have been used for dialogue modeling tasks for many years. Early work utilized HMMs to model underlying linguistic structure for the pur-poses of identifying speech acts and reducing per-plexity for speech recognition (Stolcke et al, 2000; 
Woszczyna & Waibel, 1994). These projects treat-ed underlying dialogue structure as the hidden lay-er, and dialogue utterances as observations. This treatment is analogous to the work presented in this paper, except that our observations are dialogue act tags only, rather than being constituent words in each utterance. Our goals are also different: to cre-ate a qualitatively interpretable model of dialogue structure that corresponds to widely accepted no-tions of task-oriented dialogue, and to learn a high-ly predictive dialogue policy from a human-human dialogue corpus.  HMMs rely on treating dialogue as a sequential Markov process in which each observation depends only on a finite set of preceding observations. Some other approaches that rely on this assumption treat dialogue as a Markov decision process or partially observable Markov decision process, in which state changes are associated with actions and rewards (e.g., Young et al, 2010). Such work focuses on learning an optimal policy, typically utilizing a combination of human and simulated dialogue cor-pora. Reinforcement learning techniques can then be applied to learn the optimal policy based on the observed rewards. In contrast, we start with a rich corpus of human-human dialogue, which may have poor coverage in some areas (though the dialogue act tags were empirically derived and therefore mit-igate this problem to some extent), and subsequent-ly learn a model that explains the variance in that human corpus as well as possible. Capturing the dialogue policy implicit within a corpus of human-human dialogue has been ex-plored in other work in a catalogue-ordering do-main (Bangalore, Di Fabbrizio, & Stent, 2006). That work utilized maximum entropy modeling to predict human agents? dialogue moves within a vector-based framework. Although a vector-based approach differs in many regards from the sequen-tial HMM approach described here, both approach-es assume a dependence only on a finite history. HMMs accomplish this through graphical depend-encies, while vector-based approaches accomplish it by including features for a restricted window of left-hand context. The results of this catalogue-ordering project highlight how challenging it is to predict human agents? dialogue moves in a task-oriented domain. 
50
3 Corpus  The corpus was collected during a human-human tutoring study. Students solved an introductory computer programming problem in the Java pro-gramming language. Tutors were located in a sepa-rate room and communicated with students through textual dialogue while viewing a synchronized view of the student?s problem-solving workspace. Forty-eight students interacted for approximately one hour each with a tutor. Students exhibited sta-tistically significant learning gains from pretest to posttest, indicating that the tutoring was effective (Boyer, Phillips, Ingram, et al, in press). The cor-pus contains 1,468 student moves and 3,338 tutor moves. Overlapping utterances, which are common in dialogue platforms such as instant messaging, were prevented by permitting only one user to con-struct a dialogue message at a time. Because the corpus is textual, utterances were segmented at tex-tual message boundaries except when the lead dia-logue annotator noted the presence of two separate dialogue acts within non-overlapping chunks of text. In these events the utterance was segmented by the primary annotator prior to being tagged by the second dialogue act annotator.  In addition to dialogue act annotation, the cor-pus was manually annotated for task structure and correctness (Section 3.2), and for delayed tutor feedback (Seciton 3.3). The appendix displays an excerpt from the annotated corpus.  3.1 Dialogue Act Annotation As part of prior work, the corpus was annotated with dialogue acts for both tutor (Boyer, Phillips, Ingram, et al, in press) and student (Boyer, Ha, et al, 2010) utterances (Table 1). One annotator tagged the entire corpus, while a second annotator independently tagged a randomly selected 10% of tutoring sessions. The inter-annotator agreement Kappa score was 0.80.  3.2 Task Annotation The corpus includes 97,509 keystroke-level task events (computer programming actions), all taken by the student. Tutors viewed synchronously, but could not edit, the computer program. The task ac-tions were manually clustered and labeled for sub-task structure (Boyer, Phillips, et al, 2010). The task structure annotation was hierarchical, with 
leaves corresponding to specific subtasks such as creating a temporary variable in order to swap two variables? values (subtask 3-c-iii-2). Each problem-solving cluster, or subtask, was then labeled for correctness (Table 2). These correctness labels are utilized in the models presented in this paper. The Kappa agreement statistic for the correctness anno-tation on 20% of the corpus was 0.80. Table 1. Dialogue act tags Dialogue Act Tutor Example ASSESSING Q. Which type should that be? EXTRA-DOMAIN A coordinator will be there soon. GROUNDING Ok. LUKEWARM FDBK That?s close. LUKEWARM CONTENT FDBK Almost there, but the second parameter isn?t quite right. NEGATIVE FDBK That?s not right. NEGATIVE CONTENT FDBK No, the counter has to be an int. POSITIVE  FDBK Perfect. POSITIVE CONTENT FDBK Right, the array is a local varia-ble. QUESTION Which approach do you prefer? RESPONSE It will be an int. STATEMENT They start at 0. Table 2. Task correctness tags Correctness Tag Description CORRECT Fully conforming to the require-ments of the task. BUGGY Violating the requirements of the task. These task events typically require tutorial remediation. INCOMPLETE Not violating, but not yet fulfilling, the requirements of the task. 
DISPREFERRED Technically fulfilling requirements but not utilizing the target con-cepts being tutored. These events typically require tutorial remediation. 3.3 Annotation for Delayed Tutor Feedback The dialogue act and task annotations reflect posi-tive evidence regarding what did occur in the dia-logues. An additional annotation was introduced for what did not occur?specifically, instances in which tutors did not to make a dialogue move in response to students? relevant task actions. The task in our corpus is computer programming, so bugs in the task correspond to errors either in syntax or se-
51
mantics of the computer program compared to the desired outcome. The human tutors were working with only one student at a time and were carefully monitoring student task actions during the dialogue, so we take the absence of a dialogue move at a rel-evant point to be an intentional choice by the tutor to delay feedback as part of the tutorial strategy. The automatic annotation for delayed feedback in-troduced two new event tags: NO-MENTION of cor-rectly completed subtasks, and NO-REMEDIATION of existing bugs within the task.  The intuition behind these tags is that within a learned dialogue policy, specifically modeling when not to intervene is crucial. Typically human tutors mention correctly completed subtasks, but at times other tutorial goals eclipse the importance of doing so. The NO-MENTION tag captures these in-stances. On the other hand, typically when working with novices, human tutors remediate an existing bug quickly. However, tutors may choose to delay this remediation for a variety of reasons such as remediating a different bug instead or asking a con-ceptual question to encourage the student to reflect on the issue. The NO-REMEDIATION tag captures these instances of the absence of remediation given that a bug was present. These two annotations for delayed feedback were performed automatically (Boyer, Phillips, Ha, et al, in press).  3.4 Adjacency Pair Modeling Prior work has demonstrated that adjacency pairs can be identified in an unsupervised fashion from a corpus (Midgley, Harrison, & MacNish, 2006). This technique relies on statistical analysis to de-termine the significant dependencies that exist be-tween pairs of dialogue acts, or in our task-oriented corpus, pairs of dialogue acts or task actions. After the pairs of dependent events are identified, they are joined within the corpus algorithmically (Boyer et al, 2009). Joining a pair of dependent moves in this way is equivalent to introducing a deterministic (probability=1) succession between observation symbols. This type of dependency cannot be learned in the traditional first-order HMM frame-work, but is desirable when two observations are strongly linked.1                                                             1 Enhanced HMM structures, such as autoregressive HMMs, which allow for direct graphical links between observation symbols, can learn such a dependency but only in stochastic terms. 
The experiment that is described in Section 4 utilizes different feature sets to learn and compare HMMs. Table 3 shows these feature sets and their most highly statistically significant adjacency pairs. Table 3. Experimental conditions and top three ad-jacency pairs (subscripts denote speaker, Student or Tutor) Condition Description Significant Adjacency Pairs DAONLY Dialogue acts only QS~RspT  GroundS~GroundT AssessQT~PosFdbkS 
DATASK Dialogue acts & task cor-rectness events 
QS~RspT CorrectTaskS~CorrectTaskS GroundS~GroundT 
DATASK-DELAY 
Dialogue acts, task correctness, & delayed feedback  
QS~RspT NoRemediateT~BuggyTaskS CorrectTaskS~CorrectTaskS 
4 Models HMMs were selected as the modeling framework for this work because their sequential nature is well suited to the structure of human dialogue, and their ?hidden? variable corresponds to widely held con-ceptions of dialogue as having an unobservable, but influential, layer of stochastic structure. For exam-ple, in tutoring, an ?explanation? mode is common, in which the tutor presents new information and the student provides acknowledgments or takes task actions accordingly. Although the presence of the ?explanation? goal is not directly observable in most dialogues, it may be inferred from the obser-vations. These sequences correspond to the input observations for learning an HMM.  4.1 Hidden Markov Models HMMs explicitly model hidden states within a doubly stochastic structure (Rabiner, 1989). A first-order HMM, in which each hidden state depends only on the immediately preceding hidden state, is defined by the following components: ? ? = {?1, ?2, ?, ?M}, the observation sym-bol alphabet ? S = {s1,s2,?,sN}, the set of hidden states 
52
? ?=[?i], i=1,?,N, the initial probability dis-tribution, where ?i is the probability of the model beginning in hidden state si in S  ? A=[aij], a transition probability distribution, where aij is the probability of the model transitioning from hidden state i to hidden state j for i,j=1,?,N ? B=[bik], an emission probability distribu-tion where bik is the probability of state i (i=1,?,N) emitting (or generating) obser-vation symbol k (k=1,?,M). 4.2 Dialogue Modeling with HMMs In this work, the observation symbol alphabet ? is given. For each experimental condition, ? is either 1) all dialogue act tags, 2) all dialogue acts plus task correctness tags, or 3) dialogue act, task cor-rectness, and delayed feedback tags. The transition probability distribution A, emission probability dis-tribution B, and initial probability distribution ? are learned by the standard Baum-Welch algorithm for optimizing HMM parameters (Rabiner, 1989). This algorithm is susceptible to becoming trapped in local optima, so our approach uses ten-time random restart with new initial parameters for each model to reduce the probability of selecting a model that represents only a local optimum.  The hyperparameter N, which is the best number of hidden states, is also learned rather than fixed. This process involves running the full HMM train-ing algorithm, including random restarts in ten-fold cross-validation, across the data and selecting the N that corresponds to the best mean goodness-of-fit measure. For HMMs, a typical goodness-of-fit measure is log-likelihood, which captures how like-ly the observations would be under the current model. The log is taken for practical reasons, to avoid numerical underflow. Higher log-likelihood corresponds to improved model fit. However, typi-cally it is desirable to penalize a higher number of hidden states, since increasing the model complexi-ty results in tradeoffs that may not be fully warrant-ed by the improvement in model fit. In this work, we utilize the Akaike Information Criterion (AIC), a standard penalized log-likelihood metric (Akaike, 1976).     
AIC = 2*N ? 2*ln(likelihood) Lower values of AIC indicate better model fit. 4.3 Experimental Conditions HMMs were learned using three separate feature sets, each providing a progressively more complete picture of the task-oriented dialogues: dialogue acts only (DAONLY), dialogue acts and task events (DATASK), and dialogue acts with both task cor-rectness events and tags for delayed tutor feedback (DATASKDELAY).  In addition to the three different feature sets, each condition included one of two types of pre-processing. Each type of model was trained on un-altered sequences of the annotated tags, which we refer to as the UNIGRAM condition. Additionally, each type of model was trained on sequences with statistically dependent adjacency pairs joined in a preprocessing step as described in Section 3.4. The UNIGRAM and ADJPAIR conditions were explored for each of the three feature sets, resulting in six experimental conditions. These conditions were chosen in order to explore the convergence behav-ior of HMMs under the different feature sets and preprocessing, and to compare measures of descrip-tive fit with measures of predictive power.  4.4 Learned HMMs Figures 1 and 2 show a subset of the DAONLY UNIGRAM model and the DATASKDELAY ADJPAIR model. These figures depict the structure of our HMMs: each hidden state is associated with an emission probability distribution over the possible observation symbols.  5 Goodness-of-Fit Curves The learning algorithm described in Section 4.2 was applied to input sequences under the six exper-imental conditions to learn the best-fit HMM pa-rameters. Figure 3 displays these AIC results, which are discussed in detail in the remainder of this section.   
53
 Figure 1. Subset of learned HMM (N=13) for DAONLY UNIGRAM condition  
 Figure 2. Subset of learned HMM (N=9) for DATASKDELAY ADJPAIR condition  5.1 Impact of Experimental Conditions  For the DAONLY condition, both the UNIGRAM and ADJPAIR models generally improve until N=12 or 13, after which the fit generally worsens. A differ-
ent pattern emerges for the DATASK condition, in which the UNIGRAM sequences are optimally fit to a model with 16 states, while the ADJPAIR se-quences are fit to a model with 8 states. Finally, for the DATASKDELAY condition, the UNIGRAM se-quences are best fit by a model with 10 hidden states, while the ADJPAIR sequences are fit best by 9. Typically, we see that ADJPAIR sequences are fit to slightly simpler models in terms of the hy-perparameter N, number of hidden states.   
Figure 3. Number of hidden states and cor-responding adjusted AIC, shifted to a mini-mum score of zero indicating the best-fit N 
Adjust
ed AIC
 
a) Dialogue ActsOnly (DAONLY) 
  N (number of hidden states) 
Adjust
ed AIC
 
b) Dialogue Act and Task Events (DATASK) 
  N (number of hidden states) 
Adjust
ed AIC
 
c) Dialogue Act, Task, & Delayed Feedback (DATASKDELAY)  
  N (number of hidden states) 
54
Stability in the hyperparameter N is an im-portant consideration because an underlying as-sumption of our work is that the hidden states correspond to unobserved stochastic structures of the real world process?that is, we hypothesize that a ?true? value for N exists. We would like models to exhibit decreasing variation in goodness of fit measures around this true N. To examine this stability we consider the three best AIC values for each condition and their corresponding Ns: the set {Nk-best | k=1,2,3}. The range of this set indicates how ?far apart? the best three Ns are: for example, in the DAONLY UNIGRAM condition, the top three models have Ns of {13,10,15}, yielding a range of 5. Intuitively, a small value for this metric indicates that the model has converged tightly on N.  Figure 4 shows the stability results for the six different experimental conditions. As shown in the figure, for the DATASK and DATASKDELAY condi-tions, the ADJPAIR models achieve the smallest range among the top three values of N; these mod-els converge most tightly to the ?best? value.   
 Figure 4. Stability of N (range of {N1best, N2best, N3best}) ? smaller implies tighter convergence to ?best? N 6 Predictive Analysis Section 5 presented an analysis of the goodness-of-fit curves of HMMs learned from the corpus. The measures of stability and discrimination for N cap-ture important aspects of the behavior of HMMs toward this parameter, which is conceived of as representing ?true? real-world stochastic behavior. In this way, Section 5 has presented a descriptive view of HMM dialogue models.  This section presents a predictive view of the models. Specifically, we consider prediction accu-racy, defined as the percent of tutor dialogue moves 
that the model is able to correctly predict given the dialogue history sequence up to that point.  6.1 Impact of Dependent Adjacency Pairs We first explore whether the preprocessing step of joining dependent adjacency pairs impacted predic-tion accuracy. The prediction accuracy of the best-fit model in each condition is displayed in Figure 5. This figure includes prediction accuracy on training data, which were used to learn model parameters, as well as prediction accuracy on testing data, which were withheld from model training.  
 Figure 5. Prediction accuracy for tutor moves  As shown in Figure 5, joining the adjacency pairs improved model performance on the training sets of all three conditions, indicating that the varia-tion within the training data was better explained by ADJPAIR models. (This measure of predictive power is different from a goodness-of-fit criterion as described in the previous section, a relationship that will be discussed further in Section 7.) In con-trast to the training set performance, the ADJPAIR models performed better than UNIGRAM models for the testing set only in the DATASKDELAY condi-tion.   6.2 Impact of Task-Oriented Feature Sets As illustrated in Figure 5, the three feature sets per-form similarly under the UNIGRAM condition. This performance is slightly above baseline (DAONLY and DATASK baselines = 0.38; DATASKDELAY baseline = 0.30), and diminishes little between the training and testing sets. In contrast, under the ADJPAIR condition, performance between condi-tions and across training and testing sets varies. The DATask model performs far better on predicting observations in the training than the testing set, 
55
suggesting possible overfitting to the training set. This relationship is discussed further in Section 7. The DATASKDELAY model performs well during both training and testing, though with a slight de-crease in accuracy on the testing set.   6.3 Relationship Between Predictive and De-scriptive Metrics Measures of fit such as log-likelihood and AIC cap-ture the likelihood of observing the data given a model. Predictive accuracy, on the other hand, measures the probability that the model can predict the next observation given a partial sequence. In general, we would expect these measures to corre-late well; however, there is not perfect correlation between these metrics because the mechanism by which log-likelihood (and thereby AIC) is derived involves maximizing likelihood over complete se-quences, while prediction is performed over partial sequences.  To examine how well AIC and prediction accu-racy correlate, Figure 6 displays these values for a subset of the models in the DAONLY UNIGRAM condition and the DATASKDELAY ADJPAIR condi-tion. These two conditions represent the extremes of the experimental conditions, with DAONLY con-taining the least information about the task-oriented dialogue while DATASKDELAY contains the most information.  As shown in Figure 6, the correlation for DAONLY UNIGRAM roughly conforms to what would be expected: lower AIC, indicating better model fit, is associated with the highest prediction accuracies. The relationship is less clear for the DATASKDELAY ADJPAIR condition. While its worst AIC is associated with the lowest prediction accuracy as expected, the best AIC is not associated with the highest prediction accuracy. This phenom-enon may be due to the lack of spread among AIC values overall for this condition; as seen in Figure 3, the DATASKDELAY ADJPAIR condition has the flattest AIC curve of all conditions, indicating that for this condition the difference between best-fit and worst-fit models is smaller than for any other condition. The inconsistent relationship between AIC and prediction accuracy, therefore, may be the product of noise surrounding a large set of ?good? models, whereas for the DAONLY UNIGRAM condi-tion, the set of good models is smaller.   
 7 Discussion The results suggest several important findings re-garding feature sets and preprocessing for learning HMMs of task-oriented dialogue. First, the models? convergence patterns to a best-fit N, number of hidden states, indicate that more information em-bedded within the sequences may correspond with a flatter goodness-of-fit curve. Adding more infor-mation to the input sequences may introduce some regularities that partly mitigate the limitations of even a poorly fit HMM. This additional infor-mation may come in the form of adjacency pairs discovered in an unsupervised fashion, which im-proved the stability of convergence on the best-fit N under the DATASK and DATASKDELAY condi-tions. This increased stability is likely due to the fact that under these conditions, leveraging adja-cency pair information augments the HMM?s struc-ture with contextual dependencies that could otherwise not be learned under the traditional HMM framework.  For predictive accuracy, the benefits of richer input sequences are also highlighted. The most highly predictive models included all three sources 
Predic
tion Ac
curacy
 a) DAOnly UNIGRAM Condition 
  AIC 
Predic
tion Ac
curacy
 b) DATASKDELAY ADJPAIR Condition  
  AIC Figure 6. Prediction accuracy vs. AIC 
56
of information: dialogue acts, task events, and de-layed feedback tags. However, with the addition of this rich information to the input sequences and the accompanying flatter goodness-of-fit curve as dis-cussed above, we noted an irregular pattern of cor-relation between goodness-of-fit and predictive accuracy that is worthy of future exploration. Spe-cifically, it appears that the most highly predictive DATASKDELAY ADJPAIR model, which is the most highly predictive of all models in all conditions, does not correspond to the best (lowest) AIC for that condition (Figure 3). This finding suggests that when a predictive task is the primary goal, a predic-tive metric should be used to select the best-fit model. Additional support for such an approach is provided by the close correspondence between training and testing set prediction accuracy. 8 Conclusion Understanding how HMMs behave under different feature sets is an important step toward learning effective models of task-oriented dialogue. This paper has examined how HMMs converge to a best number of hidden states under different experi-mental conditions. We have also considered how well HMMs under these conditions predict tutor dialogue acts within a corpus of task-oriented tutor-ing, a crucial step toward learning dialogue policies from human corpora. The findings highlight the importance of adding rich task-based features to the input sequences in order to learn HMMs that con-verge tightly on the best-fit number of hidden states. The results also indicate that caution should be used when utilizing traditional goodness-of-fit metrics, which are appropriate for descriptive ap-plications, if the goal is to learn a highly predictive model.  This line of research is part of a larger research program of learning unsupervised models of human task-oriented dialogue that can be used to define the behavior of dialogue systems. Developing a framework for learning a dialogue policy from hu-man corpora, as discussed here, is a critical step toward that goal. Future work should focus on un-supervised dialogue act classification, and address the challenges of user plan recognition.  
Acknowledgments. This work is supported in part by National Science Foundation through Grants REC-0632450, IIS-0812291, DRL-1007962 and the STARS 
Alliance Grant CNS-0739216. Any opinions, findings, conclusions, or recommendations expressed in this re-port are those of the participants, and do not necessarily represent the official views, opinions, or policy of the National Science Foundation. References Akaike, H. (1976). An information criterion (AIC). Math. Sci., 14(153), 5-9. Bangalore, S., Di Fabbrizio, G., & Stents, A. (2006). Learning the structure of task-driven human-human dialogs. Proceedings of ACL ?06, 201-208.  Boyer, K. E., Ha, E. Y., Phillips, R., Wallis, M. D., Vouk, M. A., & Lester, J. C. (2010). Dialogue Act Modeling in a Complex Task-Oriented Domain. Proceedings of SIGDIAL (pp. 297-305).  Boyer, K. E., Phillips, R., Ha, E. Y., Wallis, M. D., Vouk, M. A., & Lester, J. C. (in press). Learning a Tutorial Dialogue Policy for Delayed Feedback. Proceedings of the 24th International FLAIRS Con-ference. Boyer, K. E., Phillips, R., Ha, E. Y., Wallis, M. D., Vouk, M. A., & Lester, J. C. (2009). Modeling dia-logue structure with adjacency pair analysis and hid-den Markov models. Proceedings of NAACL HLT, Companion Volume, 49-52.  Boyer, K. E., Phillips, R., Ha, E. Y., Wallis, M. D., Vouk, M. A., & Lester, J. C. (2010). Leveraging Hidden Dialogue State to Select Tutorial Moves. Proceedings of the NAACL HLT 2010 Fifth Work-shop on Innovative Use of NLP for Building Educa-tional Applications (pp. 66-73). Boyer, K. E., Phillips, R., Ingram, A., Young, E., Wallis, M., Vouk, M., et al (in press). Investigating the Re-lationship Between Dialogue Structure and Tutoring Effectiveness: A Hidden Markov Modeling Ap-proach. International Journal of Artificial Intelli-gence in Education. Di Eugenio, B., Xie, Z., & Serafin, R. (2010). Dialogue Act Classification, Higher Order Dialogue Structure, and Instance-Based Learning. Dialogue & Dis-course, 1(2), 1-24.  Georgila, K., Lemon, O., Henderson, J., & Moore, J. D. (2009). Automatic annotation of context and speech acts for dialogue corpora. Natural Language Engi-neering, 15(3), 315-353.  Midgley, T. D., Harrison, S., & MacNish, C. (2006). Empirical verification of adjacency pairs using dia-logue segmentation. Proceedings of SIGDIAL, 104-108.  Rabiner, L. R. (1989). A tutorial on hidden Markov models and selected applications in speech recogni-tion. Proceedings of the IEEE, 77(2), 257-286.  
57
Rotaru, M., & Litman, D. J. (2009). Discourse Structure and Performance Analysis : Beyond the Correlation. Proceedings of SIGDIAL (pp. 178-187). Stolcke, A., Ries, K., Coccaro, N., Shriberg, E., Bates, R., Jurafsky, D., et al (2000). Dialogue Act Model-ing for Automatic Tagging and Recognition of Con-versational Speech. Computational Linguistics, 26(3), 339-373.  VanLehn, K., Graesser, A. C., Jackson, G. T., Jordan, P., Olney, A., & Rose, C. P. (2007). When Are Tutorial 
Dialogues More Effective Than Reading? Cognitive Science: A Multidisciplinary Journal, 30(1), 3-62.  Woszczyna, M., & Waibel, A. (1994). Inferring linguis-tic structure in spoken language. Proceedings of the International Conference on Spoken Language Pro-cessing (pp. 847-850). Young, S., Ga?i?, M., Keizer, S., Mairesse, F., Schatzmann, J., Thomson, B., et al (2010). The Hid-den Information State model: A practical framework for POMDP-based spoken dialogue management. Computer Speech & Language, 24(2), 150-174.   Appendix. Excerpt from task-oriented textual human-human tutoring corpus. Speaker Utterance or Event Tag Student: [Task action on subtask 3-c-i-4] BUGGY Student: [Task action on subtask 3-c-ii-5] CORRECT Tutor: [Does not provide remediation for existing bug] NOREMEDIATION Student: [Task action on subtask 3-c-iii-1] BUGGY Student: i don't remember off the top of my head how the swap function worked. most of the time i just copied and pasted it from some of my older code NEGATIVECONTENTFDBK Tutor: The easiest way to swap x and y is to make a tempo-rary variable  Student: Ok ACK Student: do i need to pass the entire array and the indecies of the items to swap? ASSESSQ Tutor:  if you want to use a seperate method to swap, then yes, you'll have to pass those things  POSCONTENTFDBK Tutor:  [Does not mention a correctly completed subtask]	 ? NOMENTIONCOMP Student: oh. i guess i could just swap it in the same method. it is problably easier that way, and less code. we were showed in class how to do it separately, but i had never thought of doing it the other way.  
STMT 
Student: [Task action on subtask 3-c-iii-2] DISPREFERRED Tutor:  Both ways work, but it?s definitely less code to just do it inside this method.  STMT Student: Ok ACK  
58
Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 94?98,
Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational Linguistics
 
 
From Strangers to Partners: Examining Convergence within a  Longitudinal Study of Task-Oriented Dialogue 
 Christopher M. Mitchell Kristy Elizabeth Boyer James C. Lester  Department of Computer Science North Carolina State University Raleigh, NC, USA {cmmitch2, keboyer, lester}@ncsu.edu      Abstract Convergence is thought to be an important phenomenon in dialogue through which interlocutors adapt to each other. Yet, its mechanisms and relationship to dialogue outcomes are not fully understood. This paper explores convergence in textual task-oriented dialogue during a longitudinal study. The results suggest that over time, convergence between interlocutors increases with successive dialogues. Additionally, for the tutorial dialogue domain at hand, convergence metrics were found to be significant predictors of dialogue outcomes such as learning, mental effort, and emotional states including frustration, boredom, and confusion. The results suggest ways in which dialogue systems may leverage convergence to enhance their interactions with users.  1 Introduction Convergence is a widely observed phenomenon in dialogue, in which interlocutors adapt to the patterns in each other?s utterances (Brennan 1996; Pickering and Garrod 2004). These patterns can include lexical choice (Hirschberg 2008; Ward and Litman 2007), syntactic choice (Reitter et al 2006; Stoyanchev and Stent 2009) and loudness (Coulston et al 2002). It is believed that convergence is indicative of shared understanding (Pickering and Garrod 2004), which makes it an important consideration for task-oriented dialogue systems.     In addition to facilitating shared understanding, convergence has also been associated with the success of dialogues in several domains (Steinhauser et al 2011; Ward and Litman 2007), 
and can also be leveraged for lexical and syntactic priming that may improve performance of spoken dialogue systems via more accurate speech recognition (Stoyanchev and Stent 2009). While such results have established that convergence is an important dialogue phenomenon, the field does not yet fully understand how convergence is associated with dialogue success.  This paper examines surface-level and lexical convergence within textual task-oriented dialogues. The analysis considers three levels of convergence: utterance-level short-term priming effects, conversation-level convergence effects, and longitudinal convergence effects, as interlocutors participate in six conversations together over the course of several weeks. Using these measures, we build multiple regression models that indicate ways in which convergence can predict both desirable and undesirable outcomes of task-oriented dialogues.      This paper makes several contributions. First, by examining convergence at several granularity levels and across multiple dialogues with the same partners, we gain insight into how convergence phenomena unfold over time. Second, the findings provide confirmatory evidence that in some domains, such as the tutorial dialogue considered here, lexical priming be associated with unintended consequences. Finally, we demonstrate that dialogue convergence is also associated with affective components such as frustration, engagement, and confusion. These results contribute to an understanding of convergence that may enable us to harness this phenomenon more effectively within dialogue systems.  
94
  
2 Related Work Convergence and the related concepts of alignment and priming have been extensively studied. Alignment, or the development of shared understanding, has been studied by Pickering and Garrod (2004) who propose that alignment on lower-level observable features is indicative of alignment at the level of conceptual models. The influence of shared representation in dialogue has also been explored in the context of learning; for example, Ward and Litman (2007) studied lexical convergence in human-human tutoring and found that the rate of priming, which measures student re-use of tutor words at various distances, was positively associated with learning for students with low initial test scores. Conversely, Steinhauser et al (2011) analyzed lexical convergence in an automated dialogue-based physics tutor, and found that the level of the student mimicking the tutor was negatively correlated with learning. Thus, the relationship between dialogue convergence and learning is not fully understood, and may be highly dependent on context.   In addition to a theoretical link to shared representations, convergence has practical implications, in particular for speech recognition (Stoyanchev and Stent 2009). Brennan (1996) found that users adapt their lexical choices to match those of an automated system in both text-based and speech-based interactions, even when it is apparent that the system understood the user?s original lexical choice. Convergence has even been found to occur in non-lexical aspects of a dialogue, such as users adapting their loudness levels to match that of a software agent (Coulston et al 2002). Together, these results suggest that convergence has implications beyond lexical and syntactic choice. 3 Corpus  The corpus consists of text-based tutorial dialogues between two interlocutors, a tutor and a student, working together to complete tasks in the domain of introductory computer science (excerpt in Appendix A). The corpus was collected over two semesters, in which 67 first-year university students were selected from an introductory engineering course and assigned to one of seven 
tutors of varying levels of tutoring experience. Each student engaged in six task-based dialogues with a single tutor over four weeks with the goal of producing a working software artifact during each session. Each session included several subtasks, and time was strictly limited to forty minutes duration. The remote collaboration interface, shown in Figure 1, facilitated a real-time synchronized view of the workspace and dialogue. This paper considers dialogue utterances only, leaving to future work the analysis of task-related artifacts.  
 Figure 1. Task-oriented dialogue interface  The effectiveness of the dialogue was measured in several ways. First, student learning was measured as difference in score on pre-test and post-tests. Student engagement, or level of involvement during the dialogue, was measured with a brief survey after each dialogue (O?Brien and Toms 2010), as were student?s satisfaction with the exchange, and a rating of how mentally challenging the task was perceived to be (Hart and Staveland 1988). Finally, the tutors were asked to rate their satisfaction with the effectiveness of each session and to report on their perceptions of the affective states of both interlocutors during the session. The students were not asked about their own affective states, as this may have introduced bias in subsequent dialogues.  4 Analysis The goal of the analysis is to identify the characteristics of the dialogues that are predictive of the outcomes of interest, including learning, engagement, affect, and overall success of the 
95
  
dialogue as rated by the interlocutors. Summary statistics for the dialogues were computed, including time duration of the session, number of utterances, number of words, number of characters, mean word length, and lexicon size (Table 1). Stop words were not excluded from the analysis, in part due to specialized usage of common vocabulary in the computer science domain (e.g., for, if ). Although not traditionally considered a form of convergence, we were interested in the relationship between the levels of activity of the two interlocutors.  To this end, we analyzed the number of utterances, words, and characters used by tutor and student, and found a significant positive correlation on these metrics (p<0.0001 for each). The first convergence phenomenon considered centers on lexical priming, the tendency for one interlocutor to re-use words previously introduced by the other. We have utilized a priming metric computed as follows: Interlocutor A?s Priming Ratio (PR) is the percent of Interlocutor A?s words reused by Interlocutor B at a given distance d, where distance is measured in terms of number of Interlocutor B?s utterances. Negative slope of PR over distance indicates a priming effect because an interlocutor was more likely to reuse a word shortly after its use by the other interlocutor. This metric has been used to investigate tutor priming (Steinhauser et al 2011; Ward and Litman 2007), and we generalize it to measure priming for both interlocutors. Note that student PR, which reflects the extent to which the tutor adopted the student?s lexical choice, is of particular interest from the perspective of dialogue system design, in which tutor utterances are system-generated.    Tutor  mean (SD) Student mean (SD) Surface Features Number of utterances 83.7 (28.8) 35.6 (13.1) Number of words 580.9 (202.3) 170.1 (92.6) Number of characters 2383.4 (886.6) 667.3 (386.0) Mean word length 4.1 (0.2) 3.9 (0.3) Lexicon size 329.7 (87.3) 106.3 (47.3)    Convergence Metrics Priming Ratio (1-10) .030 (.02) .047 (.02) ?Priming Ratio (1-10) -.011 (.02) -.017 (.04) Max Priming Ratio .052 (.02) .091 (.04) Matched Word Ratio .233 (.09) .386 (.08) Table 1. Statistics for each metric  In addition to the Priming Ratio, we also computed a metric to reflect convergence: Interlocutor A?s 
Matched Word Ratio (MWR) is the percent of Interlocutor A?s words that had been previously used by Interlocutor B at any point in the dialogue history. Because it is backward-looking, this metric is applicable not only in a corpus study, but could also be used within a runtime system to track convergence as the dialogue unfolds. 5 Models and Results  Mean Matched Word Ratio for both interlocutors increased as sessions progressed, reflecting that the two dialogue partners used more of each other?s words as they spent more time together. The Priming Ratio also revealed several phenomena in the corpus. Similarly to prior observations from tutorial dialogue (Ward and Litman 2007), we found that student reuse of tutor primes decreased with distance, indicating that a lexical priming effect occurred (Figure 2). This trend also occurred for tutor reuse of student primes (Figure 3).  The effect was more pronounced in the tutor?s PR than the student?s PR; that is, there was more evidence that tutors converged to students in the short term. This finding may be associated in part with the higher number of tutor utterances: a distance in terms of number of tutor utterances represents fewer combined student and tutor utterances than the same distance in terms of student utterances. Additionally, tutor convergence may reflect a dimension of intentional pedagogical choice.  The Priming Ratio is designed to reflect short-term priming. However, there is evidence of a longer-term effect as the two interlocutors engaged in dialogue across multiple sessions. Figures 2 and 3 display Tutor?s PR and Student?s PR, respectively, by task set, of which there were six in the corpus study. The last task set displays an overall higher level of lexical convergence than the earlier sessions, and there is a general trend of increasing convergence as the number of sessions together increases. In order to identify the features that were most predictive of dialogue outcomes, all of the convergence metrics and surface summary features were provided as input to a stepwise linear regression model. Standard greedy variable addition and removal was performed, with additional post-processing and re-training to eliminate instances of multicollinearity. The learned models (Appendix B) include a mixture of 
96
  
convergence metrics and surface features, as well as structural features such as the task set number and the time duration of the dialogue. At least one convergence metric was found to be associated with each outcome in the generated models, with the exception of Engagement.  
 Figure 2. Tutor's Priming Ratio aggregated by task set (TS = Task Set)   
 Figure 3. Student's Priming Ratio aggregated by task set (TS = Task Set)  Several significant relationships emerged within the models. We discuss a subset of these here. First, tutor Priming Ratio was a significant predictor for outcomes as rated by both tutor and student. Higher tutor Priming Ratio was associated with higher tutor perception of dialogue success, perhaps because students reflected tutor lexical choice more frequently. The same metric was associated with lower student score for how mentally demanding the tasks were perceived to be, which suggests that a shared lexicon may be associated with decreased cognitive load.        Another significant finding is the relationship between student Priming Ratio and student boredom, confusion, and frustration. In all the models, increased reports of these student 
emotions by the tutor corresponded to lower student Priming Ratio.  This result suggests that tutor reuse of student lexical choice may be associated with positive affective outcomes.       Finally, the tutor?s Matched Word Ratio is a significant negative predictor of learning gains, and also a significant negative predictor for student confusion. This finding may be related to the fact that by reusing more student language, the tutor may be effectively introducing fewer novel contributions that might lead to confusion.  6 Conclusion and Future Work Understanding how convergence unfolds holds significant promise for designing more effective dialogue systems. Toward that end, this paper has explored convergence in task-oriented dialogue at three levels: at the level of pairs of utterances, across a single conversation, and over multiple conversations with the same interlocutors. The results demonstrate that within the corpus, the two interlocutors display increasing levels of convergence longitudinally. Additionally, the results suggest ways in which short-term and long-term convergence are associated with particular positive and negative aspects of dialogue success and user affect.       The findings have significant implications for dialogue systems. First, they suggest that not only may successful lexical priming aid in understanding (Stoyanchev and Stent 2009), it may also be associated with lower cognitive load for users. Additionally, it may be possible to leverage convergence to positively impact users? affective states with respect to emotions such as boredom, confusion, and frustration. These potential relationships suggest that work to further elucidate convergence phenomena is particularly promising because dialogue systems stand to benefit from strategically leveraging convergence and adaptation. Acknowledgments This work is supported in part by the National Science Foundation through Grants DRL-1007962 and CNS-1042468. Any opinions, findings, conclusions, or recommendations expressed in this report are those of the participants, and do not necessarily represent the official views, opinions, or policy of the National Science Foundation. 
0.015 
0.02 
0.025 
0.03 
0.035 
0.04 
0.045 
0 2 4 6 8 10 
Tu
to
r'
s 
P
ri
m
in
g 
R
at
io
 
Distance (number of utterances) 
TS1 
TS2 
TS3 
TS4 
TS5 
TS6 
0.03 
0.035 
0.04 
0.045 
0.05 
0.055 
0.06 
0.065 
0.07 
0 2 4 6 8 10 
S
tu
de
nt
's
 P
ri
m
in
g 
R
at
io
 
Distance (number of utterances) 
TS1 
TS2 
TS3 
TS4 
TS5 
TS6 
97
  
References  Brennan, S. (1996). Lexical Entrainment in Spontaneous Dialog. In Proceedings of the 1996 International Symposium on Spoken Dialogue, 41-44. Coulston, R., Oviatt, S., and Darves, C. (2002). Amplitude Convergence in Children?s Conversational Speech with Animated Personas. In Proceedings of the 7th International Conference on Spoken Language Processing, 2689?2692. Hart, S. and Staveland, L. (1988). Development of NASA-TLX (Task Load Index): Results of Empirical and Theoretical Research. In P.A. Hancock and N. Meshkati, eds., Human Mental Workload. 1988, 139-183. Hirschberg, J. (2008). High Frequency Word Entrainment in Spoken Dialogue. In Proceedings of ACL HLT, 169-172. O?Brien, H. and Toms, E. (2010). The Development and Evaluation of a Survey to Measure User Engagement. Journal of the American Society for Information Science and Technology, 6(1), 50-69. Pickering, M. and Garrod, S. (2004). Toward a Mechanistic Psychology of Dialogue. Behavioral and Brain Sciences, 27(2), 169-226. Reitter, D., Moore, J., and Keller, F. (2006). Priming of Syntactic Rules in Task-Oriented Dialogue and Spontaneous Conversation. In Proceedings of the 28th Annual Conference of the Cognitive Science Society, 685-690. Steinhauser, N., Campbell, G., Taylor, L., Scott, C., Dzikovska, M., and Moore, J. (2011). Talk Like an Electrician: Student Dialogue Mimicking Behavior in an Intelligent Tutoring System. In Proceedings of the 15th International Conference on Artificial Intelligence in Education, 361-368. Stoyanchev, S. and Stent, A. (2009). Lexical and Syntactic Priming and Their Impact in Deployed Spoken Dialog Systems. In Proceedings of NAACL HLT, 189-192. Ward, A. and Litman, D. (2007). Dialog Convergence and Learning. In Proceedings of the 13th International Conference on Artificial Intelligence in Education, 262-269.  Appendix A. Corpus Excerpt T: yes so what happens with the other paths? S: is it because the last statement is fullfilled so it has no need to print the error? S: i understand what is happening but i do not know how to explain it T: ok so you noticed that when the if statement directly before it is true then it does not go to the else T: but if the if statement directly before the else statement is false then it goes to the else statement S: yes. S: so i need to make all of them else if statements? T: yes 
 
Appendix B. Regression Models  ? p Norm. Learning Gain, R2 = .0687 Tutor?s MWR -.169 .0160 Task set number -.144 .0392  Engagement (Student-reported) R2 = .0892 Tutor?s number of characters -.527 .0007 Student?s mean word length .159 .0033 Tutor?s mean word length -.169 .0053 Tutor?s lexicon size .369 .0189  Mentally demanding (Student-reported) R2 = .217 Tutor?s PR (distances 1-5) -.128 .0118 Session length (ms) .174 .0065 Combined number of utterances .579 .0005 Tutor?s number of utterances -.475 .0040 Tutor?s number of characters -.439 .0031 Tutor?s mean word length -.118 .0496 Tutor?s lexicon size .627 <.0001  Student confusion*, R2 = .319 Student?s PR (distances 1-10) -.233 <.0001 Tutor?s number of matched words 1.04 <.0001 Tutor?s MWR -.523 <.0001 Task set number -.122 .0105 Session length (ms) .292 <.0001 Student?s number of characters .247 .0048 Combined lexicon size -.594 <.0001  Student frustration*, R2 = .300 Max value of Student?s PR .156 .0035 Session length (ms) .239 <.0001 Tutor?s number of utterances .460 <.0001 Tutor?s number of words .342 .0135 Tutor?s lexicon size -.748 <.0001  Student boredom*, R2 = .202 Student?s PR (distances 1-5) -.234 <.0001 Tutor?s number of utterances .261 .0001 Tutor?s lexicon size -.412 <.0001  Session successful overall*, R2 = .246 Tutor?s PR (distances 1-3) .186 .0002 ? Student?s PR (distances 1-10) .122 .0079 Session length (ms) -.420 <.0001 Tutor?s number of utterances .518 <.0001 Tutor?s number of words -.473 .0006 Tutor?s lexicon size .275 .0340 * = from tutor perception survey;  ? = standardized regression coefficient  
98
Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 247?256,
Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational Linguistics
Combining Verbal and Nonverbal Features to Overcome the ?Information Gap? in Task-Oriented Dialogue 
 Eun Young Ha, Joseph F. Grafsgaard, Christopher M. Mitchell,  Kristy Elizabeth Boyer, and James C. Lester Department of Computer Science North Carolina State University Raleigh, NC, USA {eha, jfgrafsg, cmmitch2, keboyer, lester}@ncsu.edu    Abstract Dialogue act modeling in task-oriented dialogue poses significant challenges. It is particularly challenging for corpora consisting of two interleaved communication streams: a dialogue stream and a task stream. In such corpora, information can be conveyed implicitly by the task stream, yielding a dialogue stream with seemingly missing information. A promising approach leverages rich resources from both the dialog and the task streams, combining verbal and non-verbal features. This paper presents work on dialogue act modeling that leverages body posture, which may be indicative of particular dialogue acts. Combining three information sources (dialogue exchanges, task context, and users? posture), three types of machine learning frameworks were compared. The results indicate that some models better preserve the structure of task-oriented dialogue than others, and that automatically recognized postural features may help to disambiguate user dialogue moves.  1 Introduction Dialogue act classification is concerned with understanding users? communicative intentions as reflected in their utterances. It is an important first step toward building automated dialogue systems. To date, the majority of work on dialogue act 
modeling has addressed spoken dialogue (Samuel et al, 1998; Stolcke et al, 2000; Surendran and Levow, 2006; Bangalore et al, 2008; Sridhar et al, 2009; Di Eugenio et al, 2010). However, with the increasing popularity of computer-mediated means of conversation, such as instant messaging and social networking services, automated analysis of textual dialogue holds much appeal. Dialogue act modeling for textual conversations has many practical application areas, which include web-based intelligent tutoring systems (Boyer et al, 2010a), chat-based online customer service (Kim et al, 2010), and social media analysis (Joty et al, 2011). Human interaction involves not only verbal communication but also nonverbal communication. Research on nonverbal communication (Knapp and Hall, 2006; Mehrabian, 2007; Russell et al, 2003) has identified a range of nonverbal cues, such as posture, gestures, eye gaze, and facial and vocal expressions. However, the utility of these nonverbal cues has not been fully explored within the context of dialogue act classification research. Previous research has leveraged prosodic cues (Sridhar et al, 2009; Stolcke et al, 2000) and facial expressions (Boyer et al, 2011) for automatic dialogue act classification, but other types of nonverbal cues remain unexplored. As a first step toward a dialogue system that learns its behavior from a human corpus, this paper proposes a novel approach to dialogue act classification that leverages information about users? posture. Posture has been found to be a significant indicator of a broad range of emotions (D?Mello and Graesser, 2010; Kapoor et al, 2007; Woolf et al, 2009). Based on the premise that emotion plays an 
247
important role in dialogue, this work hypothesizes that adding posture features will improve the performance of automatic dialogue act models.   The domain considered in this paper is task-oriented textual dialogue collected in a human tutoring study. In contrast to conventional task-oriented dialogue corpora (e.g., Carletta et al, 1997; Jurafsky et al, 1998; Ivanovic, 2008) in which conversational exchanges are carried out within a single channel of dialogue between the dialogue participants, the corpus used in this work utilizes two separate and interleaved streams of communication. One stream is the textual conversation between a student and a tutor (dialogue stream). The other is the student?s problem-solving activity (task stream). As will be described in Section 3, the interface used in the corpus collection was designed to allow the tutor to monitor the student?s problem-solving activities. Thus, the student?s problem-solving activities and the tutor?s monitoring of those activities functioned as an implicit communication channel. This characteristic of the corpus poses significant challenges for dialogue act modeling. First, because the dialogue stream and the task stream are interleaved, the dialogue stream alone may not be coherent. Second, since information can be exchanged implicitly via the task stream, the dialogue likely contains substantial information gaps1. Addressing these challenges, the dialogue act models described in this paper combine three sources of information: the verbal information from the dialogue stream, the task-related context from the task stream, and information about users? posture. This paper makes several contributions to the dialogue research community. First, it is the first effort to explore posture as a nonverbal cue for dialogue act classification. Second, the proposed approach is fully automatic and ready for real-world application. Third, this paper explicitly defines the notion of information gap in task-oriented dialogue consisting of multiple communication channels, which has only begun to be explored in the context of dialogue act classification (Boyer et al, 2010a). Finally, this                                                 1 In this paper, information gap is defined as the information that is missing from the explicit verbal exchanges between the dialogue participants but conveyed by the implicit task stream. 
paper examines adaptability of previous dialogue act classification approaches in conventional task-oriented domains by comparing three classifiers previously applied to dialogue act modeling for task-oriented dialogue. 2 Related Work A rich body of research has addressed data-driven approaches for dialogue act modeling. Russell et al (2003) applied a transformation-based learning approach for dialogue act tagging for spoken dialogue, using speaker direction, punctuation, marks, and cue phrases. Stolcke et al (2000) modeled the structure of dialogue as an HMM, treating the dialogue acts as the observations emitted from the hidden states of the learned HMM. More recently, Bangalore et al (2008) proposed a unified approach to task-oriented dialogue, in which both the user dialogue act classification and the system dialogue act selection were informed by a shared maximum entropy dialogue act classifier. Sridhar et al (2009) also used a maximum entropy model, exploring the utility of different representations of prosodic features. Di Eugenio et al (2010) used a memory-based classifier, in combination with a modified latent semantic analysis (LSA) technique by augmenting the original word-document matrix in LSA with rich linguistic features. While most work on dialogue act modeling has focused on spoken dialogue, a recent line of investigation has explored the analysis of textual conversation, such as asynchronous online chat conversation (Wu et al, 2005; Forsyth, 2007; Reitter et al, 2010; Joty et al, 2011) and synchronous online chat conversation   (Ivanovic, 2008; Kim et al, 2010; Boyer et al, 2010a). Wu et al (2005) proposed a transformation-based learning approach for an asynchronous chat posting domain, utilizing regular expression-based selection rules. For a similar domain, Forsyth (2007) applied neural networks and Na?ve Bayes classification technique using lexical cues. Ritter et al (2010) and Joty et al (2011) applied unsupervised learning approaches to dialogue act modeling for Twitter conversations, in which dialogue acts were automatically discovered by clustering raw utterances. Work by Ivanovic (2008) and Kim et al (2010) analyzed one-to-one synchronous online chat dialogue in a task-oriented 
248
customer service domain. Ivanovic (2008) applied maximum entropy, na?ve Bayes, and support vector machines using word n-gram features. Kim et al (2010) compared the CRF, HMM-SVM, and Na?ve Bayes classifiers using word n-grams and features extracted from the dialogue structure, in which CRF achieved the highest performance. Boyer et al (2010a) investigated dialogue act modeling for task-oriented tutorial dialogue, applying a logistic regression approach using lexical, syntactic, dialogue structure, and task structure features. Some previous dialogue act modeling work (Boyer et al, 2011; Sridhar et al, 2009; Stolcke et al, 2000) leveraged nonverbal information such as prosodic cues (Sridhar et al, 2009; Stolcke et al, 2000) and facial expressions (Boyer et al, 2011). Stolcke et al (2000) combined various prosodic features such as pitch, duration, and energy. Sridhar et al (2009) represented the sequence of prosodic features as n-grams. Boyer et al (2011) leveraged confusion-related facial expressions for tutorial dialogue. Like Boyer et al (2010a), this work addresses dialogue act classification for task-oriented textual conversation in a web-based tutoring domain. In contrast to Boyer et al (2010a), whose approach directly leveraged manually annotated features, making it challenging to apply the proposed model to a real-world system, the present work is fully automatic and ready for real-world application.  A novel feature of this work is its utilization of nonverbal cues carried by users? posture. This is the first dialogue act classification work that leverages posture information. 3 Data The corpus used in this paper consists of textual exchanges between a student and a tutor in a web-based remote-tutoring interface for introductory programming in Java. The corpus was collected from a series of six tutoring lessons, covering progressive topics in computer science over the course of four weeks. The tutoring interface consisted of four windows: a task window displaying the current programming task; a code window in which the student writes Java code; an output window for displaying the result of compiling and running the code; and a chat window for instant exchange of textual dialogue 
between the student and tutor. With this tutoring interface, the student and the tutor were able to exchange textual dialogue and share a synchronized view of the task. Apart from sending dialogue messages, the only action the tutor could perform to affect the student?s interface was advancing to the next programming task.  3.1 Data Collection The data collection conducted in Fall 2011 paired 42 students with one of four tutors for six forty-minute tutoring sessions on introductory computer science topics.  The students were chosen from a first-year engineering course and were pre-screened to filter out those with significant programming experience. The tutors were graduate students with previous tutoring or teaching experience in Java programming. Students were compensated for their participation with partial course credit. The students worked with the same tutor for the entire study. Each lesson consisted of between four and thirteen distinct subtasks. During each tutoring session, the dialogue text exchanged between the student and the tutor was logged to a database. Additional runtime data including content of the student?s Java code, the result (e.g., success or failure) of compiling and running the student?s code, and the IDs of the subtask were logged. All logged data were time-stamped at a millisecond precision. Students? body posture was recorded at a rate of 8 frames per second with a Kinect depth camera, which emits infrared rays to measure distance for each pixel in a depth image frame. The camera was positioned above the student?s computer monitor, ensuring the student?s upper body is centered in the recorded image. Tutors were not recorded. 3.2 Dialogue Act Annotation For the work described in this paper, a subset of the collected data was manually annotated, which include the first of the six tutoring lessons from 21 students. This corpus contains 2564 utterances (1777 tutor, 787 student). The average number of utterances per tutoring session was 122 (min = 74; max = 201). The average number of tutor utterances per session was 84.6 (min = 51; max = 137) and the average number of student utterances per session was 37.4 (min = 22; max = 64). 
249
Extending a previous annotation scheme used for similar task-oriented tutorial dialogue (Boyer et al, 2010b), the scheme used in this work consists of 13 dialogue act tags (Appendix). The dialogue turns that contained more than one dialogue function were segmented into multiple utterances before being assigned a dialogue act tag. The annotation scheme did not constrain any of the dialogue act tags as applying either to students? or tutors? utterances only; however, the resulting distribution of the tags in the annotated corpus show certain dialogue act tags were more relevant to either students? or tutors? utterances. Figure 1 depicts an excerpt from the corpus with the manually applied dialogue act annotations.  
 Three human annotators were trained to apply the scheme. The training consisted of an iterative process involving collaborative and independent tagging, followed by refinements of the tagging protocol. At the initial phase of training, the annotators tagged the corpus collaboratively. In later phases annotators tagged independently. To compute agreement between different annotators, 24% (5 of the 21 sessions) of the corpus were doubly annotated by two annotators. All possible 
pairs of the annotators participated in double annotation. The aggregate agreement was .80 in Cohen?s Kappa (Cohen, 1960). 3.3 Posture Estimation Posture has been found to be a significant indicator of a broad range of emotions such as anxiety, boredom, confusion, engaged concentration (or flow), frustration, and joy (D?Mello and Graesser, 2010; Kapoor et al, 2007; Woolf et al, 2009). Early investigations into posture utilized pressure-sensitive chairs which provided indirect measures of upper-body posture (D?Mello and Graesser, 2010; Kapoor et al, 2007; Woolf et al, 2009). Newer, computer vision-based techniques provide more detailed postural data (Sanghvi et al, 2011). The present work uses a posture estimation algorithm developed to automatically detect the head, mid torso, and lower torso through depth image recordings of seated individuals (Grafsgaard et al, 2012). With this estimation algorithm, posture is represented as a triple of head depth (distance between camera and head), mid torso depth, and lower torso depth. A dataset of depth camera recordings from the first of the six tutoring lessons consists of 512,977 depth image frames collected across 18.5 hours of computer-mediated human-human tutoring among 33 participants.2 For each depth image frame, the posture algorithm scanned through the three middle regions that corresponded to head, mid-torso, and lower-torso of the recorded person, and selected a single representative depth pixel from each region. The boundaries for each region were heuristically determined relying on the placement of the students? chairs in the middle of the depth recording view at a common distance. Given these constraints, the model was manually verified by two independent human judges to have 95.1% accuracy across 1,109 depth image snapshots corresponding to one-minute intervals across the dataset. The algorithm output for each depth image was labeled as erroneous if either judge found that any of the posture tracking points did not coincide with its target region. Example output of the algorithm is shown in Figure 2.  
                                                2 The other 9 sessions were not successfully recorded because of technical errors. 
Tutor: hang on :) [S] Tutor: When we show you example code, it is not the code you need to write. [S] Tutor: Look at the task again. [H] Student writes programming code Tutor: YUP [PF] Tutor: Perfect [PF] Tutor: OK. Go ahead and test. [DIR] Student: And I don't need anything in the parentheses? [Q] Tutor: Line 9 is correct. You do NOT need anything inside the parentheses. [A] Student: Ok [ACK] Student compiles and runs code successfully Tutor: Good. [PF]  Tutor: Moving on. [S] Tutor advances to the next task. Student writes programming code Tutor: Syntactically correct. But there is a logic error [LF] Tutor: When will the output statement display your request to the player? [Q] Student: AFTER they put in their name [A] Tutor: Exactly [PF] Figure 1. Corpus Excerpt with Dialogue Act Annotation 
250
4 Features For web-based one-to-one dialogue systems, it is important to achieve efficient runtime performance. To maximize real-world feasibility of the learned dialogue act classifiers, this work only considers the features that can be automatically extracted at runtime. In addition, the use of linguistic analysis software, such as a part-of-speech tagger and a syntactic parser, is intentionally restrained. One might argue that rich linguistic analysis may provide additional information to dialogue act classifiers, potentially improving the performance of learned models. However, there is a trade-off between additional information obtained by rich linguistic analysis and processing time. In addition, previous work (Boyer et al, 2010a) found part-of-speech and syntax features did not provide obvious benefit for dialogue act classification in a domain similar to the one considered in this work. The dialogue act classifiers described in this paper integrate four classes of features automatically extracted from three sources of information: the textual dialogue utterances, task-related runtime information logged into the database, and the images of the students recorded by depth cameras. Each feature class is explained in the following subsections. 4.1 Lexical Features Based on previous dialogue act classification research (Bangalore et al, 2008; Boyer et al, 2010a; Kim et al, 2010), this work utilizes word n-grams as features for dialogue act classification. In the experiment reported in Section 5, unigrams and 
bigrams were used. Adding higher order n-grams did not improve model accuracies. In our corpus (Section 3), the nature of the student dialogues is informal and utterances contain many typos. To remove undesirable noise in the data such as typos and rare words, n-grams were filtered out according to their frequency in the training data (i.e., n-grams that appear less than a predefined cutoff threshold in the training data are not included as features). The value of the cutoff threshold was empirically determined by testing the values between 0 and 10 on a development data set that consisted of 20% of randomly selected dialogue sessions. The value of 3 was selected as it yielded the highest classification accuracy. 4.2 Dialogue Context Features While lexical features characterize the intrinsic nature of individual utterances, the context of the utterance within a larger dialogue structure provides additional information about a given utterance in relation with other utterances. This work considers the following dialogue context features: ? Utterance Position: Specifies the relative position of an utterance at a given turn. The value of this feature indicates whether the utterance is the first one in a given turn, the second or later one in a given turn, or the given turn consists of a single utterance. ? Length: Specifies the number of a given utterance in terms of individual word tokens. ? Previous Author: Indicates whether the author of the previous utterance was student or tutor. ? Previous Tutor Dialogue Act: Specifies dialogue act of the most recent tutor utterance. The value of this feature is directly extracted from the manual annotation in the corpus, because in the broader context of our work, tutor dialogue moves will be determined by an external dialogue management module.   4.3 Task Context Features In our data, students? problem-solving activities (e.g., reading the problem description, writing computer programming code, and compiling and running the code) functioned as an implicit communication channel between students and tutors (Section 1). Because of the existence of this 
Figure 2. Automatically detected posture points (H = headDepth, M = midTorsoDepth, L = lowerTorsoDepth)  
 H 
 M  L 
251
implicit communication channel, the dialogue exchanges between students and tutors likely contain substantial information gaps. To overcome such information gaps, it is important to identify effective task context features. The present work leverages the following task context features, which can be automatically extracted during runtime: ? Previous Task Action: Specifies the type of the most recent problem-solving action performed by the student. The value could be message (writing a textual message to the tutor) code (writing code in the code window), or compile_run (compiling or running the code). ? Task Begin Flag: A binary feature that indicates whether a given utterance is the first one since the current problem task was posted.  ? Task Activity Flag: Another binary feature indicating that a given utterance was preceded by a student?s task activity. ? Last Compile/Run Status: Specifies the status (e.g., begin, stop, success, error, input sent) of the most recent compile/run action performed by the students.  In addition to the listed task context features, the utility of time information was also explored, such as the amount of time taken for previous coding activity and the elapsed time since the beginning of the current task. However, these features did not positively impact the performance of the learned models and were thus excluded. 4.4 Posture Features After preprocessing recorded image frames with the estimation algorithm (Section 3.3), students? postures were represented as tuples of three different integer values, each respectively representing head depth, mid torso depth, and lower torso depth. To extract posture features, the time window of n seconds directly preceding a given utterance was compared with the previous time window of the same size in terms of min, max, median, average, and variance of each depth value. The indicators of whether each of these values has increased, decreased or remained the same were considered as potential posture features. To avoid introducing errors to the model by insignificant changes in posture, an error tolerance ?  was allowed (i.e., the two compared postures 
were considered the same unless the amount of the change in the posture was greater than ?). Optimal values for n and ?  were empirically determined, selecting the values that maximized classification accuracy on the development data set. For n, the values between 0 and 60 were compared at an interval of 10. The value of 50 was selected for head depth and 60 for both mid torso depth and lower torso depth.  Similarly, the value of ?  was determined by comparing the values between 0 and 200 with an increment of 10. The selected value was 100.  All the potential posture features were examined in an informal experiment, in which each of the potential posture features were added to the combination of the lexical, the dialogue context, and the task context features. The posture features that improved the classification accuracy after adding them were included in the present dialogue act models. The selected posture features are min of head depth and max, median, and average of lower torso depth. None of the mid torso depth features were selected. 5 Experiment The goal of this experiment is twofold: (1) to evaluate the effectiveness of the feature classes and (2) to compare the performance of three classifiers: maximum entropy (ME), na?ve Bayes (NB), and conditional random field (CRF). These classifiers are chosen because they have been shown effective for dialogue act modeling in traditional task-oriented textual dialogue, in which conversational exchanges were carried out by a single channel of dialogue (Ivanovic, 2008; Kim et al, 2010). Previous result by Kim et al (2010) suggests a structured model such as CRF yields more accurate dialogue act model compared to unstructured models (e.g., Na?ve Bayes), because of its ability to model the sequential patterns in target classification labels. This experiment examines whether a similar finding is observed for our domain, which exhibits substantial information gaps due to the existence of an implicit communication channel, the task stream. 5.1 Dialogue Act Modeling All classifiers were built using the MALLET package (McCallum, 2002). This experiment used the manually annotated portion of the data 
252
described in Section 3. The original dialogue scheme (Section 3.2) was slightly modified by introducing an additional dialogue act, GR, in order to distinguish conventional expressions, such as greetings and thanks, from other information-delivering utterances. For this modified scheme, annotator agreement was 0.81 in Cohen?s Kappa on the doubly annotated portion of the corpus. 6 among the 21 dialogue sessions in the annotated data do not have accompanying images due to technical problems with the depth camera, thus these sessions were excluded from this experiment. Table 1 shows the distribution of the student dialogue act tags in the resulting corpus of 15 dialogues used in this experiment. The most frequent tag was A (answer), followed by ACK (acknowledgement) and Q (question). The features were extracted by aligning three sources of information (the textual dialogue corpus, the task-related runtime log data, and the recorded images) by timestamp. Word boundaries in the dialogue corpus were recognized by the surrounding white spaces and punctuations. The dialogue context features (D) leveraged in this paper includes previous tutor dialogue act. This feature takes the manually annotated value in the corpus, because this work assumes the existence of an external dialogue manager. However, since the external dialogue manager is not likely to achieve 100% accuracy in predicting human tutor dialogue acts, it would be informative to estimate a reasonable range of the accuracies of the student dialogue act model, taking into account the errors introduced by the dialogue manager. For this reason, two versions of the dialogue context features were considered in this experiment: one that leverages the full set of dialogue context features (D) and the other that excludes previous 
tutor dialogue act (D-). These respectively provide the maximum and the minimum expected accuracy of the student dialogue act model, when used with a dialogue manager. The models were trained and tested using five-fold cross validation, in which the 15 dialogue sessions were partitioned into 5 non-overlapping sets of the same size (i.e., 3 sessions per partition). Each set was used for testing exactly once. 5.2 Results Table 2 reports the average classification accuracies from the five-fold cross validation. The majority baseline accuracy for our data is .347, when the classifier always chooses the most frequent dialog act (A). The first group of rows in Table 3 report the accuracies of individual feature classes. All of the individual features performed better than the baseline. The improvement from the baseline was significant except for D- with CRF. The most powerful feature class was dialogue context class when the full set was used. The second group in Table 3 shows the effects of incrementally combining the feature classes. Adding dialogue act features to the lexical features (L + D) brought significant improvement in the classification accuracy for ME and CRF. Adding posture features (L + D + T + P) also improved the accuracy of ME by a statistically significant margin. The last group shows similar results for ME when the previous tutor dialogue act was excluded from the dialogue context, except that the improvement achieved by adding the posture features (L + D- + T + P) was not significant.  
Student Dialogue Act Distribution A (answer) 192 (34.7%) ACK (acknowledgement) 124 (22.4%) Q (question)  92 (16.6%)  S (statement) 76 (13.7%) GR (greeting and thanks) 52 (9.4%) C (clarification) 6 (1.0%) RF (request for feedback) 5 (.9%) RC (request confirmation) 2 (.4%) O (other) 5 (.9%) Total 554 Table 1. Student dialogue acts in the experiment data 
Features ME NB CRF 
 Indiv
idual  Lexical (L)     .696
**     .703**     .599**  Dialogue (D)     .711**     .715**     .696**  Dialogue- (D-)     .477**     .473**     .405  Task (T)     .405**     .396*      .386*  Posture (P)     .382*     .385*     .399* 
 Max  L + D     .772
??     .724     .692??  L + D + T     .777     .729     .694  L + D + T + P     .789?     .714     .682 
 Min  L + D-     .724
??     .681     .606  L + D- + T     .733     .671     .627  L + D- + T + P     .750     .676     .644 Table 2. Classification accuracies (*p < .05, **p < .01 compared to baseline; ??p < .01 compared to L; and ?p < .05 compared to L + D + T, with paired-samples t-test)  
253
The highest accuracy was achieved by ME when using all four classes of the features, with maximum (L + D + T + P) .789 and minimum (L + D- + T + P) .750. For both the maximum and the minimum conditions, the differences among the classifiers were significant (p < .01, one-way repeated measure ANOVA), with post-hoc Tukey HSD tests revealing ME was significantly better than both NB (p < .05) and CRF (p < .01). There was no significant difference between NB and CRF. 6 Discussion The experiment described in Section 5 compared the utility of lexical, dialogue context, task context, and posture features for dialogue act classification. The results indicate the effectiveness of these features. Particularly, adding the dialogue context and the posture features improved the accuracy of the maximum entropy model. Although the margin of improvement achieved by adding posture features was relatively small, the improvement was statistically significant (p < .05) for the maximum condition (L + D + T + P), which suggests that the users? posture during computer-mediated textual dialogue conveys important communicative messages. The experiment also compared three classifiers: maximum entropy, na?ve Bayes, and CRF. Interestingly, CRF was the worst-performing model for our data, contradicting the previous finding by Kim et al (2010), in which CRF (a structured classifier) performed significantly better than Na?ve Bayes (a non-structured classifier). This contradictive result suggests that, in our domain, the presence of an implicit communication channel resulted in substantial information gaps in the dialogue and it poses new challenges that were not encountered by conventional task-oriented domains consisting of a single communication channel.  The maximum entropy classifier achieved the best overall performance, reaching accuracy of .789. This is an encouraging result compared to previous work in a similar domain. Boyer et al (2010a) reported an accuracy of .628 for dialogue act classification in a similar domain. However, a direct comparison is not applicable since different data were used in their work. 
7 Conclusions and Future Work Dialogue act modeling for a task-oriented domain in which the dialogue stream is interleaved with the task stream poses significant challenges. With the goal of effective dialogue act modeling, this work leverages information about users? posture as non-verbal features. An experiment found that posture is a significant indicator of dialogue acts, in addition to lexical features, dialogue context, and task context. The experiment also compared three statistical classifiers: maximum entropy, naive Bayes, and CRF. The best performing model was maximum entropy. Using all features, the maximum entropy achieved .789 in accuracy. Several directions for future work are promising. First, given the encouraging finding that nonverbal information plays a significant role as a communicative means for task-oriented dialogue, various types of non-verbal information can be investigated, such as gesture and facial expressions. Second, incorporating richer task features, such as in our case, deep analysis of student code, may contribute to more accurate dialogue act modeling. Third, it is important to generalize the findings to a larger data set, including across other task-oriented domains.  Finally, the community is embracing a move toward annotation-lean approaches such as unsupervised or semi-supervised learning, which hold great promise for dialogue modeling. Acknowledgments This research was supported by the National Science Foundation under Grant DRL-1007962. Any opinions, findings, and conclusions expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation. References Bangalore, S., Di Fabbrizio, G., & Stent, A. (2008). Learning the structure of task-driven human-human dialogs. IEEE Transactions on Audio, Speech, and Language Processing, 16(7), 1249-1259. Boyer, K. E., Grafsgaard, J. F., Ha, E. Y., Phillips, R., & Lester, J. C. (2011). An affect-enriched dialogue act classification model for task-oriented dialogue. Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human 
254
Language Technologies (pp. 1190-1199). Portland, OR. Boyer, K. E., Ha, E. Y., Phillips, R., Wallis, M. D., Vouk, M. A., & Lester, J. C. (2010a). Dialogue Act Modeling in a Complex Task-Oriented Domain. Proceedings of the 11th Annual SIGDIAL Meeting on Discourse and Dialogue (pp. 297-305). Tokyo, Japan. Boyer, K. E., Phillips, R., Ingram, A., Ha, E. Y., Wallis, M., Vouk, M., & Lester, J. (2010b). Characterizing the effectiveness of tutorial dialogue with hidden markov models. Proceedings of the 10th international conference on Intelligent Tutoring Systems (pp. 55-64). Pittsburgh, PA. Carletta, J., Isard, A., Isard, S., Kowtko, J., Doherty-Sneddon, G., & Anderson, A. (1997). The reliability of a dialogue structure coding scheme. Computational Linguistics, 23, 13?31. Cavicchio, F. (2009). The modulation of cooperation and emotion in dialogue: The REC corpus. Proceedings of the ACL-IJCNLP 2009 Student Research Workshop (pp. 81 - 87). Suntec, Singapore. Cohen, J. (1960). A coefficient of agreement for nominal scales. Educational and Psychological Measurement, 20(1), 37 - 46. Di Eugenio, B., Xie, Z., & Serafin, R. (2010). Dialogue act classification, instance-based learning, and higher order dialogue structure. Dialogue and Discourse, 1(2), 81 - 104. D?Mello, S., & Graesser, A. (2010). Mining Bodily Patterns of Affective Experience during Learning. Proceedings of the 3rd International Conference on Educational Data Mining (pp. 31-40). Pittsburgh, PA. Forsyth, E. N. (2007). Improving Automated Lexical and Discourse Analysis of Online Chat Dialog. Master's thesis. Naval Postgraduate School. Grafsgaard, J. F., Boyer, K. E., Wiebe, E. N., & Lester, J. C. (2012). Analyzing Posture and Affect in Task-Oriented Tutoring. Proceedings of the 25th Florida Artificial Intelligence Research Society Conference (pp. 438-443). Marco Island, FL. Ivanovic, E. (2008). Automatic instant messaging dialogue using statistical models and dialogue acts. Master's thesis. The University of Melbourne. Joty, S. R., Carenini, G., & Lin, C.-Y. (2011). Unsupervised Modeling of Dialog Acts in Asynchronous Conversations. Proceedings of the 22nd International Joint Conference on Artificial 
Intelligence (pp. 1807-1813). Barcelona, Catalonia, Spain. Jurafsky, D., Bates, R., Coccaro, N., Martin, R., Meteer, M., Ries, K., Shriberg, E., et al (1998). Switchboard discourse language modeling project report. Baltimore, MD. Kapoor, A., Burleson, W., & Picard, R. W. (2007). Automatic prediction of frustration. International Journal of Human-Computer Studies, 65(8), 724-736. Kim, S. N., Cavedon, L., & Baldwin, T. (2010). Classifying dialogue acts in one-on-one live chats. Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing (pp. 862-871). Cambridge, MA. Knapp, M. L., & Hall, J. A. (2006). Nonverbal Communication in Human Interaction (6th ed.). Belmont, CA: Wadsworth/Thomson Learning. McCallum, A. K. (2002). MALLET: A Machine Learning for Language Toolkit. Available from  http://mallet.cs.umass.edu Mehrabian, A. (2007). Nonverbal Communication. New Brunswick, NJ: Aldine Transaction. Ritter, A., Cherry, C., & Dolan, B. (2010). Unsupervised modeling of twitter conversations. Proceedings of Human Language Technologies: The 2010 Annual Conference of the North American Chapter (pp. 172 - 180). Los Angeles, CA. Russell, J. A., Bachorowski, J. A., & Fernandez-dols, J. M. (2003). Facial and vocal expressions of emotion. Annual Review of Psychology, 54, 329-349. Sanghvi, J., Castellano, G., Leite, I., Pereira, A., McOwan, P. W., & Paiva, A. (2011). Automatic analysis of affective postures and body motion to detect engagement with a game companion. Proceedings of the 6th international conference on Human-robot interaction (pp. 305-312). Lausanne, Switzerland. Sridhar, R., Bangalore, S., & Narayanan, S. (2009). Combining lexical, syntactic and prosodic cues for improved online dialog act tagging. Computer Speech and Language, 23(4), 407 - 422. Stolcke, A., Ries, K., Coccaro, N., Shriberg, E., Bates, R., Jurafsky, D., Taylor, P., et al (2000). Dialogue act modeling for automatic tagging and recognition of conversational speech. Computational Linguistics, 26(3), 339-373. Surendran, D., & Levow, G.-A. (2006). Dialog act tagging with support vector machines and hidden 
255
Markov models. Proceedings of Interspeech (pp. 1950 - 1953). Pittsburgh, PA. Woolf, B., Burleson, W., Arroyo, I., Dragon, T., Cooper, D., & Picard, R. W. (2009). Affect-aware tutors recognising and responding to student affect. International Journal of Learning Technology, 4(3/4), 129-164. Wu, T., Khan, F. M., Fisher, T. A., Shuler, L. A., & Pottenger, W. M. (2005). Posting Act Tagging Using Transformation-Based Learning. In T. Y. Lin, S. Ohsuga, C.-J. Liau, X. Hu, & S. Tsumoto (Eds.), Foundations of Data Mining and knowledge Discovery (pp. 319 - 331). Springer. 
  
Appendix. Dialogue Act Annotation Scheme and Inter-rater Agreement Tag Description Frequency Agreement (k) H  Hint:  The tutor gives advice to help the student proceed with the task Tutor:     Student:     133 0 .50 DIR   Directive:  The tutor explicitly tells the student the next step to take Tutor:     Student:     121 0 .63 ACK   Acknowledgement:  Either the tutor or the student acknowledges previous utterance; conversational grounding Tutor:       Student:  41 175 .73 RC   Request for Confirmation:  Either the tutor or the student requests confirmation from the other participant (e.g., ?Make sense??) Tutor:       Student:  11 2 Insufficient data RF   Request for Feedback:  The student requests an assessment of performance or work from the tutor Tutor:     Student:    0 5 1.0 PF  Positive Feedback:  The tutor provides a positive assessment of the student?s performance Tutor:     Student:     327 0 .90 LF Lukewarm Feedback:  The tutor provides an assessment that has both positive and negative elements Tutor:      Student:    13 0 .80 NF Negative Feedback:  The tutor provides a negative assessment of the student?s performance Tutor:        Student:     1 0 .40 Q Question:  A question regarding the task that is not a direct request for confirmation or feedback Tutor:     Student:  327 120   .95 A Answer:  An answer to an utterance marked Q Tutor:       Student:  96 295 .94 C Correction:  Correction of a typo in a previous utterance Tutor:       Student:  10 6 .54 S  Statement:  A statement regarding the task that does not fit into any of the above categories Tutor:     Student:  681 174 .71 O Other: Other utterances, usually containing only affective content Tutor:     Student:  6 10 .69 
256
Proceedings of the SIGDIAL 2013 Conference, pages 204?213,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Learning Dialogue Management Models for Task-Oriented Dialogue with Parallel Dialogue and Task Streams 
 Eun Young Ha, Christopher M. Mitchell, Kristy Elizabeth Boyer,  and James C. Lester Department of Computer Science North Carolina State University Raleigh, NC 27695, USA {eha,cmmitch2,keboyer,lester}@ncsu.edu     Abstract 
Learning dialogue management models poses significant challenges. In a complex task-oriented domain in which information is ex-changed via parallel, interleaved dialogue and task streams, effective dialogue management models should be able to make dialogue moves based on both the dialogue and the task context. This paper presents a data-driven ap-proach to learning dialogue management mod-els that determine when to make dialogue moves to assist users? task completion activi-ties, as well as the type of dialogue move that should be selected for a given user interaction context. Combining features automatically ex-tracted from the dialogue and the task, we compare two alternate modeling approaches. The results of an evaluation indicate the learned models are effective in predicting both the timing and the type of system dialogue moves. 1 Introduction Automated dialogue systems allow users to in-teract with information systems in a natural and intuitive manner. With the growth of speech-enabled applications for mobile devices, the de-mands for practical dialogue systems have been increasing at an accelerating pace. The core tasks of automated dialogue systems include dialogue management, which is concerned with selecting system actions in response to a given user input. Traditionally, dialogue managers have been manually constructed. However, manually craft-ing dialogue managers is labor-intensive and yields systems that are brittle with respect to un-expected user behaviors. For rapid creation of robust and adaptive dialogue systems, data-driven approaches to dialogue management hold 
much appeal. Recent work on dialogue systems has explored machine learning techniques to au-tomatically learn dialogue managers from corpo-ra (Scheffler and Young, 2002; Hardy et al, 2006; Williams and Young, 2007; Bangalore et al, 2008; Sridhar et al, 2009). To support more natural human-computer dia-logue, earlier work on dialogue systems envi-sioned rich interaction environments that take into account observed user actions for selecting optimal dialogue strategies (Carberry, 1990; Rich and Sidner, 1998; Allen et al, 2001). However, recent data-driven approaches have primarily focused on application domains in which infor-mation between the user and the system are communicated solely by dialogue, such as tele-phone-based systems (Hardy et al, 2006; Bangalore et al, 2008) and online chat dialogues (Ivanovic, 2008; Kim et al, 2010). With increas-ing demands for natural human-computer inter-action beyond these restricted application do-mains, dialogue systems are required to support more complex types of interaction, in which us-ers perform tasks in parallel to exchanging dia-logue. For instance, dialogue interfaces for task-assistance systems, such as intelligent tutoring systems, should be able to monitor users? task completion activities and incorporate the ob-served activities into dialogue management deci-sions such that the systems can provide users with spontaneous assistance (e.g., providing hints) even without an explicit request from the user.  We have been exploring data-driven ap-proaches for a complex task-oriented application domain in which information is delivered both by exchanging dialogue with users and by ob-serving users? task completion activities. Our previous work has focused on the automatic in-terpretation of user dialogue input (Boyer et al, 
204
2010; Ha et al, 2012). Findings suggest that identifying an effective representation to com-bine information from dialogue and users? task completion activities is key to effective dialogue processing in a domain consisting of parallel dia-logue and task streams. As the next step in this line of investigation on complex task-oriented domains with parallel dia-logue and task streams, this work proposes an approach to automatically learning dialogue management models from a human dialogue cor-pus. The proposed approach combines infor-mation from a dialogue stream and a task stream in order to create spontaneous dialogue interven-tions for users based on monitoring users? activi-ties. Two subtasks of dialogue management are addressed: the first is to determine when to pro-vide dialogue feedback (timing), and the second is to determine what kind of dialogue feedback to provide (type). Dialogue managers in conven-tional domains have primarily focused on the selection of feedback type. However, determin-ing the appropriate timing of system moves is critical for dialogue systems that support parallel dialogue and task streams.  The work presented here makes three contri-butions. First, it endeavors to expand data-driven dialogue management by addressing more com-plex task-oriented domains consisting of parallel dialogue and task streams. Second, it proposes a timing intervention model that determines the correct time to make spontaneous system inter-ventions. Third, it presents a maximum entropy dialogue management model and compares al-ternate approaches. It also compares the predic-tive power of the dialogue and task streams on the targeted dialogue management tasks. 2 Related Work Data-driven approaches to dialogue management continue to be the subject of increasing attention within the dialogue community. Prominent among these are reinforcement learning ap-proaches for learning dialogue policies from cor-pora (Henderson et al, 2008; Levin et al, 2000; Lewis and Di Fabbrizio, 2006; Roy et al, 2000; Scheffler and Young, 2002; Singh et al, 2002; Williams and Young, 2007; Young, 2002). These approaches model dialogue as Markov decision processes, either fully observable (MDPs) or par-tially observable (POMDPs), in which the transi-tions of dialogue states are associated with sys-tem actions and rewards. The goal of reinforce-ment learning is to learn optimal policies that 
maximize aggregate expected rewards, such as user satisfaction (Walker et al, 1997). Learned policies that result from RL exploration do not, by design, necessarily reflect the patterns in the bootstrap dialogue corpus. Additionally, to cover all possible state spaces, reinforcement learning typically requires a very large set of training da-ta, which limits the complexity of the dialogue system in its representation of the dialogue states and the system actions (Young et al, 2013).  A second body of related work focuses on dia-logue act classification. Classification-based ap-proaches aim at learning the patterns of dialogue that are present in the corpus. A variety of ma-chine learning frameworks have been exploited, including hidden Markov models (Stolcke et al, 2000; Boyer et al,  2010), maximum entropy models (Bangalore et al, 2008; Sridhar et al, 2009; Ha et al, 2012), support vector machines (Ivanovic, 2008), conditional random fields (Kim et al, 2010),  and memory-based classifiers in combination with latent semantic analysis (Di Eugenio et al, 2010). Classification-based ap-proaches incorporate rich sets of features, includ-ing not only lexical information, syntactic fea-tures, and dialogue structure, but also prosodic features in the case of spoken dialogue (Stolcke et al, 2000; Sridhar et al, 2009) and non-verbal features such as facial expressions (Boyer et al, 2011) and shifts in posture (Ha et al, 2012). While most work on dialogue act classifica-tion has focused on either offline analysis of dia-logue (Stolcke et al, 2000; Ivanovic, 2008; Kim et al, 2010; Di Eugenio et al, 2010) or interpre-tation of user dialogue (Boyer et al, 2010; Ha et al, 2012), Bangalore et al (2008) utilized dia-logue act classification as a mechanism for de-termining system dialogue moves. They pro-posed a unified dialogue act classification ap-proach for both the interpretation of user utter-ances and selection of system dialogue moves. Our work is similar to Bangalore et al (2008) in that it takes a dialogue act classification ap-proach to the task of selecting system dialogue moves. However, it addresses the problems posed by complex task-oriented application do-mains in which information is communicated not only by dialogue exchanges but also by monitor-ing users? task performance. In such domains, a user?s task activities constitute a full communica-tive stream in its own right, separate from the dialogue stream. The challenges of parallel dia-logue and task streams are addressed by exploit-ing automatically obtained task features com-bined with dialogue features. In contrast to pre-
205
vious work (Bangalore et al 2008, Boyer et al, 2010), in which task information was derived from manual annotation, our work utilizes auto-matically computed task features. Our work also focuses on a growing applica-tion area of dialogue systems: intelligent tutor-ing. In support of student learning, recent work in this area utilized human tutorial dialogue cor-pora to learn effective tutorial strategies using MDPs (Chi et al, 2010; Mitchell et al, 2013), to develop tutorial dialogue models that adapt to students? affective states (Forbes-Riley and Litman, 2011), and to improve robustness of a symbolic tutorial dialogue system (Dzikovska et al, 2013).  3 Task-Oriented Dialogue Corpus To learn dialogue management models from nat-urally occurring human-to-human dialogue we utilize a human tutorial dialogue corpus we col-lected in the domain of introductory program-ming in Java. The corpus consists of textual dia-logue exchanges between students and tutors in a web-based remote-tutoring interface, aligned with task context logs (Appendix A). A subset of the corpus was annotated with dialogue acts, which was used to train and test the dialogue management models described in this paper. 3.1 Human tutoring study The data collection study involved forty-two un-dergraduate students who were paired with one of four tutors. The students were enrolled in a first-year engineering course and were pre-screened to filter out those with significant pro-gramming experience. The students were com-pensated for their participation with partial course credit. The tutors were graduate students with previous tutoring or teaching experience in Java programming, and the students worked with the same tutor for the entire study. Each lesson consisted of between four and thirteen distinct subtasks.  The students completed six forty-minute tutor-ing lessons, covering progressive topics in intro-ductory computer science over four weeks. Each lesson consisted of four to thirteen subtasks, in which later subtasks built upon earlier ones. Dur-ing each tutoring session, the paired student and tutor interacted remotely using a web-based tu-toring interface. With this tutoring interface, the student and the tutor were able to exchange tex-tual dialogue and share a synchronized view of the task.  
For each lesson, students completed a pre-test and a post-test before and after the main tutoring session. The pre- and post-test consisted of the same set of questions to assess students? knowledge related to the lesson?s objectives. Compared to students? pre-test results, signifi-cant learning gains were observed on the post-test, which indicates that the tutorial dialogue was effective for student learning (Mitchell et al, 2012).  3.2 Dialogue annotation A subset of the collected data was manually an-notated with dialogue acts using an annotation scheme consisting of 13 dialogue act tags for task-oriented tutorial dialogue (Table 1). The annotated corpus consists of the first of the six tutoring lessons from 21 students, which contains 2564 utterances (1777 tutor, 787 student). The average number of utterances per tutoring ses-sion was 122 (min = 74; max = 201). The aver-age number of tutor utterances per session was 84.6 (min = 51; max = 137), and the average number of student utterances per session was 37.4 (min = 22; max = 64). Three human annotators were trained to apply the scheme. The training consisted of an iterative process involving collaborative and independent tagging, followed by refinements of the tagging protocol. At the initial phase of training, the an-notators tagged the corpus collaboratively. In later phases annotators tagged independently. To compute agreement between different annotators, 24% (5 of the 21 sessions) of the corpus were doubly annotated by two annotators. All possible pairs of the annotators participated in double an-notation. The aggregate agreement was 0.80 in Cohen?s Kappa (Cohen, 1960). 4 Dialogue Management Models To support a task-oriented dialogue system capa-ble of not only responding to users? dialogue in-put but also providing spontaneous system inter-vention during users? task activities, a dialogue manager should provide two functionalities. The first is to determine the timing of a system dia-logue move (i.e., whether or not to provide a tu-torial dialogue move at a given context). The second is to determine the type of dialogue move (i.e., selecting from available system dialogue acts). In this work, the problem of determining the system?s next dialogue move is cast as a clas-sification task. In previous work we found a maximum entropy approach was effective for 
206
classifying user dialogue acts for task-oriented dialogue with parallel dialogue and task streams (Ha, 2012). Maximum entropy outperformed both Naive Bayes and conditional random fields. Building on these results, we employ a maximum entropy classifier to learn dialogue management models that predict both the timing and the type of the system dialogue move. The following sec-tions describe two alternate approaches to dia-logue management that can both determine the timing and determine the type of system dialogue interventions.  4.1 One-step dialogue management model In the first model, the two dialogue management tasks are framed as a single classification prob-lem by treating the decision of not to make a tu-torial dialogue move as a special dialogue act. Thus, a finite set of dialogue moves allowed for the system is defined as ? = ??,??,? ,?? , in which ? = ?? ? ? ?????? and ?? ={???, ???,? , ???}  is the set of dialogue acts available for the system. Given ?  and the ??? step in a given user interaction history ????? =  ????? , ??????,? , ??, the goal of the dia-logue management model is to predict system?s dialogue move ???? for the next step, which is determined by the following equation. ???? = ?????????? ? ?????              (1) 
The task-oriented dialogue considered in this work includes two parallel and interleaved data streams: an explicit dialogue stream, consisting of textual exchanges between a student and a tutor, and an implicit task stream, consisting of the student?s problem-solving activities. Thus, a given interaction history can be decomposed into a dialogue history and a task history, rewriting equation (1) as follows, ???? = ?????????? ? ????? , ?????     (2) in which ????? = ???? , ??????,? , ??  and ????? = ???? , ??????,? , ??  represent the history of dialogue utterances and the history of student task activities, respectively. In this work, the conditional probability distri-bution in Equation (2) is estimated using the maximum entropy framework (Berger et al, 1996). The maximum entropy framework selects a probability distribution that results in the high-est entropy among all possible solutions. Given a vector ? of feature set, the conditional probabil-ity distribution is estimated by the following equation, ? ? = ?? ? =  ? ??(?) ??????                     (3) in which ? represents weights and ? is a normal-izing factor. This work used MALLET 
Tag Description Agreement H Hint: The tutor gives advice to help the student proceed with the task .50 DIR  Directive: The tutor explicitly tells the student the next step to take .63 ACK  Acknowledgement: Either the tutor or the student acknowledges previous utterance; conversational grounding .73 RC  Request for Confirmation: Either the tutor or the student requests confirmation from the other participant (e.g., ?Make sense??) Insufficient data RF  Request for Feedback: The student requests an assessment of his performance or his work from the tutor 1.0 PF  Positive Feedback: The tutor provides a positive assessment of the student?s perfor-mance .90 LF Lukewarm Feedback: The tutor provides an assessment that has both positive and nega-tive elements .80 NF Negative Feedback: The tutor provides a negative assessment of the student?s perfor-mance .40 Q Question: A question which does not fit into any of the above categories .95 A Answer: An answer to an utterance marked Q .94 C Correction: Correction of a typo in a previous utterance .54 S  Statement: A statement which does not fit into any of the above categories .71 O Other: Other utterances, usually containing only affective content .69 Table 1.  Dialogue act annotation scheme and inter-rater agreement 
207
(McCallum, 2002) to estimate this conditional distribution.  4.2 Two-step dialogue management model A potential shortcoming of the one-step model is that the probability distribution over dialogue acts is prone to distortion depending on the por-tion of NoMove in the training data. To avoid this, the second model takes a two-step approach, treating each dialogue management task as an independent classification task. The two-step model first determines whether or not to make a dialogue move. If a decision is made to provide a dialogue move, the second classifier is called for a selection of the type of dialogue move.  In this model, system?s dialogue move ???? for the next interaction step is determined by a function ? ????? , such that ? ????? = ??????, ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? (4) when  ?? ?????? ????? > ? ???? ?????                               ? ????? = ??????? ? ? ? ?? ?????       (5) otherwise.  Similar to the one-step model, Equation (5) can be written as ? ????? = ??????? ? ? ? ?? ????? , ?????  (6) This conditional probability distribution is also estimated by the maximum entropy framework. 5 Features To learn high-performing dialogue management models for task-oriented dialogue with parallel dialogue and task streams, it is crucial to have an effective representation of user interaction state that captures information from all available data streams. The dialogue management models de-scribed in the previous section determine the sys-tem?s next dialogue move based on user interac-tion state specified by the features extracted from the dialogue and the task streams. In contrast to previous work on task-oriented dialogue, in which task information is incorporated into dia-logue utterances by manual tagging (Bangalore et al, 2008; Boyer et al, 2010), our work does not require manual effort to obtain the relevant task information. Instead, we rely on task context logs generated during students? interactions with the tutoring interface, as well as a notion of stu-dents? task progress automatically estimated by a task analysis algorithm. The same set of features 
is used for the prediction of both the timing and the type of system move. 5.1 Automatic task analysis In order to provide a measure of students? task progress through each of the tutoring sessions, an edit distance metric was implemented. This met-ric computes the minimum edit distance between a student?s program at a particular time t and a representative solution for a given programming task, in order to estimate how far away the stu-dent is from completing the task. Because our tutors were experienced in the subject matter and were familiar with the lesson structures, we can safely assume that they knew what this final state of the code would be and thus had an intuitive metric of student progress, which is analogous to our edit distance metric. As this value changes over a session, one can observe how the stu-dent?s progress is affected by tutor dialogue acts. Because a character-based edit distance would not capture the relative functional importance of each part of the student?s program, our edit dis-tance metric is based on tokenized versions of the program, as generated by the Java compiler, and the output is the number of tokens that differ from the solution for that task. In this way, comments, variable names, or string literals with many characters are all treated as single tokens and do not artificially inflate the edit distance. This tokenization also allows for abstraction of these comments, variable names, and string liter-als into generalized tokens so that they can be more easily compared between students.  5.2 Dialogue features Previous work on dialogue act classification has shown that lexical features extracted from dia-logue utterances are good predictors of dialogue acts (Bangalore et al, 2008; Boyer et al, 2010a; Kim et al, 2010). However, this finding does not apply when the goal of dialogue act classification is to learn dialogue management models because determining system moves precedes system ut-terance generation. Instead, this work exploits features that represent local interaction structure within dialogue streams, which includes current student dialogue act, current tutor dialogue act, previous tutor dialogue act, and tutor utterance count. ? Current student dialogue act represents the interpreted dialogue act for the previ-ous user dialogue input. Student dialogue act interpretation is not addressed in this 
208
paper, assuming the existence of an exter-nal module that carries out user dialogue interpretation (e.g., Ha et al, 2012). ? Current tutor dialogue act represents the type of system dialogue act at the current interaction step. In our tutorial dialogue corpus, we observed tutors often made several dialogue utterances in succession, such as a feedback (?Great Job.?) fol-lowed by a question (?Do you have any questions??). Thus, the value of the cur-rent tutor dialogue act impacts the proba-bility distribution over the tutor?s next dia-logue move. This feature captures such temporal patterns present in tutor dialogue moves as observed in the corpus. ? Previous tutor dialogue act represents the type of system dialogue act generated for the previous interaction step. This is simi-lar to the current tutor dialogue act fea-ture, but models longer temporal patterns by extending the size of interaction history. ?  Tutor utterance count represents the number of system dialogue acts generated in succession without interruption until the current interaction step. In our corpus, it was observed that the tutor dialogue turns often consist of multiple utterances. This feature is included to model system dia-logue turns consisting of multiple utteranc-es. 5.3 Task features To create a rich representation of task context, a number of features were automatically extracted from task streams. Three groups of task infor-mation were considered, including types of task activity taken by user, the amount of time taken between certain activities, and the user?s task progress estimated by the task analysis algorithm (Section 5.1). Alternate representations of these features were empirically compared, resulting in the following task features incorporated in cur-rent dialogue management models. ? Current log type represents the type of activity taken at the current interaction step either by the user or the system. A complete list of log types is shown in Appendix B.  ? Previous log type represents the type of activity taken at the previous interaction step. Analogous to previous tutor dia-logue act in dialogue stream, this feature 
models temporal patterns among task ac-tivities. ? Same log type is a binary feature indi-cating the type of activities at the current and previous interaction step is identical.  ? Previous and current log type is a fea-ture that combines the current and previ-ous log types (i.e., a bigram of log types). ? Elapsed time is the amount of time since the last logged activity, which rep-resents the duration of the user?s inac-tivity. This feature is included to enable the learned dialogue management model to make spontaneous dialogue interven-tions when a user has been detected to be inactive for an excessive period of time.  ? Elapsed coding time specifies the amount of time the user has taken since the beginning of current coding task.  6 Evaluation The dialogue act models were trained and tested using the manually annotated portion of the task-oriented tutorial dialogue corpus described in Section 3. The textual dialogue exchanges in the corpus were aligned with the logged task-completion activities based on the timestamp, resulting in 6919 total interaction logs. Table 2 shows the distribution of different types of ac-tivities in the resulting interaction logs. It was observed that tutors made a dialogue move in 26.5% of the total logged interactions (Table 3). 
Among the thirteen dialogue acts in the origi-nal annotation scheme (Section 3.2), four rarely occurring dialogue acts were combined into other categories, which include LF (lukewarm feed-back) merged with NF (negative feedback) and RC (request for confirmation), RF (request for feedback), and C (correction) merged to O (oth-er).  A new category, GREET (greetings) was 
Interaction Type Frequency (%) Programming 38.2 Compiling the Program 10.8 Running the Program 12.2 Progressing to Next Task 4.2 Exchanging Dialogue 34.6 Table 2. Distribution of interaction types 
Tutor Dialogue Move Frequency (%) Move 26.5 NoMove 73.5 Table 3. Distribution of system dialogue move 
209
added to distinguish conventional expressions for greetings and thanks from more general state-ments and questions. Table 4 shows the resulting distribution of tutor dialogue acts in the corpus. 
The performance of the dialogue act models were evaluated in a ten-fold cross validation. In the cross validation, the corpus was partitioned to ten non-overlapping sets and each set was used as testing data exactly once, while models were trained using the remaining nine sets. 6.1 Results The first study compared the accuracies of the dialogue management models on predicting the timing and the type of tutor dialogue moves. The accuracy of the timing prediction was calculated for all user interaction logs in the data, including both dialogue exchanges and task-completion activities. The accuracy of the type prediction was calculated for dialogue activities by tutors only. The results are shown in Table 5. 
Both the one-step (t(9) = 4.14, p = 0.0013) and the two-step (t(9) = 6.26, p < .0001) models per-formed significantly better than the majority baseline in predicting the timing of tutorial dia-logue moves. The two-step model achieved higher accuracy than the one-step model. The difference between the two models was statisti-cally significant (t(9) = 2.17, p = 0.0291).  The one-step (t(9) = 2.68, p = 0.0126) and the two-step (t(9) = 10.93, p < 0.0001) models 
achieved significantly higher accuracies over the baseline for the task of predicting the type of tu-torial dialogue moves, as well. Again, the two-step model performed significantly better than the one-step model (t(9) = 4.22, p = .0011).  6.2 Comparing dialogue and task streams The second study compared the predictive power of the dialogue stream and the task stream on the given two dialogue management tasks. In this study, the accuracy of the two-step model was compared in three conditions: using the dialogue features only (Dialogue), using the task features only (Task), and using all features (All). Table 6 reports the results. 
For determining when to intervene, the dia-logue and the task features exhibited similar pre-dictive power. No statistical significance was found for the difference between the dialogue and the task conditions. The highest accuracy was achieved by the All condition. Compared to the All condition, the Dialogue condition showed statistically significant decrease in accuracy (t(9) = 2.21, p = 0.0272), which implies the task stream provided important features for the dia-logue management model in determining the tim-ing of tutorial dialogue moves. A similar trend was observed for determining what type of dialogue move to make. The Dia-logue and the Task conditions achieved similar accuracies, with the highest accuracy achieved by the All condition. The drops in accuracy com-pared to the All condition were statistically sig-nificant for both the Dialogue (t(9) = 3.38, p = 0.0040) and the Task conditions. (t(9) = 4.36, p = 0.0009). The results imply that the prediction of the type of tutorial dialogue moves required in-formation from both the dialogue and the task streams.  7 Discussion The experiments presented in Section 6 com-pared two alternate approaches to learning dia-logue management models for two given sub-tasks: determining when to provide the user with a dialogue move, and determining which type of 
Dialogue Act Frequency (%) S (Statement) 35.4 PF (Positive Feedback) 19.8 Q (Question) 16.0 H (Hint)  8.0 DIR (Directive)  6.6 A (Answer)  5.7 GREET (Greetings)  3.1 ACK (Acknowledgement)  2.3 NF (Negative Feedback)  1.5 O (Other)  1.6 Table 4. Distribution of tutor dialogue acts 
Model Timing Type Baseline 73.5 35.4 One-step 79.2* 40.5* Two-step  80.3*?  49.7*? Table 5. Model accuracy (%) on dialogue man-agement tasks (*statistical significance over baseline, ?statistical significance over one-step model) 
Features Timing Type Dialogue 79.6 45.0 Task 80.1 44.9 All  80.3*  49.7*? Table 6. Comparison of features on dialogue management tasks (*statistical significance over Dialogue, ?statistical significance over Task) 
210
dialogue move to choose. The results suggest that the two-step approach, which models the two subtasks as separate classifiers, was more effective than the alternate one-step approach, which combined the two subtasks into a single classification problem. The two-step model achieved higher performance than the one-step model in both the timing and the type prediction. However, the difference in the performance of the two models was more apparent in the type prediction, with the two-step model achieving over 22% higher accuracy than the one-step model. One possible explanation for the superi-ority of the two step-model over the one-step model is that the corpus used to train the models was highly skewed. For more than 73% of the total interaction logs in the corpus, the tutors did not provide any dialogue feedback. Since the one-step model treated NoMove as a special dia-logue act, the skewed distribution over NoMove and Move impacted the learned distribution over dialogue acts.  Two previous investigations reported the accu-racies of dialogue act classification on system utterances. Bangalore et al (2008) reported a prediction accuracy of 55% for system dialogue acts when a flat task model was used in a cata-logue-ordering domain. When a hierarchical task structure was used in the same domain, the achieved prediction accuracy for system dialogue acts was 35.6% (Bangalore and Stent, 2009). Boyer (2010) achieved accuracy of 57% for sys-tem dialogue acts in a task-oriented tutorial dia-logue. While both of these lines of investigation employed task structure features that were manu-ally annotated, our best-performing two-step dia-logue management model resulted in comparable performance utilizing only automatic features, achieving an accuracy of 49.7%. A crucial distinction between user and system dialogue act classification is that lexical features for a given dialogue turn are not available for system dialogue act classification because a sys-tem utterance is generated after a system dia-logue act is selected. The absence of lexical fea-tures poses a significant challenge to system dia-logue act classification, given that lexical fea-tures have been among the most predictive fea-tures for this task. To address this challenge, fu-ture research should continue exploring larger spaces of features to improve prediction accura-cies of learned models. 
8 Conclusions and Future Work Automatically learning dialogue management models for complex task-oriented domains with separate dialogue and task streams poses signifi-cant challenges. Effective dialogue management models in such domains should be able to proac-tively intervene by making spontaneous dialogue moves based on the observed history of both the dialogue and the user?s task activities. With the overarching goal of creating a data-driven auto-mated dialogue system that incorporates parallel dialogue and task streams, this paper has pre-sented classification-based dialogue management models that integrate a rich set of features auto-matically extracted from parallel dialogue and task streams. Two subtasks of dialogue manage-ment were considered: when the system should provide user with a dialogue move and what type of system dialogue act the system should select for a given user interaction context. An evalua-tion found that a two-step approach that modeled the two subtasks as separate classifiers were ef-fective, achieving significantly higher perfor-mance than an alternate approach that modeled the two subtasks with a single classifier. The results suggest several promising direc-tions for future work. First, incorporating richer features may improve the accuracies of learned models, such as more global interaction histories and deeper dialogue structures. Second, develop-ing more sophisticated task analyses will inform the learned models with a representation of the user task context, guiding the models to make more context-appropriate decisions. Finally, it will be important to evaluate the learned models by incorporating them into a dialogue manage-ment system and validating their effectiveness in interactions with users in rich task-oriented dia-logue.  Acknowledgments This research was supported by the National Sci-ence Foundation under Grant DRL-1007962. Any opinions, findings, and conclusions ex-pressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation. References  Allen, J., Ferguson, G., & Stent, A. (2001). An architecture for more realistic conversational systems. Proceedings of Intelligent User Interfaces (pp. 1?8). Santa Fe, NM. 
211
Bangalore, S., Di Fabbrizio, G., & Stent, A. (2008). Learning the structure of task-driven human-human dialogs. IEEE Transactions on Audio, Speech, and Language Processing, 16(7), 1249?1259. Bangalore, S., & Stent, A. J. (2009). Incremental parsing models for dialog task structure. Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics (pp. 94?102). Athens, Greece. Berger, A. L., Della Pietra, V. J., & Della Pietra, S. A. (1996). A maximum entropy approach to natural language processing. Computational Linguistics, 22(1), 39?71. Boyer, K. E. (2010). Structural and Dialogue Act Modeling in Task-Oriented Tutorial Dialogue. Ph.D. Dissertation. Department of Computer Science, North Carolina State University. Boyer, K. E., Grafsgaard, J. F., Ha, E. Y., Phillips, R., & Lester, J. C. (2011). An affect-enriched dialogue act classification model for task-oriented dialogue. Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (pp. 1190?1199). Portland, OR. Boyer, K. E., Ha, E. Y., Phillips, R., Wallis, M. D., Vouk, M. A., & Lester, J. C. (2010). Dialogue Act Modeling in a Complex Task-Oriented Domain. Proceedings of the 11th Annual SIGDIAL Meeting on Discourse and Dialogue (pp. 297?305). Tokyo, Japan. Carberry, S. (1991). Plan Recognition in Natural Language Dialogue. MIT Press. Cavicchio, F. (2009). The modulation of cooperation and emotion in dialogue: The REC corpus. Proceedings of the ACL-IJCNLP 2009 Student Research Workshop (pp. 81?87). Suntec, Singapore. Chi, M., VanLehn, K., Litman, D., & Jordan, P. (2010). Inducing Effective Pedagogical Strategies Using Learning Context Features. Proceedings of the Eighteenth International Conference on User Modeling, Adaptation, and Personalization (pp 147-158). Big Island, HI. Cohen, J. (1960). A coefficient of agreement for nominal scales. Educational and Psychological Measurement, 20(1), 37 ? 46. Di Eugenio, B., Xie, Z., & Serafin, R. (2010). Dialogue act classification, instance-based learning, and higher order dialogue structure. Dialogue and Discourse, 1(2), 81 ? 104. Dzikovska, M.O., Farrow, E, & Moore, J.D. (2013). Combining deep parsing and classification for improved explanation processing in a tutorial dialogue system. Proceedings of the 16th International Conference on Artificial Intelligence in Education (pp. 279 - 288). Memphis, TN. Forbes-Riley, K. & Litman, D. (2011). Designing and evaluating a wizarded uncertainty-adaptive spoken 
dialogue tutoring system. Computer Speech and Language, 25(1), 105-126. Ha, E. Y., Grafsgaard, J. F., & Mitchell, C. M. (2012). Combining Verbal and Nonverbal Features to Overcome the ?Information Gap? in Task-Oriented Dialogue. Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (pp. 247?256). Seoul, South Korea. Hardy, H., Biermann, A., Inouye, R. B., McKenzie, A., Strzalkowski, T., Ursu, C., Webb, N., et al (2006). The Amiti?s system: Data-driven techniques for automated dialogue. Speech Communication, 48(3-4), 354?373. Henderson, J., Lemon, O., & Georgila, K. (2008). Hybrid reinforcement/supervised learning of dialogue policies from fixed data sets. Computational Linguistics, 34(4), 487?511. Ivanovic, E. (2008). Automatic instant messaging dialogue using statistical models and dialogue acts. The University of Melbourne. Kim, S. N., Cavedon, L., & Baldwin, T. (2010). Classifying dialogue acts in one-on-one live chats. Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing (pp. 862?871). Cambridge, MA, USA: Association for Computational Linguistics. Levin, E., Pieraccini, R., & Eckert, W. (2000). A Stochastic Model of Human-Machine Interaction for Learning Dialog Strategies. IEEE Transactions on Speech and Audio Processing, 8(1), 11?23. Lewis, C., & Di Fabbrizio, G. (2006). Prompt selection with reinforcement learning in an AT&T call routing application. Proceedings of the Ninth International Conference on Spoken Language Processing (pp. 96?103). Mitchell, C.M., Boyer, K.E., & Lester, J.C. (2013). A Markov Decision Process Model of Tutorial Intervention in Task-Oriented Dialogue.  Proceedings of the International Conference on Artificial Intelligence in Education (pp. 828-831), Memphis, TN. Mitchell, C. M., Ha, E. Y., Boyer, K. E., & Lester, J. C. (2012). Recognizing effective and student-adaptive tutor moves in task-oriented tutorial dialogue. Proceedings of the Intelligent Tutoring Systems Track of the 25th International Conference of the Florida Artificial Intelligence Research Society (pp. 450?455). Rich, C., & Sidner, C. (1998). COLLAGEN: A col-laboration manager for software interface agents. User Modeling and User-Adapted Inter-action, 8(3-4), 315?350. Roy, N., Pineau, J., & Thrun, S. (2000). Spoken dialogue management using probabilistic reasoning. Proceedings of the 38th Annual Meeting on Association for Computational Linguistics (pp. 93?100). Wanchai, Hong Kong. Scheffler, K., & Young, S. (2002). Automatic learning of dialogue strategy using dialogue simulation and reinforcement learning. 
212
Proceedings of the second international conference on Human Language Technology Research (pp. 12?19). San Diego, CA. Singh, S., Litman, D. J., Kearns, M., & Walker, M. (2002). Optimizing Dialogue Management with Reinforcement Learning: Experiments with the NJFun System. Journal of Artificial Intelligence Research, 16, 105?133. Sridhar, R., Bangalore, S., & Narayanan, S. (2009). Combining lexical, syntactic and prosodic cues for improved online dialog act tagging. Computer Speech and Language, 23(4), 407 ? 422. Stolcke, A., Ries, K., Coccaro, N., Shriberg, E., Bates, R., Jurafsky, D., Taylor, P., et al (2000). Dialogue act modeling for automatic tagging and recognition of conversational speech. Computational Linguistics, 26(3), 339?373. Walker, M., Litman, D., Kamm, C., & Abella, A. (1997). Paradise: A framework for evaluating 
spoken dialogue agents. Proceedings of ACL (pp. 271?280). Madrid, Spain. Williams, J., & Young, S. (2007). Partially Observable Markov Decision Processes for Spoken Dialog Systems. Computer Speech and Language, 21(2), 393?422. Young, S. (2002). Talking to machines (statistically speaking). Proceedings of ICSLP (pp. 32?41). Denver, CO. Young, S., Gasic, M., Thomson, B., & Williams, J. (2013). POMDP-Based Statistical Spoken Dialog Systems: A Review. Proceedings of the IEEE, 101(5), 1160?1179.    
Appendix A. An Excerpt from the Task-Oriented Dialogue Corpus Lesson ID Task ID Role Type Text Timestamp 1 4 STUDENT CODING System.out.printIn("Hello World" 2011-09-21 08:17:17.737 1 4 STUDENT CODING System.out.printIn("Hello World") 2011-09-21 08:17:19.407 1 4 STUDENT CODING System.out.printIn("Hello World"); 2011-09-21 08:17:19.812 1 4 TUTOR MESSAGE good. 2011-09-21 08:17:24.913 1 4 TUTOR MESSAGE also you can try to compile at anytime. 2011-09-21 08:17:33.805 1 4 STUDENT COMPILE_ BEGIN studentCode\jt101\JavaTutor3.java 2011-09-21 08:17:38.080 1 4 STUDENT COMPILE_ ERROR line 1  : cannot find symbol symbol  : method printIn(java.lang.String) location: class java.io.PrintStream System.out.printIn("Hello World");           ^ 1 error 
2011-09-21 08:17:38.220 
1 4 TUTOR MESSAGE carefully compare your line with the example 2011-09-21 08:17:57.330 Appendix B.  Types of Activity Logs in Corpus Log Type Description Action Initiator MESSAGE Either student or tutor has sent a chat message. Student, Tutor SESSION_PROGRESS Tutor has allowed student to progress to next task. Tutor CODING Student has written programming code. Student COMPILE_BEGIN Student has begun compiling code. Student COMPILE_SUCCESS Recent code compilation has ended successfully. N/A COMPILE_ERROR Recent code compilation has failed with errors. N/A RUN_BEGIN Student has begun running code. Student INPUT_SENT Student has sent an input to a running code. Student RUN_SUCCESS Recent code running has ended successfully. N/A RUN_STOP Tutor has stopped running student?s code because of errors in the code. Tutor 
213
Proceedings of the SIGDIAL 2013 Conference, pages 324?328,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
In-Context Evaluation of Unsupervised Dialogue Act Models  for Tutorial Dialogue 
  Aysu Ezen-Can Department of Computer Science North Carolina State University Raleigh, North Carolina 27695 aezen@ncsu.edu 
Kristy Elizabeth Boyer Department of Computer Science North Carolina State University Raleigh, North Carolina 27695 keboyer@ncsu.edu     Abstract Unsupervised dialogue act modeling holds great promise for decreasing the develop-ment time to build dialogue systems. Work to date has utilized manual annota-tion or a synthetic task to evaluate unsu-pervised dialogue act models, but each of these evaluation approaches has substan-tial limitations. This paper presents an in-context evaluation framework for an un-supervised dialogue act model within tuto-rial dialogue. The clusters generated by the model are mapped to tutor responses by a handcrafted policy, which is applied to unseen test data and evaluated by hu-man judges. The results suggest that in-context evaluation may better reflect the performance of a model than comparing against manual dialogue act labels. 1 Introduction A central focus within the dialogue systems re-search community is developing techniques for rapidly constructing dialogue systems. One tech-nique that has proven highly promising is to take a corpus-based approach to dialogue system au-thoring, for example by bootstrapping policy learning (Henderson, Lemon, & Georgila, 2008; Williams & Young, 2003), predicting what a human agent would do (Bangalore, Di Fabbrizio, & Stent, 2008), or learning supervised dialogue act models (Stolcke et al, 2000). Traditionally, these corpus-based approaches require some amount of manual annotation prior to learning the dialogue models.  In many cases, this manual annotation is a problematic bottleneck for system development.  
For tutorial dialogue systems, which aim to support students in acquiring skills or knowledge, heavy manual annotation is often required for learning models that classify student utterances with respect to dialogue acts (Forbes-Riley & Litman, 2005; Serafin & Di Eugenio, 2004), questioning strategies (Becker, Palmer, Vuuren, & Ward, 2012), or information sharing (Mayfield, Adamson, & Ros?, 2012) For dialogue act modeling in particular, recent work has demonstrated the great promise of un-supervised approaches, which are learned with-out the use of manual labels (Crook, Granell, & Pulman, 2009; Ezen-Can & Boyer, 2013; Ritter, Cherry, & Dolan, 2010). However, because gold standard labels are not a part of model learning, how to best evaluate unsupervised models repre-sents a significant open research question (Vlachos, 2011).  Most quantitative evaluations of unsupervised dialogue act models have relied on agreement with manual dialogue act annotations, though these annotations were not used in model learn-ing (Crook et al, 2009; Rus, Moldovan, Niraula, & Graesser, 2012; Ezen-Can & Boyer, 2013). Relying on manually tagged dialogue act labels to evaluate an unsupervised model has two major drawbacks: it does not fully avoid the manual annotation bottleneck, and it imposes a hand-authored criterion onto a fully data-driven model, which may be unnecessarily limiting. Distinc-tions made by an unsupervised model may be useful within a dialogue system, even if these categories are different from the distinctions made within a hand-authored dialogue act tagset.  This paper presents a novel evaluation framework for unsupervised dialogue act classi-fication of user utterances within tutorial dia-logue. Instead of attempting to evaluate the mod-el intrinsically, we evaluate its performance on 
324
an external task: triggering an appropriate utter-ance via a simple dialogue policy. This evalua-tion, which does not require an end-to-end dia-logue system, judges the model in the simulated context of the target task. The results demon-strate that this in-context evaluation may be equally useful as comparing against gold stand-ard dialogue act labels, while substantially reduc-ing the time required for human annotation.   2 Related Work Perhaps the earliest unsupervised approach for dialogue act modeling investigated hidden Mar-kov models with a bag-of-words approach in a meeting scheduling domain (Woszczyna & Waibel, 1994), using perplexity with respect to manual labels for evaluating the number of hid-den states. Dirichlet process clustering has been investigated for dialogue act classification in the train fares and scheduling domain (Crook et al, 2009), evaluating on intra-cluster similarity and inter-cluster similarity along with error rates with respect to manual labels. Another Bayesian ap-proach utilized hidden Markov models and topic modeling to classify Twitter posts (Ritter et al, 2010). Notably, Ritter et al utilize an utterance ordering task, rather than manual labels, for quantitative evaluation. Most recently, standard k-means and EM clustering algorithms were used for dialogue act clustering on an educational cor-pus, and the model?s accuracy was again evalu-ated with respect to manual labels (Rus et al, 2012). The current paper builds on these prior findings by applying a recently developed clus-tering framework and proposing a novel in-context evaluation scheme that can be used re-gardless of the unsupervised dialogue act model-ing technique underlying it. 3 Dialogue Act Clustering We consider an unsupervised dialogue act classi-fication model on a corpus of human-human stu-dent and tutor dialogues centered on a computer programming task within a textual dialogue envi-ronment (Boyer et al, 2009). There are 1,525 student utterances and 3,332 tutor utterances in the corpus. This paper focuses on dialogue act classification for student utterances, since in a tutorial dialogue system the tutor dialogue acts are system-generated.  The corpus was manually labeled in prior work with nine dialogue acts tailored to capture phenomena of interest within tutorial dialogue: general Question, Evaluation Question (request 
specific feedback on the task), Statement, Posi-tive Feedback, Lukewarm Feedback, Negative Feedback, Grounding, Greeting, and Extra-domain (utterances that are off topic). The Kappa for agreement on these manual tags was 0.76. These tags will be used within the present work to compare the in-context performance of the unsupervised policy with a manual-tag policy, but the tags are not used to learn or tune the un-supervised model. The unsupervised dialogue act model evaluat-ed here is based on a recently developed ap-proach that adapts the query-likelihood technique from information retrieval to rank utterances similar to each target utterance (Ezen-Can & Boyer, 2013). Each utterance within the training set is queried against all other utterances within the training set using bigram features.  Vectors encode the resulting utterance simi-larity, and these vectors are provided to a k-means clustering algorithm to partition the utter-ances into dialogue acts. Our recent work (Ezen-Can & Boyer, 2013) evaluated query-likelihood dialogue act clustering against two other ap-proaches with respect to classifying manual la-bels, and the query-likelihood approach outper-formed k-means clustering using leading tokens (Rus et al, 2012) and Dirichlet process cluster-ing (Crook et al, 2009). In the current work we add to the feature vectors the first level of the parse tree as provided by the Stanford parser (Klein & Manning, 2003).  The number of clusters was selected based on sum of squared errors (SSE). As with many pa-rameterized models, model fit tends to increase with more parameters, but there are important tradeoffs in computation time and risk of overfit-ting. In experiments, k=number of clusters ranged from 2 to 24. 21 clusters were chosen, corresponding to the rightmost ?knee? within the SSE graph (see Appendix).1 4 Evaluation Framework Evaluating unsupervised dialogue act clusters presents numerous challenges. In prior evalua-tions of query-likelihood clustering, we comput-ed accuracy with respect to the manually applied dialogue act tags described earlier, demonstrating 41.64% accuracy for a model with 8 clusters, compared to 34.90% accuracy for the Rus et al                                                 1 Selecting the number of clusters is a subjective deci-sion. Nonparametric techniques, such as variations on Dirichlet process clustering, hold promise for address-ing this limitation in the future. 
325
(2012) k-means approach and 24.48% accuracy for Dirichlet process clustering (Crook et al, 2009) on our corpus. However, the goal of the current work is to substantially reduce the human tagging required to evaluate the model. We also aim to test the hypothesis that comparing against manual labels under-represents the utility of the unsupervised model. That is, a dialogue policy built on the unsupervised model could perform better than the relatively low classification accu-racy for manual tags would suggest. Our evalua-tion will explore this hypothesis. In order to achieve these goals, we first trained an unsupervised dialogue act model on 75% of the corpus using the query-likelihood approach described in Section 3. The resulting model has 21 clusters. Then, we handcrafted a dialogue pol-icy for tutor responses by qualitatively examin-ing each cluster of training data and creating one tutor response for each cluster. Some clusters and their corresponding tutor utterances are depicted in Figure 1. This policy was applied by classify-ing unseen utterances from a held-out test set (25% of the corpus) using the learned model (Figure 2). The result of this process is that for each student utterance from the test set, a tutor response is generated based on the policy. This process resulted in 373 student utterances, one for each utterance in the 25% testing set, each paired with a corresponding tutor response gen-erated by the hand-authored policy. The evaluation goal is to determine whether the responses made by this policy are reasonable, which will represent the utility of the unsuper-vised dialogue act model for its intended use within a dialogue manager. We used human judges to rate the output of the policy. Thirty student utterances and tutor responses were ran-domly selected from the available utterances generated by the test set. An example set of ut-terances and policies can be seen in the Appen-dix. These items were placed in a survey that asked the reader to rate the extent to which each tutor response makes sense given the student ut-terance. (One item was inadvertently omitted from the survey, resulting in 29 items that were evaluated by the judges and that will be analyzed here.) To avoid bias introduced by the ordering of items, they were presented in a different ran-domized order for each of the seven judges who completed the survey. (29 items from a compari-son condition using manual tags were also ran-domly interleaved into the survey, as described later in this section.) Judges used a rating scale from 1 to 4 (1=makes no sense, 2=makes a little 
sense, 3=makes a lot of sense, and 4=makes per-fect sense). Since the models only used the cur-rent student utterance, the dialogue history was also not shown to the human raters.  Across the seven judges, the average rating of the tutor responses selected by the unsupervised policy was 2.35. We also collapsed the ratings into positive (?2.5 average across seven judges) and negative (<2.5 average). With this binary categorization, 44.8% of the time tutor responses generated by the unsupervised policy were rated positively. It is important to note that no infor-mation other than dialogue act was considered for generating the tutor responses; the tutor utter-ances were relatively content-free and based only on the dialogue act categorization given by the unsupervised model.               Figure 1: Clusters from unsupervised dialogue act modeling and corresponding dialogue policy  (typographical errors originated in corpus)  For comparison, we also constructed a hand-crafted dialogue policy using the manual dia-logue act labels and applied this policy to the same utterances as were used to evaluate the un-supervised model. These pairs of student utter-ances and tutor responses were interleaved ran-domly on the same survey provided to seven human judges. The same tutor responses as in the unsupervised policy were used whenever possi-ble for this manual-tag policy. The tutor respons-es generated from the manual-tag policy received an average score of 2.22, slightly lower than the average of 2.35 for tutor responses generated by the unsupervised policy. The binary positive-negative split for these ratings reveals that 31% were rated positively (?2.5 average), compared to 44.8% for the unsupervised policy. Direct comparisons between the unsupervised policy and the manual-tag policy must be inter-preted with caution, in part because the unsuper-vised policy was more granular (based on 21 
326
clusters) than the manual-tag policy (based on 9 tags) and also because it can be difficult to en-sure that the two policies were of equal quality. On the other hand, the unsupervised policy uti-lized no manual labels and was applied to an un-seen test set, while the manual-tag policy was based on reliable tags applied to the actual utter-ances from the testing set.              
 Finally, we evaluated the extent to which the 4-category rating scheme was reliable across judges. The weighted Kappa (Cohen, 1968), used for ordinal scales because it penalizes disagree-ments less if they are closer together, was 0.30 averaged across all pairs of judges, indicating fair agreement (Landis & Koch, 2013). For the collapsed binary ratings, average pairwise ordi-nary Kappa was 0.36. 5 Discussion It was hypothesized that evaluating an unsuper-vised dialogue act model against manual labels may be an inappropriately strict metric, requiring the model to conform to the criteria used by hu-mans to handcraft the manual tagset. Indeed, the accuracy of the unsupervised dialogue act model presented here with 21 clusters was 30.4% for identifying manual labels (arrived at by assigning the majority class tag to each unsupervised clus-ter after clustering was complete). The majority class baseline (most frequent student dialogue act tag) was Evaluation Question with a relative fre-quency of 25.87%, so on accuracy for identifying manual labels, the unsupervised model improved modestly over baseline. In contrast, when this unsupervised model was used to select a tutor response within a dialogue policy, the response was judged positively 44.8% of the time by hu-man judges. Moreover, recall that the tutor re-sponses were content free and took only the dia-
logue act label into account (no information state or topic). Therefore, it is meaningful to consider what percent of the time the responses were rated as making some sense (receiving a 2, 3, or 4 rat-ing average across the human judges). By this criterion, 65.5% of tutor responses selected by the unsupervised policy were rated as sensible.  Finally, this evaluation approach demon-strates promise for alleviating the bottleneck of manual annotation for dialogue act models. Each item within the current evaluation survey re-quired approximately 15 seconds to judge, using untrained human judges, for a total of approxi-mately 1 hour of effort across all seven judges. The time required for handcrafting policies was relatively small, approximately 1 hour. In con-trast, the dialogue act annotation scheme re-quired approximately 35 seconds per utterance (amortizing substantial up-front training time for each annotator) when applied as part of previous work, for a total of approximately 50 hours per annotator.  6 Conclusion Unsupervised dialogue act modeling holds great promise for decreasing development time of dia-logue systems. We have presented an unsuper-vised dialogue act model and an evaluation framework to judge the utility of the unsuper-vised model within a dialogue management task. The results demonstrate that in-context evalua-tion of an unsupervised dialogue act model, ra-ther than accuracy against manual labels, may better reflect the usefulness of the model for dia-logue management. Furthermore, this evaluation technique may greatly reduce the time required by human judges to evaluate the model.  One of the most promising directions for fu-ture work involves devising unsupervised dia-logue act models that leverage a richer represen-tation in order to perform better. These rich fea-tures may include dialogue history, adjacency pair information, and topic modeling. Addition-ally, it is important for the community to evalu-ate unsupervised dialogue models in the full con-text of deployed systems. Acknowledgments. This material is based upon work supported by the National Science Founda-tion under Grants DRL-1007962 and CNS-1042468. Any opinions, findings, conclusions, or recommendations expressed in this report are those of the authors and do not necessarily repre-sent the views of the National Science Founda-tion. 
Figure 2: Evaluation framework structure 
327
References  Bangalore, S., Di Fabbrizio, G., & Stent, A. (2008). Learning the Structure of Task-Driven Human-Human Dialogs. IEEE Transactions on Audio, Speech and Language Processing, 16(7), 1249?1259. Becker, L., Palmer, M., Vuuren, S. Van, & Ward, W. (2012). Learning to Tutor Like a Tutor: Ranking Questions in Context. Proceedings of the Inter-national Conference on Intelligent Tutoring Systems, 368?378. Boyer, K. E., Philips, R., Ha, E. Y., Wallis, M. D., Vouk, M. A., & Lester, J. C. (2009). Modeling Dialogue Structure with Adjacency Pair Analysis and Hidden Markov Models. Proceedings of NAACL HLT, 49?52. Cohen, J. (1968). Weighted kappa: Nominal scale agreement provision for scaled disagreement or partial credit. Psychological Bulletin, 70(4), 213-220. Crook, N., Granell, R., & Pulman, S. (2009). Unsupervised Classification of Dialogue Acts Using a Dirichlet Process Mixture Model. Proceedings of SIGDIAL, 341?348. Ezen-Can, A., & Boyer, K. E. (2013). Unsupervised Classification of Student Dialogue Acts With Query-likelihood Clustering. International Conference on Educational Data Mining, 20?27. Forbes-Riley, K., & Litman, D. J. (2005). Using Bigrams to Identify Relationships Between Student Certainness States and Tutor Responses in a Spoken Dialogue Corpus. Proceedings of SIGDIAL, 87?96.  Henderson, J., Lemon, O., & Georgila, K. (2008). Hybrid Reinforcement / Supervised Learning of Dialogue Policies from Fixed Data Sets. Computational Linguistics, 34(4), 487-511. Klein, D., & Manning, C. D. (2003). Accurate Unlexicalized Parsing. Proceedings of ACL, 423?430. Landis, J. R., & Koch, G. G. (1994). The Measurement of Observer Agreement for Categorical Data Data for Categorical of Observer Agreement The Measurement. International Biometric Society, 33(1), 159?174. Mayfield, E., Adamson, D., & Ros?, C. P. (2012). Hierarchical Conversation Structure Prediction in Multi-Party Chat. Proceedings of SIGDIAL, 60?69. Ritter, A., Cherry, C., & Dolan, B. (2010). Unsupervised Modeling of Twitter Conversations. Proceedings of NAACL HLT, 172?180. Rus, V., Moldovan, C., Niraula, N., & Graesser, A. C. (2012). Automated Discovery of Speech Act 
Categories in Educational Games. Proceedings of the International Conference on Educational Data Mining, 25-32. Serafin, R., & Di Eugenio, B. (2004). FLSA??: Extending Latent Semantic Analysis with features for dialogue act classification. Proceedings of ACL, 692?699. Stolcke, A., Ries, K., Coccaro, N., Shriberg, E., Bates, R., Jurafsky, D., Taylor, P., et al (2000). Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech. Computational Linguistics, 26(3), 339?373. Vlachos, A. (2011). Evaluating unsupervised learning for natural language processing tasks. Proceedings of EMNLP, 35?42. Williams, J. D., & Young, S. (2003). Using Wizard-of-Oz simulations to bootstrap Reinforcement-Learning- based dialog management systems. Proceedings of SIGDIAL, 135?139. Woszczyna, M., & Waibel, A. (1994). Inferring linguistic structure in spoken language. Proceedings of ICSLP, 847-850. Appendix 
 Figure 3: Sum of squared errors graph  Table 1: Example student utterances  and tutor responses Student  Utterance Tutor Response (Unsupervised Policy) Tutor Re-sponse (Manu-al-Tag Policy) I'm trying to think, heh                                                                                                                                      Don't worry about it. I'm here to help. That part is tricky. how can I pull values out of an array or can I reference them with code like zipDigits[1]?                                                                                                                                                                       
Great question. Let's look at the task decription together to see if it can help. 
Good question. Let's analyze the code together to see if it is right. 
thanks for the reminder                                                                                                                                                                                                                                              I'm here to help! Great, seems like we agreed.  does that mean I should declare it at the top of the code?                                                                                                                                                             
Maybe you should try it out. Good question. Let's analyze the code together to see if it is right. 
328
Proceedings of the SIGDIAL 2013 Conference, pages 339?343,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Evaluating State Representations for Reinforcement  Learning of Turn-Taking Policies in Tutorial Dialogue 
 Christopher M. Mitchell Kristy Elizabeth Boyer James C. Lester Department of Computer Science North Carolina State University Raleigh, NC, USA {cmmitch2, keboyer, lester}@ncsu.edu     Abstract 
Learning and improving natural turn-taking behaviors for dialogue systems is a topic of growing importance. In task-oriented dia-logue where the user can engage in task ac-tions in parallel with dialogue, unrestricted turn taking may be particularly important for dialogue success. This paper presents a novel Markov Decision Process (MDP) representa-tion of dialogue with unrestricted turn taking and a parallel task stream in order to automat-ically learn effective turn-taking policies for a tutorial dialogue system from a corpus. It also presents and evaluates an approach to auto-matically selecting features for an MDP state representation of this dialogue. The results suggest that the MDP formulation and the feature selection framework hold promise for learning effective turn-taking policies in task-oriented dialogue systems. 1 Introduction Determining when to make a dialogue move is a topic of growing importance in dialogue systems. While systems historically relied on explicit turn-taking cues, more recent work has focused on learning and improving on natural turn-taking behaviors (Raux and Eskenazi 2012; Selfridge et al 2012). For tutorial dialogue in particular, ef-fectively timing system moves can substantially impact the success of the dialogue. For example, failing to provide helpful feedback to a student who is confused may lead to decreased learning (Shute 2008) or to disengagement (Forbes-Riley and Litman 2012), while providing tutorial feed-back or interventions at inappropriate times could also have a negative impact on the out-come of the dialogue (D?Mello et al 2010).  Reinforcement Learning (RL) is a widely used approach to constructing effective dialogue poli-
cies using either MDPs or POMDPs (Williams and Young 2007). To date, RL has been applied to learn the most effective dialogue move to make, but has not been applied to learning the timings of these moves, although the related con-cept of when to release a turn has been explored (English and Heeman 2005). The domain of tuto-rial dialogue poses an additional modeling chal-lenge: the dialogue is task-oriented, but unlike many task-oriented dialogues in which all infor-mation is communicated via dialogue, students solve problems within a separate task stream which conveys essential information for dialogue management decisions.  This paper addresses dialogue with both unre-stricted turn taking and a parallel task stream with a novel Markov Decision Process represen-tation. Because turn boundaries are not clearly defined or enforced, we apply RL to the problem of when to make a dialogue move, rather than what type of dialogue move to make. In order to determine which criteria are most relevant to making this decision, the approach utilizes a fea-ture selection approach based on a new Separa-tion Ratio metric and compares the selected fea-tures against an existing approach based on ex-pected cumulative reward (Chi et al 2011). Fi-nally, the resulting feature spaces are evaluated with simulated users acquired in a supervised fashion from held-out portions of the corpus. The results inform the development of turn-taking policies in task-oriented dialogue systems. 2 Corpus The corpus used for this work was collected dur-ing 2011 and 2012 as part of the JavaTutor tuto-rial dialogue project. It consists of 66 textual dia-logues between human tutors and students, with an average of 90 tutor dialogue moves and 36 student dialogue moves. Each pair interacted for through a computer-mediated interface to com-
339
plete introductory computer programming tasks. Students edited their computer programs within a parallel task stream also collected as part of the corpus (see Appendix A). Tutors viewed the task actions synchronously through the interface. The success of each dialogue was measured by learn-ing gain between pretest and posttest. Overall the dialogues were effective; the average learning gain was 42.3% (statistically > 0; p < .0001). The substantial variation in learning gains (min=-28.6%; max= 100%) will be leveraged within the MDP reward structure. 3 MDP Representation A Markov Decision Process (MDP) models a system in which a policy can be learned to max-imize reward (Sutton and Barto 1998). It consists of a set of states S, a set of actions A representing possible actions by an agent, a set of transition probabilities indicating how likely it is for the model to transition to each state s? ? S from each state s ? S when the agent performs each action a ? A in state s, and a reward function R that maps real values onto transitions and/or states, thus signifying their utility.  Previous applications of RL to dialogue sys-tems, using both MDPs and POMDPs, have dealt with the decision of what type of dialogue move to make (Chi et al 2011; Williams and Young 2007). These systems make this decision either at predetermined decision points (Tetreault and Litman 2008), following the trigger of a silence threshold (Raux and Eskenazi 2012), or when the system determines it has enough information to advance the dialogue (Selfridge et al 2012). For the JavaTutor corpus, however, the tutor could choose to make a move at any time. Rather than applying handcrafted rules to determine decision points, we apply RL to learn when to make a dia-logue move in order to maximize the success of the dialogue. For this MDP, the action set is de-fined as A = {TutorMove, NoMove}.  The states for the MDP consist of combina-tions of features representing the current state of the session. The possible features available for selection are described in Table 1, and are all automatically extracted from system logs. The Task Trajectory and Edit Distance features are based on computing a token-level edit distance from a student?s program with respect to that student?s final correct solution. This distance measures a student?s progress over the course of a dialogue while avoiding the need to manually annotate the task stream. In a deployed system, 
this edit distance can be estimated by comparing to previously acquired solutions from other stu-dents.   Feature Description Values Current Action The current action being taken by the student  ? TASK ? STUDENTDIAL ? NOACTION 
Task  Trajectory 
The effect of the last task action on the edit distance to the final task solution 
? CLOSER ? FARTHER ? NOCHANGE 
Last  Action Last turn taken by either interlocutor ? TUTORDIAL ? STUDENTDIAL ? TASK Number of Tutor Moves Number of tutor turns taken thus far in the dialogue ? LOW  (< 30) ? MID   (30-59) ? HIGH (> 60) Edit  Distance The edit distance to the final solution ? LOW  (< 20) ? MID   (20-49) ? HIGH (> 50) Elapsed Idle Time The number of se-conds since the last student action ? LOW  (< 7) ? MID   (7-15) ? HIGH (> 15)  Table 1. Features available to be selected Tutor moves are encoded as MDP actions, while student actions are encoded as transitions to a new state with a NoMove tutor action. To account for the possibility that both interlocutors could construct messages simultaneously or that dialogue and task actions could happen at the same time, the following protocol was applied: if a tutor was making a dialogue move (i.e., typing a message), the state transition accompanying a student action was made after the tutor move was complete, and the student move was associated with that TutorMove action.  Another important consideration for this rep-resentation was how to segment the task stream into discrete actions. Through empirical investi-gation the timeout threshold of 1.5 seconds was selected as a balance between large numbers of successive task events or very few, most of which overlapped with tutor turns.  There were three additional states in the MDP: the Initial state and two final states, FinalHigh and FinalLow, occurring only at the end of a dia-logue and providing rewards of +100 and ?100, respectively. A median split on student learning gains was used to assign each dialogue to either the FinalHigh state or FinalLow state. 4 Feature Selection While retaining all six features would allow for a rich state representation, it would also lead to 
340
issues with sparsity (Singh et al 2002). In fact, nearly 90% of states averaged less than one visit per dialogue when using all six features, leading to inadequate coverage of the state space on which to build reliable MDP policies. This sec-tion compares two methods used to select fea-tures from among the six available. The first approach is based on the Expected Cumulative Reward (ECR) in the initial state, a metric previously used to evaluate state represen-tations for a tutorial dialogue system using RL (Chi et al 2011; Tetreault and  Litman 2008). A higher initial-state ECR indicates a higher proba-bility of achieving a favorable outcome when following a reward-maximizing policy. Maxim-izing ECR has also been the focus of other fea-ture selection approaches for RL (Misu and Kashioka 2012,  Li et al 2009). While initial-state ECR provides a measure of the likelihood of a favorable outcome, it does not address how well a particular state representation captures key decision points. That is, it does not directly represent the extent to which each deci-sion along the path to a successful outcome con-tributed to that outcome, or whether the second-best decision in a particular state would have been equally useful. In order to measure this dif-ference, we introduce the Separation Ratio (SR), which represents how much better a particular policy is compared to its alternatives. SR for a state is calculated by taking the absolute differ-ence between the estimated values of two actions in that state and dividing by the mean of the two values. SR for a policy is the mean of the SRs across all states.  An SR near zero for a state indicates that the decision to take one action over another in that state is likely to have little effect on the final out-come of the dialogue. On the other hand, a high SR indicates a crucial decision point, where tak-ing an off-policy action leads to a much lower probability of a successful outcome. The intui-tion behind this metric is that a state representa-tion that supports policies with high SR high-lights features that are useful in executing an ef-fective turn-taking policy, while a state represen-tation that produces policies with low SR fails to capture this information. Using these two metrics, we evaluated the util-ity of each of the six features. Starting with two empty state representations, one for each metric, a greedy algorithm added one feature at a time to each. That is, at each step for each metric, the feature was added that led to the highest value on the metric when combined with the features al-
ready chosen. For each of the two metrics, we built a state representation and used it as the ba-sis for an MDP. This MDP was then trained with policy iteration (Sutton and Barto 1998), and the two state representations that led to the highest value on each metric were carried over to the next iteration. The goal here is to evaluate the relative utility of each feature, so we continued adding features until they were exhausted, lead-ing to a full ordering of features for each condi-tion (Table 2).   Iteration Initial-State ECR Feature Ordering Mean SR  Feature Ordering 1 Last Action Number of Tutor Moves 2 Task Trajectory Edit Distance 3 Current Action Last Action 4 Elapsed Idle Time Current Action 5 Number of Tutor Moves Elapsed Idle Time 6 Edit Distance Task Trajectory  Table 2. Feature selection using Expected Cumu-lative Reward (ECR) and Separation Ratio (SR) Given the orderings in Table 2, the next step in building a RL system is to decide which iteration of the feature spaces to use. That is, how does a system designer determine when to stop adding features? Previous work (Chi et al 2011; Tetreault and Litman 2008) viewed an absolute increase in the value of initial-state ECR as a signal for the quality of a newly added feature. So, one could say that feature addition should stop if initial-state ECR does not increase be-tween iterations. In the current analysis, howev-er, this would result in termination at the second iteration for the mean SR ordering and termina-tion at the first iteration for the initial-state ECR ordering. These undesirably early terminations most likely occur because the first features se-lected in both orderings represent tutor actions: a tutor can always choose to make a move, thus setting the Last Action feature to TUTORDIAL, and a tutor has direct control over the value of Number of Tutor Moves. This control of features leads to deterministic control of state if the con-text provided by student-driven features is ab-sent. This can allow a policy to remain in the state that maximizes the transition probability to the end state, thus increasing ECR for all states due to deterministic transitions. Therefore, a dif-ferent type of stopping criterion is required. 
341
A stopping criterion must balance two com-peting goals. On the one hand, the size of the state space must be limited to avoid issues with sparsity, as state-action pairs that are not well explored during training might not be assigned values proportional to their expected rewards in a deployed system. On the other hand, a feature space that is too small may not sufficiently repre-sent the possible states of the world, and might fail to capture the criteria most relevant to mak-ing decisions. These competing goals of com-pactness and descriptive power must both be considered when choosing an appropriate feature space for a RL model.  In an attempt to balance these goals, we pro-pose a stopping criterion based on the ratio of states that are sparse states. A sparse state is de-fined as any state that occurs less than once per dialogue on average. A sharp increase in sparse states was observed between the third and fourth iterations for both metrics (15% to 56% for ECR and 26% to 47% for SR), so feature addition stopped at the third iteration. This resulted in only one of the three selected features being shared among the two conditions: the Last Action made by either person (Table 2). In addition, both feature sets include a feature related to the task progress of the student: Task Trajectory for ECR and Edit Distance for SR. The next section reports on an experiment to evaluate these two feature spaces. 5 Evaluation A series of simulated dialogues was used to evaluate the two resulting feature spaces via the policies derived using them. These simulations were based on five-fold cross-validation, as in prior work (Henderson et al 2008), with policies trained on four of the five folds and simulated users learned from the remaining fold. As noted above, the rewards in the MDP were based on student learning gain, but learning gain (like user satisfaction in other dialogue domains) is not directly observable during the dialogues. However, we found that students in the high learning gain group had fewer non-zero task ac-tions (actions that changed the edit distance to the final task solution) than students in the low learning gain group (p < 0.05). Therefore, num-ber of non-zero task actions is used as a measure of dialogue success, with lower numbers being better. We derived the average change in edit distance on each state transition from the testing folds, and defined that a simulated dialogue 
would end when the edit distance reached zero (i.e., the student arrived at the correct solution).  Table 3 shows the results of running 5,000 simulations in each fold for both the learned pol-icy and for an anti-policy where each decision was reversed. The anti-policy is included to pro-vide a point of comparison for the policies learned in each feature space, and offers insight into the quality of the learned policies, similar to the inverse policies learned in prior work (Chi et al 2011). The table shows that the learned poli-cies in the ECR feature space had slightly better results overall (lower number of non-zero task actions), while the SR feature space had larger separation between the learned policies and anti-policies. These results suggest that feature selec-tion based on SR was able to identify important decision criteria with only a minor decrease in reward compared to ECR.    Feature Space Policy Average non-zero task action count ECR Learned policy 43.2 Anti-policy 49.6 SR Learned policy 47.3 Anti-policy 97.4  Table 3. Results of simulated dialogues (lower non-zero task action count is better) 6 Conclusion Modeling unrestricted turn taking within an RL framework, particularly for task-oriented dia-logue with both a dialogue and a parallel task stream, presents numerous challenges. This pa-per has presented a novel representation of such dialogue with a tutoring domain, and has pre-sented and evaluated a feature selection method based on a new Separation Ratio metric, which can inform the development of turn-taking poli-cies in dialogue systems. Future work includes a more fine-grained analysis of the timing of dia-logue moves as well as an evaluation of these results in a deployed system.  Acknowledgements This work is supported in part by the National Science Foundation through Grants DRL-1007962 and CNS-1042468. Any opinions, findings, conclusions, or recommendations expressed in this report are those of the participants, and do not necessarily represent the official views, opinions, or policy of the National Science Foundation.   
342
References Chi, M., VanLehn, K., Litman, D., and Jordan, P. (2011). An Evaluation of Pedagogical Tutorial Tactics for a Natural Language Tutoring System: a Reinforcement Learning Approach. International Journal of Artificial Intelligence in Educa-tion, 21(1), 83?113. D?Mello, S.K., Olney, A., and Person, N. (2010). Mining Collaborative Patterns in Tutorial Dia-logues. Journal of Educational Data Mining, 2(1), 1?37. English, M.S. and Heeman, P.A. (2005). Learning Mixed Initiative Dialog Strategies By Using Rein-forcement Learning On Both Conversants. In Pro-ceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing, 1011?1018. Forbes-Riley, K. and Litman, D.J. (2012). Adapting to Multiple Affective States in Spoken Dialogue. In Proceedings of the 13th Annual SIGDIAL Meeting on Discourse and Dialogue, 217?226. Henderson, J., Lemon, O., and Georgila, K. (2008). Hybrid Reinforcement/Supervised Learning of Dialogue Policies from Fixed Data Sets. Computa-tional Linguistics, 34(4), 487?511. Li, L., Williams, J. D., and Balakrishnan, S. (2009). Reinforcement Learning for Dialog Management Using Least-Squares Policy Iteration and Fast Fea-ture Selection. In Proceedings of the Conference of the International Speech Communication Associa-tion. 2475?2478. Misu, T., and Kashioka, H. (2012). Simultaneous Fea-ture Selection and Parameter Optimization for Training of Dialog Policy by Reinforcement Learn-ing. In Proceedings of the IEEE Workshop on Spo-ken Language Technology, 1?6. Raux, A. and Eskenazi, M. (2012). Optimizing the Turn-Taking Behavior of Task-Oriented Spoken Dialog Systems. Transactions on Speech and Lan-guage Processing, 9(1), 1?23. 
Selfridge, E.O., Arizmendi, I., Heeman, P.A., and Williams, J.D. (2012). Integrating Incremental Speech Recognition and POMDP-based Dialogue Systems. In Proceedings of the 13th Annual SIG-DIAL Meeting on Discourse and Dialogue, 275?279. Shute, V.J. (2008). Focus on Formative Feedback. Review of Educational Research, 78(1), 153?189. Singh, S., Litman, D., Kearns, M., and Walker, M. (2002). Optimizing Dialogue Management with Reinforcement Learning: Experiments with the NJFun System. Journal of Artificial Intelligence Research, 16, 105?133. Sutton, R. and Barto, A. (1998). Reinforcement Learning. MIT Press, Cambridge, MA, 1998. Tetreault, J.R. and Litman, D.J. (2008). A Reinforce-ment Learning Approach to Evaluating State Rep-resentations in Spoken Dialogue Systems. Speech Communication, 50(8), 683?696. Williams, J.D. and Young, S. (2007). Partially Ob-servable Markov Decision Processes for Spoken Dialog Systems. Computer Speech & Language, 21(2), 393?422.   Appendix A. Corpus excerpt 1. Student begins declaring a String variable. 2. Student starts typing a message. 3. Student message: Could I type in String The Adventure Quest; ? or would I need to put in quotes or something? 4. Student resumes working on task. 5. Tutor starts typing a message. 6. Tutor message: TheAdventureQuest is fine 7. Student declares variable called The Adven-ture Quest (Incorrect Java syntax) 8. Tutor starts typing a message. 9. Student catches mistake and renames variable to TheAdventureQuest 10. Tutor message: Can't have spaces :) 11. Tutor starts typing a message 12. Tutor message: Good job   
Appendix B. Dialogue interface 343
Proceedings of the SIGDIAL 2014 Conference, pages 41?50,
Philadelphia, U.S.A., 18-20 June 2014.
c
?2014 Association for Computational Linguistics
Adapting to Personality Over Time: Examining the Effectiveness of
Dialogue Policy Progressions in Task-Oriented Interaction
Alexandria Katarina Vail and Kristy Elizabeth Boyer
Department of Computer Science
North Carolina State University
Raleigh, North Carolina, USA
{akvail, keboyer}@ncsu.edu
Abstract
This paper explores dialogue adaptation
over repeated interactions within a task-
oriented human tutorial dialogue corpus.
We hypothesize that over the course of
four tutorial dialogue sessions, tutors
adapt their strategies based on the person-
ality of the student, and in particular to
student introversion or extraversion. We
model changes in strategy over time and
use them to predict how effectively the
tutorial interactions support student learn-
ing. The results suggest that students lean-
ing toward introversion learn more effec-
tively with a minimal amount of inter-
ruption during task activity, but occasion-
ally require a tutor prompt before voicing
uncertainty; on the other hand, students
tending toward extraversion benefit signif-
icantly from increased interaction, partic-
ularly through tutor prompts for reflection
on task activity. This line of investiga-
tion will inform the development of future
user-adaptive dialogue systems.
1 Introduction
Throughout dialogue interactions, humans adapt
to each other in a variety of ways (Cohen et al.,
1981; Power, 1974; Wahlster and Kobsa, 1989).
Some recent studies suggest that dialogue systems
that mirror these adaptations to the user, e.g., by
adopting the user?s vocabulary (Niederhoffer and
Pennebaker, 2002) or linguistically aligning to the
user?s context (Pickering and Garrod, 2004), may
be more effective than those that do not. For sup-
porting human dialogue, it has been demonstrated
that tutorial dialogue systems improve in effective-
ness when they adapt to user uncertainty (Forbes-
Riley and Litman, 2007) or perform ?small talk?
to increase the user?s trust in the system (Cassell
and Bickmore, 2003). Some studies have provided
evidence that adapting to the user at the person-
ality level also increases effectiveness; for exam-
ple, users may become more agreeable when sys-
tems mirror their personality (Reeves and Nass,
1997), and varying levels of encouragement may
help users of extraverted or introverted personali-
ties accomplish a task more effectively (Tapus and
Mataric, 2008).
With this substantial evidence that adapting to
user personality may improve the effectiveness of
a dialogue system, there is little investigation of
how personality affects repeated interactions. For
supporting human learning in particular, we hy-
pothesize that taking personality into account may
enhance outcomes by providing a more tailored
experience. To explore this hypothesis, this paper
presents an analysis that uses the change in human
tutorial dialogue policies over repeated interaction
with introverted and extraverted students to pre-
dict the effectiveness of the tutoring. We utilize a
widely-used and validated questionnaire, the Big
Five Inventory, to determine a personality profile
for each student. We hypothesize that introverted
and extraverted students learn more effectively un-
der different dialogue policies. The results sug-
gest dialogue policy progressions that could aid in
the future development of personality-based user-
adaptive tutorial dialogue systems.
2 Related Work
Humans adapt to their dialogue partner in a va-
riety of ways: for example, using knowledge ac-
quired through the dialogue to inform subsequent
utterances (Carberry, 1989), maintaining a set of
subdialogues (Litman and Allen, 1987), and struc-
turing dialogue to achieve a common goal (Power,
1974), including asking particular sorts of ques-
tions (Cohen et al., 1981), reaching dialogue con-
vergence (Mitchell et al., 2012), and understand-
ing context-specific vocabulary (Grosz, 1983). It
41
has been strongly suggested by a number of stud-
ies that dialogue systems would benefit greatly
from mirroring this sort of adaptation, e.g., by
adopting the user?s syntax (Niederhoffer and Pen-
nebaker, 2002), goal-oriented language (Brennan,
1996), and dialogue structure (Levelt and Kelter,
1982).
Some of these factors have been successfully
applied to task-oriented dialogue systems. For
example, ?entrainment? (the alignment between
partners at various linguistic levels) has been
shown to be predictive of task success in tele-
phone conversation (Nenkova et al., 2008) and
of less misunderstanding in personality-matching
systems (Mairesse and Walker, 2010).
In order to gauge user personality, we utilize
the Big Five Factor model, which was developed
to objectively measure five particular aspects of a
person?s personality (Goldberg, 1993). This per-
sonality model has been widely implemented in a
number of studies of personality in dialogue sys-
tems, including recommender systems (Dunn et
al., 2009) and conversational systems (Mairesse
and Walker, 2010). The investigation of person-
ality as it pertains to tutorial dialogue systems is a
natural step for user-adaptive dialogue systems.
3 Tutorial Dialogue Corpus
The corpus under examination in this study con-
sists of computer-mediated human-human textual
dialogue (Mitchell et al., 2013; Ha et al., 2013).
For each dialogue session, participants included
one tutor and one student who cooperated with
the goal of creating a working software artifact,
a text-based adventure game, by the end of the re-
peated interactions. Students were first-year uni-
versity students from an introductory engineering
course who volunteered in exchange for course
credit. No previous computer science knowledge
was assumed or required. The tutors were primar-
ily graduate students with previous experience in
tutoring or teaching Java programming.
The tutorial sessions were conducted within a
web-based textual dialogue interface for introduc-
tory programming in Java. The tutorial dialogue
interface, displayed in Figure 1, consists of four
panes in which the student interacts: the task de-
scription, the compilation and execution output,
the student?s Java source code, and the textual di-
alogue messages between the tutor and the stu-
dent. The student could modify, compile, and ex-
Figure 1: The task-oriented tutorial dialogue inter-
face.
ecute Java code from within the interface, in ad-
dition to conversing with the tutor via the textual
dialogue pane. The content of the interface was
synchronized in real time between the student and
the tutor; however, the tutor?s interactions with the
environment were constrained to the textual di-
alogue with the student and the progression be-
tween tasks.
The corpus was collected during two university
semesters in Fall 2011 and Spring 2012. A total
of N = 67 students interacted with one of seven
tutors to complete the series of interactions during
this time frame. The tutoring curriculum was com-
posed of six task-based lessons completed over
four weeks, each constrained to forty minutes in
duration. Each lesson consisted of multiple sub-
tasks, with each lesson concluding at a milestone.
This paper considers only the first four of the six
lessons, because the fifth lesson suffered from sig-
nificant data loss due to a database connectivity
error, and the sixth lesson consisted of an unstruc-
tured review of the previous five lessons, and is
therefore a different type of dialogue than the prior
lessons. The structure of the corpus is illustrated
in Table 1.
The sessions under consideration contained
67 students, with a total of 45, 904 utterances:
13, 732 student utterances and 32, 172 tutor utter-
ances. There were an average of 117 utterances
per session: 82 tutor utterances (652 words) and
35 student utterances (184 words). Introverted stu-
dents averaged 36 utterances and 172 words per
session, while extraverted students averaged 34 ut-
terances and 187 words per session. There was
no statistically significant difference between in-
42
Tutor Student Lessons
1 1 L1 L2 L3 L4 L5 L6
1 2 L1 L2 L3 L4 L5 L6
.
.
.
2 15 L1 L2 L3 L4 L5 L6
.
.
.
3 18 L1 L2 L3 L4 L5 L6
3 19 L1 L2 L3 L4 L5 L6
.
.
.
Table 1: A diagram of the structure of the corpus.
Gray cells indicate dialogue sessions that were not
considered in the present analysis.
troverts and extraverts on these counts. The possi-
ble extraversion score on the questionnaire ranges
from ?10 (highly introverted) to 25 (highly ex-
traverted), and the mean extraversion score of the
students in our corpus was 6.40 (standard devia-
tion 6.42). The distribution of scores across the
sample was comparable to a normal distribution,
as demonstrated by the histogram in Figure 2.
Figure 2: Histogram of extraversion scores across
students in the corpus. Lighter bars indicate fe-
male students, while darker bars indicate male stu-
dents.
3.1 Learning Gain
Students completed an identical pretest and
posttest for each lesson. The average pretest and
posttest scores for students scoring above and be-
low the median extraversion score in the four
lessons are detailed in Table 3 (determination of
extraversion is detailed in Section 3.2). There
was no statistically significant difference between
the scores of extraverted and introverted students.
The tutoring was statistically significantly effec-
tive overall and within each student group (p 
0.0001, on all accounts).
Lesson
Pretest Posttest
Introvert Extravert Introvert Extravert
L1 50.69% 47.42% 71.63% 68.18%
L2 43.70% 38.96% 71.01% 73.59%
L3 55.88% 54.55% 67.65% 64.85%
L4 68.79% 65.66% 80.56% 79.97%
Table 3: Average pretest and posttest scores for
each lesson.
This equation adjusts for negative learning gain
in the rare cases that posttest score is less than
pretest score (Marx and Cummings, 2007).
norm gain =
{
post?pre
1?pre
post > pre
post?pre
pre
post ? pre
(1)
Since pretest and posttest scores for introverts and
extraverts were not identical, normalized learning
gain was standardized within each group before
developing models to predict learning (Section 4).
3.2 Extraversion vs. Introversion
One of the standard frameworks for identifying
personality traits is the Big Five Factor model
of personality (Goldberg, 1993). The standard
method of testing for the Big Five personality
traits is by questionnaire (John and Srivastava,
1999; Gosling et al., 2003). The students un-
der consideration in this study were adminis-
tered a Big Five Inventory survey, a type of self-
assessment of personality, prior to any interac-
tion with the tutorial dialogue system. The Big
Five Inventory consists of 44 items to measure
an individual on the Big Five Factors of per-
sonality: Openness, Conscientiousness, Extraver-
sion, Agreeableness, and Neuroticism (Goldberg,
1993). This study focuses on a student?s responses
to the items reflective of extraversion and introver-
sion. These items are identified in Table 4. Ex-
traversion is defined as the part of the Big Five
Factors that identifies gregariousness, assertive-
ness, activity, excitement-seeking, positive emo-
tions, and warmth (John and Srivastava, 1999).
3.3 Dialogue Act Annotation
As described in the previous section, the corpus
being considered consists of 268 dialogues, four
43
Extraverted Student Dialogue Excerpt
STUDENT: So do we need an else statement for each
one? [QI]
TUTOR: That wouldn?t actually work. [AWH]
STUDENT: Really? [FNU]
TUTOR: See, because it?s testing them each independently.
[E]
TUTOR: So when it gets to 2 and 4, any other combination
goes to its else. [E]
Pause for 29 seconds.
TUTOR: If we added an else clause for each statement,
we?d end up with 3 of them printing out for every valid
input. [E]
STUDENT: Oh. [ACK]
Pause for 44 seconds.
TUTOR: What else do you think we could try? [QP]
Pause for 49 seconds.
STUDENT: Well the first one worked last time be-
cause it was checking only playerChoice . . . maybe
currentChoice has something to do with this case.
[AWH]
Introverted Student Dialogue Excerpt
STUDENT: The else applies no matter what because it
doesn?t have an else if to combine with? [QI]
TUTOR: Well, it?s a little different than that. [AWH]
TUTOR: Each if statement applies no matter what. [I]
TUTOR: So, instead of checking the values as mutually
exclusive conditions, each if is checked in sequence. [I]
Pause for 22 seconds.
TUTOR: Your else occurs only with the final if, regard-
less of what happened with the previous if statements!
[E]
Pause for 31 seconds.
TUTOR: Let?s fix it by doing the change that you started
much earlier. [D]
Pause for 50 seconds.
TUTOR: Much better. :) [FP]
STUDENT: Thanks! [ACK]
Pause for 22 seconds.
TUTOR: Do you have any issues with the input checking
as it is now? [QP]
Pause for 46 seconds.
STUDENT: I do not! [AYN]
Table 2: Excerpts of similar dialogue between an extraverted and an introverted student.
I see myself as someone who . . .
. . . is talkative.
. . . is reserved.*
. . . is full of energy.
. . . generates a lot of enthusiasm.
. . . tends to be quiet.*
. . . has an assertive personality.
. . . is sometimes shy, inhibited.*
. . . is outgoing, sociable.
Table 4: Items of the Big Five Inventory reflective
of a student?s extraversion traits. Asterisks repre-
sent items negatively associated with extraversion.
for each of 67 students, with 45, 904 utterances to-
tal. As described in this section, a portion of these
dialogues were manually annotated, and then a
supervised dialogue act classifier was trained on
them and was used to tag the remaining dialogues.
The annotation scheme applied to the corpus
consisted of 31 dialogue act tags grouped into
four high-level categories (Statement, Question,
Answer, Feedback) (Vail and Boyer, In press).
This tagset represents a refinement of previous di-
alogue act tagsets developed for task-oriented tu-
toring (Ha et al., 2013). During this refinement,
emphasis was placed on decomposing frequent
tags that tended to be broad, such as STATEMENT
and QUESTION, in order to capture more fine-
grained pedagogical and social phenomena in the
dialogues. The annotation scheme is detailed in
Table 5.
A total of 30 sessions (4, 035 utterances) were
manually annotated by a single annotator. Of
those 30 sessions, 37% were annotated by a sec-
ond independent annotator. Inter-annotator agree-
ment on this subset reached a Cohen?s kappa of
?=0.87 (agreement of 89.6%). These manually
annotated sessions form the basis for developing
an automated classifier.
The automated classifier was trained using the
WEKA machine learning software (Hall et al.,
2009). We used a J48 decision tree classifier,
which has a low running time (Verbree et al.,
2006) and as we will see, performed very well for
this task. The classifier was provided the features
listed in Table 6.
Before the construction of the classifier, the 30
sessions of the manually annotated corpus were
systematically split into a training and a test set,
consisting of 24 and 6 sessions, respectively; the
test set contained the first three sessions with stu-
dents identified as introverts and the first three ses-
sions with students identified as extraverts. Ut-
terances were defined as single textual messages.
44
Tag Example
Session Type
?
Introvert Extravert
ACKNOWLEDGE (ACK) Okay. 10.46% 10.36% 0.872
EXTRA-DOMAIN ANSWER (AEX) I?m doing great. 1.33% 1.42% 0.813
READY ANSWER (AR) I?m ready. 2.75% 3.08% 0.963
WH-QUESTION ANSWER (AWH) Line 9. 8.14% 8.10% 0.819
YES/NO ANSWER (AYN) No, sir. 2.99% 3.73% 0.839
CORRECTION (CO) *exclamation 0.43% 0.41% 0.700
DIRECTIVE (D) Test what you have. 6.01% 5.97% 0.888
EXPLANATION (E) Your code stops on line 2. 31.48% 26.70% 0.822
NEGATIVE FEEDBACK (FN) No, that?s wrong. 0.02% 0.02% 0.615
ELABORATED NEGATIVE FEEDBACK (FNE) You?re using the wrong function. 0.21% 0.14% 0.689
NOT UNDERSTANDING FEEDBACK (FNU) I?m not sure. 0.05% 0.04% 0.749
OTHER FEEDBACK (FO) That?s okay. 0.17% 0.16% 0.614
ELABORATED OTHER FEEDBACK (FOE) What you had was fine. 0.29% 0.27% 0.665
POSITIVE FEEDBACK (FP) Very good! 6.78% 5.45% 0.927
ELABORATED POSITIVE FEEDBACK (FPE) That?s a very good approach. 0.05% 0.12% 0.705
UNDERSTANDING FEEDBACK (FU) Ohh, I see! 0.76% 0.92% 0.804
GREETING (GRE) Hello! 2.59% 3.03% 0.941
INFORMATION (I) Variable names must be one word. 4.55% 5.33% 0.859
OBSERVATION (O) As you see, we have a bug. 0.25% 0.31% 0.760
EXTRA-DOMAIN OTHER (OEX) Calculus is difficult. 1.49% 2.22% 0.789
CONFIRMATION QUESTION (QC) Does that work? 0.16% 0.16% 0.857
DIRECTION QUESTION (QD) What do I do now? 0.68% 0.58% 0.758
EVALUATIVE QUESTION (QE) Does that make sense? 0.87% 0.83% 0.763
EXTRA-DOMAIN QUESTION (QEX) How are you today? 0.42% 0.45% 0.781
FACTUAL QUESTION (QF) What line is it waiting on? 4.10% 5.12% 0.832
INFORMATION QUESTION (QI) How do you add spaces? 4.06% 4.91% 0.820
OPEN QUESTION (QO) How can you fix it? 0.15% 0.14% 0.725
PROBING QUESTION (QP) Do you think that looks correct? 4.99% 4.76% 0.731
QUESTION PROMPT (QQ) Any questions? 2.49% 2.24% 0.978
READY QUESTION (QR) Are you ready to move on? 2.47% 2.75% 0.989
REASSURANCE (R) We have plenty of time left. 0.12% 0.15% 0.763
Table 5: Dialogue act tags comprising the annotation scheme, the average composition of a Lesson 4
session with introverted and extraverted students, and the Cohen?s kappa achieved by the automated
classifier.
Feature Description
Number of Features
Initial Selected
TUTOR or STUDENT 1 1
Two-step tag history 2 2
Two-step category history 2 2
Number of tokens in the utterance 1 1
Existence of a question mark 1 1
Existence of word unigrams 1459 160
Existence of word bigrams 8959 150
Existence of POS unigrams 50 31
Existence of POS bigrams 928 152
Table 6: Features provided to the J48 automatic
dialogue act classifier.
Feature selection was performed on the features
occurring more than three times in the training
set using the WEKA machine learning software:
various top-N cut-offs were examined for perfor-
mance on tenfold cross-validation after ranking
the features by information gain. A peak in per-
formance during cross-validation on the training
set was observed at N=500 features.
The final dialogue act classifier includes the fol-
lowing features: speaker role, two-step dialogue
act history (category and tag), utterance length, ex-
istence of the ??? token, existence of 160 unigrams
and 150 bigrams, and existence of 31 part-of-
speech unigrams and 152 part-of-speech bigrams.
45
The part-of-speech tagger used in this analysis was
an n-gram tagger within the Natural Language
Tool Kit for Python, trained on the NPS chat cor-
pus (Bird et al., 2009; Forsyth and Martell, 2007).
The classifier performance on the held-out test set
consisting of 714 utterances was 80.11% accuracy,
Cohen?s kappa of 0.786. This classifier was then
used to tag dialogue acts in the remaining 41, 869
utterances.
4 Extraversion and Dialogue Policy
With the annotated corpus in hand, the goal is to
examine how dialogue policy progression, as rep-
resented by tutors? contextualized dialogue acts,
occurs over time with students tending toward ex-
traversion or introversion. We hypothesize that
tutors adapt differently to introverted and ex-
traverted students, and that students of different
extraverted or introverted tendencies learn more
effectively from different dialogue policies.
Students were binned into two groups, the ?in-
troverts?, consisting of the students scoring below
or equal to the median extraversion score of 7, and
the ?extraverts?, consisting of the students scoring
above the median score
1
. These groups included
34 and 33 students, respectively.
We describe tutor dialogue policy by identify-
ing the conditional probabilities of a tutor move
following a student move (i.e., the probabilities
Pr(T
n
|S
n?1
)) during each session. In other
words, we compute bigram probabilities over di-
alogue acts, where the second dialogue act of the
bigram is a tutor move. Because the task-oriented
nature of the dialogue allows for extended periods
of dialogue silence while the student is working
on the task, a WAIT tag was added to the corpus
when there was a pause in the dialogue for more
than twenty seconds. This threshold was chosen
based upon qualitative inspection of the corpus. To
identify the changes in this policy over time, we
calculated the difference in the probability of each
dialogue act bigram between the first and fourth
lessons of each student-tutor pair. Finally, in or-
der to allow for directly comparing parameter val-
ues across models, each column of predictors was
standardized by subtracting the mean and dividing
1
We split on the median introversion/extraversion score
as observed in our student sample rather than splitting on a
larger population median because the range of personality
traits differs significantly based on the sample. To date, no
large study has examined university students in order to es-
tablish personality norms.
by the standard deviation.
After all of the bigram probabilities were stan-
dardized, we split the students into two groups
based on median extraversion score: those tend-
ing toward extraversion and those tending toward
introversion. A feature selection algorithm was
then applied to each of these sets in order to iden-
tify the most relevant dialogue act bigram fea-
tures for predicting learning. Any feature that
provided non-positive information gain was elim-
inated from consideration. A stepwise linear re-
gression model was then applied using the SAS
statistical modeling software, resulting in the mod-
els displayed in Tables 7 and 8. Subscripts indicate
the speaker of the dialogue act, student or tutor.
Note that in each of these tables, the predictors are
not just bigram probabilities, but change in that
particular bigram probability from the first to the
fourth dialogue within repeated-interactions tutor-
ing.
Students Tending Toward Extraversion
Normalized Learning Gain = Partial R
2
p
1.244 * OEX
S
? FP
T
0.228 < 0.001
?0.445 * AYN
S
? R
T
0.169 < 0.001
0.440 * E
S
? QE
T
0.139 0.001
0.359 * QI
S
? QF
T
0.092 0.002
?0.298 * AWH
S
? QO
T
0.081 0.013
0.207 * WAIT? QP
T
0.050 0.037
?0.226 * QI
S
? I
T
0.038 0.041
0.000 (intercept) 1.000
RSME = 50.97% of range in Normalized Learning Gain
Table 7: Stepwise linear regression model for stan-
dardized Normalized Learning Gain in students
scoring above the median in extraversion.
Students Tending Toward Introversion
Normalized Learning Gain = Partial R
2
p
?0.447 * QI
S
? R
T
0.262 0.003
0.371 * QI
S
? QP
T
0.125 0.007
?0.331 * QI
S
? QQ
T
0.092 0.015
?0.278 * WAIT? FPE
T
0.083 0.018
0.384 * AYN
S
? QQ
T
0.067 0.010
0.288 * ACK
S
? E
T
0.067 0.022
0.000 (intercept) 1.000
RSME = 60.89% of range in Normalized Learning Gain
Table 8: Stepwise linear regression model for stan-
dardized Normalized Learning Gain in students
scoring below the median in extraversion.
46
Several tutorial dialogue policy progressions
were identified as statistically significantly asso-
ciated with learning gain in both extraverted and
introverted students. An increase in factual ques-
tions following extra-domain statements was asso-
ciated with increased learning in students scoring
above the median in extraversion, as was an in-
crease in evaluative questions after explanations,
an increase in the number of factual questions fol-
lowing information questions, and an increase in
probing questions initiated after the conclusion of
a sub-dialogue. On the other hand, extraverted
students achieved a lower learning gain when tu-
tors offered increasing reassurance after yes/no an-
swers, asked more open questions after answers to
WH-questions, or gave increasing instruction after
an information question.
A similar number of tutorial dialogue policy
progressions were identified as statistically signif-
icantly correlated with learning gain in introverted
students. For these students, a higher learning gain
was achieved when tutors followed more infor-
mation questions with a probing question, more
yes/no answers with a prompt for questions, or
offered increasing explanation after acknowledge-
ments. Students scoring below the median in ex-
traversion achieved a lower learning gain when
tutors offered more reassurance after information
questions, more prompts for questions after infor-
mation questions, or increasing elaborated positive
feedback after pauses in the dialogue.
5 Discussion
This section examines the tutorial dialogue pol-
icy progressions that were identified as statisti-
cally significant to learning gain in these groups
of students; recall that each feature represents a
change over time in the probability that the second
dialogue act follows the first. First we examine
the extraverted student model, and then we exam-
ine the introverted student model. Dialogue ex-
cerpts illustrating these dialogue interactions are
displayed in Appendix 1.
5.1 Extraverted Students
Students scoring higher in extraversion tend to
be assertive, outgoing, and energetic (Goldberg,
1993). As the models show, these characteris-
tics likely influence the extent to which particu-
lar dialogue policies are effective for supporting
learning for extraverted students. For example,
the high energy nature of the extraversion per-
sonality trait may influence how dialogues tran-
sition. The model shows that students learned
more when tutors progressed over time toward
more positive feedback following extra-domain
statements (Extra-Domain Statement
S
? Positive
Feedback
T
) and toward more probing questions
following pauses (Wait ? Probing Question
T
).
Both of these bigrams indicate important transi-
tion points within dialogue. For the former, extra-
domain statements represent off-topic utterances,
whereas tutor positive feedback can only be task-
related (if it were a positive response to an extra-
domain statement, the response would also have
been tagged extra-domain). For tutor probing
questions following pauses, it is likely that ex-
traverted students benefited from this adaptation
over time because in being asked to reflect and
explain their current understanding or goals, they
may have been re-engaged. It should be noted that
in general, asking students to self-explain can sup-
port learning (VanLehn et al., 1992).
Another example of a dialogue policy progres-
sion that emerged in the model and illustrates a
widely known fact about tutoring is reflected in the
Information Question
S
? Information
T
bigram,
which when tutors progressed more toward this
approach, is associated with decreased learning.
Our prior work has shown that directing students
what to do, even if they have just asked for such
direction, is strongly associated with decreased
learning (Mitchell et al., 2013).
Extraverted students tend to be assertive, and
this characteristic influences how they make and
interpret particular dialogue moves. An example
of this can be seen within the model: when tutors
progressed toward providing more reassurance af-
ter student yes/no answers, students learned less.
This Yes/No Answer
S
? Reassurance
T
policy is
likely a form of indirect feedback or politeness,
both of which have been shown to be unhelpful,
and sometimes harmful, to learning (Johnson and
Rizzo, 2004), and this seems to be a particularly
marked effect for extraverted students who may
benefit more from direct evaluations of their an-
swers. Another example of this indirect approach
may be within the WH-Question Answer
S
?Open
Question
T
tutor policy, whose increasing use over
time was associated with lower student learning.
Like reassurance, a follow-up question may be in-
terpreted by extraverted students as an indirect in-
47
dication that the previous answer was incorrect,
and a more direct approach may have been more
helpful.
Finally, extraverted students tend to be talkative.
This tendency is consistent with two of the
model?s findings regarding the helpfulness of par-
ticular types of tutor questions. Students tended
to learn more when tutors progressed toward fol-
lowing student explanations with evaluative ques-
tions (Explanation
S
? Evaluative Question
T
).
Although students? responses to evaluative ques-
tions (e.g., ?Do you understand??) are frequently
considered to be inherently inaccurate, especially
when students are first introduced to material, it
may be the case that as students work on a task for
an extended period of time, evaluative questions
may become increasingly helpful. Another tutor
questioning policy was also positively associated
with learning gain for extraverted students: Infor-
mation Question
S
? Factual Question
T
involves
the tutor answering a question with a question,
potentially a very helpful strategy for talkative or
highly social students.
5.2 Introverted Students
Students scoring lower in extraversion tend to be
less talkative, more reserved, and more shy (Gold-
berg, 1993). This may result in introverted stu-
dents being less outspoken about their understand-
ing, and less likely to ask questions about misun-
derstandings. These characteristics affect the way
that tutor choices impact student learning during
tutoring. For example, when less talkative stu-
dents ask information questions and tutors tend to
provide more reassurance as time goes on, this In-
formation Question
S
? Reassurance
T
pair is as-
sociated with decreased student learning. It is pos-
sible that since introverts are less likely to speak
up with a question, the ?stakes? or importance of
providing a direct answer may be higher for these
students. Another dialogue policy progression that
is not helpful for student learning is to provide
elaborated positive feedback after a pause in di-
alogue (Wait ? Elaborated Positive Feedback
T
).
Because pauses in the dialogue typically corre-
spond to student task actions, it is possible that in-
troverted students who are on the right track would
benefit more from the tutor allowing them to con-
tinue working.
Introverted students also tend to describe them-
selves as shy or inhibited, which may be influential
in the apparent helpfulness of tutors? increasing
their question prompts following student answers
(Answer Yes/No
S
? Question Prompt
T
). This
could be due to the fact that introverted students
are prone to giving terse responses, and may need
extra encouragement to ask questions if they are
uncertain. Increasing the number of these prompts
could increase the likelihood that more of the stu-
dent?s questions are voiced. Another helpful type
of question for introverted students seems to be
probing questions, even when they follow a stu-
dent question (Question Information
S
? Probing
Question
T
). A probing question is an indirect re-
quest for reflection, prompting the student to re-
consider her approach; this has previously been
shown to have a positive effect on learning gain
(VanLehn et al., 1992).
6 Conclusion and Future Work
Adapting to personality during dialogue may
substantially improve the effectiveness of both
human-human interactions as well as interactions
with dialogue systems. We have investigated the
ways in which human tutorial dialogue policy pro-
gressions are associated with learning within a
repeated-interactions dialogue study. The models
indicate that depending on a student?s tendencies
toward introversion or extraversion, different di-
alogue policy progressions support higher learn-
ing. In particular, introverts may benefit from ad-
ditional prompting and encouragement to speak
their mind, while extraverts may benefit from be-
ing given opportunities to discuss their thoughts
with a tutor.
While this study has focused on the extraversion
facet of personality, future work may benefit from
examining the other facets of the Big Five Fac-
tors: Neuroticism, Openness, Conscientiousness,
and Agreeableness. How we may best design a tu-
torial dialogue policy around a more fully-featured
model of the student?s personality is an important
research area. It will also be important to examine
task actions closely in future analyses, as this may
have significant effects on task-oriented dialogue
system design in particular. Additionally, analyz-
ing the intermediate sessions in order to capture
a fuller picture of the interaction over time is a
promising direction. Finally, examining tutor per-
sonality may also reveal important insight for the
design of tutorial systems. It is hoped that these
lines of investigation will lead to a next generation
48
of user-adaptive dialogue systems with increased
effectiveness facilitated by their adaptation to per-
sonality traits.
Acknowledgements
The authors wish to thank the members of the
LearnDialogue group at North Carolina State Uni-
versity for their helpful input. This work is sup-
ported in part by the Department of Computer Sci-
ence at North Carolina State University and the
National Science Foundation through Grant DRL-
1007962 and the STARS Alliance, CNS-1042468.
Any opinions, findings, conclusions, or recom-
mendations expressed in this report are those of
the participants, and do not necessarily represent
the official views, opinions, or policy of the Na-
tional Science Foundation.
References
Steven Bird, Ewan Klein, and Edward Loper. 2009. Natural
language processing with Python. O?Reilly Media, Inc.
Susan E Brennan. 1996. Lexical entrainment in spontaneous
dialog. In Proceedings of ISSD, pages 41?44.
Sandra Carberry. 1989. Plan recognition and its use in under-
standing dialog. In User Models in Dialog Systems, pages
133?162. Springer.
Justine Cassell and Timothy Bickmore. 2003. Negotiated
collusion: Modeling social language and its relationship
effects in intelligent agents. User Modeling and User-
Adapted Interaction, 13(1-2):89?132.
Philip R Cohen, C Raymond Perrault, and James F Allen.
1981. Beyond Question-Answering. Technical report,
DTIC Document.
Greg Dunn, Jurgen Wiersema, Jaap Ham, and Lora Aroyo.
2009. Evaluating interface variants on personality acqui-
sition for recommender systems. In User Modeling, Adap-
tation, and Personalization, pages 259?270. Springer.
Kate Forbes-Riley and Diane Litman. 2007. Investigating
human tutor responses to student uncertainty for adaptive
system development. In Affective Computing and Intelli-
gent Interaction, pages 678?689. Springer.
Eric N Forsyth and Craig H Martell. 2007. Lexical and dis-
course analysis of online chat dialog. In Semantic Com-
puting, 2007. ICSC 2007. International Conference on,
pages 19?26. IEEE.
Lewis R. Goldberg. 1993. The structure of phenotypic per-
sonality traits. American Psychologist, 48(1):26?34.
Samuel D Gosling, Peter J Rentfrow, and William B Swann
Jr. 2003. A very brief measure of the Big-Five personality
domains. Journal of Research in personality, 37(6):504?
528.
Barbara J. Grosz. 1983. TEAM: A Transportable Natural-
language Interface System. In Proceedings of the First
Conference on Applied Natural Language Processing,
pages 39?45, Santa Monica, California. Association for
Computational Linguistics.
Eun Young Ha, Christopher M Mitchell, Kristy Elizabeth
Boyer, and James C Lester. 2013. Learning Dialogue
Management Models for Task-Oriented Dialogue with
Multiple Communicative Channels. In Proceedings of the
14th Annual SIGDIAL Meeting on Discourse and Dia-
logue, pages 204?213, Metz, France.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Reutemann, and Ian H. Witten. 2009. The
WEKA Data Mining Software: An Update. SIGKDD Ex-
plorations, 11(1).
Oliver P. John and Sanjay Srivastava. 1999. The Big Five
trait taxonomy: History, measurement, and theoretical
perspectives. Handbook of personality: Theory and re-
search1, 2:102?138.
W Lewis Johnson and Paola Rizzo. 2004. Politeness in tu-
toring dialogs: ?Run the factory, thats what Id do?. In
Intelligent Tutoring Systems, pages 67?76. Springer.
Willem JM Levelt and Stephanie Kelter. 1982. Surface form
and memory in question answering. Cognitive psychol-
ogy, 14(1):78?106.
Diane J Litman and James F Allen. 1987. A plan recogni-
tion model for subdialogues in conversations. Cognitive
Science, 11(2):163?200.
Francois Mairesse and Marilyn A Walker. 2010. To-
wards personality-based user adaptation: psychologically
informed stylistic language generation. User Modeling
and User-Adapted Interaction, 20(3):227?278.
Jeffrey D. Marx and Karen Cummings. 2007. Normalized
change. American Journal of Physics, 75(1):87.
Christopher M Mitchell, Kristy Elizabeth Boyer, and James C
Lester. 2012. From strangers to partners: examining con-
vergence within a longitudinal study of task-oriented dia-
logue. In Special Interest Group on Discourse and Dia-
logue, pages 94?98.
Christopher M Mitchell, Eun Young Ha, Kristy Elizabeth
Boyer, and James C Lester. 2013. Learner characteristics
and dialogue: recognising effective and student-adaptive
tutorial strategies. International Journal of Learning
Technology (IJLT), 8(4):382?403.
Ani Nenkova, Agustin Gravano, and Julia Hirschberg. 2008.
High frequency word entrainment in spoken dialogue. In
Proceedings of the 46th Annual Meeting of the Association
for Computational Linguistics on Human Language Tech-
nologies, pages 169?172. Association for Computational
Linguistics.
Kate G Niederhoffer and James W Pennebaker. 2002. Lin-
guistic style matching in social interaction. Journal of
Language and Social Psychology, 21(4):337?360.
Martin J Pickering and Simon Garrod. 2004. Toward a mech-
anistic psychology of dialogue. Behavioral and brain sci-
ences, 27(2):169?190.
Richard Power. 1974. A computer model of conversation.
Byron Reeves and C Nass. 1997. The Media equation: how
people treat computers, television, and new media. Cam-
bridge University Press.
Adriana Tapus and Maja J Mataric. 2008. Socially Assistive
Robots: The Link between Personality, Empathy, Physi-
ological Signals, and Task Performance. In AAAI Spring
Symposium: Emotion, Personality, and Social Behavior,
pages 133?140.
Alexandria Katarina Vail and Kristy Elizabeth Boyer. In
press. Identifying Effective Moves in Tutorial Dialogue:
On the Refinement of Speech Act Annotation Schemes.
In Proceedings of the 12th International Conference on
Intelligent Tutoring Systems, Honolulu, Hawaii, USA.
Kurt VanLehn, Randolph M Jones, and Michelene TH Chi.
1992. A model of the self-explanation effect. The Journal
of the Learning Sciences, 2(1):1?59.
Daan Verbree, Rutger Rienks, and Dirk Heylen. 2006.
Dialogue-act tagging using smart feature selection; results
on multiple corpora. In Spoken Language Technology
Workshop, pages 70?73. IEEE.
Wolfgang Wahlster and Alfred Kobsa. 1989. User models in
dialog systems. Springer.
49
Extraverted Student Dialogue Excerpt
Extra-Domain Other? Positive Feedback
STUDENT: I haven?t really done this before.
TUTOR: You?re doing well!
Yes/No Answer? Reassurance
STUDENT: Yes.
TUTOR: Actually, I was wording the question incor-
rectly. . .
Explanation? Evaluative Question
STUDENT: But it still prompts for 3 or 4. . .
TUTOR: Yes; does that make sense from what you
learned about sequential program flow?
Information Question? Factual Question
STUDENT: What did I do wrong?
TUTOR: What is your Scanner?s name?
WH-Question Answer? Open Question
STUDENT: Previous.
TUTOR: Why did previousChoice get assigned
a value?
Wait? Probing Question TUTOR: What do you think about your program?s
behavior?
Information Question? Instruction
STUDENT: There wouldn?t have been any output?
TUTOR: Yeah, but more than that, the program
would report an error.
Introverted Student Dialogue Excerpt
Information Question? Reassurance
STUDENT: So the previous answer needs to be
stored as a part of PlayerInput2?
TUTOR: That would work fine.
Information Question? Probing Question
STUDENT: That?s not what I want?
TUTOR: Do you really want ?or??
Information Question? Question Prompt
STUDENT: So I need an else if for every if
statement?
TUTOR: Do you have any questions?
Wait? Elaborated Positive Feedback TUTOR: Nice, you compiled the code.
Yes/No Answer? Question Prompt
STUDENT: No, I got it.
TUTOR: Any questions so far?
Acknowledgement? Explanation
STUDENT: Okay.
TUTOR: When Java gets to the nextLine(), it
will stop.
Appendix 1: Dialogue excerpts illustrating the dialogue interactions emergent as significant in the anal-
ysis. All excerpts originate from Lesson 4, at the end of the series of dialogue sessions.
50
Proceedings of the SIGDIAL 2014 Conference, pages 113?122,
Philadelphia, U.S.A., 18-20 June 2014.
c?2014 Association for Computational Linguistics
Combining Task and Dialogue Streams in
Unsupervised Dialogue Act Models
Aysu Ezen-Can and Kristy Elizabeth Boyer
Department of Computer Science
North Carolina State University
aezen,keboyer@ncsu.edu
Abstract
Unsupervised machine learning ap-
proaches hold great promise for recog-
nizing dialogue acts, but the performance
of these models tends to be much lower
than the accuracies reached by supervised
models. However, some dialogues, such
as task-oriented dialogues with parallel
task streams, hold rich information that
has not yet been leveraged within unsu-
pervised dialogue act models. This paper
investigates incorporating task features
into an unsupervised dialogue act model
trained on a corpus of human tutoring in
introductory computer science. Exper-
imental results show that incorporating
task features and dialogue history fea-
tures significantly improve unsupervised
dialogue act classification, particularly
within a hierarchical framework that gives
prominence to dialogue history. This
work constitutes a step toward building
high-performing unsupervised dialogue
act models that will be used in the next
generation of task-oriented dialogue
systems.
1 Introduction
Dialogue acts represent the underlying intent of ut-
terances (Austin, 1975; Searle, 1969), and consti-
tute a crucial level of representation for dialogue
systems (Sridhar et al., 2009). The task of auto-
matic dialogue act classification has been exten-
sively studied for decades within several domains
including train fares and timetables (Allen et al.,
1995; Core and Allen, 1997; Crook et al., 2009;
Traum, 1999), virtual personal assistants (Chen
and Di Eugenio, 2013), conversational telephone
speech (Stolcke et al., 2000), Wikipedia talk pages
(Ferschke et al., 2012) and as in the case of this
paper, tutorial dialogue (Serafin and Di Eugenio,
2004; Forbes-Riley and Litman, 2005; Boyer et
al., 2011; Dzikovska et al., 2013).
Most of the prior work on dialogue act classi-
fication has depended on manually applying dia-
logue act tags and then leveraging supervised ma-
chine learning (Di Eugenio et al., 2010; Keizer
et al., 2002; Reithinger and Klesen, 1997; Ser-
afin and Di Eugenio, 2004). This process involves
engineering a dialogue act taxonomy (or using an
existing one, though domain-specific phenomena
can be difficult to capture within multi-purpose di-
alogue act taxonomies) and manually annotating
each utterance in the corpus. Then, the tagged
utterances are provided to a supervised machine
learner. This supervised approach can achieve
strong performance, in excess of 75% accuracy
on manual tags, approaching the agreement level
that is sometimes observed between human anno-
tators (Sridhar et al., 2009; Serafin and Di Euge-
nio, 2004; Chen and Di Eugenio, 2013).
However, the supervised approach has several
major drawbacks, including the fact that hand-
crafting dialogue act tagsets and applying them
manually tend to be bottlenecks within the re-
search and design process. To overcome these
drawbacks, the field has recently seen growing
momentum surrounding unsupervised approaches,
which do not require any manual labels during
model training (Crook et al., 2009; Joty et al.,
2011; Lee et al., 2013). A variety of unsupervised
machine learning techniques have been investi-
gated for dialogue act classification, and each line
of investigation has explored which features best
support this goal. However, to date the best per-
forming unsupervised models achieve in the range
of 40% (Rus et al., 2012) to 60% (Joty et al., 2011)
training set accuracy on manual tags, substantially
lower than the mid-70% accuracy (Sridhar et al.,
2009) often achieved on testing sets with super-
vised models.
113
In order to close this performance gap between
unsupervised and supervised techniques, we sug-
gest that it is crucial to enrich the features available
to unsupervised models. In particular, when a di-
alogue is task-oriented and includes a rich source
of information within a parallel task stream, these
features may substantially boost the ability of an
unsupervised model to distinguish dialogue acts.
For example, in situated dialogue, features rep-
resenting the state of the physical world may
be highly influential for dialogue act modeling
(Grosz and Sidner, 1986).
Human tutorial dialogue, which is the domain
being considered in the current work, often ex-
hibits this structure: the task artifact is external to
the dialogue utterances themselves (in the case of
our work, this artifact is a computer program that
the student is constructing). Task features have
already been shown beneficial for supervised di-
alogue act classification in our domain (Ha et al.,
2012). We hypothesize that including these task
features within an unsupervised model will signif-
icantly improve its performance. In addition, we
hypothesize that including dialogue history as a
prominent feature within an unsupervised model
will provide significant improvement.
This paper represents the first investigation into
combining task and dialogue features within an
unsupervised dialogue act classification model.
First, we discuss representation of these task fea-
tures and dialogue structure features, and compare
these representations within both flat and hierar-
chical clustering approaches. Second, we report
on experiments that demonstrate that the inclusion
of task features significantly improves dialogue
act classification, and that a hierarchical cluster
structure which explicitly captures dialogue his-
tory performs best. Finally, we break down the
model?s performance by dialogue act and investi-
gate which features are most beneficial for distin-
guishing particular acts. These contributions con-
stitute a step toward building high-performing un-
supervised dialogue act models that can be used in
the next generation of task-oriented dialogue sys-
tems.
2 Related Work
There is a rich body of work on dialogue act clas-
sification. Supervised approaches for dialogue act
classification aimed at improving performance by
using several features such as dialogue structure
including position of the turn (Ferschke et al.,
2012), speaker of an utterance (Tavafi et al., 2013),
previous dialogue acts (Kim et al., 2010), lexical
features such as words (Stolcke et al., 2000), syn-
tactic features including part-of-speech tags (Ban-
galore et al., 2008; Marineau et al., 2000), task-
subtask structure (Boyer et al., 2010) acoustic and
prosodic cues (Sridhar et al., 2009; Jurafsky et al.,
1998), and body posture (Ha et al., 2012).
For the growing body of work in unsupervised
dialogue act classification a subset of these fea-
tures have been utilized. The words (Crook et
al., 2009), topic words (Ritter et al., 2010), func-
tion words (Ezen-Can and Boyer, 2013b), begin-
ning portions of utterances (Rus et al., 2012), part-
of-speech tags and dependency trees (Joty et al.,
2011), and state transition probabilities in Markov
models (Lee et al., 2013) are among the list of
features investigated for unsupervised modeling of
dialogue acts. However, the accuracies achieved
by the best of these models are well below the ac-
curacies achieved by supervised techniques. To
improve performance of unsupervised models for
task-oriented dialogue, utilizing a combination of
task and dialogue features is a promising direction.
3 Corpus
The task-oriented dialogue corpus used in this
work was collected in a computer-mediated hu-
man tutorial dialogue study. Students (n =
42) and tutors interacted through textual dialogue
within an online learning environment for intro-
ductory Java programming (Ha et al., 2012). The
students were novices, never having programmed
in Java previously. The tutorial dialogue inter-
face consisted of four windows, one describing the
learning task, another where students wrote pro-
gramming code, beneath that the output of either
compiling or executing the program, and finally
the textual dialogue window (Figure 1).
As students and tutors interacted through this
interface, all dialogue messages and keystroke-
level task events were logged to a database. Only
students could compose, compile, and execute the
code, so task actions represent student actions
while dialogue messages were composed by both
participants. The corpus contains six lessons for
each student-tutor pair, of which only the first les-
son was annotated with dialogue act tags (?=0.80).
This annotated set contains 5,705 utterances
(4,065 tutor and 1,640 student). The average num-
114
Figure 1: The tutorial dialogue interface with four
windows.
ber of utterances (both tutor and student) per tutor-
ing session was 116 (min = 70, max = 211). The
average number of tutor utterances per session is
96 (min=44, max=156) whereas for students it is
39 (min=18, max=69) for the annotated set. The
average number of words per utterance for stu-
dents is 4.4 and for tutors it is 5.4. This annotated
set is used in the current analysis for both training
and testing where cross-validation is applied. As
described later, a separate set containing 462 un-
annotated utterances is used as a development set
for determining the number of clusters.
The dialogue stream of this corpus was manu-
ally annotated as part of previous work on super-
vised dialogue act modeling which achieved 69%
accuracy with Conditional Random Fields (Ha et
al., 2012). A brief description of the student di-
alogue act tags, which are the focus of the mod-
els reported in this paper, is shown in Table 1.
The most frequent dialogue act (A) constitutes the
baseline chance (39.85%). In the current work, the
manually applied dialogue act labels are not uti-
lized during model training, but are only used for
evaluation purposes as our models? accuracies are
reported for manual tags on a held-out test set.
An excerpt from the corpus is shown in Table 2.
Note that the current work focuses on classifying
student dialogue act tags, since in an automated di-
alogue system the tutor moves would be generated
by the system and their dialogue acts tags would
therefore be known.
4 Features
A key issue for dialogue act classification in task-
oriented dialogue involves how to represent dia-
Student Dialogue Act Distribution
Answer (A) 39.85
Acknowledgement (ACK) 21.31
Statement (S) 21.20
Question (Q) 15.15
Request for Feedback (RF) 0.98
Clarification (C) 0.79
Other (O) 0.61
Table 1: Student dialogue act tags and their fre-
quencies.
Tutor: ready? [Q]
Student: yep [A]
Tutor moves on to next task
Student: cool [S]
Student compiles and runs the code.
Program output: ?Hello World?
Tutor: excellent [PF]
Tutor: add a space to make the output look
prettier [DIR]
Student: why doesnt it stop on the next line
in this case? [Q]
Program halts
Tutor: it did [A]
Student runs the program successfully.
Tutor: good. [PF]
Table 2: Excerpt of dialogue from the corpus and
the task action that follows utterances.
logue and task events. This section describes how
features were extracted from the corpus of human
tutorial dialogue.
We use three sets of features: lexical features,
dialogue context features, and task features. The
lexical and dialogue context features are extracted
from the textual dialogue utterances within the
corpus. The task features are extracted from the
interaction traces within the computer-mediated
learning environment and represent a keystroke-
level log of events as students worked toward solv-
ing the computer programming problems.
4.1 Lexical Features
Because one of the main goals of our work in the
longer term is to perform automatic dialogue act
classification in real time, we took as a primary
consideration the ability to quickly extract lexical
features. The features utilized in the current in-
vestigation consist only of word unigrams. In ad-
115
dition to their ease of extraction, our prior work
has shown that addition of part-of-speech tags and
and syntax features did not significantly improve
the accuracy of supervised dialogue act classifiers
in this domain (Boyer et al., 2010), and these fea-
tures can be time-consuming to extract in real time
(Ha et al., 2012).
The choice to use word unigrams rather than
higher order n-grams is further facilitated by the
fact that our clustering technique leverages the
longest common sub-sequence (LCS) metric to
measure distances between utterances. This met-
ric counts shared sub-sequences of not-necessarily
contiguous words (Hirschberg, 1975). In this way,
the LCS metric provides a flexible way for n-
grams and skip-n-grams to be treated as impor-
tant units within the clustering, while the raw fea-
tures themselves consist only of word unigrams.
(We report on a comparison between LCS and bi-
grams later in the discussion section.) Utilizing
LCS, there exists a distance (1-similarity) value
from each utterance to every other utterance.
4.2 Dialogue Context Features
Based on previous work on a similar human tuto-
rial dialogue corpus (Ha et al., 2012), we utilize
four features that provide information about the di-
alogue structure. These features are depicted in
Table 3. Note that our goal within this work is to
classify student dialogue moves, not tutor moves,
because in a dialogue system the tutor?s moves are
system-generated with associated known dialogue
acts.
Feature Description
Utterance
position
The relative position of an
utterance from the beginning of
the dialogue.
Utterance
length
The number of tokens in the
utterance, including words and
punctuation.
Previous
author
Author of the previous dialogue
message (tutor or student) at the
time message sent.
Previous
tutor
dialogue act
Dialogue act of the previous
tutor utterance.
Table 3: Dialogue context features and their de-
scriptions.
4.3 Task Features
As described previously, the corpus contains two
channels of information: the dialogue utterances,
from which the lexical and dialogue context fea-
tures were extracted, and in addition, the task
stream consisting of student problem-solving ac-
tivities such as authoring code, compiling, and ex-
ecuting the program. The programming activities
of students were logged to a database along with
all of the dialogue events during tutoring.
A set of task features was found to be impor-
tant for dialogue act classification in this domain
in prior work, including most recent programming
action, status of the most recent task activity and
task activity flag representing whether the utter-
ance was preceded by a student?s task activity (Ha
et al., 2012). We expand this set of features as
shown in Table 4.
5 Experiments
The goal of this work is to investigate the im-
pact of including task and dialogue context fea-
tures on unsupervised dialogue act models. We
hypothesize that incorporating task features will
significantly improve the performance of an un-
supervised model, and we also hypothesize that
properly incorporating dialogue context features,
which are at a different granularity than the lex-
ical features extracted from utterances, will sub-
stantially improve model accuracy.
5.1 Dialogue Act Modeling With k-medoids
Clustering
The unsupervised models investigated here use k-
medoids clustering, which is a well-known clus-
tering technique that takes actual data points as
the center of each cluster (Ng and Han, 1994),
in contrast to k-means which may have synthetic
points as centroids. In k-medoids, the centroids
are initially selected and then the algorithm iter-
ates, reassigning data points in each iteration, un-
til the clusters converge. In standard k-medoids
clustering the initial seeds are selected randomly
and then a correct distribution of data points is
identified through the iteration and convergence
process. For dialogue act classification, the in-
fluence of the initial seeds is substantial because
the frequencies across dialogue tags are typically
unbalanced. To overcome this challenge, we use
a greedy seed selection approach similar to the
one used in k-means++ (Arthur and Vassilvitskii,
116
Feature Description
prev action
Most recent action of the
student (composing a dialogue
utterance, constructing code,
compiling or executing code).
task begin
Whether the student utterance is
the first utterance since the
beginning of the subtask.
task stu
Whether the student utterance
was preceded by a task event.
task prev tut
Task activity flag indicating
whether the closest tutor
utterance in this subtask was
preceded by a task activity.
task status
The status of the most recent
coding action (begin, stop,
success, error and input sent).
time elapsed
Time elapsed between the
previous tutor message and the
current student utterance.
errors
Number of errors in the
student?s latest code.
delta errors
Difference in the number of
errors in the task between two
utterances in the same dialogue.
stu # task
Number of student dialogue
messages sent within the current
task.
stu # dial
Number of student dialogue
messages sent within the current
dialogue.
tut # task
Number of tutor dialogue
messages sent within the current
subtask.
tut # dial
Number of tutor dialogue
messages sent within the current
dialogue.
Table 4: Task features extracted from student com-
puter programming activities.
2007) which selects the first seed randomly and
then greedily chooses seeds that are farthest from
the chosen seeds. The goal of using this approach
in our application is to choose seeds from different
dialogue acts so that the final model achieves good
coverage. Our preliminary experiments demon-
strated that this greedy seed selection combined
with k-medoids outperforms other clustering ap-
proaches including those utilized in our prior work
(Ezen-Can and Boyer, 2013a).
In order to select the number of clusters k,
a subset of the corpus, constituting 25% of the
full corpus (that were not tagged) composed of
462 utterances, was separated as a development
set. First, we examined the coherence of clus-
ters at different values of k using intra-cluster dis-
tances. This technique involves identifying an ?el-
bow? where the decrease in intra-cluster distance
becomes less rapid (since adding more clusters can
continue to decrease intra-cluster distance to the
point of overfitting) (Figure 2). The graph sug-
gests an elbow at k=5. Because there may be mul-
tiple elbows in the intra-cluster distance, a sec-
ond method utilizing Bayesian Information Crite-
rion (BIC) was used which penalizes models as
the number of parameters increases. The lower the
BIC value, the better the model is, achieved at k=5
as well.
Figure 2: Intra-cluster distances with varying
number of clusters.
Unlike many other investigations into unsuper-
vised dialogue act classification, the current ap-
proach reports accuracy on held-out test data, not
on the data on which the model was trained. Even
though the model training process does not utilize
available manual tags, requiring the learned unsu-
pervised model to perform well on held-out test
data more closely mimics the broader goal of our
work which is to utilize these unsupervised mod-
els within deployed dialogue systems, where most
utterances to be classified have never been encoun-
tered by the model before.
The procedure for model training and test-
ing uses leave-one-student-out cross-validation.
Rather than other forms of leave-one-out or strat-
ified cross-validation, leave-one-student-out en-
sures that each student?s set of dialogue utterances
are treated as the testing set while the model is
trained on all other students? utterances. This
process is repeated until each student?s utterances
117
have served as a held-out test set (in our case, this
results in n=42 folds). Within each fold, the clus-
ters are learned during training and then for each
utterance in the test set, its closest cluster is com-
puted by taking the average distance of the test ut-
terance to the elements in the cluster. The majority
label of the closest cluster is assigned as the dia-
logue act tag for the test utterance. If the assigned
dialogue act tag matches the manual label of the
test utterance, the utterance is counted as correct
classification. The average accuracy is computed
as the number of correct classifications divided by
the total number of classifications.
5.2 Experimental Results
We conducted experiments with seven different
feature combinations: L, lexical features only,
T , task features only, D, dialogue context fea-
tures only, and then the combinations of these fea-
tures, T + D, T + L, D + L, and T + D + L.
We hypothesized that the addition of task features
would significantly improve the models? accuracy.
As shown in Table 5, adding task features to di-
alogue context features significantly outperforms
dialogue context features alone (T + D > D).
Similarly, adding task features to lexical features
provides significant improvement (T + L > L).
However, adding task features to the dialogue con-
text plus lexical features model does not provide
benefit, and in fact slightly (not significantly) de-
grades performance (T + D + L 6> D + L). As
reflected by the Kappa scores, the test set perfor-
mance attained by these models is hardly better
than would be expected by chance.
Features
Accuracy
(%)
Kappa
F
l
a
t
C
l
u
s
t
e
r
i
n
g
L 33 0.02
T 37.7 0.07
D 37.6 0.07
T+D 39.1* 0.07
T+L 38* 0.06
D+L 38.3 0.07
T+D+L 37.3 0.05
Table 5: Test set accuracies and Kappa for the flat
clustering model (L: Lexical features, D: Dialogue
context features, T: Task features) *indicates sta-
tistically significant compared to the similar model
without task features (p < 0.05).
5.3 Utilizing Dialogue History
The importance of dialogue history, particularly
the influence of the most recent turn on an upcom-
ing turn, is widely recognized within dialogue re-
search, notably by work on adjacency pairs (Sche-
gloff and Sacks, 1973; Forbes-Riley et al., 2007;
Midgley et al., 2009). Based on these findings, we
hypothesized that dialogue history would be sub-
stantially beneficial for unsupervised dialogue act
models as it has been observed to be in numer-
ous studies on supervised classification. However,
as seen in the previous section, adding these di-
alogue context features with equal weight to the
model using Cosine distance only improved its
performance slightly though statistically signifi-
cantly (for example, T+D > T ), while the overall
performance is still barely above random chance.
In an attempt to substantially boost the perfor-
mance of the unsupervised dialogue act classi-
fier, we experimented with a hierarchical cluster-
ing structure in which the model first branches on
the previous tutor move, and then the clustering
models are learned as described previously at the
leaves of the tree (Figure 3).
This branching approach results in some
branches with too few utterances to train a multi-
cluster model. To deal with this situation we set a
threshold of n=10 utterances. For those subgroups
with fewer than 10 utterances, we take a simple
majority vote to classify test cases, and for those
subgroups with 10 or larger utterances we train a
cluster model and use it to classify test cases. For
the entire corpus, the number of utterances in each
branch is presented in Table 6.
Tutor?s Previous Dialogue Act
Q S PF A
doclustering
...doclusteringdoclusteringdoclustering
Figure 3: Branching student utterances according
to previous tutor dialogue act.
As the results in Table 7 show, the performance
of the model with hierarchical structure is signif-
icantly better than the flat clustering model. Note
that each feature in this table leverages previous
118
Tutor Dialogue
Act
# of student
utterances
Q 818
S 464
H 125
PF 91
A 61
ACK 11
C 8
O 8
RACK 6
Table 6: The number of student utterances after
branching on the previous tutor dialogue act.
tutor dialogue act while branching. Branching
on previous tutor move boosted the model?s accu-
racy for student move dialogue act classification
by approximately 30% accuracy across all feature
sets, a difference that is statistically significant in
every case. With the hierarchical model struc-
ture, the best performance is achieved by includ-
ing all three types of features: lexical, dialogue
context and task. However, our hypothesis that
task features would significantly improve the ac-
curacy does not hold within the hierarchical clus-
tering model (T +D 6> D and T + L 6> L).
Features
Accuracy
(%)
Kappa
H
i
e
r
a
r
c
h
i
c
a
l
T 64.2
?
0.45
D 63.2
?
0.46
L 60.7
?
0.41
T+D 62.1
?
0.44
T+L 63.3*
?
0.45
D+L 63.6
?
0.46
T+D+L 65*
?
0.48
Table 7: Test set accuracies and Kappa for branch-
ing on previous tutor dialogue act (L: Lexical fea-
tures, D: Dialogue context features, T: Task fea-
tures) *indicates statistically significant compared
to the similar model without task features and ? in-
dicates hierarchical clustering performing signifi-
cantly better than flat with same features. (p <
0.05).
6 Discussion
The experimental results provide compelling ev-
idence that an inclusive approach to features for
unsupervised dialogue act modeling holds great
promise. However, we observed a stark difference
in model performance when the tutor?s previous
move was simply included as one of many features
within a flat clustering model compared to when
the previous tutor move was treated as a branch-
ing feature. In this section we take a closer look
and discuss the features that help distinguish par-
ticular dialogue acts from each other.
Using the hierarchical T +D+L model which
performed best within the experiments, we exam-
ine the confusion matrix (Figure 4). Statements
and acknowledgments prove challenging for the
model, 51.3% and 61.5% accuracy overall. More-
over, these two tags are easily confused with each
other: 29.7% of statements were misclassified
as acknowledgments, while 21.2% of acknowl-
edgments were misclassified as statements. The
worst overall classification accuracy was for ques-
tions (6%) and the best was achieved for answers
(95.3%).
Figure 4: Confusion matrix for hierarchical model
utilizing all features: T+D+L.
When we analyze the performance of different
sets of features with respect to individual dialogue
acts, some interesting results emerge. The anal-
ysis shows that task features are especially good
for classifying statements. Using only task fea-
tures, the model correctly classified 61.8% state-
ments, compared to the lower 51.3% accuracy that
the overall best model (T + D + L) achieved on
statements. When we consider the nature of the
statement dialogue act within this corpus, we note
that it is a large category that encompasses a vari-
ety of utterances, some of which have lexical fea-
tures in common with acknowledgments. In this
case, task features are particularly helpful.
For acknowledgments, a combination of task
and lexical features performed best (63.6% ac-
119
curacy) compared to the overall best performing
model which achieved a slightly lower 61.5% ac-
curacy on acknowledgments. Acknowledgments
are another example of an act that may take am-
biguous surface form; for example, in our cor-
pus an utterance ?yes? appears as both an answer
and an acknowledgment depending on its context.
Therefore, higher level features such as the ones
provided by task may be more helpful.
For questions, the highest performing feature
set is L. However, as shown in Table 8, the model
performed poorly on questions. Inspection of the
models reveals that questions are varied in terms
of structure throughout the corpus and it is hard to
distinguish them from other dialogue acts. For in-
stance there are two consequent utterances ?i need
a write statement? and ?don?t i?, both of which are
manually labeled as questions. However, in terms
of the structure, the first utterance looks very sim-
ilar to a statement and therefore the model has dif-
ficulty grouping it with questions. Due to the large
variety of question forms in the corpus, it is pos-
sible that the clustering performed poorly on this
dialogue act. In future work it will be promising to
investigate the dialogue structures which produce
questions and to weight them more in the feature
set in order to increase performance of clustering
for questions.
We performed one additional experiment to
compare the performance of the LCS metric with
bigrams. For bigrams, the average leave-one-
student-out test accuracy was 25% with flat clus-
tering compared to the lexical-only case using
LCS (L) which reached 33%.
Features S A Q ACK
L 21.5 41.3 14.2 20.4
T 61.76 95.27 7.30 40.90
D 48.16 95.27 3.00 60.30
T+D 52.69 94.68 3.43 51.64
T+L 42.78 95.13 6.01 63.58
D+L 43.63 94.98 8.58 62.09
T+D+L 51.27 95.27 6.01 61.49
Table 8: Accuracies for individual dialogue acts.
Acts with fewer than 10 utterances after branching
are omitted from the table.
7 Conclusion and Future Work
Dialogue act classification is crucial for dialogue
management, and unsupervised modeling ap-
proaches hold great promise for automatically ex-
tracting classification models from corpora. This
paper has focused on unsupervised dialogue act
classification for task-oriented dialogue, investi-
gating the impact of task features and dialogue
context features on model accuracy within both
flat and hierarchical clusterings. Experimental
results confirm that utilizing a combination of
task and dialogue features improves accuracy and
that incorporating one previous tutor move as a
high-level branching feature a provides particu-
larly marked benefit. Moreover, it was found that
task features are particularly important for iden-
tifying particular dialogue moves such as state-
ments, for which the model with task features only
outperformed the model with all features.
In addition to the task stream, future work
should consider other sources of nonverbal cues
such as posture, gesture and facial expressions to
investigate the extent to which these can be suc-
cessfully incorporated in unsupervised dialogue
act models. Second, models that are built in spe-
cialized ways to different user groups (e.g., by
gender or by incoming skill level) should be inves-
tigated. Finally, the performance of unsupervised
dialogue act classification models must ultimately
move toward evaluation within implemented dia-
logue systems (Ezen-Can and Boyer, 2013a). The
overarching goal of these investigations is to cre-
ate unsupervised dialogue act models that perform
well enough to be used within deployed dialogue
systems and enable the system to respond success-
fully. It is hoped that in the future, dialogue act
classification models for many domains can be ex-
tracted automatically from corpora of human dia-
logue in those domains without the need for any
manual annotation.
Acknowledgments
Thanks to the members of the LearnDialogue
group at North Carolina State University for their
helpful input. This work is supported in part by the
National Science Foundation through Grant DRL-
1007962 and the STARS Alliance, CNS-1042468.
Any opinions, findings, conclusions, or recom-
mendations expressed in this report are those of
the participants, and do not necessarily represent
the official views, opinions, or policy of the Na-
tional Science Foundation.
120
References
James F. Allen, Lenhart K. Schubert, George Ferguson,
Peter Heeman, Chung Hee Hwang, Tsuneaki Kato,
Marc Light, Nathaniel Martin, Bradford Miller,
Massimo Poesio, et al. 1995. The TRAINS project:
A case study in building a conversational planning
agent. Journal of Experimental & Theoretical Arti-
ficial Intelligence, 7(1):7?48.
David Arthur and Sergei Vassilvitskii. 2007. k-
means++: The advantages of careful seeding. In
Proceedings of the 18th ACM-SIAM Symposium on
Discrete Algorithms, pages 1027?1035. Society for
Industrial and Applied Mathematics.
John Langshaw Austin. 1975. How To Do Things with
Words, volume 1955. Oxford University Press.
Srinivas Bangalore, Giuseppe Di Fabbrizio, and
Amanda Stent. 2008. Learning the structure of
task-driven human?human dialogs. IEEE Transac-
tions on Audio, Speech, and Language Processing,
16(7):1249?1259.
Kristy Elizabeth Boyer, Eun Young Ha, Robert
Phillips, Michael D. Wallis, Mladen A. Vouk, and
James C. Lester. 2010. Dialogue act modeling in
a complex task-oriented domain. In Proceedings of
SIGDIAL, pages 297?305. Association for Compu-
tational Linguistics.
Kristy Elizabeth Boyer, Eun Young Ha, Robert
Phillips, and James Lester. 2011. The impact of
task-oriented feature sets on HMMs for dialogue
modeling. In Proceedings of SIGDIAL, pages 49?
58. Association for Computational Linguistics.
Lin Chen and Barbara Di Eugenio. 2013. Multimodal-
ity and dialogue act classification in the RoboHelper
project. In Proceedings of SIGDIAL, pages 183?
192.
Mark G. Core and James Allen. 1997. Coding dialogs
with the DAMSL annotation scheme. In Proceed-
ings of the AAAI Fall Symposium on Communicative
Action in Humans and Machines, pages 28?35.
Nigel Crook, Ramon Granell, and Stephen Pulman.
2009. Unsupervised classification of dialogue acts
using a Dirichlet process mixture model. In Pro-
ceedings of SIGDIAL, pages 341?348. Association
for Computational Linguistics.
Barbara Di Eugenio, Zhuli Xie, and Riccardo Serafin.
2010. Dialogue act classification, higher order di-
alogue structure, and instance-based learning. Dia-
logue & Discourse, 1(2):1?24.
Myroslava O. Dzikovska, Elaine Farrow, and Jo-
hanna D. Moore. 2013. Combining semantic inter-
pretation and statistical classification for improved
explanation processing in a tutorial dialogue system.
In Artificial Intelligence in Education, pages 279?
288.
Aysu Ezen-Can and Kristy Elizabeth Boyer. 2013a.
In-context evaluation of unsupervised dialogue act
models for tutorial dialogue. In Proceedings of SIG-
DIAL, pages 324?328.
Aysu Ezen-Can and Kristy Elizabeth Boyer. 2013b.
Unsupervised classification of student dialogue acts
with query-likelihood clustering. In International
Conference on Educational Data Mining, pages 20?
27.
Oliver Ferschke, Iryna Gurevych, and Yevgen Chebo-
tar. 2012. Behind the article: Recognizing dialog
acts in Wikipedia talk pages. In Proceedings of the
13th Conference of the European Chapter of the As-
sociation for Computational Linguistics, pages 777?
786.
Kate Forbes-Riley and Diane J. Litman. 2005. Us-
ing bigrams to identify relationships between stu-
dent certainness states and tutor responses in a spo-
ken dialogue corpus. In Proceedings of the SIG-
DIAL Workshop, pages 87?96.
Kate Forbes-Riley, Mihai Rotaru, Diane J. Litman, and
Joel Tetreault. 2007. Exploring affect-context de-
pendencies for adaptive system development. In
Human Language Technologies 2007: The Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 41?44.
Barbara J. Grosz and Candace L. Sidner. 1986. Atten-
tion, intentions, and the structure of discourse. Com-
putational Linguistics, 12(3):175?204.
Eun Young Ha, Joseph F. Grafsgaard, Christopher M.
Mitchell, Kristy Elizabeth Boyer, and James C.
Lester. 2012. Combining verbal and nonverbal
features to overcome the ?information gap? in task-
oriented dialogue. In Proceedings of SIGDIAL,
pages 247?256.
Daniel S. Hirschberg. 1975. A linear space al-
gorithm for computing maximal common subse-
quences. Communications of the ACM, 18(6):341?
343.
Shafiq Joty, Giuseppe Carenini, and Chin-Yew Lin.
2011. Unsupervised modeling of dialog acts in
asynchronous conversations. In Proceedings of the
22nd International Joint Conference on Artificial In-
telligence, pages 1807?1813.
Daniel Jurafsky, Elizabeth Shriberg, Barbara Fox, and
Traci Curl. 1998. Lexical, prosodic, and syn-
tactic cues for dialog acts. In Proceedings of the
ACL/COLING-98 Workshop on Discourse Relations
and Discourse Markers, pages 114?120.
Simon Keizer, Rieks op den Akker, and Anton Nijholt.
2002. Dialogue act recognition with Bayesian net-
works for Dutch dialogues. In Proceedings of the
SIGDIAL Workshop, pages 88?94.
121
Su Nam Kim, Lawrence Cavedon, and Timothy Bald-
win. 2010. Classifying dialogue acts in one-on-
one live chats. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language
Processing, pages 862?871.
Donghyeon Lee, Minwoo Jeong, Kyungduk Kim, and
Seonghan Ryu. 2013. Unsupervised spoken lan-
guage understanding for a multi-domain dialog sys-
tem. IEEE Transactions On Audio, Speech, and
Language Processing, 21(11):2451?2464.
Johanna Marineau, Peter Wiemer-Hastings, Derek Har-
ter, Brent Olde, Patrick Chipman, Ashish Karnavat,
Victoria Pomeroy, Sonya Rajan, Art Graesser, Tutor-
ing Research Group, et al. 2000. Classification of
speech acts in tutorial dialog. In Proceedings of the
Workshop on Modeling Human Teaching Tactics and
Strategies at the Intelligent Tutoring Systems Con-
ference, pages 65?71.
T. Daniel Midgley, Shelly Harrison, and Cara Mac-
Nish. 2009. Empirical verification of adjacency
pairs using dialogue segmentation. In Proceedings
of SIGDIAL, pages 104?108.
Raymond Ng and Jiawei Han. 1994. Efficient and ef-
fective clustering methods for spatial data mining.
In Proceedings of the 20th International Conference
on Very Large Data Bases, pages 144?155.
Norbert Reithinger and Martin Klesen. 1997. Dia-
logue act classification using language models. In
Proceedings of EuroSpeech, pages 2235?2238.
Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Unsu-
pervised modeling of twitter conversations. In Pro-
ceedings of the Association for Computational Lin-
guistics, pages 172?180.
Vasile Rus, Cristian Moldovan, Nobal Niraula, and
Arthur C. Graesser. 2012. Automated discovery of
speech act categories in educational games. In Inter-
national Conference on Educational Data Mining,
pages 25?32.
Emanuel A. Schegloff and Harvey Sacks. 1973. Open-
ing up closings. Semiotica, 8(4):289?327.
John R. Searle. 1969. Speech Acts: An Essay in
the Philosophy of Language. Cambridge University
Press.
Riccardo Serafin and Barbara Di Eugenio. 2004.
FLSA: Extending latent semantic analysis with fea-
tures for dialogue act classification. In Proceedings
of the 42nd Annual Meeting on Association for Com-
putational Linguistics, pages 692?699. Association
for Computational Linguistics.
Rangarajan Sridhar, Vivek Kumar, Srinivas Bangalore,
and Shrikanth Narayanan. 2009. Combining lexi-
cal, syntactic and prosodic cues for improved online
dialog act tagging. Computer Speech & Language,
23(4):407?422.
Andreas Stolcke, Klaus Ries, Noah Coccaro, Eliza-
beth Shriberg, Rebecca Bates, Daniel Jurafsky, Paul
Taylor, Rachel Martin, Carol Van Ess-Dykema, and
Marie Meteer. 2000. Dialogue act modeling for
automatic tagging and recognition of conversational
speech. Computational Linguistics, 26(3):339?373.
Maryam Tavafi, Yashar Mehdad, Shafiq Joty, Giuseppe
Carenini, and Raymond Ng. 2013. Dialogue act
recognition in synchronous and asynchronous con-
versations. In Proceedings of SIGDIAL, pages 117?
121.
David R. Traum. 1999. Speech acts for dialogue
agents. In Foundations of Rational Agency, pages
169?201. Springer.
122
