Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 963?970, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Query Expansion with the Minimum User Feedback
by Transductive Learning
Masayuki OKABE
Information and Media Center
Toyohashi University of Technology
Aichi, 441-8580, Japan
okabe@imc.tut.ac.jp
Kyoji UMEMURA
Information and Computer Sciences
Toyohashi University of Technology
Aichi, 441-8580, Japan
umemura@tutics.tut.ac.jp
Seiji YAMADA
National Institute of Informatics
Tokyo,101-8430, Japan
seiji@nii.ac.jp
Abstract
Query expansion techniques generally se-
lect new query terms from a set of top
ranked documents. Although a user?s
manual judgment of those documents
would much help to select good expansion
terms, it is difficult to get enough feedback
from users in practical situations. In this
paper we propose a query expansion tech-
nique which performs well even if a user
notifies just a relevant document and a
non-relevant document. In order to tackle
this specific condition, we introduce two
refinements to a well-known query expan-
sion technique. One is application of a
transductive learning technique in order to
increase relevant documents. The other is
a modified parameter estimation method
which laps the predictions by multiple
learning trials and try to differentiate the
importance of candidate terms for expan-
sion in relevant documents. Experimen-
tal results show that our technique outper-
forms some traditional query expansion
methods in several evaluation measures.
1 Introduction
Query expansion is a simple but very useful tech-
nique to improve search performance by adding
some terms to an initial query. While many query
expansion techniques have been proposed so far, a
standard method of performing is to use relevance
information from a user (Ruthven, 2003). If we
can use more relevant documents in query expan-
sion, the likelihood of selecting query terms achiev-
ing high search improvement increases. However it
is impractical to expect enough relevance informa-
tion. Some researchers said that a user usually noti-
fies few relevance feedback or nothing (Dumais and
et al, 2003).
In this paper we investigate the potential perfor-
mance of query expansion under the condition that
we can utilize little relevance information, espe-
cially we only know a relevant document and a non-
relevant document. To overcome the lack of rele-
vance information, we tentatively increase the num-
ber of relevant documents by a machine learning
technique called Transductive Learning. Compared
with ordinal inductive learning approach, this learn-
ing technique works even if there is few training ex-
amples. In our case, we can use many documents
in a hit-list, however we know the relevancy of few
documents. When applying query expansion, we use
those increased documents as if they were true rel-
evant ones. When applying the learning, there oc-
curs some difficult problems of parameter settings.
We also try to provide a reasonable resolution for
the problems and show the effectiveness of our pro-
posed method in experiments.
The point of our query expansion method is that
we focus on the availability of relevance information
in practical situations. There are several researches
which deal with this problem. Pseudo relevance
feedback which assumes top n documents as rele-
vant ones is one example. This method is simple and
relatively effective if a search engine returns a hit-
963
list which contains a certain number of relative doc-
uments in the upper part. However, unless this as-
sumption holds, it usually gives a worse ranking than
the initial search. Thus several researchers propose
some specific procedure to make pseudo feedback
be effective (Yu and et al 2003; Lam-Adesina and
Jones, 2001). In another way, Onoda (Onoda et al,
2004) tried to apply one-class SVM (Support Vec-
tor Machine) to relevance feedback. Their purpose
is to improve search performance by using only non-
relevant documents. Though their motivation is sim-
ilar to ours in terms of applying a machine learning
method to complement the lack of relevance infor-
mation, the assumption is somewhat different. Our
assumption is to utilizes manual but the minimum
relevance judgment.
Transductive leaning has already been applied in
the field of image retrieval (He and et al, 2004). In
this research, they proposed a transductive method
called the manifold-ranking algorithm and showed
its effectiveness by comparing with active learn-
ing based Support Vector Machine. However, their
setting of relevance judgment is not different from
many other traditional researches. They fix the total
number of images that are marked by a user to 20.
As we have already claimed, this setting is not prac-
tical because most users feel that 20 is too much for
judgment. We think none of research has not yet an-
swered the question. For relevance judgment, most
of the researches have adopted either of the follow-
ing settings. One is the setting of ?Enough relevant
documents are available?, and the other is ?No rele-
vant document is available?. In contrast to them, we
adopt the setting of ?Only one relevant document is
available?. Our aim is to achieve performance im-
provement with the minimum effort of judging rele-
vancy of documents.
The reminder of this paper is structured as fol-
lows. Section 2 describes two fundamental tech-
niques for our query expansion method. Section 3
explains a technique to complement the smallness
of manual relevance judgment. Section 4 introduces
a whole procedure of our query expansion method
step by step. Section 5 shows empirical evidence
of the effectiveness of our method compared with
two traditional query expansion methods. Section 6
investigates the experimental results more in detail.
Finally, Section 7 summarizes our findings.
2 Basic Methods
2.1 Query Expansion
So far, many query expansion techniques have been
proposed. While some techniques focus on the
domain specific search which prepares expansion
terms in advance using some domain specific train-
ing documents (Flake and et al 2002; Oyama and et
al, 2001), most of techniques are based on relevance
feedback which is given automatically or manually.
In this technique, expansion terms are selected
from relevant documents by a scoring function. The
Robertson?s wpq method (Ruthven, 2003) is often
used as such a scoring function in many researches
(Yu and et al 2003; Lam-Adesina and Jones, 2001).
We also use it as our basic scoring function. It cal-
culates the score of each term by the following for-
mula.
wpqt =
(rt
R ?
nt ? rt
N ? R
)
?log rt/(R ? rt)(nt ? rt)/(N ? nt ? R + rt)
(1)
where rt is the number of seen relevant documents
containing term t. nt is the number of documents
containing t. R is the number of seen relevant doc-
uments for a query. N is the number of documents
in the collection. The second term of this formula
is called the Robertson/Spark Jones weight (Robert-
son, 1990) which is the core of the term weighting
function in the Okapi system (Robertson, 1997).
This formula is originated in the following for-
mula.
wpqt = (pt ? qt) log
pt(1? qt)
qt(1? pt)
(2)
where pt is the probability that a term t appears in
relevant documents. qt is the probability that a term
t appears in non-relevant documents. We can easily
notice that it is very important how the two prob-
ability of pt and qt should be estimated. The first
formula estimates pt with rtR and qt with
Nt?Rt
N?R . For
the good estimation of pt and qt, plenty of relevant
document is necessary. Although pseudo feedback
which automatically assumes top n documents as
relevant is one method and is often used, its perfor-
mance heavily depends on the quality of an initial
search. As we show later, pseudo feedback has lim-
ited performance.
We here consider a query expansion technique
which uses manual feedback. It is no wonder
964
manual feedback shows excellent and stable perfor-
mance if enough relevant documents are available,
hence the challenge is how it keeps high perfor-
mance with less amount of manual relevance judg-
ment. In particular, we restrict the manual judgment
to the minimum amount, namely only a relevant
document and a non-relevant document. In this
assumption, the problem is how to find more rele-
vant documents based on a relevant document and a
non-relevant document. We use transductive learn-
ing technique which is suitable for the learning prob-
lem where there is small training examples.
2.2 Transductive Learning
Transductive learning is a machine learning tech-
nique based on the transduction which directly de-
rives the classification labels of test data without
making any approximating function from training
data (Vapnik, 1998). Because it does not need to
make approximating function, it works well even if
the number of training data is small.
The learning task is defined on a data set X
of n points. X consists of training data set
L = (x?1, x?2, ..., x?l) and test data set U =
(x?l+1, x?l+2, ..., x?l+u); typically l ? u. The purpose
of the learning is to assign a label to each data point
in U under the condition that the label of each data
point in L are given.
Recently, transductive learning or semi-
supervised learning is becoming an attractive
subject in the machine learning field. Several
algorithms have been proposed so far (Joachims,
1999; Zhu and et al, 2003; Blum and et al, 2004)
and they show the advantage of this approach in
various learning tasks. In order to apply transductive
learning to our query expansion, we select an algo-
rithm called ?Spectral Graph Transducer (SGT)?
(Joachims, 2003), which is one of the state of the art
and the best transductive learning algorithms. SGT
formalizes the problem of assigning labels to U with
an optimization problem of the constrained ratiocut.
By solving the relaxed problem, it produces an
approximation to the original solution.
When applying SGT to query expansion, X cor-
responds to a set of top n ranked documents in a
hit-list. X does not corresponds to a whole docu-
ment collection because the number of documents
in a collection is too huge1 for any learning sys-
tem to process. L corresponds to two documents
with manual judgments, a relevant document and
a non-relevant document. Furthermore, U corre-
sponds to the documents of X ? L whose rele-
vancy is unknown. SGT is used to produce the rel-
evancy of documents in U . SGT actually assigns
values around ?+ ? ? for documents possibly be-
ing relevant and ?? ? ? for documents possibly be-
ing non-relevant. ?+ = +
?
1?fp
fp , ?? = ?
?
fp
1?fp ,
? = 12(?+ + ??), and fp is the fraction of relevant
documents in X . We cannot know the true value of
fp in advance, thus we have to estimate its approxi-
mation value before applying SGT.
According to Joachims, parameter k (the number
of k-nearest points of a data x?) and d (the number
of eigen values to ...) give large influence to SGT?s
learning performance. Of course those two parame-
ters should be set carefully. However, besides them,
fp is much more important for our task because it
controls the learning performance. Since extremely
small L (actually |L| = 2 is our setting) give no
information to estimate the true value of fp, we do
not strain to estimate its single approximation value
but propose a new method to utilize the results of
learning with some promising fp. We describe the
method in the next section.
3 Parameter Estimations based on
Multiple SGT Predictions
3.1 Sampling for Fraction of Positive Examples
SGT prepares 2 estimation methods to set fp au-
tomatically. One is to estimate from the fraction
of positive examples in training examples. This
method is not suitable for our task because fp is
always fixed to 0.5 by this method if the number
of training examples changes despite the number of
relevant documents is small in many practical situa-
tions. The other is to estimate with a heuristic that
the difference between a setting of fp and the frac-
tion of positive examples actually assigned by SGT
should be as small as possible. The procedure pro-
vided by SGT starts from fp = 0.5 and the next fp is
set to the fraction of documents assigned as relevant
in the previous SGT trial. It repeats until fp changes
1Normally it is more than ten thousand.
965
Input
Ntr // the number of training examples
Output
S // a set of sampling points
piv = ln(Ntr); // sampling interval
nsp = 0; // the number of sampling points
for(i = piv; i ? Ntr ? 1; i+ = piv){
add i to ;
nsp++;
if(nsp == 10){ exit; }
}
Figure 1: Pseudo code of sampling procedure for fp
five times or the difference converges less than 0.01.
This method is neither works well because the con-
vergence is not guaranteed at all.
Presetting of fp is primarily very difficult problem
and consequently we take another approach which
laps the predictions of multiple SGT trials with some
sampled fp instead of setting a single fp. This ap-
proach leads to represent a relevant document by not
a binary value but a real value between 0 and 1. The
sampling procedure for fp is illustrated in Figure 1.
In this procedure, sampling interval changes accord-
ing to the number of training examples. In our pre-
liminary test, the number of sampling points should
be around 10. However this number is adhoc one,
thus we may need another value for another corpus.
3.2 Modified estimations for pt and qt
Once we get a set of sampling points S = {f ip :
i = 1 ? 10}, we run SGT with each f ip and laps
each resultant of prediction to calculate pt and qt as
follows.
pt =
?
i rit
?
i Ri
(3)
qt =
?
i nt ? rit
?
i N ?Ri
(4)
Here, Ri is the number of documents which SGT
predicts as relevant with ith value of f ip, and rit is
the number of documents in Ri where a term t ap-
pears. In each trial, SGT predicts the relevancy of
documents by binary value of 1 (for relevant) and 0
(for non-relevant), yet by lapping multiple resultant
of predictions, the binary prediction value changes
to a real value which can represents the relevancy of
documents in more detail. The main merit of this
approach in comparison with fixing fp to a single
value, it can differentiate a value of pt if Ntr is small.
4 Expansion Procedures
We here explain a whole procedure of our query ex-
pansion method step by step.
1. Initial Search: A retrieval starts by inputting a
query for a topic to an IR system.
2. Relevance Judgment for Documents in a
Hit-List: The IR system returns a hit-list for
the initial query. Then the hit-list is scanned
to check whether each document is relevant or
non-relevant in descending order of the rank-
ing. In our assumption, this reviewing pro-
cess terminates when a relevant document and
a non-relevant one are found.
3. Finding more relevant documents by trans-
ductive learning: Because only two judged
documents are too few to estimate pt and qt
correctly, our query expansion tries to increase
the number of relevant documents for the wpq
formula using the SGT transductive learning al-
gorithm. As shown in Figure2, SGT assigns a
value of the possibility to be relevant for the
topic to each document with no relevance judg-
ment (documents under the dashed line in the
Fig) based on two judged documents (docu-
ments above the dashed line in the Figure).
1. Document     1
2. Document     0
3. Document     ?
4. Document     ?
              :
i.  Document     ?
              :
Manually
assigned
Assigned by
Transductive
 Learning
Labels
Hit list
?1? means a positive label
?0? means a negative label
??? means an unknown label
Figure 2: A method to find tentative relevant docu-
ments
966
4. Selecting terms to expand the initial query:
Our query expansion method calculates the
score of each term appearing in relevant docu-
ments (including documents judged as relevant
by SGT) using wpq formula, and then selects
a certain number of expansion terms according
to the ranking of the score. Selected terms are
added to the initial query. Thus an expanded
query consists of the initial terms and added
terms.
5. The Next Search with an expanded query:
The expanded query is inputted to the IR sys-
tem and a new hit-list will be returned. One
cycle of query expansion finishes at this step.
In the above procedures, we naturally intro-
duced transductive learning into query expan-
sion as the effective way in order to automati-
cally find some relevant documents. Thus we
do not need to modify a basic query expan-
sion procedure and can fully utilize the poten-
tial power of the basic query expansion.
The computational cost of transductive learn-
ing is not so much. Actually transductive learn-
ing takes a few seconds to label 100 unla-
beled documents and query expansion with all
the labeled documents also takes a few sec-
onds. Thus our system can expand queries suf-
ficiently quick in practical applications.
5 Experiments
This section provides empirical evidence on how
our query expansion method can improve the per-
formance of information retrieval. We compare our
method with other traditional methods.
5.1 Environmental Settings
5.1.1 Data set
We use the TREC-8 data set (Voorhees and Har-
man, 1999) for our experiment. The document cor-
pus contains about 520,000 news articles. Each doc-
ument is preprocessed by removing stopwords and
stemming. We also use fifty topics (No.401-450)
and relevance judgments which are prepared for ad-
hoc task in the TREC-8. Queries for an initial search
are nouns extracted from the title tag in each topic.
5.1.2 Retrieval Models
We use two representative retrieval models which
are bases of the Okapi (Robertson, 1997) and
SMART systems. They showed highest perfor-
mance in the TREC-8 competition.
Okapi : The weight function in Okapi is BM25. It
calculates each document?s score by the follow-
ing formula.
score(d) =
?
T?Q
w(1) ? (k1 + 1)tf(k3 + 1)qtf(K + tf)(k3 + qtf)
(5)
w(1) = log (rt + 0.5)/(R ? rt + 0.5)(nt ? rt + 0.5)/(N ? nt ? R + rt + 0.5)
(6)
K = k1
(
(1 ? b) + b dlavdl
)
(7)
where Q is a query containing terms T , tf
is the term?s frequency in a document, qtf is
the term?s frequency in a text from which Q
was derived. rt and nt are described in sec-
tion 2. K is calculated by (7), where dl and
avdl denote the document length and the av-
erage document length. In our experiments,
we set k1 = 1.2, k3 = 1000, b = 0.75, and
avdl = 135.6. Terms for query expansion are
ranked in decreasing order of rt ? w(1) for the
following Okapi?s retrieval tests without SGT
(Okapi manual and Okapi pseudo) to make
conditions the same as of TREC-8.
SMART : The SMART?s weighting function is as
follows2.
score(d) =
?
T?Q
{1 + ln(1 + ln(tf))} ? log(N + 1df ) ? pivot (8)
pivot = 1
0.8 + 0.2 ? dlavdl
(9)
df is the term?s document frequency. tf , dl and
avdl are the same as Okapi. When doing rele-
vance feedback, a query vector is modified by
the following Rocchio?s method (with parame-
ters ? = 3, ? = 2, ? = 2).
Q?new = ?Q?old+
?
|Drel|
?
Drel
d?? ?|Dnrel|
?
Dnrel
d? (10)
2In this paper, we use AT&T?s method (Singhal et al, 1999)
applied in TREC-8
967
Table 1: Results of Initial Search
P10 P30 RPREC MAP R05P
Okapi ini 0.466 0.345 0.286 0.239 0.195
SMART ini 0.460 0.336 0.271 0.229 0.187
Drel and Dnrel are sets of seen relevant and
non-relevant documents respectively. Terms
for query expansion are ranked in decreasing
order of the above Rocchio?s formula.
Table 1 shows their initial search results of Okapi
(Okapi ini) and SMART (SMART ini). We adopt
five evaluation measures. Their meanings are as fol-
lows (Voorhees and Harman, 1999).
P10 : The precision after the first 10 documents are
retrieved.
P30 : The precision after the first 30 documents are
retrieved.
R-Prec : The precision after the first R documents
are retrieved, where R is the number of relevant
documents for the current topic.
MAP : Mean average precision (MAP) is the aver-
age precision for a single topic is the mean of
the precision obtained after each relevant doc-
ument is retrieved (using zero as the precision
for relevant documents that are not retrieved).
R05P : Recall at the rank where precision first dips
below 0.5 (after at least 10 documents have
been retrieved).
The performance of query expansion or relevance
feedback is usually evaluated on a residual collec-
tion where seen documents are removed. However
we compare our method with pseudo feedback based
ones, thus we do not use residual collection in the
following experiments.
5.1.3 Settings of Manual Feedback
For manual feedback, we set an assumption that
a user tries to find relevant and non-relevant doc-
uments within only top 10 documents in the result
of an initial search. If a topic has no relevant doc-
ument or no non-relevant document in the top 10
documents, we do not apply manual feedback, in-
stead we consider the result of the initial search for
Table 2: Results of Okapi sgt (5 terms expanded)
P10 P30 RPREC MAP R05P
20 0.516 0.381 0.308 0.277 0.233
50 0.494 0.380 0.286 0.265 0.207
100 0.436 0.345 0.283 0.253 0.177
Table 3: Results of Okapi sgt (10 terms expanded)
P10 P30 RPREC MAP R05P
20 0.508 0.383 0.301 0.271 0.216
50 0.520 0.387 0.294 0.273 0.208
100 0.494 0.365 0.283 0.261 0.190
Table 4: Results of Okapi sgt (15 terms expanded)
P10 P30 RPREC MAP R05P
20 0.538 0.381 0.298 0.274 0.223
50 0.528 0.387 0.298 0.283 0.222
100 0.498 0.363 0.280 0.259 0.197
Table 5: Results of Okapi sgt (20 terms expanded)
P10 P30 RPREC MAP R05P
20 0.546 0.387 0.307 0.289 0.235
50 0.520 0.385 0.299 0.282 0.228
100 0.498 0.369 0.272 0.255 0.188
such topics. There are 8 topics 3 which we do not
apply manual feedback methods.
5.2 Basic Performance
Firstly, we evaluate the basic performance of our
query expansion method by changing the number
of training examples. Since our method is based on
Okapi model, we represent it as Okapi sgt (with pa-
rameters k = 0.5?Ntr, d = 0.8?Ntr. k is the num-
ber of nearest neighbors, d is the number of eigen
values to use and Ntr is the number of training ex-
amples).
Table 2-5 shows five evaluation measures of
Okapi sgt when the number of expansion terms
changes. We test 20, 50 and 100 as the number of
training examples and 5, 10 15 and 20 for the num-
ber of expansion terms. As for the number of train-
ing examples, performance of 20 and 50 does not
differ so much in all the number of expansion terms.
However performance of 100 is clearly worse than
of 20 and 50. The number of expansion terms does
not effect so much in every evaluation measures. In
the following experiments, we compare the results
of Okapi sgt when the number of training examples
is 50 with other query expansion methods.
3Topic numbers are 409, 410, 424, 425, 431, 432, 437 and
450
968
Table 6: Results of Manual Feedback Methods
(MAP)
5 10 15 20
Okapi sgt 0.265 0.273 0.274 0.282
Okapi man 0.210 0.189 0.172 0.169
SMART man 0.209 0.222 0.220 0.219
Table 7: Results of Manual Feedback Methods (10
terms expanded)
P10 P30 RPREC MAP R05P
Okapi sgt 0.520 0.387 0.294 0.273 0.208
Okapi man 0.420 0.285 0.212 0.189 0.132
SMART man 0.434 0.309 0.250 0.222 0.174
5.3 Comparison with other Manual Feedback
Methods
We next compare our query expansion method with
the following manual feedback methods.
Okapi man : This method simply uses only one
relevant document judged by hand. This is
called incremental relevance feedback (Aal-
bersberg, 1992; Allan, 1996; Iwayama, 2000).
SMART man : This method is SMART?s manual
relevance feedback (with parameters ? = 3,
? = 2, ? = 0). ? is set to 0 because the perfor-
mance is terrible if ? is set to 2.
Table 6 shows the mean average precision of
three methods when the number of expansion terms
changes. Since the number of feedback docu-
ments is extremely small, two methods except for
Okapi sgt get worse than their initial searches.
Okapi man slightly decreases as the number of ex-
pansion terms increases. Contrary, SMART man
do not change so much as the number of expansion
terms increases. Table 7 shows another evaluation
measures with 10 terms expanded. It is clear that
Okapi sgt outperforms the other two methods.
5.4 Comparison with Pseudo Feedback
Methods
We finally compare our query expansion method
with the following pseudo feedback methods.
Okapi pse : This is a pseudo version of Okapi
which assumes top 10 documents in the initial
search as relevant ones as well as TREC-8 set-
tings.
Table 8: Results of Pseudo Feedback Methods
(MAP)
5 10 15 20
Okapi sgt 0.265 0.273 0.274 0.282
Okapi pse 0.253 0.249 0.247 0.246
SMART pse 0.236 0.243 0.242 0.242
Table 9: Results of Pseudo Feedback Methods (10
terms expanded)
P10 P30 RPREC MAP R05P
Okapi sgt 0.520 0.387 0.294 0.273 0.208
Okapi pse 0.478 0.369 0.279 0.249 0.206
SMART pse 0.466 0.359 0.272 0.243 0.187
SMART pse : This is a pseudo version of SMART.
It also assumes top 10 documents as relevant
ones. In addition, it assumes top 500-1000 doc-
uments as non-relevant ones.
In TREC-8, above two methods uses TREC1-5 disks
for query expansion and a phase extraction tech-
nique. However we do not adopt these methods in
our experiments4. Since these methods showed the
highest performance in the TREC-8 adhoc task, it
is reasonable to compare our method with them as
competitors.
Table 8 shows the mean average precision of
three methods when the number of expansion terms
changes. Performance does not differ so much if the
number of expansion terms changes. Okapi sgt out-
performs at any number of expansion. Table 9 shows
the results in other evaluation measures. Okapi sgt
also outperforms except for R05P. In particular, per-
formance in P10 is quite well. It is preferable behav-
ior for the use in practical situations.
6 Discussion
In the experiments, the feedback documents for
Okapi sgt is top ranked ones. However some users
do not select such documents. They may choose
another relevant and non-relevant documents which
rank in top 10. Thus we test an another experiment
where relevant and non-relevant documents are se-
lected randomly from top 10 rank. Table 10 shows
the result. Compared with table 2, the performance
seems to become slightly worse. This shows that a
4Thus the performance in our experiments is a bit worse than
the result of TREC-8
969
Table 10: Results of Okapi sgt with random feed-
back (5 terms expanded)
P10 P30 RPREC MAP R05P
20 0.498 0.372 0.288 0.265 0.222
50 0.456 0.359 0.294 0.268 0.200
100 0.452 0.335 0.270 0.246 0.186
user should select higher ranked documents for rel-
evance feedback.
7 Conclusion
In this paper we proposed a novel query expansion
method which only use the minimum manual judg-
ment. To complement the lack of relevant docu-
ments, this method utilizes the SGT transductive
learning algorithm to predict the relevancy of un-
judged documents. Since the performance of SGT
much depends on an estimation of the fraction of
relevant documents, we propose a method to sam-
ple some good fraction values. We also propose a
method to laps the predictions of multiple SGT tri-
als with above sampled fraction values and try to
differentiate the importance of candidate terms for
expansion in relevant documents. The experimental
results showed our method outperforms other query
expansion methods in the evaluations of several cri-
teria.
References
I. J. Aalbersberg. 1992. Incremental relevance feedback.
In Proceedings of SIGIR ?92, pages 11?22.
J. Allan. 1996. Incremental relevance feedback for infor-
mation filtering. In Proceedings of SIGIR ?96, pages
270?278.
A. Blum and et al 2004. Semi-supervised learning using
randomized mincuts. In Proceedings of ICML 2004.
S. Dumais and et al 2003. Sigir 2003 workshop report:
Implicit measures of user interests and preferences. In
SIGIR Forum.
G. W. Flake and et al 2002. Extracting query modifi-
cation from nonlinear svms. In Proceedings of WWW
2002.
J. He and et al 2004. Manifold-ranking based image
retrieval. In Proceedings of Multimedia 2004, pages
9?13. ACM.
M. Iwayama. 2000. Relevance feedback with a small
number of relevance judgements: Incremental rele-
vance feedback vs. document clustering. In Proceed-
ings of SIGIR 2000, pages 10?16.
T. Joachims. 1999. Transductive inference for text clas-
sification using support vector machines. In Proceed-
ings of ICML ?99.
T. Joachims. 2003. Transductive learning via spectral
graph partitioning. In Proceedings of ICML 2003,
pages 143?151.
A. M. Lam-Adesina and G. J. F. Jones. 2001. Applying
summarization techniques for term selection in rele-
vance feedback. In Proceedings of SIGIR 2001, pages
1?9.
T. Onoda, H. Murata, and S. Yamada. 2004. Non-
relevance feedback document retrieva. In Proceedings
of CIS 2004. IEEE.
S. Oyama and et al 2001. keysword spices: A new
method for building domain-specific web search en-
gines. In Proceedings of IJCAI 2001.
S. E. Robertson. 1990. On term selection for query ex-
pansion. Journal of Documentation, 46(4):359?364.
S. E. Robertson. 1997. Overview of the okapi projects.
Journal of the American Society for Information Sci-
ence, 53(1):3?7.
I. Ruthven. 2003. Re-examining the potential effective-
ness of interactive query expansion. In Proceedings of
SIGIR 2003, pages 213?220.
A. Singhal, S. Abney, B. Bacchiani, M. Collins, D. Hin-
dle, and F. Pereira. 1999. At&t at trec-8.
V Vapnik. 1998. Statistical learning theory. Wiley.
E. Voorhees and D. Harman. 1999. Overview of the
eighth text retrieval conference.
S. Yu and et al 2003. Improving pseud-relevance feed-
back in web information retrieval using web page seg-
mentation. In Proceedings of WWW 2003.
X Zhu and et al 2003. Semi-supervised learning using
gaussian fields and harmonic functions. In Proceed-
ings of ICML 2003, pages 912?914.
970
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 176?184,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
Non-humanlike Spoken Dialogue: A Design Perspective
Kotaro Funakoshi
Honda Research Institute Japan Co., Ltd.
8-1 Honcho, Wako
Saitama, Japan
funakoshi@jp.honda-ri.com
Mikio Nakano
Honda Research Institute Japan Co., Ltd.
8-1 Honcho, Wako
Saitama, Japan
nakano@jp.honda-ri.com
Kazuki Kobayashi
Shinshu University
4-17-1 Wakasato, Nagano
Nagano, Japan
kby@shinshu-u.ac.jp
Takanori Komatsu
Shinshu University
3-15-1 Tokida, Ueda
Nagano, Japan
tkomat@shinshu-u.ac.jp
Seiji Yamada
National Institute of Informatics
2-1-2 Hitotsubashi, Chiyoda
Tokyo, Japan
seiji@nii.ac.jp
Abstract
We propose a non-humanlike spoken di-
alogue design, which consists of two el-
ements: non-humanlike turn-taking and
non-humanlike acknowledgment. Two ex-
perimental studies are reported in this pa-
per. The first study shows that the pro-
posed non-humanlike spoken dialogue de-
sign is effective for reducing speech colli-
sions. It also presents pieces of evidence
that show quick humanlike turn-taking is
less important in spoken dialogue system
design. The second study supports a hy-
pothesis found in the first study that user
preference on response timing varies de-
pending on interaction patterns. Upon re-
ceiving these results, this paper suggests a
practical design guideline for spoken dia-
logue systems.
1 Introduction
Speech and language are owned by humans.
Therefore, spoken dialogue researchers tend to
pursue a humanlike spoken dialogue. Only a few
researchers positively investigate restricted (i.e.,
non-humanlike) spoken dialogue design such as
(Ferna?ndez et al, 2007).
Humanlikeness is a very important concept and
sometimes it is really useful to design machines /
interactions. Machines are, however, not humans.
We believe humanlikenss cannot be the dominant
factor, or gold-standard, for designing spoken dia-
logues.
Pursuing humanlikeness has at least five criti-
cal problems. (1) Cost: in general, humanlikeness
demands powerful and highly functional hardware
and software, and highly integrated systems re-
quiring top-grade experts both for development
and maintenance. All of them lead to cost over-
run. (2) Performance: sometimes, humanlikeness
forces performance to be compromised. For ex-
ample, achieving quick turn-taking which humans
do in daily conversations forces automatic speech
recognizers, reasoners, etc. to be compromised to
enable severe real-time processing. (3) Applicabil-
ity: differences in cultures, genders, generations,
situations limit the applicability of a humanlike
design because it often accompanies a rigid char-
acter. For example, Shiwa et al (2008) succeeded
in improving users? impression for slow responses
from a robot by using a filler but obviously use
of such a filler is limited by social appropriate-
ness. (4) Expectancy: humanlike systems induce
too much expectancy of users that they are as in-
telligent as humans. It will result in disappoint-
ments (Komatsu and Yamada, 2010) and may re-
duce users? willingness to use systems. Keeping
high willingness is quite important from the view-
point of both research (for collecting data from
users to improve systems) and business (for con-
tinuously selling systems with limited functional-
ity). (5) Risk: Although it is not verified, what is
called the uncanny valley (Bartneck et al, 2007)
probably exists. It is commonly observed that peo-
ple hate imperfect humanlike systems.
We try to avoid these problems rather than over-
come them. Our position is positively exploring
non-humanlike spoken dialogue design. This pa-
176
per focuses on its two elements, i.e., decelerated
dialogues as non-humanlike turn-taking and an ar-
tificial subtle expression (ASE) as non-humanlike
acknowledgment1, and presents two experimental
studies regarding these two elements. ASEs, de-
fined by the authors in (Komatsu et al, 2010), are
simple expressions suitable for artifacts, which in-
tuitively notify users about artifacts? internal states
while avoiding the above five problems.
In Section 2, the first study, which was pre-
viously reported in (Funakoshi et al, 2010), is
summarized and shows that the proposed non-
humanlike spoken dialogue design is effective for
reducing speech collisions. It also presents pieces
of evidence that shows quick humanlike turn-
taking is less important in designing spoken dia-
logue systems (SDSs). In Section 3, the second
study, which is newly reported in this paper, shows
a tendency supporting a hypothesis found in the
first study that user preference on response timing
varies depending on interaction patterns. Upon re-
ceiving the results of the two experiments, a design
guideline for SDSs is suggested in Section 4.
2 Study 1: Reducing Speech Collisions
with an Artificial Subtle Expression
in a Decelerated Dialogue
An important issue in SDSs is the management of
turn-taking. Failures of turn-taking due to sys-
tems? end-of-turn misdetection cause undesired
speech collisions, which harm smooth communi-
cation and degrade system usability.
There are two approaches to reducing speech
collisions due to end-of-turn misdetection. The
first approach is using machine learning tech-
niques to integrate information from multiple
sources for accurate end-of-turn detection in early
timing. The second approach is to make a long in-
terval after the user?s speech signal ends and be-
fore the system replies simply because a longer
interval means no continued speech comes. As
far as the authors know, all the past work takes
the first approach (e.g., (Kitaoka et al, 2005;
Raux and Eskenazi, 2009)) because the second ap-
proach deteriorates responsiveness of SDSs. This
choice is based on the presumption that users pre-
fer a responsive system to less responsive systems.
The presumption is true in most cases if the sys-
1In this paper, acknowledgment denotes that at the level 1
of the joint action ladder (Clark, 1996), which communicates
the listener?s identifying the signal presented by the speaker.
B l i n k i n g L E D
Figure 1: Interface robot with an embedded LED
tem?s performance is at human level. However, if
the system?s performance is below human level,
high responsiveness might not be vital or even be
harmful. For instance, Hirasawa et al (1999) re-
ported that immediate overlapping backchannels
can cause users to have negative impressions. Ki-
taoka et al (2005) also reported that the familiarity
of an SDS with backchannels was inferior to that
without backchannels due to a small portion of er-
rors even though the overall timing and frequency
of backchannels was fairly good (but did not come
up to human operators). Technologies are advanc-
ing but they are still below human level. We chal-
lenge the past work that took the first approach.
The second approach is simple and sta-
ble against user differences and environmental
changes. Moreover, it can afford to employ more
powerful but computationally expensive speech
processing or to build systems on small devices
with limited resources. A concern with this ap-
proach is debasement of user experience due to
poor responsiveness as stated above. Another is-
sue is speech collisions due to users? following-
up utterances such as repetitions. Slow responses
tend to induce such collision-eliciting speech.
This section shows the results of the experiment
in which participants engaged in hotel reservation
tasks with an SDS equipped with an ASE-based
acknowledging method, which intuitively notified
a user about the system?s internal state (process-
ing). The results suggest that the method can re-
duce speech collisions and provide users with pos-
itive impressions. The comparisons of evaluations
between systems with a slow reply speed and a
moderate reply speed suggest that users of SDSs
do not care about slow replies. These results in-
dicate that decelerating spoken dialogues is not a
bad idea.
2.1 Experiment
System An SDS that can handle a hotel reserva-
tion domain was built. The system was equipped
177
USER
SYSTEM
VAD tail margin wait interval
processing delay
blinking LED (artificial subtle expression)
short pauses
detected speech onset detected end-of-turn
X Y
system speech
user speech
time
Figure 2: Behavior of the dialogue system along a timeline
with an interface robot with an LED attached to
its chest (see Figure 1). Participants? utterances
were recognized by an automatic speech recog-
nizer Julius2, and interpreted by an in-house lan-
guage understander. The robot?s utterances were
voiced by a commercial speech synthesizer. The
LCD monitor in Figure 1 was used only to show
reservation details at last.
Julius output a recognition result to the system
at 400 msec after an input speech signal ended, but
the system awaited the next input for a fixed inter-
val (wait interval, whose length is given as an ex-
perimental factor). If the system received an addi-
tional input, it awaited the next input for the same
interval again. Otherwise, the system replied.
The LED started blinking at 1/30 sec even-
intervals when a speech signal was detected and
stopped when the system started replying. The
basic function of the blinking light expression is
similar to hourglass icons used in GUIs. A big
difference is that basically GUIs can ignore any in-
put while they are showing those icons, but SDSs
must accept successive speech while it is blink-
ing an LED. What we intend to do is to suppress
only collision-eliciting speech such as repetitions
(we call them follow-ups) which are negligible
but difficult to be automatically distinguished from
barge-ins. Barge-ins are not negligible.
Conditions and participants Two experimen-
tal factors were set-up, that is, the reply speed
factor (moderate or slow reply speed) and the
blinking light factor (with or without a blinking
light), resulting in four conditions:
A: slow reply speed, with a blinking light,
B: slow reply speed, without a blinking light,
C: moderate reply speed, with a blinking light,
D: moderate reply speed, without a blinking light.
We randomly assigned 48 Japanese participants
2http://julius.sourceforge.jp/
(mean age 30.9) to one of the four conditions.
A reply speed depends on a wait interval for
which the dialogue system awaits the next input.
Shiwa et al (2008) showed that the best reply
speed for a conversational robot was one second.
Thus we chose 800 msec as the wait interval for
the moderate reply speed because an actual reply
speed was the accumulation of the wait interval
and a delay for processing a user request, and 800
msec is simply twice the default length (the VAD
tail margin) by which the Julius speech recognizer
recognizes the end of a speech. For the slow reply
speed, we chose 4 sec as the wait interval. Wait
intervals include the VAD tail margin.
Figure 2 shows how the system and the LED
work along with user speech. In this figure, a user
utters a continuous speech with a rather long pause
that is longer than the VAD tail margin but shorter
than the wait interval. If the system detects the
end of the user?s turn and starts speaking within
the interval marked with an ?X?, a speech collision
would occur. If the user utters a follow-up within
the interval marked with a ?Y?, a speech collision
would occur, too. We try to suppress the former
speech collision by decelerating dialogues and the
latter by using a blinking light as an ASE.
Method The experiment was conducted in a
room for one participant at one time. Participants
entered the room and sat on a chair in front of a
desk as shown in Figure 1.
The experimenter gave the participants instruc-
tions so as to reserve hotel rooms five times by
talking with the robot in front of them. All of them
were given the same five tasks which require them
to reserve several rooms (one to three) at the same
time. The meaning of the blinking light expres-
sion was not explained to them. After giving the
instructions, the experimenter left the participants,
and they began tasks when the robot started to talk
to them. Each task was limited to up to three min-
utes. After finishing the tasks, the participants an-
178
swered a questionnaire. Figure 5 and Figure 6 in
the appendix show one of the five task instructions,
and a dialogue on that task, respectively.
2.2 Results
Reply speeds Averages of observed reply
speeds were calculated from the timestamps in
transcripts. They were 4.53 sec for the slow con-
ditions and 1.42 sec for the moderate conditions.
Task completion The average number of com-
pleted tasks in the four conditions A, B, C, and D
were 4.00, 3.83, 3.83, and 4.33, respectively. An
ANOVA did not find any significant difference.
Speech collisions We counted speech collisions
for which the SDS was responsible, that is, the
cases where the robot spoke while participants
were talking (i.e., end-of-turn misdetections). Of
course, there were speech collisions for which par-
ticipants were responsible, that is, the cases where
participants intentionally spoke while the robot
was talking (i.e., barge-ins). These speech colli-
sions were not the targets, hence they were not in-
cluded in the counts.
Speech collisions due to participants? back-
channel feedbacks were not included, either. We
think that it is possible to filter out such feedback
because feedback utterances are usually very short
and variations are small. On the other hand, as
we mentioned above, it is not easy to automat-
ically distinguish negligible speech such as rep-
etitions from barge-ins. We want to suppress
only such speech negligible but hard to distinguish
from other not negligible speech.
The number of observed speech collisions in
the four conditions A, B, C, and D were 5, 11,
45, and 30, respectively. First we performed an
ANOVA on the number of collisions. The interac-
tion effect was not significant (p = 0.24). A sig-
nificant difference on the reply speed factor was
found (p < 0.005). This result confirms that de-
celerating dialogues reduces collisions. The ef-
fect of the blinking light factor was not significant
(p = 0.60).
Next we performed a Fisher?s exact test (one-
side) on the number of participants who had
speech collisions between the two conditions of
the slow reply speed (3 out of 12 for A and 8 out
of 12 for B). The test found a significant difference
(p < 0.05). This result indicates that the blinking
light can reduce speech collisions by suppressing
users? follow-ups in decelerated dialogues.
Impression on the dialogue and robot The par-
ticipants rated 38 positive-negative adjective pairs
(such as smooth vs. rough) for evaluating both the
dialogue and the robot. The ratings are based on a
seven-point Likert scale.
An ANOVA found a positive marginal signifi-
cance (p = 0.07) for the blinking light in the com-
fortableness factor extracted by a factor analysis
for the impression on the dialogue. In addition,
an ANOVA found a positive marginal significance
(p = 0.07) for the slow reply speed in the mod-
esty factor extracted by a factor analysis for the
impression on the robot. Surprisingly, no signifi-
cant negative effect for the slow reply speed was
found.
System evaluations The participants evaluated
the SDS in two measures on a scale from 1 to 7,
that is, the convenience of the system and their
willingness to use the system. The greater the
evaluation value is, the higher the degree of con-
venience or willingness.
The average scores of convenience in the four
conditions A, B, C, and D were 3.50, 3.17, 3.17,
and 3.92, respectively. Those of willingness were
3.58, 2.58, 2.83, and 3.42, respectively. ANOVAs
did not find any significant difference among the
four conditions both for the two measures.
Discussion on user preference The analysis of
the questionnaire suggests that the blinking light
expression gives users a comfortable impression
on the dialogue. The analysis also suggests that
the slow reply speed gives users a modest impres-
sion on the interface robot. Meanwhile, no neg-
ative impression with a statistical significance is
found on the slow reply speed.
Although no statistically significant difference
is found between the four conditions, numbers
of completed tasks and convenience are strongly
correlated. However, users? willingness to use
the systems, which is the most important mea-
sure for systems, is inverted between condition
A and D. Convenience will be primarily domi-
nated by what degree a user?s purpose (reserving
rooms) is achieved, thus, it is reasonable that con-
venience scores correlate with the number of com-
pleted tasks. On the other hand, willingness will
be dominated by not only practical usefulness but
also overall usability or experience. Therefore,
we can interpret that the improvements in impres-
sions and reduction in aversive speech collisions
179
let condition A have the highest score for willing-
ness. These results indicate that decelerating spo-
ken dialogues is not a bad idea in contradiction
to the common design policy in human-computer
interfaces (HCIs), and they suggest to exploit mer-
its provided by decelerating dialogues rather than
pursuing quickly responding humanlike systems.
Our finding contradicts not only the com-
mon design policy in HCIs but also the de-
sign policy in human-robot interaction found by
Shiwa et al (2008), that is, the best response tim-
ing of a communication robot is at one second. We
think this contradiction is superficial and is ascrib-
able to the following four major differences be-
tween their study and our study.
? They adopted a within-subjects experimental
design while we adopted a between-subjects
design. A within-subjects design makes sub-
jects do relative evaluations and tends to em-
phasis differences.
? Their question was specific in terms of re-
sponse timing. Our questions were overall
ratings of the system such as convenience.
? They assumed a perfect machine (Wizard-of-
Oz experiment). Our system was elaborately
crafted but still far from perfect.
? Our system quickly returns non-verbal re-
sponses even if verbal responses are delayed.
From these differences, we hypothesize that re-
sponse timing has no significant impact on the us-
ability of SDSs in an absolute and holistic context
at least in the current state of the art spoken dia-
logue technology, even though users prefer a sys-
tem which responds quickly to a system which re-
sponds slowly when they compare them with each
other directly, given an explicit comparison metric
on response timing with perfect machines.
3 Study 2: Uncovering Comfortableness
of Response Timing under Different
Interaction Patterns
Our conclusion in Section 2 is that SDSs do not
need to quickly respond verbally as long as they
quickly respond non-verbally by showing their in-
ternal states with an ASE, while many researchers
try to make them verbally respond as fast as pos-
sible. Decelerating a dialogue has many practical
advantages as stated above.
However, through the experiment, we have also
suspected that this conclusion is not valid in some
specific cases. That is, we think in some situa-
tions users feel uncomfortable with slow verbal re-
sponses primordially, and those situations are such
as when users simply reply to systems? yes-no-
questions or greetings. Our hypothesis is that users
expect quick verbal responses (and hate slow ver-
bal responses) only when users expect that it is not
difficult for systems to understand their responses
or to decide next actions. This section reports the
experiment validating this hypothesis.
3.1 Experiment
To validate the hypothesis described above, we
conducted a Wizard-of-Oz experiment using fixed
scenarios. Participants engaged in short interac-
tions with an interface robot and evaluated re-
sponse timing of the robot. Three experimental
factors were interaction patterns, response timing
(wait interval), and existence of a blinking light.
Interaction patterns Five interaction patterns
were setup to see the differences between situa-
tions. Each pattern consisted of three utterances.
The first utterance was from the system. Upon re-
ceiving the utterance, a participant as a user of the
system replied with the second utterance. Then
the system responded after the given wait interval
(1 sec or 4 sec) with the third utterance. Partic-
ipants evaluated this interval between the second
utterance and the third utterance in a measure of
comfortableness.
The patterns with scenarios are shown in Fig-
ure 3. They will be referred to by abbreviations
(PGG, QYQ, QNQ, PSQ, PLQ) in what follows.
Note that the scenarios are originally in Japanese.
Here, RequestS and RequestL mean a short re-
quest and a long request, respectively. YNQues-
tion and WhQuestion mean a yes-no-question and
a wh-question, respectively. According to the hy-
pothesis, we can predict that the reported com-
fortableness for the longer wait interval (4 sec)
are worse for short and formulaic cases such as
PGG and QYQ than for the long request case (i.e.,
PLQ). In addition, we can predict that the reported
comfortableness for longer intervals improves for
PLQ if the robot?s light blinks, while that does not
improve for PGG and QYQ.
System We used the same interface robot and
the LCD monitor as study 1. The experiment in
this study, however, was conducted using a WOZ
system.
180
Prompt-Greeting-Greeting (PGG)
S: Welcome to our Hotel. May I help you?
U: Hello.
S: Hello.
YNQuestion-Yes-WhQuestion (QYQ)
S: Welcome to our Hotel. Will you stay tonight?
U: Yes.
S: Can I ask your name?
YNQuestion-No-WhQuestion (QNQ)
S: Welcome to our Hotel. Will you stay tonight?
U: No.
S: How may I help you?
Prompt-RequestS-WhQuestion (PSQ)
S: Welcome to our Hotel. May I help you?
U: I would like to reserve a room from tomorrow.
S: How long will you stay?
Prompt-RequestL-WhQuestion (PLQ)
S: Welcome to our Hotel. May I help you?
U: I would like to reserve rooms with breakfast from to-
morrow, one single room and one double room, non-
smoking and smoking, respectively.
S: How long will you stay?
Figure 3: Interaction patterns and scenarios
First the WOZ system presents an instruction to
the participant on the LCD monitor, which reveals
the robot?s first utterance of the given scenario
(e.g., ?Welcome to our Hotel. May I help you??)
and indicates the participant?s second utterance
(e.g., ?Hello.?). Two seconds after the participant
clicks the OK button on the monitor with a com-
puter mouse, the system makes the robot utter the
first utterance. Then, the participant replies, and
the operator of the system end-points the end of
participant?s speech by clicking a button shown in
another monitor for the operator in the room next
to the participant?s room. After the end-pointing,
the system waits for the wait interval (one second
or four seconds) and makes the robot utter the third
utterance of the scenario. One second after, the
system asks the participant to evaluate the com-
fortableness of the response timing of the robot?s
third utterance on a scale from 1 to 7 (1:very un-
comfortable, 4:neutral, 7:very comfortable) on the
LCD monitor.
Conditions and participants Forty participants
(mean age 28.8, 20 males and 20 females) engaged
in the experiment. No participant had engaged in
study 1. They were randomly assigned to one of
two groups (gender was balanced). The groups
correspond to one of two levels of the experi-
mental factor of the existence of a blinking light.
For one group, the robot blinked its LED when it
was waiting. For the other group, the robot did
not blink the LED. We refer to the former group
(condition) as BL (Blinking Light, n=20) and the
later as NL (No Light, n=20). In summary, this
experiment is within-subjects design with regard
to interaction patterns and response timing and is
between-subjects design with regard to the blink-
ing light.
Method The experiment was conducted in a
room for one participant at one time. Participants
entered the room and sat on a chair in front of a
desk as shown in Figure 1, but they did not wear
headphones this time.
The experimenter gave the participants instruc-
tions so as to engage in short dialogues with the
robot in front of them. They engaged in each of
five scenarios shown in Figure 3 six times (three
times with a 1 sec wait interval and three with
4 sec), resulting in 30 dialogues (5? 3? 2 = 30).
The order of scenarios and intervals was random-
ized. The existence and meaning of the blinking
light expression was not explained to them. They
were not told that the systemwas operated by a hu-
man operator, either. After giving the instructions,
the experimenter left the participants, and they
practiced one time. This practice used a Prompt-
RequestM-WhQuestion3 type scenario with a wait
interval of two seconds. Then, thirty dialogues
were performed. Short breaks were inserted af-
ter ten dialogues. Each dialogue proceeded as ex-
plained above.
3.2 Results
End-pointing errors End-pointing was done by
a fixed operator. We obtained 1,184 dialogues out
of 1,200 (= 30 ? 40) after removing dialogues
in which end-pointing failed (failures were self-
reported by the operator). We sampled 30 dia-
logues from the 1,184 dialogues and analyzed end-
pointing errors in the recorded speech data. The
average error was 84.6 msec (SD=89.6).
Comfortableness This experiment was de-
signed to grasp a preliminary sense on our
hypothesis as much as possible with a limited
number of participants in exchange for aban-
donment of use of statistical tests, because this
study involved multiple factors and the interaction
pattern factor was complex by itself. Therefore,
in the following discussion on comfortableness,
we do not refer to statistical significances.
3The request utterance is longer than that of RequestS and
shorter than that of RequestL.
181
!"#$
"%&'(
)*+,,
,+! ,+!
!"#$
"%&'(
)*+,,
,+! ,+!
Figure 4: Comfortableness (Left: without a blinking light (NL), right: with a blinking light (BL))
Figure 4 shows regression lines obtained from
the 1,184 dialogues in the two graphs for NL and
BL (Detailed values are shown in Table 1). The
X axes in the graphs correspond to response tim-
ing, that is, the two wait intervals of 1 sec and
4 sec. The Y axes correspond to comfortableness
reported in a scale from 1 to 7. Obviously, with or
without a blinking light effected comfortableness.
The results shown in the graphs support the pre-
dictions made in Section 3.1. The scores of PGG
and QYQ are worse than that of PLQ at 4 sec.
PGG and QYQ show no difference between NL
and BL. QNQ and PSQ show differences. PLQ
shows the biggest difference. In case of PLQ, the
reported comfortableness at 4 sec shifted to al-
most the neutral position (score 4) by presenting a
blinking light. This indicates that a blinking light
ASE can allay the debasement of impression due
to slow responses only in non-formulaic cases.
Interestingly, the blinking light expression at-
tracted comfortableness scores to neutral both at
1 sec and at 4 sec. We can make two hypotheses
on this result. One is that the blinking light expres-
sion has a negative effect which degrades comfort-
ableness at 1 sec. The other is that the blinking
light expression makes participants difficult to see
differences between 1 sec and 4 sec, therefore, re-
ported scores converge to neutral. At this stage we
think that the later is more probable than the for-
mer because the scores of PGG and QYQ should
be degraded at 1 sec if the former is true.
4 A Practical Design Guideline for SDSs
Summarizing the results of the experiments pre-
sented in Section 2 and Section 3, we suggest a
twofold design guideline for SDSs, especially for
task-oriented systems. Some interaction-oriented
systems such as chatting systems are out of scope
of this guideline. In what follows, first the guide-
line is presented and then a commentary on the
guideline is described.
The guideline
(1) Never be obsessed with quick turn-taking
but acknowledge users immediately
Quick turn-taking will not recompense your ef-
forts, resources inputted, etc. Pursue it only af-
ter accomplishing all you can do without compro-
mising performance in other elements of dialogue
systems and only if it does not make system devel-
opment and maintenance harder. However, quick
(possibly non-verbal) acknowledgment is a requi-
site. You can compensate for the debasement of
user experience due to slow verbal responses just
by using an ASE such as a tiny blinking LED to
acknowledge user speech. No instruction about
the ASE is needed for users.
(2) Think of users? expectations
Users expect rather quick verbal responses to their
greetings and yes-answers. ASEs will be ineffec-
tive for them. Thus it is recommended to enable
your systems to quickly respond verbally to such
utterances. Fortunately it is easy to anticipate such
utterances. Greetings usually occur only at the be-
ginning of dialogues or after tasks were accom-
plished. Yes-answers will come only after yes-no-
questions. Therefore it will be able to implement
an SDS that quickly responds verbally to greeting
and yes-answers both without increasing develop-
ment / maintenance costs and without decreasing
182
recognition performance, etc.
However, you should keep in mind that too
quick verbal responses (0 sec interval or overlap-
ping) may not be welcomed (Hirasawa et al, 1999;
Shiwa et al, 2008). They may also induce too
much expectancy in users and result in disappoint-
ments to your systems after some interactions.
Commentary on the guideline
The guideline was constructed so as to avoid the
five problems pointed out in Section 1. The first
point of the guideline is induced mainly from the
results of study 1, and the second point is induced
mainly from the results of study 2.
Although the results of study 2 indicate users
prefer quick responses to slow ones as presup-
posed in past literature, note that the experiment
in study 2 is within-subjects design with regard to
the response timing factor and that within-subjects
design tends to emphasis differences as discussed
at the end of Section 2. The results of study 1
suggested that such an emphasized difference (i.e.,
preference for quick responses) has no significant
impact on the usability of SDSs on the whole.
5 Conclusion
This paper proposed a non-humanlike spoken di-
alogue design, which consists of two elements:
non-humanlike turn-taking and acknowledgment.
Two experimental studies were reported regarding
these two elements. The first study showed that the
proposed non-humanlike spoken dialogue design
is effective for reducing speech collisions. This
study also presented pieces of evidence that show
quick humanlike turn-taking is less important in
spoken dialogue system (SDS) design. The second
study showed a tendency supporting a hypothesis
found in the first study that user preference on re-
sponse timing varies depending on interaction pat-
terns in terms of comfortableness. Upon receiving
these results, a practical design guideline for SDSs
was suggested, that is, (1) never be obsessed with
quick turn-taking but acknowledge users immedi-
ately and (2) think of users? expectations.
Our non-humanlike acknowledging method us-
ing an LED-based artificial subtle expression
(ASE) can apply to any interfaces on wearable /
handheld devices, vehicles, whatever. It is, how-
ever, difficult to directly apply it to call-centers
(i.e., telephone interfaces), which occupy a big
portion of the deployed SDSs pie. Yet, the un-
derlying concept: decelerated dialogues accom-
panied by an ASE will be applicable even to tele-
phone interfaces by using an auditory ASE, which
is to be explored in future work.
The guideline is supported by findings in a
rather hypothetical stage. More experiments are
necessary to confirm these findings. In addition,
the guideline is for the current transitory period
in which intelligence technologies such as auto-
matic recognition, language processing, reasoning
etc. are below human level. In that sense, the con-
tribution of this paper might be limited. However,
this period will last until a decisive paradigm shift
occurs in intelligence technologies. It may come
after a year, a decade, or a century.
References
C. Bartneck, T. Kanda, H. Ishiguro, and N. Hagita.
2007. Is the uncanny valley an uncanny cliff? In
Proc. RO-MAN 2007.
H. Clark. 1996. Using Language. Cambridge U. P.
R. Ferna?ndez, D. Schlangen, and T. Lucht. 2007.
Push-to-talk ain?t always bad! comparing different
interactivity settings in task-oriented dialogue. In
Proc. DECALOG 2007.
K. Funakoshi, K. Kobayashi, M. Nakano, T. Komatsu,
and S. Yamada. 2010. Reducing speech collisions
by using an artificial subtle expression in a deceler-
ated spoken dialogue. In Proc. 2nd Intl. Symp. New
Frontiers in Human-Robot Interaction.
J. Hirasawa, M. Nakano, T. Kawabata, and K. Aikawa.
1999. Effects of system barge-in responses on user
impressions. In Proc. EUROSPEECH?99.
N. Kitaoka, M. Takeuchi, R. Nishimura, and S. Nak-
agawa. 2005. Response timing detection us-
ing prosodic and linguistic information for human-
friendly spoken dialog systems. Journal of The
Japanese Society for AI, 20(3).
T. Komatsu and S. Yamada. 2010. Effects of adapta-
tion gap on user?s variation of impressions of artifi-
cial agents. In Proc. WMSCI 2010.
T. Komatsu, S. Yamada, K. Kobayashi, K. Funakoshi,
and M. Nakano. 2010. Artificial subtle expressions:
Intuitive notification methodology of artifacts. In
Proc. CHI 2010.
A. Raux and M. Eskenazi. 2009. A finite-state turn-
taking model for spoken dialog systems. In Proc.
NAACL-HLT 2009.
T. Shiwa, T. Kanda, M. Imai, H. Ishiguro, and
N. Hagita. 2008. How quickly should communi-
cation robots respond? In Proc. HRI 2008.
183
Hotel Reservation Task 3
Reserve rooms as below
Stay
Room
Twin, 1 room, non-smoking
Double, 1 room, non-smoking
As specified with the orange-colored frameon the calendar 
Figure 5: One of the five task instructions used in study 1
S: Welcome to Hotel Wakamatsu-Kawada. May I help you?
U: I want to stay from March 10th to 11th.
S: What kind of room would you like?
U: One non-smoking twin room and one non-smoking double room.
S: Are your reservation details correctly shown on the screen?
U: Yes. No problem.
S: Your reservation has been accepted. Thank you for using us.
Figure 6: A successful dialogue observed with the task shown in Figure 5 (translated into English)
Table 1: Detailed comfortableness scores in study 2
Interaction pattern PGG QYQ QNQ PSQ PLQ
Condition NL BL NL BL NL BL NL BL NL BL
1 sec
mean 5.34 5.36 5.55 5.56 5.48 5.25 5.09 4.73 5.13 4.41
s.d. 1.00 1.17 1.10 1.00 1.02 1.04 1.12 1.09 1.14 1.20
p-value 0.93 0.96 0.23 0.09 0.001
4 sec
mean 3.12 3.16 3.37 3.36 3.28 3.52 3.43 3.52 3.54 3.83
s.d. 0.94 1.04 0.78 0.93 0.76 0.93 0.81 0.87 0.95 0.87
p-value 0.83 0.98 0.14 0.59 0.08
p-values were obtained by two-sided t-tests between NL and BL. Those are shown just for reference.
184
