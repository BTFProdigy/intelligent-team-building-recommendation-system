Unsupervised Discovery of Scenario-Level Patterns for 
Information Extraction 
Roman Yangarber  Ra lph  Gr i shman 
roman?as, nyu. edu grishman?cs, nyu. edu 
Courant Inst i tute of Courant Inst i tute of 
Mathematical  Sciences Mathematical  Sciences 
New York University New York University 
Pas i  Tapana inen  )~ Si l ja  Hut tunen $ 
tapanain?conexor, fi sihuttun~ling.helsinki, fi 
t Conexor Oy :~ University of Helsinki 
Helsinki, F inland F in land 
Abst rac t  
Information Extraction (IE) systems are com- 
monly based on pattern matching. Adapting 
an IE system to a new scenario entails the 
construction of a new pattern base---a time- 
consuming and expensive process. We have 
implemented a system for finding patterns au- 
tomatically from un-annotated text. Starting 
with a small initial set of seed patterns proposed 
by the user, the system applies an incremental 
discovery procedure to identify new patterns. 
We present experiments with evaluations which 
show that the resulting patterns exhibit high 
precision and recall. 
0 I n t roduct ion  
The task of Information Extraction (I-E) is 
the selective extraction of meaning from free 
natural language text. I "Meaning" is under- 
stood here in terms of a fixed set of semantic 
objects--entities, relationships among entities, 
and events in which entities participate. The 
semantic objects belong to a small number of 
types, all having fixed regular structure, within 
a fixed and closely circumscribed subject do- 
main. The extracted objects are then stored in 
a relational database. In this paper, we use the 
nomenclature accepted in current IE literature; 
the term subject domain denotes a class of tex- 
tual documents to be processed, e.g., "business 
news," and scenario denotes the specific topic 
of interest within the domain, i.e., the set of 
facts to be extracted. One example of a sce- 
nario is "management succession," the topic of 
MUC-6 (the Sixth Message Understanding Con- 
ference); in this scenario the system seeks to 
identify events in which corporate managers left 
1For general references on IE, cf., e.g., (Pazienza, 
1997; muc, 1995; muc, 1993). 
their posts or assumed new ones. We will con- 
sider this scenario in detail in a later section 
describing experiments. 
IE systems today are commonly based on pat- 
tern matching. The patterns are regular ex- 
pressions, stored in a "pattern base" containing 
a general-purpose component and a substantial 
domain- and scenario-specific component. 
Portability and performance are two major 
problem areas which are recognized as imped- 
ing widespread use of IE. This paper presents a
novel approach, which addresses both of these 
problems by automatically discovering good 
patterns for a new scenario. The viability of 
our approach is tested and evaluated with an 
actual IE system. 
In the next section we describe the problem in 
more detail in the context of our IE system; sec- 
tions 2 and 3 describe our algorithm for pattern 
discovery; section 4 describes our experimental 
results, followed by comparison with prior work 
and discussion, in section 5. 
1 The  IE  Sys tem 
Our IE system, among others, contains a a back- 
end core engine, at the heart of which is a 
regular-e~xpression pattern matcher. The engine 
draws on attendant knowledge bases (KBs) of 
varying degrees of domain-specificity. The KB 
components are commonly factored out to make 
the systems portable to new scenarios. There 
are four customizable knowledge bases in our IE 
system: the Lexicon contains general dictionar- 
ies and scenario-specific terms; the concept base 
groups terms into classes; the predicate base de- 
scribes the logical structure of events to be ex- 
tracted, and the pattern base contains patterns 
that catch the events in text. 
Each KB has a. substantial domain-specific 
component, which must be modified when mov-  
282 
ing to new domains and scenarios. The system 
allows the user (i.e. scenario developer) to start 
with example sentences in text which contain 
events of interest, the candidates, and general- 
ize them into patterns. However, the user is 
ultimately responsible for finding all the can- 
didates, which amounts to manually processing 
example sentences in a very large training cor- 
pus. Should s/he fail to provide an example 
of a particular class of syntactic/semantic con- 
struction, the system has no hope of recovering 
the corresponding events. Our experience has 
shown that (1) the process of discovering candi- 
dates is highly expensive, and (2) gaps in pat- 
terns directly translate into gaps in coverage. 
How can the system help automate the pro- 
cess of discovering new good candidates? The 
system should find examples of all common lin- 
guistic constructs relevant o a scenario. While 
there has been prior research on identifying the 
primary lexical patterns of a sub-language or 
corpus (Grishman et al, 1986; Riloff, 1996), the 
task here is more complex, since we are typi- 
cally not provided in advance with a sub-corpus 
of relevant passages; these passages must them- 
selves be found as part of the discovery process. 
The difficulty is that one of the best indications 
of the relevance of the passages i precisely the 
presence of these constructs. Because of this 
circularity, we propose to acquire the constructs 
and passages in tandem. 
2 So lu t ion  
We outline our procedure for automatic ac- 
quisition of patterns; details are elaborated in 
later sections. The procedure is unsupervised 
in that it does not require the training corpus 
to be manually annotated with events of inter- 
est, nor a pro-classified corpus with relevance 
judgements, nor any feedback or intervention 
from the user 2. The idea is to combine IR-style 
document selection with an iterative relaxation 
process; this is similar to techniques used else- 
where in NLP, and is inspired in large part, if 
remotely, by the work of (Kay and RSscheisen, 
1993) on automatic alignment of sentences and 
words in a bilingual corpus. There, the reason- 
ing was: sentences that are translations of each 
2however, it may be supervised after each iteration, 
where the user can answer yes/no questions to improve 
the quality of the results 
other are good indicators that words they con- 
tain are translation pairs; conversely, words that 
are translation pairs indicate that the sentences 
which contain them correspond to one another. 
In our context, we observe that documents 
that are relevant to the scenario will neces- 
sarily contain good patterns; conversely, good 
patterns are strong indicators of relevant docu- 
ments. The outline of our approach is as follows. 
. 
. 
Given: (1) a large corpus of un-annotated 
and un-classified documents in the domain; 
(2) an initial set of trusted scenario pat- 
terns, as chosen ad hoc by the user--the 
seed; as will be seen, the seed can be quite 
small--two or three patterns eem to suf- 
fice. (3) an initial (possibly empty) set of 
concept classes 
The pattern set induces a binary partition 
(a split) on the corpus: on any document, 
either zero or more than zero patterns will 
match. Thus the universe of documents, U, 
is partitioned into the relevant sub-corpus, 
R, vs. the non-relevant sub-corpus, R = 
U - R, with respect o the given pattern 
set. Actually, the documents are assigned 
weights which are 1 for documents matched 
by the trusted seed, and 0 otherwise. 3 
2. Search for new candidate patterns: 
(a) Automatically convert each sentence 
in the corpus,into a set of candidate 
patterns, 4 
(b) Generalize each pattern by replacing 
each lexical item which is a member of 
a concept class by the class name. 
(c) Working from the relevant documents, 
select those patterns whose distribu- 
tion is strongly correlated with other 
relevant documents (i.e., much more 
3R represents he trusted truth through the discovery 
iterations, since it was induced by the manually-selected 
seed. 
4Here, for each clause in the sentence we extract a 
tuple of its major roles: the head of the subject, the 
verb group, the object, object complement, asdescribed 
below. This tuple is considered to be a pattern for the 
present purposes of discovery; it is a skeleton for the 
rich, syntactically transformed patterns our system uses 
in the extraction phase. 
283 
densely distributed among the rele- 
vant documents than among the non- 
relevant ones). The idea is to consider 
those candidate patterns, p, which 
meet the density, criterion: 
IHnRI IRI - - > >  
IHnUI IUI 
where H = H(p) is the set of docu- 
ments where p hits. 
(d) Based on co-occurrence with the cho- 
sen patterns, extend the concept 
classes. 
3. Optional: Present he new candidates and 
classes to the user for review, retaining 
those relevant o the scenario. 
4. The new pattern set induces a new parti- 
tion on the corpus. With this pattern set, 
return to step 1. Repeat he procedure un- 
til no more patterns can be added. 
3 Methodo logy  
3.1 Pre-proeess ing:  Normal i za t ion  
Before applying the discovery procedure, we 
subject the corpus to several stages o f  pre- 
processing. First, we apply a name recognition 
module, and replace each name with a token 
describing its class, e.g. C-Person, C-Company, 
etc. We collapse together all numeric expres- 
sions, currency values, dates, etc., using a single 
token to designate ach of these classes. 
3.2 Syntact ic  Analys is  
We then apply a parser to perform syntactic 
normalization to transform each clause into a 
common predicate-argument structure. We use 
the general-purpose d pendency parser of En- 
glish, based on the FDG formalism (Tapanainen 
and J~rvinen, 1997) and developed by the Re- 
search Unit for Multilingual Language Technol- 
ogy at the University of Helsinki, and Conexor 
Oy. The parser (modified to understand the 
name labels attached in the previous step) is 
used for reducing such variants as passive and 
relative clauses to a tuple, consisting of several 
elements. 
1. For each claus, the first element is the sub- 
ject, a "semantic" subject of a non-finite 
sentence or agent of the passive. 5 
2. The second element is the verb. 
3. The third element is the object, certain 
object-like adverbs, subject of the passive 
or subject complement 6 
4. The fourth element is a phrase which 
refers to the object or the subject. A 
typical example of such an argument is 
an object complement, such as Com- 
pany named John Smith pres ident .  An- 
other instance is the so-called copredica- 
tire (Nichols, 1978), in the parsing system 
(J~irvinen and Tapanainen, 1997). A co- 
predicative refers to a subject or an object, 
though this distinction is typically difficult 
to resolve automatically/ 
Clausal tuples also contain a locative modifier, 
and a temporal modifier. We used a corpus of 
5,963 articles from the Wall Street Journal, ran- 
domly chosen. The parsed articles yielded a to- 
tal of 250,000 clausal tuples, of which 135,000 
were distinct. 
3.3 Genera l i za t ion  and  Concept  Classes 
Because tuples may not repeat with sufficient 
frequency to obtain reliable statistics, each tu- 
ple is reduced to a set of pairs: e.g., a verb- 
object pair, a subject-object pair, etc. Each 
pair is used as a generalized pattern during 
the candidate selection stage. Once we have 
identified pairs which are relevant o the sce- 
nario, we use them to construct or augment con- 
cept classes, by grouping together the missing 
roles, (for example, a class of verbs which oc- 
cur with a relevant subject-object pair: "com- 
pany (hire/fire/expel...} person"). This is sim- 
ilar to work by several other groups which 
aims to induce semantic lasses through syn- 
tactic co-occurrence analysis (Riloff and Jones, 
1999; Pereira et al, 1993; Dagan et al, 1993; 
Hirschman et al, 1975), although in .our case 
the contexts are limited to selected patterns, 
relevant o the scenario. 
SE.g., " John sleeps", "John is appointed by 
Company" ,  "I saw a dog which sleeps", "She asked 
John  to buy a car". 
6E.g., " John is appointed by Company", "John is the 
pres ident  of Company", "I saw a dog which sleeps", 
The dog  which I saw sleeps. 
7For example, "She gave us our coffee black",  "Com- 
pany appointed John Smith as pres ident" .  
284 
3.4 Pattern Discovery 
Here we present he results from experiments 
we conducted on the MUC-6 scenario, "man- 
agement succession". The discovery procedure 
was seeded with a small pattern set, namely: 
Subject Verb Direct Object 
C-Company C-Appoint C-Person 
C-Person C-Resign 
Documents are assigned relevance scores on 
a scale between 0 and 1. The seed patterns 
are accepted as ground truth; thus the docu- 
ments they match have relevance 1. On sub- 
sequent iterations, the newly accepted patterns 
are not trusted as absolutely. On iteration um- 
ber i q- 1, each pattern p is assigned a precision 
measure, based on the relevance of the docu- 
ments it matches: 
Here C-Company and C-Person denote se- 
mantic classes containing named entities of the 
corresponding semantic types. C-Appoirlt de- 
notes a class of verbs, containing four verbs 
{ appoint, elect, promote, name}; C-Resign = 
{ resign, depart, quit, step-down }. 
During a single iteration, we compute the 
score s, L(p), for each candidate pattern p: 
L(p) = Pc(P)" log {H A R\] (1) 
where R denotes the relevant subset, and H -- 
H(p) the documents matching p, as above, and 
\[gnR\[ Pc(P) -- Igl is the conditional probability of 
relevance. We further impose two support cri- 
teria: we distrust such frequent patterns where 
\[HA U{ > a\[U\[ as uninformative, and rare pat- 
terns for which \[H A R\[ </3  as noise. ? At the 
end of each iteration, the system selects the pat- 
tern with the highest score, L(p), and adds it to 
the seed set. The documents which the winning 
pattern hits are added to the relevant set. The 
pattern search is then restarted. 
3.5 Re-computat lon  of  Document  
Relevance 
The above is a simplification of the actual pro- 
cedure, in several important respects. 
Only generalized patterns are considered for 
candidacy, with one or more slots filled with 
wild-cards. In computing the score of the gen- 
eralized pattern, we do not take into considera- 
tion all possible values of the wild-card role. We 
instead constrain the wild-card to those values 
which themselves in turn produce patterns with 
high scores. These values then become members 
of a new class, which is output in tandem with 
the winning pattern 1? 
Ssimilarly to (Riloff, 1996) 
?U denotes the universe of documents. We used c~ = 
0.i and ~----- 2. 
1?The classes are currently unused by subsequent i er- 
ations; this important issue is considered in future work. 
Preci+l(p) = 1 {H(p){ ~ Reli(d) (2) 
dEH(p) 
where Reli(d) is the relevance of the document 
from the previous iteration, and H(p) is the set 
of documents where p matched. More generally, 
if K is a classifier consisting of a set of patterns, 
we can define H(K) as the set of documents 
where all of patterns p E K match, and the 
"cumulative" precision 11 of K as 
Preci+l(K) = 1 ~ Reli(d) (3) 
IH(K)\[ riCH(K) 
Once the new winning pattern is accepted, 
the relevance scores of the documents are re- 
adjusted as follows. For each document d which 
is matched by some (non-empty) subset of the 
currently accepted patterns, we can view that 
subset of patterns as a classifier K d = {py}. 
These patterns determine the new relevance 
score of the document 
Reli+l(d) = max (Rel~(d),Prec~+l(Kd)) (4) 
This ensures that the relevance score grows 
monotonically, and only when there is sufficient 
positive evidence, as the patterns in effect vote 
"conjunctively" on the documents. The results 
which follow use this measure. 
Thus in the formulas above, R is not sim- 
ply the count of the relevant documents, but 
is rather their cumulative relevance. The two 
formulas, (3) and (4), capture the mutual de- 
pendency of patterns and documents; this re- 
computation and growing of precision and rele- 
vance scores is at the heart of the procedure. 
11Of course, this measure is defined only when 
H(K) # 0. 
285 
4 Resu l ts  1 
An objective measure of goodness of a pattern o. 9 
is not trivial to establish since the patterns can- 
not be used for extraction directly, without be- o. s 
ing properly incorporated into the knowledge 
base. Thus, the discovery procedure does not o. v 
lend itself easily to MUC-style evaluations, ince 
0.6  a pattern lacks information about which events 
it induces and which slots its arguments should 0.5  
fill. 
However, it is possible to apply some objec- o. a 
tive measures of performance. One way we eval- 
uated the system is by noting that in addition o.  
to growing the pattern set, the procedure also 
grows the relevance of documents. The latter o. 2 
can be objectively evaluated. 
0.1  
We used a test corpus of 100 MUC-6 formal- 
training documents (which were included in the o 
main development corpus of about 6000 docu- 
ments) plus another 150 documents picked at 
random from the main corpus and judged by 
hand. These judgements constituted the ground 
truth and were used only for evaluation, (not in 
the discovery procedure). 
4.1 Text  F i l ter ing 
Figure 1 shows the recall/precision measures 
with respect to the test corpus of 250 docu- 
ments, over a span of 60 generations, tarting 
with the seed set in table 3.4. The Seed pat- 
terns matched 184 of the 5963 documents, yield- 
ing an initial recall of .11 and precision of .93; 
by the last generation it searched through 982 
documents with non-zero relevance, and ended 
with .80 precision and .78 recall. This facet of 
the discovery procedure is closely related to the 
MUC '%ext-filtering" sub-task, where the sys- 
tems are judged at the level of documents rather 
than event slots. It is interesting to compare the 
results with other MUC-6 participants, shown 
anonymously in figure 2. Considering recall and 
precision separately, the discovery procedure at- 
tains values comparable to those achieved by 
some of the participants, all of which were ei- 
ther heavily-supervised or manually coded sys- 
tems. It is important o bear in mind that the 
discovery procedure had no benefit of training 
material, or any information beyond the seed 
pattern set. 
I I I I I I 
......i"{+X'N+~v i P ~ e c i s i o n  ' i 
" "  ... .  i . .~ .  i Re~aa - - -?- - -  
. . . . . . . . . .  ~ . . . . . . . . . .  ~ . . . . . . . . .  ~ . . . . . . . . . . .  ~ . . . . . . . . . .  ~ . . . . . . . . . .  ~ . . . . . . . . .  
...... iiiiiiiiiiiiiilEi   ........... ........ 
/... 
. . . . . . . . . .  ~ . . . . . . . . . .  '." . . . . . . . .  ": . . . . . . . . . .  ~ . . . . . . . . . . .  r . . . . . . . . . .  ~ .. . . . . . . . .  ! . . . . . . . . .  
2111111ji.. iii121122;1211111;ii122221ilSiiii12112121SiiiiSiii: . . . . . .
0 I0  20  30  40  50  60  70  
G e n e r a t i o n  # 
80 
Figure h Recall/Precision curves for Manage- 
ment Succession 
4.2 Cho ice  of  Test  Corpus  
Figure 2 shows two evaluations of our discovery 
procedure, tested against the original MUC-6 
corpus of 100 documents, and against our test 
corpus, which consists of an additional 150 doc- 
uments judged manually. The two plots in the 
figure show a slight difference in results, indi- 
cating that in some sense, the MUC corpus was 
more "random", or that our expanded corpus 
was somewhat skewed in favor of more common 
patterns that the system is able to find more 
easily. 
4.3 Cho ice  of  Eva luat ion  Met r i c  
The graphs shown in Figures 1 and 2 are based 
on an "objective" measure we adopted during 
the experiments. This is the same measure of 
relevance used internally by the discovery proce- 
dure on each iteration (relative to the "truth" of 
relevance scores of the previous iteration), and 
is not quite the standard measure used for text 
filtering in IR. According to this measure, the 
system gets a score for each document based on 
the relevance which it assigned to the document. 
Thus if the system .assigned relevance of X per- 
cent to a relevant document, it only received X 
286 
0 
( I )  
0 . 9  
0 . 8  
0 . 7  
I I  I I I I I I : : . , . : ? 
. . . . . . .  . . . . . . . .  . . . . . . . . . . . . . . .  - - -  ! . . . . . . . .  . . . . . . .  
i i i :: i i i i i ~ 
. . . . . .  i . . . . . . . .  i . . . . . . . .  \[ . . . . . . .  ? .. . . . . . .  f . . . . . . . . . . . .  T . . . . . . .  
. . . . . .  J . . . . . . . .  i . . . . . . . .  i .  . . . . . . .  i . . . . . . . .  ~ . . . . . . .  .; . . . . . . . . . . .  ..: . . . . . . . .  i . . . . :  
0 6 . . . . . . . . . . . . . . . . . . . . . . . .  .'7 . . . . . . .  ' . . . . . . . .  ~ . . . . . . .  ~ . . . . . . . .  ~ . . . . . . . . . . . . . . . . . . . . . . .  
i 
i 
i 
0 . 5 . . . . . . .  '. . . . . . . .  , . . . . . . . .  ~" . . . . . . .  , . . . . . . . .  ~ . . . . . . .  ~ . . . . . . . .  * . . . . . . .  "=. . . . . . . .  , . . . . . . .  
i z 
0 . 4  i I 
0 0 . 1 0 . 2 0 . 3 0 . 4 0 . 5 0 . 6 0 . 7 0 . 8 0 . 9  1 
R e c a l l  
Figure 2: Precision vs. Recall 
I 
i . i ~ iB  i 
. . . . . .  e . . . . . . . .  i . . . . . . . .  ! . . . . . . . .  ~ . . . . . . . .  ~ . . .  ' . . . . . . . .  ! . . . . . . . .  i . . . . . . .  0 . 9  
i i i i i im~ ! ! 
! i i D iE  c 
! i i i i i i i i 
0 . '7  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  ~" . . . . . . . . . . . . . . .  
C o n ~ i n ~ o u -  ? ~ut i - -o  f 
0 . 6 . . . . . .  ~ . . . . . . . .  ~ . . . . . . . .  ~ . . . . . . .  ' . . . . . . . .  ~ . . . . . . .  ~ . . . . . . . .  ; . . . . . . .  ~ . . . . . . . .  ~ . . . . . . .  
0 . 5 ...... ~ ........ , ........ ~ ....... ~ ........ ~ ....... ~ ........ ! ........ : ........ 4 ....... 
0 . 4  
0 0 . i 0 . 2 0 . 3 0 . 4 0 . 5 0 . 6 0 . 7 0 . 8 0 . 9  1 
R e c a l l  
Figure 3: Results on the MUC corpus 
percent on the recall score for classifying that 
document correctly. Similarly, if the system as- 
signed relevance Y to an irrelevant document, 
it was penalized only for the mis-classified Y
percent on the precision score. To make our re- 
sults more comparable to those of other MUC 
competitors, we chose a cut-off point and force 
the system to make a binary relevance decision 
on each document. The cut-off of 0.5 seemed 
optimal from empirical observations. Figure 3 
shows a noticeable improvement in scores, when 
using our continuous, "objective" measure, vs. 
the cut-off measure, with the entire graph essen- 
tially translated to the right for a gain of almost 
10 percentage points of recall. 
4.4 Eva luat ing  Pat terns  
Another effective, if simple, measure of perfor- 
mance is  how many of the patterns the pro- 
cedure found, and comparing them with those 
used by an extraction engine which was manu- 
ally constructed for the same task. Our MUC-6 
system used approximately 75 clause level pat- 
terns, with 30 distinct verbal heads. In one 
conservative experiment, we observed that the 
discovery procedure found 17 of these verbs, or 
57%. However, it also found at least 8 verbs the 
manual system lacked, which seemed relevant to 
the scenario: 
company-bring-person-\[as?officer\] 12 
person-come-\[to+eompanv\]-\[as+oZScer\] 
person-rejoin- company-\[as + o25cer\] 
person-{ ret  , conti,  e, remai, ,stay}-\[as + o25cer\] 
person-pursue-interest 
At the risk of igniting a philosophical de- 
bate over what is or is not relevant o a sce- 
nario, we note that the first four of these verbs 
are evidently essential to the scenario in the 
strictest definition, since they imply changes of 
post. The next three are "staying" verbs, and 
are actually also needed, since higher-level infer- 
ences required in tracking events for long-range 
merging over documents, require knowledge of 
persons occupying posts, rather than only as- 
suming or leaving them. The most curious one 
is "person-pursue-interesf'; urprisingly, it too 
is useful, even in the strictest MUC sense, cf., 
(muc, 1995). Systems are judged on filling a 
slot called "other-organization", i dicating from 
or to which company the person came or went. 
This pattern is consistently used in text to indi- 
nbracketed  const i tuents  a re  outs ide  o f  the  cent ra l  
SVO t r ip le t ,  inc luded here  fo r  c la r i ty .  
287 
cate that the person left to pursue other, undis- 
closed interests, the knowledge of which would 
relieve the system from seeking other informa- 
tion in order to fill this slot. This is to say that 
here strict evaluation is elusive. 
5 D iscuss ion  and  Cur rent  Work  
Some of the prior research as emphasized in- 
teractive tools to convert examples to extraction 
patterns, cf. (Yangarber and Grishman, 1997), 
while others have focused on methods for au- 
tomatically converting a corpus annotated with 
extraction examples into such patterns (Lehn- 
ert et al, 1992; Fisher et al, 1995; Miller et 
al., 1998). These methods, however, do not re- 
duce the burden of finding the examples to an- 
notate. With either approach, the portability 
bottleneck is shifted from the problem of build- 
ing patterns to that of finding good candidates. 
The prior work most closely related to this 
study is (Riloff, 1996), which, along with (Riloff, 
1993), seeks automatic methods for filling slots 
in event templates. However, the prior work 
differs from that presented here in several cru- 
cial respects; firstly, the prior work does not at- 
tempt to find entire events, after the fashion 
of MUC's highest-level scenario-template task. 
Rather the patterns produced by those systems 
identify NPs that fill individual slots, without 
specifying how these slots may be combined 
at a later stage into complete vent templates. 
The present work focuses on directly discovering 
event-level, multi-slot relational patterns. Sec- 
ondly, the prior work either relies on a set of 
documents with relevance judgements to find 
slot fillers where they are relevant o events, 
(Riloff, 1996), or utilizes an un-classified cor- 
pus containing a very high proportion of rele- 
vant documents o find all instances of a seman- 
tic class, (Riloff and Jones, 1999). By contrast, 
our procedure requires no relevance judgements, 
and works on the assumption that the corpus is 
balanced and the proportion of relevant docu- 
ments is small. Classifying documents by hand, 
although admittedly easier than tagging event 
instances in text for automatic training, is still 
a formidable task. When we prepared the test 
corpus, it took 5 hours to mark 150 short doc- 
uments. 
The presented results indicate that our 
method of corpus analysis can be used to rapidly 
identify a large number of relevant patterns 
without pre-classifying a large training corpus. 
We are at the early stages of understanding 
how to optimally tune these techniques, and 
there are number of areas that need refinement. 
We are working on capturing the rich informa- 
tion about concept classes which is currently re- 
turned as part of our pattern discovery proce- 
dure, to build up a concept dictionary in tandem 
with the pattern base. We are also consider- 
ing the proper selection of weights and thresh- 
olds for controlling the rankings of patterns and 
documents, criteria for terminating the itera- 
tion process, and for dynamic adjustments of 
these weights. We feel that the generalization 
technique in pattern discovery offers a great 
opportunity for combating sparseness of data, 
though this requires further research. Lastly, 
we are studying these algorithms under several 
unrelated scenarios to determine to what extent 
scenario-specific phenomena affect their perfor- 
mance. 
References 
Ido Dagan, Shaul Marcus, and Shaul 
Markovitch. 1993. Contextual word simi- 
larity and estimation from sparse data. In 
Proceedings of the 31st Annual Meeting of 
the Assn. for Computational Linguistics, 
pages 31-37, Columbus, OH, June. 
David Fisher, Stephen Soderland, Joseph Mc- 
Carthy, Fang-fang Feng, and Wendy Lehnert. 
1995. Description of the UMass system as 
used for MUC-6. In Proc. Si;zth Message Un- 
derstanding Conf. (MUC-6), Columbia, MD, 
November. Morgan Kaufmann. 
R. Grishman, L. Hirschman, and N.T. Nhan. 
1986. Discovery procedures for sublanguage 
selectional patterns: Initial experiments. 
Computational Linguistics, 12(3):205-16. 
Lynette Hirschman, Ralph Grishman, and 
Naomi Sager. 1975. Grammatically-based 
automatic word class formation. Information 
Processing and Management, 11(1/2):39-57. 
Timo J/irvinen and Pasi Tapanainen. 1997. A 
dependency parser for English. Technical Re- 
port TR-1, Department of General Linguis- 
tics, University of Helsinki, Finland, Febru- 
ary. 
Martin Kay and Martin RSscheisen. 1993. 
288 
Text-translation alignment. Computational 
Linguistics, 19(1). 
W. Lehnert, C. Cardie, D. Fisher, J. McCarthy, 
E. Riloff, and S. Soderland. 1992. Univer- 
sity of massachusetts: MUC-4 test results 
and analysis. In Proc. Fourth Message Un- 
derstanding Conf., McLean, VA, June. Mor- 
gan Kaufmann. 
Scott Miller, Michael Crystal, Heidi Fox, 
Lance Ramshaw, Richard Schwartz, Rebecca 
Stone, Ralph Weischedel, and the Annota- 
tion Group. 1998. Algorithms that learn to 
extract information; BBN: Description of the 
SIFT system as used for MUC-7. In Proc. of 
the Seventh Message Understanding Confer- 
ence, Fairfax, VA. 
1993. Proceedings of the Fifth Message Un- 
derstanding Conference (MUC-5), Baltimore, 
MD, August. Morgan Kaufmann. 
1995. Proceedings of the Sixth Message Un- 
derstanding Conference (MUC-6), Columbia, 
M_D, November. Morgan Kaufmann. 
Johanna Nichols. 1978. Secondary predicates. 
Proceedings of the 4th Annual Meeting of 
Berkeley Linguistics Society, pages 114-127. 
Maria Teresa Pazienza, editor. 1997. Infor- 
mation Extraction. Springer-Verlag, Lecture 
Notes in Artificial Intelligence, Rome. 
Fernando Pereira, Naftali Tishby, and Lillian 
Lee. 1993. Distributional clustering of En- 
glish words. In Proceedings of the 31st An- 
nual Meeting of the Assn. for Computational 
Linguistics, pages 183-190, Columbus, OH, 
June. 
Ellen Riloff and Rosie Jones. 1999. Learn- 
ing dictionaries for information extraction by 
multi-level bootstrapping. In Proceedings of
Sixteenth National Conference on Artificial 
Intelligence (AAAI-99), Orlando, Florida, 
Ellen Riloff. 1993. Automatically construct- 
ing a dictionary for information extraction 
tasks. In Proceedings of Eleventh National 
Conference on Artificial Intelligence (AAAI- 
93), pages 811-816. The AAAI Press/MIT 
Press. 
Ellen Riloff. 1996. Automatically generating 
extraction patterns from untagged text. In 
Proceedings of Thirteenth National Confer- 
ence on Artificial Intelligence (AAAL96), 
pages 1044-1049. The AAAI Press/MIT 
Press. 
Pasi Tapanainen and Timo J~rvinen. 1997. A 
non-projective dependency parser. In Pro- 
ceedings of the 5th Conference on Applied 
Natural Language Processing, pages 64-71, 
Washington, D.C., April. ACL. 
Roman Yangarber and Ralph Grishman. 1997. 
Customization of information extraction sys- 
tems. In Paola Velardi, editor, International 
Workshop on Lexically Driven Information 
Extraction, pages 1-11, Frascati, Italy, July. 
Universit?~ di Roma. 
289 
Automatic Acquisition of Domain Knowledge for Information 
Extraction 
Roman Yangarber, Ralph Grishman Past Tapanainen 
Courant  Inst i tute of Conexor oy 
Mathemat ica l  Sciences Helsinki, F in land 
New York University 
{roman \[ grishman}@cs, nyu. edu Pasi. Tapanainen@conexor. fi 
Si!ja Ituttunen 
University of Helsinki 
F inland 
sihuttun@ling.helsinki.fi 
Abstract  
In developing an Infbrmation Extraction tIE) 
system tbr a new class of events or relations, one 
of the major tasks is identifying the many ways 
in which these events or relations may be ex- 
pressed in text. This has generally involved the 
manual analysis and, in some cases, the anno- 
tation of large quantities of text involving these 
events. This paper presents an alternative ap- 
proach, based on an automatic discovery pro- 
cedure, ExDIsCO, which identifies a set; of rele- 
wmt documents and a set of event patterns from 
un-annotated text, starting from a small set of 
"seed patterns." We evaluate ExDIScO by com- 
paring the pertbrmance of discovered patterns 
against that of manually constructed systems 
on actual extraction tasks. 
0 Introduct ion 
Intbrmation Extraction is the selective xtrac- 
tion of specified types of intbrmation from nat- 
ural language text. The intbrmation to be 
extracted may consist of particular semantic 
classes of objects (entities), relationships among 
these entities, and events in which these entities 
participate. The extraction system places this 
intbrmation into a data base tbr retrieval and 
subsequent processing. 
In this paper we shall be concerned primar- 
ily with the extraction of intbrmation about 
events. In the terminology which has evolved 
ti'om the Message Understanding Conferences 
(muc, 1995; muc, 1993), we shall use the term 
subject domain to refer to a broad class of texts, 
such as business news, and tile term scenario to 
refer to tile specification of tile particular events 
to be extracted. For example, the "Manage- 
ment Succession" scenario for MUC-6, which we 
shall refer to throughout this paper, involves in- 
formation about corporate executives tarting 
and leaving positions. 
The fundamental problem we face in port- 
ing an extraction system to a new scenario is 
to identify the many ways in which intbrmation 
about a type of event may be expressed in the 
text;. Typically, there will be a few common 
tbrms of expression which will quickly come to 
nfind when a system is being developed. How- 
ever, the beauty of natural language (and the 
challenge tbr computational linguists) is that 
there are many variants which an imaginative 
writer cast use, and which the system needs to 
capture. Finding these variants may involve 
studying very large amounts of text; in the sub- 
ject domain. This has been a major impediment 
to the portability and performance of event ex- 
traction systems. 
We present; in this paper a new approach 
to finding these variants automatically fl'om a 
large corpus, without the need to read or amLo- 
tate the corpus. This approach as been evalu- 
ated on actual event extraction scenarios. 
In the next section we outline the strncture of 
our extraction system, and describe the discov- 
ery task in the context of this system. Sections 
2 and 3 describe our algorithm for pattern dis- 
covery; section 4 describes our experimental re- 
sults. This is tbllowed by comparison with prior 
work and discussion in section 5. 
1 The Extract ion System 
In the simplest terms, an extraction system 
identifies patterns within the text, and then 
mat)s some constituents of these patterns into 
data base entries. (This very simple descrip- 
lion ignores the problems of anaphora nd in- 
tersentential inference, which must be addressed 
by any general event extraction system.) AI- 
though these l)atterns could in principle be 
stated in terms of individual words, it is much 
940 
easier to state them in terms of larger SylltaC- 
tic constituents, uch as noun phrases and verb 
groups. Consequently, extraction ormally con- 
sists of an analysis of the l;e.xt in terms of general 
linguistic structures and dolnain-specifio con- 
structs, tbllowed by a search for the scenario- 
specific patterns. 
It is possible to build these constituent struc- 
tures through a flfll syntactic analysis of the 
text, and the discovery procedure we describe 
below woul(1 be applicable to such an architec- 
ture. Howe, ver, for re&sellS of slme,(t , coverage, 
and system rolmstness, the more (:ommon ap- 
t)roa(:h at present is to peribrni a t)artial syn- 
tactic analysis using a cascade of finite-state 
transducers. This is the at)t)roa(:h used by our 
e.xtraction system (Grishman, 1995; Yangarber 
and Grishman, 1998). 
At; the heart of our syslx'an is a regular ex- 
pression pattern matcher which is Cal)al)le of 
matching a set of regular exl)ressions against 
a partially-analyzed text and producing addi- 
tional annotations on the text. This core draws 
on a set of knowledge bases of w~rying degrees 
of domain- and task-specificity. The lexicon in- 
cludes both a general English dictionary and 
definitions of domain and scenario terms. The 
concept base arranges the domain terms into 
a semantic hierarchy. The predicate base. de- 
s('ribes the, logical structure of I;he events to be 
extracl;od. 'Fire pattern \])ase consists of sets of 
patterns (with associated actions), whi(;h make 
r(;ferollCO to information Kern the other knowl- 
e(lge bases. Some t)attorn sots, su(:h as those for 
n(mn and verb groups, are broadly apl)licable , 
wlfile other sets are spe(:ifio to the scenario. 
V~Ze, have previously (Yangarl)er and Grish- 
man, 1.997) (lescrit)ed a user interface which 
supt)orts the rapid cust;omization of the extrac- 
tion system to a new scenario. This interface 
allows the user to provide examples of role- 
wmt events, which are automatically converted 
into the appropriate patterns and generalized to 
cover syntactic variants (passive, relative clause, 
etc.). Through this internee, the user can also 
generalize l;he pattern semanti('ally (to (:over a 
broader class of words) and modify the concet)t 
base and lexicon as needed. Given an appro- 
priate set; of examples, thereibre, it; has become 
possible to adapt the extraction system quite 
ral)idly. 
However, the burden is still on the user to 
find the appropriate set of examples, which may 
require a painstaldng and expensive search of a 
large corpus. Reducing this cost is essential for 
enhanced system portability; this is the problem 
addressed by the current research. 
Ilow can we automatically discover a suitable 
set; of candidate patterns or examples (patterns 
which at least have a high likelihood of being 
relevant to the scenario)? The basic idea is to 
look for linguistic patterns which apt)ear with 
relatively high frequency in relevant documents. 
While there has been prior research oll idea|i- 
lying the primary lexical t)atterns of a sublan- 
guage or cortms (Orishman et al 1986; Riloff, 
1996), the task here is more complex, since we 
are tyt)ically not provided in advance with a 
sub-corpus of relevmlt passages; these passages 
must themselves be tbund as part of t;t1(; discov- 
ery i)rocedure. The difficulty is that one of the 
l)est imlic~tions of the relevance of the passages 
is t)recisely the t)resence of these constructs. Bo- 
(:ause of this (:ircularity, we l)ropose to a(:quire. 
the constructs and t)assagos in tandem. 
2 ExDISCO: the  D iscovery  P rocedure  
We tirst outline ExDIsco ,  our procedure for 
discovery of oxl,raction patterns; details of some 
of the stops arc l)rcse, nted in the section which 
follows, and an earlier t)~q)er on our at)l)roach 
(Yang~u:bcr ot al., 2000). ExDIscO is mi ml- 
supervised 1)rocedure: the training (:ortms does 
not need to t)e amlotated with the specific event 
intbrmatkm to be. e.xtracted, or oven with infor- 
mation as to whi(;h documents in the ('orpus are 
relevant o the scenario. 'i7tlo only intbrmation 
the user must provide, as described below, is a 
small set of seed patterns regarding the s(:enario. 
Starting with this seed, the system automati- 
(:ally pertbnns a repeated, automatic expansion 
of the pattern set. This is analogous to the pro- 
cess of automatic t;enn expansion used in s()me 
information retrieval systems, where, the terlns 
Dora the most relewmt doculncnts are added 
to the user query and then a new retriewfl is 
imrformed. However, by expanding in terms of 
1)atl;erns rather than individual terms, a more 
precise expansion is possit)le. This process pro- 
coeds as tbllows: 
0. We stm:t with a large, corlms of documents 
in the domain (which have not been anne- 
941 
tared or classified in any way) and an initial 
"seed" of scenario patterns selected by the 
user - -  a small set of patterns whose pres- 
ence reliably indicates thai; the document 
is relevant o the scenario. 
. The pattern set is used to divide the cor- 
tins U into a set of relewmt documents, R
(which contain at; least one instance of one 
of the patterns), and a set of non-relevant 
documents R = U - R. 
2. Search tbr new candidate patterns: 
? automatically convert each document 
in the eorIms into a set of candidate 
patterns, one for each clause 
? rank patterns by the degree to which 
their distribution is correlated with 
docmnent relevance (i.e., appears with 
higher frequency in relevant docu- 
ments than in non-relewmt ones). 
3. Add the highest ranking pattern to the pat- 
tern set. (Optionally, at this point, we may 
present he pattern to the user for review.) 
4. Use the new pattern set; to induce a new 
split of the corpus into relevant and non- 
relevant documents. More precisely, docu- 
ments will now be given a relevance confi- 
dence measure; documents containing one 
of the initial seed patterns will be given 
a score of 1, while documents which arc 
added to the relevant cortms through newly 
discovered patterns will be given a lower 
score. I/,epeat the procedure (from step 1) 
until some iteration limit is reached, or no 
more patterns can be added. 
3 Methodo logy  
3.1 Pre-processing: Syntact ic Analysis 
Before at)plying ExDIsco ,  we pre-proeessed 
the cortms using a general-purpose d pendency 
parser of English. The parser is based on 
the FDG tbrmalism (Tapanainen and Jgrvi- 
hen, 1997) and developed by the Research Unit 
for Multilingual Language Technology at the 
University of Helsinki, and Conexor Oy. The 
parser is used ibr reducing each clause or noun 
phrase to a tuple, consisting of the central ar- 
guments, ms described in detail in (Yangarber 
et al, 2000). We used a corlms of 9,224 articles 
from the Wall Street; Journal. The parsed arti- 
cles yielded a total of 440,000 clausal tuples, of 
which 215,000 were distinct. 
3.2 Normal izat ion 
We applied a name recognition module prior to 
parsing, and replaced each name with a token 
describing its (:lass, e.g. C-Person, C-Company, 
etc. We collapsed together all numeric expres- 
sions, currency wflues, dates, etc., using a single 
token to designate ach of these classes. Lastly, 
the parser performed syntactic normalization to 
transtbrm such variants ms the various passive 
and relative clauses into a common tbrm. 
3.3 General izat ion and Concept Classes 
Because tuples may not repeat with sufficient 
frequency to obtain reliable statistics, each tu- 
ple is reduced to a set of pints: e.g., a verb- 
object pair, a subject-object pair, etc. Each pair 
is used as a generalized pattern during the can- 
didate selection stage. Once we have identitied 
pairs which are relevant o the scenario, we use 
them to gather the set; of words for the miss- 
ing role(s) (tbr example, a class of verbs which 
occur with a relevant subject-ot@ct pair: "com- 
pany {hire/fire/expel...} person"). 
3.4 Pat tern  Discovery 
We (-onducte(1 exi)eriments in several scenarios 
within news domains such as changes in cor- 
porate ownership, and natural disasters. Itere 
we present results on the "Man~geme.nt Suc- 
cession" and "Mergers/Acquisitions" cenarios. 
ExDIsco  was seeded with lninimal pattern sets, 
namely: 
Subject Verb Direct Object 
C-Company C-At)point C-Person 
C-Person C-Resign 
ibr the Mmmgement task, and 
Subject Verb Direct Object 
* C-Buy C-Conlt)any 
C-Company merge * 
for Acquisitions. Here C-Company and C- 
Person denote semantic classes containing 
named entities of the corresponding types. C- 
Appoint denotes the list of verbs { appoint, elect, 
promote, name, nominate}, C-Resign = { re- 
sign, depart, quit }, and C-Buy = { buy , pur- 
chase }. 
942 
\ ])uring ~ single iter~tion, we conqmt(; the 
score, See're(p), for each cm~(lidate 1)attern p, 
using (;he fornmla~: 
S, :o ' , ' ,@)  = IH n l~l 
IHI - 1,,~ IHn  ~.1 (:t) 
where 12. (Icnotes (;h(', l'clewmt subsc(; of docu- 
ments, mid I t=  It(p) the, ( locmnents imttching 
p, as above; the Iirst (;erm a(:(:ounts for the con- 
(lition~fl t)robabil ity of relev;m('e oil p~ and |;11(; 
second tbr its support .  We further impose two 
support criteria: we distrust such frequent pat- 
(;,~.,-.~ w\]le,:e I1~ n UI > ,~IUI, ,~ uninforn,,~tive, 
mid rare patte.rns \['or which I1\] r-I \]~.1 < fl as 
noise. 2 At the end ot' (.aeh il;eratiol~, the sysl;em 
selects the pal;tern with the highest Sco'/'d(p)~ 
and adds it (;o (;lie seed scl;. The (to(:un~enl;s 
which t;he winning t)~(;t;ern hits are added (;o 
t;111(; relevant set. The  t)al;l;(;rn s(;areh is then 
r(;sl;m:l;(;d. 
3.5 Document  Re- rank ing  
Th(: above is a simt)lifi(';~l;ion of (;he a,(:tual pro- 
cedlll'(}~ in severa\] r(',st)e('(;s. 
Only generalized t)ntl;erns are (:onsidered fi)r 
(:audi(t~my, with one or mot(', slol;s fill(:(1 wi(;h 
wihl-cm'ds. In comput ing the score of th(', ge, n- 
(;raliz(:d \]);tttern, w(: do not take into ('onsi(h:r- 
;i,1;i()11 all possible va,hw, s of the, wil(1-('m:d role. 
\?e instea.d (:()llS(;raJll (;he wild-(:ar(l to thos(~ wd- 
u(:s wlli(:h l;ht',llls(;lv(;s ill (;llrH \]l;tV(: high scores. 
Th(:se v~du(:s l;lw, n |)e(:on~e lllClll\])(;l'S of }/. II(:W 
(:lass, whi(:h is l)rOdu(:ed in (;:tlldClll with the 
wimfing 1)att(:rn. 
\])o('umel~tS reh:wm('e is s(-ored (m ~ s(;ah: l)e- 
(;ween 0 and 1. Tlm seed t)atterns a.re a.(:cet)ted 
~,s trut\]~; the do('mlw, nts (;hey mat(:\]1 hnve rcle- 
vmme 1. On i(;er;~tion i + 1, e~mh t)a(;tern p is 
assigned a precision measure, t)ase(l on the rel- 
(':Vall(;e of |;11(; (locllnlelfl;s i|; 111a, l;(;ll(',,q: 
~ ".d~(d) (~) 
f f , , :d  +~ (v)  - -  IH(v) l ,~.(,,) 
where l~,eli(d) is the re, levmlce of' 1;11(: doeunmn(; 
fi'om t;t1(', previous iteration, ~md l I(p) is the set 
of documents where p matched, in general, if K 
is a classifier (:onsisting of ~ set of l)al;terns, w(', 
define H (K)  as the st:l; of documents  where all 
~similar to that used in (liiloff, 1996) 
~W(: used ,:-- 0.1 and fl = 2. 
of t)~d;terns p C K m~l;(:h, mid the "cunmlative" 
precision of K as 
1 ~ 1~4~(a,) (3) P~.~d +~(1() = IH U()I <.(K)  
Once the wimfing pa,l;l;ern is accepted, the rel- 
ewmee of the documents is re-adjusted. For 
(;~mh document  d which is matched by some 
subset of l;he currently accet)t('d pntterns, we 
can view thai; sul)s(',t; of  l)~tterns as ~ classitier 
Kd = {pj}. These  patterns (tel;ermilm the new 
reh;wmce score of the document  as 
J~, "~l,~ " ( ,0  : 111~x (:tc,,.1,*(,O,v,.,;, .~" (K , ) )  (~:) 
This ensures tha.(; l;he rclewmce score grows 
monotonical ly, and only when there is sufliei(mt 
positive evidence, as (;he i)ntterns in etl'e(:I; vote 
"conjmmtively" on the (loculncnl;s. 
We also tr ied an alternative, ::disjun(:tive" 
voting scheme, with weights wlfich accounts tbr 
vm:intion in support of the p~ttterns, 
J,.,.1, (d) . . . .  ~ "~ I I  (1 - ~',.~,.c~(p))"",' (5) 
~c K(d) 
where t;11(', weights ,wp arc (tetint;d using the tel- 
ewm(:(: of the (loeuments, a,s the total  SUl)l)or(; 
which the pa, I;I;ern p receives: 
% = log ~ l;.d,(d) 
dE 11 (p) 
and ;,7 is (;11(' largest weight. The  r(',cursive for- 
nmb~s ('apl;m:e (;he mul;u~fl dependency of t)~t- 
terns ~md documents;  this re-computat ion ~md 
growing of precision and relevmlce rmlks is the 
core of the t)rocedure. :~ 
4 Resu l ts  
4 .1  Event  Ext ract ion  
'l'he, most nal;m'a.l measm'e of efl'ecl;iveness of our 
discovery procedure is the performmme of ml ex- 
tract ion systmn using the, discovered t)~tterns. 
However, il; is not 1)ossil)le to apply this reel;- 
rio direei;ly because the discovered t)al;terns lack 
some of the information required tbr entries ill 
:{\V('. did not el)serve a significam; difl'erencc in 1)crfi)r- 
lIiHl\[CO, bet, ween the two tormulas 4 alt(t 5 in o111" experi- 
in(mrs; the results whit:h tbllow use 5. 
943 
the pattern base: information about the event 
type (predicate) associated with the pattern, 
and the mapping from pattern elements to pred- 
icate arguments. We have evaluated ExDIsco  
by manually incorporating the discovered pat- 
terns into the Proteus knowledge bases and run- 
ning a full MUC-style evaluation. 
We started with our extraction system, Pro- 
tens, which was used in MUC-6 in 1995, and 
has undergone continual improvements since 
the MUC evaluation. We removed all the 
scenario-specific clause and nominalization pat- 
terns. 4 We then reviewed all the patterns which 
were generated by the ExDIsco,  deleting those 
which were not relewmt to the task, or which 
did not correspond irectly to a predicate al- 
ready implemented tbr this task)  The remain- 
ing pat;terns were augmented with intbnnation 
about the corresponding predicate, and the re- 
lation between the pattern and the predicate 
al'guments, a The resulting variants of Proteus 
were applied to the formal training corpus and 
the (hidden) formal test corpus for MUC-6, and 
the output evaluated with the MUC scorer. 
The results on the training corpus are: 
Pattern Base Recall Precision 
Seed 38 83 
Ex I ) Isco 62 80 
Union 69 __79 
Manual-MUC ~ 71 L~1.9~ 
Manual-NOW 6(3~ 79 L7!~z\[)_t_j 
and on the test cortms: 
4There are also a few noun phrase patterns which can 
give rise to scenario events. For example, "Mr Smith, 
former president of IBM", may produce an event record 
where l%ed Smith left IBM. These patterns were left in 
Proteus for all the runs, and they make some contribu- 
tion to the relatively high baseline scores obtained using 
just the seed event patterns. 
~ExD~sco f und patterns which were relevant to the 
task lint could not be easily aceomodated in Proteus. 
For instance "X remained as president" could be rele- 
vant, particularly in the case of a merger creating anew 
corporate ntity, but Proteus was not equipped to trun- 
dle such iIfformation, and has not yet been extended to 
incorporate such patterns. 
6As with all clause-level patterns in Proteus, these 
patterns m-e automatically generalized tohandle syntac- 
tic wn'iants uch as passive, relative clause, etc. 
Pattern Base Recall Precision F 
Seed 27 74 39.58 
ExDIsco 52 72 60.16 
Union 57 73 63.56 
Manual-NOW -- 56 75 6404. 
The tables show the recall and precision mea- 
sures for the patterns, with F-measure being 
the harmonic mean of the two. The Seed pat- 
tern base consists of just the initial pattern set, 
given in the table on the previous page. ~ib this 
we added the patterns which the system discov- 
ered automatically after about 100 iterations, 
producing the pattern set called ExDIsco.  For 
comparison, M anual-MUC is the pattern base 
lnanually develot)ed on the MUC-6 training 
corpus-1)repared over the course of 1 month 
of full-time work by at least one computational 
linguist (during which the 100-document train- 
ing corpus was studied in detail). The last row, 
Manual-now, shows the current pertbrmance of
the Proteus system. The base called Ultiolt con- 
tains the union of ExDIScO and Manual-No'w. 
We find these results very encouraging: Pro- 
teus performs better with the patterns discov- 
ered by ExI)IscO than it did after one month 
of manual tinting and development; in fact, this 
perfi)rmance is close to current levels, which 
are the result of substantial additional devel- 
opmeut. These results umst be interpreted, 
however, with several caveats. First, Proteus 
performance depends on many fimtors besides 
the event patterns, such as the quality of name 
re, cognition, syntactic mmlysis, anaphora reso~ 
lution, inferencing, etc. Several of these were 
improved since the MUC formal evaluation, so 
some of the gain over the MUC formal evalua- 
tion score is attritmtable to these factors. How~ 
ever, all of the other scores are comparable in 
these regards. Second, as we noted above, the 
patterns were reviewed and augmented manu- 
ally, so the overall procedure is not entirely au- 
tomatic. However, the review and augmenta- 
tion process took little time, as compared to 
the manual corpus analysis and development of
the pattern base. 
4.2 Text  f i l ter ing 
We can obtain a second measure of pertbr- 
mance by noting that, in addition to growing 
the tmttern set, ExDIsco  also grows the rele- 
944 
0.9 
0.8 
0.7 
0.6 
0.5 
_ . r -~H . . . . . . . . . . . . . . . . .  r . . . . . . .  T ~ ~ : : ~  T 
;!\ >. g~t : 
- ' il 
%i 
\[!\] 
7 
\[!J 
Legend: 
Management/Test ? .-{~ ...... 
ManagemenVl-raie - :*: -- 
MUC-6 ? 
0.2 0.4 0.6 0.8 
Recall 
Figure l: Management Suc('cssion 
0.9 
0.8 
0.7 
0.6 
0.5 
L_~/r 
Legend: 
Acquisition 
0.2 0.4 0.6 
Recall 
0.8 
Figme 2: Mergers/A(:quisitions 
vance rankings of documents. The latter cnn be 
evahlated irectly, wil;hollt human intervention. 
We tested Exl)IsC, o ~tgainst wo cor\])orn: th(; 
100 documents from MUC-6 tbrmal training, 
a:nd the 100 documents from the MUC-6 for- 
mal test (both are contained anlong the 10,000 
ExDIsoO training set) r. Figure 1 shows recall 
t)\]otted against precision on the two corpora, 
over 100 iterations, starting with the seed pat- 
te, nls in section 3.d. This view on the discovery 
procedure is closely related to the MUC %ext- 
till;ering" task, in which the systems are jlulged 
at the \]evel of doc,wm, e,'nt.s rather thmt event slots. 
It; is interesting to (:omt)m:e Exl)IsCO's results 
with how other MUC-6 part\]tit)ants performed 
on the MUC-b '  test cortms , shown anonymously. 
ExDIscO attains values within the range of 
the MUC participald;S, all of which were either 
heavily-supervised or m~mually coded systems. 
II; is important to bear in mind that Ex I ) I sco  
had no benefit of training material, or any in- 
tbrmation beyond the seed pattern set. 
Figure 2 shows the 1)ertbrmance, of text fil- 
tering on the Acquisition task, again, given the 
seed in section 3.4. ExDisco  was trained on 
|;lie same WSJ eorlms, and tested against a set 
of 200 documents. We retrieved this set using 
keyword-based IR, search, and judged their rel- 
evance by halId. 
rThesc judgements constituted the truth which was 
used only for evaluation, not visible to ExDISCO 
5 Discuss ion  
The development of a w~riety of information 
extra(:tion systems over the last decade has 
demonstrated their feasibility but also the lim- 
itations on their portability and t)erformance. 
Prcl)aring good t)atterns tbr these syste, ms re- 
quires (:onsiderable skill, and achieving good 
(:overage requires |;lie analysis of a large amount 
of text. These t)rol)lems h~ve t)een impedinmnts 
to the -wide\].' use of extraction systenls. 
These dit\[iculties have stimulate.d resear('h on 
1)attel . 'n a ( : ( lu i s i t ion .  So lne  o f  th i s  work  has  en l -  
i)hasized il\]teractive tools to (:onvert examples 
to extractioi~ t)atterlls (Yangarber and Grish- 
man, 1997); nmch ot:' the re, search has focused on 
methods for automatically converting a cortms 
annotated with extraction examples into pat- 
terns (Lehnert et al, 1992; Fisher et al, 1995; 
Miller el; al., 1998). These techniques may re- 
duce the level of systeln expertise required to 
develop a new extraction N)plieation, but they 
do not lessen the lmrden of studying a large cor- 
lms in order to .find relevant candidates. 
The prior work most closely related to our 
own is that of (R.ilotf, 1996), who also seeks to 
lmild pattenls automatically without the need 
to annotate a corpus with the information to 
be extracted. Itowever, her work ditfers t'rom 
01217 own in several i lnportant respects. First, 
her patterns identit~y phrases that fill individual 
slots in the template, without specifying how 
these slots may be combined at a later stage 
into complete templates. In contrast, our pro- 
cedure discovers complete, multi-slot event pat- 
945 
terns. Second, her procedure relies on a cort)us 
in which |;tie documents have been classified for 
relevance by hand (it was applied to the MUC-3 
task, tbr which over 1500 classified documents 
are available), whereas ExDIsco requires no 
manual relevance judgements. While classify- 
ing documents tbr relevance is much easier than 
annotating docunlents with the information to 
be extracted, it; is still a significant ask, and 
places a limit on |:tie size of the training corpus 
that can be effectively used. 
Our research as demonstrated that for the 
studied scenarios automatic pattern discovery 
Call yield extraction perfi)rmance colnt)arabh~ to
that obtained through extensive corpus anal- 
ysis. There are many directions in which the 
work reported here needs to be extended: 
? nsing larger training corpora, in order to 
find less frequent exanlplcs, and in that way 
hopefully exceeding the i)erfornlancc of our 
best hand-trained system 
? cat)luring the word classes which are gen- 
erated as a by-product of our pattern dis- 
covery 1)rocedure (in a manner similar to 
(Riloff and ,Jones, 1999)) and using them 
to discover less frequent )atterns in subse- 
quent iterations 
- evaluating the effectiveness of the discov- 
cry procedure on other scenarios. In par- 
titular, we need to be able to identi\[y top- 
its which cast be most effbctively charac- 
terized by clause-level patterns (as was the 
case tbr the business domain), and topics 
which can be better characterized by other 
means. We. wouM also like to understand 
how the topic clusters (of documents and 
patterns) which are developed by our pro- 
cedure line up with pre-specified scenarios. 
References 
David Fisher, Stephen Soderland, Joseph Mc- 
Carthy, Fangfang Feng, and Wendy Lelmert. 
1995. Description of the UMass system as 
used fbr MUC-6. In Prec. Sixth Message Un- 
dcrstandin9 Conf. (MUC-6), Columbia, MD, 
November. Morgan Kauflnann. 
R.alph Grishman. 1995. The NYU systenl tbr 
MUC-6, or where's the syntax? Ill Prec. 
Sixth Message Understanding Conf. (MUC- 
6), pages 167 176, Columl)ia, MD, Novem- 
ber. Morgan Kauflnann. 
W. Lehnert, C. Cardie, D. Fisher, J. McCarthy, 
E. Riloff, and S. Soderland. 1992. Univer- 
sity of nlassachusetts: MUC-4 test results 
and analysis. Ill P,'oe. Fourth Message Un- 
der.standing Con.\[., McLean, VA, June. Mor- 
gan Kauflnaml. 
Scott Miller, Michael Crystal, Heidi Fox, 
Lance II,amshaw, R,ichard Schwartz, Rebecca 
Stone, Rall)h Weischedel, and the Annota- 
tion Group. 1998. Algorithms that learn to 
extract intbrmation; BBN: Description of the 
SIFT systenl as used for MUC-7. In PTve. 7th 
Mc.ssagc Understanding Co~:f., FMrfax, VA. 
1993. Proceedings of the F'~ifth Message UTz.- 
derstanding Confer(race (MUC-5), Baltimore, 
MD, August. Morgan Kauflnann. 
1995. PTveeedings of the Sixth Message U~I,- 
derstav, ding Conference (MUC-6), Colmnt)ia, 
MD, November. Morgan Kauflnaml. 
Ellen Rilotf and Rosie Jones. 1999. Learn- 
ing dictionaries for infbrmation extraction by 
multi-level bootstrat)ping. In Prec. 16th Nat'l 
Cord'erenee on Art'~i\[icial Intelli9enee (AAA I 
99), Orlando, Florida. 
Ellen Riloff. 1996. Automatically generating 
extraction patterns from m~tagged text. In 
Prec. I3th Nat'l Co~~:f. on Art~ificial Intel- 
ligence (AAAI-96). The AAAI Press/MIT 
Press. 
l?asi '\])~panainen a d Time .J/h:vinen. 1997. A 
non-t)rojectivc dependency parser. In P'mc. 
5th Conf. on Applied Nat'aral Language P~v- 
cessiu9, pages 64-71, Washington, D.C. ACL. 
Roman Yangarber and RalI)h Grishman. 1997. 
Customization of intbrmation extraction sys- 
tems. In Paola Velardi, editor, I~tt'l Work- 
shop on Lexically Driven I~7:forrnation Extrac- 
tion, Frascati, Italy. Universith di Roma. 
Roman Yangarl)er and Ralph Grishman. 1998. 
NYU: Description of thc Protens/PET sys- 
tem as used tbr MUC-7 ST. In 7th Message 
Understanding Conference, Columbia, MD. 
Roman Yangarl)er, Ralph Grishman, Past 
Tapanainen, and Silja Huttunen. 2000. Un- 
supervised discovery of scenario-level pat- 
terns tbr information extraction. Ill PTve. 
Co~@ on Applied Nat'aral Lang'aage Pr'ocess- 
tug (ANLP-NAACL), Seattle, WA. 
946 
