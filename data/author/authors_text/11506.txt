Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 193?196,
Suntec, Singapore, 4 August 2009.
c
?2009 ACL and AFNLP
A Rose is a Roos is a Ruusu: Querying Translations for Web Image Search
Janara Christensen Mausam Oren Etzioni
Turing Center
Dept. of Computer Science and Engineering
University of Washington, Seattle, WA 98105 USA
{janara, mausam, etzioni}@cs.washington.edu
Abstract
We query Web Image search engines with
words (e.g., spring) but need images that
correspond to particular senses of the word
(e.g., flexible coil). Querying with poly-
semous words often yields unsatisfactory
results from engines such as Google Im-
ages. We build an image search engine,
IDIOM, which improves the quality of re-
turned images by focusing search on the
desired sense. Our algorithm, instead of
searching for the original query, searches
for multiple, automatically chosen trans-
lations of the sense in several languages.
Experimental results show that IDIOM out-
performs Google Images and other com-
peting algorithms returning 22% more rel-
evant images.
1 Introduction
One out of five Web searches is an image search
(Basu, 2009). A large subset of these searches
is subjective in nature, where the user is looking
for different images for a single concept (Linsley,
2009). However, it is a common user experience
that the images returned are not relevant to the in-
tended concept. Typical reasons include (1) exis-
tence of homographs (other words that share the
same spelling, possibly in another language), and
(2) polysemy, several meanings of the query word,
which get merged in the results.
For example, the English word ?spring? has sev-
eral senses ? (1) the season, (2) the water body, (3)
spring coil, and (4) to jump. Ten out of the first fif-
teen Google images for spring relate to the season
sense, three to water body, one to coil and none to
the jumping sense. Simple modifications to query
do not always work. Searching for spring water
results in many images of bottles of spring water
and searching for spring jump returns only three
images (out of fifteen) of someone jumping.
Polysemous words are common in English. It
is estimated that average polysemy of English is
more than 2 and average polysemy of common
English words is much higher (around 4). Thus,
it is not surprising that polysemy presents a signif-
icant limitation in the context of Web Search. This
is especially pronounced for image search where
query modification by adding related words may
not help, since, even though the new words might
be present on the page, they may not be all associ-
ated with an image.
Recently Etzioni et al (2007) introduced PAN-
IMAGES, a novel approach to image search, which
presents the user with a set of translations. E.g., it
returns 38 translations for the coil sense of spring.
The user can query one or more translations to get
the relevant images. However, this method puts
the onus of choosing a translation on the user. A
typical user is unaware of most properties of lan-
guages and has no idea whether a translation will
make a good query. This results in an added bur-
den on the user to try different translations before
finding the one that returns the relevant images.
Our novel system, IDIOM, removes this addi-
tional burden. Given a desired sense it automati-
cally picks the good translations, searches for as-
sociated images and presents the final images to
the user. For example, it automatically queries the
French ressort when looking for images of spring
coil. We make the following contributions:
? We automatically learn a predictor for "good"
translations to query given a desired sense. A
good translation is one that is monosemous
and is in a major language, i.e., is expected to
yield a large number of images.
? Given a sense we run our predictor on all its
translations to shortlist a set of three transla-
tions to query.
? We evaluate our predictor by comparing the
images that its shortlists return against the
193
images that several competing methods re-
turn. Our evaluation demonstrates that ID-
IOM returns at least one good image for 35%
more senses (than closest competitor) and
overall returns 22% better images.
2 Background
IDIOM makes heavy use of a sense disambiguated,
vastly multilingual dictionary called PANDIC-
TIONARY (Mausam et al, 2009). PANDIC-
TIONARY is automatically constructed by prob-
abilistic inference over a graph of translations,
which is compiled from a large number of multi-
lingual and bilingual dictionaries. For each sense
PANDICTIONARY provides us with a set of trans-
lations in several languages. Since it is gener-
ated by inference, some of the asserted transla-
tions may be incorrect ? it additionally associates
a probability score with each translation. For
our work we choose a probability threshold such
that the overall precision of the dictionary is 0.9
(evaluated based on a random sample). PANDIC-
TIONARY has about 80,000 senses and about 1.8
million translations at precision 0.9.
We use Google Image Search as our underlying
image search engine, but our methods are indepen-
dent of the underlying search engine used.
3 The IDIOM Algorithm
At the highest level IDIOM operates in three main
steps: (1) Given a new query q it looks up its vari-
ous senses in PANDICTIONARY. It displays these
senses and asks the user to select the intended
sense, s
q
. (2) It runs Algorithm 1 to shortlist three
translations of s
q
that are expected to return high
quality images. (3) It queries Google Images us-
ing the three shortlisted translations and displays
the images. In this fashion IDIOM searches for
images that are relevant to the intended concept
as opposed to using a possibly ambiguous query.
The key technical component is the second step
? shortlisting the translations. We first use PAN-
DICTIONARY to acquire a set of high probability
translations of s
q
. We run each of these transla-
tions through a learned classifier, which predicts
whether it will make a good query, i.e., whether
we can expect images relevant to this sense if
queried using this translation. The classifier ad-
ditionally outputs a confidence score, which we
use to rank the various translations. We pick the
top three translations, as long as they are above a
minimum confidence score, and return those as the
shortlisted queries. Algorithm 1 describes this as
a pseudo-code.
Algorithm 1 findGoodTranslationsToQuery(s
q
)
1: translations = translations of s
q
in PANDICTIONARY
2: for all w ? translations do
3: pd = getPanDictionaryFeatures(w, s
q
)
4: g = getGoogleFeatures(w, s
q
)
5: conf[w] = confidence in Learner.classify(pd, g)
6: sort all words w in decreasing order of conf scores
7: return top three w from the sorted list
3.1 Features for Classifier
What makes a translation w good to query? A
desired translation is one that (1) is in a high-
coverage language, so that the number of images
returned is large, (2) monosemously expresses the
intended sense s
q
, or at least has this sense as
its dominant sense, and (3) does not have homo-
graphs in other languages. Such a translation is
expected to yield images relevant to only the in-
tended sense. We construct several features that
provide us evidence for these desired characteris-
tics. Our features are automatically extracted from
PANDICTIONARY and Google.
For the first criterion we restrict the transla-
tions to a set of high-coverage languages includ-
ing English, French, German, Spanish, Chinese,
Japanese, Arabic, Russian, Korean, Italian, and
Portuguese. Additionally, we include the lan-
guage as well as number of documents returned by
Google search of w as features for the classifier.
To detect if w is monosemous we add a feature
reflecting the degree of polysemy of w: the num-
ber of PANDICTIONARY senses thatw belongs to.
The higher this number the more polysemous w
is expected to be. We also include the number of
languages that have w in their vocabulary, thus,
adding a feature for the degree of homography.
PANDICTIONARY is arranged such that each
sense has an English source word. If the source
word is part of many senses but s
q
is much more
popular than others or s
q
is ordered before the
other senses then we can expect s
q
to be the dom-
inant sense for this word. We include features like
size of the sense and order of the sense.
Part of speech of s
q
is another feature. Finally
we also add the probability score that w is a trans-
lation of s
q
in our feature set.
3.2 Training the Classifier
To train our classifier we used Weka (Witten and
Frank, 2005) on a hand labeled dataset of 767 ran-
194
0 100 200 300 4000.
00
0.10
0.20
Number of Good Images Returned
Prec
ision IDIOMSWSW+GRSW+R
IDIOM SW SW+G SW+R R
Perc
enta
ge C
orrec
t
0
20
40
60
IDIOM SW SW+G SW+R R
Perc
enta
ge C
orrec
t
0
20
40
60
Figure 1: (a): Precision of images vs. the number of relevant images returned. IDIOM covers the maximum area. (b,c) The
percentage of senses for which at least one relevant result was returned, for (b) all senses and (c) for minor senses of the queries.
domly chosen word sense pairs (e.g., pair of ?pri-
mavera,? and ?the season spring?). We labeled a
pair as positive if googling the word returns at least
one good image for the sense in the top three. We
compared performance among a number of ma-
chine learning algorithms and found that Random
Forests (Breiman, 2001) performed the best over-
all with 69% classification accuracy using ten fold
cross validation versus 63% for Naive Bayes and
62% for SVMs. This high performance of Ran-
dom Forests mirrors other past experiments (Caru-
ana and Niculescu-Mizil, 2006).
Because of the ensemble nature of Random
Forests it is difficult to inspect the learned clas-
sifier for analysis. Still, anecdotal evidence sug-
gests that the classifier is able to learn an effective
model of good translations. We observe that it fa-
vors English whenever the English word is part of
one or few senses ? it picks out auction when the
query is ?sale? in the sense of ?act of putting up
for auction to highest bidder". In cases where En-
glish is more ambiguous it chooses a relatively less
ambiguous word in another language. It chooses
the French word ressort for finding ?spring? in the
sense of coil. For the query ?gift? we notice that it
does not choose the original query. This matches
our intuition, since gift has many homographs ?
the German word ?Gift? means poison or venom.
4 Experiments
Can querying translations instead of the original
query improve the quality of image search? If so,
then how much does our classifier help compared
to querying random translations? We also analyze
our results and study the variation of image qual-
ity along various dimensions, like part of speech,
abstractness/concreteness of the sense, and ambi-
guity of the original query.
As a comparison, we are interested in how ID-
IOM performs in relation to other methods for
querying Google Images. We compare IDIOM to
several methods. (1) Source Word (SW): Querying
with only the source word. This comparison func-
tions as our baseline. (2) Source Word + Gloss
(SW+G): Querying with the source word and the
gloss for the sense
1
. This method is one way to fo-
cus the source word towards the desired sense. (3)
Source Word + Random (SW+R): Querying with
three pairs of source word and a random transla-
tion. This is another natural way to extend the
baseline for the intended sense. (4) Random (R):
Querying with three random translations. This
tests the extent to which our classifier improves
our results compared to randomly choosing trans-
lations shown to the user in PANIMAGES.
We randomly select fifty English queries from
PANDICTIONARY and look up all senses contain-
ing these in PANDICTIONARY, resulting in a total
of 134 senses. These queries include short word
sequences (e.g., ?open sea?), mildly polysemous
queries like ?pan? (means Greek God and cooking
vessel) and highly polysemous ones like ?light?.
For each sense of each word, we query Google
Images with the query terms suggested by each
method and evaluate the top fifteen results. For
methods in which we have three queries, we eval-
uate the top five results for each query. We evalu-
ate a total of fifteen results because Google Images
fits fifteen images on each page for our screen size.
Figure 1(a) compares the precision of the five
methods with the number of good images re-
turned. We vary the number of images in con-
sideration from 1 to 15 to generate various points
in the graph. IDIOM outperforms the others by
wide margins overall producing a larger number of
good images and at higher precision. Surprisingly,
the closest competitor is the baseline method as
opposed to other methods that try to focus the
search towards the intended sense. This is prob-
ably because the additional words in the query (ei-
ther from gloss or a random translation) confuse
Google Images rather than focusing the search.
IDIOM covers 41% more area than SW. Overall
1
PANDICTIONARY provides a gloss (short explanation)
for each sense. E.g., a gloss for ?hero? is ?role model.?
195
1 sense 2 or 3 senses >3 senses
Perc
enta
ge C
orrec
t
02
04
06
08
0 IDIOMSWSW+GSW+RR
Noun Verb AdjectiveP
erce
ntag
e Co
rrect
0
20
40
60
80 IDIOMSWSW+GSW+RR
Concrete Abstract
Perc
enta
ge C
orrec
t
0
20
40
60
80 IDIOMSWSW+GSW+RR
Figure 2: The percentage of senses for which at least one relevant result was returned varied along several dimensions: (a)
polysemy of original query, and (b) part of speech of the sense, (c) abstractness/concreteness of the sense.
IDIOM produces 22% better images compared to
SW (389 vs 318).
We also observe that random translations return
much worse images than IDIOM suggesting that a
classifier is essential for high quality images.
Figure 1(b) compares the percentage of senses
for which at least one good result was returned in
the fifteen. Here IDIOM performs the best at 51%.
Each other method performs at about 40%. The re-
sults are statistically highly significant (p < 0.01).
Figure 1(c) compares the performance just on
the subset of the non-dominant senses of the query
words. All methods perform worse than in Figure
1(b) but IDIOM outperforms the others.
We also analyze our results across several di-
mensions. Figure 2(a) compares the performance
as a function of polysemy of the original query. As
expected, the disparity in methods is much more
for high polysemy queries. Most methods perform
well for the easy case of unambiguous queries.
Figure 2(b) compares along the different parts
of speech. For nouns and verbs, IDIOM returns the
best results. For adjectives, IDIOM and SW per-
form the best. Overall, nouns are the easiest for
finding images and we did not find much differ-
ence between verbs and adjectives.
Finally, Figure 2(c) reports how the methods
perform on abstract versus concrete queries. We
define a sense as abstract if it does not have a nat-
ural physical manifestation. For example, we clas-
sify ?nest? (a bird built structure) as concrete, and
?confirm? (to strengthen) as abstract. IDIOM per-
forms better than the other methods, but the results
vary massively between the two categories.
Overall, we find that our new system consis-
tently produces better results across the several di-
mensions and various metrics.
5 Related Work and Conclusions
Related Work: The popular paradigm for image
search is keyword-based, but it suffers due to pol-
ysemy and homography. An alternative paradigm
is content based (Datta et al, 2008), which is very
slow and works on simpler images. The field
of cross-lingual information retrieval (Ballesteros
and Croft, 1996) often performs translation-based
search. Other than PANIMAGES (which we out-
perform), no one to our knowledge has used this
for image search.
Conclusions: The recent development of PAN-
DICTIONARY (Mausam et al, 2009), a sense-
distinguished, massively multilingual dictionary,
enables a novel image search engine called ID-
IOM. We show that querying unambiguous trans-
lations of a sense produces images for 35% more
concepts compared to querying just the English
source word. In the process we learn a classi-
fier that predicts whether a given translation is a
good query for the intended sense or not. We
plan to release an image search website based
on IDIOM. In the future we wish to incorporate
knowledge from WordNet and cross-lingual links
in Wikipedia to increase IDIOM?s coverage beyond
the senses from PANDICTIONARY.
References
L. Ballesteros and B. Croft. 1996. Dictionary methods for
cross-lingual information retrieval. In DEXA Conference
on Database and Expert Systems Applications.
Dev Basu. 2009. How To Leverage Rich Me-
dia SEO for Small Businesses. In Search En-
gine Journal. http://www.searchenginejournal.com/rich -
media-small-business-seo/9580.
L. Breiman. 2001. Random forests. Machine Learning,
45(1):5?32.
R. Caruana and A. Niculescu-Mizil. 2006. An empiri-
cal comparison of supervised learning algorithms. In
ICML?06, pages 161?168.
R. Datta, D. Joshi, J. Li, and J. Wang. 2008. Image retrieval:
Ideas, influences, and trends of the new age. ACM Com-
puting Surveys, 40(2):1?60.
O. Etzioni, K. Reiter, S. Soderland, and M. Sammer. 2007.
Lexical translation with application to image search on the
Web. In Machine Translation Summit XI.
Peter Linsley. 2009. Google Image Search. In SMX West.
Mausam, S. Soderland, O. Etzioni, D. Weld, M. Skinner, and
J. Bilmes. 2009. Compiling a massive, multilingual dic-
tionary via probabilistic inference. In ACL?09.
I. Witten and E. Frank. 2005. Data Mining: Practical Ma-
chine Learning Tools and Techniques. Morgan Kaufmann.
196
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 503?513,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Instance-Driven Attachment of Semantic Annotations
over Conceptual Hierarchies
Janara Christensen?
University of Washington
Seattle, Washington 98195
janara@cs.washington.edu
Marius Pas?ca
Google Inc.
Mountain View, California 94043
mars@google.com
Abstract
Whether automatically extracted or human
generated, open-domain factual knowledge
is often available in the form of semantic
annotations (e.g., composed-by) that take
one or more specific instances (e.g., rhap-
sody in blue, george gershwin) as their ar-
guments. This paper introduces a method
for converting flat sets of instance-level
annotations into hierarchically organized,
concept-level annotations, which capture
not only the broad semantics of the desired
arguments (e.g., ?People? rather than ?Loca-
tions?), but also the correct level of general-
ity (e.g., ?Composers? rather than ?People?,
or ?Jazz Composers?). The method refrains
from encoding features specific to a partic-
ular domain or annotation, to ensure imme-
diate applicability to new, previously un-
seen annotations. Over a gold standard of
semantic annotations and concepts that best
capture their arguments, the method sub-
stantially outperforms three baselines, on
average, computing concepts that are less
than one step in the hierarchy away from
the corresponding gold standard concepts.
1 Introduction
Background: Knowledge about the world can
be thought of as semantic assertions or anno-
tations, at two levels of granularity: instance
level (e.g., rhapsody in blue, tristan und isolde,
george gershwin, richard wagner) and concept
level (e.g., ?Musical Compositions?, ?Works of
Art?, ?Composers?). Instance-level annotations
correspond to factual knowledge that can be
found in repositories extracted automatically from
text (Banko et al 2007; Wu and Weld, 2010)
?Contributions made during an internship at Google.
or manually created within encyclopedic re-
sources (Remy, 2002). Such facts could state, for
instance, that rhapsody in blue was composed-
by george gershwin, or that tristan und isolde
was composed-by richard wagner. In compar-
ison, concept-level annotations more concisely
and effectively capture the underlying semantics
of the annotations by identifying the concepts cor-
responding to the arguments, e.g., ?Musical Com-
positions? are composed-by ?Composers?.
The frequent occurrence of instances, relative
to more abstract concepts, in Web documents and
popular Web search queries (Barr et al 2008;
Li, 2010), is both an asset and a liability from
the point of view of knowledge acquisition. On
one hand, it makes instance-level annotations rel-
atively easy to find, either from manually created
resources (Remy, 2002; Bollacker et al 2008),
or extracted automatically from text (Banko et
al., 2007). On the other hand, it makes concept-
level annotations more difficult to acquire di-
rectly. While ?Rhapsody in Blue was composed
by George Gershwin [..]? may occur in some
form within Web documents, the more abstract
?Musical compositions are composed by musi-
cians [..]? is unlikely to occur. A more practical
approach to collecting concept-level annotations
is to indirectly derive them from already plenti-
ful instance-level annotations, effectively distill-
ing factual knowledge into more abstract, concise
and generalizable knowledge.
Contributions: This paper introduces a method
for converting flat sets of specific, instance-
level annotations into hierarchically organized,
concept-level annotations. As illustrated in Fig-
ure 1, the resulting annotations must capture not
just the broad semantics of the desired arguments
(e.g., ?People? rather than ?Locations? or ?Prod-
503
People
Composers Musicians
Composers by genre Cellists Singers
Baroque Composers Jazz Composers
Annotations
Conceptual hierarchy
composed?by    lives?in    instrument?played    sung?by
Figure 1: Hierarchical Semantic Annotations: The
attachment of semantic annotations (e.g., composed-
by) into a conceptual hierarchy, a portion of which is
shown in the diagram, requires the identification of the
correct concept at the correct level of generality (e.g.,
?Composers? rather than ?Jazz Composers? or ?Peo-
ple?, for the right argument of composed-by).
ucts?, as the right argument of the annotation
composed-by), but actually identify the concepts
at the correct level of generality/specificity (e.g.,
?Composers? rather than ?Artists? or ?Jazz Com-
posers?) in the underlying conceptual hierarchy.
To ensure portability to new, previously unseen
annotations, the proposed method avoids encod-
ing features specific to a particular domain or an-
notation. In particular, the use of annotations? la-
bels (composed-by) as lexical features might be
tempting, but would anchor the annotation model
to that particular annotation. Instead, the method
relies only on features that generalize across an-
notations. Over a gold standard of semantic anno-
tations and concepts that best capture their argu-
ments, the method substantially outperforms three
baseline methods. On average, the method com-
putes concepts that are less than one step in the
hierarchy away from the corresponding gold stan-
dard concepts of the various annotations.
2 Hierarchical Semantic Annotations
2.1 Task Description
Data Sources: The computation of hierarchical
semantic annotations relies on the following data
sources:
? a target annotation r (e.g., acted-in) that takes
M arguments;
? N annotations I={<i1j , . . . , iMj>}Nj=1 of
r at instance level, e.g., {<leonardo dicaprio,
inception>, <milla jovovich, fifth element>} (in
this example, M=2);
? mappings {i?c} from instances to con-
cepts to which they belong, e.g., milla jovovich
? ?American Actors?, milla jovovich ? ?People
from Kiev?, milla jovovich? ?Models?;
? mappings {cs?cg} from more specific con-
cepts to more general concepts, as encoded in a
hierarchy H , e.g., ?American Actors???Actors?,
?People from Kiev???People from Ukraine?,
?Actors???Entertainers?.
Thus, the main inputs are the conceptual hi-
erarchy H , and the instance-level annotations I .
The hierarchy contains instance-to-concept map-
pings, as well as specific-to-general concept map-
pings. Via transitivity, instances (milla jovovich)
and concepts (?American Actors?) may be im-
mediate children of more general concepts (?Ac-
tors?), or transitive descendants of more general
concepts (?Entertainers?). The hierarchy is not re-
quired to be a tree; in particular, a concept may
have multiple parent concepts. The instance-level
annotations may be created collaboratively by hu-
man contributors, or extracted automatically from
Web documents or some other data source.
Goal: Given the data sources, the goal is to de-
termine to which concept c in the hierarchy H the
arguments of the target concept-level annotation
r should be attached. While the left argument of
acted-in could attach to ?American Actors?, ?Peo-
ple from Kiev?, ?Entertainers? or ?People?, it is
best attached to the concept ?Actors?. The goal
is to select the concept c that most appropriately
generalizes across the instances. Over the set I
of instance-level annotations, selecting a method
for this goal can be thought of as a minimization
problem. The metric to be minimized is the sum
of the distances between each predicted concept c
and the correct concept cgold, where the distance
is the number of edges between c and cgold in H .
Intuitions and Challenges: Given instances such
as milla jovovich that instantiate an argument of
an annotation like acted-in, the conceptual hierar-
chy can be used to propagate the annotation up-
wards, from instances to their concepts, then in
turn further upwards to more general concepts.
The best concept would be one of the many can-
didate concepts reached during propagation. In-
tuitively, when compared to other candidate con-
cepts, a higher proportion of the descendant in-
stances of the best concept should instantiate (or
match) the annotation. At the same time, rela-
tive to other candidate concepts, the best concept
should have more descendant instances.
While the intuitions seem clear, their inclu-
sion in a working method faces a series of prac-
tical challenges. First, the data sources may be
noisy. One form of noise is missing or erroneous
504
Conceptual hierarchy
Entities
Locations People
Singers Actors
American Actors English Actors
Instance-level annotations
acted-in(leonardo dicaprio, inception)
acted-in(milla jovovich, fifth element)
acted-in(judy dench, casino royale)
acted-in(colin firth, the king?s speech)
Instance to concept mappings
leonardo dicaprio: American Actors
milla jovovich: American Actors
judy dench: English Actors
colin firth: English Actors
Candidate concepts
Entities
People
Actors
American Actors
English Actors
Raw statistics
Entities, 4, 0.01 . . .
People, 3, 0.1 . . .
Actors, 2, 0.7 . . .
American Actors, 1, 0.9 . . .
English Actors, 1, 0.8 . . .
Features Depth, Instance Percent . . .
Query logs
fifth element actors
fifth element costumes
inception quotes
out of africa actors
the king?s speech oscars
Classified data
0, People-Actors, 3/2, 0.1/0.7 . . .
1, Actors-People, 2/3, 0.7/0.1 . . .
1, Actors-American Actors, 2/1, 0.7/0.9 . . .
0, American Actors-Actors, 1/2, 0.9/0.7 . . .
.
.
.
Training/testing data
People-Actors, 3/2, 0.1/0.7 . . .
Actors-People, 2/3, 0.7/0.1 . . .
Actors-American Actors, 2/1, 0.7/0.9 . . .
American Actors-Actors, 1/2, 0.9/0.7 . . .
.
.
.
Ranked data (for Concept-level annotations)
4, Actors
3, People
2, American Actors
1, English Actors
0, Entities
Concept-level annotations
acted-in(Actors, ?)
Figure 2: Method Overview: Inferring concept-level annotations from instance-level annotations.
instance-level annotations, which may artificially
skew the distribution of matching instances to-
wards a less than optimal region in the hierarchy.
If the input annotations for acted-in are available
almost exhaustively for all descendant instances
of ?American Actors?, and are available for only a
few of the descendant instances of ?Belgian Ac-
tors?, ?Italian Actors? etc., then the distribution
over the hierarchy may incorrectly suggest that
the left argument of acted-in is ?American Actors?
rather than the more general ?Actors?. In another
example, if virtually all instances that instantiate
the left argument of the annotation won-award are
mapped to the concept ?Award Winning Actors?,
then it would be difficult to distinguish ?Award
Winning Actors? from the more general ?Actors?
or ?People?, as best concept to be computed for
the annotation. Another type of noise is missing
or erroneous edges in the hierarchy, which could
artificially direct propagation towards irrelevant
regions of the hierarchy, or prevent propagation
from even reaching relevant regions of the hier-
archy. For example, if the hierarchy incorrectly
maps ?Actors? to ?Entertainment?, then ?Entertain-
ment? and its ancestor concepts incorrectly be-
come candidate concepts during propagation for
the left argument of acted-in. Conversely, if miss-
ing edges caused ?Actors? to not have any children
in the hierarchy, then ?Actors? would not even be
reached and considered as a candidate concept
during propagation.
Second, to apply evidence collected from some
annotations to a new annotation, the evidence
must generalize across annotations. However,
collected evidence or statistics may vary widely
across annotations. Observing that 90% of all de-
scendant instances of the concept ?Actors? match
an annotation acted-in constitutes strong evidence
that ?Actors? is a good concept for acted-in. In
contrast, observing that only 0.09% of all descen-
dant instances of the concept ?Football Teams?
match won-super-bowl should not be as strong
negative evidence as the percentage suggests.
2.2 Inferring Concept-Level Annotations
Determining Candidate Concepts: As illus-
trated in the left part of Figure 2, the first step to-
wards inferring concept-level from instance-level
annotations is to propagate the instances that in-
stantiate a particular argument of the annota-
tion, upwards in the hierarchy. Starting from the
left arguments of the annotation acted-in, namely
leonardo dicaprio, milla jovovich etc., the prop-
agation reaches their parent concepts ?American
Actors?, ?English Actors?, then their parent and
ancestor concepts ?Actors?, ?People?, ?Entities?
etc. The concepts reached during upward prop-
agation become candidate concepts. In subse-
quent steps, the candidates are modeled, scored
and ranked such that ideally the best concept is
ranked at the top.
Ranking Candidate Concepts: The identifica-
505
tion of a ranking function is cast as a semi-
supervised learning problem. Given the cor-
rect (gold) concept of an annotation, it would be
tempting to employ binary classification directly,
by marking the correct concept as a positive ex-
ample, and all other candidate concepts as nega-
tive examples. Unfortunately, this would produce
a highly imbalanced training set, with thousands
of negative examples and, more importantly, with
only one positive example. Another disadvan-
tage of using binary classification directly is that
it is difficult to capture the preference for concepts
closer in the hierarchy to the correct concept, over
concepts many edges away. Finally, the absolute
values of the features that might be employed may
be comparable within an annotation, but incompa-
rable across annotations, which reduces the porta-
bility of the resulting model to new annotations.
To address the above issues, the ranking func-
tion proposed does not construct training exam-
ples from raw features collected for each indi-
vidual candidate concept. Instead, it constructs
training examples from pairwise comparisons of
a candidate concept with another candidate con-
cept. Concretely, a pairwise comparison is la-
beled as a positive example if the first concept is
closer to the correct concept than the second, or as
negative otherwise. The pairwise formulation has
three immediate advantages. First, it accomodates
the preference for concepts closer to the gold con-
cept. Second, the pairwise formulation produces
a larger, more balanced training set. Third, deci-
sions of whether the first concept being compared
is more relevant than the second are more likely to
generalize across annotations, than absolute deci-
sions of whether (and how much) a particular con-
cept is relevant for a given annotation.
Compiling Ranking Features: The features are
grouped into four categories: (A) annotation co-
occurrence features, (B) concept features, (C) ar-
gument co-occurrence features, and (D) combina-
tion features, as described below.
(A) Annotation Co-occurrence Features: The
annotation co-occurrence features emphasize how
well an annotation applies to a concept. These
features include (1) MATCHED INSTANCES the
number of descendant instances of the concept
that appear with the annotation, (2) INSTANCE
PERCENT the percentage of matched instances in
the concept, (3) MORE THAN THREE MATCHING
INSTANCES and (4) MORE THAN TEN MATCH-
ING INSTANCES, which indicate when the match-
ing descendant instances might be noise.
Also in this category are features that relay in-
formation about the candidate concept?s children
concepts. These features include (1) MATCHED
CHILDREN the number of child concepts con-
taining at least one matching instance, (2) CHIL-
DREN PERCENT the percentage of child concepts
with at least one matching instance, (3) AVG IN-
STANCE PERCENT CHILDREN the average per-
centage of matching descendant instances of the
child concepts, and (4) INSTANCE PERCENT TO
INSTANCE PERCENT CHILDREN the ratio be-
tween INSTANCE PERCENT and AVERAGE IN-
STANCE PERCENT OF CHILDREN. The last fea-
ture is meant to capture dramatic changes in per-
centages when moving in the hierarchy from child
concepts to the candidate concept in question.
(B) Concept Features: Concept features ap-
proximate the generality of the concepts: (1)
NUM INSTANCES the number of descendant in-
stances of the concept, (2) NUM CHILDREN the
number of child concepts, and (3) DEPTH the dis-
tance to the concept?s farthest descendant.
(C) Argument Co-occurrence Features: The ar-
gument co-occurrence features model the likeli-
hood that an annotation applies to a concept by
looking at co-occurrences with another argument
of the same annotation. Intuitively, if a con-
cept representing one argument has a high co-
occurrence with an instance that is some other ar-
gument, a relationship more likely exists between
members of the concept and the instance. For ex-
ample, given acted-in, ?Actors? is likely to have a
higher co-occurrence with casablanca than ?Peo-
ple? is. These features are generated from a set of
Web queries. Therefore, the collected values are
likely to be affected by different noise than that
present in the original dataset. For every concept
and instance pair from the arguments of a given
annotation, they feature the number of times each
of the tokens in the concept appears in the same
query with each of the tokens in the instance,
normalizing to the respective number of tokens.
The procedure generates, for each candidate con-
cept, an average co-occurrence score (AVG CO-
OCCURRENCE) and a total co-occurrence score
(TOTAL CO-OCCURRENCE) over all instances the
concept is paired with.
(D) Combination Features: The last group
of features are combinations of the above fea-
tures: (1) DEPTH, INSTANCE PERCENT which is
DEPTH multiplied by INSTANCE PERCENT, and
506
Concept Distance Match Total Match Total AvgInst Depth Avg Total
ToCorrect Inst Inst Child Child PercOfChild Cooccur Cooccur
People 4 36512 879423 22 29 4% 14 0.67 33506
Actors 0 29101 54420 6 10 32% 6 2.08 99971
English Actors 2 3091 5922 3 4 37% 3 2.75 28378
Labeled Concept Pair Annotation Co-occurrence Concept Arg Co-occurrence Combination
Features Features Features Features
Concept Label Match Inst Match Child AvgInst Num Num Depth Avg Total Depth DepthInst
Pair Inst Perc Child Perc PercChild Inst Child Cooccur Cooccur InstPerc PercChild
People-Actors 0 1.25 0.08 3.67 1.26 0.13 1.25 3.67 2.33 0.32 0.34 0.18 0.66
Actors-People 1 0.8 12.88 0.27 0.79 7.65 0.8 0.27 0.43 3.11 2.98 5.52 1.51
Actors-English Actors 1 9.41 1.02 2.0 0.8 0.87 9.41 2.0 2.0 0.76 3.52 2.05 4.1
English Actors-Actors 0 0.11 0.98 0.5 1.25 1.15 0.11 0.5 0.5 1.32 0.28 0.49 0.24
English Actors-People 1 0.08 12.57 0.14 0.99 8.82 0.08 0.14 0.21 4.12 0.85 2.69 0.37
People-English Actors 0 11.81 0.08 7.33 1.01 0.11 11.81 7.33 4.67 0.24 1.18 0.37 2.72
Table 1: Training/Testing Examples: The top table shows examples of raw statistics gathered for three candidate
concepts for the left argument of the annotation acted-in. The second table shows the training/testing examples
generated from these concepts and statistics. Each example represents a pair of concepts which is labeled positive
if the first concept is closer to the correct concept than the second concept. Features shown here are the ratio
between a statistic for the first concept and a statistic for the second (e.g. DEPTH for Actors-English Actors is 2
as ?Actors? has depth of 6 and ?English Actors? has depth of 3). Some features omitted due to space constraints.
(2) DEPTH, INSTANCE PERCENT, CHILDREN,
which is the DEPTH multipled by the INSTANCE
PERCENT multiplied by MATCHED CHILDREN.
Both these features seek to balance the perceived
relevance of an annotation to a candidate concept,
with the generality of the candidate concept.
Generating Learning Examples: For a given
annotation, the ranking features described so far
are computed for each candidate concept (e.g.,
?Movie Actors?, ?Models?, ?Actors?). However,
the actual training and testing examples are gener-
ated for pairs of candidate concepts (e.g., <?Film
Actors?, ?Models?>, <?Film Actors?, ?Actors?>,
<?Models, ?Actors?>). A training example rep-
resents a comparison between two candidate con-
cepts, and specifies which of the two is more rele-
vant. To create training and testing examples, the
values of the features of the first concept in the
pair are respectively combined with the values of
the features of the second concept in the pair to
produce values corresponding to the entire pair.
Following classification of testing examples,
concepts are ranked according to the number of
other concepts which they are classified as more
relevant than. Table 1 shows examples of train-
ing/testing data.
3 Experimental Setting
3.1 Data Sources
Conceptual Hierarchy: The experiments com-
pute concept-level annotations relative to a con-
ceptual hierarchy derived automatically from the
Wikipedia (Remy, 2002) category network, as de-
scribed in (Ponzetto and Navigli, 2009). The hi-
erarchy filters out edges (e.g., from ?British Film
Actors? to ?Cinema of the United Kingdom?) from
the Wikipedia category network that do not corre-
spond to IsA relations. A concept in the hierarchy
is a Wikipedia category (e.g., ?English Film Ac-
tors?) that has zero or more Wikipedia categories
as child concepts, and zero or more Wikipedia
categories (e.g., ?English People by Occupation?,
?British Film Actors?) as parent concepts. Each
concept in the hierarchy has zero or more in-
stances, which are the Wikipedia articles listed (in
Wikipedia) under the respective categories (e.g.,
colin firth is an instance of ?English Actors?).
Instance-Level Annotations: The experiments
exploit a set of binary instance-level annotations
(e.g., acted-in, composed) among Wikipedia in-
stances, as available in Freebase (Bollacker et
al., 2008). The annotation is a Freebase prop-
erty (e.g., /music/composition/composer). Inter-
nally, the left and right arguments are Freebase
topic identifiers mapped to their corresponding
Wikipedia articles (e.g., /m/03f4k mapped to the
Wikipedia article on george gershwin). In this pa-
per, the derived annotations and instances are dis-
played in a shorter, more readable form for con-
ciseness and clarity. As features do not use the
label of the annotation, labels are never used in
the experiments and evaluation.
507
Web Search Queries: The argument co-
occurrence features described above are com-
puted over a set of around 100 million
anonymized Web search queries from 2010.
3.2 Experimental Runs
The experimental runs exploit ranking features
described in the previous section, employing:
? one of three learning algorithms: naive Bayes
(NAIVEBAYES), maximum entropy (MAXENT),
or perceptron (PERCEPTRON) (Mitchell, 1997),
chosen for their scalability to larger datasets via
distributed implementations.
? one of three ways of combining the values
of features collected for individual candidate con-
cepts into values of features for pairs of candidate
concepts: the raw ratio of the values of the re-
spective features of the two concepts (0 when the
denominator is 0); the ratio scaled to the interval
[0, 1]; or a binary value indicating which of the
values is larger.
For completeness, the experiments include
three additional, baseline runs. Each baseline
computes scores for all candidate concepts based
on the respective metric; then candidate concepts
are ranked in decreasing order of their scores. The
baselines metrics are:
? INSTPERCENT ranks candidate concepts by
the percentage of matched instances that are de-
scendants of the concept. It emphasizes concepts
which are ?proven? to belong to the annotation;
? ENTROPY ranks candidate concepts by the
entropy (Shannon, 1948) of the proportion of
matched descendant instances of the concept;
? AVGDEPTH ranks candidate concepts by
their distances to half of the maximum hierarchy
height, emphasizing a balance of generality and
specificity.
3.3 Evaluation Procedure
Gold Standard of Concept-Level Annotations:
A random, weighted sample of 200 annotation la-
bels (e.g., corresponding to composed-by, play-
instrument) is selected, out of the set of labels
of all instance-level annotations collected from
Freebase. During sampling, the weights are the
counts of distinct instance-level annotations (e.g.,
<rhapsody in blue, george gershwin>) avail-
able for the label. The arguments of the anno-
tation labels are then manually annotated with
a gold concept, which is the category from the
Wikipedia hierarchy that best captures their se-
mantics. The manual annotation is carried out
independently by two human judges, who then
verify each other?s work and discard inconsisten-
cies. For example, the gold concept of the left
argument of composed-by is annotated to be the
Wikipedia category ?Musical Compositions?. In
the process, some annotation labels are discarded,
when (a) it is not clear what concept captures an
argument (e.g., for the right argument of function-
of-building), or (b) more than 5000 candidate con-
cepts are available via propagation for one of the
arguments, which would cause too many train-
ing or testing examples to be generated via con-
cept pairs, and slow down the experiments. The
retained 139 annotation labels, whose arguments
have been labeled with their respective gold con-
cepts, form the gold standard for the experiments.
More precisely, an entry in the resulting gold stan-
dard consists of an annotation label, one of its
arguments being considered (left or right), and
a gold concept that best captures that argument.
The set of annotation labels from the gold stan-
dard is quite diverse and covers many domains of
potential interest, e.g., has-company(?Industries?,
?Companies?), written-by(?Films?, ?Screenwrit-
ers?), member-of (?Politicians?,?Political Parties?),
or part-of-movement(?Artists?, ?Art Movements?).
Evaluation Metric: Following previous work
on selectional preferences (Kozareva and Hovy,
2010; Ritter et al 2010), each entry in the gold
standard, (i.e., each argument for a given annota-
tion) is evaluated separately. Experimental runs
compute a ranked list of candidate concepts for
each entry in the gold standard. In theory, a com-
puted candidate concept is better if it is closer
semantically to the gold concept. In practice,
the accuracy of a ranked list of candidate con-
cepts, relative to the gold concept of the anno-
tation label, is measured by two scoring metrics
that correspond to the mean reciprocal rank score
(MRR) (Voorhees and Tice, 2000) and a modifi-
cation of it (DRR) (Pas?ca and Alfonseca, 2009):
MRR =
1
N
N?
i=1
max
rank
1
ranki
N is the number of annotations and ranki is the
rank of the gold concept in the returned list for
MRR. An annotation ai receives no credit for
MRR if the gold concept does not appear in the
corresponding ranked list.
DRR =
1
N
N?
i=1
max
rank
1
ranki ? (1 + Len)
For DRR, ranki is the rank of a candidate con-
cept in the returned list and Len is the length of
508
Annotation (Number of Candidate Concepts) Examples of Instances Top Ranked Concepts
Composers compose Musical Compositions (3038) aaron copland; black sabbath Music by Nationality; Composers; Classical
Composers
Musical Compositions composed-by Composers (1734) we are the champions; yor-
ckscher marsch
Musical Compositions; Compositions by
Composer; Classical Music
Foods contain Nutrients (1112) acca sellowiana; lasagna Foods; Edible Plants; Food Ingredients
Organizations has-boardmember People (3401) conocophillips; spence school Companies by Stock Exchange; Companies
Listed on the NYSE; Companies
Educational Organizations has-graduate Alumni (4072) air force institute of technology;
deering high school
Education by Country; Schools by Country;
Universities and Colleges by Country
Television Actors guest-role Fictional Characters (4823) melanie griffith; patti laBelle Television Actors by Nationality; Actors;
American Actors
Musical Groups has-member Musicians (2287) steroid maximus; u2 Musical Groups; Musical Groups by Genre;
Musical Groups by Nationality
Record Labels represent Musician (920) columbia records; vandit Record Labels; Record Labels by Country;
Record Labels by Genre
Awards awarded-to People (458) academy award for best original
song; erasmus prize
Film Awards; Awards; Grammy Awards
Foods contain Nutrients (177) lycopene; glutamic acid Carboxylic Acids ; Acids; Essential Nutrients
Architects design Buildings and Structures (4811) 20 times square; berkeley build-
ing
Buildings and Structures; Buildings and Struc-
tures by Architect; Houses by Country
People died-from Causes of Death (577) malaria; skiing Diseases; Infectious Diseases; Causes of
Death
Art Directors direct Films (1265) batman begins; the lion king Films; Films by Director; Film
Episodes guest-star Television Actors (1067) amy poehler; david caruso Television Actors by Nationality; Actors;
American Actors
Television Network has-tv-show Television Series (2492) george of the jungle; great expec-
tations
Television Series by Network; Television Se-
ries; Television Series by Genre
Musicians play Musical Instruments (423) accordion; tubular bell Musical Instruments; Musical Instruments by
Nationality; Percussion Instruments
Politicians member-of Political Parties (938) independent moralizing front;
national coalition party
Political Parties; Political Parties by Country;
Political Parties by Ideology
Table 2: Concepts Computed for Gold-Standard Annotations: Examples of entries from the gold standard and
counts of candidate concepts (Wikipedia categories) reached from upward propagation of instances (Wikipedia
instances). The target gold concept is shown in bold. Also shown are examples of Wikipedia instances, and the
top concepts computed by the best-performing learning algorithm for the respective gold concepts.
the minimum path in the hierarchy between the
concept and the gold concept. Len is minimum
(0) if the candidate concept is the same as the gold
standard concept. A given annotation ai receives
no credit for DRR if no path is found between the
returned concepts and the gold concept.
As an illustration, for a single annotation, the
right argument of composed-by, the ranked list
of concepts returned by an experimental may
be [?Symphonies by Anton Bruckner?, ?Sym-
phonies by Joseph Haydn?, ?Symphonies by Gus-
tav Mahler?, ?Musical Compositions?, ..], with the
gold concept being ?Musical Compositions?. The
length of the path between ?Symphonies by An-
ton Bruckner? etc. and ?Musical Compositions? is
2 (via ?Symphonies?). Therefore, the MRR score
would be 0.25 (given by the fourth element of
the ranked list), whereas the DRR score would be
0.33 (given by the first element of the ranked list).
MRR and DRR are computed in five-fold cross
validation. Concretely, the gold standard is split
into five folds such that the sets of annotation la-
bels in each fold are disjoint. Thus, none of
the annotation labels in testing appears in train-
ing. This restriction makes the evaluation more
rigurous and conservative as it actually assesses
the extent the models learned are applicable to
new, previously unseen annotation labels. If
this restriction were relaxed, the baselines would
preform equivalently as they do not depend on
the training data, but the learned methods would
likely do better.
4 Evaluation Results
4.1 Quantitative Results
Conceptual Hierarchy: The conceptual hierar-
chy contains 108,810 Wikipedia categories, and
its maximum depth, measured as the distance
from a concept to its farthest descendant, is 16.
Candidate Concepts: On average, for the gold
standard, the method propagates a given annota-
tion from instances to 1,525 candidate concepts,
from which the single best concept must be deter-
mined. The left part of Table 2 illustrates the num-
ber of candidate concepts reached during propa-
gation for a sample of annotations.
509
Experimental Run Accuracy
N=1 N=20
MRR DRR MRR DRR
?With raw-ratio features:
NAIVEBAYES 0.021 0.180 0.054 0.222
MAXENT 0.029 0.168 0.045 0.208
PERCEPTRON 0.029 0.176 0.045 0.216
?With scaled-ratio features:
NAIVEBAYES 0.050 0.170 0.112 0.243
MAXENT 0.245 0.456 0.430 0.513
PERCEPTRON 0.245 0.391 0.367 0.461
?With binary features:
NAIVEBAYES 0.115 0.297 0.224 0.361
MAXENT 0.165 0.390 0.293 0.441
PERCEPTRON 0.180 0.332 0.330 0.429
? For baselines:
INSTPERCENT 0.029 0.173 0.045 0.224
ENTROPY 0.000 0.110 0.007 0.136
AVGDEPTH 0.007 0.018 0.028 0.045
Table 3: Precision Results: Accuracy of ranked lists
of concepts (Wikipedia categories) computed by var-
ious runs, as an average over the gold standard of
concept-level annotations, considering the top N can-
didate concepts computed for each gold standard entry.
4.2 Qualitative Results
Precision: Table 3 compares the precision of the
ranked lists of candidate concepts produced by the
experimental runs. The MRR and DRR scores in
the table consider either at most 20 of the concepts
in the ranked list computed by a given experimen-
tal run, or only the first, top ranked computed con-
cept. Note that, in the latter case, the MRR and
DRR scores are equivalent to precision@1 scores.
Several conclusions can be drawn from the re-
sults. First, as expected by definition of the
scoring metrics, DRR scores are higher than the
stricter MRR scores, as they give partial credit
to concepts that, while not identical to the gold
concepts, are still close approximations. This is
particularly noticeable for the runs MAXENT and
PERCEPTRON with raw-ratio features (4.6 and
4.8 times higher respectively). Second, among
the baselines, INSTPERCENT is the most accu-
rate, with the computed concepts identifying the
gold concept strictly at rank 22 on average (for
an MRR score 0.045), and loosely at an aver-
age of 4 steps away from the gold concept (for
a DRR score of 0.224). Third, the accuracy of
the learning algorithms varies with how the pair-
wise feature values are combined. Overall, raw-
ratio feature values perform the worst, and scaled-
ratio the best, with binary in-between. Fourth,
the scores of the best experimental run, MAXENT
with scaled-ratio features, are 0.430 (MRR) and
0.513 (DRR) over the top 20 computed concepts,
and 0.245 (MRR) and 0.456 (DRR) when consid-
ering only the first concept. These scores corre-
spond to the ranked list being less than one step
away in the hierarchy. The very first computed
concept exactly matches the gold concept in about
one in four cases, and is slightly more than one
step away from it. In comparison, the very first
concept computed by the best baseline matches
the gold concept in about one in 35 cases (0.029
MRR), and is about 6 steps away (0.173 DRR).
The accuracies of the various learning algorithms
(not shown) were also measured and correlated
roughly with the MRR and DRR scores.
Discussion: The baseline runs INSTPERCENT
and ENTROPY produce categories that are far
too specific. For the gold annotation composed-
by(?Composers?, ?Musical Compositions?), INST-
PERCENT produces ?Scottish Flautists? for the left
argument and ?Operas by Ernest Reyer? for the
right. AVGDEPTH does not suffer from over-
specification, but often produces concepts that
have been reached via propagation, yet are not
close to the gold concept. For composed-by,
AVGDEPTH produces ?Film? for the left argument
and ?History by Region? for the right.
4.3 Error Analysis
The right part of Table 2 provides a more de-
tailed view into the best performing experimental
run, showing actual ranked lists of concepts pro-
duced for a sample of the gold standard entries
by MAXENT with scaled-ratio. A separate analy-
sis of the results indicates that the most common
cause of errors is noise in the conceptual hier-
archy, in the form of unbalanced instance-level
annotations and missing hierarchy edges. Un-
balanced annotations are annotations where cer-
tain subtrees of the hierarchy are artificially more
populated than other subtrees. For the left argu-
ment of the annotation has-profession, 0.05% of
?New York Politicians? are matched but 70% of
?Bushrangers? are matched. Such imbalances may
be inherent to how annotations are added to Free-
base: different human contributors may add new
annotations to particular portions of Freebase, but
miss other relevant portions.
The results are also affected by missing edges
in the hierarchy. Of the more than 100K con-
cepts in the hierarchy, 3479 are roots of subhier-
archies that are mutually disconnected. Exam-
ples are ?People by Region?, ?Shades of Red?, and
510
?Members of the Parliament of Northern Ireland?,
all of which should have parents in the hierarchy.
If a few edges are missing in a particular region
of the hierarchy, the method can recover, but if so
many edges are missing that a gold concept has
very few descendants, then propagation can be
substantially affected. In the worst case, the gold
concept becomes disconnected, and thus will be
missing from the set of candidate concepts com-
piled during propagation. For example, for the
annotation team-color(?Sports Clubs?, ?Colors?),
the only descendant concept of ?Colors? in the hi-
erarchy is ?Horse Coat Colors?, meaning that the
gold concept ?Colors? is not reached during prop-
agation from instances upwards in the hierarchy.
5 Related Work
Similar to the task of attaching a semantic anno-
tation to the concept in a hierarchy that has the
best level of generality is the task of finding se-
lectional preferences for relations. Most relevant
to this paper is work that seeks to find the appro-
priate concept in a hierarchy for an argument of
a specific relation (Ribas, 1995; McCarthy, 1997;
Li and Abe, 1998). Li and Abe (1998) address
this problem by attempting to identify the best tree
cut in a hierarchy for an argument of a given verb.
They use the minimum description length princi-
ple to select a set of concepts from a hierarchy to
represent the selectional preferences. This work
makes several limiting assumptions including that
the hierarchy is a tree, and every instance belongs
to just one concept. Clark and Weir (2002) inves-
tigate the task of generalizing a single relation-
concept pair. A relation is propagated up a hier-
archy until a chi-square test determines the differ-
ence between the probability of the child and par-
ent concepts to be significant where the probabili-
ties are relation-concept frequencies. This method
has no direct translation to the task discussed here;
it is unclear how to choose the correct concept if
instances generalize to different concepts.
In other research on selectional preferences,
Pantel et al(2007), Kozareva and Hovy (2010)
and Ritter et al(2010) focus on generating ad-
missible arguments for relations, and Erk (2007)
and Bergsma et al(2008) investigate classifying
a relation-instance pair as plausible or not.
Important to this paper is the Wikipedia cate-
gory network (Remy, 2002) and work on refin-
ing it. Ponzetto and Navigli (2009) disambiguate
Wikipedia categories by using WordNet synsets
and use this semantic information to construct a
taxonomy. The resulting taxonomy is the concep-
tual hierarchy used in the evaluation.
Another related area of work is the discovery of
relations between concepts. Nastase and Strube
(2008) use Wikipedia category names and cate-
gory structure to generate a set of relations be-
tween concepts. Yan et al(2009) discover re-
lations between Wikipedia concepts via deep lin-
guistic information and Web frequency informa-
tion. Mohamed et al(2011) generate candi-
date relations by coclustering text contexts for ev-
ery pair of concepts in a hierarchy. In a sense,
this area of research is complementary to that dis-
cussed in this paper. These methods induce new
relations, and the proposed method can be used
to find appropriate levels of generalization for the
arguments of any given relation.
6 Conclusions
This paper introduces a method to convert flat sets
of instance-level annotations to hierarchically or-
ganized, concept-level annotations. The method
determines the appropriate concept for a given se-
mantic annotation in three stages. First, it propa-
gates annotations upwards in the hierarchy, form-
ing a set of candidate concepts. Second, it classi-
fies each candidate concept as more or less appro-
priate than each other candidate concept within an
annotation. Third, it ranks candidate concepts by
the number of other concepts relative to which it
is classified as more appropriate. Because the fea-
tures are comparisons between concepts within a
single semantic annotation, rather than consider-
ations of individual concepts, the method is able
to generalize across annotations, and can thus be
applied to new, previously unseen annotations.
Experiments demonstrate that, on average, the
method is able to identify the concept of a given
annotation?s argument within one hierarchy edge
of the gold concept.
The proposed method can take advantage of
existing work on open-domain information ex-
traction. The output of such work is usually
instance-level annotations, although often at sur-
face level (non-disambiguated arguments) rather
than semantic level (disambiguated arguments).
After argument disambiguation (e.g., (Dredze et
al., 2010)), the annotations can be used as input
to determining concept-level annotations. Thus,
the method has the potential to generalize any
existing database of instance-level annotations to
concept-level annotations.
511
References
Michele Banko, Michael Cafarella, Stephen Soder-
land, Matt Broadhead, and Oren Etzioni. 2007.
Open information extraction from the Web. In Pro-
ceedings of the 20th International Joint Conference
on Artificial Intelligence (IJCAI-07), pages 2670?
2676, Hyderabad, India.
Cory Barr, Rosie Jones, and Moira Regelson. 2008.
The linguistic structure of English Web-search
queries. In Proceedings of the 2008 Conference
on Empirical Methods in Natural Language Pro-
cessing (EMNLP-08), pages 1021?1030, Honolulu,
Hawaii.
Shane Bergsma, Dekang Lin, and Randy Goebel.
2008. Discriminative learning of selectional pref-
erence from unlabeled text. In Proceedings of the
2008 Conference on Empirical Methods in Natural
Language Processing (EMNLP-08), pages 59?68,
Honolulu, Hawaii.
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: A
collaboratively created graph database for struc-
turing human knowledge. In Proceedings of the
2008 International Conference on Management of
Data (SIGMOD-08), pages 1247?1250, Vancouver,
Canada.
Stephen Clark and David Weir. 2002. Class-based
probability estimation using a semantic hierarchy.
Computational Linguistics, 28(2):187?206.
Mark Dredze, Paul McNamee, Delip Rao, Adam Ger-
ber, and Tim Finin. 2010. Entity disambiguation
for knowledge base population. In Proceedings
of the 23rd International Conference on Compu-
tational Linguistics (COLING-10), pages 277?285,
Beijing, China.
Katrin Erk. 2007. A simple, similarity-based model
for selectional preferences. In Proceedings of the
45th Annual Meeting of the Association for Com-
putational Linguistics (ACL-07), pages 216?223,
Prague, Czech Republic.
Zornitsa Kozareva and Eduard Hovy. 2010. Learning
arguments and supertypes of semantic relations us-
ing recursive patterns. In Proceedings of the 48th
Annual Meeting of the Association for Computa-
tional Linguistics (ACL-10), pages 1482?1491, Up-
psala, Sweden.
Hang Li and Naoki Abe. 1998. Generalizing case
frames using a thesaurus and the mdl principle. In
Proceedings of the ECAI-2000 Workshop on Ontol-
ogy Learning, pages 217?244, Berlin, Germany.
Xiao Li. 2010. Understanding the semantic struc-
ture of noun phrase queries. In Proceedings of the
48th Annual Meeting of the Association for Com-
putational Linguistics (ACL-10), pages 1337?1345,
Uppsala, Sweden.
Diana McCarthy. 1997. Word sense disambiguation
for acquisition of selectional preferences. In Pro-
ceedings of the ACL/EACL Workshop on Automatic
Information Extraction and Building of Lexical Se-
mantic Resources for NLP Applications, pages 52?
60, Madrid, Spain.
Tom Mitchell. 1997. Machine Learing. McGraw Hill.
Thahir Mohamed, Estevam Hruschka, and Tom
Mitchell. 2011. Discovering relations between
noun categories. In Proceedings of the 2011 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP-11), pages 1447?1455, Edin-
burgh, United Kingdom.
Vivi Nastase and Michael Strube. 2008. Decoding
Wikipedia categories for knowledge acquisition. In
Proceedings of the 23rd National Conference on
Artificial Intelligence (AAAI-08), pages 1219?1224,
Chicago, Illinois.
M. Pas?ca and E. Alfonseca. 2009. Web-derived
resources for Web Information Retrieval: From
conceptual hierarchies to attribute hierarchies. In
Proceedings of the 32nd International Conference
on Research and Development in Information Re-
trieval (SIGIR-09), pages 596?603, Boston, Mas-
sachusetts.
Patrick Pantel, Rahul Bhagat, Timothy Chklovski, and
Eduard Hovy. 2007. ISP: Learning inferential se-
lectional preferences. In Proceedings of the Annual
Meeting of the North American Chapter of the Asso-
ciation for Computational Linguistics (NAACL-07),
pages 564?571, Rochester, New York.
Simone Paolo Ponzetto and Roberto Navigli. 2009.
Large-scale taxonomy mapping for restructuring
and integrating Wikipedia. In Proceedings of
the 21st International Joint Conference on Ar-
tifical Intelligence (IJCAI-09), pages 2083?2088,
Barcelona, Spain.
Melanie Remy. 2002. Wikipedia: The free encyclope-
dia. Online Information Review, 26(6):434.
Francesc Ribas. 1995. On learning more appropriate
selectional restrictions. In Proceedings of the 7th
Conference of the European Chapter of the Asso-
ciation for Computational Linguistics (EACL-97),
pages 112?118, Madrid, Spain.
Alan Ritter, Mausam, and Oren Etzioni. 2010. A la-
tent dirichlet alcation method for selectional pref-
erences. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguis-
tics (ACL-10), pages 424?434, Uppsala, Sweden.
Claude Shannon. 1948. A mathematical theory of
communication. Bell System Technical Journal,
27:379?423,623?656.
Ellen Voorhees and Dawn Tice. 2000. Building a
question-answering test collection. In Proceedings
of the 23rd International Conference on Research
and Development in Information Retrieval (SIGIR-
00), pages 200?207, Athens, Greece.
512
Fei Wu and Daniel S. Weld. 2010. Open information
extraction using wikipedia. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics (ACL-10), pages 118?127, Up-
psala, Sweden.
Yulan Yan, Naoaki Okazaki, Yutaka Matsuo, Zhenglu
Yang, and Mitsuru Ishizuka. 2009. Unsupervised
relation extraction by mining Wikipedia texts using
information from the Web. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP (ACL-
IJCNLP-09), pages 1021?1029, Suntec, Singapore.
513
Proceedings of NAACL-HLT 2013, pages 1163?1173,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Towards Coherent Multi-Document Summarization
Janara Christensen, Mausam, Stephen Soderland, Oren Etzioni
Computer Science & Engineering
University of Washington
Seattle, WA 98195, USA
{janara,mausam,soderlan,etzioni}@cs.washington.edu
Abstract
This paper presents G-FLOW, a novel system
for coherent extractive multi-document sum-
marization (MDS).1 Where previous work on
MDS considered sentence selection and or-
dering separately, G-FLOW introduces a joint
model for selection and ordering that balances
coherence and salience. G-FLOW?s core rep-
resentation is a graph that approximates the
discourse relations across sentences based on
indicators including discourse cues, deverbal
nouns, co-reference, and more. This graph en-
ables G-FLOW to estimate the coherence of a
candidate summary.
We evaluate G-FLOW on Mechanical Turk,
and find that it generates dramatically bet-
ter summaries than an extractive summarizer
based on a pipeline of state-of-the-art sentence
selection and reordering components, under-
scoring the value of our joint model.
1 Introduction
The goal of multi-document summarization (MDS)
is to produce high quality summaries of collections
of related documents. Most previous work in ex-
tractive MDS has studied the problems of sentence
selection (e.g., (Radev, 2004; Haghighi and Vander-
wende, 2009)) and sentence ordering (e.g., (Lapata,
2003; Barzilay and Lapata, 2008)) separately, but
we believe that a joint model is necessary to produce
coherent summaries. The intuition is simple: if the
sentences in a summary are first selected?without
regard to coherence?then a satisfactory ordering of
the selected sentences may not exist.
1System and data at http://knowitall.cs.washington.edu/gflow/
doc1: Bomb-
ing in
Jerusalem
doc1: Anger
from Israelis
doc1: Suspen-
sion of peace
accord due to
bombing
doc2: Hamas
claims respon-
sibility
doc5: Pales-
tinians con-
demn attack
doc4: Mubarak
urges peace
accord
doc5: Pales-
tinians urge
peace accord
doc3: Clinton
urges peace
accord
Figure 1: An example of a discourse graph covering a
bombing and its aftermath, indicating the source docu-
ment for each node. A coherent summary should begin
with the bombing and then describe the reactions. Sen-
tences are abbreviated for compactness.
An extractive summary is a subset of the sen-
tences in the input documents, ordered in some
way.2 Of course, most possible summaries are in-
coherent. Now, consider a directed graph where the
nodes are sentences in the collection, and each edge
represents a pairwise ordering constraint necessary
for a coherent summary (see Figure 1 for a sample
graph). By definition, any coherent summary must
obey the constraints in this graph.
Previous work has constructed similar graphs au-
tomatically for single document summarization and
manually for MDS (see Section 2). Our system,
G-FLOW extends this research in two important
ways. First, it tackles automatic graph construction
for MDS, which requires novel methods for identi-
fying inter-document edges (Section 3). It uses this
2We focus exclusively on extractive summaries, so we drop
the word ?extractive? henceforth.
1163
State-of-the-art MDS system G-FLOW
? The attack took place Tuesday near Cailaco in East Timor, a
former Portuguese colony, according to a statement issued by the
pro-independence Christian Democratic Union of East Timor.
? The United Nations does not recognize Indonesian claims to East
Timor.
? In a decision welcomed as a landmark by Portugal, European Union
leaders Saturday backed calls for a referendum to decide the fate of East
Timor, the former Portuguese colony occupied by Indonesia since 1975.
? Indonesia invaded East Timor in 1975 and annexed it the following
year.
? Bhichai Rattakul, deputy prime minister and president of the
Bangkok Asian Games Organizing Committee, asked the Foreign
Ministry to urge the Saudi government to reconsider withdrawing
its 105-strong team.
? The games will be a success.
? Thailand won host rights for the quadrennial games in 1995, but
setbacks in preparations led officials of the Olympic Council of Asia late
last year to threaten to move the games to another country.
? Thailand showed its nearly complete facilities for the Asian Games to
a tough jury Thursday - the heads of the organizing committees from the
43 nations competing in the December event.
Table 1: Pairs of sentences produced by a pipeline of a state-of-the-art sentence extractor (Lin and Bilmes, 2011) and
sentence orderer (Li et al, 2011a), and by G-FLOW.
graph to estimate coherence of a candidate summary.
Second, G-FLOW introduces a novel methodology
for joint sentence selection and ordering (Section 4).
It casts MDS as a constraint optimization problem
where salience and coherence are soft constraints,
and redundancy and summary length are hard con-
straints. Because this optimization problem is NP-
hard, G-FLOW uses local search to approximate it.
We report on a Mechanical Turk evaluation that
directly compares G-FLOW to state-of-the-art MDS
systems. Using DUC?04 as our test set, we com-
pare G-FLOW against a combination of an extractive
summarization system with state-of-the-art ROUGE
scores (Lin and Bilmes, 2011) followed by a state-
of-the-art sentence reordering scheme (Li et al,
2011a). We also compare G-FLOW to a combina-
tion of an extractive system with state-of-the-art co-
herence scores (Nobata and Sekine, 2004) followed
by the reordering system. In both cases participants
substantially preferred G-FLOW. Participants chose
G-FLOW 54% of the time when compared to Lin,
and chose Lin?s system 22% of the time. When com-
pared to Nobata, participants chose G-FLOW 60%
of the time, and chose Nobata only 20% of the time.
The remainder of the cases were judged equivalent.
A further analysis shows that G-FLOW?s sum-
maries are judged superior along several dimensions
suggested in the DUC?04 evaluation (including co-
herence, repetitive text, and referents). A compar-
ison against manually written, gold standard sum-
maries, reveals that while the gold standard sum-
maries are preferred in direct comparisons, G-FLOW
has nearly equivalent scores on almost all dimen-
sions suggested in the DUC?04 evaluation.
The paper makes the following contributions:
? We present G-FLOW, a novel MDS system that
jointly solves the sentence selection and order-
ing problems to produce coherent summaries.
? G-FLOW automatically constructs a domain-
independent graph of ordering constraints over
sentences in a document collection, based on
syntactic cues and redundancy across docu-
ments. This graph is the backbone for estimat-
ing the coherence of a summary.
? We perform human evaluation on blind test
sets and find that G-FLOW dramatically outper-
forms state-of-the-art MDS systems.
2 Related Work
Most existing research in multi-document summa-
rization (MDS) focuses on sentence selection for in-
creasing coverage and does not consider coherence
of the summary (Section 2.1). Although coherence
has been used in ordering of summary sentences
(Section 2.2), this work is limited by the quality of
summary sentences given as input. In contrast, G-
FLOW incorporates coherence in both selection and
ordering of summary sentences.
G-FLOW can be seen as an instance of discourse-
driven summarization (Section 2.3). There is prior
work in this area, but primarily for summarization of
single documents. There is some preliminary work
on the use of manually-created discourse models in
MDS. Our approach is fully automated.
2.1 Subset Selection in MDS
Most extractive summarization research aims to in-
crease the coverage of concepts and entities while
reducing redundancy. Approaches include the use of
maximum marginal relevance (Carbonell and Gold-
stein, 1998), centroid-based summarization (Sag-
gion and Gaizauskas, 2004; Radev et al, 2004), cov-
1164
ering weighted scores of concepts (Takamura and
Okumura, 2009; Qazvinian et al, 2010), formula-
tion as minimum dominating set problem (Shen and
Li, 2010), and use of submodularity in sentence se-
lection (Lin and Bilmes, 2011). Graph centrality has
also been used to estimate the salience of a sentence
(Erkan and Radev, 2004). Approaches to content
analysis include generative topic models (Haghighi
and Vanderwende, 2009; Celikyilmaz and Hakkani-
Tur, 2010; Li et al, 2011b), and discriminative mod-
els (Aker et al, 2010).
These approaches do not consider coherence as
one of the desiderata in sentence selection. More-
over, they do not attempt to organize the selected
sentences into an intelligible summary. They are
often evaluted by ROUGE (Lin, 2004), which is
coherence-insensitive. In practice, these approaches
often result in incoherent summaries.
2.2 Sentence Reordering
A parallel thread of research has investigated taking
a set of summary sentences as input and reordering
them to make the summary fluent. Various algo-
rithms use some combination of topic-relatedness,
chronology, precedence, succession, and entity co-
herence for reordering sentences (Barzilay et al,
2001; Okazaki et al, 2004; Barzilay and Lapata,
2008; Bollegala et al, 2010). Recent work has also
used event-based models (Zhang et al, 2010) and
context analysis (Li et al, 2011a).
The hypothesis in this research is that a pipelined
combination of subset selection and reordering will
produce high-quality summaries. Unfortunately,
this is not true in practice, because sentences are se-
lected primarily for coverage without regard to co-
herence. This methodology often leads to an inad-
vertent selection of a set of disconnected sentences,
which cannot be put together in a coherent sum-
mary, irrespective of how the succeeding algorithm
reorders them. In our evaluation, reordering had lim-
ited impact on the quality of the summaries.
2.3 Coherence Models and Summarization
Research on discourse analysis of documents pro-
vides a basis for modeling coherence in a docu-
ment. Several theories have been developed for
modeling discourse, e.g., Centering Theory, Rhetor-
ical Structure Theory (RST), Penn Discourse Tree-
Bank (Grosz and Sidner, 1986; Mann and Thomp-
son, 1988; Wolf and Gibson, 2005; Prasad et al,
2008). Numerous discourse-guided summariza-
tion algorithms have been developed (Marcu, 1997;
Mani, 2001; Taboada and Mann, 2006; Barzilay and
Elhadad, 1997; Louis et al, 2010). However, these
approaches have been applied to single document
summarization and not to MDS.
Discourse models have seen some application to
summary generation in MDS, for example, using a
detailed semantic representation of the source texts
(McKeown and Radev, 1995; Radev and McKe-
own, 1998). A multi-document extension of RST
is Cross-document Structure Theory (CST), which
has been applied to MDS (Zhang et al, 2002; Jorge
and Pardo, 2010). However, these systems require
a stronger input, such as a manual CST-annotation
of the set of documents. Our work can be seen as
an instance of summarization based on lightweight
CST. However, a key difference is that our proposed
algorithm is completely automated and does not re-
quire any additional human annotation. Addition-
ally, while incorporating coherence into selection,
this work does not attempt to order the sentences
coherently, while our approach performs joint selec-
tion and ordering.
Discourse models have also been used for evalu-
ating summary quality (Barzilay and Lapata, 2008;
Louis and Nenkova, 2009; Pitler et al, 2010). Fi-
nally, there is work on generating coherent sum-
maries in specific domains, such as scientific articles
(Saggion and Lapalme, 2002; Abu-Jbara and Radev,
2011) using domain-specific cues like citations. In
contrast, our work generates summaries without any
domain-specific knowledge. Other research has fo-
cused on identifying coherent threads of documents
rather than sentences (Shahaf and Guestrin, 2010).
3 Discourse Graph
As described in Section 1, our goal is to identify
pairwise ordering constraints over a set of input sen-
tences. These constraints specify a multi-document
discourse graph, which is used by G-FLOW to eval-
uate the coherence of a candidate summary.
In this graph G, each vertex is a sentence and an
edge from si to sj indicates that sj can be placed
right after si in a coherent summary. In other words,
the two share a discourse relationship. In the fol-
1165
lowing three sentences (from possibly different doc-
uments) there should be an edge from s1 to s2, but
not between s3 and the other sentences:
s1 Militants attacked a market in Jerusalem.
s2 Arafat condemned the bombing.
s3 The Wye River Accord was signed in Oct.
Discourse theories have proposed a variety of re-
lationships between sentences such as background
and interpretation. RST has 17 such relations (Mann
and Thompson, 1988) and PDTB has 16 (Prasad et
al., 2008). While we seek to identify pairs of sen-
tences that have a relationship, we do not attempt to
label the edges with the exact relation.
We use textual cues from the discourse literature
in combination with the redundancy inherent in re-
lated documents to generate edges. Because this
methodology is noisy, the graph used by G-FLOW is
an approximation, which we refer to as an approx-
imate discourse graph (ADG). We first describe the
construction of this graph, and then discuss the use
of the graph for summary generation (Section 4).
3.1 Deverbal Noun Reference
Often, the main description of an event is mentioned
in a verbal phrase and subsequent references use
deverbal nouns (nominalization of verbs) (e.g., ?at-
tacked? and ?the attack?). In this example, the noun
is derivationally related to the verb, but that is not al-
ways the case. For example, ?bombing? in s2 above
refers to ?attacked? in s1.
We identify verb-noun pairs with this relationship
as follows. First, we locate a set of candidate pairs
from WordNet: for each verb v, we determine po-
tential noun references n using a path length of up to
two in WordNet (moving from verb to noun is pos-
sible via WordNet?s ?derivationally related? links).
This set captures verb-noun pairs such as (?to at-
tack?, ?bombing?), but also includes generic pairs
such as (?to act?, ?attack?). To filter such errors
we score the candidate references. Our goal is to
emphasize common pairs and to deemphasize pairs
with common verbs or verbs that map to many
nouns. To this end, we score pairs by (c/p) ? (c/q),
where c is the number of times the pair (v, n) ap-
pears in adjacent sentences, p is the number of times
the verb appears, and q is the number of times that
v appears with a different noun. We generate these
statistics over a background corpus of 60,000 arti-
cles from the New York Times and Reuters, and
filter out candidate pairs scoring below a threshold
identified over a small training set.
We construct edges in the ADG between pairs of
sentences containing these verb to noun mappings.
To our knowledge, we are the first to use deverbal
nouns for summarization.
3.2 Event/Entity Continuation
Our second indicator is related to lexical chains
(Barzilay and Lapata, 2008). We add an edge in
the ADG from a sentence si to sj if they contain
the same event or entity and the timestamp of si is
less than or equal to the timestamp of sj (timestamps
generated with (Chang and Manning, 2012)).
3.3 Discourse Markers
We use 36 explicit discourse markers (e.g., ?but?,
?however?, ?moreover?) to identify edges between
two adjacent sentences of a document (Marcu and
Echihabi, 2002). This indicator lets us learn an edge
from s4 to s5 below:
s4 Arafat condemned the bombing.
s5 However, Netanyahu suspended peace talks.
3.4 Inferred Edges
We exploit the redundancy of information in MDS
documents to infer edges to related sentences. An
edge (s, s??) can be inferred if there is an existing
edge (s, s?) and s? and s?? express similar informa-
tion. As an example, the edge (s6, s7) can be in-
ferred based on edge (s4, s5):
s6 Arafat condemned the attack.
s7 Netanyahu has suspended the talks.
To infer edges we need an algorithm to identify
sentences expressing similar information. To iden-
tify these pairs, we extract Open Information Extrac-
tion (Banko et al, 2007) relational tuples for each
sentence, and we mark any pair of sentences with
an equivalent relational tuple as redundant (see Sec-
tion 4.3). The inferred edges allow us to propagate
within-document discourse information to sentences
from other documents.
3.5 Co-referent Mentions
A sentence sj will not be clearly understood in iso-
lation and may need another sentence si in its con-
text, if sj has a general reference (e.g., ?the presi-
1166
dent?) pointing to a specific entity or event in si (e.g.,
?President Bill Clinton?). We construct edges based
on coreference mentions, as predicted by Stanford?s
coreference system (Lee et al, 2011). We are able
to identify syntactic edge (s8, s9):
s8 Pres. Clinton expressed sympathy for Israel.
s9 He said the attack should not derail the deal.
3.6 Edge Weights
We weight each edge in the ADG by adding the
number of distinct indicators used to construct that
edge ? if sentences s and s? have an edge because
of a discourse marker and a deverbal reference, the
edge weight wG(s, s?) will be two. We also include
negative edges in the ADG. wG(s, s?) is negative if
s? contains a deverbal noun reference, a discourse
marker, or a co-reference mention that is not fulfilled
by s. For example, if s? contains a discourse marker,
and s is neither the sentence directly preceding s?
and there is no inferred discourse link between s and
s?, then we will add a negative edge wG(s, s?).
3.7 Preliminary Graph Evaluation
We evaluated the quality of the ADG used by G-
FLOW, which is important not only for its use in
MDS, but also because the ADG may be used for
other applications like topic tracking and decompos-
ing an event into sub-events. One author randomly
chose 750 edges and labeled an edge correct if the
pair of sentences did have a discourse relationship
between them and incorrect otherwise. 62% of the
edges accurately reflected a discourse relationship.
Our ADG has on average 31 edges per sentence for
a dataset in which each document cluster has on av-
erage 253 sentences. This evaluation includes only
the positive edges.
4 Summary Generation
We denote a candidate summary X to be a sequence
of sentences ?x1, x2, . . . , x|X|?. G-FLOW?s summa-
rization algorithm searches through the space of or-
dered summaries and scores each candidate sum-
mary along the dimensions of coherence (Section
4.1), salience (Section 4.2) and redundancy (Section
4.3). G-FLOW returns the summary that maximizes
a joint objective function (Section 4.4).
weight feature
-0.037 position in document
0.033 from first three sentences
-0.035 number of people mentions
0.111 contains money
0.038 sentence length > 20
0.137 length of sentence
0.109 #sentences verbs appear in (any form)
0.349 #sentences common nouns appear in
0.355 #sentences proper nouns appear in
Table 2: Linear regression features for salience.
4.1 Coherence
G-FLOW estimates coherence of a candidate sum-
mary via the ADG. We define coherence as the sum
of edge weights between successive summary sen-
tences. For disconnected sentence pairs, the edge
weight is zero.
Coh(X) =
?
i=1..|X|?1
wG+(xi, xi+1) + ?wG?(xi, xi+1)
wG+ represents positive edges and wG? represents
negative edge weights. ? is a tradeoff coefficient for
positive and negative weights, which is tuned using
the methodology described in Section 4.4.
4.2 Salience
Salience is the inherent value of each sentence to
the documents. We compute salience of a summary
(Sal(X)) as the sum of the saliences of individual
sentences (
?
i Sal(xi)).
To estimate salience of a sentence, G-FLOW uses
a linear regression classifier trained on ROUGE
scores over the DUC?03 dataset. The classifier uses
surface features designed to identify sentences that
cover important concepts. The complete list of fea-
tures and learned weights is in Table 2. The clas-
sifier finds a sentence more salient if it mentions
nouns or verbs that are present in more sentences
across the documents. The highest ranked features
are the last three ? number of other sentences that
mention a noun or a verb in the given sentence. We
use the same procedure as in deverbal nouns for de-
tecting verb mentions that appear as nouns in other
sentences (Section 3.1).
4.3 Redundancy
We also wish to avoid redundancy. G-FLOW first
processes each sentence with a state-of-the-art Open
Information extractor OLLIE (Mausam et al, 2012),
which converts a sentence into its component re-
lational tuples of the form (arg1, relational phrase,
1167
arg2).3 For example, it finds (Militants, bombed, a
marketplace) as a tuple from sentence s12.
Two sentences will express redundant information
if they both contain the same or synonymous com-
ponent fact(s). Unfortunately, detecting synonymy
even at relational tuple level is very hard. G-FLOW
approximates this synonymy by considering two re-
lational tuples synonymous if the relation phrases
contain verbs that are synonyms of each other, have
at least one synonymous argument, and are times-
tamped within a day of each other. Because the in-
put documents cover related events, these relatively
weak rules provide good performance. The same
algorithm is used for inferring edges for the ADG
(Section 3.4). This algorithm can detect that the fol-
lowing sentences express redundant information:
s12 Militants bombed a marketplace in Jerusalem.
s13 He alerted Arafat after assailants attacked the
busy streets of Mahane Yehuda.
4.4 Objective Function
The objective function needs to balance coherence,
salience and redundancy and also honor the given
budget, i.e., maximum summary lengthB. G-FLOW
treats redundancy and budget as hard constraints and
coherence and salience as soft. Coherence is neces-
sarily soft as the graph is approximate. While previ-
ous MDS systems specifically maximized coverage,
in preliminary experiments on a development set, we
found that adding a coverage term did not improve
G-FLOW?s performance. We optimize:
maximize: F (x) , Sal(X) + ?Coh(X)? ?|X|
s.t.
?
i=1..|X| len(xi) < B
?xi, xj ? X : redundant(xi, xj) = 0
Here len refers to the sentence length. We add |X|
term (the number of sentences in the summary) to
avoid picking many short sentences, which may in-
crease coherence and salience scores at the cost of
overall summary quality.
The parameters ?, ? and ? (see Section 4.1) are
tuned automatically using a grid search over a de-
velopment set as follows. We manually generate ex-
tractive summaries for each document cluster in our
development set (DUC?03) and choose the parame-
ter setting that minimizes |F (XG-FLOW) ? F (X?)|
3Available from http://ollie.cs.washington.edu
summed over all document clusters. F is the objec-
tive function, XG-FLOW is the summary produced by
G-FLOW and X? is the manual summary.
This constraint optimization problem is NP hard,
which can be shown by using a reduction of the
longest path problem. For this reason, G-FLOW uses
local search to reach an approximation of the opti-
mum. G-FLOW employs stochastic hill climbing
with random restarts as the base search algorithm.
At each step, the search either adds a sentence, re-
moves a sentence, replaces a sentence by another, or
reorders a pair of sentences. The initial summary for
random restarts is constructed as follows. We first
pick the highest salience sentence with no incoming
negative edges as the first sentence. The following
sentences are probabilistically added one at a time
based on the summary score up to that sentence. The
initial summary is complete when there are no possi-
ble sentences left to fit within the budget. Intuitively,
this heuristic chooses a good starting point by se-
lecting a first sentence that does not rely on context
and subsequent sentences that build a high scoring
summary. As with all local search algorithms, this
algorithm is highly scalable and can easily apply to
large collections of related documents, but does not
guarantee global optima.
5 Experiments
Because summaries are intended for human con-
sumption we focused on human evaluations. We
hired workers on Amazon Mechanical Turk (AMT)
to evaluate the summaries. Our evaluation addresses
the following questions: (1) how do G-FLOW sum-
maries compare against the state-of-the-art in MDS
(Section 5.2)? (2) what is G-FLOW?s performance
along important summarization dimensions such as
coherence and redundancy (Section 5.3)? (3) how
does G-FLOW perform on coverage as measured
by ROUGE (Section 5.3.1)? (4) how much do the
components of G-FLOW?s objective function con-
tribute to performance (Section 5.4)? (5) how do G-
FLOW?s summaries compare to human summaries?
5.1 Data and Systems
We evaluated the systems on the Task 2 DUC?04
multi-document summarization dataset. This dataset
consists of 50 clusters of related documents, each of
which contains 10 documents. Each cluster of doc-
1168
uments also includes four gold standard summaries
used for evaluation. As in the DUC?04 competition,
we allowed 665 bytes for each summary including
spaces and punctuation. We used DUC?03 as our
development set, which contains 30 document clus-
ters, again with approximately 10 documents each.
We compared G-FLOW against four systems. The
first is a recent MDS extractive summarizer, which
we choose for its state-of-the-art ROUGE scores
(Lin and Bilmes, 2011).4 The second is a pipeline
of Lin?s system followed by a reimplementation of
a state-of-the-art sentence reordering system (Li et
al., 2011a). We refer to these systems as LIN and
LIN-LI, respectively. This second baseline allows
us to quantify the advantage of using coherence as a
factor in both sentence extraction and ordering.
We also compare against the system that had the
highest coherence ratings at DUC?04 (Nobata and
Sekine, 2004), which we refer to as NOBATA. As
this system did not preform sentence ordering on its
output, we also compare against a pipeline of No-
bata?s system and the sentence reordering system.
We refer to this system as NOBATA-LI.
Lastly, to evaluate how well the system performs
against human generated summaries, we compare
against the gold standard summaries provided by
DUC.
5.2 Overall Summary Quality
Following (Haghighi and Vanderwende, 2009) and
(Celikyilmaz and Hakkani-Tur, 2010), to compare
overall summary quality, we asked AMT workers
to compare two candidate system summaries. The
workers first read a gold standard summary, fol-
lowed by the two system summaries, and were then
asked to choose the better summary from the pair.
The system summaries were shown in a random or-
der to remove any bias.
To ensure that workers provided high quality data
we added two quality checks. First, we restricted
to workers who have an overall approval rating of
over 95% on AMT. Second, we asked the workers
to briefly describe the main events of the summary.
We manually filtered out work where this descrip-
tion was incorrect.
4We thank Lin and Bilmes for providing us with their code.
Unfortunately, we were unable to obtain other recent MDS sys-
tems from their authors.
Six workers compared each pair of summaries.
We recorded the scores for each cluster, and report
three numbers: the percentages of clusters where a
system is more often preferred over the other and the
percentage where the two systems are tied. G-FLOW
is preferred almost three times as often as LIN:
G-FLOW Indifferent LIN
56% 24% 20%
Next, we compared G-FLOW and LIN-LI. Sen-
tence reordering improves performance, but G-
FLOW is still overwhelmingly preferred:
G-FLOW Indifferent LIN-LI
54% 24% 22%
These results suggest that incorporating coher-
ence in sentence extraction adds significant value to
a summarization system. In these experiments, LIN
and LIN-LI are preferred in some cases. We an-
alyzed those summaries more carefully, and found
that occasionally, G-FLOW will sacrifice a small
amount of coverage for coherence, resulting in lower
performance in those cases (see Section 5.3.1).
We also compared LIN and LIN-LI, and found
that reordering does not improve performance by
much.
LIN-LI Indifferent LIN
32% 38% 30%
While the scores presented above represent com-
parisons between G-FLOW and a summarization
system with state-of-the-art ROUGE scores, we
also compared against a summarization system with
state-of-the-art coherence scores ? the system with
the highest coherence scores from DUC?04, (No-
bata and Sekine, 2004). We found that G-FLOW was
again preferred:
G-FLOW Indifferent NOBATA
68% 10% 22%
Adding in sentence ordering again improved the
scores for the comparison system somewhat:
G-FLOW Indifferent NOBATA-LI
60% 20% 20%
While these scores show a significant improve-
ment over previous sytems, they do not convey how
well G-FLOW compares to the gold standard ? man-
ually generated summaries. As a final experiment,
we compared G-FLOW and a second, manually gen-
erated summary:
1169
G-FLOW Indifferent Gold
14% 18% 68%
While we were pleased that in 32% of the cases,
Turkers either preferred G-FLOW or were indiffer-
ent, there is clearly a lot of room for improvement
despite the gains reported over previous sytems.
5.3 Comparison along Summary Dimensions
A high quality summary needs to be good along sev-
eral dimensions. We asked AMT workers to rate
summaries using the quality questions enumerated
in DUC?04 evaluation scheme.5 These questions
concern: (1) coherence, (2) useless, confusing, or
repetitive text, (3) redundancy, (4) nouns, pronouns,
and personal names that are not well-specified (5)
entities rementioned in an overly explicit way, (6)
ungrammatical sentences, and (7) formatting errors.
We evaluated G-FLOW LIN-LI and NOBATA-LI
against the gold standard summaries, using the same
AMT scheme as in the previous section. To assess
automated performance with respect to the standards
set by human summaries, we also evaluated a (dif-
ferent) gold standard summary for each document
cluster, using the same Mechanical Turk scheme as
in the previous section. The 50 summaries produced
by each system were evaluated by four workers. The
results are shown in Figure 2.
G-FLOW was rated significantly better than LIN-
LI in all categories except ?Redundancy? and signif-
icant better than NOBATA-LI on ?Coherence? and
?Referents?. The ratings for ?Coherence?, ?Refer-
ents?, and ?OverlyExplicit? are not surprising given
G-FLOW?s focus on coherence. The results for
?UselessText? may also be due to G-FLOW?s focus
on coherence which ideally prevents it from getting
off topic. Lastly, G-FLOW may perform better on
?Grammatical? and ?Formatting? because it tends to
choose longer sentences than other systems, which
are less likely to be sentence segmentation errors.
There may also be some bleeding from one dimen-
sion to the other ? if a worker likes one summary she
may score it highly for many dimensions.
Finally, somewhat surprisingly, we find G-
FLOW?s performance to be nearly that of human
summaries. G-FLOW is rated statistically signifi-
cantly lower than the gold summaries on only ?Re-
5http://duc.nist.gov/duc2004/quality.questions.txt
System R F
NOBATA 30.44 34.36
Best system in DUC-04 38.28 37.94
Takamura and Okumura (2009) 38.50 -
LIN 39.35 38.90
G-FLOW 37.33 37.43
Gold Standard Summaries 40.03 40.03
Table 3: ROUGE-1 recall and F-measure results (%) on
DUC-04. Some values are missing because not all sys-
tems reported both F-measure and recall.
dundancy?. Given the results from the previous sec-
tion, G-FLOW is likely performing worse on cate-
gories not conveyed in these scores, such as Cover-
age, which we examine next.
5.3.1 Coverage Evaluation using ROUGE
Most recent research has focused on the ROUGE
evaluation, and thus implicitly on coverage of in-
formation in a summary. To estimate the coverage
of G-FLOW, we compared the systems on ROUGE
(Lin, 2004). We calculated ROUGE-1 scores for
G-FLOW, LIN, and NOBATA.6 As sentence order-
ing does not matter for ROUGE, we do not include
LIN-LI or NOBATA-LI in this evaluation. Because
our algorithm does not explicitly maximize coverage
while LIN does, we expected G-FLOW to perform
slightly worse than LIN.
The ROUGE-1 scores for G-FLOW, LIN, NO-
BATA and other recent MDS systems are listed in Ta-
ble 3. We also include the ROUGE-1 scores for the
gold summaries (compared to the other gold sum-
maries). G-FLOW has slightly lower scores than
LIN and the gold standard summaries, but much
higher scores than NOBATA. G-FLOW only scores
significantly lower than LIN and the gold standard
summaries.
We can conclude that good summaries have both
the characteristics listed in the quality dimensions,
and good coverage. The gold standard summaries
outperform G-FLOW on both ROUGE scores and
the quality dimension scores, and therefore, out-
perform G-FLOW on overall comparison. How-
ever, G-FLOW is preferred to LIN-LI in addition to
NOBATA-LI indicating that its quality scores out-
weigh its ROUGE scores in that comparison. An
improvement to G-FLOW may focus on increasing
6ROUGE version 1.5.5 with options: -a -c 95 -b 665 -m -n
4 -w 1.2
1170
Coherence UselessText Redundancy Referents OverlyExplicit Grammatical Formatting
Rat
ing
0
1
2
3
4
GoldG?FlowNobata?LiLin?Li
Figure 2: Ratings for the systems. 0 is the lowest possible score and 4 is the highest possible score. G-FLOW is rated
significantly higher than LIN-LI on all categories, except for ?Redundancy?, and significantly higher than NOBATA-LI
on ?Coherence? and ?Referents?. G-FLOW is only significantly lower than the gold standard on ?Redundancy?.
coverage while retaining strengths such as coher-
ence.
5.4 Ablation Experiments
In this ablation study, we evaluated the contribution
of the main components of G-FLOW ? coherence
and salience. The details of the experiments are the
same as in the experiment described in Section 5.2.
We first measured the importance of coherence in
summary generation. This system G-FLOW-SAL is
identical to the full system except that it does not
include the coherence term in the objective function
(see Section 4.4). The results show that coherence is
very important to G-FLOW?s performance:
G-FLOW Indifferent G-FLOW-SAL
54% 26% 20%
Similarly, we evaluated the contribution of
salience. This system G-FLOW-COH does not in-
clude the salience term in the objective function:
G-FLOW Indifferent G-FLOW-COH
60% 20% 20%
Without salience, the system produces readable,
but highly irrelevant summaries.
5.5 Agreement of Expert & AMT Workers
Because summary evaluation is a relatively complex
task, we compared AMT workers? annotations with
expert annotations from DUC?04. We randomly
selected ten summaries from each of the seven
DUC?04 annotators, and asked four Turk workers
to annotate them on the DUC?04 quality questions.
For each DUC?04 annotator, we selected all pairs
of summaries where one summary was judged more
than one point better than the other summary. We
compared whether the workers (voting as in Sec-
tion 5.2) likewise judged that summary better than
the second summary. We found that the annotations
agreed in 75% of cases. When we looked only at
pairs more than two points different, the agreement
was 80%. Thus, given the subjective nature of the
task, we feel reasonably confident that the AMT an-
notations are informative, and that the dramatic pref-
erence of G-FLOW over the baseline systems is due
to a substantial improvement in its summaries.
6 Conclusion
In this paper, we present G-FLOW, a multi-
document summarization system aimed at generat-
ing coherent summaries. While previous MDS sys-
tems have focused primarily on salience and cov-
erage but not coherence, G-FLOW generates an or-
dered summary by jointly optimizing coherence and
salience. G-FLOW estimates coherence by using
an approximate discourse graph, where each node
is a sentence from the input documents and each
edge represents a discourse relationship between
two sentences. Manual evaluations demonstrate that
G-FLOW generates substantially better summaries
than a pipeline of state-of-the-art sentence selec-
tion and reordering components. ROUGE scores,
which measure summary coverage, show that G-
FLOW sacrifices a small amount of coverage for
overall readability and coherence. Comparisons to
gold standard summaries show that G-FLOW must
improve in coverage to equal the quality of manu-
ally written summaries. We believe this research has
applications to other areas of summarization such as
update summarization and query based summariza-
tion, and we are interested in investigating these top-
ics in future work.
1171
Acknowledgements
We thank Luke Zettlemoyer, Lucy Vanderwende, Hal
Daume III, Pushpak Bhattacharyya, Chris Quirk, Erik
Frey, Tony Fader, Michael Schmitz, Alan Ritter, Melissa
Winstanley, and the three anonymous reviewers for help-
ful conversations and feedback on earlier drafts. We also
thank Lin and Bilmes for providing us with the code for
their system. This research was supported in part by NSF
grant IIS-0803481, ONR grant N00014-11-1-0294, and
DARPA contract FA8750-13-2-0019, and carried out at
the University of Washington?s Turing Center. This pa-
per was also supported in part by the Intelligence Ad-
vanced Research Projects Activity (IARPA) via Air Force
Research Laboratory (AFRL) contract number FA8650-
10-C-7058. The U.S. Government is authorized to repro-
duce and distribute reprints for Governmental purposes
notwithstanding any copyright annotation thereon. The
views and conclusions contained herein are those of the
authors and should not be interpreted as necessarily rep-
resenting the official policies or endorsements, either ex-
pressed or implied, of IARPA, AFRL, or the U.S. Gov-
ernment.
References
Amjad Abu-Jbara and Dragomir R. Radev. 2011. Coher-
ent citation-based summarization of scientific papers.
In Proceedings of ACL 2011, pages 500?509.
Ahmet Aker, Trevor Cohn, and Robert Gaizauskas. 2010.
Multi-document summarization using A * search and
discriminative training. In Proceedings of EMNLP
2010.
Michele Banko, Michael Cafarella, Stephen Soderland,
Matt Broadhead, and Oren Etzioni. 2007. Open in-
formation extraction from the web. In Proceedings of
IJCAI 2007, pages 68?74.
Regina Barzilay and Michael Elhadad. 1997. Using lex-
ical chains for text summarization. In Proceedings of
the ACL Workshop on Intelligent Scalable Text Sum-
marization, pages 10?17.
Regina Barzilay and Mirella Lapata. 2008. Modeling
local coherence: An entity-based approach. Computa-
tional Linguistics, 34(1):1?34.
Regina Barzilay, Noemie Elhadad, and Kathleen R McK-
eown. 2001. Sentence ordering in multidocument
summarization. In Proceedings of HLT 2001, pages
1?7.
Danushka Bollegala, Naoaki Okazaki, and Mitsuru
Ishizuka. 2010. A bottom-up approach to sentence
ordering for multi-document summarization. Informa-
tion Process Management, 46(1):89?109.
Jaime Carbonell and Jade Goldstein. 1998. The use of
MMR, diversity-based reranking for reordering docu-
ments and producing summaries. In Proceedings of
SIGIR 1998, pages 335?336.
Asli Celikyilmaz and Dilek Hakkani-Tur. 2010. A hy-
brid hierarchical model for multi-document summa-
rization. In Proceedings of ACL 2010, pages 815?824.
Angel Chang and Christopher Manning. 2012. SU-
TIME: A library for recognizing and normalizing time
expressions. In Proceedings of LREC 2012.
Gunes Erkan and Dragomir R Radev. 2004. LexRank:
Graph-based centrality as salience in text summa-
rization. Journal of Artificial Intelligence Research,
22(1):457?479.
Barbara Grosz and Candace Sidner. 1986. Attention,
intentions, and the structure of discourse. Computa-
tional Linguistics, 12(3):175?204.
Aria Haghighi and Lucy Vanderwende. 2009. Explor-
ing content models for multi-document summariza-
tion. Proceedings of NAACL 2009, pages 362?370.
Maria Lucia Castro Jorge and Thiago Alexan-
dre Salgueiro Pardo. 2010. Multi-Document
Summarization: Content Selection based on CST
Model (Cross-document Structure Theory). Ph.D.
thesis, Nu?cleo Interinstitucional de Lingu???stica
Computacional (NILC).
Mirella Lapata. 2003. Probabilistic text structuring: Ex-
periments with sentence ordering. In Proceedings of
ACL 2003, pages 545?552.
Heeyoung Lee, Yves Peirsman, Angel Chang, Nathanael
Chambers, Mihai Surdeanu, and Dan Jurafsky. 2011.
Stanford?s multi-pass sieve coreference resolution sys-
tem at the CoNLL-2011 shared task. In CoNLL 2011
Shared Task.
Peifeng Li, Guangxi Deng, and Qiaoming Zhu. 2011a.
Using context inference to improve sentence ordering
for multi-document summarization. In Proceedings of
IJCNLP 2011, pages 1055?1061.
Peng Li, Yinglin Wang, Wei Gao, and Jing Jiang. 2011b.
Generating aspect-oriented multi-document summa-
rization with event-aspect model. In Proceedings of
EMNLP 2011, pages 1137?1146.
Hui Lin and Jeff Bilmes. 2011. A class of submodular
functions for document summarization. In Proceed-
ings of ACL 2011, pages 510?520.
Chin-Yew Lin. 2004. ROUGE: A package for auto-
matic evaluation of summaries. In Text Summarization
Branches Out: Proceedings of the ACL-04 Workshop,
pages 74?81.
Annie Louis and Ani Nenkova. 2009. Automatic sum-
mary evaluation without using human models. In Pro-
ceedings of EMNLP 2009, pages 306?314.
Annie Louis, Aravind Joshi, Rashmi Prasad, and Ani
Nenkova. 2010. Using entity features to classify im-
plicit discourse relations. In Proceedings of SIGDIAL
2010, pages 59?62.
1172
Inderjeet Mani. 2001. Automatic Summarization. John
Benjamins Publishing Co, Amsterdam/Philadelphia.
William C. Mann and Sandra A. Thompson. 1988.
Rhetorical structure theory: Toward a functional the-
ory of text organization. Text, 8(3):243?281.
Daniel Marcu and Abdessamad Echihabi. 2002. An
unsupervised approach to recognizing discourse rela-
tions. In Proceedings of ACL 2002, pages 368?375.
Daniel Marcu. 1997. From discourse structures to text
summaries. In Proceedings of the ACL Workshop on
Intelligent Scalable Text Summarization, pages 82?88.
Mausam, Michael Schmitz, Robert Bart, Stephen Soder-
land, and Oren Etzioni. 2012. Open language learning
for information extraction. In Proceedings of EMNLP
2012, pages 523?534.
Kathleen McKeown and Dragomir Radev. 1995. Gen-
erating summaries of multiple news articles. In Pro-
ceedings of SIGIR 1995, pages 74?82.
Chikashi Nobata and Satoshi Sekine. 2004. Crl/nyu
summarization system at duc-2004. In Proceedings of
DUC 2004.
Naoaki Okazaki, Yutaka Matsuo, and Mitsuru Ishizuka.
2004. Improving chronological sentence ordering by
precedence relation. In Proceedings of COLING 2004,
pages 750?756.
Emily Pitler, Annie Louis, and Ani Nenkova. 2010.
Automatic evaluation of linguistic quality in multi-
document summarization. In Proceedings of ACL
2010, pages 544?554.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bonnie
Webber. 2008. The Penn Discourse TreeBank 2.0.
In Proceedings of LREC 2008.
Vahed Qazvinian, Dragomir R. Radev, and Arzucan
O?zgu?r. 2010. Citation summarization through
keyphrase extraction. In Proceedings of COLING
2010, pages 895?903.
Dragomir R. Radev and Kathleen R. McKeown. 1998.
Generating natural language summaries from mul-
tiple on-line sources. Computational Linguistics,
24(3):469?500.
Dragomir R. Radev, Hongyan Jing, Malgorzata Stys, and
Daniel Tam. 2004. Centroid-based summarization
of multiple documents. Information Processing and
Management, 40(6):919?938.
Dragomir R. Radev. 2004. LexRank: Graph-based lexi-
cal centrality as salience in text summarization. Jour-
nal of Artificial Intelligence Research, 22(1):457?479.
Horacio Saggion and Robert Gaizauskas. 2004. Multi-
document summarization by cluster/profile relevance
and redundancy removal. In Proceedings of DUC
2004.
Horacio Saggion and Guy Lapalme. 2002. Generating
indicative-informative summaries with sumUM. Com-
putational Linguistics, 28(4):497?526.
Dafna Shahaf and Carlos Guestrin. 2010. Connecting the
dots between news articles. In Proceedings of KDD
2010, pages 623?632.
Chao Shen and Tao Li. 2010. Multi-document summa-
rization via the minimum dominating set. In Proceed-
ings of Coling 2010, pages 984?992.
Maite Taboada and William C. Mann. 2006. Applica-
tions of rhetorical structure theory. Discourse Studies,
8(4):567?588.
Hiroya Takamura and Manabu Okumura. 2009. Text
summarization model based on maximum coverage
problem and its variant. In Proceedings of EACL 2009,
pages 781?789.
Florian Wolf and Edward Gibson. 2005. Representing
discourse coherence: A corpus-based study. Compu-
tational Linguistics, 31(2):249?288.
Zhu Zhang, Sasha Blair-Goldensohn, and Dragomir R.
Radev. 2002. Towards CST-enhanced summarization.
In Proceedings of AAAI 2002, pages 439?445.
Renxian Zhang, Li Wenjie, and Lu Qin. 2010. Sen-
tence ordering with event-enriched semantics and two-
layered clustering for multi-document news summa-
rization. In Proceedings of COLING 2010, pages
1489?1497.
1173
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 902?912,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Hierarchical Summarization:
Scaling Up Multi-Document Summarization
Janara Christensen Stephen Soderland
Computer Science & Engineering
University of Washington
Seattle, USA
janara@cs.washington.edu
soderlan@cs.washington.edu
Gagan Bansal Mausam
Computer Science & Engineering
Indian Institute of Technology
Delhi, India
gaganbansal1993@gmail.com
mausam@cse.iitd.ac.in
Abstract
Multi-document summarization (MDS)
systems have been designed for short, un-
structured summaries of 10-15 documents,
and are inadequate for larger document
collections. We propose a new approach
to scaling up summarization called hierar-
chical summarization, and present the first
implemented system, SUMMA.
SUMMA produces a hierarchy of relatively
short summaries, in which the top level
provides a general overview and users can
navigate the hierarchy to drill down for
more details on topics of interest. SUMMA
optimizes for coherence as well as cover-
age of salient information. In an Amazon
Mechanical Turk evaluation, users pref-
ered SUMMA ten times as often as flat
MDS and three times as often as timelines.
1 Introduction
The explosion in the number of documents
on the Web necessitates automated approaches
that organize and summarize large document col-
lections on a complex topic. Existing methods
for multi-document summarization (MDS) are de-
signed to produce short summaries of 10-15 doc-
uments.
1
MDS systems do not scale to data sets
ten times larger and proportionately longer sum-
maries: they either cannot run on large input or
produce a disorganized summary that is difficult
to understand.
We present a novel MDS paradigm, hierarchi-
cal summarization, which operates on large doc-
ument collections, creating summaries that orga-
nize the information coherently. It mimics how
someone with a general interest in a complex topic
would learn about it from an expert ? first, the ex-
pert would provide an overview, and then more
1
In the DUC evaluations, summaries have a budget of 665
bytes and cover 10 documents.
Hierarchical Summarization: Scaling Up Multi-Document
Summarization
bstract
For topics that cover large amounts of in-
formation, simple, short summaries are in-
sufficient ? complex topics require more
information and more structure to under-
stand. We propose a new approach to scal-
ing up summarization called hierarchical
summarization, and present the first imple-
mented system, SUMMA.
SUMMA produces a hierarchy of relatively
short summaries, where the top level pro-
vides a general overview and users can
navigate the hierarchy to drill down for
more details on topics of interest. Com-
pared to flat multi-document summaries,
users prefer SUMMA ten times as often
and learn just as much, and compared to
timelines, users prefer SUMMA three t mes
as often and learn more in twice as many
cases.
1 Introduction
The explosion in the number of documents over
the Web necessitates automated approaches that
organize and summarize large document collec-
tions on a complex topic. Existing methods for
multi-document summarization (MDS) can handle
10-15 documents and create a short flat summary,
but are insufficient for large-scale summarization.
For large-scale summarization, we need summa-
rizers that organize the information coherently and
enable personalized interaction with the summary
so that users can explore the various aspects of in-
formation in different levels of detail based on in-
dividual interest.
To this end, we present a novel MDS paradigm,
hierarchical summarization. Hierarchical summa-
rization is designed to operate on large document
collections. It mimics how someone with a gen-
eral interest in a complex topic would learn about
x1,1
x1,2
x1,3
x4,1
x4,2
x4,3
x9,1
x9,2
x8,1
x8,2
x8,3
x7,1
x7,2
x3,1
x3,2
x3,3
x2,1
x2,2 x6,1
x6,2
x5,1
x5,2
x5,3
On Aug 7 1998, car bombs
exploded outside US em-
bassies in Kenya and Tanza-
nia. Several days later, the
US began investigations into
bombings. The US retali-
ated with missile strikes on
suspected terrorist camps
in Afghanistan and Sudan
on Aug 20.
Some questioned the timing
of Clinton?s decision to launch
strikes. On Aug 22, with bin
Laden having survived the
strikes, the US outlined other
efforts to damage his net-
work. Russia, Sudan, Pakistan,
and Afghanistan condemned the
strikes.
Clinton proposed meth-
ods to inflict financial
damage on bin Laden.
Another possibility is
for the United States to
negotiate with the Tal-
iban to surrender bin
Laden. But diplomats
who have dealt with
the Taliban doubt that
anything could come of
such negotiations.
Figure 1: An example of a hierarchical summary for the 1998
embassy bombings, with one branch of the hierarchy high-
lighted. Each rectangle represents a summary and each x
i,j
represents a sentence within a summary. The root summary
provides an overview of the events of August 1998. When the
last sentence is selected, a more detailed summary of the mis-
sile strikes is produced, and when the middle sentence of that
summary is selected, a more detailed summary bin Laden?s
escape is produced.
it from an expert ? first, the expert would give
an overview, and then more specific information
about various aspects. It has the following novel
characteristics:
i r A hierarchical summary of the 1998 embassy
bombings. Each rectangle represents a summary and each
x
i,j
is a sentenc within a summary. The root summary pro-
vides an overview of the events of August 1998. When the
third sentence is selected, a more detailed summary of the
missile strikes is displayed. Selecting the second sentence of
that summary produces a more detailed summary of the US?
options.
specific information about various aspects. Hi-
erarchical summarization has the following novel
characteristics:
? The summary is hierarchically organized
along one or more organizational principles
such as time, location, entities, or events.
? Each non-leaf summary is associated with a
set of child summaries where each gives de-
tails of an element (e.g. sentence) in the par-
ent summary.
? A user can navigate within the hierarchical
summary by clicking on an element of a par-
ent summary to view the associated child
summary.
For example, given the topic, ?1998 embassy
bombings,? the first summary (Figure 1) might
902
mention that the US retaliated by striking
Afghanistan and Sudan. The user can click on this
information to learn more about these attacks. In
this way, the system can present large amounts of
information without overwhelming the user, and
the user can tailor the output to their interests.
In this paper, we describe SUMMA, the first
hierarchical summarization system for multi-
document summarization.
2
It operates on a corpus
of related news articles. SUMMA hierarchically
clusters the sentences by time, and then summa-
rizes the clusters using an objective function that
optimizes salience and coherence.
We conducted an Amazon Mechanical Turk
(AMT) evaluation where AMT workers compared
the output of SUMMA to that of timelines and flat
summaries. SUMMA output was judged superior
more than three times as often as timelines, and
users learned more in twice as many cases. Users
overwhelmingly preferred hierarchical summaries
to flat summaries (92%) and learned just as much.
Our main contributions are as follows:
? We introduce and formalize the novel task of
hierarchical summarization.
? We present SUMMA, the first hierarchical
summarization system, which operates on
news corpora and summarizes over an or-
der of magnitude more documents than tra-
ditional MDS systems, producing summaries
an order of magnitude larger.
? We present a user study which demonstrates
the value of hierarchical summarization over
timelines and flat multi-document summaries
in learning about a complex topic.
In the next section, we formalize hierarchical
summarization. We then describe our methodol-
ogy to implement the SUMMA hierarchical sum-
marization system: hierarchical clustering in Sec-
tion 3 and creating summaries based on that clus-
tering in Section 4. We discuss our experiments in
Section 5, related work in Section 6, and conclu-
sions in Section 7.
2 Hierarchical Summarization
We propose a new task for large-scale summariza-
tion called hierarchical summarization. Input to a
hierarchical summarization system is a set of re-
lated documents D and a budget b for each sum-
mary within the hierarchy (in bytes, words, or sen-
tences). The output is the hierarchical summary
H , which we define formally as follows.
2
http://knowitall.cs.washington.edu/summa/
Definition A hierarchical summary H of a docu-
ment collection D is a set of summaries X orga-
nized into a hierarchy. The top of the hierarchy
is a summary X
1
representing all of D, and each
summary X
i
consists of summary units x
i,j
(e.g.
the jth sentence of summary i) that point to a child
summary, except at the leaf nodes of the hierarchy.
A child summary adds more detail to the infor-
mation in its parent summary unit. The child sum-
mary may include sub-events or background and
reactions to the event or topic in the parent.
We define several metrics in Section 4 for
a well-constructed hierarchical summary. Each
summary should maximize coverage of salient in-
formation; it should minimize redundancy; and
it should have intra-cluster coherence as well as
parent-to-child coherence.
Hierarchical summarization has two important
strengths in the context of large-scale summariza-
tion. First, the information presented at the start
is small and grows only as the user directs it, so
as not to overwhelm the user. Second, each user
directs his or her own experience, so a user inter-
ested in one aspect need only explore that section
of the data without having to view or understand
the entire summary. The parent-to-child links pro-
vide a means for a user to navigate, drilling down
for more details on topics of interest.
There are several possible organizing principles
for the hierarchy ? by date, by entities, by loca-
tions, or by events. Some organizing principles
will fit the data in a document collection better
than others. A system may select different orga-
nization for different portions of the hierarchy, for
example, organizing first by location or prominent
entity and then by date for the next level.
3 Hierarchical Clustering
Having defined the task, we now describe
the methodology behind our implementation,
SUMMA. In future work we intend to design a
system that dynamically selects the best organiz-
ing principle for each level of the hierarchy. In
this first implementation, we have opted for tem-
poral organization, since this is generally the most
appropriate for news events.
The problem of hierarchical summarization as
described in Section 2 has all of the requirements
of MDS, and additional complexities of inducing a
hierarchical structure, processing an order of mag-
nitude bigger input, generating a much larger out-
put, and enforcing coherence between parent and
903
Hierarchical Clustering H
s1
. . .
sN
sj+1
. . .
sN
sl+1
. . .
sN
sj+1
. . .
sl
si+1
. . .
sj
s1
. . .
si
sk+1
. . .
si
s1
. . .
sk
Hierarchical Summary X
x1,1
x1,2
x1,3
x4,1
x4,2
x8,1
x8,2
x7,1
x7,2
x7,3
x3,1
x3,2
x3,3
x2,1
x2,2
x6,1
x6,2
x5,1
x5,2
x5,3
Figure 2: Examples of a hierarchical clustering and a hier-
archical summary, where the input sentences are s 2 S, the
number of input sentences is N , and the summary sentences
are x 2 X . The hierarchical clustering determines the struc-
ture of the hierarchical summary.
hierarchical structure, processing an order of mag-
nitude bigger input, generating a much larger out-
put, and enforcing coherence between parent and
child summaries.
We simplify the problem by decomposing it into
two steps: hierarchical clustering and summariz-
ing over the clustering (see Figure 2 for an exam-
ple). A hierarchical clustering is a tree in which if
a cluster g
p
is the parent of cluster g
c
, then each
sentence in g
c
is also in g
p
. This organizes the
information into manageable, semantically-related
sections and induces a hierarchical structure over
the input.
The hierarchical clustering serves as input to the
second step ? summarizing given the hierarchy.
The hierarchical summary follows the hierarchi-
cal structure of the clustering. Each node in the
hierarchy has an associated flat summary, which
summarizes the sentences in that cluster. More-
over, the number of sentences in a flat summary is
exactly equal to the number of child clusters of the
node, since the user will click a sentence to get to
the child summary. See Figure 2 for an illustration
of this correspondence.
Because we are interested in temporal hierar-
chical summarization, we hierarchically cluster all
the sentences in the input documents by time.
Unfortunately, neither agglomerative nor divisive
clustering is suitable, since both assume a binary
split at each node (Berkhin, 2006). The number of
clusters at each split should be what is most natural
for the input data. We design a recursive clustering
algorithm that automatically chooses the appropri-
ate number of clusters at each split.
Before clustering, we timestamp all sentences.
We use SUTime (Chang and Manning, 2012) to
normalize temporal references, and we parse the
sentences with the Stanford parser (Klein and
Manning, 2003) and use a set of simple heuristics
to determine if the timestamps in the sentence re-
fer to the root verb. If no timestamp is given, we
use the article date.
3.1 Temporal Clustering
After acquiring the timestamps, we must hierar-
chically cluster the sentences into sets that make
sense to summarize together. Since we wish to
partition along the temporal dimension, our prob-
lem reduces to identifying the best dates at which
to split a cluster into subclusters. We identify these
dates by looking for bursts of activity.
News tends to be bursty ? many articles on a
topic appear at once and then taper out (Kleinberg,
2002). For example, Figure 3 shows the number of
articles per day related to 1998 embassy bombings
published in the New York Times (identified using
a key word search). There were two main events
? on the 7th, the embassies were bombed and
on the 20th, US retaliated through missile strikes.
The figure shows a correspondence between these
events and news spikes.
Ideal splits for this example would occur just
before each spike in coverage. However, when
there is little differentiation in news coverage, we
prefer clusters evenly spaced across time. We thus
choose clusters C = {c
1
, . . . , c
k
} as follows:
maximize
C
B(C) + ?E(C)
(1)
where C is a clustering, B(C) is the burstiness
of the set of clusters, E(C) is the evenness of the
clusters, and ? is the tradeoff parameter.
B(C) =
X
c2C
burst(c) (2)
burst(c) is the difference in the number of sen-
tences published the day before the first date in c
and the average number of sentences published on
the first and second date of c:
burst(c) =
pub(d
i
) + pub(d
i+1
)
2
  pub(d
i 1
) (3)
where d is a date indexed over time, such that d
j
is a day before d
j+1
, and d
i
is the first date in c.
Figure 2: Examples of input and output o hierarchical sum-
marization. The input sentences ar s ? S, the number of
input sentences is N , and the summary sentences are x ? X .
child summaries.
We simplify the problem by decomposing it into
two steps: hierarchical clustering and summariz-
ing over the clustering (see Figure 2 for an exam-
ple). A hierarchical clustering is a tree in which if
a cluster g
p
is the parent of cluster g
c
, then each
sentence in g
c
is also in g
p
. This organizes the
information into manageable, semantically-related
sections and induces a hierarchical structure over
the input.
The hierarchical clustering serves as input to the
second step ? summarizing given the hierarchy.
The hierarchical summary follows the hierarchi-
cal structure of the clustering. Each node in the
hierarchy has an associated flat summary, which
summarizes the senten in that cluster. More-
over, the number of sent es in a flat summary is
exactly equal to the number of child clusters of the
node, since the user will click a sentence to get to
the child summary. See Figure 2 for an illustration
of this correspondence.
Because we are inter st d in temporal hierar-
chical summarization, we hierarchically cluster all
the sentences in the input documents by time.
Unfortunately, neither agglomerative nor divisive
clustering is suitable, since both assume a binary
split at each no e (Berkhin, 2006). The number of
clusters at each split should be what is most natural
for the input data. We design a recursive clustering
algorithm that automatically chooses the appropri-
ate number of clusters at each split.
Before clustering, we timestamp all sentences.
We use SUTime (Chang and Manning, 2012) to
normalize temporal references, and we parse the
sentences with the Stanford parser (Klein and
Manning, 2003) and use a set of simple heuristics
to determine if the timestamps in the sentence re-
fer to the root verb. If no timestamp is given, we
use the article date.
3.1 Temporal Clustering
After acquiring the timestamps, we must hierar-
chically cluster the sentences into sets that make
sense to summarize together. Since we wish to
partition along the temporal dimension, our prob-
lem reduces to identifying the best dates at which
to split a cluster into subclusters. We identify these
dates by looking for bursts of activity.
News tends to be bursty ? many articles on a
topic appear at once and then taper out (Kleinberg,
2002). For exampl , Figure 3 show the number of
articles per day related to the 1998 embassy bomb-
ings published in the New York Times (identified
using a key word search). There were two main
events ? on the 7th, the embassies were bombed
an on the 20th, the US retaliated through mis-
sile strikes Th figure shows a correspondence
between these events and news spikes.
Ideal splits for this example would occur just
before each spike in coverage. However, when
there is little differentiation in news coverage, we
prefer clusters e enly spaced across time. We thus
choose clusters C = {c
1
, . . . , c
k
} as follows:
maximize
C
B(C) + ?E(C)
(1)
where is a clustering, B(C) is the burstiness
of the set of clusters, E(C) is the evenness of the
clusters, and ? is the trad off parameter.
B(C) =
?
c?C
burst(c) (2)
burst(c) is the difference in the number of sen-
tences published the day before the first date in c
and the average number of sentences published on
the first a d sec nd date of c:
burst(c) =
b(d
i
) + pub(d
i+1
)
2
? pub(d
i?1
) (3)
where d is a date indexed over time, such that d
j
is a day before d
j+1
, and d
i
is the first date in c.
pub(d
i
) is the number of sentences published on
d
i
. The evenness of the split is measured by:
E(C) = min
c?C
size(c) (4)
where size(c) is the number of dates in cluster c.
We perform hierarchical clustering top-down, at
each point solving for Equation 1. ? was set using
a grid-search over a development set.
904
6 8 10 12 14 16 18 20 22 24
0
20
40
Day of Month
N
u
m
b
e
r
o
f
A
r
t
i
c
l
e
s
1
Figure 3: News coverage by date for the embassy bombings
in Tanzania and Kenya. There are spikes in the number of
articles published at the two major events.
3.2 Choosing the number of clusters
We cannot know a priori the number of clusters
for a given topic. However, when the number of
clusters is too large for the given summary budget,
the sentences will have to be too short, and when
the number of clusters is too small, we will not use
enough of the budget. We set the maximum num-
ber of clusters k
max
and minimum number of clus-
ters k
min
to be a function of the budget b and the
average sentence length in the cluster s
avg
, such
that k
max
? s
avg
? b and k
min
? s
avg
? b/2.
Given a maximum and minimum number of
clusters, we must determine the appropriate num-
ber of clusters. At each level, we cluster the sen-
tences by the method described above and choose
the number of clusters k according to the gap
statistic (Tibshirani et al, 2000). Specifically, for
each level, the algorithm will cluster repeatedly
with k varying from the minimum to the maxi-
mum. The algorithm will return the k that max-
imizes the gap statistic:
Gap
n
(k) = E
?
n
{log(W
k
)} ? log(W
k
) (5)
where W
k
is the score for the clusters computed
with Equation 1, and E
?
n
is the expectation under
a sample of size n from a reference distribution.
Ideally, the maximum depth of the clustering
would be a function of the number of sentences
in each cluster, but in our implementation, we set
the maximum depth to three, which works well for
the size of the datasets we use (300 articles).
4 Summarizing within the Hierarchy
After the sentences are clustered, we have a struc-
ture for the hierarchical summary that dictates the
number of summaries and the number of sentences
in each summary. We also have the set of sen-
tences from which each summary is drawn.
Intuitively, each cluster summary in the hierar-
chical summary should convey the most salient
information in that cluster. Furthermore, the hier-
archical summary should not include redundant
sentences. A hierarchical summary that is only
salient and nonredundant may still not be suitable
if the sentences within a cluster summary are dis-
connected or if the parent sentence for a summary
does not relate to the child summary. Thus, a hi-
erarchical summary must also have intra-cluster
coherence and parent-to-child coherence.
4.1 Salience
Salience is the value of each sentence to the topic
from which the documents are drawn. We measure
salience of a summary (Sal(X)) as the sum of the
saliences of individual sentences (
?
i
Sal(x
i
)).
Following previous research in MDS, we com-
puted individual saliences using a linear regres-
sion classifier trained on ROUGE scores over the
DUC?03 dataset (Lin, 2004; Christensen et al,
2013). This method finds those sentences more
salient that mention nouns or verbs that occur fre-
quently in the cluster.
In preliminary experiments, we noticed that
many sentences that were reaction sentences were
given a higher salience than action sentences. For
example, the reaction sentence, ?President Clinton
vowed to track down the perpetrators behind the
bombs that exploded outside the embassies in Tan-
zania and Kenya on Friday,? would have a higher
score than the action sentence, ?Bombs exploded
outside the embassies in Tanzania and Kenya on
Friday.? This problem occurs because the first sen-
tence has a higher ROUGE score (it covers more
important words than the second sentence). To ad-
just for this problem, we use only words identified
in the main clause (heuristically identified via the
parse tree) to compute our salience scores.
4.2 Redundancy
We identify redundant sentences using a linear
regression classifier trained on a manually la-
beled subset of the DUC?03 sentences. The fea-
tures include shared noun counts, sentence length,
TF*IDF cosine similarity, timestamp difference,
and features drawn from information extraction
such as number of shared tuples in Open IE
(Mausam et al, 2012).
905
4.3 Summary Coherence
We require two types of coherence: coherence be-
tween the parent and child summaries and coher-
ence within each summary X
i
.
We rely on the approximate discourse graph
(ADG) that was proposed in (Christensen et al,
2013) as the basis for measuring coherence. Each
node in the ADG is a sentence from the dataset.
An edge from sentence s
i
to s
j
with positive
weight indicates that s
j
may follow s
i
in a coher-
ent summary, e.g. continued mention of an event
or entity, or coreference link between s
i
and s
j
.
A negative edge indicates an unfulfilled discourse
cue or co-reference mention.
Parent-to-Child Coherence: Users navigate the
hierarchical summary from parent sentence to
child summary, so if the parent sentence bears no
relation to the child summary, the user will be un-
derstandably confused. The parent sentence must
have positive evidence of coherence with the sen-
tences in its child summary.
We estimate parent to child coherence as the co-
herence between a parent sentence and each sen-
tence in its child summary as:
PCoh(X) =
?
c?C
?
i=1..|X
c
|
w
G+
(x
p
c
, x
c,i
)) (6)
where x
p
c
is the parent sentence for cluster c and
w
G+
(x
p
c
, x
c,i
) is the sum of the positive edge
weights from x
p
c
to x
c,i
in the ADG G.
Intra-cluster Coherence: In traditional MDS, the
documents are usually quite focused, allowing for
highly focused summaries. In hierarchical sum-
marization, however, a cluster summary may span
hundreds of documents and a wide range of infor-
mation. For this reason, we may consider a sum-
mary acceptable even if it has limited positive evi-
dence of coherence in the ADG, as long as there
is no negative evidence in the form of negative
edges. For example, the following is a reasonable
summary for events spanning two weeks:
s
1
Bombs exploded at two US embassies.
s
2
US missiles struck in Afghanistan and Sudan.
Our measure of intra-cluster coherence mini-
mizes the number of missing references. These
are coreference mentions or discourse cues where
none of the sentences read before (either in an an-
cestor summary or in the current summary) con-
tain an antecedent:
CCoh(X) = ?
?
c?C
?
i=1..|X
c
|
#missingRef(x
c,i
) (7)
4.4 Objective Function
Having estimated salience, redundancy, and two
forms of coherence, we can now put this informa-
tion together into a single objective function that
measures the quality of a candidate hierarchical
summary.
Intuitively, the objective function should bal-
ance salience and coherence. Furthermore, the
summary should not contain redundant informa-
tion and each cluster summary should honor the
given budget, i.e., maximum summary length b.
We treat redundancy and budget as hard con-
straints and coherence and salience as soft con-
straints. Lastly, we require that sentences are
drawn from the cluster that they represent and that
the number of sentences in the summary corre-
sponding to each non-leaf cluster c is equivalent
to the number of child clusters of c. We optimize:
maximize: F (x) , Sal(X) + ?PCoh(X) + ?CCoh(X)
s.t. ?c ? C :
?
i=1..|X
c
|
len(x
c,i
) < b
?x
i
, x
j
? X : redundant(x
i
, x
j
) = 0
?c ? C, ?x
c
? X
c
: x
c
? c
?c ? C : |X
c
| = #children(c)
The tradeoff parameters ? and ? were set based
on a development set.
4.5 Algorithm
Optimizing this objective function is NP-hard, so
we approximate a solution by using beam search
over the space of partial hierarchical summaries.
Notice the contribution from a sentence depends
on individual salience, coherence (CCoh) based
on sentences visible on the user path down the hi-
erarchy to this sentence, and coherence (PCoh)
based on its parent sentence and its child sum-
mary. Since most of the sentence contributions de-
pend on the path from the root to the sentence, we
build our partial summary by incrementally adding
a sentence top-down in the hierarchy and from first
sentence to last within a cluster summary.
To account for PCoh, we estimate the contribu-
tion of the sentence by jointly identifying its best
child summary. However, we do not fix the child
summary at this time ? we simply use it to estimate
PCohwhen using that sentence. Since computing
the best child summary is also intractable we ap-
proximate a solution by a local search algorithm
over the child cluster.
Overall, our algorithm is a two level nested
search algorithm ? beam search in the outer loop to
906
search through the space of partial summaries and
local search (hill climbing with random restarts) in
the inner loop to pick the best sentence to add to
the existing partial summary. We use a beam of
size ten in our implementation.
5 Experiments
Our experiments are designed to evaluate how ef-
fective hierarchical summarization is in summa-
rizing a large, complex topic and how well this
helps users learn about the topic. Our evaluation
addresses the following questions:
? Do users prefer hierarchical summaries for
topic exploration? (Section 5.1)
? Are hierarchical summaries more effective
than other methods for learning about com-
plex events? (Section 5.2)
? How informative are the hierarchical sum-
maries compared to the other methods? (Sec-
tion 5.3)
? How coherent is the hierarchical structure in
the summaries? (Section 5.4)
We compared SUMMA against two baseline sys-
tems which represent the main NLP methods for
large-scale summarization: an algorithm for cre-
ating timelines over sentences (Chieu and Lee,
2004),
3
and a state-of-the-art flat MDS system
(Lin and Bilmes, 2011).
4
Each system was given
the same budget (over 10 times the traditional
MDS budget, which is 665 bytes).
We evaluated the questions on ten news topics,
representing a range of tasks: (1) Pope John Paul
II?s death and the 2005 Papal Conclave, (2) Bush v.
Gore, (3) the Tulip Revolution, (4) Daniel Pearl?s
kidnapping, (5) the Lockerbie bombing handover
of suspects, (6) the Kargil War, (7) NATO?s bomb-
ing of Yugoslavia in 1999, (8) Pinochet?s arrest in
London, (9) the 2005 London bombings, and (10)
the crash and investigation of SwissAir Flight 111.
We chose topics containing a set of related events
that unfolded over several months and were promi-
nent enough to be reported in at least 300 articles.
We drew our articles from the Gigaword corpus,
which contains articles from the New York Times
and other major newspapers. For each topic, we
used the 300 documents that best matched a key
3
Unfortunately, we were unable to obtain more recent
timeline systems from authors of the systems.
4
(Christensen et al, 2013) is a state-of-the-art coherent
MDS system, but does not scale to 300 documents.
word search. We selected topics which were be-
tween five and fifteen years old so that evaluators
would have relatively less pre-existing knowledge
about the topic.
5.1 User Preference
In our first experiment, we simply wished to eval-
uate which system users most prefer. We hired
Amazon Mechanical Turk (AMT) workers and as-
signed two topics to each worker. We paired up
workers such that one worker would see output
from SUMMA for the first topic and a competing
system for the second and the other worker would
see the reverse. For quality control, we asked
workers to complete a qualification task first, in
which they were required to write a short summary
of a news article. We also manually removed spam
from our results. Previous work has used AMT
workers for summary evaluations and has shown
high correlations with expert ratings (Christensen
et al, 2013). Five workers were hired to view each
topic-system pair.
We asked the workers to choose which format
they preferred and to explain why. The results are
as follows:
SUMMA 76% TIMELINE 24%
SUMMA 92% FLAT-MDS 8%
Users preferred the hierarchical summaries
three times more often than timelines and over
ten times more often than flat summaries. When
we examined the reasons given by the users, we
found that the people who preferred the hierar-
chical summaries liked that they gave a big pic-
ture overview and were then allowed to drill down
deeper. Some also explained that it was eas-
ier to remember information when presented with
the overview first. Typical responses included,
?Could gather and absorb the information at my
own pace,? and, ?Easier to follow and understand.?
When users preferred the timelines, they usually
remarked that it was more familiar, i.e. ?I liked
the familiarity of the format. I am used to these
timelines and they feel comfortable.? Users com-
plained that the flat summaries were disjointed,
confusing, and very frustrating to read.
5.2 Knowledge Acquisition
Evaluating how much a user learned is inherently
difficult, more so when the goal is to allow the user
the freedom to explore information based on indi-
vidual interest. For this reason, instead of asking a
set of predefined questions, we assess the knowl-
907
edge gain by following the methodology of (Sha-
haf et al, 2012) ? asking users to write a paragraph
summarizing the information learned.
Using the same setup as in the previous exper-
iment, for each topic, five AMT workers spent
three minutes reading through a timeline or sum-
mary and were then asked to write a description
of what they had learned. Workers were not al-
lowed to see the timeline or summary while writ-
ing. We collected five descriptions for each topic-
system combination. We then asked other AMT
workers to read and compare the descriptions writ-
ten by the first set of workers. Each evaluator was
presented with a corresponding Wikipedia article
and descriptions from a pair of users (timeline vs.
SUMMA or flat MDS vs. SUMMA). The descrip-
tions were randomly ordered to remove bias. The
workers were asked which user appeared to have
learned more and why. For each pair of descrip-
tions, four workers evaluated the pair. Standard
checks such as approval rating, location filtering,
etc. were used for removing spam. The results of
this experiment are as follows:
Prefer Indiff. Prefer
SUMMA 58% 17% TIMELINE 25%
SUMMA 40% 22% FLAT-MDS 38%
Descriptions written by workers using SUMMA
were preferred over twice as often as those from
timelines. We looked more closely at those cases
where the participants either preferred the time-
lines or were indifferent and found that this pref-
erence was most common when the topic was not
dominated by a few major events, but was instead
a series of similarly important events. For exam-
ple, in the kidnapping and beheading of Daniel
Pearl there were two or three obviously major
events, whereas in the Kargil War there were many
smaller important events. In latter cases, the hier-
archical summaries provided little advantage over
the timelines because it was more difficult to ar-
range the sentences hierarchically.
Since SUMMA was judged to be so much supe-
rior to flat MDS systems in Section 5.1, it is sur-
prising that users descriptions from flat MDS were
preferred nearly as often as those from SUMMA.
While the flat summaries were disjointed, they
were good at including salient information, with
the most salient tending to be near the start of the
summary. Thus, descriptions from both SUMMA
and flat MDS generally covered the most salient
information.
5.3 Informativeness
In this experiment, we assess the salience of the
information captured by the different systems, and
the ability of SUMMA to organize the information
so that more important information is placed at
higher levels.
ROUGE Evaluation: We first automatically
assessed informativeness by calculating the
ROUGE-1 scores of the output of each of the sys-
tems. For the gold standard comparison summary,
we use the Wikipedia articles for the topics.
5
Note that there is no good translation of ROUGE
for hierarchical summarization. Thus, we simply
use the traditional ROUGE metric, which will
not capture any of the hierarchical format. This
score will essentially serve as a rough measure of
coverage of the entire summary to the Wikipedia
article. The scores for each of the systems are as
follows:
P R F1
SUMMA 0.25 0.67 0.31
TIMELINE 0.28 0.65 0.33
FLAT-MDS 0.30 0.64 0.34
None of the differences are significant. From
this evaluation, one can gather that the systems
have similar coverage of the Wikipedia articles.
Manual Evaluation: While ROUGE serves as a
rough measure of coverage, we were interested in
gathering more fine-grained information on the in-
formativeness of each system. We performed an
additional manual evaluation that assesses the re-
call of important events for each system.
We first identified which events were most im-
portant in a news story. Because reading 300 arti-
cles per topic is impractical, we asked AMT work-
ers to read a Wikipedia article on the same topic
and then identify the three most important events
and the five most important secondary events. We
aggregated responses from ten workers per topic
and chose the three most common primary and five
most common secondary events.
One of the authors then manually identified the
presence of these events in the hierarchical sum-
maries, the timelines and the flat MDS summaries.
Below we show event recall (the percentage of the
events that were mentioned).
5
We excluded one topic (the handover of the Lockerbie
bombing suspects) because the corresponding Wikipedia ar-
ticle had insufficient information.
908
Events SUMMA TIMELINE FLAT-MDS
Prim. 96% 74% 93%
Sec. 76% 53% 64%
The difference in recall between SUMMA and
TIMELINE was significant in both cases, and the
difference between SUMMA and FLAT-MDS was
not. In general, the flat summaries were quite re-
dundant, which contributed to the slightly lower
event recall. The timelines, on the other hand,
were both incoherent and at the same time re-
ported less important facts.
We also evaluated at what level in the hierar-
chy the events were identified for the hierarchical
summaries. The event recall shows the percentage
of events mentioned at that level or above in the
hierarchical summary:
Events Level 1 Level 2 Level 3
Prim. 63% 81% 96%
Sec. 27% 51% 76%
81% of the primary events are present in the first
or second level, and 76% of the secondary events
are mentioned by the third level. While recog-
nizing primary events is relatively simple because
they are repeated frequently, identification of im-
portant secondary events often requires external
knowledge.
5.4 Parent-to-Child Coherence
We next tested the hierarchical coherence. One of
the authors graded how much each non-leaf sen-
tence in a summary was coherent with its child
summary on a scale of one to five, with one be-
ing incoherent and five being perfectly coherent.
We used the coherence scale from DUC?04.
6
Level 1 Level 2
Coherence 3.8 3.4
We found that for the top level of the summary,
the parent sentence generally represented the most
important event in the cluster and the child sum-
mary usually expressed details or reactions of the
event. The lower coherence scores were often the
result of too few lexical connections or lack of a
theme or story. While the facts of the sentences
made sense together, the summaries sometimes
did not read as if they were written by a human,
but as a series of disparate sentences.
For the second level, the problems were more
basic. The parent sentence occasionally expressed
a less important fact that the child summary did
6
http://duc.nist.gov/duc2004/quality.questions.txt
not then expand on or, more commonly, the child
summary was not focused enough. This result
stems from two problems in our algorithm. First,
summarizing sentences are rare, making good
choices for parent sentences difficult to find. The
second problem relates to the difficulty in identify-
ing whether two sentences are on the same topic.
For example, suppose the parent sentence is, ?A
Swissair plane Wednesday night crashed off Nova
Scotia, Canada.? A very good child sentence is,
?The airline confirmed that all passengers died.?
However, based on their surface features, the sen-
tence, ?A plane made an unscheduled landing after
a Swissair plane crashed off the coast of Canada,?
appears to be a better choice.
Even though there is scope for improvement, we
find these coherence scores encouraging for a first
algorithm for the task.
6 Related Work
Traditional approaches to large-scale summariza-
tion have included flat summaries and timelines.
There are two primary shortcomings to these ap-
proaches: first, they require the user to sort
through large amounts of potentially overwhelm-
ing information, and second, the output is static
? users with different interests will see the same
information. Below we describe related work on
traditional MDS, structured summaries, timelines,
discovering threads of documents and the uses of
hierarchies in generating summaries.
6.1 Traditional MDS
Traditionally, MDS systems have focused on three
to six sentence summaries covering 10-15 docu-
ments. Most extractive summarization research
aims to maximize coverage while reducing redun-
dancy (e.g. (Carbonell and Goldstein, 1998; Sag-
gion and Gaizauskas, 2004; Radev et al, 2004)).
Lin and Bilmes (2011) proposed a state-of-the-art
system that uses submodularity in sentence selec-
tion to accomplish these goals. Christensen et al
(2013) presented an algorithm for coherent MDS,
but it does not scale to larger output.
Structured Summaries: Some research has ex-
plored generating structured summaries. These
approaches attempt to identify major aspects of
a topic, but do not compile content to describe
those aspects. Rather, they rely on pre-existing, la-
beled paragraphs (for example, a paragraph titled,
?Symptoms of Meningitis?). Aspects are identi-
fied either by a training corpus of articles in the
909
same domain (Sauper and Barzilay, 2009), by an
entity-aspect LDA model (Li et al, 2010), or by
Wikipedia templates of related topics (Yao et al,
2011). These methods assume a common struc-
ture for all topics in a category, and do not allow
for more than two levels in the structure.
Timeline Generation: Recent papers in timeline
generation have emphasized the relationship with
summarization. Yan et al (2011b) balanced co-
herence and diversity to create timelines, Yan et
al. (2011a) used inter-date and intra-date sentence
dependencies, and Chieu and Lee (2004) used sen-
tence similarity. Others have emphasized identify-
ing important dates, primarily by bursts of news
(Swan and Allen, 2000; Akcora et al, 2010; Hu
et al, 2011; Kessler et al, 2012). While time-
lines can be useful for understanding events, they
do not generalize to other domains. Additionally,
long timelines can be overwhelming, short time-
lines have low information content, and there is
no method for personalized exploration.
Document Threads: A related track of research
investigates discovering threads of documents.
While we aim to summarize collections of infor-
mation, this track seeks to identify relationships
between documents. This research operates on the
document level, while ours operates on the sen-
tence level. Shahaf and Guestrin (2010) formal-
ized the characteristics of a good chain of articles
and proposed an algorithm to connect two speci-
fied articles. Gillenwater et al (2012) proposed
a probabilistic technique for extracting a diverse
set of threads from a given collection. Shahaf et
al. (2012) extended work on coherent threads to
finding coherent maps of documents, where a map
is set of intersecting threads representing how the
threads interact and relate.
Summarization and Hierarchies: A few papers
have examined the relationship between summa-
rization and hierarchies. Some focused on cre-
ating a hierarchical summary of a single docu-
ment (Buyukkokten et al, 2001; Otterbacher et
al., 2006), relying on the structure inherent in sin-
gle documents. Others investigated creating hier-
archies of words or phrases to organize documents
(Lawrie et al, 2001; Lawrie, 2003; Takahashi et
al., 2007; Haghighi and Vanderwende, 2009).
Other research identifies the hierarchical struc-
ture of the documents and generates a summary
that prioritizes more general information accord-
ing to the structure (Ouyang et al, 2009; Celikyil-
maz and Hakkani-Tur, 2010), or gains coverage by
drawing sentences from different parts of the hier-
archy (Yang and Wang, 2003; Wang et al, 2006).
7 Conclusions
We have introduced a new paradigm for large-
scale summarization called hierarchical summa-
rization, which allows a user to navigate a hier-
archy of relatively short summaries. We present
SUMMA, an implemented hierarchical news sum-
marization system,
7
and demonstrate its effective-
ness in a user study that compares SUMMA with
a timeline system and a flat MDS system. When
compared to timelines, users learned more with
SUMMA in twice as many cases, and SUMMA was
preferred more than three times as often. When
compared to flat summaries, users overwhelming
preferred SUMMA and learned just as much.
This first implementation performs temporal
clustering ? in future work, we will investigate dy-
namically selecting an organizing principle that is
best suited to the data at each level of the hierar-
chy: by entity, by location, by event, or by date.
We also intend to scale the system to even larger
document collections, and explore joint clustering
and summarization. Lastly, we plan to research
hierarchical summarization in other domains.
Acknowledgments
We thank Amitabha Bagchi, Niranjan Balasubra-
manian, Danish Contractor, Oren Etzioni, Tony
Fader, Carlos Guestrin, Prachi Jain, Lucy Van-
derwende, Luke Zettlemoyer, and the anonymous
reviewers for their helpful suggestions and feed-
back. We thank Hui Lin and Jeff Bilmes for
providing us with their code. This research was
supported in part by ARO contract W911NF-
13-1-0246, DARPA Air Force Research Labora-
tory (AFRL) contract FA8750-13-2-0019, UW-
IITD subcontract RP02815, and the Yahoo! Fac-
ulty Research and Engagement Award. This pa-
per is also supported in part by the Intelligence
Advanced Research Projects Activity (IARPA)
via AFRL contract number FA8650-10-C-7058.
The U.S. Government is authorized to reproduce
and distribute reprints for Governmental purposes
notwithstanding any copyright annotation thereon.
The views and conclusions contained herein are
those of the authors and should not be interpreted
as necessarily representing the official policies
or endorsements, either expressed or implied, of
IARPA, AFRL, or the U.S. Government.
7
http://knowitall.cs.washington.edu/summa/
910
References
C. G. Akcora, M. A. Bayir, M. Demirbas, and H. Fer-
hatosmanoglu. 2010. Identifying breakpoints in
public opinion. In 1st KDD Workshop on Social Me-
dia Analytics.
Berkhin Berkhin. 2006. A survey of clustering data
mining techniques. Grouping Multidimensional
Data, pages 25?71.
Orkut Buyukkokten, Hector Garcia-Molina, and An-
dreas Paepcke. 2001. Seeing the whole in parts:
Text summarization for web browsing on handheld
devices. In Proceedings of WWW 2001, pages 652?
662.
Jaime Carbonell and Jade Goldstein. 1998. The use of
MMR, diversity-based reranking for reordering doc-
uments and producing summaries. In Proceedings
of SIGIR 1998, pages 335?336.
Asli Celikyilmaz and Dilek Hakkani-Tur. 2010. A hy-
brid hierarchical model for multi-document summa-
rization. In Proceedings of ACL 2010, pages 815?
824.
Angel Chang and Christopher Manning. 2012. SU-
Time: A library for recognizing and normalizing
time expressions. In Proceedings of LREC 2012.
Hai Leong Chieu and Yoong Keok Lee. 2004. Query
based event extraction along a timeline. In Proceed-
ings of SIGIR 2004, pages 425?432.
Janara Christensen, Mausam, Stephen Soderland, and
Oren Etzioni. 2013. Towards coherent multi-
document summarization. In Proceedings of
NAACL 2013.
Jennifer Gillenwater, Alex Kulesza, and Ben Taskar.
2012. Discovering diverse and salient threads in
document collections. In Proceedings of EMNLP-
CoNLL 2012, pages 710?720.
Aria Haghighi and Lucy Vanderwende. 2009. Explor-
ing content models for multi-document summariza-
tion. Proceedings of NAACL 2009, pages 362?370.
Po Hu, Minlie Huang, Peng Xu, Weichang Li, Adam K.
Usadi, and Xiaoyan Zhu. 2011. Generating
breakpoint-based timeline overview for news topic
retrospection. In Proceedings of ICDM 2011.
Remy Kessler, Xavier Tannier, Caroline Hag`ege,
V?eronique Moriceau, and Andr?e Bittar. 2012. Find-
ing salient dates for building thematic timelines. In
Proceedings of ACL 2012, pages 730?739.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Meeting of the Association for Computational
Linguistics, pages 423?430.
Jon Kleinberg. 2002. Bursty and hierarchical struc-
ture in streams. In Proceedings of the Eighth ACM
SIGKDD International Conference on Knowledge
Discovery and Data Mining, KDD ?02, pages 91?
101.
Dawn Lawrie, W. Bruce Croft, and Arnold Rosenberg.
2001. Finding topic words for hierarchical summa-
rization. In Proceedings of SIGIR ?01, pages 349?
357.
Dawn J. Lawrie. 2003. Language models for hierar-
chical summarization. Ph.D. thesis, University of
Massachusetts Amherst.
Peng Li, Jing Jiang, and Yinglin Wang. 2010. Gener-
ating templates of entity summaries with an entity-
aspect model and pattern mining. In Proceedings of
ACL 2010, pages 640?649.
Hui Lin and Jeff Bilmes. 2011. A class of submodular
functions for document summarization. In Proceed-
ings of ACL 2011, pages 510?520.
Chin-Yew Lin. 2004. ROUGE: A package for au-
tomatic evaluation of summaries. In Text Summa-
rization Branches Out: Proceedings of the ACL-04
Workshop, pages 74?81.
Mausam, Michael Schmitz, Robert Bart, Stephen
Soderland, and Oren Etzioni. 2012. Open language
learning for information extraction. In Proceedings
of EMNLP 2012, pages 523?534.
Jahna Otterbacher, Dragomir Radev, and Omer Ka-
reem. 2006. News to go: Hierarchical text sum-
marization for mobile devices. In Proceedings of
SIGIR 2006, pages 589?596.
You Ouyang, Wenji Li, and Qin Lu. 2009. An
integrated multi-document summarization approach
based on word hierarchical representation. In Pro-
ceedings of the ACLShort 2009, pages 113?116.
Dragomir R. Radev, Hongyan Jing, Malgorzata Stys,
and Daniel Tam. 2004. Centroid-based summariza-
tion of multiple documents. Information Processing
and Management, 40(6):919?938.
Horacio Saggion and Robert Gaizauskas. 2004. Multi-
document summarization by cluster/profile rele-
vance and redundancy removal. In Proceedings of
DUC 2004.
Christina Sauper and Regina Barzilay. 2009. Automat-
ically generating Wikipedia articles: A structure-
aware approach. In Proceedings of ACL 2009, pages
208?216.
Dafna Shahaf and Carlos Guestrin. 2010. Connecting
the dots between news articles. In Proceedings of
KDD 2010, pages 623?632.
Dafna Shahaf, Carlos Guestrin, and Eric Horvitz.
2012. Trains of thought: Generating information
maps. In Proceedings of WWW 2012.
911
Russell Swan and James Allen. 2000. Automatic gen-
eration of overview timelines. In Proceedings of SI-
GIR 2000, pages 49?56.
Kou Takahashi, Takao Miura, and Isamu Shioya. 2007.
Hierarchical summarizing and evaluating for web
pages. In Proceedings of the 1st workshop on
emerging research opportunities for Web Data Man-
agement (EROW 2007).
Robert Tibshirani, Guenther Walther, and Trevor
Hastie. 2000. Estimating the number of clusters in
a dataset via the gap statistic. Journal of the Royal
Statistical Society, Series B, 32(2):411?423.
Fu Lee Wang, Christopher C. Yang, and Xiaodong Shi.
2006. Multi-document summarization for terrorism
information extraction. In Proceedings of ISI?06.
Rui Yan, Liang Kong, Congrui Huang, Xiaojun Wan,
Xiaoming Li, and Yan Zhang. 2011a. Timeline gen-
eration through evolutionary trans-temporal summa-
rization. In Proceedings of EMNLP 2011, pages
433?443.
Rui Yan, Xiaojun Wan, Jahna Otterbacher, Liang Kong,
Xiaoming Li, and Yan Zhang. 2011b. Evolutionary
timeline summarization: A balanced optimization
framework via iterative substitution. In Proceeding
of SIGIR 2011, pages 745?754.
Christopher C. Yang and Fu Lee Wang. 2003. Fractal
summarization: summarization based on fractal the-
ory. In Proceedings of SIGIR 2003, pages 391?392.
Conglei Yao, Xu Jia, Sicong Shou, Shicong Feng, Feng
Zhou, and Hongyan Liu. 2011. Autopedia: Auto-
matic domain-independent wikipedia article genera-
tion. In Proceedings of WWW 2011, pages 161?162.
912
Proceedings of the NAACL HLT 2010 First International Workshop on Formalisms and Methodology for Learning by Reading, pages 52?60,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Semantic Role Labeling for Open Information Extraction
Janara Christensen, Mausam, Stephen Soderland and Oren Etzioni
University of Washington, Seattle
Abstract
Open Information Extraction is a recent
paradigm for machine reading from arbitrary
text. In contrast to existing techniques, which
have used only shallow syntactic features, we
investigate the use of semantic features (se-
mantic roles) for the task of Open IE. We com-
pare TEXTRUNNER (Banko et al, 2007), a
state of the art open extractor, with our novel
extractor SRL-IE, which is based on UIUC?s
SRL system (Punyakanok et al, 2008). We
find that SRL-IE is robust to noisy heteroge-
neous Web data and outperforms TEXTRUN-
NER on extraction quality. On the other
hand, TEXTRUNNER performs over 2 orders
of magnitude faster and achieves good pre-
cision in high locality and high redundancy
extractions. These observations enable the
construction of hybrid extractors that output
higher quality results than TEXTRUNNER and
similar quality as SRL-IE in much less time.
1 Introduction
The grand challenge of Machine Reading (Etzioni
et al, 2006) requires, as a key step, a scalable
system for extracting information from large, het-
erogeneous, unstructured text. The traditional ap-
proaches to information extraction (e.g., (Soderland,
1999; Agichtein and Gravano, 2000)) do not oper-
ate at these scales, since they focus attention on a
well-defined small set of relations and require large
amounts of training data for each relation. The re-
cent Open Information Extraction paradigm (Banko
et al, 2007) attempts to overcome the knowledge
acquisition bottleneck with its relation-independent
nature and no manually annotated training data.
We are interested in the best possible technique
for Open IE. The TEXTRUNNER Open IE system
(Banko and Etzioni, 2008) employs only shallow
syntactic features in the extraction process. Avoid-
ing the expensive processing of deep syntactic anal-
ysis allowed TEXTRUNNER to process at Web scale.
In this paper, we explore the benefits of semantic
features and in particular, evaluate the application of
semantic role labeling (SRL) to Open IE.
SRL is a popular NLP task that has seen sig-
nificant progress over the last few years. The ad-
vent of hand-constructed semantic resources such as
Propbank and Framenet (Martha and Palmer, 2002;
Baker et al, 1998) have resulted in semantic role la-
belers achieving high in-domain precisions.
Our first observation is that semantically labeled
arguments in a sentence almost always correspond
to the arguments in Open IE extractions. Similarly,
the verbs often match up with Open IE relations.
These observations lead us to construct a new Open
IE extractor based on SRL. We use UIUC?s publicly
available SRL system (Punyakanok et al, 2008) that
is known to be competitive with the state of the art
and construct a novel Open IE extractor based on it
called SRL-IE.
We first need to evaluate SRL-IE?s effectiveness
in the context of large scale and heterogeneous input
data as found on the Web: because SRL uses deeper
analysis we expect SRL-IE to be much slower. Sec-
ond, SRL is trained on news corpora using a re-
source like Propbank, and so may face recall loss
due to out of vocabulary verbs and precision loss due
to different writing styles found on the Web.
In this paper we address several empirical ques-
52
tions. Can SRL-IE, our SRL based extractor,
achieve adequate precision/recall on the heteroge-
neous Web text? What factors influence the relative
performance of SRL-IE vs. that of TEXTRUNNER
(e.g., n-ary vs. binary extractions, redundancy, local-
ity, sentence length, out of vocabulary verbs, etc.)?
In terms of performance, what are the relative trade-
offs between the two? Finally, is it possible to design
a hybrid between the two systems to get the best of
both the worlds? Our results show that:
1. SRL-IE is surprisingly robust to noisy hetero-
geneous data and achieves high precision and
recall on the Open IE task on Web text.
2. SRL-IE outperforms TEXTRUNNER along di-
mensions such as recall and precision on com-
plex extractions (e.g., n-ary relations).
3. TEXTRUNNER is over 2 orders of magnitude
faster, and achieves good precision for extrac-
tions with high system confidence or high lo-
cality or when the same fact is extracted from
multiple sentences.
4. Hybrid extractors that use a combination of
SRL-IE and TEXTRUNNER get the best of
both worlds. Our hybrid extractors make effec-
tive use of available time and achieve a supe-
rior balance of precision-recall, better precision
compared to TEXTRUNNER, and better recall
compared to both TEXTRUNNER and SRL-IE.
2 Background
Open Information Extraction: The recently pop-
ular Open IE (Banko et al, 2007) is an extraction
paradigm where the system makes a single data-
driven pass over its corpus and extracts a large
set of relational tuples without requiring any hu-
man input. These tuples attempt to capture the
salient relationships expressed in each sentence. For
instance, for the sentence, ?McCain fought hard
against Obama, but finally lost the election? an
Open IE system would extract two tuples <McCain,
fought (hard) against, Obama>, and <McCain, lost,
the election>. These tuples can be binary or n-ary,
where the relationship is expressed between more
than 2 entities such as <Gates Foundation, invested
(arg) in, 1 billion dollars, high schools>.
TEXTRUNNER is a state-of-the-art Open IE sys-
tem that performs extraction in three key steps. (1)
A self-supervised learner that outputs a CRF based
classifier (that uses unlexicalized features) for ex-
tracting relationships. The self-supervised nature al-
leviates the need for hand-labeled training data and
unlexicalized features help scale to the multitudes of
relations found on the Web. (2) A single pass extrac-
tor that uses shallow syntactic techniques like part of
speech tagging, noun phrase chunking and then ap-
plies the CRF extractor to extract relationships ex-
pressed in natural language sentences. The use of
shallow features makes TEXTRUNNER highly effi-
cient. (3) A redundancy based assessor that re-ranks
these extractions based on a probabilistic model of
redundancy in text (Downey et al, 2005). This ex-
ploits the redundancy of information in Web text and
assigns higher confidence to extractions occurring
multiple times. All these components enable TEX-
TRUNNER to be a high performance, general, and
high quality extractor for heterogeneous Web text.
Semantic Role Labeling: SRL is a common NLP
task that consists of detecting semantic arguments
associated with a verb in a sentence and their classi-
fication into different roles (such as Agent, Patient,
Instrument, etc.). Given the sentence ?The pearls
I left to my son are fake? an SRL system would
conclude that for the verb ?leave?, ?I? is the agent,
?pearls? is the patient and ?son? is the benefactor.
Because not all roles feature in each verb the roles
are commonly divided into meta-roles (A0-A7) and
additional common classes such as location, time,
etc. Each Ai can represent a different role based
on the verb, though A0 and A1 most often refer to
agents and patients respectively. Availability of lexi-
cal resources such as Propbank (Martha and Palmer,
2002), which annotates text with meta-roles for each
argument, has enabled significant progress in SRL
systems over the last few years.
Recently, there have been many advances in SRL
(Toutanova et al, 2008; Johansson and Nugues,
2008; Coppola et al, 2009; Moschitti et al, 2008).
We use UIUC-SRL as our base SRL system (Pun-
yakanok et al, 2008). Our choice of the system is
guided by the fact that its code is freely available and
it is competitive with state of the art (it achieved the
highest F1 score on the CoNLL-2005 shared task).
UIUC-SRL operates in four key steps: pruning,
argument identification, argument classification and
53
inference. Pruning involves using a full parse tree
and heuristic rules to eliminate constituents that are
unlikely to be arguments. Argument identification
uses a classifier to identify constituents that are po-
tential arguments. In argument classification, an-
other classifier is used, this time to assign role labels
to the candidates identified in the previous stage. Ar-
gument information is not incorporated across argu-
ments until the inference stage, which uses an inte-
ger linear program to make global role predictions.
3 SRL-IE
Our key insight is that semantically labeled argu-
ments in a sentence almost always correspond to the
arguments in Open IE extractions. Thus, we can
convert the output of UIUC-SRL into an Open IE
extraction. We illustrate this conversion process via
an example.
Given the sentence, ?Eli Whitney created the cot-
ton gin in 1793,? TEXTRUNNER extracts two tuples,
one binary and one n-ary, as follows:
binary tuple:
arg0 Eli Whitney
rel created
arg1 the cotton gin
n-ary tuple:
arg0 Eli Whitney
rel created (arg) in
arg1 the cotton gin
arg2 1793
UIUC-SRL labels constituents of a sentence with
the role they play in regards to the verb in the sen-
tence. UIUC-SRL will extract:
A0 Eli Whitney
verb created
A1 the cotton gin
temporal in 1793
To convert UIUC-SRL output to Open IE format,
SRL-IE treats the verb (along with its modifiers and
negation, if present) as the relation. Moreover, it
assumes SRL?s role-labeled arguments as the Open
IE arguments related to the relation. The arguments
here consist of all entities labeled Ai, as well as any
entities that are marked Direction, Location, or Tem-
poral. We order the arguments in the same order as
they are in the sentence and with regard to the re-
lation (except for direction, location and temporal,
which cannot be arg0 of an Open IE extraction and
are placed at the end of argument list). As we are
interested in relations, we consider only extractions
that have at least two arguments.
In doing this conversion, we naturally ignore part
of the semantic information (such as distinctions be-
tween various Ai?s) that UIUC-SRL provides. In
this conversion process an SRL extraction that was
correct in the original format will never be changed
to an incorrect Open IE extraction. However, an in-
correctly labeled SRL extraction could still convert
to a correct Open IE extraction, if the arguments
were correctly identified but incorrectly labeled.
Because of the methodology that TEXTRUNNER
uses to extract relations, for n-ary extractions of the
form <arg0, rel, arg1, ..., argN>, TEXTRUNNER
often extracts sub-parts <arg0, rel, arg1>, <arg0,
rel, arg1, arg2>, ..., <arg0, rel, arg1, ..., argN-1>.
UIUC-SRL, however, extracts at most only one re-
lation for each verb in the sentence. For a fair com-
parison, we create additional subpart extractions for
each UIUC-SRL extraction using a similar policy.
4 Qualitative Comparison of Extractors
In order to understand SRL-IE better, we first com-
pare with TEXTRUNNER in a variety of scenarios,
such as sentences with lists, complex sentences, sen-
tences with out of vocabulary verbs, etc.
Argument boundaries: SRL-IE is lenient in de-
ciding what constitutes an argument and tends to
err on the side of including too much rather than
too little; TEXTRUNNER is much more conservative,
sometimes to the extent of omitting crucial informa-
tion, particularly post-modifying clauses and PPs.
For example, TEXTRUNNER extracts <Bunsen, in-
vented, a device> from the sentence ?Bunsen in-
vented a device called the Spectroscope?. SRL-IE
includes the entire phrase ?a device called the Spec-
troscope? as the second argument. Generally, the
longer arguments in SRL-IE are more informative
than TEXTRUNNER?s succinct ones. On the other
hand, TEXTRUNNER?s arguments normalize better
leading to an effective use of redundancy in ranking.
Lists: In sentences with a comma-separated lists of
nouns, SRL-IE creates one extraction and treats the
entire list as the argument, whereas TEXTRUNNER
separates them into several relations, one for each
item in the list.
Out of vocabulary verbs: While we expected
54
TEXTRUNNER to handle unknown verbs with lit-
tle difficulty due to its unlexicalized nature, SRL-
IE could have had severe trouble leading to a lim-
ited applicability in the context of Web text. How-
ever, contrary to our expectations, UIUC-SRL has
a graceful policy to handle new verbs by attempt-
ing to identify A0 (the agent) and A1 (the patient)
and leaving out the higher numbered ones. In prac-
tice, this is very effective ? SRL-IE recognizes the
verb and its two arguments correctly in ?Larry Page
googled his name and launched a new revolution.?
Part-of-speech ambiguity: Both SRL-IE and
TEXTRUNNER have difficulty when noun phrases
have an identical spelling with a verb. For example,
the word ?write? when used as a noun causes trouble
for both systems. In the sentence, ?Be sure the file
has write permission.? SRL-IE and TEXTRUNNER
both extract <the file, write, permission>.
Complex sentences: Because TEXTRUNNER only
uses shallow syntactic features it has a harder time
on sentences with complex structure. SRL-IE,
because of its deeper processing, can better handle
complex syntax and long-range dependencies, al-
though occasionally complex sentences will create
parsing errors causing difficulties for SRL-IE.
N-ary relations: Both extractors suffer significant
quality loss in n-ary extractions compared to binary.
A key problem is prepositional phrase attachment,
deciding whether the phrase associates with arg1 or
with the verb.
5 Experimental Results
In our quantitative evaluation we attempt to answer
two key questions: (1) what is the relative difference
in performance of SRL-IE and TEXTRUNNER on
precision, recall and computation time? And, (2)
what factors influence the relative performance of
the two systems? We explore the first question in
Section 5.2 and the second in Section 5.3.
5.1 Dataset
Our goal is to explore the behavior of TEXTRUN-
NER and SRL-IE on a large scale dataset containing
redundant information, since redundancy has been
shown to immensely benefit Web-based Open IE ex-
tractors. At the same time, the test set must be a
manageable size, due to SRL-IE?s relatively slow
processing time. We constructed a test set that ap-
proximates Web-scale distribution of extractions for
five target relations ? invent, graduate, study, write,
and develop.
We created our test set as follows. We queried a
corpus of 500M Web documents for a sample of sen-
tences with these verbs (or their inflected forms, e.g.,
invents, invented, etc.). We then ran TEXTRUNNER
and SRL-IE on those sentences to find 200 distinct
values of arg0 for each target relation, 100 from each
system. We searched for at most 100 sentences that
contain both the verb-form and arg0. This resulted
in a test set of an average of 6,000 sentences per re-
lation, for a total of 29,842 sentences. We use this
test set for all experiments in this paper.
In order to compute precision and recall on this
dataset, we tagged extractions by TEXTRUNNER
and by SRL-IE as correct or errors. A tuple is cor-
rect if the arguments have correct boundaries and
the relation accurately expresses the relationship be-
tween all of the arguments. Our definition of cor-
rect boundaries does not favor either system over
the other. For instance, while TEXTRUNNER ex-
tracts <Bunsen, invented, a device> from the sen-
tence ?Bunsen invented a device called the Spectro-
scope?, and SRL-IE includes the entire phrase ?a
device called the Spectroscope? as the second argu-
ment, both extractions would be marked as correct.
Determining the absolute recall in these experi-
ments is precluded by the amount of hand labeling
necessary and the ambiguity of such a task. Instead,
we compute pseudo-recall by taking the union of
correct tuples from both methods as denominator.1
5.2 Relative Performance
Table 1 shows the performance of TEXTRUNNER
and SRL-IE on this dataset. Since TEXTRUNNER
can output different points on the precision-recall
curve based on the confidence of the CRF we choose
the point that maximizes F1.
SRL-IE achieved much higher recall at substan-
tially higher precision. This was, however, at the
cost of a much larger processing time. For our
dataset, TEXTRUNNER took 6.3 minutes and SRL-
1Tuples from the two systems are considered equivalent if
for the relation and each argument, the extracted phrases are
equal or if one phrase is contained within the phrase extracted
by the other system.
55
TEXTRUNNER SRL-IE
P R F1 P R F1
Binary 51.9 27.2 35.7 64.4 85.9 73.7
N-ary 39.3 28.2 32.9 54.4 62.7 58.3
All 47.9 27.5 34.9 62.1 79.9 69.9
Time 6.3 minutes 52.1 hours
Table 1: SRL-IE outperforms TEXTRUNNER in both re-
call and precision, but has over 2.5 orders of magnitude
longer run time.
IE took 52.1 hours ? roughly 2.5 orders of magni-
tude longer. We ran our experiments on quad-core
2.8GHz processors with 4GB of memory.
It is important to note that our results for TEX-
TRUNNER are different from prior results (Banko,
2009). This is primarily due to a few operational
criteria (such as focusing on proper nouns, filtering
relatively infrequent extractions) identified in prior
work that resulted in much higher precision, proba-
bly at significant cost of recall.
5.3 Comparison under Different Conditions
Although SRL-IE has higher overall precision,
there are some conditions under which TEXTRUN-
NER has superior precision. We analyze the perfor-
mance of these two systems along three key dimen-
sions: system confidence, redundancy, and locality.
System Confidence: TEXTRUNNER?s CRF-based
extractor outputs a confidence score which can be
varied to explore different points in the precision-
recall space. Figure 1(a) and Figure 2(a) report the
results from ranking extractions by this confidence
value. For both binary and n-ary extractions the con-
fidence value improves TEXTRUNNER?s precision
and for binary the high precision end has approxi-
mately the same precision as SRL-IE. Because of
its use of an integer linear program, SRL-IE does
not associate confidence values with extractions and
is shown as a point in these figures.
Redundancy: In this experiment we use the re-
dundancy of extractions as a measure of confidence.
Here redundancy is the number of times a relation
has been extracted from unique sentences. We com-
pute redundancy over normalized extractions, ignor-
ing noun modifiers, adverbs, and verb inflection.
Figure 1(b) and Figure 2(b) display the results for
binary and n-ary extractions, ranked by redundancy.
We use a log scale on the x-axis since high redun-
dancy extractions account for less than 1% of the
recall. For binary extractions, redundancy improved
TEXTRUNNER?s precision significantly, but at a dra-
matic loss in recall. TEXTRUNNER achieved 0.8
precision with 0.001 recall at redundancy of 10 and
higher. For highly redundant information (common
concepts, etc.) TEXTRUNNER has higher precision
than SRL-IE and would be the algorithm of choice.
In n-ary relations for TEXTRUNNER and in binary
relations for SRL-IE, redundancy actually hurts
precision. These extractions tend to be so specific
that genuine redundancy is rare, and the highest fre-
quency extractions are often systematic errors. For
example, the most frequent SRL-IE extraction was
<nothing, write, home>.
Locality: Our experiments with TEXTRUNNER led
us to discover a new validation scheme for the ex-
tractions ? locality. We observed that TEXTRUN-
NER?s shallow features can identify relations more
reliably when the arguments are closer to each other
in the sentence. Figure 1(c) and Figure 2(c) report
the results from ranking extractions by the number
of tokens that separate the first and last arguments.
We find a clear correlation between locality and
precision of TEXTRUNNER, with precision 0.77 at
recall 0.18 for TEXTRUNNER where the distance is
4 tokens or less for binary extractions. For n-ary re-
lations, TEXTRUNNER can match SRL-IE?s preci-
sion of 0.54 at recall 0.13. SRL-IE remains largely
unaffected by locality, probably due to the parsing
used in SRL.
6 A TEXTRUNNER SRL-IE Hybrid
We now present two hybrid systems that combine
the strengths of TEXTRUNNER (fast processing time
and high precision on a subset of sentences) with the
strengths of SRL-IE (higher recall and better han-
dling of long-range dependencies). This is set in a
scenario where we have a limited budget on com-
putational time and we need a high performance ex-
tractor that utilizes the available time efficiently.
Our approach is to run TEXTRUNNER on all sen-
tences, and then determine the order in which to pro-
cess sentences with SRL-IE. We can increase preci-
sion by filtering out TEXTRUNNER extractions that
are expected to have low precision.
56
0.0 0.2 0.4 0.6 0.8 1.0
0.
0
0.
4
0.
8
Recall
P
re
ci
si
on
TextRunner
SRL?IE
1e?04 1e?03 1e?02 1e?01 1e+00
0.
0
0.
4
0.
8
Recall
P
re
ci
si
on
TextRunner
SRL?IE
0.0 0.2 0.4 0.6 0.8 1.0
0.
0
0.
4
0.
8
Recall
P
re
ci
si
on
TextRunner
SRL?IE
Figure 1: Ranking mechanisms for binary relations. (a) The confidence specified by the CRF improves TEXTRUN-
NER?s precision. (b) For extractions with highest redundancy, TEXTRUNNER has higher precision than SRL-IE. Note
the log scale for the x-axis. (c) Ranking by the distance between arguments gives a large boost to TEXTRUNNER?s
precision.
0.0 0.2 0.4 0.6 0.8 1.0
0.
0
0.
4
0.
8
Recall
P
re
ci
si
on
TextRunner
SRL?IE
1e?04 1e?03 1e?02 1e?01 1e+00
0.
0
0.
4
0.
8
Recall
P
re
ci
si
on
TextRunner
SRL?IE
0.0 0.2 0.4 0.6 0.8 1.0
0.
0
0.
4
0.
8
Recall
P
re
ci
si
on
TextRunner
SRL?IE
Figure 2: Ranking mechanisms for n-ary relations. (a) Ranking by confidence gives a slight boost to TEXTRUNNER?s
precision. (b) Redundancy helps SRL-IE, but not TEXTRUNNER. Note the log scale for the x-axis. (c) Ranking by
distance between arguments raises precision for TEXTRUNNER and SRL-IE.
A naive hybrid will run TEXTRUNNER over all
the sentences and use the remaining time to run
SRL-IE on a random subset of the sentences and
take the union of all extractions. We refer to this
version as RECALLHYBRID, since this does not lose
any extractions, achieving highest possible recall.
A second hybrid, which we call PRECHYBRID,
focuses on increasing the precision and uses the fil-
ter policy and an intelligent order of sentences for
extraction as described below.
Filter Policy for TEXTRUNNER Extractions: The
results from Figure 1 and Figure 2 show that TEX-
TRUNNER?s precision is low when the CRF confi-
dence in the extraction is low, when the redundancy
of the extraction is low, and when the arguments are
far apart. Thus, system confidence, redundancy, and
locality form the key factors for our filter policy: if
the confidence is less than 0.5 and the redundancy
is less than 2 or the distance between the arguments
in the sentence is greater than 5 (if the relation is
binary) or 8 (if the relation is n-ary) discard this tu-
ple. These thresholds were determined by a param-
eter search over a small dataset.
Order of Sentences for Extraction: An optimal
ordering policy would apply SRL-IE first to the sen-
tences where TEXTRUNNER has low precision and
leave the sentences that seem malformed (e.g., in-
complete sentences, two sentences spliced together)
for last. As we have seen, the distance between the
first and last argument is a good indicator for TEX-
TRUNNER precision. Moreover, a confidence value
of 0.0 by TEXTRUNNER?s CRF classifier is good ev-
idence that the sentence may be malformed and is
unlikely to contain a valid relation.
We rank sentences S in the following way, with
SRL-IE processing sentences from highest ranking
to lowest: if CRF.confidence = 0.0 then S.rank = 0,
else S.rank = average distance between pairs of ar-
guments for all tuples extracted by TEXTRUNNER
from S.
While this ranking system orders sentences ac-
cording to which sentence is likely to yield maxi-
mum new information, it misses the cost of compu-
tation. To account for computation time, we also
estimate the amount of time SRL-IE will take to
process each sentence using a linear model trained
on the sentence length. We then choose the sentence
57
that maximizes information gain divided by compu-
tation time.
6.1 Properties of Hybrid Extractors
The choice between the two hybrid systems is a
trade-off between recall and precision: RECALLHY-
BRID guarantees the best recall, since it does not lose
any extractions, while PRECHYBRID is designed to
maximize the early boost in precision. The evalua-
tion in the next section bears out these expectations.
6.2 Evaluation of Hybrid Extractors
Figure 3(a) and Figure 4(a) report the precision of
each system for binary and n-ary extractions mea-
sured against available computation time. PRECHY-
BRID starts at slightly higher precision due to our
filtering of potentially low quality extractions from
TEXTRUNNER. For binary this precision is even
better than SRL-IE?s. It gradually loses precision
until it reaches SRL-IE?s level. RECALLHYBRID
improves on TEXTRUNNER?s precision, albeit at a
much slower rate and remains worse than SRL-IE
and PRECHYBRID throughout.
The recall for binary and n-ary extractions are
shown in Figure 3(b) and Figure 4(b), again mea-
sured against available time. While PRECHYBRID
significantly improves on TEXTRUNNER?s recall, it
does lose recall compared to RECALLHYBRID, es-
pecially for n-ary extractions. PRECHYBRID also
shows a large initial drop in recall due to filtering.
Lastly, the gains in precision from PRECHYBRID
are offset by loss in recall that leaves the F1 mea-
sure essentially identical to that of RECALLHYBRID
(Figures 3(c),4(c)). However, for a fixed time bud-
get both hybrid F-measures are significantly bet-
ter than TEXTRUNNER and SRL-IE F-measures
demonstrating the power of the hybrid extractors.
Both methods reach a much higher F1 than TEX-
TRUNNER: a gain of over 0.15 in half SRL-IE?s
processing time and over 0.3 after the full process-
ing time. Both hybrids perform better than SRL-IE
given equal processing time.
We believe that most often constructing a higher
quality database of facts with a relatively lower
recall is more useful than vice-versa, making
PRECHYBRID to be of wider applicability than RE-
CALLHYBRID. Still the choice of the actual hybrid
extractor could change based on the task.
7 Related Work
Open information extraction is a relatively recent
paradigm and hence, has been studied by only a
small number of researchers. The most salient is
TEXTRUNNER, which also introduced the model
(Banko et al, 2007; Banko and Etzioni, 2008).
A version of KNEXT uses heuristic rules and syn-
tactic parses to convert a sentence into an unscoped
logical form (Van Durme and Schubert, 2008). This
work is more suitable for extracting common sense
knowledge as opposed to factual information.
Another Open IE system, Kylin (Weld et al,
2008), suggests automatically building an extractor
for each relation using self-supervised training, with
training data generated using Wikipedia infoboxes.
This work has the limitation that it can only extract
relations expressed in Wikipedia infoboxes.
A paradigm related to Open IE is Preemptive IE
(Shinyama and Sekine, 2006). While one goal of
Preemptive IE is to avoid relation-specificity, Pre-
emptive IE does not emphasize Web scalability,
which is essential to Open IE.
(Carlson et al, 2009) presents a semi-supervised
approach to information extraction on the Web. It
learns classifiers for different relations and couples
the training of those classifiers with ontology defin-
ing constraints. While we attempt to learn unknown
relations, it learns a pre-defined set of relations.
Another related system is WANDERLUST (Akbik
and Bro?, 2009). The authors of this system anno-
tated 10,000 sentences parsed with LinkGrammar,
resulting in 46 general linkpaths as patterns for rela-
tion extraction. With these patterns WANDERLUST
extracts binary relations from link grammar link-
ages. In contrast to our approaches, this requires a
large set of hand-labeled examples.
USP (Poon and Domingos, 2009) is based on
Markov Logic Networks and attempts to create a
full semantic parse in an unsupervised fashion. They
evaluate their work on biomedical text, so its appli-
cability to general Web text is not yet clear.
8 Discussion and Future Work
The Heavy Tail: It is well accepted that informa-
tion on the Web is distributed according to Zipf?s
58
0 10 20 30 40 50
0.
0
0.
2
0.
4
0.
6
Time (hours)
P
re
ci
si
on
TextRunner
SRL?IE
RecallHybrid
PrecHybrid
0 10 20 30 40 50
0.
0
0.
4
0.
8
Time (hours)
R
ec
al
l
TextRunner
SRL?IE
RecallHybrid
PrecHybrid
0 10 20 30 40 50
0.
0
0.
4
0.
8
Time (hours)
F?
m
ea
su
re
TextRunner
SRL?IE
RecallHybrid
PrecHybrid
Figure 3: (a) Precision for binary extractions for PRECHYBRID starts higher than the precision of SRL-IE. (b) Recall
for binary extractions rises over time for both hybrid systems, with PRECHYBRID starting lower. (c) Hybrid extractors
obtain the best F-measure given a limited budget of computation time.
0 10 20 30 40 50
0.
0
0.
2
0.
4
0.
6
Time (hours)
P
re
ci
si
on
TextRunner
SRL?IE
RecallHybrid
PrecHybrid
0 10 20 30 40 50
0.
0
0.
4
0.
8
Time (hours)
R
ec
al
l
TextRunner
SRL?IE
RecallHybrid
PrecHybrid
0 10 20 30 40 50
0.
0
0.
4
0.
8
Time (hours)
F?
m
ea
su
re
TextRunner
SRL?IE
RecallHybrid
PrecHybrid
Figure 4: (a) PRECHYBRID also gives a strong boost to precision for n-ary extractions. (b) Recall for n-ary extractions
for RECALLHYBRID starts substantially higher than PRECHYBRID and finally reaches much higher recall than SRL-
IE alone. (c) F-measure for n-ary extractions. The hybrid extractors outperform others.
Law (Downey et al, 2005), implying that there is a
heavy tail of facts that are mentioned only once or
twice. The prior work on Open IE ascribes prime
importance to redundancy based validation, which,
as our results show (Figures 1(b), 2(b)), captures a
very tiny fraction of the available information. We
believe that deeper processing of text is essential to
gather information from this heavy tail. Our SRL-
IE extractor is a viable algorithm for this task.
Understanding SRL Components: UIUC-SRL
as well as other SRL algorithms have different sub-
components ? parsing, argument classification, joint
inference, etc. We plan to study the effective con-
tribution of each of these components. Our hope is
to identify the most important subset, which yields
a similar quality at a much reduced computational
cost. Another alternative is to add the best perform-
ing component within TEXTRUNNER.
9 Conclusions
This paper investigates the use of semantic features,
in particular, semantic role labeling for the task of
open information extraction. We describe SRL-IE,
the first SRL based Open IE system. We empirically
compare the performance of SRL-IE with TEX-
TRUNNER, a state-of-the-art Open IE system and
find that on average SRL-IE has much higher re-
call and precision, however, TEXTRUNNER outper-
forms in precision for the case of highly redundant
or high locality extractions. Moreover, TEXTRUN-
NER is over 2 orders of magnitude faster.
These complimentary strengths help us design hy-
brid extractors that achieve better performance than
either system given a limited budget of computation
time. Overall, we provide evidence that, contrary to
belief in the Open IE literature (Banko and Etzioni,
2008), semantic approaches have a lot to offer for
the task of Open IE and the vision of machine read-
ing.
10 Acknowledgements
This research was supported in part by NSF grant
IIS-0803481, ONR grant N00014-08-1-0431, and
DARPA contract FA8750-09-C-0179, and carried
out at the University of Washington?s Turing Cen-
ter.
59
References
Eugene Agichtein and Luis Gravano. 2000. Snowball:
Extracting relations from large plain-text collections.
In Proceedings of the Fifth ACM International Con-
ference on Digital Libraries.
Alan Akbik and Ju?gen Bro?. 2009. Wanderlust: Extract-
ing semantic relations from natural language text us-
ing dependency grammar patterns. In Proceedings of
the Workshop on Semantic Search (SemSearch 2009)
at the 18th International World Wide Web Conference
(WWW 2009).
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The berkeley framenet project. In Proceedings
of the 17th international conference on Computational
linguistics, pages 86?90.
Michele Banko and Oren Etzioni. 2008. The tradeoffs
between open and traditional relation extraction. In
Proceedings of ACL-08: HLT, pages 28?36.
Michele Banko, Michael J. Cafarella, Stephen Soderland,
Matt Broadhead, and Oren Etzioni. 2007. Open infor-
mation extraction from the web. In IJCAI?07: Pro-
ceedings of the 20th international joint conference on
Artifical intelligence, pages 2670?2676.
Michele Banko. 2009. Open Information Extraction for
the Web. Ph.D. thesis, University of Washington.
Andrew Carlson, Justin Betteridge, Estevam R. Hruschka
Jr., and Tom M. Mitchell. 2009. Coupling semi-
supervised learning of categories and relations. In
Proceedings of the NAACL HLT 2009 Workskop on
Semi-supervised Learning for Natural Language Pro-
cessing.
Bonaventura Coppola, Alessandro Moschitti, and
Giuseppe Riccardi. 2009. Shallow semantic parsing
for spoken language understanding. In NAACL ?09:
Proceedings of Human Language Technologies: The
2009 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
Companion Volume: Short Papers, pages 85?88.
Doug Downey, Oren Etzioni, and Stephen Soderland.
2005. A probabilistic model of redundancy in infor-
mation extraction. In IJCAI ?05: Proceedings of the
20th international joint conference on Artifical intelli-
gence, pages 1034?1041.
Oren Etzioni, Michele Banko, and Michael J. Cafarella.
2006. Machine reading. In AAAI?06: proceedings of
the 21st national conference on Artificial intelligence,
pages 1517?1519.
Richard Johansson and Pierre Nugues. 2008. The ef-
fect of syntactic representation on semantic role label-
ing. In Proceedings of the 22nd International Con-
ference on Computational Linguistics (Coling 2008),
pages 393?400.
Paul Kingsbury Martha and Martha Palmer. 2002. From
treebank to propbank. In In Proceedings of LREC-
2002.
Alessandro Moschitti, Daniele Pighin, and Roberto
Basili. 2008. Tree kernels for semantic role labeling.
Computational Linguistics, 34(2):193?224.
Hoifung Poon and Pedro Domingos. 2009. Unsuper-
vised semantic parsing. In EMNLP ?09: Proceedings
of the 2009 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1?10.
V. Punyakanok, D. Roth, and W. Yih. 2008. The impor-
tance of syntactic parsing and inference in semantic
role labeling. Computational Linguistics, 34(2).
Yusuke Shinyama and Satoshi Sekine. 2006. Preemp-
tive information extraction using unrestricted relation
discovery. In Proceedings of the main conference
on Human Language Technology Conference of the
North American Chapter of the Association of Com-
putational Linguistics, pages 304?311.
Stephen Soderland. 1999. Learning information extrac-
tion rules for semi-structured and free text. Machine
Learning, 34(1-3):233?272.
Kristina Toutanova, Aria Haghighi, and Christopher D.
Manning. 2008. A global joint model for semantic
role labeling. Computational Linguistics, 34(2):161?
191.
Benjamin Van Durme and Lenhart Schubert. 2008. Open
knowledge extraction through compositional language
processing. In STEP ?08: Proceedings of the 2008
Conference on Semantics in Text Processing, pages
239?254.
Daniel S. Weld, Raphael Hoffmann, and Fei Wu. 2008.
Using wikipedia to bootstrap open information extrac-
tion. SIGMOD Rec., 37(4):62?68.
60
Proceedings of the NAACL HLT 2010 First International Workshop on Formalisms and Methodology for Learning by Reading, pages 87?95,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Machine Reading at the University of Washington
Hoifung Poon, Janara Christensen, Pedro Domingos, Oren Etzioni, Raphael Hoffmann,
Chloe Kiddon, Thomas Lin, Xiao Ling, Mausam, Alan Ritter, Stefan Schoenmackers,
Stephen Soderland, Dan Weld, Fei Wu, Congle Zhang
Department of Computer Science & Engineering
University of Washington
Seattle, WA 98195
{hoifung,janara,pedrod,etzioni,raphaelh,chloe,tlin,xiaoling,mausam,
aritter,stef,soderland,weld,wufei,clzhang}@cs.washington.edu
Abstract
Machine reading is a long-standing goal of AI
and NLP. In recent years, tremendous progress
has been made in developing machine learning
approaches for many of its subtasks such as
parsing, information extraction, and question
answering. However, existing end-to-end so-
lutions typically require substantial amount of
human efforts (e.g., labeled data and/or man-
ual engineering), and are not well poised for
Web-scale knowledge acquisition. In this pa-
per, we propose a unifying approach for ma-
chine reading by bootstrapping from the easi-
est extractable knowledge and conquering the
long tail via a self-supervised learning pro-
cess. This self-supervision is powered by joint
inference based on Markov logic, and is made
scalable by leveraging hierarchical structures
and coarse-to-fine inference. Researchers at
the University of Washington have taken the
first steps in this direction. Our existing work
explores the wide spectrum of this vision and
shows its promise.
1 Introduction
Machine reading, or learning by reading, aims to
extract knowledge automatically from unstructured
text and apply the extracted knowledge to end tasks
such as decision making and question answering. It
has been a major goal of AI and NLP since their
early days. With the advent of the Web, the billions
of online text documents contain virtually unlimited
amount of knowledge to extract, further increasing
the importance and urgency of machine reading.
In the past, there has been a lot of progress in
automating many subtasks of machine reading by
machine learning approaches (e.g., components in
the traditional NLP pipeline such as POS tagging
and syntactic parsing). However, end-to-end solu-
tions are still rare, and existing systems typically re-
quire substantial amount of human effort in manual
engineering and/or labeling examples. As a result,
they often target restricted domains and only extract
limited types of knowledge (e.g., a pre-specified re-
lation). Moreover, many machine reading systems
train their knowledge extractors once and do not
leverage further learning opportunities such as ad-
ditional text and interaction with end users.
Ideally, a machine reading system should strive to
satisfy the following desiderata:
End-to-end: the system should input raw text, ex-
tract knowledge, and be able to answer ques-
tions and support other end tasks;
High quality: the system should extract knowledge
with high accuracy;
Large-scale: the system should acquire knowledge
at Web-scale and be open to arbitrary domains,
genres, and languages;
Maximally autonomous: the system should incur
minimal human effort;
Continuous learning from experience: the
system should constantly integrate new infor-
mation sources (e.g., new text documents) and
learn from user questions and feedback (e.g.,
via performing end tasks) to continuously
improve its performance.
These desiderata raise many intriguing and chal-
lenging research questions. Machine reading re-
search at the University of Washington has explored
87
a wide spectrum of solutions to these challenges and
has produced a large number of initial systems that
demonstrated promising performance. During this
expedition, an underlying unifying vision starts to
emerge. It becomes apparent that the key to solving
machine reading is to:
1. Conquer the long tail of textual knowledge via
a self-supervised learning process that lever-
ages data redundancy to bootstrap from the
head and propagates information down the long
tail by joint inference;
2. Scale this process to billions of Web documents
by identifying and leveraging ubiquitous struc-
tures that lead to sparsity.
In Section 2, we present this vision in detail, iden-
tify the major dimensions these initial systems have
explored, and propose a unifying approach that sat-
isfies all five desiderata. In Section 3, we reivew
machine reading research at the University of Wash-
ington and show how they form synergistic effort
towards solving the machine reading problem. We
conclude in Section 4.
2 A Unifying Approach for Machine
Reading
The core challenges to machine reading stem from
the massive scale of the Web and the long-tailed dis-
tribution of textual knowledge. The heterogeneous
Web contains texts that vary substantially in subject
matters (e.g., finance vs. biology) and writing styles
(e.g., blog posts vs. scientific papers). In addition,
natural languages are famous for their myraid vari-
ations in expressing the same meaning. A fact may
be stated in a straightforward way such as ?kale con-
tains calcium?. More often though, it may be stated
in a syntactically and/or lexically different way than
as phrased in an end task (e.g., ?calcium is found in
kale?). Finally, many facts are not even stated ex-
plicitly, and must be inferred from other facts (e.g.,
?kale prevents osteoporosis? may not be stated ex-
plicitly but can be inferred by combining facts such
as ?kale contains calcium? and ?calcium helps pre-
vent osteoporosis?). As a result, machine reading
must not rely on explicit supervision such as manual
rules and labeled examples, which will incur pro-
hibitive cost in the Web scale. Instead, it must be
able to learn from indirect supervision.





	






Figure 1: A unifying vision for machine reading: boot-
strap from the head regime of the power-law distribu-
tion of textual knowledge, and conquer the long tail in
a self-supervised learning process that raises certainty on
sparse extractions by propagating information via joint
inference from frequent extractions.
A key source of indirect supervision is meta
knowledge about the domains. For example, the
TextRunner system (Banko et al, 2007) hinges on
the observation that there exist general, relation-
independent patterns for information extraction. An-
other key source of indirect supervision is data re-
dundancy. While a rare extracted fact or inference
pattern may arise by chance of error, it is much less
likely so for the ones with many repetitions (Downey
et al, 2010). Such highly-redundant knowledge can
be extracted easily and with high confidence, and
can be leveraged for bootstrapping. For knowledge
that resides in the long tail, explicit forms of redun-
dancy (e.g., identical expressions) are rare, but this
can be circumvented by joint inference. For exam-
ple, expressions that are composed with or by sim-
ilar expressions probably have the same meaning;
the fact that kale prevents osteoporosis can be de-
rived by combining the facts that kale contains cal-
cium and that calcium helps prevent osteoporosis via
a transitivity-through inference pattern. In general,
joint inference can take various forms, ranging from
simple voting to shrinkage in a probabilistic ontol-
ogy to sophisticated probabilistic reasoning based
on a joint model. Simple ones tend to scale bet-
ter, but their capability in propagating information
is limited. More sophisticated methods can uncover
implicit redundancy and propagate much more in-
88
formation with higher quality, yet the challenge is
how to make them scale as well as simple ones.
To do machine reading, a self-supervised learning
process, informed by meta knowledege, stipulates
what form of joint inference to use and how. Effec-
tively, it increases certainty on sparse extractions by
propagating information from more frequent ones.
Figure 1 illustrates this unifying vision.
In the past, machine reading research at the Uni-
versity of Washington has explored a variety of so-
lutions that span the key dimensions of this uni-
fying vision: knowledge representation, bootstrap-
ping, self-supervised learning, large-scale joint in-
ference, ontology induction, continuous learning.
See Section 3 for more details. Based on this ex-
perience, one direction seems particularly promising
that we would propose here as our unifying approach
for end-to-end machine reading:
Markov logic is used as the unifying framework for
knowledge representation and joint inference;
Self-supervised learning is governed by a joint
probabilistic model that incorporates a small
amount of heuristic knowledge and large-scale
relational structures to maximize the amount
and quality of information to propagate;
Joint inference is made scalable to the Web by
coarse-to-fine inference.
Probabilistic ontologies are induced from text to
guarantee tractability in coarse-to-fine infer-
ence. This ontology induction and popula-
tion are incorporated into the joint probabilistic
model for self-supervision;
Continuous learning is accomplished by combin-
ing bootstrapping and crowdsourced content
creation to synergistically improve the reading
system from user interaction and feedback.
A distinctive feature of this approach is its empha-
sis on using sophisticated joint inference. Recently,
joint inference has received increasing interest in
AI, machine learning, and NLP, with Markov logic
(Domingos and Lowd, 2009) being one of the lead-
ing unifying frameworks. Past work has shown that
it can substantially improve predictive accuracy in
supervised learning (e.g., (Getoor and Taskar, 2007;
Bakir et al, 2007)). We propose to build on these ad-
vances, but apply joint inference beyond supervised
learning, with labeled examples supplanted by indi-
rect supervision.
Another distinctive feature is that we propose
to use coarse-to-fine inference (Felzenszwalb and
McAllester, 2007; Petrov, 2009) as a unifying
framework to scale inference to the Web. Es-
sentially, coarse-to-fine inference leverages the
sparsity imposed by hierarchical structures that
are ubiquitous in human knowledge (e.g., tax-
onomies/ontologies). At coarse levels (top levels in
a hierarchy), ambiguities are rare (there are few ob-
jects and relations), and inference can be conducted
very efficiently. The result is then used to prune un-
promising refinements at the next level. This process
continues down the hierarchy until decision can be
made. In this way, inference can potentially be sped
up exponentially, analogous to binary tree search.
Finally, we propose a novel form of continuous
learning by leveraging the interaction between the
system and end users to constantly improve the per-
formance. This is straightforward to do in our ap-
proach given the self-supervision process and the
availability of powerful joint inference. Essentially,
when the system output is applied to an end task
(e.g., answering questions), the feedback from user
is collected and incorporated back into the system
as a bootstrap source. The feedback can take the
form of explicit supervision (e.g., via community
content creation or active learning) or indirect sig-
nals (e.g., click data and query logs). In this way,
we can bootstrap an online community by an initial
machine reading system that provides imperfect but
valuable service in end tasks, and continuously im-
prove the quality of system output, which attracts
more users with higher degree of participation, thus
creating a positive feedback loop and raising the ma-
chine reading performance to a high level that is dif-
ficult to attain otherwise.
3 Summary of Progress to Date
The University of Washington has been one of the
leading places for machine reading research and has
produced many cutting-edge systems, e.g., WIEN
(first wrapper induction system for information ex-
traction), Mulder (first fully automatic Web-scale
question answering system), KnowItAll/TextRunner
(first systems to do open-domain information extrac-
89
tion from the Web corpus at large scale), Kylin (first
self-supervised system for Wikipedia-based infor-
mation extraction), UCR (first unsupervised corefer-
ence resolution system that rivals the performance of
supervised systems), Holmes (first Web-scale joint
inference system), USP (first unsupervised system
for semantic parsing).
Figure 2 shows the evolution of the major sys-
tems; dashed lines signify influence in key ideas
(e.g., Mulder inspires KnowItAll), and solid lines
signify dataflow (e.g., Holmes inputs TextRunner tu-
ples). These systems span a wide spectrum in scal-
ability (assessed by speed and quantity in extrac-
tion) and comprehension (assessed by unit yield of
knowledge at a fixed precision level). At one ex-
treme, the TextRunner system is highly scalable, ca-
pable of extracting billions of facts, but it focuses on
shallow extractions from simple sentences. At the
other extreme, the USP and LOFT systems achieve
much higher level of comprehension (e.g., in a task
of extracting knowledge from biomedical papers and
answering questions, USP obtains more than three
times as many correct answers as TextRunner, and
LOFT obtains more than six times as many correct
answers as TextRunner), but are much less scalable
than TextRunner.
In the remainder of the section, we review the
progress made to date and identify key directions for
future work.
3.1 Knowledge Representation and Joint
Inference
Knowledge representations used in these systems
vary widely in expressiveness, ranging from sim-
ple ones like relation triples (<subject, relation,
object>; e.g., in KnowItAll and TextRunner), to
clusters of relation triples or triple components (e.g.,
in SNE, RESOLVER), to arbitrary logical formulas
and their clusters (e.g., in USP, LOFT). Similarly,
a variety forms of joint inference have been used,
ranging from simple voting to heuristic rules to so-
phisticated probabilistic models. All these can be
compactly encoded in Markov logic (Domingos and
Lowd, 2009), which provides a unifying framework
for knowledge representation and joint inference.
Past work at Washington has shown that in su-
pervised learning, joint inference can substantially
improve predictive performance on tasks related to
machine reading (e.g., citation information extrac-
tion (Poon and Domingos, 2007), ontology induc-
tion (Wu and Weld, 2008), temporal information
extraction (Ling and Weld, 2010)). In addition, it
has demonstrated that sophisticated joint inference
can enable effective learning without any labeled
information (UCR, USP, LOFT), and that joint in-
ference can scale to millions of Web documents by
leveraging sparsity in naturally occurring relations
(Holmes, Sherlock), showing the promise of our uni-
fying approach.
Simpler representations limit the expressiveness
in representing knowledge and the degree of sophis-
tication in joint inference, but they currently scale
much better than more expressive ones. A key direc-
tion for future work is to evaluate this tradeoff more
thoroughly, e.g., for each class of end tasks, to what
degree do simple representations limit the effective-
ness in performing the end tasks? Can we automate
the choice of representations to strike the best trade-
off for a specific end task? Can we advance joint
inference algorithms to such a degree that sophisti-
cated inference scales as well as simple ones?
3.2 Bootstrapping
Past work at Washington has identified and lever-
aged a wide range of sources for bootstrapping. Ex-
amples include Wikipedia (Kylin, KOG, IIA, WOE,
WPE), Web lists (KnowItAll, WPE), Web tables
(WebTables), Hearst patterns (KnowItAll), heuristic
rules (TextRunner), semantic role labels (SRL-IE),
etc.
In general, potential bootstrap sources can be
broadly divided into domain knowledge (e.g., pat-
terns and rules) and crowdsourced contents (e.g., lin-
guistic resources, Wikipedia, Amazon Mechanical
Turk, the ESP game).
A key direction for future work is to combine
bootstrapping with crowdsourced content creation
for continuous learning. (Also see Subsection 3.6.)
3.3 Self-Supervised Learning
Although the ways past systems conduct self-
supervision vary widely in detail, they can be di-
vided into two broad categories. One uses heuristic
rules that exploit existing semi-structured resources
to generate noisy training examples for use by su-
pervised learning methods and with cotraining (e.g.,
90

	


		

	
 	