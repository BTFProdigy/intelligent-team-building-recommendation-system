Harvesting the Bitexts of the Laws of Hong Kong From the Web
Chunyu Kit Xiaoyue Liu KingKui Sin Jonathan J. Webster
Department of Chinese, Translation and Linguistics
City University of Hong Kong, Tat Chee Ave., Kowloon, Hong Kong
{ctckit, xyliu0, ctsinkk, ctjjw}@cityu.edu.hk
Abstract
In this paper we present our recent work
on harvesting English-Chinese bitexts
of the laws of Hong Kong from the
Web and aligning them to the subpara-
graph level via utilizing the number-
ing system in the legal text hierarchy.
Basic methodology and practical tech-
niques are reported in detail. The re-
sultant bilingual corpus, 10.4M English
words and 18.3M Chinese characters,
is an authoritative and comprehensive
text collection covering the specific and
special domain of HK laws. It is par-
ticularly valuable to empirical MT re-
search. This piece of work has also laid
a foundation for exploring and harvest-
ing English-Chinese bitexts in a larger
volume from the Web.
1 Introduction
Bitexts, also referred to as parallel texts or bilin-
gual corpora, collections of bilingual text pairs
aligned at various levels of granularity, have been
playing a critical role in the current development
of machine translation technology. It is such
large data sets that give rise to the plausibility
of empirical approaches to machine translation,
most of which involve the application of a variety
of machine learning techniques to infer various
types of translation knowledge from bitext data
to facilitate automatic translation and enhance
translation quality. Large volumes of training
data of this kind are indispensable for construct-
ing statistical translation models (Brown et al,
1993; Melamed, 2000), acquiring bilingual lex-
icon (Gale and Church, 1991; Melamed, 1997),
and building example-based machine translation
(EBMT) systems (Nagao, 1984; Carl and Way,
2003; Way and Gough, 2003). They also provide
a basis for inferring lexical connection between
vocabularies in cross-languages information re-
trieval (Davis and Dunning, 1995).
Existing parallel corpora have illustrated their
particular value in empirical NLP research, e.g.,
Canadian Hansard Corpus (Gale and Church,
1991b), HK Hansard (Wu, 1994), INTERSECT
(Salkie, 1995), ENPC (Ebeling, 1998), the Bible
parallel corpus (Resnik et al, 1999) and many
others. The Web is being explored not only as a
super corpus for NLP and linguistic research (Kil-
garriff and Grefenstette, 2003) but also, more im-
portantly to MT research, as a treasure for mining
bitexts of various language pairs (Resnik, 1999;
Chen and Nie, 2000; Nie and Cai, 2001; Nie
and Chen, 2002; Resnik and Smith, 2003; Way
and Gough, 2003). The Web has been the play-
ground for many NLPers. More and more Web
sites are found to have cloned their Web pages in
several languages, aiming at conveying informa-
tion to audience in different languages. This gives
rise to a huge volume of wonderful bilingual or
multi-lingual resources freely available from the
Web for research. What we need to do is to har-
vest the right resources for the right applications.
In this paper we present our recent work on
harvesting English-Chinese parallel texts of the
laws of Hong Kong from the Web and construct-
71
ing a subparagraph-aligned bilingual corpus of
about 20 million words. The bilingual texts of the
laws is introduced in Section 2, with an emphasis
on HK?s legislation text hierarchy and its num-
bering system that can be utilized for text align-
ment to subparagraph level. Section 3 presents
basic methodology and technical details for har-
vesting and aligning bilingual Web page pairs, ex-
tracting content texts from the pages, and align-
ing text structures in terms of the text hierarchy
via utilizing consistent intrinsic features in the
Web pages and content texts. Section 4 presents
XML schema for encoding the alignment results
and illustrates the display mode for browsing the
aligned bilingual corpus. Section 5 concludes
the paper, highlighting the value of the corpus in
term of its volume, translation quality, specificity
and comprehensiveness, and alignment granular-
ity. Our future work to explore the Web for har-
vesting more quantities of parallel bitexts is also
briefly outlined.
2 Bilingual Texts of the Laws of HK
The laws of Hong Kong (HK) before 1987 were
exclusively enacted in English. They were trans-
lated into Chinese in the run-up to the handover
in 1997. Since then all HK laws have been en-
acted in both English and Chinese, both versions
being equally authentic. This gives rise to a valu-
able set of bitexts in large quantity and high qual-
ity that can be utilized to facilitate empirical MT
research.
2.1 BLIS Corpus
The bilingual texts of the laws of Hong Kong
have been made available to the public in re-
cent years by the Justice Department of the HK-
SAR through the bilingual laws information sys-
tem (BLIS). All these texts are freely accessible
from http://www.justice.gov.hk/.
BLIS provides the most comprehensive docu-
mentation of HK legislation. It contains all statute
laws of Hong Kong currently in operation, includ-
ing all ordinances and subsidiary legislation of
HK (and some of their past versions dating back
to 60 June 1997), the Basic Law and the Sino-
British Joint Declaration, the constitution of PRC
and national laws that apply in HK, and other rel-
evant instruments. The entire bilingual corpus of
Figure 1: Illustration of BLIS hierarchy
BLIS legal texts contains approximately 10 mil-
lion English words and 18 million Chinese char-
acters. Lexical resources of this kind are particu-
larly useful in bilingual legal terminology studies
and text alignment work.
2.2 Text Hierarchy
BLIS organizes the legal texts in terms of the
hierarchy of the Loose-Leaf Edition of the Laws
of Hong Kong. At the top level, the ordinances
are arranged by chapters, each of which is identi-
fied by an assigned number and a short title, e.g.,
Chapter 5 OFFICIAL LANGUAGES ORDINANCE /
?5? ??????. The assigned number for a
subsidiary legislation chapter consists of a chap-
ter number and a following uppercase letter, e.g.,
CAP 5C HIGH COURT CIVIL PROCEDURE (USE
OF LANGUAGE) RULES / ?5C? ???????
?(????)??.
The content of an ordinance, exclusive of its
long title, is divided and identified according to a
very rigid numbering system which encodes the
hierarchy of the texts of the laws. Both the Chi-
nese and English versions of an ordinance fol-
low exactly the same hierarchical structures such
as chapters (?), parts (?), sections (?), sub-
sections (?), paragraphs (?) and subparagraphs
(?). This allows us to align the bitexts along
72
Figure 2: BLIS texts in pair
this hierarchical structure, once they are down-
loaded from the BLIS official site. To our knowl-
edge, a well-aligned bilingual corpus of this size
covering a special domain so comprehensively is
seldom readily available for the Chinese-English
language pair.
Excerpts from the BLIS corpus are illustrated
in Figure 1 and 2, one illustrating its hierarchy and
the other a pair of BLIS bitexts. From the excerpts
we can see that not everything has an exact match
between a pair of BLIS Web pages. For example,
the Chinese side has a gazette number ?25 of 1998
s. 2? and a piece of ?remarks? at the beginning of
content text, whereas its English counterpart has
none of them.
3 Harvesting Bitexts from the Web
Basically two phases are involved in construct-
ing the bilingual corpus of the laws of HK. The
first phase is to harvest the monolingual texts of
HK laws from the BLIS site and align them into
pairs. It involves the following steps: (1) down-
loading Web pages one by one with the aid of a
Web crawler, (2) extracting the texts from them
by filtering out the HTML markup, and (3) align-
ing the extracted monolingual texts into bilingual
Figure 3: BLIS web pages connected as two dou-
ble linked lists
pairs. The second phase is to align finer-grained
text structures within each text pair.
3.1 Downloading BLIS Web Pages
A BLIS Web page does not necessarily corre-
spond to any particular text structure such as a
chapter, a part, a section, a subsection, or a para-
graph in the BLIS hierarchy. A chapter, espe-
cially a short one, may be organized into a few
sections in a Web page or in several contiguous
pages. Some sections, e.g., the long ones, are di-
vided into several pages. In general, BLIS does
not maintain any reliable match between its Web
pages and any particular text hierarchical struc-
tures.
Fortunately, in most cases a BLIS page always
has a counterpart in the other language. There is
a ?switch language? button on each page to link
to the counterpart page. Such linkage allows us
to download the Web pages in pairs and, conse-
quently, harvest a list of page-to-page aligned bi-
texts.
In addition to the pair link, each BLIS page also
carries links for the ?next? and the ?previous sec-
tion of enactment?. These two kinds of linkage
turn the pages into two double linked lists, each
in a language, as illustrated in Figure 3, with each
page as a node. Nodes in pairs are also double
linked between the two lists.
However, the pairwise linkage is not reliable
in the BLIS site, because there are missing Web
pages in one of the two languages in question
(see Table 3 below for more details). In order to
download all bitexts of legislation from the site,
we need to go through one linked list and down-
load each page and its counterpart, if there is one,
in the other language. Such scanning gives a list
of text pairs, where some pages may have a null
73
Total time Downloaded files
English 17 hours 50,638 (429MB)
Chinese 18 hours 50,510 (460MB)
Table 1: File downloading
File name
BLIS HTML page title Chinese English
Cap 5A ... 5A c.txt 5A e.txt
Cap 5A s 1 ... 5A-1 c.txt 5A-1 e.txt
Cap 5A s 2 ... 5A-2 c.txt 5A-2 e.txt
Cap 5A s 3 ... 5A-3 c.txt 5A-3 e.txt
Table 2: Naming downloaded files in terms of
BLIS numbering
counterpart. An alternative strategy is to down-
load each list separately, and then match the pages
into pairs sequentially with the aid of numbering
information in the header of each page ? see 3.2
below. These two strategies verify one another,
making sure that all pages are downloaded and
put in the right pairs.
The downloading is carried out by a Web
crawler implemented in Java. In order to accom-
plish the above strategies, it also has to handle a
number of technical issues.
? It sleeps for a while (e.g., 10 seconds) when
it finishes downloading a certain number of
pages (e.g., 50 pages), because the BLIS site
refuses continuous access from one site for a
too long time.
? When an error occurs, it remembers the cur-
rent URL. Then it re-starts from where it
stops.
The data about the file downloading from BLIS
site is given in Table 1. One can conceive that
if the time intervals for sleep and downloading
could be automatically tuned by the crawler to
maximize the downloading efficiency, it would
get the job done significantly more quickly. Our
option for 10 seconds sleep between every 50 files
is based on error records of a number of test runs.
3.2 Aligning Web Pages
Every BLIS Web page is identified by a subti-
tle that carries numbering information about the
page, as illustrated in Figure 1. Such a subtitle
is exactly retained in the page as its HTML title.
Files English Chinese
Aligned 50,506 (62.3MB)a 50,506 (38.5MB)
Missing 132 4
Total 50,638 50,510
Sizeb 10.4M words 18.3M char.s
aThe size of extracted texts.
bExclusive of punctuation marks.
Table 3: The number of aligned and missing files
This feature is utilized to align BLIS pages: all
downloaded files are named in terms of the num-
bering information extracted from their HTML ti-
tles, as illustrated in Table 2. Consequently, all
files are naturally aligned in pairs by their names.
Any file names not in a pair indicate the missing
counterparts in the other language. The statistics
of file alignment are given in Table 3.
3.3 Text Extraction
Basically, this task involves two aspects, namely,
filtering HTML markup and extracting content
text. A straightforward strategy is that we first
clean up HTML tags in each page and then the
non-legal content. The tags are in brackets, and
non-legal content in a consistent pattern through-
out all BLIS pages. However, a more convenient
way to do it is to make use of a reliable feature
in the BLIS pages: legal content is placed in be-
tween two ? the only two ? horizontal bars in each
page. Accordingly, we implement a strategy to
first extract every thing in between the two bars
and then clean up remaining HTML tags. The
output from this procedure includes
? a header as a fixed set of items, including
chapter number, title, heading, etc., and
? a piece of content text as a list of numbered
items each in a line. (See the header and con-
tent text in Figure 2.)
The text in a BLIS page is displayed as a sequence
of hierarchically numbered items, such as subsec-
tions, paragraphs and subparagraphs.
3.4 Text Alignment within Text Pairs
After page (or file) alignment, each page finds its
counterpart in the other language. After text ex-
traction, a page gives a content text consisting of
a list of numbered items, each in a line. A such
74
Remarks:
Adaptation amendments retroactively made - see
26 of 1999 s.3//a
(1) All Ordinances shall be enacted and published
in both official languages.//
(2) Nothing in subsection (1) shall require an
Ordinance to be enacted and published in
both official languages where that Ordinance
amends another Ordinance and-//
(a) that other Ordinance was enacted in the
English language only; and//
(b) no authentic text of that Ordinance has been
published in the Chinese language under
section 4B(1).//
(3) Nothing in subsection (1) shall require an
Ordinance to be enacted and published in both
official languages where the Chief Executive
in Council- (Amended 26 of 1999 s.3)//
aIndicating a text line break.
Table 4: Anchors in a sample text
item can be divided into a numbering item and the
remaining content text in the line, as illustrated in
Table 4. The Chinese counterpart of this text car-
ries similar lines, if no missing line in any page of
the pair.
Unfortunately, missing lines are found in some
BLIS pages, as exemplified in Figure 2. There is
no guarantee that matching text lines one by one
in sequence would carry out the expected align-
ment within a page pair. However, the numbering
items at the beginning of each line can be utilized
as anchors to facilitate the alignment. The strat-
egy along this line is given as follows.
1. Anchor identification: numbering items at
the beginning of each line are recognized
as anchors, with the beginning and the end
of the whole content text as two special an-
chors, resulting in a list of anchors for each
page;
2. Anchor alignment: match the two lists of an-
chors sequentially. If a pair of anchors does
not match, give up the smaller one (in terms
of the BLIS numbering hierarchy) and move
on to the next possible pair, working in ex-
actly the same procedure as matching iden-
tical anchor pairs between two sorted lists of
anchors.
3. Text line alignment: a pair of matched an-
chors give a pair of matched lines; an un-
matched anchor indicates a missing line in
the other language.
4 XML Markup for the Aligned Corpus
XML is applied to encode the text alignment
outcomes output from the above alignment pro-
cedure. It has been a standard for data repre-
sentation and exchange on the Web, and also
accepted by the NLP community as a standard
for linguistic data annotation and representation
(Ide et al, 2000; Mengel and Lezius, 2000;
Kim et al, 2001). There are a series of yearly
NLPXML workshops for it since 2001. It pro-
vides a platform-independent flexible and sophis-
ticated plain text format for data encoding and
manipulation. It is particularly suitable for hier-
archical linguistic data such as the hierarchically-
aligned bilingual corpus that we have produced.
What?s more, converting data to XML format not
only significantly reduces the complexity of data
exchange among different computer systems but
also enhances data transmission reliability and
eases Web browsing.
There have been many corpora that are anno-
tated with XML, e.g., HCRC Map Task Corpus
(Anderson et al, 1991), American National Cor-
pus (Ide and Macleod, 2001), the La Republica
corpus (Baroni et al, 2004). Below we present
the XML schema for our subparagraph-aligned
BLIS bitexts, with sample annotation, and nec-
essary Web browsing.
4.1 XML Schema
The current version of the XML schema for the
bilingual BLIS corpus, as given in Figure 4, fo-
cuses on encoding all text structures in the BLIS
hierarchy, including all elements in each BLIS
Web page. It is to be extended to cover finer-
grained structures such as clauses, phrases and
words, as we proceed to align the BLIS bitexts
at these linguistic levels. For simplicity, we al-
low para to subsume all types of text line, be
they a section, subsection, paragraph or subpara-
graph. The annotation of a sample bitext with this
schema is illustrated in Figure 5. Annotation of
this kind is carried out by a Java program auto-
matically for the entire bitext corpus.
4.2 Corpus Browsing
A number of display modes are designed for
browsing the subparagraph-aligned bitexts, in-
cluding bilingual modes and monolingual modes.
75
Figure 4: XML schema for aligned BLIS bitexts
In a bilingual mode, text line pairs are displayed
in sequence. Switch of language order or from
one mode to another is allowed any time during
browsing. The bilingual display mode is illus-
trated in Figure 6.
5 Conclusion
We have presented in the above sections our re-
cent work on harvesting and aligning the bitexts
of the laws of Hong Kong, including basic tech-
niques for downloading English-Chinese bilin-
gual legal texts from BLIS official site, sound
strategies for aligning the bitexts by utilizing the
numbering system in the legal texts, and neces-
sary XML annotation for the alignment results.
The value of the outcomes, i.e., the subparagraph-
aligned bilingual corpus, can be evaluated in
terms of the following aspects.
Corpus size The entire corpus is of 10.4M En-
glish words and 18.3M Chinese characters,
several times larger than the well-known
Penn Treebank Corpus in size.
Figure 5: Sample bitext in XML encoding
Translation quality All texts of the corpus are
prepared by the Law Drafting Division of
the Department of Justice, Hong Kong Gov-
ernment. Legal texts are known to be more
precise and less ambiguous than most other
types of text.
Specificity and comprehensiveness The corpus
covers specifically the domain of Hong Kong
legislation. It is the most authoritative and
complete text collection of the laws of Hong
Kong.
Alignment granularity The entire corpus is
aligned precisely to the subparagraph level.
Most subparagraphs in the legal texts are
phrases, fragments of a clause, or clauses; as
shown in Table 4.
76
Figure 6: Illustration of browsing modes
A bilingual corpus of this size and quality cov-
ering a specific domain so comprehensively is
particularly useful not only in empirical MT re-
search but also in computational studies of bilin-
gual terminology and legislation. Our future work
will focus on word alignment for inferring bilin-
gual lexical resources and on automatic recogni-
tion of legal terminology.
Also, our experience in constructing this bilin-
gual corpus has laid a foundation for us to con-
tinue to harvest more bilingual text materials from
the Web, e.g., from Hong Kong government?s
Web sites. We find that almost all Hong Kong
government web sites, which are in large num-
bers, maintain their Web pages consistently par-
allel in English and Chinese. We are not sure if
such bitexts in such pages are larger than that in
the BLIS site in volume. We do know they cover
a large number of distinct domains. This is partic-
ularly useful for MT. If we can harvest and align
the bitexts from such Web pages efficiently via
utilizing their intrinsic characteristics of URL cor-
respondence and text structure, it would not be a
dream any more to put an end to the time of hav-
ing too few existing translation materials for em-
pirical MT studies, at least, for the language pair
of Chinese and English.
Acknowledgements
The work described in this paper was supported
by the Research Grants Council of HKSAR,
China, through the CERG grants 9040861 and
9040482. We wish to thank our team members
for their help.
References
Anne H. Anderson, Miles Bader, Ellen G. Bard, Eliz-
abeth Boyle, Gwyneth Doherty, Simon Garrod,
Stephen Isard, Jacqueline Kowtko, Jan McAllis-
ter, Jim Miller, Catherine Sotillo, Henry Thompson,
and Regina Weinert. 1991. The HCRC map task
corpus. Language and Speech, 34(4):351?366.
Marco Baroni, Silvia Bernardini, Federica Comastri,
Lorenzo Piccioni, Alessandra Volpi, Guy Aston,
and Marco Mazzoleni. 2004. Introducing the La
Repubblica corpus: A large, annotated, TEI(XML)-
compliant corpus of newspaper Italian. In LREC
2004, pp. 1771-1774.
Simon P. Botley, Anthony M. McEnery, and Andrew
Wilson (eds.). 2000. Multilingual Corpora in
Teaching and Research. Amsterdam: Rodopi.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation: Pa-
rameter estimation. Computational Linguistics,
19(2):263?311.
Michael Carl and Andy Way (eds.). 2003. Recent
Advances in Example-based Machine Translation.
Dordrecht: Kluwer.
Jiang Chen and Jian Y. Nie. 2000. Parallel Web text
mining for cross-language information retrieval. In
RIAO?2000, pp. 62?77. Paris.
Mark Davis and Ted Dunning. 1995. A TREC evalu-
ation of query translation methods for multi-lingual
text retrieval. In TREC-4, pp. 483?498. NIST.
Jarle Ebeling. 1998. Contrastive linguistics, transla-
tion, and parallel corpora. In Meta, 43(4):602?615.
William A. Gale and Kenneth W. Church. 1991. Iden-
tifying word correspondences in parallel texts. In
Fourth DARPA Workshop on Speech and Natural
Language, pp. 152?157. Asilomar, California.
William A. Gale and Kenneth W. Church. 1991b. A
Program for Aligning Sentences in Bilingual Cor-
pora. In ACL?91, pp. 177?184. Berkeley.
Nancy Ide, Patrice Bonhomme, and Laurent Romary.
2000. XCES: an XML-based encoding standard
for linguistic corpora. In LREC2000, pp. 825?830.
Athens, Greece.
77
Nancy Ide and Catherine Macleod. 2001. The Amer-
ican National Corpus: A Standardized Resource of
American English. Proceedings of Corpus Linguis-
tics 2001, Lancaster UK.
Adam Kilgarriff and Gregory Grefenstette. 2003. In-
troduction to the Special Issue on the Web as Cor-
pus. Computational Linguistics, 29(3):333?347.
Jin-Dong Kim, Tomoko Ohta, Yuka Tateisi, Hideki
Mima and Jun?ichi Tsujii. 2001. XML-based lin-
guistic annotation of corpus. In NLPXML-1, pp. 47?
54. Tokyo.
I. Dan Melamed. 1997. Automatic discovery of
non-compositional compounds in parallel data. In
EMNLP?97, pp. 97?108. Brown University, Au-
gust.
I. Dan Melamed. 2000. Models of translational equiv-
alence among words. Computational Linguistics,
26(2):221?249.
Andreas Mengel and Wolfgang Lezius. 2000. An
XML-based representation format for syntactically
annotated corpora. In LREC2000, Volume 1,
pp. 121?126. Athens, Greece.
Makoto Nagao. 1984. A framework of a mechanical
translation between Japanese and English by anal-
ogy principle. Artificial and Human Intelligence,
pp. 173?180. Amsterdam: North-Holland.
Jian Y. Nie and Jian Cai. 2001. Filtering noisy paral-
lel corpora of Web pages. In IEEE Symposium on
Natural Language Processing and Knowledge En-
gineering, pp. 453?458. Tucson, AZ.
Jian Y. Nie and Jiang Chen. 2002. Exploiting the
Web as Parallel Corpora for Cross-Language Infor-
mation Retrieval. Web Intelligence, pp. 218?239.
Philip Resnik, Mari B. Olse, and Mona Diab. 1999.
The Bible as a parallel corpus: Annotating the
?Book of 2000 Tongues?. Computers and the Hu-
manities, 33(1-2):129?153.
Philip Resnik. 1999b. Mining the Web for Bilingual
Text. In ACL?99, pp. 527?534. Maryland.
Philip Resnik and Noah A. Smith. 2003. The Web
as a Parallel Corpus. Computational Linguistics,
29(3):349?380.
Raphael Salkie. 1995. INTERSECT: a parallel cor-
pus project at Brighton University. Computers and
Texts 9 (May 1995), pp. 4?5.
Jean Veronis. 2000. Parallel Text Processing. Dor-
drecht: Kluwer.
Andy Way and Nano Gough. 2003. wEBMT:
Developing and validating an example-based ma-
chine translation system using the World Wide Web.
Computational Linguistics, 29(3):421?457.
Dekai Wu. 1994. Aligning a parallel English-Chinese
corpus statistically with lexical criteria. In ACL?94,
pp. 80?87. Las Cruces, New Mexico, U.S.A.
78
A Morpheme-based Part-of-Speech Tagger for Chinese 
Guohong Fu 
School of Computer Science and Technology 
Heilongjiang University 
Harbin 150080, P.R. China 
ghfu@hotmail.com 
Jonathan J. Webster 
Department of Chinese, Translation and Linguistics 
City University of Hong Kong 
83 Tat Chee Avenue, Hong Kong, P.R. China 
ctjjw@cityu.edu.hk 
 
 
Abstract 
This paper presents a morpheme-based 
part-of-speech tagger for Chinese. It con-
sists of two main components, namely a 
morpheme segmenter to segment each 
word in a sentence into a sequence of mor-
phemes, based on forward maximum 
matching, and a lexical tagger to label each 
morpheme with a proper tag indicating its 
position pattern in forming a word of a 
specific class, based on lexicalized hidden 
Markov models. This system have partici-
pated four closed tracks for POS tagging at 
the Fourth International Chinese Language 
Processing Bakeoff sponsored by the ACL-
SIGHAN.  
1 Introduction 
Part-of-speech (POS) tagging aims to assign each 
word in a sentence with a proper tag indicating its 
POS category. While a number of successful POS 
tagging systems have been available for English 
and many other languages, it is still a challenge to 
develop a practical POS tagger for Chinese due to 
its language-specific issues. Firstly, Chinese words 
do not have a strict one-to-one correspondence be-
tween their POS categories and functions in a sen-
tence. Secondly, an ambiguous Chinese word can 
act as different POS categories in different con-
texts without changing its form. Thirdly, there are 
many out-of-vocabulary (OOV) words in real Chi-
nese text whose POS categories are not defined in 
the dictionary used. All these factors make it much 
more difficult to achieve a high-performance POS 
tagger for Chinese. 
Recent studies in Chinese POS tagging focus on 
statistical or machine learning approaches with 
either characters or words as basic units for tagging 
(Ng and Low, 2004; Fu and Luke, 2006). Very 
little research has been devoted to resolving Chi-
nese POS tagging problems based on morphemes. 
In our system, we prefer morphemes to characters 
or words as tagging units for three reasons. First, 
words are made of morphemes instead of charac-
ters (Wu and Tseng, 1995; Packard, 2000). Sec-
ond, most morphemes are productive in word for-
mation (Baayen, 1989; Sproat and Shih, 2002; Ni-
shimoto, 2003), particularly in the formation of 
morphologically-derived words (MDWs) and 
proper nouns, which are the major source of OOV 
words in Chinese texts. Third, Packard (2000) in-
dicates that Chinese do have morphology. More-
over, morphology proves to be a very informative 
cue for predicting POS categories of Chinese OOV 
words (Tseng et al 2005).  Therefore, we believe 
that a morpheme-based framework would be more 
effective than the character- or word-based ones in 
capturing both word-internal morphological fea-
tures and word-external contextual information for 
Chinese POS disambiguation and unknown word 
guessing (UWG) as well. 
Thus we present a morpheme-based POS tagger 
for Chinese in this paper. It consists of two main 
components, namely a morpheme segmentation 
component for segmenting each word in a sentence 
into a sequence of morphemes, based on the for-
ward maximum matching (FMM) technique, and a 
lexical tagging component for labeling each seg-
mented morpheme with a proper tag indicating its 
position pattern in forming a word of a specific 
type, based on lexicalized hidden Markov models 
(HMMs). Lack of a large morphological knowl-
124
Sixth SIGHAN Workshop on Chinese Language Processing
edge base is a major obstacle to Chinese morpho-
logical analysis (Tseng and Chen, 2002). To over-
come this problem and to facilitate morpheme-
based POS tagging as well, we have also devel-
oped a statistically-based technique for automati-
cally extracting morphemes from POS-tagged cor-
pora. We participated in four closed tracks for POS 
tagging at the Fourth International Chinese Lan-
guage Processing Bakeoff sponsored by the ACL-
SIGHAN and tested our system on different testing 
corpora. In this paper, we also made a summary of 
this work and give some brief analysis on the re-
sults. 
The rest of this paper is organized as follows: 
Section 2 is a brief description of our system. Sec-
tion 3 details the settings of our system for differ-
ent testing tracks and presents the scored results of 
our system at this bakeoff. Finally, we give our 
conclusions in Section 4. 
2 System Description 
2.1 Chinese Morphemes 
In brief, Chinese morphemes can be classified 
into free morphemes and bound morphemes. A 
free morpheme can stand by itself as a word (viz. a 
basic word), whereas a bound morpheme can show 
up if and only if being attached to other 
morphemes to form a word. Free morphemes can 
be subdivided into true free morphemes and 
pseudo free morphemes. A pseudo free morpheme 
??such as  ran2-er2 ?however? can only stand 
alone, while a true free morpheme like ?? 
SHENG-CHAN ?produce? can stand alone by itself 
as a word or occur as parts of other words. Chinese 
affixes include prefixes (e.g. ? fei1 ?non-?, ? 
wei3 ?pseudo?), infixes (e.g. ??  fei1-zhi1) or 
suffixes (e.g. ? xing4 ?-ity?, ?? zhu3-yi4 ?-
ism?), in terms of their positions within a word. 
2.2 Formulation 
To perform morpheme-based Chinese POS tag-
ging, we represent a POS-tagged word in a Chi-
nese sentence as a sequence of lexical chunks with 
the aid of an extended IOB2 tag set (Fu and Luke 
2005). A lexical chunk consists of a sequence of 
constituent morphemes associated with their corre-
sponding lexical chunk tags. A lexical chunk tag 
follows the format T1-T2, indicating the POS cate-
gory T2 of a word and the position pattern T1 of a 
constituent morpheme within the word. As shown 
in Table 1, four position patterns are involved in 
our system, namely O for a single morpheme as a 
word by itself, I for a morpheme inside a word, B 
for a morpheme at the beginning of a word and E 
for a morpheme at the end of a word. 
 
Tag Definition 
Corresponding 
morpheme types 
O A morpheme as a word by itself Free morphemes 
I 
A morpheme inside a 
word 
Free morphemes and 
infixes 
B 
A word-initial mor-
pheme 
Free morphemes and 
prefixes 
E A word-final morpheme Free morphemes and 
suffixes 
Table 1. Extended IOB2 tag set 
2.3 Affix Extraction 
Due to the increasing involvement of affixation in 
Chinese word formation, affixes play a more and 
more important role in Chinese POS tagging. In 
morpheme extraction, affixes are very useful in 
determining whether a given word is derived by 
affixation. To extract affixes from corpora, we 
consider three statistics, i.e. morpheme-position 
frequency )1,( TmCount , morpheme-position 
probability )()1,()1,( mCountTmCountTmMPP =  
and morphological productivity. Following the 
proposal in (Baayen, 1989), the morphological 
productivity of a morpheme m  with a position pat-
tern 1T , denoted as )1,( TmMP , can be defined as 
)1,(
)1,(1)1,(
TmCount
TmnTmMP =                 (1) 
where )1,(1 Tmn  is the number of word types that 
occur only once in the training corpus and at the 
same time, are formed by the morpheme m  with 
the position pattern 1T . 
To estimate the above statistics for affix extrac-
tion, we only take into account the three position 
patterns B, I and E, for prefixes, infixes and suf-
fixes, respectively. Thus we can extract affixes 
from training data with the following three condi-
tions: MPFTHTmCount ?)1,( , MPPTHTmMPP ?)1,(  
and MPTHTmMP ?)1,( , where THMPF, THMPP and 
THMP are three empirically-determined thresholds. 
125
Sixth SIGHAN Workshop on Chinese Language Processing
2.4 Morpheme Extraction 
The goal of morpheme extraction is to identify 
MDWs and proper nouns in training corpora and 
prevent them from getting into the morpheme dic-
tionary for POS tagging. In the present system, the 
following criteria are applied to determine whether 
a word in training data should enter the morpheme 
dictionary. 
Completeness. With a view to the completeness 
of the morpheme dictionary, all characters in train-
ing data will be collected as morphemes. 
Word length. In general, shorter morphemes 
are more productive than longer ones in word for-
mation. As such, the length of a morpheme should 
not exceed four characters. 
Word frequency. By this criterion, a word is 
selected as a morpheme if its frequency of occur-
rences in training data is higher than a given 
threshold. 
MDWs. By this criterion, words formed by 
morphological patterns such as affixation, com-
pounding, reduplication and abbreviation will be 
excluded from the morpheme dictionary. 
Proper nouns. In some training corpora like the 
PKU corpus, some special tags are specified for 
proper nouns. In this case, they will be used to fil-
ter proper nouns during morpheme extraction. 
2.5 Lexicalized HMM Tagger 
As shown in Figure 1, our system works in three 
main steps as follows. 
 
Figure 1. Overall architecture of our system 
 
Morpheme segmentation. In this step, the 
FMM technique is employed to segment each word 
in a sentence to a sequence of morphemes associ-
ated with their position tags within the word.  
Tag candidate generation. In this step, all pos-
sible POS candidates are generated for each word 
in the sentence by consulting the morpheme dic-
tionary with its constitute morphemes and their 
related position patterns. All these candidates are 
stored in a lattice. 
Scoring and Decoding. In this step, the lexical-
ized HMMs are first employed to score each can-
didate in the lattice and the Viterbi decoding algo-
rithm is further used to search an optimal sequence 
of POS tags for the sentence. The details of lexical-
ized HMMs can be seen in (Lee et al 200) and (Fu 
and Luke, 2005).  
3 Evaluation Results 
3.1 System Settings for Different Tracks 
The POS tagging task at the fourth ACL-SIGHAN 
bakeoff consists of five closed tracks. We partici-
pated four of them, namely CKIP, CTB, NCC and 
PKU. Therefore our system is trained only using 
the relevant training corpora provided for the 
bakeoff. Furthermore, the morpheme dictionaries 
for these tracks are also extracted automatically 
from the relevant training data with the method 
presented in Sections 2.3 and 2.4. Table 2 illus-
trated the number of morphemes extracted from 
different training data.  
 
Source Training data (tokens/word types) 
Number of 
morphemes 
CKIP 721551 / 48045 30757 
CTB 642246 / 42133 26330 
NCC 535023 / 45108 28432 
PKU 1116754 / 55178 30085 
Table 2. Number of morphemes extracted from the 
training data for SIGHAN POS tagging bakeoff 
 
3.2 Evaluation Results 
 
Track Total-A IV-R OOV-R MT-R 
CKIP-O 0.9124 0.9549 0.4756 0.8953 
CTB-O 0.9234 0.9507 0.52 0.9051 
NCC-O 0.9395 0.969 0.4086 0.9059 
PKU-C 0.9266 0.9574 0.4386 0.9079 
Table 3. Scores of our system for different tracks 
 
Table 3 presents the scores of our system for dif-
ferent tracks. It should be noted that four measures 
are employed in the 4th ACL-SIGHAN bakeoff to 
Tag Candidate Generation 
Scoring & Decoding 
Morph dictionary 
A segmented sentence 
Morpheme Segmentation 
A POS-tagged sentence 
Lexicalized HMMs 
126
Sixth SIGHAN Workshop on Chinese Language Processing
score the performance of a POS tagging system, 
namely the overall accuracy (Total-A) and the re-
call with respect to in-vocabulary words (IV-R), 
OOV words (OOV-R) or multi-POS words (MT-
R).  
Although our system has achieved a promising 
performance, there is still much to be done to im-
prove it. First, the quality of the morpheme dic-
tionary is of particular importance to morpheme-
based POS tagger. Although the present study pro-
posed a statistical technique to extract morphemes 
from tagged corpora, further exploration is still 
needed on the optimization of this technique to 
acquire a more desirable morpheme dictionary for 
Chinese POS tagging. Second, morphological pat-
terns prove to be informative cues for Chinese POS 
disambiguation and OOV word prediction. How-
ever, such a knowledge base is not publicly avail-
able for Chinese. As such, in the present study we 
only made use of certain surface morphological 
features, namely the position patterns of mor-
phemes in word formation. Future research might 
usefully extend the present method to explore sys-
tematically more precise morphological features, 
including morpheme POS categories and morpho-
syntactic rules for Chinese POS tagging. 
4 Conclusion 
In this paper we have presented a morpheme-based 
POS tagger for Chinese. We participated in four 
closed tracks at the fourth SIGHAN bakeoff. The 
scored results show that our system can achieve an 
overall accuracy of 0.9124-0.9395 for different 
corpora. However, the present system is still under 
development, especially in morphological knowl-
edge acquisition. For future work, we hope to im-
prove our system with a higher quality morpheme 
dictionary and more deep morphological knowl-
edge such as morpheme POS categories and mor-
pho-syntactic rules. 
Acknowledgments 
This study was supported in part by CityU Stra-
tegic Research Grant for fundable CERG (No. 
7001879 & 7002037). 
References 
E. Nishimoto. 2003. Measuring and comparing the pro-
ductivity of Mandarin Chinese suffixes. Computa-
tional Linguistics and Chinese Language Processing, 
8(1): 49-76. 
G. Fu and K.-K. Luke. 2005. Chinese named entity rec-
ognition using lexicalized HMMs. ACM SIGKDD 
Explorations Newsletter, 7(1): 19-25. 
G. Fu and K.-K. Luke. 2006. Chinese POS disambigua-
tion and unknown word guessing with lexicalized 
HMMs. International Journal of Technology and 
Human Interaction, 2(1): 39-50. 
H. Tseng and K.-J. Chen. 2002. Design of Chinese mor-
phological analyzer. In: Proceedings of the 1st 
SIGHAN Workshop on Chinese Language Process-
ing, 1-7. 
H. Tseng, D. Jurafsky, and C. Manning. 2005. Morpho-
logical features help POS tagging of unknown words 
across language varieties. In: Proceedings of the 
Fourth SIGHAN Workshop on Chinese Language 
Processing. 
H.T. Ng and J.K. Low. 2004. Chinese part-of-speech 
tagging: One-at-a-time or all-at-once? Word-based or 
character-based?. In: Proceedings of the 2004 Con-
ference on Empirical Methods in Natural Language 
Processing (EMNLP 2004), Barcelona, Spain, 277-
284. 
J. Packard. 2000. Morphology of Chinese: A linguistic 
and cognitive approach. Cambridge University Press, 
Cambridge, UK. 
R. Sproat and C. Shih. 2002. Corpus-based methods in 
Chinese morphology. In: Proceedings of the 19th In-
ternational Conference on Computational Linguistics 
(COLING 2002), Taipei, Taiwan. 
R.H. Baayen. 1989. A corpus-based study of morpho-
logical productivity: Statistical analysis and psycho-
logical interpretation. Ph.D. thesis, Free University, 
Amsterdam. 
S.-Z. Lee, T.-J. Tsujii, and H.-C. Rim. 2000. Lexical-
ized hidden Markov models for part-of-speech tag-
ging. In: Proceedings of the 18th International Con-
ference on Computational Linguistics (COLING 
2000), Saarbruken, Germany, 481-487. 
Z. Wu, G. Tseng. 1995. ACTS: An automatic Chinese 
text segmentation systems for full text retrieval. 
Journal of the American Society for Information Sci-
ence, 46(2): 83-96. 
 
127
Sixth SIGHAN Workshop on Chinese Language Processing
Integrating Ngram Model and Case-based Learning
For Chinese Word Segmentation
Chunyu Kit Zhiming Xu Jonathan J. Webster
Department of Chinese, Translation and Linguistics
City University of Hong Kong
Tat Chee Ave., Kowloon, Hong Kong
{ctckit, ctxuzm, ctjjw}@cityu.edu.hk
Abstract
This paper presents our recent work
for participation in the First Interna-
tional Chinese Word Segmentation Bake-
off (ICWSB-1). It is based on a general-
purpose ngram model for word segmen-
tation and a case-based learning approach
to disambiguation. This system excels
in identifying in-vocabulary (IV) words,
achieving a recall of around 96-98%.
Here we present our strategies for lan-
guage model training and disambiguation
rule learning, analyze the system?s perfor-
mance, and discuss areas for further im-
provement, e.g., out-of-vocabulary (OOV)
word discovery.
1 Introduction
After about two decades of studies of Chinese word
segmentation, ICWSB-1 (henceforth, the bakeoff)
is the first effort to put different approaches and
systems to the test and comparison on common
datasets. We participated in the bakeoff with a
segmentation system that is designed to integrate a
general-purpose ngram model for probabilistic seg-
mentation and a case- or example-based learning
approach (Kit et al, 2002) for disambiguation.
The ngram model, with words extracted from
training corpora, is trained with the EM algorithm
(Dempster et al, 1977) using unsegmented train-
ing corpora. Originally it was developed to en-
hance word segmentation accuracy so as to facili-
tate Chinese-English word alignment for our ongo-
ing EBMT project, where only unsegmented texts
are available for training. It is expected to be ro-
bust enough to handle novel texts, independent of
any segmented texts for training. To simplify the
EM training, we used the uni-gram model for the
bakeoff and relied on the Viterbi algorithm (Viterbi,
1967) for the most probable segmentation, instead of
attempting to exhaust all possible segmentations of
each sentence for a complicated full version of EM
training.
The case-based learning works in a straightfor-
ward way. It first extracts case-based knowledge,
as a set of context-dependent transformation rules,
from the segmented training corpus, and then ap-
plies them to ambiguous strings in a test corpus in
terms of the similarity of their contexts. The simi-
larity is empirically computed in terms of the length
of relevant common affixes of context strings.
The effectiveness of this integrated approach is
verified by its outstanding performance on IV word
identification. Its IV recall rate, ranging from 96%
to 98%, stands at the top or the next to the top in all
closed tests in which we have participated. Unfortu-
nately, its overall performance is not sustainable at
the same level, due to the lack of a module for OOV
word detection.
This paper is intended to present the implementa-
tion of the system and analyze its performance and
problems, aiming at exploration of directions for fur-
ther improvement. The remaining sections are or-
ganized as follows. Section 2 presents the ngram
model and its training with the EM algorithm, and
Section 3 presents the case-based learning for dis-
ambiguation. The overall architecture of our system
is given in Section 4, and its performance and prob-
lems are analyzed in Section 5. Section 6 concludes
the paper and previews future work.
2 Ngram model and training
An ngram model can be utilized to find the most
probable segmentation of a sentence. Given a Chi-
nese sentence s = c1c2 ? ? ? cm (also denoted as cn1 ),
its probabilistic segmentation into a word sequence
w1w2 ? ? ?wk (also denoted as wk1 ) with the aid of an
ngram model can be formulated as
seg(s) = arg max
s= w1?w2?????wk
k
?
i
p(wi|wi?1i?n+1) (1)
where ? denotes string concatenation, wi?1i?n+1 the
context (or history) of wi, and n is the order of the
ngram model in use. We have opted for uni-gram for
the sake of simplicity. Accordingly, p(wi|wi?1i?n+1)
in (1) becomes p(wi), which is commonly estimated
as follows, given a corpus C for training.
p(wi) .= f(wi)/
?
w?C
f(w) (2)
In order to estimate a reliable p(wi), the ngram
model needs to be trained with the EM algorithm
using the available training corpus. Each EM itera-
tion aims at approaching to a more reliable f(w) for
estimating p(w), as follows:
fk+1(w) =
?
s?C
?
s??S(s)
pk(s?) fk(w ? s?) (3)
where k denotes the current iteration, S(s) the set of
all possible segmentations for s, and f k(w ? s?) the
occurrences of w in a particular segmentation s?.
However, assuming that every sentence always
has a segmentation, the following equation holds:
?
s??S(s)
pk(s?) = 1 (4)
Accordingly, we can adjust (3) as (5) with a normal-
ization factor ? = ?s??S(s) pk(s?), to avoid favor-
ing words in shorter sentences too much. In general,
shorter sentences have higher probabilities.
fk+1(w) =
?
s?C
?
s??S(s)
pk(s?)
? f
k(w ? s?) (5)
Following the conventional idea to speed up the
EM training, we turned to the Viterbi algorithm. The
underlying philosophy is to distribute more prob-
ability to more probable events. The Viterbi seg-
mentation, by utilizing dynamic programming tech-
niques to go through the word trellis of a sentence
efficiently, finds the most probable segmentation un-
der the current parameter estimation of the language
model, fulfilling (1)). Accordingly, (6) becomes
fk+1(w) =
?
s?C
pk(seg(s)) fk(w ? seg(s)) (6)
and (5) becomes
fk+1(w) =
?
s?C
fk(w ? seg(s)) (7)
where the normalization factor is skipped, for
only the Viterbi segmentation is used for EM re-
estimation. Equation (7) makes the EM training
with the Viterbi algorithm very simple for the uni-
gram model: iterate word segmentation, as (1), and
word count updating, via (7), sentence by sentence
through the training corpus until there is a conver-
gence.
Since the EM algorithm converges to a local max-
ima only, it is critical to start the training with an
initial f 0(w) for each word not too far away from its
?true? value. Our strategy for initializing f 0(w) is
to assume all possible words in the training corpus
as equiprobable and count each of them as 1; and
then p0(w) is derived using (2). This strategy is sup-
posed to have a weaker bias to favor longer words
than maximal matching segmentation.
For the bakeoff, the ngram model is trained with
the unsegmented training corpora together with the
test sets. It is a kind of unsupervised training.
Adding the test set to the training data is reasonable,
to allow the model to have necessary adaptation to-
wards the test sets. Experiments show that the train-
ing converges very fast, and the segmentation per-
formance improves significantly from iteration to it-
eration. For the bakeoff experiments, we carried out
the training in 6 iterations, because more iterations
than this have not been observed to bring any signif-
icant improvement on segmentation accuracy to the
training sets.
3 Case-based learning for disambiguation
No matter how well the language model is trained,
probabilistic segmentation cannot avoid mistakes on
ambiguous strings, although it resolves most ambi-
guities by virtue of probability. For the remaining
unresolved ambiguities, however, we have to resort
to other strategies and/or resources. Our recent study
(Kit et al, 2002) shows that case-based learning is
an effective approach to disambiguation.
The basic idea behind the case-based learning is
to utilize existing resolutions for known ambiguous
strings to do disambiguation if similar ambiguities
occur again. This learning strategy can be imple-
mented in two straightforward steps:
1. Collection of correct answers from the train-
ing corpus for ambiguous strings together with
their contexts, resulting in a set of context-
dependent transformation rules;
2. Application of appropriate rules to ambiguous
strings.
A transformation rule of this type is actually an ex-
ample of segmentation, indicating how an ambigu-
ous string is segmented within a particular context.
It has the following general form:
C l? Cr : ? ? w1 w2 ? ? ?wk
where ? is the ambiguous string, C l and Cr its left
and right contexts, respectively, and w1 w2 ? ? ?wk
the correct segmentation of ? given the contexts.
In our implementation, we set the context length on
each side to two words.
For a particular ambiguity, the example with the
most similar context in the example (or, rule) base
is applied. The similarity is measured by the sum
of the length of the common suffix and prefix of,
respectively, the left and right contexts. The details
of computing this similarity can be found in (Kit et
al., 2002) . If no rule is applicable, its probabilistic
segmentation is retained.
For the bakeoff, we have based our approach to
ambiguity detection and disambiguation rule extrac-
tion on the assumption that only ambiguous strings
cause mistakes: we detect the discrepancies of our
probabilistic segmentation and the standard segmen-
tation of the training corpus, and turn them into
transformation rules. An advantage of this approach
is that the rules so derived carry out not only disam-
biguation but also error correction. This links our
disambiguation strategy to the application of Brill?s
(1993) transformation-based error-driven learning to
Chinese word segmentation (Palmer, 1997; Hocken-
maier and Brew, 1998).
4 System architecture
The overall architecture of our word segmentation
system is presented in Figure 1.
Figure 1: Overall architecture of the system
5 Performance and analysis
The performance of our system in the bakeoff is pre-
sented in Table 1 in terms of precision (P), recall
(R) and F score in percentages, where ?c? denotes
closed tests. Its IV word identification performance
is remarkable.
However, its overall performance is not in bal-
ance with this, due to the lack of a module for OOV
word discovery. It only gets a small number of OOV
words correct by chance. The higher OOV propor-
tion in the test set, the worse is its F score. The rel-
atively high Roov for PKc track is, mostly, the result
of number recognition with regular expressions.
Test P R F OOV Roov Riv
SAc 95.2 93.1 94.2 02.2 04.3 97.2
CTBc 80.0 67.4 73.2 18.1 07.6 95.9
PKc 92.3 86.7 89.4 06.9 15.9 98.0
Table 1: System performance, in percentages (%)
5.1 Error analysis
Most errors on IV words are due to the side-effect
of the context-dependent transformation rules. The
rules resolve most remaining ambiguities and cor-
rect many errors, but at the same time they also cor-
rupt some proper segmentations. This side-effect is
most likely to occur when there is inadequate con-
text information to decide which rules to apply.
There are two strategies to remedy, or at least al-
leviate, this side-effect: (1) retrain probabilistic seg-
mentation ? a conservative strategy; or, (2) incorpo-
rate Brill?s error-driven learning with several rounds
of transformation rule extraction and application, al-
lowing mistakes caused by some rules in previous
rounds to be corrected by other rules in later rounds.
However, even worse than the above side-effect is
a bug in our disambiguation module: it always ap-
plies the first available rule, leading to many unex-
pected errors, each of which may result in more than
one erroneous word. For instance, among 430 er-
rors made by the system in the SA closed test, some
70 are due to this bug. A number of representative
examples of these errors are presented in Table 2,
together with some false errors resulting from the
inconsistency in the standard segmentation.
Errors Standard False errors Standard
2? (8) 2 ? ???D ? ? ?D
? u (7) ?u tjs? tj s?
. ? (7) .? ???? ?? ??
_ A (5) _A P_,? P_ ,?
?  (4) ? 1w?? 1w? ?
n? (4) n ? .??? . ? ? ?
Table 2: Errors and false errors
6 Conclusion and future work
We have presented our recent work for partici-
pation in ICWSB-1 based on a general-purpose
ngram model for probabilistic word segmentation
and a case-based learning strategy for disambigua-
tion. The ngram model is trained using available
unsegmented texts with the EM algorithm with the
aid of Viterbi segmentation. The learning strategy
acquires a set of context-dependent transformation
rules to correct mistakes in the probabilistic segmen-
tation of ambiguous substrings. This integrated ap-
proach demonstrates an impressive effectiveness by
its outstanding performance on IV word identifica-
tion. With elimination of the bug and false errors, its
performance could be significantly better.
6.1 Future work
The above problem analysis points to two main di-
rections for improvement in our future work: (1)
OOV word detection; (2) a better strategy for learn-
ing and applying transformation rules to reduce the
side-effect. In addition, we are also interested in
studying the effectiveness of higher-order ngram
models and variants of EM training for Chinese
word segmentation.
Acknowledgements
The work is part of the CERG project ?EBMT for
HK Legal Texts? funded by HK UGC under the
grant #9040482, with Jonathan J. Webster as the
principal investigator and Chunyu Kit, Caesar S.
Lun, Haihua Pan, King Kuai Sin and Vincent Wong
as investigators. The authors wish to thank all team
members for their contribution to this paper.
References
E. Brill. 1993. A Corpus-Based Approach to Language
Learning. Ph.D. thesis, University of Pennsylvania,
Philadelphia, PA.
A. P. Dempster, N. M. Laird, and D. B.Rubin. 1977.
Maximum likelihood from incomplete data via the em
algorithm. Journal of the Royal Statistical Society, Se-
ries B, 34:1?38.
J. Hockenmaier and C. Brew. 1998. Error-driven learn-
ing of Chinese word segmentation. In PACLIC-12,
pages 218?229, Singapore. Chinese and Oriental Lan-
guages Processing Society.
C. Kit, H. Pan, and H. Chen. 2002. Learning case-based
knowledge for disambiguating Chinese word segmen-
tation: A preliminary study. In COLING2002 work-
shop: SIGHAN-1, pages 33?39, Taipei.
D. Palmer. 1997. A trainable rule-based algorithm
for word segmentation. In ACL-97, pages 321?328,
Madrid.
A. J. Viterbi. 1967. Error bounds for convolutional codes
and an asymptotically optimum decoding algorithm.
IEEE Transactions on Information Theory, IT-13:260?
267.
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 248?252,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
A Corpus of Textual Revisions in Second Language Writing
John Lee and Jonathan Webster
The Halliday Centre for Intelligent Applications of Language Studies
Department of Chinese, Translation and Linguistics
City University of Hong Kong
{jsylee,ctjjw}@cityu.edu.hk
Abstract
This paper describes the creation of the first
large-scale corpus containing drafts and fi-
nal versions of essays written by non-native
speakers, with the sentences aligned across
different versions. Furthermore, the sentences
in the drafts are annotated with comments
from teachers. The corpus is intended to sup-
port research on textual revision by language
learners, and how it is influenced by feedback.
This corpus has been converted into an XML
format conforming to the standards of the Text
Encoding Initiative (TEI).
1 Introduction
Learner corpora have been playing an increasingly
important role in both Second Language Acquisition
and Foreign Language Teaching research (Granger,
2004; Nesi et al, 2004). These corpora contain
texts written by non-native speakers of the lan-
guage (Granger et al, 2009); many also annotate
text segments where there are errors, and the cor-
responding error categories (Nagata et al, 2011). In
addition, some learner corpora contain pairs of sen-
tences: a sentence written by a learner of English
as a second language (ESL), paired with its correct
version produced by a native speaker (Dahlmeier
and Ng, 2011). These datasets are intended to sup-
port the training of automatic text correction sys-
tems (Dale and Kilgarriff, 2011).
Less attention has been paid to how a language
learner produces a text. Writing is often an iterative
and interactive process, with cycles of textual revi-
sion, guided by comments from language teachers.
Discipline # drafts
Applied Physics 988
Asian and International Studies 410
Biology 2310
Building Science and Technology 705
Business 1754
Computer Science 466
Creative Media 118
Electronic Engineering 1532
General Education 651
Law 31
Linguistics 2165
Management Sciences 1278
Social Studies 912
Total 13320
Table 1: Draft essays are collected from courses in vari-
ous disciplines at City University of Hong Kong. These
drafts include lab reports, data analysis, argumentative
essays, and article summaries. There are 3760 distinct
essays, most of which consist of two to four successive
drafts. Each draft has on average 44.2 sentences, and the
average length of a sentence is 13.3 words. In total, the
corpus contains 7.9 million words.
Understanding the dynamics of this process would
benefit not only language teachers, but also the de-
sign of writing assistance tools that provide auto-
matic feedback (Burstein and Chodorow, 2004).
This paper presents the first large-scale corpus
that will enable research in this direction. After a re-
view of previous work (?2), we describe the design
and a preliminary analysis of our corpus (?3).
248
Figure 1: On top is a typical draft essay, interleaved with comments from a tutor (?3.2): two-digit codes from the
Comment Bank are enclosed in angled brackets, while open-ended comments are enclosed in angled brackets. On the
bottom is the same essay in TEI format, the output of the process described in ?3.3.
2 Previous Research
In this section, we summarize previous research on
feedback in language teaching, and on the nature of
the revision process by language learners.
2.1 Feedback in Language Learning
Receiving feedback is a crucial element in language
learning. While most agree that both the form and
content of feedback plays an important role, there
is no consensus on their effects. Regarding form,
some argue that direct feedback (providing correc-
tions) are more effective in improving the quality of
writing than indirect feedback (pointing out an er-
ror but not providing corrections) (Sugita, 2006), but
others reached opposite conclusions (Ferris, 2006;
Lee, 2008).
Regarding content, it has been observed that
teachers spend a disproportionate amount of time
on identifying word-level errors, at the expense of
those at higher levels, such as coherence (Furneaux
et al, 2007; Zamel, 1985). There has been no large-
scale empirical study, however, on the effectiveness
of feedback at the paragraph or discourse levels.
2.2 Revision Process
While text editing in general has been ana-
lyzed (Mahlow and Piotrowski, 2008), the nature
of revisions by language learners ? for example,
whether learners mostly focus on correcting me-
chanical, word-level errors, or also substantially re-
organize paragraph or essay structures ? has hardly
been investigated. One reason for this gap in the
literature is the lack of corpus data: none of the ex-
isting learner corpora (Izumi et al, 2004; Granger
et al, 2009; Nagata et al, 2011; Dahlmeier and Ng,
2011) contains drafts written by non-native speakers
that led to the ?final version?. Recently, two cor-
pora with text revision information have been com-
piled (Xue and Hwa, 2010; Mizumoto et al, 2011),
but neither contain feedback from language teach-
ers. Our corpus will allow researchers to not only
examine the revision process, but also investigate
any correlation with the amount and type of feed-
back.
3 Corpus Description
We first introduce the context in which our data was
collected (?3.1), then describe the kinds of com-
ments in the drafts (?3.2). We then outline the
conversion process of the corpus into XML format
(?3.3), followed by an evaluation (?3.4) and an anal-
ysis (?3.5).
3.1 Background
Between 2007 and 2010, City University of Hong
Kong hosted a language learning project where
English-language tutors reviewed and provided
feedback on academic essays written by students,
249
Paragraph level Sentence level Word level
Coherence: more 680 Conjunction missing 1554 Article missing 10586
elaboration is needed
Paragraph: new paragraph 522 Sentence: new sentence 1389 Delete this 9224
Coherence: sign posting 322 Conjunction: wrong use 923 Noun: countable 7316
Coherence: missing 222 Sentence: fragment 775 Subject-verb 4008
topic sentence agreement
Table 2: The most frequent error categories from the Comment Bank, aimed at errors at different levels.
most of whom were native speakers of Chi-
nese (Webster et al, 2011). More than 300 TESOL
students served as language tutors, and over 4,200
students from a wide range of disciplines (see Ta-
ble 1) took part in the project.
For each essay, a student posted a first draft1 as
a blog on an e-learning environment called Black-
board Academic Suite; a language tutor then directly
added comments on the blog. Figure 1 shows an ex-
ample of such a draft. The student then revised his or
her draft and may re-post it to receive further com-
ments. Most essays underwent two revision cycles
before the student submitted the final version.
3.2 Comments
Comments in the draft can take one of three forms:
Code The tutor may insert a two-digit code, repre-
senting one of the 60 common error categories
in our ?Comment Bank?, adopted from the
XWiLL project (Wible et al, 2001). These cat-
egories address issues ranging from the word
level to paragraph level (see Table 2), with
a mix of direct (e.g., ?new paragraph?) and
indirect feedback (e.g., ?more elaboration is
needed?).
Open-ended comment The tutor may also provide
personally tailored comments.
Hybrid Both a code and an open-ended comment.
For every comment2, the tutor highlights the prob-
lematic words or sentences at which it is aimed.
Sometimes, general comments about the draft as a
whole are also inserted at the beginning or the end.
1In the rest of the paper, these drafts will be referred to ?ver-
sion 1?, ?version 2?, and so on.
2Except those comments indicating that a word is missing.
3.3 Conversion to XML Format
The data format for the essays and comments was
not originally conceived for computational analysis.
The drafts, downloaded from the blog entries, are in
HTML format, with comments interspersed in them;
the final versions are Microsoft Word documents.
Our first task, therefore, is to convert them into a
machine-actionable, XML format conforming to the
standards of the Text Encoding Initiative (TEI). This
conversion consists of the following steps:
Comment extraction After repairing irregularities
in the HTML tags, we eliminated attributes that
are irrelevant to comment extraction, such as
font and style. We then identified the Comment
Bank codes and open-ended comments.
Comment-to-text alignment Each comment is
aimed at a particular text segment. The text
segment is usually indicated by highlighting
the relevant words or changing their back-
ground color. After consolidating the tags for
highlighting and colors, our algorithm looks
for the nearest, preceding text segment with a
color different from that of the comment.
Title and metadata extraction From the top of the
essay, our algorithm scans for short lines with
metadata such as the student and tutor IDs,
semester and course codes, and assignment and
version numbers. The first sentence in the es-
say proper is taken to be the title.
Sentence segmentation Off-the-shelf sentence
segmentators tend to be trained on newswire
texts (Reynar and Ratnaparkhi, 1997), which
significantly differ from the noisy text in our
corpus. We found it adequate to use a stop-list,
supplemented with a few regular expressions
250
Evaluation Precision Recall
Comment extraction
- code 94.7% 100%
- open-ended 61.8% 78.3%
Comment-to-text alignment 86.0% 85.2%
Sentence segmentation 94.8% 91.3%
Table 3: Evaluation results of the conversion process de-
scribed in ?3.3. Precision and recall are calculated on
correct detection of the start and end points of comments
and boundaries.
that detect exceptions, such as abbreviations
and digits.
Sentence alignment Sentences in consecutive ver-
sions of an essay are aligned using cosine simi-
larity score. To allow dynamic programming,
alignments are limited to one-to-one, one-to-
two, two-to-one, or two-to-two3. Below a cer-
tain threshold4, a sentence is no longer aligned,
but is rather considered inserted or deleted. The
alignment results are stored in the XCES for-
mat (Ide et al, 2002).
3.4 Conversion Evaluation
To evaluate the performance of the conversion algo-
rithm described in ?3.3, we asked a human to manu-
ally construct the TEI XML files for 14 pairs of draft
versions. These gold files are then compared to the
output of our algorithm. The results are shown in
Table 3.
In comment extraction, codes can be reliably
identified. Among the open-ended comments, how-
ever, those at the beginning and end of the drafts
severely affected the precision, since they are of-
ten not quoted in brackets and are therefore indistin-
guishable from the text proper. In comment-to-text
alignment, most errors were caused by inconsistent
or missing highlighting and background colors.
The accuracy of sentence alignment is 89.8%,
measured from the perspective of sentences in Ver-
sion 1. It is sometimes difficult to decide whether a
sentence has simply been edited (and should there-
fore be aligned), or has been deleted with a new sen-
tence inserted in the next draft.
3That is, the order of two sentences is flipped.
4Tuned to 0.5 based on a random subset of sentence pairs.
3.5 Preliminary Analysis
As shown in Table 4, the tutors were much more
likely to use codes than to provide open-ended com-
ments. Among the codes, they overwhelmingly em-
phasized word-level issues, echoing previous find-
ings (?2.1). Table 2 lists the most frequent codes.
Missing articles, noun number and subject-verb
agreement round out the top errors at the word level,
similar to the trend for Japanese speakers (Lee and
Seneff, 2008). At the sentence level, conjunctions
turn out to be challenging; at the paragraph level,
paragraph organization, sign posting, and topic sen-
tence receive the most comments.
In a first attempt to gauge the utility of the com-
ments, we measured their density across versions.
Among Version 1 drafts, a code appears on aver-
age every 40.8 words, while an open-ended com-
ment appears every 84.7 words. The respective fig-
ures for Version 2 drafts are 65.9 words and 105.0
words. The lowered densities suggest that students
were able to improve the quality of their writing af-
ter receiving feedback.
Comment Form Frequency
Open-ended 47072
Hybrid 1993
Code 88370
- Paragraph level 3.2%
- Sentence level 6.0%
- Word level 90.8%
Table 4: Distribution of the three kinds of comments
(?3.2), with the Comment Bank codes further subdivided
into different levels (See Table 2).
4 Conclusion and Future Work
We have presented the first large-scale learner cor-
pus which contains not only texts written by non-
native speakers, but also the successive drafts lead-
ing to the final essay, as well as teachers? comments
on the drafts. The corpus has been converted into an
XML format conforming to TEI standards.
We plan to port the corpus to a platform for text
visualization and search, and release it to the re-
search community. It is expected to support stud-
ies on textual revision of language learners, and the
effects of different types of feedback.
251
Acknowledgments
We thank Shun-shing Tsang for his assistance with
implementing the conversion and performing the
evaluation. This project was partially funded by a
Strategic Research Grant (#7008065) from City Uni-
versity of Hong Kong.
References
Jill Burstein and Martin Chodorow. 2004. Automated
Essay Evaluation: The Criterion online writing ser-
vice. AI Magazine.
Daniel Dahlmeier and Hwee Tou Ng. 2011. Grammat-
ical Error Correction with Alternating Structure Opti-
mization. Proc. ACL.
Robert Dale and Adam Kilgarriff. 2011. Helping Our
Own: The HOO 2011 Pilot Shared Task. Proc. Eu-
ropean Workshop on Natural Language Generation
(ENLG), Nancy, France.
Dana Ferris. 2006. Does Error Feedback Help Student
Writers? New Evidence on the Short- and Long-Term
Effects of Written Error Correction. In Feedback in
Second Language Writing: Contexts and Issues, Ken
Hyland and Fiona Hyland (eds). Cambridge Univer-
sity Press.
Clare Furneaux, Amos Paran, and Beverly Fairfax. 2007.
Teacher Stance as Reflected in Feedback on Student
Writing: An Empirical Study of Secondary School
Teachers in Five Countries. International Review of
Applied Linguistics in Language Teaching 45(1): 69-
94.
Sylviane Granger. 2004. Computer Learner Corpus Re-
search: Current Status and Future Prospect. Language
and Computers 23:123?145.
Sylviane Granger, Estelle Dagneaux, Fanny Meunier, and
Magali Paquot. 2009. International Corpus of Learner
English v2. Presses universitaires de Louvain, Bel-
gium.
Nancy Ide, Patrice Bonhomme, and Laurent Romary.
2000. XCES: An XML-based Encoding Standard for
Linguistic Corpora. Proc. LREC.
Emi Izumi, Kiyotaka Uchimoto, and Hitoshi Isahara.
2004. The NICT JLE Corpus: Exploiting the Lan-
guage Learners? Speech Database for Research and
Education. International Journal of the Computer, the
Internet and Management 12(2):119?125.
Icy Lee. 2008. Student Reactions to Teacher Feedback
in Two Hong Kong Secondary Classrooms. Journal of
Second Language Writing 17(3):144-164.
John Lee and Stephanie Seneff. 2008. An Analysis of
Grammatical Errors in Nonnative Speech in English.
Proc. IEEE Workshop on Spoken Language Technol-
ogy.
Cerstin Mahlow and Michael Piotrowski. 2008. Linguis-
tic Support for Revising and Editing. Proc. Interna-
tional Conference on Computational Linguistics and
Intelligent Text Processing.
Tomoya Mizumoto, Mamoru Komachi, Masaaki Nagata,
and Yuji Matsumoto. 2011. Mining Revision Log of
Language Learning SNS for Automated Japanese Er-
ror Correction of Second Language Learners. Proc.
IJCNLP.
Ryo Nagata, Edward Whittaker, and Vera Sheinman.
2011. Creating a Manually Error-tagged and Shallow-
parsed Learner Corpus. Proc. ACL.
Jeffrey C. Reynar and Adwait Ratnaparkhi. 1997. A
Maximum Entropy Approach to Identifying Sentence
Boundaries. Proc. 5th Conference on Applied Natural
Language Processing, Washington DC.
Yoshihito Sugita. 2006. The Impact of Teachers? Com-
ment Types on Students? Revision. ELT Journal
60(1):34?41.
Hilary Nesi, Gerard Sharpling, and Lisa Ganobcsik-
Williams. 2004. Student Papers Across the Cur-
riculum: Designing and Developing a Corpus of
British Student Writing. Computers and Composition
21(4):439?450.
Frank Tuzi. 2004. The Impact of E-Feedback on the Re-
visions of L2 Writers in an Academic Writing Course.
Computers and Composition 21(2):217-235.
Jonathan Webster, Angela Chan, and John Lee. 2011.
Online Language Learning for Addressing Hong Kong
Tertiary Students? Needs in Academic Writing. Asia
Pacific World 2(2):44?65.
David Wible, Chin-Hwa Kuo, Feng-Li Chien, Anne Liu,
and Nai-Lung Tsao. 2001. A Web-Based EFL Writ-
ing Environment: Integrating Information for Learn-
ers, Teachers, and Researchers. Computers and Edu-
cation 37(34):297-315.
Huichao Xue and Rebecca Hwa. 2010. Syntax-Driven
Machine Translation as a Model of ESL Revision.
Proc. COLING.
Vivian Zamel. 1985. Responding to Student Writing.
TESOL Quarterly 19(1):79-101.
252
