Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 535?544,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Improving Alignment of System Combination by Using
Multi-objective Optimization
Tian Xia+, Zongcheng Ji?, Shaodan Zhai+, Yidong Chen++, Qun Liu?, Shaojun Wang+
++ Xiamen University, Xiamen 361005, P.R. China
+ Wright State University, 3640 Colonel Glenn Hwy, Dayton, OH 45435, USA
? Institute of Computing Technology, Chinese Academy of Sciences
P.O. Box 2704, Beijing 100190, China
{jizongcheng, liuqun}@ict.ac.cn and ydchen@xmu.edu.cn
{xia.7, zhai.6, shaojun.wang}@wright.edu
Abstract
This paper proposes a multi-objective opti-
mization framework which supports heteroge-
neous information sources to improve align-
ment in machine translation system combi-
nation techniques. In this area, most of
techniques usually utilize confusion networks
(CN) as their central data structure to com-
pact an exponential number of an potential hy-
potheses, and because better hypothesis align-
ment may benefit constructing better quality
confusion networks, it is natural to add more
useful information to improve alignment re-
sults. However, these information may be het-
erogeneous, so the widely-used Viterbi algo-
rithm for searching the best alignment may
not apply here. In the multi-objective opti-
mization framework, each information source
is viewed as an independent objective, and
a new goal of improving all objectives can
be searched by mature algorithms. The so-
lutions from this framework, termed Pareto
optimal solutions, are then combined to con-
struct confusion networks. Experiments on
two Chinese-to-English translation datasets
show significant improvements, 0.97 and 1.06
BLEU points over a strong Indirected Hidden
Markov Model-based (IHMM) system, and
4.75 and 3.53 points over the best single ma-
chine translation systems.
1 Introduction
System combination (SC) techniques have the
power of boosting translation quality in BLEU by
several percent over the best among all input ma-
chine translation systems (Bangalore et al, 2001;
Matusov et al, 2006; Sim et al, 2007; Rosti et al,
2007b; Rosti et al, 2007a; Huang and Papineni,
2007; He et al, 2008; Rosti et al, 2008; He and
Toutanova, 2009; Li et al, 2009; Feng et al, 2009;
Pauls et al, 2009). A central data structure in the
SC is the confusion network, and its quality greatly
affects the final performance. He et al (2008) pro-
posed a new hypothesis alignment algorithm for
constructing high-quality confusion networks called
Indirect Hidden Markov Model (IHMM), which
does better in synonym matching compared with
the classic translation edit rate (TER) based algo-
rithm (Rosti et al, 2007b; Rosti et al, 2008; Sim et
al., 2007). Now, current state-of-the-art SC systems
have been using IHMM or variants in their align-
ment algorithms more or less (Li et al, 2009; Feng
et al, 2009).
Our motivation derives from an observation that
in an ideal alignment of a pair of sentences, many-to-
many alignments often exist. For instance, ?be about
to? has the same meaning with ?be on the point
of?. Because Hidden Markov Model based align-
ment algorithms, e.g. IHMM for system combina-
tion, HMM in GIZA++ software for statistical ma-
chine translation (SMT) (Och and Ney, 2000; Koehn
et al, 2003), are designed for one-to-many align-
ment, and running GIZA++ from two directions to
gain better performance turns into a standard opera-
tion in SMT, therefore we are seeking a way to em-
power IHMM by introducing bi-directional informa-
tion.
However, it appears to be intractable in an IHMM
model to search the optimal solution by simply
defining a new goal as a product of probabilities
535
from two directions. To bypass this problem, Liang
et al (2006) adopts a simple and effective variational
inference algorithm.
Further, different alignment algorithms capture
different information and linguistic phenomena for
a pair of sentences, hence more information would
be expected to benefit the final alignment. Liang?s
method may not be suitable for this expected out-
come.
We propose to adopt multi-objective optimiza-
tion framework to support heterogeneous informa-
tion sources which may induce difficulties in a
conventional search algorithm. In this framework,
there exist a variety of matured multi-objective op-
timization algorithms, e.g. evolutionary algorithm
(Deb et al, 2000; Deb et al, 2002), Tabu search
(Hansen, 1997), ants colony (Engelbrecht, 2005),
and simulated annealing (Serafini, 1994). In this
work, we select the multi-objective evolutionary al-
gorithm because of its public open source software
(http://www.iitk.ac.in/kangal/codes.shtml). On the
other hand, this framework is also totally unsuper-
vised. It prevents weights of a linearly combined
goal from training even if all information is homoge-
neous and applicable in a Viterbi search (Forney Jr,
1973). This framework views any useful informa-
tion benefiting alignment as an independent objec-
tive, and researchers just need to write short codes
for objective definitions. The search algorithm seeks
for potentially better solutions which are no worse
than the current solution set. The output from multi-
objective optimization algorithms includes a set of
solutions, called Pareto optimal solutions, each one
being a many-to-many alignment. We then com-
bine and normalize them into a unique one-to-one
alignment to perform confusion network construc-
tion (Section 3.3).
Our work is conducted on the classic pipeline
which has three modules, pair-wise hypothesis
alignment, confusion network construction, and
training. Now many work integrates neighboring
modules to avoid propagated errors to gain improved
performance. For example, Rosti et al (2008), and
Li et al (2009) combine the first and the second
module, and He and Toutanova (2009) combine all
modules into one directly. Nevertheless, the classic
structure also owns its merits. Because of the in-
dependence between modules, a system is relatively
simple to maintain, and improvements on each mod-
ule might contribute to final performance additively.
Based on our work, lattice-based minimum error
rate training (lattice-MERT) and minimum bayes
risk training techniques (Kumar et al, 2009) could
be adopted on the third module. And Feng et al
(2009) in the second module adopts a different data
structure called lattice which could directly use our
better many-to-many alignment for construction.
Experiments on the Chinese-to-English task on
two datasets use four objectives, IHMM probabil-
ity (Section 3.2.1), and alignment probability from
GIZA++ (Section 3.2.2) from two directions. Re-
sults show multi-objective optimization framework
efficiently integrates different information to gain
approximately 1 BLEU point improvement over a
strong baseline.
2 Background
We briefly give an introduction to confusion net-
works, and because the IHMM based alignment is
an important objective in our multi-objective frame-
work, here we also provide detailed definition of for-
mulas for completeness of content.
2.1 Confusion Network
Table 1 shows hypotheses h1 and h2 are aligned to
selected backbone h0. When alignment algorithm
obtains good enough results, the expected output
?he prefers apples? is included in its corresponding
confusion network in Figure 1. This suggests de-
veloping better alignment algorithm may help creat-
ing high-quality confusion networks. This also mo-
tivates us to use the BLEU of oracle hypotheses to
approximately measure the quality of a set of CNs.
We hereafter call it an oracle BLEU of a CN. See
more in Section 5.1.
h0 :he feels like apples
h1 :he prefer ? apples
h2 :him prefers to apples
Table 1: A toy example of hypothesis alignment, where
h0 is the backbone hypothesis. h1and h2 are aligned to
the backbone separately. The resulting confusion net-
work is in Figure 1.
A confusion network G = (V,E) is a directed
acyclic graph with a unique source and sink vertex,
536
b b b b
him
he
prefers
prefer
feel
?
like
like
b
apples
Figure 1: A classic confusion network, and the bold path
the expected output.
formally a weighted finite state automation (FSA),
where V is the set of nodes andE is the set of edges.
Each edge is restricted to attach to a single word as
well as an associated probability. A special mark ?
is a place-holder denoting no word here.
2.2 IHMM-based Alignment
Indirected Hidden Markov Model (IHMM) was
firstly proposed by He et. al (2008). Compared with
TER-based alignment performing literal matching,
IHMM supports synonym comparison in redefining
emission probabilities in an IHMM model.
Let f I = (f1, . . . fI) be a backbone hypothesis,
and eJ = (e1, . . . eJ) be a hypothesis aligned to the
backbone, both being English sentences in our ex-
periments. Let aJ = {a1, . . . aj} be an alignment.
Suppose the aj th word in f I is aligned to jth word
in eJ , and the conditional probability that the hy-
pothesis is generated by the backbone, shown in the
upper graph of Figure 3, is given by
p(f I , eJ) =
?
aJ
J?
j=1
{pt(aj |aj?1, I)po(ej |faj )}
(1)
The distortion probability pt(aj |aj?1, I) from po-
sition aj?1 to aj , relies on jumped distance, which
is computed as follows:
pt(i? |i, I) = c(i
?
? i)
?I
t=1 c(t? i)
(2)
The distortion parameters c(d) are grouped into
11 buckets, c(? ?4),c(?3),c(?2). . .c(5),c(? 6).
Because all the hypotheses in system combina-
tion are in the same language, the IHMM model
would support more monotonic alignments, and
non-monotonic alignments will be penalized.
c(d) = (1 + |d? 1|)?K , d = ?4 . . . 6 (3)
where K is tuned on held-out data.
Let p0 be the probability of jumping to a null
word state, which is also tuned on held-out data, and
the accurate transition probability becomes:
pt(i? |i, I) =
{
p0 if i? = null
(1? p0)pt(i
?
|i, I) otherwise
(4)
The output probability po(e|f) from the state
word f to the observation word e, also called trans-
lation probability, is a linear interpolation of se-
mantic similarity psem(e|f) and surface similarity
psur(e|f), and ? is the interpolation factor:
po(e|f) = ?psem(e|f) + (1? ?)psur(e|f) (5)
When calculating semantic similarity psem(e|f),
source sentence src is needed, and a bilingual prob-
abilistic dictionary pdic(w1|w2) is necessary.
psem(e|f) ?
?
c?src
pdic(c|f) ? pdic(e|c) (6)
Note that psem(e|f) has been updated with differ-
ent source sentences.
The surface similarity psur(e|f) is measured by
the literal matching rate:
psur(e, f) = exp{?[ LMP(f, e)max(|f |, |e|) ? 1]} (7)
where LMP(f, e) is the length of the longest
matched prefix, and ? is a smoothing parameter.
3 Multi-objective Optimization
Many decision making problems in the real world
consider more than one objective. One natural way
is to scalarize multiple objectives into one by assign-
ing it with a weight vector. This method allows a
simple optimization algorithm in many cases, while
in system combination, it would cause problems.
In the first module, in order to train suitable
weights of objectives, extra labeled data is needed,
besides that, the efficient Viterbi algorithm for
searching the optimal alignment would not work for
537
the alignment objectives in this work. More, the pa-
rameter training in the third module relies on the
CNs constructed from the output of the first mod-
ule, which increases the instability of the whole sys-
tem. Therefore, an unsupervised multi-objective al-
gorithm may be a good choice allowing for more
alignment information.
There exist other alternative optimization algo-
rithms in the multi-objective optimization frame-
work, though the evolutionary algorithm is adopted
here, we only introduce some general concepts.
3.1 Pareto Optimal Solutions
A general multi-objective optimization problem
consists of a number of objectives and is associated
with a number of constraints. Mathematically, the
problem can be written as follows (Deb, 2001)
Maximize fi(x) i = 1 . . .M
s.t. gj(x) ? 0 j = 1 . . . N
hk(x) = 0 k = 1 . . .K
where x denotes a potential solution, its structure re-
lying on different problems, and the number of con-
straints M,N,K depend on different problems. All
the functions fi, gj , hk map a solution x into a scalar.
We will explain them in terms of system combina-
tion.
In this work, we refer to x = {xi,j |xi,j ? {0, 1}}
as a potential alignment of a pair of hypotheses,
where xi,j is a boolean value to denote whether the
ith word in the first hypothesis is aligned to the jth
word in the second hypothesis. Here the definition of
x seems different from that of a in Formula 1, and
they could convert to each other. Using a line-based
access style, a matrix can be unfolded as a vector.
We refer to f as IHMM alignment probability (He et
al., 2008) and GIZA++ alignment probability (Chen
et al, 2009), total four objectives from two direc-
tions, and the larger the objectives, the better. The
gjs and hks serve as the role of checking if x repre-
sents a legal alignment. For instance, the subscripts
of xi,j are not in bounds.
Definition 1. Let x, x? be two potential align-
ments. If fi(x) ? fi(x?) holds for all i, we call
the alignment x dominates the alignment x?. If there
0
1
2
3
4
5
0 1 2 3 4 5
b
p3
b
p5
b
p7
b
p1
?p2
? p6
?p4
X: Reversed IHMM Probability (1e-8)
Y:D
irec
tIH
MM
Pro
bab
ility
(1e
-8)
Figure 2: Sample solutions with only two objectives.
Pareto Optimal Solutions p1, p3, p5, p7. Other points
p2, p4, p6 are dominated by at least one point in the Pareto
optimal solutions.
does not exist any alignment x?? to dominate x, we
call the alignment x to be non-dominated.
Definition 2. A alignment x is said to be Pareto
optimal if there is no other alignment x? found to
dominate x.
In Figure 2, p1 dominates p2, and p2 dominates
p4. To summarize, a point is dominated by the ones
on its upper and right side with ties. In this example,
p1, p3, p5, p7 are Pareto optimal.
In some cases, Pareto optimal solutions can be
used for good candidate solutions. Considering
the IHMM model, maximizing Y axis, the top-4
best alignments are p1, p2, p3, p4. But from the
view of Pareto optimal, the top-4 alignments would
be p1, p3, p5, p7 without order, which considers a
greater range than a single optimization model. In
our method, we just combine these Pareto optimal
solutions equally into a unique alignment (Section
3.3).
Our adopted multi-objective optimization search-
ing algorithm is the non-dominated sorting ge-
netic algorithm II (NSGA-II) (Deb et al, 2000;
Deb et al, 2002) with an open source software
(http://www.iitk.ac.in/kangal/codes.shtml). NSGA-
II has a complexity of O(mn2), wherem is the num-
ber of objectives and n is the population size in an
evolutionary algorithm.
3.2 Objectives in Evolutionary Algorithm
The optimization objectives in our experiments can
be categorized as an IHMM alignment probability
(He et al, 2008) and GIZA++ alignment probability
538
b b b
b b bO:
S: f1 f2 f3
e1 e2 e3
b b b
b b b
S:
O:
e1 e2 e3
f1 f2 f3
Backbone
Backbone
Figure 3: The same alignment (f1, e1)(f1, e2)(f2, e3) in
two IHMM models. The upper one is a typical example
in IHMM, and in the bottom one, because any word in the
observation is required not to correspond to two statuses,
it has a minor trouble. S: status sequence, O: observation
sequence.
(Chen et al, 2009), total four from two directions.
3.2.1 IHMM Probability
A typical IHMM alignment is demonstrated
in the upper graph of Figure 3, where a
backbone is acting the role of a status se-
quence. The unnormalized conditional align-
ment probability is [pt(1|null)] ? [pt(1|1)pt(2|1)] ?
[po(e1|f1)po(e2|f1)po(e3|f2)]. However, the same
alignment (f1, e1)(f1, e2)(f2, e3), if we change the
alignment direction, the backbone being observa-
tions, would be a bit different. We offer a minor
modification to Formula 1.
Look at the bottom graph of Figure 3, the obser-
vation f1 has two statuses, e1 and e2 at the same
time, it becomes ambiguous to compute the tran-
sitional probability between pt(3|1) and pt(3|2).
This is because IHMM algorithm deals with one-
to-many alignments, and MOEA permits many-to-
many alignments.
We hence empirically modify the IHMM model
to support many-to-many alignments. A new status
is defined, rather than a single position pt(j|i), but
as a set of positions pt({j}|{i}). The positions in
one status need not to be adjacent to each other.
The redefined transitional probability
pt({j}|{i}) =
1
|{j}| ? |{i}|
?
i,j
pt(j|i)
The redefined emission probability
po(j|{i}) =
?
i
po(j|i)
We need to note that there is no guarantee on
the closed property of probabilities, though these
approximations prove to be effective in a practical
sense. Straightforwardly, when there is only one po-
sition in a new status, the expanded IHMM degener-
ates to the standard IHMM.
Let us return to the second IHMM ex-
ample. The new probability becomes
[pt(1|null)pt(2|null)] ? [12pt(3|1)pt(3|2) ?pt(null|3)] ?
[po(f1|e1)po(f1|e2)po(f2|e3)po(f3|null)].
3.2.2 Alignment Probability
GIZA++ considers very different and more in-
formation in alignment, we attempt to utilize them.
All probabilities appearing in below formulas can be
looked up in GIZA++.
Given a pair of hypotheses f I = (f1, . . . fI),
eJ = (e1, . . . eJ), and their alignment a, the align-
ment probability could be calculated as follows
pGiza(eJ |f I ,a) =
?
ei
T (ei|f
I ,a)
T (ei|f
I ,a) =
{
n(?i|ei)
?
(j,i)?a t(ei|fj)a(j|i)/?i if?i 6= 0
n(0|ei)t(ei|null)a(0|i) otherwise
?i = |{j|(i, j) ? a}|
where ?i is the fertility number, t(e|c) the transla-
tion probability for the word pair, z(j|i) alignment
probability to show how likely a target word at posi-
tion i could be translated into a source word at posi-
tion j, and n(?|e) is the fertility probability to show
how likely a given target word e is translated into ?
source words.
In order to increase the coverage of words, we col-
lect all the hypothesis pairs in both the tuning set
and the test set and feed them into GIZA++. This
is an off-line operation, which makes it not suitable
for an online translation system. In some circum-
stances, users submit a pile of documents in the hope
of high-quality translations, thus more useful knowl-
edge sources would be helpful. In our experiments,
a pure GIZA++ based system combination does not
perform as well as IHMM based, but does benefit
the final translation quality if combined in our multi-
objective optimization framework.
539
3.3 Configuration of Evolutionary Algorithm
3.3.1 Encoding
Given a sentence pair <f I , eJ>, we define a two-
dimensional matrix x = {zi,j |zij ? {0, 1}} to en-
code a set of possible alignments. Using a line-based
access style, the matrix could be unfolded as a vector
with |I| ? |J | bits of length.
3.3.2 Initialization
Because in NSGA-II software the initial popu-
lation are generated at random. In order to make
NSGA-II more consistent and flexible, better initial
seeds should be fed with, thus we combine an ex-
isting word alignment results as input. Here we use
together two N-best lists generated from directional
HMM and reversed HMM respectively for initializa-
tion.
3.3.3 Normalization of Pareto Optimal
Solutions
Multi-objective optimization algorithms do not
pose weights on objectives, thus they output a set
of so-called Pareto optimal solutions, each of which
is a many-to-many alignment. We can understand
them as an N-best alignment list without explicit
preferences. We also empirically compare it with the
idea that directly cuts an N-best list from the IHMM
based alignment.
We describe a two-stage strategy for normaliza-
tion. Firstly, we use a simple and effective voting
strategy to combine a set of many-to-many align-
ments into a single many-to-many alignment, and
Secondly we normalize it into a one-to-one align-
ment for confusion network construction. In the first
stage, we count the number of word-to-word align-
ments on each position pair (i, j). If there is more
than a half number of alignments, then we output 1,
otherwise 0. In the second stage, if any word relates
to more than one word alignment, the one with the
highest posterior probability is selected (He et al,
2008; Feng et al, 2009). The posterior probabili-
ties can be computed in a classic forward-backward
procedure in IHMM (He et al, 2008).
4 Training and Decoding
Our work does not change the classic pipeline, thus
the model and features are nearly identical to the
ones in (Rosti et al, 2007b; He et al, 2008), which
are modeled in a log-linear fashion in Eq. 8. Trans-
lation on a CN is just a concatenation of edges tra-
versed, on which 4 categories of features are defined.
1. word posterior probabilities. In Eq. 8,
p(w|sys, span) are word confidence scores. If
the word w comes from the kth hypothesis of
thesys-th system, the raw score should be 1k+1 ,and then it would be normalized by the same
sys and span. The same word coming from
different systems owns a different score, so
there are sys system weights ?sys.
2. logarithm of language model score, L(h).
3. number of null edge, Numnull.
4. number of words, Numw.
log(h) =
?
span log(
?
sys ?sysp(w|sys, span))
+ w0L(h) + w1Numnull + w2Numw
(8)
Decoding a confusion network is straightforward,
traversing each node from left to right, and the beam
search algorithm will retain for each node an N-
best list. The final N-best can be acquired following
(Huang and Chiang, 2005).
The training process follows minimum error rate
training (MERT) described in (Och, 2003; Koehn et
al., 2003). In each iteration, the Powell algorithm
would attempt to predict the optimal parameters on
the cumulative N-best list.
5 Experiments
We evaluate our method in two datasets in the
Chinese-to-English task. In the first one, NIST MT
2002 and 2005 are used for tuning and testing re-
spectively, and in the second, the newswire part of
MT 2006 and 2008 are for tuning and testing. A 5-
gram language model is trained on the Xinhua por-
tion of the Gigaword corpus. We report the case-
sensitive NIST-BLEU score.
Four single machine translation systems partici-
pating in the system combination consist of a BTG-
based system using a Max-Entropy based reordering
model, a hierarchical phrase-based system, a Moses
decoder and a syntax-based system. 10-best unique
hypotheses from a single system on the development
540
SYSTEM MT 2005 MT 2008(news)
best single 0.3207 0.3016
IHMM* 0.3585(+3.78%) 0.3263(+2.47%)
IncIHMM 0.3639(+4.32%) 0.3320(+3.04%)
GIZA++ 0.3438(+2.31%) 0.3166(+1.50%)
PPBD 0.3619(+4.10%) 0.3306(+2.90%)
N-best IHMM 0.3590(+3.83%) 0.3270(+2.54%)
dH+rH 0.3604 0.3284
dH+dT 0.3610 0.3290
dH+rH+dT 0.3609 0.3289
dH+rH+rT 0.3630?(+4.27%) 0.3320?(+3.04%)
dH+rH+dT+rT 0.3682??(+4.75%) 0.3369??(+3.53%)
Table 2: PPBD is a posterior probabilistic-based decod-
ing (section 5.3). N-best IHMM simulates the Pareto op-
timal solutions in our method (section 5.3). The last five
systems adopt different objective combinations. The im-
provement percents in parentheses are compared to the
best single. dH: directed IHMM, rH: reversed IHMM,
dT: directed translation probability, rT: reversed transla-
tion probability. ?? significance at 0.01 level, and ? sig-
nificance at 0.05 level over the IHMM model.
and test sets are collected as the input of the system
combination.
Our baseline systems are described as follows.
Two main baseline systems are IHMM based and in-
cremental IHMM (Li et al, 2009). The first system
differs from our method just in hypothesis alignment
algorithm, and the second combines the first and sec-
ond module of the system combination pipeline.
Because our method utilizes bidirectional infor-
mation, we also provide another two alternative
systems for comparison, which are GIZA++ based
alignment and the posterior probability based align-
ment (Liang et al, 2006). Finally, we also provide
an N-best alignment IHMM system, which com-
bines an N-best alignment list to simulate the Pareto
optimal solutions in our method.
The method that linearly combines all objectives
is not listed as our baseline like (Duh et al, 2012)
does, because their algorithm finds the best weighted
solution in a fixed and small solution set, while
in our problem, the solution space is a trellis-style
structure consisting of an exponential number of so-
lutions, and no efficient algorithms apply here.
The IHMM based alignment utilizes typical set-
tings (He et al, 2008; Feng et al, 2009). The
smoothing factor for the surface similarity model,
and ? = 3 the controlling factor for the distor-
tion model, K = 2. The bilingual probabilistic
dictionary is trained in the FBIS corpus which in-
cludes about 230k parallel sentence pairs. GIZA++
based system is to run GIZA++ from two directions
to align all the hypotheses, and make the intersec-
tion using grow-diag-final heuristics (Koehn et al,
2003). The many-to-many alignments are normal-
ized with the same method with ours. Our system
employs NSGA-II software to realize the MOEA al-
gorithm. The main parameters, generation number,
cross probability and mutation probability, and pop-
ulation size, are empirically set as 100, 0.9, 0.001
and 40, and we examine the influence of difference
populations sizes in the full system combination.
5.1 The Quality of Confusion Networks
This experiment shows the relationship between hy-
pothesis alignment and confusion network. Intu-
itively, we expect a better hypothesis alignment
would reduce the error in constructing confusion
networks, and then improve the final translation
quality.
We first use the alignment error rate (AER) (Och
and Ney, 2000), which is widely used to measure
the quality of hypothesis alignment. The smaller,
the better. For convenience, we only examine exact
literal matching. IHMM based alignment reaches
around 0.15 in AER, and our method 0.145.
As the AER may not vividly reflect the relations
between alignment and the final BLEU of systems,
and the quality of confusion network is hard to mea-
sure directly, we assume that the quality of confu-
sion networks could be measured by the oracle hy-
potheses that could be generated from them. We test
the BLEU of the oracle hypotheses.
From this angle, we demonstrate several oracle
BLEU of CNs generated from some conventional
alignment algorithms. The results are shown in Ta-
ble 3.
We find the confusion network from IHMM based
alignment (He et al, 2008) is better than that from
TER based alignment (Rosti et al, 2007b) by about
1 point in both two datasets. These quantities agree
with the final improvements in the BLEU score in
(He et al, 2008). As confusion networks from
MOEA based alignment also show superiority over
541
alignment MT02 MT05
GIZA++ 0.5690 0.5228
TER 0.5720 0.5270
IHMM 0.5883 0.5382
IncIHMM 0.5931 0.5453
MOEA 0.6017 0.5526
Table 3: Oracle BLEUs of CNs. GIZA++: invoking
GIZA++ software. TER: minimum translation edit rate.
IHMM: indirect hidden markov model. IncIHMM: in-
cremental indirect hidden markov model. MOEA: multi-
objective evolution algorithm.
that from IHMM based in the oracle BLEU, we ex-
pect our final translation quality would be improved.
In Table 3, GIZA++ and TER perform simi-
larly, because the former is more capable of tackling
many-to-many alignments over the latter, while lat-
ter based might obtain relatively more precise align-
ment information. Both of the two do not consider
synonym matching compared to IHMM.
Our method and IncIHMM overpass IHMM on
this metric due to different strategies. Obtaining bet-
ter hypothesis alignment or better construction of
confusion networks benefit the quality of CNs.
5.2 Different Objective Combinations
As our framework is convenient to support different
alignment information, we test the influence of dif-
ferent objective combinations to the final translation
quality. We adopt four objectives to depict the can-
didate alignment, directed IHMM probability (dH),
reversed IHMM probability (rH), directed alignment
probability (dT), and reversed alignment probability
(rT). Table 2 demonstrates all the results.
We can see that the IHMM based system out-
performs the GIZA++ based system by about 1-1.5
points in BLEU, which agrees with the difference of
oracle BLEU in Table 1. From (He et al, 2008), the
IHMM based system outperforms the TER based by
1 point, which also agrees with our results in Table
1. Our system, using dH + rH + dT + rT, improves
BLEU score by about 1 points over the IHMM based
system. This comparison verifies our assumption,
improving the quality of the confusion network does
improve system performance.
The different feature combinations exhibit inter-
esting results. The system with dH + rH + dT is
0.05 point better than the system with dH + rH, and
the system dH + rH + rT is 0.3 point better than sys-
tem with dH + rH, so the contributions of feature
dT and rT are 0.05 and 0.3 respectively. While the
two features are used together in the fourth system,
the contribution is about 0.8 point, rather than 0.35.
This phenomenon also proves the correlations be-
tween different features.
Our method explores a way to integrate GIZA++
and IHMM, and is supportive of useful features.
Compared to the classic and powerful IHMM based
system, we obtained an improvement of 0.97 points
on MT 05 and 1.06 points on news of MT 2008,
and equivalently over the best single system by 4.75
points and 3.53 points respectively. More, compared
with the incremental IHMM, our system also shows
moderate improvement, though not much. We hope
these two ideas could be effectively combined in the
future work.
5.3 Comparison with Other Bi-directional
Alignment Methods
Our method introduces multiple alignment infor-
mation into system combination to obtain improve-
ments, thus it would be interesting to explore other
alternative methods for utilizing this information.
We provide three alternative methods similar to our
motivations, and they fall into two categories.
The first category is from the angle of bi-
directional alignment. We use GiZA++ alignment
and the posterior probability decoding-based align-
ment for comparison. The basic idea for the lat-
ter is setting a word-to-word alignment xi,j as 1,
if its approximate posterior marginal probability
q(xi,j , x) = pd(xi,j |x, ?d) ? pr(xi,j |x, ?r) is greater
than a threshold ?, where pd and pr are posterior
marginal probabilities from directed and reversed
IHMM models, which could be conveniently com-
puted with a forward-backward algorithm, and the ?
is tuned on a validation-set optimized data. We just
list some ? values to examine its best performance
shown in Table 4.
The second class is because our method combines
the Pareto optimal solutions that consist of several
candidate alignments, thus for fairness we also use
a 100-best outputs from the directed IHMM model
and conduct the same normalization technique.
The general results are shown in Table 2. We can
542
? MT 2005 MT 2008
IHMM 0.3585 0.3263
0.15 0.3556 0.3391
0.2 0.3619 0.3306
0.25 0.3575 0.3278
0.3 0.3608 0.3259
Table 4: Posterior decoding. When threshold ? are set
to suitable values, simple bi-directional alignment could
overpass the baseline.
see that, GIZA++ leads to the worst performance,
which can be explained as GIZA++ does not support
synonym matching like IHMM. The N-best IHMM
has a minor improvement over the IHMM method.
We found differences in the N-best list are not obvi-
ous enough. In comparison, the posterior decoding
method brings relatively significant improvements
on both datasets. However, the threshold ? must
be selected suitably. Table 4 lists the ideal results,
which will be hampered when tuning on a validation
set.
All of the three candidate methods can not conve-
niently support extra alignment information, and a
linear model poses restrictions on features to get an
efficient decoding, the multi-objective optimization
may be a good selection as an inference algorithm in
many circumstances.
5.4 Population Size
We test the influence of final translation quality and
time consumed by different population size.
population BLEU
size MT 2005
20 0.3597
40 0.3682
60 0.3655
Table 5: Big population size consumes more CPU time.
In our experiments, we use a multi-thread technique to
speed up the alignment, and choose 40 as the parameter
to leverage the time and BLEU.
We expect enlarging the population size would
improve the translation quality, but the BLEU in
population size set as 60 does not overpass when set
as 40. We conjecture that, in our code, if the N-best
size from IHMM (we set as 50-best) does not reach
the population size, we would use randomly gener-
ated seeds, which may hamper the performance of
MOEA. We also tried a larger population in MOEA,
but did not receive obvious improvement on perfor-
mance.
We exerted a hard restriction on the genes in evo-
lutionary algorithm, that is many-to-many discon-
tiguous alignment is forbidden. This trick speeds up
running by about 20 times, and does not harm sys-
tem performance. Now our method runs about 0.9
seconds to align a pair of hypotheses. In practice,
we utilize multi-thread to speed up.
6 Conclusion
In this paper, we explore a multi-objective frame-
work to conveniently support more useful alignment
objectives to improve the hypothesis alignment. By
a minor modification of the first module in the
classic pipeline, we successfully combine GIZA++
and IHMM to obtain significant improvement over
a powerful and state-of-the-art IHMM based sys-
tem. In comparison with another genre of improving
system combination by combing adjacent modules
of the pipeline, more powerful incremental IHMM
here, our system also show moderate improvement.
Though, our best system may not overpass He and
Toutanova (2009) who combine all the modules into
a unified training procedure, we believe our method
could boost many work on the higher modules of the
pipeline to obtain a further improvement to match
their work.
7 Acknowledgement
This research is partially supported by Air Force
Office of Scientific Research under grant FA9550-
10-1-0335, the National Science Foundation under
grant IIS RI-small 1218863 and a Google research
award. We thank the anonymous reviewers for their
insightful comments.
References
B Bangalore, German Bordel, and Giuseppe Riccardi.
2001. Computing consensus translation from multi-
ple machine translation systems. In Automatic Speech
Recognition and Understanding.
Yidong Chen, Xiaodong Shi, Changle Zhou, and
Qingyang Hong. 2009. A word alignment
543
model based on multiobjective evolutionary algo-
rithms. Computers and Mathematics with Applica-
tions, 57.
Kalyanmoy Deb, Samir Agrawal, Amrit Pratap, and
Tanaka Meyarivan. 2000. A fast elitist non-dominated
sorting genetic algorithm for multi-objective optimiza-
tion: Nsga-ii. Lecture notes in computer science,
1917:849?858.
Kalyanmoy Deb, Amrit Pratap, Sameer Agarwal, and
TAMT Meyarivan. 2002. A fast and elitist multiob-
jective genetic algorithm: Nsga-ii. Evolutionary Com-
putation, IEEE Transactions on, 6(2):182?197.
Kalyanmoy Deb. 2001. Multi-objective optimization.
Multi-objective optimization using evolutionary algo-
rithms, pages 13?46.
John DeNero, Shankar Kumar, Ciprian Chelba, and Franz
Och. 2010. Model combination for machine transla-
tion. In Proc. of NAACL, pages 975?983.
Kevin Duh, Katsuhito Sudoh, Xianchao Wu, Hajime
Tsukada, and Masaaki Nagata. 2012. Learning to
translate with multiple objectives. In Proc. of ACL,
pages 1?10.
Andries P Engelbrecht. 2005. Fundamentals of compu-
tational swarm intelligence, volume 1. Wiley Chich-
ester.
Yang Feng, Yang Liu, Haitao Mi, Qun Liu, and Ya-
juan L?. 2009. Lattice-based system combination for
statistical machine translation. In Proc. of EMNLP,
EMNLP ?09.
G David Forney Jr. 1973. The viterbi algorithm. Proc.
of the IEEE, 61(3):268?278.
Michael Pilegaard Hansen. 1997. Tabu search for mul-
tiobjective optimization: Mots. In Proc. of Multiple
Criteria Decision Making, pages 574?586.
Xiaodong He and Kristina Toutanova. 2009. Joint opti-
mization for machine translation system combination.
In Proc. of EMNLP.
Xiaodong He, Mei Yang, Jianfeng Gao, Patrick Nguyen,
and Robert Moore. 2008. Indirect-hmm-based hy-
pothesis alignment for combining outputs from ma-
chine translation systems. In Proc. of EMNLP.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proc. of IWPT.
Fei Huang and Kishore Papineni. 2007. Hierarchical
system combination for machine translation. In Proc.
of EMNLP-CoNLL.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
of NAACL.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, et al 2007. Moses: Open source toolkit for sta-
tistical machine translation. In Proc. of ACL: Poster,
pages 177?180.
Shankar Kumar, Wolfgang Macherey, Chris Dyer, and
Franz Och. 2009. Efficient minimum error rate train-
ing and minimum bayes-risk decoding for translation
hypergraphs and lattices. In Proc. of Joint ACL and
AFNLP.
Zhifei Li and Sanjeev Khudanpur. 2009. Forest rerank-
ing for machine translation with the perceptron algo-
rithm. GALE book chapter on MT From Text.
Chi-Ho Li, Xiaodong He, Yupeng Liu, and Ning Xi.
2009. Incremental hmm alignment for mt system com-
bination. In Proc. of Joint ACL and AFNLP.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proc. of NAACL.
Evgeny Matusov, Nicola Ueffing, and Hermann Ney.
2006. Computing consensus translation from multiple
machine translation systems using enhanced hypothe-
ses alignment. In Proc. of EACL.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. Proc. of ACL-08: HLT, pages 192?
199.
F. J. Och and H. Ney. 2000. Improved statistical align-
ment models. pages 440?447, October.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. of ACL, pages
160?167.
Adam Pauls, John DeNero, and Dan Klein. 2009. Con-
sensus training for consensus decoding in machine
translation. In Proc. of EMNLP.
Antti-Veikko I Rosti, Necip Fazil Ayan, Bing Xiang, Spy-
ros Matsoukas, Richard Schwartz, and Bonnie Dorr.
2007a. Combining outputs from multiple machine
translation systems. In Proc. of NAACL-HLT.
Antti-Veikko I Rosti, Spyros Matsoukas, and Richard
Schwartz. 2007b. Improved word-level system com-
bination for machine translation. In Proc. of ACL, vol-
ume 45.
Antti-Veikko I Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2008. Incremental hypothesis
alignment for building confusion networks with appli-
cation to machine translation system combination. In
Proc. of WSMT.
Paolo Serafini. 1994. Simulated annealing for multi ob-
jective optimization problems. In Proc. of Multiple
Criteria Decision Making, pages 283?292. Springer.
Khe Chai Sim, William J Byrne, Mark JF Gales, Hichem
Sahbi, and Phil C Woodland. 2007. Consensus net-
work decoding for statistical machine translation sys-
tem combination. In Proc. of ICASSP, volume 4.
Yong Zhao and Xiaodong He. 2009. Using n-gram based
features for machine translation system combination.
In Proc. of NAACL: Short Papers, pages 205?208.
544
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 459?468,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Translation Model Adaptation for Statistical Machine Translation with
Monolingual Topic Information?
Jinsong Su1,2, Hua Wu3, Haifeng Wang3, Yidong Chen1, Xiaodong Shi1,
Huailin Dong1, and Qun Liu2
Xiamen University, Xiamen, China1
Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China2
Baidu Inc., Beijing, China3
{jssu, ydchen, mandel, hldong}@xmu.edu.cn
{wu hua, wanghaifeng}@baicu.com
liuqun@ict.ac.cn
Abstract
To adapt a translation model trained from
the data in one domain to another, previous
works paid more attention to the studies of
parallel corpus while ignoring the in-domain
monolingual corpora which can be obtained
more easily. In this paper, we propose a
novel approach for translation model adapta-
tion by utilizing in-domain monolingual top-
ic information instead of the in-domain bilin-
gual corpora, which incorporates the topic in-
formation into translation probability estima-
tion. Our method establishes the relationship
between the out-of-domain bilingual corpus
and the in-domain monolingual corpora vi-
a topic mapping and phrase-topic distribution
probability estimation from in-domain mono-
lingual corpora. Experimental result on the
NIST Chinese-English translation task shows
that our approach significantly outperforms
the baseline system.
1 Introduction
In recent years, statistical machine translation(SMT)
has been rapidly developing with more and more
novel translation models being proposed and put in-
to practice (Koehn et al, 2003; Och and Ney, 2004;
Galley et al, 2006; Liu et al, 2006; Chiang, 2007;
Chiang, 2010). However, similar to other natural
language processing(NLP) tasks, SMT systems of-
ten suffer from domain adaptation problem during
practical applications. The simple reason is that the
underlying statistical models always tend to closely
?Part of this work was done during the first author?s intern-
ship at Baidu.
approximate the empirical distributions of the train-
ing data, which typically consist of bilingual sen-
tences and monolingual target language sentences.
When the translated texts and the training data come
from the same domain, SMT systems can achieve
good performance, otherwise the translation quality
degrades dramatically. Therefore, it is of significant
importance to develop translation systems which can
be effectively transferred from one domain to anoth-
er, for example, from newswire to weblog.
According to adaptation emphases, domain adap-
tation in SMT can be classified into translation mod-
el adaptation and language model adaptation. Here
we focus on how to adapt a translation model, which
is trained from the large-scale out-of-domain bilin-
gual corpus, for domain-specific translation task,
leaving others for future work. In this aspect, pre-
vious methods can be divided into two categories:
one paid attention to collecting more sentence pairs
by information retrieval technology (Hildebrand et
al., 2005) or synthesized parallel sentences (Ueffing
et al, 2008; Wu et al, 2008; Bertoldi and Federico,
2009; Schwenk and Senellart, 2009), and the other
exploited the full potential of existing parallel cor-
pus in a mixture-modeling (Foster and Kuhn, 2007;
Civera and Juan, 2007; Lv et al, 2007) framework.
However, these approaches focused on the studies of
bilingual corpus synthesis and exploitation while ig-
noring the monolingual corpora, therefore limiting
the potential of further translation quality improve-
ment.
In this paper, we propose a novel adaptation
method to adapt the translation model for domain-
specific translation task by utilizing in-domain
459
monolingual corpora. Our approach is inspired by
the recent studies (Zhao and Xing, 2006; Zhao and
Xing, 2007; Tam et al, 2007; Gong and Zhou, 2010;
Ruiz and Federico, 2011) which have shown that a
particular translation always appears in some spe-
cific topical contexts, and the topical context infor-
mation has a great effect on translation selection.
For example, ?bank? often occurs in the sentences
related to the economy topic when translated into
?y?inha?ng?, and occurs in the sentences related to the
geography topic when translated to ?he?a`n?. There-
fore, the co-occurrence frequency of the phrases in
some specific context can be used to constrain the
translation candidates of phrases. In a monolingual
corpus, if ?bank? occurs more often in the sentences
related to the economy topic than the ones related
to the geography topic, it is more likely that ?bank?
is translated to ?y?inha?ng? than to ?he?a`n?. With the
out-of-domain bilingual corpus, we first incorporate
the topic information into translation probability es-
timation, aiming to quantify the effect of the topical
context information on translation selection. Then,
we rescore all phrase pairs according to the phrase-
topic and the word-topic posterior distributions of
the additional in-domain monolingual corpora. As
compared to the previous works, our method takes
advantage of both the in-domain monolingual cor-
pora and the out-of-domain bilingual corpus to in-
corporate the topic information into our translation
model, thus breaking down the corpus barrier for
translation quality improvement. The experimental
results on the NIST data set demonstrate the effec-
tiveness of our method.
The reminder of this paper is organized as fol-
lows: Section 2 provides a brief description of trans-
lation probability estimation. Section 3 introduces
the adaptation method which incorporates the top-
ic information into the translation model; Section
4 describes and discusses the experimental results;
Section 5 briefly summarizes the recent related work
about translation model adaptation. Finally, we end
with a conclusion and the future work in Section 6.
2 Background
The statistical translation model, which contains
phrase pairs with bi-directional phrase probabilities
and bi-directional lexical probabilities, has a great
effect on the performance of SMT system. Phrase
probability measures the co-occurrence frequency of
a phrase pair, and lexical probability is used to vali-
date the quality of the phrase pair by checking how
well its words are translated to each other.
According to the definition proposed by (Koehn
et al, 2003), given a source sentence f = fJ1 =
f1, . . . , fj , . . . , fJ , a target sentence e = eI1 =
e1, . . . , ei, . . . , eI , and its word alignment a which
is a subset of the Cartesian product of word position-
s: a ? (j, i) : j = 1, . . . , J ; i = 1, . . . , I , the phrase
pair (f? , e?) is said to be consistent (Och and Ney,
2004) with the alignment if and only if: (1) there
must be at least one word inside one phrase aligned
to a word inside the other phrase and (2) no words
inside one phrase can be aligned to a word outside
the other phrase. After all consistent phrase pairs are
extracted from training corpus, the phrase probabil-
ities are estimated as relative frequencies (Och and
Ney, 2004):
?(e?|f?) =
count(f? , e?)
?
e??
count(f? , e??)
(1)
Here count(f? , e?) indicates how often the phrase pair
(f? , e?) occurs in the training corpus.
To obtain the corresponding lexical weight, we
first estimate a lexical translation probability distri-
bution w(e|f) by relative frequency from the train-
ing corpus:
w(e|f) =
count(f, e)
?
e?
count(f, e?)
(2)
Retaining the alignment a? between the phrase pair
(f? , e?), the corresponding lexical weight is calculated
as
pw(e?|f? , a?) =
|e?|?
i=1
1
|{j|(j, i) ? a?}|
?
?(j,i)?a?
w(ei|fj) (3)
However, the above-mentioned method only
counts the co-occurrence frequency of bilingual
phrases, assuming that the translation probability is
independent of the context information. Thus, the
statistical model estimated from the training data is
not suitable for text translation in different domains,
resulting in a significant drop in translation quality.
460
3 Translation Model Adaptation via
Monolingual Topic Information
In this section, we first briefly review the principle
of Hidden Topic Markov Model(HTMM) which is
the basis of our method, then describe our approach
to translation model adaptation in detail.
3.1 Hidden Topic Markov Model
During the last couple of years, topic models such
as Probabilistic Latent Semantic Analysis (Hof-
mann, 1999) and Latent Dirichlet Allocation mod-
el (Blei, 2003), have drawn more and more attention
and been applied successfully in NLP community.
Based on the ?bag-of-words? assumption that the or-
der of words can be ignored, these methods model
the text corpus by using a co-occurrence matrix of
words and documents, and build generative model-
s to infer the latent aspects or topics. Using these
models, the words can be clustered into the derived
topics with a probability distribution, and the corre-
lation between words can be automatically captured
via topics.
However, the ?bag-of-words? assumption is an
unrealistic oversimplification because it ignores the
order of words. To remedy this problem, Gruber et
al.(2007) propose HTMM, which models the topics
of words in the document as a Markov chain. Based
on the assumption that all words in the same sen-
tence have the same topic and the successive sen-
tences are more likely to have the same topic, HTM-
M incorporates the local dependency between words
by Hidden Markov Model for better topic estima-
tion.
HTMM can also be viewed as a soft clustering
tool for words in training corpus. That is, HT-
MM can estimate the probability distribution of a
topic over words, i.e. the topic-word distribution
P (word|topic) during training. Besides, HTMM
derives inherent topics in sentences rather than in
documents, so we can easily obtain the sentence-
topic distribution P (topic|sentence) in training
corpus. Adopting maximum likelihood estima-
tion(MLE), this posterior distribution makes it pos-
sible to effectively calculate the word-topic distri-
bution P (topic|word) and the phrase-topic distribu-
tion P (topic|phrase) both of which are very impor-
tant in our method.
3.2 Adapted Phrase Probability Estimation
We utilize the additional in-domain monolingual
corpora to adapt the out-of-domain translation mod-
el for domain-specific translation task. In detail, we
build an adapted translation model in the following
steps:
? Build a topic-specific translation model to
quantify the effect of the topic information on
the translation probability estimation.
? Estimate the topic posterior distributions of
phrases in the in-domain monolingual corpora.
? Score the phrase pairs according to the prede-
fined topic-specific translation model and the
topic posterior distribution of phrases.
Formally, we incorporate monolingual topic in-
formation into translation probability estimation,
and decompose the phrase probability ?(e?|f?)1 as
follows:
?(e?|f?) =
?
tf
?(e?, tf |f?)
=
?
tf
?(e?|f? , tf ) ? P (tf |f?) (4)
where ?(e?|f? , tf ) indicates the probability of trans-
lating f? into e? given the source-side topic tf ,
P (tf |f?) denotes the phrase-topic distribution of f? .
To compute ?(e?|f?), we first apply HTMM to re-
spectively train two monolingual topic models with
the following corpora: one is the source part of
the out-of-domain bilingual corpus Cf out, the oth-
er is the in-domain monolingual corpus Cf in in the
source language. Then, we respectively estimate
?(e?|f? , tf ) and P (tf |f?) from these two corpora. To
avoid confusion, we further refine ?(e?|f? , tf ) and
P (tf |f?) with ?(e?|f? , tf out) and P (tf in|f?), respec-
tively. Here, tf out is the topic clustered from the
corpus Cf out, and tf in represents the topic derived
from the corpus Cf in.
However, the two above-mentioned probabilities
can not be directly multiplied in formula (4) be-
cause they are related to different topic spaces from
1Due to the limit of space, we omit the description of the cal-
culation method of the phrase probability ?(f? |e?), which can be
adjusted in a similar way to ?(e?|f?) with the help of in-domain
monolingual corpus in the target language.
461
different corpora. Besides, their topic dimension-
s are not assured to be the same. To solve this
problem, we introduce the topic mapping probabili-
ty P (tf out|tf in) to map the in-domain phrase-topic
distribution into the one in the out-domain topic s-
pace. To be specific, we obtain the out-of-domain
phrase-topic distribution P (tf out|f?) as follows:
P (tf out|f?) =
?
tf in
P (tf out|tf in) ? P (tf in|f?) (5)
Thus formula (4) can be further refined as the fol-
lowing formula:
?(e?|f?) =
?
tf out
?
tf in
?(e?|f? , tf out)
?P (tf out|tf in) ? P (tf in|f?) (6)
Next we will give detailed descriptions of the cal-
culation methods for the three probability distribu-
tions mentioned in formula (6).
3.2.1 Topic-Specific Phrase Translation
Probability ?(e?|f? , tf out)
We follow the common practice (Koehn et al,
2003) to calculate the topic-specific phrase trans-
lation probability, and the only difference is that
our method takes the topical context information in-
to account when collecting the fractional counts of
phrase pairs. With the sentence-topic distribution
P (tf out|f) from the relevant topic model of Cf out,
the conditional probability ?(e?|f? , tf out) can be eas-
ily obtained by MLE method:
?(e?|f? , tf out)
=
?
?f ,e??Cout
count?f ,e?(f? , e?) ? P (tf out|f)
?
e??
?
?f ,e??Cout
count?f ,e?(f? , e??) ? P (tf out|f)
(7)
where Cout is the out-of-domain bilingual training
corpus, and count?f ,e?(f? , e?) denotes the number of
the phrase pair (f? , e?) in sentence pair ?f , e?.
3.2.2 Topic Mapping Probability P (tf out|tf in)
Based on the two monolingual topic models re-
spectively trained from Cf in and Cf out, we com-
pute the topic mapping probability by using source
word f as the pivot variable. Noticing that there
are some words occurring in one corpus only, we
use the words belonging to both corpora during the
mapping procedure. Specifically, we decompose
P (tf out|tf in) as follows:
P (tf out|tf in)
=
?
f?Cf out
?
Cf in
P (tf out|f) ? P (f |tf in) (8)
Here we first get P (f |tf in) directly from the top-
ic model related to Cf in. Then, considering the
sentence-topic distribution P (tf out|f) from the rel-
evant topic model of Cf out, we define the word-
topic distribution P (tf out|f) as:
P (tf out|f)
=
?
f?Cf out
countf (f) ? P (tf out|f)
?
tf out
?
f?Cf out
countf (f) ? P (tf out|f)
(9)
where countf (f) denotes the number of the word f
in sentence f .
3.2.3 Phrase-Topic Distribution P (tf in|f? )
A simple way to compute the phrase-topic distri-
bution is to take the fractional counts from Cf in
and then adopt MLE to obtain relative probability.
However, it is infeasible in our model because some
phrases occur in Cf out while being absent in Cf in.
To solve this problem, we further compute this pos-
terior distribution by the interpolation of two model-
s:
P (tf in|f?) = ? ? Pmle(tf in|f?) +
(1? ?) ? Pword(tf in|f?) (10)
where Pmle(tf in|f?) indicates the phrase-topic dis-
tribution by MLE, Pword(tf in|f?) denotes the
phrase-topic distribution which is decomposed into
the topic posterior distribution at the word level, and
? is the interpolation weight that can be optimized
over the development data.
Given the number of the phrase f? in sentence f
denoted as countf (f?), we compute the in-domain
phrase-topic distribution in the following way:
Pmle(tf in|f?)
=
?
f?Cf in
countf (f?) ? P (tf in|f)
?
tf in
?
f?Cf in
countf (f?) ? P (tf in|f)
(11)
462
Under the assumption that the topics of all word-
s in the same phrase are independent, we consid-
er two methods to calculate Pword(tf in|f?). One is
a ?Noisy-OR? combination method (Zens and Ney,
2004) which has shown good performance in calcu-
lating similarities between bags-of-words in differ-
ent languages. Using this method, Pword(tf in|f?) is
defined as:
Pword(tf in|f?)
= 1? Pword(t?f in|f?)
? 1?
?
fj?f?
P (t?f in|fj)
= 1?
?
fj?f?
(1? P (tf in|fj)) (12)
where Pword(t?f in|f?) represents the probability that
tf in is not the topic of the phrase f? . Similarly,
P (t?f in|fj) indicates the probability that tf in is not
the topic of the word fj .
The other method is an ?Averaging? combination
one. With the assumption that tf in is the topic of f?
if at least one of the words in f? belongs to this topic,
we derive Pword(tf in|f?) as follows:
Pword(tf in|f?) ?
?
fj?f?
P (tf in|fj)/|f? | (13)
where |f? | denotes the number of words in phrase f? .
3.3 Adapted Lexical Probability Estimation
Now we briefly describe how to estimate the adapted
lexical weight for phrase pairs, which can be adjust-
ed in a similar way to the phrase probability.
Specifically, adopting our method, each word is
considered as one phrase consisting of only one
word, so
w(e|f) =
?
tf out
?
tf in
w(e|f, tf out)
?P (tf out|tf in) ? P (tf in|f) (14)
Here we obtain w(e|f, tf out) with a simi-
lar approach to ?(e?|f? , tf out), and calculate
P (tf out|tf in) and P (tf in|f) by resorting to
formulas (8) and (9).
With the adjusted lexical translation probability,
we resort to formula (4) to update the lexical weight
for the phrase pair (f? , e?).
4 Experiment
We evaluate our method on the Chinese-to-English
translation task for the weblog text. After a brief de-
scription of the experimental setup, we investigate
the effects of various factors on the translation sys-
tem performance.
4.1 Experimental setup
In our experiments, the out-of-domain training cor-
pus comes from the FBIS corpus and the Hansard-
s part of LDC2004T07 corpus (54.6K documents
with 1M parallel sentences, 25.2M Chinese words
and 29M English words). We use the Chinese Sohu
weblog in 20091 and the English Blog Authorship
corpus2 (Schler et al, 2006) as the in-domain mono-
lingual corpora in the source language and target
language, respectively. To obtain more accurate top-
ic information by HTMM, we firstly filter the noisy
blog documents and the ones consisting of short sen-
tences. After filtering, there are totally 85K Chinese
blog documents with 2.1M sentences and 277K En-
glish blog documents with 4.3M sentences used in
our experiments. Then, we sample equal numbers of
documents from the in-domain monolingual corpo-
ra in the source language and the target language to
respectively train two in-domain topic models. The
web part of the 2006 NIST MT evaluation test da-
ta, consisting of 27 documents with 1048 sentences,
is used as the development set, and the weblog part
of the 2008 NIST MT test data, including 33 docu-
ments with 666 sentences, is our test set.
To obtain various topic distributions for the out-
of-domain training corpus and the in-domain mono-
lingual corpora in the source language and the tar-
get language respectively, we use HTMM tool devel-
oped by Gruber et al(2007) to conduct topic model
training. During this process, we empirically set the
same parameter values for the HTMM training of d-
ifferent corpora: topics = 50, ? = 1.5, ? = 1.01,
iters = 100. See (Gruber et al, 2007) for the
meanings of these parameters. Besides, we set the
interpolation weight ? in formula (10) to 0.5 by ob-
serving the results on development set in the addi-
tional experiments.
We choose MOSES, a famous open-source
1http://blog.sohu.com/
2http://u.cs.biu.ac.il/ koppel/BlogCorpus.html
463
phrase-based machine translation system (Koehn
et al, 2007), as the experimental decoder.
GIZA++ (Och and Ney, 2003) and the heuristics
?grow-diag-final-and? are used to generate a word-
aligned corpus, from which we extract bilingual
phrases with maximum length 7. We use SRILM
Toolkits (Stolcke, 2002) to train two 4-gram lan-
guage models on the filtered English Blog Author-
ship corpus and the Xinhua portion of Gigaword
corpus, respectively. During decoding, we set the
ttable-limit as 20, the stack-size as 100, and per-
form minimum-error-rate training (Och and Ney,
2003) to tune the feature weights for the log-linear
model. The translation quality is evaluated by
case-insensitive BLEU-4 metric (Papineni et al,
2002). Finally, we conduct paired bootstrap sam-
pling (Koehn, 2004) to test the significance in BLEU
score differences.
4.2 Result and Analysis
4.2.1 Effect of Different Smoothing Methods
Our first experiments investigate the effect of dif-
ferent smoothing methods for the in-domain phrase-
topic distribution: ?Noisy-OR? and ?Averaging?.
We build adapted phrase tables with these two meth-
ods, and then respectively use them in place of the
out-of-domain phrase table to test the system perfor-
mance. For the purpose of studying the generality of
our approach, we carry out comparative experiments
on two sizes of in-domain monolingual corpora: 5K
and 40K.
Adaptation
Method
(Dev) MT06
Web
(Tst) MT08
Weblog
Baseline 30.98 20.22
Noisy-OR (5K) 31.16 20.45
Averaging (5K) 31.51 20.54
Noisy-OR (40K) 31.87 20.76
Averaging (40K) 31.89 21.11
Table 1: Experimental results using different smoothing
methods.
Table 1 reports the BLEU scores of the translation
system under various conditions. Using the out-of-
domain phrase table, the baseline system achieves
a BLEU score of 20.22. In the experiments with
the small-scale in-domain monolingual corpora, the
BLEU scores acquired by two methods are 20.45
and 20.54, achieving absolute improvements of 0.23
and 0.32 on the test set, respectively. In the exper-
iments with the large-scale monolingual in-domain
corpora, similar results are obtained, with absolute
improvements of 0.54 and 0.89 over the baseline
system.
From the above experimental results, we know
that both ?Noisy-OR? and ?Averaging? combination
methods improve the performance over the base-
line, and ?Averaging? method seems to be slight-
ly better. This finding fails to echo the promis-
ing results in the previous study (Zens and Ney,
2004). This is because the ?Noisy-OR? method in-
volves the multiplication of the word-topic distribu-
tion (shown in formula (12)), which leads to much
sharper phrase-topic distribution than ?Averaging?
method, and is more likely to introduce bias to the
translation probability estimation. Due to this rea-
son, all the following experiments only consider the
?Averaging?method.
4.2.2 Effect of Combining Two Phrase Tables
In the above experiments, we replace the out-of-
domain phrase table with the adapted phrase table.
Here we combine these two phrase tables in a log-
linear framework to see if we could obtain further
improvement. To offer a clear description, we repre-
sent the out-of-domain phrase table and the adapted
phrase table with ?OutBP? and ?AdapBP?, respec-
tively.
Used Phrase
Table
(Dev) MT06
Web
(Tst) MT08
Weblog
Baseline 30.98 20.22
AdapBp (5K) 31.51 20.54
+ OutBp 31.84 20.70
AdapBp (40K) 31.89 21.11
+ OutBp 32.05 21.20
Table 2: Experimental results using different phrase ta-
bles. OutBp: the out-of-domain phrase table. AdapBp:
the adapted phrase table.
Table 2 shows the results of experiments using d-
ifferent phrase tables. Applying our adaptation ap-
proach, both ?AdapBP? and ?OutBP + AdapBP?
consistently outperform the baseline, and the lat-
464
Figure 1: Effect of in-domain monolingual corpus size on
translation quality.
ter produces further improvements over the former.
Specifically, the BLEU scores of the ?OutBP +
AdapBP? method are 20.70 and 21.20, which ob-
tain 0.48 and 0.98 points higher than the baseline
method, and 0.16 and 0.09 points higher than the
?AdapBP? method. The underlying reason is that the
probability distribution of each in-domain sentence
often converges on some topics in the ?AdapBP?
method and some translation probabilities are over-
estimated, which leads to negative effects on the
translation quality. By using two tables together, our
approach reduces the bias introduced by ?AdapBP?,
therefore further improving the translation quality.
4.2.3 Effect of In-domain Monolingual Corpus
Size
Finally, we investigate the effect of in-domain
monolingual corpus size on translation quality. In
the experiment, we try different sizes of in-domain
documents to train different monolingual topic mod-
els: from 5K to 80K with an increment of 5K each
time. Note that here we only focus on the exper-
iments using the ?OutBP + AdapBP? method, be-
cause this method performs better in the previous
experiments.
Figure 1 shows the BLEU scores of the transla-
tion system on the test set. It can be seen that the
more data, the better translation quality when the
corpus size is less than 30K. The overall BLEU
scores corresponding to the range of great N val-
ues are generally higher than the ones correspond-
ing to the range of small N values. For example, the
BLEU scores under the condition within the range
[25K, 80K] are all higher than the ones within the
range [5K, 20K]. When N is set to 55K, the BLEU
score of our system is 21.40, with 1.18 gains on the
baseline system. This difference is statistically sig-
nificant at P < 0.01 using the significance test tool
developed by Zhang et al(2004). For this experi-
mental result, we speculate that with the increment
of in-domain monolingual data, the corresponding
topic models provide more accurate topic informa-
tion to improve the translation system. However,
this effect weakens when the monolingual corpora
continue to increase.
5 Related work
Most previous researches about translation model
adaptation focused on parallel data collection. For
example, Hildebrand et al(2005) employed infor-
mation retrieval technology to gather the bilingual
sentences, which are similar to the test set, from
available in-domain and out-of-domain training da-
ta to build an adaptive translation model. With
the same motivation, Munteanu and Marcu (2005)
extracted in-domain bilingual sentence pairs from
comparable corpora. Since large-scale monolin-
gual corpus is easier to obtain than parallel corpus,
there have been some studies on how to generate
parallel sentences with monolingual sentences. In
this respect, Ueffing et al (2008) explored semi-
supervised learning to obtain synthetic parallel sen-
tences, and Wu et al (2008) used an in-domain
translation dictionary and monolingual corpora to
adapt an out-of-domain translation model for the in-
domain text.
Differing from the above-mentioned works on
the acquirement of bilingual resource, several stud-
ies (Foster and Kuhn, 2007; Civera and Juan, 2007;
Lv et al, 2007) adopted mixture modeling frame-
work to exploit the full potential of the existing par-
allel corpus. Under this framework, the training cor-
pus is first divided into different parts, each of which
is used to train a sub translation model, then these
sub models are used together with different weights
during decoding. In addition, discriminative weight-
ing methods were proposed to assign appropriate
weights to the sentences from training corpus (Mat-
soukas et al, 2009) or the phrase pairs of phrase ta-
ble (Foster et al, 2010). Final experimental result-
s show that without using any additional resources,
these approaches all improve SMT performance sig-
465
nificantly.
Our method deals with translation model adap-
tation by making use of the topical context, so let
us take a look at the recent research developmen-
t on the application of topic models in SMT. As-
suming each bilingual sentence constitutes a mix-
ture of hidden topics and each word pair follows a
topic-specific bilingual translation model, Zhao and
Xing (2006,2007) presented a bilingual topical ad-
mixture formalism to improve word alignment by
capturing topic sharing at different levels of linguis-
tic granularity. Tam et al(2007) proposed a bilin-
gual LSA, which enforces one-to-one topic corre-
spondence and enables latent topic distributions to
be efficiently transferred across languages, to cross-
lingual language modeling and translation lexicon
adaptation. Recently, Gong and Zhou (2010) also
applied topic modeling into domain adaptation in
SMT. Their method employed one additional feature
function to capture the topic inherent in the source
phrase and help the decoder dynamically choose re-
lated target phrases according to the specific topic of
the source phrase.
Besides, our approach is also related to context-
dependent translation. Recent studies have shown
that SMT systems can benefit from the utiliza-
tion of context information. For example, trigger-
based lexicon model (Hasan et al, 2008; Mauser et
al., 2009) and context-dependent translation selec-
tion (Chan et al, 2007; Carpuat and Wu, 2007; He
et al, 2008; Liu et al, 2008). The former gener-
ated triplets to capture long-distance dependencies
that go beyond the local context of phrases, and the
latter built the classifiers which combine rich con-
text information to better select translation during
decoding. With the consideration of various local
context features, these approaches all yielded stable
improvements on different translation tasks.
As compared to the above-mentioned works, our
work has the following differences.
? We focus on how to adapt a translation mod-
el for domain-specific translation task with the
help of additional in-domain monolingual cor-
pora, which are far from full exploitation in the
parallel data collection and mixture modeling
framework.
? In addition to the utilization of in-domain
monolingual corpora, our method is differen-
t from the previous works (Zhao and Xing,
2006; Zhao and Xing, 2007; Tam et al, 2007;
Gong and Zhou, 2010) in the following aspect-
s: (1) we use a different topic model ? HTMM
which has different assumption from PLSA and
LDA; (2) rather than modeling topic-dependent
translation lexicons in the training process, we
estimate topic-specific lexical probability by
taking account of topical context when extract-
ing word pairs, so our method can also be di-
rectly applied to topic-dependent phrase proba-
bility modeling. (3) Instead of rescoring phrase
pairs online, our approach calculate the transla-
tion probabilities offline, which brings no addi-
tional burden to translation systems and is suit-
able to translate the texts without the topic dis-
tribution information.
? Different from trigger-based lexicon model and
context-dependent translation selection both of
which put emphasis on solving the translation
ambiguity by the exploitation of the context in-
formation at the sentence level, we adopt the
topical context information in our method for
the following reasons: (1) the topic informa-
tion captures the context information beyond
the scope of sentence; (2) the topical context in-
formation is integrated into the posterior prob-
ability distribution, avoiding the sparseness of
word or POS features; (3) the topical context
information allows for more fine-grained dis-
tinction of different translations than the genre
information of corpus.
6 Conclusion and future work
This paper presents a novel method for SMT sys-
tem adaptation by making use of the monolingual
corpora in new domains. Our approach first esti-
mates the translation probabilities from the out-of-
domain bilingual corpus given the topic information,
and then rescores the phrase pairs via topic mapping
and phrase-topic distribution probability estimation
from in-domain monolingual corpora. Experimental
results show that our method achieves better perfor-
mance than the baseline system, without increasing
the burden of the translation system.
In the future, we will verify our method on oth-
466
er language pairs, for example, Chinese to Japanese.
Furthermore, since the in-domain phrase-topic dis-
tribution is currently estimated with simple smooth-
ing interpolations, we expect that the translation sys-
tem could benefit from other sophisticated smooth-
ing methods. Finally, the reasonable estimation of
topic number for better translation model adaptation
will also become our study emphasis.
Acknowledgement
The authors were supported by 863 State Key
Project (Grant No. 2011AA01A207), National
Natural Science Foundation of China (Grant Nos.
61005052 and 61103101), Key Technologies R&D
Program of China (Grant No. 2012BAH14F03). We
thank the anonymous reviewers for their insightful
comments. We are also grateful to Ruiyu Fang and
Jinming Hu for their kind help in data processing.
References
Michiel Bacchiani and Brian Roark. 2003. Unsuper-
vised Language Model Adaptation. In Proc. of ICAS-
SP 2003, pages 224-227.
Michiel Bacchiani and Brian Roark. 2005. Improving
Machine Translation Performance by Exploiting Non-
Parallel Corpora. Computational Linguistics, pages
477-504.
Nicola Bertoldi and Marcello Federico. 2009. Domain
Adaptation for Statistical Machine Translation with
Monolingual Resources. In Proc. of ACL Workshop
2009, pages 182-189.
David M. Blei. 2003. Latent Dirichlet Allocation. Jour-
nal of Machine Learning, pages 993-1022.
Ivan Bulyko, Spyros Matsoukas, Richard Schwartz, Long
Nguyen and John Makhoul. 2007. Language Model
Adaptation in Machine Translation from Speech. In
Proc. of ICASSP 2007, pages 117-120.
Marine Carpuat and Dekai Wu. 2007. Improving Statis-
tical Machine Translation Using Word Sense Disam-
biguation. In Proc. of EMNLP 2007, pages 61-72.
Yee Seng Chan, Hwee Tou Ng, and David Chiang. 2006.
Word sense disambiguation improves statistical ma-
chine translation. In Proc. of ACL 2007, pages 33-40.
Boxing Chen, George Foster and Roland Kuhn. 2010.
Bilingual Sense Similarity for Statistical Machine
Translation. In Proc. of ACL 2010, pages 834-843.
David Chiang. 2007. Hierarchical Phrase-Based Trans-
lation. Computational Linguistics, pages 201-228.
David Chiang. 2010. Learning to Translate with Source
and Target Syntax. In Proc. of ACL 2010, pages 1443-
1452.
Jorge Civera and Alfons Juan. 2007. Domain Adaptation
in Statistical Machine Translation with Mixture Mod-
elling. In Proc. of the Second Workshop on Statistical
Machine Translation, pages 177-180.
Matthias Eck, Stephan Vogel and Alex Waibel. 2004.
Language Model Adaptation for Statistical Machine
Translation Based on Information Retrieval. In Proc.
of Fourth International Conference on Language Re-
sources and Evaluation, pages 327-330.
Matthias Eck, Stephan Vogel and Alex Waibel. 2005.
Low Cost Portability for Statistical Machine Transla-
tion Based on N-gram Coverage. In Proc. of MT Sum-
mit 2005, pages 227-234.
George Foster and Roland Kuhn. 2007. Mixture Model
Adaptation for SMT. In Proc. of the Second Workshop
on Statistical Machine Translation, pages 128-135.
George Foster, Cyril Goutte and Roland Kuhn. 2010.
Discriminative Instance Weighting for Domain Adap-
tation in Statistical Machine Translation. In Proc. of
EMNLP 2010, pages 451-459.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang and Ignacio Thay-
er. 2006. Scalable Inference and Training of Context-
Rich Syntactic Translation Models. In Proc. of ACL
2006, pages 961-968.
Zhengxian Gong and Guodong Zhou. 2010. Improve
SMT with Source-side Topic-Document Distributions.
In Proc. of MT SUMMIT 2010, pages 24-28.
Amit Gruber, Michal Rosen-Zvi and Yair Weiss. 2007.
Hidden Topic Markov Models. In Journal of Machine
Learning Research, pages 163-170.
Sas?a Hasan, Juri Ganitkevitch, Hermann Ney and Jesu?s
Andre?s-Ferrer 2008. Triplet Lexicon Models for S-
tatistical Machine Translation. In Proc. of EMNLP
2008, pages 372-381.
Zhongjun He, Qun Liu and Shouxun Lin. 2008. Improv-
ing Statistical Machine Translation using Lexicalized
Rule Selection. In Proc. of COLING 2008, pages 321-
328.
Almut Silja Hildebrand. 2005. Adaptation of the Trans-
lation Model for Statistical Machine Translation based
on Information Retrieval. In Proc. of EAMT 2005,
pages 133-142.
Thomas Hofmann. 1999. Probabilistic Latent Semantic
Indexing. In Proc. of SIGIR 1999, pages 50-57.
Franz Joseph Och and Hermann Ney. 2003. A Systemat-
ic Comparison of Various Statistical Alignment Mod-
els. Computational Linguistics, pages 19-51.
Franz Joseph Och and Hermann Ney. 2004. The Align-
ment Template Approach to Statistical Machine Trans-
lation. Computational Linguistics, pages 417-449.
467
Philipp Koehn, Franz Josef Och and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proc. of HLT-
NAACL 2003, pages 127-133.
Philipp Koehn. 2004. Statistical Significance Tests for
Machine Translation Evaluation. In Proc. of EMNLP
2004, pages 388-395.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proc. of
ACL 2007, Demonstration Session, pages 177-180.
Yang Liu, Qun Liu and Shouxun Lin. 2006. Tree-
to-String Alignment Template for Statistical Machine
Translation. In Proc. of ACL 2006, pages 609-616.
Yajuan Lv, Jin Huang and Qun Liu. 2007. Improv-
ing Statistical Machine Translation Performance by
Training Data Selection and Optimization. In Proc.
of EMNLP 2007, pages 343-350.
Arne Mauser, Richard Zens and Evgeny Matusov, Sas?a
Hasan and Hermann Ney. 2006. The RWTH Statisti-
cal Machine Translation System for the IWSLT 2006
Evaluation. In Proc. of International Workshop on
Spoken Language Translation, pages 103-110.
Arne Mauser, Sas?a Hasan and Hermann Ney 2009. Ex-
tending Statistical Machine Translation with Discrimi-
native and Trigger-Based Lexicon Models. In Proc. of
ACL 2009, pages 210-218.
Spyros Matsoukas, Antti-Veikko I. Rosti and Bing Zhang
2009. Discriminative Corpus Weight Estimation for
Machine Translation. In Proc. of EMNLP 2009, pages
708-717.
Nick Ruiz and Marcello Federico. 2011. Topic Adapta-
tion for Lecture Translation through Bilingual Latent
Semantic Models. In Proc. of ACL Workshop 2011,
pages 294-302.
Kishore Papineni, Salim Roukos, Todd Ward and WeiJing
Zhu. 2002. BLEU: A Method for Automatic Evalu-
ation of Machine Translation. In Proc. of ACL 2002,
pages 311-318.
Jonathan Schler, Moshe Koppel, Shlomo Argamon and
James Pennebaker. 2006. Effects of Age and Gender
on Blogging. In Proc. of 2006 AAAI Spring Sympo-
sium on Computational Approaches for Analyzing We-
blogs.
Holger Schwenk and Jean Senellart. 2009. Translation
Model Adaptation for an Arabic/french News Transla-
tion System by Lightly-supervised Training. In Proc.
of MT Summit XII.
Andreas Stolcke. 2002. Srilm - An Extensible Language
Modeling Toolkit. In Proc. of ICSLP 2002, pages 901-
904.
Yik-Cheung Tam, Ian R. Lane and Tanja Schultz. 2007.
Bilingual LSA-based adaptation for statistical machine
translation. Machine Translation, pages 187-207.
Nicola Ueffing, Gholamreza Haffari and Anoop Sarkar.
2008. Semi-supervised Model Adaptation for Statisti-
cal Machine Translation. Machine Translation, pages
77-94.
Hua Wu, Haifeng Wang and Chengqing Zong. 2008. Do-
main Adaptation for Statistical Machine Translation
with Domain Dictionary and Monolingual Corpora. In
Proc. of COLING 2008, pages 993-1000.
Richard Zens and Hermann Ney. 2004. Improvments in
phrase-based statistical machine translation. In Proc.
of NAACL 2004, pages 257-264.
Ying Zhang, Almut Silja Hildebrand and Stephan Vogel.
2006. Distributed Language Modeling for N-best List
Re-ranking. In Proc. of EMNLP 2006, pages 216-223.
Bing Zhao, Matthias Eck and Stephan Vogel. 2004.
Language Model Adaptation for Statistical Machine
Translation with Structured Query Models. In Proc.
of COLING 2004, pages 411-417.
Bing Zhao and Eric P. Xing. 2006. BiTAM: Bilingual
Topic AdMixture Models for Word Alignment. In
Proc. of ACL/COLING 2006, pages 969-976.
Bing Zhao and Eric P. Xing. 2007. HM-BiTAM: Bilin-
gual Topic Exploration, Word Alignment, and Trans-
lation. In Proc. of NIPS 2007, pages 1-8.
Qun Liu, Zhongjun He, Yang Liu and Shouxun Lin.
2008. Maximum Entropy based Rule Selection Model
for Syntax-based Statistical Machine Translation. In
Proc. of EMNLP 2008, pages 89-97.
468
Chinese Personal Name Disambiguation: 
 Technical Report of Natural Language Processing Lab of 
Xiamen University 
Xiang Zhu, Xiaodong Shi, Ningfeng Liu, YingMei Guo, Yidong Chen 
Natural Language Processing Lab, Department of Cognitive Science, Xiamen 
University , Xiamen 361005 
zhuxiang_sm@163.com,mandel@xmu.edu.cn,nffliu@gmail.com 
Abstract 
This report presents the work of our 
group in the Chinese personal name 
disambiguation workshop. We propose a 
system which uses a HAC algorithm to 
cluster the mentions referring to the same 
person with features extracted from the 
documents.  
1 Introduction 
Personal name disambiguation is actually a 
task to group those documents according to 
whether the given personal name appearing 
in that document refers to the same person in 
reality. It becomes an active research topic 
recently, and the evaluation campaign for the 
English personal name has been held twice. 
Chinese personal name disambiguation is 
thought to be more challenging due to the 
need for word segmentation which could 
bring errors into the subsequent processes. 
The most widely used method for personal 
name disambiguation is unsupervised 
clustering, we adopt a Hierarchical 
Agglomerative Clustering algorithm in our 
system, which is also the most popular 
clustering algorithm used by the teams of the 
second Web People Search Evaluation 
campaign. 
The remaining part of this report is 
organized as follows. Section 2 introduces 
the document preprocessing. Section 3 
describes the feature used for the clustering 
and the section 4 addresses the clustering 
algorithm. The workshop requests two 
different tests, a formal test, and a diagnosis 
test, we will discuss the difference between 
them with our system?s result in section 5. 
2 Document preprocessing  
Different from the document preprocessing 
for English, we have to use a segmentation 
tool and a part-of-speech tagger to do some 
preprocessing work. For example, without 
the segmenting process, the documents only 
contain the string ???????? could be 
clustered when the query name is ????, 
and we need the part-of-speech tagger to 
detect whether the Chinese word ??? ? 
stands for a personal name or a toponym. 
Our experiments prove that the system is 
sensitive to the tools? performance, the 
different result between the formal test and 
the diagnosis test also proves this.  
These tools are trained with the 90% data 
of The People's Daily published in 
Jan.1998,and tested with the others 10% data. 
The performances of the tools are? 
 Precision Recall F 
seg 97.746% 97.793% 97.769% 
tag 94.197% 94.242% 94.219% 
Table 1: the performances of the tools 
3 Extracted features 
As mentioned previously, our goal is to 
group those documents. In our approach, 
each document is represented by a vector of 
features extracted from it automatically. We 
use three kinds of token-based features:  
   1) Nouns occur around the ambiguous 
personal name. 
This kind of features is selected based on 
the idea that words around the ambiguous 
personal name are more relevant to it, and 
Nouns can provide a more diagnostic 
description of the person. We use a window 
to select the nouns. 
   2) Personal names (except the ambiguous 
personal name) and toponyms occur in the 
document. 
It is intuitive that the identical person 
often associated with the same personal 
names and toponyms. 
   3)  Words with high TFIDF value. In our 
final system, we use the ten words with 
highest TFIDF value. 
This kind of features can reflect the theme 
of an article, an identical person often be 
mentioned in articles with the same theme. 
Using these features simultaneously can 
alleviate the problem caused by spare data. 
The following table presents a quantitative 
analysis: 
used features highest F score on 
dev data 
Feature 1 83.76 
Feature 1,2 86.18 
Feature 1,3 84.83 
Feature 1,2,3 86.76 
Table 2:analysis of features 
These results are obtained by using a 
initial version of the preprocessing toools, 
and when we improve the performances of 
the tools, the highest F score increases from 
86.76 to 89.61 . 
The similarity between documents was 
measured with the cosine of feature vectors. 
When computing the similarity between two 
documents, we proposed a weighting method 
for the features as:  
If the token occurs in the document, the 
weight will be 1, else 0. 
We have tried another method that count 
the tokens? weight by its frequency, but 
experiments prove it is a less effective one, 
we can interpret it by this example: 
If a word "??" which refers to a person 
named " ? ? " appears twice in one 
document  while three times in another 
document, these two documents are very 
likely referring to the same person, but the 
latter weighting method decreases the 
similarity between them.   
4 Clustering algorithm 
A Hierarchical Agglomerative Clustering 
algorithm is adopted in our system, which 
determines the number of the cluster by a 
fixed similarity threshold learned from the 
train data. At each stage of the clustering, the 
two most similar clusters are merged into a 
new one, and other clusters? similarities with 
the new cluster is the larger one of their 
similarities with the two old clusters. 
We have tried some other clustering 
methods such as modified k-means 
clustering with the same features, but the 
performances are worse. 
A pre-judgment stage before clustering is 
useful in the experiment, which can be done 
as follow: 
If two documents have the same triple 
tokens ?token1 token2 PersonName? (token2 
is tagged as noun), then they are classified to 
one cluster. 
5 Result and discussion 
The difference between the formal test and 
the diagnosis test is that the ambiguous 
personal name in each document have been 
told in the latter, but you have to find it in the 
former by yourself. The method we adopt to 
detect the ambiguous personal name in the 
formal test is to find the token which is 
tagged as personal name while contains the 
query name. 
   Our system?s performances are:  
B-Cubed precision Recall F 
Formal 
test 
90.55 84.88 85.72 
Diagnosis 
test 
89.84 89.84 89.08 
Table 3: the performances in the B-Cubed 
criterion 
P-IP P IP F 
Formal 
test 
93.3 89.22 89.9 
Diagnosis 
test 
92.77 93.33 92.71 
Table 4: the performances in the P-IP 
criterion 
From the results we can know that 
Chinese personal name disambiguation can 
be affected by the segmentation tool and the 
part-of-speech tagger. 
References 
Y. Chen, S. Y. M. Lee, and C.-R. Huang. 
Polyuhk: A robust information extraction 
system for web personal names. In 2nd Web 
People Search Evaluation Workshop (WePS 
2009), 18th WWW Conference, 2009.  
J. Gong and D. Oard. Determine the entity 
number in hierarchical clustering for web 
personal name disambiguation. In 2nd Web 
People Search Evaluation Workshop (WePS 
2009), 18th WWW Conference, 2009. 
P. Kalmar and D. Freitag. Features for web 
person disambiguation. In 2nd Web People 
Search Evaluation Workshop (WePS 2009), 
18th WWW Conference, 2009. 
1 
 
Chinese Word Sense Induction based on Hierarchical Clustering 
Algorithm 
Ke Cai, Xiaodong Shi, Yidong Chen?Zhehuang Huang, Yan Gao                    
           Cognitive Science Department, Xiamen University, Xiamen, 361005, China 
 
Abstract 
Sense induction seeks to automatically identify word senses of polysemous words 
encountered in a corpus. Unsupervised word sense induction can be viewed as a clustering 
problem. In this paper, we used the Hierarchical Clustering Algorithm as the classifier for 
word sense induction. Experiments show the system can achieve 72% F-score about 
train-corpus and 65% F-score about test-corpus. 
1. Introduction 
Word sense induction is a central problem in many natural language processing tasks such 
as information extraction, information retrieval, and machine translation [Vickrey et al, 
2005]. 
   Clp 2010 launches totally 4 tasks for evaluation exercise, these are: Chinese word 
segmentation, Chinese parsing, Chinese Personal Name disambiguation and Chinese Word 
Sense Induction. We participated in task 4, which is Chinese Word Sense Induction..  
Because the contents surround an ambiguous word is related to its meaning, we solve the 
sense problem by grouping the instances of the target word into the supposed number of 
clusters according to the similarity of contexts of the instance. In this paper we used the 
hierarchical clustering algorithm to accomplish the problem. 
   The task can be defined as two stage process: Feature selection and word clustering. 
Researchers have proposed much approach to the sense induction task which involved the use 
of basic word co-occurrence features and application of classical clustering algorithms.  
   Because the meanings of unknown words can be inferred from the contexts in which they 
appear, Pantel and Lin (2002) map the senses to WordNet. More recently, the mapping has 
been used to test the system on publicly available benchmarks (Purandare and Pedersen, 2004; 
Niu et al, 2005). 
However, this approach does not generalize to multiple-sense words. Each sense of a 
polysemous word can appear in a different context, there have been many attempts in recent 
years to apply classical clustering algorithms to this problem. 
  Clustering algorithms have been employed ranging from k-means (Purandare and Pedersen, 
2004), to agglomerative clustering (Sch?utze, 1998), and the Information Bottleneck (Niu et 
al., 2007). Senses are induced by identifying highly dense subgraphs (hubs) in the 
co-occurrence graph (V?eronis, 2004).The sIB algorithm was used to estimate cluster 
structure, which measures the similarity of contexts of instances according to the similarity of 
their feature conditional distribution(Slonim, et al,2002). Each algorithm treats words as 
feature vectors, using the same similarity function based on context information. 
The remainder of this paper is organized as follows. In section 2 the Featured set and 
word similarity definition is introduced. The hierarchical clustering algorithm is presented in 
section 3. Section 4 provides the experimental results and conclusion is drawn in section 5. 
2 
 
 
2. Feature Selection and Word Similarity Definition  
 
  2.1 Feature Selection 
 
 A feature set is used designed to capture both immediate local context in our 
experiment, wider context and syntactic context. Specifically, we experimented with several 
feature categories: ?5-word window (5w), ?3-word window (3w), part-of speech n-grams and 
dependency relations. These features have been widely adopted in various word sense 
induction algorithms. The overall best scores are achieved with local (5 words) context 
windows. 
 
. 2.2 Similarity Definition 
 
We treat the context words as feature vectors, using the same similarity function. 
Suppose 
1 2( , )i i i inC w w w?  is the contexts set of sentence iS , and 1 2( , )j j j jnC w w w?  is the 
contexts set of sentence
jS .  
Then we defined ( , ) ( , )
jk i
jl j
i j kl ik jl
W C
W C
sim S S w sim w w
?
?
? ? , here klw  is variable weight, 
Where
( , )
( , )ik jl ik jl
sim w w
dis w w
?
?
?
?
, ? is an adjustable parameter with a value of 1.2, and 
( , )ik jlDis w w  is the path length between ikw  and jlw  based on the semantic tree structure 
used for TongYiCi CiLin (?????). 
     
3. The Hierarchical Clustering Algorithm Used In Word Sense Induction 
 
Sense induction is viewed as an unsupervised clustering problem where to group a 
word?s contexts into different classes, each representing a word sense. In this paper, we use 
the bottom-up clumping approach, which begin with n singleton clusters and successively 
merge clusters to produce the other ones.  
Table1: Hierarchical Clustering Algorithm: 
1. initialize number of senses n  ? number of clusters m  
and clusters 1 2( , ), 1,2i i iC w w i m?? ?  
2. Set k n?  
3. Set  1k k? ?   
3 
 
4. Find the nearest clusters iC and jC  , Merge iC and jC  
5. If  k m?  ,  go to step 3, otherwise go to step 6; 
6.  return m  clusters  
 
The merging of the two clusters in step 4 simply corresponds to adding an edge between 
the nearest pair of nodes in iC and jC .To find the nearest clusters, the following clustering 
similarity function is used: 
( , ) ( , )
jk i
jl j
i j kl ik jl
W C
W C
sim S S w sim w w
?
?
? ?
. 
Our model incorporates features based on lexical information and parts of speech.             
So we propose a improved hierarchical clustering algorithm based on parts of speech. 
Table2: improved algorithm based on parts of speech. 
1.  initialize number of senses n  ? number of clusters m  
and clusters 1 2( , ), 1,2i i iC w w i m?? ?  
2.  Part of Speech Tagging on the corpus  
3. Divided n  senses into nn  classes base on the information of parts of 
speech. 
4. If nn m? , return m  clusters 
5. If nn m?  , invoke hierarchical clustering algorithm in different 
classes?merge clusters into m cluster. 
6.if nn m?  , invoke hierarchical clustering algorithm in different 
tagging, merge clusters into m  cluster. 
7. return m  clusters  
 
4. Experimental Results 
 
The test data includes totally 100 ambiguous Chinese words,  every word have 50 
4 
 
untagged instances. Table3 show the best/worst/average F-Score of our system about 
train-corpus and test-corpus. 
   
 Best word Worst word  All  words 
Train-corpus 0.98  0.5 0.73 
Test-corpus ------ ----- 0.65 
                    Table 3 Model performance with deferent corpus 
  Table 4 shows the performance of our model about train-corpus when using 3w and 5w 
word windows, which represent more immediate, local context. 
 
 Best word Worst word  All  words 
3w(?3-word 
window) 
0.98  0.5 0.73 
5w(?5-word 
window) 
0.92 0.52 0.72 
                    Table 4 Model performance with deferent windows  
 Table 5 summarizes the F-score in our system about train-corpus when using deferent 
similarity definition.  
 Best word Worst word  All  words 
This article 0.98  0.5 0.73 
Qun LIU  0.99 0.59 0.78 
                    Table 5 Model performance with deferent similarity definition  
  
Experimental results show that the Hierarchical Clustering Algorithm can be applied to 
sense induction. Considering words to be feature vectors and applying clustering algorithm 
can improve accuracy of the task. A significant gap still exists between the results of these 
techniques and the gold standard of manually compiled word sense dictionaries. 
 
5. Conclusions 
 
Sense induction is treated as an unsupervised clustering problem. In this paper we adopt 
hierarchical clustering algorithm to accomplish the problem. Generate context words 
according to this distribution of key words and formalize the induction problem in a 
generative mode. Experiments show the system can achieved 72% F-score about train-corpus 
and 65% F-score about test-corpus. The basic cluster algorithm can sorts the word sense into 
clusters corresponding to the context. 
 
References 
 
Boyd-Graber, Jordan, David Blei, and Xiaojin Zhu. 2007.A topic model for word sense 
disambiguation. In Proceedings of the EMNLP-CoNLL. Prague, Czech Republic,pages 
1024?1033. 
David Vickrey, Luke Biewald, Marc Teyssler, and Daphne Koller. Word-sense 
disambiguation for machine translation. In Proceedings of the conference on Human 
Language Technology and Empirical Methods in Natural Language Processing, page 
5 
 
771-778, 2005. 
Qun LIU , Sujian LI. Word Similarity Computing Based on How-net. Computational 
Linguistics and Chinese Language Processing 
Niu, Zheng-Yu, Dong-Hong Ji, and Chew-Lim Tan. 2007. I2r: Three systems for word 
sense discrimination, chineseword sense disambiguation, and english word sense 
disambiguation. In Proceedings of the Fourth International Workshop on Semantic 
Evaluations (SemEval-2007). Association for Computational Linguistics, Prague, Czech 
Republic, pages 177?182. 
Niu, Z.Y., Ji, D.H., & Tan, C.L. 2005. Word Sense Disambiguation Using Label 
Propagation Based Semi-Supervised Learning. Proceedings of the 43rd Annual Meeting of 
the Association for Computational Linguistics. 
Pantel, Patrick and Dekang Lin. 2002. Discovering word senses from text. In Proceedings 
of the 8th KDD. New York, NY, pages 613?619. 
Pedersen, Ted. 2007. Umnd2 : Senseclusters applied to the sense induction task of 
senseval-4. In Proceedings of SemEval-2007. Prague, Czech Republic, pages 394?397. 
Purandare, Amruta and Ted Pedersen. 2004. Word sense discrimination by clustering 
contexts in vector and similarity spaces. In Proceedings of the CoNLL. Boston, MA, pages 
41?48 
V?eronis, Jean. 2004. Hyperlex: lexical cartography for information retrieval. Computer 
Speech & Language. 18(3):223?252. 
 
Proceedings of the 2012 Student Research Workshop, pages 67?72,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Towards Automatic Construction of Knowledge Bases from Chinese Online
Resources
Liwei Chen, Yansong Feng, Yidong Chen, Lei Zou, Dongyan Zhao
Institute of Computer Science and Technology
Peking University
Beijing, China
{clwclw88,fengyansong,chenyidong,zoulei,zhaodongyan}@pku.edu.cn
Abstract
Automatically constructing knowledge bases
from online resources has become a crucial
task in many research areas. Most existing
knowledge bases are built from English re-
sources, while few efforts have been made for
other languages. Building knowledge bases
for Chinese is of great importance on its own
right. However, simply adapting existing tool-
s from English to Chinese yields inferior re-
sults.In this paper, we propose to create Chi-
nese knowledge bases from online resources
with less human involvement.This project will
be formulated in a self-supervised framework
which requires little manual work to extrac-
t knowledge facts from online encyclopedia
resources in a probabilistic view.In addition,
this framework will be able to update the con-
structed knowledge base with knowledge facts
extracted from up-to-date newswire.Currently,
we have obtained encouraging results in our
pilot experiments that extracting knowledge
facts from infoboxes can achieve a high accu-
racy of around 95%, which will be then used
as training data for the extraction of plain web-
pages.
1 Introduction
As the development of world wide web (WWW),
the volume of web data is growing exponentially
in recent years. Most of the data are unstructured,
while a few are manually structured and only a s-
mall part of them are machine-readable. How to
make these data accessible and useable for end user-
s has become a key topic in many research areas,
such as information retrieval, natural language pro-
cessing, semantic web(Tim et al, 2001) and so on.
Among others, constructing knowledge bases (KB)
from web data has been considered as a preliminary
step. However, it is not trivial to extract knowledge
facts from unstructured web data, especially in open
domain, and the accuracy is usually not satisfacto-
ry. On the other hand, with the development of We-
b2.0, there are increasing volume of online encyclo-
pedias which are collectively created by active vol-
unteers, e.g., Wikipedia1. Surprisingly, experiment
evidences show that the confidence of Wikipedia is
even comparable with that of British Encyclopedi-
a (Giles, 2005). Therefore, many efforts have been
made to distill knowledge facts from Wikipedia or
similar resources and further build KBs, for example
YAGO(Suchanek et al, 2007), DBpedia(Bizer et al,
2009) and KOG(Wu and Weld, 2008).
In the literature, most KBs constructed recently
are in English as it takes up an overwhelming major-
ity on the web, while other major languages receives
less attention, for example, Chinese features similar
amounts of web pages with English yet is less fre-
quently studied with regarding to building KBs. Al-
though continuous works have been made to process
English resources, building Chinese KBs is of great
value on its own. To the best of our knowledge, few
efforts have been made to construct a KB in Chi-
nese until now. Despite of necessary special pre-
processings, e.g., word segmentation, for Chinese,
building a Chinese KB from web data is quite differ-
ent from building English ones, since we have lim-
ited resources available in Chinese that are of lower
1http://www.wikipedia.com
67
quality compared to their English counterparts. This
brings more difficulties than that of English. As a
result, the approaches used in English may not work
well in Chinese.
In this paper, we propose a new framework to
build a KB in Chinese from online resources with-
out much human involvement. Since the Chinese
portion of Wikipedia is much smaller than its En-
glish part, we harvest knowledge facts from a Chi-
nese online encyclopedia, HudongBaike2. Hudong-
Baike is the largest Chinese online encyclopedia and
features similar managing rules and writing styles
with Wikipedia. We first obtain knowledge facts by
parsing the infoboxes of HudongBaike. Then we use
these triples as seeds, and adopt the idea of distant
supervision(Mintz et al, 2009; Riedel et al, 2010;
Yao et al, 2010) to extract more facts from other
HudongBaike articles and build a KB accordingly.
Moreover, to make the knowledge base more up-to-
date, we also propose to propagate the KBwith news
events.
The rest of this paper is organized as follows: we
first introduce the related work, and briefly introduce
two online encyclopedias. In Section 4 we describe
our framework in detail. Our current work are dis-
cussed in Section 5. In Section 6 we conclude this
paper.
2 Related Work
KB construction is an important task and has at-
tracted many research efforts from artificial intelli-
gence, information retrieval, natural language pro-
cessing, and so on. Traditional KBs are most-
ly manually created, including WordNet(Stark and
Riesenfeld, 1998), Cyc or OpenCyc(Matuszek et al,
2006), SUMO(Niles and Pease, 2001), and also
some domain-specific ontologies such as GeneOn-
tology3. These KBs achieve a high accuracy since
they are manually built or filtered by domain ex-
perts. However, manually creating KB is a time-
consuming and labor-intensive work, and continu-
ous annotation is required to keep the KB up-to date.
Most of them thus suffers from the coverage issue in
practice.
In recent years, many researchers turn to auto-
2http://www.hudong.com
3http://www.geneontology.org
matically extract knowledge to construct KBs. One
kind of methods extract knowledge facts from gener-
al text corpus. These approaches, such as TextRun-
ner(Banko et al, 2007) and KnowItAll(Etzioni et al,
2004), use rule based information extraction tech-
nologies to extract relations between entity pairs.
Recently, TextRunner is expanded by a life long
learning strategy, which can acquire new facts. An-
other type of approaches aims to automatically de-
rive facts from online encyclopedias. Collectively
created by many volunteers, online encyclopedias
are more reliable than general web pages. They al-
so contain semi-structured knowledge such as hand-
crafted infoboxes. Therefore, the accuracy of the
facts extracted will be higher. Researchers utilize
these semi-structured data resources for knowledge
extraction, for example, YAGO extract facts from in-
foboxes and category names of Wikipedia, and use
WordNet as its taxonomy(Suchanek et al, 2007).
A similar approach is adopted by DBpedia, which
also extract knowledge facts from infoboxes(Bizer
et al, 2009). Unlike YAGO and DBpedia, Kylin us-
es the infoboxes and the Wikipedia pages containing
these infoboxes to build a training set, and use ma-
chine learning methods to extract facts from plain
Wikipedia articles(Wu and Weld, 2007). Although
Kylin achieves a high precision, it is corpus-specific,
which means it can only be used in Wikipedia-like
corpora. It is noticed that all the above works fo-
cus on building an English KB, and few efforts have
been made in building a Chinese one until now.
3 Online Encyclopedia
Wikipedia is known as an accurate online encyclo-
pedia whose accuracy is comparable with Encyclo-
pedia Britannica(Giles, 2005). It?s created by thou-
sands of volunteers around the whole world. Until
now, the English version ofWikipedia has 3,878,200
content pages, making it the largest English on-
line encyclopedia. The Chinese version contains
402,781 content pages, which is much smaller than
the English version.
HudongBaike is the largest Chinese online ency-
clopedia with over 5 million content pages. Similar-
ly with Wikipedia, HudongBaike is also created by
volunteers, and relies on the community to ensure
its quality. Many HudongBaike pages also contains
68
Preprocessed HudongBaike Pages
Extracted Triples HudongBaike Articles
Triples Extracted from Articles
Knowledge Base
 Up-to-Date Data
Semantic Elements 
Propagated KB
Analyzing Infoboxes Cleaning pages
Mapping
Distant supervision
KB construction Semantic Elements Extraction
Propagating KB
Figure 2: The framework of Our project
a hand-crafted summary box, infobox. An infobox
summarizes the knowledge of the corresponding en-
tity. The information in the infobox is reliable since
these are collaboratively crafted by many volunteer-
s. Figure 1 is an example page with an infobox from
HudongBaike, introducing a US general ????
?? (George Marshall).
4 The Framework
In this paper, we formulated the KB construction
task in a semi-supervised learning fashion which re-
quires little manual annotation and supports knowl-
edge propagation by up-to-date feeds. Because
the Chinese part of Wikipedia is relatively smal-
l and may suffer from the coverage problem, we use
HudongBaike to build our KB in this project. In fu-
ture we may merge the Wikipedia part into our KB.
After necessary preprocessings including word seg-
mentation and named entity extraction, we are able
to apply our framework shown in Figure 2.
In general, our framework contains the follow-
ing steps: (1)Extracting knowledge from online
encyclopedia; (2)Linking triples and building KB;
(3)Propagating KB with up-to-date data.
4.1 Entity Relation Extraction
Compared to other resources on the Web, online
encyclopedias contain less noises and feature more
regular structures, thus are considered easier for us
to extract knowledge facts.
Analyzing Infoboxes As mentioned before, many
HudongBaike pages contains an infobox, which
has high accuracy and can be used directly for
relation extraction. We can conveniently parse
these infoboxes into < S,P,O > triples. For
example, from the first entry of this infobox,
we can derive the following triple: < ?
????? , ??? , ???? >(<
GeorgeMarshall, BirthP lace, Uniontown >).
The precision of the extraction is over 95%, and
these triples can form a valuable knowledge source.
Extracting relations with Distant Supervision
Extracting knowledge from infoboxes is efficien-
t and can achieve a high precision. However, many
web pages in HudongBaike do not have infoboxes.
There is much richer knowledge in the main arti-
cles of HudongBaike, which we should also take in-
to consideration.
Extracting knowledge from unstructured articles
is a challenging task. Traditionally, researchers
use manually created templates to extract relation-
s. These templates need lots of human efforts and
are domain-specific. Recent methods trend to re-
ly on machine learning models, which need a large
amount of labeled data. One idea is to utilize the
infoboxes to form the training data set, and train an
extractor to extract relations from the pages with-
out an infobox(Wu and Weld, 2007). However, the
relations extracted from a page are restricted to the
infobox template used by the current page catego-
ry, and their subject must be the entity that this page
describes. For example, when we extract relation-
s from the page of ????? (Charles Yeager,
Ace of US in WWII) which does not contain an in-
fobox, the subject of these relations must be Charles
Yeager, and we can only extract the relation types
listed in infobox template for a military person. As
a result, this method can only be used in online en-
cyclopedias in a Wikipedia style, and the recall will
be relatively low.
Distant supervision is widely used in relation ex-
traction in recent years. It hardly need any manual
work, and can overcome the above problems. It can
be used in any reliable corpus, and doesn?t have the
strict restrictions as previous methods. We adopt its
idea in our framework. The basic assumption of dis-
tant supervision is the sentences containing two en-
69
Figure 1: A HudongBaike page about a US general George Marshall
tities should express the relation between them more
or less. It only needs a reliable seed KB (in the form
of relation triples) and a corpus. Here, we can use
the knowledge facts extracted from infoboxes previ-
ously as the seed KB, and the articles of Hudong-
Baike as text corpus. For each triple in the seed K-
B, we generate positive training data by finding sen-
tences containing both its subject and object in the
corpus. For example, we can map the first entry in
Figure 1 to the sentence 1880?12?31?????
??????? (On December 31th, 1880, Mar-
shall was born in Uniontown). The negative training
data can be generated by randomly select some sen-
tences which contain neither of the subject and the
object. A predictive model such as logistic regres-
sion model is trained with the training data. We can
use the model to give predictions for the relations
in a textual knowledge source. For a HudongBaike
page, we should decide the entity pairs we are in-
terested in. A simple strategy is to select all entity
pairs. But it will be time-consuming, and may suffer
from weak-related entity pairs. So we extract top-
ic entities which have high tfidf weights from this
page, and generate entity pairs under the restriction
that they must contain at least one topic entity. For
each entity pair, we find the sentences which contain
both the subject and object and use the predictive
model to give the possible relations between them
and the confidence of the relations.
However, the predictions of distant supervision
is less accurate than those of supervised method-
s. So we should adopt some heuristics to filter the
relations extracted. An easy strategy is to set up a
threshold for relation confidences to avoid uncertain
relations and improve the precision. We adopt this
method in our project. Furthermore, we can also use
the strategies of Riedel et al (2010) or Yao et al
(2010).
4.2 Knowledge Base Construction
After the relation extraction, we must link the ex-
tracted knowledge triples in order to construct the
knowledge base. In our scenario this linking task can
be formulated as: given a base KB, a bunch of newly
extracted knowledge triples with the sentences de-
scribing them and their contexts, the task of entity
linking aims to link each of the entity mentions in
the plain texts (these sentences mentioned above) to
its corresponding entity in the base KB. At the very
beginning, we initiate a base KB by using the taxon-
omy of HudongBaike thus are able to map relations
between entities into the KB through entity linking.
In online encyclopedias, the synonyms of an en-
tity are represented by redirect links. Synonyms are
important in entity linking because they provide al-
ternative names for entities, and we may miss some
mappings without them. For example, we have an
entity ?????? (United States of America)
in the KB, and an mention ?? (USA) in a piece
of text. Redirect links can tell us that we can create
a mapping between them. Basically, for each men-
tion, we can find matching candidates for them in a
KB through exact matching. However, if we can-
not find an exact match for a mention, we will try
70
fuzzy matching since a mention may not match ex-
actly with its referent entity in KB.
Now we need to solve the entity linking task. Tra-
ditional methods did not exploit global interdepen-
dence between entity linking decisions. We thus
adopt the collective entity linking approach of Han
et al (2011) to solve this problem. This method cap-
tures the rich semantic relatedness knowledge be-
tween entities, and take the interdependence of link-
ing decisions into consideration. They construct a
graph by linking name mentions and candidate enti-
ties in pairwise using the semantic relatedness be-
tween them. Then they use a random walk algo-
rithm on the graph to solve the problem. However,
they did not take the NIL problem into considera-
tion. That is, in entity linking, if the referent enti-
ty of an name mention is not in our KB, it should
be linked to a pseudo entity NIL. In our case, we
should abandon the mapping of the current triple by
deciding whether this entity has been listed in the
KB(Zheng et al, 2010).
4.3 Knowledge base Propagation
Although we can extract millions of relations and
built a KB in previous subsections, it has the same
shortage as most existing KBs: the knowledge ex-
tracted are mostly statical attributes of entities (such
as birthdate or occupation of a person) and can not
describe the latest updates of an entity (such as a
politician is currently visiting a country).
In order to settle this problem, we use the dy-
namical knowledge extracted from up-to-date data
to expand our KB. One possible solution is extract-
ing semantic event elements from online news. In
this project, we will synchronies our KB with a Chi-
nese newspaper, RenMinRiBao (People?s Daily).
5 Current Work
Currently, we have extracted triples from the in-
foboxes of HudongBaike and built the base KB.
Manual evaluation shows that the precision of struc-
tured content extraction is over 95%. Most errors
are caused by the web page?s own mistakes or edit-
ing errors in infoboxes.
To assess the quality of HudongBaike data, in our
preliminary experiments(Yidong et al, 2012), we
extract relation facts from plain HudongBaike arti-
cles without infoboxes in a way similar to Kylin. We
focus on three categories, including ?? (Nation),
?? (Person) and ?? (Actor or Actress). In each
category we select several representative attributes
from its infobox template. We manually annotated
more than 200 testing examples for evaluation: 100
in Person, 33 in Nation and 91 in Actor or Actress.
The results shows that the HudongBaike data can be
used to extract knowledge facts with a high precision
in all three categories: in ?? the average precision
is 79.43%, in ?? it is 78.9%, and in ?? it even
goes up to 90.8%.
Distant Supervision We further adopt the ap-
proach of distant supervision(Mintz et al, 2009) in
a Chinese dataset. We generate a dataset from Ren-
MinRiBao with 10000 sentences, and each sentence
contains at least a pair of entities which correspond
to a knowledge triple in HudongBaike?s infobox ex-
traction. We use 60% of the sentences as training
set and 40% as the testing set. Our experiments
show that when the recall is 10%, we can obtain a
high precision of 87%, which indicates the feasibili-
ty of our model. However, as the recall raises, the
precision drops dramatically. For example, when
the recall is 29% the precision is about 65%. This
can be remedied by adopting more encyclopedia-
specific filtering strategies and assumptions during
the distant supervision modeling.
6 Conclusions
In this project, we proposed a framework to build
KBs in Chinese. It uses the infoboxes of Hudong-
Baike as a seed knowledge base, the articles of
HudongBaike as extra textual resources, adopts the
idea of distant supervision to extract knowledge fact-
s from unstructured data and link the triples to build
a knowledge base. This framework requires lit-
tle manual work, and can be used in other reliable
knowledge resources. Our preliminary experimental
results are encouraging, showing that the Hudong-
Baike provides reasonable resources for building
knowledge bases and the distant supervision fashion
can be adapted to work well in Chinese.
For the next, we will further adapt our frame-
work into a self-training manner. By using higher
threshold for confidence in distant supervision we
can make sure the precision of extracted knowledge
71
is high enough for bootstrapping. Then we put the
extracted knowledge facts into the seed KB, and the
framework will repeat iteratively. On the other hand,
we can extract knowledge facts from other reliable
knowledge resource, such as Wikipedia, academic
literature, and merge knowledge from different re-
sources into one KB. Moreover, we can also make
our KB multilingual by adopting our framework in
other languages.
References
Banko, M., Cafarella, M. J., Soderland, S., Broad-
head, M., and Etzioni, O. (2007). Open informa-
tion extraction from the web. In Proceedings of
IJCAI, IJCAI?07, pages 2670?2676.
Bizer, C., Lehmann, J., Kobilarov, G., Auer, S.,
Becker, C., Cyganiak, R., and Hellmann, S.
(2009). Dbpedia - a crystallization point for the
web of data. Web Semant., 7:154?165.
Etzioni, O., Cafarella, M., Downey, D., Kok, S.,
Popescu, A.-M., Shaked, T., Soderland, S., Weld,
D. S., and Yates, A. (2004). Web-scale informa-
tion extraction in knowitall. In Proceedings of the
13th WWW, WWW ?04, pages 100?110.
Giles, J. (2005). Internet encyclopaedias go head to
head. Nature, 438:900?901.
Han, X., Sun, L., and Zhao, J. (2011). Collective
entity linking in web text: a graph-based method.
In SIGIR, SIGIR ?11, pages 765?774, New York,
NY, USA. ACM.
Matuszek, C., Cabral, J., Witbrock, M., and DeO-
liveira, J. (2006). An introduction to the syntax
and content of cyc. In Proceedings of the 2006
AAAI Spring Symposium.
Mintz, M., Bills, S., Snow, R., and Jurafsky, D.
(2009). Distant supervision for relation extraction
without labeled data. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the A-
CL and the 4th IJCNLP of the AFNLP: Volume 2
- Volume 2, ACL ?09, pages 1003?1011.
Niles, I. and Pease, A. (2001). Towards a standard
upper ontology. In Proceedings of FIOS - Volume
2001, pages 2?9. ACM Press, New York.
Riedel, S., Yao, L., and McCallum, A. (2010).
Modeling relations and their mentions without la-
beled text. In Machine Learning and Knowledge
Discovery in Databases, volume 6323 of Lec-
ture Notes in Computer Science, pages 148?163.
Springer Berlin / Heidelberg.
Stark, M. M. and Riesenfeld, R. F. (1998). Wordnet:
An electronic lexical database. In Proceedings of
11th Eurographics Workshop on Rendering. MIT
Press.
Suchanek, F. M., Kasneci, G., and Weikum, G.
(2007). Yago: a core of semantic knowledge.
In Proceedings of WWW, WWW ?07, pages 697?
706, New York, NY, USA. ACM.
Tim, B.-L., J., H., and O., L. (2001). The semantic
web. Scientific American.
Wu, F. and Weld, D. S. (2007). Autonomously
semantifying wikipedia. In CIKM, CIKM ?07,
pages 41?50, New York, NY, USA. ACM.
Wu, F. and Weld, D. S. (2008). Automatically re-
fining the wikipedia infobox ontology. In WWW,
WWW ?08, pages 635?644, New York, NY, USA.
ACM.
Yao, L., Riedel, S., and McCallum, A. (2010). Col-
lective cross-document relation extraction with-
out labelled data. In Proceedings of EMNLP,
EMNLP ?10, pages 1013?1023, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Yidong, C., Liwei, C., and Kun, X. (2012). Learning
chinese entity attributes from online encyclopedi-
a. In Proceedings of IEKB workshop in APWeb
2012.
Zheng, Z., Li, F., Huang, M., and Zhu, X. (2010).
Learning to link entities with knowledge base. In
HLT-NAACL 2010, pages 483?491, Stroudsburg,
PA, USA.
72
