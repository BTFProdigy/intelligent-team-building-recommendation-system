Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, pages 98?106,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Emotional Perception of Fairy Tales:
Achieving Agreement in Emotion Annotation of Text
Ekaterina P. Volkova1,2, Betty J. Mohler2, Detmar Meurers1, Dale Gerdemann1, Heinrich H. Bu?lthoff2
1 Universita?t Tu?bingen, Seminar fu?r Sprachwissenschaft
19 Wilchelmstr., Tu?bingen, 72074, Germany
2 Max Planck Institute for Biological Cybernetics
38 Spemannstr., Tu?bingen, 72076, Germany
Abstract
Emotion analysis (EA) is a rapidly developing
area in computational linguistics. An EA
system can be extremely useful in fields such
as information retrieval and emotion-driven
computer animation. For most EA systems,
the number of emotion classes is very limited
and the text units the classes are assigned
to are discrete and predefined. The question
we address in this paper is whether the set
of emotion categories can be enriched and
whether the units to which the categories
are assigned can be more flexibly defined.
We present an experiment showing how an
annotation task can be set up so that untrained
participants can perform emotion analysis
with high agreement even when not restricted
to a predetermined annotation unit and using
a rich set of emotion categories. As such it
sets the stage for the development of more
complex EA systems which are closer to the
actual human emotional perception of text.
1 Introduction
As a first step towards developing an emotion
analysis (EA) system simulating human emotional
perception of text, it is important to research the
nature of the emotion analysis performed by humans
and examine whether they can reliably perform
the task. To investigate these issues, we conducted
an experiment to find out the strategies people
use to annotate selected folk fairy tale texts for
emotions. The participants had to choose from a set
of fifteen emotion categories, a significantly larger
set than typically used in EA, and assign them to an
unrestricted range of text.
To explore whether human annotators can reliably
perform a task, inter-annotator agreement (IAA)
(Artstein and Poesio, 2008) is the relevant measure.
This measure can be calculated between every two
individual annotations in order to find pairs or even
teams of annotators whose strategies seem to be
consistent and coherent enough so that they can be
used further as the gold-standard annotation suited
to train a machine learning approach for automatic
EA analysis. A resulting EA system, capable of
simulating human emotional perception of text,
would be useful for information retrieval and many
other fields.
There are two main aspects of the resulting anno-
tations to be researched. First, how consistently can
people perceive and locate the emotional aspect of
fairy tale texts? Second, how do they express their
perception of text by means of annotation strategies?
In the next sections, we address these questions and
provide details of an experiment we conducted to
empirically advance our understanding of the issues.
2 Motivation and Aimed Application
Most existing EA systems are implemented for and
used in specific predefined areas. The application
field could be anything from extracting appraisal
expressions (Whitelaw et al, 2005) to opinion
mining of customer feedback (Lee et al, 2008).
In our case, the intended application of the EA
system predominantly is emotion enhancement of
human-computer interaction, especially in virtual
or augmented reality. Emotion enhancement of
98
computer animation, especially when it deals with
spoken or written text, is primarily done through
manual annotation of text, even if a rich database
of perceptually guided animations for behavioral
scripts compilation is available (Cunningham and
Wallraven, 2009). The resulting system of our
project is meant to be a bridge between unprocessed
input text (generated or provided) and visual and
auditory information, coming from the virtual
character, like generated speech, facial expressions
and body language. In this way a virtual character
would be able to simulate emotional perception and
production of text in story telling scenarios.
3 Related Work
Although EA is often referred to as a developing
field, the amount of work carried out during the last
decades is phenomenal. This section is not meant as
a full overview of the related research as that scope
is too great for the length of this paper. To contextu-
alize the research presented in this paper we focus on
the projects that inspired us and fostered the ideas.
The work done by Alm (Alm and Sproat, 2005;
Alm et al, 2005; Alm, 2008) is close to our
project in its sprit and goals. Alm, (2008) aims at
implementing affective text-to-speech system for
storytelling scenarios. An EA system, detecting
sentences with emotions expressed in written text
is a crucial element for achieving this goal. The
annotated corpus was composed of three sets of
children?s stories written by Beatrix Potter, H. C.
Andersen, and the Brothers Grimm.
Like Liu et al (2003), Alm (2008) uses sev-
eral emotional categories, while most research in
automatic EA works with pure polarities. The set
of emotion categories used is essentially the list of
basic emotions (Ekman, 1993), which has a justified
preference for negative emotion categories. Ek-
mann?s list of basic emotions was extended by Alm,
since the emotion of surprise is validly taken as am-
bivalent and was thus split into positive surprise and
negative surprise. The EA system described in Alm
et al (2005) is machine learning based, where the
EA problem is defined as multi-class classification
problem, with sentences as classification units.
Liu et al (2003) have combined an emotion
lexicon and handcrafted rules, which allowed them
to create affect models and thus form a representa-
tion of the emotional affinity of a sentence. Their
annotation scheme is also sentence-based. The
EA system was tested on short user-composed text
emails describing emotionally colored events.
In the research on recognizing contextual polarity
done by Wilson et al (2009) a rich prior-polarity
lexicon and dependency parsing technique were
employed to detect and analyze subjectivity on
phrasal level, taking into account all the power of
context, captured through such features as negation,
polarity modification and polarity shifters. The
work presents auspicious results of high accuracy
scores for classification between neutrality and
polarized private states and between negative and
positive subjective phrases. A detailed account
of several ML algorithms performance tests is
discussed in thought-provoking manner. This work
encouraged us to build a lexicon of subjective clues
and use sentence structure information for future
feature extraction and ML architecture training.
Another thought-provoking work by Polanyj
(2006) shows the influence of the context on subjec-
tive clues. This is relevant to our project since we
are collecting lexicons of subjective clues and the
mechanisms of contextual influence may prove to
be of value for future automatic EA system training.
Bethard et at. (2004) provide valuable informa-
tion about corpus annotation for EA means and give
accounts on the performance of various existing ML
algorithms. They provide excellent analysis of au-
tomatic extraction of opinion proposition and their
holders. For feature extraction, the authors employ
such well-known resources as WordNet (Miller et
al., 1990), PropBank (Kingsbury et al, 2002) and
FrameNet (Baker et al, 1998). Several types of
classification tasks involve evaluation on the level
of documents. For example, detecting subjective
sentences, expressions, and other opinionated items
in documents representing certain press categories
(Wiebe et al, 2004) and measuring strength of
subjective clauses (Wilson et al, 2004). All these
and many more helped us to decide upon our own
strategies, provided many examples of corpus col-
lection and annotation, feature extraction and ML
techniques usage in ways specific for the EA task.
99
4 Experimental Setup
Having established the research context, we now
turn to the questions we investigate in this paper:
the use of an enriched category set and the flexible
annotation units, and their influence on annotation
quality. We describe the experiment we conducted
and its main results. Each participant performed
several tasks for each session. The first task always
was a cognitive task on emotion categories taken
outside the fairy tales context. The results are dis-
cussed in Sections 4.1 and 4.2. The next assignment
discussed in Section 4.3 was to annotate a list of
words for their inherent polarities. The third task
was to read the text out loud to the experimenter.
This allowed the participant to feel immersed into
the story telling scenario and also get used to the
text of the story they were about to annotate for
the full set of emotion categories. The annotation
process is described in Section 4.4. The last exercise
was to read the full fairy tale text out loud again,
with the difference that this time their voice and
face were recorded by means of a microphone and a
camera. The potential importance of the extra data
sources like speech melody and facial expressions
are further discussed in Section 8 as future work.
Ten German native speakers voluntarily partic-
ipated in the experiment. The participants were
divided into two groups and each participant worked
on five of the eight texts. The fairy tale sets for each
group overlapped in two texts, which allowed us to
achieve a high number of individual annotations in a
short amount of time and compare the performance
of people working on different sets of texts (see
Table 1). Each participant annotated their texts in
five sessions, dealing with only one text per session.
The fatigue effect was avoided as no annotator had
more than one session a day.
4.1 Determining Emotion Categories
First, we needed to define the set of emotions to
be used in the experiment. Based on the current
emotion theories from comparative literature and
cognitive psychology (Ekman, 1993; Auracher,
2007; Fontaine et al, 2007), we compiled a set of
fifteen emotion categories: seven positive, seven
negative, and neutral (see Table 2). We chose an
equal number of negative and positive emotions,
User Fairy Tale ID
JG D R BR FH DS BM SJ
A1 ? ? ? ? ?
A2 ? ? ? ? ?
A3 ? ? ? ? ?
A4 ? ? ? ? ?
A5 ? ? ? ? ?
A6 ? ? ? ? ?
A7 ? ? ? ? ?
A8 ? ? ? ? ?
A9 ? ? ? ? ?
A10 ? ? ? ? ?
Table 1: Annotation Sets
Positive Negative
Entspannung (relief) Unruhe (disturbance)
Freude (joy) Trauer (sadness)
Hoffnung (hope) Verzweiflung ( despair)
Interesse (interest) Ekel (disgust)
Mitgefu?hl (compassion) Hass (hatred)
U?berraschung (surprise) Angst (fear)
Zustimmung (approval) A?rger (anger)
Table 2: Emotion Categories Used in the Experiment
since in our experiment the main focus is on the
freedom and equality of choice of emotion cate-
gories. We aimed at the set to be comprehensive and
we also expected the participants to be able to detect
each of the emotions in the text as well as express
them through speech melody and facial expressions.
The polarity of each category was determined
experimentally. Participants were asked to decide
on the underlying polarity of each emotion category
and then to evaluate each emotion on an intensity
scale [1:5], ?5? marking extreme polarization, ?1?
being close to neutral. All participants were in full
agreement concerning the underlying polarity of
the emotions in the set, while the numerical values
varied. It is important to note, that the category
U?berraschung (surprise) was stably estimated as
positive. In English the word surprise is reported
to be ambivalent (Alm and Sproat, 2005), but we
found that in German its most common translation
is clearly positive.
4.2 Emotion Categories Clustering
In the second part of the experiment we asked partic-
ipants to organize the fifteen emotions into clusters.
Each cluster was to represent a situation in which
100
Cluster Polarity
{relief, hope, joy} positive
{joy, surprise} positive
{joy, approval} positive
{approval, interest} positive
{disgust, anger, hatred} negative
{fear, despair, disturbance} negative
{fear, disturbance, sadness} negative
{sadness, compassion} mixed
Table 3: Emotion Clusters
several emotions were equally likely to co-occur,
e.g. a situation formulated by a participant as ?When
a friend gives me a nicely wrapped birthday present
and I am about to open it.? was reported to involve
such emotions as joy, interest and surprise. On
average, each participant has formed 5 clusters with
3?4 items per cluster. The clusters were encoded as
sets on unordered pairs of items. Pairs were filtered
out if they were indicated by fewer than seven par-
ticipants. As the result, the following eight clusters
were obtained (see Table 3). For most clusters, the
categories composing them share one polarity. The
{sadness, compassion} cluster is the only exception.
It is important to note that the clusters were
determined through this cognitive task, indepen-
dently of the annotations. Since the annotators
agree well on clustering the emotions, employing
this information captures conceptual agreement
between individual annotations even if the specific
emotion categories for the same stretch of text do
not coincide. However, we intend to keep the full
set of emotions for the future corpus expansions.
4.3 Word list Annotation
For each text, we compiled its word list by taking the
set of words contained in the text, normalizing each
word to its lemma and filtering the set for most com-
mon German stop words (function words, pronouns,
auxiliaries). Like full story texts, word lists were
divided into two annotation sets. At each session,
before seeing the full text of the fairy tale, the partic-
ipant was to annotate each item of the corresponding
word list for its inherent polarity. All the words were
taken out their contexts and were neutral by default.
The annotator?s task was to label only those words
that had the potential to change the polarity of the
context in which they could occur. We purposefully
German Title English Title Abbr.
Arme Junge im Grab Poor Boy in Grave JG
Bremer Stadtmusikanten Bremen Musicians BM
Dornro?schen Little Briar-Rose BR
Eselein Donkey D
Frau Holle Mother Hulda FH
Heilige Joseph im Walde St. Joseph in Forest SJ
Hund und Sperling Dog and Sparrow DS
Ra?tsel Riddle R
Table 4: Stories Used (the titles are shortened)
did not limit the task to the words occurring in all
texts in order to be able to investigate the stability
of participants? decisions. Every annotator worked
with five word lists, one for each fairy tale text. The
total number of unique items for the first annotation
set was 893 words and 823 words long for the
second set; 267 and 236 words correspondingly
occurred in more than one word list. These words
could potentially be marked with different polarity
categories, but in fact only about 15% of those
words (4% from the total number of items on each
of the word lists) were ?unstable?, namely, labeled
with different polarities by the same annotator. The
labels received in these cases were either {positive,
neutral} or {negative, neutral}. These words were
further ?stabilized? by either choosing the most
frequent label or the neutral label if the unstable
word had received only two label instances. The
results show that such annotation tasks could be
used further for subjective clues lexicon collection.
4.4 Text Annotation
For the third and main part of the experiment, we
selected eight Grimm?s fairy tales, each 1200 ? 1400
words long and written in Standard German (see
Table 4). The texts were chosen based on their
genre, for in spite of the depth of all the hidden
and open references to human psyche and national
traditions that were shown in works of (von Franz,
1996; Propp and Dundes, 1977), folk fairy tales
are relatively uncomplicated in the plot-line and
the characters? personalities. Due to this relative
simplicity of the content, we expect the participants?
emotional reactions to folk fairy tale texts to be more
coherent than to other texts of fiction literature.
The task for the participants was to locate and
mark stretches of text where an emotion was to be
101
conveyed through the speech melody and/or facial
expressions if the participant was to read the text
out loud. To make the annotation process and its
further analysis time-efficient and convenient for
both, annotators and experimenters, a simple tool
was developed. We created the Manual Emotion
Annotation Tool (MEAT) which allows the user
to annotate text for emotion by selecting stretches
of text and labeling it with one of fifteen emotion
categories. The application also has a special mode
for word list annotation, where only the three
polarity categories are available: positive, negative
and neutral. The user can always undo their labels
or change them until they are satisfied with the
annotation and can submit the results. The main
part of the experiment resulted in fifty individual
annotations which produced 150 annotation pairs.
5 Analyzing Inter-annotator Agreement
For each of the 150 pairs (two texts annotated
by ten annotators, six texts annotated by five
annotators), the IAA rate was calculated. However,
the calculation of IAA is not as straightforward
in this situation as it might seem. In many types
of corpus annotation, e.g., in POS tagging, there
are previously identified discrete elements. In this
experiment we intentionally have no predefined
units, even if this makes the IAA calculation more
difficult. Consider the following examples:
(1) A1: ?. . . [the evil wolf]X ate the girl?
A2: ?. . . the [evil wolf ate the girl]X?
(2) A1: ?. . . [the evil wolf]X ate the girl?
A2: ?. . . [the evil wolf]Y ate the girl?
(3) A1: ?. . . [the evil wolf]X ate the girl?
A2: ?. . . the evil wolf ate [the girl]X?
(4) A1: ?. . . [the evil wolf]X ate [the girl]Z?
A2: ?. . . [the evil wolf ate the girl]X?
In example (1) both annotators marked certain
stretches of text with the same category X, but the
annotations do not completely coincide, there is
only an overlap. This situation is similar to that in
syntactic annotation, where one needs to distinguish
between bracketing and labeling of the constituent
and measures such as Parseval (Carroll et al, 2002)
have been much debated.
Both annotators in example (1) recognize evil
wolf as marked for X and thus this example should
be counted towards agreement, while examples (2)
and (3) should not. A second type of evaluation
arises if the emotion clusters are taken into account.
According to this evaluation type, example (2) is
counted towards agreement if the categories X and
Y belong to the same cluster.
Example (4) provides an illustration of how IAA
is accounted for in a more complex case. Annotator
A1 has marked two stretches of text with two
different emotion categories, while annotator A2
has united both stretches under the same emotion
category. Both annotators agree that the evil wolf is
marked for X, but disagree on the emotion category
for the girl. In order to avoid the crossing brackets
problem (Carroll et al, 2002), we treat the evil
wolf ate as agreement, and the girl as disagree-
ment. Although ate was left unmarked by one of
the annotators, it is counted towards agreement
because it is next to a stretch of text on which both
annotators agree. Stretches of text the annotators
agree or disagree upon also receive weight values:
the higher the number of words that belong to open
word classes in a stretch, the higher its weight.
The general calculation formulae for the IAA
measure are taken from (Artstein and Poesio, 2008):
? =
Ao ?Ae
1?Ae
Ao =
1
i
?
i?I
argi
Ae =
1
I2
?
k?K
nc1knc2k
Ao is the observed agreement, Ae is the expected
agreement, I is the number of annotation items, K
is the set of all categories used by both annotators,
nck is the number of items assigned by annotator c
to category k.
6 Analyzing Annotation Strategies
Analysis of IAA, presented in Section 5 can answer
the first question we aim to investigate: How consis-
tently do people perceive and locate the emotional
aspect of fairy tale texts? The second issue nec-
essary for investigation is the annotation strategies
people use to express their emotional perception
of text. In our experiment conditions, the resulting
strategies can be investigated via three aspects:
a) length of user-defined flexible units b) emotional
102
0%?1%?
2%?3%?
4%?5%?
6%?7%?
8%?9%?
10%?
1? 3? 5? 7? 9? 11? 13? 15? 17? 19? 21? 23? 25? 27? 29?
Unit?
lengt
h?Fre
quen
cy?(%
)?
Unit?Length?(in?word?tokens)?
Figure 1: Annotator Defined Unit Length Rating
composition of fairy tales c) emotional flow of the
fairy tales. In this section we give a brief account of
our findings concerning the given aspects.
The participants were always free to select text
stretches of the length they considered to be appro-
priate for a specific emotional category label. The
only guideline they received was to mark the entire
stretch of text which, according to their judgement,
was marked by the chosen emotion category and,
if read without the surrounding context, would
still allow one to clearly perceive the applied
emotion category label. As Figure 1 shows, the
most frequent unit length consists of four to seven
word tokens, which corresponds to short phrases,
e.g., a verb phrase with a noun phrase argument.
We consider the findings to be encouraging, since
this observation could be used favorably for the
automatic EA system training.
Emotional composition of a fairy tale helps to re-
veal the overall character of the text and establish
if the story is abundant with various emotions or is
overloaded with only a few. For our overall research
goal, we would prefer the former kind of stories,
since they would build a rich training corpus. Fig-
ures 2 and 3 give an overview on the average shares
various emotion categories hold over the eight texts.
It is important to note that 65%? 75% of the text was
left neutral. The results show that most stories are
rich in positive rather than negative emotions, with
two exceptions we would like to elaborate upon. The
stories The Poor Boy in the Grave and The Dog
and the Sparrow belonged to different annotation
sets and thus no annotator dealt with both stories.
These texts were selected partially for their potential
0%?
5%?
10%?
15%?
20%?
25%?
JG? DS? BR? BT? SJ? D? FH? R?
approval?compassion?hope?interest?joy?relief?surprise?
Figure 2: Distribution of Positive Emotion Categories in Texts
0%?
5%?
10%?
15%?
20%?
25%?
JG? DS? BR? BT? SJ? D? FH? R?
anger?despair?disgust?disturbance?fear?hatered?sadness?
Figure 3: Distribution of Negative Emotion Categories in Texts
overcharge with negative emotions. The hypothesis
proved to be true, since the annotators have labeled
on average 20% of text with negative emotions, like
hatred and sadness. The only positive emotion cate-
gory salient for the The Poor Boy in the Grave story
is compassion, which is also mostly triggered by sad
events happening to a positive character.
The emotional flow in the fairy tales is illustrated
by the graph presented in Figure 4. In order to build
it, we used the numerical evaluations obtained in
the first part of the experiment and described in
section 4.1. For each fairy tale text, each word token
was mapped to the absolute value of the average
numerical evaluation of its emotional categories
assigned by all participants. The word tokens also
received its relative position in the text, where the
first word was at position 0.0 and the last at 1.0.
Thus, the emotional trajectories of all texts were
correlated despite the fact that their actual lengths
differed. The polynomial fit graph, taken over thus
acquired emotional flow common for all fairy tale
texts has a wave-shaped form and is similar to the
103
0 . 0 0 . 2 0 . 4 0 . 6 0 . 8 1 . 00 . 00 . 20 . 40 . 6
0 . 81 . 01 . 21 . 4
1 . 6
S t o r y  p r o g r e s s  [ r u ]Emotional r
esponse [ru]
Figure 4: Emotional Trajectory over all Stories
emotional trajectory reported by Alm and Sproat
(2005). The emotional charge increases and falls
steeply in the beginning of the fairy tale, then cycles
though rise and fall phases (which do not exceed
in their intensity the average rate of 0.6) and then
ascents steeply at the end of the story. We agree with
the explanation of such a trajectory, given by Propp
and Dundes (1977) and also elaborated by Alm and
Sproat (2005) ? the first emotional intensity peak
in the story line corresponds to the rising action,
after the main characters have been introduced and
the plot develops through a usually unexpected
event. At the end of the story the intensity is high-
est, regardless whether the denouement is a happy
ending or a tragedy. The fact that the fairy tale texts
we chose for the experiment are relatively short is
probably responsible for the steep peak of intensity
in the very beginning of the story ? the stories are
too short to include a proper exposition. However,
we need to investigate further how much of this is a
property of texts themselves and how much ? the
perception (and thus annotation) of emotions.
7 Results
The IAA scores were calculated using the emotion
clusters information, for according to the results,
participants would often stably use different emo-
tions from same clusters at the same stretch of text.
Four out of ten participants, two from each
group (marked gray in Table 1), had very low IAA
scores (? < 0.40 average per participant), a high
proportion of unmarked text, and they used few
emotion categories ( < 7 categories average per
participant), so for the evaluation part their data was
discarded. The final IAA evaluation was calculated
on all the annotation pairs obtained from the six
remaining participants (marked black in table 1),
whose average agreement score in the original set
of participants was originally higher than 0.50. The
total number of annotation pairs amounted to 48:
two texts annotated by all the six annotators, six
texts annotated by three annotators for each of the
two annotation sets.
According to the interpretation of ? by (Landis
and Koch, 1977), the annotator agreement was mod-
erate on average (0.53), and some pairs approached
the almost perfect IAA rate (0.83). The IAA rates,
calculated on the full set of fifteen emotions, with-
out taking the emotion clusters into consideration,
gave a moderate IAA rate on average (0.34) and
reached substantial level (0.62) at maximum. The
? rates are considerably high for the hard task and
are comparable with the results presented in (Alm
and Sproat, 2005). The word lists have a somewhat
lower ? IAA (0.45 on average, 0.72 at maximum),
which is due to the low number of categories and
the heavy bias towards the neutral category. The
observed agreement on word lists is considerably
high: 0.81 on average, reaching 0.91 at maximum.
While our approach may seem very similar to
the one of Alm (2005), there are some important
differences. We gave the participants the freedom of
using flexible annotation units, which allowed the
annotators to define the source of emotion more pre-
cisely and mark several emotions in one sentence. In
fact, in 39% of all annotated sentences represented a
mixture of the neutral category and ?polarized? cat-
egories, 20% of which included more than one ?po-
larized? categories. Another difference is the rich set
of emotion categories, with equal number of positive
and negative items. The results show that people can
successfully use the large set to express their emo-
tional perception of text (e.g., see Figures 3 and 2).
Other important findings include the fact that
short phrases are the naturally preferred annotation
unit among our participants and that the emotional
trajectory of a general story line corresponds to the
one proposed by Propp and Dundes (1977).
104
8 Future Work
8.1 Corpus Expansion
In the near future, we will expand the collections
of annotated text in order to compile a substantially
large training corpus. We plan to work further
with three annotators that have formed a natural
team, since their group has always attained the
highest annotation scores for their annotation set,
exceeding the highest scores in the other annotation
set. The task defined for the three annotators is
similar to the experiment described in the paper,
with several differences. For the corpus expansion
we chose 85 stories by the Grimm Brothers 1400
? 4500 tokens long. We expect that longer texts
have more potential space for an emotionally rich
plot. Each text will be annotated by two people,
the third annotator will tie-break disagreements by
choosing the most appropriate of the conflicting
categories, similar to the method described by (Alm
and Sproat, 2005). It is also probable that a basic
annotation unit will be defined and imposed on the
annotators, for, as the studies discussed in Section 6
show, short phrases are a language unit most often
naturally chosen by annotators.
Each of the annotators will also work with a sin-
gle word list, compiled from all texts and filtered for
the most common stop-words. Each of the words on
the word list should be annotated with its inherent
polarity (positive, negative or neutral). Since each
word on the list is free of its context, the lists
provide valuable information about the word and its
context interaction in full texts, which can be further
used for machine learning architecture training.
We also plan to keep the fifteen emotion cat-
egories and their clustering, since it gives the
annotator more freedom of expression and simulta-
neously allows the researches to find the common
cognitive ground behind the labels if they vary
within one cluster
8.2 Feature Extraction and Machine Learning
Architecture Training
When the corpus is large enough, the relevant
features will be extracted automatically by means
of existing NLP tools, followed by training a ma-
chine learning architecture, most probably TiMBL
(Daelemans et al, 2004), to map textual units to
the emotion categories. It is yet to be determined
which features to use, one compulsory parameter
is that all the features should be available through
automatic processing tools. This is crucial, since
the resulting EA system has to be fully automated
with no manual work involved.
8.3 Extra Information Sources and their
Potential Contribution
We also plan to collect data from other information
sources, like video and audio recordings, by inviting
amateur actors for story-telling sessions. This will
allow emotion retrieval from the speech melody,
facial expressions and body language. The manual
annotation and the extra data sources can be aligned
by means of Text and Speech Aligner (Rapp, 1995),
which allows to track correspondences between
them. This alignment would most certainly ben-
efit the facial and body animation of the virtual
characters, since there is no clear understanding
of time correlation between emotions labeled in
written text and the ones expressed through speech
and facial clues in a story telling scenario. An EA
system could also be perfected through a careful
analysis of recorded speech and video of story
telling sessions ? regular recurrence of subjectivity
of certain contexts will be even more significant
if the transmission of the emotions from the story
teller to the listener via mentioned information
sources is successful.
9 Conclusions
In this paper, we reported on an experiment inves-
tigating the inter-annotator agreement levels which
can be achieved by untrained human annotators per-
forming emotion analysis of variable units of text.
While EA is a very difficult task, our experiment
shows that even untrained annotators can have high
agreement rates, even given considerable freedom
in expressing their emotional perception of text. To
the best of our knowledge, this is the first attempt at
emotion analysis that operates on flexible, annotator
defined units and uses a relatively rich inventory of
emotion categories. We consider the resulting IAA
rates to be high enough to accept the annotations
as suitable for gold-standard corpus compilation in
the frame of this research. As such, we view this
work as the first step towards the development of a
more complex EA system, which aims to simulate
the actual human emotional perception of text.
105
References
C.O. Alm and R. Sproat. 2005. Emotional sequencing
and development in fairy tales. In Proceedings of the
First International Conference on Affective Computing
and Intelligent Interaction (ACII05). Springer.
C.O. Alm, D. Roth, and R. Sproat. 2005. Emotions from
text: Machine learning for text-based emotion predic-
tion. In Proceedings of HLT/EMNLP, volume 2005.
C.O. Alm. 2008. Affect in Text and Speech.
lrc.cornell.edu.
R. Artstein and M. Poesio. 2008. Inter-coder agreement
for computational linguistics. Computational Linguis-
tics, 34(4):555?596.
Jan Auracher. 2007. ... wie auf den allma?chtigen Schlag
einer magischen Rute. Psychophysiologische Messun-
gen zur Textwirkung. Ars poetica ; 3. Dt. Wiss.-Verl.
C.F. Baker, C.J. Fillmore, and J.B. Lowe. 1998.
The berkeley framenet project. In Proceedings of
the 17th international conference on Computational
linguistics-Volume 1, pages 86?90. Association for
Computational Linguistics Morristown, NJ, USA.
Steven Bethard, Hong Yu, Ashley Thornton, Vasileios
Hatzivassiloglou, and Dan Jurafsky. 2004. Automatic
extraction of opinion propositions and their holders. In
2004 AAAI Spring Symposium on Exploring Attitude
and Affect in Text, page 2224.
J. Carroll, A. Frank, D. Lin, D. Prescher, and H. Uszkor-
eit. 2002. Beyond Parseval-Towards improved evalua-
tion measures for parsing systems. In Workshop at the
3rd International Conference on Language Resources
and Evaluation LREC-02., Las Palmas.
D. W. Cunningham and C. Wallraven. 2009. Dynamic
information for the recognition of conversational ex-
pressions. Journal of Vision, 9(13:7):1?17, 12.
Walter Daelemans, Jakub Zavrel, Ko van der Sloot, and
Antal van den Bosch. 2004. Timbl: Tilburg memory
based learner, version 5.1, reference guide. ilk techni-
cal report 04-02. Technical report.
P. Ekman. 1993. Facial Expression and Emotion. Amer-
ican Psychologist, 48(4):384?392.
JR Fontaine, KR Scherer, EB Roesch, and PC Ellsworth.
2007. The world of emotions is not two-dimensional.
Psychological science: a journal of the American Psy-
chological Society/APS, 18(12):1050.
P. Kingsbury, M. Palmer, and M. Marcus. 2002. Adding
semantic annotation to the Penn Treebank. In Pro-
ceedings of the Human Language Technology Confer-
ence, pages 252?256. Citeseer.
J.R. Landis and G.G. Koch. 1977. The measurement of
observer agreement for categorical data. Biometrics,
33(1):159?174.
D. Lee, O.R. Jeong, and S. Lee. 2008. Opinion min-
ing of customer feedback data on the web. In Pro-
ceedings of the 2nd international conference on Ubiq-
uitous information management and communication,
page 230235, New York, New York, USA. ACM.
Hugo Liu, Henry Lieberman, and Ted Selker. 2003.
A model of textual affect sensing using real-world
knowledge. In Proceedings of the 8th international
conference on Intelligent user interfaces - IUI ?03,
page 125, New York, New York, USA. ACM Press.
G.A. Miller, R. Beckwith, C. Fellbaum, D. Gross, and
K.J. Miller. 1990. Introduction to Wordnet: An on-
line lexical database*. International Journal of lexi-
cography, 3(4):235.
L. Polanyi and A. Zaenen. 2006. Contextual valence
shifters. Computing Attitude and Affect in Text: The-
ory and Applications, page 110.
V.I.A. Propp and A. Dundes. 1977. Morphology of the
Folktale. University of Texas Press.
S. Rapp. 1995. Automatic phonemic transcription and
linguistic annotation from known text with Hidden
Markov Models. In Proceedings of ELSNET Goes
East and IMACS Workshop. Citeseer.
M.L. von Franz. 1996. The interpretation of fairy tales.
Shambhala Publications.
C. Whitelaw, N. Garg, and S. Argamon. 2005. Using ap-
praisal groups for sentiment analysis. In Proceedings
of the 14th ACM international conference on Informa-
tion and knowledge management, page 631. ACM.
J. Wiebe, T. Wilson, R. Bruce, M. Bell, and M. Martin.
2004. Learning subjective language. Computational
linguistics, 30(3):277?308.
T. Wilson, J. Wiebe, and R. Hwa. 2004. Just how mad
are you? Finding strong and weak opinion clauses. In
Proceedings of the National Conference on Artificial
Intelligence, pages 761?769. Menlo Park, CA; Cam-
bridge, MA; London; AAAI Press; MIT Press; 1999.
T. Wilson, J. Wiebe, and P. Hoffmann. 2009. Recogniz-
ing Contextual Polarity: an exploration of features for
phrase-level sentiment analysis. Computational Lin-
guistics, 35(3):399433, September.
106
Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, ACL-HLT 2011, pages 111?117,
24 June, 2011, Portland, Oregon, USA c?2011 Association for Computational Linguistics
Automatic?Sentiment?Classification?of?Product?Reviews?Using?Maximal?
Phrases?Based?Analysis
Maria?Tchalakova
Textkernel?BV
Nieuwendammerkade?
28A?17
NL?1022?AB?Amsterdam
The?Netherlands
maria.tchalakova@gm
ail.com
Dale?Gerdemann
University?of?T?bingen
Wilhelmstr.?19?23
72074?T?bingen
Germany
dale.gerdemann@googl
email.com
Detmar?Meurers
University?of?T?bingen
Wilhelmstr.?19?23
72074?T?bingen?
Germany
dm@sfs.uni-
tuebingen.de
Abstract
In ? this ? paper ? we ? explore ? the ? use ? of ? phrases?
occurring ? maximally ? in ? text ? as ? features ? for?
sentiment?classification?of?product?reviews.?The?
goal?is?to?find?in?a?statistical?way?representative?
words ? and ? phrases ? used ? typically ? in ? positive?
and ?negative ? reviews. ?The?approach ?does ?not?
rely?on?predefined?sentiment?lexicons,?and?the?
motivation ? for ? this ? is ? that ? potentially ? every?
word ? could ? be ? considered ? as ? expressing?
something?positive?and/or?negative?in?different?
situations,?and?that?the?context?and?the?personal?
attitude?of?the?opinion?holder?should?be?taken?
into?account?when?determining?the?polarity?of?
the ? phrase, ? instead ? of ? doing ? this ? out ? of?
particular?context.
1 Introduction
As?human?beings?we?use?different?ways?to?express?
opinions ? or ? sentiments. ? The ? field ? of ? sentiment?
analysis?tries?to?identify?the?ways,?in?which?people?
express?opinions?or?sentiments?towards?a?particular?
target ? or ? entity. ? The ? entities ? could ? be ? persons,?
products,?events,?etc.?With?the?development?of?the?
Internet?technologies?and?robust?search?engines?in?
the ? last ? decade, ? people ? nowadays ? have ? a ? huge?
amount?of?free?information.?Because?of?this?huge?
amount, ? however, ? the ? data ? needs ? to ? be ? first?
effectively?processed?so?that?it?could?be?used?in?a?
helpful ? way. ? The ? automatic ? identification ? of?
sentiments?would?make?possible?the?processing?of?
large?amounts?of?such?opinionated?data.?? 
???The?focus?of?this?paper?is?sentiment?classification?
at?document?level,?namely?classification?of?product?
reviews ? in ? the ? categories ? positive ? polarity ? or?
negative?polarity.?Training?and?testing?data?for?our?
experiments ? is ? the ? Multi?Domain ? Sentiment?
Dataset ? (Blitzer ? et ? al., ? 2007), ? which ? consists ? of?
product?reviews?of?different?domains,?downloaded?
from ? Amazon1. ? We ? explore ? the ? use ? of ? phrases?
occurring ? maximally ? in ? text ? as ? features ? for?
sentiment ? classification ? of ? product ? reviews. ?In?
contrast ? to ? many ? related ? works ? on ? sentiment?
classification?of?documents,?we?do?not?use?general?
polarity ? lexicons, ? which ? contain ? predefined?
positive?and?negative?words.?Very?often?the?same?
word?or?phrase?could?express?something?positive?in?
one?situation?and?something?negative? in ?another.?
We?identify?words?and?phrases,?which?are?typically?
used?in?positive?and?negative?documents?of?some?
specific?domains,?based?on?the?frequencies?of?the?
words?and?phrases?in?the?domain?specific?corpora.?
After ? that ?we ?use ? these ?phrases ? to ? classify ?new?
sentiment ? documents ? from ? the ? same ? type ? of?
documents,?from?which?the?phrases?are?extracted.
1http://www.amazon.com/
111
2 Phrase?Extraction
In?order?to?extract?distinctive?phrases?we?use?the?
approach?of?Burek?and?Gerdemann?(2009),?who?try?
to?identify?phrases,?which?are?distinctive?for?each?
of ? the ? four ?different ? categories ?of ? documents ? in?
their ? medical ? data. ? With ? distinctive ? they ? mean?
phrases, ? which ? occur ? predominantly ? in ? one?
category ? of ? the ? documents ? or ? another. ? The?
algorithm?extracts?phrases?of?any?length.?The?idea?
is ? that ? if ? a ? phrase ? is ? distinctive ? for ? a ? particular?
category,?it?does?not?matter?how?long?the?phrase?is.?
The?algorithm?looks?for?repeats?of?phrases?of?any?
length, ? and ? could ? also ? count ? different ? types ? of?
occurrences ? of ? phrases, ? e.g. ? maximal, ? left?
maximal,?or?right?maximal.?Considering?such?types?
of?occurrences,?it?is?possible?to?restrict?the?use?of?
certain ? phrases, ? which ? might ? not ? be ? much?
distinctive ? and ? therefore ? might ? not ? be?
representative?for?a?category.?Similar?to?Burek?and?
Gerdemann?(2009)?we?experiment?with?using?all?
types ?of ?occurrences ?of ? a ? phrase ? as ? long ?as ? the?
phrase?occurs?maximally?at? least?one?time?in?the?
text.
2.1 Distinctiveness?of?Phrases
Distinctive ? phrases ? are ? phrases, ? which?
predominantly ? occur ? in ? one ? particular ? type ? of?
documents ? (Burek ? and ? Gerdemann, ? 2009). ? The?
presence?of?such?phrases?in?a?document?is?a?good?
indicator?of?the?category?(or?type)?of?the?document.?
The?general?rule,?as?Burek?and?Gerdemann?(2009)?
point ? out, ? is ? that ? if ? some ?phrases ? are ?uniformly?
distributed ? in ? a ? set ? of ? documents ? with ? different?
categories, ? then ? these ?phrases ?are ?not ?distinctive?
for?any?of?the?categories?in?the?collection.?On?the?
other?hand,?if?particular?phrases?appear?more?often?
in?one?category?of?documents?than?in?another,?they?
are?good?representatives?for?the?documents?of?this?
type,?and?consequently?are?said?to?be?distinctive2.??
??There?are?different?weighting?schemes,?which?one?
can?use?to?determine?the?importance?of?a?term?for?
the ? semantics ? of ? a ? document. ? Burek ? and?
Gerdemann ? (2009) ? implement ? their ? own ? scoring?
2If ? the?number?of ?occurrences?of ? such?phrase? in ? the?whole?
collection?of?documents?is?very?small,?however,?the?clustering?
of?the?phrase?in?some?documents?of?a?specific?category,?may?
be?purely?accidental.?(Burek?and?Gerdemann,?2009)
function?for?weighting?the?extracted?phrases.?One?
of?their?reasons?not?to?use?the?standard?weighting?
function?tf?idf?is?that?the?idf?measure?does?not?take?
into?account?what?the?category?of?the?documents?is,?
in?which?the?term?occurs.?This?is?important?in?their?
case,?because?their?data?consist?of?four?categories,?
which ? could ? be ? grouped ? in ? two ? main ? classes,?
namely ?excellent? and ?good? on?the?one?hand,?and?
fair? and?poor?on?the?other?hand.?A?problem?when?
using?tf?idf?will ?appear, ? if ? there?is?a?rare?phrase,?
which ? occurs ? in ? a ? small ? number ? of ? documents,?
however, ? it ? clusters ? in ?documents ? from ? the ? two?
different?classes,?for?example,?in?excellent?and?fair,?
or ? in ?good? and ?poor. ? This ? will ? not ? be ? a ? good?
distinctive ? phrase ? for ? this ? categorization ? of ? the?
data. ? Another ? motivation ? to ? develop ? their ? own?
scoring ? function ? is ? to ?cope ?with ? the ?problem?of?
burstiness?(see?section?2.2.1).
2.2 Extraction?of?Phrases
This?section?describes?the?algorithm?of?Burek?and?
Gerdemann ? (2009) ? for ? extracting ? distinctive?
phrases?and?how?we?have?modified?and?used?it?in?
the?context ?of ?our?work. ?We?first ?show?how?the?
phrases?are?ranked,?so?that?one?knows?what?phrases?
are?more?or?less?distinctive?than?others.
2.2.1 The?Scoring?Algorithm
The ? extracted ? phrases ? are ? represented ? by?
occurrence ? vectors. ? These ? vectors ? have ? two?
elements???one?for?the?number?of?documents?with?
category ?positive ? polarity, ? and ? another ? for ? the?
negative?polarity.?Each?element?of?the?vector?stores?
the ?number ? of ? distinct ? documents, ? in ?which ? the?
phrase?occurs.?For?example,?if?a?phrase?occurs?in?
10 ? positive ? reviews, ? and ? 1 ? negative ? review, ? the?
occurrence?vector?of?this?phrase?is?<10,?1>?.?This?
shows?that?for?the?representation?of?the?phrases?we?
take? into?account? the?document?frequency?of? the?
phrase,?and?not?its?term?frequency.?The?motivation?
behind?this?choice?is?to?cope?with?the?problem?of?
burstiness?of?terms.?Madsen?et?al.?(2005)?explain?
burstiness ? in ? the ? following ? way: ?The ? term?
burstiness ?(Church?and?Gale, ?1995;?Katz, ?1996) ?
describes?the?behavior?of?a?rare?word?appearing?
many?times?in?a?single?document.?Because?of?the ?
large ?number ?of ?possible ?words, ?most ?words ?do?
not ?appear ? in ?a ?given ?document. ?However, ? if ?a ?
112
word?does?appear?once,?it?is?much?more?likely?to ?
appear?again,?i.e.?words?appear?in?bursts.
? ?We ?assign ? a ? score ? to ? a ? phrase ? by ?giving ? the?
phrase ? one ? point, ? if ? the ? phrase ? occurs ? in ? a?
document?with?positive?polarity?and?zero?points,?if?
it?occurs?in?a?document?with?negative?polarity.
? ? ?Let?us?take?again?the?occurrence?vector?of?<10,?
1>?.?According?to?the?way?the?points?are?given,?the?
vector?will?be?assigned?a?score?of?10?((1?point?*?
10)? ?+?(0?points?*?1)?=?10).?Is?this?a?good?score,?
which ? indicates ? that ? the ?phrase ? is ?distinctive ? for?
documents?of?category?positive?polarity??We?can?
answer ? this ? question, ? if ? we ? randomly ? choose?
another?phrase,?which?occurs?in?11?documents,?and?
see?what?the?probability?is,?that?this?phrase?would?
have?a?score,?which?is?higher?than?or?equally?high?
to?the?score?of?the?phrase?in?question?(Burek?and?
Gerdemann, ? 2009). ? In ? order ? to ? calculate ? this?
probability, ? the ? scoring ? method ? performs ? a?
simulation, ? in ? which ? occurrence ? vectors ? for?
randomly?chosen?phrases?are?created.?Let?us?pick?
randomly?one?phrase,?which?hypothetically?occurs?
in?11?reviews.?Let?also,?have?a?data?of?600?positive?
reviews?and?600?negative?reviews.?The?probability?
then, ? that ? the ? random?phrase ?would ?occur ? ? in ?a?
positive?or?a?negative?review?is?0.5.?Based?on?these?
probabilities, ? the ? simulation ? process ? constructs?
random?vectors?for?the?random?phrase,?indicating?
whether ? the ? phrase ? occurs ? in ? a ? positive ? or ? in?
negative ? review. ? For ? example, ? if ? in ? a ? particular?
run, ? the ? simulation ? says ? that ? the ? random ?phrase?
occurs?in?a?positive?review,?then?we?have?a?random?
vector?of?<1,?0>.?Otherwise,?<0,?1>?for?a?negative?
review. ?The?program?calculates ?as ?many?random?
vectors ? as ? the ?number ?of ? reviews, ? in ?which ? the?
random?phrase?is?said?to?occur.?In?this?example,?the?
number?of?documents?is?11.?Therefore,?11?random?
vectors ?will ?be ?constructed. ?They?may? look? like?
this:?<1,?0>,?<1,?0>,?<0,?1>?,?<1,?0>,?<1,?0>,?<0,?
1>,?<0,?1>,?<0,?1>,?<1,?0>,?<1,?0>,?<0,?1>.?These?
vectors?are?then?summed?up,?and?the?result?vector?
<6, ? 5> ? is ? the ? random? occurrence ?vector ? for ? the?
random ? phrase. ? It ? tells ? us ? that ? the ? phrase,?
hypothetically, ? occurs ? in ? 6 ? positive ? and ? in ? 5?
negative?reviews.?The?score?for?the?random?phrase?
is?now?calculated?in?the?same?way?as?for?the?non?
random ? phrases: ? 1 ? point ? is ? given ? for ? each?
occurrence?of?the?phrase?in?a?positive?review,?and?0?
points?otherwise.?So,?the?score?for?this?phrase?is?6?(?
((1?point?*?6)??+?(0?points?*?5)?=?6)?).?This?process?
is ?performed?a ?certain ?number ?of ? times. ?For ? the?
experiments?presented? in?section?3.2, ?we?run?the?
simulation?10,000?times?for?each?extracted?phrase.?
This?means?that?10,000?random?vectors?per?phrase?
are?created.
? ?The ? last ? step ? is ? to ? compare ? the ? scores ?of ? the?
random?phrase?with?the?score?of?the?actual?phrase,?
and?to?see?how?many?of?the?10,000?random?vectors?
give ? a ? score ? higher ? than ?or ? equally ? high ? to ? the?
score?of?the?actual?phrase.?If?the?number?of?random?
vectors,?which?give?a?higher?than?or?equally?high?
score ? to ? the ? actual ? phrase, ? is ? bigger ? than ? the?
number ? of ? random ? vectors, ? which ? give ? a ? score?
lower?than?the?actual?phrase,?then?the?actual?phrase?
is?assigned?a?positive?score,?and?the?value?of?this?
score ? is ? the ? approximate ? number ? of ? random?
vectors, ? from?which ?higher ? than?or ?equally ?high?
scores?to?the?actual?phrase?score?are?calculated.?If?
the ?number ? is ? lower, ? the ?phrase ? is ? assigned ? the?
approximate ? number ? of ? random ? vectors, ? from?
which?lower?scores?than?the?actual?phrase?score?are?
calculated, ? and ? a ? minus ? sign ? is ? attached ? to ? the?
number,?making?the?score?negative.
2.2.2 The?Phrase?Extraction?Algorithm
The?main?idea?of?the?algorithm?is?that?if?a?phrase?is?
distinctive ? for ? a ? particular ? category, ? it ? does ? not?
matter?how?long?the?phrase?is???as?long?as?it?helps?
for ? distinguishing ? one ? type ? of ? document ? from?
another,?it?should?be?extracted. ?In?order?to?extract?
phrases ? in ? this ? way, ? the ? whole ? collection ? of?
documents?is?represented?as?one?long?string.?Each?
phrase?is?then?a?substring?of?this?string.?It?will?be?
very?expensive?to?compute?statistics?(i.e.?tf?and?df)?
and?to?run? the?simulation?process?(see?2.2.1) ?for?
each?substring?in?the?text. ?The?reason?is?that ?the?
amount?of?substrings?might?be?huge???there?are?a?
total ? of ? N(N ? + ? 1) ? / ? 2 ? substrings ? in ? a ? corpus?
(Yamamoto ? and ? Church, ? 2001). ? Yamamoto ? and?
Church ? (2001) ? show ? how ? this ? problem ? can ? be?
overcome ? by ? grouping ? the ? substrings ? into?
equivalence?classes?and?performing?operations?(i.e.?
computing?statistics)?on?these?classes?instead?of?on?
the?individual?elements?of?the?classes.?They?use?for?
this?the?suffix?array?data?structure.?The?number?of?
the?classes?is?at?most?2N???1.
113
2.2.3 Maximal?Occurrence?of?a?Phrase
The ? suffix ? array ? data ? structure ? allows ? for ? easy?
manipulation?of?the?strings.?The?algorithm?extracts?
phrases ? if ? they? repeat ? in ? text, ?and?if ? the ?phrases?
occur?maximally?at ? least ?once? in ? the? text. ? If ? the?
phrase?do?not?occur?maximally?at?least?one?time,?
then? it ?may?not ?be?a ?good?linguistic?unit, ?which?
could? stand?on? its ?own. ?Example?of ?such ?words?
might ? be ? the ? different ? parts ? of ? certain ? named?
entities. ? For ? instance, ? the ? name ?Bugs ? Bunny. ? If?
Bugs?or?Bunny?never?appear?apart?from?each?other?
in? the ? text, ? then? this ? imply? that ? they?comprise?a?
single ? entity ? and ? they ? should ? always ? appear?
together?in?the?text.?In?this?case?it?does?not?make?
sense, ? for ? example, ? to ? count ? only ?Bugs? or ? only?
Bunny? and ? calculate ? statistics ? (e.g. ? tf ? or ? df) ? for?
each?of?them.?They?should?be?grouped?instead?into?
a?class.???
? Burek ? and ? Gerdemann ? (2009) ? mention ? three?
different ? types ? of ? occurrences ? of ? a ? phrase: ? left?
maximal, ? right ? maximal, ? and ? maximal. ? A ? left?
maximal?occurrence?of?a?phrase?S[i,j]?means?that?
the ? longer ?phrase ?S[i?1,j] ?does ?not ? repeat ? in ? the?
corpus ? (Burek ? and ? Gerdemann, ? 2009). ? For?
example, ? in ? the ? sentences ? below, ? the ? phrase?
recommend? is?not?left?maximal,?because?it?can?be?
extended?to?the?left?with?the?word?highly:
I?highly?recommend?the?book.?
You?highly?recommend?this?camera.
? ??On?the?other?hand?the?phrase?highly?recommend?
is?left?maximal.?
? ?In?a?similar ?way?we?define? the?notion?of?right?
maximal ? occurrence ? of ? a ? phrase. ? A ? maximal?
occurrence?of?a?phrase?is?when?the?occurrence?of?
the?phrase?is?both?left?maximal?and?right?maximal?
(Burek?and?Gerdemann,?2009).?The?phrase?highly ?
recommend? in?the?example?sentences?above?is?in?
this?sense?maximal.
??It?is?not?clear?a?priori?which?of?these?types?should?
be?taken?into?account?for?the?successful?realization?
of?a?given?application.?One?could?consider?only?the?
left ?maximal, ?only ? the ? ? right ?maximal, ?only ? the?
maximal ? occurrences ? of ? the ? phrases, ? or ? all?
occurrences. ? We ? experimented ? with ?all?
occurrences. ? Our ? motivation ? is ? that ? using ? all?
phrases ?would ?give ?us ? a ? big ? enough ?number ?of?
distinctive?phrases?and?we?will?most?probably?not?
have?a?problem?with?data?sparseness.
3 Sentiment ? Classification ? of ? Product?
Reviews
For ? the ? experiments ? presented ?below?we ?used ? a?
supervised ? machine ? learning ? approach, ? and?
different ? sets ? of ? features. ? Reviews ? from ? two?
domains,?books?and?cameras?&?photos,?are?used?as?
training?and?testing?data.
3.1 Choosing ? Distinctive ? Phrases ? for?
Classification
Once ? the ? phrases ? with ? which ? we ? would ? like ? to?
represent?the?documents?are?extracted,?we?need?to?
consider?two?things?in?the?very?beginning.?On?the?
one ? hand, ? the ? phrases ? should ? be ? as ? much?
distinctive ? as ? possible. ?On ? the ?other ? hand, ? even?
though ? a ? phrase ? might ? occur ? predominantly ? in?
negative ? reviews, ? it ? occurs ? very ? often ? also ? in?
positive ?reviews? (once?or ?at ? least ? several ? times),?
and?vice?versa.?Should?we?consider?such?phrases??
If?yes,?what?would?be?the?least?acceptable?number?
of?occurrences?of?the?phrases?in?the?opposite?type?
of ? reviews? ? We ? might ? choose ? as ? distinctive?
phrases?those?which?occur?only?in?positive?or?only?
in?negative?reviews,?however,?these?phrases?will?be?
very?few,?and?we?might?have?the?problem?of?data?
sparseness.?On?the?other?hand,?using?all?extracted?
phrases?might?bring?a?lot?of?noise,?because?many?of?
the?phrases?will?not?be?very?good?characteristics?of?
the?data. ?We?experimented?with?several ?different?
subsets?of?the?set?of?all?extracted?phrases.?
? ? In ? order ? to ? decide ? what ? subsets ? of ? extracted?
phrases?to?use,?we?analyzed?the?set?of?all?extracted?
phrases ?paying ?attention? to ? their ?vectors ?and ? the?
scores,?trying?to?find?a?trade?off?between?the?two?
mentioned?considerations?above.
3.2 Experiments?
??SVM?is?used?as?a?machine?learning?algorithm?for?
the?experiments?(the?implementation?of?the?SVM?
package?LibSVM3?in?GATE4).
3Libsvm:?a?library?for?support?vector?machines,?2001.?software?
available?at?http://www.csie.ntu.edu.tw/?cjlin/libsvm.
4http://gate.ac.uk/
114
? ?For?each?experiment?we?first?divide?the?reviews?
of?each?domain?into?training?and?testing?data?with?
ratio?two?to?one.?From?this?training?data?we?extract?
the ? distinctive ? phrases, ? which ? are ? later ? used ? as?
features ? to ? the ? learning ?algorithm. ?As?evaluation?
method?we?apply?the?k?fold?cross?validation?test,?
with?k=10.?For?all?experiments?we?used?the?default?
tf?idf?weight?for?the?n?grams.?For?each?domain?we?
conduct?five?different?experiments,?each?time?using?
different ? subsets ? of ? distinctive ? phrases. ? ? All?
experiments?were?performed?with?GATE.
??For?each?domain?the?training?data?from?which?the?
phrases ? are ? extracted ? consists ? of ? about ? 665?
negative?and?665?positive?reviews.?The?testing?data?
consists?of?333?negative?and?333?positive?reviews.?
? ?It?is?interesting?to?notice?that?although?the?results?
of ? the ? experiments ? are ? different, ? they ? are ? very?
close?to?each?other,?regardless?of?the?big?difference?
in ? the ? number ? of ? phrases ? used ? as ? features.?
Therefore, ? we ? decided ? to ? experiment ? with ? all?
extracted?phrases.?It?turned?out?that?the?results?of?
that?experiment?are?the?best.?This?would?imply?that?
the ? bigger ? number ? of ? phrases ? is ? helpful ? and ? it?
compensates ? for ? the ?use ?of ? phrases ? that ? are ?not?
much?distinctive.
? ?The?results?of?all?experiments?for?domain ?books?
are ? summarized ? in ? Table ? 1. ? The ? best ? achieved?
results?of?81%?precision,?recall,?and?F?measure?are?
given ? in ? bold. ? The ? rightmost ? column ? gives ? the?
number?of?negative?(n.)?and?positive?(p.)? ?phrases?
used?in?each?experiment.
Experiment Reviews P R F?m Phrases?used
Exp1 Negative
Positive
Overall
0.77
0.80
0.78
0.80
0.77
0.78
0.79
0.78
0.78
1685?n.
1116?p.
Exp2 Negative
Positive
Overall
0.75
0.80
0.77
0.80
0.74
0.77
0.77
0.76
0.77
924?n.
568?p.
Exp3 Negative
Positive
Overall
0.76
0.78
0.77
0.78
0.76
0.77
0.77
0.77
0.77
349?n.?
178?p.
Exp4 Negative
Positive
Overall
0.77
0.79
0.78
0.79
0.77
0.78
0.78
0.78
0.78
10552?n.?
9084?p.
Exp5 Negative
Positive
Overall
0.80
0.81
0.81
0.81
0.80
0.81
0.80
0.80
0.81
All:
24107?n.?
21149?p.
Table?1:?Domain?books
? ? Table ? 2 ? summarizes ? the ? results ? for ? domain?
camera&photos, ?showing?the?best?results?of?86%?
precision,?recall,?and?F?measure?in?bold.
? ? Similar ? to ? the ? experiments ? with ? reviews ? of?
domain?books,?the?results?for?camera&photos?in?all?
five ? experiments ? are ? very ? close. ? Again ? the ? best?
results?are?obtained?when?all?extracted?distinctive?
phrases?are?considered.?
Experiment Reviews P R F?m Phrases?
used
Exp1 Negative
Positive
Overall
0.85
0.83
0.84
0.83
0.85
0.84
0.84
0.84
0.84
1746n.?1883?
p.
Exp2 Negative
Positive
Overall
0.84
0.81
0.83
0.81
0.85
0.83
0.82
0.83
0.83
1013n.?1053?
p.
Exp3 Negative
Positive
Overall
0.86
0.83
0.85
0.83
0.87
0.85
0.85
0.85
0.85
384?n.?
432?p.
Exp4 Negative
Positive
Overall
0.85
0.83
0.84
0.83
0.86
0.84
0.84
0.84
0.84
7572?n.?
9821?p.
Exp5 Negative
Positive
Overall
0.86
0.85
0.86
0.85
0.87
0.86
0.86
0.86
0.86
All:
16378?n.?
17951?p.
Table?2:?Domain?camera&photos.
In ?order ? to ?evaluate ?how?well ? the ? results ?of ? the?
experiments ? are ? we ? performed ? several ? more?
experiments, ? in?which?the?texts?were?represented?
with?unigrams?(1?grams) ?and?bigrams?(2?grams).?
Pang?and ?Lee ? (2008) ?note ? that: ?whether ?higher?
order?n?grams?are?useful?features?appears?to?be?a?
matter?of?some?debate.?For?example,?Pang?et?al. ?
(2002) ?report ? that ?unigrams?outperform?bigrams?
when ? classifying ? movie ? reviews ? by ? sentiment ?
polarity,?but?Dave?et?al.?(2003)?find?that?in?some?
settings, ? bigrams ? and ? trigrams ? yield ? better ?
product?review?polarity?classification.?Bekkerman?
and ?Allan ? (2004) ? review? the ? results ? of ? different?
experiments ? on ? text ? categorization ? in ? which ? n?
gram?approaches?were?used,?and?conclude?that?the?
use?of?bigrams?for?the?representation?of?texts?does?
not ? show ? general ? improvement ? (Burek ? and?
Gerdemann, ? 2009). ? It ? seems ? intuitive ? that ? when?
bigrams ? are ? used, ? we ? would ? have ? a ? better?
representation ? of ? the ? texts, ? because ? we ? would?
know?what?words?combine?with?what?other?words?
in? the ? texts. ?However, ? there ? is ?a ?data?sparseness?
problem.??
? ? It ? seems ? interesting ? to ? compare ? the ? results?
obtained ? by ? representing ? the ? texts ? as ? unigrams,?
bigrams, ? and ? distinctive ? (maximally ? occurring)?
phrases,?because?the?model?based?on?phrases?might?
use?both?unigrams?and?bigrams,?and?it?allows?also?
any ? other ? higher ? n?grams, ? that ? is, ? more ? context?
115
(and?semantics)?of?the?text?is?preserved.?
? ? Tables ? 3 ? and ? 4 ? present ? the ? results ? of ? the?
experiments?using?bag?of?tokens?(1?gram)?models,?
while?Tables?5?and?6?present?the?experiments?with?
the?2?gram?models.?GATE?was?used?as?a?working?
environment,?and?SVM?as?learning?algorithm.
Reviews Precision Recall F?measure
Negative? 0.77 0.82 0.79
Positive? 0.82 0.75 0.78
Overall 0.79 0.79 0.79
Table?3:?Domain?books,?1?gram.
Reviews Precision Recall F?measure
Negative 0.86 0.84 0.85
Positive 0.84 0.86 0.85
Overall 0.85 0.85 0.85
Table?4:?Domain?camera&photos,?1?gram.
Reviews Precision Recall F?measure
Negative 0.72 0.80 0.75
Positive 0.78 0.69 0.73
Overall 0.75 0.75 0.75
Table?5:?Domain?books,?2?gram.
Reviews Precision Recall F?measure
Negative 0.84 0.83 0.83
Positive 0.83 0.84 0.83
Overall 0.83 0.83 0.83
Table?6:?Domain?camera&photos,?2?gram.
Features Precision Recall F?measure
All?phrases 0.81 0.81 0.81
1?gram 0.79 0.79 0.79
2?gram 0.75 0.75 0.75
Table?7:?Comparison,?Domain?books.
Features Precision Recall F?measure
All?phrases 0.86 0.86 0.86
1?gram 0.85 0.85 0.85
2?gram 0.83 0.83 0.83
Table?8:?Comparison,?Domain?camera&photos.
Tables?7?and?8?summarize?the?overall?results?using?
1?gram?and?2?gram?models?and?a?model?based?on?
distinctive ? phrases ? for ? the ? representation ? of ? the?
texts. ? For ? both ? domains ? the ? best ? results ? are?
achieved ? with ? the ? model ? based ? on ? phrases ? (all?
phrases). ? For ? the ? domain ?books? the ? overall?
precision, ? recall ? and ? F?measure ? results ? achieved?
with ? that ? model ? (81%) ? are ? 2% ? higher ? than ? the?
results?obtained?using?the?1?gram?model,?and?6%?
higher?than?the?results?obtained?using?the?2?gram?
model. ? For ? domain ?cameras ? & ? photos, ? an?
improvement?of?1%?and?3%?is?achieved?with?the?
phrase?model?in?comparison?with?the?1?gram?and?
2?gram?models,?respectively.
4 Related?Work
Close?to?our?work?seems?to?be?Funk?et?al.?(2008).?
They?classify?product ?and?company?reviews? into?
one?of?the?1?star?to?5?star?categories.?The?features?
to? the ? learning?algorithm?(also?SVM)?are?simple?
linguistic ? features ? of ? single ? tokens. ? They ? report?
best ? results ? with ? the ? combinations ?root ? &?
orthography, ? and ?only ? root. ? Another ? interesting?
related?work?is?that?of?Turney?(2002).?He?uses?an?
unsupervised ? learning ? algorithm ? to ? classify ? a?
review?as?recommended?or?not?recommended.?The?
algorithm ? extracts ? phrases ? from ? a ?given ? review,?
and?determines?their?pointwise?mutual?information?
with?the?words ?excellent? and ?poor.?Turney?(2002)?
points ?out ? that ? the ?contexual ? information? is ?very?
often?necessary?for?the?correct?determination?of?the?
sentiment?polarity?of?a?certain?word.
5 Conclusion
This ? paper ? presented ? different ? experiments ? on?
classifying?product?reviews?of?domains ?books? and?
cameras ?& ?photos? under ? the ? categories ?positive?
polarity? and ?negative ? polarity? using ? distinctive?
(maximally ? occurring) ? phrases ? as ? features. ? For?
both?domains?best?results ?were?achieved?with?all?
extracted ? distinctive ? phrases ? as ? features. ? This?
approach?outperforms?slightly ? the?1?gram?and?2?
gram?experiments?on?this?data?and?shows?that?the?
use?of?phrases?occurring?maximally?in?text?could?
be ? successfully ? applied ? in ? the ? classification ? of?
sentiment?data?and?that?it?is?worth?experimenting?
with?classifying?sentiment?data?without?necessarily?
relying?on?general?predefined?sentiment?lexicons.
116
References
Ron?Bekkerman?and?James?Allan.?2004.?Using?bigrams?
in ? text ? categorization. ?Technical ? Report ? IR?408,?
Center ? of ? Intelligent ? Information ? Retrieval, ? UMass?
Amherst.?
John?Blitzer,?Mark?Dredze,?and?Fernando?Pereira.?2007.?
Biographies, ? bollywood, ? boomboxes ? and ? blenders:?
Domain ? adaptation ? for ? sentiment ? classification. ? In?
Proceedings ? of ? the ? 45th ? Annual ? Meeting ? of ? the ?
Association ? of ? Computational ? Linguistics, ? pp.?
440?447, ? Prague, ? Czech ? Republic. ? Association ? for?
Computational?Linguistics.
Gaston ? Burek ? and ? Dale ? Gerdemann. ? 2009. ? Maximal?
phrases ? based ? analysis ? for ? prototyping ? online?
discussion ? forums ? postings. ? In ?Proceedings ? of ? the?
workshop?on?Adaptation?of?Language?Resources?and ?
Technologies ? to ? New ? Domains ? (AdaptLRTtoND),?
Borovets,?Bulgaria.
Kenneth ? W. ? Church ? and ? William ? A. ? Gale. ? 1995.?
Poisson ? mixtures. ?Natural ? Language ? Engineering,?
1:163?190.
Kushal?Dave,?Steve?Lawrence?and?David?M.?Pennock.?
2003.?Mining?the?peanut?gallery: ?Opinion?extraction?
and ? semantic ? classification ? of ? product ? reviews. ? In?
Proceedings?of?WWW,?pp.?519?528.
Adam ? Funk, ? Yaoyong ? Li, ? Horacio ? Saggion, ? Kalina?
Bontcheva, ? and ? Christian ? Leibold. ? 2008. ? Opinion?
analysis ? for ? business ? intelligence ? applications. ? In?
Proceedings ? of ?First ? International ? Workshop ? on?
Ontology?supported?Business ? Intelligence ?(OBI2008)?
at ? the ?7th ? International ? Semantic ? Web ? Conference ?
(ISWC),?Karlsruhe,?Germany.
Slava?M.?Katz.?1996.??Distribution?of?content?words?and?
phrases ? in ? text ? and ? language ? modelling. ?Natural?
Language?Engineering,?2(1):15?59.
Rasmus?E.?Madsen,?David?Kauchak,?and?Charles?Elkan.?
2005. ? Modeling ? word ? burstiness ? sing ? the ? dirichlet?
distribution.?In?Proceedings?of?the?22nd?International ?
Conference?on?Machine?Learning,?pp.?545?552.
Bo?Pang?and ?Lillian ?Lee. ?2008. ?Opinion ?mining ?and?
sentiment ? analysis. ?Foundations ? and ? Trends ? in ?
Information?Retrieval,?Vol.?2,?Nos.?1?2?(2008)?1?135.
Bo?Pang,?Lillian?Lee.,?and?Shivakumar?Vaithyanathan.?
2002. ? Thumbs ? up? ? Sentiment ? classification ? using?
machine ? learning ? techniques. ? In ?Proceedings ?of ? the?
Conference ? on ? Empirical ? Methods ? in ? Natural ?
Language?Processing?(EMNLP),?pp.?79?86.
Peter ?D. ?Turney. ?2002.?Thumbs?up?or ? thumbs?down??
Semantic ? orientation ? applied ? to ? unsupervised?
classification?of?reviews.?In ?Proceedings?of?the?40th?
Annual ? Meeting ? on ? Association ? for ? Computational ?
Linguistics,?pp.?417?424.
Mikio?Yamamoto?and?Kenneth?W.?Church.?2001.?Using?
suffix ? arrays ? to ? compute ? term ? frequency ? and?
document?frequency?for?all?substrings?in?a?corpus.?In?
Computational??Linguistics,?27(1):1?30.
117
Proceedings of the Twelfth Meeting of the Special Interest Group on Computational Morphology and Phonology (SIGMORPHON2012), pages 17?25,
Montre?al, Canada, June 7, 2012. c?2012 Association for Computational Linguistics
Phrase-Based Approach for Adaptive Tokenization   Jianqiang Ma Dale Gerdemann Department of Linguistics                University of T?bingen  Department of Linguistics                  University of T?bingen Wilhelmstr. 19, T?bingen, 72074, Germany Wilhelmstr. 19, T?bingen, 72074, Germany jma@sfs.uni-tuebingen.de dg@sfs.uni-tuebingen.de     Abstract 
Fast re-training of word segmentation models is required for adapting to new resources or domains in NLP of many Asian languages without word delimiters. The traditional tokenization model is efficient but inaccurate. This paper proposes a phrase-based model that factors sentence tokenization into phrase tokenizations, the dependencies of which are also taken into account. The model has a good OOV recognition ability, which improves the overall performance significantly. The training is a linear time phrase extraction and MLE procedure, while the decoding is via dynamic programming based algorithms. 
1 Introduction In many Asian languages, including Chinese, a sentence is written as a character sequence without word delimiters, thus word segmentation remains a key research topic in language processing for these languages. Although many reports from evaluation tasks present quite positive results, a fundamental problem for real word applications is that most systems heavily depend on the data they were trained on. In order to utilize increasingly available language resources such as user contributed annotations and web lexicon and/or to dynamically construct models for new domains, we have to either frequently re-build models or rely on techniques such as incremental learning and transfer learning, which are unsolved problems themselves.      In the case of frequent model re-building, the most efficient approach is the tokenization model  
 (using the terminology in Huang et al, 2007), in which the re-training is just the update of the dictionary and the segmentation is a greedy string matching procedure using the dictionary and some disambiguation heuristics, e.g. Liang (1986) and Wang et al (1991). An extension of this approach is the dynamic programming search of the most probable word combination on the word lattice, such as Ma (1996) and Sproat et al (1996), which utilize information such as word frequency statistics in a corpus to build the model and are less efficient but more accurate.     However, all the methods mentioned above are mostly based on the knowledge of in-vocabulary words and usually suffer from poor performance, as the out-of-vocabulary words (OOV) rather than segmentation ambiguities turn out to the dominant error source for word segmentation on real corpora (Huang and Zhao, 2007). This fact has led to a shift of the research focus to modeling the roles of individual characters in the word formation process to tackle the OOV problem. Xue (2003) proposes a character classification model, which classifies characters according to their positions in a word using the maximum entropy classifier (Berger et al, 1996). Peng et al (2004) has further extended this model to its sequential form, i.e. sequence labeling, by adopting linear-chain conditional random fields (CRFs, Lafferty et al, 2001). As it is capable of capturing the morphological behaviors of characters, the character classification model has significantly better performance in OOV recognition and overall segmentation accuracy, and has been the state-of-art since its introduction, suggested by the leading performances of systems based on it in recent international Chinese word 
17
segmentation bakeoffs (Emerson, 2005; Levow, 2006; Zhao and Liu, 2010).      The tokenization model has advantages in simplicity and efficiency, as the basic operation in segmentation is string matching with linear time complexity to the sentence length and it only needs a dictionary thus requires no training as in the character classification model, which can easily have millions of features and require hundreds of iterations in the training phase. On the other hand, it has inferior performance, caused by its poor OOV induction ability.      This work proposes a framework called phrase-based tokenization as a generalization of the tokenization model to cope with its deficiencies in OOV recognition, while preserving its advantages of simplicity and efficiency, which are important for adaptive word segmentation. The segmentation hypothesis unit is extended from a word to a phrase, which is a character string of arbitrary length, i.e. combinations of partial and/or complete words. And the statistics of different tokenizations of the same phrase are collected and used for parameters estimation, which leads to a linear time model construction procedure. This extension makes hypothesis units capable of capturing richer context and describing morphological behavior of characters, which improves OOV recognition. Moreover, overlapping hypothesis units can be combined once certain consistency conditions are satisfied, which avoids the unrealistic assumption of independence among the tokenizations of neighboring phrases.      Phrase-based tokenization decomposes the sentence tokenization into phrase tokenizations. We use a graph called phrase tokenization lattice to represent all the hypotheses of phrase tokenization in a given sentence. Under such a formulation, tokenizing a sentence is transformed to the shortest path search problem on the graph, which can be efficiently solved by dynamic programming techniques similar to the Viterbi (1967) algorithm.   2 Phrase-Based Model The hypothesis unit of the tokenization model is the word, i.e. it selects the best word sequence from all the words that can be matched by substrings of the sentence (usually in a greedy manner). Once a word is chosen, the corresponding 
boundaries are determined. This implies that as the characters in a word are always considered as a whole, the morphological behavior of an individual character, e.g. the distribution of its positions in words, is ignored thus makes it impossible to model the word formation process and recognize OOV.       Critical tokenization (Guo, 1997) suggests a method of discovering all and only unambiguous token boundaries (critical points) and generating longest substrings with all inner positions ambiguous (critical fragments) under the assumption of complete dictionary. Then an example-based method using the context can be adopted to disambiguate the tokenization of critical fragments (Hu et al 2004). However, the complete dictionary assumption is not realistic in practice, as the word formation is so dynamic and productive that there is no dictionary that is even close to the complete lexicon. Given the presence of OOV, a word, including a monosyllabic word, in the original dictionary may be a substring, i.e. a partial word, of an OOV. In this case, the critical points found by the dictionary are not guaranteed to be unambiguous.      As the complete dictionary does not exist as a static object, a possible solution is to make a dynamic dictionary, which induces words on the fly. But this will not be discussed in this paper. Instead, we attempt to generalize the tokenization model to work without the complete dictionary. Different from making distinctions of critical fragments and ?non-critical? fragments in critical tokenization, we suggest using phrases to represent potentially ambiguous fragments of sentences in a unified way. We define a phrase as a substring of a sentence, the boundaries of which, depending on the tokenization, may or may not necessarily match word boundaries. The fact that partial words, including single characters, may appear on both ends of a phrase makes it possible to describe ?morphemes in the context? for OOV induction. A consequence of introducing phrase in tokenization is that a manually segmented corpus is needed in order to collect phrases. 2.1   Tokenization Tokenization is the process of separating words or word-like units from sentences or character strings. We can consider sentence tokenization as a mapping from each position in the sentence to a 
18
binary value, which indicates the presence (denoted as #) or the absence of word boundary (denoted as $) at that position. A specific tokenization realization of a sentence can be represented by a list of binary values, which can be generated by the concatenations of its sub-lists. In other words, a tokenization of a given sentence can be represented as the concatenation of the tokenizations of its component phrases.      If we assume that the tokenization of a phrase is independent of other phrases in the same sentence, the sentence tokenization problem is decomposed to smaller phrase tokenization problems, which are unrelated to each other. The independency assumption is not necessarily true but in general is a good approximation. We take this assumption by default, unless there exists evidence that suggests otherwise. In that case, we introduce a method called backward dependency match to fix the problem, which will be discussed in Section 3.3.  2.2   Phrase Tokenization Lattice Informally a phrase tokenization lattice, or lattice in short, is a set of hypothesized tokenization of phrases in the given sentence, which is a compact representation of all the possible tokenization for that sentence. Using the notations in Mohri (2002), we formally define a lattice as a weighted directed graph < V , E >  with a mapping 
 
W : E ! A
, where V  is the set of nodes, E  is the set of edges, and the mapping W  assigns each edge a weight w  from the semiring < A ,!,", 0,1> 1.      For a given sentence S [0... m ] , each node v ! V , denotes a sentence position (the position between a pair of adjacent characters in a untokenized sentence). Each edge e ! E  from node va to node v b , denotes a tokenization of the phrase between the positions defined by va  and v b . And for each edge e , a weight w  is determined by the mapping W , denotes the phrase tokenization probability, the probability of the phrase defined by the two nodes of the edge being tokenized as the tokenization defined by that edge. A path !  in the lattice is a sequence of consecutive edges, i.e. ! = e1e2...ek , where ei  and                                                             1 A semiring defines an algebra system with certain rules to compute path probabilities and the max probabilities from a node to another. See Mohri (2002) for details.   
e
i+1
are connected with a node. The weight for the path !  can be defined as: w ( ! ) = !i=1k w ( ei )                   (1) which is the product of the weights of its component edges. A path from the source node to the sink node, represents a tokenization of the sentence being factored as the concatenation of tokenizations of phrases represented by those edges of on that path.      For example, with some edges being pruned, the lattice for the sentence ????? ?Someone questions him? is shown in Figure 1.  
 Figure 1. A pruned phrase tokenization lattice. Edges are tokenizations of phrases, e.g. e 5  represents tokenizing ?? ?question? into a word and e 7  represents tokenizing?? ?doubt him? into a partial word ?  ?doubt? followed by a word ? ?him?.  
2.3   Tokenization as the Best Path Search After the introduction of the lattice, we formally describe the tokenization (disambiguation) problem as the best path searching on the lattice:  
 
T
!
= argmax
T ! D
w T
( )
               (2) where D  is the set of all paths from the source node to the sink node, and 
 
T
! is the path with the highest weight, which represents the best tokenization of the sentence. Intuitively, we consider the product of phrase tokenization probabilities as the probability of the sentence tokenization that is generated from the concatenation of these phrase tokenizations.      Note that every edge in the lattice is from a node represents an earlier sentence position to a node that represents a later one. In other words, the lattice is acyclic and has a clear topological order. 
19
In this case, the best path can be found using the Viterbi (1967) algorithm efficiently2. 3 Training and Inference Algorithms 
3.1   Model Training In order to use the lattice to tokenize unseen sentences, we first have to build a model that can generate the edges and their associated weight, i.e. the tokenization of all the possible phrases and their corresponding phrase tokenization probability. We do it by collecting all the phrases that have occurred in a training corpus and use maximum likelihood estimation (MLE) to estimate the phrase tokenization probabilities.  The estimation of the probability that a particular phrase A = a
1
a
2
... a
n
 being tokenized as the tokenization T = t
1
t
2
... t
m
 is given in equation (3), where C (? )  represents the empirical count, and the set of all T ' stands for all possible tokenizations of A . To avoid extreme cases in which there is no path at all, techniques such as smoothing can be applied. 
P ( T | A ) =
C ( T , A )
C ( T ' , A )
T '
!
=
C ( T , A )
C ( A )
               (3) 
    The result of the MLE estimation is stored in a data structure called phase tokenization table, from which one can retrieval all the possible tokenizations with their corresponding probabilities for the every phrase that has occurred in the training corpus. With this model, we can construct the lattice, i.e. determine the set of edges E and the mapping function W (defining nodes is trivial) for a given sentence in a simple string matching and table retrieval manner: when a substring of sentence is matched to a stored phrase, an edge is built from the its starting and ending node to represent a tokenization of that phrase, with the weight of the edge equals to the MLE estimation of the stored phrase-tokenization pair.  3.2   Simple Dynamic Programming Once the model is built, we can tokenize a given sentence by the inference on the lattice which represents that sentence. The proposed simple dynamic programming algorithm (Algorithm 1, as                                                             2 More rigid mathematical descriptions of this family of problems and generic algorithms based on semirings are discussed in Mohri (2002) and Huang (2008). 
shown in Figure 2) can be considered as the phrase tokenization lattice version of the evalUtterance algorithm in Venkataraman (2001). The best tokenization of the partial sentence up to a certain position is yielded by the best combination of one previous best tokenization and one of the phrase tokenizations under consideration at the current step.      The upper bound of the time complexity of Algorithm 1 is O ( k n 2 ) , where n  is the sentence length and k is the maximum number of the possible tokenization for a phrase. But in practice, it is neither necessary nor possible (due to data sparseness) to consider phrases of arbitrary length, so we set a constraint of maximum phrase length of about 10, which makes the time complexity de-facto linear.   
!"#$%&'()*+,**-&)."/*0123)&4*5%$#%3))&2#
!"#"#$%&'&(#)**!+",'#*-./#0&1,(&.0*-,23#*4!-5*62.7')***6#0(#07#**689:::;<62&'&3"&83'&$2,
=#'(67."#>*!?@&A#0'&.0*1#".*B#7(."
=#'(-./#0&1,(&.0>*!?@&A#0'&.0*0%33?'("&0C*B#7(."
!"#$%&'()9,
:$%*">D***(.***;***;$*,*************EE*")*7%""#0(*F.'&(&.0*&0*(+#*'#0(#07#
********:$%***#>"?D***(.***9****;$*,****EE*#)*'(,"(&0C*F.'&(&.0*.G*(+#*3,'(*F+",'#
****************$%&'()>68#)"<*************
****************&:***$%&'()***&0***!-*,**
************************+,-)."/'+",.>H#(-.F-./#0&1,(&.04!-I*$%&'()5**
************************+,-)."/'+",.0$&,1>H#(!".2,2&3&(J4!-I*+,-)."/'+",.5
************************(2,&)>=#'(67."#8#<*K*+,-)."/'+",.0$&,1*
************************&:***(2,&)L=#'(67."#8"<*,
********************************=#'(-./#0&1,(&.08"<>+,-)."/'+",.********************************=#'(67."#8"<3(2,&)********************************1'2-0$,".+)&4"53#***************
****************/"9/)
************************<%/3=****EE*&G*(+#*F+",'#*0.(*&0*!-I*#M&'(*(+#*&00#"*3..F
*
=#'(!,(+*N**!,(+*(",7#@*2,7/*G".A*2,7/OF.&0(#"8;<*
6#0(#07#-./#0&1,(&.0N*P.07,(#0,(&.0*.G*(+#*=#'(-./#0&1,&(.0*
#0("&#'*.G*(+#*#@C#'*.0*(+#*=#'(!,(+*4Q+&7+*,"#*F+",'#*(./#0&1,(&.0'5*
>7'.7')****6#0(#07#-./#0&1,(&.0!!  Figure 2. The pseudo code of Algorithm 1.        The key difference to a standard word-lattice based dynamic programming lies in the phrase lattice representation that the algorithm runs on. Instead of representing a word candidate as in Venkataraman (2001), each edge now represents a 
20
tokenization of a phrase defined by two nodes of the edge, which can include full and partial words. The combination of phrase tokenizations may yield new words that are not in the dictionary, i.e. our method can recognize OOVs.     Let us consider a slightly modified version of the lattice in Figure 1. Suppose edge e
5
 =#?$?# does not exist , i.e. the word ?? ?question? is not in the dictionary, and there is new edge e 5!= #?$ that links node 2 and node 3 and represents a partial word. Two of possible tokenizations of the sentence are path p
1
= e
1
e
4
e
6
e
8
and path p 2 = e2 e5!e7 . Note that p 2 recognizes the word??  ?question? by combining two partial words, even though the word itself has not seen before. Of course, this OOV is finally recognized only if a path that can yield it is the best path found by the decoding algorithm.     Once the best path is found, the procedure of mapping it back to segmented words is as follows. The phrase tokenizations represented by the edges of the best path are concatenated, before substituting meta symbols # and $ into white space and empty string, respectively. For example, if  p 2 = e2 e5!e7  is the best path, the concatenation of the phrase tokenizations of the three edges on the path will be #?#?##?$$?#?#, and removal of $ and substitution of # into the white space will further transform it into ?   ?   ??   ? ?Somebody questions him?, which is the final result of the algorithm.  3.3   Compatibility and Backward Dependency Match As mentioned in Section 2, the independency assumption of phrase tokenization is not always true. Considering the example in Figure 1, e
4
and and e
7
are not really compatible, as e
4
 represents a word while e
7
 represents a partial word that expects the suffix of its preceding phrase to form a word with its prefix. To solve this problem, we require that the last (meta) symbol of the preceding tokenization must equal to the first (meta) symbol of the following tokenization in order to concatenate the two. This, however, has the consequence that there may be no valid 
tokenization at all for some positions. As a result, we have to maintain the top k hypotheses and use the k-best path search algorithms instead of 1-best (Mohri, 2002). We adopt the na?ve k-best path search, but it is possible to use more advanced techniques (Huang and Chiang, 2005).      The compatibility problem is just the most salient example of the general problem of variable independency assumptions, which is the "unigram model" of phrase tokenization. A natural extension is a higher order Markov model. But that is inflexible, as it assumes a fixed variable dependency structure (the current variable is always dependent on previous n variables). So we propose a method called backward dependency match, in which we start from the independency assumption, then try to explore the longest sequence of adjacent dependencies that we can reach via string match for a given phrase and its precedent.     To simplify the discussion, we use sequence labeling, or conditional probability notation of the tokenization. A tokenization of the given character sequence (sentence) is represented as a specific label sequence of same length. The label can be those in the standard 4-tag set of word segmentation (Huang and Zhao, 2007) or the #/$ labels indicating the presence or absence of a word boundary after a specific character.      The possible tokenizations of character sequence 
a
1
a
2
a
3
are represented as the probability distribution P ( t
1
t
2
t
3
| a
1
a
2
a
3
) , where t
1
t
2
t
3
 are labels of a
1
a
2
a
3
. If a tokenization hypothesis of 
a
1
a
2
a
3
decomposes its tokenization into the concatenation of the tokenization of a
1
a
2
 and the tokenization of a
3
, this factorization can be expressed as 
P (t
1
t
2
| a
1
a
2
) ! P (t
3
| a
3
)
, as shown in Figure 3a. For a specific assignment 
< a
1
a
2
a
3
; t
1
t
2
t
3
>
, if we find that 
< a
2
a
3
>
 can be tokenized as 
< t
2
t
3
>
, it suggests that 
t
3
 may be dependent on 
a
2
and t
2
 as well, so we update the second part of the factorization (at least for this assignment) to: 
P(t
3
| a
3
; a
2
t
2
)
, which can be estimated as:  
P ( t
3
| a
3
; a
2
t
2
) =
C ( a
2
a
3
t
2
t
3
)
C ( a
2
a
3
t
2
t
3
)
t
3
!
                     (4)  
21
In this case, the factorization of the tokenization 
P ( t
1
t
2
t
3
| a
1
a
2
a
3
)  is 
P (t
1
t
2
| a
1
a
2
)! P (t
3
| a
3
; a
2
t
2
)
, as shown in Figure 3b.    
  Figure 3a. The factorization of P ( t
1
t
2
t
3
| a
1
a
2
a
3
)  into
P (t
1
t
2
| a
1
a
2
)! P (t
3
| a
3
)
.   
  Figure 3b. The factorization of P ( t
1
t
2
t
3
| a
1
a
2
a
3
)  into
P (t
1
t
2
| a
1
a
2
)! P ( t
3
| a
3
; a
2
t
2
)
. Note that in the 2nd factor, in addition to a3, a2 and t2 are also observed variables and all of them are treated as a unit (shown by the L-shape). The shadowed parts (a2 and t2)  represent the matched items.      Algorithm 2 is based on the k-best search algorithm, which calls the backward dependency match after a successful compatibility check, and match as far as possible to get the largest probability of each tokenization hypothesis. In 
extreme cases, where no tokenization hypothesis survives the compatibility check, the algorithm backs off to Algorithm 1. 4 Experiments We use the training and testing sets from the second international Chinese word segmentation bakeoff (Emerson, 2005), which are freely available and most widely used in evaluations. There are two corpora in simplified Chinese provided by Peking University (PKU) and Microsoft Research (MSR) and two corpora in traditional Chinese provided by Academic Sinica (AS) and the City University of Hong Kong (CityU). The experiments are conducted in a closed-test manner, in which no extra recourse other than the training corpora is used. We use the same criteria and the official script for evaluation from the bakeoff, which measure the overall segmentation performance in terms of F-scores, and the OOV recognition capacity in terms of Roov.     Precision is defined as the number of correctly segmented words divided by the total number of words in the segmentation result, where the correctness of the segmented words is determined by matching the segmentation with the gold standard test set. Recall is defined as the number of correctly segmented words divided by the total number of words in the gold standard test set. The evenly-weighted F-score is calculated by:  F = 2 ! p ! r / ( p + r )               (5) Roov is the recall of all the OOV words. And Riv is the recall of words that have occurred in the training corpus. The evaluation in this experiment is done automatically using the script provided with the second bakeoffs data.     We have implemented both Algorithm 1 and Algorithm 2 in Python with some simplifications, e.g. only processing phrase up to the length of 10 characters, ignoring several important details such as pruning. The performances are compared with the baseline algorithm maximum matching (MM), described in Wang et al (1991), and the best bakeoff results. The F-score, Roov and Riv are summarized in Table 1, Table 2, and Table 3, respectively.     All the algorithms have quite similar recall for the in-vocabulary words (Riv), but their Roov vary 
22
greatly, which leads to the differences in F-score. In general both Algorithm 1 and Algorithm 2 improves OOV Recall significantly, compared with the baseline algorithm, maximum matching, which has barely any OOV recognition capacity. This confirms the effectiveness of the proposed phrase-based model in modeling morphological behaviors of characters. Moreover, Algorithm 2 works consistently better than Algorithm 1, which suggests the usefulness of its strategy of dealing with dependencies among phrase tokenizations.     Besides, the proposed method has the linear training and testing (when setting a maximum phrase length) time complexity, while the training complexity of CRF is the proportional to the feature numbers, which are often over millions. Even with current prototype, our method takes only minutes to build the model, in contrast with several hours that CRF segmenter needs to train the model for the same corpus on the same machine.       Admittedly, our model still underperforms the best systems in the bakeoff. This may be resulted from that 1) our system is still a prototype that ignores many minor issues and lack optimization and 2) as a generative model, our model may suffer more from the data sparseness problem, compared with discriminative models, such as CRF.     As mentioned earlier, the OOV recognition is the dominant factor that influences the overall accuracy. Different from the mechanism of tokenization combination in our approach, state-of-art systems such as those based on MaxEnt or CRF, achieve OOV recognition basically in the same way as in-dictionary word recognition. The segmentation is modeled as assigning labels to characters. And the probability of the label assignment for a character token is mostly determined by its features, which are usually local contexts in the form of character co-occurrences.       There are many other OOV recognition methods proposed in literature before the rise of machine learning in the field. For example, the Sproat et al (1996) system can successfully recognize OOVs of strong patterns, such as Chinese personal names, transliterations, using finite-state techniques.  Another typical example is Ma and Chen (2003), which proposed context free grammar like rules together with a recursive bottom-up merge algorithm that merges possible morphemes after an initial segmentation using maximum matching. It 
would be fairer to compare the OOV recognition performance of our approach with these methods, rather than maximum matching. But most earlier works are not evaluated on standard bake-off corpora and the implementations are not openly available, so it is difficult to make direct comparisons.  F-score As CityU MSR PKU 
Best Bakeoff 0.952 0.943 0.964 0.950 
Algorithm 2 0.919 0.911 0.946 0.912 
Algorithm 1 0.897 0.888 0.922 0.890 
MM 0.882 0.833 0.933 0.869 
Table 1. The F-score over the bakeoff-2 data.  Roov AS CityU MSR PKU 
Best Bakeoff 0.696 0.698 0.717 0.636 
Algorithm 2 0.440 0.489 0.429 0.434 
Algorithm 1 0.329 0.367 0.411 0.416 
MM 0.004 0.000 0.000 0.059 Table 2. The Roov over the bakeoff-2 data.  
Riv AS CityU MSR PKU 
Best Bakeoff 0.963 0.961 0.968 0.972 
Algorithm 2 0.961 0.961 0.970 0.951 
Algorithm 1 0.955 0.940 0.950 0.940 
MM 0.950 0.952 0.981 0.956 Table 3. The Riv over the bakeoff-2 data. 
5 Conclusion In this paper, we have presented the phrase-based tokenization for adaptive word segmentation. The proposed model is efficient in both training and decoding, which is desirable for fast model re-construction. It generalizes the traditional 
23
tokenization model by considering the phrase instead of the word as the segmentation hypothesis unit, which is capable of describing ?morphemes in the context? and improves the OOV recognition performance significantly. Our approach decomposes sentence tokenization into phrase tokenizations. The final tokenization of the sentence is determined by finding the best combination of the tokenizations of phrases that cover the whole sentence. The tokenization hypotheses of a sentence are represented by a weighed directed acyclic graph called phrase tokenization lattice. Using this formalism, the sentence tokenization problem becomes a shortest path search problem on the graph.     In our model, one only needs to estimate the phrase tokenization probabilities in order to segment new sentences. The training is thus a linear time phrase extraction and maximum likelihood estimation procedure. We adopted a Viterbi-style dynamic programming algorithm to segment unseen sentences using the lattice. We also proposed a method called backward dependency match to model the dependencies of adjacent phrases to overcome the limitations of the assumption that tokenizations of neighboring phrases is independent. The experiment showed the effectiveness of the proposed phrase-based model in recognizing out-of-vocabulary words and its superior overall performance compared with the traditional tokenization model. It has both the efficiency of the tokenization model and the high performance of the character classification model.        One possible extension of the proposed model is to apply re-ranking techniques (Collins and Koo, 2005) to the k-best list generated by Algorithm 2. A second improvement would be to combine our model with other models in a log linear way as in Jiang et al (2008). Since phrase-based tokenization is a model that can be accompanied by different training algorithms, it is also interesting to see whether discriminative training can lead to better performance. Acknowledgments The research leading to these results has received funding from the European Commission?s 7th Framework Program under grant agreement n? 238405 (CLARA). 
References  Adam Berger, Stephen Della Pietra, and Vincent Della Pietra. 1992. A Maximum Entropy Approach to Natural Language Processing. 1996. Computational Linguistics, 22(1): 39-71 Michael Collins and Terry Koo. 2005. Discriminative Reranking for Natural Language Parsing. Computational Linguistics, 31(1):25-69. Thomas Emerson. 2005. The second international Chi- nese word segmentation bakeoff. In Proceedings of Forth SIGHAN Workshop on Chinese Language Processing. Jeju Island, Korea. Jin Guo. 1997. Critical tokenization and its properties. Computational Linguistics, 23(4): 569-596 Qinan Hu, Haihua Pan, and Chunyu Kit. 2004. An example-based study on Chinese word segmentation using critical fragments. In Proceedings of IJCNLP-2004. Hainan Island, China Changning Huang and Hai Zhao. 2007. Chinese Word Segmentation: a Decade Review. Journal of Chinese Information Processing, 21(3): 8-20 Chu-Ren Huang, Petr Simon, Shu-Kai Hsieh, and Laurent Pr?vot. Rethinking Chinese word segmentation: tokenization, character classification, or wordbreak identification. In Proceedings of ACL-2007. Prague, Czech Liang Huang. 2008. Advanced dynamic programming in semiring and hypergraph frameworks. In Proceedings of COLING 2008. Manchester, UK. Liang Huang and David Chiang. 2005. Better k-best parsing. In Proceedings of the Ninth International Workshop on Parsing Technology. Vancouver, Canada Wenbin Jiang, Liang Huang, Qun Liu, Yajuan Lu. 2008. A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging. In Proceedings of ACL 2008: HLT. Columbus, USA John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of ICML 2001. Williamstown, MA, USA Gina-Anne Levow. 2006. The third international Chinese language processing bakeoff: Word segmentation and named entity recognition. In Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing. Sydney, Australia 
24
Nanyuan Liang. 1986. On computer automatic word segmentation of written Chinese. Journal of Chinese Information Processing, 1(1). Wei-Yun Ma and Keh-Jiann Chen. 2003. A bottom-up merging algorithm for Chinese unknown word extraction. In Proceedings of the second SIGHAN workshop on Chinese language processing. Sapporo, Japan Yan Ma. 1996. The study and realization of an evaluation-based automatic segmentation system. In Changning Huang and Ying Xia, editors, Essays in Language Information Processing. Tsinghua University Press, Beijing, China. Mehryar Mohri. 2002. Semiring frameworks and algorithms for shortest-distance problems. Journal of Automata, Languages and Combinatorics, 7(3):321?350. Fuchun Peng, Fangfang Feng, and Andrew McCallum. 2004. Chinese segmentation and new word detection using conditional random fields. In Proceedings of COLING. Stroudsburg, PA, USA. Richard Sproat, Chilin Shih, William Gale, and Nancy Chang. 1996. A stochastic finite-state word-segmentation algorithm for Chinese. Computational Linguistics, 22(3):377-404. Anand Venkataraman. 2001. A Statistical Model for Word Discovery in Transcribed Speech. Computational Linguistics, 27(3): 351-372 Andrew Viterbi (1967). Error bounds for convolutional codes and an asymptotically optimum decoding algorithm. IEEE Transactions on Information Theory, 13 (2): 260?269. Xiaolong Wang, Kaizhu Wang, and Xiaohua Bai. 1991. Separating syllables and characters into words in natural language understanding. Journal of Chinese Information Processing, 5(3):48-58. Nianwen Xue. 2003. Chinese Word Segmentation as Characater Tagging. Computational Linguistics and Chinese Language Processing, 8(1): 29-48 Hongmei Zhao and Qun Liu. 2010. The CIPS-SIGHAN CLP 2010 Chinese Word Segmentation Bakeoff. In Proceedings of the First CPS-SIGHAN Joint Conference on Chinese Language Processing. Beijing, China.  
25
Proceedings of the 10th International Workshop on Finite State Methods and Natural Language Processing, pages 10?19,
Donostia?San Sebastia?n, July 23?25, 2012. c?2012 Association for Computational Linguistics
Practical Finite State Optimality Theory
Dale Gerdemann
University of Tu?bingen
dg@sfs.nphil.uni-tuebingen.de
Mans Hulden
University of the Basque Country
IXA Group
IKERBASQUE, Basque Foundation for Science
mhulden@email.arizona.edu
Abstract
Previous work for encoding Optimality The-
ory grammars as finite-state transducers has
included two prominent approaches: the so-
called ?counting? method where constraint vi-
olations are counted and filtered out to some
set limit of approximability in a finite-state
system, and the ?matching? method, where
constraint violations in alternative strings are
matched through violation alignment in order
to remove suboptimal candidates. In this pa-
per we extend the matching approach to show
how not only markedness constraints, but also
faithfulness constraints and the interaction of
the two types of constraints can be captured
by the matching method. This often produces
exact and small FST representations for OT
grammars which we illustrate with two practi-
cal example grammars. We also provide a new
proof of nonregularity of simple OT gram-
mars.
1 Introduction
The possibility of representing Optimality Theory
(OT) grammars (Prince and Smolensky, 1993) as
computational models and finite-state transducers,
in particular, has been widely studied since the in-
ception of the theory itself. In particular, construct-
ing an OT grammar step-by-step as the composition
of a set of transducers, akin to rewrite rule com-
position in (Kaplan and Kay, 1994), has offered
the attractive possibility of simultaneously model-
ing OT parsing and generation as a natural conse-
quence of the bidirectionality of finite-state trans-
ducers. Two main approaches have received atten-
tion as practical options for implementing OT with
finite-state transducers: that of Karttunen (1998)
and Gerdemann and van Noord (2000).1 Both ap-
1Earlier finite-state approaches do exist, see e.g. Ellison
(1994) and Hammond (1997).
proaches model constraint interaction by construct-
ing a GEN-transducer, which is subsequently com-
posed with filtering transducers that mark violations
of constraints, and remove suboptimal candidates?
candidates that have received more violation marks
than the optimal candidate, with the general tem-
plate:
Grammar = Gen .o. MarkC1 .o. FilterC1 ...
MarkCN .o. FilterCN
In Karttunen?s system, auxiliary ?counting? trans-
ducers are created that first remove candidates with
maximally k violation marks for some fixed k, then
k?1, and so on, until nothing can be removed with-
out emptying the candidate set, using a finite-state
operation called priority union. Gerdemann and van
Noord (2000) present a similar system that they call
a ?matching? approach, but which does not rely on
fixing a maximal number of distinguishable viola-
tions k. The matching method is a procedure by
which we can in many cases (though not always)
distinguish between infinitely many violations in a
finite-state system?something that is not possible
when encoding OT by the alternative approach of
counting violations.
In this paper our primary purpose is to both ex-
tend and simplify this ?matching? method. We
will include interaction of both markedness and
faithfulness constraints (MAX, DEP, and IDENT
violations)?going beyond both Karttunen (1998)
and Gerdemann and van Noord (2000), where only
markedness constraints were modeled. We shall also
clarify the notation and markup used in the matching
approach as well as present a set of generic trans-
ducer templates for EVAL by which modeling vary-
ing OT grammars becomes a simple matter of mod-
ifying the necessary constraint transducers and or-
dering them correctly in a series of compositions.
10
We will first give a detailed explanation of the
?matching? approach in section 2?our encoding,
notation, and tools differ somewhat from that of
Gerdemann and van Noord (2000), although the core
techniques are essentially alike. This is followed by
an illustration of our encoding and method through
a standard OT grammar example in section 3. In
that section we also give examples of debugging OT
grammars using standard finite state calculus meth-
ods. In section 4 we also present an alternate en-
coding of an OT account of prosody in Karttunen
(2006) illustrating devices where GEN is assumed to
add metrical and stress markup in addition to chang-
ing, inserting, or deleting segments. We also com-
pare this grammar to both a non-OT grammar and an
OT grammar of the same phenomenon described in
Karttunen (2006). In section 5, we conclude with a
brief discussion about the limitations of FST-based
OT grammars in light of the method developed in
this paper, as well as show a new proof of nonregu-
larity of some very simple OT constraint systems.
1.1 Notation
All the examples discussed are implemented with
the finite-state toolkit foma (Hulden, 2009b). The
regular expressions are also compilable with the Xe-
rox tools (Beesley and Karttunen, 2003), although
some of the tests of properties of finite-state trans-
ducers, crucial for debugging, are unavailable. The
regular expression formalism used is summarized in
table 1.
2 OT evaluation with matching
In order to clarify the main method used in this pa-
per to model OT systems, we will briefly recapitu-
late the ?matching? approach to filter out suboptimal
candidates, or candidates with more violation marks
in a string representation, developed in Gerdemann
and van Noord (2000).2
2.1 Worsening
The fundamental technique behind the finite-state
matching approach to OT is a device which we call
?worsening?, used to filter out strings from a trans-
ducer containing more occurrences of some desig-
nated special symbol s (e.g. a violation marker),
2Also discussed in Ja?ger (2002).
AB Concatenation
A|B Union
?A Complement
? Any symbol in alphabet
% Escape symbol
[ and ] Grouping brackets
A:B Cross product
T.l Output projection of T
A -> B Rewrite A as B
A (->) B Optionally rewrite A as B
|| C D Context specifier
[..] -> A Insert one instance of A
A -> B ... C Insert B and C around A
.#. End or beginning of string
Table 1: Regular expression notation in foma.
than some other candidate string in the same pool
of strings. This method of transducer manipulation
is perhaps best illustrated through a self-contained
example.
Consider a simple morphological analyzer en-
coded as an FST, say of English, that only
adds morpheme-boundaries?+-symbols?to input
words, perhaps consulting a dictionary of affixes and
stems. Some of the mappings of such a transducer
could be ambiguous: for example, the words decon-
struction or incorporate could be broken down in
two ways by such a morpheme analyzer:
Suppose our task was now to remove alternate
morpheme breakdowns from the transducer so that,
if an analysis with a smaller number of morphemes
was available for any word, a longer analysis would
not be produced. In effect, deconstruction should
only map to deconstruct+ion, since the other al-
ternative has one more morpheme boundary. The
worsening trick is based on the idea that we can
use the existing set of words from the output side
of the morphology, add at least one morpheme
boundary to all of them, and use the resulting set
of words to filter out longer ?candidates? from the
original morphology. For example, one way of
adding a +-symbol to de+construction produces
11
de+construct+ion, which coincides with the orig-
inal output in the morphology, and can now be used
to knock out this suboptimal division. This process
can be captured through:
AddBoundary = [?* 0:%+ ?*]+;
Worsen = Morphology .o. AddBoundary;
Shortest = Morphology .o. ?Worsen.l;
the effect of which is illustrated for the word de-
construction in figure 1. Here, AddBoundary is
a transducer that adds at least one +-symbol to the
input. The Worsen transducer is simply the origi-
nal transducer composed with the AddBoundary
transducer. The Shortest morphology is then
constructed by extracting the output projection of
Worsen, and composing its negation with the orig-
inal morphology.
Figure 1: Illustration of a worsening filter for morpheme
boundaries.
2.2 Worsening in OT
The above ?worsening? maneuver is what the
?matching? approach to model OT syllabification is
based upon. Evaluation of competing candidates
with regard to a single OT constraint can be per-
formed in the same manner. This, of course, pre-
supposes that we are using transducers to mark con-
straint violations in input strings, say by the sym-
bol *. Gerdemann and van Noord (2000) illustrate
this by constructing a GEN-transducer that syllabi-
fies words,3 and another set of transducers that mark
3Although using a much more complex set of markup sym-
bols than here.
violations of some constraint. Then, having a con-
straint, NOCODA, implemented as a transducer that
adds violation marks when syllables end in conso-
nants, we can achieve the following sequence of
markup by composition of GEN and NOCODA, for
a particular example input bebop:
The above transducers could be implemented very
simply, by epenthesis replacement rules:
# Insert periods arbitrarily inside words
Gen = [..] (->) %. || \.#. _ \.#. ;
# Insert *-marks after C . or C .#.
NoCoda = [..] -> %* || C+ [%. | .#.] _ ;
Naturally, at this point in the composition
chain we would like to filter out the suboptimal
candidates?that is, the ones with fewer violation
marks, then remove the marks, and continue with
the next constraint, until all constraints have been
evaluated. The problem of filtering out the subopti-
mal candidates is now analogous to the ?worsening?
scenario above: we can create a ?worsening?-filter
automaton by adding violation marks to the entire
set of candidates. In this example, the candidate
be.bop? would produce a worse candidate be?.bop?,
which (disregarding for the moment syllable bound-
ary marks and the exact position of the violation) can
be used to filter out the suboptimal beb?.op?.
3 An OT grammar with faithfulness and
markedness constraints
As previous work has been limited to working with
only markedness constraints as well as a some-
what impoverished GEN?one that only syllabifies
words?our first task when approaching a more
complete finite-state methodology of OT needs to
address this point. In keeping with the ?richness
of the base?-concept of OT, we require a suitable
12
GEN to be able to perform arbitrary deletions (eli-
sions), insertions (epentheses), and changes to the
input. A GEN-FST that only performs this task
(maps ?? ? ??) on input strings is obviously fairly
easy to construct. However, we need to do more than
this: we also need to keep track of which parts of
the input have been modified by GEN in any way
to later be able to pinpoint and mark faithfulness
violations?places where GEN has manipulated the
input?through an FST.
3.1 Encoding of GEN
Perhaps the simplest possible encoding that meets
the above criteria is to have GEN not only change
the input, but also mark each segment in its output
with a marker whereby we can later distinguish how
the input was changed. To do so, we perform the
following markup:
? Every surface segment (output) is surrounded
by brackets [ . . . ].
? Every input segment that was manipulated by
GEN is surrounded by parentheses ( . . . ).
For example, given the input a, GEN would pro-
duce an infinite number of outputs, and among them:
[a] GEN did nothing
(a)[] GEN deleted the a
(a)[e] GEN changed the a to e
()[d](a)[i] GEN inserted a d and changed a to i
...
This type of generic GEN can be defined through:
Gen = S -> %( ... %) %[ (S) %] ,,
S -> %[ ... %] ,,
[..] (->) [%( %) %[ S %]]* ;
assuming here that S represents the set of segments
available.
3.2 Evaluation of faithfulness and markedness
constraints
As an illustrative grammar, let us consider a standard
OT example of word-final obstruent devoicing?as
in Dutch or German?achieved through the interac-
tion of faithfulness and markedness constraints. The
constraints model the fact that underlyingly voiced
obstruents surface as devoiced in word-final posi-
tion, as in pad ? pat. A set of core constraints to
illustrate this include:
? ?VF: a markedness constraint that disallows fi-
nal voiced obstruents.
? IDENTV: a faithfulness constraint that militates
against change in voicing.
? VOP: a markedness constraint against voiced
obstruents in general.
The interaction of these constraints to achieve de-
voicing can be illustrated by the following tableau.4
bed ?VF IDENTV VOP
+ bet * *
pet **!
bed *! **
ped *! * *
The tableau above represents a kind of shorthand
often given in the linguistic literature where, for the
sake of conciseness, higher-ranked faithfulness con-
straints are omitted. For example, there is nothing
preventing the candidate bede to rank equally with
bet, were it not for an implicit high-ranked DEP-
constraint disallowing epenthesis. As we are build-
ing a complete computational model with an unre-
stricted GEN, and no implicit assumptions, we need
to add a few constraints not normally given when
arguing about OT models. These include:
? DEP: a faithfulness constraint against epenthe-
sis.
? MAX: a faithfulness constraint against dele-
tion.
? IDENTPL: a faithfulness constraint against
changes in place of articulation of segments.
This is crucial to avoid e.g. bat or bap being
equally ranked with bet in the above example.5
4The illustration roughly follows (Kager, 1999), p. 42.
5Note that a generic higher-ranked IDENT will not do, be-
cause then we would never get the desired devoicing in the first
place.
13
Including these constraints explicitly allows us to
rule out unwanted candidates that may otherwise
rank equal with the candidate where word-final ob-
struents are devoiced, as illustrated in the following:
bed DE
P
M
AX
ID
EN
TP
L
? V
F
ID
EN
TV
VO
P
+ bet * *
pet **!
bed *! **
ped *! * *
bat *! * *
bep *! * *
be *! *
bede *! **
Once we have settled for the representation
of GEN, the basic faithfulness constraint markup
transducers?whose job is to insert asterisks wher-
ever violations occur?can be defined as follows:
Dep = [..] -> {*} || %( %) _ ;
Max = [..] -> {*} || %[ %] _ ;
Ident = [..] -> {*} || %( S %) %[ S %] _ ;
That is, DEP inserts a *-symbol after ( )-
sequences, which is how GEN marks epenthesis.
Likewise, MAX-violations are identified by the se-
quence [ ], and IDENT-violations by a parenthesized
segment followed by a bracketed segment. To define
the remaining markup transducers, we shall take ad-
vantage of some auxiliary template definitions, de-
fined as functions:
def Surf(X) [X .o. [0:%[ ? 0:%]]*].l/
[ %( (S) %) | %[ %] ];
def Change(X,Y) [%( X %) %[ Y %]];
Here, Surf(X) in effect changes the language X
so that it can match every possible surface encod-
ing produced by GEN; for example, a surface se-
quence ab may look like [a][b], or [a](a)[b], etc.,
since it may spring from various different underly-
ing forms. This is a useful auxiliary definition that
will serve to identify markedness violations. Like-
wise Change(X,Y) reflects the GEN representa-
tion of changing a segment X to Y needed to con-
cisely identify changed segments. Using the above
we may now define the remaining violation markups
needed.
CVOI = [b|d|g];
Voiced = [b|d|g|V];
Unvoiced = [p|t|k];
define VC Change(Voiced,Unvoiced) |
Change(Unvoiced,Voiced);
define Place Change(p,?-b)|Change(t,?-d)|
Change(k,?-g)|Change(b,?-p)|
Change(d,?-t)|Change(g,?-k)|
Change(a,?)|Change(e,?)|
Change(i,?)|Change(o,?)|
Change(u,?);
VF = [..] -> {*} || Surf(CVOI) _ .#. ;
IdentV = [..] -> {*} || VC _ ;
VOP = [..] -> {*} || Surf(CVOI) _ ;
IdentPl = [..] -> {*} || Place _ ;
The final remaining element for a complete imple-
mentation concerns the question of ?worsening? and
its introduction into a chain of transducer composi-
tion. To this end, we include a few more definitions:
AddViol = [?* 0:%* ?*]+;
Worsen = [Gen.i .o. Gen]/%* .o. AddViol;
def Eval(X) X .o. ?[X .o. Worsen].l .o. %*->0;
Cleanup = %[|%] -> 0 .o. %( \%)* %) -> 0;
Here, AddViol is the basic worsening method
discussed above whereby at least one violation mark
is added. However, because GEN adds markup to
the underlying forms, we need to be a bit more flex-
ible in our worsening procedure when matching up
violations. It may be the case that two different com-
peting surface forms have the same underlying form,
but the violation marks will not align correctly be-
cause of interfering brackets. Given two competing
candidates with a different number of violations, for
example (a)[b]* and [a], we would like the latter to
match the former after adding a violation mark since
they both originate in the same underlying form a.
The way to achieve this is to undo the effect of GEN,
and then redo GEN in every possible configuration
before adding the violation marks. The transducer
Worsen, above, does this by a composition of the
inverse GEN, followed by GEN, ignoring already ex-
isting violations. For the above example, this leads
to representations such as:
[a] Gen.i? a Gen? (a)[b] AddViol? (a)[b]*.
14
0p t k a e i o u 
1b d g 
2g:0 
4
d:0 5
b:0 
p t k a e i o u 
b d g g:0 
d:0 
b:0 
3
0:k 
0:t 
0:p 
Figure 2: OT grammar for devoicing compiled into an
FST.
We also define a Cleanup transducer that re-
moves brackets and parts of the underlying form.
Now we are ready to compile the entire system
into an FST. To apply only GEN and the first con-
straint, for example, we can calculate:
Eval(Gen .o. Dep) .o. Cleanup;
and likewise the entire grammar can be calculated
by:
Eval(Eval(Eval(Eval(Eval(Eval(
Gen .o. Dep) .o. Max) .o. IdentPl) .o.
VF) .o. IdentV) .o. VOP) .o. Cleanup;
This yields an FST of 6 states and 31 transitions
(see figure 2)?it can be ascertained that the FST
indeed does represent a relation where word-final
voiced obstruents are always devoiced.
3.3 Permutation of violations
As mentioned in Gerdemann and van Noord
(2000), there is an additional complication with the
?worsening?-approach. It is not always the case that
in the pool of competing candidates, the violation
markers line up, which is a prerequisite for filtering
out suboptimal ones by adding violations?although
in the above grammar the violations do line up cor-
rectly. However, for the vast majority of OT gram-
mars, this can be remedied by inserting a violation-
permuting transducer that moves violations markers
around before worsening, to attempt to produce a
correct alignment. Such a permuting transducer can
be defined as in figure 3.
If the need for permutation arises, repeated per-
mutations can be included as many times as war-
ranted in the definition of Worsen:
Figure 3: Violation permutation transducer.
Permute = [%*:0 ?* 0:%*|0:%* ?* %*:0]*/?;
Worsen = [Gen.i .o. Gen]/%* .o.
Permute .o. ... .o. Permute .o.
AddViol;
Knowing how many permutations are necessary
for the transducer to be able to distinguish between
any number of violations in a candidate pool is pos-
sible as follows: we can can calculate for some con-
straint ConsN in a sequence of constraints,
Eval(Eval(Gen .o. Cons1) ... .o. ConsN) .o.
ConsN .o. \%* -> 0;
Now, this yields a transducer that maps every un-
derlying form to n asterisks, n being the number
of violations with respect to ConsN in the candi-
dates that have successfully survived ConsN. If this
transducer represents a function (is single-valued),
then we know that two candidates with a different
number of violations have not survived ConsN, and
that the worsening yielded the correct answer. Since
the question of transducer functionality is known
to be decidable (Blattner and Head, 1977), and
an efficient algorithm is given in Hulden (2009a),
which is included in foma (with the command test
functional) we can address this question by cal-
culating the above for each constraint, if necessary,
and then permute the violation markers until the
above transducer is functional.
3.4 Equivalence testing
In many cases, the purpose of an OT grammar is
to capture accurately some linguistic phenomenon
through the interaction of constraints rather than by
other formalisms. However, as has been noted by
15
Karttunen (2006), among others, OT constraint de-
bugging is an arduous task due to the sheer num-
ber of unforeseen candidates. One of the advantages
in encoding an OT grammar through the worsening
approach is that we can produce an exact represen-
tation of the grammar, which is not an approxima-
tion bounded by the number of constraint violations
it can distinguish (as in Karttunen (1998)), or by the
length of strings it can handle. This allows us to
formally calculate, among other things, the equiva-
lence of an OT grammar represented as an FST and
some other transducer. For example, in the above
grammar, the intention was to model end-of-word
obstruent devoicing through optimality constraints.
Another way to model the same thing would be to
compile the replacement rule:
Rule = b -> p, d -> t, g -> k || _ .#. ;
The transducer resulting from this is shown in fig-
ure 4.
0
@ k p t 
1b d g 
2b:p d:t g:k
@ k p t 
b d g 
b:p d:t g:k
Figure 4: Devoicing transducer compiled through a rule.
As is seen, the OT transducer (figure 2) and
the rule transducer (figure 4) are not structurally
identical. However, both transducers represent a
function?i.e. for any given input, there is always
a unique winning candidate. Although transducer
equivalence is not testable by algorithm in the gen-
eral case, it is decidable in the case where one of
two transducers is functional. If this is the case it is
sufficient to test that domain(?1) = domain(?2) and
that ??12 ? ?1 represents identity relations only. As
an algorithm to decide if a transducer is an identity
transducer is also included in foma, it can be used to
ascertain that the two above transducers are in fact
identical, and that the linguistic generalization cap-
tured by the OT constraints is correct:
regex Rule.i .o. Grammar;
test identity
which indeed returns TRUE. For a small grammar,
such as the devoicing grammar, determining the cor-
rectness of the result by other means is certainly fea-
sible. However, for more complex systems the abil-
ity to test for equivalence becomes a valuable tool in
analyzing constraint systems.
4 Variations on GEN: an OT grammar of
stress assignment
Most OT grammars that deal with phonological phe-
nomena with faithfulness and markedness gram-
mars are implementable through the approach given
above, with minor variations according to what spe-
cific constraints are used. In other domains, how-
ever, in may be the case that GEN, as described
above, needs modification. A case in point are gram-
mars that mark prosody or perform syllabification
that often take advantage of only markedness con-
straints. In such cases, there is often no need for
GEN to insert, change, and delete material if all
faithfulness constraints are assumed to outrank all
markedness constraints. Or alternatively, if the OT
grammar is assumed to operate on a different stra-
tum where no faithfulness constraints are present.
However, GEN still needs to insert material into
strings, such as stress marks or syllable boundaries.
To test the approach with a larger ?real-
world? grammar we have reimplemented a Finnish
stress assignment grammar, originally implemented
through the counting approach of Karttunen (1998)
in Karttunen (2006), following a description in
Kiparsky (2003). The grammar itself contains nine
constraints, and is intended to give a complete ac-
count of stress placement in Finnish words. Without
going into a line-by-line analysis of the grammar,
the crucial main differences in this implementation
to that of the previous sections are:
? GEN only inserts symbols ( ) ? and ?
to mark feet and stress
? Violations need to be permuted in Worsen to
yield an exact representation
? GEN syllabifies words correctly through a re-
placement rule (no constraints are given in the
grammar to model syllabification; this is as-
sumed to be already performed)
16
kainostelijat -> (ka?i.nos).(te?.li).jat
kalastelemme -> (ka?.las).te.(le?m.me)
kalasteleminen -> *(ka?.las).te.(le?.mi).nen
kalastelet -> (ka?.las).(te?.let)
kuningas -> (ku?.nin).gas
strukturalismi -> (stru?k.tu).ra.(li?s.mi)
ergonomia -> (e?r.go).(no?.mi).a
matematiikka -> (ma?.te).ma.(ti?ik.ka)
Figure 5: Example outputs of matching implementation
of Finnish OT.
Compiling the entire grammar through the same
procedure as above outputs a transducer with 134
states, and produces the same predictions as Kart-
tunen?s counting OT grammar.6 As opposed to the
previous devoicing grammar, compiling the Finnish
prosody grammar requires permutation of the viola-
tion markers, although only one constraint requires
it (STRESS-TO-WEIGHT, and in that case, compos-
ing Worsen with one round of permutation is suffi-
cient for convergence).
Unlike the counting approach, the current ap-
proach confers two significant advantages. The first
is that we can compile the entire grammar into an
FST that does not restrict the inputs in any way. That
is, the final product is a stand-alone transducer that
accepts as input any sequence of any length of sym-
bols in the Finnish alphabet, and produces an output
where the sequence is syllabified, marked with feet,
and primary and secondary stress placement (see fig-
ure 5). The counting method, in order to compile at
all, requires that the set of inputs be fixed to some
very limited set of words, and that the maximum
number of distinguishable violations (and indirectly
word length) be fixed to some k.7 The second ad-
vantage is that, as mentioned before, we are able to
formally compare the OT grammar (because it is not
an approximation), to a rule-based grammar (FST)
that purports to capture the same phenomena. For
example, Karttunen (2006), apart from the count-
ing OT implementation, also provides a rule-based
account of Finnish stress, which he discovers to be
distinct from an OT account by finding two words
6Including replicating errors in Kiparsky?s OT analysis dis-
covered by Karttunen, as seen in figure 5.
7Also, compiling the grammar is reasonably quick: 7.04s on
a 2.8MHz Intel Core 2, vs. 2.1s for a rewrite-rule-based account
of the same phenomena.
where their respective predictions differ. However,
by virtue of having an exact transducer, we can for-
mally analyze the OT account together with the rule-
based account to see if they differ in their predictions
for any input, without having to first intuit a differ-
ing example:
regex RuleGrammar.i .o. OTGrammar;
test identity
Further, we can subject the two grammars to the
usual finite-state calculus operations to gain possible
insight into what kinds of words yield different pre-
dictions with the two?something useful for linguis-
tic debugging. Likewise, we can use similar tech-
niques to analyze for redundancy in grammars. For
example, we have assumed that the VOP-constraint
plays no role in the above devoicing tableaux. Using
finite-state calculus, we can prove it to be so for any
input if the grammar is constructed with the method
presented here.
5 Limits on FST implementation
We shall conclude the presentation here with a brief
discussion of the limits of FST representability, even
of simple OT grammars. Previous analyses have
shown that OT systems are beyond the generative
capacity of finite-state systems, under some assump-
tions of what GEN looks like. For example, Frank
and Satta (1998) present such a constraint system
where GEN is taken to be defined through a trans-
duction equivalent to:8
Gen = [a:b|b:a]* | [a|b]*;
That is, a relation which either maps all a?s to b?s
and vice versa, or leaves the input unchanged. Now,
let us assume the presence of a single markedness
constraint ?a, militating against the letter a. In that
case, given an input of the format a?b? the effective
mapping of the entire system is one that is an identity
relation if there are fewer a?s than b?s; otherwise the
a?s and b?s are swapped. As is easily seen, this is not
a regular relation.
One possible objection to this analysis of non-
regularity is that linguistically GEN is usually as-
sumed to perform any transformation to the input
8The idea is attributed to Markus Hiller in the article.
17
whatsoever?not just limiting itself to a proper sub-
set of ?? ? ??. However, it is indeed the case
that even with a canonical GEN-function, some very
simple OT systems fall outside the purview of finite-
state expressibility, as we shall illustrate by a differ-
ent example here.
5.1 A simple proof of OT nonregularity
Assume a grammar that has four very basic con-
straints: IDENT, forbidding changes, DEP, for-
bidding epenthesis, ?ab, a markedness constraint
against the sequence ab, and MAX, forbidding dele-
tion, ranked IDENT,DEP  ?ab  MAX. We as-
sume GEN to be as general as possible?performing
arbitrary deletions, insertions, and changes.
It is clear, as is illustrated in table 2, that for all in-
puts of the format anbm the grammar in question de-
scribes a relation that deletes all the a?s or all the b?s
depending on which there are fewer instances of, i.e.
anbm ? an if m < n, and anbm ? bm if n < m.
This can be shown by a simple pumping argument
to not be realizable through an FST.
aaabb IDENT DEP ?ab MAX
aaaaa *!*
aaacbb *!
aaabb *!
aaab *! *
bb ***!
+ aaa **
Table 2: Illustrative tableau for a simple constraint sys-
tem not capturable as a regular relation.
Implementing this constraint system with the
methods presented here is an interesting exercise
and serves to examine the behavior of the method.
We define GEN, DEP, MAX, and IDENT as be-
fore, define a universal alphabet (excluding markup
symbols), and the constraint ?ab naturally as:
S = ? - %( - %) - %[ - %] - %* ;
NotAB = [..] -> {*} || Surf(a b) _ ;
Now, with one round of permutation of the viola-
tion markers in Worsen as follows:
Worsen = [Gen.i .o. Gen]/{*} .o.
AddViol .o. Permute;
we calculate
define Grammar Eval(Eval(Eval(Eval(
Gen .o. Ident) .o. Dep) .o. NotAB) .o.
Max) .o. Cleanup;
which produces an FST that cannot distinguish be-
tween more than two a?s or b?s in a string. While
it correctly maps aab to aa and abb to bb, the
tableau example of aaabb is mapped to both aaa
and bb. However, with one more round of permu-
tation in Worsen, we produce an FST that can in-
deed cover the example, mapping aaabb uniquely
to bb, while failing with aaaabbb (see figure 6).
This illustrates the approximation characteristic of
the matching method: for some grammars (proba-
bly most natural language grammars) the worsening
approach will at some point of permutation of the vi-
olation markers terminate and produce an exact FST
representation of the grammar, while for some gram-
mars such convergence will never happen. How-
ever, if the permutation of markers terminates and
produces a functional transducer when testing each
violation as described above, the FST is guaranteed
to be an exact representation.
0
b @ 
1
a 
5a:0 
@ 2
b:0 
3
a 
@ 
a 
@ 
b:0 
4
a 
@ 
a b:0 
b 
6a:0 b 7
a:0 
b 
a:0 
Figure 6: An non-regular OT approximation.
It is an open question if it is decidable by exam-
ining a grammar whether it will yield an exact FST
representation. We do not expect this question to be
easy, since it cannot be determined by the nature of
the constraints alone. For example, the above four-
constraint system does have an exact FST represen-
tation in some orderings of the constraints, but not
in the particular one given above.
18
6 Conclusion
We have presented a practical method of implement-
ing OT grammars as finite-state transducers. The ex-
amples, definitions, and templates given should be
sufficient and flexible enough to encode a wide vari-
ety of OT grammars as FSTs. Although no method
can encode all OT grammars as FSTs, the funda-
mental advantage with the system outlined is that
for a large majority of practical cases, an FST can
be produced which is not an approximation that can
only tell apart a limited number of violations. As
has been noted elsewhere (e.g. Eisner (2000b,a)),
some OT constraints, such as Generalized Align-
ment constraints, are on the face of it not suitable
for FST implementation. We may add to this that
some very simple constraint systems, assuming a
canonical GEN, and only using the most basic faith-
fulness and markedness constraints, are likewise not
encodable as regular relations, and seem to have the
generative power to encode phenomena not found
in natural language. However, for most practical
purposes?and this includes modeling actual phe-
nomena in phonology and morphology?the present
approach offers a fruitful way to implement, ana-
lyze, and debug OT grammars.
References
Beesley, K. R. and Karttunen, L. (2003). Finite State
Morphology. CSLI Publications, Stanford, CA.
Blattner, M. and Head, T. (1977). Single-valued a-
transducers. Journal of Computer and System Sci-
ences, 15(3):328?353.
Eisner, J. (2000a). Directional constraint evaluation
in optimality theory. In Proceedings of the 18th
conference on Computational linguistics, pages
257?263. Association for Computational Linguis-
tics.
Eisner, J. (2000b). Easy and hard constraint ranking
in optimality theory. In Finite-state phonology:
Proceedings of the 5th SIGPHON, pages 22?33.
Ellison, T. M. (1994). Phonological derivation in op-
timality theory. In Proceedings of COLING?94?
Volume 2, pages 1007?1013.
Frank, R. and Satta, G. (1998). Optimality theory
and the generative complexity of constraint viola-
bility. Computational Linguistics, 24(2):307?315.
Gerdemann, D. and van Noord, G. (2000). Approx-
imation and exactness in finite state optimality
theory. In Proceedings of the Fifth Workshop of
the ACL Special Interest Group in Computational
Phonology.
Hammond, M. (1997). Parsing syllables: Modeling
OT computationally. Rutgers Optimality Archive
(ROA), 222-1097.
Hulden, M. (2009a). Finite-state Machine Construc-
tion Methods and Algorithms for Phonology and
Morphology. PhD thesis, The University of Ari-
zona.
Hulden, M. (2009b). Foma: a finite-state compiler
and library. In EACL 2009 Proceedings, pages
29?32.
Ja?ger, G. (2002). Gradient constraints in finite state
OT: the unidirectional and the bidirectional case.
More than Words. A Festschrift for Dieter Wun-
derlich, pages 299?325.
Kager, R. (1999). Optimality Theory. Cambridge
University Press.
Kaplan, R. M. and Kay, M. (1994). Regular mod-
els of phonological rule systems. Computational
Linguistics, 20(3):331?378.
Karttunen, L. (1998). The proper treatment of op-
timality theory in computational phonology. In
Finite-state Methods in Natural Language Pro-
cessing.
Karttunen, L. (2006). The insufficiency of paper-
and-pencil linguistics: the case of Finnish
prosody. Rutgers Optimality Archive.
Kiparsky, P. (2003). Finnish noun inflection. Gener-
ative approaches to Finnic linguistics. Stanford:
CSLI.
Prince, A. and Smolensky, P. (1993). Optimality
theory: Constraint interaction in generative gram-
mar. ms. Rutgers University Cognitive Science
Center.
Riggle, J. (2004). Generation, recognition, and
learning in finite state Optimality Theory. PhD
thesis, University of California, Los Angeles.
19
