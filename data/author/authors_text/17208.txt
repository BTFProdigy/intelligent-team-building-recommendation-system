First Joint Conference on Lexical and Computational Semantics (*SEM), pages 597?602,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
UNITOR: Combining Semantic Text Similarity functions
through SV Regression
Danilo Croce, Paolo Annesi, Valerio Storch and Roberto Basili
Department of Enterprise Engineering
University of Roma, Tor Vergata
00133 Roma, Italy
{croce,annesi,storch,basili}@info.uniroma2.it
Abstract
This paper presents the UNITOR system that
participated to the SemEval 2012 Task 6: Se-
mantic Textual Similarity (STS). The task is
here modeled as a Support Vector (SV) regres-
sion problem, where a similarity scoring func-
tion between text pairs is acquired from exam-
ples. The semantic relatedness between sen-
tences is modeled in an unsupervised fashion
through different similarity functions, each
capturing a specific semantic aspect of the
STS, e.g. syntactic vs. lexical or topical vs.
paradigmatic similarity. The SV regressor ef-
fectively combines the different models, learn-
ing a scoring function that weights individual
scores in a unique resulting STS. It provides a
highly portable method as it does not depend
on any manually built resource (e.g. WordNet)
nor controlled, e.g. aligned, corpus.
1 Introduction
Semantic Textual Similarity (STS) measures the de-
gree of semantic equivalence between two phrases
or texts. An effective method to compute similar-
ity between short texts or sentences has many appli-
cations in Natural Language Processing (Mihalcea
et al, 2006) and related areas such as Information
Retrieval, e.g. to improve the effectiveness of a se-
mantic search engine (Sahami and Heilman, 2006),
or databases, where text similarity can be used in
schema matching to solve semantic heterogeneity
(Islam and Inkpen, 2008).
STS is here modeled as a Support Vector (SV) re-
gression problem, where a SV regressor learns the
similarity function over text pairs. Regression learn-
ing has been already applied to different NLP tasks.
In (Pang and Lee, 2005) it is applied to Opinion
Mining, in particular to the rating-inference prob-
lem, wherein one must determine an author evalua-
tion with respect to a multi-point scale. In (Albrecht
and Hwa, 2007) a method is proposed for develop-
ing sentence-level MT evaluation metrics using re-
gression learning without directly relying on human
reference translations. In (Biadsy et al, 2008) it has
been used to rank candidate sentences for the task
of producing biographies from Wikipedia. Finally,
in (Becker et al, 2011) SV regressor has been used
to rank questions within their context in the multi-
modal tutorial dialogue problem.
In this paper, the semantic relatedness between
two sentences is modeled as a combination of dif-
ferent similarity functions, each describing the anal-
ogy between the two texts according to a specific
semantic perspective: in this way, we aim at captur-
ing syntactic and lexical equivalences between sen-
tences and exploiting either topical relatedness or
paradigmatic similarity between individual words.
The variety of semantic evidences that a system can
employ here grows quickly, according to the genre
and complexity of the targeted sentences. We thus
propose to combine such a body of evidence to learn
a comprehensive scoring function y = f(~x) over in-
dividual measures from labeled data through SV re-
gression: y is the gold similarity score (provided by
human annotators), while ~x is the vector of the dif-
ferent individual scores, provided by the chosen sim-
ilarity functions. The regressor objective is to learn
the proper combination of different functions redun-
dantly applied in an unsupervised fashion, without
involving any in-depth description of the target do-
main or prior knowledge. The resulting function se-
lects and filters the most useful information and it
597
is a highly portable method. In fact, it does not de-
pend on manually built resources (e.g. WordNet),
but mainly exploits distributional analysis of unla-
beled corpora.
In Section 2, the employed similarity functions
are described and the application of SV regression
is presented. Finally, Section 3 discusses results on
the SemEval 2012 - Task 6.
2 Combining different similarity function
through SV regression
This section describes the UNITOR systems partic-
ipating to the SemEval 2012 Task 6: in Section 2.1
the different similarity functions between sentence
pairs are discussed, while Section 2.2 describes how
the SV regression learning is applied.
2.1 STS functions
Each STS depends on a variety of linguistic aspects
in data, e.g. syntactic or lexical information. While
their supervised combination can be derived through
SV regression, different unsupervised estimators of
STS exist.
Lexical Overlap (LO). A basic similarity function
is first employed as the lexical overlap between sen-
tences, i.e. the cardinality of the set of words occur-
ring in both sentences.
Document-oriented similarity based on Latent
Semantic Analysis (LSA). This function captures
latent semantic topics through LSA. The adjacency
terms-by-documents matrix is first acquired through
the distributional analysis of a corpus and reduced
through the application of Singular Value Decom-
position (SVD), as described in (Landauer and Du-
mais, 1997). In this work, the individual sentences
are assumed as pseudo documents and represented
by vectors in the lower dimensional LSA space. The
cosine similarity between vectors of a sentence pair
is the metric hereafter referred to as topical similar-
ity.
Compositional Distributional Semantics (CDS).
Lexical similarity can also be extended to account
for syntactic compositions between words. This
makes sentence similarity to depend on the set of in-
dividual compounds, e.g. subject-verb relationship
instances. While basic lexical information can still
be obtained by distributional analysis, phrase level
Figure 1: Example of dependency graph
similarity can be here modeled as a specific func-
tion of the co-occurring words, i.e. a complex alge-
braic composition of their corresponding word vec-
tors. Differently from the document-oriented case
used in the LSA function, base lexical vectors are
here derived from co-occurrence counts in a word
space, built according to the method discussed in
(Sahlgren, 2006; Croce and Previtali, 2010). In or-
der to keep dimensionality as low as possible, SVD
is also applied here (Annesi et al, 2012). The result
is that every noun, verb, adjective and adverb is then
projected in the reduced word space and then dif-
ferent composition functions can be applied as dis-
cussed in (Mitchell and Lapata, 2010) or (Annesi et
al., 2012).
Convolution kernel-based similarity. The similar-
ity function is here the Smoothed Partial Tree Ker-
nel (SPTK) proposed in (Croce et al, 2011). This
convolution kernel estimates the similarity between
sentences, according to the syntactic and lexical in-
formation in both sentences. Syntactic representa-
tion of a sentence like ?A man is riding a bicycle? is
derived from the dependency parse tree, as shown
in Fig. 1. It allows to define different tree struc-
tures over which the SPTK operates. First, a tree
including only lexemes, where edges encode their
dependencies, is generated and called Lexical Only
Centered Tree (LOCT), see Fig. 2. Then, we add
to each lexical node two leftmost children, encod-
ing the grammatical function and the POS-Tag re-
spectively: it is the so-called Lexical Centered Tree
(LCT), see Fig. 3. Finally, we generate the Gram-
matical Relation Centered Tree (GRCT), see Fig.
4, by setting grammatical relation as non-terminal
nodes, while PoS-Tags are pre-terminals and fathers
of their associated lexemes. Each tree representation
provides a different kernel function so that three dif-
ferent SPTK similarity scores, i.e. LOCT, LCT and
GRCT, are here obtained.
598
be::v
ride::v
bicycle::n
a::d
man::n
a::d
Figure 2: Lexical Only Centered Tree (LOCT)
be::v
VBZROOTride::v
VBGVCbicycle::n
NNOBJa::d
DTNMOD
man::n
NNSBJa::d
DTNMOD
Figure 3: Lexical Centered Tree (LCT)
ROOT
VC
OBJ
NN
bicycle::n
NMOD
DT
a::d
VBG
ride::v
VBZ
be::v
SBJ
NN
man::n
NMOD
DT
a::d
Figure 4: Grammatical Relation Centered Tree (GRCT)
2.2 Combining STSs with SV Regression
The similarity functions described above provide
scores capturing different linguistic aspects and an
effective way to combine such information is made
available by Support Vector (SV) regression, de-
scribed in (Smola and Scho?lkopf, 2004). The idea
is to learn a higher level model by weighting scores
according to specific needs implicit in training data.
Given similarity scores ~xi for the i-th sentence pair,
the regressor learns a function yi = f(~xi), where yi
is the score provided by human annotators.
The ?-SV regression (Vapnik, 1995) algorithm al-
lows to define the best f approximating the train-
ing data, i.e. the function that has at most ? de-
viation from the actually obtained targets yi for
all the training data. Given a training dataset
{(~x1, y1), . . . , (~xl, yl)} ? X ? R, where X is the
space of the input patterns, i.e. the original similar-
ity scores, we can acquire a linear function
f(~x) = ?~w, ~x?+ b with ~w ? X, b ? R
by solving the following optimization problem:
minimize
1
2
||~w||2
subject to
{
yi ? ?~w, ~xi? ? b ? ?
?~w, ~xi?+ b? yi ? ?
Since the function f approximating all pairs
(~xi, yi) with ? precision, may not exist, i.e. the con-
vex optimization problem is infeasible, slack vari-
ables ?i, ??i are introduced:
minimize
1
2
||~w||2 + C
l?
i=1
(?i + ?
?
i )
subject to
?
??
??
yi ? ?~w, ~xi? ? b ? ?+ ?i
?~w, ~xi?+ b? yi ? ?+ ??i
?i, ??i ? 0
where ?i, ??i measure the error introduced by training
data with a deviation higher than ? and the constant
C > 0 determines the trade-off between the norm
?~w? and the amount up to which deviations larger
than ? are tolerated.
3 Experimental Evaluation
This section describes results obtained in the Se-
mEval 2012 Task 6: STS. First, the experimental
setup of different similarity functions is described.
Then, results obtained over training datasets are re-
ported. Finally, results achieved in the competition
are discussed.
3.1 Experimental setup
In order to estimate the Latent Semantic Analysis
(LSA) based similarity function, the distributional
analysis of the English version of the Europarl Cor-
pus (Koehn, 2002) has been carried out. It is the
same source corpus of the SMTeuroparl dataset and
it allows to acquire a semantic space capturing the
same topics characterizing this dataset. A word-by-
sentence matrix models the sentence representation
space. The entire corpus has been split so that each
vector represents a sentence: the number of different
sentences is about 1.8 million and the matrix cells
contain tf-idf scores between words and sentences.
The SVD is applied and the space dimensionality
599
is reduced to k = 250. Novel sentences are im-
mersed in the reduced space, as described in (Lan-
dauer and Dumais, 1997) and the LSA-based simi-
larity between two sentences is estimated according
the cosine similarity.
To estimate the Compositional Distributional Se-
mantics (CDS) based function, a co-occurrence
Word Space is first acquired through the distribu-
tional analysis of the UKWaC corpus (Baroni et al,
2009), i.e. a Web document collection made of
about 2 billion tokens. UKWaC is larger than the
Europarl corpus and we expect it makes available
a more general lexical representation suited for all
datasets. An approach similar to the one described in
(Croce and Previtali, 2010) has been adopted for the
acquisition of the word space. First, all words occur-
ring more than 200 times (i.e. the targets) are rep-
resented through vectors. The original space dimen-
sions are generated from the set of the 20,000 most
frequent words (i.e. features) in the UKWaC cor-
pus. One dimension describes the Pointwise Mutual
Information score between one feature as it occurs
on a left or right window of 3 tokens around a target.
Left contexts of targets are treated differently from
the right ones, in order to also capture asymmetric
syntactic behaviors (e.g., useful for verbs): 40,000
dimensional vectors are thus derived for each target.
The particularly small window size allows to better
capture paradigmatic relations between targets, e.g.
hyponymy or synonymy. Again, the SVD reduction
is applied to the original matrix with a k = 250.
Once lexical vectors are available, a compositional
similarity measure can be obtained by combining
the word vectors according to a CDS operator, e.g.
(Mitchell and Lapata, 2010) or (Annesi et al, 2012).
In this work, the adopted compositional representa-
tion is the additive operator between lexical vectors,
as described in (Mitchell and Lapata, 2010) and the
similarity function between two sentences is the co-
sine similarity between their corresponding compo-
sitional vectors. Moreover, two additive operators
that only sum over nouns and verbs are also adopted,
denoted by CDSV and CDSN , respectively.
The estimation of the semantically Smoothed Par-
tial Tree Kernel (SPTK) is made available by an ex-
tended version of SVM-LightTK software1 (Mos-
1http://disi.unitn.it/moschitti/Tree-Kernel.htm
chitti, 2006) implementing the smooth matching be-
tween tree nodes. The tree representation described
in Sec. 2.1 allows to define 3 different kernels, i.e.
SPTKLOCT , SPTKLCT and SPTKGRCT . Similarity
between lexical nodes is estimated as the cosine sim-
ilarity in the co-occurrence Word Space described
above, as in (Croce et al, 2011).
In all corpus analysis and experiments, sentences
are processed with the LTH dependency parser, de-
scribed in (Johansson and Nugues, 2007), for Part-
of-speech tagging and lemmatization. Dependency
parsing of datasets is required for the SPTK appli-
cation. Finally, SVM-LightTK is employed for the
SV regression learning to combine specific similar-
ity functions.
3.2 Evaluating the impact of unsupervised
models
Table 1 compares the Pearson Correlation of differ-
ent similarity functions described in Section 2.1, i.e.
mainly the results of the unsupervised approaches,
against the challenge training data. Regarding to
MSRvid dataset, the topical similarity (LSA func-
tion) achieves the best result, i.e. 0.748. Paradig-
matic lexical information as in CDS, CDSN and LO
provides also good results, confirming the impact of
lexical generalization. However, only nouns seem
to contribute significantly, as for the poor results of
CDSV suggest. As the dataset is characterized by
short sentences with negligible syntactic differences,
SPTK-based kernels are not discriminant. On the
contrary, the SPTKLCT achieves the best result in
the MSRpar dataset, where paraphrasing phenom-
ena are peculiar. Notice that the other SPTK kernels
are not equivalently performant, in line with previ-
ous results on question classification and semantic
role labeling (Croce et al, 2011). Lexical informa-
tion provides a crucial contribution also for LO, al-
though the contribution of topical or paradigmatic
generalization seems negligible over MSRpar. Fi-
nally, in the SMTeuroparl, longer sentences are the
norm and length seems to compromise the perfor-
mance of LO. The best results seem to require the
lexical and syntactic information provided by CDS
and SPTK.
600
Models
Dataset
MSRvid MSRpar SMTeuroparl
CDS .652 .393 .681
CDSN .630 .234 .485
CDSV .219 .317 .264
LSA .748 .344 .477
SPTKLOCT .300 .251 .611
SPTKLCT .297 .464 .622
SPTKGRCT .278 .255 .626
LO .560 .446 .248
Table 1: Unsupervised results over the training dataset
3.3 Evaluating the role of SV regression
The SV regressors have been trained over a feature
space that enumerates the different similarity func-
tions: one feature is provided by the LSA function,
three by the CDS, i.e. CDS, CDSN and CDSV ,
three by SPTK, i.e. SPTKLOCT , SPTKLCT and
SPTKGRCT and one by LO, i.e. the number of
words in common. Two more features are obtained
by the sentence lengths of a pair, i.e. the number
of words in the first and second sentence, respec-
tively. Table 2 shows Pearson Correlation results
when the regressor is trained according a 10-fold
cross validation schema. First, all possible feature
combinations are attempted for the SV regression,
so that every subset of the 10 features is evaluated.
Results of the best feature combination are shown in
column bestfeat: for MSRvid, the best performance
is achieved when all 10 features are considered; in
MSRpar, SPTK combined with LO is sufficient; fi-
nally, in the SMTeuroparl the combination is LO,
CDS and SPTK. In column allfeat results achieved
by considering all features are reported. Last col-
umn specifies the performance increase with respect
to the corresponding best results in the unsupervised
settings.
Results of the regressors are always higher with
respect to the unsupervised settings, with up to a
35% improvement for the MSRpar, i.e. the most
complex domain. Moreover, differences when best
and all features are employed are negligible. It
means that SV regressor allows to automatically
combine and select the most informative similarity
aspects, confirming the applicability of the proposed
redundant approach to STS.
Dataset
Experiments
Gain
bestfeat allfeat
MSRvid .789 .789 5,0%
MSRpar .615 .612 32,4%
SMTeuroparl .692 .691 1,6%
Table 2: SV regressor results over the training dataset
3.4 Results over the SemEval Task 6
According to the above evidence, we participated to
the SemEval challenge with three different systems.
Sys1 - Best Features. Scores between pairs from a
specific dataset are obtained by applying a regressor
trained over pairs from the same dataset. It means
that, for example, the test pairs from the MSRvid
dataset are processed with a regressor trained over
the MSRvid training data. Moreover, the most rep-
resentative similarity function estimated for the col-
lection is employed: the feature combination provid-
ing the best correlation results over training pairs is
adopted for the test. The same is applied to MSRpar
and SMTeuroparl. No selection is adopted for the
Surprise data and training data for all the domains
are used, as described in Sys3.
Sys2 - All Features. Relatedness scores between
pairs from a specific dataset are obtained using a re-
gressor trained using pairs from the same dataset.
Differently from the Sys1, the similarity function
here is employed within the SV regressors trained
over all 10 similarity functions (i.e. all features).
Sys3 - All features and All domains. The SV re-
gressor is trained using training pairs from all col-
lections and over all 10 features. It means that one
single model is trained and employed to score all
test data. This approach is also used for the Surprise
data, i.e. the OnWN and SMTnews datasets.
Table 3 reports the general outcome for the UN-
ITOR systems. Rank of the individual scores with
respect to the other systems participating to the chal-
lenge is reported in parenthesis. This allows to draw
some conclusions. First, the proposed system ranks
around the 12 and 13 system positions (out of 89
systems), and the 6th group. The adoption of all pro-
posed features suggests that more evidence is better,
as it can be properly modeled by regression. It seems
generally better suited for the variety of semantic
phenomena observed in the tests. Regressors seem
601
Dataset
Results
BL Sys1 Sys2 Sys3
MSRvid .299 .821 .821 .802
MSRpar .433 .569 .576 .468
SMTeuroparl .454 .516 .510 .457
surp.OnWN .586 .659
surp.SMTnews .390 .471
ALL .311 .747 (13) .747 (12) .628 (40)
ALLnrm .673 .829 (12) .830 (11) .815 (21)
Mean .436 .632 (10) .632 ( 9) .594 (28)
Table 3: Results over the challenge test dataset
to be robust enough to select the proper features and
make the feature selection step (through collection
specific cross-validation) useless. Collection spe-
cific training seems useful, as Sys3 achieves lower
results, basically due to the significant stylistic dif-
ferences across the collections. However, the good
level of accuracy achieved over the surprise data sets
(between 11% and 17% performance gain with re-
spect to the baselines) confirms the large applica-
bility of the overall technique: our system in fact
does not depend on any manually coded resource
(e.g. WordNet) nor on any controlled (e.g. parallel
or aligned) corpus. Future work includes the study
of the learning rate and its correlation with differ-
ent and richer similarity functions, e.g. CDS as in
(Annesi et al, 2012).
Acknowledgements This research is partially
supported by the European Community?s Sev-
enth Framework Programme (FP7/2007-2013) un-
der grant numbers 262491 (INSEARCH). Many
thanks to the reviewers for their valuable sugges-
tions.
References
Joshua Albrecht and Rebecca Hwa. 2007. Regression for
sentence-level mt evaluation with pseudo references.
In Proceedings of ACL, pages 296?303, Prague, Czech
Republic, June.
Paolo Annesi, Valerio Storch, and Roberto Basili. 2012.
Space projections as distributional models for seman-
tic composition. In CICLing (1), Lecture Notes in
Computer Science, pages 323?335. Springer.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and
Eros Zanchetta. 2009. The wacky wide web: a
collection of very large linguistically processed web-
crawled corpora. Language Resources and Evalua-
tion, 43(3):209?226.
Lee Becker, Martha Palmer, Sarel van Vuuren, and
Wayne Ward. 2011. Evaluating questions in context.
Fadi Biadsy, Julia Hirschberg, and Elena Filatova. 2008.
An unsupervised approach to biography production
using wikipedia. In ACL, pages 807?815.
Danilo Croce and Daniele Previtali. 2010. Manifold
learning for the semi-supervised induction of framenet
predicates: An empirical investigation. In Proceed-
ings of the GEMS 2010 Workshop, pages 7?16, Upp-
sala, Sweden.
Danilo Croce, Alessandro Moschitti, and Roberto Basili.
2011. Structured lexical similarity via convolution
kernels on dependency trees. In Proceedings of
EMNLP, Edinburgh, Scotland, UK.
Aminul Islam and Diana Inkpen. 2008. Semantic
text similarity using corpus-based word similarity and
string similarity. ACM Trans. Knowl. Discov. Data,
2:10:1?10:25, July.
Richard Johansson and Pierre Nugues. 2007. Semantic
structure extraction using nonprojective dependency
trees. In Proceedings of SemEval-2007, Prague, Czech
Republic, June 23-24.
P. Koehn. 2002. Europarl: A multilingual corpus for
evaluation of machine translation. Draft.
Thomas K Landauer and Susan T. Dumais. 1997. A so-
lution to platos problem: The latent semantic analysis
theory of acquisition, induction, and representation of
knowledge. Psychological review, pages 211?240.
Rada Mihalcea, Courtney Corley, and Carlo Strapparava.
2006. Corpus-based and knowledge-based measures
of text semantic similarity. In In AAAI06.
Jeff Mitchell and Mirella Lapata. 2010. Composition in
distributional models of semantics. Cognitive Science,
34(8):1388?1429.
Alessandro Moschitti. 2006. Efficient convolution ker-
nels for dependency and constituent syntactic trees. In
Proceedings of ECML?06, pages 318?329.
Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting
class relationships for sentiment categorization with
respect to rating scales. In Proceedings of the ACL.
Mehran Sahami and Timothy D. Heilman. 2006. A web-
based kernel function for measuring the similarity of
short text snippets. In Proceedings of the 15th inter-
national conference on World Wide Web, WWW ?06,
pages 377?386, New York, NY, USA. ACM.
Magnus Sahlgren. 2006. The Word-Space Model. Ph.D.
thesis, Stockholm University.
Alex J. Smola and Bernhard Scho?lkopf. 2004. A tutorial
on support vector regression. Statistics and Comput-
ing, 14(3):199?222, August.
Vladimir N. Vapnik. 1995. The Nature of Statistical
Learning Theory. Springer?Verlag, New York.
602
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 59?65, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
UNITOR-CORE TYPED: Combining Text Similarity
and Semantic Filters through SV Regression
Danilo Croce, Valerio Storch and Roberto Basili
Department of Enterprise Engineering
University of Roma, Tor Vergata
00133 Roma, Italy
{croce,storch,basili}@info.uniroma2.it
Abstract
This paper presents the UNITOR system that
participated in the *SEM 2013 shared task on
Semantic Textual Similarity (STS). The task is
modeled as a Support Vector (SV) regression
problem, where a similarity scoring function
between text pairs is acquired from examples.
The proposed approach has been implemented
in a system that aims at providing high ap-
plicability and robustness, in order to reduce
the risk of over-fitting over a specific datasets.
Moreover, the approach does not require any
manually coded resource (e.g. WordNet), but
mainly exploits distributional analysis of un-
labeled corpora. A good level of accuracy is
achieved over the shared task: in the Typed
STS task the proposed system ranks in 1st and
2nd position.
1 Introduction
Semantic Textual Similarity (STS) measures the de-
gree of semantic equivalence between two phrases
or texts. An effective method to compute similarity
between sentences or semi-structured material has
many applications in Natural Language Processing
(Mihalcea et al, 2006) and related areas such as
Information Retrieval, improving the effectiveness
of semantic search engines (Sahami and Heilman,
2006), or databases, using text similarity in schema
matching to solve semantic heterogeneity (Islam and
Inkpen, 2008).
This paper describes the UNITOR system partic-
ipating in both tasks of the *SEM 2013 shared task
on Semantic Textual Similarity (STS), described in
(Agirre et al, 2013):
? the Core STS tasks: given two sentences, s1
and s2, participants are asked to provide a score
reflecting the corresponding text similarity. It is
the same task proposed in (Agirre et al, 2012).
? the Typed-similarity STS task: given two
semi-structured records t1 and t2, containing
several typed fields with textual values, partic-
ipants are asked to provide multiple similarity
scores: the types of similarity to be studied in-
clude location, author, people involved, time,
events or actions, subject and description.
In line with several participants of the STS 2012
challenge, such as (Banea et al, 2012; Croce et al,
2012a; S?aric? et al, 2012), STS is here modeled as
a Support Vector (SV) regression problem, where a
SV regressor learns the similarity function over text
pairs. The semantic relatedness between two sen-
tences is first modeled in an unsupervised fashion
by several similarity functions, each describing the
analogy between the two texts according to a spe-
cific semantic perspective. We aim at capturing sep-
arately syntactic and lexical equivalences between
sentences and exploiting either topical relatedness or
paradigmatic similarity between individual words.
Such information is then combined in a supervised
schema through a scoring function y = f(~x) over
individual measures from labeled data through SV
regression: y is the gold similarity score (provided
by human annotators), while ~x is the vector of the
different individual scores, provided by the chosen
similarity functions.
For the Typed STS task, given the specificity of
the involved information and the heterogeneity of
target scores, individual measures are not applied to
entire texts. Specific phrases are filtered according
to linguistic policies, e.g. words characterized by
specific Part-of-Speech (POS), such as nouns and
verbs, or Named Entity (NE) Category, i.e. men-
59
tions to specific name classes, such as of a PER-
SON, LOCATION or DATE. The former allows to
focus the similarity functions over entities (nouns)
or actions (verbs), while the latter allows to focus on
some aspects connected with the targeted similarity
functions, such as person involved, location or time.
The proposed approach has been implemented in
a system that aims at providing high applicability
and robustness. This objective is pursued by adopt-
ing four similarity measures designed to avoid the
risk of over-fitting over each specific dataset. More-
over, the approach does not require any manually
coded resource (e.g. WordNet), but mainly exploits
distributional analysis of unlabeled corpora. Despite
of its simplicity, a good level of accuracy is achieved
over the 2013 STS challenge: in the Typed STS task
the proposed system ranks 1st and 2nd position (out
of 18); in the Core STS task, it ranks around the 37th
position (out of 90) and a simple refinement to our
model makes it 19th.
In the rest of the paper, in Section 2, the employed
similarity functions are described and the applica-
tion of SV regression is presented. Finally, Section
3 discusses results on the *SEM 2013 shared task.
2 Similarity functions, regression and
linguistic filtering
This section describes the approach behind the UN-
ITOR system. The basic similarity functions and
their combination via SV regressor are discussed in
Section 2.1, while the linguistic filters are presented
in Section 2.2.
2.1 STS functions
Each STS function depends on a variety of linguistic
aspects in data, e.g. syntactic or lexical information.
While their supervised combination can be derived
through SV regression, different unsupervised esti-
mators of STS exist.
Lexical Overlap. A basic similarity function is
modeled as the Lexical Overlap (LO) between sen-
tences. Given the sets Wa and Wb of words oc-
curring in two generic texts ta and tb, LO is esti-
mated as the Jaccard Similarity between the sets, i.e.
LO= |Wa?Wb||Wa?Wb| . In order to reduce data sparseness,
lemmatization is applied and each word is enriched
with its POS to avoid the confusion between words
from different grammatical classes.
Compositional Distributional Semantics. Other
similarity functions are obtained by accounting for
the syntactic composition of the lexical information
involved in the sentences. Basic lexical information
is obtained by a co-occurrence Word Space that is
built according to (Sahlgren, 2006; Croce and Pre-
vitali, 2010). Every word appearing in a sentence is
then projected in such space. A sentence can be thus
represented neglecting its syntactic structure, by ap-
plying an additive linear combination, i.e. the so-
called SUM operator. The similarity function be-
tween two sentences is then the cosine similarity be-
tween their corresponding vectors.
A second function is obtained by applying a Dis-
tributional Compositional Semantics operator, in
line with the approaches introduced in (Mitchell and
Lapata, 2010), and it is adopted to account for se-
mantic composition. In particular, the approach de-
scribed in (Croce et al, 2012c) has been applied.
It is based on space projection operations over ba-
sic geometric lexical representations: syntactic bi-
grams are projected in the so called Support Sub-
space (Annesi et al, 2012), aimed at emphasiz-
ing the semantic features shared by the compound
words. The aim is to model semantics of syntac-
tic bi-grams as projections in lexically-driven sub-
spaces. In order to extend this approach to handle
entire sentences, we need to convert them in syn-
tactic representations compatible with the compo-
sitional operators proposed. A dependency gram-
mar based formalism captures binary syntactic re-
lations between the words, expressed as nodes in
a dependency graph. Given a sentence, the parse
structure is acquired and different triples (w1, w2, r)
are generated, where w1 is the relation governor, w2
is the dependent and r is the grammatical type. In
(Croce et al, 2012c) a simple approach is defined,
and it is inspired by the notion of Soft Cardinal-
ity, (Jimenez et al, 2012). Given a triple set T =
{t1, . . . , tn} extracted from a sentence S and a sim-
ilarity sim(ti, tj), the Soft Cardinality is estimated
as |S|?sim u
?|T |
ti (
?|T |
tj sim(ti, tj)
p)?1, where pa-
rameter p controls the ?softness? of the cardinality:
with p = 1 element similarities are unchanged while
higher value will tend to the Classical Cardinality
measure. Notice that differently from the previous
60
usage of the Soft Cardinality notion, we did not ap-
ply it to sets of individual words, but to the sets of
dependencies (i.e. triples) derived from the two sen-
tences. The sim function here can be thus replaced
by any compositional operator among the ones dis-
cussed in (Annesi et al, 2012). Given two sen-
tences, higher Soft Cardinality values mean that the
elements in both sentences (i.e. triples) are different,
while the lower values mean that common triples are
identical or very similar, suggesting that sentences
contain the same kind of information. Given the sets
of triples A and B extracted from the two candidate
sentences, our approach estimates a syntactically re-
stricted soft cardinality operator, the Syntactic Soft
Cardinality (SSC) as SSC(A,B) = 2|A?B|
?
|A|?+|B|? , as
a ?soft approximation? of Dice?s coefficient calcu-
lated on both sets1.
capture::v
VBNROOTmarine::n
NNSPREP-BYmexico::n
NNPPREP-IN
lord::n
NNNSUBJdrug::n
NNNN
Figure 1: Lexical Centered Tree (LCT)
Convolution kernel-based similarity. The similar-
ity function is here the Smoothed Partial Tree Ker-
nel (SPTK) proposed in (Croce et al, 2011). SPTK
is a generalized formulation of a Convolution Ker-
nel function (Haussler, 1999), i.e. the Tree Kernel
(TK), by extending the similarity between tree struc-
tures with a function of node similarity. The main
characteristic of SPTK is its ability to measure the
similarity between syntactic tree structures, which
are partially similar and whose nodes can differ but
are semantically related. One of the most important
outcomes is that SPTK allows ?embedding? exter-
nal lexical information in the kernel function only
through a similarity function among lexical nodes,
namely words. Moreover, SPTK only requires this
similarity to be a valid kernel itself. This means that
such lexical information can be derived from lexical
resources or it can be automatically acquired by a
Word Space. The SPTK is applied to a specific tree
representation that allowed to achieve state-of-the-
1Notice that, since the intersection |A ? B|? tends to be too
strict, we approximate it from the union cardinality estimation
|A|? + |B|? ? |A ?B|?.
art results on several complex semantic tasks, such
as Question Classification (Croce et al, 2011) or
Verb Classification (Croce et al, 2012b): each sen-
tence is represented through the Lexical Centered
Tree (LCT), as shown in Figure 1 for the sentence
?Drug lord captured by Marines in Mexico?. It is de-
rived from the dependency parse tree: nodes reflect
lexemes and edges encode their syntactic dependen-
cies; then, we add to each lexical node two leftmost
children, encoding the grammatical function and the
POS-Tag respectively.
Combining STSs with SV Regression The similar-
ity functions described above provide scores captur-
ing different linguistic aspects and an effective way
to combine such information is made available by
Support Vector (SV) regression, described in (Smola
and Scho?lkopf, 2004). The idea is to learn a higher
level model by weighting scores according to spe-
cific needs implicit in training data. Given similar-
ity scores ~xi for the i-th sentence pair, the regressor
learns a function yi = f(~xi), where yi is the score
provided by human annotators. Moreover, since the
combination of kernel is still a kernel, we can ap-
ply polynomial and RBF kernels (Shawe-Taylor and
Cristianini, 2004) to the regressor.
2.2 Semantic constraints for the Typed STS
Typed STS insists on records, i.e. sequence of typed
textual fields, rather than on individual sentences.
Our aim is to model the typed task with the same
spirit as the core one, through a combination of
different linguistic evidences, which are modeled
through independent kernels. The overall similarity
model described in 2.1 has been thus applied also to
the typed task according to two main model changes:
? Semantic Modeling. Although SV regression
is still applied to model one similarity type,
each type depends on a subset of the multiple
evidences originating from individual fields:
one similarity type acts as a filter on the set of
fields, on which kernels will be then applied.
? Learning Constraints. The selected fields pro-
vide different evidences to the regression steps.
Correspondingly, each similarity type corre-
sponds to specific kernels and features for its
fields. These constraints are applied by select-
ing features and kernels for each field.
61
dcTitle dcSubject dcDescription dcCreator dcDate dcSource
author - - PER ? - -
people inv. PER PER PER - - -
time DATE DATE DATE - ? -
location LOC LOC LOC - - -
event N , V , N ? V N , V , N ? V N , V , N ? V - - -
subject N , V , J , N ? J ? V N , V , J , N ? J ? V - - - -
description - - N , V , J , N ? J ? V - - -
general + + + ? ? ?
Table 1: Filtering Schema adopted for the Typed STS task.
Notice how some kernels loose significance in the
typed STS task. Syntactic information is no useful
so that no tree kernel and compositional kernel is
applied here. Most of the fields are non-sentential2.
Moreover, not all morpho-syntactic information are
extracted as feature from some fields. Filters usu-
ally specify some syntactic categories or Named En-
tities (NEs): they are textual mentions to specific
real-world categories, such as of PERSONS (PER),
LOCATIONS (LOC) or DATES. They are detected
in a field and made available as feature to the cor-
responding kernel: this introduces a bias on typed
measures and emphasizes specific semantic aspects
(e.g. places LOC or persons PER, in location or au-
thor measures, respectively). For example, in the
sentence ?The chemist R.S. Hudson began manufac-
turing soap in the back of his small shop in West
Bomich in 1837?, when POS tag filters are applied,
only verbs (V), nouns (N) or adjectives (J) can be
selected as features. This allows to focus on spe-
cific actions, e.g. the verb ?manufacture?, entities,
e.g. nouns ?soap? and ?shop?, or some properties,
e.g. the adjective ?small?. When Named Entity cat-
egories are used, a mention to a person like ?R.S.
Hudson? or to a location, e.g. ?West Bomich?, or
date, e.g. ?1837?, can be useful to model the the
person involved, the location or time similarity mea-
sures, respectively.
The Semantic Modeling and the Learning Con-
straints system adopted to model the Typed STS
task are defined in Table 1. There rows are the
different target similarities, while columns indicate
document fields, such as dcTitle, dcSubject,
dcDescription, dcCreator, dcDate and
2The dcDescription is also made of multiple sen-
tences and it reduces the applicability of SPTK and SSC: parse
trees have no clear alignment.
dcSource, as described in the *SEM 2013 shared
task description. Each entry in the Table represents
the feature set for that fields, i.e. POS tags (i.e. V ,
N , J) or Named Entity classes. The ??? symbol
corresponds to all features, i.e. no restriction is
applied to any POS tag or NE class. Finally, the
general similarity function makes use of every NE
class and POS tags adopted for that field in any
measure, as expressed by the special notation +, i.e.
?all of the above features?.
Every feature set denoted in the Table 1 sup-
ports the application of a lexical kernel, such as
the LO described in Section 2.1. When different
POS tags are requested (such as N and V ) mul-
tiple feature sets and kernels are made available.
The ?-? symbol means that the source field is fully
neglected from the SV regression. As an exam-
ple, the SV regressor for the location similarity
has been acquired considering the fields dcTitle,
dcSubject, dcDescription. Only features used
for the kernel correspond to LOCATIONs (LOC). For
each of the three feature, the LO and SUM simi-
larity function has been applied, giving rise to an
input 6-dimensional feature space for the regressor.
Differently, in the subject similarity, nouns, adjec-
tives and verbs are the only features adopted from
the fields dcSubject, dcTitle, so that 8 feature
sets are used to model these fields, giving rise to a
16-dimensional feature space.
3 Results and discussion
This section describes results obtained in the *SEM
2013 shared task. The experimental setup of differ-
ent similarity functions is described in Section 3.1.
Results obtained over the Core STS task and Typed
STS task are described in Section 3.2 and 3.3.
62
3.1 Experimental setup
In all experiments, sentences are processed with the
Stanford CoreNLP3 system, for Part-of-Speech tag-
ging, lemmatization, named entity recognition4 and
dependency parsing.
In order to estimate the basic lexical similarity
function employed in the SUM, SSC and SPTK
operators, a co-occurrence Word Space is acquired
through the distributional analysis of the UkWaC
corpus (Baroni et al, 2009), a Web document col-
lection made of about 2 billion tokens. The same
setting of (Croce et al, 2012a) has been adopted
for the space acquisition. The same setup described
in (Croce et al, 2012c) is applied to estimate the
SSC function. The similarity between pairs of syn-
tactically restricted word compound is evaluated
through a Symmetric model: it selects the best 200
dimensions of the space, selected by maximizing the
component-wise product of each compound as in
(Annesi et al, 2012), and combines the similarity
scores measured in each couple subspace with the
product function. The similarity score in each sub-
space is obtained by summing the cosine similarity
of the corresponding projected words. The ?soft car-
dinality? is estimated with the parameter p = 2.
The estimation of the semantically Smoothed Par-
tial Tree Kernel (SPTK) is made available by an ex-
tended version of SVM-LightTK software5 (Mos-
chitti, 2006) implementing the smooth matching
between tree nodes. Similarity between lexical
nodes is estimated as the cosine similarity in the
co-occurrence Word Space described above, as in
(Croce et al, 2011). Finally, SVM-LightTK is em-
ployed for the SV regression learning to combine
specific similarity functions.
3.2 Results over the Core STS
In the Core STS task, the resulting text similarity
score is measured by the regressor: each sentence
pair from all datasets is modeled according to a 13
dimensional feature space derived from the different
functions introduced in Section 2.1, as follows.
The first 5 dimensions are derived by applying
3
http://nlp.stanford.edu/software/corenlp.shtml
4The TIME and DURATION classes are collapsed with
DATE, while the PERSON and LOCATION classes are consid-
ered without any modification.
5
http://disi.unitn.it/moschitti/Tree-Kernel.htm
Run1 Run2 Run3 Run?1
headlines .635 (50) .651 (39) .603 (58) .671 (30)
OnWN .574 (33) .561 (36) .549 (40) .637 (25)
FNWN .352 (35) .358 (32) .327 (44) .459 (07)
SMT .328 (39) .310 (49) .319 (44) .348 (21)
Mean .494 (37) .490 (42) .472 (52) .537 (19)
Table 2: Results over the Core STS task
the LO operator over lemmatized words in the noun,
verb, adjective and adverb POS categories: 4 ker-
nels look at individual categories, while a fifth ker-
nel insists on the union of all POS. A second set of
5 dimensions is derived by the same application of
the SUM operator to the same syntactic selection of
features. The SPTK is then applied to estimate the
similarity between the LCT structures derived from
the dependency parse trees of sentences. Then, the
SPTK is applied to derive an additional score with-
out considering any specific similarity function be-
tween lexical nodes; in this setting, the SPTK can be
considered as a traditional Partial Tree Kernel (Mos-
chitti, 2006), in order to capture a more strict syn-
tactical similarity between texts. The last score is
generated by applying the SSC operator.
We participated in the *SEM challenge with three
different runs. The main difference between each
run is the dataset employed in the training phase
and the employed kernel within the regressor. With-
out any specific information about the test datasets,
a strategy to prevent the regressor to over-fit train-
ing material has been applied. We decided to use
a training dataset that achieved the best results over
datasets radically different from the training material
in the STS challenge of Semeval 2012. In particular,
for the FNWN and OnWN datasets, we arbitrarily
selected the training material achieving best results
over the 2012 surprise.OnWN; for the headlines and
SMT datasets we maximized performance training
over surprise.SMTnews. In Run1 the SVM regres-
sor is trained using dataset combinations providing
best results according to the above criteria: MSR-
par, MSRvid, SMTeuroparl and surprise.OnWN are
employed against FNWN and OnWN; MSRpar,
SMTeuroparl and surprise.SMTnews are employed
against headline and SMT. A linear kernel is ap-
plied when training the regressor. In Run2, differ-
ently from the previous one, the SVM regressor is
63
rank general author people inv. time location event subject description mean
Run1 1 .7981 .8158 .6922 .7471 .7723 .6835 .7875 .7996 .7620
Run2 2 .7564 .8076 .6758 .7090 .7351 .6623 .7520 .7745 .7341
Table 3: Results over the Typed STS task
trained using all examples from the training datasets.
A linear kernel is applied when training the regres-
sor. Finally, in Run3 the same training dataset selec-
tion schema of Run1 is applied and a gaussian kernel
is employed in the regressor.
Table 2 reports the general outcome for the UN-
ITOR systems in term of Pearson Correlation. The
best system, based on the linear kernel, ranks around
the 35th position (out of 90 systems), that reflects
the mean rank of all the systems in the ranking of
the different datasets. The gaussian kernel, em-
ployed for the Run3 does not provide any contri-
bution, as it ranks 50th. We think that the main
reason of these results is due to the intrinsic dif-
ferences between training and testing datasets that
have been heuristically coupled. This is first mo-
tivated by lower rank achieved by Run2. More-
over, it is in line with the experimental findings of
(Croce et al, 2012a), where a performance drop is
shown when the regressor is trained over data that
is not constrained over the corresponding source.
In Run?1 we thus optimized the system by manu-
ally selecting the training material that does provides
best performance on the test dataset: MSRvid, SM-
Teuroparl and surprise.OnWN are employed against
OnWN; surprise.OnWN against FNWN, SMTeu-
roparl against headlines; SMTeuroparl and sur-
prise.SMTnews against SMT. A linear kernel within
the regressor allow to reach the 19th position, even
reducing the complexity of the representation to a
five dimensional feature space: LO and SUM with-
out any specific filter, SPTK, PTK and SSC.
3.3 Results over the Typed STS
SV regression has been also applied to the Typed
STS task through seven type-specific regressors plus
a general one. Each SV regressor insists on the LO
and SUM kernel as applied to the features in Table
1. Notice that it was mainly due to the lack of rich
syntactic structures in almost all fields.
As described in Section 2.2, a specific modeling
strategy has been applied to derive the feature space
of each target similarity. For example, the regres-
sor associated with the event similarity score is fed
with 18 scores. Each of the 3 fields, , i.e. dcTitle,
dcSubject and dcDescription, provides the 2
kernels (LO and SUM) with 3 feature sets (i.e. N ,
V and N ? V ). In particular, the general simi-
larity function considers all extracted features for
each field, giving rise to a space of 51 dimensions.
We participated in the task with two different runs,
whose main difference is the adopted kernel within
the SV regressor. In Run1, a linear kernel is used,
while in Run2 a RBF kernel is applied.
Table 3 reports the general outcome for the UN-
ITOR system. The adopted semantic modeling, as
well as the selection of the proper information, e.g.
the proper named entity, allows the system to rank
in the 1st and 2nd positions (out of 18 systems). The
proposed selection schema in Table 1 is very effec-
tive, as confirmed by the results for almost all typed
similarity scores. Again, the RBF kernel does not
improve result over the linear kernel. The impact
of the proposed approach can be noticed for very
specific scores, such as time and location, especially
for text pairs where structured information is absent,
such as in the dcDate field. Moreover, the regres-
sor is not affected by the differences between train-
ing and test dataset as for the previous Core STS
task. A deep result analysis showed that some simi-
larity scores are not correctly estimated within pairs
showing partial similarities. For example, the events
or actions typed similarity is overestimated for the
texts pairs ?The Octagon and Pavilions, Pavilion
Garden, Buxton, c 1875? and ?The Beatles, The Oc-
tagon, Pavillion Gardens, St John?s Road, Buxton,
1963? because they mention the same location (i.e.
?Pavillion Gardens?).
Acknowledgements This work has been partially
supported by the Regione Lazio under the project
PROGRESS-IT (FILAS-CR-2011-1089) and the
Italian Ministry of Industry within the ?Industria
2015? Framework, project DIVINO (MI01 00234).
64
References
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 task 6: A pilot
on semantic textual similarity. In *SEM 2012, pages
385?393, Montre?al, Canada, 7-8 June.
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2013. *sem 2013 shared
task: Semantic textual similarity, including a pilot on
typed-similarity. In *SEM 2013: The Second Joint
Conference on Lexical and Computational Semantics.
Association for Computational Linguistics.
Paolo Annesi, Valerio Storch, and Roberto Basili. 2012.
Space projections as distributional models for seman-
tic composition. In CICLing (1), Lecture Notes in
Computer Science, pages 323?335. Springer.
Carmen Banea, Samer Hassan, Michael Mohler, and
Rada Mihalcea. 2012. Unt: A supervised synergistic
approach to semantic text similarity. In *SEM 2012,
pages 635?642, Montre?al, Canada, 7-8 June. Associa-
tion for Computational Linguistics.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and
Eros Zanchetta. 2009. The wacky wide web: a
collection of very large linguistically processed web-
crawled corpora. Language Resources and Evalua-
tion, 43(3):209?226.
Danilo Croce and Daniele Previtali. 2010. Manifold
learning for the semi-supervised induction of framenet
predicates: An empirical investigation. In Proceed-
ings of the GEMS 2010 Workshop, pages 7?16, Upp-
sala, Sweden.
Danilo Croce, Alessandro Moschitti, and Roberto Basili.
2011. Structured lexical similarity via convolution
kernels on dependency trees. In Proceedings of
EMNLP, Edinburgh, Scotland, UK.
Danilo Croce, Paolo Annesi, Valerio Storch, and Roberto
Basili. 2012a. Unitor: Combining semantic text simi-
larity functions through sv regression. In *SEM 2012,
pages 597?602, Montre?al, Canada, 7-8 June.
Danilo Croce, Alessandro Moschitti, Roberto Basili, and
Martha Palmer. 2012b. Verb classification using dis-
tributional similarity in syntactic and semantic struc-
tures. In Proceedings of the 50th Annual Meeting
of the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 263?272, Jeju Island, Ko-
rea, July.
Danilo Croce, Valerio Storch, Paolo Annesi, and Roberto
Basili. 2012c. Distributional compositional seman-
tics and text similarity. 2012 IEEE Sixth International
Conference on Semantic Computing, 0:242?249.
David Haussler. 1999. Convolution kernels on discrete
structures. Technical report, University of Santa Cruz.
Aminul Islam and Diana Inkpen. 2008. Semantic
text similarity using corpus-based word similarity and
string similarity. ACM Trans. Knowl. Discov. Data,
2:10:1?10:25, July.
Sergio Jimenez, Claudia Becerra, and Alexander Gel-
bukh. 2012. Soft cardinality: A parameterized sim-
ilarity function for text comparison. In *SEM 2012,
pages 449?453, Montre?al, Canada, 7-8 June. Associa-
tion for Computational Linguistics.
Rada Mihalcea, Courtney Corley, and Carlo Strapparava.
2006. Corpus-based and knowledge-based measures
of text semantic similarity. In In AAAI06.
Jeff Mitchell and Mirella Lapata. 2010. Composition in
distributional models of semantics. Cognitive Science,
34(8):1388?1429.
Alessandro Moschitti. 2006. Efficient convolution ker-
nels for dependency and constituent syntactic trees. In
ECML, pages 318?329, Berlin, Germany, September.
Machine Learning: ECML 2006, 17th European Con-
ference on Machine Learning, Proceedings.
Mehran Sahami and Timothy D. Heilman. 2006. A web-
based kernel function for measuring the similarity of
short text snippets. In Proceedings of the 15th inter-
national conference on World Wide Web, WWW ?06,
pages 377?386, New York, NY, USA. ACM.
Magnus Sahlgren. 2006. The Word-Space Model. Ph.D.
thesis, Stockholm University.
John Shawe-Taylor and Nello Cristianini. 2004. Kernel
Methods for Pattern Analysis. Cambridge University
Press, New York, NY, USA.
Alex J. Smola and Bernhard Scho?lkopf. 2004. A tutorial
on support vector regression. Statistics and Comput-
ing, 14(3):199?222, August.
Frane S?aric?, Goran Glavas?, Mladen Karan, Jan S?najder,
and Bojana Dalbelo Bas?ic?. 2012. Takelab: Systems
for measuring semantic text similarity. In *SEM 2012,
pages 441?448, Montre?al, Canada, 7-8 June.
65
