Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 879?887,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Comparing Objective and Subjective Measures of Usability
in a Human-Robot Dialogue System
Mary Ellen Foster and Manuel Giuliani and Alois Knoll
Informatik VI: Robotics and Embedded Systems
Technische Universita?t Mu?nchen
Boltzmannstra?e 3, 85748 Garching bei Mu?nchen, Germany
{foster,giuliani,knoll}@in.tum.de
Abstract
We present a human-robot dialogue sys-
tem that enables a robot to work together
with a human user to build wooden con-
struction toys. We then describe a study in
which na??ve subjects interacted with this
system under a range of conditions and
then completed a user-satisfaction ques-
tionnaire. The results of this study pro-
vide a wide range of subjective and ob-
jective measures of the quality of the in-
teractions. To assess which aspects of the
interaction had the greatest impact on the
users? opinions of the system, we used a
method based on the PARADISE evalua-
tion framework (Walker et al, 1997) to de-
rive a performance function from our data.
The major contributors to user satisfac-
tion were the number of repetition requests
(which had a negative effect on satisfac-
tion), the dialogue length, and the users?
recall of the system instructions (both of
which contributed positively).
1 Introduction
Evaluating the usability of a spoken language dia-
logue system generally requires a large-scale user
study, which can be a time-consuming process
both for the experimenters and for the experimen-
tal subjects. In fact, it can be difficult even to
define what the criteria are for evaluating such a
system (cf. Novick, 1997). In recent years, tech-
niques have been introduced that are designed to
predict user satisfaction based on more easily mea-
sured properties of an interaction such as dialogue
length and speech-recognition error rate. The de-
sign of such performance methods for evaluating
dialogue systems is still an area of open research.
The PARADISE framework (PARAdigm for
DIalogue System Evaluation; Walker et al (1997))
describes a method for using data to derive a per-
formance function that predicts user-satisfaction
scores from the results on other, more easily com-
puted measures. PARADISE uses stepwise mul-
tiple linear regression to model user satisfaction
based on measures representing the performance
dimensions of task success, dialogue quality, and
dialogue efficiency, and has been applied to a wide
range of systems (e.g., Walker et al, 2000; Litman
and Pan, 2002; Mo?ller et al, 2008). If the result-
ing performance function can be shown to predict
user satisfaction as a function of other, more eas-
ily measured system properties, it will be widely
applicable: in addition to making it possible to
evaluate systems based on automatically available
data from log files without the need for extensive
experiments with users, for example, such a per-
formance function can be used in an online, incre-
mental manner to adapt system behaviour to avoid
entering a state that is likely to reduce user satis-
faction, or can be used as a reward function in a
reinforcement-learning scenario (Walker, 2000).
Automated evaluation metrics that rate sys-
tem behaviour based on automatically computable
properties have been developed in a number of
other fields: widely used measures include BLEU
(Papineni et al, 2002) for machine translation and
ROUGE (Lin, 2004) for summarisation, for exam-
ple. When employing any such metric, it is cru-
cial to verify that the predictions of the automated
evaluation process agree with human judgements
of the important aspects of the system output. If
not, the risk arises that the automated measures do
not capture the behaviour that is actually relevant
for the human users of a system. For example,
Callison-Burch et al (2006) presented a number of
879
counter-examples to the claim that BLEU agrees
with human judgements. Also, Foster (2008) ex-
amined a range of automated metrics for evalua-
tion generated multimodal output and found that
few agreed with the preferences expressed by hu-
man judges.
In this paper, we apply a PARADISE-style pro-
cess to the results of a user study of a human-robot
dialogue system. We build models to predict the
results on a set of subjective user-satisfaction mea-
sures, based on objective measures that were either
gathered automatically from the system logs or de-
rived from the video recordings of the interactions.
The results indicate that the most significant con-
tributors to user satisfaction were the number of
system turns in the dialogues, the users? ability to
recall the instructions given by the robot, and the
number of times that the user had to ask for in-
structions to be repeated. The former two mea-
sures were positively correlated with user satisfac-
tion, while the latter had a negative impact on user
satisfaction; however the correlation in all cases
was relatively low. At the end of the paper, we
discuss possible reasons for these results and pro-
pose other measures that might have a larger effect
on users? judgements.
2 Task-Based Human-Robot Dialogue
This study makes use of the JAST human-robot
dialogue system (Rickert et al, 2007) which sup-
ports multimodal human-robot collaboration on a
joint construction task. The user and the robot
work together to assemble wooden construction
toys on a common workspace, coordinating their
actions through speech, gestures, and facial dis-
plays. The robot (Figure 1) consists of a pair
of manipulator arms with grippers, mounted in
a position to resemble human arms, and an an-
imatronic talking head (van Breemen, 2005) ca-
pable of producing facial expressions, rigid head
motion, and lip-synchronised synthesised speech.
The system can interact in English or German.
The robot is able to manipulate objects in the
workspace and to perform simple assembly tasks.
In the system that was used in the current study,
the robot instructs the user on building a partic-
ular compound object, explaining the necessary
assembly steps and retrieving pieces as required,
with the user performing the actual assembly ac-
tions. To make joint action necessary for success
in the assembly task, the workspace is divided into
Figure 1: The JAST dialogue robot
SYSTEM First we will build a windmill. Okay?
USER Okay.
SYSTEM To make a windmill, we must make a
snowman.
SYSTEM [picking up and holding out red cube]
To make a snowman, insert the green bolt
through the end of this red cube and screw
it into the blue cube.
USER [takes cube, performs action] Okay.
SYSTEM [picking up and holding out a small
slat] To make a windmill, insert the yellow
bolt through the middle of this short slat and
the middle of another short slat and screw it
into the snowman.
USER [takes slat, performs action] Okay.
SYSTEM Very good!
Figure 2: Sample human-robot dialogue
880
(a) Windmill (b) Snowman (c) L Shape (d) Railway signal
Figure 3: The four target objects used in the experiment
two areas?one belonging to the robot and one to
the user?so that the robot must hand over some
pieces to the user. Figure 2 shows a sample dia-
logue in which the system explains to the user how
to build an object called a ?windmill?, which has a
sub-component called a ?snowman?.
3 Experiment Design
The human-robot system was evaluated via a user
study in which subjects interacted with the com-
plete system; all interactions were in German. As
a between-subjects factor, we manipulated two as-
pects of the generated output: the strategy used by
the dialogue manager to explain a plan to the user,
and the type of referring expressions produced by
the system. Foster et al (2009) give the details
of these factors and describes the effects of each
individual manipulation. In this paper, we concen-
trate on the relationships among the different fac-
tors that were measured during the study: the effi-
ciency and quality of the dialogues, the users? suc-
cess at building the required objects and at learn-
ing the construction plans for new objects, and the
users? subjective reactions to the system.
3.1 Subjects
43 subjects (27 male) took part in this experi-
ment; the results of one additional subject were
discarded due to technical problems with the sys-
tem. The mean age of the subjects was 24.5, with a
minimum of 14 and a maximum of 55. Of the sub-
jects who indicated an area of study, the two most
common areas were Informatics (12 subjects) and
Mathematics (10). On a scale of 1?5, subjects
gave a mean assessment of their knowledge of
computers at 3.4, of speech-recognition systems
at 2.3, and of human-robot systems at 2.0. The
subjects were compensated for their participation
in the experiment.
3.2 Scenario
In this experiment, each subject built the same
three objects in collaboration with the system,
always in the same order. The first target
was a ?windmill? (Figure 3a), which has a sub-
component called a ?snowman? (Figure 3b). Once
the windmill was completed, the system then
walked the user through building an ?L shape?
(Figure 3c). Finally, the robot instructed the user
to build a ?railway signal? (Figure 3d), which com-
bines an L shape with a snowman. During the con-
struction of the railway signal, the system asked
the user if they remembered how to build a snow-
man and an L shape. If the user did not remember,
the system explained the building process again; if
they did remember, the system simply told them to
build another one.
3.3 Dependent Variables
We gathered a wide range of dependent measures:
objective measures derived from the system logs
and video recordings, as well as subjective mea-
sures based on the users? own ratings of their ex-
perience interacting with the system.
3.3.1 Objective Measures
We collected a range of objective measures from
the log files and videos of the interactions. Like
Litman and Pan (2002), we divided our objective
measures into three categories based on those used
in the PARADISE framework: dialogue efficiency,
dialogue quality, and task success.
The dialogue efficiency measures concentrated
on the timing of the interaction: the time taken to
complete the three construction tasks, the number
of system turns required for the complete interac-
tion, and the mean time taken by the system to re-
spond to the user?s requests.
We considered four measures of dialogue qual-
ity. The first two measures looked specifically for
signs of problems in the interaction, using data au-
881
tomatically extracted from the logs: the number of
times that the user asked the system to repeat its
instructions, and the number of times that the user
failed to take an object that the robot attempted to
hand over. The other two dialogue quality mea-
sures were computed based on the video record-
ings: the number of times that the user looked at
the robot, and the percentage of the total inter-
action that they spent looking at the robot. We
considered these gaze-based measures to be mea-
sures of dialogue quality since it has previously
been shown that, in this sort of task-based interac-
tion where there is a visually salient object, par-
ticipants tend to look at their partner more often
when there is a problem in the interaction (e.g.,
Argyle and Graham, 1976).
The task success measures addressed user suc-
cess in the two main tasks undertaken in these in-
teractions: assembling the target objects following
the robot?s instructions, and learning and remem-
bering to make a snowman and an L shape. We
measured task success in two ways, correspond-
ing to these two main tasks. The user?s success in
the overall assembly task was assessed by count-
ing the proportion of target objects that were as-
sembled as intended (i.e., as in Figure 3), which
was judged based on the video recordings. To
test whether the subjects had learned how to build
the sub-components that were required more than
once (the snowman and the L shape), we recorded
whether they said yes or no when they were asked
if they remembered each of these components dur-
ing the construction of the railway signal.
3.3.2 Subjective Measures
In addition to the above objective measures, we
also gathered a range of subjective measures. Be-
fore the interaction, we asked subjects to rate their
current level on a set of 22 emotions (Ortony
et al, 1988) on a scale from 1 to 4; the subjects
then rated their level on the same emotional scales
again after the interaction. After the interaction,
the subjects also filled out a user-satisfaction ques-
tionnaire, which was based on that used in the user
evaluation of the COMIC dialogue system (White
et al, 2005), with modifications to address spe-
cific aspects of the current dialogue system and the
experimental manipulations in this study. There
were 47 items in total, each of which requested
that the user choose their level of agreement with
a given statement on a five-point Likert scale. The
items were divided into the following categories:
Mean (Stdev) Min Max
Length (sec) 305.1 (54.0) 195.2 488.4
System turns 13.4 (1.73) 11 18
Response time (sec) 2.79 (1.13) 1.27 7.21
Table 1: Dialogue efficiency results
Opinion of the robot as a partner 21 items ad-
dressing the ease with which subjects were
able to interact with the robot
Instruction quality 6 items specifically address-
ing the quality of the assembly instructions
given by the robot
Task success 11 items asking the user to rate how
well they felt they performed on the various
assembly tasks
Feelings of the user 9 items asking users to rate
their feelings while using the system
At the end of the questionnaire, subjects were also
invited to give free-form comments.
4 Results
In this section, we present the results of each of
the individual dependent measures; in the follow-
ing section, we examine the relationship among
the different types of measures. These results are
based on the data from 40 subjects: we excluded
results from two subjects for whom the video data
was not clear, and from one additional subject who
appeared to be ?testing? the system rather than
making a serious effort to interact with it.
4.1 Objective Measures
Dialogue efficiency The results on the dialogue
efficiency measures are shown in Table 1. The
average subject took 305.1 seconds?that is, just
over five minutes?to build all three of the objects,
and an average dialogue took 13 system turns to
complete. When a user made a request, the mean
delay before the beginning of the system response
was about three seconds, although for one user this
time was more than twice as long. This response
delay resulted from two factors. First, prepar-
ing long system utterances with several referring
expressions (such as the third and fourth system
turns in Figure 2) takes some time; second, if a
user made a request during a system turn (i.e., a
?barge-in? attempt), the system was not able to re-
spond until the current turn was completed.
882
Mean (Stdev) Min Max
Repetition requests 1.86 (1.79) 0 6
Failed hand-overs 1.07 (1.35) 0 6
Looks at the robot 23.55 (8.21) 14 50
Time looking at robot (%) 27 (8.6) 12 51
Table 2: Dialogue quality results
These three measures of efficiency were cor-
related with each other: the correlation between
length and turns was 0.38; between length and re-
sponse time 0.47; and between turns and response
time 0.19 (all p < 0.0001).
Dialogue quality Table 2 shows the results for
the dialogue quality measures: the two indica-
tions of problems, and the two measures of the
frequency with which the subjects looked at the
robot?s head. On average, a subject asked for an
instruction to be repeated nearly two times per
interaction, while failed hand-overs occurred just
over once per interaction; however, as can be seen
from the standard-deviation values, these mea-
sures varied widely across the data. In fact, 18
subjects never failed to take an object from the
robot when it was offered, while one subject did so
five times and one six times. Similarly, 11 subjects
never asked for any repetitions, while five subjects
asked for repetitions five or more times.1 On aver-
age, the subjects in this study spent about a quarter
of the interaction looking at the robot head, and
changed their gaze to the robot 23.5 times over
the course of the interaction. Again, there was a
wide range of results for both of these measures:
15 subjects looked at the robot fewer than 20 times
during the interaction, 20 subjects looked at the
robot between 20 to 30 times, while 5 subjects
looked at the robot more than 30 times.
The two measures that count problems were
mildly correlated with each other (R2 = 0.26, p <
0.001), as were the two measures of looking at the
robot (R2 = 0.13, p < 0.05); there was no correla-
tion between the two classes of measures.
Task success Table 3 shows the success rate for
assembling each object in the sequence. Objects
in italics represent sub-components, as follows:
the first snowman was constructed as part of the
windmill, while the second formed part of the rail-
way signal; the first L-shape was a goal in itself,
1The requested repetition rate was significantly affected
by the description strategy used by the dialogue manager; see
Foster et al (2009) for details.
Object Rate Memory
Snowman 0.76
Windmill 0.55
L shape 0.90
L shape 0.90 0.88
Snowman 0.86 0.70
Railway signal 0.71
Overall 0.72 0.79
Table 3: Task success results
while the second was also part of the process of
building the railway signal. The Rate column indi-
cates subjects? overall success at building the rel-
evant component?for example, 55% of the sub-
jects built the windmill correctly, while both of
the L-shapes were built with 90% accuracy. For
the second occurrence of the snowman and the L-
shape, the Memory column indicates the percent-
age of subjects who claimed to remember how to
build it when asked. The Overall row at the bottom
indicates subjects? overall success rate at building
the three main target objects (windmill, L shape,
railway signal): on average, a subject built about
two of the three objects correctly.
The overall correct-assembly rate was corre-
lated with the overall rate of remembering objects:
R2 = 0.20, p < 0.005. However, subjects who said
that they did remember how to build a snowman or
an L shape the second time around were no more
likely to do it correctly than those who said that
they did not remember.
4.2 Subjective Measures
Two types of subjective measures were gath-
ered during this study: responses on the user-
satisfaction questionnaire, and self-assessment of
emotions. Table 4 shows the mean results for each
category from the user-satisfaction questionnaire
across all of the subjects, in all cases on a 5-point
Likert scale. The subjects in this study gave a
generally positive assessment of their interactions
with the system?with a mean overall satisfaction
score of 3.75?and rated their perceived task suc-
cess particularly highly, with a mean score of 4.1.
To analyse the emotional data, we averaged all
of the subjects? emotional self-ratings before and
after the experiment, counting negative emotions
on an inverse scale, and then computed the differ-
ence between the two means. Table 5 shows the re-
sults from this analysis; note that this value was as-
sessed on a 1?4 scale. While the mean emotional
883
Question category Mean (Stdev)
Robot as partner 3.63 (0.65)
Instruction quality 3.69 (0.71)
Task success 4.10 (0.68)
Feelings 3.66 (0.61)
Overall 3.75 (0.57)
Table 4: User-satisfaction questionnaire results
Mean (Stdev) Min Max
Before the study 2.99 (0.32) 2.32 3.68
After the study 3.05 (0.32) 2.32 3.73
Change +0.06 (0.24) ?0.55 +0.45
Table 5: Mean emotional assessments
score across all of the subjects did not change over
the course of the experiment, the ratings of indi-
vidual subjects did show larger changes. As shown
in the final row of the table, one subject?s mean rat-
ing decreased by 0.55 over the course of the inter-
action, while that of another subject increased by
0.45. There was a slight correlation between the
subjects? description of their emotional state after
the experiment and their responses to the question-
naire items asking for feelings about the interac-
tion: R2 = 0.14, p < 0.01.
5 Building Performance Functions
In the preceding section, we presented results on a
number of objective and subjective measures, and
also examined the correlation among measures of
the same type. The results on the objective mea-
sures varied widely across the subjects; also, the
subjects generally rated their experience of using
the system positively, but again with some varia-
tion. In this section, we examine the relationship
among measures of different types in order to de-
termine which of the objective measures had the
largest effect on users? subjective reactions to the
dialogue system.
To determine the relationship among the fac-
tors, we employed the procedure used in the
PARADISE evaluation framework (Walker et al,
1997). The PARADISE model uses stepwise mul-
tiple linear regression to predict subjective user
satisfaction based on measures representing the
performance dimensions of task success, dialogue
quality, and dialogue efficiency, resulting in a pre-
dictor function of the following form:
Satisfaction =
n
?
i=1
wi ?N (mi)
The mi terms represent the value of each measure,
while the N function transforms each measure
into a normal distribution using z-score normali-
sation. Stepwise linear regression produces coef-
ficients (wi) describing the relative contribution of
each predictor to the user satisfaction. If a predic-
tor does not contribute significantly, its wi value is
zero after the stepwise process.
Using stepwise linear regression, we computed
a predictor function for each of the subjective mea-
sures that we gathered during our study: the mean
score for each of the individual user-satisfaction
categories (Table 4), the mean score across the
whole questionnaire (the last line of Table 4), as
well as the difference between the users? emo-
tional states before and after the study (the last line
of Table 5). We included all of the objective mea-
sures from Section 4.1 as initial predictors.
The resulting predictor functions are shown in
Table 6. The following abbreviations are used for
the factors that occur in the table: Rep for the num-
ber of repetition requests, Turns for the number of
system turns, Len for the length of the dialogue,
and Mem for the subjects? memory for the com-
ponents that were built twice. The R2 column in-
dicates the percentage of the variance that is ex-
plained by the performance function, while the
Significance column gives significance values for
each term in the function.
Although the R2 values for the predictor func-
tions in Table 6 are generally quite low, indicat-
ing that the functions do not explain most of the
variance in the data, the factors that remain after
stepwise regression still provide an indication as
to which of the objective measures had an effect
on users? opinions of the system. In general, users
who had longer interactions with the system (in
terms of system turns) and who said that they re-
membered the robot?s instructions tended to give
the system higher scores, while users who asked
for more instructions to be repeated tended to give
it lower scores; for the robot-as-partner questions,
the length of the dialogue in seconds also made a
slight negative contribution. None of the other ob-
jective factors contributed significantly to any of
the predictor functions.
6 Discussion
That the factors included in Table 6 were the most
significant contributors to user satisfaction is not
surprising. If a user asks for instructions to be re-
884
Measure Function R2 Significance
Robot as partner 3.60+0.53?N (Turns)?0.39?N (Rep)?0.18?N (Len) 0.12 Turns: p < 0.01,
Rep: p < 0.05,
Length: p? 0.17
Instruction quality 3.66?0.22?N (Rep) 0.081 Rep: p < 0.05
Task success 4.07+0.20?N (Mem) 0.058 Mem: p? 0.07
Feelings 3.63+0.34?N (Turns)?0.32?N (Rep) 0.044 Turns: p? 0.06, Rep:
p? 0.08
Overall 3.73?0.36?N (Rep)+0.31?N (Turns) 0.062 Rep: p < 0.05,
Turns: p? 0.06
Emotion change 0.07+0.14?N (Turns)+0.11?N (Mem)?0.090?N (Rep) 0.20 Turns: p < 0.05,
Mem: p < 0.01,
Rep: p? 0.17
Table 6: Predictor functions
peated, this is a clear indication of a problem in
the dialogue; similarly, users who remembered the
system?s instructions were equally clearly having
a relatively successful interaction.
In the current study, increased dialogue length
had a positive contribution to user satisfaction; this
contrasts with results such as those of Litman and
Pan (2002), who found that increased dialogue
length was associated with decreased user satis-
faction. We propose two possible explanations for
this difference. First, the system analysed by Lit-
man and Pan (2002) was an information-seeking
dialogue system, in which efficient access to the
information is an important criterion. The current
system, on the other hand, has the goal of joint task
execution, and pure efficiency is a less compelling
measure of dialogue quality in this setting. Sec-
ond, it is possible that the sheer novelty factor of
interacting with a fully-embodied humanoid robot
affected people?s subjective responses to the sys-
tem, so that subjects who had longer interactions
also enjoyed the experience more. Support for this
explanation is provided by the fact that dialogue
length was only a significant factor in the more
?subjective? parts of the questionnaire, but did not
have a significant impact on the users? judgements
about instruction quality or task success. Other
studies of human-robot dialogue systems have also
had similar results: for example, the subjects in the
study described by Sidner et al (2005) who used
a robot that moved while talking reported higher
levels of engagement in the interaction, and also
tended to have longer conversations with the robot.
While the predictor functions give useful in-
sights into the relative contribution of the objective
measures to the subjective user satisfaction, the
R2 values are generally lower than those found in
other PARADISE-style evaluations. For example,
Walker et al (1998) reported an R2 value of 0.38,
the values reported by Walker et al (2000) on the
training sets ranged from 0.39 to 0.56, Litman and
Pan (2002) reported an R2 value of 0.71, while
the R2 values reported by Mo?ller et al (2008)
for linear regression models similar to those pre-
sented here were between 0.22 and 0.57. The low
R2 values from this analysis clearly suggest that,
while the factors included in Table 6 did affect
users? opinions?particularly their opinion of the
robot as a partner and the change in their reported
emotional state?the users? subjective judgements
were also affected by factors other than those cap-
tured by the objective measures considered here.
In most of the previous PARADISE-style stud-
ies, measures addressing the performance of the
automated speech-recognition system and other
input-processing components were included in the
models. For example, the factors listed by Mo?ller
et al (2008) include several measures of word er-
ror rate and of parsing accuracy. However, the sce-
nario that was used in the current study required
minimal speech input from the user (see Figure 2),
so we did not include any such input-processing
factors in our models.
Other objective factors that might be relevant
for predicting user satisfaction in the current study
include a range of non-verbal behaviour from the
users. For example, the user?s reaction time to in-
structions from the robot, the time the users need
to adapt to the robot?s movements during hand-
over actions (Huber et al, 2008), or the time taken
for the actual assembly of the objects. Also, other
measures of the user?s gaze behaviour might be
885
useful: more global measures such as how often
the users look at the robot arms or at the objects on
the table, as well as more targeted measures exam-
ining factors such as the user?s gaze and other be-
haviour during and after different types of system
outputs. In future studies, we will also gather data
on these additional non-verbal behaviours, and we
expect to find higher correlations with subjective
judgements.
7 Conclusions and Future Work
We have presented the JAST human-robot dia-
logue system and described a user study in which
the system instructed users to build a series of tar-
get objects out of wooden construction toys. This
study resulted in a range of objective and subjec-
tive measures, which were used to derive perfor-
mance functions in the style of the PARADISE
evaluation framework. Three main factors were
found to affect the users? subjective ratings: longer
dialogues and higher recall performance were as-
sociated with increased user satisfaction, while di-
alogues with more repetition requests tended to be
associated with lower satisfaction scores. The ex-
plained variance of the performance functions was
generally low, suggesting that factors other than
those measured in this study contributed to the
user satisfaction scores; we have suggested several
such factors.
The finding that longer dialogues were associ-
ated with higher user satisfaction disagrees with
the results of many previous PARADISE-style
evaluation studies. However, it does confirm and
extend the results of previous studies specifically
addressing interactions between users and embod-
ied agents: as in the previous studies, the users in
this case seem to view the agent as a social entity
with whom they enjoy having a conversation.
A newer version of the JAST system is currently
under development and will shortly undergo a user
evaluation. This new system will support an ex-
tended set of interactions where both agents know
the target assembly plan, and will will also in-
corporate enhanced components for vision, object
recognition, and goal inference. When evaluat-
ing this new system, we will include similar mea-
sures to those described here to enable the eval-
uations of the two systems to be compared. We
will also gather additional objective measures in
order to measure their influence on the subjective
results. These additional measures will include
those mentioned at the end of the preceding sec-
tion, as well as measures targeted at the revised
scenario and the updated system capabilities?for
example, an additional dialogue quality measure
will assess how often the goal-inference system
was able to detect and correctly respond to an error
by the user.
Acknowledgements
This research was supported by the Euro-
pean Commission through the JAST2 (IST-
FP6-003747-IP) and INDIGO3 (IST-FP6-045388)
projects. Thanks to Pawel Dacka for his help in
running the experiment and analysing the data.
References
M. Argyle and J. A. Graham. 1976. The Cen-
tral Europe experiment: Looking at persons and
looking at objects. Environmental Psychology
and Nonverbal Behavior, 1(1):6?16. doi:10.
1007/BF01115461.
A. J. N. van Breemen. 2005. iCat: Experimenting
with animabotics. In Proceedings of the AISB
2005 Creative Robotics Symposium.
C. Callison-Burch, M. Osborne, and P. Koehn.
2006. Re-evaluating the role of BLEU in ma-
chine translation research. In Proceedings of
EACL 2006. ACL Anthology E06-1032.
M. E. Foster. 2008. Automated metrics that agree
with human judgements on generated output for
an embodied conversational agent. In Proceed-
ings of INLG 2008. ACL Anthology W08-1113.
M. E. Foster, M. Giuliani, A. Isard, C. Matheson,
J. Oberlander, and A. Knoll. 2009. Evaluating
description and reference strategies in a coop-
erative human-robot dialogue system. In Pro-
ceedings of IJCAI 2009.
M. Huber, M. Rickert, A. Knoll, T. Brandt, and
S. Glasauer. 2008. Human-robot interaction in
handing-over tasks. In Proceedings of IEEE
RO-MAN 2008. doi:10.1109/ROMAN.2008.
4600651.
C. Y. Lin. 2004. ROUGE: A package for auto-
matic evaluation of summaries. In Proceedings
of the ACL 2004 Workshop on Text Summariza-
tion. ACL Anthology W04-1013.
2http://www.euprojects-jast.net/
3http://www.ics.forth.gr/indigo/
886
D. J. Litman and S. Pan. 2002. Designing and
evaluating an adaptive spoken dialogue sys-
tem. User Modeling and User-Adapted Inter-
action, 12(2?3):111?137. doi:10.1023/A:
1015036910358.
S. Mo?ller, K.-P. Engelbrecht, and R. Schleicher.
2008. Predicting the quality and usability of
spoken dialogue systems. Speech Communica-
tion, 50:730?744. doi:10.1016/j.specom.
2008.03.001.
D. G. Novick. 1997. What is effective-
ness? In Working notes, CHI ?97 Work-
shop on HCI Research and Practice Agenda
Based on Human Needs and Social Responsi-
bility. http://www.cs.utep.edu/novick/
papers/eff.chi.html.
A. Ortony, G. L. Clore, and A. Collins. 1988. The
Cognitive Structure of Emotions. Cambridge
University Press.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu.
2002. BLEU: A method for automatic evalua-
tion of machine translation. In Proceedings of
ACL 2002. ACL Anthology P02-1040.
M. Rickert, M. E. Foster, M. Giuliani, T. By,
G. Panin, and A. Knoll. 2007. Integrating lan-
guage, vision and action for human robot dialog
systems. In Proceedings of HCI International
2007. doi:10.1007/978-3-540-73281-5_
108.
C. L. Sidner, C. Lee, C. D. Kidd, N. Lesh, and
C. Rich. 2005. Explorations in engagement
for humans and robots. Artificial Intelligence,
166(1?2):140?164. doi:10.1016/j.artint.
2005.03.005.
M. Walker, C. Kamm, and D. Litman. 2000. To-
wards developing general models of usability
with PARADISE. Natural Language Engineer-
ing, 6(3?4):363?377.
M. A. Walker. 2000. An application of reinforce-
ment learning to dialogue strategy selection in
a spoken dialogue system for email. Journal of
Artificial Intelligence Research, 12:387?416.
M. A. Walker, J. Fromer, G. D. Fabbrizio, C. Mes-
tel, and D. Hindle. 1998. What can I say?: Eval-
uating a spoken language interface to email.
In Proceedings of CHI 1998. doi:10.1145/
274644.274722.
M. A. Walker, D. J. Litman, C. A. Kamm, and
A. Abella. 1997. PARADISE: A framework for
evaluating spoken dialogue agents. In Proceed-
ings of ACL/EACL 1997. ACL Anthology P97-
1035.
M. White, M. E. Foster, J. Oberlander, and
A. Brown. 2005. Using facial feedback to en-
hance turn-taking in a multimodal dialogue sys-
tem. In Proceedings of HCI International 2005.
887
Situated Reference in a Hybrid Human-Robot Interaction System
Manuel Giuliani1 and Mary Ellen Foster2 and Amy Isard3
Colin Matheson3 and Jon Oberlander3 and Alois Knoll1
1Informatik VI: Robotics and Embedded Systems, Technische Universita?t Mu?nchen
2School of Mathematical and Computer Sciences, Heriot-Watt University, Edinburgh
3Institute for Communicating and Collaborative Systems, School of Informatics, University of Edinburgh
Abstract
We present the situated reference genera-
tion module of a hybrid human-robot in-
teraction system that collaborates with a
human user in assembling target objects
from a wooden toy construction set. The
system contains a sub-symbolic goal in-
ference system which is able to detect the
goals and errors of humans by analysing
their verbal and non-verbal behaviour. The
dialogue manager and reference genera-
tion components then use situated refer-
ences to explain the errors to the human
users and provide solution strategies. We
describe a user study comparing the results
from subjects who heard constant refer-
ences to those who heard references gener-
ated by an adaptive process. There was no
difference in the objective results across
the two groups, but the subjects in the
adaptive condition gave higher subjective
ratings to the robot?s abilities as a conver-
sational partner. An analysis of the objec-
tive and subjective results found that the
main predictors of subjective user satisfac-
tion were the user?s performance at the as-
sembly task and the number of times they
had to ask for instructions to be repeated.
1 Introduction
When two humans jointly carry out a mutual task
for which both know the plan?for example, as-
sembling a new shelf?it frequently happens that
one makes an error, and the other has to assist
and to explain what the error was and how it can
be solved. Humans are skilled at spotting errors
committed by another, as well as errors which
they made themselves. Recent neurological stud-
ies have shown that error monitoring?i.e., ob-
serving the errors made by oneself or by others?
plays an important role in joint activity. For ex-
ample, Bekkering et al (2009) have demonstrated
that humans show the same brain activation pat-
terns when they make an error themselves and
when they observe someone else making an error.
In this paper, we describe a human-robot inter-
action (HRI) system that is able both to analyse
the actions and the utterances of a human part-
ner to determine if the human made an error in
the assembly plan, and to explain to the human
what went wrong and what to do to solve the prob-
lem. This robot combines approaches from sub-
symbolic processing and symbolic reasoning in a
hybrid architecture based on that described in Fos-
ter et al (2008b).
During the construction process, it is frequently
necessary to refer to an object which is being used
to assemble the finished product, choosing an un-
ambigious reference to distinguish the object from
the others available. The classic reference gen-
eration algorithm, on which most subsequent im-
plementations are based, is the incremental algo-
rithm of Dale and Reiter (1995), which selects
a set of attributes of a target object to single it
out from a set of distractor objects. In real-world
tasks, the speaker and hearer often have more con-
text in common than just the knowledge of object
attributes, and several extensions have been pro-
posed, dealing with visual and discourse salience
(Kelleher and Kruijff, 2006) and the ability to pro-
duce multimodal references including actions such
as pointing (van der Sluis, 2005; Kranstedt and
Wachsmuth, 2005).
Foster et al (2008a) noted another type of mul-
timodal reference which is particularly useful in
embodied, task-based contexts: haptic-ostensive
reference, in which an object is referred to as it
is being manipulated by the speaker. Manipulat-
ing an object, which must be done in any case as
part of the task, also makes an object more salient
and therefore affords linguistic references that in-
Figure 1: The dialogue robot
dicate the increased accessibility of the referent.
This type of reference is similar to the placing-for
actions noted by Clark (1996).
An initial approach for generating referring ex-
pressions that make use of haptic-ostensive refer-
ence was described in (Foster et al, 2009a). With
this system, a study was conducted comparing the
new reference strategy to the basic Dale and Reiter
incremental algorithm. Na??ve users reported that it
was significantly easier to understand the instruc-
tions given by the robot when it used references
generated by the more sophisticated algorithm. In
this paper, we perform a similar experiment, but
making use of a more capable human-robot in-
teraction system and a more complete process for
generating situated references.
2 Hybrid Human-Robot Dialogue
System
The experiment described in this paper makes use
of a hybrid human-robot dialogue system which
supports multimodal human-robot collaboration
on a joint construction task. The robot (Figure 1)
has a pair of manipulator arms with grippers,
mounted in a position to resemble human arms,
and an animatronic talking head (van Breemen,
2005) capable of producing facial expressions,
rigid head motion, and lip-synchronised synthe-
sised speech. The subject and the robot work to-
gether to assemble wooden construction toys on
a common workspace, coordinating their actions
through speech (English or German), gestures, and
facial expressions.
The robot can pick up and move objects in the
workspace and perform simple assembly tasks. In
the scenario considered here, both of the partici-
pants know the assembly plan and jointly execute
it. The robot assists the human, explains necessary
assembly steps in case the human makes an error,
and offers pieces as required. The workspace is di-
vided into two areas?one belonging to the robot
and one to the human?to make joint action nec-
essary for task success.
The system has components which use both
sub-symbolic and symbolic processing. It in-
cludes a goal inference module based on dynamic
neural fields (Erlhagen and Bicho, 2006; Bicho
et al, 2009), which selects the robot?s next actions
based on the human user?s actions and utterances.
Given a particular assembly plan and the knowl-
edge of which objects the user has picked up, this
module can determine when the user has made
an error. The system also incorporates a dialogue
manager based on the TrindiKit dialogue manage-
ment toolkit (Larsson and Traum, 2000), which
implements the information-state based approach
to dialogue management. This unique combina-
tion of abilities means that when the robot detects
that its human partner has made an error?for ex-
ample, picking up or requesting an assembly piece
that is not needed in the current step of the building
plan?it can explain to the human what the error
was and what can be done to correct the mistake?
for example by picking up or indicating the correct
assembly piece.
Messages from all of the system?s input chan-
nels (speech, object recognition, and gesture
recognition) are processed and combined by a
multimodal fusion component based on (Giuliani
and Knoll, 2008), which is the link between the
symbolic and the sub-symbolic parts of the sys-
tem. The fusion component then communicates
with the goal inference module, which calculates
the next action instructions for the robot and also
determines if the user made an error. From there,
fusion combines the information from goal infer-
ence with the input data and sends unified hy-
potheses to the dialogue manager.
When it receives the fusion hypotheses, the dia-
logue manager uses the dialogue history and the
physical and task context to choose a response.
It then sends a high-level specification of the de-
1. System First we will build a windmill.
2. User Okay.
3. User {picks up a yellow cube, unnecessary piece for a
windmill}
4. System You don?t need a yellow cube to build a windmill.
5. System To build a windmill, you first need to build a
tower.
6. System [picking up and holding out red cube] To build
the tower, insert the green bolt through the end of this
red cube and screw it into the blue cube.
7. User [takes cube, performs action] Okay.
Figure 2: Sample human-robot dialogue, showing
adaptively-generated situated references
sired response to the output planner, which in turn
sends commands to each output channel: linguis-
tic content (including multimodal referring ex-
pressions), facial expressions and gaze behaviours
of the talking head, and actions of the robot ma-
nipulators. The linguistic outputs are realised us-
ing the OpenCCG surface realiser (White, 2006).
3 Reference Generation
In this system, two strategies were implemented
for generating references to objects in the world:
a constant version that uses only the basic incre-
mental algorithm (Dale and Reiter, 1995) to se-
lect properties, and an adaptive version that uses
more of the physical, dialogue and task context
to help select the references. The constant sys-
tem can produce a definite or indefinite reference,
and the most appropriate combination of attributes
according to the incremental algorithm. The adap-
tive system also generates pronominal and deictic
references, and introduces the concept of multiple
types of distractor sets depending on context.
Figure 2 shows a fragment of a sample interac-
tion in which the user picks up an incorrect piece:
the robot detects the error and describes the correct
assembly procedure. The underlined references
show the range of output produced by the adap-
tive reference generation module; for the constant
system, the references would all have been ?the
red cube?. The algorithms used by the adaptive
reference generation module are described below.
3.1 Reference Algorithm
The module stores a history of the referring ex-
pressions spoken by both the system and the user,
and uses these together with distractor sets to se-
lect referring expressions. In this domain there are
two types of objects which we need to refer to:
concrete objects in the world (everything which is
on the table, or in the robot?s or user?s hand), and
objects which do not yet exist, but are in the pro-
cess of being created. For non-existent objects we
do not build a distractor set, but simply use the
name of the object. In all other cases, we use one
of three types of distractor set:
? all the pieces needed to build a target object;
? all the objects referred to since the last men-
tion of this object; or
? all the concrete objects in the world.
The first type of set is used if the object under
consideration (OUC) is a negative reference to a
piece in context of the creation of a target object.
In all other cases, the second type is used if the
OUC has been mentioned before and the third type
if it has not.
When choosing a referring expression, we first
process the distractor set, comparing the proper-
ties of the OUC with the properties of all distrac-
tors. If a distractor has a different type from the
OUC, it is removed from the distractor set. With
all other properties, if the distractor has a different
value from the OUC, it is removed from the dis-
tractor set, and the OUC?s property value is added
to the list of properties to use.
We then choose the type of referring expression.
We first look for a previous reference (PR) to the
OUC, and if one exists, determine whether it was
in focus. Depending on the case, we use one of the
following reference strategies.
No PR If the OUC does not yet exist or we are
making a negative reference, we use an indef-
inite article. If the robot is holding the OUC,
we use a deictic reference. If the OUC does
exist and there are no distractors, we use a
definite; if there are distractors we use an in-
definite.
PR was focal If the PR was within the same turn,
we choose a pronoun for our next reference.
If it was in focus but in a previous turn, if
the robot is holding the OUC we use a deictic
reference, and if the robot is not holding it,
we use a pronoun.
PR was not focal If the robot is holding the
OUC, we make a deictic reference. Other-
wise, if the PR was a pronoun, definite, or de-
ictic, we use a definite article. If the PR was
indefinite and there are no distractors, we use
a definite article, if there are distractors, we
use an indefinite article.
If there are any properties in the list, and the
reference chosen is not a pronoun, we add them.
3.2 Examples of the Reference Algorithm
We will illustrate the reference-selection strategy
with two cases from the dialogue in Figure 2.
Utterance 4 ?a yellow cube?
This object is going to be referred to in a negative
context as part of a windmill under construction,
so the distractor set is the set of objects needed to
make a windmill: {red cube, blue cube, small slat,
small slat, green bolt, red bolt}.
We select the properties to use in describing the
object under consideration, processing the distrac-
tor set. We first remove all objects which do not
share the same type as our object under considera-
tion, which leaves {red cube, blue cube}. We then
compare the other attributes of our new object with
the remaining distractors - in this case ?colour?.
Since neither cube shares the colour ?yellow? with
the target object, both are removed from the dis-
tractor set, and ?yellow? is added to the list of
properties to use.
There is no previous reference to this object,
and since we are making a negative reference,
we automatically choose an indefinite article. We
therefore select the reference ?a yellow cube?.
Utterance 6 ?it? (a green bolt)
This object has been referred to before, earlier in
the same utterance, so the distractor set is all the
references between the earlier one and this one?
{red cube}. Since this object has a different type
from the bolt we want to describe, the distractor
set is now empty, and nothing is added to the list
of properties to use.
There is a previous definite reference to the ob-
ject in the same utterance: ?the green bolt?. This
reference was focal, so we are free to use a pro-
noun if appropriate. Since the previous reference
was definite, and the object being referred to does
exist, we choose to use a pronoun. We therefore
select the reference ?it?.
4 Experiment Design
In the context of the HRI system, a constant refer-
ence strategy is sufficient in that it makes it possi-
ble for the robot?s partner to know which item is
needed. On the other hand, while the varied forms
produced by the more complex mechanism can in-
crease the naturalness of the system output, they
may actually be insufficient if they are not used
in appropriate current circumstances?for exam-
ple, ?this cube? is not a particularly helpful refer-
ence if a user has no way to tell which ?this? is.
As a consequence, the system for generating such
references must be sensitive to the current state
of joint actions and?in effect?of joint attention.
The difference between the two systems is a test of
the adaptive version?s ability to adjust expressions
to pertinent circumstances. It is known that peo-
ple respond well to reduced expressions like ?this
cube? or ?it? when another person uses them ap-
propriately (Bard et al, 2008); we need to see if
the robot system can also achieve the benefits that
situated reference could provide.
To address this question, the human-robot di-
alogue system was evaluated through a user study
in which subjects interacted with the complete sys-
tem. Using a between-subjects design, this study
compared the two reference strategies, measuring
the users? subjective reactions to the system along
with their overall performance in the interaction.
Based on the findings from the user evaluation de-
scribed in (Foster et al, 2009a)?in which the pri-
mary effect of varying the reference strategy was
on the users? subjective opinion of the robot?the
main prediction for this study was as follows:
? Subjects who interact with a system using
adaptive references will rate the quality of
the robot?s conversation more highly than the
subjects who hear constant references.
We made no specific prediction regarding the
effect of reference strategy on any of the objec-
tive measures: based on the results of the user
evaluation mentioned above, there is no reason to
expect an effect either way. Note that?as men-
tioned above?if the adaptive version makes in-
correct choices, that may have a negative impact
on users? ability to understand the system?s gener-
ated references. For this reason, even a finding of
(a) Windmill (b) Railway signal
Figure 3: Target objects for the experiment
no objective difference would demonstrate that the
adaptive references did not harm the users? ability
to interact with the system, as long as it was ac-
companied by the predicted improvement in sub-
jective judgements.
4.1 Subjects
41 subjects (33 male) took part in this experiment.
The mean age of the subjects was 24.5, with a min-
imum of 19 and a maximum of 42. Of the subjects
who indicated an area of study, the two most com-
mon areas were Mathematics (14 subjects) and In-
formatics (also 14 subjects). On a scale of 1 to 5,
subjects gave a mean assessment of their knowl-
edge of computers at 4.1, of speech-recognition
systems at 2.0, and of human-robot systems at 1.7.
Subjects were compensated for their participation
in the experiment.
4.2 Scenario
This study used a between-subjects design with
one independent variable: each subject interacted
either with a system that used a constant strategy
to generate referring expressions (19 subjects), or
else with a system that used an adaptive strategy
(22 subjects).1
Each subject built two objects in collaboration
with the system, always in the same order. The
first target object was the windmill (Figure 3a);
after the windmill was completed, the robot and
human then built a railway signal (Figure 3b). For
both target objects, the user was given a building
plan (on paper). To induce an error, both of the
plans given to the subjects instructed them to use
an incorrect piece: a yellow cube instead of a red
cube for the windmill, and a long (seven-hole) slat
instead of a medium (five-hole) slat for the rail-
1The results of an additional three subjects in the constant-
reference condition could not be analysed due to technical
difficulties.
way signal. The subjects were told that the plan
contained an error and that the robot would cor-
rect them when necessary, but did not know the
nature of the error.
When the human picked up or requested an in-
correct piece during the interaction, the system de-
tected the error and explained to the human what
to do in order to assemble the target object cor-
rectly. When the robot explained the error and
when it handed over the pieces, it used referring
expressions that were generated using the constant
strategy for half of the subjects, and the adaptive
strategy for the other half of the subjects.
4.3 Experimental Set-up and Procedure
The participants stood in front of the table facing
the robot, equipped with a headset microphone for
speech recognition. The pieces required for the
target object?plus a set of additional pieces in or-
der to make the reference task more complex?
were placed on the table, using the same layout
for every participant. The layout was chosen to
ensure that there would be points in the interaction
where the subjects had to ask the robot for build-
ing pieces from the robot?s workspace, as well as
situations in which the robot automatically handed
over the pieces. Along with the building plan men-
tioned above, the subjects were given a table with
the names of the pieces they could build the ob-
jects with.
4.4 Data Acquisition
At the end of a trial, the subject responded to
a usability questionnaire consisting of 39 items,
which fell into four main categories: Intelligence
of the robot (13 items), Task ease and task suc-
cess (12 items), Feelings of the user (8 items),
and Conversation quality (6 items). The items on
the questionnaire were based on those used in the
user evaluation described in (Foster et al, 2009b),
but were adapted for the scenario and research
questions of the current study. The questionnaire
was presented using software that let the subjects
choose values between 1 and 100 with a slider. In
addition to the questionnaire, the trials were also
video-taped, and the system log files from all tri-
als were kept for further analysis.
5 Results
We analysed the data resulting from this study in
three different ways. First, the subjects? responses
Table 1: Overall usability results
Constant Adaptive M-W
Intell. 79.0 (15.6) 74.9 (12.7) p = 0.19, n.s.
Task 72.7 (10.4) 71.1 (8.3) p = 0.69, n.s.
Feeling 66.9 (15.9) 66.8 (14.2) p = 0.51, n.s.
Conv. 66.1 (13.6) 75.2 (10.7) p = 0.036, sig.
Overall 72.1 (11.2) 71.8 (9.1) p = 0.68, n.s.
to the questionnaire items were compared to de-
termine if there was a difference between the re-
sponses given by the two groups. A range of sum-
mary objective measures were also gathered from
the log files and videos?these included the dura-
tion of the interaction measured both in seconds
and in system turns, the subjects? success at build-
ing each of the target objects, the number of times
that the robot had to explain the construction plan
to the user, and the number of times that the users
asked the system to repeat its instructions. Finally,
we compared the results on the subjective and ob-
jective measures to determine which of the objec-
tive factors had the largest influence on subjective
user satisfaction.
5.1 Subjective Measures
The subjects in this study gave a generally positive
assessment of their interactions with the system on
the questionnaire?with a mean overall satisfac-
tion score of 72.0 out of 100?and rated the per-
ceived intelligence of the robot particularly highly
(overall mean of 76.8). Table 1 shows the mean
results from the two groups of subjects for each
category on the user-satisfaction questionnaire, in
all cases on a scale from 0?100 (with the scores
for negatively-posed questions inverted).
To test the effect of reference strategy on the
usability-questionnaire responses, we performed a
Mann-Whitney test comparing the distribution of
responses from the two groups of subjects on the
overall results, as well as on each sub-category of
questions. For most categories, there was no sig-
nificant difference between the responses of the
two groups, with p values ranging from 0.19 to
0.69 (as shown in Table 1). The only category
where a significant difference was found was on
the questionnaire items that asked the subjects to
assess the robot?s quality as a conversational part-
ner; for those items, the mean score from sub-
jects who heard the adaptive references was sig-
nificantly higher (p < 0.05) than the mean score
from the subjects who heard references generated
by the constant reference module. Of the six ques-
Table 2: Objective results (all differences n.s.)
Measure Constant Adaptive M-W
Duration (s.) 404.3 (62.8) 410.5 (94.6) p = 0.90
Duration (turns) 29.8 (5.02) 31.2 (5.57) p = 0.44
Rep requests 0.26 (0.45) 0.32 (0.78) p = 0.68
Explanations 2.21 (0.63) 2.41 (0.80) p = 0.44
Successful trials 1.58 (0.61) 1.55 (0.74) p = 0.93
tions that were related to the conversation quality,
the most significant impact was on the two ques-
tions which assessed the subjects? understanding
of what they were able to do at various points dur-
ing the interaction.
5.2 Objective Measures
Based on the log files and video recordings, we
computed a range of objective measures. These
measures were divided into three classes, based
on those used in the PARADISE dialogue-system
evaluation framework (Walker et al, 2000):
? Two dialogue efficiency measures: the mean
duration of the interaction as measured both
in seconds and in system turns;
? Two dialogue quality measures: the number
of times that the robot gave explanations, and
the number of times that the user asked for
instructions to be repeated; and
? One task success measure: how many of the
(two) target objects were constructed as in-
tended (i.e., as shown in Figure 3).
For each of these measures, we tested whether the
difference in reference strategy had a significant
effect, again via a Mann-Whitney test. Table 2 il-
lustrates the results on these objective measures,
divided by the reference strategy.
The results from the two groups of subjects
were very similar on all of these measures: on
average, the experiment took 404 seconds (nearly
seven minutes) to complete with the constant strat-
egy and 410 seconds with the adaptive, the mean
number of system turns was close to 30 in both
cases, just over one-quarter of all subjects asked
for instructions to be repeated, the robot gave just
over two explanations per trial, and about three-
quarters of all target objects (i.e. 1.5 out of 2)
were correctly built. The Mann-Whitney test con-
firms that none of the differences between the two
groups even came close to significance on any of
the objective measures.
5.3 Comparing Objective and Subjective
Measures
In the preceding sections, we presented results on
a number of objective and subjective measures.
While the subjects generally rated their experi-
ence of using the system positively, there was
some degree of variation, most of which could not
be attributed to the difference in reference strat-
egy. Also, the results on the objective measures
varied widely across the subjects, but again were
not generally affected by the reference strategy.
In this section, we examine the relationship be-
tween these two classes of measures in order to
determine which of the objective measures had the
largest effect on users? subjective reactions to the
HRI system.
Being able to predict subjective user satisfac-
tion from more easily-measured objective proper-
ties can be very useful for developers of interac-
tive systems: in addition to making it possible to
evaluate systems based on automatically available
data without the need for extensive experiments
with users, such a performance function can also
be used in an online, incremental manner to adapt
system behaviour to avoid entering a state that is
likely to reduce user satisfaction (Litman and Pan,
2002), or can be used as a reward function in a
reinforcement-learning scenario (Walker, 2000).
We employed the procedure used in the PAR-
ADISE evaluation framework (Walker et al,
2000) to explore the relationship between the sub-
jective and objective factors. The PARADISE
model uses stepwise multiple linear regression to
predict subjective user satisfaction based on mea-
sures representing the performance dimensions of
task success, dialogue quality, and dialogue effi-
ciency, resulting in a predictor function of the fol-
lowing form:
Satisfaction =
n
?
i=1
wi ?N (mi)
The mi terms represent the value of each measure,
while the N function transforms each measure
into a normal distribution using z-score normali-
sation. Stepwise linear regression produces coef-
ficients (wi) describing the relative contribution of
each predictor to the user satisfaction. If a predic-
tor does not contribute significantly, its wi value is
zero after the stepwise process.
Table 3 shows the predictor functions that were
derived for each of the classes of subjective mea-
sures in this study, using all of the objective mea-
sures from Table 2 as initial factors. The R2 col-
umn indicates the percentage of the variance in the
target measure that is explained by the predictor
function, while the Significance column gives sig-
nificance values for each term in the function.
In general, the two factors with the biggest in-
fluence on user satisfaction were the number of
repetition requests (which had a uniformly neg-
ative effect on user satisfaction), and the num-
ber of target objects correctly built by the user
(which generally had a positive effect). Aside
from the questions on user feelings, the R2 values
are generally in line with those found in previous
PARADISE evaluations of other dialogue systems
(Walker et al, 2000; Litman and Pan, 2002), and
in fact are much higher than those found in a pre-
vious similar study (Foster et al, 2009b).
6 Discussion
The subjective responses on the relevant items
from the usability questionnaire suggest that
the subjects perceived the robot to be a bet-
ter conversational partner if it used contextually
varied, situationally-appropriate referring expres-
sions than if it always used a baseline, constant
strategy; this supports the main prediction for this
study. The result also agrees with the findings of
a previous study (Foster et al, 2009a)?this sys-
tem did not incorporate goal inference and had a
less-sophisticated reference strategy, but the main
effect of changing reference strategy was also on
the users? subjective opinions of the robot?s inter-
active ability. These studies together support the
current effort in the natural-language generation
community to devise more sophisticated reference
generation algorithms.
On the other hand, there was no significant dif-
ference between the two groups on any of the
objective measures: the dialogue efficiency, dia-
logue quality, and task success were nearly iden-
tical across the two groups of subjects. A de-
tailed analysis of the subjects? gaze and object-
manipulation behaviour immediately after various
forms of generated references from the robot also
failed to find any significant differences between
the various reference types. These overall results
are not particularly surprising: studies of human-
human dialogue in a similar joint construction task
(Bard et al, In prep.) have demonstrated that the
collaborators preserve quality of construction in
Table 3: PARADISE predictor functions for each category on the usability questionnaire
Measure Function R2 Significance
Intelligence 76.8+7.00?N (Correct)?5.51?N (Repeats) 0.39 Correct: p < 0.001,
Repeats: p < 0.005
Task 72.4+3.54?N (Correct)?3.45?N (Repeats)?2.17?N (Explain) 0.43 Correct: p < 0.005,
Repeats: p < 0.01,
Explain: p? 0.10
Feeling 66.9?6.54?N (Repeats)+4.28?N (Seconds) 0.09 Repeats: p < 0.05,
Seconds: p? 0.12
Conversation 71.0+5.28?N (Correct)?3.08?N (Repeats) 0.20 Correct: p < 0.01,
Repeats: p? 0.10
Overall 72.0+4.80?N (Correct)?4.27?N (Repeats) 0.40 Correct: p < 0.001,
Repeats: p < 0.005
all cases, though circumstances may dictate what
strategies they use to do this. Combined with the
subjective findings, this lack of an objective effect
suggests that the references generated by the adap-
tive strategy were both sufficient and more natural
than those generated by the constant strategy.
The analysis of the relationship between the
subjective and objective measures analysis has
also confirmed and extended the findings from a
similar analysis (Foster et al, 2009b). In that
study, the main contributors to user satisfaction
were user repetition requests (negative), task suc-
cess, and dialogue length (both positive). In the
current study, the primary factors were similar,
although dialogue length was less prominent as
a factor and task success was more prominent.
These findings are generally intuitive: subjects
who are able to complete the joint construction
task are clearly having more successful interac-
tions than those who are not able to complete the
task, while subjects who need to ask for instruc-
tions to be repeated are equally clearly not hav-
ing successful interactions. The findings add ev-
idence that, in this sort of task-based, embodied
dialogue system, users enjoy the experience more
when they are able to complete the task success-
fully and are able to understand the spoken contri-
butions of their partner, and also suggest that de-
signers should concentrate on these aspects of the
interaction when designing the system.
7 Conclusions
We have presented the reference generation mod-
ule of a hybrid human-robot interaction system
that combines a goal-inference component based
on sub-symbolic dynamic neural fields with a
natural-language interface based on more tradi-
tional symbolic techniques. This combination of
approaches results in a system that is able to work
together with a human partner on a mutual con-
struction task, interpreting its partner?s verbal and
non-verbal behaviour and responding appropri-
ately to unexpected actions (errors) of the partner.
We have then described a user evaluation of this
system, concentrating on the impact of different
techniques for generating situated references in
the context of the robot?s corrective feedback. The
results of this study indicate that using an adaptive
strategy to generate the references significantly in-
creases the users? opinion of the robot as a con-
versational partner, without having any effect on
any of the other measures. This result agrees with
the findings of the system evaluation described in
(Foster et al, 2009a), and adds evidence that so-
phisticated generation techniques are able to im-
prove users? experiences with interactive systems.
An analysis of the relationship between the ob-
jective and subjective measures found that the
main contributors to user satisfaction were the
users? task performance (which had a positive ef-
fect on most measures of satisfaction), and the
number of times the users had to ask for instruc-
tions to be repeated (which had a generally neg-
ative effect). Again, these results agree with the
findings of a previous study (Foster et al, 2009b),
and also suggest priorities for designers of this
type of task-based interactive system.
Acknowledgements
This research was supported by the Euro-
pean Commission through the JAST2 (IST-
FP6-003747-IP) and INDIGO3 (IST-FP6-045388)
projects. Thanks to Pawel Dacka and Levent Kent
for help in running the experiment and analysing
the data.
2http://www.jast-project.eu/
3http://www.ics.forth.gr/indigo/
References
E. G. Bard, R. Hill, and M. E. Foster. 2008. What
tunes accessibility of referring expressions in
task-related dialogue? In Proceedings of the
30th Annual Meeting of the Cognitive Science
Society (CogSci 2008). Chicago.
E. G. Bard, R. L. Hill, M. E. Foster, and M. Arai.
In prep. How do we tune accessibility in joint
tasks: Roles and regulations.
H. Bekkering, E.R.A. de Bruijn, R.H. Cuijpers,
R. Newman-Norlund, H.T. van Schie, and
R. Meulenbroek. 2009. Joint action: Neurocog-
nitive mechanisms supporting human interac-
tion. Topics in Cognitive Science, 1(2):340?
352.
E. Bicho, L. Louro, N. Hipolito, and W. Erlhagen.
2009. A dynamic field approach to goal infer-
ence and error monitoring for human-robot in-
teraction. In Proceedings of the Symposium on
?New Frontiers in Human-Robot Interaction?,
AISB 2009 Convention. Heriot-Watt University
Edinburgh.
H. H. Clark. 1996. Using Language. Cambridge
University Press.
R. Dale and E. Reiter. 1995. Computational inter-
pretations of the Gricean maxims in the genera-
tion of referring expressions. Cognitive Science,
19(2):233?263.
W. Erlhagen and E. Bicho. 2006. The dynamic
neural field approach to cognitive robotics.
Journal of Neural Engineering, 3(3):R36?R54.
M. E. Foster, E. G. Bard, R. L. Hill, M. Guhe,
J. Oberlander, and A. Knoll. 2008a. The roles
of haptic-ostensive referring expressions in co-
operative, task-based human-robot dialogue. In
Proceedings of HRI 2008.
M. E. Foster, M. Giuliani, A. Isard, C. Matheson,
J. Oberlander, and A. Knoll. 2009a. Evaluating
description and reference strategies in a coop-
erative human-robot dialogue system. In Pro-
ceedings of IJCAI-09.
M. E. Foster, M. Giuliani, and A. Knoll. 2009b.
Comparing objective and subjective measures
of usability in a human-robot dialogue system.
In Proceedings of ACL-IJCNLP 2009.
M. E. Foster, M. Giuliani, T. Mu?ller, M. Rickert,
A. Knoll, W. Erlhagen, E. Bicho, N. Hipo?lito,
and L. Louro. 2008b. Combining goal inference
and natural-language dialogue for human-robot
joint action. In Proceedings of the 1st Interna-
tional Workshop on Combinations of Intelligent
Methods and Applications at ECAI 2008.
M. Giuliani and A. Knoll. 2008. MultiML:
A general-purpose representation language for
multimodal human utterances. In Proceedings
of ICMI 2008.
J. D. Kelleher and G.-J. M. Kruijff. 2006. Incre-
mental generation of spatial referring expres-
sions in situated dialog. In Proceedings of
COLING-ACL 2006.
A. Kranstedt and I. Wachsmuth. 2005. Incremen-
tal generation of multimodal deixis referring to
objects. In Proceedings of ENLG 2005.
S. Larsson and D. Traum. 2000. Information state
and dialogue management in the TRINDI dia-
logue move engine toolkit. Natural Language
Engineering, 6(3&4):323?340.
D. J. Litman and S. Pan. 2002. Designing and
evaluating an adaptive spoken dialogue system.
User Modeling and User-Adapted Interaction,
12(2?3):111?137.
A. J. N. van Breemen. 2005. iCat: Experimenting
with animabotics. In Proceedings of AISB 2005
Creative Robotics Symposium.
I. F. van der Sluis. 2005. Multimodal Reference:
Studies in Automatic Generation of Multimodal
Referring Expressions. Ph.D. thesis, University
of Tilburg.
M. Walker, C. Kamm, and D. Litman. 2000. To-
wards developing general models of usability
with PARADISE. Natural Language Engineer-
ing, 6(3?4):363?377.
M. A. Walker. 2000. An application of reinforce-
ment learning to dialogue strategy selection in
a spoken dialogue system for email. Journal of
Artificial Intelligence Research, 12:387?416.
M. White. 2006. Efficient realization of co-
ordinate structures in Combinatory Categorial
Grammar. Research on Language and Compu-
tation, 4(1):39?75.
Proceedings of the SIGDIAL 2013 Conference, pages 223?232,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Training and evaluation of an MDP model for social multi-user
human-robot interaction
Simon Keizer, Mary Ellen Foster,
Oliver Lemon
Interaction Lab
Heriot-Watt University
Edinburgh (UK)
{s.keizer,m.e.foster,o.lemon}@hw.ac.uk
Andre Gaschler, Manuel Giuliani
fortiss GmbH
Munich (Germany)
{gaschler,giuliani}@fortiss.org
Abstract
This paper describes a new approach to
automatic learning of strategies for social
multi-user human-robot interaction. Us-
ing the example of a robot bartender that
tracks multiple customers, takes their or-
ders, and serves drinks, we propose a
model consisting of a Social State Recog-
niser (SSR) which processes audio-visual
input and maintains a model of the social
state, together with a Social Skills Execu-
tor (SSE) which takes social state updates
from the SSR as input and generates robot
responses as output. The SSE is modelled
as two connected Markov Decision Pro-
cesses (MDPs) with action selection poli-
cies that are jointly optimised in interaction
with a Multi-User Simulation Environment
(MUSE). The SSR and SSE have been in-
tegrated in the robot bartender system and
evaluated with human users in hand-coded
and trained SSE policy variants. The re-
sults indicate that the trained policy out-
performed the hand-coded policy in terms
of both subjective (+18%) and objective
(+10.5%) task success.
1 Introduction
As the use of robot technology in the home as well
as in public spaces is increasingly gaining attention,
the need for effective and robust models for natural
and social human robot interaction becomes more
important. Whether it involves robot companions
(Vardoulakis et al, 2012), game-playing robots
(Klotz et al, 2011; Brooks et al, 2012; Cuaya?huitl
and Kruijff-Korbayova?, 2012), or robots that help
people with exercising (Fasola and Mataric, 2013),
human users should be able to interact with such
service robots in an effective and natural way, us-
ing speech as well as other modalities of commu-
nication. Furthermore, with the emergence of new
application domains there is a particular need for
methods that enable rapid development of mod-
els for such new domains. In this respect, data-
driven approaches are appealing for their capability
to automatically exploit empirical data to arrive at
realistic and effective models for interpreting user
behaviour, as well as to learn strategies for effective
system behaviour.
In spoken dialogue systems research, statisti-
cal methods for spoken language understanding,
dialogue management, and natural language gen-
eration have proven to be feasible for effective
and robust interactive systems (Rieser and Lemon,
2011; Lemon and Pietquin, 2012; Young et al,
2010; Young et al, 2013). Although such methods
have recently also been applied to (multi-modal)
human-robot interaction (Stiefelhagen et al, 2007;
Cuaya?huitl et al, 2012), work on multi-user human-
robot interaction has been limited to non-statistical,
hand-coded models (Klotz et al, 2011).
On the other hand, substantial work has been
done in the field of situated multi-party interaction
in general, including data-driven approaches. In
particular, Bohus & Horvitz (2009) have addressed
the task of recognising engagement intentions using
online learning in the setting of a screen-based em-
bodied virtual receptionist, and have also worked
on multi-party turn-taking in this context (Bohus
and Horvitz, 2011).
In this paper we describe a statistical approach
to automatic learning of strategies for selecting ef-
fective as well as socially appropriate robot actions
in a multi-user context. The approach has been de-
veloped using the example of a robot bartender (see
Figure 1) that tracks multiple customers, takes their
orders, and serves drinks. We propose a model con-
sisting of a Social State Recogniser (SSR) which
processes audio-visual input and maintains a model
of the social state, and a Social Skills Executor
(SSE) which takes social state updates from the
SSR as input and generates robot responses as out-
223
put. The SSE is modelled as a hierarchy of two con-
nected Markov Decision Processes (MDPs) with
action selection policies that are jointly optimised
in interaction with a Multi-User Simulation Envi-
ronment (MUSE).
Figure 1: The robot bartender with two customers
In the remainder of this paper we will describe
the robot system in more detail (Section 2), fol-
lowed by descriptions of the SSR (Section 3), the
SSE (Section 4), and MUSE (Section 5). In Sec-
tion 6 we then discuss in more detail the MDP
model for the SSE and the process of jointly opti-
mising the policies, and present evaluation results
on simulated data. Next, we present results of the
first evaluation of the integrated SSE-MDP compo-
nent with human users (Section 7). The paper is
concluded in Section 8.
2 Robot bartender system
The robot system we used for evaluating the models
is equipped with vision and speech input processing
modules, as well as modules controlling two robot
arms and a talking head. Based on observations
about the users in the scene and their behaviour, the
system must maintain a model of the social context,
and decide on effective and socially appropriate
responses in that context. Such a system must be
able to engage in, maintain, and close interactions
with users, take a user?s order by means of a spoken
conversation, and serve their drinks. The overall
aim is to generate interactive behaviour that is both
task- effective and socially appropriate: in addition
to efficiently taking orders and serving drinks, the
system should, e.g., deal with customers on a first-
come, first-served basis, and should manage the
customers? patience by asking them politely to wait
until the robot is done serving another customer.
As shown in Figure 1, the robot hardware con-
sists of a pair of manipulator arms with grippers,
mounted to resemble human arms, along with
an animatronic talking head capable of produc-
ing facial expressions, rigid head motion, and lip-
synchronised synthesised speech. The input sen-
sors include a vision system which tracks the loca-
tion, facial expressions, gaze behaviour, and body
language of all people in the scene in real time
(Pateraki et al, 2013), along with a linguistic pro-
cessing system (Petrick et al, 2012) combining a
speech recogniser with a natural-language parser
to create symbolic representations of the speech
produced by all users. More details of the architec-
ture and components are provided in (Foster et al,
2012). An alternative embodiment of the system is
also available on the NAO platform.
3 Social State Recogniser
The primary role of the Social State Recogniser
(SSR) is to turn the continuous stream of messages
produced by the low-level input and output com-
ponents of the system into a discrete representa-
tion of the world, the robot, and all entities in the
scene, integrating social, interaction-based, and
task-based properties. The state is modelled as a
set of relations such as facePos(A)=(x,y,z) or
closeToBar(A); see (Petrick and Foster, 2013)
for details on the representation used.
In addition to storing all of the low-level sensor
information, the SSR also infers additional rela-
tions that are not directly reported by the sensors.
For example, it fuses information from vision and
speech to determine which user should be assigned
to a recognised spoken contribution. It also pro-
vides a constant estimate of whether each customer
is currently seeking attention from the bartender
(seeksAttention(A)): the initial version of this
estimator used a hand-coded rule based on the ob-
servation of human behaviour in real bars (Huth
et al, 2012), while a later version (Foster, 2013)
makes use of a supervised learning classifier trained
on labelled recordings of humans interacting with
the first version of the robot bartender.
The SSR provides a query interface to allow
other system components access to the relations
stored in the state, and also publishes an updated
state to the SSE every time there is a change which
might require a system action in response (e.g.,
a customer appears, begins seeking attention, or
makes a drink order).
224
4 Social Skills Executor
The Social Skills Executor (SSE) controls the be-
haviour of the robot system, based on the social
state updates it receives from the SSR. The out-
put of the SSE consists of a combination of non-
communicative robot actions and/or communica-
tive actions with descriptions of their multi-modal
realisations. In the bartender domain, the non-
communicative actions typically involve serving
a specific drink to a specific user, whereas the com-
municative actions have the form of dialogue acts
(Bunt et al, 2010), directed at a specific user, e.g.
setQuestion(drink) (?What would you like to
drink??) or initialGreeting() (?Hello?).
In our design of the SSE, the decision making
process resulting in such outputs (including the ?no
action? output) consists of three stages: 1) social
multi-user coordination: managing the system?s
engagement with the users present in the scene (e.g.,
accept a user?s bid for attention, or proceed with an
engaged user), 2) single-user interaction: if pro-
ceeding with an engaged user, generating a high-
level response to that user, in the form of a com-
municative act or physical action (e.g., greeting the
user or serving him a drink), and 3) multi-modal
fission: selecting a combination of modalities for
realising a chosen response (e.g., a greeting can be
realised through speech and/or a nodding gesture).
One advantage of such a hierarchical design is that
strategies for the different stages can be developed
independently. Another is that it makes automatic
policy optimisation more scalable.
5 Multi-User Simulated Environment
In order to test and evaluate the SSE, as well as to
train SSE action selection policies, we developed
a Multi-User Simulated Environment (MUSE).
MUSE allows for rapidly exploring the large space
of possible states in which the SSE must select
actions. A reward function that incorporates in-
dividual rewards from all simulated users in the
environment is used to encode preferred system
behaviour in a principled way. A simulated user
assigns a reward if they are served the correct drink,
and gives penalties associated with their waiting
time and various other forms of undesired system
responses (see Section 6.1 for more details about
the reward function). All of this provides a practi-
cal platform for evaluating different strategies for
effective and socially appropriate behaviour. It also
paves the way for automatic optimisation of poli-
cies, for example by using reinforcement learning
techniques, as we will discuss in Section 6.1.
The simulated environment replaces the vision
and speech processing modules in the actual robot
bartender system, which means that it generates 1)
vision signals in every time-frame, and 2) speech
processing results, corresponding to sequences of
time-frames where a user spoke. The vision obser-
vations contain information about users that have
been detected, where they are in the scene, whether
they are speaking, and where their attention is di-
rected to. Speech processing results are represented
semantically, in the form of dialogue acts (e.g.,
inform(drink=coke), ?I would like a coke?). As
described in Section 3, the SSR fuses the vision and
speech input, for example to associate an incoming
dialogue act with a particular user.
The simulated signals are the result of combin-
ing the output from the simulated users in the en-
vironment. Each simulated user is initialised with
a random goal (in our domain a type of drink they
want to order), enters the scene at some point, and
starts bidding for attention at some point. Each
simulated user also maintains a state and gener-
ates responses given that state. These responses
include communicative actions directed at the bar-
tender, which are translated into a multi-channel
vision input stream processed by the SSR, and, in
case the user realises the action through speech,
a speech processing event after the user has fin-
ished speaking. Additionally, the simulated users
start with a given patience level, which is reduced
in every frame that the user is bidding for atten-
tion or being served by the system. If a user?s pa-
tience has reduced to zero, s/he gives up and leaves
the bar. However, it is increased by a given fixed
amount when the system politely asks the user to
wait, encoded as a pausing dialogue act. The be-
haviour of the simulated users is partly controlled
by a set of probability distributions that allow for
a certain degree of variation. These distributions
have been informed by statistics derived from a
corpus of human-human customer-bartender inter-
actions (Huth et al, 2012).
In addition to information about the simulated
users, MUSE also provides feedback about the
execution of robot actions to the SSR, in partic-
ular the start and end of all robot speech and non-
communicative robot actions. This type of informa-
tion simulates the feedback that is also provided in
the actual bartender system by the components that
directly control the robot head and arms. Figure 2
225
Figure 2: Social state recognition and social skills execution in a multi-user simulated environment.
shows the architecture of the system interacting
with the simulated environment.
6 MDP model for multi-user interaction
To enable automatic optimisation of strategies for
multi-user social interaction, the SSE model as de-
scribed in Section 4 was cast as a hierarchy of two
Markov Decision Processes (MDPs), correspond-
ing to the social multi-user coordination and single-
user interaction stages of decision making. Both
MDPs have their own state spaces S1 and S2, each
defined by a set of state features, extracted from
the estimated social state made available by the
SSR?see Tables 1 and 3. They also have their own
action setsA1 andA2, corresponding to the range
of decisions that can be made at the two stages (Ta-
bles 2 and 4), and two policies pi1 : S1 ? A1 and
pi2 : S2 ? A2, mapping states to actions.
6.1 Policy optimisation
Using the MDP model as described above, we
jointly optimise the two policies, based on the re-
wards received through the SSR from the simulated
environment MUSE. Since MUSE gives rewards
on a frame-by-frame basis, they are accumulated
in the social state until the SSR publishes a state
update. The SSE stores the accumulated reward
together with the last state encountered and action
taken in that state, after which that reward is reset
in the social state. After each session (involving
interactions with two users in our case), the set
of encountered state-action pairs and associated
rewards is used to update the policies.
The reward provided by MUSE in each frame
is the sum of rewards Ri given by each individual
simulated user i, and a number of general penalties
arising from the environment as a whole. User
rewards consist of a fixed reward in case their goal
is satisfied (i.e., when they have been served the
drink they wanted and ordered), a penalty in case
they are still waiting to be served, a penalty in case
they are engaged with the system but have not been
served their drink yet, and additional penalties, for
example when the system turns his attention to
another user when the user is still talking to it, or
when the system serves a drink before the user has
ordered, or when the system serves another drink
when the user already has been served their drink.
General penalties are given for example when the
system is talking while no users are present.
The policies are encoded as functions that assign
a value to each state-action pair; these so-called
Q-values are estimates of the long-term discounted
cumulative reward. Given the current state, the
policy selects the action with the highest Q-value:
pi(s) = arg max
a
Q(s, a) (1)
Using a Monte-Carlo Control algorithm (Sutton
and Barto, 1998), the policies are optimised by
running the SSR and SSE against MUSE and using
the received reward signal to update the Q-values
after each interaction sequence. During training,
the SSE uses an -greedy policy, i.e., it takes a
random exploration action with probability  = 0.2.
226
Index Feature Values
4 ? i Interaction status for user i + 1 nonEngaged/seeksAttention/engaged
4 ? i + 1 Location of user i + 1 notPresent/!closeToBar/closeToBar
4 ? i + 2 User i + 1 was served a drink no/yes
4 ? i + 3 User i + 1 asked to wait no/yes
Table 1: State features for the social multi-user coordination policy. For each user, 4 features are included
in the state space, resulting in 32 ? 22 = 36 states for interactions with up to 1 user, increasing to 1296
states for interactions with up to 2 users and 46, 656 states for up to 3 users.
Index Action
0 No action
3 ? i + 1 Ask user i + 1 to wait
3 ? i + 2 Accept bid for attention from user i + 1
3 ? i + 3 Proceed interaction with (engaged) user i + 1
Table 2: Actions for the social multi-user coordination policy.
In the policy update step, a discount factor ? = 0.95
is used, which controls the impact that rewards
received later in a session have on the value of state-
action pairs encountered earlier in that session.
Figure 3 shows the learning curve of a joint
policy optimisation, showing average rewards ob-
tained after running the SSE with trained policies
for 500 runs, at several stages of the optimisation
process (after every 2500 sessions/runs/iterations,
the trained policy was saved for evaluation). In this
particular setup, simulated users gave a reward of
550 upon goal completion but in the total score this
is reduced considerably due to waiting time (-2 per
frame), task completion time (-1 per frame) and
various other potential penalties. Also indicated
are the performance levels of two hand-coded SSE
policies, one of which uses a strategy of asking a
user to wait when already engaged with another
user (labelled HDC), and one in which that second
user is ignored until it is done with the engaged user
(labelled HDCnp). The settings for user patience
as discussed in Section 5 determine which of these
policies works best; ideally these settings should be
derived from data if available. Nevertheless, even
with the hand-coded patience settings, the learning
curve indicates that both policies are outperformed
in simulation after 10k iterations, suggesting that
the best strategy for managing user patience can be
found automatically.
7 Human user evaluation
The SSE described above has been integrated in
the full robot bartender system and evaluated for
the first time with human users. In the experiment,
both a hand-coded version and a trained version
of the SSE component were tested; see Table 6 in
Appendix A for the trajectory of state-action pairs
of an example session. The hand-coded version
uses the policy labelled HDC, not HDCnp (see
Section 6.1). In each of the sessions carried out, one
recruited subject and one confederate (one of the
experimenters) approached the bartender together
as clients and both tried to order a drink (coke or
lemonade). After each interaction, the subject filled
out the short questionnaire shown in Figure 4.
Q1: Did you successfully order a drink from the bartender?
[Y/N]
Please state your opinion on the following statements:
[ 1:strongly disagree; 2:disagree; 3:slightly disagree;
4:slightly agree; 5:agree; 6:strongly agree ]
Q2: It was easy to attract the bartender?s attention [1?6]
Q3: The bartender understood me well [1?6]
Q4: The interaction with the bartender felt natural [1?6]
Q5: Overall, I was happy about the interaction [1?6]
Figure 4: Questionnaire from the user study.
37 subjects took part in this study, resulting in a
total of 58 recorded drink-ordering interactions:
29 that used the hand-coded SSE for interaction
management, and 29 that used the trained SSE.
The results from the experiment are summarised
in Table 5. We analysed the results using a linear
mixed model, treating the SSE policy as a fixed fac-
tor and the subject ID as a random factor. Overall,
the pattern of the subjective scores suggests a slight
preference for the trained SSE version, although
227
Index Feature Values
0 Reactive pressure none/thanking/greeting/goodbye/apology
1 Status of user goal unknown/usrInf/sysExpConf/sysImpConf/
grounded/drinkServed/sysAsked
2 Own proc. state none/badASR
Table 3: State features for the single-user interaction policy. In this case, there are 5 ? 7 ? 2 = 70 states.
Index Action Example
0 No action
1 returnGreeting() ?Hello?
2 autoPositive() ?Okay?
3 acceptThanking() ?You?re welcome?
4 autoNegative() ?What did you say??
5 setQuestion(drink) ?What drink would you like??
6 acceptRequest(drink=x) + serveDrink(x) ?Here?s your coke?
Table 4: Actions for the single-user interaction policy, which correspond to possible dialogue acts, except
for ?no action? and serving a drink. The specific drink types required for two of the actions are extracted
from the fully specified user goal in the social state maintained by the SSR.
only the difference in perceived success was statis-
tically significant at the p < 0.05 level. The actual
success rate of the trained policy was also some-
what higher, although not significantly so. Also,
the interactions with the trained SSE took slightly
longer than the ones with the hand-coded SSE in
terms of the number of system turns (i.e., the num-
ber of times the SSE receives a state update and
selects a response action, excluding the times when
it selects a non-action); however, this did not have
any overall effect on the users? subjective ratings.
The higher success rate for the trained SSE could
be partly explained by the fact that fewer ASR prob-
lems were encountered when using this version;
however, since the SSE was not triggered when a
turn was discarded due to low-confidence ASR, this
would not have had an effect on the number of sys-
tem turns. There was another difference between
the hand-coded and trained policies that could have
affected both the success rate and the number of
system turns: for interactions in which a user has
not ordered yet, nor been asked for their order, the
hand-coded strategy randomly chooses between
asking the user for their order and doing nothing,
letting the user take the initiative to place the order,
whereas the trained policy always asks the user for
their order (this action has the highest Q-value, al-
though in fact the value for doing nothing in such
cases is also relatively high).
We also carried out a stepwise multiple linear
regression on the data from the user experiment
to determine which of the objective measures had
the largest effect, as suggested by the PARADISE
evaluation framework (Walker et al, 2000). The re-
sulting regression functions are shown in Figure 5.
In summary, all of the subjective responses were
significantly affected by the objective task success
(i.e., the number of drinks served); the number of
low-ASR turns also affected most of the responses,
while various measures of dialogue efficiency (such
as the system response time and the time taken to
serve drinks) also had a significant impact. In gen-
eral, these regression functions explain between
15?25% of the variance in the subjective measures.
As an initial analysis of the validity of the sim-
ulated environment, we compared the state distri-
bution of the simulated data accumulated during
policy optimisation with that of the human user
evaluation data. In terms of coverage, we found
that only 46% of all states encountered in the real
data were also encountered during training. How-
ever, many of these states do not occur very often
and many of them do not require any action by
the robot (a trained policy can easily be set to take
no-action for unseen states). If we only include
states that have been encountered at least 20 times,
the coverage increases to over 70%. For states en-
countered at least 58 times, the coverage is 100%,
though admittedly this covers only the 10 most
frequently encountered states. The similarity of
the two distributions can be quantified by comput-
ing the KL-divergence, but since such a number is
228
Figure 3: Learning curve for joint optimisation of SSE-MDP policies.
System NS PSucc* PAtt PUnd PNat POv NDSrvd NST NBAsr
SSE-TRA 29 97% 4.10 4.21 3.00 3.83 1.97 (98.5%) 7.38 3.14
SSE-HDC 29 79% 4.14 3.83 2.93 3.83 1.76 (88.0%) 6.86 3.82
TOTAL 58 88% 4.12 4.02 2.97 3.83 1.86 (93.0%) 7.12 3.48
Table 5: Overview of system performance results from the experiment. In the leftmost column SSE-TRA
and SSE-HDC refer to the trained and hand-coded SSE versions; the column NS indicates the number of
sessions; the columns PSucc (perceived success), PAtt (perceived attention recognition), PUnd (perceived
understanding), PNat (perceived naturalness), and POv (perceived overall performance) give average
scores resulting from the 5 respective questionnaire questions; NDSrvd indicates the average number of
drinks served per session (out of 2 maximum ? the percentage is given in brackets); NST indicates the
average number of system turns per session; while NBAsr indicates the average number of cases where
the user speech was ignored because the ASR confidence was below a predefined threshold. The marked
column indicates that the difference between the two SSE versions was significant at the p < 0.05 level.
hard to interpret in itself, this will only be useful
if there were a state distribution from an alterna-
tive simulator or an improved version of MUSE for
comparison.
8 Conclusion
In this paper we presented a new approach to au-
tomatic learning of strategies for social multi-user
human-robot interaction, demonstrated using the
example of a robot bartender that tracks multiple
customers, takes their orders, and serves drinks.
We presented a model consisting of a Social State
Recogniser (SSR) which processes audio-visual in-
put and maintains a model of the social state, and
a Social Skills Executor (SSE) which takes social
state updates from the SSR as input and generates
robot responses as output. The main contribution
of this work has been a new MDP-based model
for the SSE, incorporating two connected MDPs
with action selection policies that are jointly op-
timised in interaction with a Multi-User Simula-
tion Environment (MUSE). In addition to showing
promising evaluation results with simulated data,
we also presented results from a first evaluation of
the SSE component with human users. The experi-
ments showed that the integrated SSE component
worked quite well, and that the trained SSE-MDP
achieved higher subjective and objective success
rates (+18% and +10.5% respectively).
Our model currently only utilises two policies,
but in more complex scenarios the task could be
further modularised and extended by introducing
more MDPs, for example for multimodal fission
and natural language generation. The approach of
using a hierarchy of MDPs has some similarity with
the Hierarchical Reinforcement Learning (HRL)
approach which uses a hierarchy of Semi-Markov
Decision Processes (SMDPs). In (Cuaya?huitl et al,
229
PSucc = 0.88 + 0.14 ? N(NDSrvd) ? 0.07 ? N(NBAsr) (r2 = 0.21)
PAtt = 4.12 + 0.76 ? N(NDSrvd) ? 0.46 ? N(RTm) ? 0.38 ? N(FDTm) (r2 = 0.22)
PUnd = 4.02 + 0.41 ? N(NDSrvd) ? 0.36 ? N(NBAsr) ? 0.40 ? N(NST) ? 0.41 ? N(RTm) ? 0.39 ? N(STm) (r2 = 0.24)
PNat = 2.97 + 0.36 ? N(NDSrvd) ? 0.29 ? N(NBAsr) ? 0.31 ? N(NST) ? 0.44 ? N(RTm) (r2 = 0.16)
POv = 3.83 + 0.65 ? N(NDSrvd) ? 0.38 ? N(NBAsr) ? 0.52 ? N(RTm) (r2 = 0.24)
Figure 5: PARADISE regression functions from the user study. The labels are the same as those in Table 5,
with the following additions: RTm is the mean system response time per user, STm is the mean serving
time per user, and FDTm is the mean time to serve the first drink; all times are measured in milliseconds.
N represents a Z score normalisation function (Cohen, 1995).
2012) for example, this hierarchy is motivated by
the identification of multiple tasks that the robot
can carry out and for which multiple SMDP agents
are defined. In every step of the interaction, control
lies with a single SMDP agent somewhere in the
hierarchy; once it arrives at its final state it returns
control to its parent SMDP. An additional transi-
tion model is introduced to permit switching from
an incomplete SMDP to another SMDP at the same
level, making interactions more flexible. In our ap-
proach, control always starts at the top level MDP
and lower level MDPs are triggered depending on
the action taken by their parent MDP. For social
interaction with multiple users, flexible switching
between interactions with different users is impor-
tant, so an arguably more sophisticated HRL ap-
proach to multi-user interaction will rely heavily
on the transition model. Another approach to mod-
ularising the task domain through multiple policies
is described in (Lison, 2011), where ?meta-control?
of the policies relies on an activation vector. As in
the HRL SMDP approach, this approach has not
been applied in the context of multi-user interaction.
In any case, a more thorough and possibly experi-
mental analysis comparing our approach with these
other approaches would be worth investigating.
In the future, we plan to extend our MDP model
to a POMDP (Partially Observable MDP) model,
taking uncertainty about both speech and visual
input into account in the optimisation of SSE poli-
cies by incorporating alternative hypotheses and
confidence scores provided by the input modules
into the social state. Since hand-coding strategies
becomes more challenging in the face of increased
uncertainty due to noisy input, the appeal of auto-
matic strategy learning in a POMDP framework
becomes even stronger. In a previous offline ver-
sion of our combined SSR and SSE, we have shown
in preliminary simulation experiments that even in
an MDP setting, an automatically trained SSE pol-
icy outperforms a hand-coded policy when noise is
added to the speech channel (Keizer et al, 2013).
Another direction of research is to annotate the
data collected in the described experiment for fur-
ther analysis and use it to improve the features of
the simulated environment. The improved models
should lead to trained policies that perform better
when evaluated again with human users. We will
also make use of the findings of the PARADISE
regression to fine-tune the reward function used
for policy optimisation: note that two of the main
features indicated by the PARADISE procedure?
task success and dialogue efficiency?are already
those included in the current reward function, and
we will add a feature to account for the effects of
ASR performance. We are also considering using
collected data for direct supervised or off-policy
reinforcement learning of SSE strategies.
Finally, we aim to extend our domain both in
terms of interactive capabilities (e.g., handling com-
munication problems, social obligations manage-
ment, turn-taking) and task domain (e.g., handling
more than the current maximum of 2 users, group
orders, orders with multiple items). In order to
make the (PO)MDP model more scalable and thus
keeping the learning algorithms tractable, we also
aim to incorporate techniques such as value func-
tion approximation into our model.
Acknowledgments
The research leading to these results has received
funding from the European Union?s Seventh Frame-
work Programme (FP7/2007?2013) under grant
agreement no. 270435, JAMES: Joint Action for
Multimodal Embodied Social Systems, http://
james-project.eu/. Thanks to Ingmar Kessler
for help in running the user experiment.
230
References
Dan Bohus and Eric Horvitz. 2009. Learning to pre-
dict engagement with a spoken dialog system in
open-world settings. In Proceedings SIGdial, Lon-
don, UK.
Dan Bohus and Eric Horvitz. 2011. Multiparty turn
taking in situated dialog: Study, lessons, and direc-
tions. In Proceedings SIGdial, Portland, OR.
A. Brooks, J. Gray, G. Hoffman, A. Lockerd, H. Lee,
and C. Breazeal. 2012. Robot?s play: Interactive
games with sociable machines. Computers in Enter-
tainment, 2(3).
H. Bunt, J. Alexandersson, J. Carletta, J.-W. Choe, A.C.
Fang, K. Hasida, K. Lee, V. Petukhova, A. Popescu-
Belis, L. Romary, C. Soria, and D. Traum. 2010.
Towards an ISO standard for dialogue act annotation.
In Proceedings LREC, Valletta, Malta.
Paul R. Cohen. 1995. Empirical Methods for Artificial
Intelligence. MIT Press, Boston.
Heriberto Cuaya?huitl and Ivana Kruijff-Korbayova?.
2012. An interactive humanoid robot exhibiting flex-
ible sub-dialogues. In Proceedings NAACL HLT,
Montreal, Canada.
H. Cuaya?huitl, I. Kruijff-Korbayova?, and N. Dethlefs.
2012. Hierarchical dialogue policy learning using
flexible state transitions and linear function approxi-
mation. In Proceedings COLING, Mumbai, India.
Juan Fasola and Maja J. Mataric. 2013. A socially as-
sistive robot exercise coach for the elderly. Journal
of Human Robot Interaction, 2(3). To appear.
Mary Ellen Foster, Andre Gaschler, Manuel Giuliani,
Amy Isard, Maria Pateraki, and Ronald P. A. Pet-
rick. 2012. Two people walk into a bar: Dynamic
multi-party social interaction with a robot agent. In
Proceedings ICMI, Santa Monica, CA.
Mary Ellen Foster. 2013. How can I help you?
Comparing engagement classification strategies for
a robot bartender. Submitted.
K. Huth, S. Loth, and J.P. De Ruiter. 2012. Insights
from the bar: A model of interaction. In Proceedings
of Formal and Computational Approaches to Multi-
modal Communication.
Simon Keizer, Mary Ellen Foster, Zhuoran Wang, and
Oliver Lemon. 2013. Machine learning of social
states and skills for multi-party human-robot inter-
action. Submitted.
David Klotz, Johannes Wienke, Julia Peltason, Britta
Wrede, Sebastian Wrede, Vasil Khalidov, and Jean-
Marc Odobez. 2011. Engagement-based multi-
party dialog with a humanoid robot. In Proceedings
SIGdial, Portland, OR.
Oliver Lemon and Olivier Pietquin, editors. 2012.
Data-driven Methods for Adaptive Spoken Dialogue
Systems: Computational Learning for Conversa-
tional Interfaces. Springer.
Pierre Lison. 2011. Multi-policy dialogue manage-
ment. In Proceedings SIGdial, Portland, OR.
Maria Pateraki, Markos Sigalas, Georgios Chliveros,
and Panos Trahanias. 2013. Visual human-robot
communication in social settings. In the Work-
shop on Semantics, Identification and Control of
Robot-Human-Environment Interaction, held within
the IEEE International Conference on Robotics and
Automation (ICRA).
Ronald P. A. Petrick and Mary Ellen Foster. 2013.
Planning for social interaction in a robot bartender
domain. In Proceedings ICAPS, Rome, Italy.
Ronald P. A. Petrick, Mary Ellen Foster, and Amy Isard.
2012. Social state recognition and knowledge-level
planning for human-robot interaction in a bartender
domain. In AAAI 2012 Workshop on Grounding Lan-
guage for Physical Systems, Toronto, ON, Canada,
July.
Verena Rieser and Oliver Lemon. 2011. Rein-
forcement Learning for Adaptive Dialogue Systems.
Springer.
R. Stiefelhagen, H. Ekenel, C. Fu?gen, P. Gieselmann,
H. Holzapfel, F. Kraft, K. Nickel, M. Voit, and
A. Waibel. 2007. Enabling multimodal human-
robot interaction for the Karlsruhe humanoid robot.
IEEE Transactions on Robotics, 23(5):840?851.
Richard S. Sutton and Andrew G. Barto. 1998. Rein-
forcement Learning: An Introduction. MIT Press.
L. Pfeifer Vardoulakis, L. Ring, B. Barry, C. Sidner,
and T. Bickmore. 2012. Designing relational agents
as long term social companions for older adults. In
Proceedings IVA, Santa Cruz, CA.
Marilyn Walker, Candace Kamm, and Diane Litman.
2000. Towards developing general models of usabil-
ity with PARADISE. Natural Language Engineer-
ing, 6(3?4):363?377.
Steve Young, Milica Gas?ic?, Simon Keizer, Franc?ois
Mairesse, Blaise Thomson, and Kai Yu. 2010. The
Hidden Information State model: a practical frame-
work for POMDP based spoken dialogue manage-
ment. Computer Speech and Language, 24(2):150?
174.
Steve Young, M. Gas?ic?, B. Thomson, and J. Williams.
2013. POMDP-based statistical spoken dialogue
systems: a review. Proceedings of the IEEE. To
appear.
231
Appendix A: Example session with two guests ordering a drink
Timestamp Level 1 MDP Level 2 MDP DescriptionState features Action State features Action
13:28:45:966 0 1 0 0 0 0 0 0 0 - - A1 visible, but not close to bar; no response
generated yet.
13:28:48:029 1 2 0 0 0 0 0 0 2 - - A1 not close to bar and seeking attention: BT
acknowledges this and engages with A1.
13:28:53:680 3 2 0 0 1 2 0 0 4 - - A2 visible, close to the bar, and seeking atten-
tion; BT is already engaged with A1 and there-
fore asks A2 to wait.
13:28:55:715 3 2 0 0 1 2 0 1 3 0 0 0 1 BT continues his interaction with A1 and asks
for their order.
13:28:56:928 3 2 0 0 1 2 0 1 3 0 6 0 0 BT continues with A1 and waits for them to
order.
13:28:56:928 3 2 0 0 1 2 0 1 3 0 6 0 0 Same as above: BT still waiting for A1?s order.
. . . Due to repeated ASR failures, this state action
pair is encountered several times.
13:29:52:066 3 2 0 0 1 2 0 1 3 0 1 0 2 A1?s has now been successfully recognised; BT
serves the ordered drink to A1.
13:30:12:013 3 2 1 0 1 2 0 1 5 - - A2 still seeking attention; BT can now acknowl-
edge this and engage with A1.
13:30:13:307 1 2 1 0 3 2 0 1 6 0 0 0 1 BT continues with A2 and asks for their order.
13:30:14:475 1 2 1 0 3 2 0 0 6 0 6 0 0 BT continues with A2 and waits for them to
order
13:30:17:737 1 2 1 0 3 2 0 0 6 0 1 0 2 A2?s recognised; BT serves ordered drink to A2.
13:30:37:623 1 2 1 0 3 2 1 0 0 - - Both A1 and A2 have been served; BT does
nothing
13:30:41:440 1 2 1 0 3 2 1 0 0 - - Same as above.
. . .
Table 6: SSE-MDP trajectory for one session from the evaluation data, showing the states and response
actions taken for both MDPs. The states are represented via their value indices, corresponding to Tables 1
and 3; the action indices similarly correspond to the actions in Tables 2 and 4. In the descriptions, A1 and
A2 refer to the first and second user detected; BT refers to the bartender.
232
Proceedings of the SIGDIAL 2014 Conference, pages 243?250,
Philadelphia, U.S.A., 18-20 June 2014.
c
?2014 Association for Computational Linguistics
Using Ellipsis Detection and Word Similarity for Transformation of
Spoken Language into Grammatically Valid Sentences
Manuel Giuliani
fortiss GmbH
Munich, Germany
giuliani@fortiss.org
Thomas Marschall
fortiss GmbH
Munich, Germany
marschat@in.tum.de
Amy Isard
University of Edinburgh
Edinburgh, UK
amyi@inf.ed.ac.uk
Abstract
When humans speak they often use gram-
matically incorrect sentences, which is a
problem for grammar-based language pro-
cessing methods, since they expect in-
put that is valid for the grammar. We
present two methods to transform spoken
language into grammatically correct sen-
tences. The first is an algorithm for au-
tomatic ellipsis detection, which finds el-
lipses in spoken sentences and searches
in a combinatory categorial grammar for
suitable words to fill the ellipses. The sec-
ond method is an algorithm that computes
the semantic similarity of two words us-
ing WordNet, which we use to find alter-
natives to words that are unknown to the
grammar. In an evaluation, we show that
the usage of these two methods leads to
an increase of 38.64% more parseable sen-
tences on a test set of spoken sentences
that were collected during a human-robot
interaction experiment.
1 Introduction
Computer systems that are designed to interact
verbally with humans need to be able to recog-
nise and understand human speech. In this pa-
per we use as an example the robot bartender
JAMES (Joint Action for Multimodal Embodied
Social Systems),
1
shown in Figure 1. The robot
is able to take drink orders from customers and to
serve drinks. It is equipped with automatic speech
recognition, to understand what the human is say-
ing, and it has a grammar, to parse and process the
spoken utterances.
The JAMES robot grammar was initially very
restricted, and therefore during grammar devel-
opment as well as during the user studies that
1
http://www.james-project.eu
Figure 1: The robot bartender of the JAMES
project interacting with a customer.
we conducted (Foster et al., 2012; Giuliani et al.,
2013; Keizer et al., 2013), we experienced situa-
tions in which the robot was not able to process
the spoken input by humans, because they spoke
sentences with grammatical structures that could
not be parsed by the grammar, they used words
that were not part of the grammar, or they left out
words. We had for example cases where humans
approached the robot and used a sentence with an
ellipsis (?I want Coke?, but the grammar expected
a determiner in front of ?Coke?) or a sentence with
a word that was unknown to the grammar (?I need
a water?, but ?need? was not part of the gram-
mar?s word list). In these cases, the robot was un-
able to process and to respond to the spoken ut-
terance by the human. Of course, these shortcom-
ings can be overcome by extending the grammar,
but even with a much more sophisticated grammar
there will always be instances of unexpected lan-
guage, and we believe that our approach can be
very useful in extending the coverage of a gram-
mar during testing or user studies.
Therefore, we present an approach to transform
unparseable spoken language into sentences that a
given grammar can parse. For ellipsis detection,
243
we present in Section 3.1 a novel algorithm that
searches for ellipses in a sentence and computes
candidate words to fill the ellipsis with words from
a grammar. In Section 3.2, we show how we use
WordNet (Miller, 1995) to find replacements for
words that are not in the robot?s grammar. In Sec-
tion 4 we evaluate our approach with a test set
of 211 spoken utterances that were recorded in
a human-robot interaction (HRI) experiment, and
the grammar for processing used in the same ex-
periment.
2 Related Work
The work described in this paper draws on re-
search and techniques from three main areas: the
automatic detection of ellipses in sentences, cal-
culation of semantic similarity between two words
using WordNet, and spoken language processing.
This section provides a summary of relevant work
in these areas.
2.1 Ellipsis Detection
There is a wide range of research in ellipsis de-
tection in written language, where different types
of ellipses are widely defined, such as gapping,
stripping or verb phrase ellipsis (Lappin, 1996).
For example, an ellipsis occurs when a redundant
word is left out of succeeding sentences, such as
the words ?want to have? in the sentence ?I want
to have a water, and my friend a juice?, which are
omitted in the second part of the sentence.
The detection of verb phrase ellipses (VPE)
is a subfield of ellipsis detection that has re-
ceived much attention. For VPE detection, re-
searchers have used machine learning algorithms
which were trained on grammar-parsed corpora,
for example in the works of Hardt (1997), Nielsen
(2004a), Nielsen (2004b), and Smith and Rauchas
(2006). Other approaches for ellipsis detection
rely on symbolic processing of sentences, which is
similar to our work. Haddar and Hamadou (1998)
present a method for ellipsis detection in the Ara-
bic language, which is based on an augmented
transition network grammar. Egg and Erk (2001)
present a general approach for ellipsis detection
and resolution that uses a language for partial de-
scription of ?-terms called Constraint Language
for Lambda Structures.
2.2 WordNet-based Semantic Similarity
Calculation
WordNet is used in many varied natural language
processing applications, such as word sense dis-
ambiguation, determining the structure of texts,
text summarisation and annotation, information
extraction and retrieval, automatic indexing, lexi-
cal selection, and the automatic correction of word
errors in text. In our work, we use WordNet
to find similar or synonym words. In previous
work, researchers have proposed several methods
to generally compute the semantic relatedness of
two words using WordNet. Budanitsky and Hirst
(2006) review methods to determine semantic re-
latedness. Newer examples for WordNet-based
calculation of semantic similarity are the works
by Qin et al. (2009), Cai et al. (2010), Liu et al.
(2012), and Wagh and Kolhe (2012).
2.3 Spoken Language Processing
Our work addresses the processing of spoken lan-
guage, which differs from the processing of writ-
ten language in that spoken language is more of-
ten elliptical and grammatically incorrect. Previ-
ous work in this area has attempted to address this
issue at different levels of processing. Issar and
Ward (1993) presented the CMU speech process-
ing system that supports recognition for grammat-
ically ill-formed sentences. Lavie (1996) presents
GLR*, a grammar-based parser for spoken lan-
guage, which ignores unparseable words and sen-
tence parts and instead looks for the maximal sub-
set of an input sentence that is covered by the
grammar.
Other researchers in this area have designed
grammar-based approaches for incremental spo-
ken language processing: Brick and Scheutz
(2007) present RISE, the robotic incremental se-
mantic engine. RISE is able to process syntactic
and semantic information incrementally and to in-
tegrate this information with perceptual and lin-
guistic information. Kruijff et al. (2007) present
an approach for incremental processing of situ-
ated dialogue in human-robot interaction, which
maintains parallel interpretations of the current di-
alogue that are pruned by making use of the con-
text information. Schlangen and Skantze (2009)
describe a ?general, abstract model of incremental
dialogue processing?, where their goal is to pro-
vide principles for designing new systems for in-
cremental speech processing.
244
3 Approach
Our goal in this paper is to transform spoken utter-
ances which cannot be parsed by our grammar into
grammar-valid sentences. During this process, we
have to make sure that the changes to the input
sentence do not change its meaning. In this sec-
tion, we show how we implement ellipsis detec-
tion and semantic similarity computation in order
to achieve this goal. We present our ellipsis detec-
tion algorithm in Section 3.1. Section 3.2 explains
our implementation of WordNet-based word simi-
larity computation.
3.1 Ellipsis Detection Algorithm
We use the OpenCCG parser (White, 2006), which
is based on Combinatory Categorial Grammar
(Kruijff and Baldridge, 2004; Steedman, 2000), to
parse the output of our speech recognition system.
We use the properties of CCGs to solve a prob-
lem that often occurs during parsing of spoken lan-
guage. In our evaluation (Section 4) we use a test
set (Section 4.1) of spoken sentences that was col-
lected during one of our human-robot interaction
studies (Foster et al., 2012) and the CCG (Sec-
tion 4.2) that was used in the same study. In the
test set, we found that speakers leave out words.
For example, one speaker said I want water to
order a drink. The grammar used in the experi-
ment assumed that every noun is specified by an
article; the grammar was only able to parse the
sentence I want a water . Just to remind you,
of course this particular example could have been
solved by rewriting the grammar, but at the time
of running the experiment it was not possible to
us to change the grammar. Furthermore, we argue
that there will always be examples of the above
described situation where experiment participants
use grammatical structures or words that cannot be
processed by the used grammar. Thus, we present
an algorithm that automatically finds ellipses in
sentences and suggests words from the grammar
that could be used to fill the ellipses.
To illustrate our approach, we will use the ex-
ample sentence give me a water . Example (1)
shows the words of the example sentence with
their assigned categories from the used CCG, and
Example (2) shows the parsed sentence. In the ex-
amples, we use the category symbols s for sen-
tence, n for noun, and np for noun phrase. In Ex-
ample (2) the symbol> denotes the used CCG for-
ward application combinator.
(1) CCG lexicon entries
a. give := s / np / np
b. me := np
c. a := np / n
d. water := n
(2) Full parse of an example sentence
give me a water
s/np/np np np/n n
>
np
>
s/np
>
s
The algorithm consists of two parts: (i) search
for ellipses in the sentence and selection of the
most relevant ellipsis, and (ii) computation of the
category for the word that will fill the chosen el-
lipsis.
(i) Ellipsis Search
In order to find the ellipsis in the sentence, our al-
gorithm assumes that to the left and to the right of
the ellipsis, the spoken utterance consists of sen-
tence parts that the grammar can parse. In our ex-
ample, these sentence parts would be I want to the
left of the ellipsis and water to the right. In order
to automatically find the sentence part to the right,
we use the following algorithm, which we present
in Listing 1 in a Java-style pseudo code: The al-
gorithm uses the method tokenize() to split up the
string that contains the utterance into an array of
single words. It then iterates through the array and
builds a new sentence of the words in the array,
using the method buildString(). This sentence is
then processed by the parser. If the parser finds
a parse for the sentence, the algorithm returns the
result. Otherwise it cuts off the first word of the
sentence and repeats the procedure. This way, the
algorithm searches for a parseable sentence part
for the given utterance from the left to the right un-
til it either finds the right-most parseable sentence
part or there are no more words to parse. In order
to find the left-most parseable sentence part, we
implemented a method findParseReverse(), which
parses sentence parts from right to left.
One has to consider that our method for ellip-
sis detection can falsely identify ellipses in cer-
tain sentence constellations. For example, if the
word like in the sentence I would like a water
is left out and given to our ellipsis detection al-
gorithm, it would falsely find an ellipsis between
I and would , and another ellipsis between would
245
Listing 1: Ellipsis detection algorithm.
R e s u l t f i n d P a r s e ( S t r i n g u t t e r a n c e ) {
words [ ] = t o k e n i z e ( u t t e r a n c e ) ;
f o r ( i = 0 ; i < words . l e n g t h ; i ++) {
S t r i n g s e n t e n c e = b u i l d S t r i n g ( words [
i ] , words . l e n g t h ) ;
R e s u l t p a r s e = p a r s e ( s e n t e n c e ) ;
i f ( p a r s e != nu l l ) {
re turn p a r s e ;
}
}
re turn nu l l ;
}
and a. The reason for the detection of the first
ellipsis is that the categories for I and would can-
not be combined together. would and like have to
be parsed first to an auxiliary verb-verb construct.
This construct can then be combined with the pro-
noun I. To overcome this problem, we first com-
pute the category for each found ellipsis. Then we
find a word for the ellipsis with the simplest cate-
gory, which is either an atomic category or a func-
tional category with fewer functors than the other
found categories, add it to the original input sen-
tence, and parse the output sentence. If the output
sentence cannot be parsed, we repeat the step with
the next found ellipsis.
(ii) Ellipsis Category Computation
After the algorithm has determined the ellipsis in
an utterance, it computes the category of the word
that will fill the ellipsis. The goal here is to find a
category which the grammar needs to combine the
sentence parts to the left and right of the ellipsis.
For example, the left part of our example utterance
I want has the category s/np and the right part wa-
ter has the category n. Hence, the category for the
missing word needs to be np/n, because it takes
the category of the right sentence part as argument
and produces the category np, which is the argu-
ment of the category of the left sentence part.
Figure 2 shows the processing sequence dia-
gram of our algorithm for computing the category
of an ellipsis. In the diagram, left and right stand
for the categories of the sentence parts that are to
the left and right of the ellipsis. The predicates
symbolise functions: isEmpty(category) checks
if a category is empty, atom(category) checks
if a category is atomic, compl(category) checks
if a category is complex and has a slash oper-
ator that faces toward the ellipsis. The predi-
cate arg(category) returns the argument of a com-
s / right
true
isEmpty(left) isEmpty(right)false
s \ left
true
atom(left)atom(right)
s / right \ left
compl(left)compl(right)
false
false
true
true
atom(left)compl(right)
left \ arg(right)
false
true
atom(right)compl(left)
arg(left) / right
true
false
Figure 2: Processing sequence of the category
computation algorithm.
plex category. Rectangular boxes symbolise steps
where the algorithm builds the result category for
the missing word. The algorithm determines the
category with the following rules:
? if the categories to the left or to the right of
the ellipsis are empty, the ellipsis category is
s/right or s\left, respectively,
? if the categories to the right and to the left of
the ellipsis are atomic, the ellipsis category is
s/right\left,
? if the categories to the right and to the left
of the ellipsis are both complex and have a
slash operator facing toward the ellipsis, the
ellipsis category is s/right\left,
? if the category to the left of the ellipsis is
atomic and to the right of the ellipsis is com-
plex, the ellipsis category is left\arg(right),
? if the category to the right of the ellipsis is
atomic and to the left of the ellipsis is com-
plex, the ellipsis category is arg(left)\right.
After the computation of the ellipsis category,
we use the OpenCCG grammar to select words to
fill the ellipsis. This step is straightforward, be-
cause the grammar maintains a separate word list
with corresponding categories. Here, we benefit
from the usage of a categorial grammar, as the
usage of a top-down grammar formalism would
have meant a more complicated computation in
this step.
3.2 WordNet-based Word Substitution
Spoken language is versatile and there are many
ways to express one?s intentions by using differ-
246
ent expressions. Thus, in grammar-based spo-
ken language processing it often happens that sen-
tences cannot be parsed because of words that
are not in the grammar. To overcome this prob-
lem, we use WordNet (Miller, 1995) to find se-
mantically equivalent replacements for unknown
words. WordNet arranges words in sets of syn-
onyms called synsets which are connected to other
synsets by a variety of relations, which differ for
each word category. The most relevant relations
for our work are: for nouns and verbs hyperonyms
(e.g., drink is a hyperonym of juice) and hyponyms
(e.g., juice is a hyponym of drink), and for adjec-
tives we use the similar to relation.
Our implementation of word substitution exe-
cutes two steps if a word is unknown to the gram-
mar: (1) look-up of synonyms for the unknown
words. The unknown word can be substituted with
a semantically similar word directly, if the synset
of the unknown word contains a word, which is
known to the grammar. (2) Computation of simi-
lar words in the WordNet hyperonym/hyponym hi-
erarchy. If the synset of the unknown word does
not contain a substitution, we compute if one of
the hyperonyms of the unknown word has a hy-
ponym which is known to the grammar. Here, one
has to be careful not to move too far away from
the meaning of the unknown word in the Word-
Net tree, in order not to change the meaning of
the originally spoken sentence. Also, the compu-
tation of the similar word should not take too much
time. Therefore, in our implementation, we only
substitute an unknown word with one of its hyper-
onym/hyponym neighbours when the substitution
candidate is a direct hyponym of the direct hyper-
onym of the unknown word.
4 Evaluation
The goal of this evaluation is to measure how
many spoken sentences that our grammar cannot
parse can be processed after the transformation of
the sentences with our methods. In Section 4.1
we present the test set of spoken sentences that we
used in the evaluation. In Section 4.2 we give de-
tails of the used grammar. As mentioned above,
both, the test set as well as the grammar, were
taken from the human-robot interaction study re-
ported by Foster et al. (2012). Section 4.3 sum-
marises the details of the evaluation procedure. Fi-
nally, we present the evaluation results in Section
4.4 and discuss their meaning in Section 4.5.
4.1 Test Set
As test set for the evaluation, we used the spo-
ken utterances from the participants of the human-
robot interaction experiment reported by Foster et
al. (2012). In the experiment, 31 participants were
asked to order a drink from the robot bartender
shown in Figure 1. The experiment consisted of
three parts: in the first part, participants had to or-
der drinks on their own, in the second and third
part, participants were accompanied by a confed-
erate in order to have a multi-party interaction with
the robot. The spoken utterances in the test set
were annotated by hand from video recordings of
the 93 drink order sequences. Please refer to (Fos-
ter et al., 2012) for a detailed description of the
experiment.
Table 1 shows an overview of the test set. In
total, it contains 211 unique utterances; the exper-
iment participants spoke 531 sentences of which
some sentences were said repeatedly. We di-
vided the test set into the following speech acts
(Searle, 1965): Ordering (?I would like a juice
please.?), Question (?What do you have??), Greet-
ing (?Hello there.?), Polite expression (?Thank
you.?), Confirmation (?Yes.?), Other (?I am
thirsty.?).
4.2 Grammar
The grammar that we used in this evaluation was
also used in the robot bartender experiment (Fos-
ter et al., 2012). This grammar is limited in its
scope, because the domain of the experiment?the
robot hands out drinks to customers?was limited
as well. Overall, the lexicon of the grammar con-
tains 92 words, which are divided into the follow-
ing part of speech classes: 42 verbs, 11 nouns, 10
greetings, 6 pronouns, 5 prepositions, 4 adverbs, 4
determiners, 3 quantifiers, 3 confirmations, 2 rela-
tive pronouns, 1 conjunction, 1 polite expression.
4.3 Procedure
For the evaluation, we implemented a programme
that takes our test set and automatically parses
each sentence with four different settings, which
are also presented in Table 1: (1) parsing with
the grammar only, (2) application of ellipsis de-
tection and word filling before parsing, (3) appli-
cation of WordNet similarity substitution before
parsing, (4) application of a combination of both
methods before parsing. Please note that for al-
ternative (4) the sequence in which the methods
247
Speech act No. utt (1) CCG (2) Ell. det. (3) WordNet (4) Ell. + WordNet
Ordering 143 34 16 - 1
Question 19 1 - - -
Greeting 18 4 1 - -
Polite expression 14 1 - - -
Confirmation 5 4 - - -
Other 12 - - - -
Total 211 44 17 - 1
Table 1: Overview for test set and evaluation. Column No. utt contains the number of test utterances
per speech act. Column (1) CCG shows the number of utterances that were directly parsed by the
grammar. Columns (2) Ell. det., (3) WordNet, and (4) Ell. + WordNet show how many utterances were
additionally parsed using the ellipsis detection , WordNet substitution, and combination of both modules.
are applied to a given sentence can make a differ-
ence to the parsing result. In this evaluation, we
used both possible sequences: first ellipsis detec-
tion followed by WordNet substitution method or
vice versa.
4.4 Results
Table 1 shows the result of the evaluation proce-
dure. The grammar parses 44 sentences of the
211 test set sentences correctly. By using the el-
lipsis detection algorithm, 17 additional sentences
are parsed. The usage of the WordNet substitution
algorithm yields no additionally parsed sentences.
The combination of both methods (in this case,
first ellipsis detection, then WordNet substitution)
leads to the correct parse of one additional sen-
tence. None of the transformed sentences changed
its meaning when compared to the original sen-
tence.
4.5 Discussion
The evaluation results show that the application
of our ellipsis detection algorithm leads to an in-
crease of successfully parsed sentences of 38.64%.
In the class of ordering sentences, which was the
most relevant for the human-robot interaction ex-
periment from which we used the evaluation test
set, the number of successfully parsed sentences
increases by 47.06%. Compared to this, the us-
age of WordNet substitution alone does not lead to
an increase in parseable sentences. The one case
in which the combination of ellipsis detection and
WordNet substitution transformed an unparseable
sentence into a grammatically valid sentence is in-
teresting: here, the experiment participant said ?I
need Coke.? to order a drink from the robot. This
sentence contained the word ?need?, which was
not in the grammar. WordNet has the synonym
?want? in the synset for the word ?need?. How-
ever, the sentence ?I want Coke.? was also not
parseable, because the grammar expected an arti-
cle in front of every noun. The ellipsis detection
algorithm was able to find the missing article in the
sentence and filled it with an article ?a? from the
grammar, leading to a parseable sentence ?I want
a Coke.?.
Although we see an increase in parsed sen-
tences, 150 sentences of the test set were not trans-
formed by our approach. Therefore, we made an
analysis for the remaining utterances to find the
main causes for this weak performance. We found
that the following reasons cause problems for the
grammar (with number of cases in brackets behind
each reason):
? Word missing in grammar (81). The partic-
ipant used a word that was not in the gram-
mar. For example, users ordered drinks by
saying ?One water, please.? , but the gram-
mar did not contain ?one? as an article. This
result shows that the WordNet similarity sub-
stitution has the potential to lead to a large
increase in parseable sentences. However as
mentioned above, there is a risk of changing
the meaning of a sentence too much when al-
lowing the replacement of words which are
only vaguely similar to the unknown word.
? Sentence structure (25). Some participants
said sentences that were either grammatically
incorrect or had a sentence structure that was
not encoded in the grammar. For example
one participant tried to order a juice by saying
?Juice for me.?. Additionally, some partici-
pants asked questions (?Do you have coke??).
248
For the latter, please note that it was not part
of the HRI experiment, from which we use
the test set, that the experiment participants
should be allowed to ask questions to the
robot.
? Unnecessary fill words (22). Some experi-
ment participants used unnecessary fill words
that did not add meaning to the sentence, for
example one participant said ?Oh come on, I
only need water? to order a drink.
? Sentence not related to domain (22). Some
participants said sentences that were contrary
to the given experiment instructions. For ex-
ample, some participants asked questions to
the robot (?How old are you??) and to the ex-
perimenter (?Do I need to focus on the cam-
era??), or complained about the robot?s per-
formance (?You are not quite responsive right
now.?). Clearly, these sentences were out of
the scope of the grammar.
5 Conclusion
We presented two methods for transforming spo-
ken language into grammatically correct sen-
tences. The first of these two approaches is an
ellipsis detection, which automatically detects el-
lipses in sentences and looks up words in a gram-
mar that can fill the ellipsis. Our ellipsis de-
tection algorithm is based on the properties of
the combinatory categorial grammar, which as-
signs categories to each word in the grammar
and thus enables the algorithm to find suitable fill
words by calculating the category of the ellipsis.
The second approach for sentence transformation
was a WordNet-based word similarity computa-
tion and substitution. Here, we used the synsets of
WordNet to substitute words that are unknown to a
given grammar with synonyms for these words. In
an evaluation we showed that the usage of ellip-
sis detection leads to an increase of successfully
parsed sentences of up to 47.06% for some speech
acts. The usage of the WordNet similarity substi-
tution does not increase the number of parsed sen-
tences, although our analysis of the test set shows
that unknown words are the most common reason
that sentences cannot be parsed.
Our approach was specifically implemented to
help circumventing problems during development
and usage of grammars for spoken language pro-
cessing in human-robot interaction experiments,
and the example grammar was a very restricted
one. However, we believe that our method can
also be helpful with more extensive grammars, and
for developers of dialogue systems in other ar-
eas, such as telephone-based information systems
or offline versions of automatic smartphone assis-
tants like Apple?s Siri.
In the future, we will refine our methodology.
In particular, the WordNet similarity substitution
is too rigid in its current form. Here, we plan
to loosen some of the constraints that we ap-
plied to our algorithm. We will systematically test
how far away from a word one can look for suit-
able substitutes in the WordNet hierarchy, with-
out losing the meaning of a sentence. Further-
more, we plan to add a dialogue history to our
approach, which will provide an additional source
of information?besides the grammar?to the el-
lipsis detection method. Finally, we plan to work
with stop word lists to filter unnecessary fill words
from the input sentences, since these proved also
to be a reason for sentences to be unparseable.
Acknowledgements
This research was supported by the European
Commission through the project JAMES (FP7-
270435-STREP).
References
Timothy Brick and Matthias Scheutz. 2007. Incre-
mental natural language processing for hri. In Pro-
ceedings of the ACM/IEEE International Confer-
ence on Human-Robot Interaction, pages 263?270.
ACM New York, NY, USA.
Alexander Budanitsky and Graeme Hirst. 2006. Eval-
uating wordnet-based measures of lexical semantic
relatedness. Computational Linguistics, 32(1):13?
47.
Songmei Cai, Zhao Lu, and Junzhong Gu. 2010. An
effective measure of semantic similarity. In Ad-
vances in Wireless Networks and Information Sys-
tems, pages 9?17. Springer.
Markus Egg and Katrin Erk. 2001. A compositional
account of vp ellipsis. Technology, 3:5.
Mary Ellen Foster, Andre Gaschler, Manuel Giuliani,
Amy Isard, Maria Pateraki, and Ronald P. A. Pet-
rick. 2012. Two people walk into a bar: Dynamic
multi-party social interaction with a robot agent. In
Proceedings of the 14th ACM International Confer-
ence on Multimodal Interaction (ICMI 2012), Octo-
ber.
249
Manuel Giuliani, Ronald P.A. Petrick, Mary Ellen Fos-
ter, Andre Gaschler, Amy Isard, Maria Pateraki, and
Markos Sigalas. 2013. Comparing task-based and
socially intelligent behaviour in a robot bartender. In
Proceedings of the 15th International Conference on
Multimodal Interfaces (ICMI 2013), Sydney, Aus-
tralia, December.
Kais Haddar and A Ben Hamadou. 1998. An ellip-
sis detection method based on a clause parser for
the arabic language. In Proceedings of the Interna-
tional Florida Artificial Intelligence Research Soci-
ety FLAIRS98, pages 270?274.
Daniel Hardt. 1997. An empirical approach to vp el-
lipsis. Computational Linguistics, 23(4):525?541.
Sunil Issar and Wayne Ward. 1993. Cmu?s robust spo-
ken language understanding system. In Proceedings
of the Third European Conference on Speech Com-
munication and Technology (Eurospeech 1993).
Simon Keizer, Mary Ellen Foster, Oliver Lemon, An-
dre Gaschler, and Manuel Giuliani. 2013. Training
and evaluation of an MDP model for social multi-
user human-robot interaction. In Proceedings of the
SIGDIAL 2013 Conference, pages 223?232, Metz,
France, August.
Geert-Jan M. Kruijff and Jason Baldridge. 2004. Gen-
eralizing dimensionality in combinatory categorial
grammar. In Proceedings of the 20th International
Conference on Computational Linguistics (COLING
2004), Geneva, Switzerland, August.
Geert-Jan M. Kruijff, Pierre Lison, Trevor Benjamin,
Henrik Jacobsson, and Nick Hawes. 2007. In-
cremental, multi-level processing for comprehend-
ing situated dialogue in human-robot interaction. In
Luis Seabra Lopes, Tony Belpaeme, and Stephen J.
Cowley, editors, Symposium on Language and
Robots (LangRo 2007), Aveiro, Portugal, December.
Shalom Lappin. 1996. The interpretation of ellipsis.
The Handbook of Contemporary Semantic Theory,
397:399.
Alon Lavie. 1996. GLR*: A Robust Grammar-
Focused Parser for Spontaneously Spoken Lan-
guage. Ph.D. thesis, Carnegie Mellon University.
Hongzhe Liu, Hong Bao, and De Xu. 2012. Concept
vector for semantic similarity and relatedness based
on wordnet structure. Journal of Systems and Soft-
ware, 85(2):370?381.
George A. Miller. 1995. Wordnet: A lexical
database for english. Communications of the ACM,
38(11):39?41.
Leif Arda Nielsen. 2004a. Verb phrase ellipsis detec-
tion using automatically parsed text. In Proceedings
of the 20th International Conference on Computa-
tional Linguistics, page 1093. Association for Com-
putational Linguistics.
Leif Arda Nielsen. 2004b. Verb phrase ellipsis detec-
tion using machine learning techniques. Amsterdam
Studies in the Theory and History of Linguistic Sci-
ence, page 317.
Peng Qin, Zhao Lu, Yu Yan, and Fang Wu. 2009. A
new measure of word semantic similarity based on
wordnet hierarchy and dag theory. In Proceedings
of the International Conference on Web Information
Systems and Mining2009 (WISM 2009), pages 181?
185. IEEE.
David Schlangen and Gabriel Skantze. 2009. A gen-
eral, abstract model of incremental dialogue pro-
cessing. In Proceedings of the 12th Conference of
the European Chapter of the Association for Com-
putational Linguistics (EACL-09).
John R. Searle. 1965. What is a speech act? In
Robert J. Stainton, editor, Perspectives in the Phi-
losophy of Language: A Concise Anthology, pages
253?268.
Lindsey Smith and Sarah Rauchas. 2006. A machine-
learning approach for the treatment of vp ellipsis.
In Proceedings of the Seventeenth Annual Sympo-
sium of the Pattern Recognition Association of South
Africa, November.
Mark Steedman. 2000. The Syntactic Process. MIT
Press, Cambridge, MA, USA.
Kishor Wagh and Satish Kolhe. 2012. A new approach
for measuring semantic similarity in ontology and
its application in information retrieval. In Multi-
disciplinary Trends in Artificial Intelligence, pages
122?132. Springer.
Michael White. 2006. CCG chart realization from dis-
junctive inputs. In Proceedings of the Fourth Inter-
national Natural Language Generation Conference,
pages 12?19, Sydney, Australia, July. Association
for Computational Linguistics.
250
