Base Noun Phrase Translation
Using Web Data and the EM Algorithm
Yunbo Cao
Microsoft Research Asia
i-yuncao@microsoft.com
Hang Li
Microsoft Research Asia
hangli@microsoft.com
Abstract
We consider here the problem of Base Noun
Phrase translation. We propose a new method
to perform the task. For a given Base NP, we
first search its translation candidates from the
web. We next determine the possible
translation(s) from among the candidates
using one of the two methods that we have
developed. In one method, we employ an
ensemble of Na?ve Bayesian Classifiers
constructed with the EM Algorithm. In the
other method, we use TF-IDF vectors also
constructed with the EM Algorithm.
Experimental results indicate that the
coverage and accuracy of our method are
significantly better than those of the baseline
methods relying on existing technologies.
1. Introduction
We address here the problem of Base NP
translation, in which for a given Base Noun
Phrase in a source language (e.g., ?information
age? in English), we are to find out its possible
translation(s) in a target language (e.g., ?
? in Chinese).
We define a Base NP as a simple and
non-recursive noun phrase. In many cases, Base
NPs represent holistic and non-divisible concepts,
and thus accurate translation of them from one
language to another is extremely important in
applications like machine translation, cross
language information retrieval, and foreign
language writing assistance.
In this paper, we propose a new method for
Base NP translation, which contains two steps: (1)
translation candidate collection, and (2)
translation selection. In translation candidate
collection, for a given Base NP in the source
language, we look for its translation candidates in
the target language. To do so, we use a
word-to-word translation dictionary and corpus
data in the target language on the web. In
translation selection, we determine the possible
translation(s) from among the candidates. We use
non-parallel corpus data in the two languages on
the web and employ one of the two methods
which we have developed. In the first method, we
view the problem as that of classification and
employ an ensemble of Na?ve Bayesian
Classifiers constructed with the EM Algorithm.
We will use ?EM-NBC-Ensemble? to denote this
method, hereafter. In the second method, we view
the problem as that of calculating similarities
between context vectors and use TF-IDF vectors
also constructed with the EM Algorithm. We will
use ?EM-TF-IDF? to denote this method.
Experimental results indicate that our method
is very effective, and the coverage and top 3
accuracy of translation at the final stage are
91.4% and 79.8%, respectively. The results are
significantly better than those of the baseline
methods relying on existing technologies. The
higher performance of our method can be
attributed to the enormity of the web data used
and the employment of the EM Algorithm.
2. Related Work
2.1 Translation with Non-parallel
Corpora
A straightforward approach to word or phrase
translation is to perform the task by using parallel
bilingual corpora (e.g., Brown et al 1993).
Parallel corpora are, however, difficult to obtain
in practice.
To deal with this difficulty, a number of
methods have been proposed, which make use of
relatively easily obtainable non-parallel corpora
(e.g., Fung and Yee, 1998; Rapp, 1999; Diab and
Finch, 2000). Within these methods, it is usually
assumed that a number of translation candidates
for a word or phrase are given (or can be easily
collected) and the problem is focused on
translation selection.
All of the proposed methods manage to find out
the translation(s) of a given word or phrase, on
the basis of the linguistic phenomenon that the
contexts of a translation tend to be similar to the
contexts of the given word or phrase. Fung and
Yee (1998), for example, proposed to represent
the contexts of a word or phrase with a
real-valued vector (e.g., a TF-IDF vector), in
which one element corresponds to one word in
the contexts. In translation selection, they select
the translation candidates whose context vectors
are the closest to that of the given word or phrase.
Since the context vector of the word or phrase
to be translated corresponds to words in the
source language, while the context vector of a
translation candidate corresponds to words in the
target language, and further the words in the
source language and those in the target language
have a many-to-many relationship (i.e.,
translation ambiguities), it is necessary to
accurately transform the context vector in the
source language to a context vector in the target
language before distance calculation.
The vector-transformation problem was not,
however, well-resolved previously. Fung and
Yee assumed that in a specific domain there is
only one-to-one mapping relationship between
words in the two languages. The assumption is
reasonable in a specific domain, but is too strict in
the general domain, in which we presume to
perform translation here. A straightforward
extension of Fung and Yee?s assumption to the
general domain is to restrict the many-to-many
relationship to that of many-to-one mapping (or
one-to-one mapping). This approach, however,
has a drawback of losing information in vector
transformation, as will be described.
For other methods using non-parallel corpora,
see also (Tanaka and Iwasaki, 1996; Kikui, 1999,
Koehn and Kevin 2000; Sumita 2000; Nakagawa
2001; Gao et al 2001).
2.2 Translation Using Web Data
Web is an extremely rich source of data for
natural language processing, not only in terms of
data size but also in terms of data type (e.g.,
multilingual data, link data). Recently, a new
trend arises in natural language processing, which
tries to bring some new breakthroughs to the field
by effectively using web data (e.g., Brill et al
2001).
Nagata et al(2001), for example, proposed to
collect partial parallel corpus data on the web to
create a translation dictionary. They observed
that there are many partial parallel corpora
between English and Japanese on the web, and
most typically English translations of Japanese
terms (words or phrases) are parenthesized and
inserted immediately after the Japanese terms in
documents written in Japanese.
3. Base Noun Phrase Translation
Our method for Base NP translation comprises of
two steps: translation candidate collection and
translation selection. In translation candidate
collection, we look for translation candidates of a
given Base NP. In translation selection, we find
out possible translation(s) from the translation
candidates.
In this paper, we confine ourselves to
translation of noun-noun pairs from English to
Chinese; our method, however, can be extended
to translations of other types of Base NPs
between other language pairs.
3.1 Translation Candidate Collection
We use heuristics for translation candidate
collection. Figure 1 illustrates the process of
collecting Chinese translation candidates for an
English Base NP ?information age? with the
heuristics.
1. Input ?information age?;
2. Consult English-Chinese word translation dictionary:
information ->
age -> (how old somebody is)
 (historical era)
 (legal adult hood)
3. Compositionally create translation candidates in
Chinese:
;;
4. Search the candidates on web sites in Chinese and
obtain the document frequencies of them (i.e., numbers
of documents containing them):
 10000
 10
 0
5. Output candidates having non-zero document
frequencies and the document frequencies:
 10000
 10
Figure 1. Translation candidate collection
3.2 Translation Selection --
EM-NBC-Ensemble
We view the translation selection problem as that
of classification and employ EM-NBC-Ensemble
to perform the task. For the ease of explanation,
we first describe the algorithm of using only
EM-NBC and next extend it to that of using
EM-NBC-Ensemble.
Basic Algorithm
Let e~ denote the Base NP to be translated and C~
the set of its translation candidates (phrases).
Suppose that kC =|~| . Let c~ represent a random
variable on C~ . Let E denote a set of words in
English, and C a set of words in Chinese.
Suppose that nCmE == ||and|| . Let e
represent a random variable on E and c a random
variable on C. Figure 2 describes the algorithm.
Input: e~ , C~ , contexts containing e~ , contexts containing all
Cc ~~ ? ;
1. create a frequency vector )),(,),(),(( 21 mefefef L
),,1(, miEei L=? using contexts containing e~ ;
transforming the vector into )),(,),(),(( 21 nEEE cfcfcf L
),,1(, niCci L=? , using a translation dictionary
and the EM algorithm;
2. for each ( Cc ~~ ? ){
estimate with Maximum Likelihood Estimation the prior
probability )~(cP using contexts containing all Cc ~~ ? ;
create a frequency vector )),(,),(),(( 21 ncfcfcf L
),,1(, niCci L=? using contexts containing c~ ;
normalize the frequency vector , yielding
),,1(,)),~|(,),~|(),~|(( 21 niCcccPccPccP in LL =? ;
calculate the posterior probability )|~( DcP with EM-NBC
(generally EM-NBC-Ensemble), where
),,1(,)),(,),(),(( 21 niCccfcfcf inEEE LL =?=D
3. Sort Cc ~~ ? in descending order of )|~( DcP ;
Output: the top sorted results
Figure 2. Algorithm of EM-NBC-Ensemble
Context Information
As input data, we use ?contexts? in English which
contain the phrase to be translated. We also use
contexts in Chinese which contain the translation
candidates.
Here, a context containing a phrase is defined
as the surrounding words within a window of a
predetermined size, which window covers the
phrase. We can easily obtain the data by
searching for them on the web. Actually, the
contexts containing the candidates are obtained at
the same time when we conduct translation
candidate collection (Step 4 in Figure 1).
EM Algorithm
We define a relation between E and C
as CER ?? , which represents the links in a
translation dictionary. We further define
}),(|{ Rceec ?=? .
At Step 1, we assume that all the instances in
))(),..,(),(( 21 mefefef are independently generated
according to the distribution defined as:
?
?
=
Cc
cePcPeP )|()()( (1)
We estimate the parameters of the distribution by
using the Expectation and Maximization (EM)
Algorithm (Dempster et al, 1977).
Initially, we set for all Cc ?
||
1)(
C
cP = ,
??
??
?
??
??
?=
c
c
c
e
e
ceP
if,0
if,||
1
)|(
Next, we estimate the parameters by iteratively
updating them, until they converge (cf., Figure 3).
Finally, we calculate )(cf E for all Cc ? as:?
?
=
Ee
E efcPcf )()()( (2)
In this way, we can transform the frequency
vector in English ))(),..,(),(( 21 mefefef into a vector
in Chinese ))(),..,(),(( 21 nEEE cfcfcf=D .
Prior Probability Estimation
At Step 2, we approximately estimate the prior
probability )~(cP by using the document
frequencies of the translation candidates. The
data are obtained when we conduct candidate
collection (Step 4 in Figure 1).
?
?
?
?
?
?
?
??
??
Ee
Ee
Cc
ecPef
ecPef
ceP
ecPefcP
cePcP
cePcP
ecP
)|()(
)|()()|(
)|()()(StepM
)|()(
)|()()|(StepE
Figure 3. EM Algorithm
EM-NBC
At Step 2, we use an EM-based Na?ve
Bayesian Classifier (EM-NBC) to select the
candidates c~ whose posterior probabilities are
the largest:
??
???
?
+= ?
??
?
)~|(log)()~(logmaxarg
)|~(maxarg
~
~
~
~
ccPcfcP
cP
Cc
E
Cc
Cc
D
(3)
Equation (3) is based on Bayes? rule and the
assumption that the data in D are independently
generated from CcccP ?),~|( .
In our implementation, we use an equivalent
??
???
?
?? ?
??
)~|(log)()~(logminarg
~
~
ccPcfcP
Cc
E
Cc
? (4)
where 1?? is an additional parameter used to
emphasize the prior information. If we ignore the
first term in Equation (4), then the use of one
EM-NBC turns out to select the candidate whose
frequency vector is the closest to the transformed
vector D in terms of KL divergence (cf., Cover
and Tomas 1991).
EM-NBC-Ensemble
To further improve performance, we use an
ensemble (i.e., a linear combination) of
EM-NBCs (EM-NBC-Ensemble), while the
classifiers are constructed on the basis of the data
in different contexts with different window sizes.
More specifically, we calculate
where s),1,(i, L=iD denotes the data in different
contexts.
3.3 Translation Selection -- EM-TF-IDF
We view the translation selection problem as that
of calculating similarities between context
vectors and use as context vectors TF-IDF
vectors constructed with the EM Algorithm.
Figure 4 describes the algorithm in which we use
the same notations as those in
EM-NBC-Ensemble.
The idf value of a Chinese word c is calculated
in advance and as
)/)(log()( Fcdfcidf ?= (6)
where )cdf( denotes the document frequency of
c and F the total document frequency.
Input: e~ , C~ , contexts containing e~ , contexts containing
all Cc ~~ ? , Cc),cidf( ? ;
1. create a frequency vector )),(,),(),(( 21 mefefef L
),,1(, miEei L=? using contexts containing e~ ;
transforming the vector into
21 )),c(f,),c(f),c(f( nEEE L
),,1(, niCci L=? , using a translation dictionary and
the EM algorithm;
create a TF-IDF vector
11 )),cidf())c(f,),cidf()c(f( nnEE L=A ),,1(, niCci L=?
2. for each ( Cc ~~ ? ){
create a frequency vector )),(,),(),(( 21 ncfcfcf L
),,1(, niCci L=? using contexts containing c~ ;
create a TF-IDF vector
11 ))cidf())c(f,),cidf()c(f( nnL=B ),,1(, niCc i L=? ;
calculate ),cos()c~tfidf( BA= ; }
3. Sort Cc ~~ ? in descending order of )c~tfidf( ;
Output: the top sorted results
Figure 4. Algorithm of EM-TF-IDF
3.4 Advantage of Using EM Algorithm
The uses of EM-NBC-Ensemble and EM-TF-IDF
can be viewed as extensions of existing methods
for word or phrase translation using non-parallel
corpora. Particularly, the use of the EM
Algorithm can help to accurately transform a
frequency vector from one language to another.
Suppose that we are to determine if ?
? is a translation of ?information age? (actually
it is). The frequency vectors of context words for
?information age? and ?? are given in A
and D in Figure 5, respectively. If for each
English word we only retain the link connecting
to the Chinese translation with the largest
frequency (a link represented as a solid line) to
establish a many-to-one mapping and transform
vector A from English to Chinese, we obtain
vector B. It turns out, however, that vector B is
quite different from vector D, although they
should be similar to each other. We will refer to
this method as ?Major Translation? hereafter.
With EM, vector A in Figure 5 is transformed
into vector C, which is much closer to vector D,
as expected. Specifically, EM can split the
frequency of a word in English and distribute
them into its translations in Chinese in a
theoretically sound way (cf., the distributed
frequencies of ?internet?). Note that if we assume
a many-to-one (or one-to-one) mapping
?
=
=
s
i
icP
s
cP
1
)|~(1)|~( DD (5)
relationship, then the use of EM turns out to be
equivalent to that of Major Translation.
3.5 Combination
In order to further boost the performance of
translation, we propose to also use the translation
method proposed in Nagata et al Specifically, we
combine our method with that of Nagata et alby
using a back-off strategy.
Figure 6 illustrates the process of collecting
Chinese translation candidates for an English
Base NP ?information asymmetry? with Nagata et
al?s method.
In the combination of the two methods, we first
use Nagata et als method to perform translation;
if we cannot find translations, we next use our
method. We will denote this strategy ?Back-off?.
4. Experimental Results
We conducted experiments on translation of the
Base NPs from English to Chinese.
We extracted Base NPs (noun-noun pairs) from
the Encarta 1 English corpus using the tool
developed by Xun et al(2000). There were about
1 http://encarta.msn.com/Default.asp
3000 Base NPs extracted. In the experiments, we
used the HIT English-Chinese word translation
dictionary2 . The dictionary contains about 76000
Chinese words, 60000 English words, and
118000 translation links. As a web search engine,
we used Google (http://www.google.com).
Five translation experts evaluated the
translation results by judging whether or not they
were acceptable. The evaluations reported below
are all based on their judgements.
4.1 Basic Experiment
In the experiment, we randomly selected 1000
Base NPs from the 3000 Base NPs. We next used
our method to perform translation on the 1000
phrases. In translation selection, we employed
EM-NBC-Ensemble and EM-TF-IDF.
Table 1. Best translation result for each method
Accuracy (%)
Top 1 Top 3
Coverage
(%)
EM-NBC-Ensemble 61.7 80.3
Prior 57.6 77.6
MT-NBC-Ensemble 59.9 78.1
EM-KL-Ensemble 45.9 72.3
EM-NBC 60.8 78.9
EM-TF-IDF 61.9 80.8
MT-TF-IDF 58.2 77.6
EM-TF 55.8 77.8
89.9
Table 1 shows the results in terms of coverage
and top n accuracy. Here, coverage is defined as
the percentage of phrases which have translations
selected, while top n accuracy is defined as the
percentage of phrases whose selected top n
translations include correct translations.
For EM-NBC-Ensemble, we set the ? !in (4) to
be 5 on the basis of our preliminary experimental
results. For EM-TF-IDF, we used the non-web
data described in Section 4.4 to estimate idf
values of words. We used contexts with window
sizes of ?1, ?3, ?5, ?7, ?9, ?11.
2 The dictionary is created by the Harbin Institute of Technology.
A B C D


















Figure 5. Example of frequency vector transformation
1. Input ?information asymmetry?;
2. Search the English Base NP on web sites in Chinese
and obtain documents as follows (i.e., using partial parallel
corpora):
	


Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 514?522,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
HTM: A Topic Model for Hypertexts
Congkai Sun?
Department of Computer Science
Shanghai Jiaotong University
Shanghai, P. R. China
martinsck@hotmail.com
Bin Gao
Microsoft Research Asia
No.49 Zhichun Road
Beijing, P. R. China
bingao@microsoft.com
Zhenfu Cao
Department of Computer Science
Shanghai Jiaotong University
Shanghai, P. R. China
zfcao@cs.sjtu.edu.cn
Hang Li
Microsoft Research Asia
No.49 Zhichun Road
Beijing, P. R. China
hangli@microsoft.com
Abstract
Previously topic models such as PLSI (Prob-
abilistic Latent Semantic Indexing) and LDA
(Latent Dirichlet Allocation) were developed
for modeling the contents of plain texts. Re-
cently, topic models for processing hyper-
texts such as web pages were also proposed.
The proposed hypertext models are generative
models giving rise to both words and hyper-
links. This paper points out that to better rep-
resent the contents of hypertexts it is more es-
sential to assume that the hyperlinks are fixed
and to define the topic model as that of gen-
erating words only. The paper then proposes
a new topic model for hypertext processing,
referred to as Hypertext Topic Model (HTM).
HTM defines the distribution of words in a
document (i.e., the content of the document)
as a mixture over latent topics in the document
itself and latent topics in the documents which
the document cites. The topics are further
characterized as distributions of words, as in
the conventional topic models. This paper fur-
ther proposes a method for learning the HTM
model. Experimental results show that HTM
outperforms the baselines on topic discovery
and document classification in three datasets.
1 Introduction
Topic models are probabilistic and generative mod-
els representing contents of documents. Examples
of topic models include PLSI (Hofmann, 1999) and
LDA (Blei et al, 2003). The key idea in topic mod-
eling is to represent topics as distributions of words
* This work was conducted when the first author visited
Microsoft Research Asia as an intern.
and define the distribution of words in document
(i.e., the content of document) as a mixture over hid-
den topics. Topic modeling technologies have been
applied to natural language processing, text min-
ing, and information retrieval, and their effective-
ness have been verified.
In this paper, we study the problem of topic mod-
eling for hypertexts. There is no doubt that this is
an important research issue, given the fact that more
and more documents are available as hypertexts cur-
rently (such as web pages). Traditional work mainly
focused on development of topic models for plain
texts. It is only recently several topic models for pro-
cessing hypertexts were proposed, including Link-
LDA and Link-PLSA-LDA (Cohn and Hofmann,
2001; Erosheva et al, 2004; Nallapati and Cohen,
2008).
We point out that existing models for hypertexts
may not be suitable for characterizing contents of
hypertext documents. This is because all the models
are assumed to generate both words and hyperlinks
(outlinks) of documents. The generation of the latter
type of data, however, may not be necessary for the
tasks related to contents of documents.
In this paper, we propose a new topic model for
hypertexts called HTM (Hypertext Topic Model),
within the Bayesian learning approach (it is simi-
lar to LDA in that sense). In HTM, the hyperlinks
of hypertext documents are supposed to be given.
Each document is associated with one topic distribu-
tion. The word distribution of a document is defined
as a mixture of latent topics of the document itself
and latent topics of documents which the document
cites. The topics are further defined as distributions
514
of words. That means the content (topic distribu-
tions for words) of a hypertext document is not only
determined by the topics of itself but also the top-
ics of documents it cites. It is easy to see that HTM
contains LDA as a special case. Although the idea of
HTM is simple and straightforward, it appears that
this is the first work which studies the model.
We further provide methods for learning and in-
ference of HTM. Our experimental results on three
web datasets show that HTM outperforms the base-
line models of LDA, Link-LDA, and Link-PLSA-
LDA, in the tasks of topic discovery and document
classification.
The rest of the paper is organized as follows. Sec-
tion 2 introduces related work. Section 3 describes
the proposed HTM model and its learning and infer-
ence methods. Experimental results are presented in
Section 4. Conclusions are made in the last section.
2 Related Work
There has been much work on topic modeling. Many
models have been proposed including PLSI (Hof-
mann, 1999), LDA (Blei et al, 2003), and their
extensions (Griffiths et al, 2005; Blei and Lafferty,
2006; Chemudugunta et al, 2007). Inference and
learning methods have been developed, such as vari-
ational inference (Jordan et al, 1999; Wainwright
and Jordan, 2003), expectation propagation (Minka
and Lafferty, 2002), and Gibbs sampling (Griffiths
and Steyvers, 2004). Topic models have been uti-
lized in topic discovery (Blei et al, 2003), document
retrieval (Xing Wei and Bruce Croft, 2006), docu-
ment classification (Blei et al, 2003), citation analy-
sis (Dietz et al, 2007), social network analysis (Mei
et al, 2008), and so on. Most of the existing models
are for processing plain texts. There are also models
for processing hypertexts, for example, (Cohn and
Hofmann, 2001; Nallapati and Cohen, 2008; Gru-
ber et al, 2008; Dietz et al, 2007), which are most
relevant to our work.
Cohn and Hofmann (2001) introduced a topic
model for hypertexts within the framework of PLSI.
The model, which is a combination of PLSI and
PHITS (Cohn and Chang, 2000), gives rise to both
the words and hyperlinks (outlinks) of the document
in the generative process. The model is useful when
the goal is to understand the distribution of links
as well as the distribution of words. Erosheva et
al (2004) modified the model by replacing PLSI with
LDA. We refer to the modified mode as Link-LDA
and take it as a baseline in this paper. Note that the
above two models do not directly associate the top-
ics of the citing document with the topics of the cited
documents.
Nallapati and Cohn (2008) proposed an extension
of Link-LDA called Link-PLSA-LDA, which is an-
other baseline in this paper. Assuming that the cit-
ing and cited documents share similar topics, they
explicitly model the information flow from the cit-
ing documents to the cited documents. In Link-
PLSA-LDA, the link graph is converted into a bi-
partite graph in which links are connected from cit-
ing documents to cited documents. If a document
has both inlinks and outlinks, it will be duplicated
on both sides of the bipartite graph. The generative
process for the citing documents is similar to that of
Link-LDA, while the cited documents have a differ-
ent generative process.
Dietz et al(2007) proposed a topic model for ci-
tation analysis. Their goal is to find topical influ-
ence of publications in research communities. They
convert the citation graph (created from the publica-
tions) into a bipartite graph as in Link-PLSA-LDA.
The content of a citing document is assumed to be
generated by a mixture over the topic distribution
of the citing document and the topic distributions of
the cited documents. The differences between the
topic distributions of citing and cited documents are
measured, and the cited documents which have the
strongest influence on the citing document are iden-
tified.
Note that in most existing models described above
the hyperlinks are assumed to be generated and link
prediction is an important task, while in the HTM
model in this paper, the hyperlinks are assumed to
be given in advance, and the key task is topic iden-
tification. In the existing models for hypertexts, the
content of a document (the word distribution of the
document) are not decided by the other documents.
In contrast, in HTM, the content of a document is
determined by itself as well as its cited documents.
Furthermore, HTM is a generative model which can
generate the contents of all the hypertexts in a col-
lection, given the link structure of the collection.
Therefore, if the goal is to accurately learn and pre-
515
Table 1: Notations and explanations.
T Number of topics
D Documents in corpus
D Number of documents
?? , ?? Hyperparameters for ? and ?
? Hyperparameter to control the weight between
the citing document and the cited documents
? Topic distributions for all documents
? Word distribution for topic
b, c, z Hidden variables for generating word
d document (index)
wd Word sequence in document d
Nd Number of words in document d
Ld Number of documents cited by document d
Id Set of cited documents for document d
idl Index of lth cited document of document d
?d Distribution on cited documents of document d
?d Topic distribution associated with document d
bdn Decision on way of generating nth word in doc-
ument d
cdn Cited document that generates nth word in doc-
ument d
zdn Topic of nth word in document d
dict contents of documents, the use of HTM seems
more reasonable.
3 Hypertext Topic Model
3.1 Model
In topic modeling, a probability distribution of
words is employed for a given document. Specifi-
cally, the probability distribution is defined as a mix-
ture over latent topics, while each topic is future
characterized by a distribution of words (Hofmann,
1999; Blei et al, 2003). In this paper, we introduce
an extension of LDA model for hypertexts. Table 1
gives the major notations and their explanations.
The graphic representation of conventional LDA
is given in Figure 1(a). The generative process of
LDA has three steps. Specifically, in each document
a topic distribution is sampled from a prior distribu-
tion defined as Dirichlet distribution. Next, a topic is
sampled from the topic distribution of the document,
which is a multinominal distribution. Finally, a word
is sampled according to the word distribution of the
topic, which also forms a multinormal distribution.
The graphic representation of HTM is given in
Figure 1(b). The generative process of HTM is de-
scribed in Algorithm 1. First, a topic distribution
is sampled for each document according to Dirich-
let distribution. Next, for generating a word in a
document, it is decided whether to use the current
Algorithm 1 Generative Process of HTM
for each document d do
Draw ?d ? Dir(??).
end for
for each word wdn do
if Ld > 0 then
Draw bdn ? Ber(?)
Draw cdn ? Uni(?d)
if bdn = 1 then
Draw zdn ? Multi(?d)
else
Draw zdn ? Multi(?Idcdn )end if
else
Draw a topic zdn ? Multi(?d)
end if
Draw a word wdn ? P (wdn | zdn, ?)
end for
document or documents which the document cites.
(The weight between the citing document and cited
documents is controlled by an adjustable hyper-
parameter ?.) It is also determined which cited doc-
ument to use (if it is to use cited documents). Then, a
topic is sampled from the topic distribution of the se-
lected document. Finally, a word is sampled accord-
ing to the word distribution of the topic. HTM natu-
rally mimics the process of writing a hypertext docu-
ment by humans (repeating the processes of writing
native texts and anchor texts).
The formal definition of HTM is given be-
low. Hypertext document d has Nd words
wd = wd1 ? ? ?wdNd and Ld cited documents Id =
{id1, . . . , idLd}. The topic distribution of d is ?d
and topic distributions of the cited documents are
?i, i ? Id. Given ?, ?, and ?, the conditional proba-
bility distribution of wd is defined as:
p(wd|?, ?, ?) =
Nd?
n=1
?
bdn
p(bdn|?)
?
cdn
p(cdn|?d)
?
zdn
p(zdn|?d)bdnp(zdn|?idcdn )
1?bdnp(wdn|zdn, ?).
Here ?d, bdn, cdn, and zdn are hidden vari-
ables. When generating a word wdn, bdn determines
whether it is from the citing document or the cited
documents. cdn determines which cited document it
516
is when bdn = 0. In this paper, for simplicity we as-
sume that the cited documents are equally likely to
be selected, i.e., ?di = 1Ld .Note that ? represents the topic distributions of
all the documents. For any d, its word distribution
is affected by both ?d and ?i, i ? Id. There is a
propagation of topics from the cited documents to
the citing document through the use of ?i, i ? Id.
For a hypertext document d that does not have
cited documents. The conditional probability dis-
tribution degenerates to LDA:
p(wd|?d, ?) =
Nd?
n=1
?
zdn
p(zdn|?d)p(wdn|zdn, ?).
By taking the product of the marginal probabil-
ities of hypertext documents, we obtain the condi-
tional probability of the corpus D given the hyper-
parameters ?, ??, ?,
p(D|?, ??, ?) =
? D?
d=1
p(?d|??)
Nd?
n=1
?
bdn
p(bdn|?)
?
cdn
p(cdn|?d)
?
zdn
p(zdn|?d)bdnp(zdn|?Idcdn )
1?bdn
p(wdn|zdn, ?)d?. (1)
Note that the probability function (1) also covers the
special cases in which documents do not have cited
documents.
In HTM, the content of a document is decided by
the topics of the document as well as the topics of
the documents which the document cites. As a result
contents of documents can be ?propagated? along the
hyperlinks. For example, suppose web page A cites
page B and page B cites page C, then the content of
page A is influenced by that of page B, and the con-
tent of page B is further influenced by the content
of page C. Therefore, HTM is able to more accu-
rately represent the contents of hypertexts, and thus
is more useful for text processing such as topic dis-
covery and document classification.
3.2 Inference and Learning
An exact inference of the posterior probability of
HTM may be intractable, we employ the mean field
variational inference method (Wainwright and Jor-
dan, 2003; Jordan et al, 1999) to conduct approxi-
mation. Let I[?] be an indicator function. We first
define the following factorized variational posterior
distribution q with respect to the corpus:
q =
D?
d=1
q(?d|?d)
Nd?
n=1
(
q(xdn|?dn)(q(cdn|?dn)
)I[Ld>0]
q(zdn|?dn) ,
where ?, ?, ?, and ? denote free variational parame-
ters. Parameter ? is the posterior Dirichlet parameter
corresponding to the representations of documents
in the topic simplex. Parameters ?, ?, and ? cor-
respond to the posterior distributions of their asso-
ciated random variables. We then minimize the KL
divergence between q and the true posterior proba-
bility of the corpus by taking derivatives of the loss
function with respect to variational parameters. The
solution is listed as below.
Let ?iv be p(wvdn = 1|zi = 1) for the word v. If
Ld > 0, we have
E-step:
?di = ??i +
Nd?
n=1
?dn?dni +
D?
d?=1
Ld??
l=1
I [id?l = d]
Nd??
n=1
(1? ?d?n)?d?nl?d?ni .
?dni ? ?iv exp
{?dnEq [log (?di) |?d]
+ (1? ?dn)
Ld?
l=1
?dnlEq [log (?Idli) |?Idl ]
} .
?dn =
(
1 +
(
exp{
k?
i=1
((?dniEq[log(?di)|?d]
?
Ld?
l=1
?dnl?dniEq[log(?Idli)|?Idl ]
)
+ log ?? log (1? ?)}
)?1)?1
.
517
zw?
???
??
T Nd D
(a) LDA
w
??
?? ??
D T
d
z
D
Nd
?
c
?
b
Id
(b) HTM
z
w?
?
??
??
T Nd
D
z
d? ?
Ld
(c) Link-LDA
z
w
?
?
??
??
T
Nd
z
?
Ld
pi
z
wd?
N M
Cited Documents Citing Documents
d
(d) Link-PLSA-LDA
Figure 1: Graphical model representations
?dnl ? ?dl exp{(1? ?dn)
k?
i=1
?dniEq[log(?Idli)|?Idl ]}.
Otherwise,
?di = ??i +
Nd?
n=1
?dni +
D?
d?=1
Ld??
l=1
I [id?l = d]
Nd??
n=1
(1? ?d?n)?d?nl?d?ni .
?dni ? ?iv exp
{Eq [log (?di) |?d]
}.
From the first two equations we can see that the
cited documents and the citing document jointly af-
fect the distribution of the words in the citing docu-
ment.
M-step:
?ij ?
D?
d=1
Nd?
n=1
?dniwjdn.
In order to cope with the data sparseness problem
due to large vocabulary, we employ the same tech-
nique as that in (Blei et al, 2003). To be specific,
we treat ? as a K ?V random matrix, with each row
being independently drawn from a Dirichlet distri-
bution ?i ? Dir(??) . Variational inference is
modified appropriately.
4 Experimental Results
We compared the performances of HTM and three
baseline models: LDA, Link-LDA, and Link-PLSA-
LDA in topic discovery and document classification.
Note that LDA does not consider the use of link in-
formation; we included it here for reference.
4.1 Datasets
We made use of three datasets. The documents in the
datasets were processed by using the Lemur Took
kit (http://www.lemurproject.org), and the low fre-
quency words in the datasets were removed.
The first dataset WebKB (available at
http://www.cs.cmu.edu/?webkb) contains six
subjects (categories). There are 3,921 documents
and 7,359 links. The vocabulary size is 5,019.
518
The second dataset Wikipedia (available at
http://www.mpi-inf.mpg.de/?angelova) contains
four subjects (categories): Biology, Physics, Chem-
istry, and Mathematics. There are 2,970 documents
and 45,818 links. The vocabulary size is 3,287.
The third dataset is ODP composed of homepages
of researchers and their first level outlinked pages
(cited documents). We randomly selected five sub-
jects from the ODP archive. They are Cognitive
Science (CogSci), Theory, NeuralNetwork (NN),
Robotics, and Statistics. There are 3,679 pages and
2,872 links. The vocabulary size is 3,529.
WebKB and Wikipedia are public datasets widely
used in topic model studies. ODP was collected by
us in this work.
4.2 Topic Discovery
We created four topic models HTM, LDA, Link-
LDA, and Link-PLSA-LDA using all the data in
each of the three datasets, and evaluated the top-
ics obtained in the models. We heuristically set the
numbers of topics as 10 for ODP, 12 for WebKB,
and 8 for Wikipedia (i.e., two times of the number
of true subjects). We found that overall HTM can
construct more understandable topics than the other
models. Figure 2 shows the topics related to the
subjects created by the four models from the ODP
dataset. HTM model can more accurately extract
the three topics: Theory, Statistic, and NN than the
other models. Both LDA and Link-LDA had mixed
topics, labeled as ?Mixed? in Figure 2. Link-PLSA-
LDA missed the topic of Statistics. Interestingly, all
the four models split Cognitive Science into two top-
ics (showed as CogSci-1 and CogSci-2), probably
because the topic itself is diverse.
4.3 Document Classification
We applied the four models in the three datasets to
document classification. Specifically, we used the
word distributions of documents created by the mod-
els as feature vectors of the documents and used the
subjects in the datasets as categories. We further
randomly divided each dataset into three parts (train-
ing, validation, and test) and conducted 3-fold cross-
validation experiments. In each trial, we trained an
SVM classifier with the training data, chose param-
eters with the validation data, and conducted evalu-
ation on classification with the test data. For HTM,
Table 2: Classification accuracies in 3-fold cross-
validation.
LDA HTM Link-LDA Link-PLSA-LDA
ODP 0.640 0.698 0.535 0.581
WebKB 0.786 0.795 0.775 0.774
Wikipedia 0.845 0.866 0.853 0.855
Table 3: Sign-test results between HTM and the three
baseline models.
LDA Link-LDA Link-PLSA-LDA
ODP 0.0237 2.15e-05 0.000287
WebKB 0.0235 0.0114 0.00903
Wikipedia 1.79e-05 0.00341 0.00424
we chose the best ? value with the validation set in
each trial. Table 2 shows the classification accura-
cies. We can see that HTM performs better than the
other models in all three datasets.
We conducted sign-tests on all the results of the
datasets. In most cases HTM performs statistically
significantly better than LDA, Link-LDA, and Link-
PLSA-LDA (p-value < 0.05). The test results are
shown in Table 3.
4.4 Discussion
We conducted analysis on the results to see why
HTM can work better. Figure 3 shows an example
homepage from the ODP dataset, where superscripts
denote the indexes of outlinked pages. The home-
page contains several topics, including Theory, Neu-
ral network, Statistics, and others, while the cited
pages contain detailed information about the topics.
Table 4 shows the topics identified by the four mod-
els for the homepage. We can see that HTM can
really more accurately identify topics than the other
models.
The major reason for the better performance by
HTM seems to be that it can fully leverage the infor-
Table 4: Comparison of topics identified by the four mod-
els for the example homepage. Only topics with proba-
bilities > 0.1 and related to the subjects are shown.
Model Topics Probabilities
LDA Mixed 0.537
HTM Theory 0.229
NN 0.278
Statistics 0.241
Link-LDA Statistics 0.281
Link-PLSA-LDA Theory 0.527
CogSci-2 0.175
519
(a) LDA
Mixed NN Robot CogSci-1 CogSci-2
statistic learn robot visual conscious
compute conference project model psychology
algorithm system file experiment language
theory neural software change cognitive
complex network code function experience
mathematics model program response brain
model international data process theory
science compute motor data philosophy
computation ieee read move science
problem proceedings start observe online
random process build perception mind
analysis computation comment effect concept
paper machine post figure physical
method science line temporal problem
journal artificial include sensory content
(b) HTM
Theory Statistics NN Robot CogSci-1 CogSci-2
compute model learn robot conscious memory
science statistic system project visual psychology
algorithm data network software experience language
theory experiment neural motor change science
complex sample conference sensor perception cognitive
computation process model code move brain
mathematics method compute program theory human
paper analysis ieee build online neuroscience
problem response international line physical journal
lecture figure proceedings board concept society
random result machine read problem trauma
journal temporal process power philosophy press
bound probable computation type object learn
graph observe artificial comment content abuse
proceedings test intelligence post view associate
(c) Link-LDA
Statistics Mixed Robot CogSci-1 CogSci-2
statistic compute robot visual conscious
model conference project model psychology
data system software experiment cognitive
analysis learn file change language
method network motor function brain
learn computation robotics vision science
sample proceedings informatik process memory
algorithm neural program perception theory
process ieee build move philosophy
bayesian algorithm board response press
application international sensor temporal online
random science power object neuroscience
distribution complex code observe journal
simulate theory format sensory human
mathematics journal control figure mind
(d) Link-PLSA-LDA
Theory NN Robot CogSci-1 CogSci-2
compute conference robot conscious model
algorithm learn code experience process
computation science project language visual
theory international typeof book data
complex system motor change experiment
science compute control make function
mathematics network system problem learn
network artificial serve brain neural
paper ieee power world system
journal intelligence program read perception
proceedings robot software case represent
random technology file than vision
system proceedings build mind response
problem machine pagetracker theory object
lecture neural robotics content abstract
Figure 2: Topics identified by four models
Radford M.Neal
Professor, Dept. of Statistics and Dept. of Computer Science, University of Toronto
I?m currently highlighting the following :
? A new R function for performing univariate slice sampling.1
? A workshop paper on Computing Likelihood Functions for High-Energy Physics
Experiments when Distributions are Defined by Simulators with Nuisance Parameters.2
? Slides from a talk at the Third Workshop on Monte Carlo Methods on
?Short-Cut MCMC: An Alternative to Adaptation?, May 2007: Postscript, PDF.
Courses I?m teaching in Fall 2008 :
? STA 437: Methods for Multivariate Data3
? STA 3000: Advanced Theory of Statistics4
You can also find information on courses I?ve taught in the past.5
You can also get to information on :
? Research interests6 (with pointers to publications)
? Current and former graduate students7
? Current and former postdocs8
? Curriculum Vitae: PostScript, or PDF.
? Full publications list9
? How to contact me10
? Links to various places11
If you know what you want already,you may wish to go directly to :
? Software available on-line12
? Papers available on-line13
? Slides from talks14
? Miscellaneous other stuff15
Information in this hierarchy was last updated 2008-06-20.
Figure 3: An example homepage: http://www.cs.utoronto.ca/? radford/
520
Table 5: Word assignment in the example homepage.
Word bdn cdn Topic Probability
mcmc 0.544 2 Stat 0.949
experiment 0.546 2 Stat 0.956
neal 0.547 8 NN 0.985
likelihood 0.550 2 Stat 0.905
sample 0.557 2 Stat 0.946
statistic 0.559 2 Stat 0.888
parameter 0.563 2 Stat 0.917
perform 0.565 2 Stat 0.908
carlo 0.568 2 Stat 0.813
monte 0.570 2 Stat 0.802
toronto 0.572 8 NN 0.969
distribution 0.578 2 Stat 0.888
slice 0.581 2 Stat 0.957
energy 0.581 13 NN 0.866
adaptation 0.591 7 Stat 0.541
teach 0.999 11 Other 0.612
current 0.999 11 Other 0.646
curriculum 0.999 11 Other 0.698
want 0.999 11 Other 0.706
highlight 0.999 10 Other 0.786
professor 0.999 11 Other 0.764
academic 0.999 11 Other 0.810
student 0.999 11 Other 0.817
contact 0.999 11 Other 0.887
graduate 0.999 11 Other 0.901
Table 6: Most salient topics in cited pages.
URL Topic Probability
2 Stat 0.690
7 Stat 0.467
8 NN 0.786
13 NN 0.776
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95 0.9 0.85 0.8 0.75 0.7 0.65 0.6 0.55 0.5
ODP
Webkb
Wiki
A
c
c
u
r
a
c
y
?
Figure 4: Classification accuracies on three datasets with
different ? values. The cross marks on the curves cor-
respond to the average values of ? in the 3-fold cross-
validation experiments.
mation from the cited documents. We can see that
the content of the example homepage is diverse and
not very rich. It might be hard for the other base-
line models to identify topics accurately. In con-
trast, HTM can accurately learn topics by the help
of the cited documents. Specifically, if the content of
a document is diverse, then words in the document
are likely to be assigned into wrong topics by the
existing approaches. In contrast, in HTM with prop-
agation of topic distributions from cited documents,
the words of a document can be more accurately as-
signed into topics. Table 5 shows the first 15 words
and the last 10 words for the homepage given by
HTM, in ascending order of bdn, which measures
the degree of influence from the cited documents on
the words (the smaller the stronger). The table also
gives the values of cdn, indicating which cited docu-
ments have the strongest influence. Furthermore, the
topics having the largest posterior probabilities for
the words are also shown. We can see that the words
?experiment?, ?sample?, ?parameter?, ?perform?, and
?energy? are accurately classified. Table 6 gives the
most salient topics of cited documents. It also shows
the probabilities of the topics given by HTM. We can
see that there is a large agreement between the most
salient topics in the cited documents and the topics
which are affected the most in the citing document.
Parameter ? is the only parameter in HTM which
needs to be tuned. We found that the performance of
HTM is not very sensitive to the values of ?, which
reflects the degree of influence from the cited doc-
uments to the citing document. HTM can perform
well with different ? values. Figure 4 shows the clas-
sification accuracies of HTM with respect to differ-
ent ? values for the three datasets. We can see that
HTM works better than the other models in most of
the cases (cf., Table 2).
5 Conclusion
In this paper, we have proposed a novel topic
model for hypertexts called HTM. Existing models
for processing hypertexts were developed based on
the assumption that both words and hyperlinks are
stochastically generated by the model. The gener-
ation of latter type of data is actually unnecessary
for representing contents of hypertexts. In the HTM
model, it is assumed that the hyperlinks of hyper-
521
texts are given and only the words of the hypertexts
are stochastically generated. Furthermore, the word
distribution of a document is determined not only
by the topics of the document in question but also
from the topics of the documents which the doc-
ument cites. It can be regarded as ?propagation?
of topics reversely along hyperlinks in hypertexts,
which can lead to more accurate representations than
the existing models. HTM can naturally mimic hu-
man?s process of creating a document (i.e., by con-
sidering using the topics of the document and at the
same time the topics of the documents it cites). We
also developed methods for learning and inferring
an HTM model within the same framework as LDA
(Latent Dirichlet Allocation). Experimental results
show that the proposed HTM model outperforms
the existing models of LDA, Link-LDA, and Link-
PLSA-LDA on three datasets for topic discovery and
document classification.
As future work, we plan to compare the HTM
model with other existing models, to develop learn-
ing and inference methods for handling extremely
large-scale data sets, and to combine the current
method with a keyphrase extraction method for ex-
tracting keyphrases from web pages.
6 Acknowledgement
We thank Eric Xing for his valuable comments on
this work.
References
Ricardo Baeza-Yates and Berthier Ribeiro-Neto. 1999.
Modern Information Retrieval. ACM Press / Addison-
Wesley.
David Blei, Andrew Ng, and Michael Jordan. 2003. La-
tent Dirichlet alocation. Journal of machine Learning
Research, 3:993?1022.
David Blei and John Lafferty. 2005. Correlated Topic
Models. In Advances in Neural Information Process-
ing Systems 12.
David Blei and John Lafferty. 2006. Dynamic topic
models. In Proceedings of the 23rd international con-
ference on Machine learning.
Chaitanya Chemudugunta, Padhraic Smyth, and Mark
Steyvers. 2007. Modeling General and Specific As-
pects of Documents with a Probabilistic Topic Model.
In Advances in Neural Information Processing Sys-
tems 19.
David Cohn and Huan Chang. 2000. Learning to Proba-
bilistically Identify Authoritative Documents. In Pro-
ceedings of the 17rd international conference on Ma-
chine learning.
David Cohn and Thomas Hofmann. 2001. The missing
link - a probabilistic model of document content and
hypertext connectivity. In Neural Information Pro-
cessing Systems 13.
Laura Dietz, Steffen Bickel and Tobias Scheffer. 2007.
Unsupervised prediction of citation influences. In Pro-
ceedings of the 24th international conference on Ma-
chine learning.
Elena Erosheva, Stephen Fienberg, and John Lafferty.
2004. Mixed-membership models of scientific pub-
lications. In Proceedings of the National Academy of
Sciences, 101:5220?5227.
Thomas Griffiths and Mark Steyvers. 2004. Finding
Scientific Topics. In Proceedings of the National
Academy of Sciences, 101 (suppl. 1) .
Thomas Griffiths, Mark Steyvers, David Blei, and Joshua
Tenenbaum. 2005. Integrating Topics and Syntax. In
Advances in Neural Information Processing Systems,
17.
Amit Gruber, Michal Rosen-Zvi, and Yair Weiss. 2008.
Latent Topic Models for Hypertext. In Proceedings of
the 24th Conference on Uncertainty in Artificial Intel-
ligence.
Thomas Hofmann. 1999. Probabilistic Latent Semantic
Analysis. In Proceedings of the 15th Conference on
Uncertainty in Artificial Intelligence.
Michael Jordan, Zoubin Ghahramani, Tommy Jaakkola,
and Lawrence Saul. 1999. An Introduction to Varia-
tional Methods for Graphical Models. Machine Learn-
ing, 37(2):183?233.
QiaoZhu Mei, Deng Cai, Duo Zhang, and ChengXiang
Zhai. 2008. Topic Modeling with Network Regular-
ization. In Proceeding of the 17th international con-
ference on World Wide Web.
Thomas Minka and John Lafferty. 2002. Expectation-
Propagation for the Generative Aspect Model. In Pro-
ceedings of the 18th Conference in Uncertainty in Ar-
tificial Intelligence.
Ramesh Nallapati and William Cohen. 2008. Link-
PLSA-LDA: A new unsupervised model for topics and
influence of blogs. In International Conference for
Webblogs and Social Media.
Martin Wainwright, and Michael Jordan. 2003. Graph-
ical models, exponential families, and variational in-
ference. In UC Berkeley, Dept. of Statistics, Technical
Report, 2003.
Xing Wei and Bruce Croft. 2006. LDA-based document
models for ad-hoc retrieval. In Proceedings of the 29th
annual international ACM SIGIR conference on Re-
search and development in information retrieval.
522
c? 2004 Association for Computational Linguistics
Word Translation Disambiguation Using
Bilingual Bootstrapping
Hang Li? Cong Li?
Microsoft Research Asia Microsoft Research Asia
This article proposes a new method for word translation disambiguation, one that uses a machine-
learning technique called bilingual bootstrapping. In learning to disambiguate words to be trans-
lated, bilingual bootstrapping makes use of a small amount of classified data and a large amount
of unclassified data in both the source and the target languages. It repeatedly constructs classi-
fiers in the two languages in parallel and boosts the performance of the classifiers by classifying
unclassified data in the two languages and by exchanging information regarding classified data
between the two languages. Experimental results indicate that word translation disambiguation
based on bilingual bootstrapping consistently and significantly outperforms existing methods
that are based on monolingual bootstrapping.
1. Introduction
We address here the problem of word translation disambiguation. If, for example, we
were to attempt to translate the English noun plant, which could refer either to a type
of factory or to a form of flora (i.e., in Chinese, either to [gongchang] or to
[zhiwu]), our goal would be to determine the correct Chinese translation. That is, word
translation disambiguation is essentially a special case of word sense disambiguation
(in the above example, gongchang would correspond to the sense of factory and zhiwu
to the sense of flora).1
We could view word translation disambiguation as a problem of classification. To
perform the task, we could employ a supervised learning method, but since to do
so would require human labeling of data, which would be expensive, bootstrapping
would be a better choice.
Yarowsky (1995) has proposed a bootstrapping method for word sense disam-
biguation. When applied to translation from English to Chinese, his method starts
learning with a small number of English sentences that contain ambiguous English
words and that are labeled with correct Chinese translations of those words. It then
uses these classified sentences as training data to create a classifier (e.g., a decision list),
which it uses to classify unclassified sentences containing the same ambiguous words.
The output of this process is then used as additional training data. It also adopts the
one-sense-per-discourse heuristic (Gale, Church, and Yarowsky 1992b) in classifying
unclassified sentences. By repeating the above process, an accurate classifier for word
translation disambiguation can be created. Because this method uses data in a single
language (i.e., the source language in translation), we refer to it here as monolingual
bootstrapping (MB).
? 5F Sigma Center, No. 49 Zhichun Road, Haidian, Beijing, China, 100080. E-mail:{hangli,i-congl}@
microsoft.com.
1 In this article, we take English-Chinese translation as an example; but the ideas and methods described
here can be applied to any pair of languages.
2Computational Linguistics Volume 30, Number 1
In this paper, we propose a new method of bootstrapping, one that we refer to as
bilingual bootstrapping (BB). Instead of using data in one language, BB uses data in
two languages. In translation from English to Chinese, for example, BB makes use of
unclassified data from both languages. It also uses a small number of classified data
in English and, optionally, a small number of classified data in Chinese. The data in
the two languages should be from the same domain but are not required to be exactly
in parallel.
BB constructs classifiers for English-to-Chinese translation disambiguation by re-
peating the following two steps: (1) Construct a classifier for each of the languages
on the basis of classified data in both languages, and (2) use the constructed classifier
for each language to classify unclassified data, which are then added to the classified
data of the language. We can use classified data in both languages in step (1), because
words in one language have translations in the other, and we can transform data from
one language into the other.
We have experimentally evaluated the performance of BB in word translation
disambiguation, and all of our results indicate that BB consistently and significantly
outperforms MB. The higher performance of BB can be attributed to its effective use
of the asymmetric relationship between the ambiguous words in the two languages.
Our study is organized as follows. In Section 2, we describe related work. Specifi-
cally, we formalize the problem of word translation disambiguation as that of classifi-
cation based on statistical learning. As examples, we describe two such methods: one
using decision lists and the other using naive Bayes. We also explain the Yarowsky
disambiguation method, which is based on Monolingual Bootstrapping. In Section 3,
we describe bilingual bootstrapping, comparing BB with MB, and discussing the re-
lationship between BB and co-training. In Section 4, we describe our experimental
results, and finally, in Section 5, we give some concluding remarks.
2. Related Work
2.1 Word Translation Disambiguation
Word translation disambiguation (in general, word sense disambiguation) can be
viewed as a problem of classification and can be addressed by employing various
supervised learning methods. For example, with such a learning method, an English
sentence containing an ambiguous English word corresponds to an instance, and the
Chinese translation of the word in the context (i.e., the word sense) corresponds to a
classification decision (a label).
Many methods for word sense disambiguation based on supervised learning tech-
nique have been proposed. They include those using naive Bayes (Gale, Church, and
Yarowsky 1992a), decision lists (Yarowsky 1994), nearest neighbor (Ng and Lee 1996),
transformation-based learning (Mangu and Brill 1997), neural networks (Towell and
Voorhees 1998), Winnow (Golding and Roth 1999), boosting (Escudero, Marquez, and
Rigau 2000), and naive Bayesian ensemble (Pedersen 2000). The assumption behind
these methods is that it is nearly always possible to determine the sense of an ambigu-
ous word by referring to its context, and thus all of the methods build a classifier (i.e.,
a classification program) using features representing context information (e.g., sur-
rounding context words). For other related work on translation disambiguation, see
Brown et al (1991), Bruce and Weibe (1994), Dagan and Itai (1994), Lin (1997), Ped-
ersen and Bruce (1997), Schutze (1998), Kikui (1999), Mihalcea and Moldovan (1999),
Koehn and Knight (2000), and Zhou, Ding, and Huang (2001).
Let us formulate the problem of word sense (translation) disambiguation as fol-
lows. Let E denote a set of words. Let ? denote an ambiguous word in E, and let e
3Li and Li Word Translation Disambiguation Using Bilingual Bootstrapping
denote a context word in E. (Throughout this article, we use Greek letters to represent
ambiguous words and italic letters to represent context words.) Let T? denote the set
of senses of ?, and let t? denote a sense in T?. Let e? stand for an instance representing
a context of ?, that is, a sequence of context words surrounding ?:
e? = (e?,1, e?,2, . . . , (?), . . . , e?,m), e?,i ? E, (i = 1, . . . , m)
For the example presented earlier, we have ? = plant, T? = {1, 2}, where 1 represents
the sense factory and 2 the sense flora. From the phrase ?. . . computer manufacturing
plant and adjacent. . . ? we obtain e? = (. . . computer, manufacturing, (plant), and,
adjacent, . . . ).
For a specific ?, we define a binary classifier for resolving each of its ambiguities
in T? in a general form as2
P(t? | e?), t? ? T? and P(?t? | e?), t?? = T? ? {t?}
where e? denotes an instance representing a context of ?. All of the supervised learning
methods mentioned previously can automatically create such a classifier. To construct
classifiers using supervised methods, we need classified data such as those in Figure 1.
2.2 Decision Lists
Let us first consider the use of decision lists, as proposed in Yarowsky (1994). Let f?
denote a feature of the context of ?. A feature can be, for example, a word?s occurrence
immediately to the left of ?. We define many such features. For each feature f?, we
use the classified data to calculate the posterior probability ratio of each sense t? with
respect to the feature as
?(t? | f?) =
P(t? | f?)
P(?t? | f?)
For each feature f?, we create a rule consisting of the feature, the sense
arg max
t??T?
?(t? | f?)
and the score
max
t??T?
?(t? | f?)
We sort the rules in descending order with respect to their scores, provided that the
scores of the rules are larger than the default
max
t??T?
P(t?)
P(?t?)
The sorted rules form an if-then-else type of rule sequence, that is, a decision list.3 For
a new instance e?, we use the decision list to determine its sense. The rule in the list
whose feature is first satisfied in the context of e? is applied in sense disambiguation.
2 In this article we always employ binary classifiers even there are multiple classes.
3 We note that there are two types of decision lists. One is defined as here; the other is defined as a
conditional distribution over a partition of the feature space (cf. Li and Yamanishi 2002).
4Computational Linguistics Volume 30, Number 1
P1 . . .Nissan car and truck plant. . . (1)
P2 . . . computer manufacturing plant and adjacent. . . (1)
P3 . . . automated manufacturing plant in Fremont. . . (1)
P4 . . .divide life into plant and animal kingdom. . . (2)
P5 . . . thousands of plant and animal species. . . (2)
P6 . . . zonal distribution of plant life. . . (2)
. . .
. . .
Figure 1
Examples of classified data (? = plant).
2.3 Naive Bayesian Ensemble
Let us next consider the use of naive Bayesian classifiers. Given an instance e?, we can
calculate
??(e?) = max
t??T?
P(t? | e?)
P(?t? | e?)
= max
t??T?
P(t?)P(e? | t?)
P(?t?)P(e? | t??)
(1)
according to Bayes? rule and select the sense
t?(e?) = arg max
t??T?
P(t?)P(e? | t?)
P(?t?)P(e? | t??)
(2)
In a naive Bayesian classifier, we assume that the words in e? with a fixed t? are
independently generated from P(e? | t?) and calculate
P(e? | t?) =
m
?
i=1
P(e?,i | t?)
Here P(e? | t?) represents the conditional probability of e in the context of ? given t?.
We calculate P(e? | t??) similarly. We can then calculate (1) and (2) with the obtained
P(e? | t?) and P(e? | t??).
The naive Bayesian ensemble method for word sense disambiguation, as proposed
in Pedersen (2000), employs a linear combination of several naive Bayesian classifiers
constructed on the basis of a number of nested surrounding contexts4
P(t? | e?) =
1
h
h
?
i=1
P(t? | e??,i)
e??,1 ? ? ? ? ? e??,i ? ? ? ? e??,h = e?? (i = 1, . . . , h)
The naive Bayesian ensemble is reported to perform the best for word sense disam-
biguation with respect to a benchmark data set (Pedersen 2000).
2.4 Monolingual Bootstrapping
Since data preparation for supervised learning is expensive, it is desirable to develop
bootstrapping methods. Yarowsky (1995) proposed such a method for word sense
disambiguation, which we refer to as monolingual bootstrapping.
4 Here u ? v denotes that u is a sub-sequence of v.
5Li and Li Word Translation Disambiguation Using Bilingual Bootstrapping
Let L? denote a set of classified instances (labeled data) in English, each represent-
ing one context of ?:
L? = {(e?,1, t?,1), (e?,2, t?,2), . . . , (e?,k, t?,k)}
t?,i ? T? (i = 1, 2, . . . , k)
and U? a set of unclassified instances (unlabeled data) in English, each representing
one context of ?:
U? = {e?,1, e?,2, . . . , e?,l}
The instances in Figure 1 can be considered examples of L?. Furthermore, we have
LE =
?
??E
L?, UE =
?
??E
U?, T =
?
??E
T?,
An algorithm for monolingual bootstrapping is presented in Figure 2. For a better
comparison with bilingual bootstrapping, we have extended the method so that it
Input: E, T, LE, UE, Parameter: b, ?
Repeat the following processes until unable to continue
1. 1 for each (? ? E) {
2 for each (t ? T?) {
3 use L? to create classifier:
P(t | e?), t ? T? and P(?t | e?), t? ? T? ? {t}; }}
2. 4 for each (? ? E) {
5 NU ? {}; NL ? {};
6 for each (t ? T?) {
7 St ? {};
8 Qt ? {};}
9 for each (e? ? U?){
10 calculate ??(e?) = max
t?T?
P(t | e?)
P(?t | e?)
;
11 let t?(e?) = arg max
t?T?
P(t | e?)
P(?t | e?)
;
12 if (??(e?) > ? & t?(e?) = t)
13 put e? into St;}
14 for each (t ? T?){
15 sort e? ? St in descending order of ??(e?) and put the top b
elements into Qt;}
16 for each (e? ?
?
t Qt){
17 put e? into NU and put (e?, t?(e?)) into NL;}
18 L? ? L?
?
NL;
19 U? ? U? ? NU;}
Figure 2
Monolingual bootstrapping.
6Computational Linguistics Volume 30, Number 1
performs disambiguation for all the words in E. Note that we can employ any kind
of classifier here.
At step 1, for each ambiguous word ? we create binary classifiers for resolving its
ambiguities (cf. lines 1?3 of Figure 2). At step 2, we use the classifiers for each word
? to select some unclassified instances from U?, classify them, and add them to L? (cf.
lines 4?19). We repeat the process until all the data are classified.
Lines 9?13 show that for each unclassified instance e?, we classify it as having
sense t if t?s posterior odds are the largest among the possible senses and are larger
than a threshold ?. For each class t, we store the classified instances in St. Lines 14?15
show that for each class t, we only choose the top b classified instances in terms of the
posterior odds. For each class t, we store the selected top b classified instances in Qt.
Lines 16?17 show that we create the classified instances by combining the instances
with their classification labels.
After line 17, we can employ the one-sense-per-discourse heuristic to further clas-
sify unclassified data, as proposed in Yarowsky (1995). This heuristic is based on the
observation that when an ambiguous word appears in the same text several times, its
tokens usually refer to the same sense. In the bootstrapping process, for each newly
classified instance, we automatically assign its class label to those unclassified instances
that also contain the same ambiguous word and co-occur with it in the same text.
Hereafter, we will refer to this method as monolingual bootstrapping with one
sense per discourse. This method can be viewed as a special case of co-training (Blum
and Mitchell 1998).
2.5 Co-training
Monolingual bootstrapping augmented with the one-sense-per-discourse heuristic can
be viewed as a special case of co-training, as proposed by Blum and Mitchell (1998)
(see also Collins and Singer 1999; Nigam et al 2000; and Nigam and Ghani 2000). Co-
training conducts two bootstrapping processes in parallel and makes them collaborate
with each other. More specifically, co-training begins with a small number of classified
data and a large number of unclassified data. It trains two classifiers from the classified
data, uses each of the two classifiers to classify some unclassified data, makes the two
classifiers exchange their classified data, and repeats the process.
3. Bilingual Bootstrapping
3.1 Basic Algorithm
Bilingual bootstrapping makes use of a small amount of classified data and a large
amount of unclassified data in both the source and the target languages in translation.
It repeatedly constructs classifiers in the two languages in parallel and boosts the
performance of the classifiers by classifying data in each of the languages and by
exchanging information regarding the classified data between the two languages.
Figures 3 and 4 illustrate the process of bilingual bootstrapping. Figure 5 shows
the translation relationship among the ambiguous words plant, zhiwu, and gongchang.
There is a classifier for plant in English. There are also two classifiers, one each for
zhiwu and gongchang, respectively, in Chinese. Sentences containing plant in English
and sentences containing zhiwu and gongchang in Chinese are used.
In the beginning, sentences P1 and P4 on the English side are assigned labels 1 and
2, respectively (Figure 3). On the Chinese side, sentences G1 and G3 are assigned labels
1 and 3, respectively, and sentences Z1 and Z3 are assigned labels 2 and 4, respectively.
The four labels here correspond to the four links in Figure 5. For example, label 1
represents the sense factory and label 2 represents the sense flora. Other sentences are
7Li and Li Word Translation Disambiguation Using Bilingual Bootstrapping
Figure 3
Bilingual bootstrapping (1).
Figure 4
Bilingual bootstrapping (2).
8Computational Linguistics Volume 30, Number 1
~
 
?

~
Figure 5
Example of translation dictionary.
not labeled. Bilingual bootstrapping uses labeled sentences P1, P4, G1, and Z1 to create
a classifier for plant disambiguation (between label 1 and label 2). It also uses labeled
sentences Z1, Z3, and P4 to create a classifier for zhiwu and uses labeled sentences G1,
G3, and P1 to create a classifier for gongzhang. Bilingual bootstrapping next uses the
classifier for plant to label sentences P2 and P5 (Figure 4). It uses the classifier for zhiwu
to label sentences Z2 and Z4, and uses the classifier for gongchang to label sentences
G2 and G4. The process is repeated until we cannot continue.
To describe this process formally, let E denote a set of words in English, C a set of
words in Chinese, and T a set of senses (links) in a translation dictionary as shown in
Figure 5. (Any two linked words can be translations of each other.) Mathematically,
T is defined as a relation between E and C, that is, T ? E ? C. Let ? stand for an
ambiguous word in E, and ? an ambiguous word in C. Also let e stand for a context
word in E, c a context word in C, and t a sense in T.
For an English word ?, T? = {t | t = (?, ??), t ? T} represents the set of ??s possible
senses (i.e., its links), and C? = {?? | (?, ??) ? T} represents the Chinese words that can
be translations of ? (i.e., Chinese words to which ? is linked). Similarly, for a Chinese
word ?, let T? = {t | t = (??, ?), t ? T} and E? = {?? | (??, ?) ? T}.
For the example in Figure 5, when ? = plant, we have T? = {1, 2} and C? =
{gongchang, zhiwu}. When ? = gongchang, T? = {1, 3} and E? = {plant, mill}. When
? = zhiwu, T? = {2, 4} and E? = {plant, vegetable}. Note that gongchang and zhiwu
share the senses {1, 2} with plant.
Let e? denote an instance (a sequence of context words surrounding ?) in English:
e? = (e?,1, e?,2, . . . , e?,m), e?,i ? E (i = 1, 2, . . . , m)
Let c? denote an instance (a sequence of context words surrounding ?) in Chinese:
c? = (c?,1, c?,2, . . . , c?,n, c?,i ? C (i = 1, 2, . . . , n)
For an English word ?, a binary classifier for resolving each of the ambiguities in T? is
defined as
P(t? | e?), t? ? T? and P(?t? | e?), t?? = T? ? {t?}
Similarly, for a Chinese word ?, a binary classifier is defined as
P(t? | c?), t? ? T? and P(?t? | c?), t? = T? ? {t?}
Let L? denote a set of classified instances in English, each representing one context
of ?:
L? = {(e?,1, t?,1), (e?,2, t?,2), . . . , (e?,k, t?,k)}, t?,i ? T? (i = 1, 2, . . . , k)
9Li and Li Word Translation Disambiguation Using Bilingual Bootstrapping
and U? a set of unclassified instances in English, each representing one context of ?:
U? = {e?,1, e?,2, . . . , e?,l}
Similarly, we denote the sets of classified and unclassified instances with respect to ?
in Chinese as L? and U? , respectively. Furthermore, we have
LE =
?
??E
L?, LC =
?
??C
L? , UE =
?
??E
U?, UC =
?
??C
U?
We also have
T =
?
??E
T? =
?
??C
T?
Sentences P1 and P4 in Figure 3 are examples of L?. Sentences Z1, Z3 and G1, G3 are
examples of L? .
We perform bilingual bootstrapping as described in Figure 6. Note that we can,
in principle, employ any kind of classifier here.
The figure explains the process for English (left-hand side); the process for Chinese
(right-hand side) behaves similarly. At step 1, for each ambiguous word ?, we create
binary classifiers for resolving its ambiguities (cf. lines 1?3). The main point here is
that we use classified data from both languages to construct classifiers, as we describe
in Section 3.2. For the example in Figure 3, we use both L? (sentences P1 and P4) and
L? , ? ? C? (sentences Z1 and G1) to construct a classifier resolving ambiguities in
T? = {1, 2}. Note that not only P1 and P4, but also Z1 and G1, are related to {1, 2}.
At step 2, for each word ?, we use its classifiers to select some unclassified instances
from U?, classify them, and add them to L? (cf. lines 4?19). We repeat the process until
we cannot continue.
Lines 9?13 show that for each unclassified instance e?, we use the classifiers to
classify it into the class (sense) t if t?s posterior odds are the largest among the possible
classes and are larger than a threshold ?. For each class t, we store the classified
instances in St. Lines 14?15 show that for each class t, we choose only the top b
classified instances (in terms of the posterior odds), which are then stored in Qt. Lines
16?17 show that we create the classified instances by combining the instances with
their classification labels. We note that after line 17 we can also employ the one-sense-
per-discourse heuristic.
3.2 An Implementation
Although we can in principle employ any kind of classifier in BB, we use here naive
Bayes (or naive Bayesian ensemble). We also use the EM algorithm in classified data
transformation between languages. As will be made clear, this implementation of BB
can naturally combine the features of naive Bayes (or naive Bayesian ensemble) and
the features of EM. Hereafter, when we refer to BB, we mean this implementation of
BB.
We explain the process for English (left-hand side of Figure 6); the process for
Chinese (right-hand side of figure) behaves similarly. At step 1 in BB, we construct a
naive Bayesian classifier as described in Figure 7. At step 2, for each instance e?, we
use the classifier to calculate
??(e?) = max
t??T?
P(t? | e?)
P(?t? | e?)
= max
t??T?
P(t?)P(e? | t?)
P(?t?)P(e? | t??)
10
Computational Linguistics Volume 30, Number 1
Figure 6
Bilingual bootstrapping.
We estimate
P(e? | t?) =
m
?
i=1
P(e?,i | t?)
We estimate P(e? | t??) similarly. We estimate P(e? | t?) by linearly combining P(E)(e? | t?)
estimated from English and P(C)(e? | t?) estimated from Chinese:
P(e? | t?) = (1 ? ?? ?)P(E)(e? | t?) + ?P(C)(e? | t?) + ?P(U)(e?) (3)
where 0 ? ? ? 1, 0 ? ? ? 1, ? + ? ? 1, and P(U)(e?) is a uniform distribution over E,
which is used for avoiding zero probability. In this way, we estimate P(e? | t?) using
information from not only English, but also Chinese.
We estimate P(E)(e? | t?) with maximum-likelihood estimation (MLE) using L? as
data. The estimation of P(C)(e? | t?) proceeds as follows.
For the sake of readability, we rewrite P(C)(e? | t?) as P(e | t). We define a finite-
mixture model of the form P(c | t) =
?
e?E P(c | e, t)P(e | t), and for a specific ? we
assume that the data in
L? = {(c?,1, t?,1), (c?,2, t?,2), . . . , (c?,h, t?,h)}, t?,i ? T? (i = 1, . . . , h), ?? ? C?
11
Li and Li Word Translation Disambiguation Using Bilingual Bootstrapping
estimate P(E)(e? | t?) with MLE using L? as data;
estimate P(C)(e? | t?) with EM algorithm using L? for each ? ? C? as data;
calculate P(e? | t?) as a linear combination of P(E)(e? | t?) and P(C)(e? | t?);
estimate P(t?) with MLE using L?;
calculate P(e? | t??) and P(?t?) similarly.
Figure 7
Creating a naive Bayesian classifier.
are generated independently from the model. We can therefore employ the expectation-
maximization (EM) algorithm (Dempster, Laird, and Rubin 1977) to estimate the pa-
rameters of the model, including P(e | t). Note that e and c represent context words.
Recall that E is a set of words in English, C is a set of words in Chinese, and T
is a set of senses. For a specific English word e, Ce = {c? | (e, c?) ? T} represents the
Chinese words that are its possible translations.
Initially, we set
P(c | e, t) =
?
?
?
1
|Ce|
, if c ? Ce
0, if c ? Ce
P(e | t) = 1|E| , e ? E
We next estimate the parameters by iteratively updating them, as described in Figure 8,
until they converge. Here f (c, t) stands for the frequency of c in the instances which
have sense t. The context information in Chinese f (c, t?) is then ?transformed? into the
English version P(C)(e? | t?) through the links in T.
Figure 9 shows an example of estimating P(e? | t?) with respect to the factory sense
(i.e., sense 1). We first use sentences such as P1 in Figure 3 to estimate P(E)(e? | t?) with
MLE as described above. We next use sentences such as G1 to estimate P(C)(e? | t?) as
described above. Specifically, with the frequency data f (c, t?) and EM we can estimate
P(C)(e? | t?). Finally, we linearly combine P(E)(e? | t?) and P(C)(e? | t?) to obtain P(e? | t?).
3.3 Comparison of BB and MB
We note that monolingual bootstrapping is a special case of bilingual bootstrapping
(consider the situation in which ? = 0 in formula (3)).
BB can always perform better than MB. The asymmetric relationship between the
ambiguous words in the two languages stands out as the key to the higher performance
E-step: P(e | c, t) ? P(c | e, t)P(e | t)?
e?E P(c | e, t)P(e | t)
M-step: P(c | e, t) ? f (c, t)P(e | c, t)?
c?C f (c, t)P(e | c, t)
P(e | t) ?
?
c?C f (c, t)P(e | c, t)
?
c?C f (c, t)
Figure 8
The EM algorithm.
12
Computational Linguistics Volume 30, Number 1
Figure 9
Parameter estimation.
Figure 10
Example application of BB.
of BB. By asymmetric relationship we mean the many-to-many mapping relationship
between the words in the two languages, as shown in Figure 10.
Suppose that the classifier with respect to plant has two classes (denoted as A
and B in Figure 10). Further suppose that the classifiers with respect to gongchang and
zhiwu in Chinese each have two classes (C and D) and (E and F), respectively. A and
D are equivalent to one another (i.e., they represent the same sense), and so are B and
E.
Assume that instances are classified after several iterations of BB as depicted in
Figure 10. Here, circles denote the instances that are correctly classified and crosses
denote the instances that are incorrectly classified.
Since A and D are equivalent to one another, we can transform the instances with D
and use them to boost the performance of classification to A, because the misclassified
instances (crosses) with D are those mistakenly classified from C, and they will not
have much negative effect on classification to A, even though the translation from
Chinese into English can introduce some noise. Similar explanations can be given for
other classification decisions.
In contrast, MB uses only the instances in A and B to construct a classifier. When
the number of misclassified instances increases (as is inevitable in bootstrapping), its
performance will stop improving. This phenomenon has also been observed when MB
is applied to other tasks (cf. Banko and Brill 2001; Pierce and Cardie 2001).
13
Li and Li Word Translation Disambiguation Using Bilingual Bootstrapping
3.4 Relationship between BB and Co-training
We note that there are similarities between BB and co-training. Both BB and co-training
execute two bootstrapping processes in parallel and make the two processes collabo-
rate with one another in order to improve their performance. The two processes look at
different types of information in data and exchange the information in learning. How-
ever, there are also significant differences between BB and co-training. In co-training,
the two processes use different features, whereas in BB, the two processes use different
classes. In BB, although the features used by the two classifiers are transformed from
one language into the other, they belong to the same space. In co-training, on the other
hand, the features used by the two classifiers belong to two different spaces.
4. Experimental Results
We have conducted two experiments on English-Chinese translation disambiguation.
In this section, we will first describe the experimental settings and then present the
results. We will also discuss the results of several follow-on experiments.
4.1 Translation Disambiguation Using BB
Although it is possible to straightforwardly apply the algorithm of BB described in
Section 3 to word translation disambiguation, here we use a variant of it better adapted
to the task and for fairer comparison with existing technologies. The variant of BB we
use has four modifications:
1. It actually employs naive Bayesian ensemble rather than naive Bayes,
because naive Bayesian ensemble generally performs better than naive
Bayes (Pedersen 2000).
2. It employs the one-sense-per-discourse heuristic. It turns out that in BB
with one sense per discourse, there are two layers of bootstrapping. On
the top level, bilingual bootstrapping is performed between the two
languages, and on the second level, co-training is performed within each
language. (Recall that MB with one sense per discourse can be viewed as
co-training.)
3. It uses only classified data in English at the beginning. That is to say, it
requires exactly the same human labeling efforts as MB does.
4. It individually resolves ambiguities on selected English words such as
plant and interest. (Note that the basic algorithm of BB performs
disambiguation on all the words in English and Chinese.) As a result, in
the case of plant, for example, the classifiers with respect to gongchang
and zhiwu make classification decisions only on D and E and not C and
F (in Figure 10), because it is not necessary to make classification
decisions on C and F. In particular, it calculates ??(c) as ??(c) = P(c | t)
and sets ? = 0 in the right-hand side of step 2.
4.2 Translation Disambiguation Using MB
We consider here two implementations of MB for word translation disambiguation.
In the first implementation, in addition to the basic algorithm of MB, we also use
(1) naive Bayesian ensemble, (2) one sense per discourse, and (3) a small amount of
classified data in English at the beginning. (We will denote this implementation as MB-
B hereafter.) The second implementation is different from the first one only in (1). That
14
Computational Linguistics Volume 30, Number 1
Table 1
Data descriptions in Experiment 1.
'
 ! 8 8

 

	 
  
	 



  

  

)
; 
  
	.) 


	 
	
  
		
 
  

 


  
	 	
is, it employs a decision list as the classifier. This implementation is exactly the one
proposed in Yarowsky (1995). (We will denote it as MB-D hereafter.) MB-B and MB-D
can be viewed as the state-of-the-art methods for word translation disambiguation
using bootstrapping.
4.3 Experiment 1: WSD Benchmark Data
We first applied BB, MB-B, and MB-D to translation disambiguation on the English
words line and interest using a benchmark data set.5 The data set consists mainly
of articles from the Wall Street Journal and is prepared for conducting word sense
disambiguation (WSD) on the two words (e.g., Pedersen 2000).
We collected from the HIT dictionary6 the Chinese words that can be translations
of the two English words; these are listed in Table 1. One sense of an English word
links to one group of Chinese words. (For the word interest, we used only its four
major senses, because the remaining two minor senses occur in only 3.3% of the data.)
For each sense, we selected an English word that is strongly associated with the
sense according to our own intuition (cf. Table 1). We refer to this word as a seed
word. For example, for the sense of money paid for the use of money, we selected the
word rate. We viewed the seed word as a classified ?sentence,? following a similar
proposal in Yarowsky (1995). In this way, for each sense we had a classified instance
in English. As unclassified data in English, we collected sentences in news articles
from a Web site (www.news.com), and as unclassified data in Chinese, we collected
sentences in news articles from another Web site (news.cn.tom.com). Note that we
need to use only the sentences containing the words in Table 1. We observed that the
distribution of the senses in the unclassified data was balanced. As test data, we used
the entire benchmark data set.
Table 2 shows the sizes of the data sets. Note that there are in general more
unclassified sentences (and texts) in Chinese than in English, because one English
word usually can link to several Chinese words (cf. Figure 5).
As the translation dictionary, we used the HIT dictionary, which contains about
76,000 Chinese words, 60,000 English words, and 118,000 senses (links). We then used
the data to conduct translation disambiguation with BB, MB-B, and MB-D, as described
in Sections 4.1 and Section 4.2.
5 http://www.d.umn.edu/?tpederse/data.html.
6 This dictionary was created by Harbin Institute of Technology.
15
Li and Li Word Translation Disambiguation Using Bilingual Bootstrapping
Table 2
Data set sizes in Experiment 1.
Unclassified sentences (texts)
Words English Chinese Test sentences
interest 1,927 (1,072) 8,811 (2,704) 2,291
line 3,666 (1,570) 5,398 (2,894) 4,148
For both BB and MB-B, we used an ensemble of five naive Bayesian classifiers
with window sizes of ?1,?3,?5,?7, and ?9 words, and we set the parameters ?, b,
and ? to 0.2, 15, and 1.5, respectively. The parameters were tuned on the basis of our
preliminary experimental results on MB-B; they were not tuned, however, for BB. We
set the BB-specific parameter ? to 0.4, which meant that we weighted information
from English and Chinese equally.
Table 3 shows the translation disambiguation accuracies of the three methods as
well as that of a baseline method in which we always choose the most frequent sense.
Figures 11 and 12 show the learning curves of MB-D, MB-B, and BB. Figure 13 shows
the accuracies of BB with different ? values. From the results, we see that BB consistently
and significantly outperforms both MB-D and MB-B. The results from the sign test are
statistically significant (p-value < 0.001). (For the sign test method, see, for example,
Yang and Liu [1999]).
Table 4 shows the results achieved by some existing supervised learning methods
with respect to the benchmark data (cf. Pedersen 2000). Although BB is a method nearly
equivalent to one based on unsupervised learning, it still performs favorably when
compared with the supervised methods (note that since the experimental settings are
different, the results cannot be directly compared).
4.4 Experiment 2: Yarowsky?s Words
We also conducted translation on seven of the twelve English words studied in Yarowsky
(1995). Table 5 lists the words we used.
Table 3
Accuracies of disambiguation in Experiment 1.
Words Major (%) MB-D (%) MB-B (%) BB (%)
interest 54.6 54.7 69.3 75.5
line 53.5 55.6 54.1 62.7
Table 4
Accuracies of supervised methods.
interest (%) line (%)
Naive Bayesian ensemble 89 88
Naive Bayes 74 72
Decision tree 78 ?
Neural network ? 76
Nearest neighbor 87 ?
16
Computational Linguistics Volume 30, Number 1
Figure 11
Learning curves with interest.
Figure 12
Learning curves with line.
Figure 13
Accuracies of BB with different ? values.
17
Li and Li Word Translation Disambiguation Using Bilingual Bootstrapping
Table 5
Data set descriptions in Experiment 2.
'
 ! 8

  =   
=

  = 
=


  =   
=)	
	

  = 
=
	

  = 
=

	
  =  

=
.
=   
=

Table 6
Data set sizes in Experiment 2.
Unclassified sentences (texts) Test
Words English Chinese sentences
bass 142 (106) 8,811 (4,407) 200
drug 3,053 (1,048) 5,398 (3,143) 197
duty 1,428 (875) 4,338 (2,714) 197
palm 366 (267) 465 (382) 197
plant 7,542 (2,919) 24,977 (13,211) 197
space 3,897(1,494) 14,178 (8,779) 197
tank 417 (245) 1,400 (683) 199
Total 16,845 (6,954) 59,567 (33,319) 1,384
For each of the English words, we extracted about 200 sentences containing the
word from the Encarta7 English corpus and hand-labeled those sentences using our
own Chinese translations. We used the labeled sentences as test data and the unlabeled
sentences as unclassified data in English. Table 6 shows the data set sizes. We also
used the sentences in the Great Encyclopedia8 Chinese corpus as unclassified data in
Chinese. We defined, for each sense, a seed word in English as a classified instance in
English (cf. Table 5). We did not, however, conduct translation disambiguation on the
words crane, sake, poach, axes, and motion, because the first four words do not frequently
occur in the Encarta corpus, and the accuracy of choosing the major translation for
the last word already exceeds 98%.
We next applied BB, MB-B, and MB-D to word translation disambiguation. The
parameter settings were the same as those in Experiment 1. Table 7 shows the dis-
ambiguation accuracies, and Figures 14?20 show the learning curves for the seven
words.
From the results, we see again that BB significantly outperforms MB-D and MB-B.
Note that the results of MB-D here cannot be directly compared with those in Yarowsky
(1995), because the data used are different. Naive Bayesian ensemble did not perform
well on the word duty, causing the accuracies of both MB-B and BB to deteriorate.
7 http://encarta.msn.com/default.asp.
8 http://www.whlib.ac.cn/sjk/bkqs.htm.
18
Computational Linguistics Volume 30, Number 1
Figure 14 Figure 15
Learning curves with bass. Learning curves with drug.
Figure 16 Figure 17
Learning curves with duty. Learning curves with palm.
Figure 18 Figure 19
Learning curves with plant. Learning curves with space.
Figure 20
Learning curves with tank.
19
Li and Li Word Translation Disambiguation Using Bilingual Bootstrapping
Table 7
Accuracies of disambiguation in Experiment 2.
Words Major (%) MB-D (%) MB-B (%) BB (%)
bass 61.0 57.0 89.0 92.0
drug 77.7 78.7 79.7 86.8
duty 86.3 86.8 72.0 75.1
palm 82.2 80.7 83.3 92.4
plant 71.6 89.3 95.4 95.9
space 64.5 83.3 84.3 87.8
tank 60.3 76.4 76.9 84.4
Total 71.9 78.8 82.9 87.8
Table 8
Top words for interest rate sense of interest.
MB-B BB
payment saving
cut payment
earn benchmark
short whose
short-term base
yield prefer
u.s. fixed
margin debt
benchmark annual
regard dividend
4.5 Discussion
We investigated the reason for BB?s outperforming MB and found that the explanation
in Section 3.3 appears to be valid according to the following observations.
1. In a naive Bayesian classifier, words with large values of likelihood ratio P(e|t)P(e|?t)
will have strong influences on classification. We collected the words having the largest
likelihood ratio with respect to each sense t in both BB and MB-B and found that BB
obviously has more ?relevant words? than MB-B. Here words relevant to a particular
sense refer to the words that are strongly indicative of that sense according to human
judgments.
Table 8 shows the top 10 words in terms of likelihood ratio with respect to the
interest rate sense in both BB and MB-B. The relevant words are italicized. Figure 21
shows the numbers of relevant words with respect to the four senses of interest in BB
and MB-B.
2. From Figure 13, we see that the performance of BB remains high or gets higher
even when ? becomes larger than 0.4 (recall that ? was fixed at 0.2). This result strongly
indicates that the information from Chinese has positive effects.
3. One might argue that the higher performance of BB can be attributed to the
larger amount of unclassified data it uses, and thus if we increase the amount of
unclassified data for MB, it is likely that MB can perform as well as BB. We conducted
an additional experiment and found that this is not the case. Figure 22 shows the
accuracies achieved by MB-B as the amount of unclassified data increases. The plot
shows that the accuracy of MB-B does not improve when the amount of unclassified
20
Computational Linguistics Volume 30, Number 1
Figure 21
Number of relevant words.
Figure 22
When more unclassified data available.
data increases. Figure 22 plots again the results of BB as well as those of a method
referred to as MB-C. In MB-C, we linearly combined two MB-B classifiers constructed
with two different unclassified data sets, and we found that although the accuracies
are improved in MB-C, they are still much lower than those of BB.
4. We have noticed that a key to BB?s performance is the asymmetric relationship
between the classes in the two languages. Therefore, we tested the performance of
MB and BB when the classes in the two languages are symmetric (i.e., one-to-one
mapping).
We performed two experiments on text classification in which the categories were
finance and industry, and finance and trade, respectively. We collected Chinese texts
from the People?s Daily in 1998 that had already been assigned class labels. We used
half of them as unclassified training data in Chinese and the remaining as test data in
Chinese. We also collected English texts from the Wall Street Journal. We used them as
unlabeled training data in English. We used the class names (i.e., finance, industry, and
trade, as seed data (classified data)). Table 9 shows the accuracies of text classification.
From the results we see that when the classes are symmetric, BB cannot outperform
MB.
5. We also investigated the effect of the one-sense-per-discourse heuristic. Table 10
shows the performance of MB and BB on the word interest with and without the heuris-
tic. We see that with the heuristic, the performance of both MB and BB is improved.
Even without the heuristic, BB still performs better than MB with the heuristic.
21
Li and Li Word Translation Disambiguation Using Bilingual Bootstrapping
Table 9
Accuracy of text classification.
Classes MB-B (%) BB (%)
Finance and industry 93.2 92.9
Finance and trade 78.4 78.6
Table 10
Accuracy of disambiguation.
MB-D (%) MB-B (%) BB (%)
With one sense per discourse 54.7 69.3 75.5
Without one sense per discourse 54.6 66.4 71.6
5. Conclusion
We have addressed here the problem of classification across two languages. Specifically
we have considered the problem of bootstrapping. We find that when the task is word
translation disambiguation between two languages, we can use the asymmetric rela-
tionship between the ambiguous words in the two languages to significantly boost the
performance of bootstrapping. We refer to this approach as bilingual bootstrapping.
We have developed a method for implementing this bootstrapping approach that nat-
urally combines the use of naive Bayes and the EM algorithm. Future work includes a
theoretical analysis of bilingual bootstrapping (generalization error of BB, relationship
between BB and co-training, etc.) and extensions of bilingual bootstrapping to more
complicated machine translation tasks.
Acknowledgments
We thank Ming Zhou, Ashley Chang and
Yao Meng for their valuable comments and
suggestions on an early draft of this article.
We acknowledge the four anonymous
reviewers of this article for their valuable
comments and criticisms. We thank Michael
Holmes, Mark Petersen, Kevin Knight, and
Bob Moore for their checking of the English
of this article. A previous version of this
article appeared in Proceedings of the Fortieth
Annual Meeting of the Association for
Computational Linguistics.
References
Banko, Michele, and Eric Brill. 2001. Scaling
to very very large corpora for natural
language disambiguation. In Proceedings of
the 39th Annual Meeting of the Association for
Computational Linguistics, pages 26?33,
Toulouse, France.
Blum, Avrim, and Tom M. Mitchell. 1998.
Combining labeled and unlabeled data
with co-training. In Proceedings of the 11th
Annual Conference on Computational
Learning Theory, pages 92?100, Madison,
WI.
Brown, Peter F., Stephen A. Della Pietra,
Vincent J. Della Pietra, and Robert L.
Mercer. 1991. Word sense disambiguation
using statistical methods. In Proceedings of
the 29th Annual Meeting of the Association for
Computational Linguistics, pages 264?270,
University of California, Berkeley.
Bruce, Rebecca, and Janyce Weibe. 1994.
Word-sense disambiguation using
decomposable models. In Proceedings of the
32nd Annual Meeting of the Association for
Computational Linguistics, pages 139?146,
New Mexico State University, Las Cruces.
Collins, Michael, and Yoram Singer. 1999.
Unsupervised models for named entity
classification. In Proceedings of the 1999
Joint SIGDAT Conference on Empirical
Methods in Natural Language Processing and
Very Large Corpora, University of
Maryland, College Park.
Dagan, Ido, and Alon Itai. 1994. Word sense
disambiguation using a second language
monolingual corpus. Computational
Linguistics, 20(4):563?596.
22
Computational Linguistics Volume 30, Number 1
Dempster, A. P., N. M. Laird, and D. B.
Rubin. 1977. Maximum likelihood from
incomplete data via the EM algorithm.
Journal of the Royal Statistical Society B,
39:1?38.
Escudero, Gerard, Lluis Marquez, and
German Rigau. 2000. Boosting applied to
word sense disambiguation. In Proceedings
of the 12th European Conference on Machine
Learning, pages 129?141, Barcelona.
Gale, William, Kenneth Church, and David
Yarowsky. 1992a. A method for
disambiguating word senses in a large
corpus. Computers and Humanities,
26:415?439.
Gale, William, Kenneth Church, and David
Yarowsky. 1992b. One sense per discourse.
In Proceedings of DARPA Speech and Natural
Language Workshop, pages 233?237,
Harriman, NY.
Golding, Andrew R., and Dan Roth. 1999. A
Winnow-based approach to
context-sensitive spelling correction.
Machine Learning, 34:107?130.
Kikui, Genichiro. 1999. Resolving translation
ambiguity using non-parallel bilingual
corpora. In Proceedings of ACL ?99 Workshop
on Unsupervised Learning in Natural
Language Processing, University of
Maryland, College Park.
Koehn, Philipp, and Kevin Knight. 2000.
Estimating word translation probabilities
from unrelated monolingual corpora using
the EM algorithm. In Proceedings of the 17th
National Conference on Artificial Intelligence,
pages 711?715, Austin, TX.
Li, Hang, and Kenji Yamanishi. 2002. Text
classification using ESC-based stochastic
decision lists. Information Processing and
Management, 38:343?361.
Lin, Dekang. 1997. Using syntactic
dependency as local context to resolve
word sense ambiguity. In Proceedings of the
35th Annual Meeting of the Association for
Computational Linguistics, pages 64?71,
Universidad Nacional de Educacio?n a
Distancia (UNED), Madrid.
Mangu, Lidia, and Eric Brill. 1997.
Automatic rule acquisition for spelling
correction. In Proceedings of the 14th
International Conference on Machine Learning,
pages 187?194, Nashville, TN.
Mihalcea, Rada, and Dan I. Moldovan. 1999.
A method for word sense disambiguation
of unrestricted text. In Proceedings of the
37th Annual Meeting of the Association for
Computational Linguistics, pages 152?158,
University of Maryland, College Park.
Ng, Hwee Tou, and Hian Beng Lee. 1996.
Integrating multiple knowledge sources to
disambiguate word sense: An
exemplar-based approach. In Proceedings of
the 34th Annual Meeting of the Association for
Computational Linguistics, pages 40?47,
University of California, Santa Cruz.
Nigam, Kamal, Andrew McCallum,
Sebastian Thrun, and Tom M. Mitchell.
2000. Text classification from labeled and
unlabeled documents using EM. Machine
Learning, 39(2?3):103?134.
Nigam, Kamal, and Rayid Ghani. 2000.
Analyzing the effectiveness and
applicability of co-training. In Proceedings
of the 9th International Conference on
Information and Knowledge Management,
pages 86?93, McLean, VA.
Pedersen, Ted. 2000. A simple approach to
building ensembles of naive Bayesian
classifiers for word sense disambiguation.
In Proceedings of the First Meeting of the
North American Chapter of the Association for
Computational Linguistics, Seattle.
Pedersen, Ted, and Rebecca Bruce. 1997.
Distinguishing word senses in untagged
text. In Proceedings of the Second Conference
on Empirical Methods in Natural Language
Processing, pages 197?207, Providence, RI.
Pierce, David, and Claire Cardie. 2001.
Limitations of co-training for natural
language learning from large datasets. In
Proceedings of the 2001 Conference on
Empirical Methods in Natural Language
Processing, Carnegie Mellon University,
Pittsburgh.
Schutze, Hinrich. 1998. Automatic word
sense discrimination. Computational
Linguistics, 24(1):97?124.
Towell, Geoffrey, and Ellen M. Voorhees.
1998. Disambiguating highly ambiguous
words. Computational Linguistics,
24(1):125?146.
Yang, Yiming, and Xin Liu. 1999. A
re-examination of text categorization
methods. In Proceedings of the 22nd Annual
International ACM SIGIR Conference on
Research and Development in Information
Retrieval, pages 42?49, Berkeley, CA.
Yarowsky, David. 1994. Decision lists for
lexical ambiguity resolution: Application
to accent restoration in Spanish and
French. In Proceedings of the 32nd Annual
Meeting of the Association for Computational
Linguistics, pages 88?95, New Mexico
State University, Las Cruces.
Yarowsky, David. 1995. Unsupervised word
sense disambiguation rivaling supervised
methods. In Proceedings of the 33rd Annual
Meeting of the Association for Computational
Linguistics, pages 189?196.
Zhou, Ming, Yuan Ding, and Changning
Huang. 2001. Improving translation
selection with a new translation model
trained by independent monolingual
corpora. International Journal of
Computational Linguistics and Chinese
Language Processing, 6(1):1?26.
Exploring Asymmetric Clustering for Statistical Language Modeling 
Jianfeng Gao  
Microsoft Research, Asia 
Beijing, 100080, P.R.C  
jfgao@microsoft.com 
Joshua T. Goodman  
Microsoft Research, Redmond 
Washington 98052, USA  
joshuago@microsoft.com 
Guihong Cao1  
Department of Computer 
Science and Engineering of 
Tianjin University, China  
Hang Li  
Microsoft Research, Asia 
Beijing, 100080, P.R.C  
hangli@microsoft.com 
                                                     
1 This work was done while Cao was visiting Microsoft Research Asia. 
Abstract 
The n-gram model is a stochastic model, 
which predicts the next word (predicted 
word) given the previous words 
(conditional words) in a word sequence.  
The cluster n-gram model is a variant of 
the n-gram model in which similar words 
are classified in the same cluster. It has 
been demonstrated that using different 
clusters for predicted and conditional 
words leads to cluster models that are 
superior to classical cluster models which 
use the same clusters for both words.  This 
is the basis of the asymmetric cluster 
model (ACM) discussed in our study.  In 
this paper, we first present a formal 
definition of the ACM. We then describe 
in detail the methodology of constructing 
the ACM. The effectiveness of the ACM 
is evaluated on a realistic application, 
namely Japanese Kana-Kanji conversion.  
Experimental results show substantial 
improvements of the ACM in comparison 
with classical cluster models and word 
n-gram models at the same model size.  
Our analysis shows that the 
high-performance of the ACM lies in the 
asymmetry of the model. 
1 Introduction 
The n-gram model has been widely applied in many 
applications such as speech recognition, machine 
translation, and Asian language text input [Jelinek, 
1990; Brown et al, 1990; Gao et al, 2002].  It is a 
stochastic model, which predicts the next word 
(predicted word) given the previous n-1 words 
(conditional words) in a word sequence. 
The cluster n-gram model is a variant of the word 
n-gram model in which similar words are classified 
in the same cluster.  This has been demonstrated as 
an effective way to deal with the data sparseness 
problem and to reduce the memory sizes for realistic 
applications. Recent research [Yamamoto et al, 
2001] shows that using different clusters for 
predicted and conditional words can lead to cluster 
models that are superior to classical cluster models, 
which use the same clusters for both words [Brown 
et al, 1992].  This is the basis of the asymmetric 
cluster model (ACM), which will be formally 
defined and empirically studied in this paper.  
Although similar models have been used in previous 
studies [Goodman and Gao, 2000; Yamamoto et al, 
2001], several issues have not been completely 
investigated. These include: (1) an effective 
methodology for constructing the ACM, (2) a 
thorough comparative study of the ACM with 
classical cluster models and word models when they 
are applied to a realistic application, and (3) an 
analysis of the reason why the ACM is superior. 
The goal of this study is to address the above 
three issues. We first present a formal definition of 
the ACM; then we describe in detail the 
methodology of constructing the ACM including (1) 
an asymmetric clustering algorithm in which 
different metrics are used for clustering the 
predicted and conditional words respectively; and 
(2) a method for model parameter optimization in 
which the optimal cluster numbers are found for 
different clusters.  We evaluate the ACM on a real 
application, Japanese Kana-Kanji conversion, which 
converts phonetic Kana strings into proper Japanese 
orthography. The performance is measured in terms 
of character error rate (CER).  Our results show 
substantial improvements of the ACM in 
comparison with classical cluster models and word 
n-gram models at the same model size.  Our analysis 
shows that the high-performance of the ACM comes 
                Computational Linguistics (ACL), Philadelphia, July 2002, pp. 183-190.
                         Proceedings of the 40th Annual Meeting of the Association for
from better structure and better smoothing, both of 
which lie in the asymmetry of the model. 
This paper is organized as follows: Section 1 
introduces our research topic, and then Section 2 
reviews related work. Section 3 defines the ACM 
and describes in detail the method of model 
construction. Section 4 first introduces the Japanese 
Kana-Kanji conversion task; it then presents our 
main experiments and a discussion of our findings.  
Finally, conclusions are presented in Section 5. 
2 Related Work 
A large amount of previous research on clustering 
has been focused on how to find the best clusters 
[Brown et al, 1992; Kneser and Ney, 1993; 
Yamamoto and Sagisaka, 1999; Ueberla, 1996; 
Pereira et al, 1993; Bellegarda et al, 1996; Bai et 
al., 1998]. Only small differences have been 
observed, however, in the performance of the 
different techniques for constructing clusters. In this 
study, we focused our research on novel techniques 
for using clusters ? the ACM, in which different 
clusters are used for predicted and conditional words 
respectively. 
The discussion of the ACM in this paper is an 
extension of several studies below. The first similar 
cluster model was presented by Goodman and Gao 
[2000] in which the clustering techniques were 
combined with Stolcke?s [1998] pruning to reduce 
the language model (LM) size effectively. Goodman 
[2001] and Gao et al [2001] give detailed 
descriptions of the asymmetric clustering algorithm.  
However, the impact of the asymmetric clustering 
on the performance of the resulting cluster model 
was not empirically studied there.  Gao et al, [2001] 
presented a fairly thorough empirical study of 
clustering techniques for Asian language modeling. 
Unfortunately, all of the above work studied the 
ACM without applying it to an application; thus 
only perplexity results were presented.  The first real 
application of the ACM was a simplified bigram 
ACM used in a Chinese text input system [Gao et al  
2002]. However, quite a few techniques (including 
clustering) were integrated to construct a Chinese 
language modeling system, and the contribution of 
using the ACM alone was by no means completely 
investigated. 
Finally, there is one more point worth 
mentioning. Most language modeling improvements 
reported previously required significantly more 
space than word trigram models [Rosenfeld, 2000]. 
Their practical value is questionable since all 
realistic applications have memory constraints. In 
this paper, our goal is to achieve a better tradeoff 
between LM performance (perplexity and CER) and 
model size. Thus, whenever we compare the 
performance of different models (i.e. ACM vs. word 
trigram model), Stolcke?s pruning is employed to 
bring the models compared to similar sizes. 
3 Asymmetric Cluster Model 
3.1 Model  
The LM predicts the next word wi given its history h 
by estimating the conditional probability P(wi|h). 
Using the trigram approximation, we have 
P(wi|h)?P(wi|wi-2wi-1), assuming that the next word 
depends only on the two preceding words.  
In the ACM, we will use different clusters for 
words in different positions.  For the predicted word, 
wi, we will denote the cluster of the word by PWi, 
and we will refer to this as the predictive cluster. .For 
the words wi-2 and wi-1 that we are conditioning on, 
we will denote their clusters by CWi-2 and CWi-1 
which we call conditional clusters.  When we which 
to refer to a cluster of a word w in general we will 
use the notation W.  The ACM estimates the 
probability of wi given the two preceeding words wi-2 
and wi-1 as the product of the following two 
probabilities: 
(1) The probability of the predicted cluster PWi 
given the preceding conditional clusters CWi-2 
and CWi-1, P(PWi|CWi-2CWi-1), and 
(2) The probability of the word given its cluster PWi 
and the preceding conditional clusters CWi-2 and 
CWi-1, P(wi|CWi-2CWi-1PWi). 
Thus, the ACM can be parameterized by 
)|()|()|( 1212 iiiiiiii PWCWCWwPCWCWPWPhwP ???? ?? (1) 
The ACM consists of two sub-models: (1) the 
cluster sub-model P(PWi|CWi-2CWi-1), and (2) the 
word sub-model P(wi|CWi-2CWi-1PWi). To deal with 
the data sparseness problem, we used a backoff 
scheme (Katz, 1987) for the parameter estimation of 
each sub-model. The backoff scheme recursively 
estimates the probability of an unseen n-gram by 
utilizing (n-1)-gram estimates. 
The basic idea underlying the ACM is the use of 
different clusters for predicted and conditional 
words respectively.  Classical cluster models are 
symmetric in that the same clusters are employed for 
both predicted and conditional words.  However, the 
symmetric cluster model is suboptimal in practice. 
For example, consider a pair of words like ?a? and 
?an?.  In general, ?a? and ?an? can follow the same 
words, and thus, as predicted words, belong in the 
same cluster. But, there are very few words that can 
follow both ?a? and ?an?. So as conditional words, 
they belong in different clusters. 
In generating clusters, two factors need to be 
considered: (1) clustering metrics, and (2) cluster 
numbers.  In what follows, we will investigate the 
impact of each of the factors. 
3.2 Asymmetric clustering  
The basic criterion for statistical clustering is to 
maximize the resulting probability (or minimize the 
resulting perplexity) of the training data. Many 
traditional clustering techniques [Brown et al, 
1992] attempt to maximize the average mutual 
information of adjacent clusters 
?=
21 , 2
12
2121 )(
)|(log)(),(
WW WP
WWPWWPWWI , (2) 
where the same clusters are used for both predicted 
and conditional words. We will call these clustering 
techniques symmetric clustering, and the resulting 
clusters both clusters.  In constructing the ACM, we 
used asymmetric clustering, in which different 
clusters are used for predicted and conditional 
words. In particular, for clustering conditional 
words, we try to minimize the perplexity of training 
data for a bigram of the form P(wi|Wi-1), which is 
equivalent to maximizing 
?
=
?
N
i
ii WwP
1
1)|( . (3) 
where N is the total number of words in the training 
data.  We will call the resulting clusters conditional 
clusters denoted by CW. For clustering predicted 
words, we try to minimize the perplexity of training 
data of P(Wi|wi-1)?P(wi|Wi). We will call the 
resulting clusters predicted clusters denoted by PW. 
We have2 
??
= ?
?
=
? ?=?
N
i i
ii
i
iiN
i
iiii WP
wWP
wP
WwPWwPwWP
1 1
1
1
1 )(
)(
)(
)()|()|(  
  ?
=
?
?
?= N
i i
ii
i
ii
WP
WwP
wP
wWP
1
1
1 )(
)(
)(
)(  
  ?
= ??
?= N
i
ii
i
i WwPwP
wP
1
1
1
)|()(
)( . 
Now, 
)(
)(
1?i
i
wP
wP is independent of the clustering used. 
Therefore, for the selection of the best clusters, it is 
sufficient to try to maximize 
?
=
?
N
i
ii WwP
1
1 )|( . (4) 
This is very convenient since it is exactly the op-
posite of what was done for conditional clustering. It 
                                                     
2 Thanks to Lillian Lee for suggesting this justification of 
predictive clusters. 
means that we can use the same clustering tool for 
both, and simply switch the order used by the 
program used to get the raw counts for clustering.  
The clustering technique we used creates a binary 
branching tree with words at the leaves.  The ACM 
in this study is a hard cluster model, meaning that 
each word belongs to only one cluster.  So in the 
clustering tree, each word occurs in a single leaf.   In 
the ACM, we actually use two different clustering 
trees. One is optimized for predicted words, and the 
other for conditional words. 
The basic approach to clustering we used is a 
top-down, splitting clustering algorithm. In each 
iteration, a cluster is split into two clusters in the 
way that the splitting achieves the maximal entropy 
decrease (estimated by Equations (3) or (4)). Finally, 
we can also perform iterations of swapping all words 
between all clusters until convergence i.e. no more 
entropy decrease can be found3. We find that our 
algorithm is much more efficient than agglomerative 
clustering algorithms ? those which merge words 
bottom up.  
3.3 Parameter optimization 
Asymmetric clustering results in two binary 
clustering trees. By cutting the trees at a certain 
level, it is possible to achieve a wide variety of 
different numbers of clusters.  For instance, if the 
tree is cut after the 8th level, there will be roughly 
28=256 clusters.  Since the tree is not balanced, the 
actual number of clusters may be somewhat smaller. 
We use Wl to represent the cluster of a word w using 
a tree cut at level l.  In particular, if we set l to the 
value ?all?, it means that the tree is cut at infinite 
depth, i.e. each cluster contains a single word. The 
ACM model of Equation (1) can be rewritten as 
 P(PWil|CWi-2jCWi-1j)?P(wi|PWi-2kCWi-1kCWil). (5) 
To optimally apply the ACM to realistic applications 
with memory constraints, we are always seeking the 
correct balance between model size and 
performance. We used Stolcke?s pruning method to 
produce many ACMs with different model sizes. In 
our experiments, whenever we compare techniques, 
we do so by comparing the performance (perplexity 
and CER) of the LM techniques at the same model 
sizes. Stolcke?s pruning is an entropy-based cutoff 
                                                     
3 Notice that for experiments reported in this paper, we 
used the basic top-down algorithm without swapping. 
Although the resulting clusters without swapping are not 
even locally optimal, our experiments show that the 
quality of clusters (in terms of the perplexity of the 
resulting ACM) is not inferior to that of clusters with 
swapping. 
method, which can be described as follows: all 
n-grams that change perplexity by less than a 
threshold are removed from the model. For pruning 
the ACM, we have two thresholds: one for the 
cluster sub-model P(PWil|CWi-2jCWi-1j) and one for 
the word sub-model P(wi|CWi-2kCWi-1kPWil) 
respectively, denoted by tc and  tw below. 
In this way, we have 5 different parameters that 
need to be simultaneously optimized: l, j, k, tc, and 
tw, where j, k, and l are the numbers of clusters, and tc 
and tw are the pruning thresholds.  
A brute-force approach to optimizing such a large 
number of parameters is prohibitively expensive. 
Rather than trying a large number of combinations 
of all 5 parameters, we give an alternative technique 
that is significantly more efficient. Simple math 
shows that the perplexity of the overall model 
P(PWil|CWi-2jCWi-1j)? P(wi|CWi-2kCWi-1kPWil) is 
equal to the perplexity of the cluster sub-model 
P(PWil|CWi-2jCWi-1j) times the perplexity of the 
word sub-model P(wi|CWi-2kCWi-1kPWil).  The size of 
the overall model is clearly the sum of the sizes of 
the two sub-models.  Thus, we try a large number of 
values of j, l, and a pruning threshold tc for 
P(PWil|CWi-2jCWi-1j), computing sizes and 
perplexities of each, and a similarly large number of 
values of l,  k, and a separate threshold tw for 
P(wi|CWi-2kCWi-1kPWil).  We can then look at all 
compatible pairs of these models (those with the 
same value of l) and quickly compute the perplexity 
and size of the overall models.  This allows us to 
relatively quickly search through what would 
otherwise be an overwhelmingly large search space. 
4 Experimental Results and Discussion 
4.1 Japanese Kana-Kanji Conversion Task 
Japanese Kana-Kanji conversion is the standard 
method of inputting Japanese text by converting a 
syllabary-based Kana string into the appropriate 
combination of ideographic Kanji and Kana. This is 
a similar problem to speech recognition, except that 
it does not include acoustic ambiguity. The 
performance is generally measured in terms of 
character error rate (CER), which is the number of 
characters wrongly converted from the phonetic 
string divided by the number of characters in the 
correct transcript. The role of the language model is, 
for all possible word strings that match the typed 
phonetic symbol string, to select the word string 
with the highest language model probability. 
Current products make about 5-10% errors in con-
version of real data in a wide variety of domains. 
4.2 Settings 
In the experiments, we used two Japanese 
newspaper corpora: the Nikkei Newspaper corpus, 
and the Yomiuri Newspaper corpus. Both text 
corpora have been word-segmented using a lexicon 
containing 167,107 entries.  
We performed two sets of experiments: (1) pilot 
experiments, in which model performance is 
measured in terms of perplexity and (2) Japanese 
Kana-Kanji conversion experiments, in which the 
performance of which is measured in terms of CER. 
In the pilot experiments, we used a subset of the 
Nikkei newspaper corpus: ten million words of the 
Nikkei corpus for language model training, 10,000 
words for held-out data, and 20,000 words for 
testing data. None of the three data sets overlapped.  
In the Japanese Kana-Kanji conversion experiments, 
we built language models on a subset of the Nikkei 
Newspaper corpus, which contains 36 million 
words. We performed parameter optimization on a 
subset of held-out data from the Yomiuri Newspaper 
corpus, which contains 100,000 words. We 
performed testing on another subset of the Yomiuri 
Newspaper corpus, which contains 100,000 words. 
In both sets of experiments, word clusters were 
derived from bigram counts generated from the 
training corpora. Out-of-vocabulary words were not 
included in perplexity and error rate computations. 
4.3 Impact of asymmetric clustering 
As described in Section 3.2, depending on the 
clustering metrics we chose for generating clusters, 
we obtained three types of clusters: both clusters 
(the metric of Equation (2)), conditional clusters 
(the metric of Equation (3)), and predicted clusters 
(the metric of Equation (4)). We then performed a 
series of experiments to investigate the impact of 
different types of clusters on the ACM. We used 
three variants of the trigram ACM: (1) the predictive 
cluster model P(wi|wi-2wi-1Wi)? P(Wi|wi-2wi-1) where 
only predicted words are clustered, (2) the 
conditional cluster model P(wi|Wi-2Wi-1) where only 
conditional words are clustered, and (3) the IBM 
model P(wi|Wi)? P(Wi|Wi-2Wi-1) which can be treated 
as a special case of the ACM of Equation (5) by 
using the same type of cluster for both predicted and 
conditional words, and setting k = 0, and l = j. For 
each cluster trigram model, we compared their 
perplexities and CER results on Japanese Kana- 
Kanji conversion using different types of clusters. 
For each cluster type, the number of clusters were 
fixed to the same value 2^6 just for comparison.  The 
results are shown in Table 1. It turns out that the 
benefit of using different clusters in different 
positions is obvious.  For each cluster trigram 
model, the best results were achieved by using the 
?matched? clusters, e.g. the predictive cluster model 
P(wi|wi-2wi-1Wi)? P(Wi|wi-2wi-1) has the best 
performance when the cluster Wi is the predictive 
cluster PWi generated by using the metric of 
Equation (4). In particular, the IBM model achieved 
the best results when predicted and conditional 
clusters were used for predicted and conditional 
words respectively. That is, the IBM model is of the 
form P(wi|PWi)? P(PWi|CWi-2CWi-1). 
 Con Pre Both Con + Pre 
Perplexity 287.7 414.5 377.6 --- Con 
model CER (%) 4.58 11.78 12.56 --- 
Perplexity 103.4 102.4 103.3 --- Pre 
model CER (%) 3.92 3.63 3.82 --- 
Perplexity 548.2 514.4 385.2 382.2 IBM 
model CER (%) 6.61 6.49 5.82 5.36 
Table 1: Comparison of different cluster types 
with cluster-based models 
4.4 Impact of parameter optimization 
In this section, we first present our pilot experiments 
of finding the optimal parameter set of the ACM (l, j, 
k, tc, tw) described in Section 2.3. Then, we compare 
the ACM to the IBM model, showing that the 
superiority of the ACM results from its better 
structure. 
In this section, the performance of LMs was 
measured in terms of perplexity, and the size was 
measured as the total number of parameters of the 
LM: one parameter for each bigram and trigram, one 
parameter for each normalization parameter ? that 
was needed, and one parameter for each unigram.  
We first used the conditional cluster model of the 
form P(wi|CWi-2jCWi-1j). Some sample settings of 
parameters (j, tw) are shown in Figure 1. The 
performance was consistently improved by 
increasing the number of clusters j, except at the 
smallest sizes.  The word trigram model was 
consistently the best model, except at the smallest 
sizes, and even then was only marginally worse than 
the conditional cluster models.  This is not surprising 
because the conditional cluster model always 
discards information for predicting words. 
We then used the predictive cluster model of the 
form P(PWil|wi-2wi-1)?P(wi|wi-2wi-1PWil), where only 
predicted words are clustered. Some sample settings 
of the parameters (l, tc, tw) are shown in Figure 2. For 
simplicity, we assumed tc=tw, meaning that the same 
pruning threshold values were used for both 
sub-models. It turns out that predictive cluster 
models achieve the best perplexity results at about 
2^6 or 2^8 clusters. The models consistently 
outperform the baseline word trigram models.  
We finally returned to the ACM of Equation (5), 
where both conditional words and the predicted 
word are clustered (with different numbers of 
clusters), and which is referred to as the combined 
cluster model below.  In addition, we allow different 
values of the threshold for different sub-models. 
Therefore, we need to optimize the model parameter 
set l, j, k, tc, tw.  
Based on the pilot experiment results using 
conditional and predictive cluster models, we tried 
combined cluster models for values l? [4, 10], j, 
k? [8, 16]. We also allow j, k=all. Rather than plot 
all points of all models together, we show only the 
outer envelope of the points.  That is, if for a given 
model type and a given point there is some other 
point of the same type with both lower perplexity 
and smaller size than the first point, then we do not 
plot the first, worse point.  
The results are shown in Figure 3, where the 
cluster number of IBM models is 2^14 which 
achieves the best performance for IBM models in 
our experiments.  It turns out that when l? [6, 8] and 
j, k>12, combined cluster models yield the best 
results. We also found that the predictive cluster 
models give as good performance as the best 
combined ones while combined models 
outperformed very slightly only when model sizes 
are small. This is not difficult to explain. Recall that 
the predictive cluster model is a special case of the 
combined model where words are used in 
conditional positions, i.e. j=k=all. Our experiments 
show that combined models achieved good 
performance when large numbers of clusters are 
used for conditional words, i.e. large j, k>12, which 
are similar to words. 
The most interesting analysis is to look at some 
sample settings of the parameters of the combined 
cluster models in Figure 3. In Table 2, we show the 
best parameter settings at several levels of model 
size. Notice that in larger model sizes, predictive 
cluster models (i.e. j=k=all) perform the best in 
some cases. The ?prune? columns (i.e. columns 6 and 
7) indicate the Stolcke pruning parameter we used.  
First, notice that the two pruning parameters (in 
columns 6 and 7) tend to be very similar.  This is 
desirable since applying the theory of relative 
entropy pruning predicts that the two pruning 
parameters should actually have the same value.   
Next, let us compare the ACM 
P(PWil|CWi-2jCWi-1j)?P(wi|CWi-2kCWi-1kPWil) to 
traditional IBM clustering of the form 
P(Wil|Wi-2lWi-1l)?P(wi|Wil), which is equal to 
P(Wil|Wi-2lWi-1l)?P(wi|Wi-20Wi-10Wil) (assuming the   
105
110
115
120
125
130
135
140
145
150
0.0E+00 5.0E+05 1.0E+06 1.5E+06 2.0E+06 2.5E+06
size
pe
rp
lex
ity
2^12 clusters
2^14 clusters
2^16 clusters
word trigram
 
Figure 1. Comparison of conditional models 
applied with different numbers of clusters 
100
105
110
115
120
125
130
135
140
145
150
0.0E+00 5.0E+05 1.0E+06 1.5E+06 2.0E+06 2.5E+06size
pe
rp
lex
ity
2^4 clusters
2^6 clusters
2^8 clusters
2^10 clusters
word trigram
 
Figure 2. Comparison of predictive models 
applied with different numbers of clusters 
100
110
120
130
140
150
160
170
0.0E+00 5.0E+05 1.0E+06 1.5E+06 2.0E+06 2.5E+06size
pe
rp
lex
ity
ACM
IBM
word trigram
predictive model
 
Figure 3. Comparison of ACMs, predictive 
cluster model, IBM model, and word trigram 
model 
same type of cluster is used for both predictive and 
conditional words). Our results in Figure 3 show that 
the performance of IBM models is roughly an order 
of magnitude worse than that of ACMs. This is 
because in addition to the use of the symmetric 
cluster model, the traditional IBM model makes two 
more assumptions that we consider suboptimal.  
First, it assumes that j=l.  We see that the best results 
come from unequal settings of j and l.  Second, more 
importantly, IBM clustering assumes that k=0.  We 
see that not only is the optimal setting for k not 0, but 
also typically the exact opposite is the optimal: k=all 
in which case P(wi|CWi-2kCWi-1kPWil)= 
P(wi|wi-2wi-1PWil), or k=14, 16, which is very 
similar. That is, we see that words depend on the 
previous words and that an independence 
assumption is a poor one.  Of course, many of these 
word dependencies are pruned away ? but when a 
word does depend on something, the previous words 
are better predictors than the previous clusters. 
Another important finding here is that for most of 
these settings, the unpruned model is actually larger 
than a normal trigram model ? whenever k=all or 14, 
16, the unpruned model P(PWil|CWi-2jCWi-1j) ? 
P(wi|CWi-2kCWi-1kPWil) is actually larger than an 
unpruned model P(wi|wi-2wi-1). 
This analysis of the data is very interesting ? it 
implies that the gains from clustering are not from 
compression, but rather from capturing structure.  
Factoring the model into two models, in which the 
cluster is predicted first, and then the word is 
predicted given the cluster, allows the structure and 
regularities of the model to be found. This larger, 
better structured model can be pruned more 
effectively, and it achieved better performance than 
a word trigram model at the same model size. 
Model size Perplexity l j k tc tw 
2.0E+05 141.1 8 12 14 24 24 
2.5E+05 135.7 8 12 14 12 24 
5.0E+05 118.8 6 14 16 6 12 
7.5E+05 112.8 6 16 16 3 6 
1.0E+06 109.0 6 16 16 3 3 
1.3E+06 107.4 6 16 16 2 3 
1.5E+06 106.0 6 All all 2 2 
1.9E+06 104.9 6 All all 1 2 
Table 2: Sample parameter settings for the ACM 
4.5 CER results 
Before we present CER results of the Japanese 
Kana-Kanji conversion system, we briefly describe 
our method for storing the ACM in practice.  
One of the most common methods for storing 
backoff n-gram models is to store n-gram 
probabilities (and backoff weights) in a tree 
structure, which begins with a hypothetical root 
node that branches out into unigram nodes at the first 
level of the tree, and each of those unigram nodes in 
turn branches out into bigram nodes at the second 
level and so on. To save storage, n-gram 
probabilities such as P(wi|wi-1) and backoff weights 
such as ?(wi-2wi-1) are stored in a single (bigram) 
node array (Clarkson and Rosenfeld, 1997). 
Applying the above tree structure to storing the 
ACM is a bit complicated ? there are some 
representation issues. For example, consider the 
cluster sub-model P(PWil|CWi-2jCWi-1j). N-gram 
probabilities such as P(PWil|CWi-1j) and backoff 
weights such as ?(CWi-2jCWi-1j) cannot be stored in a 
single (bigram) node array, because l ? j and 
PW?CW. Therefore, we used two separate trees to 
store probabilities and backoff weights, 
respectively. As a result, we used four tree structures 
to store ACMs in practice: two for the cluster 
sub-model P(PWil|CWi-2jCWi-1j), and two for the 
word sub-model P(wi|CWi-2kCWi-1kPWil).  We found 
that the effect of the storage structure cannot be 
ignored in a real application. 
In addition, we used several techniques to 
compress model parameters (i.e. word id, n-gram 
probability, and backoff weight, etc.) and reduce the 
storage space of models significantly. For example, 
rather than store 4-byte floating point values for all 
n-gram probabilities and backoff weights, the values 
are quantized to a small number of quantization 
levels. Quantization is performed separately on each 
of the n-gram probability and backoff weight lists, 
and separate quantization level look-up tables are 
generated for each of these sets of parameters.  We 
used 8-bit quantization, which shows no 
performance decline in our experiments. 
Our goal is to achieve the best tradeoff between 
performance and model size. Therefore, we would 
like to compare the ACM with the word trigram 
model at the same model size. Unfortunately, the 
ACM contains four sub-models and this makes it 
difficult to be pruned to a specific size. Thus for 
comparison, we always choose the ACM with 
smaller size than its competing word trigram model 
to guarantee that our evaluation is under-estimated. 
Experiments show that the ACMs achieve 
statistically significant improvements over word 
trigram models at even smaller model sizes (p-value 
=8.0E-9). Some results are shown in Table 3.  
Word trigram model ACM 
Size 
(MB) 
CER Size 
(MB) 
CER  CER 
Reduction 
1.8 4.56% 1.7 4.25% 6.8% 
5.8 4.08% 4.5 3.83% 6.1% 
11.7 4.04% 10.7 3.73% 7.7% 
23.5 4.00% 21.7 3.63% 9.3% 
42.4 3.98% 40.4 3.63% 8.8% 
Table 3:  CER results of ACMs and word 
trigram models at different model sizes 
Now we discuss why the ACM is superior to 
simple word trigrams.  In addition to the better 
structure as shown in Section 3.3, we assume here 
that the benefit of our model also comes from its 
better smoothing. Consider a probability such as 
P(Tuesday| party on). If we put the word ?Tuesday? 
into the cluster WEEKDAY, we decompose the 
probability 
When each word belongs to one class, simple math 
shows that this decomposition is a strict equality. 
However, when smoothing is taken into 
consideration, using the clustered probability will be 
more accurate than using the non-clustered 
probability. For instance, even if we have never seen 
an example of ?party on Tuesday?, perhaps we have 
seen examples of other phrases, such as ?party on 
Wednesday?; thus, the probability P(WEEKDAY | 
party on) will be relatively high. Furthermore, 
although we may never have seen an example of 
?party on WEEKDAY Tuesday?, after we backoff or 
interpolate with a lower order model, we may able to 
accurately estimate P(Tuesday | on WEEKDAY). 
Thus, our smoothed clustered estimate may be a 
good one. 
Our assumption can be tested empirically by 
following experiments.  We first constructed several 
test sets with different backoff rates4. The backoff 
rate of a test set, when presented to a trigram model, 
is defined as the number of words whose trigram 
probabilities are estimated by backoff bigram 
probabilities divided by the number of words in the 
test set.  Then for each test set, we obtained a pair of 
CER results using the ACM and the word trigram 
model respectively.  As shown in Figure 4, in both 
cases, CER increases as the backoff rate increases 
from 28% to 40%. But the curve of the word trigram 
model has a steeper upward trend.  The difference of 
the upward trends of the two curves can be shown 
more clearly by plotting the CER difference between 
them, as shown in Figure 5.  The results indicate that 
because of its better smoothing, when the backoff 
rate increases, the CER using the ACM does not 
increase as fast as that using the word trigram model.  
Therefore, we are reasonably confident that some 
portion of the benefit of the ACM comes from its 
better smoothing. 
2.1
2.3
2.5
2.7
2.9
3.1
3.3
3.5
3.7
3.9
0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 0.37 0.38 0.39 0.4 0.41backoff rate
er
ro
r r
ate
word trigram model
ACM
 
Figure 4: CER vs. backoff rate. 
                                                     
4  The backoff rates are estimated using the baseline 
trigram model, so the choice could be biased against the 
word trigram model. 
P(Tuesday | party on) = P(WEEKDAY | party on)? 
P(Tuesday | party on WEEKDAY). 
0.25
0.27
0.29
0.31
0.33
0.35
0.37
0.39
0.41
0.28 0.3 0.32 0.34 0.36 0.38 0.4 0.42
backoff rate
er
ro
r r
ate
 di
ffe
re
nc
e
 
Figure 5: CER difference vs. backoff rate. 
5 Conclusion 
There are three main contributions of this paper. 
First, after presenting a formal definition of the 
ACM, we described in detail the methodology of 
constructing the ACM effectively. We showed 
empirically that both the asymmetric clustering and 
the parameter optimization (i.e. optimal cluster 
numbers) have positive impacts on the performance 
of the resulting ACM.  The finding demonstrates 
partially the effectiveness of our research focus: 
techniques for using clusters (i.e. the ACM) rather 
than techniques for finding clusters (i.e. clustering 
algorithms).  Second, we explored the actual 
representation of the ACM and evaluate it on a 
realistic application ? Japanese Kana-Kanji 
conversion.  Results show approximately 6-10% 
CER reduction of the ACMs in comparison with the 
word trigram models, even when the ACMs are 
slightly smaller.  Third, the reasons underlying the 
superiority of the ACM are analyzed. For instance, 
our analysis suggests the benefit of the ACM comes 
partially from its better structure and its better 
smoothing. 
All cluster models discussed in this paper are 
based on hard clustering, meaning that each word 
belongs to only one cluster. One area we have not 
explored is the use of soft clustering, where a word w 
can be assigned to multiple clusters W with a 
probability P(W|w) [Pereira et al, 1993]. Saul and 
Pereira [1997] demonstrated the utility of soft 
clustering and concluded that any method that 
assigns each word to a single cluster would lose 
information. It is an interesting question whether our 
techniques for hard clustering can be extended to 
soft clustering.  On the other hand, soft clustering 
models tend to be larger than hard clustering models 
because a given word can belong to multiple 
clusters, and thus a training instance P(wi|wi-2wi-1) 
can lead to multiple counts instead of just 1.  
References 
Bai, S., Li, H., Lin, Z., and Yuan, B. (1998). Building 
class-based language models with contextual statistics. In 
ICASSP-98, pp. 173-176. 
Bellegarda, J. R., Butzberger, J. W., Chow, Y. L., Coccaro, N. 
B., and Naik, D. (1996). A novel word clustering algorithm 
based on latent semantic analysis. In ICASSP-96.  
Brown, P. F., Cocke, J., DellaPietra, S. A., DellaPietra, V. J., 
Jelinek, F., Lafferty, J. D., Mercer, R. L., and Roossin, P. S. 
(1990). A statistical approach to machine translation. 
Computational Linguistics, 16(2), pp. 79-85. 
Brown, P. F., DellaPietra V. J., deSouza, P. V., Lai, J. C., and 
Mercer, R. L. (1992). Class-based n-gram models of natural 
language. Computational Linguistics, 18(4), pp. 467-479. 
Clarkson, P. R., and Rosenfeld, R. (1997). Statistical language 
modeling using the CMU-Cambridge toolkit. In Eurospeech 
1997, Rhodes, Greece. 
Gao, J. Goodman, J. and Miao, J. (2001). The use of clustering 
techniques for language model ? application to Asian 
language. Computational Linguistics and Chinese Language 
Processing. Vol. 6, No. 1, pp 27-60. 
Gao, J., Goodman, J., Li, M., and Lee, K. F. (2002). Toward a 
unified approach to statistical language modeling for Chinese. 
ACM Transactions on Asian Language Information 
Processing. Vol. 1, No. 1, pp 3-33. 
Goodman, J. (2001). A bit of progress in language modeling.  In 
Computer Speech and Language, October 2001, pp 403-434. 
Goodman, J., and Gao, J. (2000). Language model size 
reduction by predictive clustering. ICSLP-2000, Beijing. 
Jelinek, F. (1990). Self-organized language modeling for speech 
recognition. In Readings in Speech Recognition, A. Waibel 
and K. F. Lee, eds., Morgan-Kaufmann, San Mateo, CA, pp. 
450-506. 
Katz, S. M. (1987). Estimation of probabilities from sparse data 
for the language model component of a speech recognizer. 
IEEE Transactions on Acoustics, Speech and Signal 
Processing, ASSP-35(3):400-401, March. 
Kneser, R. and Ney, H. (1993).  Improved clustering techniques 
for class-based statistical language modeling. In Eurospeech, 
Vol. 2, pp. 973-976, Berlin, Germany. 
Ney, H., Essen, U., and Kneser, R. (1994). On structuring 
probabilistic dependences in stochastic language modeling. 
Computer, Speech, and Language, 8:1-38. 
Pereira, F., Tishby, N., and Lee L. (1993). Distributional 
clustering of English words. In Proceedings of the 31st Annual 
Meeting of the ACL. 
Rosenfeld, R. (2000). Two decades of statistical language 
modeling: where do we go from here. In Proceeding of the 
IEEE, 88:1270-1278, August. 
Saul, L., and Pereira, F.C.N. (1997). Aggregate and mixed-order 
Markov models for statistical language processing. In 
EMNLP-1997. 
Stolcke, A. (1998). Entropy-based Pruning of Backoff 
Language Models. Proc. DARPA News Transcription and 
Understanding Workshop, 1998, pp. 270-274. 
Ueberla, J. P. (1996). An extended clustering algorithm for 
statistical language models. IEEE Transactions on Speech 
and Audio Processing, 4(4): 313-316. 
Yamamoto, H., Isogai, S., and Sagisaka, Y. (2001). Multi-Class 
Composite N-gram Language Model for Spoken Language 
Processing Using Multiple Word Clusters. 39th Annual 
meetings of the Association for Computational Linguistics 
(ACL?01), Toulouse, 6-11 July 2001. 
Yamamoto, H., and Sagisaka, Y. (1999). Multi-class Composite 
N-gram based on Connection Direction, In Proceedings of the 
IEEE International Conference on Acoustics, Speech and 
Signal Processing, May, Phoenix, Arizona. 
Word Translation Disambiguation Using Bilingual Bootstrapping 
 
Cong Li 
Microsoft Research Asia  
5F Sigma Center, No.49 Zhichun Road, Haidian 
Beijing, China, 100080 
 i-congl@microsoft.com 
Hang Li 
Microsoft Research Asia 
5F Sigma Center, No.49 Zhichun Road, Haidian 
Beijing, China, 100080 
hangli@microsoft.com 
 
Abstract 
This paper proposes a new method for 
word translation disambiguation using 
a machine learning technique called 
?Bilingual Bootstrapping?. Bilingual 
Bootstrapping makes use of  in 
learning a small number of classified 
data and a large number of unclassified 
data in the source and the target 
languages in translation. It constructs 
classifiers in the two languages in 
parallel and repeatedly boosts the 
performances of the classifiers by 
further classifying data in each of the 
two languages and by exchanging 
between the two languages 
information regarding the classified 
data. Experimental results indicate that 
word translation disambiguation based 
on Bilingual Bootstrapping 
consistently and significantly 
outperforms the existing methods 
based on ?Monolingual 
Bootstrapping?. 
1 Introduction 
We address here the problem of word translation 
disambiguation. For instance, we are concerned 
with an ambiguous word in English (e.g., ?plant?), 
which has multiple translations in Chinese (e.g., 
? (gongchang)? and ? (zhiwu)?). Our 
goal is to determine the correct Chinese 
translation of the ambiguous English word, given 
an English sentence which contains the word. 
Word translation disambiguation is actually a 
special case of word sense disambiguation (in the 
example above, ?gongchang? corresponds to the 
sense of ?factory? and ?zhiwu? corresponds to the 
sense of ?vegetation?).1 
 
Yarowsky (1995) proposes a method for word 
sense (translation) disambiguation that is based 
on a bootstrapping technique, which we refer to 
here as ?Monolingual Bootstrapping (MB)?.  
 
In this paper, we propose a new method for word 
translation disambiguation using a bootstrapping 
technique we have developed. We refer to the 
technique as ?Bilingual Bootstrapping (BB)?. 
 
In order to evaluate the performance of BB, we 
conducted some experiments on word translation 
disambiguation using the BB technique and the 
MB technique. All of the results indicate that BB 
consistently and significantly outperforms MB. 
2 Related Work 
The problem of word translation disambiguation 
(in general, word sense disambiguation) can be 
viewed as that of classification and can be 
addressed by employing a supervised learning 
method. In such a learning method, for instance, 
an English sentence containing an ambiguous 
English word corresponds to an example, and the 
Chinese translation of the word under the context 
corresponds to a classification decision (a label). 
 
Many methods for word sense disambiguation 
using a supervised learning technique have been 
proposed. They include those using Na?ve Bayes 
(Gale et al 1992a), Decision List (Yarowsky 
1994), Nearest Neighbor (Ng and Lee 1996), 
Transformation Based Learning (Mangu and 
Brill 1997), Neural Network (Towell and 
                                                     
1
 In this paper, we take English-Chinese translation as 
example; it is a relatively easy process, however, to 
extend the discussions to translations between other 
language pairs. 
                Computational Linguistics (ACL), Philadelphia, July 2002, pp. 343-351.
                         Proceedings of the 40th Annual Meeting of the Association for
Voorhess 1998), Winnow (Golding and Roth 
1999), Boosting (Escudero et al 2000), and 
Na?ve Bayesian Ensemble (Pedersen 2000). 
Among these methods, the one using Na?ve 
Bayesian Ensemble (i.e., an ensemble of Na?ve 
Bayesian Classifiers) is reported to perform the 
best for word sense disambiguation with respect 
to a benchmark data set (Pedersen 2000). 
 
The assumption behind the proposed methods is 
that it is nearly always possible to determine the 
translation of a word by referring to its context, 
and thus all of the methods actually manage to 
build a classifier (i.e., a classification program) 
using features representing context information 
(e.g., co-occurring words). 
 
Since preparing supervised learning data is 
expensive (in many cases, manually labeling data 
is required), it is desirable to develop a 
bootstrapping method that starts learning with a 
small number of classified data but is still able to 
achieve high performance under the help of a 
large number of unclassified data which is not 
expensive anyway. 
 
Yarowsky (1995) proposes a method for word 
sense disambiguation, which is based on 
Monolingual Bootstrapping. When applied to our 
current task, his method starts learning with a 
small number of English sentences which contain 
an ambiguous English word and which are 
respectively assigned with the correct Chinese 
translations of the word. It then uses the 
classified sentences as training data to learn a 
classifier (e.g., a decision list) and uses the 
constructed classifier to classify some 
unclassified sentences containing the ambiguous 
word as additional training data. It also adopts the 
heuristics of ?one sense per discourse? (Gale et al 
1992b) to further classify unclassified sentences. 
By repeating the above processes, it can create an 
accurate classifier for word translation 
disambiguation. 
 
For other related work, see, for example, (Brown 
et al 1991; Dagan and Itai 1994; Pedersen and 
Bruce 1997; Schutze 1998; Kikui 1999; 
Mihalcea and Moldovan 1999). 
3 Bilingual Bootstrapping 
3.1 Overview 
Instead of using Monolingual Bootstrapping, we 
propose a new method for word translation 
disambiguation using Bilingual Bootstrapping. 
In translation from English to Chinese, for 
instance, BB makes use of not only unclassified 
data in English, but also unclassified data in 
Chinese. It also uses a small number of classified 
data in English and, optionally, a small number 
of classified data in Chinese. The data in English 
and in Chinese are supposed to be not in parallel 
but from the same domain. 
 
BB constructs classifiers for English to Chinese 
translation disambiguation by repeating the 
following two steps: (1) constructing classifiers 
for each of the languages on the basis of the 
classified data in both languages, (2) using the 
constructed classifiers in each of the languages to 
classify some unclassified data and adding them 
to the classified training data set of the language. 
The reason that we can use classified data in both 
languages at step (1) is that words in one 
language generally have translations in the other 
and we can find their translation relationship by 
using a dictionary. 
3.2 Algorithm 
Let E denote a set of words in English, C a set of 
words in Chinese, and T a set of links in a 
translation dictionary as shown in Figure 1. (Any 
two linked words can be translation of each other.) 
Mathematically, T is defined as a relation 
between E and C , i.e., CET ?? .  
 
Let ? stand for a random variable on E, ? a 
random variable on C. Also let e stand for a 
random variable on E, c a random variable on C, 
and t a random variable on T. While ? and 
?  represent words to be translated, e and c 
represent context words. 
 
For an English word ?, 
}),,(|{ TtttT ??== ???  represents the links 
M
M
M
M
M
M
   
Figure 1: Example of translation dictionary 
from it, and }),(|{ TC ???= ????  represents the 
Chinese words which are linked to it. For a 
Chinese word ?, let }),,(|{ TtttT ??== ???  and 
}),(|{ TE ???= ???? . We can define eC  and cE  
similarly. 
 
Let e denote a sequence of words (e.g., a sentence 
or a text) in English  
),,2,1(   },,,,{ 21 miEeeee im LL =?=  e . 
Let c denote a sequence of words in Chinese  
),,2,1(   },,,,{ 21 niCcccc in LL =?=  c . 
We view e and c as examples representing 
context information for translation 
disambiguation. 
 
For an English word ?, we define a binary 
classifier for resolving each of its translation 
ambiguities in ?T  in a general form as: 
},{ ),|(   &     ),|( tTttPTttP ??? ???? ee  
where e denotes an example in English. Similarly, 
for a Chinese word ?, we define a classifier as: 
},{  ),|(   &     ),|( tTttPTttP ??? ???? cc  
where c denotes an example in Chinese. 
 
Let ?L  denote a set of classified examples in 
English, each representing one context of ? 
),,,2,1(         
},),(,,),(,),{( 2211
kiTt
tttL
i
kk
L
L
=?
=
?
???? eee
 
and ?U  a set of unclassified examples in English, 
each representing one context of ?  
}.)(,,)(,){( 21 ???? lU eee L=  
Similarly, we denote the sets of classified and 
unclassified examples with respect to ? in 
Chinese as ?L  and ?U  respectively. 
Furthermore, we have 
.,,, ????????
UUUULLLL
C
C
E
E
C
C
E
E
????
==== UUUU  
 
We perform Bilingual Bootstrapping as 
described in Figure 2. Hereafter, we will only 
explain the process for English (left-hand side); 
the process for Chinese (right-hand side) can be 
conducted similarly.  
3.3 Na?ve Bayesian Classifier 
Input :  CCEE ULULTCE ,,,,,, ,  Parameter : ?,b  
Repeat  in parallel the following processes for English (left) and Chinese (right), until unable to continue : 
1. for each ( E?? ) { for each ( C?? ) { 
 for each (
?Tt ? ) { 
use ?L  and )( ?? ? CL ? to create classifier: 
?? TttP ?   ),|( e   &  };{   ),|( tTttP ?? ?? e }} 
for each ( ?Tt ? ) { 
use ?L  and )( ?? ? EL ? to create classifier: 
?? TttP ?   ),|( c   &  };{   ),|( tTttP ?? ?? c }} 
2. for each ( E?? ) { 
{};{}; ?? NLNU  
 for each (
?Tt ? ) { }{};{}; ?? tt QS  
 for each ( C?? ) { 
   {};{}; ?? NLNU  
for each ( ?Tt ? ) { }{};{}; ?? tt QS  
 
 for each (
?U?e ){ 
calculate 
)|(
)|(
max)(*
e
e
e
tP
tP
Tt ?
?
?
?
?
= ; 
let 
)|(
)|(
maxarg)(*
e
e
e
tP
tP
t
Tt ?
?
??
= ; 
if ( tt => )(  &  )( ** ee ?? ) 
put e into tS ;} 
for each ( ?U?c ){ 
calculate 
)|(
)|(
max)(*
c
c
c
tP
tP
Tt
?
?
?
?
?
= ; 
let 
)|(
)|(
maxarg)(*
c
c
c
tP
tP
t
Tt ?
?
??
=
; 
if ( tt => )(  &  )( ** cc ?? ) 
put c into tS ;} 
 
 for each (
?Tt ? ){ 
sort tS?e in descending order of )(* e? and  
put the top b elements into tQ ;} 
for each ( ?Tt ? ){ 
sort tS?c in descending order of )(* c? and  
put the top b elements into tQ ;} 
 
 for each ( t
t
QU?e ){ 
put e into NU and put ))(,( ee ?t  into NL;} 
for each (
t
t
QU?c ){ 
put c into NU and put ))(,( cc ?t  into NL;} 
 NLLL U?? ? ; NUUU ?? ?? ;} NLLL U?? ? ; NUUU ?? ?? ;} 
Output: classifiers in English and Chinese 
Figure 2: Bilingual Bootstrapping 
While we can in principle employ any kind of 
classifier in BB, we use here a Na?ve Bayesian 
Classifier. At step 1 in BB, we construct the 
classifier as described in Figure 3. At step 2, for 
each example e, we calculate with the Na?ve 
Bayesian Classifier: 
.)|()(
)|()(
max)|(
)|(
max)(*
tPtP
tPtP
tP
tP
TtTt e
e
e
e
e
??
??
?
?
??
?
??
==
 
The second equation is based on Bayes? rule. 
 
In the calculation, we assume that the context 
words in e (i.e., 
meee ,,, 21 L ) are independently 
generated from )|( teP?  and thus we have  
 .)|()|(
1
?
=
=
m
i
i tePtP ?? e  
We can calculate )|( tP e?  similarly.  
 
For )|( teP? , we calculate it at step 1 by linearly 
combining )|()( teP E?  estimated from English 
and )|()( teP C?  estimated from Chinese: 
),( )|(                
)|()1()|(
)()(
)(
ePteP
tePteP
UC
E
??
??
?
??
++
??=
 (1) 
where 10 ?? ? , 10 ?? ? , 1?+ ?? , and 
)()( eP U  is a uniform distribution over E , which 
is used for avoiding zero probability. In this way, 
we estimate )|( teP?  using information from not 
only English but also Chinese. 
 
For )|()( teP E? , we estimate it with MLE 
(Maximum Likelihood Estimation) using ?L  as 
data. For )|()( teP C? , we estimate it as is 
described in Section 3.4. 
3.4 EM Algorithm 
For the sake of readability, we rewrite )|()( teP C?  
as )|( teP . We define a finite mixture model of 
the form ?
?
=
Ee
tePtecPtcP )|(),|()|(  and for a 
specific ?  we assume that the data in 
??
????
? ChiTt
tttL
i
hh
??=?
=
   ),,,1(        
},),(,,),(,),{( 2211
L
L ccc
 
are independently generated on the basis of the 
model. We can, therefore, employ the 
Expectation and Maximization Algorithm (EM 
Algorithm) (Dempster et al 1977) to estimate the 
parameters of the model including )|( teP . We 
also use the relation T in the estimation. 
 
Initially, we set  
??
??
?
?
?
=
e
e
e
Cc
Cc
CtecP
 if          , 0  
 if     , ||
1
 ),|( , 
.   , ||
1)|( Ee
E
teP ?=
 
We next estimate the parameters by iteratively 
updating them ass described in Figure 4 until 
they converge. Here ),( tcf  stands for the 
frequency of c related to t. The context 
information in Chinese is then ?translated? into 
that in English through the links in T. 
4  Comparison between BB and MB 
We note that Monolingual Bootstrapping is a 
special case of Bilingual Bootstrapping (consider 
the situation in which ?  equals 0 in formula (1)).  
Moreover, it seems safe to say that BB can 
always perform better than MB. 
 
The many-to-many relationship between the 
words in the two languages stands out as key to 
the higher performance of BB. 
 
Suppose that the classifier with respect to ?plant? 
has two decisions (denoted as A and B in Figure 
5). Further suppose that the classifiers with 
estimate )|()( teP E?  with MLE using  ?L   as data; 
estimate )|()( teP C?  with EM Algorithm using  ?L   
for each ?? C?  as data; 
calculate )|( teP?  as a linear combination of 
)|()( teP E?  and )|()( teP C? ; 
estimate )(tP?  with MLE using ?L ; 
calculate )|( teP?  and )(tP? similarly. 
Figure 3: Creating Na?ve Bayesian Classifier 
E-step:      ?
?
?
Ee
tePtecP
tePtecP
tceP
)|(),|(
)|(),|(),|(  
M-step:      ?
?
?
Cc
tcePtcf
tcePtcf
tecP
),|(),(
),|(),(),|(  
?
?
?
??
Cc
Cc
tcf
tcePtcf
teP
),(
),|(),(
)|(  
Figure 4: EM Algorithm 
respect to ?gongchang? and ?zhiwu? in Chinese 
have two decisions respectively, (C and D) (E 
and F). A and D are equivalent to each other (i.e., 
they represent the same sense), and so are B and 
E. 
 
Assume that examples are classified after several 
iterations in BB as depicted in Figure 5. Here, 
circles denote the examples that are correctly 
classified and crosses denote the examples that 
are incorrectly classified. 
 
Since A and D are equivalent to each other, we 
can ?translate? the examples with D and use them 
to boost the performance of classification to A. 
This is because the misclassified examples 
(crosses) with D are those mistakenly classified 
from C and they will not have much negative 
effect on classification to A, even though the 
translation from Chinese into English can 
introduce some noises. Similar explanations can 
be stated to other classification decisions. 
 
In contrast, MB only uses the examples in A and 
B to construct a classifier, and when the number 
of misclassified examples increases (this is 
inevitable in bootstrapping), its performance will 
stop improving.  
5 Word Translation Disambiguation 
5.1 Using Bilingual Bootstrapping 
While it is possible to straightforwardly apply the 
algorithm of BB described in Section 3 to word 
translation disambiguation, we use here a variant 
of it for a better adaptation to the task and for a 
fairer comparison with existing technologies. 
 
The variant of BB has four modifications. 
 
(1)  It actually employs an ensemble of the Na?ve 
Bayesian Classifiers (NBC), because an 
ensemble of NBCs generally performs better 
than a single NBC (Pedersen 2000).  In an 
ensemble, it creates different NBCs using as data  
the words within different window sizes 
surrounding the word to be disambiguated (e.g., 
?plant? or ?zhiwu?) and further constructs a new 
classifier by linearly combining the NBCs. 
 
(2) It employs the heuristics of ?one sense per 
discourse? (cf., Yarowsky 1995) after using an 
ensemble of NBCs. 
 
(3) It uses only classified data in English at the 
beginning. 
 
(4) It individually resolves ambiguities on 
selected English words such as ?plant?, ?interest?. 
As a result, in the case of ?plant?; for example, the 
classifiers with respect to ?gongchang? and 
?zhiwu? only make classification decisions to D 
and E but not C and F (in Figure 5). It calculates 
)(* c?  as )|()(* tP cc =? and sets 0=?  at the 
right-hand side of step 2. 
5.2 Using Monolingual Bootstrapping 
We consider here two implementations of MB 
for word translation disambiguation.  
 
In the first implementation, in addition to the 
basic algorithm of MB, we also use (1) an 
ensemble of Na?ve Bayesian Classifiers, (2) the 
heuristics of ?one sense per discourse?, and (3) a 
small number of classified data in English at the 
beginning.  We will denote this implementation 
as MB-B hereafter.  
 
The second implementation is different from the 
first one only in (1). That is, it employs as a 
classifier a decision list instead of an ensemble of 
NBCs. This implementation is exactly the one 
proposed in (Yarowsky 1995), and we will 
denote it as MB-D hereafter. 
 
MB-B and MB-D can be viewed as the  
state-of-the-art methods for word translation 
disambiguation using bootstrapping. 
6 Experimental Results 
M
M
M
M
o
o
oo
o o
o o
o o
o
o
o o
o
o o
o
oo
o
o
o
o
?
?
?
?
?
?
? ?
? ?
?
?
 
Figure 5: Example of BB 
We conducted two experiments on 
English-Chinese translation disambiguation. 
6.1 Experiment 1: WSD Benchmark Data 
We first applied BB, MB-B, and MB-D to 
translation of the English words ?line? and 
?interest? using a benchmark data 2 . The data 
mainly consists of articles in the Wall Street 
Journal and it is designed for conducting Word 
                                                     
2
 http://www.d.umn.edu/~tpederse/data.html. 
Sense Disambiguation (WSD) on the two words 
(e.g., Pedersen 2000). 
 
We adopted from the HIT dictionary 3  the 
Chinese translations of the two English words, as 
listed in Table 1. One sense of the words 
corresponds to one group of translations. 
 
We then used the benchmark data as our test data. 
(For the word ?interest?, we only used its four 
major senses, because the remaining two minor 
senses occur in only 3.3% of the data) 
 
                                                     
3
 The dictionary is created by Harbin Institute of 
Technology. 
Table 1: Data descriptions in Experiment 1 
Words Chinese translations Corresponding English senses Seed words  
 
readiness to give attention show 
 
money paid for the use of money rate 
,  a share in company or business hold 
interest 
 
advantage, advancement or favor conflict 
	
, 	 a thin flexible object cut 
,  written or spoken text write 
 
telephone connection telephone 
,  formation of people or things wait 
,  an artificial division between 
line 
,  product product 
Table 2: Data sizes in Experiment 1 
Unclassified sentences Words English Chinese 
Test 
sentences 
interest 1927 8811 2291 
line 3666 5398 4148 
Table 3: Accuracies in Experiment 1 
Words Major  (%) 
MB-D 
(%) 
MB-B 
(%) 
BB  
(%) 
interest 54.6 54.7 69.3 75.5 
line 53.5 55.6 54.1 62.7 








  	 
 












 
Figure 6: Learning curves with ?interest? 









   
	













 
Figure 7: Learning curves with ?line? 







  	  
?












  
Figure 8: Accuracies of BB with different ?  
Table 4: Accuracies of supervised methods 
 interest (%) line (%) 
Ensembles of NBC 89 88 
Na?ve Bayes 74 72 
Decision Tree 78 - 
Neural Network - 76 
Nearest Neighbor 87 - 
As classified data in English, we defined a ?seed 
word? for each group of translations based on our 
intuition (cf., Table 1). Each of the seed words 
was then used as a classified ?sentence?. This way 
of creating classified data is similar to that in 
(Yarowsky, 1995). As unclassified data in 
English, we collected sentences in news articles 
from a web site (www.news.com), and as 
unclassified data in Chinese, we collected 
sentences in news articles from another web site 
(news.cn.tom.com). We observed that the 
distribution of translations in the unclassified 
data was balanced. 
 
Table 2 shows the sizes of the data. Note that 
there are in general more unclassified sentences 
in Chinese than in English because an English 
word usually has several Chinese words as 
translations (cf., Figure 5). 
 
As a translation dictionary, we used the HIT 
dictionary, which contains about 76000 Chinese 
words, 60000 English words, and 118000 links. 
 
We then used the data to conduct translation 
disambiguation with BB, MB-B, and MB-D, as 
described in Section 5. 
 
For both BB and MB-B, we used an ensemble of 
five Na?ve Bayesian Classifiers with the window 
sizes being ?1, ?3, ?5, ?7, ?9 words. For both 
BB and MB-B, we set the parameters of ?, b, and 
?  to 0.2, 15, and 1.5 respectively. The  
parameters were tuned based on our preliminary 
experimental results on MB-B, they were not 
tuned, however, for BB. For the BB specific 
parameter ?, we set it to 0.4, which meant that we 
treated the information from English and that 
from Chinese equally. 
 
Table 3 shows the translation disambiguation 
accuracies of the three methods as well as that of 
a baseline method in which we always choose the 
major translation. Figures 6 and 7 show the 
learning curves of MB-D, MB-B, and BB. Figure 
8 shows the accuracies of BB with different 
? values. 
 
From the results, we see that BB consistently and 
significantly outperforms both MB-D and MB-B. 
The results from the sign test are statistically 
significant (p-value < 0.001). 
 
Table 4 shows the results achieved by some 
existing supervised learning methods with 
respect to the benchmark data (cf., Pedersen 
2000). Although BB is a method nearly 
equivalent to one based on unsupervised learning, 
it still performs favorably well when compared 
with the supervised methods (note that since the 
experimental settings are different, the results 
cannot be directly compared). 
6.2 Experiment 2: Yarowsky?s Words 
We also conducted translation on seven of the 
twelve English words studied in (Yarowsky, 
1995). Table 5 shows the list of the words. 
 
For each of the words, we extracted about 200 
sentences containing the word from the Encarta4 
English corpus and labeled those sentences with 
Chinese translations ourselves. We used the 
labeled sentences as test data and the remaining 
sentences as unclassified data in English. We 
also used the sentences in the Great 
Encyclopedia 5  Chinese corpus as unclassified 
data in Chinese. We defined, for each translation, 
                                                     
4
 http://encarta.msn.com/default.asp 
5
 http://www.whlib.ac.cn/sjk/bkqs.htm 
Table 5: Data descriptions and data sizes in Experiment 2 
Unclassified sentences Words Chinese translations 
English Chinese 
Seed words Test 
sentences 
bass 
,  / Uncertainty Reduction in Collaborative Bootstrapping:  
Measure and Algorithm 
Yunbo Cao 
Microsoft Research Asia  
5F Sigma Center,  
No.49 Zhichun Road, Haidian 
Beijing, China, 100080 
 i-yucao@microsoft.com 
Hang Li 
Microsoft Research Asia 
5F Sigma Center,  
No.49 Zhichun Road, Haidian 
Beijing, China, 100080 
hangli@microsoft.com 
Li Lian 
Computer Science Department 
Fudan University 
No. 220 Handan Road 
Shanghai, China, 200433 
leelix@yahoo.com 
 
Abstract 
This paper proposes the use of uncertainty 
reduction in machine learning methods 
such as co-training and bilingual boot-
strapping, which are referred to, in a gen-
eral term, as ?collaborative bootstrapping?. 
The paper indicates that uncertainty re-
duction is an important factor for enhanc-
ing the performance of collaborative 
bootstrapping. It proposes a new measure 
for representing the degree of uncertainty 
correlation of the two classifiers in col-
laborative bootstrapping and uses the 
measure in analysis of collaborative boot-
strapping. Furthermore, it proposes a new 
algorithm of collaborative bootstrapping 
on the basis of uncertainty reduction. Ex-
perimental results have verified the cor-
rectness of the analysis and have 
demonstrated the significance of the new 
algorithm. 
1 Introduction 
We consider here the problem of collaborative 
bootstrapping. It includes co-training (Blum and 
Mitchell, 1998; Collins and Singer, 1998; Nigam 
and Ghani, 2000) and bilingual bootstrapping (Li 
and Li, 2002).  
Collaborative bootstrapping begins with a small 
number of labelled data and a large number of 
unlabelled data. It trains two (types of) classifiers 
from the labelled data, uses the two classifiers to 
label some unlabelled data, trains again two new 
classifiers from all the labelled data, and repeats 
the above process. During the process, the two 
classifiers help each other by exchanging the la-
belled data. In co-training, the two classifiers have 
different feature structures, and in bilingual boot-
strapping, the two classifiers have different class 
structures. 
Dasgupta et al(2001) and Abney (2002) con-
ducted theoretical analyses on the performance 
(generalization error) of co-training. Their analyses, 
however, cannot be directly used in studies of co-
training in (Nigam & Ghani, 2000) and bilingual 
bootstrapping. 
In this paper, we propose the use of uncertainty 
reduction in the study of collaborative bootstrap-
ping (both co-training and bilingual bootstrapping). 
We point out that uncertainty reduction is an im-
portant factor for enhancing the performances of 
the classifiers in collaborative bootstrapping. Here, 
the uncertainty of a classifier is defined as the por-
tion of instances on which it cannot make classifi-
cation decisions. Exchanging labelled data in 
bootstrapping can help reduce the uncertainties of 
classifiers. 
Uncertainty reduction was previously used in 
active learning. We think that it is this paper which 
for the first time uses it for bootstrapping.  
We propose a new measure for representing the 
uncertainty correlation between the two classifiers 
in collaborative bootstrapping and refer to it as 
?uncertainty correlation coefficient? (UCC). We 
use UCC for analysis of collaborative bootstrap-
ping. We also propose a new algorithm to improve 
the performance of existing collaborative boot-
strapping algorithms. In the algorithm, one classi-
fier always asks the other classifier to label the 
most uncertain instances for it. 
Experimental results indicate that our theoreti-
cal analysis is correct. Experimental results also 
indicate that our new algorithm outperforms exist-
ing algorithms. 
2 Related Work 
2.1 Co-Training and Bilingual Bootstrapping 
Co-training, proposed by Blum and Mitchell 
(1998), conducts two bootstrapping processes in 
parallel, and makes them collaborate with each 
other. More specifically, it repeatedly trains two 
classifiers from the labelled data, labels some 
unlabelled data with the two classifiers, and ex-
changes the newly labelled data between the two 
classifiers. Blum and Mitchell assume that the two 
classifiers are based on two subsets of the entire 
feature set and the two subsets are conditionally 
independent with one another given a class. This 
assumption is called ?view independence?. In their 
algorithm of co-training, one classifier always asks 
the other classifier to label the most certain in-
stances for the collaborator. The word sense dis-
ambiguation method proposed in Yarowsky (1995) 
can also be viewed as a kind of co-training. 
Since the assumption of view independence 
cannot always be met in practice, Collins and 
Singer (1998) proposed a co-training algorithm 
based on ?agreement? between the classifiers. 
As for theoretical analysis, Dasgupta et al 
(2001) gave a bound on the generalization error of 
co-training within the framework of PAC learning. 
The generalization error is a function of ?dis-
agreement? between the two classifiers. Dasgupta 
et als result is based on the view independence 
assumption, which is strict in practice. 
Abney (2002) refined Dasgupta et als result by 
relaxing the view independence assumption with a 
new constraint. He also proposed a new co-training 
algorithm on the basis of the constraint. 
Nigam and Ghani (2000) empirically demon-
strated that bootstrapping with a random feature 
split (i.e. co-training), even violating the view in-
dependence assumption, can still work better than 
bootstrapping without a feature split (i.e., boot-
strapping with a single classifier). 
For other work on co-training, see (Muslea et al
200; Pierce and Cardie 2001). 
Li and Li (2002) proposed an algorithm for 
word sense disambiguation in translation between 
two languages, which they called ?bilingual boot-
strapping?. Instead of making an assumption on the 
features, bilingual bootstrapping makes an assump-
tion on the classes. Specifically, it assumes that the 
classes of the classifiers in bootstrapping do not 
overlap. Thus, bilingual bootstrapping is different 
from co-training. 
Because the notion of agreement is not involved 
in bootstrapping in (Nigam & Ghani 2000) and 
bilingual bootstrapping, Dasgupta et aland 
Abney?s analyses cannot be directly used on them. 
2.2 Active Learning 
Active leaning is a learning paradigm. Instead of 
passively using all the given labelled instances for 
training as in supervised learning, active learning 
repeatedly asks a supervisor to label what it con-
siders as the most critical instances and performs 
training with the labelled instances. Thus, active 
learning can eventually create a reliable classifier 
with fewer labelled instances than supervised 
learning. One of the strategies to select critical in-
stances is called ?uncertain reduction? (e.g., Lewis 
and Gale, 1994). Under the strategy, the most un-
certain instances to the current classifier are se-
lected and asked to be labelled by a supervisor. 
The notion of uncertainty reduction was not 
used for bootstrapping, to the best of our knowl-
edge. 
3 Collaborative Bootstrapping and Un-
certainty Reduction 
We consider the collaborative bootstrapping prob-
lem.  
Let  denote a set of instances (feature vectors) 
and let denote a set of labels (classes). Given a 
number of labelled instances, we are to construct a 
function  ?:h . We also refer to it as a classi-
fier.  
In collaborative bootstrapping, we consider the 
use of two partial functions 1h  and 2h , which either 
output a class label or a special symbol ? denoting 
?no decision?.  
Co-training and bilingual bootstrapping are two 
examples of collaborative bootstrapping.  
In co-training, the two collaborating classifiers 
are assumed to be based on two different views, 
namely two different subsets of the entire feature 
set. Formally, the two views are respectively inter-
preted as two functions )(1 xX and )x(X2 , ?x . 
Thus, the two collaborating classifiers 1h  and 2h  in 
co-training can be respectively represented as 
))(( 11 xXh  and ))(( 22 xXh . 
In bilingual bootstrapping, a number of classifi-
ers are created in the two languages. The classes of 
the classifiers correspond to word senses and do 
not overlap, as shown in Figure 1. For example, the 
classifier )E|x(h 11  in language 1 takes sense 2 
and sense 3 as classes. The classifier )C|x(h 12  in 
language 2 takes sense 1 and sense 2 as classes, 
and the classifier )C|x(h 22  takes sense 3 and 
sense 4 as classes. Here we use 211 ,, CCE to de-
note different words in the two languages. Collabo-
rative bootstrapping is performed between the 
classifiers )(h ?1  in language 1 and the classifiers 
)(h ?2  in language 2. (See Li and Li 2002 for de-
tails). 
For the classifier )E|x(h 11 in language 1, we 
assume that there is a pseudo classifier 
)C,C|x(h 212 in language 2, which functions as a 
collaborator of )E|x(h 11 . The pseudo classifier 
)C,C|x(h 212  is based on )C|x(h 12  and 
)C|x(h 22 , and takes sense 2 and sense 3 as classes. 
Formally, the two collaborating classifiers (one 
real classifier and one pseudo classifier) in bilin-
gual bootstrapping are respectively represented as 
)|(1 Exh  and )|(2 Cxh , ?x . 
Next, we introduce the notion of uncertainty re-
duction in collaborative bootstrapping. 
Definition 1 The uncertainty )(hU of a classi-
fier h is defined as: 
}),)(|({)( ?=?= xxhxPhU  
 (1) 
In practice, we define )(hU  as  
}),  ,))((|({)(  ???<== xyyxhCxPhU ?  (2) 
where ?  denotes a predetermined threshold and 
)(?C denotes the confidence score of the classifier 
h. 
Definition 2 The conditional uncer-
tainty )|( yhU of a classifier h given a class y is 
defined as: 
)|},)(|({)|( yYxxhxPyhU =?=?=   (3) 
We note that the uncertainty (or conditional un-
certainty) of a classifier (a partial function) is an 
indicator of the accuracy of the classifier. Let us 
consider an ideal case in which the classifier 
achieves 100% accuracy when it can make a classi-
fication decision and achieves 50% accuracy when 
it cannot (assume that there are only two classes). 
Thus, the total accuracy on the entire data space is 
)(5.01 hU?? . 
Definition 3 Given the two classifiers 1h and 2h  
in collaborative bootstrapping, the uncertainty re-
duction of 1h  with respect to 2h   (denoted as 
)\( 21 hhUR ), is defined as 
}),)(,)(|({)\( 2121 ???=?= xxhxhxPhhUR  (4) 
Similarly, we have 
}),)(,)(|({)\( 2112 ?=???= xxhxhxPhhUR   
Uncertainty reduction is an important factor for 
determining the performance of collaborative boot-
strapping. In collaborative bootstrapping, the more 
the uncertainty of one classifier can be reduced by 
the other classifier, the higher the performance can 
be achieved by the classifier (the more effective 
the collaboration is). 
4 Uncertainty Correlation Coefficient 
Measure 
4.1 Measure 
We introduce the measure of uncertainty correla-
tion coefficient (UCC) to collaborative bootstrap-
ping. 
Definition 4 Given the two classifiers 1h and 2h , 
the conditional uncertainty correlation coefficient 
(CUCC) between 1h and 2h given a class y (denoted 
as yhhr 21 ), is defined as 
)|)(()|)((
)|)(,)((
21 21
21
yYxhPyYxhP
yYxhxhP
yhhr ==?==?
==?=?
=
 
(5) 
Definition 5 The uncertainty correlation coeffi-
cient (UCC) between 1h and 2h  (denoted as 21hhR ), 
is defined as 
=
y
yhhhh r)y(PR 2121  (6) 
UCC represents the degree to which the uncer-
 
Figure 1:  Bilingual Bootstrapping  
tainties of the two classifiers are related. If UCC is 
high, then there are a large portion of instances 
which are uncertain for both of the classifiers. Note 
that UCC is a symmetric measure from both classi-
fiers? perspectives, while UR is an asymmetric 
measure from one classifier?s perspective (ei-
ther )\( 21 hhUR or )\( 12 hhUR ). 
4.2 Theoretical Analysis 
Theorem 1 reveals the relationship between the 
CUCC (UCC) measure and uncertainty reduction.  
Assume that the classifier 1h can collaborate 
with either of the two classifiers 2h and 2'h . The 
two classifiers 2h and 2h? have equal conditional 
uncertainties. The CUCC values between 1h and 
2h? are smaller than the CUCC values between 1h  
and 2h . Then, according to Theorem 1, 1h should 
collaborate with 2h? , because 2h ? can help reduce its 
uncertainty more, thus, improve its accuracy more. 
Theorem 1 Given the two classifier pairs 
),( 21 hh and ),( 21 hh ? , if ?? ? yrr yhhyhh ,2121 and 
),|()|( 22 yhUyhU ?=  ?y , then we have 
)\()\( 2121 hhURhhUR ??  
Proof: 
We can decompose the uncertainty )( 1hU of 1h  as 
follows: 
)())|},)(,)(|({               
)|},)(,)(|({(          
)()|},)(|({)(
21
21
11
yYPyYxxhxhxP
yYxxhxhxP
yYPyYxxhxPhU
y
y
==???=?+
=?=?=?=
==?=?=





 
 )())|},)(,)(|({          
)|},)(|({             
)|},)(|({(       
21
2
121
yYPyYxxhxhxP
yYxxhxP
yYxxhxPr
y
yhh
==???=?+
=?=??
=?=?=



 
)())|},)(,)(|({          
)|()|((       
21
2121
yYPyYxxhxhxP
yhUyhUr
y
yhh
==???=?+
=

 
 
})),)(,)(|({                
)()|()|((        
21
2121
???=?+
==
xxhxhxP
yYPyhUyhUr
y
yhh
 
Thus,  
 =?=
???=?=
y
yhh yYPyhUyhUrhU
xxhxhxPhhUR
)()|()|()(                  
}),)(,)(|({)\(
211
2121
21

Similarly we have 
 =??=? ?
y
yhh yYPyhUyhUrhUhhUR )()|()|()()\( 21121 21  
Under the conditions, yhhyhh rr 2121 ?? , ?y  and 
),|()|( 22 yhUyhU ?= ?y , we have 
)\()\( 2121 hhURhhUR ??   
Theorem 1 states that the lower the CUCC val-
ues are, the higher the performances can be 
achieved in collaborative bootstrapping. 
Definition 6 The two classifiers in co-training 
are said to satisfy the view independence assump-
tion (Blum and Mitchell, 1998), if the following 
equations hold for any class y. 
)|(),|(
)|(),|(
221122
112211
yYxXPxXyYxXP
yYxXPxXyYxXP
======
======
 
Theorem 2 If the view independence assump-
tion holds, then 0.1
21
=yhhr holds for any class y. 
Proof: 
According to (Abney, 2002), view independence 
implies classifier independence: 
)|(),|(
)|(),|(
212
121
yYvhPuhyYvhP
yYuhPvhyYuhP
======
======
 
We can rewrite them as  
)|()|()|,,( 2121 yYvhPyYuhPyYvhuhP ========
 
Thus, we have 
)|})(|({)|},)(|({
)|},)(,)(|({
21
21
yYxxhxPyYxxhxP
yYxxhxhxP
=?=?=?=?=
=?=?=?


 
It means  
??= yr yhh     ,0.121   
Theorem 2 indicates that in co-training with 
view independence, the CUCC values 
( ??yr yhh ,21 ) are small, since by defini-
tion ?<< yhhr 210 . According to Theorem 1, it is 
easy to reduce the uncertainties of the classifiers. 
That is to say, co-training with view independence 
can perform well. 
How to conduct theoretical evaluation on the 
CUCC measure in bilingual bootstrapping is still 
an open problem. 
4.3 Experimental Results 
We conducted experiments to empirically evaluate 
the UCC values of collaborative bootstrapping. We 
also investigated the relationship between UCC 
and accuracy. The results indicate that the theoreti-
cal analysis in Section 4.2 is correct. 
In the experiments, we define accuracy as the 
percentage of instances whose assigned labels 
agree with their ?true? labels. Moreover, when we 
refer to UCC, we mean that it is the UCC value on 
the test data. We set the value of ? in Equation (2) 
to 0.8. 
Co-Training for Artificial Data Classification 
We used the data in (Nigam and Ghani 2000) to 
conduct co-training. We utilized the articles from 
four newsgroups (see Table 1). Each group had 
1000 texts.  
By joining together randomly selected texts 
from each of the two newsgroups in the first row as 
positive instances and joining together randomly 
selected texts from each of the two newsgroups in 
the second row as negative instances, we created a 
two-class classification data with view independ-
ence. The joining was performed under the condi-
tion that the words in the two newsgroups in the 
first column came from one vocabulary, while the 
words in the newsgroups in the second column 
came from the other vocabulary. 
We also created a set of classification data 
without view independence. To do so, we ran-
domly split all the features of the pseudo texts into 
two subsets such that each of the subsets contained 
half of the features. 
We next applied the co-training algorithm to the 
two data sets.  
We conducted the same pre-processing in the 
two experiments. We discarded the header of each 
text, removed stop words from each text, and made 
each text have the same length, as did in (Nigam 
and Ghani, 2000). We discarded 18 texts from the 
entire 2000 texts, because their main contents were 
binary codes, encoding errors, etc. 
We randomly separated the data and performed 
co-training with random feature split and co-
training with natural feature split in five times. The 
results obtained (cf., Table 2), thus, were averaged 
over five trials. In each trial, we used 3 texts for 
each class as labelled training instances, 976 texts 
as testing instances, and the remaining 1000 texts 
as unlabelled training instances. 
From Table 2, we see that the UCC value of the 
natural split (in which view independence holds) is 
lower than that of the random split (in which view 
independence does not hold). That is to say, in 
natural split, there are fewer instances which are 
uncertain for both of the classifiers. The accuracy 
of the natural split is higher than that of the random 
split. Theorem 1 states that the lower the CUCC 
values are, the higher the performances can be 
achieved. The results in Table 2 agree with the 
claim of Theorem 1. (Note that it is easier to use 
CUCC for theoretical analysis, but it is easier to 
use UCC for empirical analysis). 
Table 2: Results with Artificial Data 
Feature Accuracy  UCC  
Natural Split  0.928 1.006 
Random Split 0.712 2.399 
We also see that the UCC value of the natural 
split (view independence) is about 1.0. The result 
agrees with Theorem 2. 
Co-Training for Web Page Classification  
We used the same data in (Blum and Mitchell, 
1998) to perform co-training for web page classifi-
cation. 
The web page data consisted of 1051 web pages 
collected from the computer science departments 
of four universities. The goal of classification was 
to determine whether a web page was concerned 
with an academic course. 22% of the pages were 
actually related to academic courses. The features 
for each page were possible to be separated into 
two independent parts. One part consisted of words 
occurring in the current page and the other part 
consisted of words occurring in the anchor texts 
pointed to the current page.  
We randomly split the data into three subsets: 
labelled training set, unlabeled training set, and test 
set. The labelled training set had 3 course pages 
and 9 non-course pages. The test set had 25% of 
the pages. The unlabelled training set had the re-
maining data.  
Table 3: Results with Web Page Data and Bilin-
gual Bootstrapping Data 
Data Accuracy UCC 
Web Page 0.943 1.147 
bass 0.925 2.648 
drug 0.868 0.986 
duty 0.751 0.840 
palm 0.924 1.174 
plant 0.959 1.226 
space 0.878 1.007 
Word Sense Dis-
ambiguation 
tank 0.844 1.177 
We used the data to perform co-training and 
web page classification. The setting for the  
Table 1: Artificial Data for Co-Training 
Class Feature Set A Feature Set B 
Pos comp.os.ms-windows.misc talk.politics.misc 
Neg comp.sys.ibm.pc.hardware talk.politics.guns 
experiment was almost the same as that of Nigam 
and Ghani?s. One exception was that we did not 
conduct feature selection, because we were not 
able to follow their method from their paper. 
We repeated the experiment five times and 
evaluated the results in terms of UCC and accuracy. 
Table 3 shows the average accuracy and UCC 
value over the five trials. 
Bilingual Bootstrapping 
We also used the same data in (Li and Li, 2002) to 
conduct bilingual bootstrapping and word sense 
disambiguation. 
The sense disambiguation data were related to 
seven ambiguous English words, each having two 
Chinese translations. The goal was to determine 
the correct Chinese translations of the ambiguous 
English words, given English sentences containing 
the ambiguous words.  
For each word, there were two seed words used 
as labelled instances for training, a large number of 
unlabeled instances (sentences) in both English and 
Chinese for training, and about 200 labelled in-
stances (sentences) for testing. Details on data are 
shown in Table 4. 
We used the data to perform bilingual boot-
strapping and word sense disambiguation. The set-
ting for the experiment was exactly the same as 
that of Li and Li?s. Table 3 shows the accuracy and 
UCC value for each word. 
From Table 3 we see that both co-training and 
bilingual bootstrapping have low UCC values 
(around 1.0). With lower UCC (CUCC) values, 
higher performances can be achieved, according to 
Theorem 1. The accuracies of them are indeed high. 
Note that since the features and classes for each 
word in bilingual bootstrapping and those for web 
page classification in co-training are different, it is 
not meaningful to directly compare the UCC val-
ues of them. 
5 Uncertainty Reduction Algorithm 
5.1 Algorithm 
We propose a new algorithm for collaborative 
bootstrapping (both co-training and bilingual boot-
strapping). 
In the algorithm, the collaboration between the 
classifiers is driven by uncertainty reduction. Spe-
cifically, one classifier always selects the most un-
certain unlabelled instances for it and asks the 
other classifier to label.  Thus, the two classifiers 
can help each other more effectively. 
There exists, therefore, a similarity between our 
algorithm and active learning. In active learning 
the learner always asks the supervisor to label the 
Table 4: Data for Bilingual Bootstrapping 
Unlabelled instances Word 
English Chinese 
Seed words Test instances 
bass 142 8811 fish / music 200 
drug 3053 5398 treatment / smuggler 197 
duty 1428 4338 discharge / export 197 
palm 366 465 tree / hand 197 
plant 7542 24977 industry / life 197 
Space 3897 14178 volume / outer 197 
tank 417 1400 combat / fuel 199 
Total 16845 59567 - 1384 
Input: A set of labeled instances and a set of unla-
belled instances. 
Loop while there exist unlabelled instances{ 
Create classifier 1h using the labeled instances; 
Create classifier 2h using the labeled instances; 
For each class ( yY = ){ 
Pick up yb  unlabelled instances whose labels 
( yY = ) are most certain for 1h and are most 
uncertain for 2h , label them with 1h and add 
them into the set of labeled instances; 
      
Pick up yb  unlabelled instances whose labels 
( yY = ) are most certain for 2h and are most 
uncertain for 1h , label them with 2h  and add 
them into the set of labeled instances; 
} 
} 
Output: Two classifiers 1h and 2h  
Figure 2: Uncertainty Reduction Algorithm 
most uncertain examples for it, while in our algo-
rithm one classifier always asks the other classifier 
to label the most uncertain examples for it. 
Figure 2 shows the algorithm. Actually, our 
new algorithm is different from the previous algo-
rithm only in one point. Figure 2 highlights the 
point in italic fonts. In the previous algorithm, 
when a classifier labels unlabeled instances, it la-
bels those instances whose labels are most certain 
for the classifier. In contrast, in our new algorithm, 
when a classifier labels unlabeled instances, it la-
bels those instances whose labels are most certain 
for the classifier, but at the same time most uncer-
tain for the other classifier. 
As one implementation, for each class y, 1h first 
selects its most certain ya instances, 2h  next se-
lects from them its most uncertain yb  instances 
( yy ba ? ), and finally 1h labels the yb instances 
with label y (Collaboration from the opposite di-
rection is performed similarly.). We use this im-
plementation in our experiments described below. 
5.2 Experimental Results 
We conducted experiments to test the effectiveness 
of our new algorithm. Experimental results indi-
cate that the new algorithm performs better than 
the previous algorithm. We refer to them as ?new? 
and ?old? respectively. 
Co-Training for Artificial Data Classification 
We used the artificial data in Section 4.3 and con-
ducted co-training with both the old and new algo-
rithms. Table 5 shows the results.  
We see that in co-training the new algorithm 
performs as well as the old algorithm when UCC is 
low (view independence holds), and the new algo-
rithm performs significantly better than the old al-
gorithm when UCC is high (view independence 
does not hold). 
Co-Training for Web Page Classification 
We used the web page classification data in Sec-
tion 4.3 and conducted co-training using both the 
old and new algorithms. Table 6 shows the results. 
We see that the new algorithm performs as well as 
the old algorithm for this data set. Note that here 
UCC is low. 
Table 6: Accuracies with Web Page Data 
Accuracy Data Old New UCC 
Web Page 0.943 0.943 1.147 
Bilingual Bootstrapping 
We used the word sense disambiguation data in 
Section 4.3 and conducted bilingual bootstrapping 
using both the old and new algorithms. Table 7 
shows the results. We see that the performance of 
the new algorithm is slightly better than that of the 
old algorithm. Note that here the UCC values are 
also low. 
We conclude that for both co-training and bi-
lingual bootstrapping, the new algorithm performs 
significantly better than the old algorithm when 
UCC is high, and performs as well as the old algo-
rithm when UCC is low. Recall that when UCC is 
high, there are more instances which are uncertain 
for both classifiers and when UCC is low, there are 
fewer instances which are uncertain for both classi-
fiers. 
Note that in practice it is difficult to find a 
situation in which UCC is completely low (e.g., the 
view independence assumption completely holds), 
and thus the new algorithm will be more useful 
than the old algorithm in practice. To verify this, 
we conducted an additional experiment. 
Again, since the features and classes for each 
word in bilingual bootstrapping and those for web 
page classification in co-training are different, it is 
not meaningful to directly compare the UCC val-
ues of them. 
Co-Training for News Article Classification 
In the additional experiment, we used the data 
Table 5: Accuracies with Artificial Data 
Accuracy Feature Old New UCC 
Natural Split 0.928 0.924 1.006 
Random Split 0.712 0.775 2.399 
Table 7: Accuracies with Bilingual Bootstrapping 
Data 
Accuracy Word Old New UCC 
bass 0.925 0.955 2.648 
drug 0.868 0.863 0.986 
duty 0.751 0. 797 0.840 
palm 0.924 0.914 1.174 
plant 0.959 0.944 1.226 
space 0.878 0.888 1.007 
tank 0.844 0.854 1.177 
Average 0.878 0.888 - 
from two newsgroups (comp.graphics and 
comp.os.ms-windows.misc) in the dataset of 
(Joachims, 1997) to construct co-training and text 
classification. 
There were 1000 texts for each group. We 
viewed the former group as positive class and the 
latter group as negative class. We applied the new 
and old algorithms. We conducted 20 trials in the 
experimentation. In each trial we randomly split 
the data into labelled training, unlabeled training 
and test data sets. We used 3 texts per class as la-
belled instances for training, 994 texts for testing, 
and the remaining 1000 texts as unlabelled in-
stances for training. We performed the same pre-
processing as that in (Nigam and Ghani 2000). 
Table 8 shows the results with the 20 trials. The 
accuracies are averaged over each 5 trials. From 
the table, we see that co-training with the new al-
gorithm significantly outperforms that using the 
old algorithm and also ?single bootstrapping?. Here, 
?single bootstrapping? refers to the conventional 
bootstrapping method in which a single classifier 
repeatedly boosts its performances with all the fea-
tures. 
The above experimental results indicate that our 
new algorithm for collaborative bootstrapping per-
forms significantly better than the old algorithm 
when the collaboration is difficult. It performs as 
well as the old algorithm when the collaboration is 
easy. Therefore, it is better to always employ the 
new algorithm. 
Another conclusion from the results is that we 
can apply our new algorithm into any single boot-
strapping problem. More specifically, we can ran-
domly split the feature set and use our algorithm to 
perform co-training with the split subsets. 
6 Conclusion 
This paper has theoretically and empirically dem-
onstrated that uncertainty reduction is the essence 
of collaborative bootstrapping, which includes 
both co-training and bilingual bootstrapping. 
The paper has conducted a new theoretical 
analysis of collaborative bootstrapping, and has 
proposed a new algorithm for collaborative boot-
strapping, both on the basis of uncertainty reduc-
tion. Experimental results have verified the 
correctness of the analysis and have indicated that 
the new algorithm performs better than the existing 
algorithms. 
References 
S. Abney, 2002. Bootstrapping. In Proceedings of the 
40th Annual Meeting of the Association for Compu-
tational Linguistics. 
A. Blum and T. Mitchell, 1998. Combining Labeled 
Data and Unlabelled Data with Co-training. In Pro-
ceedings of the 11th Annual Conference on Compu-
tational learning Theory. 
M. Collins and Y. Singer, 1999. Unsupervised Models 
for Named Entity Classification. In Proceedings of 
the 1999 Joint SIGDAT Conference on Empirical 
Methods in Natural Language Processing and Very 
Large Corpora. 
S. Dasgupta, M. Littman and D. McAllester, 2001. PAC 
Generalization Bounds for Co-Training. In Proceed-
ings of Neural Information Processing System, 2001. 
T. Joachims, 1997. A Probabilistic Analysis of the Roc-
chio Algorithm with TFIDF for Text Categorization. 
In Proceedings of the 14th International Conference 
on Machine Learning. 
D. Lewis and W. Gale, 1994. A Sequential Algorithm 
for Training Text Classifiers. In Proceedings of the 
17th International ACM-SIGIR Conference on Re-
search and Development in Information Retrieval. 
C. Li and H. Li, 2002. Word Translation Disambigua-
tion Using Bilingual Bootstrapping. In Proceedings of 
the 40th Annual Meeting of the Association for Com-
putational Linguistics. 
I. Muslea, S.Minton, and C. A. Knoblock 2000. Selec-
tive Sampling With Redundant Views. In Proceed-
ings of the Seventeenth National Conference on 
Artificial Intelligence. 
K. Nigam and R. Ghani, 2000. Analyzing the Effective-
ness and Applicability of Co-Training. In Proceed-
ings of the 9th International Conference on 
Information and Knowledge Management.  
D. Pierce and C. Cardie 2001. Limitations of Co-
Training for Natural Language Learning from Large 
Datasets. In Proceedings of the 2001 Conference on 
Empirical Methods in Natural Language Processing 
(EMNLP-2001). 
D. Yarowsky, 1995. Unsupervised Word Sense Disam-
biguation Rivaling Supervised Methods. In Proceed-
ings of the 33rd Annual Meeting of the Association 
for Computational Linguistics. 
Table 8:  Accuracies with News Data 
Collaborative Boot-
strapping Average Accuracy 
Single Boot-
strapping Old  New  
Trial 1-5 0.725 0.737 0.768 
Trial 6-10 0.708 0.702 0.793 
Trial 11-15 0.679 0.647 0.769 
Trial 16-20 0.699 0.689 0.767 
All 0.703 0.694 0.774 
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 688?695,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
A Unified Tagging Approach to Text Normalization 
Conghui Zhu 
Harbin Institute of Technology 
Harbin, China 
chzhu@mtlab.hit.edu.cn 
Jie Tang 
Department of Computer Science
Tsinghua University, China 
jietang@tsinghua.edu.cn
Hang Li 
Microsoft Research Asia  
Beijing, China 
hangli@microsoft.com
Hwee Tou Ng 
Department of Computer Science 
National University of Singapore, Singapore 
nght@comp.nus.edu.sg 
Tiejun Zhao 
Harbin Institute of Technology 
Harbin, China 
tjzhao@mtlab.hit.edu.cn 
 
 
Abstract 
This paper addresses the issue of text nor-
malization, an important yet often over-
looked problem in natural language proc-
essing. By text normalization, we mean 
converting ?informally inputted? text into 
the canonical form, by eliminating ?noises? 
in the text and detecting paragraph and sen-
tence boundaries in the text. Previously, 
text normalization issues were often under-
taken in an ad-hoc fashion or studied sepa-
rately. This paper first gives a formaliza-
tion of the entire problem. It then proposes 
a unified tagging approach to perform the 
task using Conditional Random Fields 
(CRF). The paper shows that with the in-
troduction of a small set of tags, most of 
the text normalization tasks can be per-
formed within the approach. The accuracy 
of the proposed method is high, because 
the subtasks of normalization are interde-
pendent and should be performed together. 
Experimental results on email data cleaning 
show that the proposed method signifi-
cantly outperforms the approach of using 
cascaded models and that of employing in-
dependent models. 
1 Introduction 
More and more ?informally inputted? text data be-
comes available to natural language processing, 
such as raw text data in emails, newsgroups, fo-
rums, and blogs. Consequently, how to effectively 
process the data and make it suitable for natural 
language processing becomes a challenging issue. 
This is because informally inputted text data is 
usually very noisy and is not properly segmented. 
For example, it may contain extra line breaks, extra 
spaces, and extra punctuation marks; and it may 
contain words badly cased. Moreover, the bounda-
ries between paragraphs and the boundaries be-
tween sentences are not clear. 
We have examined 5,000 randomly collected 
emails and found that 98.4% of the emails contain 
noises (based on the definition in Section 5.1). 
In order to perform high quality natural lan-
guage processing, it is necessary to perform ?nor-
malization? on informally inputted data first, spe-
cifically, to remove extra line breaks, segment the 
text into paragraphs, add missing spaces and miss-
ing punctuation marks, eliminate extra spaces and 
extra punctuation marks, delete unnecessary tokens, 
correct misused punctuation marks, restore badly 
cased words, correct misspelled words, and iden-
tify sentence boundaries. 
Traditionally, text normalization is viewed as an 
engineering issue and is conducted in a more or 
less ad-hoc manner. For example, it is done by us-
ing rules or machine learning models at different 
levels. In natural language processing, several is-
sues of text normalization were studied, but were 
only done separately. 
This paper aims to conduct a thorough investiga-
tion on the issue. First, it gives a formalization of 
688
the problem; specifically, it defines the subtasks of 
the problem. Next, it proposes a unified approach 
to the whole task on the basis of tagging. Specifi-
cally, it takes the problem as that of assigning tags 
to the input texts, with a tag representing deletion, 
preservation, or replacement of a token. As the 
tagging model, it employs Conditional Random 
Fields (CRF). The unified model can achieve better 
performances in text normalization, because the 
subtasks of text normalization are often interde-
pendent. Furthermore, there is no need to define 
specialized models and features to conduct differ-
ent types of cleaning; all the cleaning processes 
have been formalized and conducted as assign-
ments of the three types of tags. 
Experimental results indicate that our method 
significantly outperforms the methods using cas-
caded models or independent models on normali-
zation. Our experiments also indicate that with the 
use of the tags defined, we can conduct most of the 
text normalization in the unified framework. 
Our contributions in this paper include: (a) for-
malization of the text normalization problem, (b) 
proposal of a unified tagging approach, and (c) 
empirical verification of the effectiveness of the 
proposed approach. 
The rest of the paper is organized as follows. In 
Section 2, we introduce related work. In Section 3, 
we formalize the text normalization problem. In 
Section 4, we explain our approach to the problem 
and in Section 5 we give the experimental results. 
We conclude the paper in Section 6. 
2 Related Work 
Text normalization is usually viewed as an 
engineering issue and is addressed in an ad-hoc 
manner. Much of the previous work focuses on 
processing texts in clean form, not texts in 
informal form. Also, prior work mostly focuses on 
processing one type or a small number of types of 
errors, whereas this paper deals with many 
different types of errors. 
Clark (2003) has investigated the problem of 
preprocessing noisy texts for natural language 
processing. He proposes identifying token bounda-
ries and sentence boundaries, restoring cases of 
words, and correcting misspelled words by using a 
source channel model. 
Minkov et al (2005) have investigated the prob-
lem of named entity recognition in informally in-
putted texts. They propose improving the perform-
ance of personal name recognition in emails using 
two machine-learning based methods: Conditional 
Random Fields and Perceptron for learning HMMs. 
See also (Carvalho and Cohen, 2004). 
Tang et al (2005) propose a cascaded approach 
for email data cleaning by employing Support Vec-
tor Machines and rules. Their method can detect 
email headers, signatures, program codes, and ex-
tra line breaks in emails. See also (Wong et al, 
2007). 
Palmer and Hearst (1997) propose using a Neu-
ral Network model to determine whether a period 
in a sentence is the ending mark of the sentence, an 
abbreviation, or both. See also (Mikheev, 2000; 
Mikheev, 2002). 
Lita et al (2003) propose employing a language 
modeling approach to address the case restoration 
problem. They define four classes for word casing: 
all letters in lower case, first letter in uppercase, all 
letters in upper case, and mixed case, and formal-
ize the problem as assigning class labels to words 
in natural language texts. Mikheev (2002) proposes 
using not only local information but also global 
information in a document in case restoration. 
Spelling error correction can be formalized as a 
classification problem. Golding and Roth (1996) 
propose using the Winnow algorithm to address 
the issue. The problem can also be formalized as 
that of data conversion using the source channel 
model. The source model can be built as an n-gram 
language model and the channel model can be con-
structed with confusing words measured by edit 
distance. Brill and Moore, Church and Gale, and 
Mayes et al have developed different techniques 
for confusing words calculation (Brill and Moore, 
2000; Church and Gale, 1991; Mays et al, 1991). 
Sproat et al (1999) have investigated normaliza-
tion of non-standard words in texts, including 
numbers, abbreviations, dates, currency amounts, 
and acronyms. They propose a taxonomy of non-
standard words and apply n-gram language models, 
decision trees, and weighted finite-state transduc-
ers to the normalization. 
3 Text Normalization 
In this paper we define text normalization at three 
levels: paragraph, sentence, and word level. The 
subtasks at each level are listed in Table 1. For ex-
ample, at the paragraph level, there are two sub-
689
tasks: extra line-break deletion and paragraph 
boundary detection. Similarly, there are six (three) 
subtasks at the sentence (word) level, as shown in 
Table 1. Unnecessary token deletion refers to dele-
tion of tokens like ?-----? and ?====?, which are 
not needed in natural language processing. Note 
that most of the subtasks conduct ?cleaning? of 
noises, except paragraph boundary detection and 
sentence boundary detection. 
Level Task Percentages of Noises
Extra line break deletion 49.53 
Paragraph 
Paragraph boundary detection  
Extra space deletion 15.58 
Extra punctuation mark deletion 0.71 
Missing space insertion 1.55 
Missing punctuation mark insertion 3.85 
Misused punctuation mark correction 0.64 
Sentence 
Sentence boundary detection  
Case restoration 15.04 
Unnecessary token deletion 9.69 Word 
Misspelled word correction 3.41 
Table 1. Text Normalization Subtasks 
As a result of text normalization, a text is seg-
mented into paragraphs; each paragraph is seg-
mented into sentences with clear boundaries; and 
each word is converted into the canonical form. 
After normalization, most of the natural language 
processing tasks can be performed, for example, 
part-of-speech tagging and parsing. 
We have manually cleaned up some email data 
(cf., Section 5) and found that nearly all the noises 
can be eliminated by performing the subtasks de-
fined above. Table 1 gives the statistics. 
1.  i?m thinking about buying a pocket 
2.  pc    device for my wife this christmas,. 
3.  the worry that i have is that she won?t 
4.  be able to sync it to her outlook express  
5.  contacts? 
Figure 1. An example of informal text 
I?m thinking about buying a Pocket PC device for my 
wife this Christmas.// The worry that I have is that 
she won?t be able to sync it to her Outlook Express 
contacts.// 
Figure 2. Normalized text 
Figure 1 shows an example of informally input-
ted text data. It includes many typical noises. From 
line 1 to line 4, there are four extra line breaks at 
the end of each line. In line 2, there is an extra 
comma after the word ?Christmas?. The first word 
in each sentence and the proper nouns (e.g., 
?Pocket PC? and ?Outlook Express?) should be 
capitalized. The extra spaces between the words 
?PC? and ?device? should be removed. At the end 
of line 2, the line break should be removed and a 
space is needed after the period. The text should be 
segmented into two sentences. 
Figure 2 shows an ideal output of text normali-
zation on the input text in Figure 1. All the noises 
in Figure 1 have been cleaned and paragraph and 
sentence endings have been identified. 
We must note that dependencies (sometimes 
even strong dependencies) exist between different 
types of noises. For example, word case restoration 
needs help from sentence boundary detection, and 
vice versa. An ideal normalization method should 
consider processing all the tasks together. 
4 A Unified Tagging Approach 
4.1 Process 
In this paper, we formalize text normalization as a 
tagging problem and employ a unified approach to 
perform the task (no matter whether the processing 
is at paragraph level, sentence level, or word level). 
There are two steps in the method: preprocess-
ing and tagging. In preprocessing, (A) we separate 
the text into paragraphs (i.e., sequences of tokens), 
(B) we determine tokens in the paragraphs, and (C) 
we assign possible tags to each token. The tokens 
form the basic units and the paragraphs form the 
sequences of units in the tagging problem. In tag-
ging, given a sequence of units, we determine the 
most likely corresponding sequence of tags by us-
ing a trained tagging model. In this paper, as the 
tagging model, we make use of CRF. 
Next we describe the steps (A)-(C) in detail and 
explain why our method can accomplish many of 
the normalization subtasks in Table 1. 
(A). We separate the text into paragraphs by tak-
ing two or more consecutive line breaks as the end-
ings of paragraphs. 
(B). We identify tokens by using heuristics. 
There are five types of tokens: ?standard word?, 
?non-standard word?, punctuation mark, space, and 
line break. Standard words are words in natural 
language. Non-standard words include several 
general ?special words? (Sproat et al, 1999), email 
address, IP address, URL, date, number, money, 
percentage, unnecessary tokens (e.g., ?===? and 
690
?###?), etc. We identify non-standard words by 
using regular expressions. Punctuation marks in-
clude period, question mark, and exclamation mark. 
Words and punctuation marks are separated into 
different tokens if they are joined together. Natural 
spaces and line breaks are also regarded as tokens. 
(C). We assign tags to each token based on the 
type of the token. Table 2 summarizes the types of 
tags defined. 
Token Type Tag Description 
PRV Preserve line break 
RPA Replace line break by space Line break 
DEL Delete line break 
PRV Preserve space 
Space 
DEL Delete space 
PSB Preserve punctuation mark and view it as sentence ending 
PRV Preserve punctuation mark without viewing it as sentence ending 
Punctuation 
mark 
DEL Delete punctuation mark 
AUC Make all characters in uppercase 
ALC Make all characters in lowercase 
FUC Make the first character in uppercase
Word 
AMC Make characters in mixed case 
PRV Preserve the special token 
Special token 
DEL Delete the special token 
Table 2. Types of tags 
 
Figure 3. An example of tagging 
Figure 3 shows an example of the tagging proc-
ess. (The symbol ??? indicates a space). In the fig-
ure, a white circle denotes a token and a gray circle 
denotes a tag. Each token can be assigned several 
possible tags. 
Using the tags, we can perform most of the text 
normalization processing (conducting seven types 
of subtasks defined in Table 1 and cleaning 
90.55% of the noises). 
In this paper, we do not conduct three subtasks, 
although we could do them in principle. These in-
clude missing space insertion, missing punctuation 
mark insertion, and misspelled word correction. In 
our email data, it corresponds to 8.81% of the 
noises. Adding tags for insertions would increase 
the search space dramatically. We did not do that 
due to computation consideration. Misspelled word 
correction can be done in the same framework eas-
ily. We did not do that in this work, because the 
percentage of misspelling in the data is small. 
We do not conduct misused punctuation mark 
correction as well (e.g., correcting ?.? with ???). It 
consists of 0.64% of the noises in the email data. 
To handle it, one might need to parse the sentences. 
4.2 CRF Model 
We employ Conditional Random Fields (CRF) as 
the tagging model. CRF is a conditional probability 
distribution of a sequence of tags given a sequence 
of tokens, represented as P(Y|X) , where X denotes 
the token sequence and Y the tag sequence 
(Lafferty et al, 2001). 
In tagging, the CRF model is used to find the 
sequence of tags Y* having the highest likelihood 
Y* = maxYP(Y|X), with an efficient algorithm (the 
Viterbi algorithm). 
In training, the CRF model is built with labeled 
data and by means of an iterative algorithm based 
on Maximum Likelihood Estimation. 
Transition Features 
yi-1=y?, yi=y 
yi-1=y?, yi=y, wi=w 
yi-1=y?, yi=y, ti=t 
State Features 
wi=w, yi=y 
wi-1=w, yi=y 
wi-2=w, yi=y 
wi-3=w, yi=y 
wi-4=w, yi=y 
wi+1=w, yi=y 
wi+2=w, yi=y 
wi+3=w, yi=y 
wi+4=w, yi=y 
wi-1=w?, wi=w, yi=y
wi+1=w?, wi=w, yi=y 
ti=t, yi=y 
ti-1=t, yi=y 
ti-2=t, yi=y 
ti-3=t, yi=y 
ti-4=t, yi=y 
ti+1=t, yi=y 
ti+2=t, yi=y 
ti+3=t, yi=y 
ti+4=t, yi=y 
ti-2=t??, ti-1=t?, yi=y 
ti-1=t?, ti=t, yi=y 
ti=t, ti+1=t?, yi=y 
ti+1=t?, ti+2=t??, yi=y 
ti-2=t??, ti-1=t?, ti=t, yi=y 
ti-1=t??, ti=t, ti+1=t?, yi=y 
ti=t, ti+1=t?, ti+2=t??, yi=y 
Table 3. Features used in the unified CRF model 
691
4.3 Features 
Two sets of features are defined in the CRF model: 
transition features and state features. Table 3 
shows the features used in the model. 
Suppose that at position i in token sequence x, wi 
is the token, ti the type of token (see Table 2), and 
yi the possible tag. Binary features are defined as 
described in Table 3. For example, the transition 
feature yi-1=y?, yi=y implies that if the current tag is 
y and the previous tag is y?, then the feature value 
is true; otherwise false. The state feature wi=w, 
yi=y implies that if the current token is w and the 
current label is y, then the feature value is true; 
otherwise false. In our experiments, an actual fea-
ture might be the word at position 5 is ?PC? and the 
current tag is AUC. In total, 4,168,723 features 
were used in our experiments. 
4.4 Baseline Methods 
We can consider two baseline methods based on 
previous work, namely cascaded and independent 
approaches. The independent approach performs 
text normalization with several passes on the text. 
All of the processes take the raw text as input and 
output the normalized/cleaned result independently. 
The cascaded approach also performs normaliza-
tion in several passes on the text. Each process car-
ries out cleaning/normalization from the output of 
the previous process. 
4.5 Advantages 
Our method offers some advantages. 
(1) As indicated, the text normalization tasks are 
interdependent. The cascaded approach or the in-
dependent approach cannot simultaneously per-
form the tasks. In contrast, our method can effec-
tively overcome the drawback by employing a uni-
fied framework and achieve more accurate per-
formances. 
(2) There are many specific types of errors one 
must correct in text normalization. As shown in 
Figure 1, there exist four types of errors with each 
type having several correction results. If one de-
fines a specialized model or rule to handle each of 
the cases, the number of needed models will be 
extremely large and thus the text normalization 
processing will be impractical. In contrast, our 
method naturally formalizes all the tasks as as-
signments of different types of tags and trains a 
unified model to tackle all the problems at once. 
5 Experimental Results 
5.1 Experiment Setting 
Data Sets 
We used email data in our experiments. We ran-
domly chose in total 5,000 posts (i.e., emails) from 
12 newsgroups. DC, Ontology, NLP, and ML are 
from newsgroups at Google (http://groups-
beta.google.com/groups). Jena is a newsgroup at Ya-
hoo (http://groups.yahoo.com/group/jena-dev). Weka 
is a newsgroup at Waikato University (https://list. 
scms.waikato.ac.nz). Prot?g? and OWL are from a 
project at Stanford University 
(http://protege.stanford.edu/). Mobility, WinServer, 
Windows, and PSS are email collections from a 
company. 
Five human annotators conducted normalization 
on the emails. A spec was created to guide the an-
notation process. All the errors in the emails were 
labeled and corrected. For disagreements in the 
annotation, we conducted ?majority voting?.  For 
example, extra line breaks, extra spaces, and extra 
punctuation marks in the emails were labeled. Un-
necessary tokens were deleted. Missing spaces and 
missing punctuation marks were added and marked. 
Mistakenly cased words, misspelled words, and 
misused punctuation marks were corrected. Fur-
thermore, paragraph boundaries and sentence 
boundaries were also marked. The noises fell into 
the categories defined in Table 1. 
Table 4 shows the statistics in the data sets. 
From the table, we can see that a large number of 
noises (41,407) exist in the emails. We can also see 
that the major noise types are extra line breaks, 
extra spaces, casing errors, and unnecessary tokens. 
In the experiments, we conducted evaluations in 
terms of precision, recall, F1-measure, and accu-
racy (for definitions of the measures, see for ex-
ample (van Rijsbergen, 1979; Lita et al, 2003)). 
Implementation of Baseline Methods 
We used the cascaded approach and the independ-
ent approach as baselines. 
For the baseline methods, we defined several 
basic prediction subtasks: extra line break detec-
tion, extra space detection, extra punctuation mark 
detection, sentence boundary detection, unneces-
sary token detection, and case restoration. We 
compared the performances of our method with 
those of the baseline methods on the subtasks. 
692
Data Set 
Number 
of 
Email 
Number 
of 
Noises 
Extra 
Line 
Break 
Extra 
Space 
Extra
 Punc.
Missing
Space
Missing
Punc.
Casing
Error
Spelling
Error
Misused 
Punc.
Unnece-
ssary 
Token 
Number of 
Paragraph 
Boundary 
Number of 
Sentence 
Boundary
DC 100 702 476 31 8 3 24 53 14 2 91 457 291 
Ontology 100 2,731 2,132 24 3 10 68 205 79 15 195 677 1,132 
NLP 60 861 623 12 1 3 23 135 13 2 49 244 296 
ML 40 980 868 17 0 2 13 12 7 0 61 240 589 
Jena 700 5,833 3,066 117 42 38 234 888 288 59 1,101 2,999 1,836 
Weka 200 1,721 886 44 0 30 37 295 77 13 339 699 602 
Prot?g? 700 3,306 1,770 127 48 151 136 552 116 9 397 1,645 1,035 
OWL 300 1,232 680 43 24 47 41 152 44 3 198 578 424 
Mobility 400 2,296 1,292 64 22 35 87 495 92 8 201 891 892 
WinServer 400 3,487 2,029 59 26 57 142 822 121 21 210 1,232 1,151 
Windows 1,000 9,293 3,416 3,056 60 116 348 1,309 291 67 630 3,581 2,742 
PSS 1,000 8,965 3,348 2,880 59 153 296 1,331 276 66 556 3,411 2,590 
Total 5,000 41,407 20,586 6,474 293 645 1,449 6,249 1,418 265 4,028 16,654 13,580 
Table 4. Statistics on data sets 
For the case restoration subtask (processing on 
token sequence), we employed the TrueCasing 
method (Lita et al, 2003). The method estimates a 
tri-gram language model using a large data corpus 
with correctly cased words and then makes use of 
the model in case restoration. We also employed 
Conditional Random Fields to perform case 
restoration, for comparison purposes. The CRF 
based casing method estimates a conditional 
probabilistic model using the same data and the 
same tags defined in TrueCasing. 
For unnecessary token deletion, we used rules as 
follows. If a token consists of non-ASCII charac-
ters or consecutive duplicate characters, such as 
?===?, then we identify it as an unnecessary token. 
For each of the other subtasks, we exploited the 
classification approach. For example, in extra line 
break detection, we made use of a classification 
model to identify whether or not a line break is a 
paragraph ending. We employed Support Vector 
Machines (SVM) as the classification model (Vap-
nik, 1998). In the classification model we utilized 
the same features as those in our unified model 
(see Table 3 for details). 
In the cascaded approach, the prediction tasks 
are performed in sequence, where the output of 
each task becomes the input of each immediately 
following task. The order of the prediction tasks is: 
(1) Extra line break detection: Is a line break a 
paragraph ending? It then separates the text into 
paragraphs using the remaining line breaks. (2) 
Extra space detection: Is a space an extra space? (3) 
Extra punctuation mark detection: Is a punctuation 
mark a noise? (4) Sentence boundary detection: Is 
a punctuation mark a sentence boundary? (5) Un-
necessary token deletion: Is a token an unnecessary 
token? (6) Case restoration. Each of steps (1) to (4) 
uses a classification model (SVM), step (5) uses 
rules, whereas step (6) uses either a language 
model (TrueCasing) or a CRF model (CRF). 
In the independent approach, we perform the 
prediction tasks independently. When there is a 
conflict between the outcomes of two classifiers, 
we adopt the result of the latter classifier, as de-
termined by the order of classifiers in the cascaded 
approach. 
To test how dependencies between different 
types of noises affect the performance of normali-
zation, we also conducted experiments using the 
unified model by removing the transition features. 
Implementation of Our Method 
In the implementation of our method, we used the 
tool CRF++, available at http://chasen.org/~taku 
/software/CRF++/. We made use of all the default 
settings of the tool in the experiments. 
5.2 Text Normalization Experiments 
Results 
We evaluated the performances of our method 
(Unified) and the baseline methods (Cascaded and 
Independent) on the 12 data sets. Table 5 shows 
the five-fold cross-validation results. Our method 
outperforms the two baseline methods. 
Table 6 shows the overall performances of text 
normalization by our method and the two baseline 
methods. We see that our method outperforms the 
two baseline methods. It can also be seen that the 
performance of the unified method decreases when 
removing the transition features (Unified w/o 
Transition Features). 
693
We conducted sign tests for each subtask on the 
results, which indicate that all the improvements of 
Unified over Cascaded and Independent are statis-
tically significant (p << 0.01). 
Detection Task Prec. Rec. F1 Acc.
Independent 95.16 91.52 93.30 93.81
Cascaded 95.16 91.52 93.30 93.81Extra Line Break  
Unified 93.87 93.63 93.75 94.53
Independent 91.85 94.64 93.22 99.87
Cascaded 94.54 94.56 94.55 99.89Extra Space 
Unified 95.17 93.98 94.57 99.90
Independent 88.63 82.69 85.56 99.66
Cascaded 87.17 85.37 86.26 99.66
Extra 
 Punctuation 
Mark Unified 90.94 84.84 87.78 99.71
Independent 98.46 99.62 99.04 98.36
Cascaded 98.55 99.20 98.87 98.08Sentence Boundary  
Unified 98.76 99.61 99.18 98.61
Independent 72.51 100.0 84.06 84.27
Cascaded 72.51 100.0 84.06 84.27Unnecessary Token 
Unified 98.06 95.47 96.75 96.18
Independent 27.32 87.44 41.63 96.22Case  
Restoration 
(TrueCasing) Cascaded 28.04 88.21 42.55 96.35
Independent 84.96 62.79 72.21 99.01
Cascaded 85.85 63.99 73.33 99.07
Case  
Restoration 
(CRF) Unified 86.65 67.09 75.63 99.21
Table 5. Performances of text normalization (%) 
Text Normalization Prec. Rec. F1 Acc.
Independent (TrueCasing) 69.54 91.33 78.96 97.90
Independent (CRF) 85.05 92.52 88.63 98.91
Cascaded (TrueCasing) 70.29 92.07 79.72 97.88
Cascaded (CRF) 85.06 92.70 88.72 98.92
Unified w/o Transition 
Features 86.03 93.45 89.59 99.01
Unified 86.46 93.92 90.04 99.05
Table 6. Performances of text normalization (%) 
Discussions 
Our method outperforms the independent method 
and the cascaded method in all the subtasks, espe-
cially in the subtasks that have strong dependen-
cies with each other, for example, sentence bound-
ary detection, extra punctuation mark detection, 
and case restoration. 
The cascaded method suffered from ignorance 
of the dependencies between the subtasks. For ex-
ample, there were 3,314 cases in which sentence 
boundary detection needs to use the results of extra 
line break detection, extra punctuation mark detec-
tion, and case restoration. However, in the cas-
caded method, sentence boundary detection is con-
ducted after extra punctuation mark detection and 
before case restoration, and thus it cannot leverage 
the results of case restoration. Furthermore, errors 
of extra punctuation mark detection can lead to 
errors in sentence boundary detection. 
The independent method also cannot make use 
of dependencies across different subtasks, because 
it conducts all the subtasks from the raw input data. 
This is why for detection of extra space, extra 
punctuation mark, and casing error, the independ-
ent method cannot perform as well as our method. 
Our method benefits from the ability of model-
ing dependencies between subtasks. We see from 
Table 6 that by leveraging the dependencies, our 
method can outperform the method without using 
dependencies (Unified w/o Transition Features) by 
0.62% in terms of F1-measure. 
Here we use the example in Figure 1 to show the 
advantage of our method compared with the inde-
pendent and the cascaded methods. With normali-
zation by the independent method, we obtain: 
I?m thinking about buying a pocket PC   device for my wife 
this Christmas, The worry that I have is that she won?t be able 
to sync it to her outlook express contacts.// 
With normalization by the cascaded method, we 
obtain: 
I?m thinking about buying a pocket PC device for my wife 
this Christmas, the worry that I have is that she won?t be able 
to sync it to her outlook express contacts.// 
With normalization by our method, we obtain: 
I?m thinking about buying a Pocket PC device for my wife 
this Christmas.// The worry that I have is that she won?t be 
able to sync it to her Outlook Express contacts.// 
The independent method can correctly deal with 
some of the errors. For instance, it can capitalize 
the first word in the first and the third line, remove 
extra periods in the fifth line, and remove the four 
extra line breaks. However, it mistakenly removes 
the period in the second line and it cannot restore 
the cases of some words, for example ?pocket? and 
?outlook express?. 
In the cascaded method, each process carries out 
cleaning/normalization from the output of the pre-
vious process and thus can make use of the 
cleaned/normalized results from the previous proc-
ess. However, errors in the previous processes will 
also propagate to the later processes. For example, 
the cascaded method mistakenly removes the pe-
riod in the second line. The error allows case resto-
ration to make the error of keeping the word ?the? 
in lower case. 
694
TrueCasing-based methods for case restoration 
suffer from low precision (27.32% by Independent 
and 28.04% by Cascaded), although their recalls 
are high (87.44% and 88.21% respectively). There 
are two reasons: 1) About 10% of the errors in 
Cascaded are due to errors of sentence boundary 
detection and extra line break detection in previous 
steps; 2) The two baselines tend to restore cases of 
words to the forms having higher probabilities in 
the data set and cannot take advantage of the de-
pendencies with the other normalization subtasks. 
For example, ?outlook? was restored to first letter 
capitalized in both ?Outlook Express? and ?a pleas-
ant outlook?. Our method can take advantage of the 
dependencies with other subtasks and thus correct 
85.01% of the errors that the two baseline methods 
cannot handle. Cascaded and Independent methods 
employing CRF for case restoration improve the 
accuracies somewhat. However, they are still infe-
rior to our method. 
Although we have conducted error analysis on 
the results given by our method, we omit the de-
tails here due to space limitation and will report 
them in a future expanded version of this paper. 
We also compared the speed of our method with 
those of the independent and cascaded methods. 
We tested the three methods on a computer with 
two 2.8G Dual-Core CPUs and three Gigabyte 
memory. On average, it needs about 5 hours for 
training the normalization models using our 
method and 25 seconds for tagging in the cross-
validation experiments. The independent and the 
cascaded methods (with TrueCasing) require less 
time for training (about 2 minutes and 3 minutes 
respectively) and for tagging (several seconds). 
This indicates that the efficiency of our method 
still needs improvement. 
6 Conclusion 
In this paper, we have investigated the problem of 
text normalization, an important issue for natural 
language processing. We have first defined the 
problem as a task consisting of noise elimination 
and boundary detection subtasks. We have then 
proposed a unified tagging approach to perform the 
task, specifically to treat text normalization as as-
signing tags representing deletion, preservation, or 
replacement of the tokens in the text. Experiments 
show that our approach significantly outperforms 
the two baseline methods for text normalization. 
References 
E. Brill and R. C. Moore. 2000. An Improved Error 
Model for Noisy Channel Spelling Correction, Proc. 
of ACL 2000. 
V. R. Carvalho and W. W. Cohen. 2004. Learning to 
Extract Signature and Reply Lines from Email, Proc. 
of CEAS 2004. 
K. Church and W. Gale. 1991. Probability Scoring for 
Spelling Correction, Statistics and Computing, Vol. 1. 
A. Clark. 2003. Pre-processing Very Noisy Text, Proc. 
of Workshop on Shallow Processing of Large Cor-
pora. 
A. R. Golding and D. Roth. 1996. Applying Winnow to 
Context-Sensitive Spelling Correction, Proc. of 
ICML?1996. 
J. Lafferty, A. McCallum, and F. Pereira. 2001. Condi-
tional Random Fields: Probabilistic Models for Seg-
menting and Labeling Sequence Data, Proc. of ICML 
2001. 
L. V. Lita, A. Ittycheriah, S. Roukos, and N. Kambhatla. 
2003. tRuEcasIng, Proc. of ACL 2003. 
E. Mays, F. J. Damerau, and R. L. Mercer. 1991. Con-
text Based Spelling Correction, Information Process-
ing and Management, Vol. 27, 1991. 
A. Mikheev. 2000. Document Centered Approach to 
Text Normalization, Proc. SIGIR 2000. 
A. Mikheev. 2002. Periods, Capitalized Words, etc. 
Computational Linguistics, Vol. 28, 2002. 
E. Minkov, R. C. Wang, and W. W. Cohen. 2005. Ex-
tracting Personal Names from Email: Applying 
Named Entity Recognition to Informal Text, Proc. of 
EMNLP/HLT-2005. 
D. D. Palmer and M. A. Hearst. 1997. Adaptive Multi-
lingual Sentence Boundary Disambiguation, Compu-
tational Linguistics, Vol. 23. 
C.J. van Rijsbergen. 1979. Information Retrieval. But-
terworths, London. 
R. Sproat, A. Black, S. Chen, S. Kumar, M. Ostendorf, 
and C. Richards. 1999. Normalization of non-
standard words, WS?99 Final Report. 
http://www.clsp.jhu.edu/ws99/projects/normal/. 
J. Tang, H. Li, Y. Cao, and Z. Tang. 2005. Email data 
cleaning, Proc. of SIGKDD?2005. 
V. Vapnik. 1998. Statistical Learning Theory, Springer. 
W. Wong, W. Liu, and M. Bennamoun. 2007. Enhanced 
Integrated Scoring for Cleaning Dirty Texts, Proc. of 
IJCAI-2007 Workshop on Analytics for Noisy Un-
structured Text Data. 
695
Tutorial Abstracts of ACL-IJCNLP 2009, page 5,
Suntec, Singapore, 2 August 2009.
c?2009 ACL and AFNLP
Learning to Rank
Hang Li
Microsoft Research Asia
4F Sigma Building, No 49 Zhichun Road, Haidian, Beijing China
hangli@microsoft.com
1 Introduction
In this tutorial I will introduce ?learning to rank?,
a machine learning technology on constructing a
model for ranking objects using training data. I
will first explain the problem formulation of learn-
ing to rank, and relations between learning to
rank and the other learning tasks. I will then de-
scribe learning to rank methods developed in re-
cent years, including pointwise, pairwise, and list-
wise approaches. I will then give an introduction
to the theoretical work on learning to rank and the
applications of learning to rank. Finally, I will
show some future directions of research on learn-
ing to rank. The goal of this tutorial is to give the
audience a comprehensive survey to the technol-
ogy and stimulate more research on the technol-
ogy and application of the technology to natural
language processing.
Learning to rank has been successfully applied
to information retrieval and is potentially useful
for natural language processing as well. In fact
many NLP tasks can be formalized as ranking
problems and NLP technologies may be signifi-
cantly improved by using learning to rank tech-
niques. These include question answering, sum-
marization, and machine translation. For exam-
ple, in machine translation, given a sentence in the
source language, we are to translate it to a sentence
in the target language. Usually there are multi-
ple possible translations and it would be better to
sort the possible translations in descending order
of their likelihood and output the sorted results.
Learning to rank can be employed in the task.
2 Outline
1. Introduction
2. Learning to Rank Problem
(a) Problem Formulation
(b) Evaluation
3. Learning to Rank Methods
(a) Pointwise Approach
i. McRank
(b) Pairwise Approach
i. Ranking SVM
ii. RankBoost
iii. RankNet
iv. IR SVM
(c) Listwise Approach
i. ListNet
ii. ListMLE
iii. AdaRank
iv. SVM Map
v. PermuRank
vi. SoftRank
(d) Other Methods
4. Learning to Rank Theory
(a) Pairwise Approach
i. Generalization Analysis
(b) Listwise Approach
i. Generalization Analysis
ii. Consistency Analysis
5. Learning to Rank Applications
(a) Search Ranking
(b) Collaborative Filtering
(c) Key Phrase Extraction
(d) Potential Applications in Natural Lan-
guage Processing
6. Future Directions for Learning to Rank Re-
search
7. Conclusion
5
Topic Analysis Using a Finite Mixture Model 
Hang Li and  Ken j i  Yamanish i  
NEC Corporation 
{lihang,yamanisi} @ccm.cl.nec.co.jp 
Abst rac t  
We address the issue of 'topic analysis,' by 
which is determined a text's topic structure, 
which indicates what topics are included in a 
text, and how topics change within the text. 
We propose a novel approach to this issue, one 
based on statistical modeling and learning. 
We represent topics by means of word clusters, 
and employ a finite mixture model to repre- 
sent a word distribution within a text. Our 
experimental results indicate that our method 
significantly outperforms a method that com- 
bines existing techniques. 
1 In t roduct ion  
-:We consider here the issue of 'topic analysis,' 
by which is determined a text's topic struc- 
ture, which indicates what topics are included 
in a text and how topics change within the 
text. Topic analysis consists of two main 
tasks: topic identification and text segmen- 
tation (based on topic changes). 
Topic analysis is extremely useful in a vari- 
ety of text processing applications. For exam- 
plea it can be used in the automatic indexing 
of texts for purposes of information retrieval. 
With it, one can understand what the main 
topics and subtopics of a text are, and where 
those subtopics lie within the text. 
To the best of our knowledge, however, no 
previous tudy has so far dealt with the topic 
analysis problem in the above sense. The 
most closely related are key word extraction 
and text segmentation. A keyword extrac- 
tion method (e.g., that using tf-idf (Salton 
and Yang, 1973)) generally extracts from a 
text key words which represent topics within 
the text, but it does not conduct segmenta- 
tion. A segmentation method (e.g., TextTil- 
ing (Hearst, 1997)) generally segments a text 
into blocks (paragraphs) in accord with topic 
changes within the text, but it does not iden- 
tify (or label) by itself the topics discussed in 
each of the blocks. 
The purpose of tMs paper is to provide a 
single framework for conducting topic analy- 
sis, i.e., performing both topic identification 
and text segmentation. 
The key characteristics of our framework 
are 1) representing a topic by means of a clus- 
ter of words that are closely related to the 
topic, and 2) employing a stochastic model, 
called a .finite mixture model (e.g., (Everitt 
and Hand, 1981)), to represent a word dis- 
tribution within a text. The finite mixture 
model has a hierarchical structure of probabil- 
ity distributions. The first level is a probabil- 
ity distribution of topics (topic distribution). 
The second level consists of probability distri- 
butions of words included within topics (word 
distributions). These word distributions are 
linearly combined to represent a word distri- 
bution within a text, with the topic distribu- 
tion being used as the coefficient vector. Here- 
after we refer to a finite mixture model hav- 
ing this structure as a stochastic topic model 
(STM). 
Before conducting topic analysis, we create 
word clusters (topics) on the basis of word co- 
occurrence in corpus data. We have devel- 
oped a new method for word clustering using 
stochastic omplexity (or the MDL principle) 
(Rissanen, 1996). 
In topic analysis, we estimate a sequence 
of STMs that would have given rise to a given 
text, assuming that each block of a text is gen- 
erated by an individual STM. We perform text 
segmentation by detecting significant differ- 
ences between STMs and perform topic iden- 
tification by means of estimation of STMs. 
With the results, we obtain the text's topic 
structure which consists of segmented blocks 
and their topics. 
It is possible to perform topic analysis 
by combining an existing word extraction 
method (e.g., tf-idf) and an existing text seg- 
35 
mentation method (e.g., TextTiling). Specif- 
ically, one can extract key words from a text 
using tf-idf, view these extracted key words 
as topics, segment he text into blocks us- 
ing TextTiling, and estimate the distribution 
of topics (key words) within each block. Ex- 
perimental results indicate, :however, that our 
method significantly outper~brms such a com- 
bined method in topic identification and out- 
performs it in text segmentation, because it 
utilizes word cluster information and employs 
a well-defined probability framework. 
Finite mixture models have been employed 
in a number of text processing applications, 
such as text classification (e.g., (Li and Ya- 
mauishi, 1997; Nigam et al, 2000)) and infor- 
mation retrieval (e.g., (Hofmann, 1999)). As 
will be discussed, however, our definition of a 
finite mixture model and the way we use it 
here .differs significantly. 
2 S tochast i c  Top ic  Mode l  
2.1 Topic 
While the term 'topic' is used in different ways 
in different linguistic theories, we simply view 
it here as a subject within a text. We rep- 
resent a topic by means of a cluster of words 
that are closely related to the topic, assum- 
ing that a cluster has a seed word (or several 
seed words) which indicates a topic. Figure 1 
shows an example topic with the word 'trade' 
being the seed word. 
I trade: trade export import tariff trader GATT protectionist I 
I 
Figure 1: Example topic 
2.2 Def in i t ion  o f  STM 
Let W denote a set of words, and K a set of 
topics. We first define a distribution of topics 
(clusters) P(k) : ~kEIK P(k) = 1. Then, for 
each topic k E K, we define a probability dis- 
tribution of words P(wik) : ~ ,ew P(wlk) = 
1. Here the value of P(wik) will be zeroif w is 
not included in k. We next define a Stochas- 
tic Topic Model (STM) as a finite mixture 
model, which is a linear combination of the 
word probability distributions P(w\[k), with 
the topic distribution P(k) being used as the 
coefficient vector. The probability of word w 
in W is, then, 
P(w) = ~ P(k)P(wlk ) we  W. 
kEK 
Figure 2 depicts an example STM. 
Figure 2: Example STM 
For the purposes of statistical modeling, it 
is advantageous to conceive of a text (i.e., a 
word sequence) as having been generated by 
some 'true' STMs, which we then seek to esti- 
mate as closely as possible. A text may have a 
number of blocks, and each block is assumed 
to be generated by an individual STM. The 
STMs within a text are assumed to have the 
same set of topics, but have different param- 
eter values. 
From the linguistic viewpoint, a text gener- 
ally focuses on a single main topic, but it may 
discuss different subtopics in different blocks. 
While a text is discussing any one topic, it will 
more frequently use words strongly related to 
that topic. Hence, STM is a natural represen- 
tation of statistical word occurrence based on 
topics. 
3 Word  C lus ter ing  
Before conducting topic analysis, we create 
word clusters using a large data corpus. More 
precisely, we treat all words in a vocabulary as 
seed words, and for each seed word we collect 
from the data those words which frequently 
co-occur with it and group them into a cluster. 
As one example, the word-cluster in Figure 1 
has been constructed with the word 'trade' as 
the seed word. 
We have developed a new method for reli- 
ably collecting frequently co-occurring words 
on the basis of stochastic omplexity, or the 
MDL principle. For a given data sequence 
z m = x l . . . zm and for a fixed probability 
model M, 1 the stochastic omplexity of x m 
relative to M, which we denote as SC(x m : 
M), is defined as the least code length re- 
quired to encode xrn with M (Rissanen, 1996). 
SC(x m : M) can be interpreted as the amount 
information included in x n relative to M. The 
1 Here, we use 'model' to refer to aprobabi l i ty dis- 
tnbution which has specified paxameters but unspeci- 
fied parameter values. 
36 
MDL (Minimum Description Length) princi- 
ple is a model selection criterion which asserts 
that, for a given data sequence, the lower a 
model's SC value, the greater its likelihood of 
being a model which would have actually gen- 
erated the data. MDL has many good prop- 
erties as a criterion for model selection. 2 
For a fixed seed word s, we take a word w as 
a frequently co-occurring word if the presence 
of s is a statistically significant indicator of 
the presence of w. 
Let a data sequence: (s l ,wl ) ,  (s2,w2), .-., 
(Sin,Win) be given where (si, wi) denotes the 
state of co-occurrence of words s and w in 
the i-th text in the corpus data. Here, sl E 
{1,O},wi e {1,0},(i = 1, . - . , rn) ,  1 denotes 
the presence of a word, while 0 the absence 
of it. We further denote s TM = sl . . .sm, and 
W TM ~.. W 1 ? . . W m . 
Then as in (Rissanen, 1996), the SC value of 
w TM relative to a model I in which the presence 
or absence of w is independent from those of 
s (i.e., a Bernoulli model), is calculated as 
SC(w TM : I) = mH + ~ log ~ + log 7r, 
where m + denotes the number of l 's in wm. 
Here, log denotes the logarithm to the base 
2, ~- the circular constant, and H(z) deJ 
- z logz  - (1 - z)log(1 - z), when 0 < z < 1; 
H(z) des = 0, whenz=0orz= 1. 
Let w m" be the sequence of all wi's (wi E 
w rn) such that its corresponding si is 1, where 
ms denotes the number of l 's in s ~. Let w rn'' 
be the sequence ofaU wi's (wi E w m) such that 
its corresponding si is 0, where rn.~s denotes 
the number O's in s m. The SC value of w m 
relative to a model D in which the presence 
or absence of w is dependent on those of s is 
then calculated as 
SC(w  ( s.u log ): = + ~logT~ + 
+ (m"sH (-m'-'~'~'~ W ?1?g-m-='~ W l?gr) 2~ 
where ms + denotes the number of l 's in wm', 
and w~+s the number of l 's in w m~,. 
2For an introduction to MDL,  see (Li, 1998). 
We can then calculate 
6SC = "~(SC(wm : I) - SC(wm : D)) 
\ [ ( )  m \m.~/ j  
f l  IOarn~rn- , ,~/  
-1  o j" 
(I) 
According to the MDL principle, the larger 
the 6SC value, the more likely that the pres- 
ence or absence of w is dependent on those of 
8. 3 
Actually, we may think of a word w for 
which the value of 6SC is larger than a pre- 
determined threshold 3' and P(wls ) > P(w) 
is satisfied as that which occurs significantly 
frequently with the seed word s. 
Note that the word clustering process is 
independent of topic analysis. While one 
could employ other methods (e.g., (Hofmann, 
1999)) here for word clustering, our clus- 
tering algorithm is more efficient than con- 
ventional ones. For example, Hofmann's is 
of order O(\]DIIWI2), while ours is only of 
O(ID I + \]WI2), where IDI denotes the number 
of texts and IW\] the number of words. That 
means that our method is more practical when 
a large amount of text data is available. 
4 Top ic  Ana lys i s  
4.1 Input  and  Output  
In topic analysis, we use STM to parse a 
given text and output a topic structure which 
consists of segmented blocks and their top- 
ics. Figure 3 shows an example topic struc- 
ture as output with our method. The text has 
been segmented into five blocks, and to each 
block, a number of topics having high prob- 
ability values have been assigned (topics axe 
represented by their seed words). The topic 
structure clearly represents what topics are in- 
cluded in the text and how the topics change 
within the text. 
4.2 Out l ine  
Our topic analysis consists of three processes: 
a pre-process called 'topic spotting,' text seg- 
mentation, and topic identification. In topic 
SNote that the quantity within \[---\] in (1) is (em-  
pir ical)  mutua l  inyormat ion,  which is an effective mea- 
sure for word co-occurrence calculation (cf.,(Brown et 
al., 1992)). When the sample size is small, mutual 
information values tend to be undesirably large. The 
quantity within {-..} in (1) can help avoid this unde- 
sirable tendency because its value will become large 
when data size is small. 
37 
ASIAI SXPOITERS PSAk DAEAOS Fit05 U.S.-IAPA| RIFT (25-HAE-1987) 
block 0 . . . . . . . .  t rade-expor~-car i~t- impo:rt(O,12) Japan-Japa.l~ese(O.07) U$(0.06) 
0 Sountin S t rade f r i c t ion  between the U.:3. and $opau has ra ised  fears  amen S many of l s ia*s  export ing nat ions  chat  the row could in f l i c t  . . .  
1 They to ld  Router correspondents in Asian cap i ta l s  a U.S. move aga inst  Japan might boost p ro te?t ion is t  sent iment in she U.S. ~nd lead to . . .  
2 But some exporters  said Chat while the conf l i c t  would hurt  them in the lens- run,  in the shor t - term Tokyo's loss  might be the i r  ga in .  
3 The U.S. Xas said i t  s i l l  ~apose 300 ~tn d l rs  of ta r i f f s  on imports of Japanese e lec t ron ics  seeds on Apr i l  17. in re ta l ia t ion  fo r  Japa~*s . . .  
4 Unof f i c ia l  Japanese ost~Jnates put the impact of the ta r i f f s  a t  10 b i l l i on  d l ro  and spokesmen fo r  major e lec t ron ics  1irma said they would . . .  
5 "go wouldn't be able to do bus iness , "  Isaid a spokesman fo r  l .od in  S ;apanese e lec t ron ics  ~irm Satanoh i ta  E lec t r i c  Indust r ia l  ?o Lad t l t .  
6 " I f  the ta r i f f s  remain in p lace for  any length of time beyond a ~eg months i t  s i l l  ~an the complete eros ion of experts  (o~ good8 subject  . . .  
block I . . . . . . . .  trade-export-ta~vif~-Impo:rt(O. lT)  US(O.Og) Taiwan(O.05) dlrs(O.O$) 
T In Taigan. businessmen and o f f i c ia l s  ~re a lso  worried. 
$ "We i re  agLre of the ser iousness  ot the U.5. th reat  aga ins t  Japan because i t  serves as a warning to o | , "  sa id  ? senior  Taiganese t rade  . . .  
g Taiu&n had z t rade t rade surplus of 15~6 b i l l i on  d i re  las t  year? gS pot of i t  u i tb  the U.$. 
10 The surplus helped sge l l  7aiwan's fo re ign  exchange reserves  to 53 b i l l i on  d l r s .  ninon S the wor ld 's  la rgest .  
11 "Re must quickly open our markets,  remove t rade bar r ie rs  and cut Import ta r i f f s  to al low imports o~ U.S. p red ic ts ,  i f  ue want to de~nse . . .  
12 I sen ior  o f f i c i L1  ef South \ [o rea 's  tr~Lde promotion assoc ia t ion  said the t rade  d ispute between the U.S. and Japan might a lso  lead to . . .  
13 L i s t  year South |u rea  had a t rade surplus ef 7.1 b i l l i on  dlro u i th  the U.S . .  np ~ron t .9  b i l l i on  d l r s  in 1985. 
1~ In Ha lays ia .  erode o f f i cers  and businessmen said ~ou~h curbs aga ins t  Japan might a l len  hard -h i t  producers o~ anuLicondnctors in th i rd  . . .  
block 2 . . . . . . . .  Hong-|en$(0.16) t rado-expor t - ta~i f f - imper t (O .  10) U5(0.06) 
15 In Hung long, where nauspaporo have a l leged  Japan has been na i l ing  ba ler -cos t  semiconductors, some e lec t ron icsmanu~acturnrn  share . . .  
16 "That i s  a very shor t - te rm v ies . "  sa id Lawrn~ce R i l l s ,  d i rec tor -genera l  o~ the Federat ion of Hung Eerie Industry .  
17 "I~ the uhole purpose i s  te  prevent imports ,  one day i t  g i l l  be extended to other  sources.  Hush more ser ious  fo r  Hsng Ions i s  the . . .  
18 The U.S. las t  year gas Hon K Eong's h iogest  expert  market, accounting fo r  ever 30 pot of domest ica l ly  produced exports .  
block 3 . . . . . . . .  t rade-expor t - ta r i f f - impor t (0 .14)  Button(O.08) ~apan-lapaneoe(O.07) 
19 ~ho Aust ra l ian  government is  ana i t ing  the outcome of t rade ta lks  botmean the U.S. and Japan u i tb  in teres t  and concern, Indust ry  . . .  
20 *'1his kind o~ deter io ra t ion  in t rade re la t ions  between sue countr ies  nhich &r~majer  t rad ing  par tners  of ours i s  a very . . .  
21 He said los t ra l ia*s  concerns centred en coal and beef,  Anst rn l ia :8  tee la rses t  exports to Japan and a l so  s ign i f i cant  U.S . . . .  
22 Heanwhile U.S.- JapanaSe "diplomatic manoeuvmes to solve the trade s tand-o f f  cont inue.  
block 4 . . . . . . . .  Japan-Japanese(O,12) measure(O.06) t rade-expor t - ta r i f f - i~por t (O .O5)  
23 Japan's  ru l ing  L ibera l  Democratic Party  yesterday out l ined a package of economic measuru8 to boost the ~apananu $csnony. 
24 The Measures proposed include ? lapse supplementary budget and record publ ic  works spending in the f i r so  ha l f  of ohe f inanc ia l  year .  
25 \]hey a lso ca l l  gor stepped-up spending as an emergency measure to s t imu late  the economy danpi te  Prime S in i s ter  Yasuhiro HaJ~asome . . .  
26 Deputy U.S. Trade kepreanutagive 5 ichae l  Sunth and H~koto ln r rda ,  Japan's  deputy min is ter  of In ternat iona l  Trade ~nd Zndustry (BZTZ) . . . .  
0-26; sentence id 
( . . ) :  p robab i l i ty  value 
Figure 3: Topic structure of text 
spotting, we select opics discussed in a given 
text. We can then construct STMs on the 
basis of the topics. In text segmentation, we 
segment the text on the basis of the STMs, 
assuming that each block is generated by an 
individual STM. In topic identification, we es- 
timate the parameters of the STM for each 
segmented block and select topics with high 
probabilities for the block. In this way, we 
obtain a topic structure for the text. 
4.3 Topic Spott ing 
In topic spotting, we first select key words 
from a given text. We calculate what we call 
the Shannon i formation of each word in the 
text. The Shannon information of word w in 
text t is defined as 
I(w) = -N(w)logP(w), 
where N(w) denotes the frequency of w in t, 
and P(w) the probability of the occurrence of
w as estimated from corpus data. I(w) may 
be interpreted as the amount of information 
represented by w. We select as key words the 
top I words sorted in descending order of I. 
While Shannon information is similar to 
the tf-idf widely used in information retrieval 
(e.g., (Salton and Yang, 1973)), the use of 
Shannon information can be justified on the 
basis of information theory, but that of tf-idf 
cannot. Our preliminary experimental results 
indicate that Shannon information performs 
better than or at least as well as tf-idf in key 
word extraction. 4 
From the results of word clustering, we next 
select any cluster (topic) whose seed word is 
included among the selected key words. 
We next merge any two clusters if one of 
their seed words is included in the other's clus- 
ter. For example, when a cluster with seed 
word 'trade' contains the word 'import,' and 
a cluster with seed word 'import' contains the 
word 'trade,' we merge the two. After two 
such merges, we may obtain a relatively large 
cluster with, for example, ~trade-import-tariff- 
export' as its seed words, as is shown in Fig- 
ure 3. Figure 4 shows the merging algorithm. 
In this way, we obtain the most conspicuous 
and mutually independent topics discussed in 
a given text. 
4.4 Text Segmentat ion  
In segmentation, we first identify candidates 
for points of segmentation within the given 
text. When we assume a relatively short text 
~We will discuss it in the full version of the paper. 
38 
kl, ? ? ? ,  kn: clusters, 
V = {{ki},i = 1,2, . . . ,n}.  
For each cluster pair (ki, kj), if the seed 
word of ki is included in kj and the seed 
word of kj is included in ki, then push 
(ki, kj) into queue Q; 
while (Q # 0) { 
Remove the first element (kl, kj) from Q; 
if (kl and kj belong to different sets 
W1,W2 in V) 
Replace W1 and W2 in V with 
w~ u w2 ; 
} 
For each element W of V, merge the 
clusters in it. 
Figure 4: Algorithm: merge 
for the purposes of our explanation here, all 
sentence-ending periods will be candidates. 
For each candidate, we create two pseudo- 
texts, one consisting of the h sentences pre- 
ceding it, and the other of the h sentences 
following it (when fewer than h exist in any 
..:direction, we simply use those which do exist). 
We use the EM algorithm ((Dempster et al, 
1977), cL, Figure 5) to separately estimate the 
parameters of an STM from each of the two 
pseudo texts. It is theoretically guaranteed 
that the EM algorithm converges to a local 
maximum of the likelihood. We next calculate 
the similarity (i.e., essentially the converse no- 
tion of distance s) between the STM based 
on the preceding pseudo-text, and the STM 
based on the following pseudo-text. These 
STMs axe denoted, respectively, as PL(W) and 
PR(w). The similarity between PL(W) and 
PR(w) is defined as 
S(LI\[R) = 1 - E~w \ [PL (w)  - PR(w)\[ 
2 
The numerator is referred to in statistics as 
variational distance and has good properties 
as a distance between two probability dis- 
tributions (cf., (Cover and Thomas, 1991), 
p.299). 
Figure 7 shows a graph of calculated simi- 
laxity values for each of the candidates in the 
5We use similarity rather than distance here in or- 
der to simplify comparison between our method and 
TextTiling (Hearst, 1997). 
s: predetermined number. 
For the lth iteration (I = 1,- . . ,  s), 
we calculate 
PU)(k)PU)(wlk) 
P(Z+l)(klw) = Ek~P(')(k)P(')(wlk) 
p(l+l)(k) = N(w)PU+l)(klw) 
N 
P(Z+l)(w\]k) = N(w)P(l+l)(k\[ w)
~wew g(w)P(~+ l )(k\[w) 
N(w) denotes the frequency of word w 
in the data; N = ~ew N(w). 
Figure 5: EM algorithm 
n: number of segmentation candidates, 
S(i) i(i = 0. . .  n): similarity score. 
for (i = 1;i < n -  1;i + +){ 
if (S(i - 1) > S(i) & S(i + 1) > S(i)){ 
j= i -1 ;  
while (j > 0 & S(j - 1) > S(j)) 
j - - ;  
P1 = S(j); 
j= i+ l; 
while(j  < n & S(j + 1) > S(j)) 
j++;  
P2 = S(j); 
i f (P1 - S( i )  > ~ & P2-  S( i )  > 8) 
Conduct segmentation at i. 
}) 
Figure 6: Algorithm: segment 
text shown in Figure 3. 'Valleys' (i.e., low- 
similarity values) in the graph suggest points 
for reasonable segmentations. In actual prac- 
tice, segmentation is performed for each valley 
whose similarity values is lower to a predeter- 
mined degree 0 than each of the values of its 
left 'peak' and right 'peak' (cf., Figure 6) For 
example, for the text in Figure 3, segmenta- 
tion was performed at candidates (i.e., end of 
sentences) 6, 14, 18, and 22, with 8 = 0.05. 
4.5 Topic Ident i f icat ion 
After segmentation, we separately estimate 
the parameters of the STM for each block, 
again using the EM algorithm, and obtain 
a topic (cluster) probability distribution for 
each block. We then choose those topics (dus- 
ters) in each block having.high probability val- 
ues. In this way, we construct a topic struc- 
39 
0.35 
0.3 
0.25 
0.2 
0.15 
0.1 
0.05 
% ; 
' ' "STM" ~ '  t 
i = ,o 15 2'o 2'5 
sentence nurn~,er 
Figure 7: Similarity values for segmentation 
candidates 
ture as in Figure 3 for the given text (topics 
are here represented by their seed words). 
We can view topics appearing in all the 
blocks as main topics, and topics appearing 
only in individual blocks as subtopics. In 
the text in Figure 3, the topic represented 
by seed-words 'trade-export-tariff-import' is 
the main topic, and 'Japan-Japanese,' 'Hong 
Kong,' etc., are subtopics. 
5 App l i ca t ions  
Our method can be used in a variety of text 
processing applications. 
For example, given a collection of texts 
(e.g., home pages), we can automatically con- 
struct an index of the texts on the basis of the 
extracted topics. We can indicate which topic 
is from which text or even which block of a 
text. Furthermore, we can indicate which top- 
ics are main topics of texts and which topics 
are subtopics (e.g., by displaying main topics 
in boldface, etc). In this way, users can get a 
fair sense of the contents of the texts simply 
by looking through the index. For a specific 
text, users can get a rough sense of the con- 
tent by looking at the topic structure as, for 
example, it is shown in Figure 3. 
Our method can also be useful for text min- 
ing, text summarization, information extrac- 
tion, and other text processing, which require 
one to first analyze the structure of a text. 
6 Re la ted  Work  
To the best of our knowledge, no previous 
study has so far dealt with topic identification 
and text segmentation within a single frame- 
work. 
A widely used method for key word extrac- 
tion calculates the tf-idf value of each word in 
a text and uses those words having the largest 
tf-idf values as key words for that text (e.g., 
(Salton and Yang, 1973)). One can view these 
extracted key words as the topics of the text. 
No keyword extraction method by itself, how- 
ever, is able to conduct segmentation. 
With respect o text segmentation, exist- 
ing methods can be classified into two groups. 
One is to divide a text into blocks (e.g., 
TextTiling (Hearst, 1997)), the other to di- 
vide a stream of texts into its original texts 
(e.g.,(Allan et al, 1998; Yamron et al, 1998; 
Beeferman et al, 1999; tteynar, 1999)). The 
former group generally employs unsupervised 
learning, while the latter supervised one. No 
existing segmentation method, however, has 
attempted topic identification. 
TextTiling creates for each segmentation 
candidate two pseudo-texts, one preceding it
and the other following it, and calculates as 
similarity the cosine value between the word 
frequency vectors of the two pseudo texts. It 
then conducts egmentation at valley points 
in a similar way to that of our method. Since 
the problem setting of TextTiling (in general 
the former group) is most close to that of our 
study, we use TextTiling for comparison i our 
experiments. 
Our method by its nature performs topic 
identification and segmentation within a sin- 
gle framework. While it is possible with a 
combination of existing methods to extract 
key words from a given text by using tf-idf, 
view the extracted key words as topics, seg- 
ment the text into blocks by employing Text- 
Tiling, estimate distribution of topics in each 
block, and identify topics having high prob- 
abilities in each block. Our method outper: 
forms such a combination (referred to here- 
after as 'Corn') for topic identification, be- 
cause it utilizes word duster information. It 
also performs better than Com in text seg- 
mentation because it is based on a well-defined 
probability framework. Most importantly is 
that our method is able to output an easily 
understandable topic structure, which has not 
been proposed so far. 
Note that topic analysis is different from 
text classification (e.g., (Lewis et al, 1996; Li 
and Yamanishi, 1999; Joachims, 1998; Weiss 
et al, 1999; Nigam et al, 2000)). While text 
classification uses a number of pre-determined 
categories, topic analysis includes no notion 
of category. The output of topic analysis is a 
topic structure, while the output o f  text clas- 
40 
sification is a label representing a category. 
Furthermore, text classification is generally 
based on supervised learning, which uses la- 
beled text data 6. By way of contrast, topic 
analysis is based on unsupervised learning, 
which uses only unlabeled text data. 
Finite mixture models have been used in 
a variety of applications in text processing 
(e.g., (Li and Yamanishi, 1997; Nigam et al, 
2000; Hofmann, 1999)), indicating that they 
are essential to text processing. We should 
note, however, that their definitions and the 
ways they use them axe different from those 
for STM in this paper. For example, Li and 
Yamanishi propose to employ in text classi- 
fication a mixture model (Li and Yamanishi, 
1997) defined over categories: 
P(WIC) = ~ P(klc)P(wlk),w e W,c e C, 
kEK 
where W denotes a set of words, and C a 
set of categories. In their framework, a new 
text d is assigned into a category c* such that 
c* = argmaxeee P(c\]d) is satisfied. I-Iofmann 
proposes using in information retrieval a joint 
distribution which he calls 'an aspect model,' 
.Aefined as (Hofmann, 1999) 
P(w,d) = P(d)P(wld) 
= P(d) EkeK P(kld)P(wlk), 
wEW,  dED 
where D denotes a set of texts. Furthermore, 
he proposes extracting in retrieval those texts 
whose estimated word distributions P(w\[d) 
are similar to the word distribution of a query. 
7 Exper imenta l  Resu l t s  
We have evaluated the performance of our 
topic analysis method (STM) in terms of three 
aspects: topic structure adequacy, text seg- 
mentation accuracy, and topic identification 
accuracy. 
7.1 Data  Set 
We know of no data available for the pur- 
pose of evaluation of topic analysis. We thus 
utilized Reuters news articles referred to as 
'Reuters-21578,' which has been widely used 
in text classification v. We used a prepared 
SAn exception is the method proposed in (McCal- 
lure and Nigam, 1999), which, instead of labeled texts, 
uses  unlabeled texts, pre-determined categories, and 
keywords defined by humans for each category. 
rAvailable at http://www.reseaxch.att.com/lewis/. 
split of the data 'Apte split,' which consists 
of 9603 texts for training and 3299 texts for 
test. All of the texts had already been classi- 
fied into 90 categories by human subjects. 
For each text, we used the Oxford Learner's 
Dictionary s to conduct stemming, and re- 
moved 'stop words' (e.g., 'the,' 'and') that we 
had included on a previously prepared list. 
The average length of a text was about 115 
words. (We did not use phrases, however, 
which would further improve xperimental re- 
sults.) 
7.2 Word  C luster ing  
We conducted word clustering with 9603 
training texts. 7340 individual words had a 
total frequency of more than 5, and we used 
them as seeds with which to collect frequently 
co-occurring words. The threshold for clus- 
tering 7 was set at 0.005, and this yielded 
970 word clusters having more than one word 
(i.e., not simply containing a seed word alone). 
Note that the category labels of the training 
texts need not be used in clustering. 
We next conducted a topic analysis on all 
the 3299 texts. The thresholds of l, h, and 0 
were set at 20, 3, and 0.05, respectively, on 
the basis of preliminary experimental results. 
7.3 Topic S t ruc ture  
We looked at the topic structures of the 3299 
texts obtained by our method to determine 
how well they conformed to human intuition. 
For topic identification i  this experiment, 
clusters in each block were sorted in descend- 
ing order of their probabilities, and the top 
7 seed words were extracted to represent the 
topics of the block. 
Figure 3 show results for the text with ID 
14826; they generally agree well with human 
intuition. The text has been segmented into 
5 blocks and the topics of each block is rep- 
resented by 7 seed words. The main topic is 
represented by the seed-words 'trade-export- 
tariff-import.' The subtopics are represented 
by 'Japan-Japanese,' 'Taiwan,' Hong Kong,' 
etc. There were, however, a small number 
of errors. For example, the text should also 
have been segmented after sentences 11 and 
13, but, due to limited sentence content, it was 
not. Furthermore, assigning subtopic of 'But- 
ton' (from 'Mr. Button') into block 3 (due 
to the high Shannon information value of the 
word 'Button') was also undesirable. 
SAvailable at  ftp://sable.ox.ac.uk. 
41 
Table 1:10 categories and their identification 
words 
category 
earn  
acq 
money-fx 
grain 
crude 
trade 
interest 
ship 
wheat 
corn 
identification vcords 
earning, share, profit, dividend 
acquisition, acquire, sell, buy 
currency, dollar, yen, stg 
grain, cereal, crop 
oil, crude, gas 
trade, export, import, tariff 
interest & rate 
ship, vessel, ferry, tanker 
wheat 
cori1, maize 
7.4 Main Topic Identi f icat ion 
We conducted an evaluation to determine 
whether or not the main topics in the topic 
structures obtained for the 3299 test texts 
could be approximately matched with the la- 
bels (categories) assigned to the test texts. 
Note that here labels are used only for eval- 
uation, not for training. This is in contrast 
to the situation in most text classification ex- 
periments, in which labels are generally used 
both for training and for evaluation. It is not 
particularly meaningful, then, to compare the 
results for main topic identification obtained 
here with those for text classification. 
With STM, clusters in each block were 
sorted in descending order of their probabil- 
ities, and the top k seed words were extracted 
to represent the topics of the block. Further- 
more, a seed word appearing in all the blocks 
of the text was considered to represent a main 
topic. When a text had not been segmented 
(i.e., has only one block), all top k seed words 
were considered to represent main topics. 
Table 1 lists the largest 10 categories in the 
Reuters data. On the basis of the definition of 
each of the 10 categories, we assigned based on 
our intuition to each of them the identification 
words that are listed in Table 1. 
For the evaluation, when the seed words for 
main topics contained at least one of the iden- 
tification words, we considered our method to 
have identified the corresponding main topic 
equivalent to a human-determined category. 
We then evaluated these in terms of preci- 
sion and recall. Here, precision is defined as 
the ratio of the number of decisions correctly 
made to the total number of decisions made. 
Recall is defined as the ratio of the mrmber of 
decisions correctly made to the total number 
Table 2: Main topic identification results with 
respect o 7 top words 
category 
earn 
acq 
money-fx 
grain 
crude 
trade 
interest 
ship 
wheat 
corn 
STM 
rec. pre. 
Com 
rec. pre. 
0.790 0.971 
0.245 0.854 
0.436 0.456 
0.322 0.750 
0.487 0.676 
0.667 0.473 
0.107 0.700 
0.247 0.957 
0.620 0.936 
0.429 0.960 
0.526 0.976 
0.184 0.841 
0.285 0.421 
0.174 0.650 
0.407 0.664 
0.590 0.356 
0.084 0.733 
0.270 0.828 
0.408 0.967 
0.446 1.00 
micro-average 0.515 0.824 0.365 0.774 
Table 3: Main topic identification results with 
respect o 5 top words 
category 
earn 
acq 
money-fx 
grain 
crude 
trade 
interest 
ship 
wheat 
corn 
micro-average 
STM 
rec. pre. 
0.742 0.971 
0.184 0.868 
0.413 0.503 
0.295 0.759 
0.471 0.718 
0.479 0.505 
0.053 0.700 
0.169 1.000 
0.577 0.953 
0.357 0.952 
0.461 0.850 
Corn 
rec. pre. 
0.348 0.977 
0.120 0.869 
0.268 0.471 
0.121 0.600 
0.333 0.656 
0.513 0.403 
0.069 0.818 
0.180 0.762 
0.282 0.952 
0.321 1.000 
0.257 0.767 
of decisions which should have been correctly 
made. 
We also looked at the performance of Corn 
(cf., Section 6). For Corn, we extracted from a 
text the key words with the 20 largest Shan- 
non information values, segmented the text 
using TextTiling, and extracted in each block 
the key words having the largest k probabil- 
ity values. Any key word extracted in all 
blocks was considered to represent a main 
topic. When the key words for main top- 
ics contained at least one of the identification 
words, we viewed that text as having the cor- 
responding main topic. 
Table 2 shows the results achieved with 
STM and Corn in the case of k ~-- 7. 9 Table 3 
9For the definition of micro-averaging, see, for ex- 
42 
Tit le:  gOYPY BUYS PL 480 RHSAT FLOU! - U.S. YRADSItS 
"Body: ggyp1 bought 125,723 sonnet o~ U.S. shoat ~lour in i t s  PL 
480 tender yesterdxy, trade ooQrceo sa id .  The purchase inc luded 
$t,880 tonnes ~er  Say shipn~nt ~d 73,843 tennes for  June sb ipnent .  
P r i ce  deta i l s  gere not available. 
Content Words (Freq.): tone(S) shipment(2) buy(t) deta i l ( I )  
ggypt(1) t lour ( l )  include(I) June(l) PL(I) price(t) purchase(l) 
source( l )  trade(l) US(l) 9heat(l) 
Icy Bordo (ShLn. Ing.):  tonne(17.5) ohi~4nent(1S.3) PL(IO.6) flour(9.8) 
Sgypt(9.3) detail(7.S) Juno(7.2) uheat(6.8) purchas?(6.6) source(S.S) 
U$(6.1) buy(6.0) inclnde(6.O) trade(B.3) price (S.l) 
Con Yopics (Prob.): tonn?(O.17) shipnent(O.l l) price(O.06) June(O.O6) 
in?lude(O.00) purcbaoe(O.06) source(O.O6) 
BIB Iopico (Prob.) : ilour~sheat(O.IB) tonn4(0.12) shipmont(O.tl) 
purchaoe-buy(O.tl) Egypt(O.O6) 
Cluster :  (S ieur-wheat:  ghent tonne ~lour) 
(purchase-buy: purchase bny) 
Figure 8: Topic Identification Example 
shows the results in the case of k = 5. The 
comparison may be considered fair in that it 
requires each of the two methods to provide 
the same number of words to represent top- 
ics. Results indicate that STM significantly 
outperforms Corn, particularly in terms of re- 
call. 
The main reason for the higher performance 
achieved by STM is that it utilizes word clus- 
ter information. Figure 8 shows topic analysis 
results for the text with ID 15572 labeled with 
'wheat.' The text contains only 15 content 
words (word types), thus all of the 15 words 
were extracted as key words and the text was 
not segmented by either method. Corn was 
unable to identify the main topic 'wheat,' be- 
cause the probability of each of the relevant 
key words 'wheat' and 'flour' was low. In 
contrast, STM successfully identified the topic 
because the relevant key words were classified 
into the same cluster, and its probability was 
relatively high. 
7.5 Segmentat ion  and Subtop ic  
Ident i f icat ion 
We collected the 50 longest test texts (re- 
ferred to here as 'seed texts') from each of the 
10 categories, and combined each with a test 
text randomly selected from other categories 
to produce 500 pseudo-texts. Placement of 
the seed text within its pseudo-text (i.e., be- 
fore or after the other text) was determined 
randomly. 
We used both STM and Corn to segment 
each of the pseudo-texts into two blocks and 
identify subtopics. Table 4 shows the segmen- 
tation results for the two method evaluated 
ample, (Lewis and Ringnette, 1994). 
Table 5: Subtopic identification results 
category of 
seed text 
eaxn 
acq 
money-fx 
grain 
crude 
trade 
interest 
ship 
wheat 
corn  
Average 
STM 
rec. pre. 
0.430 0.945 
0.237 0.939 
0.585 0.950 
0.276 0.947 
0.572 0.979 
0.634 0.951 
0.211 0.937 
0.260 1.000 
0.500 0.970 
0.317 1.000 
Corn 
rec. pre. 
0.324 0.973 
0.217 0.959 
0.533 0.961 
0.222 0.938 
0.557 O.990 
0.627 0.899 
0.136 1.000 
0.340 0.994 
0.395 0.980 
0.441 0.882 
0.402 0.962 0.379 0.958 
in terms of recall, precision, and error prob- 
ability. Table 5 shows the results of subtopic 
identification as evaluated in terms of recall 
and precision. Error probability is a metric 
for evaluating segmentation results proposed 
in (Allan et ai., 1998; Beeferman etal. ,  1999). 
It is defined here as the probability that a ran- 
domly chosen pair of sentences a distance of k 
sentence apart is incorrectly segmented. 1? 
Experimental results indicate that STM 
outperforms Corn in both segmentation and 
identification, n 
8 Conc lus ions  
We have proposed a new method of topic 
analysis that employs a finite mixture model, 
referred to here as a stochastic topic model 
(STM). 
Topic analysis consists of two main tasks: 
text segmentation and topic identification. 
With topic analysis, one can obtain a topic 
structure for a text. 
Our method addresses topic analysis within 
a single framework. It has the following novel 
features: 1) it represents topics by means of 
word dusters and employs a finite mixture 
model (STM) to represent a word distribution 
within a text; 2) it constructs topics on the 
basis of corpus data before conducting topic 
analysis; 3) it segments a text by detecting 
significant differences between STMs; and 4) 
it identifies topics by estimating parameters 
1?Here, k was set to 5 because the average length of 
a text was about 10 sentences. .... 
l lWe will discuss the results in the full version of 
the paper. 
43 
Table 4: Text segmentation results 
category of 
seed text 
earn 
acq 
money-fx 
grain 
crude 
trade 
interest 
ship 
wheat 
corn 
Average 
STM 
0.660 
0.820 0.820 0.059 
0.700 0.700 0.087 
0.700 0.700 0.074 
0.860 0.860 0.051 
0.800 0.800 0.072 
0.760 0.760 0.119 
0.837 0.854 0.074 
0.760 0.760 0.075 
0.625 0.625 0.147 
Corn 
rec. pre. err. rec. pre. err. 
0.660 0.167 0.640 0.640 0.171 
0.740 0.740 0.085 
0.660 0.660 0.121 
0.660 0.660 0.076 
0.820 0.820 0.066 
0.800 0.800 0.081 
0.820 0.820 0.084 
0.816 0.833 0.084 
0.640 0.640 0.130 
0.650 0.650 0.105 
0.725 0.726 0.100 0.752 0.754 0.092 
of STMs. 
Experimental results indicate that our 
method outperforms a method that combines 
existing techniques. More specifically, it sig- 
nificantly outperforms the combined method 
in topic identification. 
Re ferences  
J. Allan, J. Carbonell, G. Doddington, J. Yam- 
ron, and Y. Yang. 1998. Topic detection and 
tracking pilot study: Final report. Proc. of the 
DARPA Broadcast News Transcription and Un- 
derstanding Workshop, pages 194-218. 
D. Beeferman, A. Berger, and J. Lafferty. 
1999. Statistical models for text segmentation. 
Machi. Lrn., 34:177-210. 
P. F. Brown, V. J. Della Pietra, P. V. deSouza, 
J. C. Lai, and R. L. Mercer. 1992. Class-based 
n-gram models of natural language. Comp. 
Ling., 18(4):283-298. 
T. M. Cover and J. A. Thomas. 1991. Elements of 
Information Theory. John Wiley & Sons Inc., 
New York. 
A.P. Dempster, N.M. Laird, and D.B. Rubin. 
1977. Maximum likelihood from incomplete 
data via the em algorithm. Journ. of Roy. Star. 
Soci., Ser. B, 39(1):1-38. 
B. Everitt and D. Hand. 1981. Finite MiNute Dis- 
tribntions. Chapman and Hall. 
M. Hearst. 1997. Texttiling: Segmenting text 
into multi-paragraph subtopic passages. Comp. 
Ling., 23(1):33-64. 
Thomas Hofmann. 1999. Probabilistic latent se- 
mantic indexing. Proc. of SIGIR '99, pages 50- 
57. 
T. Joachirns. 1998. Text categorization with sup- 
port vector machines: Learning with many rel- 
evant features. Proc. of ECML '98. 
D. D. Lewis and M. Ringuette. 1994. A compar- 
ison of two learning algorithms for test catego- 
rization. Proc. of 3rd Ann. Syrup. on Doc. Ana. 
and Info. Retr., pages 81-93. 
D. D. Lewis, R. E. Schapire, J. P. Callan, and 
R. Papka. 1996. Training algorithms for linear 
text classifiers. Proc. of SIGIR'96. 
H. Li and K. Yamanishi. 1997. Document classi- 
fication using a finite mixture model. Proc. of 
A CL '97, pages 39-47. 
H. Li and K. Yamanishi. 1999. Text classification 
using ESC-based stochastic decision lists. Proc. 
of ACM-CIKM'99, pages 122-130. 
H. Li. 1998. A Probabilistic Approach to Lezical 
Semantic Knowledge Acquisition and Structural 
Disambignation. Ph.D. Thesis, Univ. of Tokyo. 
A. K. McCallum and K. Nigam. 1999. Text clas- 
sification by bootstrapping with keywords, em 
and shrinkage. Proc. of ACL'g9 Workshop Un- 
supervised Learning in NLP. 
K. Nigarn, A. K. McCallum, S. Thrun, and 
T. Mitchell. 2000. Text classification from 
labeled and unlabeled documents using era. 
Maehi. Lrn., 39:103-134. 
J. C. Reynar. 1999. Statistical models for topic 
segmentation. Proc. of ACL '99, pages 357-364. 
J. Rissanen. 1996. Fisher information and 
stochastic omplexity. 1EEE Trans. on Info. 
Thry., 42(1):40-47. 
G. Salton and C.S. Yang. 1973. On the speci- 
fication of term values in automatic indexing. 
Journ. of Doc., 29(4):351-372. 
S. M. Weiss, C. Apte, F. Damerau, F. J. Oles, 
T. Goers, and T. Hampp. 1999. Maximiz- 
ing text-mining performance. IEEE Intel. Sys., 
14(4):63-69. 
J.P. Yamron, I. Carp, L. Gillick, S. Lowe, and 
P. van Mulbregt. 1998. A Hidden Markov 
Model approach to text segmentation a d event 
tracking. Proc. of ICASSP'99, pages 333-336. 
44 
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 935?945,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
A Dataset for Research on Short-Text Conversation ?
Hao Wang? Zhengdong Lu? Hang Li? Enhong Chen?
? xdwangh@mail.ustc.edu.cn ?lu.zhengdong@huawei.com
?hangli.hl@huawei.com ?cheneh@ustc.edu.cn
?Univ. of Sci & Tech of China, China ?Noah?s Ark Lab, Huawei Technologies, Hong Kong
Abstract
Natural language conversation is widely re-
garded as a highly difficult problem, which
is usually attacked with either rule-based or
learning-based models. In this paper we
propose a retrieval-based automatic response
model for short-text conversation, to exploit
the vast amount of short conversation in-
stances available on social media. For this
purpose we introduce a dataset of short-text
conversation based on the real-world instances
from Sina Weibo (a popular Chinese mi-
croblog service), which will be soon released
to public. This dataset provides rich collec-
tion of instances for the research on finding
natural and relevant short responses to a given
short text, and useful for both training and test-
ing of conversation models. This dataset con-
sists of both naturally formed conversation-
s, manually labeled data, and a large repos-
itory of candidate responses. Our prelimi-
nary experiments demonstrate that the simple
retrieval-based conversation model performs
reasonably well when combined with the rich
instances in our dataset.
1 Introduction
Natural language conversation is one of the holy
grail of artificial intelligence, and has been taken as
the original form of the celebrated Turing test. Pre-
vious effort in this direction has largely focused on
analyzing the text and modeling the state of the con-
versation through dialogue models, while in this pa-
?The work is done when the first author worked as intern at
Noah?s Ark Lab, Huawei Techologies.
per we take one step back and focus on a much easi-
er task of finding the response for a given short text.
This task is in clear contrast with previous effort in
dialogue modeling in the following two aspects
? we do not consider the context or history of
conversations, and assume that the given short
text is self-contained;
? we only require the response to be natural, rel-
evant, and human-like, and do not require it to
contain particular opinion, content, or to be of
particular style.
This task is much simpler than modeling a complete
dialogue session (e.g., as proposed in Turing test),
and probably not enough for real conversation sce-
nario which requires often several rounds of interac-
tions (e.g., automatic question answering system as
in (Litman et al, 2000)). However it can shed impor-
tant light on understanding the complicated mecha-
nism of the interaction between an utterance and it-
s response. The research in this direction will not
only instantly help the applications of short session
dialogue such as automatic message replying on mo-
bile phone and the chatbot employed in voice assis-
tant like Siri1, but also it will eventually benefit the
modeling of dialogues in a more general setting.
Previous effort in modeling lengthy dialogues fo-
cused either on rule-based or learning-based models
(Carpenter, 1997; Litman et al, 2000; Williams and
Young, 2007; Schatzmann et al, 2006; Misu et al,
2012). This category of approaches require relative-
ly less data (e.g. reinforcement learning based) for
1http://en.wikipedia.org/wiki/Siri
935
training or no training at all, but much manual ef-
fort in designing the rules or the particular learning
algorithms. In this paper, we propose to attack this
problem using an alternative approach, by leverag-
ing the vast amount of training data available from
the social media. Similar ideas have appeared in (Ja-
farpour and Burges, 2010; Leuski and Traum, 2011)
as an initial step for training a chatbot.
With the emergence of social media, especially
microblogs such as Twitter, in the past decade, they
have become an important form of communication
for many people. As the result, it has collected con-
versation history with volume previously unthink-
able, which brings opportunity for attacking the con-
versation problem from a whole new angle. More
specifically, instead of generating a response to an
utterance, we pick a massive suitable one from the
candidate set. The hope is, with a reasonable re-
trieval model and a large enough candidate set, the
system can produce fairly natural and appropriate re-
sponses.
This retrieval-based model is somewhat like non-
parametric model in machine learning communities,
which performs well only when we have abundan-
t data. In our model, it needs only a relatively s-
mall labeled dataset for training the retrieval model,
but requires a rather large unlabeled set (e.g., one
million instances) for candidate responses. To fur-
ther promote the research in similar direction, we
create a dataset for training and testing the retrieval
model, with a candidate responses set of reason-
able size. Sina Weibo is the most popular Twitter-
like microblog service in China, hosting over 500
million registered users and generating over 100
million messages per day 2. As almost all mi-
croblog services, Sina Weibo allows users to com-
ment on a published post3, which forms a natural
one-round conversation. Due to the great abundance
of those (post, response) pairs, it provides an ideal
data source and test bed for one-round conversation.
We will make this dataset publicly available in the
near future.
2http://en.wikipedia.org/wiki/Sina_Weibo
3Actually it also allows users to comment on other users?
comments, but we will not consider that in the dataset.
2 The Dialogues on Sina Weibo
Sina Weibo is a Twitter-like microblog service, on
which a user can publish short messages (will be re-
ferred to as post in the remainder of the paper) visi-
ble to public or a group specified by the user. Simi-
lar to Twitter, Sina Weibo has the word limit of 140
Chinese characters. Other users can comment on a
published post, with the same length limit, as shown
in the real example given in Figure 6 (in Chinese).
Those comments will be referred to as responses in
the remainder of the paper.
Figure 1: An example of Sina Weibo post and the com-
ments it received.
We argue that the (post, response) pairs on Sina
Weibo provide rather valuable resource for studying
one round dialogue between users. The comments
to a post can be of rather flexible forms and diverse
topics, as illustrated in the example in Table 1. With
a post stating the user?s status (traveling to Hawaii),
the comments can be of quite different styles and
contents, but apparently all appropriate.
In many cases, the (post, response) pair is self-
contained, which means one does not need any back-
ground and contextual information to get the main
point of the conversation (Examples of that include
the responses from B, D, G and H). In some cas-
es, one may need extra knowledge to understand the
conversation. For example, the response from user
E will be fairly elusive if taken out of the context
that A?s Hawaii trip is for an international confer-
ence and he is going to give a talk there. We argue
that the number of self-contained (post, response)
pairs is vast, and therefore the extracted (post, re-
936
Post
User A: The first day at Hawaii. Watching sunset at the balcony with a big glass of wine in hand.
Responses
User B: Enjoy it & don?t forget to share your photos!
User C: Please take me with you next time!
User D: How long are you going to stay there?
User E: When will be your talk?
User F: Haha, I am doing the same thing right now. Which hotel are you staying in?
User G: Stop showing-off, buddy. We are still coding crazily right now in the lab.
User H: Lucky you! Our flight to Honolulu is delayed and I am stuck in the airport. Chewing French
fries in MacDonald?s right now.
Table 1: A typical example of Sina Weibo post and the comments it received. The original text is in Chinese, and we
translated it into English for easy access of readers. We did the same thing for all the examples throughout this paper.
sponse) pairs can serve as a rich resource for ex-
ploring rather sophisticated patterns and structures
in natural language conversation.
3 Content of the Dataset
The dataset consists of three parts, as illustrated in
Figure 2. Part 1 contains the original (post, re-
sponse) pairs, indicated by the dark-grey section in
Figure 2. Part 2, indicated by the light-gray section
in Figure 2, consists labeled (post, response) pairs
for some Weibo posts, including positive and nega-
tive examples. Part 3 collects all the responses, in-
cluding but not limited to the responses in Part 1 and
2. Some of the basic statistics are summarized in
Table 2.
# posts # responses vocab. # labeled pairs
4,6345 1,534,874 105,732 12,427
Table 2: Some statistics of the dataset
Original (Post, Response) Pairs This part of
dataset gives (post, response) pairs naturally pre-
sented in the microblog service. In other words,
we create a (post, response) pair there when the re-
sponse is actually given to the post in Sina Weibo.
The part of data is noisy since the responses given
to a Weibo post could still be inappropriate for d-
ifferent reasons, for example, they could be spams
or targeting some responses given earlier. We have
628, 833 pairs.
Labeled Pairs This part of data contains the (post,
response) pairs that are labeled by human. Note that
1) the labeling is only on a small subset of posts,
and 2) for each selected post, the labeled responses
are not originally given to it. The labeling is done
in an active manner (see Section 4 for more detail-
s), so the obtained labels are much more informative
than the those on randomly selected pairs (over 98%
of which are negative). This part of data can be di-
rectly used for training and testing of retrieval-based
response models. We have labeled 422 posts and for
each of them, about 30 candidate responses.
Responses This part of dataset contains only re-
sponses, but they are not necessarily for a certain
post. These extra responses are mainly filtered out
by our data cleaning strategy (see Section 4.2) for
original (post, response) pairs, including those from
filtered-out Weibo posts and those addressing oth-
er responses. Nevertheless, those responses are still
valid candidate for responses. We have about 1.5
million responses in the dataset.
3.1 Using the Dataset for Retrieval-based
Response Models
Our data can be used for training and testing of
retrieval-based response model, or just as a bank of
responses. More specifically, it can be used in at
least the following three ways.
Training Low-level Matching Features The
rather abundant original (post, response) pairs pro-
vide rather rich supervision signal for learning dif-
ferent matching patterns between a post and a re-
sponse. These matching patterns could be of dif-
937
Figure 2: Content of the dataset.
ferent levels. For example, one may discover from
the data that when the word ?Hawaii? occurs in the
post, the response are more likely to contain word-
s like ?trip?, ?flight?, or ?Honolulu?. On a slight-
ly more abstract level, one may learn that when an
entity name is mentioned in the post, it tends to be
mentioned again in the response. More complicated
matching pattern could also be learned. For exam-
ple, the response to a post asking ?how to? is statisti-
cally longer than average responses. As a particular
case, Ritter et al (2011) applied translation model
(Brown et al, 1993) on similar parallel data extract-
ed from Twitter in order to extract the word-to-word
correlation. Please note that with more sophisticat-
ed natural language processing, we can go beyond
bag-of-words for more complicated correspondence
between post and response.
Training Automatic Response Models Although
the original (post, response) pairs are rather abun-
dant, they are not enough for discriminative training
and testing of retrieval models, for the following rea-
sons. In the labeled pairs, both positive and negative
ones are ranked high by some baseline models, and
hence more difficult to tell apart. This supervision
will naturally tune the model parameters to find the
real good responses from the seemingly good ones.
Please note that without the labeled negative pairs,
we need to generate negative pairs with randomly
chosen responses, which in most of the cases are too
easy to differentiate by the ranking model and can-
not fully tune the model parameters. This intuition
has been empirically verified by our experiments.
Testing Automatic Response Models In testing a
retrieval-based system, although we can simply use
the original responses associated with the query post
as positive and treat all the others as negative, this
strategy suffers from the problem of spurious neg-
ative examples. In other words, with a reasonably
good model, the retrieved responses are often good
even if they are not the original ones, which brings
significant bias to the evaluation. With the labeled
pairs, this problem can be solved if we limit the test-
ing only in the small pool of labeled responses.
3.2 Using the Dataset for Other Purposes
Our dataset can also be used for other researches re-
lated to short-text conversations, namely anaphora
resolution, sentiment analysis, and speech act anal-
ysis, based on the large collection of original (post,
response) pairs. For example, to determine the sen-
timent of a response, one needs to consider both
the original post as well as the observed interaction
between the two. In Figure 3, if we want to un-
derstand user?s sentiment towards the ?invited talk?
mentioned in the post, the two responses should be
taken as positive, although the sentiment in the mere
responses is either negative or neutral.
4 Creation of the Dataset
The (post, comment) pairs are sampled from the
Sina Weibo posts published by users in a loosely
connected community and the comments they re-
ceived (may not be from this community). This
community is mainly posed of professors, re-
searchers, and students of natural language process-
ing (NLP) and related areas in China, and the users
938
Figure 3: An example (original Chinese and the English
translation) on the difficulty of sentiment analysis on re-
sponses.
commonly followed them.
The creation process of the dataset, as illustrated
in Figure 4, consists of three consecutive steps: 1)
crawling the community of users, 2) crawling their
Weibo posts and their responses, 3) cleaning the da-
ta, with more details described in the remainder of
this section.
4.1 Sampling Strategy
We take the following sampling strategy for collect-
ing the (post, response) pairs to make the topic rel-
atively focused. We first locate 3,200 users from a
loosely connected community of Natural Language
Processing (NLP) and Machine Learning (ML) in
China. This is done through crawling followees4 of
ten manually selected seed users who are NLP re-
searchers active on Sina Weibo (with no less than 2
posts per day on average) and popular enough (with
no less than 100 followers).
We crawl the posts and the responses they re-
ceived (not necessarily from the crawled communi-
ty) for two months (from April 5th, 2013, to June
5th, 2013). The topics are relatively limited due to
our choice of the users, with the most saliently ones
being:
? Research: discussion on research ideas, paper-
s, books, tutorials, conferences, and researchers
in NLP and machine learning, etc;
? General Arts and Science: mathematics,
physics, biology, music, painting, etc;
4When user A follows user B, A is called B?s follower, and
B is called A?s followee.
? IT Technology: Mobile phones, IT companies,
jobs opportunities, etc;
? Life: traveling (both touring or conference trip-
s), food, photography, etc.
4.2 Processing, Filtering, and Data Cleaning
On the crawled posts and responses, we first perform
a four-step filtering on the post and responses
? We first remove the Weibo posts and their re-
sponses if the length of post is less than 10 Chi-
nese characters or the length of the response is
less than 5 characters. The reason for that is
two-fold: 1) if the text is too short, it can bare-
ly contain information that can be reliably cap-
tured, e.g. the following example
P: Three down, two to go.
and 2) some of the posts or responses are too
general to be interesting for other cases, e.g. the
response in the example below,
P: Nice restaurant. I?d strong recommend it.
Everything here is good except the long
waiting line
R: wow.
? In the remained posts, we only keep the first
100 responses in the original (post, response)
pairs, since we observe that after the first 100
responses there will be a non-negligible propor-
tion of responses addressing things other than
the original Weibo post (e.g., the responses giv-
en earlier). We however will still keep the re-
sponses in the bank of responses.
? The last step is to filter out the potential adver-
tisements. We will find the long responses that
have been posted more than twice on different
posts and scrub them out of both original (post,
response) pairs and the response repository.
For the remained posts and responses, we remove
the punctuation marks and emoticons, and use ICT-
CLAS (Zhang et al, 2003) for Chinese word seg-
mentation.
939
Figure 4: Diagram of the process for creating the dataset.
4.3 Labeling
We employ a pooling strategy widely used in in-
formation retrieval for getting the instance to label
(Voorhees, 2002). More specifically, for a given
post, we use three baseline retrieval models to each
select 10 responses (see Section 5 for the descrip-
tion of the baselines), and merge them to form a
much reduced candidate set with size ? 30. Then
we label the reduced candidate set into ?suitable?
and ?unsuitable? categories. Basically we consider
a response suitable for a given post if we cannot tell
whether it is an original response. More specifically
the suitability of a response is judged based on the
following three criteria5:
Semantic Relevance: This requires the content of
the response to be semantically relevant to the post.
As shown in the example right below, the post P is
about soccer, and so is response R1 (hence seman-
tically relevant), whereas response R2 is about food
(hence semantically irrelevant).
P: There are always 8 English players in their
own penalty area. Unbelievable!
R1: Haha, it is still 0:0, no goal so far.
R2: The food in England is horrible.
Another important aspect of semantic relevance is
the entity association. This requires the entities in
the response to be correctly aligned with those in
the post. In other words, if the post is about entity
5Note that although our criteria in general favor short and
general answers like ?Well said!? or ?Nice?, most of these gen-
eral answers have already been filtered out due to their length
(see Section 4.2).
A, while the response is about entity B, they are very
likely to be mismatched. As shown in the following
example, where the original post is about Paris, and
the response R2 talks about London:
P: It is my last day in Paris. So hard to say
goodbye.
R1: Enjoy your time in Paris.
R2: Man, I wish I am in London right now.
This is however not absolute, since a response con-
taining a different entity could still be sound, as
demonstrated by the following two responses to the
post above
R1: Enjoy your time in France.
R2: The fall of London is nice too.
Logic Consistency: This requires the content of
the response to be logically consistent with the post.
For example, in the table right below, post P states
that the Huawei mobile phone ?Honor? is already in
the market of mainland China. Response R1 talk-
s about a personal preference over the same phone
model (hence logically consistent), whereas R2 asks
the question the answer to which is already clear
from P (hence logically inconsistent).
P: HUAWEI?s mobile phone, Honor, sells
well in Chinese Mainland.
R1: HUAWEI Honor is my favorite phone
R2: When will HUAWEI Honor get to the
market in mainland China?
Speech Act Alignment: Another important factor
in determining the suitability of a response is the
940
speech act. For example, when a question is posed in
the Weibo post, a certain act (e.g., answering or for-
warding it) is expected. In the example below, post
P asks a special question about location. Response
R1 and R2 either forwards or answers the question,
whereas R3 is a negative sentence and therefore does
not align well in speech act.
P: Any one knows where KDD will be held the
year after next?
R1: co-ask. Hopefully Europe
R2: New York, as I heard
R3: No, it is still in New York City
5 Retrieval-based Response Model
In a retrieval-based response model, for a given post
x we pick from the candidate set the response with
the highest ranking score, where the score is the en-
semble of several individual matching features
score(x, y) =
?
i??
wi?i(x, y). (1)
with y stands for a candidate response.
We perform a two-stage retrieval to handle the s-
calability associated with the massive candidate set,
as illustrated in Figure 5. In Stage I, the system em-
ploys several fast baseline matching models to re-
trieve a number of candidate responses for the giv-
en post x, forming a much reduced candidate set
C(reduced)x . In Stage II, the system uses a ranking
function with more and sophisticated features to fur-
ther evaluate all the responses in C(reduced)x , return-
ing a matching score for each response. Our re-
sponse model then decides whether to respond and
which candidate response to choose.
In Stage II, we use the linear score function de-
fined in Equation 1 with 15 features, trained with
RankSVM (Joachims, 2002). The training and test-
ing are both performed on the 422 labeled posts,
with about 12,000 labeled (post, response) pairs. We
use a 5-fold cross validation with a fixed penalty pa-
rameter for slack variable. 6
5.1 Baseline Matching Models
We use the following matching models as the base-
line model for Stage I fast retrieval. Moreover, the
6The performance is fairly insensitive to the choice of the
penalty, so we only report the result with a typical choice of it.
matching features used in the ranking function in
Stage II are generated, directly or indirectly, from
the those matching models:
POST-RESPONSE SEMANTIC MATCHING:
This particular matching function relies on a learned
mapping from the original sparse representation for
text to a low-dimensional but dense representation
for both Weibo posts and responses. The level of
matching score between a post and a response can
be measured as the inner product between their
images in the low-dimensional space
SemMatch(x, y) = x>LXL>Yy. (2)
where x and y are respectively the 1-in-N represen-
tations of x and y. This is to capture the seman-
tic matching between a Weibo post and a response,
which may not be well captured by a word-by-word
matching. More specifically, we find LX and LY
through a large margin variant of (Wu et al, 2013)
arg minLX ,LY
?
i
max(1?
?
i
x>i LXL
>
Yyi, 0)
s.t. ?Ln,X ?1 ? ?1, n = 1, 2, ? ? ? , Nx
?Lm,Y?1 ? ?1, m = 1, 2, ? ? ? , Ny
?Ln,X ?2 = ?2, n = 1, 2, ? ? ? , Nx
?Lm,Y?2 = ?2m = 1, 2, ? ? ? , Ny.
where i indices the original (post, response) pairs.
Our experiments (Section 6) indicate that this sim-
ple linear model can learn meaningful patterns, due
to the massive training set. For example, the im-
age of the word ?Italy? in the post in the latent s-
pace matches well word ?Sicily?, ?Mediterranean
sea? and ?travel?. Once the mapping LX and LY
are learned, the semantic matching score x>LXL>Yy
will be treated as a feature for modeling the overall
suitability of y as a response to post x.
POST-RESPONSE SIMILARITY: Here we use a
simple vector-space model for measuring the simi-
larity between a post and a response
simPR(x,y) =
x>y
?x??y?
. (3)
Although it is not necessarily true that a good re-
sponse has many common words as the post, but this
measurement is often helpful in finding relevant re-
sponses. For example, when the post and response
941
Figure 5: Diagram of the retrieval-based automatic response system.
both have ?National Palace Museum in Taipei?, it
is a strong signal that they are about similar topic-
s. Unlike the semantic matching feature, this simple
similarity requires no learning and works on infre-
quent words. Our empirical results show that it can
often capture the Post-Response relation failed with
semantic matching feature.
POST-POST SIMILARITY: The basic idea here is
to find posts similar to x and use their responses as
the candidates. Again we use the vector space model
for measuring the post-post similarity
simPP (x, x?) =
x>x?
?x??x??
. (4)
The intuition here is that if a post x? is similar to x its
responses might be appropriate for x. It however of-
ten fails, especially when a response to x? addresses
parts of x not contained by x, which fortunately can
be alleviated when combined with other measures.
5.2 Learning to Rank with Labeled Data
With all the matching features, we can learn a rank-
ing model with the labeled (post, response) pairs,
e.g., through off-the-shelf ranking algorithms. From
the labeled data, we can extract triples (x, y+, y?)
to ensure that score(x, y+) > score(x, y?). Appar-
ently y+ can be selected from labeled positive re-
sponse of x, while y? can be sampled either from
labeled negative negative or randomly selected ones.
Since the manually labeled negative instances are
top-ranked candidates according to some individual
retrieval model (see Section 5.1) and therefore gen-
erally yield slightly better results.
The matching features are mostly constructed by
combining the individual matching models, for ex-
ample the following two
? ?7(x, y): this feature measures the length of
the longest common string in the post and the
response;
? ?12(x, y): this feature considers both seman-
tic matching score between query post x and
candidate response y, as well as the similarity
between x and y?s original post x?:
?12(x, y) = SemMatch(x, y)simPP (x, x?).
In addition to the matching features, we also have
simple features describing responses only, such as
the length of it.
6 Experimental Evaluation
We perform experiments on the proposed dataset to
test our retrieval-based model as an algorithm for au-
tomatically generating response.
6.1 Performance of Models
We evaluate the retrieved models based on the fol-
lowing two metrics:
MAP This one measures the mean average preci-
sion (MAP)(Manning et al, 2008) associated
with the ranked list on C(reduced)x .
P@1 This one simply measures the precision of the
top one response in the ranked list:
P@1 =
#good top-1 responses
#posts
942
We perform a 5-fold cross-validation on the 422 la-
beled posts, with the results reported in Table 1. As
it shows, the semantic matching helps slightly im-
prove the overall performance on P@1.
Model MAP P@1
P2R 0.565 0.489
P2R + P2P 0.621 0.567
P2R + MATCH 0.575 0.513
P2R + P2P + MATCH 0.621 0.574
Table 3: Comparison of different choices of features,
where P2R stands for the features based on post-response
similarity, P2P stands for the features based on post-post
similarity, and MATCH stands for the semantic match fea-
ture.
To mimic a more realistic scenario on automatic
response model on Sina Weibo, we allow the system
to choose which post to respond to. Here we simply
set the response algorithm to respond only when the
highest score of the candidate response passes a cer-
tain threshold. Our experiments show that when we
choose to respond only to 50% of the posts, the P@1
increases to 0.76, while if the system only respond
to 25% of the posts, P@1 keeps increasing to 81%.
6.2 Case Study
Although our preliminary retrieval model does not
consider more complicated syntax, it is still able to
capture some useful coupling structure between the
appropriate (post, response) pairs, as well as the sim-
ilar (post, post) pairs.
Figure 6: An actual instance (the original Chinese text
and its English translation) of response returned by our
retrieval-based system.
Case study shows that our retrieval is fairly ef-
fective at capturing the semantic relevance (Section
6.2.1), but relative weak on modeling the logic con-
sistency (Section 6.2.2). Also it is clear that the se-
mantic matching feature (described in Section 5.1)
helps find matched responses that do not share any
words with the post (Section 6.2.3).
6.2.1 On Semantic Relevance
The features employed in our retrieval model are
mostly vector-space based, which are fairly good at
capturing the semantic relevance, as illustrated by
Example 1 & 2.
EXAMPLE 1:
P: It is a small town on an Spanish with 500
population, and guess what, they even
have a casino!
R: If you travel to Spain, you need to spend
some time there.
EXAMPLE 2:
P: One quote from Benjamin Franklin: ?We
are all born ignorant, but one must
work hard to remain stupid.?
R: Benjamin Franklin is a wise man, and
one of the founding fathers of USA.
However our retrieval model also makes bad
choice, especially when either the query post or the
response is long, as shown in Example 3. Here the
response is picked up because 1) the correspondence
between the word ?IT? in the post and the word
?mobile phone? in the candidate, and 2) the Chinese
word for ?lay off? in the post and the word for ?out-
dated? in the response are the same.
EXAMPLE 3:
P: As to the laying-off, I haven?t heard anything
about it. ?Elimination of the least competent?
is kind-off conventional in IT, but the ratio is
actually quite small.
R: Please don?t speak that way, otherwise you can
get outdated. Mobile phones are very expensive
when they were just out, but now they are fairly
cheap. Look forward, or you will be outdated.
The entity association is only partially addressed
with features like post-response cosine similarity,
treating entity name just as a word, which is appar-
ently not enough for preventing the following type
943
of mistakes (see Example 4 & 5) when the post and
response match well on other parts
EXAMPLE 4:
P: Professor Wang will give a curse on natural
language processing, starting next semester.
R: Jealous.. I wish I can attend Prof. Li?s
course too some time in the future.
EXAMPLE 5:
P: The fine China from Exhibition at the National
Palace Museum in Taipei
R: This drawing looks so nice. National Palace
Museum in Taipei is full of national treasures
6.2.2 On Logic Consistency
Our current model does not explicitly maintain
the logic consistency between the response and the
post, since Logic consistency requires a deeper anal-
ysis of the text, and therefore hard to capture with
just a vector space model. Below are two examples
which are semantically relevant, and correct with re-
spect to speech act, but logically inappropriate.
EXAMPLE 1:
P: I checked. Wang Fengyi is not my great grand-
father, although they?ve done similar deeds
and both were called ?Wang the Well-doer?.
R: wow, Wang Fengyi is your great grand-father
EXAMPLE 2:
P: We are looking for summer interns. We provide
books and lunch. If you are in Wu Han and
interested, drop us an email. Sorry we don?t
take any students outside Wu Han.
R: Are you looking for summer intern?
6.2.3 The Effect of Semantic Matching
The experiments also show that we may find inter-
esting and appropriate responses that have no com-
mon words as the post, as shown in the example be-
low. Our bi-linear semantic matching model how-
ever performs relatively poorly on long posts, where
the topics of the sentence cannot be well captured
by the sum of the latent vectors associated with each
word.
P: Eight England players stand in the penalty
area.
R1: What a classic match
R2: Haha, it is still 0:0, no goal so far
7 Summary
In this paper we propose a retrieval-based response
model for short-text based conversation, to leverage
the massive instances collected from social media.
For research in similar directions, we create a dataset
based on the posts and comments from Sina Weibo.
Our preliminary experiments show that our retrieval-
based response model, when combined with a large
candidate set, can achieve fairly good performance.
This dataset will be valuable for both training and
testing automatic response models for short texts.
References
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The mathematic-
s of statistical machine translation: parameter estima-
tion. Comput. Linguist., 19(2).
Rollo Carpenter. 1997. Cleverbot.
Sina Jafarpour and Christopher J. C. Burges. 2010. Fil-
ter, rank, and transfer the knowledge: Learning to chat.
Thorsten Joachims. 2002. Optimizing search engines
using clickthrough data. In Proceedings of the eighth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, KDD ?02, pages 133?
142, New York, NY, USA. ACM.
Anton Leuski and David R. Traum. 2011. Npceditor:
Creating virtual human dialogue using information re-
trieval techniques. AI Magazine, 32(2):42?56.
Diane Litman, Satinder Singh, Michael Kearns, and Mar-
ilyn Walker. 2000. Njfun: a reinforcement learning
spoken dialogue system. In Proceedings of the 2000
ANLP/NAACL Workshop on Conversational systems -
Volume 3, ANLP/NAACL-ConvSyst ?00, pages 17?
20, Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Christopher D. Manning, Prabhakar Raghavan, and Hin-
rich Schu?tze. 2008. Introduction to Information Re-
trieval. Cambridge University Press, New York, NY,
USA.
Teruhisa Misu, Kallirroi Georgila, Anton Leuski, and
David Traum. 2012. Reinforcement learning of
question-answering dialogue policies for virtual muse-
um guides. In Proceedings of the 13th Annual Meeting
944
of the Special Interest Group on Discourse and Dia-
logue, SIGDIAL ?12, pages 84?93.
Alan Ritter, Colin Cherry, and William B. Dolan. 2011.
Data-driven response generation in social media. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ?11,
pages 583?593, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Jost Schatzmann, Karl Weilhammer, Matt Stuttle, and
Steve Young. 2006. A survey of statistical user sim-
ulation techniques for reinforcement-learning of dia-
logue management strategies. Knowl. Eng. Rev., pages
97?126.
Ellen M Voorhees. 2002. The philosophy of infor-
mation retrieval evaluation. In Evaluation of cross-
language information retrieval systems, pages 355?
370. Springer.
Jason D. Williams and Steve Young. 2007. Partially ob-
servable markov decision processes for spoken dialog
systems. Comput. Speech Lang., 21(2):393?422.
Wei Wu, Zhengdong Lu, and Hang Li. 2013. Learning
bilinear model for matching queries and documents.
Journal of Machine Learning Research (2013 to ap-
pear).
Hua-Ping Zhang, Hong-Kui Yu, De-Yi Xiong, and Qun
Liu. 2003. Hhmm-based chinese lexical analyzer ict-
clas. SIGHAN ?03.
945
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 52?61,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
A Fast and Accurate Method for Approximate String Search
Ziqi Wang?
School of EECS
Peking University
Beijing 100871, China
wangziqi@pku.edu.cn
Gu Xu
Microsoft Research Asia
Building 2, No.5 Danling Street,
Beijing 100080, China
guxu@microsoft.com
Hang Li
Microsoft Research Asia
Building 2, No.5 Danling Street,
Beijing 100080, China
hangli@microsoft.com
Ming Zhang
School of EECS
Peking University
Beijing 100871, China
mzhang@net.pku.edu.cn
Abstract
This paper proposes a new method for ap-
proximate string search, specifically candidate
generation in spelling error correction, which
is a task as follows. Given a misspelled word,
the system finds words in a dictionary, which
are most ?similar? to the misspelled word.
The paper proposes a probabilistic approach to
the task, which is both accurate and efficient.
The approach includes the use of a log linear
model, a method for training the model, and
an algorithm for finding the top k candidates.
The log linear model is defined as a condi-
tional probability distribution of a corrected
word and a rule set for the correction con-
ditioned on the misspelled word. The learn-
ing method employs the criterion in candidate
generation as loss function. The retrieval al-
gorithm is efficient and is guaranteed to find
the optimal k candidates. Experimental re-
sults on large scale data show that the pro-
posed approach improves upon existing meth-
ods in terms of accuracy in different settings.
1 Introduction
This paper addresses the following problem, re-
ferred to as approximate string search. Given a
query string, a dictionary of strings (vocabulary),
and a set of operators, the system returns the top
k strings in the dictionary that can be transformed
from the query string by applying several operators
in the operator set. Here each operator is a rule
that can replace a substring in the query string with
another substring. The top k results are defined in
? Contribution during internship at Microsoft Research Asia.
terms of an evaluation measure employed in a spe-
cific application. The requirement is that the task
must be conducted very efficiently.
Approximate string search is useful in many ap-
plications including spelling error correction, sim-
ilar terminology retrieval, duplicate detection, etc.
Although certain progress has been made for ad-
dressing the problem, further investigation on the
task is still necessary, particularly from the view-
point of enhancing both accuracy and efficiency.
Without loss of generality, in this paper we ad-
dress candidate generation in spelling error correc-
tion. Candidate generation is to find the most pos-
sible corrections of a misspelled word. In such a
problem, strings are words, and the operators rep-
resent insertion, deletion, and substitution of char-
acters with or without surrounding characters, for
example, ?a???e? and ?lly???ly?. Note that can-
didate generation is concerned with a single word;
after candidate generation, the words surrounding it
in the text can be further leveraged to make the final
candidate selection, e.g., Li et al (2006), Golding
and Roth (1999).
In spelling error correction, Brill and Moore
(2000) proposed employing a generative model for
candidate generation and a hierarchy of trie struc-
tures for fast candidate retrieval. Our approach is
a discriminative approach and is aimed at improv-
ing Brill and Moore?s method. Okazaki et al (2008)
proposed using a logistic regression model for ap-
proximate dictionary matching. Their method is also
a discriminative approach, but it is largely differ-
ent from our approach in the following points. It
formalizes the problem as binary classification and
52
assumes that there is only one rule applicable each
time in candidate generation. Efficiency is also not a
major concern for them, because it is for offline text
mining.
There are two fundamental problems in research
on approximate string search: (1) how to build a
model that can archive both high accuracy and ef-
ficiency, and (2) how to develop a data structure and
algorithm that can facilitate efficient retrieval of the
top k candidates.
In this paper, we propose a probabilistic approach
to the task. Our approach is novel and unique in
the following aspects. It employs (a) a log-linear
(discriminative) model for candidate generation, (b)
an effective algorithm for model learning, and (c) an
efficient algorithm for candidate retrieval.
The log linear model is defined as a conditional
probability distribution of a corrected word and a
rule set for the correction given the misspelled word.
The learning method employs, in the training pro-
cess, a criterion that represents the goal of mak-
ing both accurate and efficient prediction (candidate
generation). As a result, the model is optimally
trained toward its objective. The retrieval algorithm
uses special data structures and efficiently performs
the top k candidates finding. It is guaranteed to find
the best k candidates without enumerating all the
possible ones.
We empirically evaluated the proposed method in
spelling error correction of web search queries. The
experimental results have verified that the accuracy
of the top candidates given by our method is signifi-
cantly higher than those given by the baseline meth-
ods. Our method is more accurate than the baseline
methods in different settings such as large rule sets
and large vocabulary sizes. The efficiency of our
method is also very high in different experimental
settings.
2 Related Work
Approximate string search has been studied by many
researchers. Previous work mainly focused on effi-
ciency rather than model. Usually, it is assumed that
the model (similarity distance) is fixed and the goal
is to efficiently find all the strings in the collection
whose similarity distances are within a threshold.
Most existing methods employ n-gram based algo-
rithms (Behm et al, 2009; Li et al, 2007; Yang et
al., 2008) or filtering algorithms (Mihov and Schulz,
2004; Li et al, 2008). Instead of finding all the can-
didates in a fixed range, methods for finding the top
k candidates have also been developed. For exam-
ple, the method by Vernica and Li (2009) utilized
n-gram based inverted lists as index structure and
a similarity function based on n-gram overlaps and
word frequencies. Yang et al (2010) presented a
general framework for top k retrieval based on n-
grams. In contrast, our work in this paper aims to
learn a ranking function which can achieve both high
accuracy and efficiency.
Spelling error correction normally consists of
candidate generation and candidate final selection.
The former task is an example of approximate string
search. Note that candidate generation is only con-
cerned with a single word. For single-word candi-
date generation, rule-based approach is commonly
used. The use of edit distance is a typical exam-
ple, which exploits operations of character deletion,
insertion and substitution. Some methods generate
candidates within a fixed range of edit distance or
different ranges for strings with different lengths (Li
et al, 2006; Whitelaw et al, 2009). Other meth-
ods make use of weighted edit distance to enhance
the representation power of edit distance (Ristad and
Yianilos, 1998; Oncina and Sebban, 2005; McCal-
lum et al, 2005; Ahmad and Kondrak, 2005).
Conventional edit distance does not take in con-
sideration context information. For example, peo-
ple tend to misspell ?c? to ?s? or ?k? depending
on contexts, and a straightforward application of
edit distance cannot deal with the problem. To ad-
dress the challenge, some researchers proposed us-
ing a large number of substitution rules containing
context information (at character level). For exam-
ple, Brill and Moore (2000) developed a genera-
tive model including contextual substitution rules;
and Toutanova and Moore (2002) further improved
the model by adding pronunciation factors into
the model. Schaback and Li (2007) proposed a
multi-level feature-based framework for spelling er-
ror correction including a modification of Brill and
Moore?s model (2000). Okazaki et al (2008) uti-
lized substring substitution rules and incorporated
the rules into a L1-regularized logistic regression
model. Okazaki et al?s model is largely different
53
from the model proposed in this paper, although
both of them are discriminative models. Their model
is a binary classification model and it is assumed that
only a single rule is applied in candidate generation.
Since users? behavior of misspelling and correc-
tion can be frequently observed in web search log
data, it has been proposed to mine spelling-error
and correction pairs by using search log data. The
mined pairs can be directly used in spelling error
correction. Methods of selecting spelling and cor-
rection pairs with maximum entropy model (Chen et
al., 2007) or similarity functions (Islam and Inkpen,
2009; Jones et al, 2006) have been developed. The
mined pairs can only be used in candidate genera-
tion of high frequency typos, however. In this paper,
we work on candidate generation at the character
level, which can be applied to spelling error correc-
tion for both high and low frequency words.
3 Model for Candidate Generation
As an example of approximate string search, we
consider candidate generation in spelling correction.
Suppose that there is a vocabulary V and a mis-
spelled word, the objective of candidate generation
is to select the best corrections from the vocabulary
V . We care about both accuracy and efficiency of the
process. The problem is very challenging when the
size of vocabulary is large, because there are a large
number of potential candidates to be verified.
In this paper, we propose a probabilistic approach
to candidate generation, which can achieve both
high accuracy and efficiency, and is particularly
powerful when the scale is large.
In our approach, it is assumed that a large num-
ber of misspelled words and their best corrections
are given as training data. A probabilistic model is
then trained by using the training data, which can
assign ranking scores to candidates. The best can-
didates for correction of a misspelled word are thus
defined as those candidates having the highest prob-
abilistic scores with respect to the training data and
the operators.
Hereafter, we will describe the probabilistic
model for candidate generation, as well as training
and exploitation of the model.
n i c o s o o f t^ $
Derived rules 
Edit-distance based aligment 
Expended rules with context 
m i c r o s o f t^ $
Figure 1: Example of rule extraction from word pair
3.1 Model
The operators (rules) represent insertion, deletion,
and substitution of characters in a word with or
without surrounding context (characters), which are
similar to those defined in (Brill and Moore, 2000;
Okazaki et al, 2008). An operator is formally rep-
resented a rule ? ? ? that replaces a substring ? in
a misspelled word with ?, where ?, ? ? {s|s =
t, s = ?t, or s = t$} and t ? ?? is the set of
all possible strings over the alphabet. Obviously,
V ? ??. We actually derive all the possible rules
from the training data using a similar approach to
(Brill and Moore, 2000) as shown in Fig. 1. First
we conduct the letter alignment based on the min-
imum edit-distance, and then derive the rules from
the alignment. Furthermore we expand the derived
rules with surrounding words. Without loss of gen-
erality, we only consider using +2,+1, 0,?1,?2
characters as contexts in this paper.
If we can apply a set of rules to transform the mis-
spelled word wm to a correct word wc in the vocab-
ulary, then we call the rule set a ?transformation?
for the word pair wm and wc. Note that for a given
word pair, it is likely that there are multiple possible
transformations for it. For example, both ?n???m?
and ?ni???mi? can transform ?nicrosoft? to ?mi-
crosoft?.
Without loss of generality, we set the maximum
number of rules applicable to a word pair to be a
fixed number. As a result, the number of possible
transformations for a word pair is finite, and usually
limited. This is equivalent to the assumption that the
number of spelling errors in a word is small.
Given word pair (wm, wc), let R(wm, wc) denote
one transformation (a set of rules) that can rewrite
54
wm to wc. We consider that there is a probabilistic
mapping between the misspelled word wm and cor-
rect word wc plus transformation R(wm, wc). We
define the conditional probability distribution of wc
andR(wm, wc) givenwm as the following log linear
model:
P (wc, R(wm, wc)|wm) (1)
=
exp
(
?
r?R(wm,wc) ?r
)
?
(w?c,R(wm,w?c))?Z(wm) exp
(
?
o?R(wm,w?c) ?o
)
where r or o denotes a rule in rule setR, ?r or ?o de-
notes a weight, and the normalization is carried over
Z(wm), all pairs of word w?c in V and transforma-
tion R(wm, w?c), such that wm can be transformed
to w?c by R(wm, w?c). The log linear model actually
uses binary features indicating whether or not a rule
is applied.
In general, the weights in Equ. (1) can be any real
numbers. To improve efficiency in retrieval, we fur-
ther assume that all the weights are non-positive, i.e.,
??r ? 0. It introduces monotonicity in rule applica-
tion and implies that applying additional rules can-
not lead to generation of better candidates. For ex-
ample, both ?office? and ?officer? are correct candi-
dates of ?ofice?. We view ?office? a better candidate
(with higher probability) than ?officer?, as it needs
one less rule. The assumption is reasonable because
the chance of making more errors should be lower
than that of making less errors. Our experimental
results have shown that the change in accuracy by
making the assumption is negligible, but the gain in
efficiency is very large.
3.2 Training of Model
Training data is given as a set of pairs T =
{
(wim, wic)
}N
i=1, where w
i
m is a misspelled word and
wic ? V is a correction ofwim. The objective of train-
ing would be to maximize the conditional probabil-
ity P (wic, R(wim, wic)|wim) over the training data.
This is not a trivial problem, however, because
the ?true? transformationR?(wim, wic) for each word
pair wim and wic is not given in the training data. It is
often the case that there are multiple transformations
applicable, and it is not realistic to assume that such
information can be provided by humans or automat-
ically derived. (It is relatively easy to automatically
find the pairs wim and wic as explained in Section
5.1).
In this paper, we assume that the transformation
that actually generates the correction among all the
possible transformations is the one that can give the
maximum conditional probability; the exactly same
criterion is also used for fast prediction. Therefore
we have the following objective function
?? =argmax
?
L(?) (2)
=argmax
?
?
i
max
R(wim,wic)
logP (wic, R(wim, wic)|wim)
where ? denotes the weight parameters and the max
is taken over the set of transformations that can
transform wim to wic.
We employ gradient ascent in the optimization in
Equ. (2). At each step, we first find the best trans-
formation for each word pair based on the current
parameters ?(t)
R?(wim, wic) (3)
= argmax
R(wim,wic)
logP?(t)(w
i
c, R(wim, wic)|wim)
Next, we calculate the gradients,
?L
??r
=
?
i logP?(t)(wic, R?(wim, wic)|wim)
??r
(4)
In this paper, we employ the bounded L-BFGS
(Behm et al, 2009) algorithm for the optimization
task, which works well even when the number of
weights ? is large.
3.3 Candidate Generation
In candidate generation, given a misspelled word
wm, we find the k candidates from the vocabu-
lary, that can be transformed from wm and have the
largest probabilities assigned by the learned model.
We only need to utilize the following ranking
function to rank a candidate wc given a misspelled
word wm, by taking into account Equs. (1) and (2)
rank(wc|wm) = max
R(wm,wc)
?
?
?
r?R(wm,wc)
?r
?
? (5)
For each possible transformation, we simply take
summation of the weights of the rules used in the
transformation. We then choose the sum as a rank-
ing score, which is equivalent to ranking candidates
based on their largest conditional probabilities.
55
aa
a NULL
NULL
......
......
......
......
......
......
___
___
e
s
a
0.0
-0.3
-0.1
failure link
leaf node link
Aho Corasick Tree
 
 
a   e
a   s
aa   a
e
ea
Figure 2: Rule Index based on Aho Corasick Tree.
4 Efficient Retrieval Algorithm
In this section, we introduce how to efficiently per-
form top k candidate generation. Our retrieval algo-
rithm is guaranteed to find the optimal k candidates
with some ?pruning? techniques. We first introduce
the data structures and then the retrieval algorithm.
4.1 Data Structures
We exploit two data structures for candidate genera-
tion. One is a trie for storing and matching words in
the vocabulary, referred to as vocabulary trie, and the
other based on what we call an Aho-Corasick tree
(AC tree) (Aho and Corasick, 1975), which is used
for storing and applying correction rules, referred to
as rule index. The vocabulary trie is the same as that
used in existing work and it will be traversed when
searching the top k candidates.
Our rule index is unique because it indexes all the
rules based on an AC tree. The AC tree is a trie with
?failure links?, on which the Aho-Corasick string
matching algorithm can be executed. Aho-Corasick
algorithm is a well known dictionary-matching al-
gorithm which can quickly locate all the words in a
dictionary within an input string. Time complexity
of the algorithm is of linear order in length of input
string plus number of matched entries.
We index all the ??s in the rules on the AC tree.
Each ? corresponds to a leaf node, and the ??s of the
? are stored in an associated list in decreasing order
of rule weights ?, as illustrated in Fig. 2. 1
1One may further improve the index structure by using a trie
rather than a ranking list to store ?s associated with the same
?. However the improvement would not be significant because
the number of ?s associated with each ? is usually very small.
4.2 Algorithm
One could employ a naive algorithm that applies all
the possible combinations of rules (??s) to the cur-
rent word wm, verifies whether the resulting words
(candidates) are in the vocabulary, uses the function
in Equ. (5) to calculate the ranking scores of the can-
didates, and find the top k candidates. This algo-
rithm is clearly inefficient.
Our algorithm first employs the Aho-Corasick al-
gorithm to locate all the applicable ??s within the in-
put word wm, from the rule index. The correspond-
ing ??s are retrieved as well. Then all the applicable
rules are identified and indexed by the applied posi-
tions of word wm.
Our algorithm next traverses the vocabulary trie
and searches the top k candidates with some pruning
techniques. The algorithm starts from the root node
of the vocabulary trie. At each step, it has multiple
search branches. It tries to match at the next position
of wm, or apply a rule at the current position of wm.
The following two pruning criteria are employed to
significantly accelerate the search process.
1) If the current sum of weights of applied rules
is smaller than the smallest weight in the top k
list, the search branch is pruned. This criterion
is derived from the non-negative constraint on
rule weights ?. It is easy to verify that the sum
of weights will not become larger if one contin-
ues to search the branch because all the weights
are non-positive.
2) If two search branches merge at the same node
in the vocabulary trie as well as the same po-
sition on wm, the search branches with smaller
sum of weights will be pruned. It is based on
the dynamic programming technique because
we take max in the ranking function in Equ. 5.
It is not difficult to prove that our algorithm is guar-
anteed to find the best k candidates in terms of the
ranking scores, because we only prune those candi-
dates that cannot give better scores than the ones in
the current top k list. Due to the limitation of space,
we omit the proof of the theorem that if the weights
of rules ? are non-positive and the ranking function
is defined as in Equ. 5, then the top k candidates ob-
tained with the pruning criteria are the same as the
top k candidates obtained without pruning.
56
5 Experimental Results
We have experimentally evaluated our approach in
spelling error correction of queries in web search.
The problem is more challenging than usual due to
the following reasons. (1) The vocabulary of queries
in web search is extremely large due to the scale, di-
versity, and dynamics of the Internet. (2) Efficiency
is critically important, because the response time of
top k candidate retrieval for web search must be kept
very low. Our approach for candidate generation is
in fact motivated by the application.
5.1 Word Pair Mining
In web search, a search session is comprised of a se-
quence of queries from the same user within a time
period. It is easy to observe from search session data
that there are many spelling errors and their correc-
tions occurring in the same sessions. We employed
heuristics to automatically mine training pairs from
search session data at a commercial search engine.
First, we segmented the query sequence from
each user into sessions. If two queries were issued
more than 5 minutes apart, then we put a session
boundary between them. We used short sessions
here because we found that search users usually cor-
rect their misspelled queries very quickly after they
find the misspellings. Then the following heuristics
were employed to identify pairs of misspelled words
and their corrections from two consecutive queries
within a session:
1) Two queries have the same number of words.
2) There is only one word difference between two
queries.
3) For the two distinct words, the word in the first
query is considered as misspelled and the sec-
ond one as its correction.
Finally, we aggregated the identified training pairs
across sessions and users and discarded the pairs
with low frequencies. Table 1 shows some examples
of the mined word pairs.
5.2 Experiments on Accuracy
Two representative methods were used as baselines:
the generative model proposed by (Brill and Moore,
2000) referred to as generative and the logistic re-
gression model proposed by (Okazaki et al, 2008)
Misspelled Correct Misspelled Correct
aacoustic acoustic chevorle chevrolet
liyerature literature tournemen tournament
shinngle shingle newpape newspaper
finlad finland ccomponet component
reteive retrieve olimpick olympic
Table 1: Examples of Word Pairs
referred to as logistic. Note that Okazaki et al
(2008)?s model is not particularly for spelling error
correction, but it can be employed in the task. When
using their method for ranking, we used outputs of
the logistic regression model as rank scores.
We compared our method with the two baselines
in terms of top k accuracy, which is ratio of the true
corrections among the top k candidates generated by
a method. All the methods shared the same settings:
973,902 words in the vocabulary, 10,597 rules for
correction, and up to two rules used in one transfor-
mation. We made use of 100,000 word pairs mined
from query sessions for training, and 10,000 word
pairs for testing.
The experimental results are shown in Fig. 3. We
can see that our method always performs the best
when compared with the baselines and the improve-
ments are statistically significant (p < 0.01). The
logistic method works better than generative, when
k is small, but its performance becomes saturated,
when k is large. Usually a discriminative model
works better than a generative model, and that seems
to be what happens with small k?s. However, logis-
tic cannot work so well for large k?s, because it only
allows the use of one rule each time. We observe
that there are many word pairs in the data that need
to be transformed with multiple rules.
Next, we conducted experiments to investigate
how the top k accuracy changes with different sizes
of vocabularies, maximum numbers of applicable
rules and sizes of rule set for the three methods. The
experimental results are shown in Fig. 4, Fig. 5 and
Fig. 6.
For the experiment in Fig. 4, we enlarged
the vocabulary size from 973,902 (smallVocab) to
2,206,948 (largeVocab) and kept the other settings
the same as in the previous experiment. Because
more candidates can be generated with a larger vo-
cabulary, the performances of all the methods de-
57
0 5 10 15 20 25 30
40%
50%
60%
70%
80%
90%
100%
top k
A
c
c
u
r
a
c
y
 Generative
 Logistic
 Our Method
Figure 3: Accuracy Comparison between Our Method
and Baselines
cline. However, the drop of accuracy by our method
is much smaller than that by generative, which
means our method is more powerful when the vo-
cabulary is large, e.g., for web search. For the exper-
iment in Fig. 5, we changed the maximum number of
rules that can be applied to a transformation from 2
to 3. Because logistic can only use one rule at a time,
it is not included in this experiment. When there
are more applicable rules, more candidates can be
generated and thus ranking of them becomes more
challenging. The accuracies of both methods drop,
but our method is constantly better than generative.
Moreover, the decrease in accuracy by our method
is clearly less than that by generative. For the ex-
periment in Fig. 6, we enlarged the number of rules
from 10,497 (smallRuleNum) to 24,054 (largeRu-
leNum). The performance of our method and those
of the two baselines did not change so much, and our
method still visibly outperform the baselines when
more rules are exploited.
5.3 Experiments on Efficiency
We have also experimentally evaluated the effi-
ciency of our approach. Because most existing work
uses a predefined ranking function, it is not fair to
make a comparison with them. Moreover, Okazaki
et al? method does not consider efficiency, and Brill
and Moore?s method is based a complicated retrieve
algorithm which is very hard to implement. Instead
of making comparison with the existing methods in
terms of efficiency, we evaluated the efficiency of
our method by looking at how efficient it becomes
with its data structure and pruning technique.
0 5 10 15 20 25 30
30%
40%
50%
60%
70%
80%
90%
100%
 
top k
A
c
c
u
r
a
c
y
 Generative (smallVocab)
 Generative (largeVocab)
 Logistic (smallVocab)
 Logistic (largeVocab)
 Our Method (smallVocab)
 Our Method (largeVocab)
Figure 4: Accuracy Comparisons between Baselines and
Our Method with Different Vocabulary Sizes
0 5 10 15 20 25 30
30%
40%
50%
60%
70%
80%
90%
100%
top k
A
c
c
u
r
a
c
y
 Generative (2 applicable rules)
 Generative (3 applicable rules)
 Our Method (2 applicable rules)
 Our Method (3 applicable rules)
Figure 5: Accuracy Comparison between Generative and
Our Method with Different Maximum Numbers of Ap-
plicable Rules
0 5 10 15 20 25 30
40%
50%
60%
70%
80%
90%
100%
A
c
c
u
r
a
c
y
top k
 Generative (largeRuleSet)
 Generative (smallRuleSet)
 Logistic (largeRuleSet)
 Logistic (smallRuleSet)
 Our Method (largeRuleSet)
 Our Method (smallRuleSet)
Figure 6: Accuracy Comparison between Baselines and
Our Method with Different Numbers of Rules
First, we tested the efficiency of using Aho-
Corasick algorithm (the rule index). Because the
58
0 5000 10000 15000 20000 25000
2
4
6
8
10
12
14
16
18
20
22
24
26
28
30
32
Number of Rules
 
Word Length
N
u
m
b
e
r
 
o
f
 
M
a
t
c
h
i
n
g
 
R
u
l
e
s
 4
 5
 6
 7
 8
 9
 10
Figure 7: Number of Matching Rules v.s. Number of
Rules
time complexity of Aho-Corasick algorithm is de-
termined by the lengths of query strings and the
number of matches, we examined how the number
of matches on query strings with different lengths
changes when the number of rules increases. The
experimental results are shown in Fig. 7. We can see
that the number of matches is not largely affected by
the number of rules in the rule index. It implies that
the time for searching applicable rules is close to a
constant and does not change much with different
numbers of rules.
Next, since the running time of our method is
proportional to the number of visited nodes on the
vocabulary trie, we evaluated the efficiency of our
method in terms of number of visited nodes. The
result reported here is that when k is 10.
Specifically, we tested how the number of visited
nodes changes according to three factors: maximum
number of applicable rules in a transformation, vo-
cabulary size and rule set size. The experimental re-
sults are shown in Fig. 8, Fig. 9 and Fig. 10 respec-
tively. From Fig. 8, with increasing maximum num-
ber of applicable rules in a transformation, number
of visited nodes increases first and then stabilizes,
especially when the words are long. Note that prun-
ing becomes even more effective because number of
visited nodes without pruning grows much faster. It
demonstrates that our method is very efficient when
compared to the non-pruning method. Admittedly,
the efficiency of our method also deteriorates some-
what. This would not cause a noticeable issue in
real applications, however. In the previous section,
1 2 3 4 5 6 7
0
2000
4000
6000
8000
10000
12000
14000
16000
18000
Maximum Number of Applicable Rules
N
u
m
b
e
r
 
o
f
 
V
i
s
i
t
e
d
 
N
o
d
e
s
Word Length
 4
 5
 6
 7
 8
 9
 10
Figure 8: Efficiency Evaluation with Different Maximum
Numbers of Applicable Rules
0 100 200 300 400 500
0
1000
2000
3000
4000
5000
6000
7000
Vocabulary Size (million)
N
u
m
b
e
r
 
o
f
 
V
i
s
i
t
e
d
 
N
o
d
e
s
Word Length
 4
 5
 6
 7
 8
 9
 10
Figure 9: Efficiency Evaluation with Different Sizes of
Vocabulary
we have seen that using up to two rules in a transfor-
mation can bring a very high accuracy. From Fig. 8
and Fig. 9, we can conclude that the numbers of vis-
ited nodes are stable and thus the efficiency of our
method keeps high with larger vocabulary size and
number of rules. It indicates that our pruning strat-
egy is very effective. From all the figures, we can see
that our method is always efficient especially when
the words are relatively short.
5.4 Experiments on Model Constraints
In Section 3.1, we introduce the non-positive con-
straints on the parameters, i.e., ??r ? 0, to en-
able the pruning technique for efficient top k re-
trieval. We experimentally verified the impact of
the constraints to both the accuracy and efficiency.
For ease of reference, we name the model with the
non-positive constraints as bounded, and the origi-
59
5000 10000 15000 20000 25000
0
1000
2000
3000
4000
5000
6000
7000
Number of Rules
N
u
m
b
e
r
 
o
f
 
V
i
s
i
t
e
d
 
N
o
d
e
s
Word Length
 4
 5
 6
 7
 8
 9
 10
Figure 10: Efficiency Evaluation with Different Number
of Rules
0 5 10 15 20 25 30
40%
50%
60%
70%
80%
90%
100%
top k
A
c
c
u
r
a
c
y
 Bounded
 Unbounded
Figure 11: Accuracy Comparison between Bounded and
Unbounded Models
nal model as unbounded. The experimental results
are shown in Fig. 11 and Fig. 12. All the experi-
ments were conducted based on the typical setting
of our experiments: 973,902 words in the vocabu-
lary, 10,597 rules, and up to two rules in one trans-
formation. In Fig. 11, we can see that the differ-
ence between bounded and unbounded in terms of
accuracy is negligible, and we can draw a conclu-
sion that adding the constraints does not hurt the ac-
curacy. From Fig. 12, it is easy to note that bounded
is much faster than unbounded because our pruning
strategy can be applied to bounded.
6 Conclusion
In this paper, we have proposed a new method for
approximate string search, including spelling error
correction, which is both accurate and efficient. Our
method is novel and unique in its model, learning
4 6 8 10 12
0
1000
2000
3000
4000
5000
6000
7000
8000
9000
10000
N
u
m
b
e
r
 
o
f
 
V
i
s
i
t
e
d
 
N
o
d
e
s
Word Length
 Bounded
 Unbounded
Figure 12: Efficiency Comparison between Bounded and
Unbounded Models
algorithm, and retrieval algorithm. Experimental re-
sults on a large data set show that our method im-
proves upon existing methods in terms of accuracy,
and particularly our method can perform better when
the dictionary is large and when there are many
rules. Experimental results have also verified the
high efficiency of our method. As future work, we
plan to add contextual features into the model and
apply our method to other data sets in other tasks.
References
Farooq Ahmad and Grzegorz Kondrak. 2005. Learning
a spelling error model from search query logs. In Pro-
ceedings of the conference on Human Language Tech-
nology and Empirical Methods in Natural Language
Processing, HLT ?05, pages 955?962, Morristown, NJ,
USA. Association for Computational Linguistics.
Alfred V. Aho and Margaret J. Corasick. 1975. Efficient
string matching: an aid to bibliographic search. Com-
mun. ACM, 18:333?340, June.
Alexander Behm, Shengyue Ji, Chen Li, and Jiaheng Lu.
2009. Space-constrained gram-based indexing for effi-
cient approximate string search. In Proceedings of the
2009 IEEE International Conference on Data Engi-
neering, pages 604?615, Washington, DC, USA. IEEE
Computer Society.
Eric Brill and Robert C. Moore. 2000. An improved
error model for noisy channel spelling correction. In
Proceedings of the 38th Annual Meeting on Associa-
tion for Computational Linguistics, ACL ?00, pages
286?293, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Qing Chen, Mu Li, and Ming Zhou. 2007. Improv-
ing query spelling correction using web search re-
60
sults. In Proceedings of the 2007 Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
pages 181?189.
Andrew R. Golding and Dan Roth. 1999. A winnow-
based approach to context-sensitive spelling correc-
tion. Mach. Learn., 34:107?130, February.
Aminul Islam and Diana Inkpen. 2009. Real-word
spelling correction using google web it 3-grams. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing: Volume 3
- Volume 3, EMNLP ?09, pages 1241?1249, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Rosie Jones, Benjamin Rey, Omid Madani, and Wiley
Greiner. 2006. Generating query substitutions. In
Proceedings of the 15th international conference on
World Wide Web, WWW ?06, pages 387?396, New
York, NY, USA. ACM.
Mu Li, Yang Zhang, Muhua Zhu, and Ming Zhou. 2006.
Exploring distributional similarity based models for
query spelling correction. In Proceedings of the 21st
International Conference on Computational Linguis-
tics and the 44th annual meeting of the Association
for Computational Linguistics, ACL-44, pages 1025?
1032, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
Chen Li, Bin Wang, and Xiaochun Yang. 2007. Vgram:
improving performance of approximate queries on
string collections using variable-length grams. In Pro-
ceedings of the 33rd international conference on Very
large data bases, VLDB ?07, pages 303?314. VLDB
Endowment.
Chen Li, Jiaheng Lu, and Yiming Lu. 2008. Effi-
cient merging and filtering algorithms for approximate
string searches. In Proceedings of the 2008 IEEE 24th
International Conference on Data Engineering, pages
257?266, Washington, DC, USA. IEEE Computer So-
ciety.
AndrewMcCallum, Kedar Bellare, and Fernando Pereira.
2005. A conditional random field for discriminatively-
trained finite-state string edit distance. In Conference
on Uncertainty in AI (UAI).
Stoyan Mihov and Klaus U. Schulz. 2004. Fast approx-
imate search in large dictionaries. Comput. Linguist.,
30:451?477, December.
Naoaki Okazaki, Yoshimasa Tsuruoka, Sophia Anani-
adou, and Jun?ichi Tsujii. 2008. A discriminative
candidate generator for string transformations. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing, EMNLP ?08, pages
447?456, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Jose Oncina and Marc Sebban. 2005. Learning unbiased
stochastic edit distance in the form of a memoryless
finite-state transducer. In International Joint Confer-
ence on Machine Learning (2005). Workshop: Gram-
matical Inference Applications: Successes and Future
Challenges.
Eric Sven Ristad and Peter N. Yianilos. 1998. Learning
string-edit distance. IEEE Trans. Pattern Anal. Mach.
Intell., 20:522?532, May.
Johannes Schaback and Fang Li. 2007. Multi-level fea-
ture extraction for spelling correction. In IJCAI-2007
Workshop on Analytics for Noisy Unstructured Text
Data, pages 79?86, Hyderabad, India.
Kristina Toutanova and Robert C. Moore. 2002. Pro-
nunciation modeling for improved spelling correction.
In Proceedings of the 40th Annual Meeting on Associ-
ation for Computational Linguistics, ACL ?02, pages
144?151, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Rares Vernica and Chen Li. 2009. Efficient top-k algo-
rithms for fuzzy search in string collections. In Pro-
ceedings of the First International Workshop on Key-
word Search on Structured Data, KEYS ?09, pages 9?
14, New York, NY, USA. ACM.
Casey Whitelaw, Ben Hutchinson, Grace Y. Chung, and
Gerard Ellis. 2009. Using the web for language in-
dependent spellchecking and autocorrection. In Pro-
ceedings of the 2009 Conference on Empirical Meth-
ods in Natural Language Processing: Volume 2 - Vol-
ume 2, EMNLP ?09, pages 890?899, Morristown, NJ,
USA. Association for Computational Linguistics.
Xiaochun Yang, Bin Wang, and Chen Li. 2008. Cost-
based variable-length-gram selection for string collec-
tions to support approximate queries efficiently. In
Proceedings of the 2008 ACM SIGMOD international
conference on Management of data, SIGMOD ?08,
pages 353?364, New York, NY, USA. ACM.
Zhenglu Yang, Jianjun Yu, and Masaru Kitsuregawa.
2010. Fast algorithms for top-k approximate string
matching. In Proceedings of the Twenty-Fourth AAAI
Conference on Artificial Intelligence, AAAI ?10, pages
1467?1473.
61
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 449?458,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
String Re-writing Kernel
Fan Bu1, Hang Li2 and Xiaoyan Zhu3
1,3State Key Laboratory of Intelligent Technology and Systems
1,3Tsinghua National Laboratory for Information Sci. and Tech.
1,3Department of Computer Sci. and Tech., Tsinghua University
2Microsoft Research Asia, No. 5 Danling Street, Beijing 100080,China
1bufan0000@gmail.com
2hangli@microsoft.com
3zxy-dcs@tsinghua.edu.cn
Abstract
Learning for sentence re-writing is a funda-
mental task in natural language processing and
information retrieval. In this paper, we pro-
pose a new class of kernel functions, referred
to as string re-writing kernel, to address the
problem. A string re-writing kernel measures
the similarity between two pairs of strings,
each pair representing re-writing of a string.
It can capture the lexical and structural sim-
ilarity between two pairs of sentences with-
out the need of constructing syntactic trees.
We further propose an instance of string re-
writing kernel which can be computed effi-
ciently. Experimental results on benchmark
datasets show that our method can achieve bet-
ter results than state-of-the-art methods on two
sentence re-writing learning tasks: paraphrase
identification and recognizing textual entail-
ment.
1 Introduction
Learning for sentence re-writing is a fundamental
task in natural language processing and information
retrieval, which includes paraphrasing, textual en-
tailment and transformation between query and doc-
ument title in search.
The key question here is how to represent the re-
writing of sentences. In previous research on sen-
tence re-writing learning such as paraphrase identifi-
cation and recognizing textual entailment, most rep-
resentations are based on the lexicons (Zhang and
Patrick, 2005; Lintean and Rus, 2011; de Marneffe
et al, 2006) or the syntactic trees (Das and Smith,
                  wrote     .                  Shakespeare  wrote  Hamlet.  
  *    was written by       .          Hamlet was written by Shakespeare.  
(B) ** 
* * 
(A) 
Figure 1: Example of re-writing. (A) is a re-writing rule
and (B) is a re-writing of sentence.
2009; Heilman and Smith, 2010) of the sentence
pairs.
In (Lin and Pantel, 2001; Barzilay and Lee, 2003),
re-writing rules serve as underlying representations
for paraphrase generation/discovery. Motivated by
the work, we represent re-writing of sentences by
all possible re-writing rules that can be applied into
it. For example, in Fig. 1, (A) is one re-writing rule
that can be applied into the sentence re-writing (B).
Specifically, we propose a new class of kernel func-
tions (Scho?lkopf and Smola, 2002), called string re-
writing kernel (SRK), which defines the similarity
between two re-writings (pairs) of strings as the in-
ner product between them in the feature space in-
duced by all the re-writing rules. SRK is different
from existing kernels in that it is for re-writing and
defined on two pairs of strings. SRK can capture the
lexical and structural similarity between re-writings
of sentences and does not need to parse the sentences
and create the syntactic trees of them.
One challenge for using SRK lies in the high com-
putational cost of straightforwardly computing the
kernel, because it involves two re-writings of strings
(i.e., four strings) and a large number of re-writing
rules. We are able to develop an instance of SRK,
referred to as kb-SRK, which directly computes the
number of common rewriting rules without explic-
449
itly calculating the inner product between feature
vectors, and thus drastically reduce the time com-
plexity.
Experimental results on benchmark datasets show
that SRK achieves better results than the state-of-
the-art methods in paraphrase identification and rec-
ognizing textual entailment. Note that SRK is very
flexible to the formulations of sentences. For ex-
ample, informally written sentences such as long
queries in search can also be effectively handled.
2 Related Work
The string kernel function, first proposed by Lodhi
et al (2002), measures the similarity between two
strings by their shared substrings. Leslie et al
(2002) proposed the k-spectrum kernel which repre-
sents strings by their contiguous substrings of length
k. Leslie et al (2004) further proposed a number of
string kernels including the wildcard kernel to fa-
cilitate inexact matching between the strings. The
string kernels defined on two pairs of objects (in-
cluding strings) were also developed, which decom-
pose the similarity into product of similarities be-
tween individual objects using tensor product (Basil-
ico and Hofmann, 2004; Ben-Hur and Noble, 2005)
or Cartesian product (Kashima et al, 2009).
The task of paraphrasing usually consists of para-
phrase pattern generation and paraphrase identifica-
tion. Paraphrase pattern generation is to automat-
ically extract semantically equivalent patterns (Lin
and Pantel, 2001; Bhagat and Ravichandran, 2008)
or sentences (Barzilay and Lee, 2003). Paraphrase
identification is to identify whether two given sen-
tences are a paraphrase of each other. The meth-
ods proposed so far formalized the problem as clas-
sification and used various types of features such
as bag-of-words feature, edit distance (Zhang and
Patrick, 2005), dissimilarity kernel (Lintean and
Rus, 2011) predicate-argument structure (Qiu et al,
2006), and tree edit model (which is based on a tree
kernel) (Heilman and Smith, 2010) in the classifica-
tion task. Among the most successful methods, Wan
et al (2006) enriched the feature set by the BLEU
metric and dependency relations. Das and Smith
(2009) used the quasi-synchronous grammar formal-
ism to incorporate features from WordNet, named
entity recognizer, POS tagger, and dependency la-
bels from aligned trees.
The task of recognizing textual entailment is to
decide whether the hypothesis sentence can be en-
tailed by the premise sentence (Giampiccolo et al,
2007). In recognizing textual entailment, de Marn-
effe et al (2006) classified sentences pairs on the
basis of word alignments. MacCartney and Man-
ning (2008) used an inference procedure based on
natural logic and combined it with the methods by
de Marneffe et al (2006). Harmeling (2007) and
Heilman and Smith (2010) classified sequence pairs
based on transformation on syntactic trees. Zanzotto
et al (2007) used a kernel method on syntactic tree
pairs (Moschitti and Zanzotto, 2007).
3 Kernel Approach to Sentence
Re-Writing Learning
We formalize sentence re-writing learning as a ker-
nel method. Following the literature of string kernel,
we use the terms ?string? and ?character? instead of
?sentence? and ?word?.
Suppose that we are given training data consisting
of re-writings of strings and their responses
((s1, t1),y1), ...,((sn, tn),yn) ? (?
????)?Y
where ? denotes the character set, ?? =
??
i=0?
i de-
notes the string set, which is the Kleene closure of
set ?, Y denotes the set of responses, and n is the
number of instances. (si, ti) is a re-writing consist-
ing of the source string si and the target string ti.
yi is the response which can be a category, ordinal
number, or real number. In this paper, for simplic-
ity we assume that Y = {?1} (e.g. paraphrase/non-
paraphrase). Given a new string re-writing (s, t) ?
?????, our goal is to predict its response y. That is,
the training data consists of binary classes of string
re-writings, and the prediction is made for the new
re-writing based on learning from the training data.
We take the kernel approach to address the learn-
ing task. The kernel on re-writings of strings is de-
fined as
K : (?????)? (?????)? R
satisfying for all (si, ti), (s j, t j) ? ?????,
K((si, ti),(s j, t j)) = ??(si, ti),?(s j, t j)?
where ? maps each re-writing (pair) of strings into
a high dimensional Hilbert space H , referred to as
450
feature space. By the representer theorem (Kimel-
dorf and Wahba, 1971; Scho?lkopf and Smola, 2002),
it can be shown that the response y of a new string
re-writing (s, t) can always be represented as
y = sign(
n
?
i=1
?iyiK((si, ti),(s, t)))
where ?i ? 0,(i = 1, ? ? ? ,n) are parameters. That is,
it is determined by a linear combination of the sim-
ilarities between the new instance and the instances
in training set. It is also known that by employing a
learning model such as SVM (Vapnik, 2000), such a
linear combination can be automatically learned by
solving a quadratic optimization problem. The ques-
tion then becomes how to design the kernel function
for the task.
4 String Re-writing Kernel
Let ? be the set of characters and ?? be the set of
strings. Let wildcard domain D ? ?? be the set of
strings which can be replaced by wildcards.
The string re-writing kernel measures the similar-
ity between two string re-writings through the re-
writing rules that can be applied into them. For-
mally, given re-writing rule set R and wildcard do-
main D, the string re-writing kernel (SRK) is defined
as
K((s1, t1),(s2, t2)) = ??(s1, t1),?(s2, t2)? (1)
where ?(s, t) = (?r(s, t))r?R and
?r(s, t) = n? i (2)
where n is the number of contiguous substring pairs
of (s, t) that re-writing rule r matches, i is the num-
ber of wildcards in r, and ? ? (0,1] is a factor pun-
ishing each occurrence of wildcard.
A re-writing rule is defined as a triple r =
(?s,?t ,?) where ?s,?t ? (? ? {?})? denote source
and target string patterns and ? ? ind?(?s)? ind?(?t)
denotes the alignments between the wildcards in the
two string patterns. Here ind?(? ) denotes the set of
indexes of wildcards in ? .
We say that a re-writing rule (?s,?t ,?) matches a
string pair (s, t), if and only if string patterns ?s and
?t can be changed into s and t respectively by sub-
stituting each wildcard in the string patterns with an
element in the strings, where the elements are de-
fined in the wildcard domain D and the wildcards
?s[i] and ?t [ j] are substituted by the same elements,
when there is an alignment (i, j) ? ? .
For example, the re-writing rule in Fig. 1 (A)
can be formally written as r = (? s,? t,?) where
? s = (?,wrote,?), ? t = (?,was,written,by,?) and
? = {(1,5),(3,1)}. It matches with the string pair in
Fig. 1 (B).
String re-writing kernel is a class of kernels which
depends on re-writing rule set R and wildcard do-
main D. Here we provide some examples. Obvi-
ously, the effectiveness and efficiency of SRK de-
pend on the choice of R and D.
Example 1. We define the pairwise k-spectrum ker-
nel (ps-SRK) K psk as the re-writing rule kernel un-
der R = {(?s,?t ,?)|?s,?t ? ?k,? = /0} and any
D. It can be shown that K psk ((s1, t1),(s2, t2)) =
Kspeck (s1,s2)K
spec
k (t1, t2) where K
spec
k (x,y) is equiv-
alent to the k-spectrum kernel proposed by Leslie et
al. (2002).
Example 2. The pairwise k-wildcard kernel (pw-
SRK) K pwk is defined as the re-writing rule kernel
under R= {(?s,?t ,?)|?s,?t ? (??{?})k,? = /0} and
D = ?. It can be shown that K pwk ((s1, t1),(s2, t2)) =
Kwc(k,k)(s1,s2)K
wc
(k,k)(t1, t2) where K
wc
(k,k)(x,y) is a spe-
cial case (m=k) of the (k,m)-wildcard kernel pro-
posed by Leslie et al (2004).
Both kernels shown above are represented as the
product of two kernels defined separately on strings
s1,s2 and t1, t2, and that is to say that they do not
consider the alignment relations between the strings.
5 K-gram Bijective String Re-writing
Kernel
Next we propose another instance of string re-
writing kernel, called the k-gram bijective string re-
writing kernel (kb-SRK). As will be seen, kb-SRK
can be computed efficiently, although it is defined
on two pairs of strings and is not decomposed (note
that ps-SRK and pw-SRK are decomposed).
5.1 Definition
The kb-SRK has the following properties: (1) A
wildcard can only substitute a single character, de-
noted as ???. (2) The two string patterns in a re-
writing rule are of length k. (3) The alignment
relation in a re-writing rule is bijective, i.e., there
is a one-to-one mapping between the wildcards in
451
the string patterns. Formally, the k-gram bijective
string re-writing kernel Kk is defined as a string
re-writing kernel under the re-writing rule set R =
{(?s,?t ,?)|?s,?t ? (??{?})k,? is bijective} and the
wildcard domain D = ?.
Since each re-writing rule contains two string pat-
terns of length k and each wildcard can only substi-
tute one character, a re-writing rule can only match
k-gram pairs in (s, t). We can rewrite Eq. (2) as
?r(s, t) = ?
?s?k-grams(s)
?
?t?k-grams(t)
??r(?s,?t) (3)
where ??r(?s,?t) = ? i if r (with i wildcards) matches
(?s,?t), otherwise ??r(?s,?t) = 0.
For ease of computation, we re-write kb-SRK as
Kk((s1, t1),(s2, t2))
= ?
?s1 ? k-grams(s1)
?t1 ? k-grams(t1)
?
?s2 ? k-grams(s2)
?t2 ? k-grams(t2)
K?k((?s1 ,?t1),(?s2 ,?t2))
(4)
where
K?k = ?
r?R
??r(?s1 ,?t1)??r(?s2 ,?t2) (5)
5.2 Algorithm for Computing Kernel
A straightforward computation of kb-SRK would
be intractable. The computation of Kk in Eq. (4)
needs computations of K?k conducted O((n? k +
1)4) times, where n denotes the maximum length
of strings. Furthermore, the computation of K?k in
Eq. (5) needs to perform matching of all the re-
writing rules with the two k-gram pairs (?s1 , ?t1),
(?s2 , ?t2), which has time complexity O(k!).
In this section, we will introduce an efficient algo-
rithm, which can compute K?k and Kk with the time
complexities of O(k) and O(kn2), respectively. The
latter is verified empirically.
5.2.1 Transformation of Problem
For ease of manipulation, our method transforms
the computation of kernel on k-grams into the com-
putation on a new data structure called lists of dou-
bles. We first explain how to make the transforma-
tion.
Suppose that ?1,?2 ? ?k are k-grams, we use
?1[i] and ?2[i] to represent the i-th characters of
them. We call a pair of characters a double. Thus
??? denotes the set of doubles and ?Ds ,?Dt ? (??
??1 = abbccbb ;               ??2 = abcccdd; 
??1 = cbcbbcb ;               ??2 = cbccdcd;  
Figure 2: Example of two k-gram pairs.
???= ?a?a???b?b?????????c?c???c?c?????????????
???= ?c?c???b?b???c?c???????????????c?c??????? 
Figure 3: Example of the pair of double lists combined
from the two k-gram pairs in Fig. 2. Non-identical dou-
bles are in bold.
?)k denote lists of doubles. The following operation
combines two k-grams into a list of doubles.
?1??2 = ((?1[1],?2[1]), ? ? ? ,(?1[k],?2[k])).
We denotes ?1 ? ?2[i] as the i-th element of the
list. Fig. 3 shows example lists of doubles combined
from k-grams.
We introduce the set of identical doubles I =
{(c,c)|c ? ?} and the set of non-identical doubles
N = {(c,c?)|c,c? ? ? and c 6= c?}. Obviously, I
?
N =
??? and I
?
N = /0.
We define the set of re-writing rules for double
lists RD = {rD = (?Ds ,?Dt ,?)|?Ds ,?Dt ? (I?{?})k,?
is a bijective alignment} where ?Ds and ?Dt are lists
of identical doubles including wildcards and with
length k. We say rule rD matches a pair of double
lists (?Ds ,?Dt ) iff. ?Ds ,?Dt can be changed into ?Ds
and ?Dt by substituting each wildcard pair to a dou-
ble in ??? , and the double substituting the wild-
card pair ?Ds [i] and ?Dt [ j] must be an identical dou-
ble when there is an alignment (i, j) ? ? . The rule
set defined here and the rule set in Sec. 4 only differ
on the elements where re-writing occurs. Fig. 4 (B)
shows an example of re-writing rule for double lists.
The pair of double lists in Fig. 3 can match with the
re-writing rule.
5.2.2 Computing K?k
We consider how to compute K?k by extending the
computation from k-grams to double lists.
The following lemma shows that computing the
weighted sum of re-writing rules matching k-gram
pairs (?s1 ,?t1) and (?s2 ,?t2) is equivalent to com-
puting the weighted sum of re-writing rules for dou-
ble lists matching (?s1??s2 ,?t1??t2).
452
                           a b * 1  c                    a b ?   c c ?   ?                         (a,a) (b,b)  ?   (c ,c)  (c ,c)  ?   ?                                          
                             
       c b c ?   ?   c ?                          (c, c ) (b,b)  (c ,c)  ?   ?   (c ,c)  ?                    
(A) (B) 
Figure 4: For re-writing rule (A) matching both k-gram
pairs shown in Fig. 2, there is a corresponding re-writing
rule for double lists (B) matching the pair of double lists
shown in Fig. 3.
?????????=??a?a?????b?b?????????????????????c?c???? 
?????????=??a?a?????b?b?????????????????????c?c???? 
Figure 5: Example of #???(?) for the two double lists
shown in Fig. 3. Doubles not appearing in both ?Ds and
?Dt are not shown.
Lemma 1. For any two k-gram pairs (?s1 ,?t1) and
(?s2 ,?t2), there exists a one-to-one mapping from
the set of re-writing rules matching them to the set of
re-writing rules matching the corresponding double
lists (?s1??s2 ,?t1??t2).
The re-writing rule in Fig. 4 (A) matches the k-
gram pairs in Fig. 2. Equivalently, the re-writing
rule for double lists in Fig. 4 (B) matches the pair
of double lists in Fig. 3. By lemma 1 and Eq. 5, we
have
K?k = ?
rD?RD
??rD(?s1??s2 ,?t1??t2) (6)
where ??rD(?Ds ,?Dt ) = ? 2i if the rewriting rule for
double lists rD with i wildcards matches (?Ds ,?Dt ),
otherwise ??rD(?Ds ,?Dt ) = 0. To get K?k, we just need
to compute the weighted sum of re-writing rules for
double lists matching (?s1 ??s2 ,?t1 ??t2). Thus,
we can work on the ?combined? pair of double lists
instead of two pairs of k-grams.
Instead of enumerating all possible re-writing
rules and checking whether they can match the given
pair of double lists, we only calculate the number of
possibilities of ?generating? from the pair of double
lists to the re-writing rules matching it, which can be
carried out efficiently. We say that a re-writing rule
of double lists can be generated from a pair of double
lists (?Ds , ?Dt ), if they match with each other. From
the definition of RD, in each generation, the identi-
cal doubles in ?Ds and ?Dt can be either or not sub-
stituted by an aligned wildcard pair in the re-writing
Algorithm 1: Computing K?k
Input: k-gram pair (?s1 ,?t1) and (?s2 ,?t2)
Output: K?k((?s1 ,?t1),(?s2 ,?t2))
1 Set (?Ds ,?Dt ) = (?s1??s2 ,?t1??t2) ;
2 Compute #???(?Ds ) and #???(?Dt );
3 result=1;
4 for each e ? ??? satisfies
#e(?Ds )+#e(?Dt ) 6= 0 do
5 ge = 0, ne = min{#e(?Ds ),#e(?Dt )} ;
6 for 0? i? ne do
7 ge = ge +a
(e)
i ? 2i;
8 result = result ?g;
9 return result;
rule, and all the non-identical doubles in ?Ds and ?Dt
must be substituted by aligned wildcard pairs. From
this observation and Eq. 6, K?k only depends on the
number of times each double occurs in the double
lists.
Let e be a double. We denote #e(?D) as the num-
ber of times e occurs in the list of doubles ?D. Also,
for a set of doubles S? ???, we denote #S(?D) as
a vector in which each element represents #e(?D) of
each double e ? S. We can find a function g such
that
K?k = g(#???(?s1??s2),#???(?t1??t2)) (7)
Alg. 1 shows how to compute K?k. #???(.) is com-
puted from the two pairs of k-grams in line 1-2. The
final score is made through the iterative calculation
on the two lists (lines 4-8).
The key of Alg. 1 is the calculation of ge based on
a(e)i (line 7). Here we use a
(e)
i to denote the number
of possibilities for which i pairs of aligned wildcards
can be generated from e in both ?Ds and ?Dt . a
(e)
i can
be computed as follows.
(1) If e ? N and #e(?Ds ) 6= #e(?Dt ), then a
(e)
i = 0
for any i.
(2) If e?N and #e(?Ds ) = #e(?Dt ) = j, then a
(e)
j =
j! and a(e)i = 0 for any i 6= j.
(3) If e ? I, then a(e)i =
(#e(?Ds )
i
)(#e(?Dt )
i
)
i!.
We next explain the rationale behind the above
computations. In (1), since #e(?Ds ) 6= #e(?Dt ), it is
impossible to generate a re-writing rule in which all
453
the occurrences of non-identical double e are substi-
tuted by pairs of aligned wildcards. In (2), j pairs of
aligned wildcards can be generated from all the oc-
currences of non-identical double e in both ?Ds and
?Dt . The number of combinations thus is j!. In (3),
a pair of aligned wildcards can either be generated
or not from a pair of identical doubles in ?Ds and
?Dt . We can select i occurrences of identical double
e from ?Ds , i occurrences from ?Dt , and generate all
possible aligned wildcards from them.
In the loop of lines 4-8, we only need to con-
sider a(e)i for 0? i?min{#e(?Ds ),#e(?Dt )}, because
a(e)i = 0 for the rest of i.
To sum up, Eq. 7 can be computed as below,
which is exactly the computation at lines 3-8.
g(#???(?Ds ),#???(?Dt )) = ?
e????
(
ne
?
i=0
a(e)i ?
2i) (8)
For the k-gram pairs in Fig. 2, we first create
lists of doubles in Fig. 3 and compute #???(?) for
them (lines 1-2 of Alg. 1), as shown in Fig. 5. We
next compute Kk from #???(?Ds ) and #???(?Dt ) in
Fig. 5 (lines 3-8 of Alg. 1) and obtain Kk = (1)(1+
? 2)(? 2)(2? 4)(1 + 6? 2 + 6? 4) = 12? 12 + 24? 10 +
14? 8 +2? 6.
5.2.3 Computing Kk
Algorithm 2 shows how to compute Kk. It pre-
pares two maps ms and mt and two vectors of coun-
ters cs and ct . In ms and mt , each key #N(.) maps a
set of values #???(.). Counters cs and ct count the
frequency of each #???(.). Recall that #N(?s1??s2)
denotes a vector whose element is #e(?s1 ??s2) for
e ? N. #???(?s1 ??s2) denotes a vector whose ele-
ment is #e(?s1??s2) where e is any possible double.
One can easily verify the output of the al-
gorithm is exactly the value of Kk. First,
K?k((?s1 ,?t1),(?s2 ,?t2)) = 0 if #N(?s1 ? ?s2) 6=
#N(?t1 ??t2). Therefore, we only need to consider
those ?s1 ??s2 and ?t1 ??t2 which have the same
key (lines 10-13). We group the k-gram pairs by
their key in lines 2-5 and lines 6-9.
Moreover, the following relation holds
K?k((?s1 ,?t1),(?s2 ,?t2)) = K?k((?
?
s1 ,?
?
t1),(?
?
s2 ,?
?
t2))
if #???(?s1??s2) = #???(?
?
s1??
?
s2) and #???(?t1?
?t2) = #???(?
?
t1 ??
?
t2), where ?
?
s1 , ?
?
s2 , ?
?
t1 , ?
?
t2 are
Algorithm 2: Computing Kk
Input: string pair (s1, t1) and (s2, t2), window
size k
Output: Kk((s1, t1),(s2, t2))
1 Initialize two maps ms and mt and two counters
cs and ct ;
2 for each k-gram ?s1 in s1 do
3 for each k-gram ?s2 in s2 do
4 Update ms with key-value pair
(#N(?s1??s2),#???(?s1??s2));
5 cs[#???(?s1??s2)]++ ;
6 for each k-gram ?t1 in t1 do
7 for each k-gram ?t2 in t2 do
8 Update mt with key-value pair
(#N(?t1??t2),#???(?t1??t2));
9 ct [#???(?t1??t2)]++ ;
10 for each key ?ms.keys?mt .keys do
11 for each vs ?ms[key] do
12 for each vt ?mt [key] do
13 result+= cs[vs]ct [vt ]g(vs,vt) ;
14 return result;
other k-grams. Therefore, we only need to take
#???(?s1??s2) and #???(?t1??t2) as the value un-
der each key and count its frequency. That is to say,
#??? provides sufficient statistics for computing K?k.
The quantity g(vs,vt) in line 13 is computed by
Alg. 1 (lines 3-8).
5.3 Time Complexity
The time complexities of Alg. 1 and Alg. 2 are
shown below.
For Alg. 1, lines 1-2 can be executed in
O(k). The time for executing line 7 is less
than #e(?Ds ) + #e(?Dt ) + 1 for each e satisfying
#e(?Ds ) 6= 0 or #e(?Dt ) 6= 0 . Since ?e???? #e(?Ds ) =
?e???? #e(?Dt ) = k, the time for executing lines 3-8
is less than 4k, which results in the O(k) time com-
plexity of Alg. 1.
For Alg. 2, we denote n = max{|s1|, |s2|, |t1|, |t2|}.
It is easy to see that if the maps and counters in the
algorithm are implemented by hash maps, the time
complexities of lines 2-5 and lines 6-9 are O(kn2).
However, analyzing the time complexity of lines 10-
454
                           a b * 1  c            
0
0.5
1
1.5
2
2.5
1 2 3 4 5 6 7 8
C/
n a
vg
2 
window size  K 
Worst
Avg.
Figure 6: Relation between ratio C/n2avg and window size
k when running Alg. 2 on MSR Paraphrases Corpus.
13 is quite difficult.
Lemma 2 and Theorem 1 provide an upper bound
of the number of times computing g(vs,vt) in line 13,
denoted as C.
Lemma 2. For ?s1 ?k-grams(s1) and ?s2 ,?
?
s2 ?k-
grams(s2), we have #???(?s1??s2) =
#???(?s1??
?
s2) if #N(?s1??s2) = #N(?s1??
?
s2).
Theorem 1. C is O(n3).
By Lemma 2, each ms[key] contains at most
n? k + 1 elements. Together with the fact that
?key ms[key] = (n? k + 1)
2, Theorem 1 is proved.
It can be also proved that C is O(n2) when k = 1.
Empirical study shows that O(n3) is a loose upper
bound for C. Let navg denote the average length of
s1, t1, s2 and t2. Our experiment on all pairs of sen-
tences on MSR Paraphrase (Fig. 6) shows that C is in
the same order of n2avg in the worst case and C/n
2
avg
decreases with increasing k in both average case and
worst case, which indicates that C is O(n2) and the
overall time complexity of Alg. 2 is O(kn2).
6 Experiments
We evaluated the performances of the three types
of string re-writing kernels on paraphrase identifica-
tion and recognizing textual entailment: pairwise k-
spectrum kernel (ps-SRK), pairwise k-wildcard ker-
nel (pw-SRK), and k-gram bijective string re-writing
kernel (kb-SRK). We set ? = 1 for all kernels. The
performances were measured by accuracy (e.g. per-
centage of correct classifications).
In both experiments, we used LIBSVM with de-
fault parameters (Chang et al, 2011) as the clas-
sifier. All the sentences in the training and test
sets were segmented into words by the tokenizer at
OpenNLP (Baldrige et al, ). We further conducted
stemming on the words with Iveonik English Stem-
mer (http://www.iveonik.com/ ).
We normalized each kernel by K?(x,y) =
K(x,y)?
K(x,x)K(y,y)
and then tried them under different
window sizes k. We also tried to combine the
kernels with two lexical features ?unigram precision
and recall? proposed in (Wan et al, 2006), referred
to as PR. For each kernel K, we tested the window
size settings of K1 + ...+Kkmax (kmax ? {1,2,3,4})
together with the combination with PR and we
report the best accuracies of them in Tab 1 and
Tab 2.
6.1 Paraphrase Identification
The task of paraphrase identification is to examine
whether two sentences have the same meaning. We
trained and tested all the methods on the MSR Para-
phrase Corpus (Dolan and Brockett, 2005; Quirk
et al, 2004) consisting of 4,076 sentence pairs for
training and 1,725 sentence pairs for testing.
The experimental results on different SRKs are
shown in Table 1. It can be seen that kb-SRK out-
performs ps-SRK and pw-SRK. The results by the
state-of-the-art methods reported in previous work
are also included in Table 1. kb-SRK outperforms
the existing lexical approach (Zhang and Patrick,
2005) and kernel approach (Lintean and Rus, 2011).
It also works better than the other approaches listed
in the table, which use syntactic trees or dependency
relations.
Fig. 7 gives detailed results of the kernels under
different maximum k-gram lengths kmax with and
without PR. The results of ps-SRK and pw-SRK
without combining PR under different k are all be-
low 71%, therefore they are not shown for clar-
Method Acc.
Zhang and Patrick (2005) 71.9
Lintean and Rus (2011) 73.6
Heilman and Smith (2010) 73.2
Qiu et al (2006) 72.0
Wan et al (2006) 75.6
Das and Smith (2009) 73.9
Das and Smith (2009)(PoE) 76.1
Our baseline (PR) 73.6
Our method (ps-SRK) 75.6
Our method (pw-SRK) 75.0
Our method (kb-SRK) 76.3
Table 1: Comparison with state-of-the-arts on MSRP.
455
                           a b * 1  c            
73.5
74
74.5
75
75.5
76
76.5
1 2 3 4
Accura
cy (%)
 
w i ndow size kmax 
kb_SR K+ PR
kb_SR K
ps_SRK +PR
pw_SRK +PR
P R
Figure 7: Performances of different kernels under differ-
ent maximum window size kmax on MSRP.
ity. By comparing the results of kb-SRK and pw-
SRK we can see that the bijective property in kb-
SRK is really helpful for improving the performance
(note that both methods use wildcards). Further-
more, the performances of kb-SRK with and without
combining PR increase dramatically with increasing
kmax and reach the peaks (better than state-of-the-art)
when kmax is four, which shows the power of the lex-
ical and structural similarity captured by kb-SRK.
6.2 Recognizing Textual Entailment
Recognizing textual entailment is to determine
whether a sentence (sometimes a short paragraph)
can entail the other sentence (Giampiccolo et al,
2007). RTE-3 is a widely used benchmark dataset.
Following the common practice, we combined the
development set of RTE-3 and the whole datasets of
RTE-1 and RTE-2 as training data and took the test
set of RTE-3 as test data. The train and test sets con-
tain 3,767 and 800 sentence pairs.
The results are shown in Table 2. Again, kb-SRK
outperforms ps-SRK and pw-SRK. As indicated
in (Heilman and Smith, 2010), the top-performing
RTE systems are often built with significant engi-
Method Acc.
Harmeling (2007) 59.5
de Marneffe et al (2006) 60.5
M&M, (2007) (NL) 59.4
M&M, (2007) (Hybrid) 64.3
Zanzotto et al (2007) 65.75
Heilman and Smith (2010) 62.8
Our baseline (PR) 62.0
Our method (ps-SRK) 64.6
Our method (pw-SRK) 63.8
Our method (kb-SRK) 65.1
Table 2: Comparison with state-of-the-arts on RTE-3.
                           a b * 1  c            
60.5
61.5
62.5
63.5
64.5
65.5
1 2 3 4
Accura
cy (%)
 
w i ndow size kmax 
kb_SR K+ PR
kb_SR K
ps_SRK +PR
pw_SRK +PR
PR
Figure 8: Performances of different kernels under differ-
ent maximum window size kmax on RTE-3.
neering efforts. Therefore, we only compare with
the six systems which involves less engineering. kb-
SRK still outperforms most of those state-of-the-art
methods even if it does not exploit any other lexical
semantic sources and syntactic analysis tools.
Fig. 8 shows the results of the kernels under dif-
ferent parameter settings. Again, the results of ps-
SRK and pw-SRK without combining PR are too
low to be shown (all below 55%). We can see that
PR is an effective method for this dataset and the
overall performances are substantially improved af-
ter combining it with the kernels. The performance
of kb-SRK reaches the peak when window size be-
comes two.
7 Conclusion
In this paper, we have proposed a novel class of ker-
nel functions for sentence re-writing, called string
re-writing kernel (SRK). SRK measures the lexical
and structural similarity between two pairs of sen-
tences without using syntactic trees. The approach
is theoretically sound and is flexible to formulations
of sentences. A specific instance of SRK, referred
to as kb-SRK, has been developed which can bal-
ance the effectiveness and efficiency for sentence
re-writing. Experimental results show that kb-SRK
achieve better results than state-of-the-art methods
on paraphrase identification and recognizing textual
entailment.
Acknowledgments
This work is supported by the National Basic Re-
search Program (973 Program) No. 2012CB316301.
References
Baldrige, J. , Morton, T. and Bierner G. OpenNLP.
http://opennlp.sourceforge.net/.
456
Barzilay, R. and Lee, L. 2003. Learning to paraphrase:
An unsupervised approach using multiple-sequence
alignment. Proceedings of the 2003 Conference of the
North American Chapter of the Association for Com-
putational Linguistics on Human Language Technol-
ogy, pp. 16?23.
Basilico, J. and Hofmann, T. 2004. Unifying collab-
orative and content-based filtering. Proceedings of
the twenty-first international conference on Machine
learning, pp. 9, 2004.
Ben-Hur, A. and Noble, W.S. 2005. Kernel methods for
predicting protein?protein interactions. Bioinformat-
ics, vol. 21, pp. i38?i46, Oxford Univ Press.
Bhagat, R. and Ravichandran, D. 2008. Large scale ac-
quisition of paraphrases for learning surface patterns.
Proceedings of ACL-08: HLT, pp. 674?682.
Chang, C. and Lin, C. 2011. LIBSVM: A library for sup-
port vector machines. ACM Transactions on Intelli-
gent Systems and Technology vol. 2, issue 3, pp. 27:1?
27:27. Software available at http://www.csie.
ntu.edu.tw/?cjlin/libsvm
Das, D. and Smith, N.A. 2009. Paraphrase identifi-
cation as probabilistic quasi-synchronous recognition.
Proceedings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of
the AFNLP, pp. 468?476.
de Marneffe, M., MacCartney, B., Grenager, T., Cer, D.,
Rafferty A. and Manning C.D. 2006. Learning to dis-
tinguish valid textual entailments. Proc. of the Second
PASCAL Challenges Workshop.
Dolan, W.B. and Brockett, C. 2005. Automatically con-
structing a corpus of sentential paraphrases. Proc. of
IWP.
Giampiccolo, D., Magnini B., Dagan I., and Dolan B.,
editors 2007. The third pascal recognizing textual en-
tailment challenge. Proceedings of the ACL-PASCAL
Workshop on Textual Entailment and Paraphrasing,
pp. 1?9.
Harmeling, S. 2007. An extensible probabilistic
transformation-based approach to the third recogniz-
ing textual entailment challenge. Proceedings of the
ACL-PASCAL Workshop on Textual Entailment and
Paraphrasing, pp. 137?142, 2007.
Heilman, M. and Smith, N.A. 2010. Tree edit models for
recognizing textual entailments, paraphrases, and an-
swers to questions. Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Linguis-
tics, pp. 1011-1019.
Kashima, H. , Oyama, S. , Yamanishi, Y. and Tsuda, K.
2009. On pairwise kernels: An efficient alternative
and generalization analysis. Advances in Knowledge
Discovery and Data Mining, pp. 1030-1037, 2009,
Springer.
Kimeldorf, G. and Wahba, G. 1971. Some results on
Tchebycheffian spline functions. Journal of Mathemat-
ical Analysis and Applications, Vol.33, No.1, pp.82-
95, Elsevier.
Lin, D. and Pantel, P. 2001. DIRT-discovery of inference
rules from text. Proc. of ACM SIGKDD Conference
on Knowledge Discovery and Data Mining.
Lintean, M. and Rus, V. 2011. Dissimilarity Kernels
for Paraphrase Identification. Twenty-Fourth Interna-
tional FLAIRS Conference.
Leslie, C. , Eskin, E. and Noble, W.S. 2002. The spec-
trum kernel: a string kernel for SVM protein classifi-
cation. Pacific symposium on biocomputing vol. 575,
pp. 564-575, Hawaii, USA.
Leslie, C. and Kuang, R. 2004. Fast string kernels using
inexact matching for protein sequences. The Journal
of Machine Learning Research vol. 5, pp. 1435-1455.
Lodhi, H. , Saunders, C. , Shawe-Taylor, J. , Cristianini,
N. and Watkins, C. 2002. Text classification using
string kernels. The Journal of Machine Learning Re-
search vol. 2, pp. 419-444.
MacCartney, B. and Manning, C.D. 2008. Modeling se-
mantic containment and exclusion in natural language
inference. Proceedings of the 22nd International Con-
ference on Computational Linguistics, vol. 1, pp. 521-
528, 2008.
Moschitti, A. and Zanzotto, F.M. 2007. Fast and Effec-
tive Kernels for Relational Learning from Texts. Pro-
ceedings of the 24th Annual International Conference
on Machine Learning, Corvallis, OR, USA, 2007.
Qiu, L. and Kan, M.Y. and Chua, T.S. 2006. Para-
phrase recognition via dissimilarity significance clas-
sification. Proceedings of the 2006 Conference on
Empirical Methods in Natural Language Processing,
pp. 18?26.
Quirk, C. , Brockett, C. and Dolan, W. 2004. Monolin-
gual machine translation for paraphrase generation.
Proceedings of EMNLP 2004, pp. 142-149, Barcelona,
Spain.
Scho?lkopf, B. and Smola, A.J. 2002. Learning with
kernels: Support vector machines, regularization, op-
timization, and beyond. The MIT Press, Cambridge,
MA.
Vapnik, V.N. 2000. The nature of statistical learning
theory. Springer Verlag.
Wan, S. , Dras, M. , Dale, R. and Paris, C. 2006. Using
dependency-based features to take the ?Para-farce?
out of paraphrase. Proc. of the Australasian Language
Technology Workshop, pp. 131?138.
Zanzotto, F.M. , Pennacchiotti, M. and Moschitti, A.
2007. Shallow semantics in fast textual entailment
457
rule learners. Proceedings of the ACL-PASCAL
workshop on textual entailment and paraphrasing, pp.
72?77.
Zhang, Y. and Patrick, J. 2005. Paraphrase identifica-
tion by text canonicalization. Proceedings of the Aus-
tralasian Language Technology Workshop, pp. 160?
166.
458
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 187?192,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Automatically Mining Question Reformulation Patterns from
Search Log Data
Xiaobing Xue?
Univ. of Massachusetts, Amherst
xuexb@cs.umass.edu
Yu Tao?
Univ. of Science and Technology of China
v-yutao@microsoft.com
Daxin Jiang Hang Li
Microsoft Research Asia
{djiang,hangli}@microsoft.com
Abstract
Natural language questions have become pop-
ular in web search. However, various ques-
tions can be formulated to convey the same
information need, which poses a great chal-
lenge to search systems. In this paper, we au-
tomatically mined 5w1h question reformula-
tion patterns from large scale search log data.
The question reformulations generated from
these patterns are further incorporated into the
retrieval model. Experiments show that us-
ing question reformulation patterns can sig-
nificantly improve the search performance of
natural language questions.
1 Introduction
More and more web users tend to use natural lan-
guage questions as queries for web search. Some
commercial natural language search engines such as
InQuira and Ask have also been developed to answer
this type of queries. One major challenge is that var-
ious questions can be formulated for the same infor-
mation need. Table 1 shows some alternative expres-
sions for the question ?how far is it from Boston to
Seattle?. It is difficult for search systems to achieve
satisfactory retrieval performance without consider-
ing these alternative expressions.
In this paper, we propose a method of automat-
ically mining 5w1h question1 reformulation pat-
terns to improve the search relevance of 5w1h ques-
tions. Question reformulations represent the alter-
native expressions for 5w1h questions. A question
?Contribution during internship at Microsoft Research Asia
15w1h questions start with ?Who?, ?What?, ?Where?,
?When?, ?Why? and ?How?.
Table 1: Alternative expressions for the original question
Original Question:
how far is it from Boston to Seattle
Alternative Expressions:
how many miles is it from Boston to Seattle
distance from Boston to Seattle
Boston to Seattle
how long does it take to drive from Boston to Seattle
reformulation pattern generalizes a set of similar
question reformulations that share the same struc-
ture. For example, users may ask similar questions
?how far is it from X1 to X2? where X1 and X2
represent some other cities besides Boston and Seat-
tle. Then, similar question reformulations as in Ta-
ble 1 will be generated with the city names changed.
These patterns increase the coverage of the system
by handling the queries that did not appear before
but share similar structures as previous queries.
Using reformulation patterns as the key concept,
we propose a question reformulation framework.
First, we mine the question reformulation patterns
from search logs that record users? reformulation
behavior. Second, given a new question, we use
the most relevant reformulation patterns to generate
question reformulations and each of the reformula-
tions is associated with its probability. Third, the
original question and these question reformulations
are then combined together for retrieval.
The contributions of this paper are summarized as
two folds. First, we propose a simple yet effective
approach to automatically mine 5w1h question re-
formulation patterns. Second, we conduct compre-
hensive studies in improving the search performance
of 5w1h questions using the mined patterns.
187
Generating 
Reformulation 
Patterns  
 Search 
Log 
Set = { (q,qr)}  
Pattern 
Base 
P = { (p,pr)}  
O ffline Phase  
qnew 
Generating 
Question 
Reformulations 
{ qrnew}  Retrieval Model  { D }  
New Question  
Question 
Reformulation  
Retrieved 
Documents  
O nline Phase  
Figure 1: The framework of reformulating questions.
2 Related Work
In the Natural Language Processing (NLP) area, dif-
ferent expressions that convey the same meaning
are referred as paraphrases (Lin and Pantel, 2001;
Barzilay and McKeown, 2001; Pang et al, 2003;
Pas?ca and Dienes, 2005; Bannard and Callison-
Burch, 2005; Bhagat and Ravichandran, 2008;
Callison-Burch, 2008; Zhao et al, 2008). Para-
phrases have been studied in a variety of NLP appli-
cations such as machine translation (Kauchak and
Barzilay, 2006; Callison-Burch et al, 2006), ques-
tion answering (Ravichandran and Hovy, 2002) and
document summarization (McKeown et al, 2002).
Yet, little research has considered improving web
search performance using paraphrases.
Query logs have become an important resource
for many NLP applications such as class and at-
tribute extraction (Pas?ca and Van Durme, 2008),
paraphrasing (Zhao et al, 2010) and language mod-
eling (Huang et al, 2010). Little research has been
conducted to automatically mine 5w1h question re-
formulation patterns from query logs.
Recently, query reformulation (Boldi et al, 2009;
Jansen et al, 2009) has been studied in web search.
Different techniques have been developed for query
segmentation (Bergsma and Wang, 2007; Tan and
Peng, 2008) and query substitution (Jones et al,
2006; Wang and Zhai, 2008). Yet, most previous
research focused on keyword queries without con-
sidering 5w1h questions.
3 Mining Question Reformulation
Patterns for Web Search
Our framework consists of three major components,
which is illustrated in Fig. 1.
Table 2: Question reformulation patterns generated for
the query pair (?how far is it from Boston to Seattle?
,?distance from Boston to Seattle?).
S1 = {Boston}:(?how far is it from X1 to Seattle?
,?distance from X1 to Seattle?)
S2 = {Seattle}:(?how far is it from Boston to X1?
,?distance from Boston to X1?)
S3 = {Boston, Seattle}:(?how far is it fromX1 toX2?
,?distance from X1 to X2?)
3.1 Generating Reformulation Patterns
From the search log, we extract all successive query
pairs issued by the same user within a certain time
period where the first query is a 5w1h question. In
such query pair, the second query is considered as
a question reformulation. Our method takes these
query pairs, i.e. Set = {(q, qr)}, as the input and
outputs a pattern base consisting of 5w1h question
reformulation patterns, i.e. P = {(p, pr)}). Specif-
ically, for each query pair (q, qr), we first collect all
common words between q and qr except for stop-
words ST 2, where CW = {w|w ? q, w ? q?, w /?
ST}. For any non-empty subset Si of CW , the
words in Si are replaced as slots in q and qr to con-
struct a reformulation pattern. Table 2 shows exam-
ples of question reformulation patterns. Finally, the
patterns observed in many different query pairs are
kept. In other words, we rely on the frequency of a
pattern to filter noisy patterns. Generating patterns
using more NLP features such as the parsing infor-
mation will be studied in the future work.
3.2 Generating Question Reformulations
We describe how to generate a set of question refor-
mulations {qnewr } for an unseen question qnew.
First, we search P = {(p, pr)} to find all ques-
tion reformulation patterns where p matches qnew.
Then, we pick the best question pattern p? accord-
ing to the number of prefix words and the total num-
ber of words in a pattern. We select the pattern that
has the most prefix words, since this pattern is more
likely to have the same information as qnew. If sev-
eral patterns have the same number of prefix words,
we use the total number of words to break the tie.
After picking the best question pattern p?, we fur-
ther rank all question reformulation patterns con-
taining p?, i.e. (p?, pr), according to Eq. 1.
2Stopwords refer to the function words that have little mean-
ing by themselves, such as ?the?, ?a?, ?an?, ?that? and ?those?.
188
Table 3: Examples of the question reformulations and their corresponding reformulation patterns
qnew: how good is the eden pure air system qnew : how to market a restaurant
p?: how good is the X p?: how to market a X
qnewr pr qnewr pr
eden pure air system X marketing a restaurant marketing a X
eden pure air system review X review how to promote a restaurant how to promote a X
eden pure air system reviews X reviews how to sell a restaurant how to sell a X
rate the eden pure air system rate the X how to advertise a restaurant how to advertise a X
reviews on the eden pure air system reviews on the X restaurant marketing X marketing
P (pr|p?) =
f(p?, pr)
?
p?r f(p
?, p?r)
(1)
Finally, we generate k question reformulations
qnewr by applying the top k question reformulation
patterns containing p?. The probability P (pr|p?) as-
sociated with the pattern (p?, pr) is assigned to the
corresponding question reformulation qnewr .
3.3 Retrieval Model
Given the original question qnew and k question re-
formulations {qnewr }, the query distribution model
(Xue and Croft, 2010) (denoted as QDist) is adopted
to combine qnew and {qnewr } using their associated
probabilities. The retrieval score of the documentD,
i.e. score(qnew,D), is calculated as follows:
score(qnew,D) = ? log P (qnew|D)
+(1 ? ?)
k
?
i=1
P (pri |p?) log P (qnewri |D) (2)
In Eq. 2, ? is a parameter that indicates the prob-
ability assigned to the original query. P (pri |p?) is
the probability assigned to qnewri . P (qnew|D) and
P (q?|D) are calculated using the language model
(Ponte and Croft, 1998; Zhai and Lafferty, 2001).
4 Experiments
A large scale search log from a commercial search
engine (2011.1-2011.6) is used in experiments.
From the search log, we extract all successive query
pairs issued by the same user within 30 minutes
(Boldi et al, 2008)3 where the first query is a 5w1h
question. Finally, we extracted 6,680,278 question
reformulation patterns.
For the retrieval experiments, we randomly sam-
ple 10,000 natural language questions as queries
3In web search, queries issued within 30 minutes are usually
considered having the same information need.
Table 4: Retrieval Performance of using question refor-
mulations. ? denotes significantly different with Orig.
NDCG@1 NDCG@3 NDCG@5
Orig 0.2946 0.2923 0.2991
QDist 0.3032? 0.2991? 0.3067?
from the search log before 2011. For each question,
we generate the top ten questions reformulations.
The Indri toolkit4 is used to implement the language
model. A web collection from a commercial search
engine is used for retrieval experiments. For each
question, the relevance judgments are provided by
human annotators. The standard NDCG@k is used
to measure performance.
4.1 Examples and Performance
Table 3 shows examples of the generated questions
reformulations. Several interesting expressions are
generated to reformulate the original question.
We compare the retrieval performance of using
the question reformulations (QDist) with the perfor-
mance of using the original question (Orig) in Table
4. The parameter ? of QDist is decided using ten-
fold cross validation. Two sided t-test are conducted
to measure significance.
Table 4 shows that using the question reformula-
tions can significantly improve the retrieval perfor-
mance of natural language questions. Note that, con-
sidering the scale of experiments (10,000 queries),
around 3% improvement with respect to NDCG is a
very interesting result for web search.
4.2 Analysis
In this subsection, we analyze the results to better
understand the effect of question reformulations.
First, we report the performance of always pick-
ing the best question reformulation for each query
(denoted as Upper) in Table 5, which provides an
4www.lemurproject.org/
189
Table 5: Performance of the upper bound.
NDCG@1 NDCG@3 NDCG@5
Orig 0.2946 0.2923 0.2991
QDist 0.3032 0.2991 0.3067
Upper 0.3826 0.3588 0.3584
Table 6: Best reformulation within different positions.
top 1 within top 2 within top 3
49.2% 64.7% 75.4%
upper bound for the performance of the question re-
formulation. Table 5 shows that if we were able
to always picking the best question reformulation,
the performance of Orig could be improved by
around 30% (from 0.2926 to 0.3826 with respect to
NDCG@1). It indicates that we do generate some
high quality question reformulations.
Table 6 further reports the percent of those 10,000
queries where the best question reformulation can be
observed in the top 1 position, within the top 2 posi-
tions and within the top 3 positions, respectively.
Table 6 shows that for most queries, our method
successfully ranks the best reformulation within the
top 3 positions.
Second, we study the effect of different types
of question reformulations. We roughly divide the
question reformulations generated by our method
into five categories as shown in Table 7. For each
category, we report the percent of reformulations
which performance is bigger/smaller/equal with re-
spect to the original question.
Table 7 shows that the ?more specific? reformula-
tions and the ?equivalent? reformulations are more
likely to improve the original question. Reformu-
lations that make ?morphological change? do not
have much effect on improving the original ques-
tion. ?More general? and ?not relevant? reformu-
lations usually decrease the performance.
Third, we conduct the error analysis on the ques-
tion reformulations that decrease the performance
of the original question. Three typical types of er-
rors are observed. First, some important words are
removed from the original question. For example,
?what is the role of corporate executives? is reformu-
lated as ?corporate executives?. Second, the refor-
mulation is too specific. For example, ?how to effec-
tively organize your classroom? is reformulated as
?how to effectively organize your elementary class-
room?. Third, some reformulations entirely change
Table 7: Analysis of different types of reformulations.
Type increase decrease same
Morphological change 11% 10% 79%
Equivalent meaning 32% 30% 38%
More specific/Add words 45% 39% 16%
More general/Remove words 38% 48% 14%
Not relevant 14% 72% 14%
Table 8: Retrieval Performance of other query processing
techniques.
NDCG@1 NDCG@3 NDCG@5
ORIG 0.2720 0.2937 0.3151
NoStop 0.2697 0.2893 0.3112
DropOne 0.2630 0.2888 0.3102
QDist 0.2978 0.3052 0.3250
the meaning of the original question. For example,
?what is the adjective of anxiously? is reformulated
as ?what is the noun of anxiously?.
Fourth, we compare our question reformulation
method with two long query processing techniques,
i.e. NoStop (Huston and Croft, 2010) and DropOne
(Balasubramanian et al, 2010). NoStop removes all
stopwords in the query and DropOne learns to drop
a single word from the query. The same query set as
Balasubramanian et al (2010) is used. Table 8 re-
ports the retrieval performance of different methods.
Table 8 shows that both NoStop and DropOne per-
form worse than using the original question, which
indicates that the general techniques developed for
long queries are not appropriate for natural language
questions. On the other hand, our proposed method
outperforms all the baselines.
5 Conclusion
Improving the search relevance of natural language
questions poses a great challenge for search systems.
We propose to automatically mine 5w1h question re-
formulation patterns from search log data. The ef-
fectiveness of the extracted patterns has been shown
on web search. These patterns are potentially useful
for many other applications, which will be studied in
the future work. How to automatically classify the
extracted patterns is also an interesting future issue.
Acknowledgments
We would like to thank W. Bruce Croft for his sug-
gestions and discussions.
190
References
N. Balasubramanian, G. Kumaran, and V.R. Carvalho.
2010. Exploring reductions for long web queries. In
Proceeding of the 33rd international ACM SIGIR con-
ference on Research and development in information
retrieval, pages 571?578. ACM.
C. Bannard and C. Callison-Burch. 2005. Paraphras-
ing with bilingual parallel corpora. In Proceedings of
the 43rd Annual Meeting on Association for Compu-
tational Linguistics, pages 597?604. Association for
Computational Linguistics.
R. Barzilay and K.R. McKeown. 2001. Extracting para-
phrases from a parallel corpus. In Proceedings of
the 39th Annual Meeting on Association for Computa-
tional Linguistics, pages 50?57. Association for Com-
putational Linguistics.
S. Bergsma and Q. I. Wang. 2007. Learning noun
phrase query segmentation. In EMNLP-CoNLL07,
pages 819?826, Prague.
R. Bhagat and D. Ravichandran. 2008. Large scale ac-
quisition of paraphrases for learning surface patterns.
Proceedings of ACL-08: HLT, pages 674?682.
Paolo Boldi, Francesco Bonchi, Carlos Castillo, Deb-
ora Donato, Aristides Gionis, and Sebastiano Vigna.
2008. The query-flow graph: model and applications.
In CIKM08, pages 609?618.
P. Boldi, F. Bonchi, C. Castillo, and S. Vigna. 2009.
From ?Dango? to ?Japanese Cakes?: Query refor-
mulation models and patterns. In Web Intelligence
and Intelligent Agent Technologies, 2009. WI-IAT?09.
IEEE/WIC/ACM International Joint Conferences on,
volume 1, pages 183?190. IEEE.
C. Callison-Burch, P. Koehn, and M. Osborne. 2006.
Improved statistical machine translation using para-
phrases. In Proceedings of the main conference on
Human Language Technology Conference of the North
American Chapter of the Association of Computa-
tional Linguistics, pages 17?24. Association for Com-
putational Linguistics.
C. Callison-Burch. 2008. Syntactic constraints on para-
phrases extracted from parallel corpora. In Proceed-
ings of the Conference on Empirical Methods in Natu-
ral Language Processing, pages 196?205. Association
for Computational Linguistics.
Jian Huang, Jianfeng Gao, Jiangbo Miao, Xiaolong Li,
Kuansan Wang, Fritz Behr, and C. Lee Giles. 2010.
Exploring web scale language models for search query
processing. In WWW10, pages 451?460, New York,
NY, USA. ACM.
S. Huston and W.B. Croft. 2010. Evaluating ver-
bose query processing techniques. In Proceeding
of the 33rd international ACM SIGIR conference on
Research and development in information retrieval,
pages 291?298. ACM.
B.J. Jansen, D.L. Booth, and A. Spink. 2009. Patterns
of query reformulation during web searching. Journal
of the American Society for Information Science and
Technology, 60(7):1358?1371.
R. Jones, B. Rey, O. Madani, andW. Greiner. 2006. Gen-
erating query substitutions. In WWW06, pages 387?
396, Ediburgh, Scotland.
D. Kauchak and R. Barzilay. 2006. Paraphrasing for
automatic evaluation.
D.-K. Lin and P. Pantel. 2001. Discovery of inference
rules for question answering. Natural Language Pro-
cessing, 7(4):343?360.
K.R. McKeown, R. Barzilay, D. Evans, V. Hatzi-
vassiloglou, J.L. Klavans, A. Nenkova, C. Sable,
B. Schiffman, and S. Sigelman. 2002. Tracking and
summarizing news on a daily basis with columbia?s
newsblaster. In Proceedings of the second interna-
tional conference on Human Language Technology
Research, pages 280?285.MorganKaufmann Publish-
ers Inc.
B. Pang, K. Knight, and D. Marcu. 2003. Syntax-based
alignment of multiple translations: Extracting para-
phrases and generating new sentences. In Proceedings
of the 2003 Conference of the North American Chap-
ter of the Association for Computational Linguistics on
Human Language Technology-Volume 1, pages 102?
109. Association for Computational Linguistics.
M. Pas?ca and P. Dienes. 2005. Aligning needles in a
haystack: Paraphrase acquisition across the web. Nat-
ural Language Processing?IJCNLP 2005, pages 119?
130.
M. Pas?ca and B. Van Durme. 2008. Weakly-supervised
acquisition of open-domain classes and class attributes
from web documents and query logs. In Proceed-
ings of the 46th Annual Meeting of the Association for
Computational Linguistics (ACL-08), pages 19?27.
J. M. Ponte and W. B. Croft. 1998. A language modeling
approach to information retrieval. In SIGIR98, pages
275?281, Melbourne, Australia.
D. Ravichandran and E. Hovy. 2002. Learning sur-
face text patterns for a question answering system. In
ACL02, pages 41?47.
B. Tan and F. Peng. 2008. Unsupervised query
segmentation using generative language models and
Wikipedia. In WWW08, pages 347?356, Bei-
jing,China.
X. Wang and C. Zhai. 2008. Mining term association
patterns from search logs for effective query reformu-
lation. In CIKM08, pages 479?488, Napa Valley, CA.
X. Xue and W. B. Croft. 2010. Representing queries
as distributions. In SIGIR10 Workshop on Query Rep-
191
resentation and Understanding, pages 9?12, Geneva,
Switzerland.
C. Zhai and J. Lafferty. 2001. A study of smoothing
methods for language models applied to ad hoc infor-
mation retrieval. In SIGIR01, pages 334?342, New
Orleans, LA.
S. Zhao, H. Wang, T. Liu, and S. Li. 2008. Pivot ap-
proach for extracting paraphrase patterns from bilin-
gual corpora. Proceedings of ACL-08: HLT, pages
780?788.
S. Zhao, H. Wang, and T. Liu. 2010. Paraphrasing with
search engine query logs. In Proceedings of the 23rd
International Conference on Computational Linguis-
tics, pages 1317?1325. Association for Computational
Linguistics.
192
Proceedings of the Second Workshop on NLP Challenges in the Information Explosion Era (NLPIX 2010), page 1,
Beijing, August 2010
Query Understanding in Web Search
- by Large Scale Log Data Mining and Statistical Learning
Hang Li
Microsoft Research Asia, China
Abstract
Query understanding is an important component of web search, like document understanding,
query document matching, ranking, and user understanding. The goal of query understanding is
to predict the user?s search intent from the given query. Needless to say, search log mining and
statistical learning are fundamental technologies to address the task of query understanding. In
this talk, I will first introduce a large-scale search log mining platform which we have devel-
oped at MSRA. I will then explain our approach to query understanding, as well as document
understanding, query document matching, and user understanding. After that, I will describe in
details about our methods for query understanding based on statistical learning. They include
query refinement using CRF, named entity recognition in query using topic model, context
aware query topic prediction using HMM.
This is joint work with Gu Xu, Daxin Jiang and other collaborators.
1
