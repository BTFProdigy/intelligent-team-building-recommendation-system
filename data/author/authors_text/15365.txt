Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1183?1193,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Nouns are vectors, adjectives are matrices:
Representing adjective-noun constructions in semantic space
Marco Baroni and Roberto Zamparelli
Center for Mind/Brain Sciences, University of Trento
Rovereto (TN), Italy
{marco.baroni,roberto.zamparelli}@unitn.it
Abstract
We propose an approach to adjective-noun
composition (AN) for corpus-based distribu-
tional semantics that, building on insights
from theoretical linguistics, represents nouns
as vectors and adjectives as data-induced (lin-
ear) functions (encoded as matrices) over
nominal vectors. Our model significantly out-
performs the rivals on the task of reconstruct-
ing AN vectors not seen in training. A small
post-hoc analysis further suggests that, when
the model-generated AN vector is not simi-
lar to the corpus-observed AN vector, this is
due to anomalies in the latter. We show more-
over that our approach provides two novel
ways to represent adjective meanings, alter-
native to its representation via corpus-based
co-occurrence vectors, both outperforming the
latter in an adjective clustering task.
1 Introduction
An influential approach for representing the mean-
ing of a word in NLP is to treat it as a vector
that codes the pattern of co-occurrence of that word
with other expressions in a large corpus of language
(Sahlgren, 2006; Turney and Pantel, 2010). This
approach to semantics (sometimes called distribu-
tional semantics) naturally captures word cluster-
ing, scales well to large lexicons and doesn?t re-
quire words to be manually disambiguated (Schu?tze,
1997). However, until recently it has been limited to
the level of content words (nouns, adjectives, verbs),
and it hasn?t tackled in a general way compositional-
ity (Frege, 1892; Partee, 2004), that crucial property
of natural language which allows speakers to de-
rive the meaning of a complex linguistic constituent
from the meaning of its immediate syntactic subcon-
stituents.
Formal semantics (FS), the research program
stemming from Montague (1970b; 1973), has oppo-
site strengths and weaknesses. Its core semantic no-
tion is the sentence, not the word; at the lexical level,
it focuses on the meaning of function words; one
of its main goals is to formulate recursive composi-
tional rules that derive the quantificational properties
of complex sentences and their antecedent-pronoun
dependencies.
Given its focus on quantification, FS treats the
meanings of nouns and verbs as pure extensions:
nouns and (intransitive) verbs are properties, and
thus denote sets of individuals. Adjectives are also
often assumed to denote properties: in this view
redadj would be the set of ?entities which are red?,
plasticadj , the set of ?objects made of plastic?, and
so forth. In the simplest case, the meaning of an at-
tributive adjective-noun (AN) constituent can be ob-
tained as the intersection of the adjective and noun
extensions A?N:
[ red car ] = {. . . red objects. . . } ? {. . . cars. . . }
However, the intersective method of combination
is well-known to fail in many cases (Kamp, 1975;
Montague, 1970a; Siegel, 1976): for instance, a
fake gun is not a gun. Even for red, the manner in
which the color combines with a noun will be dif-
ferent in red Ferrari (the outside), red watermelon
(the inside), red traffic light (the signal). These prob-
lems have prompted a more flexible FS representa-
tion for attributive adjectives ? functions from the
meaning of a noun onto the meaning of a modified
noun (Montague, 1970a). This mapping could now
be sensitive to the particular noun the adjective re-
ceives, and it does not need to return a subset of the
1183
original noun denotation (as in the case of fake N).
However, FS has nothing to say on how these func-
tions should be constructed.
In the last few years there have been attempts to
build compositional models that use distributional
semantic representations as inputs (see Section 2 be-
low), most of them focusing on the combination of a
verb and its arguments. This paper addresses instead
the combination of nouns and attributive adjectives.
This case was chosen as an interesting testbed be-
cause it has the property of recursivity (it applies in
black dog, but also in large black dog, etc.), and be-
cause very frequent adjectives such as different are
at the border between content and function words.
Following the insight of FS, we treat attributive ad-
jectives as functions over noun meanings; however,
noun meanings are vectors, not sets, and the func-
tions are learnt from corpus-based noun-AN vector
pairs.
Original contribution We propose and evaluate a
new method to derive distributional representations
for ANs, where an adjective is a linear function from
a vector (the noun representation) to another vector
(the AN representation). The linear map for a spe-
cific adjective is learnt, using linear regression, from
pairs of noun and AN vectors extracted from a cor-
pus.
Outline Distributional approaches to composi-
tionality are shortly reviewed in Section 2. In Sec-
tion 3, we introduce our proposal. The experimen-
tal setting is described in Section 4. Section 5 pro-
vides some empirical justification for using corpus-
harvested AN vectors as the target of our function
learning and evaluation benchmark. In Section 6, we
show that our model outperforms other approaches
at the task of approximating such vectors for unseen
ANs. In Section 7, we discuss how adjectival mean-
ing can be represented in our model and evaluate this
representation in an adjective clustering task. Sec-
tion 8 concludes by sketching directions for further
work.
2 Related work
The literature on compositionality in vector-based
semantics encompasses various related topics, some
of them not of direct interest here, such as how to
encode word order information in context vectors
(Jones and Mewhort, 2007; Sahlgren et al, 2008)
or sophisticated composition methods based on ten-
sor products, quantum logic, etc., that have not yet
been empirically tested on large-scale corpus-based
semantic space tasks (Clark and Pulman, 2007;
Rudolph and Giesbrecht, 2010; Smolensky, 1990;
Widdows, 2008). Closer to our current purposes is
the general framework for vector composition pro-
posed by Mitchell and Lapata (2008), subsuming
various earlier proposals. Given two vectors u and
v, they identify two general classes of composition
models, (linear) additive models:
p = Au+Bv (1)
where A and B are weight matrices, and multiplica-
tive models:
p = Cuv
where C is a weight tensor projecting the uv tensor
product onto the space of p. Mitchell and Lapata de-
rive two simplified models from these general forms.
Their simplified additive model p = ?u+?v was a
common approach to composition in the earlier liter-
ature, typically with the scalar weights set to 1 or to
normalizing constants (Foltz et al, 1998; Kintsch,
2001; Landauer and Dumais, 1997). Mitchell and
Lapata also consider a constrained version of the
multiplicative approach that reduces to component-
wise multiplication, where the i-th component of
the composed vector is given by: pi = uivi. The
simplified additive model produces a sort of (sta-
tistical) union of features, whereas component-wise
multiplication has an intersective effect. They also
evaluate a weighted combination of the simplified
additive and multiplicative functions. The best re-
sults on the task of paraphrasing noun-verb combi-
nations with ambiguous verbs (sales slump is more
like declining than slouching) are obtained using the
multiplicative approach, and by weighted combina-
tion of addition and multiplication (we do not test
model combinations in our current experiments).
The multiplicative approach also performs best (but
only by a small margin) in a later application to lan-
guage modeling (Mitchell and Lapata, 2009). Erk
and Pado? (2008; 2009) adopt the same formalism
but focus on the nature of input vectors, suggest-
ing that when a verb is composed with a noun, the
noun component is given by an average of verbs that
the noun is typically object of (along similar lines,
1184
Kintsch (2001) also focused on composite input vec-
tors, within an additive framework). Again, the mul-
tiplicative model works best in Erk and Pado??s ex-
periments.
The above-mentioned researchers do not exploit
corpus evidence about the p vectors that result from
composition, despite the fact that it is straightfor-
ward (at least for short constructions) to extract
direct distributional evidence about the composite
items from the corpus (just collect co-occurrence
information for the composite item from windows
around the contexts in which it occurs). The
main innovation of Guevara (2010), who focuses on
adjective-noun combinations (AN), is to use the co-
occurrence vectors of observed ANs to train a su-
pervised composition model (we became aware of
Guevara?s approach after we had developed our own
model, that also exploits observed ANs for training).
Guevara adopts the full additive composition form
from Equation (1) and he estimates the A and B
weights using partial least squares regression. The
training data are pairs of adjective-noun vector con-
catenations, as input, and corpus-derived AN vec-
tors, as output. Guevara compares his model to
the simplified additive and multiplicative models of
Mitchell and Lapata. Observed ANs are nearer, in
the space of observed and predicted test set ANs, to
the ANs generated by his model than to those from
the alternative approaches. The additive model, on
the other hand, is best in terms of shared neighbor
count between observed and predicted ANs.
In our empirical tests, we compare our approach
to the simplified additive and multiplicative models
of Mitchell and Lapata (the former with normaliza-
tion constants as scalar weights) as well as to Gue-
vara?s approach.
3 Adjectives as linear maps
As discussed in the introduction, we will take ad-
jectives in attributive position to be functions from
one noun meaning to another. To start simple, we
assume here that adjectives in the attributive posi-
tion (AN) are linear functions from n-dimensional
(noun) vectors onto n-dimensional vectors, an oper-
ation that can be expressed as multiplication of the
input noun column vector by a n ? n matrix, that
is our representation for the adjective (in the lan-
guage of linear algebra, an adjective is an endomor-
phic linear map in noun space). In the framework of
Mitchell and Lapata, our approach derives from the
additive form in Equation (1) with the matrix multi-
plying the adjective vector (say, A) set to 0:
p = Bv
where p is the observed AN vector, B the weight
matrix representing the adjective at hand, and v a
noun vector. In our approach, the weight matrix B is
specific to a single adjective ? as we will see in Sec-
tion 7 below, it is our representation of the meaning
of the adjective.
Like Guevara, we estimate the values in the
weight matrix by partial least squares regression.
In our case, the independent variables for the re-
gression equations are the dimensions of the corpus-
based vectors of the component nouns, whereas the
AN vectors provide the dependent variables. Unlike
Guevara, (i) we train separate models for each adjec-
tive (we learn adjective-specific functions, whereas
Guevara learns a generic ?AN-slot? function) and,
consequently, (ii) corpus-harvested adjective vectors
play no role for us (their values would be constant
across the training input vectors).
A few considerations are in order. First, although
we use a supervised learning method (least squares
regression), we do not need hand-annotated data,
since the target AN vectors are automatically col-
lected from the corpus just like vectors for single
words are. Thus, there is no extra ?external knowl-
edge? cost with respect to unsupervised approaches.
Second, our approach rests on the assumption that
the corpus-derived AN vectors are interesting ob-
jects that should constitute the target of what a com-
position process tries to approximate. We provide
preliminary empirical support for this assumption in
Section 5 below. Third, we have some reasonable
hope that our functions can capture to a certain ex-
tent the polysemous nature of adjectives: we could
learn, for example, a green matrix with large posi-
tive weights mapping from noun features that per-
tain to concrete objects to color dimensions of the
output vector (green chair), as well as large positive
weights from features characterizing certain classes
of abstract concepts to political/social dimensions in
the output (green initiative). Somewhat optimisti-
cally, we hope that chair will have near-0 values
1185
on the relevant abstract dimensions, like initiative
on the concrete features, and thus the weights will
not interfere. We do not evaluate this claim specif-
ically, but our quantitative evaluation in Section 6
shows that our approach does best with high fre-
quency, highly ambiguous adjectives. Fourth, the
approach is naturally syntax-sensitive, since we train
it on observed data for a specific syntactic position:
we would train separate linear models for, say, the
same adjective in attributive (AN) and predicative
(N is A) position. As a matter of fact, the current
model is too syntax-sensitive and does not capture
similarities across different constructions. Finally,
although adjective representations are not directly
harvested from corpora, we can still meaningfully
compare adjectives to each other or other words by
using their estimated matrix, or an average vector for
the ANs that contain them: both options are tested
in Section 7 below.
4 Experimental setup
4.1 Corpus
We built a large corpus by concatenating the
Web-derived ukWaC corpus (http://wacky.
sslmit.unibo.it/), a mid-2009 dump of the
English Wikipedia (http://en.wikipedia.
org) and the British National Corpus (http:
//www.natcorp.ox.ac.uk/). This concate-
nated corpus, tokenized, POS-tagged and lemma-
tized with the TreeTagger (Schmid, 1995), contains
about 2.83 billion tokens (excluding punctuation,
digits, etc.). The ukWaC and Wikipedia sections can
be freely downloaded, with full annotation, from the
ukWaC site.
We performed some of the list extraction and
checking operations we are about to describe on a
more manageable data-set obtained by selecting the
first 100M tokens of ukWaC; we refer to this subset
as the sample corpus below.
4.2 Vocabulary
We could in principle limit ourselves to collecting
vectors for the ANs to be analyzed (the AN test set)
and their components. However, to make the anal-
ysis more challenging and interesting, we populate
the semantic space where we will look at the be-
haviour of the ANs with a large number of adjectives
and nouns, as well as further ANs not in the test set.
We refer to the overall list of items we build seman-
tic vectors for as the extended vocabulary. We use
a subset of the extended vocabulary containing only
nouns and adjectives (the core vocabulary) for fea-
ture selection and dimensionality reduction, so that
we do not implicitly bias the structure of the seman-
tic space by our choice of ANs.
To construct the AN test set, we first selected 36
adjectives across various classes: size (big, great,
huge, large, major, small, little), denominal (Amer-
ican, European, national, mental, historical, elec-
tronic), colors (white, black, red, green) positive
evaluation (nice, excellent, important, appropriate),
temporal (old, recent, new, young, current), modal
(necessary, possible), plus some common abstract
antonymous pairs (difficult, easy, good, bad, spe-
cial, general, different, common). We were care-
ful to include intersective cases such as electronic
as well as non-intersective adjectives that are almost
function words (the modals, different, etc.). We ex-
tracted all nouns that occurred at least 300 times
in post-adjectival position in the sample corpus, ex-
cluding some extremely frequent temporal and mea-
sure expressions such as time and range, for a to-
tal of 1,420 distinct nouns. By crossing the selected
adjectives and nouns, we constructed a test set con-
taining 26,440 ANs, all attested in the sample cor-
pus (734 ANs per adjective on average, ranging from
1,337 for new to 202 for mental).
The core vocabulary contains the top 8K most
frequent noun lemmas and top 4K adjective lemmas
from the concatenated corpus (excluding the top 50
most frequent nouns and adjectives). The extended
vocabulary contains this core plus (i) the 26,440
test ANs, (ii) the 16 adjectives and 43 nouns that
are components of these ANs and that are not in the
core set, and (iii) 2,500 more ANs randomly sam-
pled from those that are attested in the sample cor-
pus, have a noun from the same list used for the test
set ANs, and an adjective that occurred at least 5K
times in the sample corpus. In total, the extended
vocabulary contains 40,999 entries: 8,043 nouns,
4,016 adjectives and 28,940 ANs.
4.3 Semantic space construction
Full co-occurrence matrix The 10K lemmas
(nouns, adjectives or verbs) that co-occur with
1186
the largest number of items in the core vocabu-
lary constitute the dimensions (columns) of our co-
occurrence matrix. Using the concatenated corpus,
we extract sentence-internal co-occurrence counts of
all the items in the extended vocabulary with the
10K dimension words. We then transform the raw
counts into Local Mutual Information (LMI) scores
(LMI is an association measure that closely approx-
imates the Log-Likelihood Ratio, see Evert (2005)).
Dimensionality reduction Since, for each test set
adjective, we need to estimate a regression model
for each dimension, we want a compact space with
relatively few, dense dimensions. A natural way to
do this is to apply the Singular Value Decomposi-
tion (SVD) to the co-occurrence matrix, and repre-
sent the items of interest with their coordinates in
the space spanned by the first n right singular vec-
tors. Applying SVD is independently justified be-
cause, besides mitigating the dimensionality prob-
lem, it often improves the quality of the semantic
space (Landauer and Dumais, 1997; Rapp, 2003;
Schu?tze, 1997). To avoid bias in favour of dimen-
sions that capture variance in the test set ANs, we
applied SVD to the core vocabulary subset of the
co-occurrence matrix (containing only adjective and
noun rows). The core 12K?10K matrix was re-
duced using SVD to a 12K?300 matrix. The other
row vectors of the full co-occurrence matrix (in-
cluding the ANs) were projected onto the same re-
duced space by multiplying them by a matrix con-
taining the first n right singular vectors as columns.
Merging the items used to compute the SVD and
those projected onto the resulting space, we obtain a
40,999?300 matrix representing 8,043 nouns, 4,016
adjectives and 28,940 ANs. This reduced matrix
constitutes a realistically sized semantic space, that
also contains many items that are not part of our test
set, but will be potential neighbors of the observed
and predicted test ANs in the experiments to follow.
The quality of the SVD reduction itself was indepen-
dently validated on a standard similarity judgment
data-set (Rubenstein and Goodenough, 1965), ob-
taining similar (and state-of-the-art-range) Pearson
correlations of vector cosines and human judgments
in both the original (r = .70) and reduced (r = .72)
spaces.
There are several parameters involved in con-
structing a semantic space (choice of full and re-
duced dimensions, co-occurrence span, weighting
method). Since our current focus is on alterna-
tive composition methods evaluated on a shared se-
mantic space, exploring parameters pertaining to the
construction of the semantic space is not one of our
priorities, although we cannot of course exclude that
the nature of the underlying semantic space affects
different composition methods differently.
4.4 Composition methods
In the proposed adjective-specific linear map (alm)
method, an AN is generated by multiplying an adjec-
tive weight matrix with a noun (column) vector. The
j weights in the i-th row of the matrix are the coeffi-
cients of a linear regression predicting the values of
the i-th dimension of the AN vector as a linear com-
bination of the j dimensions of the component noun.
The linear regression coefficients are estimated sep-
arately for each of the 36 tested adjectives from
the corpus-observed noun-AN pairs containing that
adjective (observed adjective vectors are not used).
Since we are working in the 300-dimensional right
singular vector space, for each adjective we have
300 regression problems with 300 independent vari-
ables, and the training data (the noun-AN pairs avail-
able for each test set adjective) range from about
200 to more than 1K items. We estimate the coef-
ficients using (multivariate) partial least squares re-
gression (PLSR) as implemented in the R pls pack-
age (Mevik and Wehrens, 2007). With respect to
standard least squares estimation, this technique is
more robust against over-training by effectively us-
ing a smaller number of orthogonal ?latent? vari-
ables as predictors (Hastie et al, 2009, Section 3.4),
and it exploits the multivariate nature of the prob-
lem (different regressions for each AN vector di-
mension to be predicted) when determining the la-
tent dimensions. The number of latent variables to
be used in the core regression are a free parameter of
PLSR. For efficiency reasons, we did not optimize it.
We picked instead 50 latent variables, by the rule-
of-thumb reasoning that for any adjective we can
use at least 200 noun-AN pairs for training, and the
independent-variable-to-training-item ratio will thus
never be above 1/4. We adopt a leave-one-out train-
ing regime, so that each target AN is generated by
an adjective matrix that was estimated from all the
1187
other ANs with the same adjective, minus the target.
We use PLSR with 50 latent variables also for
our re-implementation of Guevara?s (2010) single
linear map (slm) approach, in which a single re-
gression matrix is estimated for all ANs across ad-
jectives. The training data in this case are given
by the concatenation of the observed adjective and
noun vectors (600 independent variables) coupled
with the corresponding AN vectors (300 dependent
variables). For each target AN, we randomly sam-
ple 2,000 other adjective-noun-AN tuples for train-
ing (with larger training sets we run into memory
problems), and use the resulting coefficient matrix to
generate the AN vector from the concatenated target
adjective and noun vectors.
Additive AN vectors (add method) are obtained
by summing the corresponding adjective and noun
vectors after normalizing them (non-normalized ad-
dition was also tried, but it did not work nearly as
well as the normalized variant). Multiplicative vec-
tors (mult method) were obtained by component-
wise multiplication of the adjective and noun vec-
tors (normalization does not matter here since it
amounts to multiplying the composite vector by a
scalar, and the cosine similarity measure we use is
scale-invariant). Finally, the adj and noun baselines
use the adjective and noun vectors, respectively, as
surrogates of the AN vector.
For the add, mult, adj and noun methods, we ran
the tests of Section 6 not only in the SVD-reduced
space, but also in the original 10K-dimensional co-
occurrence space. Only the mult method achieved
better performance in the original space. We con-
jecture that this is because the SVD dimensions can
have negative values, leading to counter-intuitive re-
sults with component-wise multiplication (multiply-
ing large opposite-sign values results in large nega-
tive values). We tried to alleviate this problem by as-
signing a 0 to composite dimensions where the two
input vectors had different signs. The resulting per-
formance was better but still below that of mult in
original space. Thus, in Section 6 we report mult
results from the full co-occurrence matrix; reduced
space results for all other methods.
5 Study 1: ANs in semantic space
The actual distribution of ANs in the corpus, as
recorded by their co-occurrence vectors, is funda-
mental to what we are doing. Our method relies on
the hypothesis that the semantics of AN composi-
tion does not depend on the independent distribu-
tion of adjectives themselves, but on how adjectives
transform the distribution of nouns, as evidenced by
observed pairs of noun-AN vectors. Moreover, co-
herently with this view, our evaluation below will be
based on how closely the models approximate the
observed vectors of unseen ANs.
That our goal in modeling composition should be
to approximate the vectors of observed ANs is in
a sense almost trivial. Whether we synthesize an
AN for generation or decoding purposes, we would
want the synthetic AN to look as much as possible
like a real AN in its natural usage contexts, and co-
occurrence vectors of observed ANs are a summary
of their usage in actual linguistic contexts. However,
it might be the case that the specific resources we
used for our vector construction procedure are not
appropriate, so that the specific observed AN vectors
we extract are not reliable (e.g., they are so sparse in
the original space as to be uninformative, or they are
strictly tied to the domains of the input corpora). We
provide here some preliminary qualitative evidence
that this is in general not the case, by tapping into
our own intuitions on where ANs should be located
in semantic space, and thus on how sensible their
neighbors are.
First, we computed centroids from normalized
SVD space vectors of all the ANs that share the same
adjective (e.g., the normalized vectors of American
adult, American menu, etc., summed to construct
the American N centroid). We looked at the near-
est neighbors of these centroids in semantic space
among the 41K items (adjectives, nouns and ANs)
in our extended vocabulary (here and in all experi-
ments below, similarity is quantified by the cosine of
the angle between two vectors). As illustrated for a
random sample of 9 centroids in Table 1 (but apply-
ing to the remaining 27 adjectives as well), centroids
are positioned in intuitively reasonable areas of the
space, typically near the adjective itself or the corre-
sponding noun (the noun green near green N), proto-
typical ANs for that adjective (black face), elements
1188
related to the definition of the adjective (mental ac-
tivity, historical event, green colour, quick and little
cost for easy N), and so on.
American N black N easy N
Am. representative black face easy start
Am. territory black hand quick
Am. source black (n) little cost
green N historical N mental N
green (n) historical mental activity
red road hist. event mental experience
green colour hist. content mental energy
necessary N nice N young N
necessary nice youthful
necessary degree good bit young doctor
sufficient nice break young staff
Table 1: Nearest 3 neighbors of centroids of ANs that
share the same adjective.
How about the neighbors of specific ANs? Ta-
ble 2 reports the nearest 3 neighbors of 9 randomly
selected ANs involving different adjectives (we in-
spected a larger random set, coming to similar con-
clusions to the ones emerging from this table).
bad electronic historical
luck communication map
bad elec. storage topographical
bad weekend elec. transmission atlas
good spirit purpose hist. material
important route nice girl little war
important transport good girl great war
important road big girl major war
major road guy small war
red cover special collection young husband
black cover general collection small son
hardback small collection small daughter
red label archives mistress
Table 2: Nearest 3 neighbors of specific ANs.
The nearest neighbors of the corpus-based AN
vectors in Table 2 make in general intuitive sense.
Importantly, the neighbors pick up the composite
meaning rather than that of the adjective or noun
alone. For example, cover is an ambiguous word,
but the hardback neighbor relates to its ?front of a
book? meaning that is the most natural one in com-
bination with red. Similarly, it makes more sense
that a young husband (rather than an old one) would
have small sons and daughters (not to mention the
mistress!).
We realize that the evidence presented here is
of a very preliminary and intuitive nature. Indeed,
we will argue in the next section that there are
cases in which the corpus-derived AN vector might
not be a good approximation to our semantic in-
tuitions about the AN, and a model-composed AN
vector is a better semantic surrogate. One of the
most important avenues for further work will be to
come to a better characterization of the behaviour of
corpus-observed ANs, where they work and where
the don?t. Still, the neighbors of average and AN-
specific vectors of Tables 1 and 2 suggest that, for
the bulk of ANs, such corpus-based co-occurrence
vectors are semantically reasonable.
6 Study 2: Predicting AN vectors
Having tentatively established that the sort of vec-
tors we can harvest for ANs by directly collecting
their corpus co-occurrences are reasonable represen-
tations of their composite meaning, we move on to
the core question of whether it is possible to recon-
struct the vector for an unobserved AN from infor-
mation about its components. We use nearness to
the corpus-observed vectors of held-out ANs as a
very direct way to evaluate the quality of model-
generated ANs, since we just saw that the observed
ANs look reasonable (but see the caveats at the end
of this section). We leave it to further work to as-
sess the quality of the generated ANs in an applied
setting, for example adapting Mitchell and Lapata?s
paraphrasing task to ANs. Since the observed vec-
tors look like plausible representations of compos-
ite meaning, we expect that the closer the model-
generated vectors are to the observed ones, the better
they should also perform in any task that requires ac-
cess to the composite meaning, and thus that the re-
sults of the current evaluation should correlate with
applied performance.
More in detail, we evaluate here the composition
methods (and the adjective and noun baselines) by
computing, for each of them, the cosine of the test
set AN vectors they generate (the ?predicted? ANs)
with the 41K vectors representing our extended vo-
cabulary in semantic space, and looking at the posi-
tion of the corresponding observed ANs (that were
not used for training, in the supervised approaches)
1189
in the cosine-ranked lists. The lower the rank, the
better the approximation. For efficiency reasons, we
flatten out the ranks after the top 1,000 neighbors.
The results are summarized in Table 3 by the me-
dian and the other quartiles, calculated across all
26,440 ANs in the test set. These measures (unlike
mean and variance) are not affected by the cut-off
after 1K neighbors. To put the reported results into
perspective, a model with a first quartile rank of 999
does very significantly better than chance (the bino-
mial probability of 1/4 or more of 26,440 trials be-
ing successful with pi = 0.024 is virtually 0, where
the latter quantity is the probability of an observed
AN being at rank 999 or lower according to a geo-
metric distribution with pi=1/40999).
method 25% median 75%
alm 17 170 ?1K
add 27 257 ?1K
noun 72 448 ?1K
mult 279 ?1K ?1K
slm 629 ?1K ?1K
adj ?1K ?1K ?1K
Table 3: Quartile ranks of observed ANs in cosine-ranked
lists of predicted AN neighbors.
Our proposed method, alm, emerges as the best
approach. The difference with the second best
model, add (the only other model that does better
than the non-trivial baseline of using the compo-
nent noun vector as a surrogate for AN), is highly
statistically significant (Wilcoxon signed rank test,
p< 0.00001). If we randomly downsample the AN
set to keep an equal number of ANs per adjective
(200), the difference is still significant with p below
the same threshold, indicating that the general result
is not due to a better performance of alm on a few
common adjectives.1
Among the alternative models, the fact that the
performance of add is decidedly better than that of
mult is remarkable, since earlier studies found that
1The semantic space in which we rank the observed ANs
with respect to their predicted counterparts also contain the ob-
served vectors of nouns and ANs that were used to train alm.
We do not see how this should affect performance, but we nev-
ertheless repeated the evaluation leaving out, for each AN, the
observed items used in training, and we obtained the same re-
sults reported in the main text (same ordering of method perfor-
mance, and very significant difference between alm and add).
multiplicative models are, in general, better than ad-
ditive ones in compositionality tasks (see Section 2
above). This might depend on the nature of AN
composition, but there are also more technical is-
sues at hand: (i) we are not sure that previous stud-
ies normalized before summing like we did, and
(ii) the multiplicative model, as discussed in Section
4, does not benefit from SVD reduction. The sin-
gle linear mapping model (slm) proposed by Gue-
vara (2010) is doing even worse than the multiplica-
tive method, suggesting that a single set of weights
does not provide enough flexibility to model a vari-
ety of adjective transformations successfully. This
is at odds with Guevara?s experiment in which slm
outperformed mult and add on the task of ranking
predicted ANs with respect to a target observed AN.
Besides various differences in task definition and
model implementation, Guevara trained his model
on ANs that include a wide variety of adjectives,
whereas our training data were limited to ANs con-
taining one of our 36 test set adjectives. Future work
should re-evalute the performance of Guevara?s ap-
proach in our task, but under his training regime.
Looking now at the alm results in more detail, the
best median ranks are obtained for very frequent ad-
jectives. The top ones are new (median rank: 34),
great (79), American (82), large (82) and different
(97). There is a high inverse correlation between
median rank and adjective frequency (Spearman?s
? =?0.56). Although from a statistical perspec-
tive it is expected that we get better results where
we have more data, from a linguistic point of view it
is interesting that alm works best with extremely fre-
quent, highly polysemous adjectives like new, large
and different, that border on function words ? a do-
main where distributional semantics has generally
not been tested.
Although, in relative terms and considering the
difficulty of the task, alm performs well, it is still far
from perfect ? for 27% alm-predicted ANs, the ob-
served vector is not even in the top 1K neighbor set!
A qualitative look at some of the most problematic
examples indicates however that a good proportion
of them might actually not be instances where our
model got the AN vector wrong, but cases of anoma-
lous observed ANs. The left side of Table 4 com-
pares the nearest neighbors (excluding each other)
of the observed and alm-predicted vectors in 10 ran-
1190
SIMILAR DISSIMILAR
adj N obs. neighbor pred. neighbor adj N obs. neighbor pred. neighbor
common understanding common approach common vision American affair Am. development Am. policy
different authority diff. objective diff. description current dimension left (a) current element
different partner diff. organisation diff. department good complaint current complaint good beginning
general question general issue same great field excellent field gr. distribution
historical introduction hist. background same historical thing different today hist. reality
necessary qualification nec. experience same important summer summer big holiday
new actor new cast same large pass historical region large dimension
recent request recent enquiry same special something little animal special thing
small drop droplet drop white profile chrome (n) white show
young engineer young designer y. engineering young photo important song young image
Table 4: Left: nearest neighbors of observed and alm-predicted ANs (excluding each other) for a random set of ANs
where rank of observed w.r.t. predicted is 1. Right: nearest neighbors of predicted and observed ANs for random set
where rank of observed w.r.t. predicted is ? 1K.
domly selected cases where the observed AN is the
nearest neighbor of the predicted one. Here, the
ANs themselves make sense, and the (often shared)
neighbors are also sensible (recent enquiry for re-
cent request, common approach and common vision
for common understanding, etc.). Moving to the
right, we see 10 random examples of ANs where the
observed AN was at least 999 neighbors apart from
the alm prediction. First, we notice some ANs that
are difficult to interpret out-of-context (important
summer, white profile, young photo, large pass, . . . ).
Second, at least subjectively, we find that in many
cases the nearest neighbor of predicted AN is actu-
ally more sensible than that of observed AN: cur-
rent element (vs. left) for current dimension, histori-
cal reality (vs. different today) for historical thing,
special thing (vs. little animal) for special some-
thing, young image (vs. important song) for young
photo. In the other cases, the predicted AN neighbor
is at least not obviously worse than the observed AN
neighbor.
There is a high inverse correlation between the
frequency of occurrence of an AN and the rank of
the observed AN with respect to the predicted one
(? =?0.48), suggesting that our model is worse at
approximating the observed vectors of rare forms,
that might, in turn, be those for which the corpus-
based representation is less reliable. In these cases,
dissimilarities between observed and expected vec-
tors, rather than signaling problems with the model,
might indicate that the predicted vector, based on a
composition function learned from many examples,
is better than the one directly extracted from the cor-
pus. The examples in the right panel of Table 4 bring
some preliminary support to this hypothesis, to be
systematically explored in future work.
7 Study 3: Comparing adjectives
If adjectives are functions, and not corpus-derived
vectors, is it still possible to compare them mean-
ingfully? We explore two ways to accomplish this
in our framework: one is to represent adjectives by
the average of the AN vectors that contain them
(the centroid vectors whose neighbors are illustrated
in Table 1 above), and the other to compare them
based on the 300?300 weight matrices we esti-
mate from noun-AN pairs (we unfold these matri-
ces into 90K-dimensional vectors). We compare the
quality of these representations to that of the stan-
dard approach in distributional semantics, i.e., rep-
resenting the adjectives directly with their corpus
co-occurrence profile vectors (in our case, projected
onto the SVD-reduced space).
We evaluate performance on the task of cluster-
ing those 19 adjectives in our set that can be rel-
atively straightforwardly categorized into general
classes comprising a minimum of 4 items. The
test set built according to these criteria contains 4
classes: color (white, black, red, green), positive
evaluation (nice, excellent, important, major, ap-
propriate), time (recent, new, current, old, young),
and size (big, huge, little, small, large). We clus-
ter with the CLUTO toolkit (Karypis, 2003), us-
ing the repeated bisections with global optimization
1191
method, accepting all of CLUTO?s default values
for this choice. Cluster quality is evaluated by per-
centage purity (Zhao and Karypis, 2003). If nir is
the number of items from the i-th true (gold stan-
dard) class assigned to the r-th cluster, n is the to-
tal number of items and k the number of clusters,
then: Purity = 1n
?k
r=1 maxi
(nir). We calculate
empirical 95% confidence intervals around purity by
a heuristic bootstrap procedure based on 10K resam-
plings of the data set (Efron and Tibshirani, 1994).
The random baseline distribution is obtained by 10K
random assignments of adjectives to the clusters, un-
der the constraint that no cluster is empty.
Table 5 shows that all methods are significantly
better than chance. Our two ?indirect? represen-
tations achieve similar performance, and they are
(slightly) better than the traditional method based on
adjective co-occurrence vectors. We conclude that,
although our approach does not provide a direct en-
coding of adjective meaning in terms of such inde-
pendently collected vectors, it does have meaningful
ways to represent their semantic properties.
input purity
matrix 73.7 (68.4-94.7)
centroid 73.7 (63.2-94.7)
vector 68.4 (63.2-89.5)
random 45.9 (36.8-57.9)
Table 5: Percentage purity in adjective clustering with
bootstrapped 95% confidence intervals.
8 Conclusion
The work we reported constitutes an encouraging
start for our approach to modeling (AN) composi-
tion. We suggested, along the way, various direc-
tions for further studies. We consider the following
issues to be the most pressing ones.
We currently train each adjective-specific model
separately: We should explore hierarchical model-
ing approaches that exploit similarities across adjec-
tives (and possibly syntactic constructions) to esti-
mate better models.
Evaluation-wise, the differences between ob-
served and predicted ANs must be analyzed more
extensively, to support the claim that, when their
vectors differ, model-based prediction improves on
the observed vector. Evaluation in a more applied
task should also be pursued ? in particular, we will
design a paraphrasing task similar to the one pro-
posed by Mitchell and Lapata to evaluate noun-verb
constructions.
Since we do not collect vectors for the ?functor?
component of a composition process (for AN con-
structions, the adjective), our approach naturally ex-
tends to processes that involve bound morphemes,
such as affixation, where we would not need to col-
lect independent co-occurrence information for the
affixes. For example, to account for re- prefixation
we do not need to collect a re- vector (required by all
other approaches to composition), but simply vec-
tors for a set of V/reV pairs, where both members of
the pairs are words (e.g., consider/reconsider).
Our approach can also deal, out-of-the-box, with
recursive constructions (sad little red hat), and can
be easily extended to more abstract constructions,
such as determiner N (mapping dog to the/a/one
dog). Still, we need to design a good testing scenario
to evaluate the quality of such model-generated con-
structions.
Ultimately, we want to compose larger and larger
constituents, up to full sentences. It remains to be
seen if the approach we proposed will scale up to
such challenges.
Acknowledgments
We thank Gemma Boleda, Emilano Guevara,
Alessandro Lenci, Louise McNally and the anony-
mous reviewers for useful information, advice and
comments.
References
S. Clark and S. Pulman. 2007. Combining symbolic and
distributional models of meaning. In Proceedings of
the First Symposium on Quantum Interaction, pages
52?55.
B. Efron and R. Tibshirani. 1994. An Introduction to the
Bootstrap. Chapman and Hall, Boca Raton, FL.
K. Erk and S. Pado?. 2008. A structured vector space
model for word meaning in context. In Proceedings of
EMNLP, pages 897?906.
K. Erk and S. Pado?. 2009. Paraphrase assessment
in structured vector space: Exploring parameters and
datasets. In Proceedings of the EACL GEMS Work-
shop, pages 57?65.
1192
S. Evert. 2005. The Statistics of Word Cooccurrences.
Dissertation, Stuttgart University.
P. Foltz, W. Kintsch, and Th. Landauer. 1998. The mea-
surement of textual coherence with Latent Semantic
Analysis. Discourse Processes, 25:285?307.
G. Frege. 1892. U?ber sinn und bedeutung. Zeitschrift
fuer Philosophie un philosophische Kritik, 100.
E. Guevara. 2010. A regression model of adjective-noun
compositionality in distributional semantics. In Pro-
ceedings of the ACL GEMS Workshop, pages 33?37.
T. Hastie, R. Tibshirani, and J. Friedman. 2009. The El-
ements of Statistical Learning, 2nd ed. Springer, New
York.
M. Jones and D. Mewhort. 2007. Representing word
meaning and order information in a composite holo-
graphic lexicon. Psychological Review, 114:1?37.
H. Kamp. 1975. Two theories about adjectives. In
E. Keenan, editor, Formal Semantics of Natural Lan-
guage, pages 123?155. Cambridge University Press.
G. Karypis. 2003. CLUTO: A clustering toolkit. Tech-
nical Report 02-017, University of Minnesota Depart-
ment of Computer Science.
W. Kintsch. 2001. Predication. Cognitive Science,
25(2):173?202.
Th. Landauer and S. Dumais. 1997. A solution to Plato?s
problem: The Latent Semantic Analysis theory of ac-
quisition, induction, and representation of knowledge.
Psychological Review, 104(2):211?240.
B. Mevik and R. Wehrens. 2007. The pls package: Prin-
cipal component and partial least squares regression in
R. Journal of Statistical Software, 18(2).
J. Mitchell and M. Lapata. 2008. Vector-based models of
semantic composition. In Proceedings of ACL, pages
236?244.
J. Mitchell and M. Lapata. 2009. Language models
based on semantic composition. In Proceedings of
EMNLP, pages 430?439.
R. Montague. 1970a. English as a formal language. In
B. Visentini, editor, Linguaggi nella Societa` e nella
Tecnica, pages 189?224. Edizioni di Comunita`, Milan.
Reprinted in Thomason (1974).
R. Montague. 1970b. Universal grammar. Theoria,
36:373?398. Reprinted in Thomason (1974).
R. Montague. 1973. The proper treatment of quantifica-
tion in English. In K.J.J. Hintikka, editor, Approaches
to Natural Language, pages 221?242. Reidel, Dor-
drecht. Reprinted in Thomason (1974).
B. Partee. 2004. Compositionality. In Compositionality
in Formal Semantics: Selected Papers by Barbara H.
Partee. Blackwell, Oxford.
R. Rapp. 2003. Word sense discovery based on sense de-
scriptor dissimilarity. In Proceedings of the MT Sum-
mit, pages 315?322.
H. Rubenstein and J. Goodenough. 1965. Contextual
correlates of synonymy. Communications of the ACM,
8(10):627?633.
S. Rudolph and E. Giesbrecht. 2010. Compositional
matrix-space models of language. In Proceedings of
ACL.
M. Sahlgren, A. Holst, and P. Kanerva. 2008. Permu-
tations as a means to encode order in word space. In
Proceedings of CogSci, pages 1300?1305.
M. Sahlgren. 2006. The Word-Space Model. Disserta-
tion, Stockholm University.
H. Schmid. 1995. Improvements in part-of-speech tag-
ging with an application to German. In Proceedings of
the EACL-SIGDAT Workshop.
H. Schu?tze. 1997. Ambiguity Resolution in Natural Lan-
guage Learning. CSLI, Stanford, CA.
M. Siegel. 1976. Capturing the Adjective. Ph.D. thesis,
University of Massachusetts at Amherst.
P. Smolensky. 1990. Tensor product variable binding and
the representation of symbolic structures in connec-
tionist networks. Artificial Intelligence, 46:159?216.
R. H. Thomason, editor. 1974. Formal Philosophy: Se-
lected Papers of Richard Montague. Yale University
Press, New York.
P. Turney and P. Pantel. 2010. From frequency to mean-
ing: Vector space models of semantics. Journal of Ar-
tificial Intelligence Research, 37:141?188.
D. Widdows. 2008. Semantic vector products: Some ini-
tial investigations. In Proceedings of the Second Sym-
posium on Quantum Interaction, Oxford.
Y. Zhao and G. Karypis. 2003. Criterion functions for
document clustering: Experiments and analysis. Tech-
nical Report 01-40, University of Minnesota Depart-
ment of Computer Science.
1193
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 141?151,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Studying the recursive behaviour of adjectival modification
with compositional distributional semantics
Eva Maria Vecchi and Roberto Zamparelli and Marco Baroni
Center for Mind/Brain Sciences (University of Trento, Italy)
(evamaria.vecchi|roberto.zamparelli|marco.baroni)@unitn.it
Abstract
In this study, we use compositional distribu-
tional semantic methods to investigate restric-
tions in adjective ordering. Specifically, we
focus on properties distinguishing Adjective-
Adjective-Noun phrases in which there is flex-
ibility in the adjective ordering from those
bound to a rigid order. We explore a number
of measures extracted from the distributional
representation of AAN phrases which may in-
dicate a word order restriction. We find that
we are able to distinguish the relevant classes
and the correct order based primarily on the
degree of modification of the adjectives. Our
results offer fresh insight into the semantic
properties that determine adjective ordering,
building a bridge between syntax and distri-
butional semantics.
1 Introduction
A prominent approach for representing the meaning
of a word in Natural Language Processing (NLP) is
to treat it as a numerical vector that codes the pat-
tern of co-occurrence of that word with other ex-
pressions in a large corpus of language (Sahlgren,
2006; Turney and Pantel, 2010). This approach to
semantics (sometimes called distributional seman-
tics) scales well to large lexicons and does not re-
quire words to be manually disambiguated (Schu?tze,
1997). Until recently, however, this method had
been almost exclusively limited to the level of sin-
gle content words (nouns, adjectives, verbs), and had
not directly addressed the problem of composition-
ality (Frege, 1892; Montague, 1970; Partee, 2004),
the crucial property of natural language which al-
lows speakers to derive the meaning of a complex
linguistic constituent from the meaning of its imme-
diate syntactic subconstituents.
Several recent proposals have strived to ex-
tend distributional semantics with a component that
also generates vectors for complex linguistic con-
stituents, using compositional operations in the vec-
tor space (Baroni and Zamparelli, 2010; Guevara,
2010; Mitchell and Lapata, 2010; Grefenstette and
Sadrzadeh, 2011; Socher et al, 2012). All of
these approaches construct distributional represen-
tations for novel phrases starting from the corpus-
derived vectors for their lexical constituents and
exploiting the geometric quality of the representa-
tion. Such methods are able to capture complex se-
mantic information of adjective-noun (AN) phrases,
such as characterizing modification (Boleda et al,
2012; Boleda et al, 2013), and can detect seman-
tic deviance in novel phrases (Vecchi et al, 2011).
Furthermore, these methods are naturally recursive:
they can derive a representation not only for, e.g.,
red car, but also for new red car, fast new red car,
etc. This aspect is appealing since trying to extract
meaningful representations for all recursive phrases
directly from a corpus will result in a problem of
sparsity, since most large phrases will never occur in
any finite sample.
Once we start seriously looking into recursive
modification, however, the issue of modifier order-
ing restrictions naturally arises. Such restrictions
have often been discussed in the theoretical linguis-
tic literature (Sproat and Shih, 1990; Crisma, 1991;
Scott, 2002), and have become one of the key in-
141
gredients of the ?cartographic? approach to syntax
(Cinque, 2002). In this paradigm, the ordering is
derived by assigning semantically different classes
of modifiers to the specifiers of distinct functional
projections, whose sequence is hard-wired. While
it is accepted that in different languages movement
can lead to a principled rearrangement of the linear
order of the modifiers (Cinque, 2010; Steddy and
Samek-Lodovici, 2011), one key assumption of the
cartographic literature is that exactly one intonation-
ally unmarked order for stacked adjectives should
be possible in languages like English. The possi-
bility of alternative orders, when discussed at all,
is attributed to the presence of idioms (high Amer-
ican building, but American high officer), to asyn-
detic conjunctive meanings (e.g. new creative idea
parsed as [new & creative] idea, rather than [new
[creative idea]]), or to semantic category ambiguity
for any adjective which appears in different orders
(see Cinque (2004) for discussion).
In this study, we show that the existence of both
rigid and flexible order cases is robustly attested at
least for adjectival modification, and that flexible or-
dering is unlikely to reduce to idioms, coordination
or ambiguity. Moreover, we show that at least for
some recursively constructed adjective-adjective-
noun phrases (AANs) we can extract meaning-
ful representations from the corpus, approximating
them reasonably well by means of compositional
distributional semantic models, and that the seman-
tic information contained in these models character-
izes which AA will have rigid order (as with rapid
social change vs. *social rapid change), or flexible
order (e.g. total estimated population vs. estimated
total population). In the former case, we find that
the same distributional semantic cues discriminate
between correct and wrong orders.
To achieve these goals, we consider various
properties of the distributional representation of
AANs (both corpus-extracted and compositionally-
derived), and explore their correlation with restric-
tions in adjective ordering. We conclude that mea-
sures that quantify the degree to which the modifiers
have an impact on the distributional meaning of the
AAN can be good predictors of ordering restrictions
in AANs.
2 Materials and methods
2.1 Semantic space
Our initial step was to construct a semantic space for
our experiments, consisting of a matrix where each
row represents the meaning of an adjective, noun,
AN or AAN as a distributional vector, each column
a semantic dimension of meaning. We first introduce
the source corpus, then the vocabulary of words and
phrases that we represent in the space, and finally the
procedure adopted to build the vectors representing
the vocabulary items from corpus statistics, and ob-
tain the semantic space matrix. We work here with a
traditional, window-based semantic space, since our
focus is on the effect of different composition meth-
ods given a common semantic space. In addition,
Blacoe and Lapata (2012) found that a vanilla space
of this sort performed best in their composition ex-
periments, when compared to a syntax-aware space
and to neural language model vectors such as those
used for composition by Socher et al (2011).
Source corpus We use as our source corpus the
concatenation of the Web-derived ukWaC corpus, a
mid-2009 dump of the English Wikipedia and the
British National Corpus1. The corpus has been tok-
enized, POS-tagged and lemmatized with the Tree-
Tagger (Schmid, 1995), and it contains about 2.8 bil-
lion tokens. We extract all statistics at the lemma
level, meaning that we consider only the canonical
form of each word ignoring inflectional information,
such as pluralization and verb inflection.
Semantic space vocabulary The words/phrases
in the semantic space must of course include the
items that we need for our experiments (adjectives,
nouns, ANs and AANs used for model training, as
input to composition and for evaluation). Therefore,
we first populate our semantic space with a core vo-
cabulary containing the 8K most frequent nouns and
the 4K most frequent adjectives from the corpus.
The ANs included in the semantic space are com-
posed of adjectives with very high frequency in the
corpus so that they are generally able to combine
with many classes of nouns. They are composed
of the 700 most frequent adjectives and 4K most
frequent nouns in the corpus, which were manually
1http://wacky.sslmit.unibo.it, http://en.
wikipedia.org, http://www.natcorp.ox.ac.uk
142
controlled for problematic cases ? excluding adjec-
tives such as above, less, or very, and nouns such
as cant, mph, or yours ? often due to tagging errors.
We generated the set of ANs by crossing the filtered
663 adjectives and 3,910 nouns. We include those
ANs that occur at least 100 times in the corpus in
our vocabulary, which amounted to a total of 128K
ANs.
Finally, we created a set of AAN phrases com-
posed of the adjectives and nouns used to gener-
ate the ANs. Additional preprocessing of the gen-
erated AxAyNs includes: (i) control that both AxN
and AyN are attested in the corpus; (ii) discard any
AxAyN in which AxN or AyN are among the top
200 most frequent ANs in the source corpus (as in
this case, order will be affected by the fact that such
phrases are almost certainly highly lexicalized); and
(iii) discard AANs seen as part of a conjunction in
the source corpus (i.e., where the two adjectives ap-
pear separated by comma, and, or or; this addresses
the objection that a flexible order AAN might be a
hidden A(&)A conjunction: we would expect that
such a conjunction should also appear overtly else-
where). The set of AANs thus generated is then di-
vided into two types of adjective ordering:
1. Flexible Order (FO): phrases where both or-
ders, AxAyN and AyAxN, are attested (f>10
in both orders).
2. Rigid Order (RO): phrases with one order,
AxAyN, attested (20<f<200)2 and AyAxN
unattested.
All AANs that did not meet either condition were
excluded from our semantic space vocabulary. The
preserved set resulted in 1,438 AANs: 621 flexible
order and 817 rigid order. Note that there are almost
as many flexible as rigid order cases; this speaks
against the idea that free order is a marginal phe-
nomenon, due to occasional ambiguities that reas-
sign the adjective to a different semantic class. The
existence of freely ordered stacked adjectives is a ro-
bust phenomenon, which needs to be addressed.
2The upper threshold was included as an additional filter
against potential multiword expressions. Of course, the bound-
ary between phrases that are at least partially compositional and
those that are fully lexicalized is not sharp, and we leave it to
further work to explore the interplay between the semantic fac-
tors we study here and patterns of lexicalization.
Model ? M&L
CORP 0.41 0.43
W.ADD 0.41 0.44
F.ADD 0.40 ?
MULT 0.33 0.46
LFM 0.40 ?
Table 1: Correlation scores (Spearman?s ?, all signif-
icant at p<0.001) between cosines of corpus-extracted
or model-generated AN vectors and phrase similarity rat-
ings collected in Mitchell and Lapata (2010), as well as
best reported results from Mitchell & Lapata (M&L).
Semantic vector construction For each of
the items in our vocabulary, we first build 10K-
dimensional vectors by recording the item?s
sentence-internal co-occurrence with the top 10K
most frequent content lemmas (nouns, adjectives,
verbs or adverbs) in the corpus. We built a rank
of these co-occurrence counts, and excluded as
stop words from the dimensions any element of
any POS whose rank was from 0 to 300. The raw
co-occurrence counts were then transformed into
(positive) Pointwise Mutual Information (pPMI)
scores (Church and Hanks, 1990). Next, we reduce
the full co-occurrence matrix to 300 dimensions
applying the Non-negative Matrix Factorization
(NMF) operation (Lin, 2007). We did not tune the
semantic vector construction parameters, since we
found them to work best in a number of independent
earlier experiments.
Corpus-extracted vectors (corp) were computed
for the ANs and for the flexible order and attested
rigid order AANs, and then mapped onto the 300-
dimension NMF-reduced semantic space. As a san-
ity check, the first row of Table 1 reports the corre-
lation between the AN phrase similarity ratings col-
lected in Mitchell and Lapata (2010) and the cosines
of corpus-extracted vectors in our space, for the
same ANs. For the AAN vectors, which are sparser,
we used human judgements to build a reliable sub-
set to serve as our gold standard, as detailed in Sec-
tion 2.4.
2.2 Composition models
We focus on four composition functions proposed
in recent literature with high performance in a num-
ber of semantic tasks. We first consider meth-
ods proposed by Mitchell and Lapata (2010) in
143
which the model-generated vectors are simply ob-
tained through component-wise operations on the
constituent vectors. Given input vectors ~u and ~v, the
multiplicative model (MULT) computes a composed
vector by component-wise multiplication () of the
constituent vectors, where the i-th component of the
composed vector is given by pi = uivi.3 Given an
AxAyN phrase, this model extends naturally to the
recursive setting of this experiment, as seen in Equa-
tion (1).
~p = ~ax  ~ay  ~n (1)
This composition method is order-insensitive, the
formula above corresponding to the representation
of both AxAyN and AyAxN.
In the weighted additive model (W.ADD), we ob-
tain the composed vector as a weighted sum of the
two component vectors: ~p = ?~u+ ?~v, where ? and
? are scalars. Again, we can easily apply this func-
tion recursively, as in Equation (2).
~p = ?~ax + ?(?~ay + ?~n) = ?~ax + ??~ay + ?
2~n
(2)
We also consider the full extension of the addi-
tive model (F.ADD), presented in Guevara (2010)
and Zanzotto et al (2010), such that the component
vectors are pre-multiplied by weight matrices before
being added: ~p = W1~u + W2~v. Similarly to the
W.ADD model, Equation (3) describes how we apply
this function recursively.
~p = W1~ax + W2(W1~ay + W2~n) (3)
= W1~ax + W2W1~ay + W22~n
Finally, we consider the lexical function model
(LFM), first introduced in Baroni and Zamparelli
(2010), in which attributive adjectives are treated as
functions from noun meanings to noun meanings.
This is a standard approach in Montague semantics
(Thomason, 1974), except noun meanings here are
distributional vectors, not denotations, and adjec-
tives are (linear) functions learned from a large cor-
pus. In this model, predicted vectors are generated
3We conjecture that the different performance of our multi-
plicative model and M&L?s (cf. Table 1) is due to the fact that
we use log-transformed pPMI scores, making their multiplica-
tive model more akin to our additive approach.
by multiplying a function matrix U with a compo-
nent vector: ~p = U~v. Given a weight matrix, A, for
each adjective in the phrase, we apply the functions
in sequence recursively as shown in Equation (4).
~p = Ax(Ay~n) (4)
Composition model estimation Parameters for
W.ADD, F.ADD and LFM were estimated following
the strategy proposed by Guevara (2010) and Ba-
roni and Zamparelli (2010), recently extended to all
composition models by Dinu et al (2013b). Specif-
ically, we learn parameter values that optimize the
mapping from the noun to the AN as seen in ex-
amples of corpus-extracted N-AN vector pairs, us-
ing least-squares methods. All parameter estima-
tions and phrase compositions were implemented
using the DISSECT toolkit4 (Dinu et al, 2013a),
with a training set of 74,767 corpus-extracted N-
AN vector pairs, ranging from 100 to over 1K items
across the 663 adjectives. Importantly, while below
we report experimental results on capturing various
properties of recursive AAN constructions, no AAN
was seen during training, which was based entirely
on mapping from N to AN. Table 1 reports the re-
sults attained by our model implementations on the
Mitchell and Lapata AN similarity data set.
2.3 Measures of adjective ordering
Our general goal is to determine which
linguistically-motivated factors distinguish the
two types of adjective ordering. We hypothesize
that in cases of flexible order, the two adjectives
will have a similarly strong effect on the noun, thus
transforming the meaning of the noun equivalently
in the direction of both adjectives and component
ANs. For example, in the phrase creative new idea,
the idea is both new and creative, so we would
expect a similar impact of modification by both
adjectives.
On the other hand, we predict that in rigid order
cases, one adjective, the one closer to the noun, will
dominate the meaning of the phrase, distorting the
meaning of the noun by a significant amount. For
example, the phrase different architectural style in-
tuitively describes an architectural style that is dif-
4http://clic.cimec.unitn.it/composes/
toolkit
144
ferent, rather than a style that is to the same extent
architectural and different.
We consider a number of measures that could cap-
ture our intuitions and quantify this difference, ex-
ploring the distance relationship between the AAN
vectors and each of the AAN subparts. First, we
examine how the similarity of an AAN to its com-
ponent adjectives affects the ordering, using the co-
sine between the AxAyN vector and each of the
component A vectors as an expression of similarity
(we abbreviate this as cosAx and cosAy for the first
and second adjective, respectively).5 Our hypothe-
sis predicts that flexible order AANs should remain
similarly close to both component As, while rigid
order AANs should remain systematically closer to
their Ay than to their Ax.
Next, we consider the similarity between the
AxAyN vector and its component N vector (cosN ).
This measure is aimed at verifying if the degree to
which the meaning of the head noun is distorted
could be a property that distinguishes the two types
of adjective ordering. Again, vectors for flexible or-
der AANs should remain closer to their component
nouns in the semantic space, while rigid order AANs
should distort the meaning of the head noun more
notably.
We also inspect how the similarity of the AAN
to its component AN vectors affects the type of ad-
jective ordering (cosAxN and cosAyN ). Consid-
ering the examples above, we predict that the flex-
ible order AAN creative new idea will share many
properties with both creative idea and new idea, as
represented in our semantic space, while rigid or-
der AANs, like different architectural style, should
remain quite similar to the AyN, i.e., architectural
style, and relatively distant from the AxN, i.e., dif-
ferent style.
Finally, we consider a measure that does not ex-
ploit distributional semantic representations, namely
the difference in PMI between AxN and AyN
(?PMI). Based on our hypothesis described for the
other measures, we expect the association in the cor-
pus of AyN to be much greater than AxN for rigid
order AANs, resulting in a large negative ?PMI val-
ues. While flexible order AANs should have similar
5In the case of LFM, we compare the similarity of the AAN
with the AN centroids for each adjective, since the model does
not make use of A vectors (Baroni and Zamparelli, 2010).
association strengths for both AxN and AyN, thus
we expect ?PMI to be closer to 0 than for rigid or-
der AANs.
2.4 Gold standard
To our knowledge, this is the first study to use
distributional representations of recursive modifi-
cation; therefore we must first determine if the
composed AAN vector representations are seman-
tically coherent objects. Thus, for vector analysis,
a gold standard of 320 corpus-extracted AAN vec-
tors were selected and their quality was established
by inspecting their nearest neighbors. In order to
create the gold standard, we ran a crowdsourcing
experiment on CrowdFlower6 (Callison-Burch and
Dredze, 2010; Munro et al, 2010), as follows.
First, we gathered a randomly selected set of 600
corpus-extracted AANs, containing 300 flexible or-
der and 300 attested rigid order AANs. We then
extracted the top 3 nearest neighbors to the corpus-
extracted AAN vectors as represented in the seman-
tic space7. Each AAN was then presented with each
of the nearest neighbors, and participants were asked
to judge ?how strongly related are the two phrases??
on a scale of 1-7. The rationale was that if we
obtained a good distributional representation of the
AAN, its nearest neighbors should be closely related
words and phrases. Each pair was judged 10 times,
and we calculated a relatedness score for the AAN
by taking the average of the 30 judgments (10 for
each of the three neighbors).
The final set for the gold standard contains the 320
AANs (152 flexible order and 168 attested rigid or-
der) which had a relatedness score over the median-
split (3.9). Table 2 shows examples of gold stan-
dard AANs and their nearest neighbors. As these
example indicate, the gold standard AANs reside in
semantic neighborhoods that are populated by in-
tuitively strongly related expressions, which makes
them a sensible target for the compositional models
to approximate.
We also find that the neighbors for the AANs rep-
resent an interesting variety of types of semantic
6http://www.crowdflower.com
7The top 3 neighbors included adjectives, nouns, ANs and
AANs. The preference for ANs and AANs, as seen in Table 2,
is likely a result of the dominance of those elements in the se-
mantic space (c.f. Section 2.1).
145
medieval old town contemp. political issue
fascinating town cultural topic
impressive cathedral contemporary debate
medieval street contemporary politics
rural poor people British naval power
poor rural people naval war
rural infrastructure British navy
rural people naval power
friendly helpful staff last live performance
near hotel final gig
helpful staff live dvd
quick service live release
creative new idea rapid social change
innovative effort social conflict
creative design social transition
dynamic part cultural consequence
national daily newspaper new regional government
national newspaper regional government
major newspaper local reform
daily newspaper regional council
daily national newspaper fresh organic vegetable
national daily newspaper organic vegetable
well-known journalist organic fruit
weekly column organic product
Table 2: Examples of the nearest neighbors of the gold
standard, both flexible order (left column) and rigid order
(right column) AANs.
similarity. For example, the nearest neighbors to the
corpus-extracted vectors for medieval old town and
rapid social change include phrases which describe
quite complex associations, cf. Table 2. In addition,
we find that the nearest neighbors for flexible order
AAN vectors are not necessarily the same for both
adjective orders, as seen in the difference in neigh-
bors of national daily newspaper and daily national
newspaper. We can expect that the change in or-
der, when acceptable and frequent, does not neces-
sarily yield synonymous phrases, and that corpus-
extracted vector representations capture subtle dif-
ferences in meaning.
3 Results
3.1 Quality of model-generated AAN vectors
Our nearest neighbor analysis suggests that the
corpus-extracted AAN vectors in the gold standard
are meaningful, semantically coherent objects. We
can thus assess the quality of AANs recursively gen-
erated by composition models by how closely they
Gold FO RO
W.ADD 0.565 0.572 0.558
F.ADD 0.618 0.622 0.614
MULT 0.424 0.468 0.384
LFM 0.655 0.675 0.637
Table 3: Mean cosine similarities between the corpus-
extracted and model-generated gold AAN vectors. All
pairwise differences between models are significant ac-
cording to Bonferroni-corrected paired t-tests (p<0.001).
For MULT and LFM, the difference between mean flexible
order (FO) and rigid order (RO) cosines is also signifi-
cant.
approximate these vectors. We find that the perfor-
mances of most composition models in approximat-
ing the vectors for the gold AANs is quite satisfac-
tory (cf. Table 3). To put this evaluation into per-
spective, note that 99% of the simulated distribu-
tion of pairwise cosines of corpus-extracted AANs
is below the mean cosine of the worst-performing
model (MULT), that is, a cosine of 0.424 is very sig-
nificantly above what is expected by chance for two
random corpus-extracted AAN vectors. Also, ob-
serve that the two more parameter-rich models are
better than W.ADD, and that LFM also significantly
outperforms F.ADD.
Further, the results show that the models are able
to approximate flexible order AAN vectors better
than rigid order AANs, significantly so for LFM and
MULT. This result is quite interesting because it sug-
gests that flexible order AANs express a more lit-
eral (or intersective) modification by both adjectives,
which is what we would expect to be better captured
by compositional models. Clearly, a more complex
modification process is occurring in the case of rigid
order AANs, as we predicted to be the case.
3.2 Distinguishing flexible vs. rigid order
In the results reported below, we test how both our
baseline ?PMI measure and the distance from the
AAN and its component parts changes depending on
the type of adjective ordering to which the AAN be-
longs. From this point forward, we only use gold
standard items, where we are sure of the quality of
the corpus-extracted vectors. The first block of Ta-
ble 4 reports the t-normalized difference between
flexible order and rigid order mean cosines for the
corpus-extracted vectors.
146
Measure t sig.
CORP
cosAx 2.478
cosAy -4.348 * RO>FO
cosN 4.656 * FO>RO
cosAxN 5.913 * FO>RO
cosAyN 1.970
W.ADD
cosAx 4.805 * FO>RO
cosAy -1.109
cosN 1.140
cosAxN 1.059
cosAyN 0.584
F.ADD
cosAx 2.050
cosAy -1.451
cosN 4.493 * FO>RO
cosAxN -0.445
cosAyN 2.300
MULT
cosAx 3.830 * FO>RO
cosAy -0.503
cosN 5.090 * FO>RO
cosAxN 4.435 * FO>RO
cosAyN 3.900 * FO>RO
LFM
cosAx -1.649
cosAy -1.272
cosN 5.539 * FO>RO
cosAxN 3.336 * FO>RO
cosAyN 4.215 * FO>RO
?PMI 8.701 * FO>RO
Table 4: Flexible vs. Rigid Order AANs. t-normalized
differences between flexible order (FO) and rigid order
(FO) mean cosines (or mean ?PMI values) for corpus-
extracted and model-generated vectors. For significant
differences (p<0.05 after Bonferroni correction), the last
column reports whether mean cosine (or ?PMI) is larger
for flexible order (FO) or rigid order (RO) class.
These results show, in accordance with our con-
siderations in Section 2.3 above: (i) flexible or-
der AxAyNs are closer to AxN and the component
N than rigid order AxAyNs, and (ii) rigid order
AxAyNs are closer to their Ay (flexible order AANs
are also closer to Ax but the effect does not reach
significance).8 The results imply that the degree of
modification of the Ay on the noun is a significant
indicator of the type of ordering present.
8As an aside, the fact that mean cosines are significantly
larger for the flexible order class in two cases but for the rigid or-
der class in another addresses the concern, raised by a reviewer,
that the words and phrases in one of the two classes might sys-
tematically inhabit denser regions of the space than those of the
other class, thus distorting results based on comparing mean
cosines.
In particular, rigid order AxAyNs are heavily
modified by Ay, distorting the meaning of the head
noun in the direction of the closest adjective quite
drastically, and only undergoing a slight modifica-
tion when the Ax is added. In other words, in rigid
order phrases, for example rapid social change, the
AyN expresses a single concept (probably a ?kind?,
in the terminology of formal semantics), strongly re-
lated to social, social change, which is then mod-
ified by the Ax. Thus, the change is not both so-
cial and rapid, rather, the social change is rapid. On
the other hand, flexible order AANs maintain the se-
mantic value of the head noun while being modi-
fied only slightly by both adjectives, almost equiv-
alently. For example, in the phrase friendly help-
ful staff, one is saying that the staff is both friendly
and helpful. Most importantly, the corpus-extracted
distributional representations are able to model this
phenomenon inherently and can significantly distin-
guish the two adjective orders.
The results of the composition models (cf. Ta-
ble 4) show that for all models at least some prop-
erties do distinguish flexible and rigid order AANs,
although only MULT and LFM capture the two prop-
erties that show the largest effect for the corpus-
extracted vectors, namely the asymmetry in similar-
ity to the noun and the AxN (flexible order AANs
being more similar to both).
It is worth remarking that MULT approximated the
patterns observed in the corpus vectors quite well,
despite producing order-insensitive representations
of recursive structures. For flexible order AANs, or-
der is indeed only slightly affecting the meaning, so
it stands to reason that MULT has no problems mod-
eling this class. For rigid order AANs, where we
consider here the attested-order only, evidently the
order-insensitive MULT representation is sufficient
to capture their relations to their constituents.
Finally, we see that the ?PMI measure is the best
at distinguishing between the two classes of AAN
ordering. This confirms our hypothesis that a lot has
to do with how integrated Ay and N are. While it
is somewhat disappointing that ?PMI outperforms
all distributional semantic cues, note that this mea-
sure conflates semantic and lexical factors, as the
high PMI of AyN in at least some rigid order AANs
might be also a cue of the fact that the latter bigram
is a lexicalized phrase (as discussed in footnote 2, it
147
is unlikely that our filtering strategies sifted out all
multiword expressions). Moreover, ?PMI does not
produce a semantic representation of the phrase (see
how composed distributional vectors approximate of
high quality AAN vectors in Table 3). Finally, this
measure will not scale up to cases where the ANs
are not attested, whereas measures based on compo-
sition only need corpus-harvested representations of
adjectives and nouns.
3.3 Properties of the correct adjective order
Having shown that flexible order and rigid order
AANs are significantly distinguished by various
properties, we proceed now to test whether those
same properties also allow us to distinguish between
correct (corpus-attested) and wrong (unattested) ad-
jective ordering in rigid AANs (recall that we are
working with cases where the attested-order occurs
more than 20 times in the corpus, and both adjec-
tives modify the nouns at least 10 times, so we are
confident that there is a true asymmetry).
We expect that the fundamental property that dis-
tinguishes the orders is again found in the degree
of modification of both component adjectives. We
predict that the single concept created by the AyN
in attested-order rigid AANs, such as legal status
in formal legal status, is an effect of the modifica-
tion strength of the Ay on the head noun, and when
seen in the incorrect ordering, i.e., ?legal formal sta-
tus, the strong modification of legal will still domi-
nate the meaning of the AAN. Composition models
should be able to capture this effect based on the dis-
tance from both the component adjectives and ANs.
Clearly, we cannot run these analyses on corpus-
extracted vectors since the unattested order, by def-
inition, is not seen in our corpus, and therefore we
cannot collect co-occurrence statistics for the AAN
phrase. Thus, we test our measures of adjective or-
dering on the model-generated AAN vectors, for all
gold rigid order AANs in both orders.
We also consider the ?PMI measure which was
so effective in distinguishing flexible vs. rigid or-
der AANs. We expect that the greater association
with AyN for attested-order AANs will again lead
to large, negative differences in PMI scores, while
the expectation that unattested-order AANs will be
highly associated with their AxN will correspond to
large, positive differences in PMI.
Measure t sig.
W.ADD
cosAx -7.840 * U>A
cosAy 7.924 * A>U
cosN 2.394
cosAxN -5.462 * U>A
cosAyN 3.627 * A>U
F.ADD
cosAx -8.418 * U>A
cosAy 6.534 * A>U
cosN -1.927
cosAxN -3.583 * U>A
cosAyN -2.185
MULT
cosAx -5.100 * U>A
cosAy 5.100 * A>U
cosN 0.000
cosAxN -0.598
cosAyN 0.598
LFM
cosAx -7.498 * U>A
cosAy 7.227 * A>U
cosN -2.172
cosAxN -5.792 * U>A
cosAyN 0.774
?PMI -11.448 * U>A
Table 5: Attested- vs. unattested-order rigid order
AANs. t-normalized mean paired cosine (or ?PMI) dif-
ferences between attested (A) and unattested (U) AANs
with their components. For significant differences (paired
t-test p<0.05 after Bonferroni correction), last column
reports whether cosines (or ?PMI) are on average larger
for A or U.
Across all composition models, we find that the
distance between the model-generated AAN and its
component adjectives, Ax and Ay, are significant in-
dicators of attested vs. unattested adjective ordering
(cf. Table 5). Specifically, we find that rigid order
AANs in the correct order are closest to their Ay,
while we can detect the unattested order when the
rigid order AAN is closer to its Ax. This finding
is quite interesting, since it shows that the order in
which the composition functions are applied does
not alter the fact that the modification of one ad-
jective in rigid order AANs (the Ay in the case of
attested-order rigid order AANs) is much stronger
than the other. Unlike the measures that differenti-
ated flexible and rigid order AANs, here we see that
the distance from the component N is not an indi-
cator of the correct adjective ordering (trivially so
for MULT, where attested and unattested AANs are
identical).
Next, we find that for W.ADD, F.ADD and LFM,
148
the distance from the component AxN is a strong
indicator of attested- vs. unattested-order rigid order
AANs. Specifically, attested-order AANs are further
from their AxN than unattested-order AANs. This
finding is in line with our predictions and follows
the findings of the impact of the distance from the
component adjectives.
?PMI, as seen in the ability to distinguish flexi-
ble vs. rigid order AANs, is the strongest indicator
of correct vs wrong adjective ordering. This mea-
sure confirms that the association of one adjective
(the Ay in attested-order AANs) with the head noun
is indeed the most significant factor distinguishing
these two classes. However, as we mentioned be-
fore, this measure has its limitations and is likely not
to be entirely sufficient for future steps in modeling
recursive modification.
4 Conclusion
While AN constructions have been extensively stud-
ied within the framework of compositional distri-
butional semantics (Baroni and Zamparelli, 2010;
Boleda et al, 2012; Boleda et al, 2013; Guevara,
2010; Mitchell and Lapata, 2010; Turney, 2012;
Vecchi et al, 2011), for the first time, we extended
the investigation to recursively built AAN phrases.
First, we showed that composition functions ap-
plied recursively can approximate corpus-extracted
AAN vectors that we know to be of high semantic
quality.
Next, we looked at some properties of the same
high-quality corpus-extracted AAN vectors, finding
that the distinction between ?flexible? AANs, where
the adjective order can be flipped, and ?rigid? ones,
where the order is fixed, is reflected in distributional
cues. These results all derive from the intuition that
the most embedded adjective in a rigid AAN has a
very strong effect on the distributional semantic rep-
resentation of the AAN. Most compositional models
were able to capture at least some of the same cues
that emerged in the analysis of the corpus-extracted
vectors.
Finally, similar cues were also shown to distin-
guish (compositional) representations of rigid AANs
in the ?correct? (corpus-attested) and ?wrong?
(unattested) orders, again pointing to the degree to
which the (attested-order) closest adjective affects
the overall AAN meaning as an important factor.
Comparing the composition functions, we find
that the linguistically motivated LFM approach has
the most consistent performance across all our tests.
This model significantly outperformed all others in
approximating high-quality corpus-extracted AAN
vectors, it provided the closest approximation to the
corpus-observed patterns when distinguishing flexi-
ble and rigid AANs, and it was one of the models
with the strongest cues distinguishing attested and
unattested orders of rigid AANs.
From an applied point of view, a natural next step
would be to use the cues we proposed as features to
train a classifier to predict the preferred order of ad-
jectives, to be tested also in cases where neither or-
der is found in the corpus, so direct corpus evidence
cannot help. For a full account of adjectival order-
ing, non-semantic factors should also be taken into
account. As shown by the effectiveness in our ex-
periments of PMI, which is a classic measure used
to harvest idioms and other multiword expressions
(Church and Hanks, 1990), ordering is affected by
arbitrary lexicalization patterns. Metrical effects are
also likely to play a role, like they do in the well-
studied case of ?binomials? such as salt and pep-
per (Benor and Levy, 2006; Copestake and Herbe-
lot, 2011). In a pilot study, we found that indeed
word length (roughly quantified by number of let-
ters) is a significant factor in predicting adjective
ordering (the shorter adjective being more likely to
occur first), but its effect is not nearly as strong as
that of the semantic measures we considered here.
In our future work, we would like to develop an or-
der model that exploits semantic, metrical and lexi-
calization features jointly for maximal classification
accuracy.
Adjectival ordering information could be useful
in parsing: in English, it could tell whether an
AANN sequence should be parsed as A[[AN]N]
or A[A[NN]]; in languages with pre- and post-
N adjectives, like Italian or Spanish, it could tell
whether ANA sequences should be parsed as A[NA]
or [AN]A. The ability to detect ordering restric-
tions could also help Natural Language Generation
tasks (Malouf, 2000), especially for the generation
of unattested combinations of As and Ns.
From a theoretical point of view, we would like to
extend our analysis to adjective coordination (what?s
149
the difference between new and creative idea and
new creative idea?). Additionally, we could go more
granular, looking at whether compositional models
can help us to understand why certain classes of ad-
jectives are more likely to precede or follow others
(why is size more likely to take scope over color,
so that big red car sounds more natural than red big
car?) or studying the behaviour of specific adjectives
(can our approach capture the fact that strong alco-
holic drink is preferable to alcoholic strong drink
because strong pertains to the alcoholic properties
of the drink?).
In the meantime, we hope that the results we re-
ported here provide convincing evidence of the use-
fulness of compositional distributional semantics in
tackling topics, such as recursive adjectival modifi-
cation, that have been of traditional interest to theo-
retical linguists from a new perspective.
Acknowledgments
We would like to thank the anonymous reviewers,
Fabio Massimo Zanzotto, Yao-Zhong Zhang and the
members of the COMPOSES team. This research
was supported by the ERC 2011 Starting Indepen-
dent Research Grant n. 283554 (COMPOSES).
References
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of EMNLP, pages 1183?1193, Boston,
MA.
Sarah Bunin Benor and Roger Levy. 2006. The chicken
or the egg? A probabilistic analysis of english binomi-
als. Language, pages 233?278.
William Blacoe and Mirella Lapata. 2012. A comparison
of vector-based representations for semantic composi-
tion. In Proceedings of the 2012 Joint Conference on
EMNLP and CoNLL, pages 546?556, Jeju Island, Ko-
rea.
Gemma Boleda, Eva Maria Vecchi, Miquel Cornudella,
and Louise McNally. 2012. First-order vs. higher-
order modification in distributional semantics. In Pro-
ceedings of the 2012 Joint Conference on EMNLP and
CoNLL, pages 1223?1233, Jeju Island, Korea.
Gemma Boleda, Marco Baroni, Louise McNally, and
Nghia Pham. 2013. Intensionality was only alleged:
On adjective-noun composition in distributional se-
mantics. In Proceedings of IWCS, pages 35?46, Pots-
dam, Germany.
Chris Callison-Burch and Mark Dredze. 2010. Creating
speech and language data with amazon?s mechanical
turk. In Proceedings of the NAACL HLT 2010 Work-
shop on Creating Speech and Language Data with
Amazon?s Mechanical Turk, pages 1?12, Los Angeles,
CA.
Kenneth Church and Peter Hanks. 1990. Word asso-
ciation norms, mutual information, and lexicography.
Computational Linguistics, 16(1):22?29.
Guglielmo Cinque, editor. 2002. Functional Structure in
DP and IP - The Carthography of Syntactic Structures,
volume 1. Oxford University Press.
Guglielmo Cinque. 2004. Issues in adverbial syntax.
Lingua, 114:683?710.
Guglielmo Cinque. 2010. The syntax of adjectives: a
comparative study. MIT Press.
Ann Copestake and Aure?lie Herbelot. 2011. Exciting
and interesting: issues in the generation of binomials.
In Proceedings of the UCNLG+ Eval: Language Gen-
eration and Evaluation Workshop, pages 45?53, Edin-
burgh, UK.
Paola Crisma. 1991. Functional categories inside the
noun phrase: A study on the distribution of nominal
modifiers. ?Tesi di Laurea?, University of Venice.
Georgiana Dinu, Nghia The Pham, and Marco Baroni.
2013a. DISSECT: DIStributional SEmantics Compo-
sition Toolkit. In Proceedings of the System Demon-
strations of ACL 2013, East Stroudsburg, PA.
Georgiana Dinu, Nghia The Pham, and Marco Baroni.
2013b. General estimation and evaluation of compo-
sitional distributional semantic models. In Proceed-
ings of the ACL 2013 Workshop on Continuous Vec-
tor Space Models and their Compositionality (CVSC
2013), East Stroudsburg, PA.
Gottlob Frege. 1892. U?ber sinn und bedeutung.
Zeitschrift fuer Philosophie un philosophische Kritik,
100.
Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011.
Experimental support for a categorical compositional
distributional model of meaning. In Proceedings of
EMNLP, Edinburgh, UK.
Emiliano Guevara. 2010. A regression model of
adjective-noun compositionality in distributional se-
mantics. In Proceedings of the ACL GEMS Workshop,
pages 33?37, Uppsala, Sweden.
Chih-Jen Lin. 2007. Projected gradient methods for
Nonnegative Matrix Factorization. Neural Computa-
tion, 19(10):2756?2779.
Robert Malouf. 2000. The order of prenominal adjec-
tives in natural language generation. In Proceedings
of ACL, pages 85?92, East Stroudsburg, PA.
150
Jeff Mitchell and Mirella Lapata. 2010. Composition in
distributional models of semantics. Cognitive Science,
34(8):1388?1429.
Richard Montague. 1970. Universal Grammar. Theoria,
36:373?398.
Robert Munro, Steven Bethard, Victor Kuperman,
Vicky Tzuyin Lai, Robin Melnick, Christopher Potts,
Tyler Schnoebelen, and Harry Tily. 2010. Crowd-
sourcing and language studies: the new generation of
linguistic data. In Proceedings of the NAACL HLT
2010 Workshop on Creating Speech and Language
Data with Amazon?s Mechanical Turk, pages 122?130,
Los Angeles, CA.
Barbara Partee. 2004. Compositionality. In Compo-
sitionality in Formal Semantics: Selected Papers by
Barbara H. Partee. Blackwell, Oxford.
Magnus Sahlgren. 2006. The Word-Space Model. Dis-
sertation, Stockholm University.
Helmut Schmid. 1995. Improvements in part-of-speech
tagging with an application to German. In Proceed-
ings of the EACL-SIGDAT Workshop, Dublin, Ireland.
Hinrich Schu?tze. 1997. Ambiguity Resolution in Natural
Language Learning. CSLI, Stanford, CA.
Gary-John Scott. 2002. Stacked adjectival modification
and the structure of nominal phrases. In Guglielmo
Cinque, editor, Functional Structure in DP and IP. The
Carthography of Syntactic Structures, volume 1. Ox-
ford University Press.
Richard Socher, E.H. Huang, J. Pennington, Andrew Y.
Ng, and C.D. Manning. 2011. Dynamic pooling and
unfolding recursive autoencoders for paraphrase de-
tection. Advances in Neural Information Processing
Systems, 24:801?809.
Richard Socher, Brody Huval, Christopher D. Manning,
and Andrew Y. Ng. 2012. Semantic compositionality
through recursive matrix-vector spaces. In Proceed-
ings of EMNLP, pages 1201?1211, Edinburgh, UK.
Richard Sproat and Chilin Shih. 1990. The cross-
linguistics distribution of adjective ordering restric-
tions. In C. Georgopoulos and Ishihara R., editors,
Interdisciplinary approaches to language: essays in
honor of Yuki Kuroda, pages 565?593. Kluver, Dor-
drecht.
Sam Steddy and Vieri Samek-Lodovici. 2011. On the
ungrammaticality of remnant movement in the deriva-
tion of greenberg?s universal 20. Linguistic Inquiry,
42(3):445?469.
Richmond H. Thomason, editor. 1974. Formal Philoso-
phy: Selected Papers of Richard Montague. Yale Uni-
versity Press, New York.
Peter Turney and Patrick Pantel. 2010. From frequency
to meaning: Vector space models of semantics. Jour-
nal of Artificial Intelligence Research, 37:141?188.
Peter Turney. 2012. Domain and function: A dual-space
model of semantic relations and compositions. Jour-
nal of Artificial Intelligence Research, 44:533?585.
Eva Maria Vecchi, Marco Baroni, and Roberto Zampar-
elli. 2011. (Linear) maps of the impossible: Cap-
turing semantic anomalies in distributional space. In
Proceedings of the ACL Workshop on Distributional
Semantics and Compositionality, pages 1?9, Portland,
OR.
Fabio Zanzotto, Ioannis Korkontzelos, Francesca Faluc-
chi, and Suresh Manandhar. 2010. Estimating linear
models for compositional distributional semantics. In
Proceedings of COLING, pages 1263?1271, Beijing,
China.
151
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1517?1526,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Compositional-ly Derived Representations of
Morphologically Complex Words in Distributional Semantics
Angeliki Lazaridou and Marco Marelli and Roberto Zamparelli and Marco Baroni
Center for Mind/Brain Sciences (University of Trento, Italy)
first.last@unitn.it
Abstract
Speakers of a language can construct an
unlimited number of new words through
morphological derivation. This is a major
cause of data sparseness for corpus-based
approaches to lexical semantics, such as
distributional semantic models of word
meaning. We adapt compositional meth-
ods originally developed for phrases to the
task of deriving the distributional meaning
of morphologically complex words from
their parts. Semantic representations con-
structed in this way beat a strong baseline
and can be of higher quality than represen-
tations directly constructed from corpus
data. Our results constitute a novel evalua-
tion of the proposed composition methods,
in which the full additive model achieves
the best performance, and demonstrate the
usefulness of a compositional morphology
component in distributional semantics.
1 Introduction
Effective ways to represent word meaning are
needed in many branches of natural language pro-
cessing. In the last decades, corpus-based meth-
ods have achieved some degree of success in mod-
eling lexical semantics. Distributional semantic
models (DSMs) in particular represent the mean-
ing of a word by a vector, the dimensions of which
encode corpus-extracted co-occurrence statistics,
under the assumption that words that are semanti-
cally similar will occur in similar contexts (Turney
and Pantel, 2010). Reliable distributional vectors
can only be extracted for words that occur in many
contexts in the corpus. Not surprisingly, there is
a strong correlation between word frequency and
vector quality (Bullinaria and Levy, 2007), and
since most words occur only once even in very
large corpora (Baroni, 2009), DSMs suffer data
sparseness.
While word rarity has many sources, one of the
most common and systematic ones is the high pro-
ductivity of morphological derivation processes,
whereby an unlimited number of new words can
be constructed by adding affixes to existing stems
(Baayen, 2005; Bauer, 2001; Plag, 1999).1 For
example, in the multi-billion-word corpus we in-
troduce below, perfectly reasonable derived forms
such as lexicalizable or affixless never occur. Even
without considering the theoretically infinite num-
ber of possible derived nonce words, and restrict-
ing ourselves instead to words that are already
listed in dictionaries, complex forms cover a high
portion of the lexicon. For example, morphologi-
cally complex forms account for 55% of the lem-
mas in the CELEX English database (see Section
4.1 below). In most of these cases (80% according
to our corpus) the stem is more frequent than the
complex form (e.g., the stem build occurs 15 times
more often than the derived form rebuild, and the
latter is certainly not an unusual derived form).
DSMs ignore derivational morphology alto-
gether. Consequently, they cannot provide mean-
ing representations for new derived forms, nor can
they harness the systematic relation existing be-
tween stems and derivations (any English speaker
can infer that to rebuild is to build again, whether
they are familiar with the prefixed form or not)
in order to mitigate derived-form sparseness prob-
lems. A simple way to handle derivational mor-
1Morphological derivation constructs new words (in
the sense of lemmas) from existing lexical items (re-
source+ful?resourceful). In this work, we do not treat in-
flectional morphology, pertaining to affixes that encode gram-
matical features such as number or tense (dog+s). We use
morpheme for any component of a word (resource and -ful
are both morphemes). We use stem for the lexical item that
constitutes the base of derivation (resource) and affix (pre-
fix or suffix) for the element attached to the stem to derive
the new form (-ful). In English, stems are typically indepen-
dent words, affixes bound morphemes, i.e., they cannot stand
alone. Note that a stem can in turn be morphologically de-
rived, e.g., point+less in pointless+ly. Finally, we use mor-
phologically complex as synonymous with derived.
1517
phology would be to identify the stem of rare de-
rived words and use its distributional vector as a
proxy to derived-form meaning.2 The meaning of
rebuild is not that far from that of build, so the
latter might provide a reasonable surrogate. Still,
something is clearly lost (if the author of a text
felt the need to use the derived form, the stem was
not fully appropriate), and sometimes the jump in
meaning can be quite dramatic (resourceless and
resource mean very different things!).
In the past few years there has been much in-
terest in how DSMs can scale up to represent the
meaning of larger chunks of text such as phrases
or even sentences. Trying to represent the mean-
ing of arbitrarily long constructions by directly
collecting co-occurrence statistics is obviously in-
effective and thus methods have been developed
to derive the meaning of larger constructions as a
function of the meaning of their constituents (Ba-
roni and Zamparelli, 2010; Coecke et al, 2010;
Mitchell and Lapata, 2008; Mitchell and Lapata,
2010; Socher et al, 2012). Compositional distri-
butional semantic models (cDSMs) of word units
aim at handling, compositionally, the high produc-
tivity of phrases and consequent data sparseness.
It is natural to hypothesize that the same methods
can be applied to morphology to derive the mean-
ing of complex words from the meaning of their
parts: For example, instead of harvesting a rebuild
vector directly from the corpus, the latter could be
constructed from the distributional representations
of re- and build. Besides alleviating data sparse-
ness problems, a system of this sort, that automati-
cally induces the semantic contents of morpholog-
ical processes, would also be of tremendous theo-
retical interest, given that the semantics of deriva-
tion is a central and challenging topic in linguistic
morphology (Dowty, 1979; Lieber, 2004).
In this paper, we explore, for the first time (ex-
cept for the proof-of-concept study in Guevara
(2009)), the application of cDSMs to derivational
morphology. We adapt a number of composition
methods from the literature to the morphological
setting, and we show that some of these methods
can provide better distributional representations of
derived forms than either those directly harvested
from a large corpus, or those obtained by using
the stem as a proxy to derived-form meaning. Our
2Of course, spotting and segmenting complex words is a
big research topic unto itself (Beesley and Karttunen, 2000;
Black et al, 1991; Sproat, 1992), and one we completely
sidestep here.
results suggest that exploiting morphology could
improve the quality of DSMs in general, extend
the range of tasks that cDSMs can successfully
model and support the development of new ways
to test their performance.
2 Related work
Morphological induction systems use corpus-
based methods to decide if two words are mor-
phologically related and/or to segment words into
morphemes (Dreyer and Eisner, 2011; Goldsmith,
2001; Goldwater and McClosky, 2005; Goldwater,
2006; Naradowsky and Goldwater, 2009; Wicen-
towski, 2004). Morphological induction has re-
cently received considerable attention since mor-
phological analysis can mitigate data sparseness in
domains such as parsing and machine translation
(Goldberg and Tsarfaty, 2008; Lee, 2004). Among
the cues that have been exploited there is distri-
butional similarity among morphologically related
words (Schone and Jurafsky, 2000; Yarowsky and
Wicentowski, 2000). Our work, however, dif-
fers substantially from this track of research. We
do not aim at segmenting morphological complex
words or identifying paradigms. Our goal is to
automatically construct, given distributional rep-
resentations of stems and affixes, semantic repre-
sentations for the derived words containing those
stems and affixes. A morphological induction sys-
tem, given rebuild, will segment it into re- and
build (possibly using distributional similarity be-
tween the words as a cue). Our system, given
re- and build, predicts the (distributional seman-
tic) meaning of rebuild.
Another emerging line of research uses distribu-
tional semantics to model human intuitions about
the semantic transparency of morphologically de-
rived or compound expressions and how these im-
pact various lexical processing tasks (Kuperman,
2009; Wang et al, 2012). Although these works
exploit vectors representing complex forms, they
do not attempt to generate them compositionally.
The only similar study we are aware of is that
of Guevara (2009). Guevara found a systematic
geometric relation between corpus-based vectors
of derived forms sharing an affix and their stems,
and used this finding to motivate the composition
method we term lexfunc below. However, unlike
us, he did not test alternative models, and he only
presented a qualitative analysis of the trajectories
triggered by composition with various affixes.
1518
3 Composition methods
Distributional semantic models (DSMs), also
known as vector-space models, semantic spaces,
or by the names of famous incarnations such as
Latent Semantic Analysis or Topic Models, ap-
proximate the meaning of words with vectors that
record their patterns of co-occurrence with cor-
pus context features (often, other words). There
is an extensive literature on how to develop such
models and on their evaluation. Recent surveys
include Clark (2012), Erk (2012) and Turney and
Pantel (2010). We focus here on compositional
DSMs (cDSMs). Since the very inception of dis-
tributional semantics, there have been attempts to
compose meanings for sentences and larger pas-
sages (Landauer and Dumais, 1997), but inter-
est in compositional DSMs has skyrocketed in
the last few years, particularly since the influen-
tial work of Mitchell and Lapata (2008; 2009;
2010). For the current study, we have reimple-
mented and adapted to the morphological setting
all cDSMs we are aware of, excluding the tensor-
product-based models that Mitchell and Lapata
(2010) have shown to be empirically disappointing
and the models of Socher and colleagues (Socher
et al, 2011; Socher et al, 2012), that require com-
plex optimization procedures whose adaptation to
morphology we leave to future work.
Mitchell and Lapata proposed a set of simple
and effective models in which the composed vec-
tors are obtained through component-wise opera-
tions on the constituent vectors. Given input vec-
tors u and v, the multiplicative model (mult) re-
turns a composed vector c with: ci = uivi. In the
weighted additive model (wadd), the composed
vector is a weighted sum of the two input vectors:
c = ?u + ?v, where ? and ? are two scalars. In
the dilation model, the output vector is obtained
by first decomposing one of the input vectors, say
v, into a vector parallel to u and an orthogonal
vector. Following this, the parallel vector is dilated
by a factor ? before re-combining. This results in:
c = (?? 1)?u,v?u+ ?u,u?v.
Guevara (2010) and Zanzotto et al (2010) pro-
pose the full additive model (fulladd), where the
two vectors to be added are pre-multiplied by
weight matrices: c = Au+Bv
Since the Mitchell and Lapata and fulladd mod-
els were developed for phrase composition, the
two input vectors were taken to be, very straight-
forwardly, the vectors of the two words to be com-
posed into the phrase of interest. In morphological
derivation, at least one of the items to be composed
(the affix) is a bound morpheme. In our adapta-
tion of these composition models, we build bound
morpheme vectors by accumulating the contexts
in which a set of derived words containing the rel-
evant morphemes occur, e.g., the re- vector aggre-
gates co-occurrences of redo, remake, retry, etc.
Baroni and Zamparelli (2010) and Coecke et
al. (2010) take inspiration from formal semantics
to characterize composition in terms of function
application, where the distributional representa-
tion of one element in a composition (the func-
tor) is not a vector but a function. Given that
linear functions can be expressed by matrices and
their application by matrix-by-vector multiplica-
tion, in this lexical function (lexfunc) model, the
functor is represented by a matrix U to be multi-
plied with the argument vector v: c = Uv. In
the case of morphology, it is natural to treat bound
affixes as functions over stems, since affixes en-
code the systematic semantic patterns we intend
to capture. Unlike the other composition meth-
ods, lexfunc does not require the construction of
distributional vectors for affixes. A matrix repre-
sentation for every affix is instead induced directly
from examples of stems and the corresponding de-
rived forms, in line with the intuition that every af-
fix corresponds to a different pattern of change of
the stem meaning.
Finally, as already discussed in the Introduc-
tion, performing no composition at all but using
the stem vector as a surrogate of the derived form
is a reasonable strategy. We saw that morphologi-
cally derived words tend to appear less frequently
than their stems, and in many cases the meanings
are close. Consequently, we expect a stem-only
?composition? method to be a strong baseline in
the morphological setting.
4 Experimental setup
4.1 Morphological data
We obtained a list of stem/derived-form pairs from
the CELEX English Lexical Database, a widely
used 100K-lemma lexicon containing, among
other things, information about the derivational
structure of words (Baayen et al, 1995). For each
derivational affix present in CELEX, we extracted
from the database the full list of stem/derived
pairs matching its most common part-of-speech
signature (e.g., for -er we only considered pairs
1519
Affix Stem/Der. Training HQ/Tot. Avg.
POS Items Test Items SDR
-able verb/adj 177 30/50 5.96
-al noun/adj 245 41/50 5.88
-er verb/noun 824 33/50 5.51
-ful noun/adj 53 42/50 6.11
-ic noun/adj 280 43/50 5.99
-ion verb/noun 637 38/50 6.22
-ist noun/noun 244 38/50 6.16
-ity adj/noun 372 33/50 6.19
-ize noun/verb 105 40/50 5.96
-less noun/adj 122 35/50 3.72
-ly adj/adv 1847 20/50 6.33
-ment verb/noun 165 38/50 6.06
-ness adj/noun 602 33/50 6.29
-ous noun/adj 157 35/50 5.94
-y noun/adj 404 27/50 5.25
in- adj/adj 101 34/50 3.39
re- verb/verb 86 27/50 5.28
un- adj/adj 128 36/50 3.23
tot */* 6549 623/900 5.52
Table 1: Derivational morphology dataset
having a verbal stem and nominal derived form).
Since CELEX was populated by semi-automated
morphological analysis, it includes forms that are
probably not synchronically related to their stems,
such as crypt+ic or re+form. However, we did not
manually intervene on the pairs, since we are in-
terested in training and testing our methods in re-
alistic, noisy conditions. In particular, the need to
pre-process corpora to determine which forms are
?opaque?, and should thus be bypassed by our sys-
tems, would greatly reduce their usefulness. Pairs
in which either word occurred less than 20 times
in our source corpus (described in Section 4.2 be-
low) were filtered out and, in our final dataset, we
only considered the 18 affixes (3 prefixes and 15
suffixes) with at least 100 pairs meeting this con-
dition. We randomly chose 50 stem/derived pairs
(900 in total) as test data. The remaining data were
used as training items to estimate the parameters
of the composition methods. Table 1 summarizes
various characteristics of the dataset3 (the last two
columns of the table are explained in the next para-
graphs).
Annotation of quality of test vectors The qual-
ity of the corpus-based vectors representing de-
rived test items was determined by collecting hu-
man semantic similarity judgments in a crowd-
sourcing survey. In particular, we use the similar-
ity of a vector to its nearest neighbors (NNs) as a
proxy measure of quality. The underlying assump-
3Available from http://clic.cimec.unitn.it/
composes
tion is that a vector, in order to be a good represen-
tation of the meaning of the corresponding word,
should lie in a region of semantic space populated
by intuitively similar meanings, e.g., we are more
likely to have captured the meaning of car if the
NN of its vector is the automobile vector rather
than potato. Therefore, to measure the quality of
a given vector, we can look at the average simi-
larity score provided by humans when comparing
this very vector with its own NNs.
All 900 derived vectors from the test set were
matched with their three closest NNs in our se-
mantic space (see Section 4.2), thus producing a
set of 2, 700 word pairs. These pairs were admin-
istered to CrowdFlower users,4 who were asked
to judge the relatedness of the two meanings on a
7-point scale (higher for more related). In order
to ensure that participants were committed to the
task and exclude non-proficient English speakers,
we used 60 control pairs as gold standard, consist-
ing of either perfect synonyms or completely un-
related words. We obtained 30 judgments for each
derived form (10 judgments for each of 3 neighbor
comparisons), with mean participant agreement of
58%. These ratings were averaged item-wise, re-
sulting in a Gaussian distribution with a mean of
3.79 and a standard deviation of 1.31. Finally,
each test item was marked as high-quality (HQ)
if its derived form received an average score of at
least 3, as low-quality (LQ) otherwise. Table 1 re-
ports the proportion of HQ test items for each af-
fix, and Table 2 reports some examples of HQ and
LQ items with the corresponding NNs. It is worth
observing that the NNs of the LQ items, while not
as relevant as the HQ ones, are hardly random.
Annotation of similarity between stem and de-
rived forms Derived forms differ in terms of
how far their meaning is with respect to that of
their stem. Certain morphological processes have
systematically more impact than others on mean-
ing: For example, the adjectival prefix in- negates
the meaning of the stem, whereas -ly has the sole
function to convert an adjective into an adverb.
But the very same affix can affect different stems
in different ways. For example, remelt means lit-
tle more than to melt again, but rethink has subtler
implications of changing one?s way to look at a
problem, and while one of the senses of cycling is
present in recycle, it takes some effort to see their
relation.
4http://www.crowdflower.com
1520
Affix Type Derived form Neighbors
-ist HQ transcendentalist mythologist, futurist, theosophistLQ florist Harrod, wholesaler, stockist
-ity HQ publicity publicise, press, publicizeLQ sparsity dissimilarity, contiguity, perceptibility
-ment HQ advertisement advert, promotional, advertisingLQ inducement litigant, contractually, voluntarily
in- HQ inaccurate misleading, incorrect, erroneousLQ inoperable metastasis, colorectal, biopsy
re- HQ recapture retake, besiege, captureLQ rename defunct, officially, merge
Table 2: Examples of HQ and LQ derived vectors with their NNs
We conducted a separate crowdsourcing study
where participants were asked to rate the 900
test stem/derived pairs for the strength of their
semantic relationship on a 7-point scale. We
followed a procedure similar to the one de-
scribed for quality measurement; 7 judgments
were collected for each pair. Participants? agree-
ment was at 60%. The last column of Ta-
ble 1 reports the average stem/derived related-
ness (SDR) for the various affixes. Note that
the affixes with systematically lower SDR are
those carrying a negative meaning (in-, un-, -less),
whereas those with highest SDR do little more
than changing the POS of the stem (-ion, -ly, -
ness). Among specific pairs with very low related-
ness we encounter hand/handy, bear/bearable and
active/activist, whereas compulsory/compulsorily,
shameless/shamelessness and chaos/chaotic have
high SDR. Since the distribution of the average
ratings was negatively skewed (mean rating: 5.52,
standard deviation: 1.26),5 we took 5 as the rating
threshold to classify items as having high (HR) or
low (LR) relatedness to their stems.
4.2 Distributional semantic space6
We use as our source corpus the concatenation of
ukWaC, the English Wikipedia (2009 dump) and
the BNC,7 for a total of about 2.8 billion tokens.
We collect co-occurrence statistics for the top 20K
content words (adjectives, adverbs, nouns, verbs)
5The negative skew is not surprising, as derived forms
must have some relation to their stems!
6Most steps of the semantic space construction
and composition pipelines were implemented using
the DISSECT toolkit: https://github.com/
composes-toolkit/dissect.
7http://wacky.sslmit.unibo.it, http:
//en.wikipedia.org, http://www.natcorp.
ox.ac.uk
in lemma format, plus any item from the mor-
phological dataset described above that was below
this rank. The top 20K content words also con-
stitute our context elements. We use a standard
bag-of-words approach, counting collocates in a
narrow 2-word before-and-after window. We ap-
ply (non-negative) Pointwise Mutual Information
as weighting scheme and dimensionality reduc-
tion by Non-negative Matrix Factorization, setting
the number of reduced-space dimensions to 350.
These settings are chosen without tuning, and are
based on previous experiments where they pro-
duced high-quality semantic spaces (Boleda et al,
2013; Bullinaria and Levy, 2007).
4.3 Implementation of composition methods
All composition methods except mult and stem
have weights to be estimated (e.g., the ? parame-
ter of dilation or the affix matrices of lexfunc). We
adopt the estimation strategy proposed by Gue-
vara (2010) and Baroni and Zamparelli (2010),
namely we pick parameter values that optimize
the mapping between stem and derived vectors di-
rectly extracted from the corpus. To learn, say, a
lexfunc matrix representing the prefix re-, we ex-
tract vectors of V/reV pairs that occur with suffi-
cient frequency (visit/revisit, think/rethink. . . ). We
then use least-squares methods to find weights for
the re- matrix that minimize the distance between
each reV vector generated by the model given the
input V and the corresponding corpus-observed
derived vector (e.g., we try to make the model-
predicted re+visit vector as similar as possible
to the corpus-extracted one). This is a general
estimation approach that does not require task-
specific hand-labeled data, and for which simple
analytical solutions of the least-squares error prob-
1521
lem exist for all our composition methods. We use
only the training items from Section 4.1 for esti-
mation. Note that, unlike the test items, these have
not been annotated for quality, so we are adopting
an unsupervised (no manual labeling) but noisy es-
timation method.8
For the lexfunc model, we use the training items
separately to obtain weight matrices represent-
ing each affix, whereas for the other models all
training data are used together to globally de-
rive single sets of affix and stem weights. For
the wadd model, the learning process results in
0.16?affix+0.33? stem, i.e., the affix contributes
only half of its mass to the composition of the
derived form. For dilation, we stretch the stem
(i.e., v of the dilation equation is the stem vector),
since it should provide richer contents than the af-
fix to the derived meaning. We found that, on av-
erage across the training pairs, dilation weighted
the stem 20 times more heavily than the affix
(0.05?affix+1?stem). We then expect that the di-
lation model will have similar performance to the
baseline stem model, as confirmed below.9
For all methods, vectors were normalized be-
fore composing both in training and in generation.
5 Experiment 1: approximating
high-quality corpus-extracted vectors
The first experiment investigates to what extent
composition models can approximate high-quality
(HQ) corpus-extracted vectors representing de-
rived forms. Note that since the test items were
excluded from training, we are simulating a sce-
nario in which composition models must generate
representations for nonce derived forms.
Cosine similarity between model-generated and
corpus-extracted vectors were computed for all
models, including the stem baseline (i.e., co-
sine between stem and derived form). The first
row of Table 3 reports mean similarities. The
stem method sets the level of performance rel-
atively high, confirming its soundness. Indeed,
the parameter-free mult model performs below the
baseline.10 As expected, dilation performs simi-
8More accurately, we relied on semi-manual CELEX in-
formation to identify derived forms. A further step towards a
fully knowledge-free system would be to pre-process the cor-
pus with an unsupervised morphological induction system to
extract stem/derived pairs.
9The other models have thousands of weights to be es-
timated, so we cannot summarize the outcome of parameter
estimation here.
10This result does not necessarily contradict those of
stem mult dil. wadd fulladd lexfunc
All 0.47 0.39 0.48 0.50 0.56 0.54
HR 0.52 0.43 0.53 0.55 0.61 0.58
LR 0.32 0.28 0.33 0.38 0.41 0.42
Table 3: Mean similarity of composed vectors to
high-quality corpus-extracted derived-form vec-
tors, for all as well as high- (HR) and low-
relatedness (LR) test items
larly to the baseline, while wadd outperforms it,
although the effect does not reach significance
(p=.06).11 Both fulladd and lexfunc perform sig-
nificantly better than stem (p < .001). Lexfunc
provides a flexible way to account for affixation,
since it models it directly as a function mapping
from and onto word vectors, without requiring a
vector representation of bound affixes. The rea-
son at the base of its good performance is thus
quite straightforward. On the other hand, it is
surprising that a simple representation of bound
affixes (i.e., as vectors aggregating the contexts
of words containing them) can work so well, at
least when used in conjunction with the granular
dimension-by-dimension weights assigned by the
fulladd method. We hypothesize that these aggre-
gated contexts, by providing information about the
set of stems an affix combines with, capture the
shared semantic features that the affix operates on.
When the meaning of the derived form is far
from that of its stem, the stem baseline should no
longer constitute a suitable surrogate of derived-
form meaning. The LR cases (see Section 4.1
above) are thus crucial to understand how well
composition methods capture not only stem mean-
ing, but also affix-triggered semantics. The HR
and LR rows of Table 3 present the results for
the respective test subsets. As expected, the stem
approach undergoes a strong drop when perfor-
mance is measured on LR items. At the other ex-
treme, fulladd and lexfunc, while also finding the
LR cases more difficult, still clearly outperform
the baseline (p<.001), confirming that they cap-
ture the meaning of derived forms beyond what
their stems contribute to it. The effect of wadd,
again, approaches significance when compared to
the baseline (p= .05). Very encouragingly, both
Mitchell and Lapata and others who found mult to be highly
competitive. Due to differences in co-occurrence weighting
schemes (we use a logarithmically scaled measure, they do
not), their multiplicative model is closer to our additive one.
11Significance assessed by means of Tukey Honestly Sig-
nificant Difference tests (Abdi and Williams, 2010)
1522
stem mult wadd dil. fulladd lexfunc
-less 0.22 0.23 0.30 0.24 0.38 0.44
in- 0.39 0.34 0.45 0.40 0.47 0.45
un- 0.33 0.33 0.41 0.34 0.44 0.46
Table 4: Mean similarity of composed vectors to
high-quality corpus-extracted derived-form vec-
tors with negative affixes
fulladd and lexfunc significantly outperform stem
also in the HR subset (p<.001). That is, the mod-
els provide better approximations of derived forms
even when the stem itself should already be a good
surrogate. The difference between the two models
is not significant.
We noted in Section 4.1 that forms containing
the ?negative? affixes -less, un- and in- received
on average low SDR scores, since negation im-
pacts meaning more drastically than other opera-
tions. Table 4 reports the performance of the mod-
els on these affixes. Indeed, the stem baseline per-
forms quite poorly, whereas fulladd, lexfunc and,
to a lesser extent, wadd are quite effective in this
condition as well, all performing greatly above the
baseline. These results are intriguing in light of
the fact that modeling negation is a challenging
task for DSMs (Mohammad et al, 2013) as well as
cDSMs (Preller and Sadrzadeh, 2011). To the ex-
tent that our best methods have captured the negat-
ing function of a prefix such as in-, they might be
applied to tasks such as recognizing lexical op-
posites, or even simple forms of syntactic nega-
tion (modeling inoperable is just a short step away
from modeling not operable compositionally).
6 Experiment 2: Comparing the quality
of corpus-extracted and
compositionally generated words
The first experiment simulated the scenario in
which derived forms are not in our corpus, so
that directly extracting their representation from
it is not an option. The second experiment tests
if compositionally-derived representations can be
better than those extracted directly from the corpus
when the latter is a possible strategy (i.e., the de-
rived forms are attested in the source corpus). To
this purpose, we focused on those 277 test items
that were judged as low-quality (LQ, see Section
4.1), which are presumably more challenging to
generate, and where the compositional route could
be most useful.
We evaluated the derived forms generated by
corpus stem wadd fulladd lexfunc
All 2.28 3.26 4.12 3.99 3.09
HR 2.29 3.56 4.48 4.31 3.31
LR 2.22 2.48 3.14 3.12 2.52
Table 5: Average quality ratings of derived vectors
Target Model Neighbors
florist
wadd flora, fauna, ecosystem
fulladd flora, fauna, egologist
lexfunc ornithologist, naturalist, botanist
sparsity
wadd sparse, sparsely, dense
fulladd sparse, sparseness, angularity
lexfunc fragility, angularity, smallness
inducement
wadd induce, inhibit, inhibition
fulladd induce, inhibition, mediate
lexfunc impairment, cerebral, ocular
inoperable
wadd operable, palliation, biopsy
fulladd operable, inoperative, ventilator
lexfunc inoperative, unavoidably, flaw
rename
wadd name, later, namesake
fulladd name, namesake, later
lexfunc temporarily, reinstate, thereafter
Table 6: Examples of model-predicted neighbors
for words with LQ corpus-extracted vectors
the models that performed best in the first exper-
iment (fulladd, lexfunc and wadd), as well as the
stem baseline, by means of another crowdsourcing
study. We followed the same procedure used to
assess the quality of corpus-extracted vectors, that
is, we asked judges to rate the relatedness of the
target forms to their NNs (we obtained on average
29 responses per form).
The first line of Table 5 reports the average
quality (on a 7-point scale) of the representations
of the derived forms as produced by the models
and baseline, as well as of the corpus-harvested
ones (corpus column). All compositional models
produce representations that are of significantly
higher quality (p < .001) than the corpus-based
ones. The effect is also evident in qualitative
terms. Table 6 presents the NNs predicted by the
three compositional methods for the same LQ test
items whose corpus-based NNs are presented in
Table 2. These results indicate that morpheme
composition is an effective solution when the qual-
ity of corpus-extracted derived forms is low (and
the previous experiment showed that, when their
quality is high, composition can at least approxi-
mate corpus-based vectors).
With respect to Experiment 1, we obtain a dif-
ferent ranking of the models, with lexfunc being
outperformed by both wadd and fulladd (p<.001),
that are statistically indistinguishable. The wadd
1523
composition is dominated by the stem, and by
looking at the examples in Table 6 we notice that
both this model and fulladd tend to feature the
stem as NN (100% of the cases for wadd, 73%
for fulladd in the complete test set). The question
thus arises as to whether the good performance of
these composition techniques is simply due to the
fact that they produce derived forms that are near
their stems, with no added semantic value from the
affix (a ?stemploitation? strategy).
However, the stemploitation hypothesis is dis-
pelled by the observation that both models signifi-
cantly outperform the stem baseline (p<.001), de-
spite the fact that the latter, again, has good per-
formance, significantly outperforming the corpus-
derived vectors (p < .001). Thus, we confirm
that compositional models provide higher qual-
ity vectors that are capturing the meaning of de-
rived forms beyond the information provided by
the stem.
Indeed, if we focus on the third row of Ta-
ble 5, reporting performance on low stem-derived
relatedness (LR) items (annotated as described in
Section 4.1), fulladd and wadd still significantly
outperform the corpus representations (p<.001),
whereas the quality of the stem representations of
LR items is not significantly different form that of
the corpus-derived ones. Interestingly, lexfunc dis-
plays the smallest drop in performance when re-
stricting evaluation to LR items; however, since it
does not significantly outperform the LQ corpus
representations, this is arguably due to a floor ef-
fect.
7 Conclusion and future work
We investigated to what extent cDSMs can gener-
ate effective meaning representations of complex
words through morpheme composition. Several
state-of-the-art composition models were adapted
and evaluated on this novel task. Our results sug-
gest that morpheme composition can indeed pro-
vide high-quality vectors for complex forms, im-
proving both on vectors directly extracted from the
corpus and on a stem-backoff strategy. This re-
sult is of practical importance for distributional se-
mantics, as it paves the way to address one of the
main causes of data sparseness, and it confirms the
usefulness of the compositional approach in a new
domain. Overall, fulladd emerged as the best per-
forming model, with both lexfunc and the simple
wadd approach constituting strong rivals. The ef-
fectiveness of the best models extended also to the
challenging cases where the meaning of derived
forms is far from that of the stem, including nega-
tive affixes.
The fulladd method requires a vector represen-
tation for bound morphemes. A first direction for
future work will thus be to investigate which as-
pects of the meaning of bound morphemes are
captured by our current simple-minded approach
to populating their vectors, and to explore alterna-
tive ways to construct them, seeing if they further
improve fulladd performance.
A natural extension of our research is to ad-
dress morpheme composition and morphological
induction jointly, trying to model the intuition that
good candidate morphemes should have coherent
semantic representations. Relatedly, in the cur-
rent setting we generate complex forms from their
parts. We want to investigate the inverse route,
namely ?de-composing? complex words to de-
rive representations of their stems, especially for
cases where the complex words are more frequent
(e.g. comfort/comfortable).
We would also like to apply composition to in-
flectional morphology (that currently lies outside
the scope of distributional semantics), to capture
the nuances of meaning that, for example, distin-
guish singular and plural nouns (consider, e.g., the
difference between the mass singular tea and the
plural teas, which coerces the noun into a count
interpretation (Katz and Zamparelli, 2012)).
Finally, in our current setup we focus on a single
composition step, e.g., we derive the meaning of
inoperable by composing the morphemes in- and
operable. But operable is in turn composed of op-
erate and -able. In the future, we will explore re-
cursive morpheme composition, especially since
we would like to apply these methods to more
complex morphological systems (e.g., agglutina-
tive languages) where multiple morphemes are the
norm.
8 Acknowledgments
We thank Georgiana Dinu and Nghia The Pham
for helping out with DISSECT-ion and the review-
ers for helpful feedback. This research was sup-
ported by the ERC 2011 Starting Independent Re-
search Grant n. 283554 (COMPOSES).
1524
References
Herve? Abdi and Lynne Williams. 2010. Newman-
Keuls and Tukey test. In Neil Salkind, Bruce Frey,
and Dondald Dougherty, editors, Encyclopedia of
Research Design, pages 897?904. Sage, Thousand
Oaks, CA.
Harald Baayen, Richard Piepenbrock, and Leon Gu-
likers. 1995. The CELEX lexical database (re-
lease 2). CD-ROM, Linguistic Data Consortium,
Philadelphia, PA.
Harald Baayen. 2005. Morphological productivity. In
Rajmund Piotrowski Reinhard Ko?hler, Gabriel Alt-
mann, editor, Quantitative Linguistics: An Inter-
national Handbook, pages 243?256. Mouton de
Gruyter, Berlin, Germany.
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of EMNLP, pages 1183?1193, Boston,
MA.
Marco Baroni. 2009. Distributions in text. In Anke
Lu?deling and Merja Kyto?, editors, Corpus Linguis-
tics: An International Handbook, volume 2, pages
803?821. Mouton de Gruyter, Berlin, Germany.
Laurie Bauer. 2001. Morphological Productivity.
Cambridge University Press, Cambridge, UK.
Kenneth Beesley and Lauri Karttunen. 2000. Finite-
State Morphology: Xerox Tools and Techniques.
Cambridge University Press, Cambridge, UK.
Alan Black, Stephen Pulman, Graeme Ritchie, and
Graham Russell. 1991. Computational Morphol-
ogy. MIT Press, Cambrdige, MA.
Gemma Boleda, Marco Baroni, Louise McNally, and
Nghia Pham. 2013. Intensionality was only alleged:
On adjective-noun composition in distributional se-
mantics. In Proceedings of IWCS, pages 35?46,
Potsdam, Germany.
John Bullinaria and Joseph Levy. 2007. Extracting
semantic representations from word co-occurrence
statistics: A computational study. Behavior Re-
search Methods, 39:510?526.
Stephen Clark. 2012. Vector space models of lexical
meaning. In Shalom Lappin and Chris Fox, editors,
Handbook of Contemporary Semantics, 2nd edition.
Blackwell, Malden, MA. In press.
Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen
Clark. 2010. Mathematical foundations for a com-
positional distributional model of meaning. Linguis-
tic Analysis, 36:345?384.
David Dowty. 1979. Word Meaning and Montague
Grammar. Springer, New York.
Markus Dreyer and Jason Eisner. 2011. Discover-
ing morphological paradigms from plain text using
a Dirichlet process mixture model. In Proceedings
of EMNLP, pages 616?627, Edinburgh, UK.
Katrin Erk. 2012. Vector space models of word mean-
ing and phrase meaning: A survey. Language and
Linguistics Compass, 6(10):635?653.
Yoav Goldberg and Reut Tsarfaty. 2008. A single gen-
erative model for joint morphological segmentation
and syntactic parsing. In Proceedings of ACL, pages
371?379, Columbus, OH.
John Goldsmith. 2001. Unsupervised learning of the
morphology of a natural language. Computational
Linguistics, 2(27):153?198.
Sharon Goldwater and David McClosky. 2005. Im-
proving statistical MT through morphological anal-
ysis. In Proceedings of EMNLP, pages 676?683,
Vancouver, Canada.
Sharon Goldwater. 2006. Nonparametric Bayesian
Models of Lexical Acquisition. Ph.D. thesis, Brown
University.
Emiliano Guevara. 2009. Compositionality in distribu-
tional semantics: Derivational affixes. In Proceed-
ings of the Words in Action Workshop, Pisa, Italy.
Emiliano Guevara. 2010. A regression model of
adjective-noun compositionality in distributional se-
mantics. In Proceedings of GEMS, pages 33?37,
Uppsala, Sweden.
Graham Katz and Roberto Zamparelli. 2012. Quanti-
fying count/mass elasticity. In Proceedings of WC-
CFL, pages 371?379, Tucson, AR.
Victor Kuperman. 2009. Semantic transparency revis-
ited. Presentation at the 6th International Morpho-
logical Processing Conference.
Thomas Landauer and Susan Dumais. 1997. A solu-
tion to Plato?s problem: The latent semantic analysis
theory of acquisition, induction, and representation
of knowledge. Psychological Review, 104(2):211?
240.
Young-Suk Lee. 2004. Morphological analysis for sta-
tistical machine translation. In Proceedings of HLT-
NAACL, pages 57?60, Boston, MA.
Rochelle Lieber. 2004. Morphology and Lexical Se-
mantics. Cambridge University Press, Cambridge,
UK.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL, pages 236?244, Columbus, OH.
Jeff Mitchell and Mirella Lapata. 2009. Language
models based on semantic composition. In Proceed-
ings of EMNLP, pages 430?439, Singapore.
1525
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388?1429.
Saif Mohammad, Bonnie Dorr, Graeme Hirst, and Pe-
ter Turney. 2013. Computing lexical contrast. Com-
putational Linguistics. In press.
Jason Naradowsky and Sharon Goldwater. 2009. Im-
proving morphology induction by learning spelling
rules. In Proceedings of IJCAI, pages 11?17,
Pasadena, CA.
Ingo Plag. 1999. Morphological Productivity: Struc-
tural Constraints in English Derivation. Mouton de
Gruyter, Berlin, Germany.
Anne Preller and Mehrnoosh Sadrzadeh. 2011. Bell
states and negative sentences in the distributed
model of meaning. Electr. Notes Theor. Comput.
Sci., 270(2):141?153.
Patrick Schone and Daniel Jurafsky. 2000.
Knowledge-free induction of morphology us-
ing latent semantic analysis. In Proceedings of the
ConLL workshop on learning language in logic,
pages 67?72, Lisbon, Portugal.
Richard Socher, Eric Huang, Jeffrey Pennin, Andrew
Ng, and Christopher Manning. 2011. Dynamic
pooling and unfolding recursive autoencoders for
paraphrase detection. In Proceedings of NIPS, pages
801?809, Granada, Spain.
Richard Socher, Brody Huval, Christopher Manning,
and Andrew Ng. 2012. Semantic compositionality
through recursive matrix-vector spaces. In Proceed-
ings of EMNLP, pages 1201?1211, Jeju Island, Ko-
rea.
Richard Sproat. 1992. Morphology and Computation.
MIT Press, Cambrdige, MA.
Peter Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Research,
37:141?188.
Hsueh-Cheng Wang, Yi-Min Tien, Li-Chuan Hsu, and
Marc Pomplun. 2012. Estimating semantic trans-
parency of constituents of English compounds and
two-character Chinese words using Latent Semantic
Analysis. In Proceedings of CogSci, pages 2499?
2504, Sapporo, Japan.
Richard Wicentowski. 2004. Multilingual noise-
robust supervised morphological analysis using the
wordframe model. In Proceedings of SIGPHON,
pages 70?77, Barcelona, Spain.
David Yarowsky and Richard Wicentowski. 2000.
Minimally supervised morphological analysis by
multimodal alignment. In Proceedings of ACL,
pages 207?216, Hong Kong.
Fabio Zanzotto, Ioannis Korkontzelos, Francesca
Falucchi, and Suresh Manandhar. 2010. Estimat-
ing linear models for compositional distributional
semantics. In Proceedings of COLING, pages 1263?
1271, Beijing, China.
1526
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 1?8,
Dublin, Ireland, August 23-24, 2014.
SemEval-2014 Task 1: Evaluation of Compositional Distributional
Semantic Models on Full Sentences through Semantic Relatedness and
Textual Entailment
Marco Marelli
(1)
Luisa Bentivogli
(2)
Marco Baroni
(1)
Raffaella Bernardi
(1)
Stefano Menini
(1,2)
Roberto Zamparelli
(1)
(1)
University of Trento, Italy
(2)
FBK - Fondazione Bruno Kessler, Trento, Italy
{name.surname}@unitn.it, {bentivo,menini}@fbk.eu
Abstract
This paper presents the task on the evalu-
ation of Compositional Distributional Se-
mantics Models on full sentences orga-
nized for the first time within SemEval-
2014. Participation was open to systems
based on any approach. Systems were pre-
sented with pairs of sentences and were
evaluated on their ability to predict hu-
man judgments on (i) semantic relatedness
and (ii) entailment. The task attracted 21
teams, most of which participated in both
subtasks. We received 17 submissions in
the relatedness subtask (for a total of 66
runs) and 18 in the entailment subtask (65
runs).
1 Introduction
Distributional Semantic Models (DSMs) approx-
imate the meaning of words with vectors sum-
marizing their patterns of co-occurrence in cor-
pora. Recently, several compositional extensions
of DSMs (CDSMs) have been proposed, with the
purpose of representing the meaning of phrases
and sentences by composing the distributional rep-
resentations of the words they contain (Baroni and
Zamparelli, 2010; Grefenstette and Sadrzadeh,
2011; Mitchell and Lapata, 2010; Socher et al.,
2012). Despite the ever increasing interest in the
field, the development of adequate benchmarks for
CDSMs, especially at the sentence level, is still
lagging. Existing data sets, such as those intro-
duced by Mitchell and Lapata (2008) and Grefen-
stette and Sadrzadeh (2011), are limited to a few
hundred instances of very short sentences with a
fixed structure. In the last ten years, several large
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
data sets have been developed for various com-
putational semantics tasks, such as Semantic Text
Similarity (STS)(Agirre et al., 2012) or Recogniz-
ing Textual Entailment (RTE) (Dagan et al., 2006).
Working with such data sets, however, requires
dealing with issues, such as identifying multiword
expressions, recognizing named entities or access-
ing encyclopedic knowledge, which have little to
do with compositionality per se. CDSMs should
instead be evaluated on data that are challenging
for reasons due to semantic compositionality (e.g.
context-cued synonymy resolution and other lexi-
cal variation phenomena, active/passive and other
syntactic alternations, impact of negation at vari-
ous levels, operator scope, and other effects linked
to the functional lexicon). These issues do not oc-
cur frequently in, e.g., the STS and RTE data sets.
With these considerations in mind, we devel-
oped SICK (Sentences Involving Compositional
Knowledge), a data set aimed at filling the void,
including a large number of sentence pairs that
are rich in the lexical, syntactic and semantic phe-
nomena that CDSMs are expected to account for,
but do not require dealing with other aspects of
existing sentential data sets that are not within
the scope of compositional distributional seman-
tics. Moreover, we distinguished between generic
semantic knowledge about general concept cate-
gories (such as knowledge that a couple is formed
by a bride and a groom) and encyclopedic knowl-
edge about specific instances of concepts (e.g.,
knowing the fact that the current president of the
US is Barack Obama). The SICK data set contains
many examples of the former, but none of the lat-
ter.
2 The Task
The Task involved two subtasks. (i) Relatedness:
predicting the degree of semantic similarity be-
tween two sentences, and (ii) Entailment: detect-
ing the entailment relation holding between them
1
(see below for the exact definition). Sentence re-
latedness scores provide a direct way to evalu-
ate CDSMs, insofar as their outputs are able to
quantify the degree of semantic similarity between
sentences. On the other hand, starting from the
assumption that understanding a sentence means
knowing when it is true, being able to verify
whether an entailment is valid is a crucial chal-
lenge for semantic systems.
In the semantic relatedness subtask, given two
sentences, systems were required to produce a re-
latedness score (on a continuous scale) indicating
the extent to which the sentences were expressing
a related meaning. Table 1 shows examples of sen-
tence pairs with different degrees of semantic re-
latedness; gold relatedness scores are expressed on
a 5-point rating scale.
In the entailment subtask, given two sentences
A and B, systems had to determine whether the
meaning of B was entailed by A. In particular, sys-
tems were required to assign to each pair either
the ENTAILMENT label (when A entails B, viz.,
B cannot be false when A is true), the CONTRA-
DICTION label (when A contradicted B, viz. B is
false whenever A is true), or the NEUTRAL label
(when the truth of B could not be determined on
the basis of A). Table 2 shows examples of sen-
tence pairs holding different entailment relations.
Participants were invited to submit up to five
system runs for one or both subtasks. Developers
of CDSMs were especially encouraged to partic-
ipate, but developers of other systems that could
tackle sentence relatedness or entailment tasks
were also welcome. Besides being of intrinsic in-
terest, the latter systems? performance will serve
to situate CDSM performance within the broader
landscape of computational semantics.
3 The SICK Data Set
The SICK data set, consisting of about 10,000 En-
glish sentence pairs annotated for relatedness in
meaning and entailment, was used to evaluate the
systems participating in the task. The data set
creation methodology is outlined in the following
subsections, while all the details about data gen-
eration and annotation, quality control, and inter-
annotator agreement can be found in Marelli et al.
(2014).
3.1 Data Set Creation
SICK was built starting from two existing data
sets: the 8K ImageFlickr data set
1
and the
SemEval-2012 STS MSR-Video Descriptions data
set.
2
The 8K ImageFlickr dataset is a dataset of
images, where each image is associated with five
descriptions. To derive SICK sentence pairs we
randomly chose 750 images and we sampled two
descriptions from each of them. The SemEval-
2012 STS MSR-Video Descriptions data set is a
collection of sentence pairs sampled from the short
video snippets which compose the Microsoft Re-
search Video Description Corpus. A subset of 750
sentence pairs were randomly chosen from this
data set to be used in SICK.
In order to generate SICK data from the 1,500
sentence pairs taken from the source data sets, a 3-
step process was applied to each sentence compos-
ing the pair, namely (i) normalization, (ii) expan-
sion and (iii) pairing. Table 3 presents an example
of the output of each step in the process.
The normalization step was carried out on the
original sentences (S0) to exclude or simplify in-
stances that contained lexical, syntactic or seman-
tic phenomena (e.g., named entities, dates, num-
bers, multiword expressions) that CDSMs are cur-
rently not expected to account for.
The expansion step was applied to each of the
normalized sentences (S1) in order to create up to
three new sentences with specific characteristics
suitable to CDSM evaluation. In this step syntac-
tic and lexical transformations with predictable ef-
fects were applied to each normalized sentence, in
order to obtain (i) a sentence with a similar mean-
ing (S2), (ii) a sentence with a logically contradic-
tory or at least highly contrasting meaning (S3),
and (iii) a sentence that contains most of the same
lexical items, but has a different meaning (S4) (this
last step was carried out only where it could yield
a meaningful sentence; as a result, not all normal-
ized sentences have an (S4) expansion).
Finally, in the pairing step each normalized
sentence in the pair was combined with all the
sentences resulting from the expansion phase and
with the other normalized sentence in the pair.
Considering the example in Table 3, S1a and S1b
were paired. Then, S1a and S1b were each com-
bined with S2a, S2b,S3a, S3b, S4a, and S4b, lead-
1
http://nlp.cs.illinois.edu/HockenmaierGroup/data.html
2
http://www.cs.york.ac.uk/semeval-
2012/task6/index.php?id=data
2
Relatedness score Example
1.6
A: ?A man is jumping into an empty pool?
B: ?There is no biker jumping in the air?
2.9
A: ?Two children are lying in the snow and are making snow angels?
B: ?Two angels are making snow on the lying children?
3.6
A: ?The young boys are playing outdoors and the man is smiling nearby?
B: ?There is no boy playing outdoors and there is no man smiling?
4.9
A: ?A person in a black jacket is doing tricks on a motorbike?
B: ?A man in a black jacket is doing tricks on a motorbike?
Table 1: Examples of sentence pairs with their gold relatedness scores (on a 5-point rating scale).
Entailment label Example
ENTAILMENT
A: ?Two teams are competing in a football match?
B: ?Two groups of people are playing football?
CONTRADICTION
A: ?The brown horse is near a red barrel at the rodeo?
B: ?The brown horse is far from a red barrel at the rodeo?
NEUTRAL
A: ?A man in a black jacket is doing tricks on a motorbike?
B: ?A person is riding the bicycle on one wheel?
Table 2: Examples of sentence pairs with their gold entailment labels.
ing to a total of 13 different sentence pairs.
Furthermore, a number of pairs composed of
completely unrelated sentences were added to the
data set by randomly taking two sentences from
two different pairs.
The result is a set of about 10,000 new sen-
tence pairs, in which each sentence is contrasted
with either a (near) paraphrase, a contradictory or
strongly contrasting statement, another sentence
with very high lexical overlap but different mean-
ing, or a completely unrelated sentence. The ra-
tionale behind this approach was that of building
a data set which encouraged the use of a com-
positional semantics step in understanding when
two sentences have close meanings or entail each
other, hindering methods based on individual lex-
ical items, on the syntactic complexity of the two
sentences or on pure world knowledge.
3.2 Relatedness and Entailment Annotation
Each pair in the SICK dataset was annotated to
mark (i) the degree to which the two sentence
meanings are related (on a 5-point scale), and (ii)
whether one entails or contradicts the other (con-
sidering both directions). The ratings were col-
lected through a large crowdsourcing study, where
each pair was evaluated by 10 different subjects,
and the order of presentation of the sentences was
counterbalanced (i.e., 5 judgments were collected
for each presentation order). Swapping the order
of the sentences within each pair served a two-
fold purpose: (i) evaluating the entailment rela-
tion in both directions and (ii) controlling pos-
sible bias due to priming effects in the related-
ness task. Once all the annotations were collected,
the relatedness gold score was computed for each
pair as the average of the ten ratings assigned by
participants, whereas a majority vote scheme was
adopted for the entailment gold labels.
3.3 Data Set Statistics
For the purpose of the task, the data set was ran-
domly split into training and test set (50% and
50%), ensuring that each relatedness range and en-
tailment category was equally represented in both
sets. Table 4 shows the distribution of sentence
pairs considering the combination of relatedness
ranges and entailment labels. The ?total? column
3
Original pair
S0a: A sea turtle is hunting for fish S0b: The turtle followed the fish
Normalized pair
S1a: A sea turtle is hunting for fish S1b: The turtle is following the fish
Expanded pairs
S2a: A sea turtle is hunting for food S2b: The turtle is following the red fish
S3a: A sea turtle is not hunting for fish S3b: The turtle isn?t following the fish
S4a: A fish is hunting for a turtle in the sea S4b: The fish is following the turtle
Table 3: Data set creation process.
indicates the total number of pairs in each range
of relatedness, while the ?total? row contains the
total number of pairs in each entailment class.
SICK Training Set
relatedness CONTRADICT ENTAIL NEUTRAL TOTAL
1-2 range 0 (0%) 0 (0%) 471 (10%) 471
2-3 range 59 (1%) 2 (0%) 638 (13%) 699
3-4 range 498 (10%) 71 (1%) 1344 (27%) 1913
4-5 range 155 (3%) 1344 (27%) 352 (7%) 1851
TOTAL 712 1417 2805 4934
SICK Test Set
relatedness CONTRADICT ENTAIL NEUTRAL TOTAL
1-2 range 0 (0%) 1 (0%) 451 (9%) 452
2-3 range 59 (1%) 0 (0%) 615(13%) 674
3-4 range 496 (10%) 65 (1%) 1398 (28%) 1959
4-5 range 157 (3%) 1338 (27%) 326 (7%) 1821
TOTAL 712 1404 2790 4906
Table 4: Distribution of sentence pairs across the
Training and Test Sets.
4 Evaluation Metrics and Baselines
Both subtasks were evaluated using standard met-
rics. In particular, the results on entailment were
evaluated using accuracy, whereas the outputs on
relatedness were evaluated using Pearson correla-
tion, Spearman correlation, and Mean Squared Er-
ror (MSE). Pearson correlation was chosen as the
official measure to rank the participating systems.
Table 5 presents the performance of 4 base-
lines. The Majority baseline always assigns
the most common label in the training data
(NEUTRAL), whereas the Probability baseline
assigns labels randomly according to their rela-
tive frequency in the training set. The Overlap
baseline measures word overlap, again with
parameters (number of stop words and EN-
TAILMENT/NEUTRAL/CONTRADICTION
thresholds) estimated on the training part of the
data.
Baseline Relatedness Entailment
Chance 0 33.3%
Majority NA 56.7%
Probability NA 41.8%
Overlap 0.63 56.2%
Table 5: Performance of baselines. Figure of merit
is Pearson correlation for relatedness and accuracy
for entailment. NA = Not Applicable
5 Submitted Runs and Results
Overall, 21 teams participated in the task. Partici-
pants were allowed to submit up to 5 runs for each
subtask and had to choose the primary run to be in-
cluded in the comparative evaluation. We received
17 submissions to the relatedness subtask (for a
total of 66 runs) and 18 for the entailment subtask
(65 runs).
We asked participants to pre-specify a pri-
mary run to encourage commitment to a
theoretically-motivated approach, rather than
post-hoc performance-based assessment. Inter-
estingly, some participants used the non-primary
runs to explore the performance one could reach
by exploiting weaknesses in the data that are not
likely to hold in future tasks of the same kind
(for instance, run 3 submitted by The Meaning
Factory exploited sentence ID ordering informa-
tion, but it was not presented as a primary run).
Participants could also use non-primary runs to
test smart baselines. In the relatedness subtask
six non-primary runs slightly outperformed the
official winning primary entry,
3
while in the
entailment task all ECNU?s runs but run 4 were
better than ECNU?s primary run. Interestingly,
the differences between the ECNU?s runs were
3
They were: The Meaning Factory?s run3 (Pearson
0.84170) ECNU?s runs2 (0.83893) run5 (0.83500) and Stan-
fordNLP?s run4 (0.83462) and run2 (0.83103).
4
due to the learning methods used.
We present the results achieved by primary runs
against the Entailment and Relatedness subtasks in
Table 6 and Table 7, respectively.
4
We witnessed
a very close finish in both subtasks, with 4 more
systems within 3 percentage points of the winner
in both cases. 4 of these 5 top systems were the
same across the two subtasks. Most systems per-
formed well above the best baselines from Table
5.
The overall performance pattern suggests that,
owing perhaps to the more controlled nature of
the sentences, as well as to the purely linguistic
nature of the challenges it presents, SICK entail-
ment is ?easier? than RTE. Considering the first
five RTE challenges (Bentivogli et al., 2009), the
median values ranged from 56.20% to 61.75%,
whereas the average values ranged from 56.45%
to 61.97%. The entailment scores obtained on
the SICK data set are considerably higher, being
77.06% for the median system and 75.36% for
the average system. On the other hand, the re-
latedness task is more challenging than the one
run on MSRvid (one of our data sources) at STS
2012, where the top Pearson correlation was 0.88
(Agirre et al., 2012).
6 Approaches
A summary of the approaches used by the sys-
tems to address the task is presented in Table 8.
In the table, systems in bold are those for which
the authors submitted a paper (Ferrone and Zan-
zotto, 2014; Bjerva et al., 2014; Beltagy et al.,
2014; Lai and Hockenmaier, 2014; Alves et al.,
2014; Le?on et al., 2014; Bestgen, 2014; Zhao et
al., 2014; Vo et al., 2014; Bic?ici and Way, 2014;
Lien and Kouylekov, 2014; Jimenez et al., 2014;
Proisl and Evert, 2014; Gupta et al., 2014). For the
others, we used the brief description sent with the
system?s results, double-checking the information
with the authors. In the table, ?E? and ?R? refer
to the entailment and relatedness task respectively,
and ?B? to both.
Almost all systems combine several kinds of
features. To highlight the role played by com-
position, we draw a distinction between compo-
sitional and non-compositional features, and di-
vide the former into ?fully compositional? (sys-
4
ITTK?s primary run could not be evaluated due to tech-
nical problems with the submission. The best ITTK?s non-
primary run scored 78,2% accuracy in the entailment task and
0.76 r in the relatedness task.
ID Compose ACCURACY
Illinois-LH run1 P/S 84.6
ECNU run1 S 83.6
UNAL-NLP run1 83.1
SemantiKLUE run1 82.3
The Meaning Factory run1 S 81.6
CECL ALL run1 80.0
BUAP run1 P 79.7
UoW run1 78.5
Uedinburgh run1 S 77.1
UIO-Lien run1 77.0
FBK-TR run3 P 75.4
StanfordNLP run5 S 74.5
UTexas run1 P/S 73.2
Yamraj run1 70.7
asjai run5 S 69.8
haLF run2 S 69.4
RTM-DCU run1 67.2
UANLPCourse run2 S 48.7
Table 6: Primary run results for the entailment
subtask. The table also shows whether a sys-
tem exploits composition information at either the
phrase (P) or sentence (S) level.
tems that compositionally computed the meaning
of the full sentences, though not necessarily by as-
signing meanings to intermediate syntactic con-
stituents) and ?partially compositional? (systems
that stop the composition at the level of phrases).
As the table shows, thirteen systems used compo-
sition in at least one of the tasks; ten used compo-
sition for full sentences and six for phrases, only.
The best systems are among these thirteen sys-
tems.
Let us focus on such compositional methods.
Concerning the relatedness task, the fine-grained
analyses reported for several systems (Illinois-
LH, The Meaning Factory and ECNU) shows that
purely compositional systems currently reach per-
formance above 0.7 r. In particular, ECNU?s
compositional feature gives 0.75 r, The Meaning
Factory?s logic-based composition model 0.73 r,
and Illinois-LH compositional features combined
with Word Overlap 0.75 r. While competitive,
these scores are lower than the one of the best
5
ID Compose r ? MSE
ECNU run1 S 0.828 0.769 0.325
StanfordNLP run5 S 0.827 0.756 0.323
The Meaning Factory run1 S 0.827 0.772 0.322
UNAL-NLP run1 0.804 0.746 0.359
Illinois-LH run1 P/S 0.799 0.754 0.369
CECL ALL run1 0.780 0.732 0.398
SemantiKLUE run1 0.780 0.736 0.403
RTM-DCU run1 0.764 0.688 0.429
UTexas run1 P/S 0.714 0.674 0.499
UoW run1 0.711 0.679 0.511
FBK-TR run3 P 0.709 0.644 0.591
BUAP run1 P 0.697 0.645 0.528
UANLPCourse run2 S 0.693 0.603 0.542
UQeResearch run1 0.642 0.626 0.822
ASAP run1 P 0.628 0.597 0.662
Yamraj run1 0.535 0.536 2.665
asjai run5 S 0.479 0.461 1.104
Table 7: Primary run results for the relatedness
subtask (r for Pearson and ? for Spearman corre-
lation). The table also shows whether a system ex-
ploits composition information at either the phrase
(P) or sentence (S) level.
purely non-compositional system (UNAL-NLP)
which reaches the 4th position (0.80 r UNAL-NLP
vs. 0.82 r obtained by the best system). UNAL-
NLP however exploits an ad-hoc ?negation? fea-
ture discussed below.
In the entailment task, the best non-
compositional model (again UNAL-NLP)
reaches the 3rd position, within close reach of the
best system (83% UNAL-NLP vs. 84.5% obtained
by the best system). Again, purely compositional
models have lower performance. haLF CDSM
reaches 69.42% accuracy, Illinois-LH Word
Overlap combined with a compositional feature
reaches 71.8%. The fine-grained analysis reported
by Illinois-LH (Lai and Hockenmaier, 2014)
shows that a full compositional system (based
on point-wise multiplication) fails to capture
contradiction. It is better than partial phrase-based
compositional models in recognizing entailment
pairs, but worse than them on recognizing neutral
pairs.
Given our more general interest in the distri-
butional approaches, in Table 8 we also classify
the different DSMs used as ?Vector Space Mod-
els?, ?Topic Models? and ?Neural Language Mod-
els?. Due to the impact shown by learning methods
(see ECNU?s results), we also report the different
learning approaches used.
Several participating systems deliberately ex-
ploit ad-hoc features that, while not helping a true
understanding of sentence meaning, exploit some
systematic characteristics of SICK that should be
controlled for in future releases of the data set.
In particular, the Textual Entailment subtask has
been shown to rely too much on negative words
and antonyms. The Illinois-LH team reports that,
just by checking the presence of negative words
(the Negation Feature in the table), one can detect
86.4% of the contradiction pairs, and by combin-
ing Word Overlap and antonyms one can detect
83.6% of neutral pairs and 82.6% of entailment
pairs. This approach, however, is obviously very
brittle (it would not have been successful, for in-
stance, if negation had been optionally combined
with word-rearranging in the creation of S4 sen-
tences, see Section 3.1 above).
Finally, Table 8 reports about the use of external
resources in the task. One of the reasons we cre-
ated SICK was to have a compositional semantics
benchmark that would not require too many ex-
ternal tools and resources (e.g., named-entity rec-
ognizers, gazetteers, ontologies). By looking at
what the participants chose to use, we think we
succeeded, as only standard NLP pre-processing
tools (tokenizers, PoS taggers and parsers) and rel-
atively few knowledge resources (mostly, Word-
Net and paraphrase corpora) were used.
7 Conclusion
We presented the results of the first task on the
evaluation of compositional distributional seman-
tic models and other semantic systems on full sen-
tences, organized within SemEval-2014. Two sub-
tasks were offered: (i) predicting the degree of re-
latedness between two sentences, and (ii) detect-
ing the entailment relation holding between them.
The task has raised noticeable attention in the
community: 17 and 18 submissions for the relat-
edness and entailment subtasks, respectively, for a
total of 21 participating teams. Participation was
not limited to compositional models but the major-
ity of systems (13/21) used composition in at least
one of the subtasks. Moreover, the top-ranking
systems in both tasks use compositional features.
However, it must be noted that all systems also ex-
6
Participant ID Non composition features Comp features Learning Methods External Resources 
Ve
cto
r S
em
an
tic
s M
od
el 
To
pic
 M
od
el 
Ne
ura
l L
an
gu
ag
e M
od
el 
De
no
tat
ion
al 
Mo
de
l 
Wo
rd 
Ov
erl
ap
 
Wo
rd 
Sim
ila
rit
y 
Sy
nta
cti
c F
ea
tur
es
 
Se
nte
nc
e d
iffe
ren
ce
 
Ne
ga
tio
n F
ea
tur
es
 
Se
nte
nc
e C
om
po
sit
ion
 
Ph
ras
e c
om
po
sit
ion
  
SV
M 
an
d K
ern
el 
me
tho
ds
 
K-
Ne
are
st 
Ne
igh
bo
urs
 
Cla
ssi
fie
r C
om
bin
ati
on
 
Ra
nd
om
 Fo
res
t 
Fo
L/P
rob
ab
ilis
tic
 Fo
L 
Cu
rri
cu
lum
 ba
se
d l
ea
rni
ng
 
Ot
he
r 
Wo
rdN
et 
Pa
rap
hra
se
s D
B 
Ot
he
r C
orp
ora
 
Im
ag
eF
lick
er 
 ST
S M
SR
-V
ide
o 
De
scr
ipt
ion
 
ASAP R R R R R R R R R 
ASJAI B B B B B B B B E B R B 
BUAP B B B B E B E B 
UEdinburgh B B B B B E R B 
CECL B B B B B B 
ECNU B B B B B B B B B B B B B 
FBK-TR R R R E B E E B R E R R E 
haLF E E E E 
IITK B B B B B B B B B 
Illinois-LH B B B B B B B B B B B B 
RTM-DCU B B B B B 
SemantiKLUE B B B B B B B B 
StandfordNLP B B R R R B E 
The Meaning Factory R R R R R R B E R E B B R 
UANLPCourse B B B B B 
UIO-Lien E E 
UNAL-NLP B B B B R B B 
UoW B B B B B B 
UQeRsearch R R R R R R R 
UTexas B B B B B B B 
Yamarj B B B B 
Table 8: Summary of the main characteristics of the participating systems on R(elatedness), E(ntailment)
or B(oth)
ploit non-compositional features and most of them
use external resources, especially WordNet. Al-
most all the participating systems outperformed
the proposed baselines in both tasks. Further anal-
yses carried out by some participants in the task
show that purely compositional approaches reach
accuracy above 70% in entailment and 0.70 r for
relatedness. These scores are comparable with the
average results obtained in the task.
Acknowledgments
We thank the creators of the ImageFlickr, MSR-
Video, and SemEval-2012 STS data sets for grant-
ing us permission to use their data for the task. The
University of Trento authors were supported by
ERC 2011 Starting Independent Research Grant
n. 283554 (COMPOSES).
References
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 task 6: A pi-
lot on semantic textual similarity. In Proceedings of
the Sixth International Workshop on Semantic Eval-
uation (SemEval 2012), volume 2.
Ana O. Alves, Adirana Ferrugento, Mariana Lorenc?o,
and Filipe Rodrigues. 2014. ASAP: Automatica se-
mantic alignment for phrases. In Proceedings of Se-
mEval 2014: International Workshop on Semantic
Evaluation.
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of EMNLP, pages 1183?1193, Boston,
MA.
Islam Beltagy, Stephen Roller, Gemma Boleda, Katrin
Erk, and Raymon J. Mooney. 2014. UTexas: Nat-
ural language semantics using distributional seman-
tics and probablisitc logic. In Proceedings of Se-
mEval 2014: International Workshop on Semantic
Evaluation.
7
Luisa Bentivogli, Ido Dagan, Hoa T. Dang, Danilo Gi-
ampiccolo, and Bernardo Magnini. 2009. The fifth
PASCAL recognizing textual entailment challenge.
In The Text Analysis Conference (TAC 2009).
Yves Bestgen. 2014. CECL: a new baseline and a non-
compositional approach for the Sick benchmark. In
Proceedings of SemEval 2014: International Work-
shop on Semantic Evaluation.
Ergun Bic?ici and Andy Way. 2014. RTM-DCU: Ref-
erential translation machines for semantic similar-
ity. In Proceedings of SemEval 2014: International
Workshop on Semantic Evaluation.
Johannes Bjerva, Johan Bos, Rob van der Goot, and
Malvina Nissim. 2014. The Meaning Factory: For-
mal Semantics for Recognizing Textual Entailment
and Determining Semantic Similarity. In Proceed-
ings of SemEval 2014: International Workshop on
Semantic Evaluation.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The PASCAL recognising textual entailment
challenge. In Machine learning challenges. Evalu-
ating predictive uncertainty, visual object classifica-
tion, and recognising textual entailment, pages 177?
190. Springer.
Lorenzo Ferrone and Fabio Massimo Zanzotto. 2014.
haLF:comparing a pure CDSM approach and a stan-
dard ML system for RTE. In Proceedings of Se-
mEval 2014: International Workshop on Semantic
Evaluation.
Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011.
Experimental support for a categorical composi-
tional distributional model of meaning. In Proceed-
ings of EMNLP, pages 1394?1404, Edinburgh, UK.
Rohit Gupta, Ismail El Maarouf Hannah Bechara, and
Costantin Oras?an. 2014. UoW: NLP techniques de-
veloped at the University of Wolverhampton for Se-
mantic Similarity and Textual Entailment. In Pro-
ceedings of SemEval 2014: International Workshop
on Semantic Evaluation.
Sergio Jimenez, George Duenas, Julia Baquero, and
Alexander Gelbukh. 2014. UNAL-NLP: Combin-
ing soft cardinality features for semantic textual sim-
ilarity, relatedness and entailment. In Proceedings
of SemEval 2014: International Workshop on Se-
mantic Evaluation.
Alice Lai and Julia Hockenmaier. 2014. Illinois-lh: A
denotational and distributional approach to seman-
tics. In Proceedings of SemEval 2014: International
Workshop on Semantic Evaluation.
Sa?ul Le?on, Darnes Vilarino, David Pinto, Mireya To-
var, and Beatrice Beltr?an. 2014. BUAP:evaluating
compositional distributional semantic models on full
sentences through semantic relatedness and textual
entailment. In Proceedings of SemEval 2014: Inter-
national Workshop on Semantic Evaluation.
Elisabeth Lien and Milen Kouylekov. 2014. UIO-
Lien: Entailment recognition using minimal recur-
sion semantics. In Proceedings of SemEval 2014:
International Workshop on Semantic Evaluation.
Marco Marelli, Stefano Menini, Marco Baroni, Luisa
Bentivogli, Raffaella Bernardi, and Roberto Zam-
parelli. 2014. A SICK cure for the evaluation of
compositional distributional semantic models. In
Proceedings of LREC, Reykjavik.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL, pages 236?244, Columbus, OH.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388?1429.
Thomas Proisl and Stefan Evert. 2014. SemantiK-
LUE: Robust semantic similarity at multiple levels
using maximum weight matching. In Proceedings of
SemEval 2014: International Workshop on Semantic
Evaluation.
Richard Socher, Brody Huval, Christopher Manning,
and Andrew Ng. 2012. Semantic compositionality
through recursive matrix-vector spaces. In Proceed-
ings of EMNLP, pages 1201?1211, Jeju Island, Ko-
rea.
An N. P. Vo, Octavian Popescu, and Tommaso Caselli.
2014. FBK-TR: SVM for Semantic Relatedness and
Corpus Patterns for RTE. In Proceedings of Se-
mEval 2014: International Workshop on Semantic
Evaluation.
Jiang Zhao, Tian Tian Zhu, and Man Lan. 2014.
ECNU: One Stone Two Birds: Ensemble of Het-
erogenous Measures for Semantic Relatedness and
Textual Entailment. In Proceedings of SemEval
2014: International Workshop on Semantic Evalu-
ation.
8
Proceedings of the Workshop on Distributional Semantics and Compositionality (DiSCo?2011), pages 1?9,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
(Linear) Maps of the Impossible:
Capturing semantic anomalies in distributional space
Eva Maria Vecchi and Marco Baroni and Roberto Zamparelli
Center for Mind/Brain Sciences, University of Trento
Rovereto (TN), Italy
{evamaria.vecchi-1,marco.baroni,roberto.zamparelli}@unitn.it
Abstract
In this paper, we present a first attempt to
characterize the semantic deviance of com-
posite expressions in distributional seman-
tics. Specifically, we look for properties of
adjective-noun combinations within a vector-
based semantic space that might cue their lack
of meaning. We evaluate four different com-
positionality models shown to have various
levels of success in representing the mean-
ing of AN pairs: the simple additive and
multiplicative models of Mitchell and Lap-
ata (2008), and the linear-map-based models
of Guevara (2010) and Baroni and Zamparelli
(2010). For each model, we generate com-
posite vectors for a set of AN combinations
unattested in the source corpus and which
have been deemed either acceptable or seman-
tically deviant. We then compute measures
that might cue semantic anomaly, and com-
pare each model?s results for the two classes of
ANs. Our study shows that simple, unsuper-
vised cues can indeed significantly tell unat-
tested but acceptable ANs apart from impos-
sible, or deviant, ANs, and that the simple ad-
ditive and multiplicative models are the most
effective in this task.
1 Introduction
Statistical approaches to describe, represent and un-
derstand natural language have been criticized as
failing to account for linguistic ?creativity?, a prop-
erty which has been accredited to the compositional
nature of natural language. Specifically, criticisms
against statistical methods were based on the ar-
gument that a corpus cannot significantly sample a
natural language because natural language is infi-
nite (Chomsky, 1957). This cricticism also applies
to distributional semantic models that build seman-
tic representations of words or phrases in terms of
vectors recording their distributional co-occurrence
patterns in a corpus (Turney and Pantel, 2010), but
have no obvious way to generalize to word combi-
nations that have not been observed in the corpus.
To address this problem, there have been several re-
cent attempts to incorporate into distributional se-
mantic models a component that generates vectors
for unseen linguistic structures by compositional op-
erations in the vector space (Baroni and Zamparelli,
2010; Guevara, 2010; Mitchell and Lapata, 2010).
The ability to work with unattested data leads to
the question of why a linguistic expression might
not be attested in even an extremely large and well-
balanced corpus. Its absence might be motivated
by a number of factors: pure chance, the fact that
the expression is ungrammatical, uses a rare struc-
ture, describes false facts, or, finally, is nonsensi-
cal. One criticism from generative linguists is pre-
cisely that statistical methods could not distinguish
between these various possibilities.
The difficulty of solving this problem can be il-
lustrated by the difference in semantics between the
adjective-noun pairs in (1a) and (1b):
(1) a. blue rose
b. residential steak
Although it may be the case that you have never ac-
1
tually seen a blue rose, the concept is not inconceiv-
able. On the other hand, the concept of a residen-
tial steak is rather unimaginable, and intuitively its
absence in a corpus is motivated by more than just
chance or data sparseness.
The present paper is a first attempt to use com-
positionality and distributional measures to distin-
guish nonsensical, or semantically deviant, linguis-
tic expression from other types of unattested struc-
tures. The task of distinguishing between unattested
but acceptable and unattested but semantically de-
viant linguistic expressions is not only a way to ad-
dress the criticism about the meaning of ?unattest-
edness?, but also a task that could have a large im-
pact on the (computational) linguistic community as
a whole (see Section 2.1).
Our specific goal is to automatically detect se-
mantic deviance in attributive Adjective-Noun (AN)
expressions, using a small number of simple cues in
the vectorial representation of an AN as it is gener-
ated from the distributional vectors of its component
A and N by four compositional models found in the
literature. The choice of AN as our testbed is moti-
vated by two facts: first of all, ANs are common,
small constituents containing no functional mate-
rial, and secondly, ANs have already been studied in
compositional distributional semantics (Baroni and
Zamparelli, 2010; Guevara, 2010; Mitchell and La-
pata, 2010).
It is important to note that in this research we talk
about ?semantically deviant? expressions, but we do
not exclude the possibility that such expressions are
interpreted as metaphors, via a chain of associations.
In fact, distributional measures are desirable models
to account for this, since they naturally lead to a gra-
dient notion of semantic anomaly.
The rest of this paper is structured as follows.
Section 2 discusses relevant earlier work, introduc-
ing the literature on semantic deviance as well as
compositional methods in distributional semantics.
Section 3 presents some hypotheses about cues of
semantic deviance in distributional space. Our ex-
perimental setup and procedure are detailed in Sec-
tion 4, whereas the experiments? results are pre-
sented and analyzed in Section 5. We conclude by
summarizing and proposing future directions in Sec-
tion 6.
2 Related work
2.1 Semantic deviance
As far as we know, we are the first to try to model
semantic deviance using distributional methods, but
the issue of when a complex linguistic expression is
semantically deviant has been addressed since the
1950?s in various areas of linguistics. In compu-
tational linguistics, the possibility of detecting se-
mantic deviance has been seen as a prerequisite to
access metaphorical/non-literal semantic interpreta-
tions (Fass and Wilks, 1983; Zhou et al, 2007). In
psycholinguistics, it has been part of a wide debate
on the point at which context can make us perceive a
?literal? vs. a ?figurative? meaning (Giora, 2002). In
theoretical generative linguistics, the issue was orig-
inally part of a discussion on the boundaries between
syntax and semantics. Cases like Chomsky?s clas-
sic ?Colorless green ideas sleep furiously? can actu-
ally be regarded as violations of very fine-grained
syntactic selectional restrictions on the arguments
of verbs or modifiers, on the model of *much com-
puter (arguably a failure of much to combine with a
noun +COUNT). By 1977, even Chomsky doubted
that speakers could in general have intuitions about
whether ill-formedness was syntactic or semantic
(Chomsky, 1977, p. 4). The spirit of the selectional
approach persists in Asher (2011), who proposes a
detailed system of semantic types plus a theory of
type coercion, designed to account for the shift in
meaning seen in, e.g., (2) (lunch as food or as an
event).
(2) Lunch was delicious but took forever.
A practical problem with this approach is that a
full handmade specification of the features that de-
termine semantic compatibility is a very expensive
and time-consuming enterprise, and it should be
done consistently across the whole content lexicon.
Moreover, it is unclear how to model the intuition
that naval fraction, musical North or institutional
acid sound odd, in the absence of very particular
contexts, while (2) sounds quite natural. Whatever
the nature of coercion, we do not want it to run so
smoothly that any combination of A and N (or V and
its arguments) becomes meaningful and completely
acceptable.
2
2.2 Distributional approaches to meaning
composition
Although the issue of how to compose meaning has
attracted interest since the early days of distribu-
tional semantics (Landauer and Dumais, 1997), re-
cently a very general framework for modeling com-
positionality has been proposed by Mitchell and La-
pata (Mitchell and Lapata, 2008; Mitchell and La-
pata, 2009; Mitchell and Lapata, 2010). Given two
vectors u and v, they identify two general classes of
composition models, (linear) additive models:
p = Au + Bv (1)
where A and B are weight matrices, and multiplica-
tive models:
p = Cuv
where C is a weight tensor projecting the uv ten-
sor product onto the space of p. Mitchell and La-
pata derive two simplified models from these gen-
eral forms: The simplified additive model given by
p = ?u + ?v, and a simplified multiplicative ap-
proach that reduces to component-wise multiplica-
tion, where the i-th component of the composed vec-
tor is given by: pi = uivi. Mitchell and Lapata
evaluate the simplified models on a wide range of
tasks ranging from paraphrasing to statistical lan-
guage modeling to predicting similarity intuitions.
Both simple models fare quite well across tasks
and alternative semantic representations, also when
compared to more complex methods derived from
the equations above. Given their overall simplic-
ity, good performance and the fact that they have
also been extensively tested in other studies (Baroni
and Zamparelli, 2010; Erk and Pado?, 2008; Guevara,
2010; Kintsch, 2001; Landauer and Dumais, 1997),
we re-implement here both the simplified additive
and simplified multiplicative methods (we do not,
however, attempt to tune the weights of the additive
model, although we do apply a scalar normalization
constant to the adjective and noun vectors).
Mitchell and Lapata (as well as earlier re-
searchers) do not exploit corpus evidence about
the p vectors that result from composition, despite
the fact that it is straightforward (at least for short
constructions) to extract direct distributional evi-
dence about the composite items from the corpus
(just collect co-occurrence information for the com-
posite item from windows around the contexts in
which it occurs). The main innovation of Guevara
(2010), who focuses on adjective-noun combina-
tions (AN), is to use the co-occurrence vectors of
corpus-observed ANs to train a supervised compo-
sition model. Guevara, whose approach we also re-
implement here, adopts the full additive composi-
tion form from Equation (1) and he estimates the
A and B weights (concatenated into a single ma-
trix, that acts as a linear map from the space of con-
catenated adjective and noun vectors onto the AN
vector space) using partial least squares regression.
The training data are pairs of adjective-noun vec-
tor concatenations, as input, and corpus-derived AN
vectors, as output. Guevara compares his model
to the simplified additive and multiplicative models
of Mitchell and Lapata. Corpus-observed ANs are
nearer, in the space of observed and predicted test
set ANs, to the ANs generated by his model than
to those from the alternative approaches. The addi-
tive model, on the other hand, is best in terms of
shared neighbor count between observed and pre-
dicted ANs.
The final approach we re-implement is the one
proposed by Baroni and Zamparelli (2010), who
treat attributive adjectives as functions from noun
meanings to noun meanings. This is a standard ap-
proach in Montague semantics (Thomason, 1974),
except noun meanings here are distributional vec-
tors, not denotations, and adjectives are (linear)
functions learned from a large corpus. Unlike in
Guevara?s approach, a separate matrix is generated
for each adjective using only examples of ANs con-
taining that adjective, and no adjective vector is
used: the adjective is represented entirely by the ma-
trix mapping nouns to ANs. In terms of Mitchell
and Lapata?s general framework, this approach de-
rives from the additive form in Equation (1) with the
matrix multiplying the adjective vector (say, A) set
to 0, the other matrix (B) representing the adjective
at hand, and v a noun vector. Baroni and Zamparelli
(2010) show that their model significantly outper-
forms other vector composition methods, including
addition, multiplication and Guevara?s approach, in
the task of approximating the correct vectors for pre-
viously unseen (but corpus-attested) ANs. Simple
addition emerges as the second best model.
3
See Section 4.3 below for details on our re-
implementations. Note that they follow very closely
the procedure of Baroni and Zamparelli (2010), in-
cluding choices of source corpus and parameter val-
ues, so that we expect their results on the quality of
the various models in predicting ANs to also hold
for our re-implementations.
3 Simple indices of semantic deviance
We consider here a few simple, unsupervised mea-
sures to help us distinguish the representation that a
distributional composition model generates for a se-
mantically anomalous AN from the one it generates
for a semantically acceptable AN. In both cases, we
assume that the AN is not already part of the model
semantic space, just like you can distinguish be-
tween parliamentary tomato (odd) and marble iPad
(OK), although you probably never heard either ex-
pression.
We hypothesize that, since the values in the di-
mensions of a semantic space are a distributional
proxy to the meaning of an expression, a mean-
ingless expression should in general have low val-
ues across the semantic space dimensions. For ex-
ample, a parliamentary tomato, no longer being a
vegetable but being an unlikely parliamentary event,
might have low values on both dimensions char-
acterizing vegetables and dimensions characterizing
events. Thus, our first simple measure of seman-
tic anomaly is the length of the model-generated
AN. We hypothesize that anomalous AN vectors are
shorter than acceptable ANs.
Second, if deviant composition destroys or ran-
domizes the meaning of a noun, as a side effect we
might expect the resulting AN to be more distant, in
the semantic space, from the component noun. Al-
though even a marble iPad might have lost some es-
sential properties of iPads (it could for example be
an iPad statue you cannot use as a tablet), to the ex-
tent that we can make sense of it, it must retain at
least some characteristics of iPads (at the very least,
it will be shaped like an iPad). On the other hand, we
cannot imagine what a parliamentary tomato should
be, and thus cannot attribute even a subset of the reg-
ular tomato properties to it. We thus hypothesize that
model-generated vectors of deviant ANs will form
a wider angle (equivalently, will have a lower co-
sine) with the corresponding N vectors than accept-
able ANs.
Finally, if an AN makes no sense, its model-
generated vector should not have many neighbours
in the semantic space, since our semantic space is
populated by nouns, adjectives and ANs that are
commonly encountered in the corpus, and should
thus be meaningful. We expect deviant ANs to
be ?semantically isolated?, a notion that we opera-
tionalize in terms of a (neighborhood) density mea-
sure, namely the average cosine with the (top 10)
nearest neighbours. We hypothesize that model-
generated vectors of deviant ANs will have lower
density than model-generated acceptable ANs.
4 Experimental setup
4.1 Semantic space
Our initial step was to construct a semantic space for
our experiments, consisting of a matrix where each
row vector represents an adjective, noun or AN. We
first introduce the source corpus, then the vocabulary
of words and ANs that we represent in the space,
and finally the procedure adopted to build the vec-
tors representing the vocabulary items from corpus
statistics, in order to obtain the semantic space ma-
trix. We work here with a ?vanilla? semantic space
(essentially, we follow the steps of Baroni and Zam-
parelli (2010)), since our focus is on the effect of
different composition methods given a common se-
mantic space. We leave it to further work to study
how choices in semantic space construction affect
composition operations.
4.1.1 Source corpus
We use as our source corpus the concate-
nation of the Web-derived ukWaC corpus
(http://wacky.sslmit.unibo.it/),
a mid-2009 dump of the English Wikipedia
(http://en.wikipedia.org) and the British
National Corpus (http://www.natcorp.
ox.ac.uk/). The corpus has been tokenized,
POS-tagged and lemmatized with the TreeTagger
(Schmid, 1995), and it contains about 2.8 billion
tokens. We extract all statistics at the lemma level,
ignoring inflectional information.
4
4.1.2 Semantic space vocabulary
The words/ANs in the semantic space must of
course include the items that we need for our exper-
iments (adjectives, nouns and ANs used for model
training and as input to composition). Moreover, in
order to study the behaviour of the test items we are
interested in (that is, model-generated AN vectors)
within a large and less ad-hoc space, we also include
many more adjectives, nouns and ANs in our vocab-
ulary not directly relevant to our experimental ma-
nipulations.
We populate our semantic space with the 8K most
frequent nouns and 4K most frequent adjectives
from the corpus (excluding, in both cases, the top
50 most frequent elements). We extended this vo-
cabulary to include two sets of ANs (33K ANs cu-
mulatively), for a total of 45K vocabulary items in
the semantic space.
To create the ANs needed to run and evaluate the
experiments described below, we focused on a set
of adjectives which are very frequent in the corpus
so that they will be in general able to combine with
wide classes of nouns, making the unattested cases
more interesting, but not so frequent as to have such
a general meaning that would permit a free combi-
nation with nearly any noun. The ANs were there-
fore generated by crossing a selected set of 200 very
frequent adjectives (adjectives attested in the corpus
at least 47K times, and at most 740K) and the set
of the 8K nouns in our semantic space vocabulary,
producing a set of 4.92M generated ANs.
The first set of ANs included in the semantic
space vocabulary is a randomly sampled set of 30K
ANs from the generated set which are attested in
the corpus at least 200 times (to avoid noise and fo-
cus on ANs for which we can extract reasonably ro-
bust distributional data). We also extracted any unat-
tested ANs from the set of generated set (about 3.5M
unattested ANs), putting them aside to later assem-
ble our evaluation material, described in Section 4.2.
To add further variety to the semantic space, we
included a less controlled second set of 3K ANs ran-
domly picked among those that are attested and are
formed by the combination of any of the 4K adjec-
tives and 8K nouns in the vocabulary.
4.1.3 Semantic space construction
For each of the items in our vocabulary, we first
build 10K-dimensional vectors by recording their
sentence-internal co-occurrence with the top 10K
most frequent content words (nouns, adjectives or
verbs) in the corpus. The raw co-occurrence counts
are then transformed into Local Mutual Information
scores (Local Mutual Information is an association
measure that closely approximates the commonly
used Log-Likelihood Ratio while being simpler to
compute (Baroni and Lenci, 2010; Evert, 2005)).
Next, we reduce the full co-occurrence matrix
applying the Singular Value Decomposition (SVD)
operation, like in LSA and related distributional
semantic methods (Landauer and Dumais, 1997;
Rapp, 2003; Schu?tze, 1997). The original 45K-by-
10K-dimensional matrix is reduced in this way to a
45K-by-300 matrix, where vocabulary items are rep-
resented by their coordinates in the space spanned
by the first 300 right singular vectors of the SVD
solution. This step is motivated by the fact that we
will estimate linear models to predict the values of
each dimension of an AN from the dimensions of the
components. We thus prefer to work in a smaller and
denser space. As a sanity check, we verify that we
obtain state-of-the-art-range results on various se-
mantic tasks using this reduced semantic space (not
reported here for space reason).
4.2 Evaluation materials
Our goal is to study what happens when composi-
tional methods are used to construct a distributional
representation for ANs that are semantically deviant,
compared to the AN representations they generate
for ANs they have not encountered before, but that
are semantically acceptable.
In order to assemble these lists, we started from
the set of 3.5M unattested ANs described in Sec-
tion 4.1.2 above, focusing on 30 randomly chosen
adjectives. For each of these, we randomly picked
100 ANs for manual inspection (3K ANs in total).
Two authors went through this list, marking those
ANs that they found semantically highly anomalous,
no matter how much effort one would put in con-
structing metaphorical or context-dependent inter-
pretations, as well as those they found completely
acceptable (so, rating was on a 3-way scale: deviant,
5
intermediate, acceptable). The rating exercise re-
sulted in rather low agreement (Cohen?s ?=0.32),
but we reasoned that those relatively few cases (456
over 3K) where both judges agreed the AN was odd
should indeed be odd, and similarly for the even
rarer cases in which they agreed an AN was com-
pletely acceptable (334 over 3K). We thus used the
agreed deviant and acceptable ANs as test data.
Of 30 adjectives, 5 were discarded for either tech-
nical reasons or for having less than 5 agreed de-
viant or acceptable ANs. This left us with a de-
viant AN test set comprising of 413 ANs, on av-
erage 16 for each of the 25 remaining adjectives.
Some examples of ANs in this set are: academic
bladder, blind pronunciation, parliamentary potato
and sharp glue. The acceptable (but unattested) AN
test set contains 280 ANs, on average 11 for each of
the 25 studied adjectives. Examples of ANs in this
set include: vulnerable gunman, huge joystick, aca-
demic crusade and blind cook. The evaluation sets
can be downloaded from http://www.vecchi.
com/eva/resources.html.
There is no significant difference between the
length of the vectors of the component nouns in the
acceptable vs. deviant AN sets (two-tailed Welch?s t
test; t=?0.25; p>0.8). This is important, since at
least one of the potential cues to deviance we con-
sider (AN vector length) is length-dependent, and
we do not want a trivial result that can simply be
explained by systematic differences in the length of
the input vectors.
4.3 Composition methods
As discussed in Section 2.2, the experiment was car-
ried out across four compositional methods.
Additive AN vectors (add method) are simply
obtained by summing the corresponding adjective
and noun vectors after normalizing them. Multi-
plicative vectors (mult method) were obtained by
component-wise multiplication of the adjective and
noun vectors, also after normalization. Confirm-
ing the results of Baroni and Zamparelli (2010),
non-normalized versions of add and mult were also
tested, but did not produce significant results (in
the case of multiplication, normalization amounts to
multiplying the composite vector by a scalar, so it
only affects the length-dependent vector length mea-
sure). It is important to note that, as reported in
Baroni and Zamparelli (2010), the mult method can
be expected to perform better in the original, non-
reduced semantic space because the SVD dimen-
sions can have negative values, leading to counter-
intuitive results with component-wise multiplication
(multiplying large opposite-sign values results in
large negative values instead of being cancelled out).
The tests of Section 5, however, are each run in the
SVD-reduced space to remain consistent across all
models. We leave it to future work to explore the
effect on the performance of using the non-reduced
space for the models for which this option is com-
putationally viable.
In the linear map (lm) approach proposed by
Guevara (2010), a composite AN vector is obtained
by multiplying a weight matrix by the concatenation
of the adjective and noun vectors, so that each di-
mension of the generated AN vector is a linear com-
bination of dimensions of the corresponding adjec-
tive and noun vectors. That is, the 600 weights in
each of the 300 rows of the weight matrix are the
coefficients of a linear equation predicting the val-
ues of a single dimension in the AN vector as a lin-
ear combination (weighted sum) of the 300 adjective
and 300 noun dimensions. Following Guevara, we
estimate the coefficients of the equation using (mul-
tivariate) partial least squares regression (PLSR) as
implemented in the R pls package (Mevik and
Wehrens, 2007), with the latent dimension param-
eter of PLSR set to 50, the same value used by Ba-
roni and Zamparelli (2010). Coefficient matrix es-
timation is performed by feeding the PLSR a set
of input-output examples, where the input is given
by concatenated adjective and noun vectors, and the
output is the vector of the corresponding AN directly
extracted from our semantic space (i.e., the AN vec-
tors used in training are not model-generated, but
directly derived from corpus evidence about their
distribution). The matrix is estimated using a ran-
dom sample of 2K adjective-noun-AN tuples where
the AN belongs to the set of 30K frequently attested
ANs in our vocabulary.
Finally, in the adjective-specific linear map
(alm) method of Baroni and Zamparelli (2010), an
AN is generated by multiplying an adjective weight
matrix with a noun vector. The weights of each of
the 300 rows of the weight matrix are the coefficients
of a linear equation predicting the values of one of
6
the dimensions of the AN vector as a linear com-
bination of the 300 dimensions of the component
noun. The linear equation coefficients are estimated
separately for each of the 25 tested adjectives from
the attested noun-AN pairs containing that adjective
(observed adjective vectors are not used), again us-
ing PLSR with the same parameter as above. For
each adjective, the training N-AN vector pairs cho-
sen are those available in the semantic space for each
test set adjective, and range from 100 to more than
500 items across the 25 adjectives.
4.4 Experimental procedure
Using each composition method, we generate com-
posite vectors for all the ANs in the two (acceptable
and deviant) evaluation sets (see Section 4.2 above).
We then compute the measures that might cue se-
mantic deviance discussed in Section 3 above, and
compare their values between the two AN sets. In
order to smooth out adjective-specific effects, we z-
normalize the values of each measure across all the
ANs sharing an adjective before computing global
statistics (i.e., the values for all ANs sharing an ad-
jective from the two sets are transformed by sub-
tracting their mean and dividing by their variance).
We then compare the two sets, for each composition
method and deviance cue, by means of two-tailed
Welch?s t tests. We report the estimated t score,
that is, the standardized difference between the mean
acceptable and deviant AN values, with the corre-
sponding significance level. For all our cues, we
predict t to be significantly larger than 0: Accept-
able AN vectors should be longer than deviant ones,
they should be nearer ? that is, have a higher cosine
with ? the component N vectors and their neighbour-
hood should be denser ? that is, the average cosines
with their top neighbours should be higher than the
ones of deviant ANs with their top neighbours.
5 Results
The results of our experiments are summarized in
Table 1. We see that add and mult provide signif-
icant results in the expected direction for 2 over 3
cues, only failing the cosine test. With the lm model,
acceptable and deviant ANs are indistinguishable
across the board, whereas alm captures the distinc-
tion in terms of density.
LENGTH COSINE DENSITY
method t sig. t sig. t sig.
add 7.89 * 0.31 2.63 *
mult 3.16 * -0.56 2.68 *
lm 0.16 0.55 -0.23
alm 0.48 1.37 3.12 *
Table 1: t scores for difference between acceptable and
deviant ANs with respect to 3 cues of deviance: length
of the AN vector, cosine of the AN vector with the com-
ponent noun vector and density, measured as the average
cosine of an AN vector with its nearest 10 neighbours in
semantic space. For all significant results, p<0.01.
The high scores in the vector length analyses of
both the addition and the multiplication models are
an indication that semantically acceptable ANs tend
to be composed of similar adjectives and nouns, i.e.,
those which occur in similar contexts and we can as-
sume are likely to belong to the same domain, which
sounds plausible.
In Baroni and Zamparelli (2010), the alm model
performed far better than add and mult in approxi-
mating the correct vectors for unseen ANs, while on
this (in a sense, more metalinguistic) task add and
mult work better, while alm is successful only in the
more sophisticated measure of neighbor density.
The lack of significant results for the cosine mea-
sure is disappointing, but not entirely surprising. A
large angle between N and AN might be a feature of
impossible ANs common to various types of pos-
sible ANs: idioms (a red herring is probably far
from herring in semantic space), non-subsective ad-
jectives (stone lion vs. lion; fake butterfly vs. but-
terfly), plus some metaphorical constructions (aca-
demic crusade vs. crusade?one of several ANs
judged acceptable in our study, which can only be
taken as metaphors). Recall, finally, that the vector
for the base N collapses together all the meanings
of an ambiguous N. The adjective might have a dis-
ambiguating effect which would increase the cosine
distance.
To gain a better understanding of the neighbor-
hood density test we performed a detailed analysis
of the nearest neighbors of the AN vectors generated
by the three models in which the difference in neigh-
bor distance was significant across deviant and ac-
ceptable ANs: alm, multiplication and addition. For
7
each of the ANs, we looked at the top 10 semantic-
space neighbors generated by each of the three mod-
els, focusing on two aspects: whether the neighbor
was a single A or N, rather than AN, and whether
the neighbor contained the same A or N as the AN
is was the neighbor of (as in blind regatta / blind
athlete or biological derivative / partial derivative).
The results are summarized in Table 2.
method status A N A1= N1=
only only A2 N2
add
accept 11.9 8.7 14.6 2.4
deviant 12.5 6.8 14.6 2.3
mult
accept 6.9 8.0 0.7 0.1
deviant 2.7 7.3 0.5 0.1
alm
accept 4.9 17.7 7.0 0.0
deviant 7.1 19.6 6.2 0.0
Table 2: Percentage distributions of various properties of
the top 10 neighbours of ANs in the acceptable (2800)
and deviant (4130) sets for add, mult and alm. The last
two columns express whether the neighbor contains the
same Adjective or Noun as the target AN.
In terms of the properties we measured, neighbor
distributions are quite similar across acceptable and
deviant ANs. One interesting finding is that the sys-
tem is quite ?adjective-driven?: particularly for the
additive model (where we can imagine that some Ns
with low dimensional values do not shift much the
adjective position in the multidimensional space),
less so in the alm method, and not at all for mult. To
put the third and forth columns in context, the subset
of the semantic space used to generate the SVD from
which the neighbors are drawn contained 2.69% ad-
jectives, 5.24% nouns and 92.07% ANs. With re-
spect to the last two columns, it is interesting to ob-
serve that matching As are frequent for deviant ANs
even in alm, a model which has never seen A-vectors
during training. Further qualitative evaluations show
that in many deviant AN cases the similarity is be-
tween the A in the target AN and the N of the neigh-
bor (e.g. academic bladder / honorary lectureship),
while the opposite effect seems to be much harder to
find.
6 Conclusion and future work
The main aim of this paper was to propose a new
challenge to the computational distributional seman-
tics community, namely that of characterizing what
happens, distributionally, when composition leads
to semantically anomalous composite expressions.
The hope is, on the one hand, to bring further sup-
port to the distributional approach by showing that it
can be both productive and constrained; and on the
other, to provide a more general characterization of
the somewhat elusive notion of semantic deviance ?
a notion that the field of formal semantics acknowl-
edges but might lack the right tools to model.
Our results are very preliminary, but also very en-
couraging, suggesting that simple unsupervised cues
can significantly tell unattested but acceptable ANs
apart from impossible, or at least deviant, ones. Al-
though, somewhat disappointingly, the model that
has been shown in a previous study (Baroni and
Zamparelli, 2010) to be the best at capturing the se-
mantics of well-formed ANs turns out to be worse
than simple addition and multiplication.
Future avenues of research must include, first of
all, an exploration on the effect on each model when
tested in the non-reduced space where computation-
ally possible, or using different dimensionality re-
duction methods. A preliminary study demonstrates
an enhanced performance of the mult method in the
full space.
Second, we hope to provide a larger benchmark
of acceptable and deviant ANs, beyond the few hun-
dreds we used here, and sampling a larger typology
of ANs across frequency ranges and adjective and
noun classes. To this extent, we are implementing
a crowd-sourcing study to collect human judgments
from a large pool of speakers on a much larger set of
ANs unattested in the corpus. Averaging over mul-
tiple judgments, we will also be able to characterize
semantic deviance as a gradient property, probably
more accurately.
Next, the range of cues we used was quite limited,
and we intend to extend the range to include more
sophisticated methods such as 1) combining multi-
ple cues in a single score; 2) training a supervised
classifier from labeled acceptable and deviant ANs,
and studying the most distinctive features discov-
ered by the classifier; 3) trying more complex unsu-
pervised techniques, such as using graph-theoretical
methods to characterize the semantic neighborhood
of ANs beyond our simple density measure.
Finally, we are currently not attempting a typol-
8
ogy of deviant ANs. We do not distinguish cases
such as parliamentary tomato, where the adjective
does not apply to the conceptual semantic type of
the noun (or at least, where it is completely undeter-
mined which relation could bridge the two objects),
from oxymorons such as dry water, or vacuously
redundant ANs (liquid water) and so on. We real-
ize that, at a more advanced stage of the analysis,
some of these categories might need to be explicitly
distinguished (for example, liquid water is odd but
perfectly meaningful), leading to a multi-way task.
Similarly, among acceptable ANs, there are spe-
cial classes of expressions, such as idiomatic con-
structions, metaphors or other rhetorical figures, that
might be particularly difficult to distinguish from
deviant ANs. Again, more cogent tasks involving
such well-formed but non-literal constructions (be-
yond the examples that ended up by chance in our
acceptable set) are left to future work.
Acknowledgments
We thank Raffaella Bernardi, Gemma Boleda,
Louise McNally and the anonymous reviewers for
their advice and comments.
References
Nicholas Asher. 2011. Lexical Meaning in Context: A
Web of Words. Cambridge University Press.
Marco Baroni and Alessandro Lenci. 2010. Dis-
tributional Memory: A general framework for
corpus-based semantics. Computational Linguistics,
36(4):673?721.
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of EMNLP, pages 1183?1193, Boston,
MA.
Noam Chomsky. 1957. Syntactic Structures. Mouton.
Noam Chomsky. 1977. Essays on Form and Interpreta-
tion. North Holland, New York.
Katrin Erk and Sebastian Pado?. 2008. A structured vec-
tor space model for word meaning in context. In Pro-
ceedings of EMNLP, pages 897?906, Honolulu, HI,
USA.
Stefan Evert. 2005. The Statistics of Word Cooccur-
rences. Dissertation, Stuttgart University.
Dan Fass and Yorick Wilks. 1983. Preference seman-
tics, ill-formedness, and metaphor. Computational
Linguistics, 9:178?187.
Rachel Giora. 2002. Literal vs. figurative language: Dif-
ferent or equal? Journal of Pragmatics, 34:487?506.
Emiliano Guevara. 2010. A regression model of
adjective-noun compositionality in distributional se-
mantics. In Proceedings of the ACL GEMS Workshop,
pages 33?37, Uppsala, Sweden.
Walter Kintsch. 2001. Predication. Cognitive Science,
25(2):173?202.
Thomas Landauer and Susan Dumais. 1997. A solu-
tion to Plato?s problem: The latent semantic analysis
theory of acquisition, induction, and representation of
knowledge. Psychological Review, 104(2):211?240.
Bjo?rn-Helge Mevik and Ron Wehrens. 2007. The
pls package: Principal component and partial least
squares regression in R. Journal of Statistical Soft-
ware, 18(2). Published online: http://www.
jstatsoft.org/v18/i02/.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL, pages 236?244, Columbus, OH, USA.
Jeff Mitchell and Mirella Lapata. 2009. Language mod-
els based on semantic composition. In Proceedings of
EMNLP, pages 430?439, Singapore.
Jeff Mitchell and Mirella Lapata. 2010. Composition in
distributional models of semantics. Cognitive Science.
Reinhard Rapp. 2003. Word sense discovery based on
sense descriptor dissimilarity. In Proceedings of the
9th MT Summit, pages 315?322, New Orleans, LA,
USA.
Helmut Schmid. 1995. Improvements in part-of-speech
tagging with an application to German. In Proceed-
ings of the EACL-SIGDAT Workshop, Dublin, Ireland.
Hinrich Schu?tze. 1997. Ambiguity Resolution in Natural
Language Learning. CSLI, Stanford, CA.
Richmond H Thomason, editor. 1974. Formal Philoso-
phy: Selected Papers of Richard Montague. Yale Uni-
versity Press, New York.
Peter Turney and Patrick Pantel. 2010. From frequency
to meaning: Vector space models of semantics. Jour-
nal of Artificial Intelligence Research, 37:141?188.
Chang-Le Zhou, Yun Yang, and Xiao-Xi Huang. 2007.
Computational mechanisms for metaphor in lan-
guages: a survey. Journal of Computer Science and
Technology, 22:308?319.
9
