Incremental Conceptualization for Language Production
Markus Guhe
(University of Edinburgh)
Mahwah, NJ: Lawrence Erlbaum Associates (distributed by Psychology Press), 2007,
xii+260 pp; hardbound, ISBN 978-0-8058-5624-8, $75.00
Reviewed by
Paul Piwek
The Open University
For the past ten years or more, most work in the field of Natural Language Gener-
ation (NLG) has shied away from considerations regarding the processes underlying
human language production. Rather, the focus has been on systems that automatically
produce language?usually text?from non-linguistic representations, with the main
objective being generation of a text that faithfully captures the meaning of those non-
linguistic representations (see, e.g., Reiter and Dale?s 2000 textbook on NLG). There
is, however, also a different take on NLG ?as not just competent performance by a
computer but the development of a computational theory of the human capacity for
language and processes that engage it? (McDonald 1987, page 642). Guhe?s research
monograph, based on his 2003 Ph.D. thesis, is firmly situated in the latter tradition.
One of his main goals is to work out a computational architecture for Levelt?s (1989)
psycholinguistically motivated model of language production. According to Levelt?s
model, speaking involves three main activities: conceptualizing (deciding what to say),
formulating (deciding how to say it), and articulating (saying it). Guhe?s book focuses
on the mental activity of conceptualizing.
Conceptualizing is a recalcitrant object of study, partly because of the problem of
the ?initial spark?; the decision to say something appears to be the result of volitional
conscious decisions, which largely elude scientific study. Guhe avoids this problem by
investigating conceptualization in settings where the main intention is already fixed:
a speaker witnesses several events unfold and is instructed to describe what happens
(while it happens). The research challenge then is to figure out how ?subintentions? for
individual speech acts come about. The benefit of using an on-line generation setting is
that it provides information on both what a speaker says at a given point in time and
what is being reported, that is, the data that drive the speaker?s utterances.
The book consists of the usual preface and introduction, followed by four parts
(A, B, C, and Results), a list of the book?s theses, and an appendix that includes,
among other things, a glossary, bibliography, name index, and subject index. Part A
of the book is titled ?Conceptualization.? It starts with an introduction to the field of
language production, with particular reference to Levelt?s (1989) model. The notion of
conceptualization as a ?quasi-module,? partly using Fodor?s (1983) criteria, is presented
and four subtasks of conceptualization are discussed:
1. construction (mainly mapping what is perceived to concepts from
long-term memory)
2. selection (of events that are to be verbalized)
3. linearization (ordering selected events appropriate to the goal of
the discourse)
Computational Linguistics Volume 34, Number 1
4. generation of preverbal messages (mapping the conceptual
representations that have been handled so far to semantic
content that can interface with the linguistic formulator)
This chapter also introduces referential nets, the formalism that is used to represent
conceptual content.
Part B (?Incrementality?) traces the roots of the notion of incrementality in computer
science, and provides an extensive overview of various notions of incrementality. Guhe
settles on a definition of incrementality whose crux is the piecemeal processing of
information and production of output before all input has been seen. He distinguishes
between incremental processes, algorithms, and models; roughly speaking, incremental
models contain a strictly uni-directional cascade of incremental processes that recur-
sively call incremental algorithms. For Guhe, an essential characteristic of incremental
algorithms is that they use only a local context, as opposed to all available knowl-
edge, for their computations. He also adopts the common distinction between working
memory and long-term memory. The former mediates the flow of information between
incremental processes. ?Increments,? the small pieces of information that incremental
processes operate with, can be read from it and written to it. It contains ?situation and
discourse knowledge,? whereas long-term memory stores static ?encyclopedic knowl-
edge.? This ?blueprint for incrementality? is accompanied by a useful discussion of
various dimensions of incrementality, such as monotonicity, lookahead, feedback, and
discreteness.
Part C focuses on INC, the incremental conceptualizer, which is an implemented
?working model? of the blueprint for incrementality. INC is offered as a framework,
that is, a model which has been fleshed out in detail in some respects and left under-
specified in others. A central role is played by four parameters of INC which influence
its behavior. For example, two of these concern the storage of event representations in a
buffer inworkingmemorywhichmediates the flow of information between incremental
processes. One parameter, length of traverse buffer (LOTB), concerns the size of this
buffer, whereas the other, latency (LT), determines for how long an element is kept in
the buffer until it is picked up by preverbal message generation. Small values for LOTB
in combination with a large value for LT can lead to the ?forgetting? of information: If
the buffer has filled up and new information is added, the first element on the buffer
is discarded and never reaches preverbal message generation. The book presents some
evidence that variation of the parameter settings can account for some of the variation
found among human speakers. This part of the book concludes with a discussion of the
output of INC for two domains and output of human speakers for the same domains.
It concerns a visual scene, from a bird?s eye perspective, of two moving planes on a
runway, and the replay of the drawing of a simple line drawing consisting of eight lines
that represents a crossing.
The ?Results? summarizes the main contributions of the book, makes some com-
parisons with Levelt?s (1989) model, and proposes a number of future extensions, such
as the addition of Levelt?s monitor. The monitor takes as input the output of the speech-
comprehension system and uses this to influence the processing of the conceptualizer.
Finally, there are a good number of suggestions for further ways to parameterize INC.
The book is a rich source of information on language production, both from a
computational and a cognitive point of view. It includes a good introduction to con-
ceptualizing, and provides an insightful discussion of many varieties of incrementality.
INC is an excellent starting point for others interested in on-line data-driven generation
to both build on and respond to. The breadth of the work means that one gets a truly
130
Book Reviews
holistic view of the problem and is given a good impression of the many debates that
cross the boundaries of different disciplines. In this respect, the book goes against a
recent trend in computational linguistics to show less interest in other language-related
research communities (see Reiter 2007).
Although the wide scope of this book is in many ways what makes it attractive,
it also leads to some of its weaknesses. In particular, the way INC is presented in this
broad context did not feel optimal to me. Although the proper description of INC is
delayed until Part C, there are numerous forward references to INC in the preceding
parts. The reader will find several instances where a certain aspect of conceptualization
or incrementality is discussed with reference to INC, only to find out later that this
particular feature ?is not implemented yet (apart from a dummy function).? It would
have been fairer to the reader to separate a clear description of the current state of INC
from the wider discussion surrounding it. Another presentational issue concerns the
tight integration of locality and incrementality in the book?s definitions. In particular,
the virtual identification of incremental algorithms with computation on a local context
makes one question why the book speaks of incremental rather than local algorithms.
A more substantive point relates to Part C on INC. This part includes the description
of two simulations that were run with INC. Somewhat frustratingly, both descriptions
are incomplete. For instance, whereas for the first simulation the appendix contains the
texts produced by human participants for the same task, there is no systematic analysis
of the structural (dis)similarities between the output of INC and that of the human
speakers. For the second simulation, there are some analyses of the similarities between
the structure of INC?s and the human speakers? output, but no transcripts of complete
human outputs are provided. In both cases, there is also no detail about how long-
term memory, referred to as the concept storage (CS), was populated for the relevant
domains, even though the CS must have had a significant influence on the output that
INC produces.
This book will be useful to research students and researchers in natural language
generation who are interested in the study of generation systems as a computational
model of human language production. Part B of the book, on incrementality, might
also prove useful to those approaching NLG as an engineering problem. The main
reason to consult this book is that it brings together in a single place information on
conceptualization, incrementality, and various debates in philosophy, cognitive science,
and computer science affecting these topics. INC, the incremental conceptualizer which
is described in part C of the book, presents an ambitious attempt to implement a
computational model of incremental conceptualization. The verdict on its adequacy is
still out, given the limited empirical evaluation to which it has been subjected thus far.
Online generation, a central theme of this book, was adopted in 2007 at the Interna-
tional Conference on Intelligent Virtual Agents as a task (automated real-time reporting
on simulated horse races) in the GALA competition for Embodied Lifelike Agents.1
Work on embodiment and conceptualization, new insights into societal grounding of
conceptual representations (e.g., DeVault, Oved, and Stone 2006), empirical and compu-
tational studies on generation (both incremental and non-incremental, from numerical
data; e.g., vanDeemter 2006), and recent experimental techniques for studying language
production (see Roelofs 2004 for an overview) give a sense that this book could be part
of an exciting revival of cognitively motivated NLG.
1 See http://hmi.ewi.utwente.nl/gala/.
131
Computational Linguistics Volume 34, Number 1
References
van Deemter, Kees. 2006. Generating
referring expressions that involve gradable
properties. Computational Linguistics,
32(2):195?222.
DeVault, David, Iris Oved, and Matthew
Stone. 2006. Societal grounding is
essential for meaningful language use. In
Proceedings of the 21st National Conference on
Artificial Intelligence (AAAI-06), Boston,
MA, pages 747?754.
Fodor, Jerry A. 1983. The modularity of mind.
MIT Press, Cambridge, MA.
Levelt, Willem J. M. 1989. Speaking: From
Intention to Articulation. MIT Press,
Cambridge, MA.
McDonald, David. 1987. Natural language
generation. In Stuart C. Shapiro, editor,
Encyclopedia of Artificial Intelligence,
Volume 1, John Wiley & Sons, New York,
pages 642?654.
Reiter, Ehud. 2007. The shrinking
horizons of computational linguistics.
Computational Linguistics,
33(2):283?287.
Reiter, Ehud and Robert Dale. 2000.
Building Natural Language Generation
Systems. Cambridge University Press,
Cambridge, UK.
Roelofs, Ardi. 2004. The seduced speaker:
Modeling of cognitive control.
In Anja Belz, Roger Evans, and
Paul Piwek, editors, Natural Language
Generation, Third International
Conference, LNCS 3123, Springer,
Berlin, pages 1?10.
Paul Piwek is lecturer in computing at the Open University. His research interests are (multimodal)
natural language generation and dialogue modeling. Piwek?s address is: Centre for Research in
Computing, The Open University, Walton Hall, Milton Keynes, UK; e-mail: p.piwek@open.ac.uk.
132
A Formal Semantics for Generating and Editing Plurals 
Paul  P iwek  
ITRI - University of  Brighton 
Watts Building, Moulsecoomb,  
Brighton BN2 4GJ 
UK 
Paul .P iwek@itr i .br ighton.ac.uk 
Abst rac t  
Wc present a formal semantics for an object- 
oriented formalism which allows for the represen- 
tation of plura, l objects (such as 'Three N', 'Most 
of the N', 'Some N',...). The semantics is given in 
terms of it mapping to a variant of Discourse Rep- 
resentation Theory. It is motivated by its suitability 
lkw natural anguage generation and interactive dit- 
ing of the representations. 
1 In t roduct ion  
A natural anguage generator typically generates a 
noun plnase l'rom a representation consisting of an 
object with one or more attributes (cf. Reiter & 
l)ale, 2000). Usually this representation is sup- 
plemented with inl'ormation concerning the context 
in which the noun phrase has io be realized (e.g., 
the set of distractors, whether tile object is in fo- 
cus, etc.). \];or instance, the lil,ICUP, l{ system (Dale, 
1992) deals with reference to plural objects by hav- 
ing the l'ollowing three attributes on physical ob- 
jects: structure, whose wflue can be either a set or 
individual, cardinalio, which in case of a set records 
the numbers of elements which the set has, and con- 
stituents which in case of a set contains the elements 
of the set. 
Our proposal is intended to extend the representa- 
tions proposed in (Dale, 1992)) Most importantly, 
wc replace the attribute cardinalily with the more 
general attribute quant (for quantifier) whose value 
is a quantilier such as 'most', 'few', '5', '<  6' (at 
most 6), etc. Furthermole, we introduce the new at- 
tribute parl_of which takes its a value an ob.jecl of 
which the object in question is a part. ~ 
~Note that we are dealing with the generation of plurals 
from (logically) structured data as opposed lo raw data as in, 
e.g., Stone (1999). 
2We use the mcfcologicat 'part of' relation as an alternative 
It} "subset' For details, see the next section. 
The object-oriented (00) forlnalism in which we 
implement tile al'orelnentioned attributes is an ex- 
tension of standard oo  formalisms. It is known 
as Scoped Semantic Networks (SSN; Kibble et al, 
1999; Power, 1999). 3 An SSN consists of a net- 
work of objects together with a mapping o1' these 
objects to a set o1' logical contexts. This makes it 
possible to represent universal quantification, im- 
plication, negation and other logical operators. In 
particulal; Power (1999) shows how an SSN can be 
mapped into a I)iscourse Representation Structure 
(DRS; Kamp & Reyle, 1993), lhus providing a for- 
lnal semantic interpretation of SSNs. 
In tiffs paper, we provide a mapping of SSNs with 
plural objects to an adapted version of Discourse 
Represemation Theory (I)RT). The mapping is pro- 
vided to obtain t%rmal truth conditions for the SSNs. 
Such a lnaPlfing provides us with a mathenmlically 
precise characterization el'the information which is 
represented by a SSN ill terms of its truth-condilions. 
This is useful if we want to automatically nanipu- 
lale lhe information which is represented by means 
of an SSN. For example, we can formally define 
whether some piece of information is aheady im- 
plicit in some other piece of information; in other 
words, we can deline a notion of logical conse- 
quence. Related to this is the possibility to use the 
semantics in order to test the consistency of the in- 
formatiou conveyed by an SSN. For tlmt purpose, we 
can do so-called model checking: an SSN is consis- 
lent if we can construct a model -that is, a logically 
possible state of the world- in which tile SSN is true 
according to our truth-conditional semantics. 
We do not provide a direct formal semantics for 
SSN, but rather map it to a more convenient log- 
ical l'ormalistn, i.e., I)P,T. The main reason for 
tiffs approach is that phenomena which we will be 
modelling in this paper, i.e. (plural) reference and 
aScc also, e.g., Sowa (1984). 
607 
anaphora, have been studied extensively within I)RT 
(see, e.g., Kamp & Reyle, 1993; Krahmer & Van 
Deemter, 1998; Piwek, 1997). Fnrthermore, we be- 
lieve that the adaptation of DRT that we propose is 
of interest in its own right. 
The mapping which we provide from SSNs with 
plural objects to DRSs requires some modifications 
to standard DRT with plurals (Kamp & Reyle, 1993: 
Chapter 4). For networks with only singular objects, 
there is a straightforward mapping of the objects in 
a network to the discourse referents which populate 
a DRS. Things are different for networks with plural 
objects. Consider: 
(1) Susan has found most books which Bill needs. 
The DP, S for this sentence is: 
(2) 
y 
book(y) 
need(bill,y) found(susan,y) 
Intuitively, the meaning of this condition is that: fi')r 
most y which satisfy the conditions to the le/'t of the 
diamond, it holds that they also sati,@~ the condition 
on the right. Note, that the representation contains 
no plural discourse referent corresponding tothe Nt' 
'most books which Bill needs'. The 'y' in this repre- 
sentation is a referent for singular individuals. This 
might make one wonder how it is possible in stan- 
dard DRT to refer back to plural individuals as in: 
(3) Susan has found most books which Bill needs. 
They were on her desk. 
For this purpose, there is a so-called abstraction op- 
eration (Kamp & Reyle, 1993:313) with which we 
can obtain a discourse referent for the set of books 
which Bill needs and Susan Jbund. In more tech- 
nical terms, the set is obtained by the summation 
of the values which 'y' can take. Thus there is no 
direct way of mapping a plural object in a seman- 
tic network (which represent the interpretation of an 
NP) to a plural discourse referent in the correspond- 
ing DRS. For this reason we have chosen to adapt 
the DP, T formalism, so that plural noun phrases do 
directly colTelate with plural discourse referents. 
We now proceed as follows. In Section 2, we 
specify the mapping from SSNs to our version of 
DRT. In the next section (Section 3), we describe an 
application which uses the SSNs with plurals. We 
finish this paper with a conclusions ection (Section 
4). 
2 From SSNs to DRSs 
In this section, we provide a mapping from SSNs 
into discourse representation structures (DRSs) with 
plurals. We start out by specifying the target of the 
mapping, i.e., plural DRT. 
DRSs with Plurals Following Kamp & Reyle 
(1993), we treat singular objects and sets of objects 
as entities of the same kind. Both am considered 
to be individuals: atomic and non-atomic individ- 
uals, respectively. Thus, the model theory follows 
the models which Link (1983) provides for count 
nounsfl The idea is that the denotation of an NP 
which contains a count noun can be uniquely subdi- 
vided into atomic parts (as opposed to the denotata 
of mass nouns). The domain for NPs is structured by 
a prot-whole relation which satisfies the axioms of 
upper semilattices (for background information on 
these lattices see Kamp & Reyle, 1993:398-406). 
In formal terms, a model is defined as follows: 
A model _/14 is a quintuple (Lt, g, Pred, @mrzt, Name) 
which consist of: 
(1) A domain of individuals with the structure of a com- 
plete, free, atomic upper scmilattice H = (U, C) with 
zero; 
(II) A domain of eventualities with the structure of a 
complete, free, atomic upper semilattice g = @7, C); 
(III) A function Pred mapping predicates P to their ex- 
tensions in k//, such that 
(III.1) for tim relations representing thematic roles, such 
as agent and patiertt, I@ed assigns aset of tuples (c, a), 
wherecCEandaGU.  
(III.2) for eventuality predicates, Prod(P) C_ E. 
(I11.3) For object ype predicates, Prod(P) C U. 
(IV) A function Qua~tt mapping determiners DEW to 
their corresponding interpretations, i.e., a set consisting 
of tuples {a, b) (where a, b C U). 
(V) A function Name mapping constants o members of 
U. in particular, the constants c/,, where P is a predi- 
cate are mapped to ?Pred(P), i.e., the supremum, also 
known as the sum, of the interpretation f P. 
Notice that in our models there are separate domains 
for objects and eventualities (i.e., states and events). 
4Fora critical discussion and alternative to Link (1983), see 
for instance Landman (1989). 
608 
The relations agent and patient have an eventual-  
ity as their first argument and an object as second 
argument (cf. Parsons, 1990). agent(e,o) is to be 
interpreted as: object o is the agent of  eventual ity e. 
Furtherlnore, there are predicates applying to even- 
tualities and others applying to objects. 
For our purposes, the most interesting part of  the 
definition is the function Q~ta,~,t; which maps deter- 
miners to their respective interpretations. We take 
the interpretation of a determiner to be a set of tu- 
pies, where each tuple consist of a pair of  (plural) in- 
dividuals. For instance, take the deterlniner 'most ' .  
Q'~m, nt, maps it to the fol lowing interpretation: '5 
(4) Q~ga~,t(Most) = {(r ,  c) : r c c & r is a non-  
atomic entity of M & kl  -> } 
Thus 'most '  corresponds to the set of  all tuples of 
individuals, such that the first individual is a non- 
atomic part of the second one and the cardinality 
of the first is greater than or equal to the cardinal- 
ity of the second divided by two. Henceforth,  we 
will call the second individual the context individual 
(cf. Westerstfihl, 1985). Given a noun phrase, such 
as 'most  birds',  the first individual is intended as 
the interpretation of  the entire noun phrase whereas 
the second individual plays the role of the con- 
text against which the noun phrase is interpreted. 
The context individual can be restricted by extra- 
linguistic c ircumstances (e.g., the siluation in wlaich 
a noun phrase is produced) and by linguistic means 
(as in 'most  of the birds on the beach' ,  where 'the 
birds on the beach'  supplies the contextual individ- 
ual). 
Let us focus on the DRS condition which is inter- 
preted in the models in terms of @m,~,t. This con- 
dition functions as a substitute for the duplex condi- 
tions of  standard DRT 6 The condition in question is: 
'51tere we follow Ihe 'more than half' interpretation of 
'most' common fi'om the literature on GEneralized Quantiliers 
(see, e.g, I?,arwise & Cooper, 1981; Keenan & Westerstahl, 
1997). This interpretation is not entirely unproblematic; see, 
for instance, (Kamp & P, eyle, 1993). Our use of the interpre- 
tation is, however, solely for illustrative purposes. We can also 
accommodate for alternative mappings fur Q~u~nt(Most). 
Similarly we cannot go into detailed iscussions of other quan- 
tifiers such as, for instance, 'many' (of. Lappin, 1988). 
6Within the conlines of this paper it is impossible to give a 
full formal delinition of our version of plural I)RT, thcrelore we 
focus on the aforementioned condition. The other definitions 
closely lollow those in Kamp & P, eyle, 1993: 425-427, 677- 
6'79). 
If z is a discern;re referent and t is a discourse re\[er- 
ent or constant, then DETt(:c) is a condition. 
The verification condition for this condition is: 
(5) M ~f  DETt(:C) (if" 
(11 II II t IIAJ'f> 
Let us illustrate these definitions with a s imple ex- 
ample. Consider: 
(6) At most two men walk. 
The NP 'At most two men'  introduces a plural dis- 
course referent X, together with a number  of condi- 
tions on that referent. Additionally, the verb 'walk '  
supplies a condition to the effect that all the mem- 
bers of X walk. 7,s 
(7) 
X 
AT_MOST_2c ....... (X) 
man(z)  
walk(z) 
walk*(X) 
The first condition says that X consists of a subset of 
the set of all men (cm,,,~, alternatively, we could use 
a set of contextually given men) and that X should 
consist of  at most 2 individuals belonging to that 
set. '? The implicative condition is there to make sure 
there is no other set apart from X with (other) men 
who are also walking. Such a closure condition is 
particularly useful for the direct representation of  
monotonical ly decreasing quantifiers. ~? A quantor 
Q is monotonical ly decreasing if and only if for all 
7For cxpository reasons, we have left out explicit represEn- 
tations of events in this example. But, see the next section for a 
DP, S with plurals and events. 
8Note that when a predicate in a condition is marked with 
a '*', this means that the prcdicate is interpreted distributively 
over the atomic parts of the objects in its denotation. 
"JWe assume that: @Umt(AT_MOST_2) = {(r, c) : r C c 
& I,'1 < 2} 
mln Van Eijck (1983), an allemative approach is proposed 
within a fl'amework which also allows for the direct representa- 
tion of plural referents in DRT. lie proposes to reanalyse mono- 
tonically decreasing quantiliers in terms of negation and mono- 
tonically increasing ones. This, however, means that WE no 
longer have a direct correlation between plural discourse ref- 
erents and monotonically decreasing quantifiers. Furthermore, 
it prevents uch quantifiers from any anaphoric uptake as in 
'Fewer than ten students took the test. They all passed it'. 
609 
X,Y ,Z  it holds that: if QXY and Z ~ Y ,  then 
QXZ.  Thus, for instance, (a) 'At most two meu 
walk and talk' does not imply that (b) 'At most two 
men walk'. If we would represent (a) without the 
closure condition (i.e., there is a set of at most two 
men and each of them walks and talks), then (b) (i.e., 
there is a set q\[" at most two men and each of them 
walks) would follow fi'om (a). However, if we add 
to the representation f (a) that there are no other 
sets of men who walk and talk and to the represen- 
tation of (b) that that there are no other sets of  men 
who walk, then (a) no longer follows fiom (b); the 
additional information in (a) that there are no other 
sets e lmen who both walk and talk, does not entail 
that there are no other sets o/'men who walk. 
Seeped Semantic Networks A seeped semantic 
network (SSN) is a triple (D, L, f ) ,  consisting of a 
typed DAG (Directed Acyclic Graph) D, a sef of log- 
ical contexts L and a function f which assigns a log- 
ical context (which are treated as primitive objects 
separate from those in the DAG) to each of the ob- 
jects in the DAG. In the DAG, there are objects which 
correspond with logical operators, such as implica- 
tion and negation, and non-logical objects, such as 
physical objects and events. The function f ,  which 
assigns logical contexts to objects in a typed DAG 
D, satisfies the following constraints: 
(I) The root object and all the objects which are direct 
descendants of a logical operator are assigned a unique 
logical context. These contexls inherit he partial order- 
ing (in the DAG) of the objects with which they are asso- 
ciated. Furthermore, this set of logical contexts consti- 
tutes the range of f. 
(II) Logical operators which have not been assigned a
context by clause 1. are mapped to the logical context of 
their nearest ancestor to which clause 1. applies. 
(III) Objects which arc not assigned to a logical context 
by the clauses 1. and 2. are assigned to a logical context 
in accordance with DRT's accessibility rules. 
Consider, for instance, the following sentence: 
(8) If a man is happy, then he whistles. 
We can represent this sentence by means of the SSN 
in Figure 1. In this representation, the dots repre- 
sent objects, the circles represent logical contexts 
(an object inside a circle belongs to the correspond- 
ing logical context), the solid arrows represent at- 
tributes and the dotted arrows represent that the ob- 
ject fi'om which the arrow originates belongs to the 
context o which the arrow points. 
There is a straightforward procedure for mappiug 
a SSN into a I)RS: 
(I) Logical contexts are mapped into boxes, where the 
nesting of the boxes is isomorphic to the partial ordering 
of the corresponding logical contexts. 
(II) Objects are inserted into the box which corresponds 
with their logical context, except for logical operators. 
The latter are mapped onto the appropriate operators on 
the boxes of their directly subordinate objects. 
(III) Typing statements T(z) of a non-logical object are 
added to the same box as the object z itself. 
(IV) Attributions/{(.% !/), where z and !/are non-logical 
objects, are added to the same box as z. 
:~= ~ impl i cat ion  
%% 
happy(e)  ~ ? )wh is t le  
4 #. 
e'~'O0~ ""0: 
4~ 
? man 
Figure 1" Network for (8) 
By applying these rules, we obtain the following 
DP, S for the SSN in Figure 1 : 
(9) 
xe  
happy(e) 
man(x) 
agent(e,x) 
e ~ 
=> whistle(e') 
agem(e',x) 
Note how the three circles in the SSN correspond 
with the three boxes of the DRS. Furthermore, the 
discourse referent z colresponds to the object in the 
SSN of the type man and inhabits the same box as 
the conditions which correspond to the object of 
type happy and the attribute agent. 
SSNs with Plurals In this section, we describe an 
extension of SSNs for countable plural objects. This 
extension requires no changes to the format of SSNs. 
Rathel, we introduce a number of special-purpose 
610 
attributions and types. Subsequently, we specify 
their mapping to appropriate terms in a DRS. 
We introduce two attributes on cotmlable objects: 
(I) quant. The wdue of this feature is reslricted to an 
oltiect of the type det_type. Examples of tlle subtypes of 
dcl, d, ype arc 2, > 1, < 3, all,.f>w, etc. 
(11) parl,_of. The value of this feature is restricted to 
countable objects. 
The lnapping of SSNs which include these special- 
purpose attributions and types to a l)P,s is defined as 
follows: 
(1) For typing statements T(x), where T is a subtype of 
del,_type: ignore the statement 7'(x) and the object x; 
(H) For attributions quant(x,y) such that ~z : 
p(,,rt_of(:,:,z) & z is an a,,cho,'& Tt(x) & 7~(y), add 
to the box in which also x lives the lbllowing condition: 
.r = T2(c7~). Note that in this case T~ is subtype of 
&:t_type,. The role of contextual individual is played by 
(:7,~, i.e., a constant which denotes lhe supremum of the 
denotation of TI. Furthermore, we add a closure condi- 
tion; 
(I\]tl) For attributions q'uant(:r,y) such that ~z : 
part_of(x, z) & T1 (x) & 7)(y) add to the box in which 
also :r lives the following condition: x = 5/)(z) .Further- 
more, we add a closure condition; 
(IV) Otherwise apply the standard mapping rules for 
SSNs (see the previous ection). 
Consider, lbr instance, the (phual) SSN for lhe sen- 
tence 'At most two men walk' in Figure (2). 
:, ? ) wa lk  
-.f 
: 133 
V 
? mar  
, 
at most 2 
Figure 2: Network for 'At most two men wall<' 
This SSN contains only one logical context which is 
inhabited by the objects of type man and walk. The 
object of type man is possibly plural: its quant at- 
tribute points to an object of type at.anost_2. The 
value of the other attribute, i.e., part_oJ; is not in- 
stantiated in this case. This is represented by means 
of the empty box. When we apply the rules for map- 
ping SSNs to DRSS, we obtain the following repre- 
sentation: 
(lO) 
at  
AT_MOST_2  c' ...... (X )  
man(X) 
walk(c) 
agent(e,X) 
z e I 
man(z) z c X 
agent(e',z) => 
walk(e') ~e . '~  
The first four conditions correspond to the types of 
the nodes and the attributes of the SSN. They are 
followed by the closure condition. 
3 Ed i t ing  P lura ls  
In tiffs section, we describe how plural SSNs can be 
used for WYSIWYM editing (Power et al, 1998). 1~ 
WYSIWYM stallds for What Yott See \]s What Yott 
Meant. it is a technology for directly manipulat- 
ing knowledge representations u ing natural lan- 
guage feedback. WYSIWYM \ ] las been used in var- 
ious systems for (multilingual) document authoring 
and query formulation. The proposal which is pre- 
sented in tiffs paper has been inaplemented as part 
of the M ILF, query-answering system (e.g., Piwek ct 
al., 2000). 
The basic idea underlying WYSIWYM editing can 
be presented by means of a simple diagram. 
W~- " update ,~'(!llt't'{ll(! 
Feedback text with anchors 
\[ 
/ 
/ 
. ~  select, paste, VIeW cut, copy 
Figure 3: The editing cycle 
11SCC also:  
http://www.itri.l~righton.ac.uk/resea,'ch.htnfl:ff:WYSlWYM 
611 
Figure 3. represents the editing cycle. Given a 
Semantic Network (SN) in a knowledge base (KB), 
the system generates a description of the SN in the 
form of a 'feedback text' containing 'auchors' rep- 
reseuting places where the knowledge base can be 
extended. Each anchor is associated with pop-up 
menus, which present he possible editing opera- 
tions on the SN. On the basis of the operation that 
the user selects, the knowledge base is updated and 
a new feedback text is generated from the slew con- 
tents of the SN. 
? conjunction 
fitted_with ?/~- -~ ? conjunction 
carrierS?lid bulkl~ bilge'~l 
? pump ? 
v 
1 3 
? purpose 
states 
equipment fire. fighting 
Figure 4: Network underlying (11) 
Let us slow go through an example of editing plurals 
as it is supported by our prototype system. Let us 
join in at a point where the network in figure 4 has 
been constructed. 12This network is presented to the 
user by means of the following feedback text: 
( l l )  A solid bulk carries" is fitted with three bilge 
pumps. Some equipment is used fox" firefight- 
ing. Some states. 
copy 
copy some 
cut 
Figure 5: Pop-up menu on 'three bilge pumps' 
The spans in bold face indicate where tile network is 
still incomplete. Other spans of text represent spe- 
cific objects in the network. For instance, the span 
'three bilge pumps' is associated with a plural ob- 
ject of the type 'bilge pump'. When the user clicks 
12In order to keep the example transparent, not all informa- 
tion in the network has been represented. Attribute names on 
the edges, attributes without a value which arc not expressed in 
the feedback text and the mapping fi'om objects to their logical 
contexts have been ommited. 
on this span, tile menu of Figure 5. pops up. Let 
us assume that the user selects 'copy'. In that case, 
the object which is associated with the span is saved 
in a buffer. Subsequently, the user can click on the 
span 'Some equipment'. This causes tile following 
menu to pop up: 
I insert new 
paste 
Now, file user can paste the object from tile buffer 
into tlle location in tile network which is associated 
with 'Some equipment'. This gives rise to the net- 
work in figure 6 and the following feedback text: 
(12) A solid bulk carrier is fitted with three bilge 
pumps. They ax'e used for firefighting. Some 
states, 
? conjunction 
fittedwith ? ? conjunction 
carrierS?lid bulk/~ bilge Zl ? ~purpose 
pump ? Z.. states 
fire_fighting 
v 
1 3 
Figure 6: Network underlying (12) 
Note that now tile first attributes of both 'fitted_with' 
aud 'purpose' point to the same object. In the feed- 
back text, this is expressed by using a pronoun for 
the second reference to the object. 
Van Deemter and Power (1998) originally defined 
the 'copy' operation for singular objects. When we 
move to plurals, alternatives to a simple copy op- 
eration become available. Here, we want to dis- 
cuss one of those operations, i.e., copying part of 
an object, instead of the entire object. Let us return 
to (l 1). Suppose that the user had chosen 'copy 
some' on the menu of Figure 5. The effect would 
have been that a new object would have been cre- 
ated in the buffer with its attribute 'part_of' pointing 
to the object conesponding to 'three bilge pumps' 
(its 'quant' attribute would still have to be filled 
ill). Pasting this object into tile location marked by 
'Some equipment' would have yielded the follow- 
ing result: 
612 
(13) A solid bulk carrier is fitted with three bilge 
pumps. Some number  of  them is used for fire- 
lighting. Some states. 
Note that the text contains an anchor for the yet to 
be specified value of  the 'quant'  attribute. Clicking 
on the anchor activates the fol lowing menu: 
Selection of  'one'  yields the fol lowing text, which 
is generated from the network in Figure 7: 
(14) A solid bulk carrier is fitted with three bilge 
pumps. One of them is used for firefighting. 
Some states. 
* conjunction 
fitted with ? 
/ f 
solid bulk / 
carrier 
V 
1 
-k ? conjunction 
, ,  bilge J-- 
bi lge, pumpz~ ?purp?se \[& 
pump~ Z. ~,~..o~ 4 I 
states 
? fire_fighting 
t 
0 
3 
Figure 7: Network underlying (14) 
4 Conc lus ions  
In this papel, we have described some editing oper- 
ations on object-oriented networks with plural ob- 
jects and provided a Deeise formal interpretation 
for these networks in terms of  a version of  Dis- 
course Representation Theory. The networks which 
we have used are an extension of  commonly  used 
oo  networks for natural language generation. In 
particulm, our networks cover quantificational plu- 
ral noun phrases such 'most  N' ,  ' few N' ,  etc. 
Acknowledgements The research reported in this 
paper was carried out as part of the EC Esprit funded 
CLIME proiect (EP 25.414). Thanks are due to Lynne 
Cahill, Roger Evans and Neil Tipper for stimulating co- 
of, eration within the CLIME temn at the University of 
Brighton. Furthermore, i would like to thank Alexander 
Boer, Kces vail Deemtcr, Rodger Kibble, Richard Power 
and two anonynlous COLING reviewers for commenting 
on earlier versions of tiffs paper. 
References 
Barwise, J. & P,. Cooper (I 981), Generalized Quantifiers and 
Natural Language, in: Linguistics and Philosophy 4. 
Dale, R. (1992), Generating Referring Expressions, M1T 
Press, Cambridge. 
Kamp, 1t. & U. Reyle (1993), From Discourse to Logic, 
Kluwer Academic Publishers, Dordrecht. 
Keenan, E. & D. Westcrstfihl (1997), Generalized quantifiers 
in linguistics and logic, ill: Van 13enthem, J. & A. ter 
Meulen, Handbook of Logic and Language, Elsevim, 
Amsterdam, 837-894. 
Kibble, R., P,. Power & K. van l)eemtcr (1999), Editing logi- 
cally complex discourse meanings, in: Proceedings of 
1WCS II1, 147-162. 
Kmhmel; E. & K. van l)ccmter (1998), 'On the Interpretation 
of Anaphoric Noun Phrases'. in: Journal of Semantics, 
15 (3/4), 355-392. 
l,andman, F. (1989), G,'oups I and II. in: Linguistics and Phi- 
losophy, 12, 559-605,723-744. 
Lappin, S. (1988), The Semantics of 'Many' as a Weak Deter- 
miner, in: Linguistics, 26, 1021-1037. 
Link, G. (1983), The Logical Analysis of l)lurals and Mass 
Terms. in: Baeucrle, R., C. Schwarze & A. yon Stcchow 
(eds.), de Gmyter, P, erlin/New York, 303-323. 
Parsons, T. (1990), Events in the Semantics of English. The 
M1T Press, Cambridge, Massachusetts. 
Piwek, P. (1997), Accent Interpretation, Anaphora P, csolution 
and hnplicature l)erivation, in: The Proceedings of lhe 
1 l th Amsterdmn Colloquium, University of Amsterdam, 
55-60. 
Piwek, P., R. Ewms, L. Cahill & N. Tippe, (2000), Natural 
Language Generation in the MILl- System, in: Pro- 
ceedings of the IMPACTS in NLG Workshol), Schloss 
t)agstuhl, Germany. 
Powe,', R., 1). Scott and P,. Evans (1998), What Yott See Is 
What You Meant, Proceedings of ECAI 98, \]h'ighton, 
UK. 
Powcl; R. (1999), Controlling logical scope in text generation, 
Proceedings of the European Workshop on Natural Lan- 
guage Generation, Toulouse, France. 
P, eitct; E. & P,. Dale (2000), Bt, ilding Natural Language Gen- 
eration Systems, Cambridge University Press, Cam- 
bridge. 
Sowa, J. (19841, Conceptual Structures, Addison Wesley, 
Reading, Massachusetts. 
Stone, M. (1999), Describing Sets with Covers and Sets of Of  
dinary Assigmnents, in: Proe. of The Generation Nomi- 
nal Expressions, workshop associated with ESSLLI 99. 
Van l)eemter, K. and R. Power (1998), Corefe,'enee in knowl- 
edge editing, in: Proceedings of the COLING-ACL 
workshop on the Computational Treatment of .Nominals, 
Montreal Canada, 56-60. 
Van Eijck, J. (1983), l)iscourse Representation Theory and 
l)lurality. In: ter Meulen, A. (ed.) Studies in Modeltheo- 
relic Semantics, Foris, GRASS-I. 
Westcrstfihl, D. (1985), l)eterminers and Context Sets. in: J. 
van Benthem and A. ter Meulen (eds.) Generalized 
Quantiliers in Natural Language, Foris, GRASS-4. 
613 
151
152
153
154
A Flexible Pragmatics-driven Language Generator for Animated Agents
Paul Piwek
ITRI ? Information Technology Research Institute
University of Brighton
Paul.Piwek@itri.bton.ac.uk
Abstract
This paper describes the NECA MNLG;
a fully implemented Multimodal Natu-
ral Language Generation module. The
MNLG is deployed as part of the NECA
system which generates dialogues be-
tween animated agents. The genera-
tion module supports the seamless inte-
gration of full grammar rules, templates
and canned text. The generator takes in-
put which allows for the specification of
syntactic, semantic and pragmatic con-
straints on the output.
1 Introduction
This paper introduces the NECA MNLG; a Multi-
modal Natural Language Generator. It has been
developed in the context of the NECA system.1
The NECA system generates dialogue scripts for
animated characters. A first demonstrator in the
car sales domain (ESHOWROOM) has been imple-
mented. It allows a user to browse a database of
cars, select a car, select two characters and their
attributes, and subsequently view an automatically
generated film of a dialogue about the selected car.
The demonstrator takes the following input:
? A database with facts about the selected car (maximum
speed, horse power, etc.).
? A database which correlates facts with value judge-
ments.
1NECA stands for ?Net Environment for Embodied Emo-
tional Conversational Agents? and is an EU-IST project.
? Information about the characters: 1. Personality traits
such as extroversion and agreeableness. 2. Personal
preferences concerning cars (e.g., a preference for safe
cars). 3. Role of the character (seller or customer).
This input is processed in a pipeline that consists
of the following modules in this order:
? A DIALOGUE PLANNER, which produces an abstract
description of the dialogue (the dialogue plan).
? A MULTI-MODAL NATURAL LANGUAGE GENERA-
TOR which specifies linguistic and non-linguistic real-
izations for the dialogue acts in the dialogue plan.
? A SPEECH SYNTHESIS MODULE, which adds infor-
mation for Speech.
? A GESTURE ASSIGNMENT MODULE, which controls
the temporal coordination of gestures and speech.
? A PLAYER, which plays the animated characters and
the corresponding speech sound files.
Each step in the pipeline adds more concrete in-
formation to the dialogue plan/script until finally
a player can render it. A single XML compliant
representation language, called RRL, has been de-
veloped for representing the Dialogue Script at its
various stages of completion (Piwek et al, 2002).
In this paper, we describe the requirements for
the NECA MNLG, how these have been translated
into design solutions and finally some of aspects
of the implementation.
2 Requirements
The requirements in this section derive primarly
from the use case of the NECA system. We do,
however, try to indicate in what respects these re-
quirements transcend this specific application and
are desirable for generation systems in general.
REQUIREMENT 1: The linguistic resources of the gen-
erator should support seamless integration of canned text,
templates and full grammar rules.
In the NECA system, the dialogue planner creates
a dialogue plan consisting of (1) a description
of the participants, (2) a characterization of the
common ground at the outset of the dialogue in
terms of Discourse Representation Theory (Kamp
and Reyle, 1993) and (3) a set of dialogue acts
and their temporal ordering. For each dialogue
act, the type, speaker, set of addressees, semantic
content, what it is a reaction to (i.e., its rhetorical
relation to other dialogue acts), and emotions
of the speaker can be specified. The amount of
information which the dialogue planner actually
provides for each of these attributes varies,
however, per dialogue act: for some dialogue acts,
a full semantic content can be provided ?in the
form of a Discourse Representation Structure?
whereas for other acts, no semantic content is
available at all. Typically, the dialogue planner
can provide detailed semantics for utterances
whose content is covered by the domain model
(e.g., the car domain) whereas this is not possible
for utterances which play an important role in the
conversation but are not part of the domain model
(e.g., greetings). This state of affairs is shared
with most real-world applications.
Since generation by grammar rules is primarily
driven by the input semantics, for certain dialogue
acts full grammar rules cannot be used. These
dialogue acts may be primarily characterized in
terms of their, possibly domain specific, dialogue
act type (greeting, refusal, etc.). Thus, we need
a generator which can cope with both types of
input, and map them to the appropriate output.
Input with little or no semantic content can typ-
ically be dealt with through templates or canned
text, whereas input with fully specified semantic
content can be dealt with through proper grammar
rules. Summarizing, we need a generator which
can cope with (linguistic) resources that contain
an arbritary combination of grammar rules,
templates and canned text.
REQUIREMENT 2: The generator should allow for
combinations of different types of constraints on its the out-
put, such as syntactic, semantic and pragmatic constraints
In the NECA project the aim is to generate
behaviour for animated agents which simulates
affective situated face-to-face conversational
interaction. This means that the utterances of the
agents have to be adapted not only to the content
of the information which is exchanged but also to
many other properties of the interlocutors, such as
their emotional state, gender, cultural background,
etc. The generator should therefore allow for such
parameters to be part of its input.
REQUIREMENT 3: The generator should be sufficiently fast
to be of use in real-world applications
The application in which our generator is
being used is currently fielded as part of a net-
environment. The application will be evaluated
with users through online questionnaires which
are integrated in the application and analysis of
log files (to answer questions such as ?Do users
try different settings of the application??, etc. See
Krenn et al, 2002). Therefore, the generator will
have to be fast in order for it not to negatively
affect the user experience of the system.
3 Design Solutions
The NECA MNLG adopts the conventional pipeline
architecture for generators (Reiter and Dale,
2000). Its input is a RRL dialogue plan. This
is parsed and internally represented as a PROFIT
typed feature structure (Erbach, 1995). Subse-
quently, the dialogue acts in the plan are realized
in accordance with their temporal order. For each
act, first a deep syntactic structure is generated.
The deep structure of referring expressions is dealt
with in a separate module, which takes the com-
mon ground of the interlocutors into account. Sub-
sequently, lexical realization (agreement, inflec-
tion) and punctuation is performed. Finally, turn-
taking gestures are added and the output is mapped
back into the RRL XML format.
Here let us concentrate on our approach to the
generation of deep syntactic structure and how it
satisfies the first two requirements. The input to
the MNLG is a node (i.e., feature structure) stipu-
lating the syntactic type of the output (e.g., sen-
tence: <s), semantics and further information on
the current dialogue act in PROFIT:2
(<s &
sem!drs([c_27],
[type(c_27,prestigious),
arg1(c_27,x_1)])&
currentAct!speaker!
(name!john &
polite!yes & ...)
)
Thus various types of information are combined
within one input node. Generation consists of tak-
ing the input node and using it to create a tree
representation of the output. For this purpose,
the MNLG tries to match the input node with the
mother node of one of the trees in its tree repos-
itory. This tree repository contains trees which
can represent proper grammar rules, templates and
canned text. Matching trees might in turn have in-
complete daughter nodes. These are recursively
expanded by matching them with the trees in the
repository, until all daughters are complete.
A daughter node is complete if it is lexically
realized (i.e., the attribute form of the node has
a value) or it is of the type <np and the seman-
tics is an open variable. In the latter instance, the
node is expanded in a separate step by the refer-
ring expressions generation module. This module
finds the discourse referent in the common ground
which binds the open variable and constructs a de-
scription of the object in question. The descrip-
tion is composed of the properties which the ob-
ject has according to the common ground, but can
also be empty if the object is highly salient. The
module is based on the work of Krahmer and The-
une (2002). The (empty) description is mapped
to a deep syntactic structure using the tree repos-
itory. Lexicalization subsequently yields expres-
sions such as ?it? (empty descriptive content) or,
for instance, ?the red car?.
Let us return to the tree repository and illus-
trate how templates and rules can be represented
uniformly. The representation of a tree is of the
2That is, PROLOG with some sugaring for the rep-
resentation of feature structures. Feature structures are
also used in the FUF/SURGE generator. It is different
from the NECA MNLG in that it takes as input thematic
trees with content words. Furthermore, it allows for con-
trol annotations in the grammar and uses a special inter-
preter for unification, rather than directly PROLOG. See
http://www.cs.bgu.ac.il/surge/.
form (Node,[Tree1,Tree2,...]), where
the list of trees can be empty, yielding a tree con-
sisting of one node: (Node,[]). The following
is a template for dialogue acts of type greeting
with no semantic content and a polite speaker.
(<s &
currentAct!
(type!greeting &
speaker!polite!"yes" &
speaker!name!Speaker) &
sem!"none",
[(<s & form!"hello!",[]),
(<fragment &
form!"My name is",[]),
(<np &
sem!concept(Speaker),[])
]).
This is a template for the text ?Hello! My name is
SPEAKER?. Where SPEAKER is a variable which
is bound to the name of the speaker of the utter-
ance. The noun phrase (<np) for this name is gen-
erated by the referring expression generation mod-
ule. The following is a tree representing a gram-
mar rule of the familiar type S ? NP VP:
(<s &
currentAct!type!statement &
currentAct!CA &
argGap!ArgGap &
auxGap!AuxGap &
sem!drs(_,[negation(
drs(_,
[type(E,Type)
arg1(E,X)|R]))]
),
[(<np &
currentAct!CA &
sem!X,[]),
(<vp &
argGap!ArgGap &
auxGap!AuxGap &
negated!<true &
sem!drs(_,[type(E,Type)|R]) &
currentAct!CA,_)
]).
Note that this rule applies to an input node whose
semantic content contains a negation. The nega-
tion is passed on to the VP subtree via the feature
negated. The attributes argGap and auxGap
allow us to capture unbounded dependencies via
feature perlocation. Our use of trees is related to
the Tree Adjoining Grammar approach to genera-
tion (e.g., Stone and Doran, 1997).3
3Their generation algorithm is, however, very different
from the one proposed here. Whereas they propose an in-
tegrated planning approach, we advocate a very modular sys-
The value of the attribute currentAct is
passed on from the mother node to the daughter
nodes. Thus any pragmatic information (personal-
ity, politeness, emotion, etc.) is passed on through
the tree and can be accessed at a later stage, for
instance, when lexical items are selected.
4 Implementation
The NECA MNLG has been implemented in PRO-
LOG. The output is in the form of an RRL XML
document. Table 1 provides a sample of the re-
sponse times of the compiled code running on a
Pentium III Mobile 1200 Mhz with Sicstus 3.8.5
PROLOG. We timed the complete generation pro-
cess from parsing the XML input to producing
XML output, including generation of deep syn-
tactic structure, referring expressions, turn taking
gestures (not discussed in this paper), etc.
input # acts = 1 ? 10
A 19 0.230s 0.741s
B 22 0.290s 0.872s
C 23 0.290s 0.801s
D 31 0.431s 1.372s
Table 1: Response Times of the MNLG
The results show generation times for entire di-
alogues and according to whether the generator
was asked to produce exactly one solution or se-
lect at random a solution from a set of at most ten
generated solutions (the latter strategy was imple-
mented to obtain more variation in the generator
output). On average for = 1 the generation time
for an individual dialogue act is almost 1100 of a
second. For ? 10 it is 4100 of a second. The
generator uses a repository of 138 trees (includ-
ing the two examples given above). The repos-
itory has been developed for and integrated into
the ESHOWROOM system which is currently be-
ing fielded. A start is being made with porting the
MNLG to a new domain and documentation is be-
ing created to allow our project partners to carry
out this task. We hope that our efforts will con-
tribute to addressing a challenge expressed in (Re-
tem, supporting fast generation. Moreover, by using features
for unbounded dependencies we do not require the adjunction
operation, which is incompatible with our topdown genera-
tion approach. We follow Nicolov et al (1996), who also use
TAG, in their commitment to flat semantics. Their generator
does, however, not take pragmatic constraints into account.
iter, 1999): ?We hope that future systems such as
STOP will be able to make more use of deep tech-
niques, because of advances in linguistics and the
development of reusable wide-coverage NLG com-
ponents that are robust, well-documented and well
engineered as software artifacts.?
In our view the best way to approach this goal
is by providing a framework which allows for the
flexible integration of shallow and deep genera-
tion, thus making it possible that in the course of
various projects, deep analyses can be developed
alongside the shallow solutions which are diffi-
cult to avoid altogether in software development
projects, due to the pressure to deliver a complete
system within a certain span of time.
Acknowledgements
This research is supported by the EU Project NECA
IST-2000-28580. For comments and discussion
thanks are due the EACL reviewers and my col-
leagues in the NECA project.
References
Gregor Erbach. 1995. PROFIT 1.54 user?s guide. University
of the Saarland, December 3, 1995.
Hans Kamp and Uwe Reyle. 1993. From Discourse to
Logic. Kluwer, Dordrecht.
Emiel Krahmer and Marie?t Theune. 2002. Efficient context-
sensitive generation of referring expressions. In: Kees
Van Deemter and Rodger Kibble (eds.), Information
Sharing, CSLI, Stanford.
Brigitte Krenn, Erich Gstrein, Barbara Neumayr and Mar-
tine Grice. 2002. What can we learn from users
of avatars in net environments?. In: Proc. of the
AAMAS workshop ?Embodied conversational agents -
let?s specify and evaluate them!?, Bologna, Italy.
Nicholas Nicolov, Chris Mellish & Graeme Ritchie. 1996.
Approximate Generation from Non-Hierarchical Rep-
resentattions, Proc. 8th International Workshop on
Natural Language Generation, Herstmonceux Castle,
UK.
Paul Piwek, Brigitte Krenn, Marc Schro?der, Martine Grice,
Stefan Baumann and Hannes Pirker. 2002. RRL: A
Rich Representation Language for the Description of
Agent Behaviour in NECA. Proc. of the AAMAS work-
shop ?Embodied conversational agents - let?s specify
and evaluate them!?, Bologna, Italy.
Ehud Reiter. 1999. Shallow vs. Deep Techniques for han-
dling Linguistic Constraints and Optimisations. Proc.
of KI-99 Workshop ?May I speak freely?.
Ehud Reiter and Robert Dale. 2000. Building natural
language generation systems. Cambridge University
Press, Cambridge.
Matthew Stone and Christy Doran. 1997. Sentence Plan-
ning as Description Using Tree-Adjoining Grammar.
Proc. ACL 1997, Madrid, Spain.
The ALLIGATOR Theorem Prover for
Dependent Type Systems: Description and
Proof Sample
Paul Piwek
Centre for Research in Computing
The Open University, Milton Keynes, UK
p.piwek@open.ac.uk
Abstract
This paper introduces the Alligator theorem prover for Dependent Type Systems (dts).
We start with highlighting a number of properties of dts that make them specifically suited
for computational semantics. We then briefly introduce dts and our implementation. The
paper concludes with an example of a dts proof that illustrates the suitability of dts for
modelling anaphora resolution.
1 Introduction
Automated symbolic inference requires a formal language as the substratum for
reasoning. Blackburn and Bos ([7]) make a good case for the use of First Order
Predicate Logic (fopl) in computational semantics, citing both practical (availabil-
ity of high performance theorem provers and to a lesser extent model builders) and
theoretical reasons (they discuss a range of interesting phenomena which can be
dealt with in fopl).
We agree with the idea that fopl is a good starting point, but also think that for
computational semantics to develop further as a field, extensions going beyond fopl
should be actively explored. In this paper, a research tool is described that takes
such explorations in one particular direction. The tool ? alligator ? is a theorem
prover for Dependent Type Systems (dts) [4,5]. The Sicstus Prolog source code
of this prover is available, free of charge, for research purposes ([18]). dts are an
attractive option for computational semantics for a number of reasons:
(i) Dynamic potential (cf. [15]): The notion of a context that is built up incre-
mentally is inherent to dts.
(ii) Flexibility: By varying a limited number of parameters, it is possible to
switch from, for example, propositional to predicate logic, or first order to
higher order logics. Additionally, although the basic underlying logic is con-
structive, dts allows for the flexible use of axioms to regain full classical
logic, or more fine-grained alternatives. For example, it is possible to specify
for individual predicates whether they are bivalent.
(iii) Extensibility: A dts-context includes what is known as the signature in fopl.
Consequently, the signature can be extended incrementally, making it possible
to model the acquisition of new concepts by language users.
(iv) Proof-objects: In dts, Gentzen-style natural deduction proofs are first-class
citizens. This gives us the following advantages: (a) Reliability: It allows us
to heed the de Bruijn criterion for reliable proof systems: ?A proof assistant
satisfies the de Bruijn criterion if it generates ?proof-objects? (of some form)
that can be checked by an easy algorithm.? (cited from [5]) (b) Naturalness:
dts proofs correspond with natural deduction proofs. This is of interest if one
is concerned with models of human reasoning in natural language understand-
ing. In psychology, some schools of thought argue that natural deduction is a
good approximation of human reasoning (see, e.g., [21]). (c) Relevance: Proof
objects can help to identify proofs which are valid but spurious in the sense
that they do not really consume their premises (see [14]). (d) Justification of
behaviour: Explicit proof objects provide direct access to the justifications
that an agent has for the conclusions and the interpretations that it constructs.
This is particularly useful for dialogue agents that need to respond to utter-
ances of other agents. Such responses can themselves again be queried, for
example, through clarificatory questions (cf. [22]) and why questions (A:p, B:
no, ?p, A: Why ?p?). In order to respond appropriately, the agent needs to ac-
cess its own background knowledge and how it was used to draw conclusions.
dts proof objects provide a compact representation of this information.
(v) Applications: dts-style analyses exist for a wide range of linguistic phe-
nomena including donkey sentences ([23]), anaphoric expressions and tem-
poral reference ([20]), belief revision ([8]), bridging anaphora ([19]), clar-
ification ellipsis ([10]), metonymy ([9]), inter-agent communication, knowl-
edge and observation ([1]), ontological reasoning for feedback dialogues ([6]),
and human-machine dialogue ([2]). Additionally, there is research on relating
dts proof-theoretic natural language semantics to model-theoretic approaches
([12]), and there are studies employing the related formalism of labelled de-
duction to natural language semantics ([16]). In 2005, the 2nd Workshop on
Lambda-Calculus, Type Theory, and Natural Language took place at King?s
College London ([11]).
We concede that none of the properties we have listed is on its own unique to dts.
However, to the best of our knowledge, no extant logical calculus combines all
these properties in a single system with well-understood meta-mathematical prop-
erties (dts play a central role in theoretical computer science, see [4]).
2 Dependent Type Systems
dts come in a wide variety of flavours and variations. All these systems share,
however, two features: a typing system and a notion of dependency. Firstly, dts are
type systems. That is, given a set of assumptions ?, also known as the context, they
provide rules for determining whether a particular object, say a, belongs to a given
type, say t. We write ? ? a : t, if, given the context ?, a is of type t, i.e., a inhabits
type t. The objects that are classified using type systems are (normalizing) terms
of the ?-calculus. ? is a sequence of statements x1 : t1, . . . , xn : tn (with n ? 0).
Dependency is the second feature of dts, and it comes in two forms. First, there
is dependency between statements in the context: in order to use a type tk to classify
an object xk, this type tk needs to have been introduced in that part of the context
that precedes it or tk has to be a sort. In other words, tk can only be used if (1) it
itself inhabits a type or can be constructed from other types that are available in the
context preceding it, or (2) it belongs to a fixed and usually small set of designated
types that are called sorts. Because sorts need no preceding context, they make it
possible to keep contexts finite.
Second, there is a variety of dependency that occurs inside types. Since type
systems are used to classify terms of the ?-calculus, they can also deal with func-
tions. A function f from objects of type t1 to objects of type t2 inhabits the function
type t1 ? t2. Dependent function types are a generalization of function types: a
dependent function type is a function type where the range of the function changes
depending on the object to which the function is applied. The notation for depen-
dent function types is ?x : A.B (we also use our own alternative ?arrow notation?:
[x : A] ? B). If we apply an inhabitant of this function type, say f , to an object of
type A, then the resulting object fa (f applied to a) is of type B, but with all free
occurrences of x in B substituted with a (that is, the type of fa is B[x := a]).
One way to make the leap from type systems to logic is as follows. From a log-
ical point of view, we are interested in propositions as the constituents of deductive
arguments. In classical logic, one focuses on judgements of the following form:
the truth of proposition q follows/can be derived from the truth of the propositions
p1, . . . , pn. We reason from the truth of the premises to the truth of the conclu-
sion. To do logic in a dts, we move from truth to proof: we, now, reason from the
proofs that we (assume to) have for the premises to a proof for the conclusion. In
other words, we are interested in judgements of the following form: a is proof of
proposition q follows/can be derived assuming that a1 is a proof of p1, a2 is a proof
of p2, . . ., and an is a proof pn. Such a judgement can be formalized in a dts as
a1 : p1, . . . , an : pn ? a : p. Thus, we read a : p as ?a is a proof for p?. Thus, we
model proofs as (?-calculus) terms and propositions as (a certain class of) types in
dts. This is known as the Curry-Howard-de Bruijn embedding.
The embedding is grounded in the Brouwer-Heyting-Kolmogorov interpreta-
tion of proofs as constructions; e.g., a proof for a conditional p ? q is identified
with a method that transforms a proof of p into a proof for q. In a dts, this is for-
malized by modelling the proof f for a type p ? q as a function from objects of
type p to objects of type q, such that if a is a proof of p, then f applied to a is a
proof of q (i.e., fa : q). Universal quantification is dealt with along the same lines.
In a dts, the counterpart for universal quantification is the dependent function type.
In particular, ?x ? A : P (x) becomes (?x : A.Px). A proof for this type is a
function f which, given any object a : A, returns the proof fa for Pa.
Pure Type Systems (pts; [4]) are of particular interest, because of their gen-
erality. With a small number of parameters, pts can be tailored to match a wide
variety of dts. Alligator implements an extension of pts with ? types. ? types
are also known as dependent product types and can be used to model ? and ?.
3 System Architecture, Implementation and Proof Sample
There is no room for a detailed description of the system here, for that we refer to
the documentation and code available at [18]. What we can offer is, firstly, a list of
differences between alligator and other dts provers: (a) alligator directly con-
structs proof objects for natural deduction proofs. Other provers for dts typically
work with internal representations that are only at the end of the reasoning process
translated to natural deduction proof objects. For example, Cocktail ([13]) uses
tableaux and translates these, whereas tps ([3]) is based on the mating method.
The handbook chapter by Barendregt and Geuvers on proof assistants for dts ([5])
lists a number of further automated theorem provers, none of which works directly
with proof objects. (b) alligator was not developed with mathematical/program
specification reasoning in mind, but rather for inferences in language interpretation.
As a consequence, it has been streamlined to link up with notation and function-
ality relevant to computational semantics (specifically, allowing for notation which
is close to [15] and omission of inductive types). (3) To the best of our knowl-
edge, alligator is the only automated theorem prover which directly conforms to
the specification of Pure Type Systems ([4]), the most general and flexible kind of
dts (most dts can be emulated in pts; see [17] for an overview of dts and their
counterparts in pts).
Alligator 1.0 has been implemented in Sicstus Prolog and been tested with
version 3.12.2 of Sicstus. An overview of the architecture is presented in Figure
1.a. Note that the system applies both forward and backward inferencing. Most
of the forward inferencing takes place before backward inferencing (though some
backward inferencing rules do also have forward inferencing component). Reduc-
tion of terms is also carried out mainly before backward inferencing. Inferencing
is done with a flattened representations of dts terms (the arrow notation). Proofs
are checked at the end of the inferencing process for their correctness (the code for
proof checking is separate from the theorem proving code).
Currently, alligator has the status of an experimental research tool. It is in-
tended for testing computational solutions to theoretically challenging problems in
computational semantics. Scalability has, so far, not been given much attention,
though it will obviously need to be addressed if the system is to be used in large-
scale practical applications. Currently, the system is merely intended as a baseline
and starting point for implementing efficient and effective proof search heuristics.
We now conclude with an example of the use of alligator.
The discourse ?The barn contains a chain saw or a power drill. It . . .? (p. 205 of
[15]) poses a problem for the structural approach to anaphora resolution proposed in
Fig. 1. Overview of Alligator 1.0 Architecture (a) and graphical representation of a
proof-object (b)
[15]: the first sentence does not directly introduce an accessible discourse referent
that can bind the pronoun ?It?. Rather, an object (something that is either a power
drill or a chain saw) needs to be inferred from a disjunction. An inferential account
of anaphora resolution that can deal with such cases is presented in [19]. In a
nutshell, the idea is that an anaphoric expression triggers a proof goal that needs to
be filled with a proof from the context.
Alligator can construct an antecedent for ?It? from the context (for lack of
space, our formalization is in propositional logic). Firstly, the relevant proposi-
tions need to be available: false:prop (false is a proposition), p:prop (there is a
chain saw is a proposition), q:prop (there is a power drill is a proposition), u:prop
(there is something is a proposition). Given these propositions, we can now in-
troduce background information such as a1:p?u (roughly, chain saws are things)
and a2:q?u (power drills are things). Although dts are constructive, we can en-
gage in classical reasoning by including the double negation rule in our background
knowledge: dn pr:([P:prop]?(((P?false)?false)?P)). It involves higher or-
der quantification over all propositions P. Finally, assume that the context has
been updated with the information expressed by the sentence ?The barn contains
a chain saw or a power drill?, with disjunction modelled using implication and
negation: a3:(p?false)?q. Given this information, alligator can generate the
proof/antecedent for ?It? (modelled here as u) given in Figure 1.b.
Acknowledgements We would like to thank the anonymous reviewers of ICoS-5 for their
helpful comments and suggestions.
References
[1] Ahn, R., ?Agents, Objects and Events: A computational approach to knowledge, observation
and communication,? Ph.D. thesis, Eindhoven University of Technology (2001).
[2] Ahn, R., R. Beun, T. Borghuis, H. Bunt and C. van Overveld, The DenK-architecture: a
fundamental approach to user-interfaces, Artificial Intelligence Review Journal 8 (1994),
pp. 431?445.
[3] Andrews, P., M. Bishop, S. Issar, D. Nesmith, F. Pfennig and H. Xi, TPS: A Theorem-Proving
System for Classical Type Theory, Journal of Automated Reasoning 16 (1996), pp. 321?353.
[4] Barendregt, H., Lambda Calculi with Types, in: Handbook of Logic in Computer Science, 2,
Clarendon Press, Oxford, 1992 .
[5] Barendregt, H. and H. Geuvers, Proof-assistants using Dependent Type Systems, in: Handbook
of Automated Reasoning, Elsevier, 2001.
[6] Beun, R., R. van Eijk and H. Pru?st, Ontological Feedback in Multiagent Systems, in:
N. Jennings, C. Sierra, L. Sonenberg and M. Tambe, editors, International Joint Conference
on Autonomous Agents and Multiagent Systems (2004), pp. 110?117.
[7] Blackburn, P. and J. Bos, Computational Semantics, Theoria 18 (2003), pp. 27?45.
[8] Borghuis, T. and R. Nederpelt, Belief Revision with Explicit Justifications: An Exploration in
Type Theory, CS-Report 00-17, Eindhoven University of Technology (2000).
[9] Bunt, H. and L. Kievit, Agent-dependent metonymy in a context-change model of
communication, in: Computing Meaning II, Studies in Linguistics and Philosophy 77, Kluwer
Academic Publishers, Dordrecht, 2001 pp. 75?95.
[10] Cooper, R. and J. Ginzburg, Clarification ellipsis in dependent type theory, in: J. Bos
and C. Matheson, editors, Proceedings of Edilog,the 6th Workshop on the Semantics and
Pragmatics of Dialogue, University of Edinburgh, 2002.
[11] Ferna?ndez, M., S. Lappin and C. Fox, editors, ?Procs. of Lambda Calculus, Type Theory and
Natural Language Workshop,? King?s College, London, 2005.
[12] Fernando, T., A type reduction from proof-conditional to dynamic semantics, Journal of
Philosophical Logic (2001), pp. 121?153.
[13] Franssen, M. and H. de Swart, Cocktail: A Tool for Deriving Correct Programs, Rev. R. Acad.
Cien. Serie A. Mat. 98 (2004), pp. 95?111.
[14] Helman, G., ?Restrictions on Lambda Abstraction and the Interpretation of Some Non-
Classical Logics,? Ph.D. thesis, University of Pittsburgh (1977).
[15] Kamp, H. and U. Reyle, ?From Discourse to Logic,? Kluwer Academic Publishers, Dordrecht,
1993.
[16] Kempson, R., W. Meyer-Viol and D. Gabbay, Syntactic Computation as Labelled Deduction:
WH a case study, in: R. Borsley and I. Roberts, editors, Syntactic Categories, Academic Press,
2000.
[17] Laan, T., ?The Evolution of Type Theory in Logic and Mathematics,? Ph.D. thesis, Eindhoven
University of Technology (1997).
[18] Piwek, P., Alligator Theorem Prover Home Page, mcs.open.ac.uk/pp2464/alligator (2006).
[19] Piwek, P. and E. Krahmer, Presuppositions in Context: Constructing Bridges, in: P. Bonzon,
M. Cavalcanti and R. Nossum, editors, Formal Aspects of Context, Applied Logic Series 20,
Kluwer Academic Publishers, Dordrecht, 2000.
[20] Ranta, A., ?Type-Theoretical Grammar,? Clarendon Press, 1994.
[21] Rips, L., ?The Psychology of Proof: Deductive Reasoning in Human Thinking,? The MIT
Press, Cambridge, Massachusetts, 1994.
[22] Stone, M., Specifying Generation of Referring Expressions by Example, in: Procs AAAI Spring
Symposium on Natural Language Generation in Spoken and Written Dialogue, Stanford, 2003.
[23] Sundholm, G., Proof Theory and Meaning, in: Handbook of Philosophical Logic III, D.
Reidel, 1986 pp. 471?506.
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 333?336,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Generating Expository Dialogue from Monologue:
Motivation, Corpus and Preliminary Rules
Paul Piwek
Centre for Research in Computing
The Open University
Walton Hall, Milton Keynes, UK
p.piwek@open.ac.uk
Svetlana Stoyanchev
Centre for Research in Computing
The Open University
Walton Hall, Milton Keynes, UK
s.stoyanchev@open.ac.uk
Abstract
Generating expository dialogue from mono-
logue is a task that poses an interesting and re-
warding challenge for Natural Language Pro-
cessing. This short paper has three aims:
firstly, to motivate the importance of this
task, both in terms of the benefits of ex-
pository dialogue as a way to present in-
formation and in terms of potential applica-
tions; secondly, to introduce a parallel cor-
pus of monologues and dialogues which en-
ables a data-driven approach to this challenge;
and, finally, to describe work-in-progress on
semi-automatic construction of Monologue-
to-Dialogue (M2D) generation rules.
1 Introduction
The tasks of text generation ? e.g., Reiter et al
(2005) and Demir et al (2008) ? and generation
in dialogue ? e.g., Stent (2002) and DeVault et al
(2008) ? are central topics in Natural Language Gen-
eration (NLG). What sets the two tasks apart is the
interactive nature of dialogue, where participants
need to adapt their contributions to each other.
This paper introduces an NLG task, the genera-
tion of expository dialogue, to the Computational
Linguistics community which occupies the middle
ground between these two tasks. An expository di-
alogue is an authored conversation between two fic-
tive characters. It can be presented as text, audio or
film. Although there is no real-time interactivity, in
expository dialogue the contributions of the charac-
ters do need to mesh with each other. The main pur-
pose of expository dialogue is to present informa-
tion (a description, explanation or definition) to the
reader, hearer or viewer, in contrast with dramatic
dialogue, which tells a story.
The use of expository dialogue goes back as far as
Plato (c. 470-399 BC), who expressed his ideas as
dialogues between Socrates and his contemporaries.
Recently, a number of empirical studies show that
for some purposes expository dialogue has advan-
tages over monologue: for learners, dialogue can be
more memorable, stimulate them to formulate their
own questions (Craig et al, 2000), and get them to
talk with each other (Lee et al, 1998). Expository
dialogue has also been found to be more effective
for persuasion (Suzuki and Yamada, 2004).
Additionally, dialogue lends itself very well
for multimedia presentations by computer-animated
agents (Andre? et al, 2000; van Deemter et al,
2008). Potential application domains include ed-
ucation, (serious) games and E-Health. In educa-
tion, information from textbooks could be presented
in dialogue form, possibly using virtual reality plat-
forms such as Second Life. Automatically gener-
ating dialogue from text for non-player characters
could have a tremendous impact on the gaming in-
dustry; e.g., (IGDA Game Writers SIG, 2003) state
that the amount of dialogue script for a character-
driven computer game is usually many times that
for the average film. In connection with E-health,
consider patient information leaflets, which are of-
ten left unread; presenting them as movies between
a virtual pharmacist and client may help address this.
Thus instead of being presented with
(1) a. You can take aspirin,
b. if you have a headache.
333
c. Though aspirin does have side effects:
d. it can harm circulation.
the patient could watch a movie on their mobile de-
vice of an exchange between a virtual client (lay-
man, L) and pharmacist (expert, E):
(2) L: What if I have a headache?
E: You can take aspirin
L: But does it have side effects?
E: Yes, it can harm circulation.
So far, research on generating expository dialogue
has been firmly rooted in classical AI approaches.
Work in this area starts from knowledge represen-
tations or databases (Andre? et al, 2000), and even
research that does take text as input ? e.g., Piwek
et al (2007) describe a system for generating di-
alogues such as Example 2 ? relies on handcrafted
rules. Two challenges present themselves for NLP
research: 1) generation of expository dialogue from
text, and 2) use of data-driven, rather than manually
authored, generation rules.
Apart from the cost of manually authoring gener-
ation rules, previous research has found that human-
authored rules can result in ?too much information
[being] given too quickly? (Williams et al, 2007),
which can be addressed by conversational padding.
We argue that rather than trying to invent padding
rules, the best strategy is to learn rules automatically
from professionally authored dialogues.
2 The CODA Corpus
To make inroads into data-driven dialogue genera-
tion, we first need to have the necessary resources.
We propose to view Monologue-to-Dialogue (M2D)
generation as analogous to machine translation; con-
sequently we need a parallel corpus for learning
mappings from the source (monologue) to the tar-
get (dialogue) texts. In the ongoing CODA1 project
we have created such a corpus. It consists of profes-
sionally authored dialogues2 that have been aligned
with monologues (written by ourselves) expressing
the same information. Since our ultimate aim is to
generate dialogues that resemble those written by
1COherent Dialogue Automatically generated from text
2Most dialogues are from the Gutenberg library to facilitate
our planned release of the corpus to the research community.
Sp Dialog act Dialogue Turn Monologue
E: Complex
Question
When you have
a pain in your
foot, how do
you know it?
When you
have a pain in
your foot (i)
you know it
because you
L: Explain I feel it. can feel it. (ii)
E: Explain-
Contradict
But you do not
feel it until a
nerve reports
the hurt to the
brain.
But you do not
feel it until a
nerve reports
the hurt to the
brain. (iii)
E: YN-
Question
Yet the brain is
the seat of the
mind , is it not?
Yet the brain is
the seat of the
mind. (iv)
Table 1: Parallel Monologue and Dialogue Example from
Mark Twain?s ?What is Man??
acclaimed authors, we started with professionally
authored dialogues and created the corresponding
monologues. From a practical point of view, it was
more feasible to use existing dialogue by acclaimed
authors than to hire professional authors to write di-
alogue based on monologues.
We have annotated both dialogues and mono-
logues: dialogue with dialogue acts and monologue
with discourse relations.3 We achieved 91% agree-
ment on segmentation and kappa=.82 for dialogue
act annotation on 11 dialogue act tags. We devel-
oped a D2MTranslation tool for monologue author-
ing, segmentation and dialogue annotation.
In January 2010, the corpus included 500 turns
from ?What is man??, a dialogue by Mark Twain,
and 88 turns from ?Evolving Algebras?, an aca-
demic paper in the form of dialogue by Yuri Gure-
vich.4 Both of these expository dialogues present
conversation between an expert (Old Man in Twain
and Author in Gurevich) and a layman (Young Man
in Twain and Quisani in Gurevich). Table 1 shows
an example of a dialogue fragment, aligned mono-
logue and dialogue act annotations. The discourse
structure of the monologue is depicted in Figure 1.
Table 2 shows the distribution of the dialogue acts
between expert and layman. In both dialogues, the
3See (Stoyanchev and Piwek, 2010) for details.
4In addition to these dialogues we are working on a dialogue
by Berkeley (Three Dialogues between Hylas and Philonous)
and a selection of shorter fragments (for copyrights reasons) by
authors such as Douglas Hofstadter and Paul Feyerabend.
334
Figure 1: Discourse structure of the monologue in Table 1
most frequent dialogue act is Explain, where a char-
acter presents information (as a new idea or as a re-
sponse to another utterance). Also, in both dialogues
the layman asks more often for clarification than
the expert. The distribution over information re-
quests (yes/no, factoid, and complex questions) and
responses (yes, no, factoid) differs between the two
dialogues: in Twain?s dialogue, the expert mostly
requests information and the layman responds to re-
quests, whereas in Gurevich?s dialogue it is the other
way around.
The differences in style suggests that the M2D
mapping rules will be author or style-specific. By
applying M2D rules obtained from two different au-
thors (e.g., Twain and Gurevich) to the same text
(e.g., the aspirin example) we can generate two dif-
ferent dialogues. This will enable us to vary the pre-
sentation style of automatically generated dialogues.
Twain Gurevich
Tag Expert Layman Expert Layman
Explain 69 55 49 24
Clarify 1 15 0 6
Request 60 26 2 29
Response 14 43 9 0
Table 2: Dialogue act tag frequencies for expert and lay-
man in a sample of 250 turns from Twain and 88 turns
from Gurevich dialogues.
3 Rules
We automatically derive M2D rules from the aligned
discourse relations and dialogue acts in our parallel
corpus of monologues and dialogues. Table 3 shows
three rules generated from the parallel dialogue?
monologue fragment in Table 1. The first rule, R1,
is based on the complete discourse structure of the
monologue (i?iv), whereas R2 and R3 are based on
only a part of it: R2 is based on i?iii, whereas R3 is
based on i and ii. By generating rules from subtrees
of a discourse structure, we obtain several rules from
a single dialogue fragment in the corpus.
Condition Elaboration
b a dc
Contrast
Condition Elaboration
b a dc
Condition
b a
Contrast
c ? d
(1)
(2)
Elaboration
dc
Contrast
a ? b
(4)
(3)
Figure 2: Discourse structures of the monologue in Ex-
ample 1. a-b and c-d indicate a concatenation of two
clauses.
Let us illustrate the use of such rules by applying
them to Example 1 about aspirin. The relations be-
tween the clauses of the example are depicted in Fig-
ure 2 (1). To generate a dialogue, we apply a match-
ing M2D rule. Alternatively, we can first simplify
the discourse structure of the monologue by remov-
ing relation nodes as illustrated in Figure 2 (2?4).
The simplified structure in Figure 2 (2) matches
rule R2 from Table 3. By applying R2 we gener-
ate the dialogue in Table 4: the expert asks a com-
plex question composed of clauses a and b, which
the layman answers with an explanation generated
from the same set of clauses. Then the expert offers
a contradicting explanation generated from c and d.
To generate dialogue sentences for a corresponding
discourse structure we are adapting the approach to
paraphrasing of Barzilay and McKeown (2001).
4 Conclusion
This short paper presented three angles on the
Monologue-to-Dialogue (M2D) task. First, as an
opinion piece, it motivates the task of generating ex-
pository dialogue from monologue. We described
empirical research that provides evidence for the
effectiveness of expository dialogue and discussed
applications from education, gaming and E-health.
Second, we introduced the CODA corpus for ad-
dressing the task. Finally, we reported on work-
in-progress on semi-automatic construction of M2D
rules. Our implemented algorithm extracts several
M2D rules from the corpus that are applicable even
to a relatively simple input. Additionally, frequency
analysis of dialogue tags suggests that there is scope
for generating different dialogue styles.
The timeliness of this research is evidenced by the
emergence of a Question Generation (QG) commu-
335
ID Dialogue Structure Monologue Structure
R1 E: Complex Question (i-ii) Contrast (Contrast (Condition(i,ii), iii, iv))
L: Explain (i-ii)
E: Explain-Contradict (iii)
E: YNQuestion (iv)
R2 E: Complex Question (i-ii) Contrast (Condition(i,ii), iii)
L: Explain(i-ii)
E: Explain-Contradict (iii)
R3 E: Complex Question (i-ii) Condition (i,ii)
L: Explain (i-ii)
Table 3: Monologue-to-Dialogue rules extracted from the parallel example in Table 1
Sp Dialogue act Dialogue Turn
E: Complex Ques-
tion a-b
If you have a headache, what
do you do?
L: Explain a-b Take aspirin.
E: Explain-
Contradict
c-d
But aspirin does have side
effects: it can harm circula-
tion
Table 4: A dialogue generated from the monologue about
aspirin by applying the rule R2 (see Table 3)
nity. QG is a subtask of M2D. The first QG work-
shop was held at the end of 2008, resulting in pro-
posals for a Shared Task and Evaluation Campaign
(Rus and Graesser, 2009) for 2010. The CODA cor-
pus should prove to be a useful resource not only for
M2D researchers, but also for the QG community.
Acknowledgments
The research reported in this paper was funded by
the UK Engineering and Physical Sciences Research
Council under grant EP/G/020981/1.
References
E. Andre?, T. Rist, S. van Mulken, M. Klesen, and
S. Baldes. 2000. The automated design of believable
dialogues for animated presentation teams. In Em-
bodied Conversational Agents, pages 220?255. MIT
Press, Cambridge, Mass.
R. Barzilay and K. McKeown. 2001. Extracting para-
phrases from a parallel corpus. In Proc. of ACL/EACL,
Toulouse.
S. Craig, B. Gholson, M. Ventura, A. Graesser, and the
Tutoring Research Group. 2000. Overhearing dia-
logues and monologues in virtual tutoring sessions.
International Journal of Artificial Intelligence in Ed-
ucation, 11:242?253.
S. Demir, S. Carberry, and K. McCoy. 2008. Generating
Textual Summaries of Bar Charts . In Procs of INLG
2008, Ohio, June.
D. DeVault, D. Traum, and R. Artstein. 2008. Making
Grammar-Based Generation Easier to Deploy in Dia-
logue Systems. In Procs SIGdial 2008, Ohio, June.
J. Lee, F. Dinneen, and J. McKendree. 1998. Supporting
student discussions: it isn?t just talk. Education and
Information Technologies, 3:217?229.
P. Piwek, H. Hernault, H. Prendinger, and M. Ishizuka.
2007. T2D: Generating Dialogues between Virtual
Agents Automatically from Text. In Intelligent Virtual
Agents, LNAI 4722, pages 161?174. Springer Verlag.
E. Reiter, S. Sripada, J. Hunter, J. Yu, and I. Davy. 2005.
Choosing Words in Computer-Generated Weather
Forecasts. Artificial Intelligence, 167:137?169.
V. Rus and A. Graesser, editors. 2009. The Ques-
tion Generation Shared Task and Evaluation Chal-
lenge. The University of Memphis. Available at:
http://www.questiongeneration.org/.
IGDA Game Writers SIG. 2003. International game
developers association?s (IGDA) guide to writing for
games. IGDA White Paper.
A. Stent. 2002. A conversation acts model for generating
spoken dialogue contributions. Computer Speech and
Language, 16(3-4):313?352.
S. Stoyanchev and P. Piwek. 2010. Constructing the
CODA corpus. In Procs of LREC 2010, Malta, May.
S. V. Suzuki and S. Yamada. 2004. Persuasion through
overheard communication by life-like agents. In Procs
of the 2004 IEEE/WIC/ACM International Conference
on Intelligent Agent Technology, Beijing, September.
K. van Deemter, B. Krenn, P. Piwek, M. Klesen,
M. Schro?der, and S. Baumann. 2008. Fully gener-
ated scripted dialogue for embodied agents. Artificial
Intelligence Journal, 172(10):1219?1244.
S. Williams, P. Piwek, and R. Power. 2007. Generat-
ing Monologue and Dialogue to Present Personalised
Medical Information to Patients. In Procs ENLG
2007, pages 167?170, Schloss Dagstuhl, Germany.
336
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 242?247,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Data-oriented Monologue-to-Dialogue Generation
Paul Piwek
Centre for Research in Computing
The Open University
Walton Hall, Milton Keynes, UK
p.piwek@open.ac.uk
Svetlana Stoyanchev
Centre for Research in Computing
The Open University
Walton Hall, Milton Keynes, UK
s.stoyanchev@open.ac.uk
Abstract
This short paper introduces an implemented
and evaluated monolingual Text-to-Text gen-
eration system. The system takes mono-
logue and transforms it to two-participant di-
alogue. After briefly motivating the task
of monologue-to-dialogue generation, we de-
scribe the system and present an evaluation in
terms of fluency and accuracy.
1 Introduction
Several empirical studies show that delivering in-
formation in the form of a dialogue, as opposed to
monologue, can be particularly effective for educa-
tion (Craig et al, 2000; Lee et al, 1998) and per-
suasion (Suzuki and Yamada, 2004). Information-
delivering or expository dialogue was already em-
ployed by Plato to communicate his philosophy. It
is used primarily to convey information and possibly
also make an argument; this in contrast with dra-
matic dialogue which focuses on character develop-
ment and narrative.
Expository dialogue lends itself well for presenta-
tion through computer-animated agents (Prendinger
and Ishizuka, 2004). Most information is however
locked up as text in leaflets, books, newspapers,
etc. Automatic generation of dialogue from text in
monologue makes it possible to convert information
into dialogue as and when needed.
This paper describes the first data-oriented
monologue-to-dialogue generation system which re-
lies on the automatic mapping of the discourse
relations underlying monologue to appropriate se-
quences of dialogue acts. The approach is data-
oriented in that the mapping rules have been auto-
matically derived from an annotated parallel mono-
logue/dialogue corpus, rather than being hand-
crafted.
The paper proceeds as follows. Section 2 reviews
existing approaches to dialogue generation. Section
3 describes the current approach. We provide an
evaluation in Section 4. Finally, Section 5 describes
our conclusions and plans for further research.
2 Related Work
For the past decade, generation of information-
delivering dialogues has been approached primarily
as an AI planning task. Andre? et al (2000) describe
a system, based on a centralised dialogue planner,
that creates dialogues between a virtual car buyer
and seller from a database; this approach has been
extended by van Deemter et al (2008). Others have
used (semi-) autonomous agents for dialogue gener-
ation (Cavazza and Charles, 2005; Mateas and Stern,
2005).
More recently, first steps have been taken towards
treating dialogue generation as an instance of Text-
to-Text generation (Rus et al, 2007). In particu-
lar, the T2D system (Piwek et al, 2007) employs
rules that map text annotated with discourse struc-
tures, along the lines of Rhetorical Structure Theory
(Mann and Thompson, 1988), to specific dialogue
sequences. Common to all the approaches discussed
so far has been the manual creation of generation
resources, whether it be mappings from knowledge
representations or discourse to dialogue structure.
242
With the creation of the publicly available1 CODA
parallel corpus of monologue and dialogue (Stoy-
anchev and Piwek, 2010a), it has, however, become
possible to adopt a data-oriented approach. This cor-
pus consists of approximately 700 turns of dialogue,
by acclaimed authors such as Mark Twain, that are
aligned with monologue that was written on the ba-
sis of the dialogue, with the specific aim to express
the same information as the dialogue.2 The mono-
logue side has been annotated with discourse rela-
tions, using an adaptation of the annotation guide-
lines of Carlson and Marcu (2001), whereas the di-
alogue side has been marked up with dialogue acts,
using tags inspired by the schemes of Bunt (2000),
Carletta et al (1997) and Core and Allen (1997).
As we will describe in the next section, our ap-
proach uses the CODA corpus to extract mappings
from monologue to dialogue.
3 Monologue-to-Dialogue Generation
Approach
Our approach is based on five principal steps:
I Discourse parsing: analysis of the input mono-
logue in terms of the underlying discourse rela-
tions.
II Relation conversion: mapping of text annotated
with discourse relations to a sequence of dia-
logue acts, with segments of the input text as-
signed to corresponding dialogue acts.
III Verbalisation: verbal realisation of dialogue
acts based on the dialogue act type and text of
the corresponding monologue segment.
IV Combination Putting the verbalised dialogues
acts together to create a complete dialogue, and
V Presentation: Rendering of the dialogue (this
can range for simple textual dialogue scripts to
computer-animated spoken dialogue).
1computing.open.ac.uk/coda/data.html
2Consequently, the corpus was not constructed entirely of
pre-existing text; some of the text was authored as part of the
corpus construction. One could therefore argue, as one of the re-
viewers for this paper did, that the approach is not entirely data-
driven, if data-driven is interpreted as ?generated from unadul-
terated, free text, without any human intervention needed?.
For step I we rely on human annotation or existing
discourse parsers such as DAS (Le and Abeysinghe,
2003) and HILDA (duVerle and Prendinger, 2009).
For the current study, the final step, V, consists sim-
ply of verbatim presentation of the dialogue text.
The focus of the current paper is with steps II and
III (with combination, step IV, beyond the scope of
the current paper). Step II is data-oriented in that
we have extracted mappings from discourse relation
occurrences in the corpus to corresponding dialogue
act sequences, following the approach described in
Piwek and Stoyanchev (2010). Stoyanchev and Pi-
wek (2010b) observed in the CODA corpus a great
variety of Dialogue Act (DA) sequences that could
be used in step II, however in the current version
of the system we selected a representative set of the
most frequent DA sequences for the five most com-
mon discourse relations in the corpus. Table 1 shows
the mapping from text with a discourse relations
to dialogue act sequences (i indicates implemented
mappings).
DA sequence A C C E M TR
D T R M T
YNQ; Expl i i d
YNQ; Yes; Expl i i i d
Expl; CmplQ; Expl i d
ComplQ; Expl i/t i/t i i c
Expl; YNQ;Yes i d
Expl; Contrad. i d
FactQ; FactA; Expl i c
Expl; Agr; Expl i d
Expl; Fact; Expl t c
Table 1: Mappings from discourse relations (A = Attribu-
tion, CD = Condition, CT = Contrast, ER = Explanation-
Reason, MM = Manner-Means) to dialogue act sequences
(explained below) together with the type of verbalisation
transformation TR being d(irect) or c(omplex).
For comparison, the table also shows the much
less varied mappings implemented by the T2D sys-
tem (indicated with t). Note that the actual mappings
of the T2D system are directly from discourse rela-
tion to dialogue text. The dialogue acts are not ex-
plicitly represented by the system, in contrast with
the current two stage approach which distinguishes
between relation conversion and verbalisation.
243
Verbalisation, step III, takes a dialogue act type
and the specification of its semantic content as given
by the input monologue text. Mapping this to the
appropriate dialogue act requires mappings that vary
in complexity.
For example, Expl(ain) can be generated by sim-
ply copying a monologue segment to dialogue utter-
ance. The dialogue acts Yes and Agreement can be
generated using canned text, such as ?That is true?
and ?I agree with you?.
In contrast, ComplQ (Complex Question), FactQ
(Factoid Question), FactA (Factiod Answer) and
YNQ (Yes/No Question) all require syntactic ma-
nipulation. To generate YNQ and FactQ, we use
the CMU Question Generation tool (Heilman and
Smith, 2010) which is based on a combination
of syntactic transformation rules implemented with
tregex (Levy and Andrew, 2006) and statistical
methods. To generate the Compl(ex) Q(uestion) in
the ComplQ;Expl Dialogue Act (DA) sequence, we
use a combination of the CMU tool and lexical trans-
formation rules.3 The GEN example in Table 2 il-
lustrates this: The input monologue has a Manner-
Means relations between the nucleus ?In September,
Ashland settled the long-simmering dispute? and the
satellite ?by agreeing to pay Iran 325 million USD?.
The satellite is copied without alteration to the Ex-
plain dialogue act. The nucleus is processed by ap-
plying the following template-based rule:
Decl? How Yes/No Question(Decl)
In words, the input consisting of a declarative sen-
tence is mapped to a sequence consisting of the word
?How? followed by a Yes/No-question (in this case
?Did Ashland settle the long-simmering dispute in
December??) that is obtained with the CMU QG tool
from the declarative input sentence. A similar ap-
proach is applied for the other relations (Attribution,
Condition and Explanation-Reason) that can lead to
a ComplQ; Expl dialogue act sequence (see Table 1).
Generally, sequences requiring only copying or
canned text are labelled d(irect) in Table 1, whereas
those requiring syntactic transformation are labelled
c(omplex).
3In contrast, the ComplQ in the DA sequence
Expl;ComplQ;Expl is generated using canned text such as
?Why?? or ?Why is that??.
4 Evaluation
We evaluate the output generated with both complex
and direct rules for the relations of Table 1.
4.1 Materials, Judges and Procedure
The input monologues were text excerpts from the
Wall Street Journal as annotated in the RST Dis-
course Treebank4. They consisted of a single sen-
tence with one internal relation, or two sentences
(with no internal relations) connected by a single
relation. To factor out the quality of the discourse
annotations, we used the gold standard annotations
of the Discourse Treebank and checked these for
correctness, discarding a small number of incorrect
annotations.5 We included text fragments with a
variety of clause length, ordering of nucleus and
satellite, and syntactic structure of clauses. Table 2
shows examples of monologue/dialogue pairs: one
with a generated dialogue and the other from the cor-
pus.
Our study involved a panel of four judges, each
fluent speakers of English (three native) and ex-
perts in Natural Language Generation. We collected
judgements on 53 pairs of monologue and corre-
sponding dialogue. 19 pairs were judged by all four
judges to obtain inter-annotator agreement statistics,
the remainder was parcelled out. 38 pairs consisted
of WSJ monologue and generated dialogue, hence-
forth GEN, and 15 pairs of CODA corpus monologue
and human-authored dialogue, henceforth CORPUS
(instances of generated and corpus dialogue were
randomly interleaved) ? see Table 2 for examples.
The two standard evaluation measures for lan-
guage generation, accuracy and fluency (Mellish and
Dale, 1998), were used: a) accuracy: whether a
dialogue (from GEN or CORPUS) preserves the in-
formation of the corresponding monologue (judge-
ment: ?Yes? or ?No?) and b) monologue and dialogue
fluency: how well written a piece of monologue or
dialogue from GEN or CORPUS is. Fluency judge-
ments were on a scale from 1 ?incomprehensible? to
5 ?Comprehensible, grammatically correct and nat-
urally sounding?.
4www.isi.edu/?marcu/discourse/Corpora.html
5For instance, in our view ?without wondering? is incorrectly
connected with the attribution relation to ?whether she is mov-
ing as gracefully as the scenery.?
244
GEN Monologue
In September, Ashland settled the
long-simmering dispute by agreeing to
pay Iran 325 million USD.
Dialogue (ComplQ; Expl)
A: How did Ashland settle the
long-simmering dispute in December?
B: By agreeing to pay Iran 325
million USD.
CORPUS Monologue
If you say ?I believe the world is
round?, the ?I? is the mind.
Dialogue (FactQ; FactA)
A: If you say ?I believe the world is round?,
who is the ?I? that is speaking?
B: The mind.
Table 2: Monologue-Dialogue Instances
4.2 Results
Accuracy Three of the four judges marked 90%
of monologue-dialogue pairs as presenting the same
information (with pairwise ? of .64, .45 and .31).
One judge interpreted the question differently and
marked only 39% of pairs as containing the same
information. We treated this as an outlier, and ex-
cluded the accuracy data of this judge. For the in-
stances marked by more than one judge, we took the
majority vote. We found that 12 out of 13 instances
(or 92%) of dialogue and monologue pairs from the
CORPUS benchmark sample were judged to contain
the same information. For the GEN monologue-
dialogue pairs, 28 out of 31 (90%) were judged to
contain the same information.
Fluency Although absolute agreement between
judges was low,6 pairwise agreement in terms of
Spearman rank correlation (?) is reasonable (aver-
age: .69, best: .91, worst: .56). For the subset of in-
stances with multiple annotations, we used the data
from the judge with the highest average pair-wise
agreement (? = .86)
The fluency ratings are summarised in Figure 1.
Judges ranked both monologues and dialogues for
6For the four judges, we had an average pairwise ? of .34
with the maximum and minimum values of .52 and .23, respec-
tively.
Figure 1: Mean Fluency Rating for Monologues and Dia-
logues (for 15 CORPUS and 38 GEN instances) with 95%
confidence intervals
the GEN sample higher than for the CORPUS sam-
ple (possibly as a result of slightly greater length of
the CORPUS fragments and some use of archaic lan-
guage). However, the drop in fluency, see Figure 2,
from monologue to dialogue is greater for GEN sam-
ple (average: .89 points on the rating scale) than the
CORPUS sample (average: .33) (T-test p<.05), sug-
gesting that there is scope for improving the genera-
tion algorithm.
Figure 2: Fluency drop from monologue to correspond-
ing dialogue (for 15 CORPUS and 38 GEN instances). On
the x-axis the fluency drop is marked, starting from no
fluency drop (0) to a fluency drop of 3 (i.e., the dialogue
is rated 3 points less than the monologue on the rating
scale).
245
Direct versus Complex rules We examined the
difference in fluency drop between direct and com-
plex rules. Figure 3 shows that the drop in fluency
for dialogues generated with complex rules is higher
than for the dialogues generated using direct rules
(T-test p<.05). This suggests that use of direct rules
is more likely to result in high quality dialogue. This
is encouraging, given that Stoyanchev and Piwek
(2010a) report higher frequencies in professionally
authored dialogues of dialogue acts (YNQ, Expl) that
can be dealt with using direct verbalisation (in con-
trast with low frequency of, e.g., FactQ).
Figure 3: Decrease in Fluency Score from Monologue
to Dialogue comparing Direct (24 samples) and Complex
(14 samples) dialogue generation rules
5 Conclusions and Further Work
With information presentation in dialogue form be-
ing particularly suited for education and persua-
sion, the presented system is a step towards mak-
ing information from text automatically available
as dialogue. The system relies on discourse-to-
dialogue structure rules that were automatically ex-
tracted from a parallel monologue/dialogue corpus.
An evaluation against a benchmark sample from the
human-written corpus shows that both accuracy and
fluency of generated dialogues are not worse than
that of human-written dialogues. However, drop in
fluency between input monologue and output dia-
logue is slightly worse for generated dialogues than
for the benchmark sample. We also established a dif-
ference in quality of output generated with complex
versus direct discourse-to-dialogue rules, which can
be exploited to improve overall output quality.
In future research, we aim to evaluate the accu-
racy and fluency of longer stretches of generated di-
alogue. Additionally, we are currently carrying out
a task-related evaluation of monologue versus dia-
logue to determine the utility of each.
Acknowledgements
We would like to thank the three anonymous
reviewers for their helpful comments and sug-
gestions. We are also grateful to our col-
leagues in the Open University?s Natural Lan-
guage Generation group for stimulating discussions
and feedback. The research reported in this pa-
per was carried out as part of the CODA re-
search project (http://computing.open.ac.uk/coda/)
which was funded by the UK?s Engineering and
Physical Sciences Research Council under Grant
EP/G020981/1.
References
E. Andre?, T. Rist, S. van Mulken, M. Klesen, and
S. Baldes. 2000. The automated design of believable
dialogues for animated presentation teams. In Jus-
tine Cassell, Joseph Sullivan, Scott Prevost, and Eliz-
abeth Churchill, editors, Embodied Conversational
Agents, pages 220?255. MIT Press, Cambridge, Mas-
sachusetts.
H. Bunt. 2000. Dialogue pragmatics and context spec-
ification. In H. Bunt and W. Black, editors, Abduc-
tion, Belief and Context in Dialogue: Studies in Com-
putational Pragmatics, volume 1 of Natural Language
Processing, pages 81?150. John Benjamins.
J. Carletta, A. Isard, S. Isard, J. Kowtko, G. Doherty-
Sneddon, and A. Anderson. 1997. The reliability of
a dialogue structure coding scheme. Computational
Linguistics, 23:13?31.
L. Carlson and D. Marcu. 2001. Discourse tagging
reference manual. Technical Report ISI-TR-545, ISI,
September.
M. Cavazza and F. Charles. 2005. Dialogue Gener-
ation in Character-based Interactive Storytelling. In
Proceedings of the AAAI First Annual Artificial Intel-
ligence and Interactive Digital Entertainment Confer-
ence, Marina Del Rey, California, USA.
M. Core and J. Allen. 1997. Coding Dialogs with
the DAMSL Annotation Scheme. In Working Notes:
AAAI Fall Symposium on Communicative Action in
Humans and Machine.
246
S. Craig, B. Gholson, M. Ventura, A. Graesser, and the
Tutoring Research Group. 2000. Overhearing dia-
logues and monologues in virtual tutoring sessions.
International Journal of Artificial Intelligence in Ed-
ucation, 11:242?253.
D. duVerle and H. Prendinger. 2009. A novel discourse
parser based on support vector machines. In Proc 47th
Annual Meeting of the Association for Computational
Linguistics and the 4th Int?l Joint Conf on Natural
Language Processing of the Asian Federation of Nat-
ural Language Processing (ACL-IJCNLP?09), pages
665?673, Singapore, August.
M. Heilman and N. A. Smith. 2010. Good question!
statistical ranking for question generation. In Proc. of
NAACL/HLT, Los Angeles.
Huong T. Le and Geehta Abeysinghe. 2003. A study to
improve the efficiency of a discourse parsing system.
In Proceedings 4th International Conference on Intel-
ligent Text Processing and Computational Linguistics
(CICLing-03), Springer LNCS 2588, pages 101?114.
J. Lee, F. Dinneen, and J. McKendree. 1998. Supporting
student discussions: it isn?t just talk. Education and
Information Technologies, 3:217?229.
R. Levy and G. Andrew. 2006. Tregex and tsurgeon:
tools for querying and manipulating tree data struc-
tures. In 5th International Conference on Language
Resources and Evaluation (LREC 2006)., Genoa, Italy.
William C. Mann and Sandra A. Thompson. 1988.
Rhetorical structure theory: Toward a functional the-
ory of text organization. Text, 8(3):243?281.
M. Mateas and A. Stern. 2005. Structuring content in the
faade interactive drama architecture. In Proc. of Artifi-
cial Intelligence and Interactive Digital Entertainment
(AIIDE), Marina del Rey, Los Angeles, June.
C. Mellish and R. Dale. 1998. Evaluation in the context
of natural language generation. Computer Speech and
Language, 12:349?373.
P. Piwek and S. Stoyanchev. 2010. Generating Exposi-
tory Dialogue from Monologue: Motivation, Corpus
and Preliminary Rules. In Human Language Tech-
nologies: The 2010 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 333?336, Los Angeles, Cali-
fornia, June.
P. Piwek, H. Hernault, H. Prendinger, and M. Ishizuka.
2007. T2D: Generating Dialogues between Virtual
Agents Automatically from Text. In Intelligent Vir-
tual Agents: Proceedings of IVA07, LNAI 4722, pages
161?174. Springer Verlag.
H. Prendinger and M. Ishizuka, editors. 2004. Life-Like
Characters: Tools, Affective Functions, and Applica-
tions. Cognitive Technologies Series. Springer, Berlin.
V. Rus, A. Graesser, A. Stent, M. Walker, and M. White.
2007. Text-to-Text Generation. In R. Dale and
M. White, editors, Shared Tasks and Comparative
Evaluation in Natural Language Generation: Work-
shop Report, Arlington, Virginia.
S. Stoyanchev and P. Piwek. 2010a. Constructing the
CODA corpus. In Procs of LREC 2010, Malta, May.
S. Stoyanchev and P. Piwek. 2010b. Harvesting re-usable
high-level rules for expository dialogue generation. In
6th International Natural Language Generation Con-
ference (INLG 2010), Dublin, Ireland, 7-8, July.
S. V. Suzuki and S. Yamada. 2004. Persuasion through
overheard communication by life-like agents. In Procs
of the 2004 IEEE/WIC/ACM International Conference
on Intelligent Agent Technology, Beijing, September.
K. van Deemter, B. Krenn, P. Piwek, M. Klesen,
M. Schroeder, and S. Baumann. 2008. Fully Gen-
erated Scripted Dialogue for Embodied Agents. Arti-
ficial Intelligence Journal, 172(10):1219?1244.
247
Harvesting Re-usable High-level Rules
for Expository Dialogue Generation
Svetlana Stoyanchev
Centre for Research in Computing
The Open University
Walton Hall, Milton Keynes, UK
s.stoyanchev@open.ac.uk
Paul Piwek
Centre for Research in Computing
The Open University
Walton Hall, Milton Keynes, UK
p.piwek@open.ac.uk
Abstract
This paper proposes a method for extract-
ing high-level rules for expository dialogue
generation. The rules are extracted from di-
alogues that have been authored by expert
dialogue writers. We examine the rules that
can be extracted by this method, focusing on
whether different dialogues and authors ex-
hibit different dialogue styles.
1 Introduction
In the past decade, a new area of Natural Language
Generation (NLG) has emerged: the automated gen-
eration of expository dialogue, also often referred to
as scripted, authored or fictive dialogue. Research in
this area began with the seminal study by Andre? et
al. (2000), which explored generation of dialogues
between a virtual car buyer and seller from technical
data on a car. This strand of work was developed fur-
ther in the NECA project (van Deemter et al, 2008)
and has since been extended to other domains, in-
cluding explanation of medical histories (Williams
et al, 2007), patient information leaflets (Piwek et
al., 2007) and Wall Street Journal articles (Hernault
et al, 2008).
Systems for generating expository dialogue have
explored different inputs (databases, knowledge rep-
resentations and text), generation methods (e.g.,
rule versus constraint-based approaches) and out-
puts (from dialogue scripts in text form to audio and
computer-animated dialogue). A common trait of all
these systems is, however, that at some point in the
generation process, they produce a dialogue script, a
text file which specifies what the interlocutors say,
possibly enriched with mark-up for dialogue acts,
speech and gestures ? see, e.g., Piwek et al (2002).
These systems are different from conventional dia-
logue systems in that the system does not engage in
a dialogue with the user; rather, the system generates
a dialogue between two or more fictitious charac-
ters for the user/audience to view and learn from. In
other words, the dialogue is used to deliver informa-
tion to the user or audience, rather than between the
interlocutors. Piwek (2008) discusses several empir-
ical studies that identify benefits of the use of expos-
itory dialogue for education and persuasion.
In this paper, we take a step towards addressing
two shortcomings of the work so far. Firstly, all
the work cited has relied on hand-crafted resources
(typically rules) for creating the dialogue. With the
resources being created by non-expert dialogue au-
thors (e.g., academic researchers), generated dia-
logues based on these resources may not be optimal;
for instance, Williams et al (2007) found that gener-
ated dialogues can be too information-dense, requir-
ing conversational padding. Secondly, the resources
for creating dialogue are tied to a specific domain,
making it hard to redeploy a system in new domains.
We propose to address the first issue by automat-
ically creating dialogue generation resources from a
corpus of dialogues written by known effective dia-
logue authors. This fits in with a trend in dialogue
modelling and generation to create resources from
empirical data (Oh and Rudnicky, 2002; DeVault et
al., 2008; Henderson et al, 2008; Belz and Kow,
2009).
The second issue is addressed by specifying di-
alogue generation rules at a level of detail that ab-
stracts over the particulars of the domain and fits in
with existing NLG architectures. The reference ar-
chitecture of Reiter and Dale (2000) identifies three
principal NLG tasks: Document Planning (DP),
Microplanning and Realisation. DP is primarily
non-linguistic: it concerns selection of information
and organization of this information into a coherent
whole. The latter is achieved by making sure that
the information is tied together by Rhetorical Rela-
tions such as Contrast, Elaboration and Explanation,
in other words, it is part of a Rhetorical Structure.
We propose that dialogue generation rules interface
with Rhetorical Structure and map to a Sequence of
Dialogue Acts.
Interestingly, the interface between DP and Mi-
croplanning has also been identified as a place where
decisions and preferences regarding style take an ef-
fect (McDonald and Pustejovsky, 1985). A ques-
tion that we explore in this paper is whether dialogue
styles exist at the highly abstract level we focus on
in this paper. We concentrate on style in the sense of
?[t]he manner of expression characteristic of a par-
ticular writer?1.
The remainder of this paper is set up as follows.
In Section 2, we introduce the corpus that we use to
extract dialogue generation resources. Section 3 ex-
amines the dialogues in the corpus for prima facie
evidence for stylistic differences between authors at
the dialogue level. In Section 4, we describe our ap-
proach to extracting high-level dialogue generation
rules from the corpus. Next, in Section 5 we anal-
yse the resulting rules, looking for further evidence
of different dialogue styles. We also compare the
rules that were harvested from our corpus with hand-
crafted rules in terms of content and variety. Finally,
Section 6 contains our conclusions and a discussion
of avenues for further research.
2 A Parallel Monologue-Dialogue Corpus
The current work makes use of a corpus of human-
authored dialogues, the CODA corpus.2 In total, this
corpus consist of about 800 dialogue turns. This
1From definition 13.a. of the Oxford English Dictionary at
http://dictionary.oed.com
2Further information on the construction of this cor-
pus can be found in the annotation manual at comput-
ing.open.ac.uk/coda/AnnotationManual.pdf.
paper is based on three dialogues from the cor-
pus: George Berkeley?s ?Dialogues between Hylas
and Philonous? (extract of 172 turns), Mark Twain?s
?What is man?? (extract of 445 turns) and Yuri Gure-
vich?s ?Evolving Algebras? (extract of 89 turns).
Berkeley?s dialogue is one of the classics of philoso-
phy, arguing for the, at first sight, extravagant claim
that ?there is no such thing as material substance in
the world?. Twain, according to the Encyclopaedia
Britannica ?one of America?s best and most beloved
writers?, takes on the concept of free will. Gure-
vich?s dialogue deals with the mathematical concept
of evolving algebras. Of these dialogues, Twain is
by a large margin the longest (over 800 turns in total)
and the only one which is aimed specifically at the
general public, rather than an academic/specialist
audience.
For each of the dialogues, the corpus also con-
tains human-authored monologue which expresses
the same content as the dialogue. Monologue and
dialogue are aligned through mappings from mono-
logue snippets to dialogue spans. As a result, the
CODA corpus is a parallel monologue-dialogue cor-
pus. Both the monologue and dialogue come with
annotations: the monologue with Rhetorical Struc-
ture Theory (RST) relations (Mann and Thompson,
1988; Carlson and Marcu, 2001) and the dialogue
side with an adaptation of existing Dialogue Act an-
notation schemes (Carletta et al, 1997; Core and
Allen, 1997). Table 2 contains an overview of these
RST relations and Dialogue Act labels.
3 Dialogue Analysis
In this section we examine whether there is prima
facie evidence for differences in style between the
three dialogues. Whereas existing work in NLG on
style has focused on lexical and syntactic choice,
see Reiter and Williams (2008), here we focus on
higher-level characteristics of the dialogues, in par-
ticular, proportion of turns with multiple dialogue
acts, frequencies of dialogue act bigrams, and rela-
tion between dialogue acts and speaker roles.
An important reason for determining whether
there are different styles involved, is that this has
implications for how we use the corpus to create
expository dialogue generation resources. If differ-
ent dialogues employ different styles, we need to be
RST relations Dialogue Acts
Enablement, Cause, Evaluation (Subjective, Inferred),
Comment, Attribution, Condition-Hypothetical, Contrast,
Comparison, Summary, Manner-means, Topic-Comment
(Problem-Solution, Statement-Response, Question-
Answer, Rhetorical Question) Background, Temporal,
Elaboration/Explanation, (Additional, General-Specific,
Example, Object-attribute, Definition, Evidence, Reason),
Same-unit
Explain, Info-Request (Init-Factoid-
InfoReq, Init-YN-InfoReq, Init-
Complex-InfReq), Init-Request-
Clarify, Response-Answer (Resp-
Answer-Yes/No, and Resp-Answer-
Factoid), Resp-Agree, Resp-
Contradict
Table 1: RST relations and Dialogue Acts used in the CODA corpus. Annotators used the fine-grained
categories in italics that are listed in brackets. For the current study, we rely, however, on the higher-level
categories that preceed the fine-grained categories and which combine several of them.
careful with creating resources which combine data
from different dialogues. Merging such data, if any-
thing, may lead to the generation of dialogues which
exhibit features from several possibly incompatible
styles. Since our aim is specifically to generate dia-
logues that emulate the masters of dialogue author-
ing, it is then probably better to create resources
based on data from a single master or dialogue.
3.1 Multi-act Turns
One of the characteristics of dialogue is the pace
and the amount of information presented in each
of the speaker?s turns. In a fast-paced dialogue
turns are concise containing a single dialogue act.
Such dialogues of the form A:Init B:Response A:Init
B:Response ... are known as ?pingpong? dialogue.
Twain?s ?What is man?? dialogue starts in this fash-
ion (O.M. = Old Man; Y.M = Young Man):
O.M. What are the materials of
which a steam-engine is made?
Y.M. Iron, steel, brass, white-metal,
and so on.
O.M. Where are these found?
Y.M In the rocks.
O.M. In a pure state?
Y.M. No?in ores.
. . .
One character serves as the initiator and the other
replies with a response. With turns that contain more
than one dialogue, henceforth multi-act turns, this
pattern can be broken:
O.M. . . .
And you not only did not make that
Author Twain Gurevich Berkeley
Multi-act 34% 43% 24%
Layman/Expert 45%/55% 36%/64% 51%/49%
Table 2: Proportion of multi-act utterances and their
distribution between Layman and Expert
machinery yourself, but you have NOT
EVEN ANY COMMAND OVER IT.
Y.M. This is too much.
You think I could have formed no
opinion but that one?
O.M. Spontaneously? No. And . . .
Multi-act turns are turns comprised of multiple dia-
logue acts, such as the Young Man?s in the exam-
ple above, where a Resp-Contradict (?This is too
much.?) is followed by an Init-YN-Request (?You
think I could have formed no opinion but that one??).
The dialogue pace may vary throughout a dia-
logue. We, however, find that overall proportions
of multi-act turns and their distribution between ex-
pert and layman vary between the authors (see Ta-
ble 2). Gurevich?s dialogue has the highest propor-
tion (43%) of multi-act turns and majority of them
are attributed to the expert. Only 24% of Berkeley?s
dialogue turns consist of multiple dialogue acts and
they are evenly split between the expert and the lay-
man. Gurevich?s dialogue is the type of dialogue
where an expert gives a lesson to a layman while
in Berkeley?s dialogue one character often comple-
ments ideas of the other character making it difficult
to determine which of the characters is an expert.
The amount of multi-act turns seems to be one of
the stylistic choices made by a dialogue author.
3.2 Dialogue Diversity
Figure 1: Bigram coverage for the 1-st to 4th most
frequent bigrams.
Dialogues are essentially a sequence of turns,
where each turn consists of one or more dialogue
acts. For our measure of dialogue diversity we focus
on two-turn sequences (i.e., turn bigrams), where a
turn is identified by the sequence of dialogue acts it
contains.
We define bigram coverage for i as the percent-
age that the top i most frequent bigrams contribute
to all bigrams in the corpus. Diversity of the dia-
logue is inversely related to the dialogue coverage.
In a dialogue with minimal diversity, the same turn,
consisting of one or more dialogue acts, is repeated
throughout the dialogue. The turn bigram consisting
of two such turns has 100% bigram coverage.
Figure 1 shows the coverage for 1 ? i ? 4 for
each author in the corpus.3 Out of the three authors,
Twain?s dialogues are the most diverse where the top
4 bigrams constitute only 15% of all bigrams. In
Gurevich?s dialogues the four most frequent bigrams
constitute 25% and in Berkeley 40%.
Note that for all three authors the dialogue cov-
erage for the 4 most frequent bigrams is quite low
indicating high variability in bigrams used. To
achieve such variability in automatically generated
dialogues we need a large number of distinct gener-
ation rules.
3This range was chosen for illustration purposes. Bigram
coverage can be compared for any i ?total number of distinct
bigrams.
3.3 Dialogue Acts and Speaker Roles
One of the most frequent bigrams for all three au-
thors was, not unexpectedly, the sequence:
A: InfoRequest
B: Response-Answer
There is, however, a difference in the roles of speak-
ers A and B. In all dialogues, one of the speakers
took on the expert role and the other the layman role.
For the aforementioned bigram, both in Berkeley?s
and Gurevich?s dialogues the layman typically ini-
tiates the request for information and the expert re-
sponds (and often goes on to explain the response in
Gurevich?s dialogue):
Q: Is it difficult to define basic
transition rules in full generality?
A: No. Here is the definition.
? Any local function update is a rule.
. . .
(From Gurevich?s dialogue)
In contrast, in Twain?s dialogues the roles are typ-
ically reversed: the expert asks and the layman re-
sponds:
O.M. Then the impulse which moves you
to submit to the tax is not ALL
compassion, charity, benevolence?
Y.M. Well?perhaps not.
Both techniques allow the author to convey a par-
ticular piece of information, but each giving rise its
very own dialogue style.
4 Approach to Rule Extraction
Comparing statistics for individual dialogues gives
us some idea about whether different styles are in-
volved. The true test for whether different styles are
involved is, however, whether for the same content
different realizations are generated. Unfortunately,
for our three dialogues the content is different to be-
gin with. The parallel corpus allows us, however, to
get around this problem. From the parallel corpus
we can extract rules which map RST structures to
dialogue act sequences. The Lefthand Side (LHS)
of a rule represents a particular rhetorical structure
found in the monologue side, whereas the Right-
hand Side (RHS) of the rule represents the dialogue
act sequence with which it is aligned in the corpus.
Such rules can be compared between the different
dialogues: in particular, we can examine whether the
same LHS gives rise to similar or different RHSs.
4.1 Comparison with previous work
Hernault et al (2008) manually construct surface-
level rules mapping monologue to dialogue.
Surface-level rules execute text-to-text conversion
operating directly on the input string. In our ap-
proach, we separate the conversion into two stages.
A first stage converts RST structures to Dialogue
Act sequences. A second stage, which is beyond
the scope of this paper, converts Dialogue Act se-
quences to text.
A further difference between the current approach
and Hernault et al?s is that the LHS of our rules
can match nested RST structures. This covers, what
we call, simple rules (involving a single RST re-
lation, e.g., Contrast(X,Y)) and complex rules (in-
volving 2 or more nested RST relations, e.g., Con-
trast(Condition(X,Y),Z)). Hernault et al only allow
for simple rules. A detailed comparison between our
approach and that of Hernault et al, using the attri-
bution rule as an example, can be found in Section
5.3.
id DA turns
0 Init-YN-
InfoReq
Is your mind a part of your PHYSI-
CAL equipment ?
0 Resp-
Answer-No
No.
1 Explain It is independent of it ; it is spiritual
2 Init-YN-
InfoReq
Being spiritual, it cannot be af-
fected by physical influences?
2 Resp-
Answer-No
No.
3 Init-YN-
InfoReq
Does the mind remain sober with
the body is drunk ?
- decorative Well?
3 Resp-
Answer-No
No.
Table 3: Example of annotated dialogue (from Mark
Twain?s ?What is man??).
4.2 Rule Extraction Algorithm
Table 3 and Figure 2 show annotated dialogue (au-
thored by Twain) and its annotated monologue trans-
lation. Each terminal node of the RST structure
corresponds to a part of a monologue snippet. All
nodes with the same id correspond to a complete
Condition
Attribution
id=2
Contrast
id=0
id=1
Being spiritual,
by phisical influences.
nuc
id=3
Let?s for a minute 
assume that
Explanation
it can not be affected
your mind is not partid=0
of your physical equipment, it is spiritual.that it is independent of it,
However, 
the mind    does not
remain sober
when the bodyis drunk.
nuc
nuc
Figure 2: RST structure for the translation of dia-
logue in Table 3
span rule
0-0 Attribution(0, 0)
0-1 Attribution( Explanation(0, 1))
2-3 Contrast(2, 3)
0-3 Condition (Attribution( Ex-
plain(0, 1)), Contrast(2, 3))
Table 4: RST sub-structures: LHS of monologue-to-
dialogue mapping rules
snippet and are linked to the dialogue act(s) with the
same ids. The relation between monologue snippets
and dialogue act segments is one-to-many. In other
words, one snippet (e.g. snippets with id=0, id=2)
can be expressed by multiple dialogue act segments.
Rules are extracted as follows: For each (auto-
matically extracted) sub-structure of the RST struc-
tures on the monologue side, a rule is created (see
Table 4). Two constraints restrict extraction of sub-
structures: 1) spans of the structure?s terminal nodes
must be consecutive and 2) none of the ids of the
terminal nodes are shared with a node outside the
sub-structure.
For example, Explanation(0, 1) is not extracted
because the node with id=0 appears also under the
Attribution relation which is not a part of this sub-
structure.
Additionally, rules are generated by removing a
relation and its satellite node and moving a nucleus
node one level up. Attribution(0, 0) was extracted
from a tree that had the Explanation relation and its
satellite child 1 pruned. This operation relies on the
validity of the following principle for RST (Marcu,
1997): ?If a relation holds between two textual spans
of the tree structure of a text, that relation also holds
between the most important units of the constituent
subspans.?
The RST sub-structure is the LHS of a rule and
dialogue act sequences are the RHS of a rule.
5 Results: Analysis of the Rules
In this section we describe the rules collected from
the corpus. We compare the rules collected from the
dialogues of different authors. We also compare the
rules constructed manually in previous work with
the rules collected from the corpus, specifically for
the attribution relation.
5.1 Rule Statistics
relation Twain Gurev Berk all
simple 31 (33) 29 (38) 25 (26) 81 (97)
complex 19 26 16 61 (61)
null 15 (22) 9 (18) 9 (27) 25 (67)
total 65 64 50 167
# turns 85 78 96 259
Table 5: Numbers of extracted distinct structural
rules (total occurrences are parenthesized)
relation Twain Gurevich Berkley
attribution 15% 2% 12%
contrast 18% 9% 17%
expl/elab 34% 47% 26%
eval 9% 6% 21%
other 24% 36% 24%
total 100% 100% 100%
Table 6: Proportions of relations expressed as rules
relation Twain Gurevich Berkley
overall 2.4 1.9 2.9
contrast 2.3 2 2.6
elab/expl 2.7 1.7 3.3
eval 2 2 2.5
Table 7: Average number of turns in simple rules
Simple rules are the rules with one RST relation in
the LHS. Complex rules are the rules with multiple
RST relations in the LHS. In Table 4, rules for the
LHS 0-0 and 2-3 are simple while the rules for 0-1
and 0-3 are complex. Null rules are the rules with no
RST relation in the LHS.
From our sample of 259 translated and annotated
dialogue turns from the corpus, we extracted 81 sim-
ple, 61 complex, and 25 null rules (null rules involve
no RST structure and are discussed below). Table 5
shows the number of distinct rules per author.4 In
parentheses we show the number of actual (not nec-
essarily distinct) rule occurrences in corpus. The
majority of simple rules in the corpus (65 out of 81)
occur only once.5 This shows that the dialogue au-
thors use a variety of dialogue act sequences when
presenting their arguments in dialogue.
To compare dialogue styles we compare the rules
across the dialogues of different authors. Table 6
shows the proportions of relation types in each au-
thor?s dialogues that are mapped to a dialogue struc-
ture and produce a mapping rule.6 Not all relations
in monologue are mapped to a dialogue structure.
For example, Explain moves may contain multiple
clauses that are presented by a single character in
the same turn. We find differences in distributions
of relation types mapped to dialogue between the
three authors (Fisher?s exact test p<.01). Berkeley?s
dialogues produce more mapping rules with Eval-
uation and less with Explanation/Elaboration rela-
tions than the other two authors. Gurevich?s di-
alogues produce less mapping rules with Attribu-
tion and Contrast relations than the other two au-
thors. This difference between distributions of re-
lation types mapped to dialogue has an important
implication for dialogue generation. Dialogue gen-
eration programs may vary the style of a dialogue
by choosing which discourse relations of the mono-
logue are mapped to dialogue turns.
Another relevant property of a rule is the number
of turns in the RHS of the rule. Number of turns in a
rule shows how many times the dialogue characters
switch to present information of the monologue cor-
responding to the LHS of the rule. The average num-
bers of turns in the RHS of all rules of the Twain,
Gurevich, and Berkeley dialogues are 2.4, 1.9, 2.9
respectively (see Table 7). They are all pairwise sig-
nificantly different (t-test p < .05) ranking the au-
4Two rules are distinct if either their LHS (relation in mono-
logue) or RHSs (sequence of dialogue acts) are different.
565=81-(97-81)
6This includes simple and complex rules
thors in the order Gurevich < Twain < Berkeley
according to the number of turns in the RHS of the
rule. Similar ranking also appears as a trend for in-
dividual relations suggesting that this is the effect of
the author?s style rather than the relations (the dis-
tribution of relation types is different across the au-
thors). This suggests that dialogue generation may
affect the style of automatically generated dialogue
by selectively choosing rules with longer (or shorter)
RHS.
5.2 Null Rule
A null rule is a rule where a sequence of dialogue
turns between two characters corresponds with a text
segment with no rhetorical relation. A text segment
without a rhetorical relation corresponds to a leaf
node in the RST structure. A null rule typically cre-
ates a dialogue fragment consisting of a yes/no ques-
tion (Init-YN-Info-Req) followed by yes/no answer,
or a complex information request (e.g. What is your
opinion on X?) followed by an Explain dialogue act,
or a presentation of an argument (Explain dialogue
act) followed by a response that signals agreement
(Resp-Agree). Null rules create more interactivity in
the dialogue.
The monologue segment corresponding to the
LHS of a null rule may be in a rhetorical relation
with another segment, such that the LHS of the null
rule is embedded into another rule. Table 8 shows an
example of a null rule embedded in a contrast rule.
Turns 1 - 3 correspond to the RHS of the Null rule
and 1 - 4 correspond to the RHS of the Contrast rule.
Null rules can be used to turn information into
dialogue, even when there is no RST relation. For
example, we may want to convey the piece of in-
formation A,B,C,D,E in that order, with rel1(A,B)
and rel2(D,E). Whereas a simple rule may apply to
relations and turn them into dialogue, C is left un-
touched. However, a null rule can be applied to C, to
also turn its presentation into a dialogue exchange.
5.3 Case Study: the Attribution Rule
In this section we present a comparison of manu-
ally created rules for the RST attribution relation and
rules extracted from the CODA corpus.
Hernault et al manually construct two surface-
level rules for the Attribution (S,N)7 relation (see
7N is a nucleus phrase that carries main information and S is
Table 9). In the Dialogue Act column we show
the dialogue act representation of the correspond-
ing surface-level rules. The first rule converts attri-
bution relation into a Complex-Info-Request by the
Layman followed with the Explain by the Expert.
The second rule converts the attribution relation into
Explain by the Expert, Factoid-Info-Request by the
Layman and Factoid-Response by Expert. In both
rules, the Expert is the one providing information
(N) to the Layman and information is presented in
Explain dialogue act
Table 10 shows six attribution rules we collected
from phrases with attribution relation in the corpus
(Twain1-4,Berkeley1,Gurevich)8. We notice several
differences with the manually constructed rules:
? The variety of dialogue act sequences: each
RHS of the rule (or dialogue act sequence) is
different.
? Main information (N) can be presented by
either the expert (Twain1, Twain2, Twain3,
Berkeley1) or by the layman (Twain4, Gure-
vich1).
? Main information (N) can be presented in
different dialogue acts: Explain dialogue act
(Twain1, Twain4, Berkeley), YN-Info-Request
(Twain2, Twain3), or Complex-Info-Request
(Gurevich).
? Contextual information is part of the rule and
may be used when choosing which rule to ap-
ply.
6 Conclusions and Further Work
In this paper, we have introduced a new approach to
creating resources for automatically generating ex-
pository dialogue. The approach is based on ex-
tracting high-level rules from RST relations to Di-
alogue Act sequences using a parallel Monologue-
Dialogue corpus. The approach results in rules that
are reusable across applications and based on known
expert dialogue authors.
After examining differences between the dia-
logues in the corpus in order to obtain prima facie
evidence for differences in style, we conducted a
detailed evaluation of the rules that were extracted
a satellite phrase that contains the entity to whom N is attributed
8These are all the rules for attribution RST relation from 50
annotated turns for each author
Turn Speaker Dialogue act Dialogue
Contrast rule. Segment with contrast relation:
[He never does anything for any one else?s comfort , spiritual or physical.] [EXCEPT ON THOSE DISTINCT TERMS
? that it shall FIRST secure HIS OWN spiritual comfort ].
Null rule. Segment without rhetorical relation:
He never does anything for any one else?s comfort , spiritual or physical
1 Layman decorative Come!
2 Expert Init-YN-Request He never does anything for any one else ? s comfort , spiritual or physical ?
3 Expert Resp-Answer-No No
4 Expert Explain EXCEPT ON THOSE DISTINCT TERMS ? that it shall FIRST secure HIS
OWN spiritual comfort .
Table 8: Contrast rule example containing null rule from Twain dialogue.
Rule 1
Speaker Surface-level Rule Dialogue act Example Dialogue
Layman What did + GetSubject(S+N) + Getmain-
VerbLemma(S+N)
Complex-Info-Request What did S say?
Expert AddifNotPresentIn(N, That) + N Explain N
Rule 2
Expert RemoveIfPresentIn(N, That) + N Explain N
Layman Who GetMainVerb(N) that? Factoid-Info-Req Who said that?
Expert GetSubjectFromSentence(S+N) Factoid-Response S did
Table 9: Manually created rules for Attribution(S,N) relation (Hernault et al, 2008)
from the corpus. We extracted 167 distinct rules and
discussed the three types of rules: null, simple and
complex (depending on the number of RST relation
in the LHS: 0, 1 or more).
We found differences between authors in several
respects, specifically:
? number of turns per simple rule
? number of dialogue acts per simple rule
? combination of speaker roles and dialogue acts
A detailed comparison between our automatically
extracted attribution rule and the hand-crafted rules
used by Hernault et al showed up a number of
differences. Apart from the fact that the corpus
yielded many more rules than the two manually cre-
ated ones, there were differences in which interlocu-
tor presented particular information and which dia-
logue acts were being used.
The current work has focussed on high-level map-
ping rules which can be used both for generation
from databases and knowledge representations and
also for generation from text. In future work, we
will focus on mapping text (in monologue form) to
dialogue. For this we need to combine the high-
level rules with rules for paraphrasing the text in the
monologue with text for the dialogue acts that ex-
press the same information in dialogue form. For
automatically extracting these surface level map-
pings we will draw on the approach to learning para-
phrases from a corpus that is described in Barzilay
and McKeown (2001). An important component of
our future effort will be to evaluate whether automat-
ically generating dialogues from naturally-occurring
monologues, following the approach described here,
results in dialogues that are fluent and coherent and
preserve the information from the input monologue.
Acknowledgements
We would like to thank the anonymous reviewers
of INLG2010 for their helpful comments and our
colleagues in the Open University?s NLG group
for stimulating discussions on the content of this
paper. The research reported in this paper was
carried out as part of the CODA project (CO-
herent Dialogue Automatically generated from
text; see http://computing.open.ac.uk/coda/)
which is funded by the UK Engineering and
Physical Sciences Research Council under grant
EP/G/020981/1.
References
E. Andre?, T. Rist, S. van Mulken, M. Klesen, and
S. Baldes. 2000. The automated design of believable
dialogues for animated presentation teams. In Em-
Speaker Dialogue act Dialogue
Twain1 I will put that law into words, keep it in your mind: FROM HIS CRADLE TO HIS GRAVE A MAN NEVER DOES...
Satellite of Summary
Layman Init-YN-InfoReq Will you put that law into words?
Expert Resp-Answer-Yes Yes.
Expert Resp-Explain This is the law, keep it in your mind. FROM HIS CRADLE TO HIS GRAVE A
MAN NEVER DOES...
Twain2 I can not imagine that there is some other way of looking at it. Satellite of Explanation
Expert Init-Complex-InfoReq /clarify What makes you think that?
Layman decorative Pray what else could I think?
Expert Init-YN-InfoReq Do you imagine that there is some other way of looking at it?
Twain3 One cannot doubt that he felt well.Satellite of Evaluation-Conclusion
Expert Init-YN-InfoReq He felt well?
Layman Resp-Answer-Yes One cannot doubt it.
Twain4 As I said a minute ago Hamilton fought that duel to get PUBLIC approval. Nucleus of Explanation
Layman Init-Explain/contradict A minute ago you said Hamilton fought that duel to get PUBLIC approval.
Resp-Agree Resp-Agree I did.
Berkeley1 You can not conceive a vehement sensation to be without pain or pleasure.
Expert Init-Explain Again, try in your thoughts, Hylas, if you can conceive a vehement sensation to
be without pain or pleasure.
Layman Resp-Contradict You can not.
Gurevich I will explain what static algebras are exactly. Nucleus of Statement-response
Layman Init-Complex-InfoReq Please explain to me what static algebras are exactly.
Expert Resp-Agree Gladly.
Table 10: Attribution Examples. Satellite is italicised.
bodied Conversational Agents, pages 220?255. MIT
Press, Cambridge, Mass.
R. Barzilay and K. McKeown. 2001. Extracting Para-
phrases from a Parallel Corpus. In Proceedings of the
ACL, Toulouse, France.
A. Belz and E. Kow. 2009. System Building Cost vs.
Output Quality in Data-to-Text Generation. In Pro-
ceedings of the 12th European Workshop on Natural
Language Generation (ENLG?09), Athens, Greece.
J. Carletta, A. Isard, and J. C. Kowtko. 1997. The relia-
bility of a dialogue structure coding scheme. Compu-
tational Linguistics, 23:13?31.
L. Carlson and D. Marcu. 2001. Discourse tagging
reference manual. Technical Report ISI-TR-545, ISI,
September.
M. Core and J. Allen. 1997. Coding dialogs with the
damsl annotation scheme. In Working Notes: AAAI
Fall Symposium on Communicative Action in Humans
and Machine.
D. DeVault, D. Traum, and R. Artstein. 2008. Making
Grammar-Based Generation Easier to Deploy in Dia-
logue Systems. In Procs SIGdial 2008, Ohio, June.
E.Reiter and S. Williams. 2008. Three approaches to
generating texts in different styles. In Proceedings of
the Symposium on Style in text: creative generation
and identification of authorship.
J. Henderson, O. Lemon, and K. Georgila. 2008. Hy-
brid Reinforcement / Supervised Learning of Dialogue
Policies from Fixed Datasets. Computational Linguis-
tics, 34(4):487?511.
H. Hernault, P. Piwek, H. Prendinger, and M. Ishizuka.
2008. Generating dialogues for virtual agents using
nested textual coherence relations. In IVA08: 8th In-
ternational Conference on Intelligent Virtual Agents.
William C. Mann and Sandra A. Thompson. 1988.
Rhetorical structure theory: Toward a functional the-
ory of text organization. Text, 8(3):243?281.
D. Marcu. 1997. From Discourse Structures to
Text Summaries. In The Proceedings of the
ACL?97/EACL?97 Workshop on Intelligent Scalable
Text Summarization, pages 82?88, Madrid, Spain.
D. McDonald and J. Pustejovsky. 1985. A computational
theory of prose style for natural language generation.
In Proceedings of the second conference on European
chapter of the Association for Computational Linguis-
tics, pages 187?193, Geneva, Switzerland.
A. Oh and A. Rudnicky. 2002. Stochastic natural lan-
guage generation for spoken dialog. Computer Speech
and Language, 16(3/4):387?407.
P. Piwek, B. Krenn, M. Schroeder, M. Grice, S. Bau-
mann, and H. Pirker. 2002. RRL: A Rich Repre-
sentation Language for the Description of Agent Be-
haviour in NECA. In Proceedings of the AAMAS work-
shop ?Embodied conversational agents - let?s specify
and evaluate them!?, Bologna, Italy, July.
P. Piwek, H. Hernault, H. Prendinger, and M. Ishizuka.
2007. T2D: Generating Dialogues between Virtual
Agents Automatically from Text. In Intelligent Virtual
Agents, LNAI 4722, pages 161?174. Springer Verlag.
P. Piwek. 2008. Presenting Arguments as Fictive Dia-
logue. In Proceedings of 8th Workshop on Computa-
tional Models of Natural Argument (CMNA08), Patras,
Greece, July. ISBN 978-960-6843-12-9.
E. Reiter and R. Dale. 2000. Building Natural Lan-
guage Generation Systems. Cambridge University
Press, Cambridge.
K. van Deemter, B. Krenn, P. Piwek, M. Klesen,
M. Schroeder, and S. Baumann. 2008. Fully gener-
ated scripted dialogue for embodied agents. Artificial
Intelligence Journal, 172(10):1219?1244.
S. Williams, P. Piwek, and R. Power. 2007. Generat-
ing Monologue and Dialogue to Present Personalised
Medical Information to Patients. In Procs ENLG
2007, pages 167?170, Schloss Dagstuhl, Germany.
The First Question Generation Shared Task Evaluation 
Challenge 
Vasile Rus1, Brendan Wyse2, Paul Piwek2, Mihai Lintean1, Svetlana Stoyanchev2 
and Cristian Moldovan1
 
1 Department of Computer Science/Institute for Intelligent Systems, The University of 
Memphis, Memphis, TN, 38152, USA 
{vrus,mclinten,cmoldova}@memphis.edu 
2 Centre for Research in Computing, Open University, UK 
bjwyse@gmail.com and {p.piwek, s.stoyanchev}@open.ac.uk 
 
Abstract. The paper briefly describes the First Shared Task Evaluation 
Challenge on Question Generation that took place in Spring 2010. The 
campaign included two tasks: Task A ? Question Generation from Paragraphs 
and Task B ? Question Generation from Sentences. An overview of each of the 
tasks is provided.   
Keywords: question generation, shared task evaluation campaign. 
1   Introduction 
Question Generation is an essential component of learning environments, help 
systems, information seeking systems, multi-modal conversations between virtual 
agents, and a myriad of other applications (Lauer, Peacock, and Graesser, 1992; 
Piwek et al, 2007). 
Question Generation has been recently defined as the task (Rus & Graesser, 2009) 
of automatically generating questions from some form of input. The input could vary 
from information in a database to a deep semantic representation to raw text. 
The first Shared Task Evaluation Challenge on Question Generation (QG-STEC) 
follows a long tradition of STECs in Natural Language Processing (see the annual 
tasks run by the Conference on Natural Language Learning - CoNLL). In particular, 
the idea of a QG-STEC was inspired by the recent activity in the Natural Language 
Generation (NLG) community to offer shared task evaluation campaigns as a 
potential avenue to provide a focus for research in NLG and to increase the visibility 
of NLG in the wider Natural Language Processing (NLP) community (White and 
Dale, 2008). It should be noted that the QG is currently perceived as a discourse 
processing task rather than a traditional NLG task (Rus & Graesser, 2009). 
Two core aspects of a question are the goal of the question and its importance. It is 
difficult to determine whether a particular question is good without knowing the 
context in which it is posed; ideally one would like to have information about what 
counts as important and what the goals are in the current context. This suggests that a 
STEC on QG should be tied to a particular application, e.g. tutoring systems. 
However, an application-specific STEC would limit the pool of potential participants 
to those interested in the target application. Therefore, the challenge was to find a 
framework in which the goal and importance are intrinsic to the source of questions 
and less tied to a particular context/application. One possibility was to have the 
general goal of asking questions about salient items in a source of information, e.g. 
core ideas in a paragraph of text. Our tasks have been defined with this concept in 
mind. Adopting the basic principle of application-independence has the advantage of 
escaping the problem of a limited pool of participants (to those interested in a 
particular application had that application been chosen as the target for a QG STEC). 
Another decision aimed at attracting as many participants as possible and 
promoting a more fair comparison environment was the input for the QG tasks. 
Adopting a specific representation for the input would have favored some participants 
already familiar with such a representation. Therefore, we have adopted as a second 
guiding principle for the first QG-STEC tasks: no representational commitment. That 
is, we wanted to have as generic an input as possible. The input to both task A and B 
in the first QG STEC is raw text. 
The First Workshop on Question Generation (www.questiongeneration.org) has 
identified four categories of QG tasks (Rus & Graesser, 2009): Text-to-Question, 
Tutorial Dialogue,  Assessment, and Query-to-Question. The two tasks in the first QG 
STEC are part of the Text-to-Question category or part of the Text-to-text Natural 
Language Generation task categories (Dale & White, 2007). It is important to say that 
the two tasks offered in the first QG STEC were selected among 5 candidate tasks by 
the members of the QG community. A preference poll was conducted and the most 
preferred tasks, Question Generation from Paragraphs (Task A) and Question 
Generation from Sentences (Task B), were chosen to be offered in the first QG STEC. 
The other three candidate tasks were: Ranking Automatically Generated Questions 
(Michael Heilman and Noah Smith), Concept Identification and Ordering (Rodney 
Nielsen and Lee Becker), and Question Type Identification (Vasile Rus and Arthur 
Graesser). 
There is overlap between Task A and B. This was intentional with the aim of 
encouraging people preferring one task to participate in the other. The overlap 
consists of the specific questions in Task A which are more or less similar with the 
type of questions targeted by Task B. 
Overall, we had 1 submission for Task A and 4 submissions for Task B. We also 
had an additional submission on development data for Task A. 
2   TASK A: Question Generation from Paragraphs 
1.1   Task Definition 
The Question Generation from Paragraphs (QGP) task challenges participants to 
generate a list of 6 questions from a given input paragraph. The six questions should 
be at three scope levels: 1 x broad (entire input paragraph), 2 x medium (multiple 
sentences), and 3 x specific (sentence or less). The scope is defined by the portion of 
the paragraph that answers the question. 
The Question Generation from Paragraphs (QGP) task has been defined such that it 
is application-independent. Application-independent means questions will be judged 
based on content analysis of the input paragraph; questions whose answers span more 
input text are ranked higher. 
We show next an example paragraph together with six interesting, application-
independent questions that could be generated. We will use the paragraph and 
questions to describe the judging criteria. 
Table 1.  Example of input paragraph (from  http://en.wikipedia.org/wiki/Abraham_lincoln).  
Input Paragraph 
Abraham Lincoln (February 12, 1809 ? April 15, 1865), the 16th 
President of the United States, successfully led his country through 
its greatest internal crisis, the American Civil War, preserving the 
Union and ending slavery. As an outspoken opponent of the 
expansion of slavery in the United States, Lincoln won the 
Republican Party nomination in 1860 and was elected president 
later that year. His tenure in office was occupied primarily with the 
defeat of the secessionist Confederate States of America in the 
American Civil War. He introduced measures that resulted in the 
abolition of slavery, issuing his Emancipation Proclamation in 1863 
and promoting the passage of the Thirteenth Amendment to the 
Constitution. As the civil war was drawing to a close, Lincoln 
became the first American president to be assassinated. 
Table 2.  Examples of questions and scores for the paragraph in Table 1.  
Questions Scope 
Who is Abraham Lincoln? General 
What major measures did President Lincoln introduce? Medium 
How did President Lincoln die? Medium 
When was Abraham Lincoln elected president? Specific 
When was President Lincoln assassinated? Specific 
What party did Abraham Lincoln belong to? Specific 
 
 
A set of five scores, one for each criterion (specificity, syntax, semantics, question 
type correctness, diversity), and a composite score will be assigned to each question. 
Each question at each position will be assigned a composite score ranging from 1 
(first/top ranked, best) to 4 (worst rank), 1 meaning the question is at the right level of 
specificity given its rank (e.g. the broadest question that the whole paragraph answers 
will get a score of 1 if in the first position) and also it is syntactically and semantically 
correct as well as unique/diverse from other generated questions in the set. 
Ranking of questions based on scope assures a maximum score for the six 
questions of 1, 2, 2, 3, 3 and 3, respectively. A top-rank score of 1 is assigned to a 
broad scope question that is also syntactically and semantically correct or acceptable, 
i.e. if it is semantically ineligible then a decision about its scope cannot be made and 
thus a worst-rank score of 4 is assigned. A maximum score of 2 is assigned to 
medium-scope questions while a maximum score of 3 is assigned to specific 
questions. The best configuration of scores (1, 2, 2, 3, 3, 3) would only be possible for 
paragraphs that could trigger the required number of questions at each scope level, 
which may not always be the case. 
1.3   Data Sources and Annotation 
The primary source of input paragraphs were: Wikipedia, OpenLearn, 
Yahoo!Answers. We collected 20 paragraphs from each of these three sources. We 
collected both a development data set (65 paragraphs) and a test data set (60 
paragraphs). For the development data set we manually generated and scored 6 
questions per paragraph for a total of 6 x 65 = 390 questions.  
Paragraphs were selected such that they are self-contained (no need for previous 
context to be interpreted, e.g. will have no unresolved pronouns) and contain around 
5-7 sentences for a total of 100-200 tokens (excluding punctuation). In addition, we 
aimed for a diversity of topics of general interest. 
We also provided discourse relations based on HILDA, a freely available 
automatic discourse parser (duVerle & Prendinger, 2009). 
2   TASK B: Question Generation from Sentences 
2.1   Task Definition 
Participants were given a set of inputs, with each input consisting of:  
 
? a single sentence and  
? a specific target question type (e.g., WHO?, WHY?, HOW?, WHEN?; see 
below for the complete list of types used in the challenge).  
 
For each input, the task was to generate 2 questions of the specified target question 
type.  
Input sentences, 60 in total, were selected from OpenLearn, Wikipedia and Yahoo! 
Answers (20 inputs from each source). Extremely short or long sentences were not 
included. Prior to receiving the actual test data, participants were provided with a 
development data set consisting of sentences from the aforementioned sources and, 
for one or more target question types, examples of questions. These questions were 
manually authored and cross-checked by the team organizing Task B.  
The following example is taken from the development data set. Each instance has a 
unique identifier and information on the source it was extracted from. The <text> 
element contains the input sentence and the <question> elements contain possible 
questions. The <question> element has the type attribute for specification of the target 
question type. 
 
<instance id="3">  
 <id>OpenLearn</id>  
 <source>A103_5</source>  
 <text> 
  The poet Rudyard Kipling lost his only son  
  in the trenches in 1915. 
 </text>  
 <question type="who"> 
  Who lost his only son in the trenches in 1915? 
 </question> 
 <question type="when"> 
  When did Rudyard Kipling lose his son? 
 </question> 
 <question type="how many"> 
  How many sons did Rudyard Kipling have? 
 </question> 
</instance> 
 
Note that input sentences were provided as raw text. Annotations were not 
provided. There are a variety of NLP open-source tools available to potential 
participants and the choice of tools and how these tools are used was considered a 
fundamental part of the challenge.  
This task was restricted to the following question types: WHO, WHERE, WHEN, 
WHICH, WHAT, WHY, HOW MANY/LONG, YES/NO. Participants were provided 
with this list and definitions of each of the items in it. 
2.2   Evaluation criteria for System Outputs and Human Judges 
The evaluation criteria fulfilled two roles. Firstly, they were provided to the 
participants as a specification of the kind of questions that their systems should aim to 
generate. Secondly, they also played the role of guidelines for the judges of system 
outputs in the evaluation exercise.   
For this task, five criteria were identified: relevance, question type, syntactic 
correctness and fluency, ambiguity, and variety. All criteria are associated with a 
scale from 1 to N (where N is 2, 3 or 4), with 1 being the best score and N the worst 
score. 
The procedure for applying these criteria is as follows: 
 
? Each of the criteria is applied independently of the other criteria to each of 
the generated questions (except for the stipulation provided below).  
 
We need some specific stipulations for cases where no question is returned in 
response to an input. For each target question type, two questions are expected. 
Consequently, we have the following two possibilities regarding missing questions: 
 
? No question is returned for a particular target question type: for each of 
the missing questions, the worst score is recorded for all criteria. 
? Only one question is returned: For the missing question, the worst score is 
assigned on all criteria. The question that is present is scored following 
the criteria, with the exception of the VARIETY criterion for which the 
lowest possible score is assigned.  
 
We compute the overall score on a specific criterion. We can also compute a score 
which aggregates the overall scores for the criteria. 
Conclusions 
The submissions to the first QG STEC are now being evaluated using peer-review 
mechanism in which participants blindly evaluate their peers questions. At least two 
reviews per submissions are performed with the results to be made public at the 3rd 
Workshop on Question Generation that will take place in June 2010. 
 
Acknowledgments. We are grateful to a number of people who contributed to the 
success of the First Shared Task Evaluation Challenge on Question Generation: 
Rodney Nielsen, Amanda Stent, Arthur Graesser, Jose Otero, and James Lester. Also, 
we would like to thank the National Science Foundation who partially supported this 
work through grants RI-0836259 and RI-0938239 (awarded to Vasile Rus) and the 
Engineering and Physical Sciences Research Council who partially supported the 
effort on Task B through grant EP/G020981/1 (awarded to Paul Piwek). The views 
expressed in this paper are solely the authors?. 
References 
1. Lauer, T., Peacock, E., & Graesser, A. C. (1992) (Eds.). Questions and information systems. 
Hillsdale, NJ: Erlbaum. 
2. Rus, V. and Graesser, A.C. (2009). Workshop Report: The Question Generation Task and 
Evaluation Challenge, Institute for Intelligent Systems, Memphis, TN, ISBN: 978-0-615-
27428-7.  
3. Piwek, P., H. Hernault, H. Prendinger, M. Ishizuka (2007). T2D: Generating Dialogues 
between Virtual Agents Automatically from Text. In: Intelligent Virtual Agents: 
Proceedings of IVA07, LNAI 4722, September 17-19, 2007, Paris, France, (Springer-
Verlag, Berlin Heidelberg) pp.161-174 
4. Dale, R. & M. White (2007) (Eds.). Position Papers of the Workshop on Shared Tasks and 
Comparative Evaluation in Natural Language Generation. 
5. duVerle, D. and Prendinger, H. (2009). A novel discourse parser based on Support Vector 
Machines. Proc 47th Annual Meeting of the Association for Computational Linguistics and 
the 4th Int'l Joint Conf on Natural Language Processing of the Asian Federation of Natural 
Language Processing (ACL-IJCNLP'09), Singapore, Aug 2009 (ACL and AFNLP), pp 665-
673. 
 
 
Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 335?337,
Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational Linguistics
The CODA System for Monologue-to-Dialogue Generation
Svetlana Stoyanchev
Centre for Research in Computing
The Open University
Walton Hall, Milton Keynes, UK
s.stoyanchev@open.ac.uk
Paul Piwek
Centre for Research in Computing
The Open University
Walton Hall, Milton Keynes, UK
p.piwek@open.ac.uk
Abstract
This paper describes an implemented mono-
lingual Text-to-Text generation system. The
system takes monologue and transforms it to
two-participant dialogue. The system uses
mappings between discourse relations in text
and dialogue acts in dialogue. These map-
pings are extracted from a parallel monologue
and dialogue corpus.
1 Introduction
This paper describes the CODA system,1 a Text-to-
Text generation system that converts text parsed with
discourse relations (Mann and Thompson, 1988)
into information-delivering dialogue between two
characters. By information-delivering dialogue, we
mean dialogue (akin to that used by Plato) that is
used primarily to convey information and possibly
also to make an argument; this in contrast with dra-
matic dialogue which focuses on character develop-
ment and narrative.
Several empirical studies show that delivering
information as dialogue, rather than monologue,
can be particularly effective for education (Craig
et al, 2000; Lee et al, 1998) and persuasion
(Suzuki and Yamada, 2004). Information-delivering
dialogue also lends itself well for presentation
through computer-animated agents (Prendinger and
Ishizuka, 2004).
1CODA stands for COherent Dialogue Automatically gen-
erated from text (see http://computing.open.ac.uk/coda/). The
CODA project is funded by the UK?s Engineering and Physical
Sciences Research Council under Grant EP/G020981/1.
With most information locked up in text (books,
newspapers, leaflets, etc.), automatic generation of
dialogue from text in monologue makes it possible
to convert information into dialogue on demand.
In contrast to previous Text-to-Dialogue sys-
tems (Piwek et al, 2007), the CODA system is data-
driven and modular. The system is composed of
three modules: Dialogue Modeller, Verbalizer, and
Dialogue Merger.
The Dialogue modeller determines appropriate
dialogue act sequences that can be used for con-
verting a segment of input text containing a sin-
gle discourse relation into dialogue. The mod-
ule is data-oriented in that the mappings it uses
between discourse structure and dialogue act se-
quences have been derived from the CODA paral-
lel monologue/dialogue corpus (Stoyanchev and Pi-
wek, 2010).
The Verbalizer converts text segments together
with a specification of the target dialogue act types
into dialogue utterances.
The Dialogue modeller and verbaliser compo-
nents overgenerate possible outputs for each dis-
course relation in monologue. The Dialogue Merger
component selects one of the proposed outputs for
each text segment of the input and merges them into
a single coherent dialogue.
2 System Design
In this section we describe the three components of
the system: dialogue modeller, verbalizer, and dia-
logue merger.
Before we look at each of the modules, we, how-
ever, first need to specify more precisely what the
335
Input MANNER-MEANS [In September,
Ashland settled the long-simmering
dispute] [by agreeing to pay Iran
$325 million.]
Dialogue 1. (ComplexQ; Explain)
Modeller 2. (Explain; ComplexQ; Explain)
3. (Explain; YesNoQ; Explain)
Verbalizer
DA Seq1
A: How did Ashland settle the long-
simmering dispute in September?
B: By agreeing to pay Iran $325
million.
Verbalizer
DA Seq2
A: In September, Ashland settled
the long-simmering dispute.
B: How?
A: By agreeing to pay Iran $325
million.
Verbalizer
DA Seq3
A: In September, Ashland settled
the long-simmering dispute.
B: By agreeing to pay Iran $325
million?
A: Correct.
Dialogue
Merger
Select one of the DA sequences
based on overall dialogue
Table 1: Example of the output from each component
input for our system is. The system expects text that
has already been annotated with a discourse struc-
ture. There have been recent encouraging advances
in the automatic parsing of discourse structure, e.g.,
see duVerle and Prendinger (2009), but the state-of-
the-art is not yet at a point where it provides suffi-
ciently reliable inputs for our purposes. To demon-
strate the functionality of our system without relying
on still imperfect discourse parsing, we use the RST-
parsed Wall Street Journal corpus as input (Carlson
et al, 2001).
Throughout the remainder of this section, we use
the outputs for each of the modules in Table 1 as a
running example.
2.1 Dialogue Modeller
The Dialogue Modeller component takes as input a
snippet of monologue text annotated with discourse
structure. For each input Discourse Relation struc-
ture (DR), the dialogue modeller outputs a set of dia-
logue act (DA) sequences appropriate for expressing
the same information, but now in dialogue form.
The Dialogue modeller uses a configuration XML
file to look up possible DA sequences for the input
DA sequence
YesNoQ; Explain
YesNoQ; Yes; Explain
Explain; ComplexQ; Explain
ComplexQ; Explain
Explain; YesNoQ; Resp-Answer-Yes
Explain; Contradict
Factoid-Info-Req;Factoid-Resp;Explain
Exlain; Resp-Agree;Explain
Table 2: Dialogue act sequences
discourse structure. In the current system configu-
ration we extract these mappings from the CODA
parallel corpus of professionally authored dialogues
and parallel monologues. We use the eight most fre-
quent DA sequences (see Table2) that occur on the
dialogue side of discourse relations in the parallel
dataset. Each discourse relation is mapped to one
or more DA sequences with a score indicating fre-
quency of this mapping in the CODA corpus.
The dialogue modeller can be customised with
mappings from other sources such as a different cor-
pus, manually authored mappings or a mapping ar-
rived at through experimental methods.
The current version of the dialogue modeller sup-
ports input with only one level of discourse structure
annotation. As a result, all input structures contain
parts made of two segments and one discourse rela-
tion between these segments. In the future work, we
plan to implement a dialogue modeller that accepts
more complex (nested) discourse structures.
2.2 Verbalizer
The verbalizer is rule-based and has three types of
rules: discourse relation (DR)-specific, generic, and
canned. All of the rules take as input a monologue
segment and a target dialogue act. DR-specific rules
also use the discourse relation and segment nuclear-
ity of the input segment.2 The verbalization rules are
ordered according to their priority with DR-specific
rules having a higher priority.
Generic and DR-specific rules use the CMU ques-
tion generation tool (Heilman and Smith, 2010) in
combination with syntactic and lexical manipulation
rules. Canned text rules are used to generate An-
swerYes, Agree and Clarify dialogue acts by proba-
2Nucleus is the more salient segment in a relation.
336
bilistic selection from a set of utterances extracted
from the CODA corpus. For example, the Agree
dialogue act is verbalized as one of the statements:
I agree with you; I agree; I couldn?t agree more;
I completely agree; Absolutely; Very true; Right;
True. Probabilistic selection from a list allows us
to generate non-repetitive dialogues. The system is
extendible, such that new rules can be easily added
to the implementation.
2.3 Dialogue Merger
The Dialogue Merger component takes as input ver-
balized dialogue act sequences. The tasks of the Di-
alogue Merger include: 1) selecting the best ver-
balized sequence and 2) assigning speaker roles
(TEACHER or STUDENT) to dialogue turns.
We aim to create diverse dialogues, in particular,
by avoiding repetitive use of the same dialogue act
sequences. This is achieved as follows. Selection of
DA sequence is incremental, considering one rela-
tion at a time. For each relation, the dialogue merger
selects a dialogue act sequence that has been suc-
cessfully verbalized by the verbalizer and which, so
far, has been used the smallest number of times (out
of all the sequences that have been used up to this
point).
Although in the original authored dialogues, both
TEACHER and STUDENT ask questions and give ex-
planations, in our preliminary experiments observers
made negative comments about mixing initiative be-
tween the STUDENT and the TEACHER in the gen-
erated dialogues. In the current version, the speaker
roles are assigned based on the dialogue act. All
questions and clarification requests are assigned to
the STUDENT and other dialogue acts are assigned
to the TEACHER.
As an additional post-processing step, to main-
tain perspective in the dialogue, we change pronouns
in the dialogue turns. The turns assigned to the
TEACHER character remain unchanged. The turns
assigned to the STUDENT character change the per-
spective: non-possessive pronouns are inverted, e.g.
you ? I, we ? us, my ? your.
3 Conclusions and Further Work
In this paper, we described a Text-to-Dialogue gen-
eration system that converts text annotated with dis-
course relations into dialogue. The system is modu-
lar, data-driven, and takes advantage of state-of-the-
art question generation tools. Our evaluation of the
dialogue modeller and verbalizer components de-
scribed in (Piwek and Stoyanchev, 2011) shows that
both accuracy and fluency of generated dialogues
are not worse than that of human-written dialogues.
We plan to release the CODA Text-to-Dialogue
system as open source code later this year. The sys-
tem can be used as a starting point for researchers
interested in evaluating NLP tools for question gen-
eration, dialogue modelling and paraphrasing in a
dialogue generation task.
References
L. Carlson, D. Marcu, and M. E. Okurowski. 2001.
Building a discourse-tagged corpus in the framework
of rhetorical structure theory. In Proceedings of the
Second SIGdial Workshop on Discourse and Dialogue,
SIGDIA.
S. Craig, B. Gholson, M. Ventura, A. Graesser, and the
Tutoring Research Group. 2000. Overhearing dia-
logues and monologues in virtual tutoring sessions.
International Journal of Artificial Intelligence in Ed-
ucation, 11:242?253.
D. duVerle and H. Prendinger. 2009. A novel discourse
parser based on support vector machines. In Procs of
ACL-IJCNLP), pages 665?673, Singapore, August.
M. Heilman and N. A. Smith. 2010. Good question!
statistical ranking for question generation. In Proc. of
NAACL/HLT, Los Angeles.
J. Lee, F. Dinneen, and J. McKendree. 1998. Supporting
student discussions: it isn?t just talk. Education and
Information Technologies, 3:217?229.
W. C. Mann and S. A. Thompson. 1988. Rhetorical
structure theory: Toward a functional theory of text
organization. Text, 8(3):243?281.
P. Piwek and S. Stoyanchev. 2011. Data-oriented
Monologue-to-Dialogue Generation. In Procs of ACL.
P. Piwek, H. Hernault, H. Prendinger, and M. Ishizuka.
2007. T2D: Generating Dialogues between Virtual
Agents Automatically from Text. In Procs of IVA07,
LNAI 4722, pages 161?174. Springer Verlag.
H. Prendinger and M. Ishizuka, editors. 2004. Life-Like
Characters: Tools, Affective Functions, and Applica-
tions. Cognitive Technologies Series. Springer, Berlin.
S. Stoyanchev and P. Piwek. 2010. Constructing the
CODA corpus. In Procs of LREC, Malta.
S. V. Suzuki and S. Yamada. 2004. Persuasion through
overheard communication by life-like agents. In Procs
of the 2004 IEEE/WIC/ACM International Conference
on Intelligent Agent Technology, Beijing.
337
INLG 2012 Proceedings of the 7th International Natural Language Generation Conference, pages 110?114,
Utica, May 2012. c?2012 Association for Computational Linguistics
Planning Accessible Explanations for Entailments in OWL Ontologies
Tu Anh T. Nguyen, Richard Power, Paul Piwek, Sandra Williams
The Open University
Milton Keynes, United Kingdom
{t.nguyen,r.power,p.piwek,s.h.williams}@open.ac.uk
Abstract
A useful enhancement of an NLG system for
verbalising ontologies would be a module ca-
pable of explaining undesired entailments of
the axioms encoded by the developer. This
task raises interesting issues of content plan-
ning. One approach, useful as a baseline, is
simply to list the subset of axioms relevant
to inferring the entailment; however, in many
cases it will still not be obvious, even to OWL
experts, why the entailment follows. We sug-
gest an approach in which further statements
are added in order to construct a proof tree,
with every step based on a relatively simple
deduction rule of known difficulty; we also de-
scribe an empirical study through which the
difficulty of these simple deduction patterns
has been measured.
1 Introduction
A practical problem in developing ontologies for
the semantic web is that mistakes are hard to spot.
One reason for this lies in the opacity of the stan-
dard OWL formalisms, such as OWL/RDF, which
are designed for efficient processing by computer
programs and not for fast comprehension by peo-
ple. Various tools have been proposed to address
this problem, including not only graphical interfaces
such as Prote?ge?, but NLG (Natural Language Gener-
ation) programs that verbalise the axioms of an on-
tology as text (Kaljurand and Fuchs, 2007; Schwit-
ter and Meyer, 2007; Hart et al, 2008). Using such a
tool, a mistaken axiom presented through a sentence
like ?Every person is a movie? immediately leaps to
the eye.
Although there is evidence that verbalisation
helps developers to check individual axioms
(Stevens et al, 2011), there remains a more subtle
problem of undesired entailments, often based on in-
teractions among axioms. The difference between
axioms and entailments is that whereas axioms are
statements encoded by the developer, entailments
are statements inferred from axioms by automated
reasoners such as FaCT++ (Tsarkov and Horrocks,
2006). Because reasoning systems interpret state-
ments absolutely literally, it is quite common for ap-
parently innocuous axioms to lead to absurd conclu-
sions such as ?Everything is a person?, ?Nothing is
a person?, or indeed ?Every person is a movie?. The
standard reasoning algorithms, based on tableau al-
gorithms, will compute these entailments efficiently,
but they provide no information that helps explain
why an undesired conclusion was drawn, and hence
which axiom or axioms need to be corrected.
To provide an explanation of an entailment, the
first step is obviously to determine which axioms are
relevant to the inference. A set of relevant axioms
is known technically as a justification of the entail-
ment, defined as any minimal subset of the ontology
from which the entailment can be drawn (Kalyan-
pur, 2006). The minimality requirement here means
that if any axiom is removed from a justification, the
entailment will no longer be inferable.
Drawing on Kalyanpur?s work, the most direct
strategy for planning an explanation is simply to
verbalise the axioms in the justification, followed
by the entailment, with no additional content. This
strategy serves as a useful baseline for comparison,
and might even be effective for some simple justi-
110
Entailment Person v Movie Every person is a movie.
1. GoodMovie ? ?hasRating.FourStars 1. A good movie is anything that only has ratings of four stars.
Justification 2. Domain(hasRating) = Movie 2. Anything that has a rating is a movie.
3. GoodMovie v StarRatedMovie 3. Every good movie is a star-rated movie.
4. StarRatedMovie v Movie 4. Every star-rated movie is a movie.
Table 1: An example justification that requires further explanation
fications; however, user studies have shown that in
many cases even OWL experts are unable to work
out how the conclusion follows from the premises
without further explanation (Horridge et al, 2009).
This raises two problems of content planning that
we now address: (a) how we can ascertain that fur-
ther explanation is needed, and (b) what form such
explanation should take.
2 Explaining complex justifications
An example of a justification requiring further ex-
planation is shown in Table 1. Statements are pre-
sented in mathematical notation in the middle col-
umn (rather than in OWL, which would take up a
lot more space), with a natural language gloss in the
right column. Since these sentences are handcrafted
they should be more fluent than the output of a ver-
baliser, but even with this benefit, it is extremely
hard to see why the entailment follows.
The key to understanding this inference lies in the
first axiom, which asserts an equivalence between
two classes: good movies, and things that only have
ratings of four stars. The precise condition for an in-
dividual to belong to the second class is that all of its
ratings should be four star, and this condition would
be trivially satisfied if the individual had no ratings
at all. From this it follows that people, parrots,
parsnips, or in general things that cannot have a rat-
ing, all belong to the second class, which is asserted
to be equivalent to the class of good movies. If in-
dividuals with no rating are good movies, then by
axioms 3 and 4 they are also movies, so we are left
with two paradoxical statements: individuals with a
rating are movies (axiom 2), and individuals without
a rating are movies (the intermediate conclusion just
derived). Since everything that exists must either
have some rating or no rating, we are driven to the
conclusion that everything is a movie, from which it
follows that any person (or parrot, etc.) must also be
a movie: hence the entailment. Our target explana-
tion for this case is as follows:
Every person is a movie because the ontology
implies that everything is a movie.
Everything is a movie because (a) anything that
has a rating is a movie, and (b) anything that has
no rating at all is a movie.
Statement (a) is stated in axiom 2 in the justifica-
tion. Statement (b) is inferred because the ontology
implies that (c) anything that has no rating at all
is a good movie, and (d) every good movie is a
movie.
Statement (d) is inferred from axioms 3 and 4 in
the justification. Statement (c) is inferred from
axiom 1, which asserts an equivalence between
two classes: ?good movie? and ?anything that has
as rating only four stars?. Since the second class
trivially accepts anything that has no rating at all,
we conclude that anything that has no rating at all
is a good movie.
Note that in this or any other intelligible explana-
tion, a path is traced from premises to conclusion by
introducing a number of intermediate statements, or
lemmas. Sometimes a lemma merely unpacks part
of the meaning of an axiom ? the part that actually
contributes to the entailment. This is clearly what
we are doing when we draw from axiom 1 the im-
plication that all individuals with no ratings are good
movies. Alternatively a lemma could be obtained by
combining two axioms, or perhaps even more. By
introducing appropriate lemmas of either type, we
can construct a proof tree in which the root node is
the entailment, the terminal nodes are the axioms in
the justification, and the other nodes are lemmas. An
explanation based on a proof tree should be easier to
understand because it replaces a single complex in-
ference step with a number of simpler ones.
Assuming that some kind of proof tree is needed,
the next question is how to construct proof trees that
provide effective explanations. Here two conditions
need to be met: (1) the proof tree should be correct,
in the sense that all steps are valid; (2) it should be
111
accessible, in the sense that all steps are understand-
able. As can be seen, one of these conditions is logi-
cal, the other psychological. Several research groups
have proposed methods for producing logically cor-
rect proof trees for description logic (McGuinness,
1996; Borgida et al, 1999; Horridge et al, 2010),
but explanations planned in this way will not nec-
essarily meet our second requirement. In fact they
could fail in two ways: either they might employ a
single reasoning step that most people cannot fol-
low, or they might unduly complicate the text by
including multiple steps where a single step would
have been understood equally well. We believe this
problem can be addressed by constructing the proof
tree from deduction rules for which the intuitive dif-
ficulty has been measured in an empirical study.1
3 Collecting Deduction Rules
For our purposes, a deduction rule consists of a
conclusion (i.e., an entailment) and up to three
premises from which the conclusion logically fol-
lows. Both conclusion and premises are generalised
by using variables that abstract over class and prop-
erty names, as shown in Table 2, where for example
the second rule corresponds to the well-known syl-
logism that from ?Every A is a B? and ?Every B is a
C?, we may infer ?Every A is a C?.
Our deduction rules were derived through a cor-
pus study of around 500 OWL ontologies. First
we computed entailment-justification pairs using the
method described in Nguyen et al (2010), and
collated them to obtain a list of deduction patterns
ranked by frequency. From this list, we selected pat-
terns that were simple (in a sense that will be ex-
plained shortly) and frequent, subsequently adding
some further rules that occurred often as parts of
more complex deduction patterns, but were not com-
puted as separate patterns because of certain limi-
tations of the reasoning algorithm.2 The deduction
rules required for the previous example are shown
1Deduction rules were previously used by Huang for re-
constructing machine-generated mathematical proofs; however,
these rules were not for description logic based proofs and
assumed to be intuitive to people (Huang, 1994). The out-
put proofs were then enhanced (Horacek, 1999) and verbalised
(Huang, 1994).
2Reasoning services for OWL typically compute only some
kinds of entailment, such as subclass and class membership
statements, and ignore others.
in Table 2. So far, 41 deduction rules have been ob-
tained in this way; these are sufficient to generate
proof trees for 48% of the justifications of subsump-
tion entailments in the corpus (i.e., over 30,000 jus-
tifications).
As a criterion of simplicity we considered the
number of premises (we stipulated not more than
three) and also what is called the ?laconic? property
(Horridge et al, 2008) ? that an axiom should not
contain information that is not required for the en-
tailment to hold. We have assumed that deduction
rules that are simple in this sense are more likely to
be understandable by people; we return to this issue
in section 5, which describes an empirical test of the
understandability of the rules.
4 Constructing Proof Trees
A proof tree can be defined as any tree linking the
axioms of a justification (terminal nodes) to an en-
tailment (root node), in such a way that every local
tree (i.e., every node and its children) corresponds
to a deduction rule. This means that if the entail-
ment and justification already correspond to a de-
duction rule, no further nodes (i.e., lemmas) need
to be added. Otherwise, a proof can be sought by
applying the deduction rules, where possible, to the
terminal nodes, so introducing lemmas and grow-
ing the tree bottom-up towards the root. Exhaus-
tive search using this method may yield zero, one or
multiple solutions ? e.g., for our example two proof
trees were generated, as depicted in Figure 1.3
5 Measuring understandability
To investigate the difficulty of deduction rules em-
pirically, we have conducted a survey in which 43
participants (mostly university staff and students un-
familiar with OWL) were shown the premises of the
rule, expressed as English sentences concerning fic-
titious entities, and asked to choose the correct con-
clusion from four alternatives. They were also asked
to rate the difficulty of this choice on a five-point
scale. For instance, in one problem the premises
3In the current implementation, the proof tree can also be de-
veloped by adding lemmas that unpack part of the meaning of
an axiom, using the method proposed by Horridge et al(2008).
These steps in the proof are not always obvious, so their under-
standability should also be measured.
112
ID Deduction Rule Example Success Rate
1 ?r.? v C Anything that has no ratings at all is a movie. 65%
?r.> v C Anything that has a rating is a movie.
? > v C ? Everything is a movie.
2 C v D Anything that has no ratings at all is a good movie. 88%
D v E Every good movie is a movie.
? C v E ? Anything that has no ratings at all is a movie.
3 C ? ?r.D A good movie is anything that only has ratings of four stars. ?
? ?r.? v C ? Anything that has no ratings at all is a good movie.
Table 2: Deduction rules for the example in Table 1
Figure 1: Proof trees generated by our current system
Figure 2: Results of the empirical study. In our difficulty
scale, 1 means ?very easy? and 5 means ?very difficult?
were ?Every verbeeg is a giantkin; no giantkin is
a verbeeg.?; to answer correctly, participants had to
tick ?Nothing is a verbeeg? and not ?Nothing is a gi-
antkin?.
So far 9/41 deduction rules have been measured
in this way. Figure 2 shows the success rates and the
means of difficulty of those rules. For most prob-
lems the success rates were around 80%, confirm-
ing that the rules were understandable, although in
a few cases performance fell to around 50%, sug-
gesting that further explanation would be needed.
The study also indicates a statistically significant re-
lationship between the accuracy of the participants?
performance and their perceptions of difficulty (r =
0.82, p < 0.01). Two of the three rules in Table 2
were measured in this way. The third rule has not
been tested yet; however, its success rate is expected
to be very low as it was proved to be a very difficult
inference (Horridge et al, 2009).
6 Conclusion
This paper has reported our work in progress on con-
tent planning for explanations of entailments. The
main steps involved in the planning process are sum-
113
Figure 3: Our approach for the content planning. E, J, Pn
are entailments, justifications and proofs respectively; d1
and d2 are difficulty scores and d2 ? d1
marised in Figure 3. We have focused on one as-
pect: the introduction of lemmas that mediate be-
tween premises and conclusion, so organising the
proof into manageable steps. Lemmas are derived
by applying deduction rules collected through a cor-
pus study on entailments and their justifications.
Through a survey we have measured the difficulty of
some of these rules, as evidenced by performance on
the task of choosing the correct conclusion for given
premises. These measures should indicate which
steps in a proof are relatively hard, and thus perhaps
in need of further elucidation, through special strate-
gies that can be devised for each problematic rule.
Our hypothesis is that these measures will also allow
an accurate assessment of the difficulty of a candi-
date proof tree, so providing a criterion for choos-
ing among alternatives ? e.g., by using the success
rates as an index of difficulty, we can sum the in-
dex over a proof tree to obtain a simple measure
of its difficulty. Our verbaliser currently translates
OWL statements literally, and needs to be improved
to make sure any verbalisations do not give rise to
unwanted presuppositions and Gricean implicatures.
Acknowledgments
This research was undertaken as part of the ongo-
ing SWAT project (Semantic Web Authoring Tool),
which is supported by the UK Engineering and
Physical Sciences Research Council (EPSRC). We
thank our colleagues and the anonymous viewers.
References
Alexander Borgida, Enrico Franconi, Ian Horrocks, Deb-
orah L. McGuinness, and Peter F. Patel-Schneider.
1999. Explaining ALC Subsumption. In DL 1999,
International Workshop on Description Logics.
Glen Hart, Martina Johnson, and Catherine Dolbear.
2008. Rabbit: developing a control natural language
for authoring ontologies. In ESWC 2008, European
Semantic Web Conference, pages 348?360.
Helmut Horacek. 1999. Presenting Proofs in a Human-
Oriented Way. In CADE 1999, International Confer-
ence on Automated Deduction, pages 142?156.
Matthew Horridge, Bijan Parsia, and Ulrike Sattler.
2008. Laconic and Precise Justifications in OWL. In
ISWC 2008, International Semantic Web Conference,
pages 323?338.
Matthew Horridge, Bijan Parsia, and Ulrike Sattler.
2009. Lemmas for Justifications in OWL. In DL 2009,
International Workshop on Description Logics.
Matthew Horridge, Bijan Parsia, and Ulrike Sattler.
2010. Justification Oriented Proofs in OWL. In ISWC
2010, International Semantic Web Conference, pages
354?369.
Xiaorong Huang. 1994. Human Oriented Proof Presen-
tation: A Reconstructive Approach. Ph.D. thesis, The
University of Saarbru?cken, Germany.
Kaarel Kaljurand and Norbert Fuchs. 2007. Verbaliz-
ing OWL in Attempto Controlled English. In OWLED
2007, International Workshop on OWL: Experiences
and Directions.
Aditya Kalyanpur. 2006. Debugging and repair of OWL
ontologies. Ph.D. thesis, The University of Maryland,
US.
Deborah Louise McGuinness. 1996. Explaining reason-
ing in description logics. Ph.D. thesis, The State Uni-
versity of New Jersey, US.
Tu Anh T. Nguyen, Paul Piwek, Richard Power, and San-
dra Williams. 2010. Justification Patterns for OWL
DL Ontologies. Technical Report TR2011/05, The
Open University, UK.
Rolf Schwitter and Thomas Meyer. 2007. Sydney OWL
Syntax - towards a Controlled Natural Language Syn-
tax for OWL 1.1. In OWLED 2007, International
Workshop on OWL: Experiences and Directions.
Robert Stevens, James Malone, Sandra Williams,
Richard Power, and Allan Third. 2011. Automating
generation of textual class definitions from OWL to
English. Journal of Biomedical Semantics, 2(S 2:S5).
Dmitry Tsarkov and Ian Horrocks. 2006. FaCT++ De-
scription Logic Reasoner: System Description. In IJ-
CAR 2006, International Joint Conference on Auto-
mated Reasoning, pages 292?297.
114
