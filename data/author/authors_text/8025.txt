Proceedings of NAACL HLT 2009: Short Papers, pages 181?184,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Improving SCL Model for Sentiment-Transfer Learning 
 
Songbo Tan 
Institute of Computing Technology 
Beijing, China 
tansongbo@software.ict.ac.cn
Xueqi Cheng 
Institute of Computing Technology 
Beijing, China 
cxq@ict.ac.cn 
 
ABSTRACT 
In recent years, Structural Correspondence 
Learning (SCL) is becoming one of the most 
promising techniques for sentiment-transfer 
learning. However, SCL model treats each 
feature as well as each instance by an 
equivalent-weight strategy. To address the two 
issues effectively, we proposed a weighted 
SCL model (W-SCL), which weights the 
features as well as the instances. More 
specifically, W-SCL assigns a smaller weight 
to high-frequency domain-specific (HFDS) 
features and assigns a larger weight to 
instances with the same label as the involved 
pivot feature. The experimental results 
indicate that proposed W-SCL model could 
overcome the adverse influence of HFDS 
features, and leverage knowledge from labels 
of instances and pivot features. 
 
1   Introduction 
In the community of sentiment analysis (Turney 
2002; Pang et al, 2002; Tang et al, 2009), 
transferring a sentiment classifier from one source 
domain to another target domain is still far from a 
trivial work, because sentiment expression often 
behaves with strong domain-specific nature.  
Up to this time, many researchers have 
proposed techniques to address this problem, such 
as classifiers adaptation, generalizable features 
detection and so on (DaumeIII et al, 2006; Jiang 
et al, 2007; Tan et al, 2007; Tan et al, 2008; Tan 
et al, 2009). Among these techniques, SCL 
(Structural Correspondence Learning) (Blitzer et 
al., 2006) is regarded as a promising method to 
tackle transfer-learning problem. The main idea 
behind SCL model is to identify correspondences 
among features from different domains by 
modeling their correlations with pivot features (or 
generalizable features). Pivot features behave 
similarly in both domains. If non-pivot features 
from different domains are correlated with many 
of the same pivot features, then we assume them 
to be corresponded with each other, and treat them 
similarly when training a sentiment classifier. 
However, SCL model treats each feature as well 
as each instance by an equivalent-weight strategy. 
From the perspective of feature, this strategy fails 
to overcome the adverse influence of high-
frequency domain-specific (HFDS) features. For 
example, the words ?stock? or ?market? occurs 
frequently in most of stock reviews, so these non-
sentiment features tend to have a strong 
correspondence with pivot features. As a result, 
the representative ability of the other sentiment 
features will inevitably be weakened to some 
degree.  
To address this issue, we proposed Frequently 
Exclusively-occurring Entropy (FEE) to pick out 
HFDS features, and proposed a feature-weighted 
SCL model (FW-SCL) to adjust the influence of 
HFDS features in building correspondence. The 
main idea of FW-SCL is to assign a smaller 
weight to HFDS features so that the adverse 
influence of HFDS features can be decreased. 
From the other perspective, the equivalent-
weight strategy of SCL model ignores the labels 
(?positive? or ?negative?) of labeled instances. 
Obviously, this is not a good idea. In fact, positive 
pivot features tend to occur in positive instances, 
so the correlations built on positive instances are 
more reliable than that built on negative instances; 
and vice versa. Consequently, utilization of labels 
of instances and pivot features can decrease the 
adverse influence of some co-occurrences, such as 
co-occurrences involved with positive pivot 
features and negative instances, or involved with 
negative pivot features and positive instances.  
In order to take into account the labels of 
labeled instances, we proposed an instance-
weighted SCL model (IW-SCL), which assigns a 
larger weight to instances with the same label as 
the involved pivot feature. In this time, we obtain 
a combined model: feature-weighted and instance-
weighted SCL model (FWIW-SCL). For the sake 
181
of convenience, we simplify ?FWIW-SCL? as 
?W-SCL? in the rest of this paper. 
2   Structural Correspondence Learning 
In the section, we provide the detailed procedures 
for SCL model. 
First we need to pick out pivot features. Pivot 
features occur frequently in both the source and 
the target domain. In the community of sentiment 
analysis, generalizable sentiment words are good 
candidates for pivot features, such as ?good? and 
?excellent?. In the rest of this paper, we use K to 
stand for the number of pivot features.  
Second, we need to compute the pivot 
predictors (or mapping vectors) using selected 
pivot features. The pivot predictors are the key job, 
because they directly decide the performance of 
SCL. For each pivot feature k, we use a loss 
function Lk, ( ) 21)( wxwxpL
i i
T
ikk ?+?=?        (1) 
where the function pk(xi) indicates whether the 
pivot feature k occurs in the instance xi, 
otherwise
xif
xp ikik
0 
1
1
)(
>
??
?
?= , 
where the weight vector w encodes the 
correspondence of the non-pivot features with the 
pivot feature k (Blitzer et al, 2006). 
Finally we use the augmented space [xT, xTW]T to 
train the classifier on the source labeled data and 
predict the examples on the target domain, where 
W=[w1,w2, ?, wK].  
3   Feature-Weighted SCL Model 
3.1 Measure to pick out HFDS features 
In order to pick out HFDS features, we proposed 
Frequently Exclusively-occurring Entropy (FEE). 
Our measure includes two criteria: occur in one 
domain as frequently as possible, while occur on 
another domain as rarely as possible. To satisfy 
this requirement, we proposed the following 
formula: 
( )( ) ( )( ) ???
?
???
?+=
)(),(min
)(),(max
log)(),(maxlog
wPwP
wPwP
wPwPf
no
no
now
(2) 
where Po(w) and Pn(w) indicate the probability of 
word w in the source domain and the target 
domain respectively: 
 ( )( )?
?
?+
+=
2
)(
)(
o
o
o N
wN
wP                     (3) 
( )
( )?
?
?+
+=
2
)(
)(
n
n
n N
wN
wP                     (4) 
where No(w) and Nn(w) is the number of examples 
with word w in the source domain and the target 
domain respectively; No and Nn is the number of 
examples in the source domain and the target 
domain respectively. In order to overcome 
overflow, we set ?=0.0001 in our experiment 
reported in section 5. 
To better understand this measure, let?s take a 
simple example (see Table 1). Given a source 
dataset with 1000 documents and a target dataset 
with 1000 documents, 12 candidate features, and a 
task to pick out 2 HFDS features. According to 
our understanding, the best choice is to pick out 
w4 and w8.  According to formula (2), fortunately, 
we successfully pick out w4, and w8. This simple 
example validates the effectiveness of proposed 
FEE formula. 
Table 1: A simple example for FEE 
FEE 
Words No(w) Nn(w) Score Rank
w1 100 100 -2.3025 6 
w2 100 90 -2.1971 4 
w3 100 45 -1.5040 3 
w4 100 4 0.9163 1 
w5 50 50 -2.9956 8 
w6 50 45 -2.8903 7 
w7 50 23 -2.2192 5 
w8 50 2 0.2231 2 
w9 4 4 -5.5214 11 
w10 4 3 -5.2337 10 
w11 4 2 -4.8283 9 
w12 1 1 -6.9077 12 
3.2 Feature-Weighted SCL model 
To adjust the influence of HFDS features in 
building correspondence, we proposed feature-
weighted SCL model (FW-SCL), ( ) 21)( wxwxpL
i ill llikk
?? +?=? ?    (5) 
where the function pk(xi) indicates whether the 
pivot feature k occurs in the instance xi; 
otherwise
xif
xp ikik
0 
1
1
)(
>
??
?
?= , 
and ?l is the parameter to control the weight of the 
HFDS feature l, 
182
 otherwise
Zlif HFDS
l
?
??
?=   
1
??  
where ZHFDS indicates the HFDS feature set and ? 
is located in the range [0,1]. When ??=0?, it 
indicates that no HFDS features are used to build 
the correspondence vectors; while ??=1? indicates 
that all features are equally used to build the 
correspondence vectors, that is to say, proposed 
FW-SCL algorithm is simplified as traditional 
SCL algorithm. Consequently, proposed FW-SCL 
algorithm could be regarded as a generalized 
version of traditional SCL algorithm. 
4 Instance-Weighted SCL Model 
The traditional SCL model does not take into 
account the labels (?positive? or ?negative?) of 
instances on the source domain and pivot features. 
Although the labels of pivot features are not given 
at first, it is very easy to obtain these labels 
because the number of pivot features is typically 
very small. 
Obviously, positive pivot features tend to occur 
in positive instances, so the correlations built on 
positive instances are more reliable than the 
correlations built on negative instances; and vice 
versa. As a result, the ideal choice is to assign a 
larger weight to the instances with the same label 
as the involved pivot feature, while assign a 
smaller weight to the instances with the different 
label as the involved pivot feature. This strategy 
can make correlations more reliable. This is the 
key idea of instance-weighted SCL model (IW-
SCL). Combining the idea of feature-weighted 
SCL model (FW-SCL), we obtain the feature-
weighted and instance-weighted SCL model 
(FWIW-SCL), 
( )( ) ( )( )
( ) ( )( )( ) ( )( )? ?
? ?
????
++??=
1),(11
1),(
2
jll lljkj
ill llikik
xwxpxk
wxwxpxkL
?????
??????
(6) 
where ? is the instance weight and the function 
pk(xi) indicates whether the pivot feature k occurs 
in the instance xi; 
otherwise
xif
xp ikik
0 
1
1
)(
>
??
?
?=  
and ?l is the parameter to control the weight of the 
HFDS feature l, 
 otherwise
Zlif HFDS
l
?
??
?=   
1
?? , 
where ZHFDS indicates the HFDS feature set and ? 
is located in the range [0,1]. 
In equation (6), the function ?(z,y) indicates 
whether the two variables z and y have the same 
non-zero value, 
( )
otherwise
0y and zzif
z,y
?=
??
?=   
0
1? ; 
and the function ?(z) is a hinge function, whose 
variables are either pivot features or instances, 
labelnegativez has aif
unknown
labelpositivez has aif
z
      
      
1
0
1
)(
??
??
?
?
=? . 
For the sake of convenience, we simplify 
?FWIW-SCL? as ?W-SCL?.  
5   Experimental Results 
5.1 Datasets 
We collected three Chinese domain-specific 
datasets: Education Reviews (Edu, from 
http://blog.sohu.com/learning/), Stock Reviews (Sto, 
from http://blog.sohu.com/stock/) and Computer 
Reviews (Comp, from http://detail.zol.com.cn/). All of 
these datasets are annotated by three linguists. We 
use ICTCLAS (a Chinese text POS tool, 
http://ictclas.org/) to parse Chinese words. 
The dataset Edu includes 1,012 negative 
reviews and 254 positive reviews. The average 
size of reviews is about 600 words. The dataset 
Sto consists of 683 negative reviews and 364 
positive reviews. The average length of reviews is 
about 460 terms. The dataset Comp contains 390 
negative reviews and 544 positive reviews. The 
average length of reviews is about 120 words. 
5.2 Comparison Methods 
In our experiments, we run one supervised 
baseline, i.e., Na?ve Bayes (NB), which only uses 
one source-domain labeled data as training data. 
For transfer-learning baseline, we implement 
traditional SCL model (T-SCL) (Blitzer et al, 
2006). Like TSVM, it makes use of the source-
domain labeled data as well as the target-domain 
unlabeled data. 
5.3 Does proposed method work? 
To conduct our experiments, we use source-
domain data as unlabeled set or labeled training 
set, and use target-domain data as unlabeled set or 
testing set. Note that we use 100 manual-
annotated pivot features for T-SCL, FW-SCL and 
W-SCL in the following experiments. We select 
183
pivot features use three criteria: a) is a sentiment 
word; b) occurs frequently in both domains; c) has 
similar occurring probability. For T-SCL, FW-
SCL and W-SCL, we use prototype classifier 
(Sebastiani, 2002) to train the final model. 
Table 2 shows the results of experiments 
comparing proposed method with supervised 
learning, transductive learning and T-SCL. For 
FW-SCL, the ZHFDS is set to 200 and ? is set to 0.1; 
For W-SCL, the ZHFDS is set to 200, ? is set to 0.1, 
and ? is set to 0.9. 
As expected, proposed method FW-SCL does 
indeed provide much better performance than 
supervised baselines, TSVM and T-SCL model. 
For example, the average accuracy of FW-SCL 
beats supervised baselines by about 12 percents, 
beats TSVM by about 11 percents and beats T-
SCL by about 10 percents. This result indicates 
that proposed FW-SCL model could overcome the 
shortcomings of HFDS features in building 
correspondence vectors. 
More surprisingly, instance-weighting strategy 
can further boost the performance of FW-SCL by 
about 4 percents. This result indicates that the 
labels of instances and pivot features are very 
useful in building the correlation vectors. This 
result also verifies our analysis in section 4: 
positive pivot features tend to occur in positive 
instances, so the correlations built on positive 
instances are more reliable than the correlations 
built on negative instances, and vice versa. 
Table 2: Accuracy of different methods 
 NB T-SCL FW-SCL W-SCL 
Edu->Sto 0.6704 0.7965 0.7917 0.8108
Edu->Comp 0.5085 0.8019 0.8993 0.9025
Sto->Edu 0.6824 0.7712 0.9072 0.9368
Sto->Comp 0.5053 0.8126 0.8126 0.8693
Comp->Sto 0.6580 0.6523 0.7010 0.7717
Comp->Edu 0.6114 0.5976 0.9112 0.9408
Average 0.6060 0.7387 0.8372 0.8720
Although SCL is a method designed for transfer 
learning, but it cannot provide better performance 
than TSVM. This result verifies the analysis in 
section 3: a small amount of HFDS features 
occupy a large amount of weight in classification 
model, but hardly carry corresponding sentiment. 
In another word, very few top-frequency words 
degrade the representative ability of SCL model 
for sentiment classification. 
6 Conclusion Remarks 
In this paper, we proposed a weighted SCL 
model (W-SCL) for domain adaptation in the 
context of sentiment analysis. On six domain-
transfer tasks, W-SCL consistently produces much 
better performance than the supervised, semi-
supervised and transfer-learning baselines. As a 
result, we can say that proposed W-SCL model 
offers a better choice for sentiment-analysis 
applications that require high-precision 
classification but hardly have any labeled training 
data.  
7 Acknowledgments 
This work was mainly supported by two funds, 
i.e., 0704021000 and 60803085, and one another 
project, i.e., 2004CB318109. 
References 
Blitzer, J. and McDonald, R. and Fernando Pereira. 
Domain adaptation with structural correspondence 
learning. EMNLP 2006. 
DaumeIII, H. and Marcu, D. Domain adaptation for 
statistical classifiers. Journal of Artificial 
Intelligence Research, 2006, 26: 101-126. 
Jiang, J., Zhai, C. A Two-Stage Approach to Domain 
Adaptation for Statistical Classifiers. CIKM 2007. 
Pang, B., Lee, L. and Vaithyanathan, S. Thumbs up? 
Sentiment classification using machine learning 
techniques. EMNLP 2002. 
Sebastiani, F. Machine learning in automated text 
categorization. ACM Computing Surveys. 2002, 
34(1): 1-47. 
S. Tan, G. Wu, H. Tang and X. Cheng. A novel 
scheme for domain-transfer problem in the context 
of sentiment analysis. CIKM 2007. 
S. Tan, Y. Wang, G. Wu and X. Cheng. Using 
unlabeled data to handle domain-transfer problem of 
semantic detection. SAC 2008. 
S. Tan, X. Cheng, Y. Wang and H. Xu. Adapting 
Naive Bayes to Domain Adaptation for Sentiment 
Analysis. ECIR 2009. 
H. Tang, S. Tan, and X. Cheng. A Survey on 
Sentiment Detection of Reviews. Expert Systems 
with Applications. Elsevier. 2009, 
doi:10.1016/j.eswa.2009.02.063. 
Turney, P. D. Thumbs Up or Thumbs Down? Semantic 
Orientation Applied to Unsupervised Classification 
of Reviews. ACL 2002. 
 
184
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 81?84,
Prague, June 2007. c?2007 Association for Computational Linguistics
Using Error-Correcting Output Codes with Model-Refinement to 
Boost Centroid Text Classifier 
 
Songbo Tan 
Information Security Center, ICT, P.O. Box 2704, Beijing, 100080, China 
tansongbo@software.ict.ac.cn, tansongbo@gmail.com 
 
Abstract 
In this work, we investigate the use of 
error-correcting output codes (ECOC) for 
boosting centroid text classifier. The 
implementation framework is to decompose 
one multi-class problem into multiple 
binary problems and then learn the 
individual binary classification problems 
by centroid classifier. However, this kind 
of decomposition incurs considerable bias 
for centroid classifier, which results in 
noticeable degradation of performance for 
centroid classifier. In order to address this 
issue, we use Model-Refinement to adjust 
this so-called bias. The basic idea is to take 
advantage of misclassified examples in the 
training data to iteratively refine and adjust 
the centroids of text data. The experimental 
results reveal that Model-Refinement can 
dramatically decrease the bias introduced 
by ECOC, and the combined classifier is 
comparable to or even better than SVM 
classifier in performance. 
1. Introduction 
In recent years, ECOC has been applied to 
boost the na?ve bayes, decision tree and SVM 
classifier for text data (Berger 1999, Ghani 2000, 
Ghani 2002, Rennie et al 2001). Following this 
research direction, in this work, we explore the 
use of ECOC to enhance the performance of 
centroid classifier (Han et al 2000). To the best of 
our knowledge, no previous work has been 
conducted on exactly this problem. The 
framework we adopted is to decompose one 
multi-class problem into multiple binary problems 
and then use centroid classifier to learn the 
individual binary classification problems.  
However, this kind of decomposition incurs 
considerable bias (Liu et al 2002) for centroid 
classifier. In substance, centroid classifier (Han et 
al. 2000) relies on a simple decision rule that a 
given document should be assigned a particular 
class if the similarity (or distance) of this 
document to the centroid of the class is the largest 
(or smallest). This decision rule is based on a 
straightforward assumption that the documents in 
one category should share some similarities with 
each other. However, this hypothesis is often 
violated by ECOC on the grounds that it ignores 
the similarities of original classes when 
disassembling one multi-class problem into 
multiple binary problems. 
In order to attack this problem, we use Model-
Refinement (Tan et al 2005) to reduce this so-
called bias. The basic idea is to take advantage of 
misclassified examples in the training data to 
iteratively refine and adjust the centroids. This 
technique is very flexible, which only needs one 
classification method and there is no change to 
the method in any way.  
To examine the performance of proposed 
method, we conduct an extensive experiment on 
two commonly used datasets, i.e., Newsgroup and 
Industry Sector. The results indicate that Model-
Refinement can dramatically decrease the bias 
introduce by ECOC, and the resulted classifier is 
comparable to or even better than SVM classifier 
in performance. 
2. Error-Correcting Output Coding 
Error-Correcting Output Coding (ECOC) is a 
form of combination of multiple classifiers 
(Ghani 2000). It works by converting a multi-
class supervised learning problem into a large 
number (L) of two-class supervised learning 
problems (Ghani 2000). Any learning algorithm 
that can handle two-class learning problems, such 
as Na?ve Bayes (Sebastiani 2002), can then be 
applied to learn each of these L problems. L can 
then be thought of as the length of the codewords 
81
1 Load training data and parameters; 
2 Calculate centroid for each class; 
3 For iter=1 to MaxIteration Do 
   3.1 For each document d in training set Do 
3.1.1 Classify d labeled ?A1? into class ?A2?; 
3.1.2 If (A1!=A2) Do 
   Drag centroid of class A1 to d using formula (3);
   Push centroid of class A2 against d using 
formula (4); 
TRAINING 
1 Load training data and parameters, i.e., the length of code
L and training class K. 
2 Create a L-bit code for the K classes using a kind of
coding algorithm. 
3 For each bit, train the base classifier using the binary 
class (0 and 1) over the total training data. 
TESTING 
1 Apply each of the L classifiers to the test example. 
2 Assign the test example the class with the largest votes.
with one bit in each codeword for each classifier. 
The ECOC algorithm is outlined in Figure 1. 
 
 
 
 
 
 
Figure 1: Outline of ECOC 
3. Methodology 
3.1 The bias incurred by ECOC for 
centroid classifier 
Centroid classifier is a linear, simple and yet 
efficient method for text categorization. The basic 
idea of centroid classifier is to construct a 
centroid Ci for each class ci using formula (1) 
where d denotes one document vector and |z| 
indicates the cardinality of set z. In substance, 
centroid classifier makes a simple decision rule 
(formula (2)) that a given document should be 
assigned a particular class if the similarity (or 
distance) of this document to the centroid of the 
class is the largest (or smallest). This rule is based 
on a straightforward assumption: the documents 
in one category should share some similarities 
with each other.  
?=
? icd
i
i dc
C 1
                          (1) 
???
?
???
? ?=
22
maxarg
i
i
i Cd
Cd
c c
               (2) 
For example, the single-topic documents 
involved with ?sport? or ?education? can meet 
with the presumption; while the hybrid documents 
involved with ?sport? as well as ?education? 
break this supposition. 
As such, ECOC based centroid classifier also 
breaks this hypothesis. This is because ECOC 
ignores the similarities of original classes when 
producing binary problems. In this scenario, many 
different classes are often merged into one 
category. For example, the class ?sport? and 
?education? may be assembled into one class. As 
a result, the assumption will inevitably be broken. 
Let?s take a simple multi-class classification 
task with 12 classes. After coding the original 
classes, we obtain the dataset as Figure 2. Class 0 
consists of 6 original categories, and class 1 
contains another 6 categories. Then we calculate 
the centroids of merged class 0 and merged class 
1 using formula (1), and draw a Middle Line that 
is the perpendicular bisector of the line between 
the two centroids. 
 
 
 
 
 
Figure 2: Original Centroids of Merged Class 0 and 
Class 1 
According to the decision rule (formula (2)) of 
centroid classifier, the examples of class 0 on the 
right of the Middle Line will be misclassified into 
class 1. This is the mechanism why ECOC can 
bring bias for centroid classifier. In other words, 
the ECOC method conflicts with the assumption 
of centroid classifier to some degree. 
3.2 Why Model-Refinement can reduce 
this bias? 
In order to decrease this kind of bias, we 
employ the Model-Refinement to adjust the class 
representative, i.e., the centroids. The basic idea 
of Model-Refinement is to make use of training 
errors to adjust class centroids so that the biases 
can be reduced gradually, and then the training-
set error rate can also be reduced gradually. 
 
 
 
 
 
Figure 3: Outline of Model-Refinement Strategy 
For example, if document d of class 1 is 
misclassified into class 2, both centroids C1 and 
C2 should be moved right by the following 
formulas (3-4) respectively, 
dCC ?+= ?1*1                             (3) 
dCC ??= ?2*2                            (4) 
Middle Line Class 0 Class 1
C1C0
d 
82
where ? (0<?<1) is the Learning Rate which 
controls the step-size of updating operation. 
The Model-Refinement for centroid classifier is 
outlined in Figure 3 where MaxIteration denotes 
the pre-defined steps for iteration. More details 
can be found in (Tan et al 2005). The time 
requirement of Model-Refinement is O(MTKW) 
where M denotes the iteration steps. 
With this so-called move operation, C0 and C1 
are both moving right gradually. At the end of this 
kind of move operation (see Figure 4), no 
example of class 0 locates at the right of Middle 
Line so no example will be misclassified. 
 
 
 
 
 
 
 
Figure 4: Refined Centroids of Merged Class 0 and 
Class 1 
3.3 The combination of ECOC and Model-
Refinement for centroid classifier 
In this subsection, we present the outline 
(Figure 5) of combining ECOC and Model-
Refinement for centroid classifier. In substance, 
the improved ECOC combines the strengths of 
ECOC and Model-Refinement. ECOC research in 
ensemble learning techniques has shown that it is 
well suited for classification tasks with a large 
number of categories. On the other hand, Model-
Refinement has proved to be an effective 
approach to reduce the bias of base classifier, that 
is to say, it can dramatically boost the 
performance of the base classifier. 
 
 
 
 
 
 
 
Figure 5: Outline of combining ECOC and Model-
Refinement 
4. Experiment Results 
4.1 Datasets 
In our experiment, we use two corpora: 
NewsGroup1, and Industry Sector2. 
NewsGroup The NewsGroup dataset contains 
approximately 20,000 articles evenly divided 
among 20 Usenet newsgroups. We use a subset 
consisting of total categories and 19,446 
documents. 
Industry Sector The set consists of company 
homepages that are categorized in a hierarchy of 
industry sectors, but we disregard the hierarchy. 
There were 9,637 documents in the dataset, which 
were divided into 105 classes. We use a subset 
called as Sector-48 consisting of 48 categories 
and in all 4,581 documents. 
4.2 Experimental Design 
To evaluate a text classification system, we use 
MicroF1 and MacroF1 measures (Chai et al 
2002). We employ Information Gain as feature 
selection method because it consistently performs 
well in most cases (Yang et al 1997). We employ 
TFIDF (Sebastiani 2002) to compute feature 
weight. For SVM classifier we employ 
SVMTorch. (www.idiap.ch/~bengio/projects/SVMTorch.html). 
4.3 Comparison and Analysis 
Table 1 and table 2 show the performance 
comparison of different method on two datasets 
when using 10,000 features. For ECOC, we use 
63-bit BCH coding; for Model-Refinement, we 
fix its MaxIteration as 8. For brevity, we use MR 
to denote Model-Refinement. 
From the two tables, we can observe that 
ECOC indeed brings significant bias for centroid 
classifier, which results in considerable decrease 
in accuracy. Especially on sector-48, the bias 
reduces the MicroF1 of centroid classifier from 
0.7985 to 0.6422. 
On the other hand, the combination of ECOC 
and Model-Refinement makes a significant 
performance improvement over centroid classifier. 
                                                     
 
 
1 www-2.cs.cmu.edu/afs/cs/project/theo-11/www/wwkb. 
2 www-2.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/. 
TRAINING 
1 Load training data and parameters, i.e., the length of
code L and training class K. 
2 Create a L-bit code for the K classes using a kind of
coding algorithm. 
3 For each bit, train centroid classifier using the binary 
class (0 and 1) over the total training data. 
4 Use Model-Refinement approach to adjust centroids. 
TESTING 
1 Apply each of the L classifiers to the test example. 
2 Assign the test example the class with the largest votes.
Middle Line Class 0 Class 1
C*1C*0 
d 
83
On Newsgroup, it beats centroid classifier by 4 
percents; on Sector-48, it beats centroid classifier 
by 11 percents. More encouraging, it yields better 
performance than SVM classifier on Sector-48. 
This improvement also indicates that Model-
Refinement can effectively reduce the bias 
incurred by ECOC. 
Table 1: The MicroF1 of different methods 
Method 
 
Dataset 
Centroid 
MR 
+Centroid 
ECOC 
+Centroid 
ECOC 
+ MR 
+Centroid 
SVM
Sector-48 0.7985 0.8671 0.6422 0.9122 0.8948
NewsGroup 0.8371 0.8697 0.8085 0.8788 0.8777
Table 2: The MacroF1 of different methods 
Method 
 
Dataset 
Centroid 
MR 
+Centroid 
ECOC 
+Centroid 
ECOC 
+ MR 
+Centroid 
SVM
Sector-48 0.8097 0.8701 0.6559 0.9138 0.8970
NewsGroup 0.8331 0.8661 0.7936 0.8757 0.8759
 
Table 3 and 4 report the classification accuracy 
of combining ECOC with Model-Refinement on 
two datasets vs. the length BCH coding. For 
Model-Refinement, we fix its MaxIteration as 8; 
the number of features is fixed as 10,000.  
Table 3: the MicroF1 vs. the length of BCH coding 
Bit 
Dataset 
15bit 31bit 63bit
Sector-48 0.8461 0.8948 0.9105
NewsGroup 0.8463 0.8745 0.8788
Table 4: the MacroF1 vs. the length of BCH coding 
Bit 
Dataset 
15bit 31bit 63bit
Sector-48 0.8459 0.8961 0.9122
NewsGroup 0.8430 0.8714 0.8757
 
We can clearly observe that increasing the 
length of the codes increases the classification 
accuracy. However, the increase in accuracy is 
not directly proportional to the increase in the 
length of the code. As the codes get larger, the 
accuracies start leveling off as we can observe 
from the two tables.  
5. Conclusion Remarks 
In this work, we examine the use of ECOC for 
improving centroid text classifier. The 
implementation framework is to decompose 
multi-class problems into multiple binary 
problems and then learn the individual binary 
classification problems by centroid classifier. 
Meanwhile, Model-Refinement is employed to 
reduce the bias incurred by ECOC. 
In order to investigate the effectiveness and 
robustness of proposed method, we conduct an 
extensive experiment on two commonly used 
corpora, i.e., Industry Sector and Newsgroup. The 
experimental results indicate that the combination 
of ECOC with Model-Refinement makes a 
considerable performance improvement over 
traditional centroid classifier, and even performs 
comparably with SVM classifier. 
References 
Berger, A. Error-correcting output coding for text 
classification. In Proceedings of IJCAI, 1999. 
Chai, K., Chieu, H. and Ng, H. Bayesian online 
classifiers for text classification and filtering. SIGIR. 
2002, 97-104 
Ghani, R. Using error-correcting codes for text 
classification. ICML. 2000 
Ghani, R. Combining labeled and unlabeled data for 
multiclass text categorization. ICML. 2002 
Han, E. and Karypis, G. Centroid-Based Document 
Classification Analysis & Experimental Result. 
PKDD. 2000. 
Liu, Y., Yang, Y. and Carbonell, J. Boosting to 
Correct Inductive Bias in Text Classification. CIKM. 
2002, 348-355 
Rennie, J. and Rifkin, R. Improving multiclass text 
classification with the support vector machine. In 
MIT. AI Memo AIM-2001-026, 2001. 
Sebastiani, F. Machine learning in automated text 
categorization. ACM Computing Surveys, 
2002,34(1): 1-47. 
Tan, S., Cheng, X., Ghanem, M., Wang, B. and Xu, 
H. A novel refinement approach for text 
categorization. CIKM. 2005, 469-476 
Yang, Y. and Pedersen, J. A Comparative Study on 
Feature Selection in Text Categorization. ICML. 
1997, 412-420. 
 
84
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 317?320,
Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLP
Graph Ranking for Sentiment Transfer 
 
Qiong Wu1,2, Songbo Tan1 and Xueqi Cheng1 
1Institute of Computing Technology, Chinese Academy of Sciences, China 
2 Graduate University of Chinese Academy of Sciences, China 
{wuqiong,tansongbo}@software.ict.ac.cn, cxq@ict.ac.cn 
 
Abstract 
With the aim to deal with sentiment-transfer 
problem, we proposed a novel approach, 
which integrates the sentiment orientations of 
documents into the graph-ranking algorithm. 
We apply the graph-ranking algorithm using 
the accurate labels of old-domain documents 
as well as the ?pseudo? labels of new-domain 
documents. Experimental results show that 
proposed algorithm could improve the per-
formance of baseline methods dramatically for 
sentiment transfer. 
1 Introduction 
With the rapid growth of reviewing pages, sen-
timent classification is drawing more and more 
attention (Bai et al, 2005; Pang and Lee, 2008). 
Generally speaking, sentiment classification can 
be considered as a special kind of traditional text 
classification (Tan et al, 2005; Tan, 2006). In 
most cases, supervised learning methods can per-
form well (Pang et al, 2002). But when training 
data and test data are drawn from different do-
mains, supervised learning methods always pro-
duce disappointing results. This is so-called 
cross-domain sentiment classification problem 
(or sentiment-transfer problem). 
Sentiment transfer is a new study field. In re-
cent years, only a few works are conducted on 
this field. They are generally divided into two 
categories. The first one needs a small amount of 
labeled training data for the new domain (Aue 
and Gamon, 2005). The second one needs no 
labeled data for the new domain (Blitzer et al, 
2007; Tan et al, 2007; Andreevskaia and Bergler, 
2008; Tan et al, 2008; Tan et al, 2009). In this 
paper, we concentrate on the second category 
which proves to be used more widely. 
Graph-ranking algorithm has been success-
fully used in many fields (Wan et al, 2006; Esuli 
and Sebastiani, 2007), whose idea is to give a 
node high score if it is strongly linked with other 
high-score nodes. In this work, we extend the 
graph-ranking algorithm for sentiment transfer 
by integrating the sentiment orientations of the 
documents, which could be considered as a sen-
timent-transfer version of the graph-ranking al-
gorithm. In this algorithm, we assign a score for 
every unlabelled document to denote its extent to 
?negative? or ?positive?, then we iteratively cal-
culate the score by making use of the accurate 
labels of old-domain data as well as the ?pseudo? 
labels of new-domain data, and the final score 
for sentiment classification is achieved when the 
algorithm converges, so we can label the new-
domain data based on these scores. 
2 The Proposed Approach 
2.1 Overview 
In this paper, we have two document sets: the 
test data DU = {d1,?,dn} where di is the term 
vector of the ith text document and each di?DU(i 
= 1,?,n) is unlabeled; the training data DL = 
{dn+1,?dn+m} where dj represents the term vector 
of the jth text document and each dj?DL(j = 
n+1,?,n+m) should have a label from a category 
set C = {negative, positive}. We assume the 
training dataset DL is from the related but differ-
ent domain with the test dataset DU. Our objec-
tive is to maximize the accuracy of assigning a 
label in C to di?DU (i = 1,?,n) utilizing the 
training data DL in another domain. 
The proposed algorithm is based on the fol-
lowing presumptions: 
   (1) Let WL denote the word space of old do-
main, WU denote the word space of new domain. 
WL?WU??. 
   (2) The labels of documents appear both in the 
training data and the test data should be the same.  
Based on graph-ranking algorithm, it is 
thought that if a document is strongly linked with 
positive (negative) documents, it is probably 
positive (negative). And this is the basic idea of 
learning from a document?s neighbors. 
Our algorithm integrates the sentiment orienta-
tions of the documents into the graph-ranking 
algorithm. In our algorithm, we build a graph 
317
whose nodes denote documents and edges denote 
the content similarities between documents. We 
initialize every document a score (?1? denotes 
positive, and ?-1? denotes negative) to represent 
its degree of sentiment orientation, and we call it 
sentiment score. The proposed algorithm calcu-
lates the sentiment score of every unlabelled 
document by learning from its neighbors in both 
old domain and new domain, and then iteratively 
calculates the scores with a unified formula. Fi-
nally, the algorithm converges and each docu-
ment gets its sentiment score. When its sentiment 
score falls in the range [0, 1] (or [-1, 0]], the 
document should be classified as ?positive (or 
negative)?. The closer its sentiment score is near 
1 (or -1), the higher the ?positive (or negative)? 
degree is.  
2.2 Score Documents  
Score Documents Using Old-domain Informa-
tion 
We build a graph whose nodes denote documents 
in both DL and DU and edges denote the content 
similarities between documents. If the content 
similarity between two documents is 0, there is 
no edge between the two nodes. Otherwise, there 
is an edge between the two nodes whose weight 
is the content similarity. The content similarity 
between two documents is computed with the 
cosine measure. We use an adjacency matrix U 
to denote the similarity matrix between DU and 
DL. U=[Uij]nxm is defined as follows: 
mnnjni
dd
dd
U
ji
ji
ij ++==?
?= ,...,1,,...,1,    (1) 
The weight associated with term t is computed 
with tftidft where tft is the frequency of term t in 
the document and idft is the inverse document 
frequency of term t, i.e. 1+log(N/nt), where N is 
the total number of documents and nt is the num-
ber of documents containing term t in a data set.   
In consideration of convergence, we normal-
ize U to U? by making the sum of each row equal 
to 1: 
1 1
, 0?
0,
m m
ij ij ij
j jij
U U if U
U
otherwise
= =
? ??= ???
? ?
        (2) 
In order to find the neighbors (in another word, 
the nearest documents) of a document, we sort 
every row of U?  to U% in descending order. That is: 
U% ij? U% ik (i = 1,?n; j,k = 1,?m; k?j). 
Then for di?DU (i = 1,?,n), U% ij (j = 1,?,K ) 
corresponds to K neighbors in DL. So we can get 
its K neighbors. We use a matrix [ ]ij n KN N ?=  
to denote the neighbors of DU in old domain, 
with Nij corresponding to the jth nearest neighbor 
of di. 
At last, we can calculate sentiment score si (i 
= 1,?,n) using the scores of the di?s neighbors as 
follows: 
nisUs
i
j
Nj
k
ij
k
i ,...,1,)?(
)1()( =?= ?
??
?        (3) 
where ?i means the ith row of a matrix and 
)(k
is denotes the is at the k
th iteration. 
Score Documents Using New-domain Infor-
mation 
Similarly, a graph is built, in which each node 
corresponds to a document in DU and the weight 
of the edge between any different documents is 
computed by the cosine measure. We use an ad-
jacency matrix V=[Vij]nxn to describe the similar-
ity matrix. And V is similarly normalized to V? to 
make the sum of each row equal to 1. Then we 
sort every row of V?  to V% in descending order, 
thus we can get K neighbors of di?DU (i = 
1,?,n) from V% ij (j = 1,?K), and we use a matrix 
[ ]ij n KM M ?=  to denote the neighbors of DU in 
the new domain. Finally, we can calculate si us-
ing the sentiment scores of the di?s neighbors as 
follows: 
?
??
? =?=
i
ji
Mj
k
ij
k nisVs ,...,1),?( )1()(          (4) 
2.3 Sentiment Transfer Algorithm 
Initialization 
Firstly, we classify the test data DU to get their 
initial labels using a traditional classifier. For 
simplicity, we use prototype classification algo-
rithm (Tan et al, 2005) in this work. 
Then, we give ?-1? to si(0) if di?s label is 
?negative?, and ?1? if ?positive?. So we obtain 
the initial sentiment score vector S(0) for both 
domain data. 
At last, si(0) (i = 1,?,n) is normalized as fol-
lows to make the sum of positive scores of DU 
equal to 1, and the sum of negative scores of DU 
equal to -1: 
ni
sifss
sifss
s
i
Dj
ji
i
Dj
ji
U
pos
U
neg
i
,...,1
0,
0,)(
)0()0()0(
)0()0()0(
)0( =
???
???
?
>
<?
= ?
?
?
?   (5) 
318
where U
negD and
U
posD denote the negative and 
positive document set of DU respectively. The 
same as (5), sj (0) (j =n+1,?,n+m) is normalized. 
Algorithm Introduction 
In our algorithm, we label DU by making use of 
information of both old domain and new domain. 
We fuse equations (3) and (4), and get the itera-
tive equation as follows: 
nisVsUs
i
h
i
ji
Mh
k
ih
Nj
k
ij
k ,...,1,)?()?( )1()1()( =?+?= ??
?? ?
?
?
? ?? (6) 
where 1? ?+ = , and ? and? show the relative 
importance of old domain and new domain to the 
final sentiment scores. In consideration of the 
convergence, S(k) (S at the kth iteration) is normal-
ized after each iteration. 
Here is the complete algorithm: 
1. Classify DU with a traditional classifier. 
Initialize the sentiment score si of di?DU
?DL (i = 1,?n+m) and normalize it. 
2. Iteratively calculate the S(k) of DU and 
normalize it until it achieves the conver-
gence: 
nisVsUs
i
h
i
ji
Mh
k
ih
Nj
k
ij
k ,...,1,)?()?( )1()1()( =?+?= ??
?? ?
?
?
? ??
ni
sifss
sifss
s
k
i
Dj
k
j
k
i
k
i
Dj
k
j
k
i
k
i
U
pos
U
neg ,...,1
0,
0,)(
)()()(
)()()(
)( =
???
???
?
>
<?
= ?
?
?
?
 
3. According to si? S (i = 1,?,n), assign 
each di?DU (i = 1,?n) a label. If si is be-
tween -1 and 0, assign di the label ?nega-
tive?; if si is between 0 and 1, assign di the 
label ?positive?. 
3 EXPERIMENTS 
3.1 Data Preparation 
We prepare three Chinese domain-specific data 
sets from on-line reviews, which are: Electronics 
Reviews (Elec, from http://detail.zol.com.cn/), 
Stock Reviews (Stock, from http://blog.sohu.com 
/stock/) and Hotel Reviews (Hotel, from  
http://www.ctrip.com/). And then we manually 
label the reviews as ?negative? or ?positive?. 
The detailed composition of the data sets are 
shown in Table 1, which shows the name of the 
data set (DataSet), the number of negative re-
views (Neg), the number of positive reviews 
(Pos), the average length of reviews (Length), 
the number of different words (Vocabulary) in 
this data set. 
DataSet Neg Pos Length Vocabulary
Elec 554 1,054 121 6,200 
Stock 683 364 460 13,012 
Hotel 2,000 2,000 181 11,336 
Table 1. Data sets composition 
We make some preprocessing on the datasets. 
First, we use ICTCLAS (http://ictclas.org/), a 
Chinese text POS tool, to segment these Chinese 
reviews. Second, the documents are represented 
by vector space model.  
3.2 Evaluation Setup 
In our experiment, we use prototype classifica-
tion algorithm (Tan et al, 2005) and Support 
Vector Machine experimenting on the three data 
sets as our baselines separately. The Support 
Vector Machine is a state-of-the-art supervised 
learning algorithm. In our experiment, we use 
LibSVM (www.csie.ntu.edu.tw/~cjlin/libsvm/) with a 
linear kernel and set al options by default.  
We also compare our algorithm to Structural 
Correspondence Learning (SCL) (Blitzer et al, 
2007). SCL is a state-of-the-art sentiment-
transfer algorithm which automatically induces 
correspondences among features from different 
domains. It identifies correspondences among 
features from different domains by modeling 
their correlations with pivot features, which are 
features that behave in the same way for dis-
criminative learning in both domains. In our ex-
periment, we use 100 pivot features.  
3.3 Overall Performance 
In this section, we conduct two groups of ex-
periments where we separately initialize the sen-
timent scores in our algorithm by prototype clas-
sifier and Support Vector Machine.  
There are two parameters in our algorithm, K 
and ? ( ? can be calculated by 1-? ). We set the 
parameters K and ? with 150 and 0.7 respec-
tively, which indicates we use 150 neighbors and 
the contribution from old domain is a little more 
important than that from new domain. It is 
thought that the algorithm achieves the conver-
gence when the changing between the sentiment 
score si computed at two successive iterations for 
any di?DU (i = 1,?n) falls below a given 
threshold, and we set the threshold 0.00001 in 
this work.  
Table 2 shows the accuracy of Prototype, 
LibSVM, SCL and our algorithm when training 
data and test data belong to different domains. 
319
Our algorithm is separately initialized by Proto-
type and LibSVM. 
Baseline Proposed Algorithm 
 
Prototype LibSVM 
SCL Prototype+ 
OurApproach
LibSVM+
OurApproach
Elec->Stock 0.6652 0.6478 0.7507 0.7326 0.7304 
Elec->Hotel 0.7304 0.7522 0.7750 0.7543 0.7543 
Stock->Hotel 0.6848 0.6957 0.7683 0.7435 0.7457 
Stock->Elec 0.7043 0.6696 0.8340 0.8457 0.8435 
Hotel->Stock 0.6196 0.5978 0.6571 0.7848 0.7848 
Hotel->Elec 0.6674 0.6413 0.7270 0.8609 0.8609 
Average 0.6786 0.6674 0.7520 0.7870 0.7866 
Table 2. Accuracy comparison of different methods 
As we can observe from Table 2, our algo-
rithm can dramatically increase the accuracy of 
sentiment-transfer. Seen from the 2nd column and 
the 5th column, every accuracy of the proposed 
algorithm is increased comparing to Prototype. 
The average increase of accuracy over all the 6 
problems is 10.8%. Similarly, the accuracy of 
our algorithm is higher than LibSVM in every 
problem and the average increase of accuracy is 
11.9%. The great improvement comparing with 
the baselines indicates that the proposed algo-
rithm performs very effectively and robustly. 
Seen from Table 2, our result about SCL is in 
accord with that in (Blitzer et al, 2007) on the 
whole. The average accuracy of SCL is higher 
than both baselines, which convinces that SCL is 
effective for sentiment-transfer. However, our 
approach outperforms SCL: the average accuracy 
of our algorithm is about 3.5 % higher than SCL. 
This is caused by two reasons. First, SCL is es-
sentially based on co-occurrence of words (the 
window size is the whole document), so it is eas-
ily affected by low frequency words and the size 
of data set. Second, the pivot features of SCL are 
totally dependent on experts in the field, so the 
quality of pivot features will seriously affect the 
performance of SCL. This improvement con-
vinces us of the effectiveness of our algorithm.  
4 Conclusion and Future Work 
In this paper, we propose a novel sentiment-
transfer algorithm. It integrates the sentiment 
orientations of the documents into the graph-
ranking based method for sentiment-transfer 
problem. The algorithm assigns a score for every 
document being predicted, and it iteratively cal-
culates the score making use of the accurate la-
bels of old-domain data, as well as the ?pseudo? 
labels of new-domain data, finally it labels the 
new-domain data as ?negative? or ?positive? bas-
ing on this score. The experiment results show 
that the proposed approach can dramatically im-
prove the accuracy when transferred to a new 
domain.  
In this study, we find the neighbors of a given 
document using cosine similarity. This is too 
general, and perhaps not so proper for sentiment 
classification. In the next step, we will try other 
methods to calculate the similarity. Also, our 
approach can be applied to multi-task learning. 
5 Acknowledgments 
This work was mainly supported by two funds, i.e., 
0704021000 and 60803085, and one another project, 
i.e., 2004CB318109. 
References 
B. Pang and L. Lee. 2008. Opinion mining and senti-
ment analysis. Foundations and Trends in Infor-
mation Retrieval, 2008 
S. Tan, X. Cheng, M. Ghanem, B. Wang and H. Xu. 
2005. A Novel Refinement Approach for Text 
Categorization. In Proceedings of CIKM 2005. 
S. Tan. 2006. An Effective Refinement Strategy for 
KNN Text Classifier. Expert Systems With Appli-
cations. Elsevier. 30(2): 290-298. 
B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs 
up? Sentiment classification using machine learn-
ing techniques. In Proceedings of EMNLP, 2002. 
X. Bai, R. Padman and E. Airoldi. 2005. On learning 
parsimonious models for extracting consumer 
opinions. In Proceedings of HICSS 2005. 
A. Aue and M. Gamon. 2005. Customizing sentiment 
classifiers to new domains: a case study. In 
Proceedings of RANLP 2005. 
J. Blitzer, M. Dredze, and F. Pereira. 2007. Biogra-
phies, Bollywood, Boom-boxes and Blenders: 
Domain adaptation for sentiment classification. In 
Proceedings of ACL 2007. 
S. Tan, G. Wu, H. Tang and X. Cheng. 2007. A novel 
scheme for domain-transfer problem in the context 
of sentiment analysis. In Proceedings of CIKM 
2007. 
S. Tan, Y. Wang, G. Wu and X. Cheng. 2008. Using 
unlabeled data to handle domain-transfer problem 
of semantic detection. In Proceedings of SAC 2008. 
S. Tan, X. Cheng, Y. Wang, H. Xu. 2009. Adapting 
Naive Bayes to Domain Adaptation for Sentiment 
Analysis. In Proceedings of ECIR 2009. 
A. Esuli, F. Sebastiani. 2007. Random-walk models 
of term semantics: An application to opinion-
related properties. In Proceedings of LTC 2007. 
X. Wan, J. Yang and J. Xiao. 2006. Using Cross-
Document Random Walks for Topic-Focused 
Multi-Document Summarization. In Proceedings 
of WI 2006. 
A. Andreevskaia and S. Bergler. 2008. When Special-
ists and Generalists Work Together: Overcoming 
Domain Dependence in Sentiment Tagging. In 
Proceedings of ACL 2008. 
320
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 486?493,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
An Iterative Reinforcement Approach for Fine-Grained 
Opinion Mining 
 
Weifu Du 
Haerbin Institute of Technology 
Haerbin, China 
duweifu@software.ict.ac.cn
Songbo Tan 
Institute of Computing Technology 
Beijing, China 
tansongbo@software.ict.ac.cn 
 
Abstract 
With the in-depth study of sentiment analysis 
research, finer-grained opinion mining, which 
aims to detect opinions on different review fea-
tures as opposed to the whole review level, has 
been receiving more and more attention in the 
sentiment analysis research community re-
cently. Most of existing approaches rely mainly 
on the template extraction to identify the ex-
plicit relatedness between product feature and 
opinion terms, which is insufficient to detect 
the implicit review features and mine the hid-
den sentiment association in reviews, which 
satisfies (1) the review features are not appear 
explicit in the review sentences; (2) it can be 
deduced by the opinion words in its context. 
From an information theoretic point of view, 
this paper proposed an iterative reinforcement 
framework based on the improved information 
bottleneck algorithm to address such problem. 
More specifically, the approach clusters prod-
uct features and opinion words simultaneously 
and iteratively by fusing both their semantic in-
formation and co-occurrence information. The 
experimental results demonstrate that our ap-
proach outperforms the template extraction 
based approaches. 
1 Introduction 
In the Web2.0 era, the Internet turns from a static 
information media into a platform for dynamic 
information exchanging, on which people can ex-
press their views and show their selfhood. More 
and more people are willing to record their feel-
ings (blog), give voice to public affairs (news re-
view), express their likes and dislikes on products 
(product review), and so on. In the face of the vol-
ume of sentimental information available on the 
Internet continues to increase, there is growing 
interest in helping people better find, filter, and 
manage these resources. 
Automatic opinion mining (Turney et al, 2003; 
Ku et al, 2006; Devitt et al, 2007) can play an 
important role in a wide variety of more flexible 
and dynamic information management tasks. For 
example, with the help of sentiment analysis sys-
tem, in the field of public administration, the ad-
ministrators can receive the feedbacks on one pol-
icy in a timelier manner; in the field of business, 
manufacturers can perform more targeted updates 
on products to improve the consumer experience. 
The research of opinion mining began in 1997, 
the early research results mainly focused on the 
polarity of opinion words (Hatzivassiloglou et al, 
1997) and treated the text-level opinion mining as 
a classification of either positive or negative on the 
number of positive or negative opinion words in 
one text (Turney et al, 2003; Pang et al, 2002; 
Zagibalov et al, 2008;). With the in-depth study of 
opinion mining, researchers committed their ef-
forts for more accurate results: the research of sen-
timent summarization (Philip et al, 2004; Hu et al, 
KDD 2004), domain transfer problem of the sen-
timent analysis (Kanayama et al, 2006; Tan et al, 
2007; Blitzer et al, 2007; Tan et al, 2008; An-
dreevskaia et al, 2008; Tan et al, 2009) and fine-
grained opinion mining (Hatzivassiloglou et al, 
2000; Takamura et al, 2007; Bloom et al, 2007; 
Wang et al, 2008; Titov et al, 2008) are the main 
branches of the research of opinion mining. In this 
paper, we focus on the fine-grained (feature-level) 
opinion mining.  
For many applications (e.g. the task of public 
affairs review analysis and the products review 
analysis), simply judging the sentiment orientation 
of a review unit is not sufficient. Researchers (Ku-
shal, 2003; Hu et al, KDD 2004; Hu et al, AAAI 
2004; Popescu et al, 2005) began to work on 
finer-grained opinion mining which predicts the 
sentiment orientation related to different review 
features. The task is known as feature-level opin-
ion mining. 
486
In feature-level opinion mining, most of the ex-
isting researches associate product features and 
opinion words by their explicit co-occurrence. 
Template extraction based method (Popescu et al, 
2005) and association rule mining based method 
(Hu et al, AAAI 2004) are the representative ones.  
These approaches did good jobs for identifying 
the review features that appear explicitly in re-
views, however, real reviews from customers are 
usually complicated. In some cases, the review 
features are implicit in the review sentences, but 
can be deduced by the opinion words in its context. 
The detection of such hidden sentiment association 
is a big challenge in feature-level opinion mining 
on Chinese reviews due to the nature of Chinese 
language (Qi et al, 2008). Obviously, neither the 
template extraction based method nor the associa-
tion rule mining based method is effective for such 
cases. Moreover, in some cases, even if the review 
features appear explicitly in the review sentences, 
the co-occurrence information between review 
features and opinion words is too quantitatively 
sparse to be utilized. So we consider whether it is a 
more sensible way to construct or cluster review 
feature groups and opinion words groups to mine 
the implicit or hidden sentiment association in the 
reviews. 
The general approach will cluster the two types 
of objects separately, which neglects the highly 
interrelationship. To address this problem, in this 
paper, we propose an iterative reinforcement 
framework, under which we cluster product fea-
tures and opinion words simultaneously and itera-
tively by fusing both their semantic information 
and sentiment link information. We take improved 
information bottleneck algorithm (Tishby, 1999) 
as the kernel of the proposed framework. 
The information bottleneck approach was pre-
sented by Tishby (1999). The basic idea of the ap-
proach is that it treats the clustering problems from 
the information compressing point of view, and 
takes this problem as a case of much more funda-
mental problem: what are the features of the vari-
able X that are relevant for the prediction of an-
other, relevance, variable Y? Based on the infor-
mation theory, the problem can be formulated as: 
find a compressed representation of the variable X, 
denoted C, such that the mutual information be-
tween C and Y is as high as possible, under a con-
straint on the mutual information between X and C. 
For our case, take the hotel reviews as example, X 
is one type of objects of review features (e.g. fa-
cilities, service, surrounding environment, etc) or 
opinion words (e.g. perfect, circumspect, quiet, 
etc), and Y is another one. Given some review fea-
tures (or opinion words) gained from review cor-
pus, we want to assemble them into categories, 
conserving the information about opinion words 
(or review features) as high as possible. 
The information bottleneck algorithm has some 
benefits, mainly including (1) it treats the trade-off 
of precision versus complexity of clustering model 
through the rate distortion theory, which is a sub-
field of information theory; (2) it defines the ?dis-
tance? or ?similarity? in a well-defined way based 
on the mutual information. The efficiency of in-
formation bottleneck algorithm (Slonim and 
Tishby, 2000) motivates us to take it as the kernel 
of our framework. As far as we know, this ap-
proach has not been employed in opinion mining 
yet. 
In traditional information bottleneck approach, 
the distance between two data objects is measured 
by the Jensen-Shannon divergence (Lin, 1991), 
which aims to measure the divergence between 
two probability distributions. We alter this meas-
ure to integrate more semantic information, which 
will be illustrated in detail in the following sec-
tions, and the experimental result shows the 
effectiveness of the alteration.  
It would be worthwhile to highlight several as-
pects of our work here:  
z We propose an iterative reinforcement 
framework, and under this framework, review 
feature words and opinion words are organized 
into categories in a simultaneous and iterative 
manner. 
z In the process of clustering, the semantic in-
formation and the co-occurrence information 
are integrated. 
z The experimental results on real Chinese 
web reviews demonstrate that proposed 
method outperforms the template extraction 
based algorithm. 
2 Proposed Algorithm 
2.1 The Problem  
In product reviews, opinion words are used to ex-
press opinion, sentiment or attitude of reviewers. 
Although some review units may express general 
opinions toward a product, most review units are 
487
regarding to specific features of the product. 
A product is always reviewed under a certain 
feature set F. Suppose we have got a lexical list O 
which includes all the opinion expressions and 
their sentiment polarities. For the feature-level 
opinion mining, identifying the sentiment associa-
tion between F and O is essential. The key points 
in the whole process are as follows: 
z get opinion word set O (with polarity labels) 
z get product feature set F 
z identify relationships between F and O 
The focus of the paper is on the latter two steps, 
especially for the case of hidden sentiment asso-
ciation that the review features are implicit in the 
review sentences, but can be deduced by the opin-
ion words in its context. In contrast to existing ex-
plicit adjacency approaches, the proposed 
approach detects the sentiment association 
between F and O based on review feature 
categories and opinion word groups gained from 
the review corpus. 
To this end, we first consider two sets of asso-
ciation objects: the set of product feature words F 
= {f1,f2,?,fm} and the set of opinion words O = 
{o1,o2,?on}. A weighted bipartite graph from F 
and O can be built, denoted by G = {F, O, R}. 
Here R = [rij] is the m*n link weight matrix con-
taining all the pair-wise weights between set F and 
O. The weight can be calculated with different 
weighting schemes, in this paper, we set rij by the 
co-appearance frequency of fi and oj in clause level. 
We take F and O as two random variables, and 
the question of constructing or clustering the ob-
ject groups can be defined as finding compressed 
representation of each variable that reserves the 
information about another variable as high as pos-
sible. Take F as an example, we want to find its 
compression, denoted as C, such that the mutual 
information between C and O is as high as possi-
ble, under a constraint on the mutual information 
between F and C.  
We propose an iterative reinforcement frame-
work to deal with the tasks. An improved informa-
tion bottleneck algorithm is employed in this 
framework, which will be illustrated in detail in 
the following sections. 
2.2 Information Bottleneck Algorithm 
The information bottleneck method (IB) was pre-
sented by Tishby et al (1999). According to Shan-
non?s information theory (Cover and Thomas, 
1991), for the two random variables X, Y, the mu-
tual information I(X;Y) between the random vari-
ables X, Y is given by the symmetric functional: 
,
( | )
( ; ) ( ) ( | ) log
( )x X y Y
p y x
I X Y p x p y x
p y? ?
= ?         (1) 
and the mutual information between them meas-
ures the relative entropy between their joint distri-
bution p(x, y) and the product of respective mar-
ginal distributions p(x)p(y), which is the only con-
sistent statistical measure of the information that 
variable X contains about variable Y (and vice 
versa). Roughly speaking, some of the mutual in-
formation will be lost in the process of compres-
sion, e.g. ( , ) ( , )I C Y I X Y?  (C is a compressed rep-
resentation of X). 
This representation is defined through a (possi-
bly stochastic) mapping between each value 
x X? to each representative value c C? . Formally, 
this mapping can be characterized by a conditional 
distribution p(c|x), inducing a soft partitioning of X 
values, Specifically, each value of X _is associated 
with all the codebook elements (C values), with 
some normalized probability. 
_ 
The IB method is based on the following simple 
idea. Given the empirical joint distribution of two 
variables, one variable is compressed so that the 
mutual information about the other variable is pre-
served as much as possible. The method can be 
considered as finding a minimal sufficient partition 
or efficient relevant coding of one variable with 
respect to the other one. This problem can be 
solved by introducing a Lagrange multiplier ? , 
and then minimizing the functional: 
[ ( | )] ( , ) ( , )L p c x I C X I C Y?= ?                       (2) 
This solution is given in terms of the three dis-
tributions that characterize every cluster c C? , the 
prior probability for this cluster, p(c), its member-
ship probabilities p(c|x), and its distribution over 
the relevance variable p(y|c). In general, the mem-
bership probabilities, p(c|x) is ?soft?, i.e. every 
x X? can be assigned to every c C? in some 
(normalized) probability. The information bottle-
neck principle determines the distortion measure 
between the points x and c to be 
the [ ] ( | )( | ) || ( | ) ( | )log
( | )KL y
p y x
D p y x p y c p y x
p y c
=? , the 
Kullback-Leibler divergence (Cover and Thomas, 
1991) between the conditional distributions p(y|x) 
488
and p(y|c). Specifically, the formal optimal solu-
tion is given by the following equations which 
must be solved together. 
( )
( | ) exp( [ ( | ) || ( | )])
( , )
1
( | ) ( | ) ( ) ( | )
( )
( ) ( | ) ( )
KL
x
x
p c
p c x D p y x p y c
Z x
p y c p c x p x p y x
p c
p c p c x p x
??
? = ???? =??? =??
?
?
 
(3) 
Where ( , )Z x? is a normalization factor, and the 
single positive (Lagrange) parameter ? determines 
the ?softness? of the classification. Intuitively, in 
this procedure the information contained in X 
about Y ?squeezed? through a compact ?bottleneck? 
of clusters C, that is forced to represent the ?rele-
vant? part in X w.r.t to Y.  
An important special case is the ?hard? cluster-
ing case where C is a deterministic function of X. 
That is, p(c|x) can only take values of zero or one, 
This restriction, which corresponds to the 
limit ? ?? in Eqs 3 meaning every x X?  is as-
signed to exactly one cluster c C?  with a prob-
ability of one and to all the others with a probabil-
ity of zero. This yields a natural distance measure 
between distributions which can be easily imple-
mented in an agglomerative hierarchical clustering 
procedure (Slonim and Tishby, 1999).  
1,
( | )
0,
1
( | ) ( , )
( )
( ) ( )
x c
x c
if x c
p c x
otherwise
p y c p x y
p c
p c p x
?
?
? ??= ?? ???? =??? =???
?
?
                              (4) 
The algorithm starts with a trivial partitioning 
into |X| singleton clusters, where each cluster con-
tains exactly one element of X. At each step we 
merge two components of the current partition into 
a single new component in a way that locally 
minimizes the loss of mutual information about the 
categories. Every merger, *( , )i jc c c? , is formally 
defined by the following equation: 
*
*
* *
*
1,
( | )
0,
( )( )
( | ) ( | ) ( | )
( ) ( )
( ) ( ) ( )
i j
ji
i j
i j
x c or x c
p c x
otherwise
p cp c
p y c p y c p y c
p c p c
p c p c p c
? ? ???=? ?????? = +??? = +???
     (5) 
The decrease in the mutual information I(C, Y) due 
to this merger is defined by  
( , ) ( , ) ( , )i j before afterI c c I C Y I C Y? ? ?               (6) 
When ( , )beforeI C Y  and ( , )afterI C Y are the informa-
tion values before and after the merger, respec-
tively. After a little algebra, one can see 
( )( , ) ( ) ( ) ( | ), ( | )i j i j JS i jI c c p c p c D p y c p y c? ? ?? + ? ? ?  (7)  
Where the functional DJS  is the Jensen-Shannon 
divergence (Lin, 1991), defined as  
^ ^
, || ||JS i j i KL i j KL jD p p D p p D p p? ?? ? ? ?? ? = +? ? ? ? ? ?? ? ? ?   (8) 
where in our case  
{ } { }
{ }
* *
^
, ( | ), ( | )
( )( )
, ,
( ) ( )
( | ) ( | )
i j i j
ji
i j
i i j j
p p p y c p y c
p cp c
p c p c
p p y c p y c
? ?
? ?
? ??? ? ?? ?? ? ?? ??? = +??
                     (9) 
By introducing the information optimization cri-
terion the resulting similarity measure directly 
emerges from the analysis. The algorithm is now 
very simple. At each step we perform ?the best 
possible merger?, i.e. merge the clusters { , }i jc c  
which minimize ( , )i jI c c? .  
2.3 Improved Information Bottleneck Algo-
rithm for Semantic Information 
In traditional information bottleneck approach, the 
distance between two data objects is measured by 
the difference of information values before and 
after the merger, which is measured by the Jensen-
Shannon divergence. This divergence aims to 
measure the divergence between two probability 
distributions. For our case, the divergence is based 
on the co-occurrence information between the two 
variables F and O. 
While the co-occurrence in corpus is usually 
quantitatively sparse; additionally, Statistics based 
489
on word-occurrence loses semantic related infor-
mation. To avoid such reversed effects, in the pro-
posed framework we combine the co-occurrence 
information and semantic information as the final 
distance between the two types of objects. 
( , ) ( , )
(1 ) ( , )
{ } { }
i j semantic i j
i j
i j i j
D X X D X X
I X X
where X F X F X O X O
?
? ?
=
+ ?
? ? ? ? ? ? ?
 
(10) 
In equation 10, the distance between two data 
objects Xi and Xj is denoted as a linear combination 
of semantic distance and information value differ-
ence. The parameter ?  reflects the contribution of 
different distances to the final distance. 
Input: Joint probability distribution p(f,o) 
Output: A partition of F into m clusters, ?m?
{1,?,|F|}, and a partition of O into n clusters ?
n?{1,?,|O|} 
1. t?0 
2. Repeat 
a. Construct CFt?Ft 
b. ?i, j=1,?,|CFt|, i<j, calculate 
         ( , ) (1 ) ( , )t t t t tij semantic i j i jd D cf cf I cf cf? ? ?? + ?
c. for m?|CFt|-1 to 1 
1) find the indices {i, j}, for which dijt is 
minimized 
2) merge {cfit, cfjt}into cf*t  
3) update CFt? {CFt -{cfit, cfjt}}U {cf*t} 
4) update dijt costs w.r.t cf*t 
d. Construct COt?Ot 
e. ?i, j=1,?,|COt|, i<j,calculate 
     ( , ) (1 ) ( , )t t t t tij semantic i j i jd D co co I co co? ? ?? + ?
f. for n?|COt|-1 to 1 
1) find the indices {i, j}, for which dijt is 
minimized 
2) merge {coit,cojt}into co*t  
3) update COt ? {COt -{coit,cojt}}U {co*t}
4) update dijt costs w.r.t co*t 
g. t?t+1 
3. until (CFt = CFt-1 and COt =COt-1) 
Figure 1: Pseudo-code of semantic information bot-
tleneck in iterative reinforcement framework  
 
The semantic distance can be got by the usage 
of lexicon, such as WordNet (Budanitsky and Hirst, 
2006). In this paper, we use the Chinese lexicon 
HowNet1. 
The basic idea of the iterative reinforcement 
principle is to propagate the clustered results be-
tween different type data objects by updating their 
inter-relationship spaces. The clustering process 
can begin from an arbitrary type of data object. 
The clustering results of one data object type up-
date the interrelationship thus reinforce the data 
object categorization of another type. The process 
is iterative until clustering results of both object 
types converge. Suppose we begin the clustering 
process from data objects in set F, and then the 
steps can be expressed as Figure 1. After the itera-
tion, we can get the strongest n links between 
product feature categories and opinion word 
groups. That constitutes our set of sentiment asso-
ciation. 
3 Experimental Setup 
In this section we describe our experiments and the 
data used in these experiments. 
3.1 Data 
Our experiments take hotel reviews (in Chinese) 
as example. The corpus used in the experiments is 
composed by 4000 editor reviews on hotel, includ-
ing 857,692 Chinese characters. They are extracted 
from www.ctrip.com. Each review contains a 
user?s rating represented by ?stars?, the number of 
the star denotes the user?s satisfaction. The de-
tailed information is illustrated in Table 1, 
 
Table 1: The detail information of corpus 
User?s rating Number 
1 star 555 
2 star 1375 
3 star 70 
4 star 2000 
 
Then we use ICTCLAS2, a Chinese word seg-
mentation software to extract candidate review 
features and opinion words.  
Usually, adjectives are normally used to express 
opinions in reviews. Therefore, most of the exist-
ing researches take adjectives as opinion words. In 
the research of Hu et al (2004), they proposed that 
                                                 
1 http://www.keenage.com/ 
2 www.searchforum.org.cn 
490
other components of a sentence are unlikely to be 
product features except for nouns and noun 
phrases. Some researchers (Fujii and Ishikawa, 
2006) targeted nouns, noun phrases and verb 
phrases. The adding of verb phrases caused the 
identification of more possible product features, 
while brought lots of noises. So in this paper, we 
follow the points of Hu?s, extracting nouns and 
noun phrases as candidate product feature words. 
Take the whole set of nouns and noun phrases 
as candidate features will bring some noise. In or-
der to reduce such adverse effects, we use the 
function of Named Entity Recognition (NER) in 
ICTCLAS to filter out named entities, including: 
person, location, organization. Since the NEs have 
small probability of being product features, we 
prune the candidate nouns or noun phrases which 
have the above NE taggers. 
 
Table 2: The number of candidate review features 
and opinion words in our corpus 
Extracted In-
stance Total 
Non-
Repeated 
Candidate re-
view feature 86,623 15,249 
Opinion word 26,721 1,231 
 
By pruning candidate product feature words, we 
get the set of product feature words F. And the set 
of opinion words O is composed by all the adjec-
tives in reviews. The number of candidate product 
feature words and opinion words extracted from 
the corpus are shown as Table 2: 
3.2 Experimental Procedure 
We evaluate our approach from two perspectives:  
1) Effectiveness of product feature category 
construction by mutual reinforcement based clus-
tering;  
2) Precision of sentiment association between 
product feature categories and opinion word 
groups;  
4 Experimental Results and Discussion 
4.1 Evaluation of Review Feature Category 
Construction 
To calculate agreement between the review feature 
category construction results and the correct labels, 
we make use of the Rand index (Rand, 1971). This 
allows for a measure of agreement between two 
partitions, P1 and P2, of the same data set D. Each 
partition is viewed as a collection of n*(n-1)/2 pair 
wise decisions, where n is the size of D. For each 
pair of points di and dj in D, Pi either assigns them 
to the same cluster or to different clusters. Let a be 
the number of decisions where di is in the same 
cluster as dj in P1 and in P2. Let b be the number of 
decisions where the two instances are placed in 
different clusters in both partitions. Total agree-
ment can then be calculated using 
1 2( , ) ( 1) / 2
a b
Rand P P
n n
+= ?                             (11)  
In our case, the parts of product feature words in 
the pre-constructed evaluation set are used to rep-
resent the data set D; a and b represent the parti-
tion agreements between the pairs of any two 
words in the parts and in the clustering results re-
spectively. 
In equation 10, the parameter ? reflects the re-
spective contribution of semantic information and 
co-occurrence information to the final distance. 
When 0? = or 1? = , the co-occurrence informa-
tion or the semantic information will be utilized 
alone. 
In order to get the optimal combination of the 
two type of distance, we adjust the parameter 
? from 0 to 1(stepped by 0.2), and the accuracy of 
feature category construction with different ? are 
shown in Figure 2: 
 
 
Figure 2: The accuracy of review feature category 
construction with the variation of the parameter ?  
 
From this figure, we can find that the semantic 
information (? =1) contributes much more to the 
accuracy of review feature category construction 
than the co-occurrence information ( ? =0), and 
when ? =0, the approach is equivalent to the tradi-
tional information bottleneck approach. We con-
sider this is due partly to the sparseness of the cor-
491
pus, by enlarging the scale of the corpus or using 
the search engine (e.g. google etc), we can get 
more accurate results.  
Additionally, by a sensible adjust on the pa-
rameter ? (in this experiment, we set ?  as 0.6), 
we can get higher accuracy than the two baselines 
( ? =0 and ? =1), which indicates the necessity 
and effectiveness of the integration of semantic 
information and co-occurrence information in the 
proposed approach. 
4.2 Evaluation of Sentiment Association  
We use precision to evaluate the performance of 
sentiment association. An evaluation set is con-
structed manually first, in which there are not only 
the categories that every review feature word be-
long to, but also the relationship between each 
category and opinion word. Then we define preci-
sion as: 
number of correctly associated pairs
Precision
number of detected pairs
=
                                                                          (12) 
A comparative result is got by the means of 
template-extraction based approach on the same 
test set. By the usage of regular expression, the 
nouns (phrase) and gerund (phrase) are extracted 
as the review features, and the nearest adjectives 
are extracted as the related opinion words. Because 
the modifiers of adjectives in reviews also contain 
rich sentiment information and express the view of 
customs, we extract adjectives and their modifiers 
simultaneously, and take them as the opinion 
words. 
 
Table 3: Performance comparison in sentiment asso-
ciation 
Approach Pairs Precision 
Template extraction 27,683 65.89% 
Proposed approach 141,826 78.90% 
 
Table 3 shows the advantage of our approach 
over the extraction by explicit adjacency. Using 
the same product feature categorization, our sen-
timent association approach get a more accurate 
pair set than the direct extraction based on explicit 
adjacency. The precision we obtained by the itera-
tive reinforcement approach is 78.90%, almost 13 
points higher than the adjacency approach. This 
indicates that there are a large number of hidden 
sentiment associations in the real custom reviews, 
which underlines the importance and value of our 
work. 
5 Conclusions and Further Work 
In this paper, we propose a novel iterative rein-
forcement framework based on improved informa-
tion bottleneck approach to deal with the feature-
level product opinion-mining problem. We alter 
traditional information bottleneck method by inte-
gration with semantic information, and the ex-
perimental result demonstrates the effectiveness of 
the alteration. The main contribution of our work 
mainly including:  
z We propose an iterative reinforcement in-
formation bottleneck framework, and in this 
framework, review feature words and opinion 
words are organized into categories in a simul-
taneous and iterative manner. 
z In the process of clustering, the semantic in-
formation and the co-occurrence information 
are integrated. 
z The experimental results based on real Chi-
nese web reviews demonstrate that our method 
outperforms the template extraction based al-
gorithm. 
Although our methods for candidate product 
feature extraction and filtering (see in 3.1) can 
partly identify real product features, it may lose 
some data and remain some noises. We?ll conduct 
deeper research in this area in future work. Addi-
tionally, we plan to exploit more information, such 
as background knowledge, to improve the per-
formance. 
6 Acknowledgments 
This work was mainly supported by two funds, i.e., 
0704021000 and 60803085, and one another pro-
ject, i.e., 2004CB318109. 
 
References  
A. Andreevskaia, S. Bergler. When Specialists and 
Generalists Work Together: Overcoming Domain 
Dependence in Sentiment Tagging. ACL 2008. 
A. Budanitsky and G. Hirst. Evaluating wordnetbased 
measures of lexical semantic relatedness. Computa-
tional Linguistics, 32(1):13?47, 2006. 
492
A. Devitt, K. Ahmad. Sentiment Polarity Identification 
in Financial News: A Cohesion-based Approach. 
ACL 2007. 
A. Fujii and T. Ishikawa. A system for summarizing 
and visualizing arguments in subjective documents: 
Toward supporting decision making. The Workshop 
on Sentiment and Subjectivity in Text ACL2006. 
2006. 
A. Popescu and O. Etzioni. Extracting product features 
and opinions from reviews. HLT-EMNLP 2005. 
B. Liu, M. Hu, and J. Cheng. Opinion observer: analyz-
ing and comparing opinions on the web. WWW 2005. 
B. Pang and L. Lee. Seeing stars: Exploiting class rela-
tionships for sentiment categorization with respect to 
rating scales. ACL 2005. 
B. Pang, L. Lee, and S. Vaithyanathan. Thumbs up? 
Sentiment classification using machine learning 
techniques. EMNLP 2002. 
B. Philip, T. Hastie, C. Manning, and S. Vaithyanathan. 
Exploring sentiment summarization. In AAAI Spring 
Symposium on Exploring Attitude and Affect in Text: 
Theories and Applications (AAAI tech report SS-04-
07). 2004. 
B. Wang, H. Wang. Bootstrapping Both Product Fea-
tures and Opinion Words from Chinese Customer 
Reviews with Cross-Inducing. IJCNLP 2008. 
D. Kushal, S. Lawrence, and D. Pennock. Mining the 
peanut gallery: Opinion extraction and semantic clas-
sification of product reviews. WWW 2003. 
H. Kanayama, T. Nasukawa. Fully Automatic Lexicon 
Expansion for Domain-oriented Sentiment Analysis. 
EMNLP 2006 
H. Takamura, T. Inui. Extracting Semantic Orientations 
of Phrases from Dictionary. NAACL-HLT 2007. 
I. Titov, R. McDonald. Modeling online reviews with 
multi-grain topic models. WWW 2008.  
L. Ku, Y. Liang and H. Chen. Opinion Extraction, 
Summarization and Tracking in News and Blog Cor-
pora. AAAI-CAAW 2006. 
J. Blitzer, M. Dredze, F. Pereira. Biographies, Bolly-
wood, Boom-boxes and Blenders: Domain Adapta-
tion for Sentiment Classification. ACL 2007. 
J. Lin. Divergence Measures Based on the Shannon 
Entropy. IEEE Transactions on Information theory, 
37(1):145?151, 1991. 
K. Bloom and N. Garg and S. Argamon. Extracting 
Appraisal Expressions. NAACL-HLT 2007. 
M. Hu and B. Liu. Mining and summarizing customer 
reviews. KDD 2004. 
M. Hu and B. Liu. Mining opinion features in customer 
reviews. AAAI 2004.  
N. Slonim, N. Tishby. Agglomerative information bot-
tleneck. NIPS 1999. 
N. Slonim and N. Tishby. Document Clustering Using 
word Clusters via the Information Bottleneck 
Method. SIGIR 2000. 
N. Slonim and N. Tishby. The power of word clusters 
for text classification. ECIR 2001. 
N. Tishby, F. Pereira, W. Bialek. The information bot-
tleneck method. 1999, arXiv: physics/0004057v1  
P. Turney. Thumbs up or thumbs down? Semantic ori-
entation applied to unsupervised classification of re-
views. ACL 2002. 
P. Turney and M. Littman. Measuring Praise and Criti-
cism: Inference of Semantic Orientation from Asso-
ciation. ACM Transactions on Information Systems, 
2003,21(4): 315-346. 
Q. Su, X. Xu, H. Guo, Z. Guo, X. Wu, X. Zhang, B. 
Swen and Z. Su. Hidden sentiment association in 
Chinese web opinion mining. WWW 2008. 
S. Tan, G. Wu, H. Tang and X. Cheng. A novel scheme 
for domain-transfer problem in the context of senti-
ment analysis. CIKM 2007. 
S. Tan, Y. Wang, G. Wu and X. Cheng. Using unla-
beled data to handle domain-transfer problem of 
semantic detection. SAC 2008. 
S. Tan, X. Cheng, Y. Wang and H. Xu. Adapting Naive 
Bayes to Domain Adaptation for Sentiment Analysis. 
ECIR 2009. 
T. Cover and J. Thomas. Elements of Information The-
ory. John Wiley & Sons, New York, 1991. 
T. Zagibalov, J. Carroll. Automatic Seed Word Selec-
tion for Unsupervised Sentiment Classification of 
Chinese Text. Coling 2008. 
V. Hatzivassiloglou and K. McKeown. Predicting the 
semantic orientation of adjectives. ACL 1997. 
V. Hatzivassiloglou and J. Wiebe. Effects of adjective 
orientation and gradability on sentence subjectivity. 
Coling 2000. 
W. Rand. Objective criteria for the evaluation of clus-
tering methods. Journal of the American Statistical 
Association, 66, 846-850. 1971 
 
493
Coling 2010: Poster Volume, pages 1327?1335,
Beijing, August 2010
MIEA: a Mutual Iterative Enhancement Approach for Cross-Domain  
Sentiment Classification 
 
Qiong Wu1,2, Songbo Tan1, Xueqi Cheng1 and Miyi Duan1 
1Institute of Computing Technology, Chinese Academy of Sciences 
2 Graduate University of Chinese Academy of Sciences 
{wuqiong,tansongbo}@software.ict.ac.cn 
 
Abstract 
Recent years have witnessed a large body of 
research works on cross-domain sentiment 
classification problem, where most of the re-
search endeavors were based on a supervised 
learning strategy which builds models from 
only the labeled documents or only the labeled 
sentiment words. Unfortunately, such kind of 
supervised learning method usually fails to 
uncover the full knowledge between docu-
ments and sentiment words. Taking account of 
this limitation, in this paper, we propose an it-
erative reinforcement learning approach for 
cross-domain sentiment classification by si-
multaneously utilizing documents and words 
from both source domain and target domain. 
Our new method can make full use of the rein-
forcement between documents and words by 
fusing four kinds of relationships between 
documents and words. Experimental results 
indicate that our new method can improve the 
performance of cross-domain sentiment classi-
fication dramatically. 
1 Introduction 
Sentiment classification is the task of determin-
ing the opinion (e.g., negative or positive) of a 
given document. In recent years, it has drawn 
much attention with the increasing reviewing 
pages and blogs etc., and it is very important for 
many applications, such as opinion mining and 
summarization (e.g., (Ku et al, 2006; McDonald 
et al, 2007)). 
In most cases, a variety of supervised classifi-
cation methods can perform well in sentiment 
classification. This kind of methods requires a 
condition to guarantee the accuracy of classifica-
tion: training data should have the same distribu-
tion with test data so that test data could share 
the information got from training data. So the 
labeled data in the same domain with test data is 
considered as the most valuable resources for the 
sentiment classification. However, such re-
sources in different domains are very imbalanced. 
In some traditional domains or domains of con-
cern, many labeled sentiment data are freely 
available on the web, but in other domains, la-
beled sentiment data are scarce and it involves 
much human labor to manually label reliable 
sentiment data. The challenge is how to utilize 
labeled sentiment data in one domain (that is, 
source domain) for sentiment classification in 
another domain (that is, target domain). This 
raises an interesting task, cross-domain sentiment 
classification (or sentiment transfer). In this work, 
we focus on one typical kind of sentiment trans-
fer problem, which utilizes only training data 
from source domain to improve sentiment 
classification performance for target domain, 
without any labeled data for the target domain 
(e.g., (Andreevskaia and Bergler, 2008)). 
In recent years, some studies have been con-
ducted to deal with sentiment transfer problems. 
However, most of the attempts rely on only the 
labeled documents (Aue and Gamon, 2005; Tan 
et al, 2007; Tan et al, 2009; Wu et al, 2009) or 
the labeled sentiment words (Gamon and Aue, 
2005) to improve the performance of sentiment 
transfer, so this kind of methods fails to uncover 
the full knowledge between the documents and 
the sentiment words. 
In fact, the opinion of a document can be de-
termined by the interrelated documents as well as 
by the interrelated words, and this rule is also 
tenable when determining the opinion of a sen-
timent word. This rule is based on the following 
intuitive observations:  
(1)  A document strongly linked with other posi-
tive (negative) documents could be consid-
ered as positive (negative); in the same way, 
a word strongly linked with other positive 
(negative) words could be considered as 
positive (negative). 
1327
(2)  A document containing many positive (nega-
tive) words could be considered as positive 
(negative); similarly, a word appearing in 
many positive (negative) documents could 
be considered as positive (negative). 
Inspired by these observations, we aim to take 
into account all the four kinds of relationships 
among documents and words (i.e. the relation-
ships between documents, the relationships be-
tween words, the relationships between words 
and documents, and the relationships between 
documents and words) in both source domain 
and target domain under a unified framework for 
sentiment transfer.  
In this work, we propose an iterative rein-
forcement approach to implement the above idea. 
The proposed approach makes full use of all the 
relationships among documents and words from 
both source domain and target domain to transfer 
information between domains. In our approach, 
the opinion of a document (word) is reinforced 
by the opinion of all its interrelated documents 
and words; and the updated opinion of the docu-
ment (word) will conversely reinforce the opin-
ions of its interrelated documents and words. 
That is to say, it is an iterative reinforcement 
process until it converges to a final result.  
The contribution of our work is twofold. First, 
we extend the traditional sentiment-transfer 
methods by utilizing the full knowledge between 
interrelated documents and words. Second, we 
present a reinforcement approach to get the opin-
ions of documents by making use of graph-
ranking algorithm.  
The proposed approach is evaluated on three 
domain-specific sentiment data sets. The experi-
mental results show that our approach can dra-
matically improve the accuracy when transferred 
to another target domain. And we also conduct 
extensive experiments to investigate the parame-
ters sensitivity. The results show that our algo-
rithm is not sensitive to these parameters. 
2 Proposed Methods 
2.1 Problem Definition 
In this paper, we have two document sets: the 
test documents DU = {d1,?,dnd} where di is the 
term vector of the ith text document and each 
di?DU(i = 1,?,nd) is unlabeled; the training 
documents DL = {dnd+1,?,dnd+md} where dj repre-
sents the term vector of the jth text document and 
each dj?DL(j = nd+1,?,nd+md) should have a 
label from a category set C = {negative, posi-
tive}. We assume the training dataset DL is from 
the interrelated but different domain with the test 
dataset DU. Also, we have two word sets: WU = 
{w1,?,wnw} is the word set of DU and each 
wi?WU (i = 1,?,nw) is unlabeled; WL = 
{wnw+1,?,wnw+mw} is the word set of DL and each 
wj?WL(j = nw+1,?,nw+mw) has a label from C. 
Our objective is to maximize the accuracy of as-
signing a label in C to di?DU (i = 1,?,nd) utiliz-
ing the training data DL and WL in another do-
main. 
The proposed algorithm is based on the fol-
lowing presumptions: 
   (1) WL?WU??. 
   (2) The labels of documents appear both in the 
training data and the test data should be the same. 
2.2 Overview 
The proposed approach is inspired by graph-
ranking algorithm whose idea is to give a node 
high score if it is strongly linked with other high-
score nodes. Graph-ranking algorithm has been 
successfully used in many fields (e.g. PageRank 
(Brin et al 1999), LexRank (Erkan and Radev, 
2004)). We can get the following thoughts based 
on the ideas of PageRank and HITS (Kleinberg, 
1998): 
(1)   If a document is strongly linked with other 
positive (negative) documents, it tends to be 
positive (negative); and if a word is strongly 
linked with other positive (negative) words, 
it tends to be positive (negative). 
(2)  If a document contains many positive (nega-
tive) words, it tends to be positive (nega-
tive); and if a word appears in many posi-
tive (negative) documents, it tends to be 
positive (negative). 
Given the data points of documents and words, 
there are four kinds of relationships in our prob-
lem: 
z DD-Relationship: It denotes the relation-
ships between documents, usually computed 
by their content similarity. 
z WW-Relationship: It denotes the relation-
ships between words, usually computed by 
knowledge-based approach or corpus-based 
approach. 
z DW-Relationship: It denotes the relation-
ships between documents and words, usu-
1328
ally computed by the relative importance of 
a word in a document. 
z WD-Relationship: It denotes the relation-
ships between words and documents, usu-
ally computed by the relative importance of 
a document to a word. 
Meanwhile, our problem refers to both source 
domain and target domain, so our approach con-
siders eight relationships altogether: DDO-
Relationship (the relationships between DU and 
DL), DDN-Relationship (the relationships be-
tween DU), WWO-Relationship (the relationships 
between WU and WL), WWN-Relationship (the 
relationships between WU and WU), DWO-
Relationship (the relationships between DU and 
WL), DWN-Relationship (the relationships be-
tween DU and WU), WDO-Relationship (the rela-
tionships between WU and DL), WDN-
Relationship (the relationships between WU and 
DU). The first four relationships are used to com-
pute the sentiment scores of the documents, and 
the others are used to compute the sentiment 
scores of the words. 
The iterative reinforcement approach could 
make full use of all the relationships in a unified 
framework. The framework of the proposed ap-
proach is illustrated in Figure 1.  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1. Framework of the proposed approach 
 
The framework consists of a graph-building 
phase and an iterative reinforcement phase. In 
the graph-building phase, the input includes both 
the labeled data from source domain and the 
unlabeled data from target domain. The proposed 
approach builds four graphs based on these data 
to reflect the above relationships respectively. 
For source-domain data, we initialize every 
document and word a score (?1? denotes positive, 
and ?-1? denotes negative) to represent its degree 
of sentiment orientation, and we call it sentiment 
score; for target-domain data, we set the initial 
sentiment scores to 0.  
In the iterative reinforcement phase, our ap-
proach iteratively computes the sentiment scores 
of the documents and words based on the graphs. 
When the algorithm converges, all the documents 
get their sentiment scores. If its sentiment score 
is between 0 and 1, the document should be clas-
sified as ?positive?. The closer its sentiment 
score is near 1, the higher the ?positive? degree 
is. Otherwise, if its sentiment score is between 0 
and -1, the document should be classified as 
?negative?. The closer its sentiment score is near 
-1, the higher the ?negative? degree is. 
The algorithms of sentiment graph building 
and iterative reinforcement are described in de-
tails in the next sections, respectively. 
2.3 Sentiment-Graph Building 
Symbol Definition 
In this section, we build four graphs to reflect 
eight relationships, and the meanings of symbols 
are shown in Table 1.  
Rela-
tionship
Similarity ma-
trix 
Normal-
ized form Neighbor matrix
DDO UL=[ULij]ndxmd LU?  Kndij
LL UnUn ?= ][
DDN UU=[UU ij]ndxnd UU?  Kndij
UU UnUn ?= ][
WWO VL=[VLij]nwxmw LV?  Knwij
LL VnVn ?= ][
WWN VU=[VU ij]nwxnw UV?  Knwij
UU VnVn ?= ][
DWO ML=[MLij]ndxmw LM?  Kndij
LL MnMn ?= ][
DWN MU=[MUij]ndxnw UM?  Kndij
UU MnMn ?= ][
WDO NL=[NLij]nwxmd LN?  Knwij
LL NnNn ?= ][
WDN NU=[NUij]nwxnd UN?  Knwij
UU NnNn ?= ][
Table 1: Symbol definition 
In this table, the first column denotes the name 
of the relationship; the second column denotes 
1329
the similarity matrix to reflect the corresponding 
relationship; in consideration of convergence, we 
normalize the similarity matrix, and the normal-
ized form is listed in the third column; in order to 
compute sentiment scores, we find the neighbors 
of a document or a word and the neighbor matrix 
is listed in the fourth column. 
Document-to-Document Graph  
We build an undirected graph whose nodes de-
note documents in both DL and DU and edges 
denote the content similarities between docu-
ments. If the content similarity between two 
documents is 0, there is no edge between the two 
nodes. Otherwise, there is an edge between the 
two nodes whose weight is the content similarity. 
The edges in this graph are divided into two parts: 
edges between DU and DL; edges between DU 
itself, so we build the graph in two steps. 
(1) Create DU and DL Edges  
The content similarity between two documents is 
computed with the cosine measure. We use an 
adjacency matrix UL to denote the similarity ma-
trix between DU and DL. UL=[ULij]ndxmd is defined 
as follows: 
mdjndi
dd
dd
U
ndji
ndjiL
ij ,...,1,,...,1, ==?
?=
+
+          (1) 
The weight associated with word w is com-
puted with tfwidfw where tfw is the frequency of 
word w in the document and idfw is the inverse 
document frequency of word w, i.e. 1+log(N/nw), 
where N is the total number of documents and nw 
is the number of documents containing word w in 
a data set.   
In consideration of convergence, we normalize 
UL to LU? by making the sum of each row equal to 
1: 
??
??
? ?= ?? ==
otherwise
UifUU
U
md
j
L
ij
md
j
L
ij
L
ijL
ij
,0
0,?
11              (2) 
In order to find the neighbors (in another word, 
the nearest documents) of a document, we sort 
every row of LU?  to LU~ in descending order. That 
is: ijLU~ ? ikLU~  (i = 1,?,nd; j,k = 1,?,md; k?j). 
Then for di?DU (i = 1,?,nd), ijLU~ (j = 1,?,K ) 
corresponds to K neighbors in DL. We use a ma-
trix 
Kndij
LL UnUn ?= ][  to denote the neighbors of DU 
in source domain, with ijLUn  corresponding to the 
jth nearest neighbor of di. 
(2) Create DU and DU Edges  
Similarly, the edge weight between DU itself is 
computed by the cosine measure. We get the 
similarity matrix UU=[UUij]ndxnd, the normalized 
similarity matrix UU? , and the neighbors of  DU in 
target domain: 
Kndij
UU UnUn ?= ][ . 
Word-to-Word Graph 
Similar to the Document-to-Document Graph, 
we build an undirected graph to reflect the rela-
tionship between words in WL and WU, in which 
each node corresponds to a word and the edge 
weight between any different words corresponds 
to their semantic similarity. The edges in this 
graph are divided into two parts: edges between 
WU and WL; edges between WU itself, so we also 
build the graph in two steps. 
(1) Create WU and WL Edges 
We compute the semantic similarity using cor-
pus-based approach which computes the similar-
ity between words utilizing information from 
large corpora. There are many measures to iden-
tify word semantic similarity, such as mutual 
information (Turney, 2001), latent semantic 
analysis (Landauer et al, 1998) etc. In this study, 
we compute word semantic similarity based on 
the sliding window measure, that is, two words 
are semantically similar if they co-occur at least 
once within a window of maximum Kwin words, 
where Kwin is the window size. We use an adja-
cency matrix VL to denote the similarity matrix 
between WU and WL. VL=[VLij]nwxmw is defined as 
follows: 
??
??
? ??
?
= ++
+
otherwise
wwif
wpwp
wwpN
V nwjinwji
nwji
ij
L
,0
,
)()(
),(
log        
(3) 
where N is the total number of words in DU; p(wi, 
wj) is the probability of the co-occurrence of wi 
and wj within a window, i.e. num(wi, wj)/N, 
where num(wi, wj) is the number of the times wi 
and wj co-occur within the window; p(wi) and 
p(wj) are the probabilities of the occurrences of 
wi and wj respectively, i.e. num(wi)/N and 
num(wj)/N, where num(wi) and num(wj) are the 
1330
numbers of the times wi and wj occur. We nor-
malize VL to LV? to make the sum of each row 
equal to 1. Then we sort every row of LV?  to LV~ in 
descending order, and we use a matrix 
Knwij
LL VnVn ?= ][  to denote the neighbors of WU in 
source domain. 
(2) Create WU and WU Edges 
Then we also compute the edge weight between 
any different nodes which denote words in WU 
by the sliding window measure. We get the simi-
larity matrix VU=[VUij]nwxnw, the normalized simi-
larity matrix UV? , and the neighbors of  WU in 
target domain: 
Knwij
UU VnVn ?= ][ . 
Document-to-Word Graph 
We can build a weighted directed bipartite graph 
from documents in DU and words in WL and WU 
in the following way: each node in the graph cor-
responds to a document in DU or a word in WL 
and WU; if word wj appears in document di, we 
create an edge from di to wj. The edges in this 
graph are divided into two parts: edges from DU 
to WL; edges from DU to WU, so we also build the 
graph in two steps. 
(1) Create DU to WL Edges 
The edge weight from a document in DU to a 
word in WL is proportional to the importance of 
word wj in document di. We use an adjacency 
matrix ML to denote the similarity matrix from 
DU to WL. ML=[MLij]ndxmw is defined as follows: 
?
?
?
?= ++
i
nwjnwj
dw
ww
wwL
ij idftf
idftf
M                          (4) 
where w represents a unique word in di and tfw, 
idfw are respectively the term frequency in the 
document and the inverse document frequency. 
We normalize ML to LM?  to make the sum of each 
row equal to 1. Then we sort every row of LM?  to 
LM
~ in descending order, and we use a matrix 
Kndij
LL MnMn ?= ][  to denote the neighbors of D
U in 
WL. 
(2) Create DU to WU Edges 
Similarly, we can also compute the edge weight 
from a document in DU to a word in WU in the 
same way. We get the similarity matrix 
MU=[MUij]ndxnw, the normalized similarity matrix 
UM? , and the neighbors of  DU in WU: 
Kndij
UU MnMn ?= ][ . 
Word-to-Document Graph 
In this section, we build a weighted directed 
bipartite graph from words in WU and documents 
in DL and DU in which each node in the graph 
corresponds to a word in WU and a document in 
DL or DU; if word wj appears in document di, we 
create an edge from wj to di. The edges in this 
graph are also divided into two parts: edges from 
WU to DL; edges from WU to DU. 
(1) Create WU to DL Edges 
Similar to 3.3.4, the edge weight from a word in 
WU to a document in DL is proportional to the 
importance of word wi in document dj. We use an 
adjacency matrix NL=[NLij]nwxmd to denote the 
similarity matrix from WU to DL. We normalize 
NL to LN?  to make the sum of each row equal to 1. 
Then we sort every row of LN?  to LN~ in descend-
ing order, and we use a matrix 
Knwij
LL NnNn ?= ][  to 
denote the neighbors of WU in DL. 
(2) Create WU to DU Edges 
We can also compute the edge weight from a 
word in WU to a document in DU in the same way. 
We get the similarity matrix NU=[NUij]nwxnd, the 
normalized similarity matrix UN? , and the 
neighbors of  WU in DU: 
Knwij
UU NnNn ?= ][ . 
2.4 Proposed Method 
Based on the two thoughts introduced in Sec-
tion 2.2, we fuse the eight relationships ab-
stracted from the four graphs together to itera-
tively reinforce sentiment scores, and we can 
obtain the iterative equation as follows: 
??
??
??
??
??
??
?+?+
?+?=
i
U
i
L
i
U
i
L
Mnr
rir
U
Mnl
lil
L
Unh
hih
U
Ung
gig
L
i
wsMwsM
dsUdsUds
)?()?(
)?()?(
??
??
   (5) 
??
??
??
??
??
??
?+?+
?+?=
j
U
j
L
j
U
j
L
Nnr
rjr
U
Nnl
ljl
L
Vnh
hjh
U
Vng
gjg
L
j
dsNdsN
wsVwsVws
)?()?(
)?()?(
??
??
    (6) 
where ?i  means the ith row of a matrix; Ds = 
{ds1,?,dsnd, dsnd+1,?, dsnd+md} represents the 
sentiment scores of DU and DL; Ws = 
{ws1,?,wsnw, wsnw+1,?, wsnw+mw} represents the 
sentiment scores of WU and WL; ?  and ?  show 
1331
the relative contributions to the final sentiment 
scores from source domain and target domain 
when calculating DD-Relationship and WW-
Relationship, and ?  + ? =1; ? and?show the 
relative contributions to the final sentiment 
scores from source domain and target domain 
when calculating DW-Relationship and WD-
Relationship, and ? +?=1. 
For simplicity, we merge the relationships 
from source domain and target domain. That is, 
for formula (5), we merge the first two items into 
one, the last two items into one; for formula (6), 
we merge its first two items into one, its last two 
items into one. Thus, (5) and (6) are transformed 
into (7) and (8) as follows: 
??
?? ??
??+??=
ii Mnl
lil
Ung
gigi wsMdsUds )?()?( ??           (7) 
??
?? ??
??+??=
jj Vnl
ljl
Nng
gjgj wsVdsNws )?()?( ??    (8) 
where  ? and ? show the relative contributions 
to the final sentiment scores from document sets 
and word sets, and ?+?=1. 
In consideration of the convergence, Ds and 
Ws are normalized separately after each iteration 
as follows to make the sum of positive scores 
equal to 1, and the sum of negative scores equal 
to -1: 
???
???
?
>
<?
= ?
?
?
?
0,
0,)(
i
Dj
ji
i
Dj
ji
dsifdsds
dsifdsds
ds
U
pos
U
neg
i
                (9) 
???
???
?
>
<?
= ?
?
?
?
0,
0,)(
j
Wi
ij
j
Wi
ij
j
wsifwsws
wsifwsws
ws
U
pos
U
neg              (10) 
where U
negD and
U
posD denote the negative and 
positive document set of DU respectively; 
U
negW and 
U
posW denote the negative and positive 
word set of WU respectively.  
Here is the complete algorithm: 
1. Initialize the sentiment score vector dsi 
of di?DL (i = nd+1,?, nd+md) with 1 when 
di is labeled ?positive?, and with -1 when di 
is labeled ?negative?, and initialize the sen-
timent score vector wsi of wi?WL (i = 
nw+1,?, nw+mw) with 1 when wi is labeled 
?positive?, and with -1 when wi is labeled 
?negative?. And we normalize dsi (i = 
nd+1,?, nd+md) (wsi (i = nw+1,?, 
nw+mw)) to make the sum of positive scores 
of DL (WL) equal to 1, and the sum of nega-
tive scores of DL (WL) equal to -1. Also, the 
initial sentiment scores of DU and WU are set 
to 0. 
2. Alternate the following two steps until 
convergence: 
2.1.Compute and normalize dsi (i = 1,?, 
nd) using formula (7) and (9): 
2.2.Compute and normalize wsj (j=1,?,nw) 
using formula (8) and (10): 
where )(kids and
)(k
jws denote the ids and wsj 
at the kth iteration. 
3. According to dsi?Ds (i = 1,?,nd), as-
sign each di?DU (i = 1,?,nd) a label. If dsi 
falls in the range [-1,0], assign di the label 
?negative?; if dsi falls in the range [0,1], as-
sign di the label ?positive?. 
3 Experiments 
In this section, we evaluate our approach on 
three different domains and compare it with 
some state-of-the-art algorithms, and also evalu-
ate the approach?s sensitivity to its parameters. 
Note that we conduct experiments on Chinese 
data, but the main idea in the proposed approach 
is language-independent in essence. 
3.1 Data Preparation 
We use three Chinese domain-specific data sets 
from on-line reviews, which are: Book Reviews1 
(B, www.dangdang.com/), Hotel Reviews 2  (H, 
www.ctrip.com/) and Notebook Reviews 3  (N, 
www.360buy.com/). Each dataset has 4000 la-
beled reviews (2000 positives and 2000 nega-
tives).  
We use ICTCLAS (http://ictclas.org/), a Chi-
nese text POS tool, to segment these Chinese 
reviews. Then, utilizing the part-of-speech tag-
ging function provided by ICTCLAS, we take all 
adjectives, adverbs and adjective-noun phrases as 
candidate sentiment words. After removing the 
repeated words and ambiguous words, we get a 
list of words in each domain.  
For the list of words in each domain, we 
manually label every word as ?negative?, ?posi-
                                                 
1www.searchforum.org.cn/tansongbo/corpus/Dangdang_Book_4000.rar 
2www.searchforum.org.cn/tansongbo/corpus/Ctrip_htl_4000.rar 
3www.searchforum.org.cn/tansongbo/corpus/Jingdong_NB_4000.rar 
1332
tive? or ?neutral?, and we take those ?negative? 
and ?positive? words as a sentiment word set.  
Note that we use the sentiment word set only 
for source domain, while using the candidate 
sentiment words for target domain.  
Lastly, the documents are represented by vec-
tor space model. In this model, each document is 
converted into bag-of-words presentation in the 
remaining term space. We compute term weight 
with the frequency of the term in the document. 
We choose one of the three data sets as 
source-domain data DL, and its corresponding 
sentiment word set as WL; we choose another 
data set as target-domain data DU, and its corre-
sponding candidate sentiment words as WU. 
3.2 Baseline Methods 
In this paper we compare our approach with the 
following baseline methods:  
Proto: This method applies a traditional super-
vised classifier, prototype classifier (Tan et al, 
2005), for the sentiment transfer. And it only 
uses source domain documents as training data. 
LibSVM: This method applies a state-of-the-
art supervised learning algorithm, Support Vec-
tor Machine, for the sentiment transfer. In detail, 
we use LibSVM (Chang and Lin, 2001) with a 
linear kernel and set al options as default. This 
method only uses source domain documents as 
training data. 
TSVM:  This method applies transductive 
SVM (Joachims, 1999) for the sentiment transfer 
which is a widely used method for improving the 
classification accuracy. In our experiment, we 
use Joachims?s SVM-light package 
(http://svmlight.joachims.org/) for TSVM. We 
use a linear kernel and set al parameters as de-
fault. This method uses both source domain data 
and target domain data. 
3.3 Overall Performance 
In this section, we compare proposed approach 
with the three baseline methods. There are three 
parameters in our algorithm, K, Kwin, ? (?can be 
calculated by 1-?). We set K to 50, and Kwin to 10 
respectively. With different ?, our approach can 
be considered as utilizing different relative con-
tributions from document sets and word sets. In 
order to identify the importance of both docu-
ment sets and word sets for sentiment transfer, 
we separately set ? to 0, 1, 0.5 to show the accu-
racy of utilizing only word sets (referred to as 
WORD), only document sets (referred to as 
DOC), and both the document and word sets (re-
ferred to as ALL). It is thought that the algorithm 
achieves the convergence when the changing 
between the sentiment score dsi computed at two 
successive iterations for any di?DU (i = 1,?,nd) 
falls below a given threshold, and we set the 
threshold 0.00001 in this work. The parameters 
will be studied in parameters sensitivity section. 
Table 2 shows the accuracy of Prototype, 
LibSVM, TSVM and our algorithm when train-
ing data and test data belong to different domains.  
As we can observe from Table 2, our algo-
rithm produces much better performance than 
supervised baseline methods. Compared with the 
traditional classifiers, our approach outperforms 
them by a wide margin on all the six transfer 
tasks. The great improvement compared with the 
baselines indicates that our approach performs 
very effectively and robustly. 
 Traditional Classifier Our Approach  
 Proto LibSVM
TSVM 
DOC WORD ALL
B->H 0.735 0.747 0.749 0.772 0.734 0.763
B->N 0.651 0.652 0.769 0.714 0.785 0.795
H->B 0.645 0.675 0.614 0.671 0.668 0.703
H->N 0.729 0.669 0.726 0.749 0.727 0.734
N->B 0.612 0.608 0.622 0.638 0.667 0.726
N->H 0.724 0.711 0.772 0.764 0.740 0.792
Aver-
age 0.683 0.677 0.709 0.718 0.720 0.752
Table 2: Accuracy comparison of different methods 
Table 2 shows the average accuracy of TSVM 
is higher than both traditional classifiers, since it 
utilizes the information of both source domain 
and target domain. However, the proposed ap-
proach outperforms TSVM: the average accuracy 
of the proposed approach is about 4.3% higher 
than TSVM. This is caused by two reasons. First, 
TSVM is not dedicated for sentiment-transfer 
learning. Second, TSVM requires the ratio be-
tween positive and negative examples in the test 
data to be close to the ratio in the training data, 
so its performance will be affected if this re-
quirement is not met. 
Results of ?DOC? and ?WORD? are shown in 
column 4 and 5 of Table 2. As we can observe, 
they produce better performance than all the 
baselines. This is caused by two reasons. First, 
?DOC? and ?WORD? separately utilize the sen-
1333
timent information of documents and words. 
Second, both ?DOC? and ?WORD? involve an 
iterative reinforcement process to improve their 
performance. The great improvement indicates 
that the iterative reinforcement approach is effec-
tive for sentiment transfer. 
Besides, Table 2 also shows both document 
sets and word sets are important for sentiment 
transfer. The approach ?ALL? outperforms the 
approaches ?DOC? and ?WORD? on almost all 
the six transfer tasks except ?B->H? and ?H->N?. 
The average increase of accuracy over all the six 
tasks is 3.4% and 3.2% respectively. The reason 
is: at every iteration, the classification accuracy 
of documents and words is improved by each 
other, and then the accuracy of sentiment transfer 
is improved by the documents and words that are 
classified more accurately. As for ?B->H? and 
?H->N?, the performance of utilizing only 
document sets is so good that the word sets 
couldn?t improve the performance any more. The 
improvement of the approach ?ALL? convinces 
us that not a single one of the four relationships 
can be omitted.  
3.4 Parameters Sensitivity 
The proposed algorithm has an important pa-
rameter, ? (?can be calculated by 1-?). In this 
section, we conduct experiments to show that our 
algorithm is not sensitive to this parameter. 
To investigate the sensitivity of proposed 
method involved with the parameter ?, we set K 
to 50, and Kwin to 10. And we change ? from 0 to 
1, an increase of 0.1 each. We also evaluate ? on 
the six tasks mentioned in section 3.1, and the 
results are shown in figure 2.  
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1?
Ac
cu
ra
cy
B->H B->N H->B H->N N->B N->H
 
Figure 2:  Accuracy for Different ? 
We can observe from Figure 2 that the accu-
racy first increases and then decreases when ? is 
increased from 0 to 1. The accuracy changes 
gradually when ? is near 0 or 1, and it changes 
less when ? is between 0.2 and 0.8. It is easy to 
explain this phenomenon. When ? is set to 0, this 
indicates our algorithm only uses word sets to aid 
classification, without the information of docu-
ment sets. And if ? is set to 1, our algorithm only 
uses document sets to calculate sentiment score, 
without the help of word sets. Both cases above 
don?t use all information of four relationships, so 
their accuracies are worse than to equal the con-
tributions of both document and word sets. This 
experiment shows that the proposed algorithm is 
not sensitive to the parameter ? as long as ? is 
not 0 or 1. We set ? to 0.5 in our overall-
performance experiment. 
3.5 Convergence 
Our algorithm is an iterative process that will 
converge to a local optimum. We evaluate its 
convergence on the six tasks mentioned above. 
Figure 3 shows the change of accuracy with re-
spect to the number of iterations. We can observe 
from figure 3 that the curve rises sharply during 
the first 6 iterations, and it is very stable after 10 
iterations are performed. This experiment indi-
cates that our algorithm could converge very 
quickly to get a local optimum. 
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1 4 7 10 13 16 19 22 25 28 31 34 37 40Iteration
Ac
cu
ra
cy
B->H B->N H->B H->N N->B N->H
 
Figure 3:  Performance for Iteration 
4 Conclusions 
In this paper, we propose a novel cross-domain 
sentiment classification approach, which is an 
iterative reinforcement approach for sentiment 
transfer by utilizing all the relationships among 
documents and words from both source domain 
and target domain to transfer information be-
tween domains. First, we build three graphs to 
reflect the above relationships respectively. Then, 
1334
we assign a score for every unlabelled document 
to denote its extent to ?negative? or ?positive?. 
We then iteratively calculate the score by making 
use of the graphs. Finally, the final score for sen-
timent classification is achieved when the algo-
rithm converges, so we can label the target-
domain data based on these scores.  
We conduct experiments on three domain-
specific sentiment data sets. The experimental 
results show that the proposed approach could 
dramatically improve the accuracy when trans-
ferred to a target domain. To investigate the pa-
rameter sensitivity, we conduct experiments on 
the same data sets. It is observed that our ap-
proach is not very sensitive to its four parameters, 
and could converge very quickly to get a local 
optimum.  
In this study, we employ only cosine measure, 
sliding window measure and vector measure to 
compute similarity. These are too general, and 
perhaps not so suitable for sentiment classifica-
tion. In the future, we will try other methods to 
calculate the similarity. Furthermore, we experi-
ment our approach on only three domains, and 
we will apply our approach to many more do-
mains. 
5 Acknowledgments 
This work was mainly supported by two funds, 
i.e., 60933005 & 60803085, and two another pro-
jects, i.e., 2007CB311100 & 2007AA01Z441. 
References 
Alina Andreevskaia and Sabine Bergler. 2008. When 
Specialists and Generalists Work Together: Over-
coming Domain Dependence in Sentiment Tagging. 
In Proceedings of ACL: 290-298. 
Anthony Aue and Michael Gamon. 2005. Customiz-
ing sentiment classifiers to new domains: a case 
study. In Proceedings of RANLP. 
Sergey Brin, Lawrence Page, Rajeev Motwami, and 
Terry Winograd. 1999. The PageRank citation 
ranking: bringing order to the web. Technical Re-
port 1999-0120, Stanford, CA. 
Chinchung Chang and Chinjen Lin. 2001. LIBSVM: a 
library for support vector machines. 
http://www.csie.ntu.edu.tw/~cjlin/libsvm. 
Gunes Erkan and Dragomir Radev. 2004. LexRank: 
Graph-based Centrality as Salience in Text Sum-
marization. Journal of Artificial Intelligence Re-
search, 22 (2004): 457-479. 
Michael Gamon and Anthony Aue. 2005. Automatic 
identification of sentiment vocabulary: exploiting 
low association with known sentiment terms. In 
Proceedings of the ACL Workshop on Feature En-
gineering for Machine Learning in NLP: 57-64. 
Songbo Tan, Xueqi Cheng, Moustafa Ghanem, Bin 
Wang, Hongbo Xu. 2005. A novel refinement ap-
proach for text categorization. In Proceedings of 
CIKM 2005: 469-476 
Thorsten Joachims. 1999. Transductive inference for 
text classification using support vector machines. 
In Proceedings of ICML. 
Jon Kleinberg. 1998. Authoritative sources in a hyper-
linked environment. Journal of the ACM, 46(5): 
604-632. 
Lunwei Ku, Yuting Liang, and Hsinhsi Chen. 2006. 
Opinion extraction, summarization and tracking in 
news and blog corpora. In Proceedings of AAAI. 
Thomas Landauer, Peter Foltz, and Darrell Laham. 
1998. Introduction to latent semantic analysis. Dis-
course Processes 25: 259-284. 
Ryan McDonald, Kerry Hannan, Tyler Neylon, Mike 
Wells, and Jeff Reynar. 2007. Structured models 
for fine-to-coarse sentiment analysis. In Proceed-
ings of ACL. 
Songbo Tan, Yuefen Wang, Gaowei Wu, and Xueqi 
Cheng. 2007. A novel scheme for domain-transfer 
problem in the context of sentiment analysis. In 
Proceedings of CIKM. 
Songbo Tan, Xueqi Cheng, Yuefen Wang, and 
Hongbo Xu. 2009. Adapting Na?ve Bayes to Do-
main Adaptation for Sentiment Analysis. In Pro-
ceedings of ECIR. 
Qiong Wu, Songbo Tan and Xueqi Cheng. 2009. 
Graph Ranking for Sentiment Transfer. In Proceed-
ings of ACL-IJCNLP. 
Peter Turney. 2001. Mining the web for synonyms: 
PMI-IR versus LSA on TOEFL. In Proceedings of 
ECML: 491-502. 
 
1335
