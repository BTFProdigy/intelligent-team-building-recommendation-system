Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 12?58,
Baltimore, Maryland USA, June 26?27, 2014.
c
?2014 Association for Computational Linguistics
Findings of the 2014 Workshop on Statistical Machine Translation
Ond
?
rej Bojar
Charles University in Prague
Christian Buck
University of Edinburgh
Christian Federmann
Microsoft Research
Barry Haddow
University of Edinburgh
Philipp Koehn
JHU / Edinburgh
Johannes Leveling
Dublin City University
Christof Monz
University of Amsterdam
Pavel Pecina
Charles University in Prague
Matt Post
Johns Hopkins University
Herve Saint-Amand
University of Edinburgh
Radu Soricut
Google
Lucia Specia
University of Sheffield
Ale
?
s Tamchyna
Charles University in Prague
Abstract
This paper presents the results of the
WMT14 shared tasks, which included a
standard news translation task, a sepa-
rate medical translation task, a task for
run-time estimation of machine translation
quality, and a metrics task. This year, 143
machine translation systems from 23 insti-
tutions were submitted to the ten transla-
tion directions in the standard translation
task. An additional 6 anonymized sys-
tems were included, and were then evalu-
ated both automatically and manually. The
quality estimation task had four subtasks,
with a total of 10 teams, submitting 57 en-
tries.
1 Introduction
We present the results of the shared tasks of
the Workshop on Statistical Machine Translation
(WMT) held at ACL 2014. This workshop builds
on eight previous WMT workshops (Koehn and
Monz, 2006; Callison-Burch et al., 2007, 2008,
2009, 2010, 2011, 2012; Bojar et al., 2013).
This year we conducted four official tasks: a
translation task, a quality estimation task, a met-
rics task
1
and a medical translation task. In the
translation task (?2), participants were asked to
translate a shared test set, optionally restricting
themselves to the provided training data. We held
ten translation tasks this year, between English and
each of Czech, French, German, Hindi, and Rus-
sian. The Hindi translation tasks were new this
year, providing a lesser resourced data condition
on a challenging language pair. The system out-
puts for each task were evaluated both automati-
cally and manually.
1
The metrics task is reported in a separate paper
(Mach?a?cek and Bojar, 2014).
The human evaluation (?3) involves asking
human judges to rank sentences output by
anonymized systems. We obtained large num-
bers of rankings from researchers who contributed
evaluations proportional to the number of tasks
they entered. Last year, we dramatically increased
the number of judgments, achieving much more
meaningful rankings. This year, we developed a
new ranking method that allows us to achieve the
same with fewer judgments.
The quality estimation task (?4) this year
included sentence- and word-level subtasks:
sentence-level prediction of 1-3 likert scores,
sentence-level prediction of percentage of word
edits necessary to fix a sentence, sentence-level
prediction of post-editing time, and word-level
prediction of scores at different levels of granular-
ity (correct/incorrect, accuracy/fluency errors, and
specific types of errors). Datasets were released
with English-Spanish, English-German, Spanish-
English and German-English news translations
produced by 2-3 machine translation systems and,
for some subtasks, a human translation.
The medical translation task (?5) was intro-
duced this year. Unlike the ?standard? translation
task, the test sets come from the very specialized
domain of medical texts. The aim of this task was
not only domain adaptation but also the utilization
of translation systems in a larger scenario, namely
cross-lingual information retrieval (IR). Extrinsic
evaluation in an IR setting was a part of this task
(on the other hand, manual evaluation of transla-
tion quality was not carried out).
The primary objectives of WMT are to evaluate
the state of the art in machine translation, to dis-
seminate common test sets and public training data
with published performance numbers, and to re-
fine evaluation and estimation methodologies for
machine translation. As before, all of the data,
12
translations, and collected human judgments are
publicly available.
2
We hope these datasets serve
as a valuable resource for research into statistical
machine translation and automatic evaluation or
prediction of translation quality.
2 Overview of the Translation Task
The recurring task of the workshop examines
translation between English and other languages.
As in the previous years, the other languages in-
clude German, French, Czech and Russian.
We dropped Spanish and added Hindi this year.
From a linguistic point of view, Spanish poses
similar problems as French, making its prior in-
clusion less valuable. Hindi is not only interest-
ing since it is a more distant language than the
European languages we include, but also because
we have much less training data, thus forcing re-
searchers to deal with low resource conditions, but
also providing them with a language pair that does
not suffer from the computational complexities of
having to deal with massive amounts of training
data.
We created a test set for each language pair by
translating newspaper articles and provided train-
ing data.
2.1 Test data
The test data for this year?s task was selected from
news stories from online sources, as before. How-
ever, we changed our method to create the test sets.
In previous years, we took equal amounts of
source sentences from all six languages involved
(around 500 sentences each), and translated them
into all other languages. While this produced a
multi-parallel test corpus that could be also used
for language pairs (such as Czech-Russian) that
we did not include in the evaluation, it did suf-
fer from artifacts from the larger distance between
source and target sentences. Most test sentences
involved the translation a source sentence that
was translated from a their language into a tar-
get sentence (which was compared against a trans-
lation from that third language as well). Ques-
tions have been raised, if the evaluation of, say,
French-English translation is best served when
testing on sentences that have been originally writ-
ten in, say, Czech. For discussions about trans-
lationese please for instance refer to Koppel and
Ordan (2011).
2
http://statmt.org/wmt14/results.html
This year, we took about 1500 English sen-
tences and translated them into the other 5 lan-
guages, and then additional 1500 sentences from
each of the other languages and translated them
into English. This gave us test sets of about 3000
sentences for our English-X language pairs, which
have been either written originally written in En-
glish and translated into X, or vice versa.
The composition of the test documents is shown
in Table 1. The stories were translated by the pro-
fessional translation agency Capita, funded by the
EU Framework Programme 7 project MosesCore,
and by Yandex, a Russian search engine com-
pany.
3
All of the translations were done directly,
and not via an intermediate language.
2.2 Training data
As in past years we provided parallel corpora
to train translation models, monolingual cor-
pora to train language models, and development
sets to tune system parameters. Some train-
ing corpora were identical from last year (Eu-
roparl
4
, United Nations, French-English 10
9
cor-
pus, CzEng, Common Crawl, Russian-English
Wikipedia Headlines provided by CMU), some
were updated (Russian-English parallel data pro-
vided by Yandex, News Commentary, monolin-
gual data), and a new corpus was added (Hindi-
English corpus, Bojar et al. (2014)), Hindi-English
Wikipedia Headline corpus).
Some statistics about the training materials are
given in Figure 1.
2.3 Submitted systems
We received 143 submissions from 23 institu-
tions. The participating institutions and their entry
names are listed in Table 2; each system did not
necessarily appear in all translation tasks. We also
included four commercial off-the-shelf MT sys-
tems and four online statistical MT systems, which
we anonymized.
For presentation of the results, systems are
treated as either constrained or unconstrained, de-
pending on whether their models were trained only
on the provided data. Since we do not know how
they were built, these online and commercial sys-
tems are treated as unconstrained during the auto-
matic and human evaluations.
3
http://www.yandex.com/
4
As of Fall 2011, the proceedings of the European Parlia-
ment are no longer translated into all official languages.
13
Europarl Parallel Corpus
French? English German? English Czech? English
Sentences 2,007,723 1,920,209 646,605
Words 60,125,563 55,642,101 50,486,398 53,008,851 14,946,399 17,376,433
Distinct words 140,915 118,404 381,583 115,966 172,461 63,039
News Commentary Parallel Corpus
French? English German? English Czech? English Russian? English
Sentences 183,251 201,288 146,549 165,602
Words 5,688,656 4,659,619 5,105,101 5,046,157 3,288,645 3,590,287 4,153,847 4,339,974
Distinct words 72,863 62,673 150,760 65,520 139,477 55,547 151,101 60,801
Common Crawl Parallel Corpus
French? English German? English Czech? English Russian? English
Sentences 3,244,152 2,399,123 161,838 878,386
Words 91,328,790 81,096,306 54,575,405 58,870,638 3,529,783 3,927,378 21,018,793 21,535,122
Distinct words 889,291 859,017 1,640,835 823,480 210,170 128,212 764,203 432,062
United Nations Parallel Corpus
French? English
Sentences 12,886,831
Words 411,916,781 360,341,450
Distinct words 565,553 666,077
10
9
Word Parallel Corpus
French? English
Sentences 22,520,400
Words 811,203,407 668,412,817
Distinct words 2,738,882 2,861,836
CzEng Parallel Corpus
Czech? English
Sentences 14,833,358
Words 200,658,857 228,040,794
Distinct words 1,389,803 920,824
Hindi-English Parallel Corpus
Hindi? English
Sentences 287,202
Words 6,002,418 3,953,851
Distinct words 121,236 105,330
Yandex 1M Parallel Corpus
Russian? English
Sentences 1,000,000
Words 24,121,459 26,107,293
Distinct words 701,809 387,646
Wiki Headlines Parallel Corpus
Russian? English Hindi? English
Sentences 514,859 32,863
Words 1,191,474 1,230,644 141,042 70,075
Distinct words 282,989 251,328 25,678 26,989
Europarl Language Model Data
English French German Czech
Sentence 2,218,201 2,190,579 2,176,537 668,595
Words 59,848,044 63,439,791 53,534,167 14,946,399
Distinct words 123,059 145,496 394,781 172,461
News Language Model Data
English French German Czech Russian Hindi
Sentence 90,209,983 30,451,749 89,634,193 36,426,900 32,245,651 1,275,921
Words 2,109,603,244 748,852,739 1,606,506,785 602,950,410 575,423,682 36,297,394
Distinct words 4,089,792 1,906,470 10,248,707 3,101,846 2,860,837 258,759
News Test Set
French? English German? English Czech? English Russian? English Hindi? English
Sentences 3003 3003 3003 3003 2507
Words 81,194 71,147 63,078 67,624 60,240 68,866 62,107 69,329 86,974 55,822
Distinct words 11,715 10,610 13,930 10,458 16,774 9,893 17,009 9,938 8,292 9,217
Figure 1: Statistics for the training and test sets used in the translation task. The number of words and the number of distinct
words (case-insensitive) is based on the provided tokenizer.
14
Language Sources (Number of Documents)
Czech aktu?aln?e.cz (2), blesk.cz (3), blisty.cz (1), den??k.cz (9), e15.cz (1), iDNES.cz (17), ihned.cz (14), lidovky.cz (8), medi-
afax.cz (2), metro.cz (1), Novinky.cz (5), pravo.novinky.cz (6), reflex.cz (2), tyden.cz (1), zdn.cz (1).
French BBC French Africa (1), Canoe (9), Croix (4), Cyber Presse (12), Dernieres Nouvelles (1), dhnet.be (5), Equipe (1),
Euronews (6), Journal Metro.com (1), La Libre.be (2), La Meuse.be (2), Le Devoir (3), Le Figaro (8), Le Monde (3),
Les Echos (15), Lexpress.fr (3), Liberation (1), L?independant (2), Metro France (1), Nice-Matin (6), Le Nouvel Ob-
servateur (3), Radio Canada (6), Reuters (7).
English ABC News (5), BBC (5), CBS News (5), CNN (5), Daily Mail (5), Financial Times (5), Fox News (2), Globe and
Mail (1), Independent (1), Los Angeles Times (1), New Yorker (1), News.com Australia (16), Reuters (3), Scotsman (2),
smh.com.au (2), stv.tv (1), Telegraph (6), UPI (2).
German Abendzeitung N?urnberg (1), all-in.de (2), Augsburger Allgemeine (1), AZ Online (1), B?orsenzeitung (1), come-
on.de (1), Der Westen (2), DZ Online (1), Reutlinger General-Anzeiger (1), Generalanzeiger Bonn (1), Giessener
Anzeiger (1), Goslarsche Zeitung (1), Hersfelder Zeitung (1), J?udische Allgemeine (1), Kreisanzeiger (2),
Kreiszeitung (2), Krone (1), Lampertheimer Zeitung (2), Lausitzer Rundschau (1), Mittelbayerische (1), Morgen-
post (1), nachrichten.at (1), Neue Presse (1), OP Online (1), Potsdamer Neueste Nachrichten (1), Passauer Neue
Presse (1), Recklingh?auser Zeitung (1), Rhein Zeitung (1), salzburg.com (1), Schwarzw?alder Bote (29), Segeberger
Zeitung (1), Soester Anzeiger (1), S?udkurier (17), svz.de (1), Tagesspiegel (1), Usinger Anzeiger (3), Volksblatt.li (1),
Westf?alischen Anzeiger (3), Wiener Zeitung (1), Wiesbadener Kurier (1), Westdeutsche Zeitung (1), Wilhelmshavener
Zeitung (1), Yahoo Deutschland (1).
Hindi Bhaskar (24), Jagran (61), Navbharat Times / India Times (4), ndtv (2).
Russian 168.ru (1), aif (3), altapress.ru (2), argumenti.ru (2), BBC Russian (3), belta.by (2), communa.ru (1), dp.ru (1), eg-
online.ru (1), Euronews (2), fakty.ua (2), gazeta.ru (1), inotv.rt.com (1), interfax (1), Izvestiya (1), Kommersant (7),
kp (2), lenta.ru (4), lgng (1), litrossia.ru (1), mirnov.ru (5), mk (8), mn.ru (2), newizv (2), nov-pravda.ru (1), no-
vayagazeta (1), nr2.ru (8), pnp.ru (1), rbc.ru (3), ria.ru (4), rosbalt.ru (1), sovsport.ru (6), Sport Express (10), trud.ru (4),
tumentoday.ru (1), vesti.ru (10), zr.ru (1).
Table 1: Composition of the test set. For more details see the XML test files. The docid tag gives the source and the date for
each document in the test set, and the origlang tag indicates the original source language.
3 Human Evaluation
As with past workshops, we contend that auto-
matic measures of machine translation quality are
an imperfect substitute for human assessments.
We therefore conduct a manual evaluation of the
system outputs and define its results to be the prin-
cipal ranking of the workshop. In this section, we
describe how we collected this data and compute
the results, and then present the official results of
the ranking.
This year?s evaluation was conducted a bit dif-
ferently. The main differences are:
? In contrast to the past two years, we collected
judgments entirely from researchers partici-
pating in the shared tasks and trusted friends
of the community. Last year, about two thirds
of the data were solicited from random volun-
teers on the Amazon Mechanical Turk. For
some language pairs, the Turkers data had
much lower inter-annotator agreement com-
pared to the researchers.
? As a result, we collected about seventy-five
percent less data, but were able to obtain
good confidence intervals on the clusters with
the use of new approaches to ranking.
? We compared three different ranking method-
ologies, selecting the one with the highest ac-
curacy on held-out data.
We also maintain many of our customs from
prior years, including the presentation of the re-
sults in terms of a partial ordering (clustering) of
the systems. Systems in the same cluster could not
be meaningfully distinguished and should be con-
sidered ties.
3.1 Data collection
The system ranking is produced from a large set of
pairwise annotations between system pairs. These
pairwise annotations are collected in an evaluation
campaign that enlists participants in the shared
task to contribute one hundred ?Human Intelli-
gence Tasks? (HITs) per system submitted. Each
HIT consists of three ranking tasks. In a rank-
ing task, an annotator is presented with a source
segment, a human reference translation, and the
outputs of five anonymized systems, randomly se-
lected from the set of participating systems, and
randomly ordered.
To run the evaluation, we use Appraise
5
(Fe-
dermann, 2012), an open-source tool built on
Python?s Django framework. At the top of each
HIT, the following instructions are provided:
You are shown a source sentence fol-
lowed by several candidate translations.
Your task is to rank the translations from
best to worst (ties are allowed).
5
https://github.com/cfedermann/Appraise
15
ID Institution
AFRL, AFRL-PE Air Force Research Lab (Schwartz et al., 2014)
CIMS University of Stuttgart / University of Munich (Cap et al., 2014)
CMU Carnegie Mellon University (Matthews et al., 2014)
CU-* Charles University, Prague (Tamchyna et al., 2014)
DCU-FDA Dublin City University (Bicici et al., 2014)
DCU-ICTCAS Dublin City University (Li et al., 2014b)
DCU-LINGO24 Dublin City University / Lingo24 (wu et al., 2014)
EU-BRIDGE EU-BRIDGE Project (Freitag et al., 2014)
KIT Karlsruhe Institute of Technology (Herrmann et al., 2014)
IIT-BOMBAY IIT Bombay (Dungarwal et al., 2014)
IIIT-HYDERABAD IIIT Hyderabad
IMS-TTT University of Stuttgart / University of Munich (Quernheim and Cap, 2014)
IPN-UPV-* IPN-UPV (Costa-juss`a et al., 2014)
KAZNU Amandyk Kartbayev, FBK
LIMSI-KIT LIMSI / Karlsruhe Instutute of Technology (Do et al., 2014)
MANAWI-* Universit?at des Saarlandes (Tan and Pal, 2014)
MATRAN Abu-MaTran Project: Promsit / DCU / UA (Rubino et al., 2014)
PROMT-RULE,
PROMT-HYBRID
PROMT
RWTH RWTH Aachen (Peitz et al., 2014)
STANFORD Stanford University (Neidert et al., 2014; Green et al., 2014)
UA-* University of Alicante (S?anchez-Cartagena et al., 2014)
UEDIN-PHRASE,
UEDIN-UNCNSTR
University of Edinburgh (Durrani et al., 2014b)
UEDIN-SYNTAX University of Edinburgh (Williams et al., 2014)
UU, UU-DOCENT Uppsala University (Hardmeier et al., 2014)
YANDEX Yandex School of Data Analysis (Borisov and Galinskaya, 2014)
COMMERCIAL-[1,2] Two commercial machine translation systems
ONLINE-[A,B,C,G] Four online statistical machine translation systems
RBMT-[1,4] Two rule-based statistical machine translation systems
Table 2: Participants in the shared translation task. Not all teams participated in all language pairs. The translations from the
commercial and online systems were not submitted by their respective companies but were obtained by us, and are therefore
anonymized in a fashion consistent with previous years of the workshop.
16
Figure 2: Screenshot of the Appraise interface used in the human evaluation campaign. The annotator is presented with a
source segment, a reference translation, and the outputs of five systems (anonymized and randomly ordered), and is asked to
rank these according to their translation quality, with ties allowed.
A screenshot of the ranking interface is shown in
Figure 2. Annotators are asked to rank the sys-
tems from 1 (best) to 5 (worst), with ties permit-
ted. Note that a lower rank is better. The rankings
provided by a ranking task are then reduced to a
set of ten pairwise rankings produced by consider-
ing all
(
5
2
)
combinations of systems in the ranking
task. For example, consider the following annota-
tion provided among systems A,B, F,H , and J :
1 2 3 4 5
F ?
A ?
B ?
J ?
H ?
This is reduced to the following set of pairwise
judgments:
A > B,A = F,A > H,A < J
B < F,B < H,B < J
F > H,F < J
H < J
Here,A > B should be read is ?A is ranked higher
than (worse than) B?. Note that by this procedure,
the absolute value of ranks and the magnitude of
their differences are discarded.
For WMT13, nearly a million pairwise anno-
tations were collected from both researchers and
paid workers on Amazon?s Mechanical Turk, in
a roughly 1:2 ratio. This year, we collected data
from researchers only, an ability that was enabled
by the use of a new technique for producing the
partial ranking for each task (?3.3.3). Table 3 con-
tains more detail.
3.2 Annotator agreement
Each year we calculate annotator agreement
scores for the human evaluation as a measure of
the reliability of the rankings. We measured pair-
wise agreement among annotators using Cohen?s
kappa coefficient (?) (Cohen, 1960). If P (A) be
the proportion of times that the annotators agree,
and P (E) is the proportion of time that they would
17
LANGUAGE PAIR Systems Rankings Average
Czech?English 5 21,130 4,226.0
English?Czech 10 55,900 5,590.0
German?English 13 25,260 1,943.0
English?German 18 54,660 3,036.6
French?English 8 26,090 3,261.2
English?French 13 33,350 2,565.3
Russian?English 13 34,460 2,650.7
English?Russian 9 28,960 3,217.7
Hindi?English 9 20,900 2,322.2
English?Hindi 12 28,120 2,343.3
TOTAL WMT 14 110 328,830 2,989.3
WMT13 148 942,840 6,370.5
WMT12 103 101,969 999.6
WMT11 133 63,045 474.0
Table 3: Amount of data collected in the WMT14 manual evaluation. The final three rows report summary information from
the previous two workshops.
agree by chance, then Cohen?s kappa is:
? =
P (A)? P (E)
1? P (E)
Note that ? is basically a normalized version of
P (A), one which takes into account how mean-
ingful it is for annotators to agree with each other
by incorporating P (E). The values for ? range
from 0 to 1, with zero indicating no agreement and
1 perfect agreement.
We calculate P (A) by examining all pairs of
systems which had been judged by two or more
judges, and calculating the proportion of time that
they agreed that A < B, A = B, or A > B. In
other words, P (A) is the empirical, observed rate
at which annotators agree, in the context of pair-
wise comparisons.
As for P (E), it captures the probability that two
annotators would agree randomly. Therefore:
P (E) = P (A<B)
2
+ P (A=B)
2
+ P (A>B)
2
Note that each of the three probabilities in P (E)?s
definition are squared to reflect the fact that we are
considering the chance that two annotators would
agree by chance. Each of these probabilities is
computed empirically, by observing how often an-
notators actually rank two systems as being tied.
Table 4 gives ? values for inter-annotator agree-
ment for WMT11?WMT14 while Table 5 de-
tails intra-annotator agreement scores, including
the division of researchers (WMT13
r
) and MTurk
(WMT13
m
) data. The exact interpretation of the
kappa coefficient is difficult, but according to Lan-
dis and Koch (1977), 0?0.2 is slight, 0.2?0.4 is
fair, 0.4?0.6 is moderate, 0.6?0.8 is substantial,
and 0.8?1.0 is almost perfect. The agreement rates
are more or less in line with prior years: worse for
some tasks, better for others, and on average, the
best since WMT11 (where agreement scores were
likely inflated due to inclusion of reference trans-
lations in the comparisons).
3.3 Models of System Rankings
The collected pairwise rankings are used to pro-
duce a ranking of the systems. Machine transla-
tion evaluation has always been a subject of con-
tention, and no exception to this rule exists for the
WMT manual evaluation. While the precise met-
ric has varied over the years, it has always shared
a common idea of computing the average num-
ber of times each system was judged better than
other systems, and ranking from highest to low-
est. For example, in WMT11 Callison-Burch et al.
(2011), the metric computed the percentage of the
time each system was ranked better than or equal
to other systems, and included comparisons to hu-
man references. In WMT12 Callison-Burch et al.
(2012), comparisons to references were dropped.
In WMT13, rankings were produced over 1,000
bootstrap-resampled sets of the training data. A
rank range was collected for each system across
these folds; the average value was used to order
the systems, and a 95% confidence interval across
these ranks was used to organize the systems into
equivalence classes containing systems with over-
18
LANGUAGE PAIR WMT11 WMT12 WMT13 WMT13
r
WMT13
m
WMT14
Czech?English 0.400 0.311 0.244 0.342 0.279 0.305
English?Czech 0.460 0.359 0.168 0.408 0.075 0.360
German?English 0.324 0.385 0.299 0.443 0.324 0.368
English?German 0.378 0.356 0.267 0.457 0.239 0.427
French?English 0.402 0.272 0.275 0.405 0.321 0.357
English?French 0.406 0.296 0.231 0.434 0.237 0.302
Hindi?English ? ? ? ? ? 0.400
English?Hindi ? ? ? ? ? 0.413
Russian?English ? ? 0.278 0.315 0.324 0.324
English?Russian ? ? 0.243 0.416 0.207 0.418
MEAN 0.395 0.330 0.260 0.367
Table 4: ? scores measuring inter-annotator agreement. See Table 5 for corresponding intra-annotator agreement scores.
LANGUAGE PAIR WMT11 WMT12 WMT13 WMT13
r
WMT13
m
WMT14
Czech?English 0.597 0.454 0.479 0.483 0.478 0.382
English?Czech 0.601 0.390 0.290 0.547 0.242 0.448
German?English 0.576 0.392 0.535 0.643 0.515 0.344
English?German 0.528 0.433 0.498 0.649 0.452 0.576
French?English 0.673 0.360 0.578 0.585 0.565 0.629
English?French 0.524 0.414 0.495 0.630 0.486 0.507
Hindi?English ? ? ? ? ? 0.605
English?Hindi ? ? ? ? ? 0.535
Russian?English ? ? 0.450 0.363 0.477 0.629
English?Russian ? ? 0.513 0.582 0.500 0.570
MEAN 0.583 0.407 0.479 0.522
Table 5: ? scores measuring intra-annotator agreement, i.e., self-consistency of judges, across for the past few years of the
human evaluation.
lapping ranges.
This year, we introduce two new changes. First,
we pit the WMT13 method against two new ap-
proaches: that of Hopkins and May (2013, ?3.3.2),
and another based on TrueSkill (Sakaguchi et al.,
2014, ?3.3.3). Second, we compare these two
methods against WMT13?s ?Expected Wins? ap-
proach, and then select among them by determin-
ing which of them has the highest accuracy in
terms of predicting annotations on a held-out set
of pairwise judgments.
3.3.1 Method 1: Expected Wins (EW)
Introduced for WMT13, the EXPECTED WINS has
an intuitive score demonstrated to be accurate in
ranking systems according to an underlying model
of ?relative ability? (Koehn, 2012a). The idea is
to gauge the probability that a system S
i
will be
ranked better than another system randomly cho-
sen from a pool of opponents {S
j
: j 6= i}. If
we define the function win(A,B) as the number
of times system A is ranked better than system B,
then we can define this as follows:
score
EW
(S
i
) =
1
|{S
j
}|
?
j,j 6=i
win(S
i
, S
j
)
win(S
i
, S
j
) + win(S
j
, S
i
)
Note that this score ignores ties.
3.3.2 Method 2: Hopkins and May (HM)
Hopkins and May (2013) introduced a graphical
model formulation of the task, which makes the
notion of underlying system ability even more ex-
plicit. Each system S
J
in the pool {S
j
} is repre-
sented by an associated relative ability ?
j
and a
variance ?
2
a
(fixed across all systems) which serve
as the parameters of a Gaussian distribution. Sam-
ples from this distribution represent the quality
of sentence translations, with higher quality sam-
ples having higher values. Pairwise annotations
(S
1
, S
2
, pi) are generated according to the follow-
ing process:
19
1. Select two systems S
1
and S
2
from the pool
of systems {S
j
}
2. Draw two ?translations?, adding random
Gaussian noise with variance ?
2
obs
to simulate
the subjectivity of the task and the differences
among annotators:
q
1
? N (?
S
1
, ?
2
a
) +N (0, ?
2
obs
)
q
2
? N (?
S
2
, ?
2
a
) +N (0, ?
2
obs
)
3. Let d be a nonzero real number that defines
a fixed decision radius. Produce a rating pi
according to:
pi =
?
?
?
< q
1
? q
2
> d
> q
2
? q
1
> d
= otherwise
Hopkins and May use Gibbs sampling to infer
the set of system means from an annotated dataset.
Details of this inference procedure can be found in
Sakaguchi et al. (2014). The score used to produce
the rankings is simply the system mean associated
with each system:
score
HM
(S
i
) = ?
S
i
3.3.3 Method 3: TrueSkill (TS)
TrueSkill is an adaptive, online system that em-
ploys a similar model of relative ability Herbrich
et al. (2006). It was initially developed for Xbox
Live?s online player community, where it is used
to model player ability, assign levels, and select
competitive matches. Each player S
j
is modeled
by two parameters: TrueSkill?s current estimate
of each system?s relative ability, ?
S
j
, and a per-
system measure of TrueSkill?s uncertainty of those
estimates, ?
2
S
j
. When the outcome of a match is
observed, TrueSkill uses the relative status of the
two systems to update these estimates. If a trans-
lation from a system with a high mean is judged
better than a system with a greatly lower mean, the
result is not surprising, and the update size for the
corresponding system means will be small. On the
other hand, when an upset occurs in a competition,
the means will receive larger updates. Sakaguchi
et al. (2014) provide an adaptation of this approach
to the WMT manual evaluation, and showed that
it performed well on WMT13 data.
Similar to the Hopkins and May model,
TrueSkill scores systems by their inferred means:
score
TS
(S
i
) = ?
S
i
This score is then used to sort the systems and pro-
duce the ranking.
3.4 Method Selection
We have three methods which, provided with the
collected data, produce different rankings of the
systems. Which of them is correct? More imme-
diately, which one of them should we publish as
the official ranking for the WMT14 manual eval-
uation? As discussed, the method used to com-
pute the ranking has been tweaked a bit each year
over the past few years in response to criticisms
(e.g., Lopez (2012); Bojar et al. (2011)). While the
changes were reasonable (and later corroborated),
Hopkins and May (2013) pointed out that this task
of model selection should be driven by empirical
evaluation on held-out data, and suggested per-
plexity as the metric of choice.
We choose instead a more direct gold-standard
evaluation metric: the accuracy of the rankings
produced by each method in predicting pairwise
judgments. We use each method to produce a par-
tial ordering of the systems, grouping them into
equivalence classes. This partial ordering unam-
biguously assigns a prediction pi
P
between any
pair of systems (S
i
, S
j
). By comparing the pre-
dicted relationship pi
P
to the actual annotation for
each pairwise judgment in the test data (by token),
we can compute an accuracy score for each model.
We predict accuracy in this manner using 100-
fold cross-validation. For each task, we split the
data into a fixed set of 100 randomly-selected
folds. Each fold serves as a test set, with the
remaining ninety-nine folds available as training
data for each method. Note that the total order-
ing over systems provided by the score
?
functions
defined do not predict ties. In order to do enable
the models to predict ties, we produce equivalence
classes using the following procedure:
? Assign S
1
to a cluster
? For each system S
i
, assign it to the current
cluster if score(S
i?1
) ? score(S
i
) ? r; oth-
erwise, assign it to a new cluster
The value of r (the decision radius for ties)
is tuned using accuracy on the entire training
data using grid search over the values r ?
0, 0.01, 0.02, . . . , .25 (26 values in total). This
value is tuned separately for each method on each
fold. Table 6 contains an example partial ordering.
20
System Score Rank
B 0.60 1
D 0.44 2
E 0.39 2
A 0.25 2
F -0.09 3
C -0.22 3
Table 6: The partial ordering computed with the provided
scores when r = 0.15.
Task EW HM TS Oracle
Czech?English 40.4 41.1 41.1 41.2
English?Czech 45.3 45.6 45.9 46.8
French?English 49.0 49.4 49.3 50.3
English?French 44.6 44.4 44.7 46.0
German?English 43.5 43.7 43.7 45.2
English?German 47.3 47.4 47.2 48.2
Hindi?English 62.5 62.2 62.5 62.6
English?Hindi 53.3 53.7 53.5 55.7
Russian?English 47.6 47.7 47.7 50.6
English?Russian 46.5 46.1 46.4 48.2
MEAN 48.0 48.1 48.2 49.2
Table 7: Accuracies for each method across 100 folds, for
each translation task. The oracle uses the most frequent out-
come between each pair of systems, and therefore might not
constitute a feasible ranking.
After training, each model has defined a partial
ordering over systems.
6
This is then used to com-
pute accuracy on all the pairwise judgments in the
test fold. This process yields 100 accuracies for
each method; the average accuracy across all the
folds can then be used to compute the best method.
Table 7 contains accuracy results for the three
methods on the WMT14 tasks. On average, there
is a small improvement in accuracy moving from
Expected Wins to the H&M model, and then again
to the TrueSkill model; however, there is no pat-
tern to the best model for each class. The Oracle
column is computed by selecting the most prob-
able outcome (pi ? {<,=, >}) for each system
pair, and provides an upper bound on accuracy
when predicting outcomes using only system-level
information. Furthermore, this method of oracle
computation might not represent a feasible rank-
ing or clustering,
7
.
The TrueSkill approach was best overall, so we
used it to produce the official rankings for all lan-
6
It is a total ordering when r = 0, or when all the system
scores are outside the decision radius.
7
For example, if there were a cycle of ?better than? judg-
ments among a set of systems.
guage pairs.
3.5 Rank Ranges and Clusters
Above we saw how to produce system scores for
each method, which provides a total ordering of
the systems. But we would also like to know if the
obtained system ranking is statistically significant.
Given the large number of systems that participate,
and the similarity of the underlying systems result-
ing from the common training data condition and
(often) toolsets, there will be some systems that
will be very close in quality. These systems should
be grouped together in equivalence classes.
To establish the reliability of the obtained sys-
tem ranking, we use bootstrap resampling. We
sample from the set of pairwise rankings an equal
sized set of pairwise rankings (allowing for multi-
ple drawings of the same pairwise ranking), com-
pute a TrueSkill model score for each system
based on this sample, and then rank the systems
from 1..|{S
j
}|. By repeating this procedure 1,000
times, we can determine a range of ranks, into
which system falls at least 95% of the time (i.e.,
at least 950 times) ? corresponding to a p-level
of p ? 0.05. Furthermore, given the rank ranges
for each system, we can cluster systems with over-
lapping rank ranges.
8
Table 8 reports all system scores, rank ranges,
and clusters for all language pairs and all systems.
The official interpretation of these results is that
systems in the same cluster are considered tied.
Given the large number of judgments that we col-
lected, it was possible to group on average about
two systems in a cluster, even though the systems
in the middle are typically in larger clusters.
3.6 Cluster analysis
The official ranking results for English-German
produced clusters compute at the 90% confidence
level due to the presence of a very large cluster
(of nine systems). While there is always the pos-
sibility that this cluster reflects a true ambiguity, it
is more likely due to the fact that we didn?t have
enough data: English?German had the most sys-
8
Formally, given ranges defined by start(S
i
) and end(S
i
),
we seek the largest set of clusters {C
c
} that satisfies:
?S ?C : S ? C
S ? C
a
, S ? C
b
? C
a
= C
b
C
a
6= C
b
? ?S
i
? C
a
, S
j
? C
b
:
start(S
i
) > end(S
j
) or start(S
j
) > end(S
i
)
21
Czech?English
# score range system
1 0.591 1 ONLINE-B
2 0.290 2 UEDIN-PHRASE
3 -0.171 3-4 UEDIN-SYNTAX
-0.243 3-4 ONLINE-A
4 -0.468 5 CU-MOSES
English?Czech
# score range system
1 0.371 1-3 CU-DEPFIX
0.356 1-3 UEDIN-UNCNSTR
0.333 1-4 CU-BOJAR
0.287 3-4 CU-FUNKY
2 0.169 5-6 ONLINE-B
0.113 5-6 UEDIN-PHRASE
3 0.030 7 ONLINE-A
4 -0.175 8 CU-TECTO
5 -0.534 9 COMMERCIAL1
6 -0.950 10 COMMERCIAL2
Russian?English
# score range system
1 0.583 1 AFRL-PE
2 0.299 2 ONLINE-B
3 0.190 3-5 ONLINE-A
0.178 3-5 PROMT-HYBRID
0.123 4-7 PROMT-RULE
0.104 5-8 UEDIN-PHRASE
0.069 5-8 YANDEX
0.066 5-8 ONLINE-G
4 -0.017 9 AFRL
5 -0.159 10 UEDIN-SYNTAX
6 -0.306 11 KAZNU
7 -0.487 12 RBMT1
8 -0.642 13 RBMT4
English?Russian
# score range system
1 0.575 1-2 PROMT-RULE
0.547 1-2 ONLINE-B
2 0.426 3 PROMT-HYBRID
3 0.305 4-5 UEDIN-UNCNSTR
0.231 4-5 ONLINE-G
4 0.089 6-7 ONLINE-A
0.031 6-7 UEDIN-PHRASE
5 -0.920 8 RBMT4
6 -1.284 9 RBMT1
German?English
# score range system
1 0.451 1 ONLINE-B
2 0.267 2-3 UEDIN-SYNTAX
0.258 2-3 ONLINE-A
3 0.147 4-6 LIMSI-KIT
0.146 4-6 UEDIN-PHRASE
0.138 4-6 EU-BRIDGE
4 0.026 7-8 KIT
-0.049 7-8 RWTH
5 -0.125 9-11 DCU-ICTCAS
-0.157 9-11 CMU
-0.192 9-11 RBMT4
6 -0.306 12 RBMT1
7 -0.604 13 ONLINE-C
French?English
# score range system
1 0.608 1 UEDIN-PHRASE
2 0.479 2-4 KIT
0.475 2-4 ONLINE-B
0.428 2-4 STANFORD
3 0.331 5 ONLINE-A
4 -0.389 6 RBMT1
5 -0.648 7 RBMT4
6 -1.284 8 ONLINE-C
English?French
# score range system
1 0.327 1 ONLINE-B
2 0.232 2-4 UEDIN-PHRASE
0.194 2-5 KIT
0.185 2-5 MATRAN
0.142 4-6 MATRAN-RULES
0.120 4-6 ONLINE-A
3 0.003 7-9 UU-DOCENT
-0.019 7-10 PROMT-HYBRID
-0.033 7-10 UA
-0.069 8-10 PROMT-RULE
4 -0.215 11 RBMT1
5 -0.328 12 RBMT4
6 -0.540 13 ONLINE-C
English?German
# score range system
1 0.264 1-2 UEDIN-SYNTAX
0.242 1-2 ONLINE-B
2 0.167 3-6 ONLINE-A
0.156 3-6 PROMT-HYBRID
0.155 3-6 PROMT-RULE
0.155 3-6 UEDIN-STANFORD
3 0.094 7 EU-BRIDGE
4 0.033 8-10 RBMT4
0.031 8-10 UEDIN-PHRASE
0.012 8-10 RBMT1
5 -0.032 11-12 KIT
-0.069 11-13 STANFORD-UNC
-0.100 12-14 CIMS
-0.126 13-15 STANFORD
-0.158 14-16 UU
-0.191 15-16 ONLINE-C
6 -0.307 17-18 IMS-TTT
-0.325 17-18 UU-DOCENT
Hindi?English
# score range system
1 1.326 1 ONLINE-B
2 0.559 2-3 ONLINE-A
0.476 2-4 UEDIN-SYNTAX
0.434 3-4 CMU
3 0.323 5 UEDIN-PHRASE
4 -0.198 6-7 AFRL
-0.280 6-7 IIT-BOMBAY
5 -0.549 8 DCU-LINGO24
6 -2.092 9 IIIT-HYDERABAD
English?Hindi
# score range system
1 1.008 1 ONLINE-B
2 0.915 2 ONLINE-A
3 0.214 3 UEDIN-UNCNSTR
4 0.120 4-5 UEDIN-PHRASE
0.054 4-5 CU-MOSES
5 -0.111 6-7 IIT-BOMBAY
-0.142 6-7 IPN-UPV-CNTXT
6 -0.233 8-9 DCU-LINGO24
-0.261 8-9 IPN-UPV-NODEV
7 -0.449 10-11 MANAWI-H1
-0.494 10-11 MANAWI
8 -0.622 12 MANAWI-RMOOV
Table 8: Official results for the WMT14 translation task. Systems are ordered by their inferred system means. Lines between
systems indicate clusters according to bootstrap resampling at p-level p ? .05, except for English?German, where p ? 0.1.
This method is also used to determine the range of ranks into which system falls. Systems with grey background indicate use
of resources that fall outside the constraints provided for the shared task.
22
tems (18, compared to 13 for the next languages),
yet only an average amount of per-system data.
Here, we look at this language pair in more detail,
in order to justify this decision, and to shed light
on the differences between the ranking methods.
Table 9 presents the 95% confidence-level clus-
terings for English?German computed with each
of the three methods, along with lines that show
the reorderings of the systems between them. Re-
orderings of this type have been used to argue
against the reliability of the official WMT rank-
ing (Lopez, 2012; Hopkins and May, 2013). This
table shows that these reorderings are captured en-
tirely by the clustering approach we used. This rel-
ative consensus of these independently-computed
and somewhat different models suggests that the
published ranking is approaching the true ambigu-
ity underlying systems within the same cluster.
Looking across all language pairs, we find that
the total ordering predicted by EW and TS is ex-
actly the same for eight of the ten language pair
tasks, and is constrained to reorderings within
the official cluster for the other two (German?
English ? just one adjacent swap ? and English?
German, depicted in Table 9).
3.7 Conclusions
The official ranking method employed by WMT
over the past few years has changed a few times as
a result of error analysis and introspection. Until
this year, these results were largely based on the
intuitions of the community and organizers about
deficiencies in the models. In addition to their in-
tuitive appeal, many of these changes (such as the
decision to throw out comparisons against refer-
ences) have been empirically validated Hopkins
and May (2013). The actual effect of the refine-
ments in the ranking metric has been minor pertur-
bations in the permutation of systems. The cluster-
ing method of Koehn (2012b), in which the official
rankings are presented as a partial (instead of to-
tal) ordering, alleviated many of the problems ob-
served by Lopez (2012), and also capture all the
variance across the new systems introduced this
year. In addition, presenting systems as clusters
appeals to intuition. As such, we disagree with
claims that there is a problem with irreproducibil-
ity of the results of the workshop evaluation task,
and especially disagree that there is anything ap-
proaching a ?crisis of confidence? (Hopkins and
May, 2013). These claims seem to us to be over-
stated.
Conducting proper model selection by compar-
ison on held-out data, however, is a welcome sug-
gestion, and our inclusion of this process supports
improved confidence in the ranking results. That
said, it is notable that the different methods com-
pute very similar orderings. This avoids hallu-
cinating distinctions among systems that are not
really there, and captures the intuition that some
systems are basically equivalent. The chief ben-
efit of the TrueSkill model is not in outputting a
better complete ranking of the systems, but lies in
its reduced variance, which allow us to cluster the
systems with less data. There is also the unex-
plored avenue of using TrueSkill to drive the data
collection, steering the annotations of judges to-
wards evenly matched systems during the collec-
tion phase, potentially allowing confident results
to be presented while collecting even less data.
There is, of course, more work to be done.
We have produced this year statistically significant
clusters with a third of the data required last year,
which is an improvement. Models of relative abil-
ity are a natural fit for the manual evaluation, and
the introduction of an online Bayesian approach
to data collection present further opportunities to
reduce the amount of data needed. These methods
also provide a framework for extending the models
in a variety of potentially useful ways, including
modeling annotator bias, incorporating sentence
metadata (such as length, difficulty, or subtopic),
and adding features of the sentence pairs.
4 Quality Estimation Task
Machine translation quality estimation is the task
of predicting a quality score for a machine trans-
lated text without access to reference translations.
The most common approach is to treat the problem
as a supervised machine learning task, using stan-
dard regression or classification algorithms. The
third edition of the WMT shared task on qual-
ity estimation builds on the previous editions of
the task (Callison-Burch et al., 2012; Bojar et al.,
2013), with tasks including both sentence-level
and word-level estimation, with new training and
test datasets.
The goals of this year?s shared task were:
? To investigate the effectiveness of different
quality labels.
? To explore word-level quality prediction at
23
Expected Wins Hopkins & May TrueSkill
UEDIN-SYNTAX UEDIN-SYNTAX UEDIN-SYNTAX
ONLINE-B ONLINE-B ONLINE-B
ONLINE-A UEDIN-STANFORD ONLINE-A
UEDIN-STANFORD PROMT-HYBRID PROMT-HYBRID
PROMT-RULE ONLINE-A PROMT-RULE
PROMT-HYBRID PROMT-RULE UEDIN-STANFORD
EU-BRIDGE EU-BRIDGE EU-BRIDGE
RBMT4 UEDIN-PHRASE RBMT4
UEDIN-PHRASE RBMT4 UEDIN-PHRASE
RBMT1 RBMT1 RBMT1
KIT KIT KIT
STANFORD-UNC STANFORD-UNC STANFORD-UNC
CIMS CIMS CIMS
STANFORD STANFORD STANFORD
UU UU UU
ONLINE-C ONLINE-C ONLINE-C
IMS-TTT UU-DOCENT IMS-TTT
UU-DOCENT IMS-TTT UU-DOCENT
Table 9: A comparison of the rankings produced by Expected Wins, Hopkins & May, and TrueSkill for English?German (the
task with the most systems and the largest cluster). The lines extending all the way across mark the official English?German
clustering (computed from TrueSkill with 90% confidence intervals), while bold entries mark the start of new clusters within
each method or column (computed at the 95% confidence level). The TrueSkill clusterings contain all the system reorderings
across the other two ranking methods.
different levels of granularity.
? To study the effects of training and test
datasets with mixed domains, language pairs
and MT systems.
? To examine the effectiveness of quality pre-
diction methods on human translations.
Four tasks were proposed: Tasks 1.1, 1.2, 1.3
are defined at the sentence-level (Sections 4.1),
while Task 2, at the word-level (Section 4.2). Each
task provides one or more datasets with up to four
language pairs each: English-Spanish, English-
German, German-English, Spanish-English, and
up to four alternative translations generated by:
a statistical MT system (SMT), a rule-based MT
system (RBMT), a hybrid MT system, and a hu-
man. These datasets were annotated with differ-
ent labels for quality by professional translators as
part of the QTLaunchPad
9
project. External re-
sources (e.g. parallel corpora) were provided to
participants. Any additional resources, including
additional quality estimation training data, could
9
http://www.qt21.eu/launchpad/
be used by participants (no distinction between
open and close tracks is made). Participants were
also provided with a software package to extract
quality estimation features and perform model
learning, with a suggested list of baseline features
and learning method for sentence-level prediction.
Participants, described in Section 4.3, could sub-
mit up to two systems for each task.
Data used for building specific MT systems or
internal system information (such as n-best lists)
were not made available this year as multiple MT
systems were used to produced the datasets, in-
cluding rule-based systems. In addition, part of
the translations were produced by humans. Infor-
mation on the sources of translations was not pro-
vided either. Therefore, as a general rule, partici-
pants were only allowed to use black-box features.
4.1 Sentence-level Quality Estimation
For the sentence-level tasks, two variants of the
results could be submitted for each task and lan-
guage pair:
? Scoring: An absolute quality score for each
sentence translation according to the type of
24
prediction, to be interpreted as an error met-
ric: lower scores mean better translations.
? Ranking: A ranking of sentence translations
for all source test sentences from best to
worst. For this variant, it does not matter how
the ranking is produced (from HTER predic-
tions, likert predictions, or even without ma-
chine learning).
Evaluation was performed against the true label
and/or HTER ranking using the same metrics as in
previous years:
? Scoring: Mean Average Error (MAE) (pri-
mary metric), Root Mean Squared Error
(RMSE).
? Ranking: DeltaAvg (primary metric) (Bojar
et al., 2013) and Spearman?s rank correlation.
For all sentence-level these tasks, the same 17
features as in WMT12-13 were used to build base-
line systems. The SVM regression algorithm
within QUEST (Specia et al., 2013)
10
was applied
for that with RBF kernel and grid search for pa-
rameter optimisation.
Task 1.1 Predicting post-editing effort
Data in this task is labelled with discrete and
absolute scores for perceived post-editing effort,
where:
? 1 = Perfect translation, no post-editing
needed at all.
? 2 = Near miss translation: translation con-
tains maximum of 2-3 errors, and possibly
additional errors that can be easily fixed (cap-
italisation, punctuation, etc.).
? 3 = Very low quality translation, cannot be
easily fixed.
The datasets were annotated in a ?triage? phase
aimed at selecting translations of type ?2? (near
miss) that could be annotated for errors at the
word-level using the MQM metric (see Task 2, be-
low) for a more fine-grained and systematic trans-
lation quality analysis. Word-level errors in trans-
lations of type ?3? are too difficult if not impos-
sible to annotate and classify, particularly as they
often contain inter-related errors in contiguous or
overlapping word spans.
10
http://www.quest.dcs.shef.ac.uk/
For the training of prediction models, we pro-
vide a new dataset consisting of source sen-
tences and their human translations, as well as
two-three versions of machine translations (by an
SMT system, an RBMT system and, for English-
Spanish/German only, a hybrid system), all in the
news domain, extracted from tests sets of various
WMT years and MT systems that participated in
the translation shared task:
# Source sentences # Target sentences
954 English 3,816 Spanish
350 English 1,400 German
350 German 1,050 English
350 Spanish 1,050 English
As test data, for each language pair and MT sys-
tem (or human translation) we provide a new set
of translations produced by the same MT systems
(and humans) as those used for the training data:
# Source sentences # Target sentences
150 English 600 Spanish
150 English 600 German
150 German 450 English
150 Spanish 450 English
The distribution of true scores in both training
and test sets for each language pair is given in Fig-
ures 3.
0%#
10%#
20%#
30%#
40%#
50%#
60%#
{en-
de-1
}#
{en-
de-2
}#
{en-
de-3
}#
{de-
en-1
}#
{de-
en-2
}#
{de-
en-3
}#
{en-
es-1
}#
{en-
es-2
}##
{en-
es-3
}##
{es-
en-1
}#
{es-
en-2
}#
{es-
en-3
}#
#Training##### #Test####
Figure 3: Distribution of true 1-3 scores by langauge pair.
Additionally, we provide some out of domain
test data. These translations were annotated in
the same way as above, each dataset by one Lan-
guage Service Provider (LSP), i.e, one profes-
sional translator, with two LPSs producing data in-
dependently for English-Spanish. They were gen-
erated using the LSPs? own source data (a different
domain from news), and own MT system (differ-
ent from the three used for the official datasets).
The results on these datasets were not considered
25
for the official ranking of the participating sys-
tems:
# Source sentences # Target sentences
971 English 971 Spanish
297 English 297 German
388 Spanish 388 English
Task 1.2 Predicting percentage of edits
In this task we use HTER (Snover et al., 2006) as
quality score. This score is to be interpreted as
the minimum edit distance between the machine
translation and its manually post-edited version,
and its range is [0, 1] (0 when no edit needs to
be made, and 1 when all words need to be edited).
We used TERp (default settings: tokenised, case
insensitive, etc., but capped to 1)
11
to compute the
HTER scores.
For practical reasons, the data is a subset of
Task 1.1?s dataset: only translations produced
by the SMT system English-Spanish. As train-
ing data, we provide 896 English-Spanish trans-
lation suggestions and their post-editions. As
test data, we provide a new set of 208 English-
Spanish translations produced by the same SMT
system. Each of the training and test translations
was post-edited by a professional translator using
the CASMACAT
12
web-based tool, which also col-
lects post-editing time on a sentence-basis.
Task 1.3 Predicting post-editing time
For this task systems are required to produce, for
each translation, a real valued estimate of the time
(in milliseconds) it takes a translator to post-edit
the translation. The training and test sets are a sub-
set of that uses in Task 1.2 (subject to filtering of
outliers). The difference is that the labels are now
the number of milliseconds that were necessary to
post-edit each translation.
As training data, we provide 650 English-
Spanish translation suggestions and their post-
editions. As test data, we provide a new set of 208
English-Spanish translations (same test data as for
Task 1.2).
4.2 Word-level Quality Estimation
The data for this task is based on a subset of the
datasets used for Task 1.1, for all language pairs,
11
http://www.umiacs.umd.edu/
?
snover/terp/
12
http://casmacat.eu/
human and machine translations: those transla-
tions labelled ?2? (near misses), plus additional
data provided by industry (either on the news do-
main or on other domains, such as technical doc-
umentation, produced using their own MT sys-
tems, and also pre-labelled as ?2?). All seg-
ments were annotated with word-level labels by
professional translators using the core categories
in MQM (Multidimensional Quality Metrics)
13
as
error typology (see Figure 4). Each word or se-
quence of words was annotated with a single error.
For (supposedly rare) cases where a decision be-
tween multiple fine-grained error types could not
be made, annotators were requested to choose a
coarser error category in the hierarchy.
Participants are asked to produce a label for
each token that indicates quality at different lev-
els of granularity:
? Binary classification: an OK / bad label,
where bad indicates the need for editing the
token.
? Level 1 classification: an OK / accuracy /
fluency label, specifying coarser level cate-
gories of errors for each token, or ?OK? for
tokens with no error.
? Multi-class classification: one of the labels
specifying the error type for the token (termi-
nology, mistranslation, missing word, etc.) in
Figure 4, or ?OK? for tokens with no error.
As training data, we provide tokenised transla-
tion output for all language pairs, human and ma-
chine translations, with tokens annotated with all
issue types listed above, or ?OK?. The annotation
was performed manually by professional transla-
tors as part of the QTLaunchPad project. For
the coarser variants, fine-grained errors are gen-
eralised to Accuracy or Fluency, or ?bad? for the
binary variant. The amount of available training
data varies by language pair:
# Source sentences # Target sentences
1,957 English 1,957 Spanish
715 English 715 German
350 German 350 English
900 Spanish 900 English
13
http://www.qt21.eu/launchpad/content/
training
26
Figure 4: MQM metric as error typology.
As test data, we provide additional data points
for all language pairs, human and machine trans-
lations:
# Source sentences # Target sentences
382 English 382 Spanish
150 English 150 German
100 German 100 English
150 Spanish 150 English
In contrast to Tasks 1.1?1.3, no baseline feature
set is provided to the participants.
Similar to last year (Bojar et al., 2013), the
word-level task is primarily evaluated by macro-
averaged F-measure (in %). Because the class dis-
tribution is skewed ? in the test data about 78% of
the tokens are marked as ?OK? ? we compute pre-
cision, recall, and F
1
for each class individually,
weighting F
1
scores by the frequency of the class
in the test data. This avoids giving undue impor-
tance to less frequent classes. Consider the follow-
ing confusion matrix for Level 1 annotation, i.e.
the three classes (O)K, (F)luency, and (A)ccuracy:
reference
O F A
predicted
O 4172 1482 193
F 1819 1333 214
A 198 133 69
For each of the three classes we assume a binary
setting (one-vs-all) and derive true-positive (tp),
false-positive (fp), and false-negative (fn) counts
from the rows and columns of the confusion ma-
trix as follows:
tp
O
= 4172
fp
O
= 1482 + 193 = 1675
fn
O
= 1819 + 198 = 2017
tp
F
= 1333
fp
F
= 1819 + 214 = 2033
fn
F
= 1482 + 133 = 1615
tp
A
= 69
fp
A
= 198 + 133 = 331
fn
A
= 193 + 214 = 407
We continue to compute F
1
scores for each
class c ? {O,F,A}:
precision
c
= tp
c
/(tp
c
+ fp
c
)
recall
c
= tp
c
/(tp
c
+ fn
c
)
F
1,c
=
2 ? precision
c
? recall
c
precision
c
+recall
c
yielding:
precision
O
= 4172/(4172 + 1675) = 0.7135
recall
O
= 4172/(4172 + 2017) = 0.6741
F
1,O
=
2 ? 0.7135 ? 0.6741
0.7135 + 0.6741
= 0.6932
? ? ?
F
1,F
= 0.4222
F
1,A
= 0.1575
Finally, we compute the average of F
1,c
scores
weighted by the occurrence count N(c) of c:
weightedF
1,ALL
=
1
?
c
N(c)
?
c
N
c
? F
1,c
weightedF
1,ERR
=
1
?
c:c 6=O
N(c)
?
c:c 6=O
N
c
? F
1,c
27
which for the above example gives:
weightedF
1,ALL
=
1
6189 + 2948 + 476
?
(6189 ? 0.6932 + 2948 ? 0.4222
+476 ? 0.1575) = 0.5836
weightedF
1,ERR
=
1
2948 + 476
?
(2948 ? 0.4222 + 476 ? 0.1575)
= 0.3854
We choose F
1,ERR
as our primary evaluation mea-
sure because it most closely mimics the common
application of F
1
scores in binary classification:
one is interested in the performance in detecting a
positive class, which in this case would be erro-
neous words. This does, however, ignore the num-
ber of correctly classified words of the OK class,
which is why we also report F
1,ALL
. In addition,
we follow Powers (2011) and report Matthews
Correlation Coefficient (MCC), averaged in the
same way as F
1
, as our secondary metric. Finally,
for contrast we also report Accuracy (ACC).
4.3 Participants
Table 10 lists all participating teams. Each team
was allowed up to two submissions for each task
and language pair. In the descriptions below, par-
ticipation in specific tasks is denoted by a task
identifier: T1.1, T1.2, T1.3, and T2.
Sentence-level baseline system (T1.1, T1.2,
T1.3): QUEST is used to extract 17 system-
independent features from source and trans-
lation sentences and parallel corpora (same
features as in the WMT12 shared task):
? number of tokens in the source and tar-
get sentences.
? average source token length.
? average number of occurrences of the
target word within the target sentence.
? number of punctuation marks in source
and target sentences.
? language model (LM) probability of
source and target sentences based on
models for the WMT News Commen-
tary corpus.
? average number of translations per
source word in the sentence as given by
IBM Model 1 extracted from the WMT
News Commentary parallel corpus, and
thresholded so that P (t|s) > 0.2, or
so that P (t|s) > 0.01 weighted by the
inverse frequency of each word in the
source side of the parallel corpus.
? percentage of unigrams, bigrams and tri-
grams in frequency quartiles 1 (lower
frequency words) and 4 (higher fre-
quency words) in the source language
extracted from the WMT News Com-
mentary corpus.
? percentage of unigrams in the source
sentence seen in the source side of the
WMT News Commentary corpus.
These features are used to train a Support
Vector Machine (SVM) regression algorithm
using a radial basis function kernel within
the SCIKIT-LEARN toolkit. The ?,  and C
parameters were optimised via grid search
with 5-fold cross validation on the training
set. We note that although the system is re-
ferred to as ?baseline?, it is in fact a strong
system. It has proved robust across a range
of language pairs, MT systems, and text do-
mains for predicting various forms of post-
editing effort (Callison-Burch et al., 2012;
Bojar et al., 2013).
DCU (T1.1): DCU-MIXED and DCU-SVR use
a selection of features available in QUEST,
such as punctuation statistics, LM perplex-
ity, n-gram frequency quartile statistics and
coarse-grained POS frequency ratios, and
four additional feature types: combined POS
and stop word LM features, source-side
pseudo-reference features, inverse glass-box
features for translating the translation and er-
ror grammar parsing features. For machine
learning, the QUEST framework is expanded
to combine logistic regression and support
vector regression and to handle cross- valida-
tion and randomisation in a way that training
items with the same source side are kept to-
gether. External resources are monolingual
corpora taken from the WMT 2014 transla-
tion task for LMs, the MT system used for the
inverse glass-box features (Li et al., 2014b)
and, for error grammar parsing, the Penn-
Treebank and an error grammar derived from
it (Foster, 2007).
28
ID Participating team
DCU Dublin City University Team 1, Ireland (Hokamp et al., 2014)
DFKI German Research Centre for Artificial Intelligence, Germany (Avramidis,
2014)
FBK-UPV-UEDIN Fondazione Bruno Kessler, Italy, UPV Universitat Polit`ecnica de Val`encia,
Spain & University of Edinburgh, UK (Camargo de Souza et al., 2014)
LIG Laboratoire d?Informatique Grenoble, France (Luong et al., 2014)
LIMSI Laboratoire d?Informatique pour la M?ecanique et les Sciences de l?Ing?enieur,
France (Wisniewski et al., 2014)
MULTILIZER Multilizer, Finland
RTM-DCU Dublin City University Team 2, Ireland (Bicici and Way, 2014)
SHEF-lite University of Sheffield Team 1, UK (Beck et al., 2014)
USHEFF University of Sheffield Team 2, UK (Scarton and Specia, 2014)
YANDEX Yandex, Russia
Table 10: Participants in the WMT14 Quality Estimation shared task.
DFKI (T1.2): DFKI/SVR builds upon the base-
line system (above) by adding non-redundant
data from the WMT13 task for predicting
the same label (HTER) and additional fea-
tures such as (a) rule-based language cor-
rections (language tool) (b), PCFG parsing
statistics and counts of tree labels, (c) po-
sition statistics of parsing labels, (d) posi-
tion statistics of trigrams with low probabil-
ity. DFKI/SVRxdata uses a similar setting,
with the addition of more training data from
non-minimally post-edited translation out-
puts (references), filtered based on a thresh-
old on the edit distance between the MT out-
put and the freely-translated reference.
FBK-UPV-UEDIN (T1.2, T1.3, T2): The sub-
missions for the word-level task (T2) use fea-
tures extracted from word posterior probabil-
ities and confusion network descriptors com-
puted over the 100k-best hypothesis transla-
tions generated by a phrase-based SMT sys-
tem. They also use features from word lexi-
cons, and POS tags of each word for source
and translation sentences. The predictions of
the Binary model are used as a feature for the
Level 1 and Multi-class settings. Both condi-
tional random fields (CRF) and bidirectional
long short-term memory recurrent neural net-
works (BLSTM-RNNs) are used for the Bi-
nary setting, and BLSTM-RNNs only for the
Level 1 and Multi-class settings.
The sentence-level QE submissions (T1.2
and T1.3) are trained on black-box features
extracted using QUEST in addition to fea-
tures based on word alignments, word poste-
rior probabilities and diversity scores (Souza
et al., 2013). These features are computed
over 100k-best hypothesis translations also
used for task 2. In addition, a set of ratios
computed from the word-level predictions of
the model trained on the binary setting of
task 2 is used. A total of 221 features and
the extremely randomised trees (Geurts et al.,
2006) learning algorithm are used to train re-
gression models.
LIG (T2): Conditional Random Fields classi-
fiers are trained with features used in LIG?s
WMT13 systems (Luong et al., 2013): tar-
get and source words, alignment informa-
tion, source and target alignment context,
LM scores, target and source POS tags,
lexical categorisations (stopword, punctua-
tion, proper name, numerical), constituent
label, depth in the constituent tree, target
polysemy count, pseudo reference. These
are combined with novel features: word
occurrence in multiple translation systems
and POS tag-based LM scores (longest tar-
get/source n-gram length and backoff score
for POS tag). These features require external
NLP tools and resources such as: TreeTag-
ger, GIZA++, Bekerley parser, Link Gram-
mar parser, WordNet and BabelNet, Google
Translate (pseudo-reference). For the binary
task, the optimal classification threshold is
tuned based on a development set split from
the original training set. Feature selection is
employed over the all features (for the binary
29
task only), with the Sequential Backward Se-
lection algorithm. The best performing fea-
ture set is then also used for the Level 1 and
Multi-class variants.
LIMSI (T2): The submission relies on a ran-
dom forest classifier and considers only 16
dense and continuous features. To prevent
sparsity issues, lexicalised information such
as the word or the previous word identities
is not included. The features considered are
mostly classic MT features and can be cat-
egorised into two classes: association fea-
tures, which describe the quality of the as-
sociation between the source sentence and
each target word, and fluency features, which
describe the ?quality? of the translation hy-
potheses. The latter rely on different lan-
guage models (either on POS or on words)
and the former on IBM Model 1 translation
probabilities and on pseudo- references, i.e.
translation produced by an independent MT
system. Random forests are known to per-
form well in tasks like this one, in which
only a few dense and continuous features are
available, possibly because of their ability to
take into account complex interactions be-
tween features and to automatically partition
the continuous feature values into a discrete
set of intervals that achieves the best classifi-
cation performance. Since they predict the
class probabilities, it is possible to directly
optimize the F
1
score during training by find-
ing, with a grid search method, the decision
threshold that achieved the best F
1
score on
the training set.
MULTILIZER (T1.2, T1.3): The 80 black-box
features from QUEST are used in addition to
new features based on using other MT en-
gines for forward and backward translations.
In forward translations, the idea is that dif-
ferent MT engines make different mistakes.
Therefore, when several forward translations
are similar to each other, these translations
are more likely to be correct. This is con-
firmed by the Pearson correlation of similar-
ities between the forward translations against
the true scores (above 0.5). A backward
translation is very error-prone and therefore
it has to be used in combination with for-
ward translations. A single back-translation
similar to original source segment does not
bring much information. Instead, when sev-
eral MT engines give back-translations simi-
lar to this source segment, one can conclude
that the translation is reliable. Those transla-
tions where similarities both in forward trans-
lation and backward translation are high are
intuitively more likely to be good. A simple
feature selection method that omits all fea-
tures with Pearson correlation against the true
scores below 0.2 is used. The systems sub-
mitted are obtained using linear regression
models.
RTM-DCU (T1.1, T1.2, T1.3, T2): RTM-DCU
systems are based on referential translation
machines (RTM) (Bic?ici, 2013) and parallel
feature decay algorithms (ParFDA5) (Bic?ici
et al., 2014), which allow language and MT
system-independent predictions. For each
task, individual RTM models are developed
using the parallel corpora and the language
model corpora distributed by the WMT14
translation task and the language model cor-
pora provided by LDC for English and Span-
ish. RTMs use 337 to 437 sentence-level fea-
tures for coverage and diversity, IBM1 and
sentence translation performance, retrieval
closeness and minimum Bayes retrieval risk,
distributional similarity and entropy, IBM2
alignment, character n-grams, sentence read-
ability, and parse output tree structures. The
features use ngrams defined over text or com-
mon cover link (CCL) (Seginer, 2007) struc-
tures as the basic units of information over
which similarity calculations are performed.
Learning models include ridge regression
(RR), support vector machines (SVR), and
regression trees (TREE), which are applied
after partial least squares (PLS) or feature
selection (FS). For word-level prediction,
generalised linear models (GLM) (Collins,
2002) and GLM with dynamic learning
(GLMd) (Bic?ici, 2013) are used with word-
level features including CCL links, word
length, location, prefix, suffix, form, context,
and alignment, totalling up to a couple of mil-
lion features.
SHEF-lite (T1.1, T1.2, T1.3): These submis-
sions use the framework of Multi-task Gaus-
sian Processes, where multiple datasets are
30
combined in a multi-task setting similar to
the one used by Cohn and Specia (2013).
For T1.1, data for all language pairs is put
together, and each language is considered a
task. For T1.2 and T1.3, additional datasets
from previous shared task years are used,
each encoded as a different task. For all tasks,
the QUEST framework is used to extract a set
of 80 black-box features (a superset of the 17
baseline features). To cope with the large size
of the datasets, the SHEF-lite-sparse submis-
sion uses Sparse Gaussian Processes, which
provide sensible sparse approximations using
only a subset of instances (inducing inputs)
to speed up training and prediction. For this
?sparse? submission, feature selection is per-
formed following the approach of Shah et al.
(2013) by ranking features according to their
learned length-scales and selecting the top 40
features.
USHEFF (T1.1, T1.2, T1.3): USHEFF submis-
sions exploit the use of consensus among
MT systems by comparing the MT sys-
tem output to several alternative translations
generated by other MT systems (pseudo-
references). The comparison is done using
standard evaluation metrics (BLEU, TER,
METEOR, ROUGE for all tasks, and two
metrics based on syntactic similarities from
shallow and dependency parser information
for T1.2 and T1.3). Figures extracted from
such metrics are used as features to com-
plement prediction models trained on the 17
baseline features. Different from the standard
use of pseudo-reference features, these fea-
tures do not assume that the alternative MT
systems are better than the system of inter-
est. A more realistic scenario is considered
where the quality of the pseudo-references is
not known. For T1, no external systems in
addition to those provided for the shared task
are used: for a given translation, all alter-
native translations for the same source seg-
ment (two or three, depending on the lan-
guage pair) are used as pseudo-references.
For T1.2 and T1.3, for each source sentence,
all alternative translations produced by MT
systems on the same data (WMT12/13) are
used as pseudo-references. The hypothesis
is that by using translations from several MT
systems one can find consensual information
and this can smooth out the effect of ?coinci-
dences? in the similarities between systems?
translations. SVM regression with radial ba-
sis function kernel and hyper-parameters op-
timised via grid search is used to build the
models.
YANDEX (T1.1): Both submissions are based
on the the 80 black-box features, plus an
LM score from a larger language model,
a pseudo-reference, and several additional
features based on POS tags and syntactic
parsers. The first attempt uses an extract
of the top 5 features selected with a greedy
search from the set of all features. SVM re-
gression is used as machine learning algo-
rithm. The second attempt uses the same
features processed with Yandex? implemen-
tation of the gradient tree boosting (Ma-
trixNet).
4.4 Results
In what follows we give the official results for all
tasks followed by a discussion that highlights the
main findings for each of the tasks.
Task 1.1 Predicting post-editing effort
Table 11 summarises the results for the ranking
variant of Task 1.1. They are sorted from best to
worst using the DeltaAvg metric scores as primary
key and the Spearman?s rank correlation scores as
secondary key.
The winning submissions for the ranking vari-
ant of Task 1.1 are as follows: for English-Spanish
it is RTM-DCU/RTM-TREE, with a DeltaAvg
score of 0.26; for Spanish-English it is USH-
EFF, with a DeltaAvg score of 0.23; for English-
German it is again RTM-DCU/RTM-TREE, with a
DeltaAvg score of 0.39; and for German-English it
is RTM-DCU/RTM-RR, with a DeltaAvg score of
0.38. These winning submissions are better than
the baseline system by a large margin, which indi-
cates that current best performance in MT quality
estimation has reached levels that are clearly be-
yond what the baseline system can produce. As for
the other systems, according to DeltaAvg, com-
pared to the previous year results a smaller per-
centage of systems is able to beat the baseline.
This might be a consequence of the use of the met-
ric for the prediction of only three discrete labels.
The results for the scoring task are presented in
Table 12, sorted from best to worst using the MAE
31
System ID DeltaAvg Spearman Corr
English-Spanish
? RTM-DCU/RTM-PLS-TREE 0.26 0.38
? RTM-DCU/RTM-TREE 0.26 0.41
? YANDEX/SHAD BOOSTEDTREES2 0.23 0.35
USHEFF 0.21 0.33
SHEFF-lite 0.21 0.33
YANDEX/SHAD SVR1 0.18 0.29
SHEFF-lite-sparse 0.17 0.27
Baseline SVM 0.14 0.22
Spanish-English
? USHEFF 0.23 0.30
? RTM-DCU/RTM-PLS-RR 0.20 0.35
? RTM-DCU/RTM-FS-RR 0.19 0.36
Baseline SVM 0.12 0.21
SHEFF-lite-sparse 0.12 0.17
SHEFF-lite 0.11 0.15
English-German
? RTM-DCU/RTM-TREE 0.39 0.54
RTM-DCU/RTM-PLS-TREE 0.33 0.42
USHEFF 0.26 0.41
SHEFF-lite 0.26 0.36
Baseline SVM 0.23 0.34
SHEFF-lite-sparse 0.23 0.33
German-English
? RTM-DCU/RTM-RR 0.38 0.51
? RTM-DCU/RTM-PLS-RR 0.35 0.45
USHEFF 0.28 0.30
SHEFF-lite 0.24 0.27
Baseline SVM 0.21 0.25
SHEFF-lite-sparse 0.14 0.17
Table 11: Official results for the ranking variant of the WMT14 Quality Evaluation Task 1.1. The winning submissions
are indicated by a ?. These are the top-scoring submission and those that are not significantly worse according to bootstrap
resampling (1M times) with 95% confidence intervals. The systems in the gray area are not different from the baseline system
at a statistically significant level according to the same test.
32
System ID MAE RMSE
English-Spanish
? RTM-DCU/RTM-PLS-TREE 0.49 0.61
? SHEFF-lite 0.49 0.63
? USHEFF 0.49 0.63
? SHEFF-lite/sparse 0.49 0.69
? RTM-DCU/RTM-TREE 0.49 0.61
Baseline SVM 0.52 0.66
YANDEX/SHAD BOOSTEDTREES2 0.56 0.68
YANDEX/SHAD SVR1 0.64 0.81
DCU-Chris/SVR 0.66 0.88
DCU-Chris/MIXED 0.94 1.14
Spanish-English
? RTM-DCU/RTM-FS-RR 0.53 0.64
? SHEFF-lite/sparse 0.54 0.69
? RTM-DCU/RTM-PLS-RR 0.55 0.71
USHEFF 0.57 0.67
Baseline SVM 0.57 0.68
SHEFF-lite 0.62 0.77
DCU-Chris/MIXED 0.65 0.91
English-German
? RTM-DCU/RTM-TREE 0.58 0.68
RTM-DCU/RTM-PLS-TREE 0.60 0.71
SHEFF-lite 0.63 0.74
USHEFF 0.64 0.75
SHEFF-lite/sparse 0.64 0.75
Baseline SVM 0.64 0.76
DCU-Chris/MIXED 0.69 0.98
German-English
? RTM-DCU/RTM-RR 0.55 0.67
? RTM-DCU/RTM-PLS-RR 0.57 0.74
USHEFF 0.63 0.76
SHEFF-lite 0.65 0.77
Baseline SVM 0.65 0.78
Table 12: Official results for the scoring variant of the WMT14 Quality Evaluation Task 1.1. The winning submissions
are indicated by a ?. These are the top-scoring submission and those that are not significantly worse according to bootstrap
resampling (1M times) with 95% confidence intervals. The systems in the gray area are not different from the baseline system
at a statistically significant level according to the same test.
33
metric scores as primary key and the RMSE metric
scores as secondary key.
The winning submissions for the scoring variant
of Task 1.1 are as follows: for English-Spanish it
is RTM-DCU/RTM-TREE with a MAE of 0.49;
for Spanish-English it is RTM-DCU/RTM-FS-
RR with a MAE of 0.53; for English-German
it is again RTM-DCU/RTM-TREE, with a MAE
of 0.58; and for German-English it is RTM-
DCU/RTM-RR with a MAE of 0.55. These sub-
missions are again much better than the baseline
system, which under the scoring variant seems
to perform at a middle-of-the-pack level or lower
compared to the overall pool of submissions.
Overall, more systems are able to outperform the
baseline according to the scoring metric.
The top system for most language pairs are
essentially based on the same core techniques
(RTM-DCU) according to both the DeltaAvg and
MAE metrics. The ranking of other systems, how-
ever, can be substantially different according to the
two metrics.
Task 1.2 Predicting percentage of edits
Table 13 summarises the results for the ranking
variant of Task 1.2. For readability purposes we
have used a multiplication-factor of 100 in the
scoring script, which makes the HTER numbers
(both predicted and gold) to be in the [0, 100]
range. They are sorted from best to worst using
the DeltaAvg metric scores as primary key and the
Spearman?s rank correlation scores as secondary
key.
The winning submission for the ranking vari-
ant of Task 1.2 is RTM-DCU/RTM-SVR, with a
DeltaAvg score of 9.31. There is a large mar-
gin between this score and the baseline score of
DeltaAvg 5.08, which indicates again that current
best performance has reached levels that are much
beyond what this baseline system can produce.
The vast majority of the submissions perform bet-
ter than the baseline (the only exception is the sub-
mission from SHEFF-lite, for which the authors
report a major issue with the learning algorithm).
The results for the scoring variant are presented
in Table 14, sorted from best to worst by using the
MAE metric scores as primary key and the RMSE
metric scores as secondary key.
The winning submission for the scoring variant
of Task 1.2 is FBK-UPV-UEDIN/WP with a MAE
of 12.89, while the baseline system has a MAE
of 15.23. Most of the submissions perform better
than the baseline.
Task 1.3 Predicting post-editing time
Table 15 summarises the results for the ranking
variant of Task 1.3. For readability purposes, we
have used a multiplication-factor of 0.001 in the
scoring script, which makes the time (both pre-
dicted and gold) to be measured in seconds. They
are sorted from best to worst using the DeltaAvg
metric scores as primary key and the Spearman?s
rank correlation scores as secondary key.
The winning submission for the ranking vari-
ant of Task 1.3 is RTM-DCU/RTM-RR, with a
DeltaAvg score of 17.02 (when predicting sec-
onds). The interesting aspect of these results is
that the DeltaAvg numbers have a direct real-
world interpretation, in terms of time spent (or
saved, depending on one?s view-point) for post-
editing machine-produced translations. A more
elaborate discussion on this point can be found in
Section 4.5.
The winning submission for the scoring variant
of Task 1.3 is RTM-DCU/RTM-SVR, with a MAE
of 16.77. Note that all of the submissions perform
significantly better than the baseline, which has a
MAE of 21.49, and that the majority is not signif-
icantly worse than the top scoring submission.
Task 2 Predicting word-level edits
The results for Task 2 are summarised in Tables
17?19. The results are ordered by F
1
score for
the Error (BAD) class. For comparison, two triv-
ial baselines are included, one that marks every
word as correct and that marks every word with
the most common error class found in the training
data. Both baselines are clearly useless for any ap-
plication, but help put the results in perspective.
Most teams submitted systems for a single lan-
guage pair: English-Spanish; only a single team
produced predictions for all four pairs.
Table 17 gives the results of the binary (OK vs.
BAD) classification variant of Task 2. The win-
ning submissions for this variant are as follows:
for English-Spanish it is FBK-UPV-UEDIN/RNN
with a weighted F
1
of 48.73; for Spanish-
English it is RTM-DCU/RTM-GLMd with a
weighted F
1
of 29.14; for English-German it is
RTM-DCU/RTM-GLM with a weighted F
1
of
45.30; and for German-English it is again RTM-
DCU/RTM-GLM with a weighted F
1
of 26.13.
Remarkably, for three out of four language
pairs, the systems fail to beat our trivial baseline of
34
System ID DeltaAvg Spearman Corr
English-Spanish
? RTM-DCU/RTM-SVR 9.31 0.53
? RTM-DCU/RTM-TREE 8.57 0.48
? USHEFF 7.93 0.45
SHEFF-lite/sparse 7.69 0.43
Baseline 5.08 0.31
SHEFF-lite 0.72 0.09
Table 13: Official results for the ranking variant of the WMT14 Quality Evaluation Task 1.2. The winning submissions
are indicated by a ?. These are the top-scoring submission and those that are not significantly worse according to bootstrap
resampling (100k times) with 95% confidence intervals. The systems in the gray area are not different from the baseline system
at a statistically significant level according to the same test.
System ID MAE RMSE
English-Spanish
? FBK-UPV-UEDIN/WP 12.89 16.74
? RTM-DCU/RTM-SVR 13.40 16.69
? USHEFF 13.61 17.84
RTM-DCU/RTM-TREE 14.03 17.48
DFKI/SVR 14.32 17.74
FBK-UPV-UEDIN/NOWP 14.38 18.10
SHEFF-lite/sparse 15.04 18.38
MULTILIZER 15.04 20.86
Baseline 15.23 19.48
DFKI/SVRxdata 16.01 19.52
SHEFF-lite 18.15 23.41
Table 14: Official results for the scoring variant of the WMT14 Quality Evaluation Task 1.2. The winning submissions
are indicated by a ?. They are statistically indistinguishable from the top submission according to bootstrap resampling (1M
times) with 95% confidence intervals. The systems in the gray area are not different from the baseline system at a statistically
significant level according to the same test.
System ID DeltaAvg Spearman Corr
English-Spanish
? RTM-DCU/RTM-RR 17.02 0.68
? RTM-DCU/RTM-SVR 16.60 0.67
SHEFF-lite/sparse 16.33 0.63
SHEFF-lite 16.08 0.64
USHEFF 14.98 0.59
Baseline 14.71 0.57
Table 15: Official results for the ranking variant of the WMT14 Quality Evaluation Task 1.3. The winning submissions
are indicated by a ?. They are statistically indistinguishable from the top submission according to bootstrap resampling (1M
times) with a 95% confidence interval. The systems in the gray area are not different from the baseline system at a statistically
significant level according to the same test.
35
System ID MAE RMSE
English-Spanish
? RTM-DCU/RTM-SVR 16.77 26.17
?MULTILIZER/MLZ2 17.07 25.83
? SHEFF-lite 17.13 27.33
?MULTILIZER/MLZ1 17.31 25.51
? SHEFF-lite/sparse 17.42 27.35
? FBK-UPV-UEDIN/WP 17.48 25.31
RTM-DCU/RTM-RR 17.50 25.97
FBK-UPV-UEDIN/NOWP 18.69 26.58
USHEFF 21.48 34.28
Baseline 21.49 34.28
Table 16: Official results for the scoring variant of the WMT14 Quality Evaluation Task 1.3. The winning submissions
are indicated by a ?. They are statistically indistinguishable from the top submission according to bootstrap resampling (1M
times) with a 95% confidence interval. The systems in the gray area are not different from the baseline system at a statistically
significant level according to the same test.
weighted F
1
F
1
System ID All Bad ? MCC ACC
English-Spanish
Baseline (always OK) 50.43 0.00 0.00 64.38
Baseline (always Bad) 18.71 52.53 0.00 35.62
? FBK-UPV-UEDIN/RNN 62.00 48.73 18.23 61.62
LIMSI/RF 60.55 47.32 15.44 60.09
LIG/FS 63.55 44.47 19.41 64.67
LIG/BL ALL 63.77 44.11 19.91 65.12
FBK-UPV-UEDIN/RNN+tandem+crf 62.17 42.63 16.32 63.26
RTM-DCU/RTM-GLM 60.68 35.08 13.45 63.74
RTM-DCU/RTM-GLMd 60.24 32.89 12.98 63.97
Spanish-English
Baseline (always OK) 74.41 0.00 0.00 82.37
Baseline (always Bad) 5.28 29.98 0.00 17.63
? RTM-DCU/RTM-GLMd 79.54 29.14 25.47 82.98
RTM-DCU/RTM-GLM 79.42 26.91 25.93 83.43
English-German
Baseline (always OK) 59.39 0.00 0.00 71.33
Baseline (always Bad) 12.78 44.57 0.00 28.67
? RTM-DCU/RTM-GLM 71.51 45.30 28.61 72.97
RTM-DCU/RTM-GLMd 68.73 36.91 21.32 71.41
German-English
Baseline (always OK) 67.82 0.00 0.00 77.60
Baseline (always Bad) 8.20 36.60 0.00 22.40
? RTM-DCU/RTM-GLM 72.41 26.13 16.08 76.14
RTM-DCU/RTM-GLMd 71.42 22.97 12.63 75.46
Table 17: Official results for the binary part of the WMT14 Quality Evaluation Task 2. The winning submissions are indicated
by a ?. All values are given as percentages.
36
marking all the words as wrong. This may either
indicate that the predictions themselves are of low
quality or the chosen evaluation approach is mis-
leading. On the other hand F
1
scores are a com-
mon measure of binary classification performance
and no averaging is performed here.
Table 18 gives the results of the Level 1
classification (OK, Fluency, Accuracy) variant
of Task 2. Here the second baseline is to
always predict Fluency errors, as this is the
most common error category in the training
data. The winning submissions of this vari-
ant are as follows: for English-Spanish it
is FBK-UPV-UEDIN/RNN+tandem+crf with a
weighted F
1
of 23.94 and for Spanish-English,
English-German, and German-English it is RTM-
DCU/RTM-GLMd with weighted F
1
scores of
23.94, 21.94, and 8.57 respectively.
As before, all systems fail to outperform the
single-class baseline for the Spanish-English lan-
guage pair according to our primary metric. How-
ever, for Spanish-English and English-German
both submissions are able to beat the baseline by
large margin. We also observe that the absolute
numbers vary greatly between language pairs.
Table 19 gives the results of the Multi-class
classification variant of Task 2. Again, the sec-
ond baseline is to always predict the most common
error category in the training data, which varies
depending on language pair and produces and in-
creasingly weak baseline as the number of classes
rises.
The winning submissions of this variant are
as follows: for English-Spanish, Spanish-English,
and English-German it is RTM-DCU/RTM-GLM
with weighted F
1
scores of 26.84, 8.75, and 15.02
respectively and and for German-English it is
RTM-DCU/RTM-GLMd with a weighted F
1
of
3.08. Not only do these systems perform above
our baselines for all but the German-English lan-
guage pair, they also outperform all other sub-
missions for English-Spanish. Remarkably, RTM-
DCU/RTM-GLM wins English-Spanish for all of
the proposed metrics by a sizeable margin.
4.5 Discussion
In what follows, we discuss the main accomplish-
ments of this year?s shared task starting from the
goals we had previously identified for it.
Investigating the effectiveness of different
quality labels
For the sentence-level tasks, the results of this
year?s shared task allow us to investigate the ef-
fectiveness of predicting translation quality using
three very different quality labels: perceived post-
editing effort on a scale of [1-3] (Task 1.1); HTER
scores (Task 1.2); and the time that a translator
takes to post-edit the translation (Task 1.3). One of
the ways one can compare the effectiveness across
all these different labels is to look at how well
the models can produce predictions that correlate
with the gold label that we have at our disposal.
A measure of correlation that does not depend
on the value of the labels is Spearman?s ranking
correlation. From this perspective, the label that
seems the most effective appears to be post-editing
time (Task 1.3), with the best system (RTM-
DCU/RTM-RR) producing a Spearman?s ? of 0.68
(English-Spanish translations, see Table 15). In
comparison, when perceived post-editing effort la-
bels are used (Task 1.1), the best systems achieve
a Spearman?s ? of 0.38 and 0.30 for English-
Spanish and Spanish-English translations, respec-
tively, and ? of 0.54 and 0.51 for English-German
and German-English, respectively (Table 11); for
HTER scores (Task 1.2) the best systems achieve
a Spearman?s ? of 0.53 for English-Spanish trans-
lations (Table 13).
This comparison across tasks seems to indicate
that, among the three labels we have proposed,
post-editing time seems to be the most learnable,
in the sense that automatic predictions can vest
match the gold labels (in this case, with respect
to the rankings they induce). A possible reason
for this is that post-editing time correlates with the
length of the source sentence whereas HTER is a
normalised measure.
Compared to the results regarding time predic-
tion in the Quality Evaluation shared task from
2013 (Bojar et al., 2013), we note that this time
all submissions were able to beat the baseline sys-
tem (compared to only 1/3 of the submissions in
2013). In addition, better handling of the data
acquisition reduced the number of outliers in this
year?s dataset allowing for numbers that are more
reliably interpretable. As an example of its in-
terpretability, consider the following: the winning
submission for the ranking variant of Task 1.3 is
RTM-DCU/RTM-RR, with a a Spearman?s ? of
0.68 and a DeltaAvg score of 17.02 (when predict-
37
weighted F
1
weighted MCC
System ID All Errors ? All Errors ACC
English-Spanish
Baseline (always OK) 50.43 0.00 0.00 0.00 64.38
Baseline (always fluency) 14.39 40.41 0.00 0.00 30.67
? FBK-UPV-UEDIN/RNN+tandem+crf 58.36 38.54 16.63 13.89 57.98
FBK-UPV-UEDIN/RNN 60.32 37.25 18.22 15.51 61.75
LIG/BL ALL 58.97 31.79 14.95 11.48 61.13
LIG/FS 58.95 31.78 14.92 11.46 61.10
RTM-DCU/RTM-GLMd 58.23 26.62 12.60 12.76 62.94
RTM-DCU/RTM-GLM 56.47 29.91 8.11 7.96 58.56
Spanish-English
Baseline (always OK) 74.41 0.00 0.00 0.00 82.37
Baseline (always fluency) 2.67 15.13 0.00 0.00 12.24
? RTM-DCU/RTM-GLMd 78.89 23.94 25.41 25.45 83.17
RTM-DCU/RTM-GLM 78.78 21.96 26.31 26.99 83.69
English-German
Baseline (always OK) 59.39 0.00 0.00 0.00 71.33
Baseline (always fluency) 3.83 13.35 0.00 0.00 14.82
? RTM-DCU/RTM-GLMd 64.58 21.94 17.69 15.92 69.26
RTM-DCU/RTM-GLM 64.43 21.10 16.99 14.93 69.34
German-English
Baseline (always OK) 67.82 0.00 0.00 0.00 77.60
Baseline (always fluency) 3.34 14.92 0.00 0.00 13.79
? RTM-DCU/RTM-GLMd 69.17 8.57 10.61 5.76 75.91
RTM-DCU/RTM-GLM 69.09 8.26 9.95 5.76 75.97
Table 18: Official results for the Level 1 classification part of the WMT14 Quality Evaluation Task 2. The winning submissions
are indicated by a ?. All values are given as percentages.
38
weighted F
1
weighted MCC
System ID All Errors ? All Errors ACC
English-Spanish
Baseline (always OK) 50.43 0.00 0.00 0.00 64.38
Baseline (always unintelligible) 7.93 22.26 0.00 0.00 21.99
? RTM-DCU/RTM-GLM 60.52 26.84 23.77 21.45 66.83
FBK-UPV-UEDIN/RNN+tandem+crf 52.96 23.07 15.17 10.74 52.13
LIG/BL ALL 56.66 20.50 18.56 13.39 60.39
LIG/FS 56.66 20.50 18.56 13.39 60.39
FBK-UPV-UEDIN/RNN 52.84 17.09 7.66 4.24 57.18
RTM-DCU/RTM-GLMd 51.87 3.22 10.16 4.04 64.42
Spanish-English
Baseline (always OK) 74.41 0.00 0.00 0.00 82.37
Baseline (always word order) 0.34 1.96 0.00 0.00 4.24
? RTM-DCU/RTM-GLM 76.34 8.75 19.82 13.43 83.27
RTM-DCU/RTM-GLMd 76.21 8.19 19.35 15.32 83.17
English-German
Baseline (always OK) 59.39 0.00 0.00 0.00 71.33
Baseline (always mistranslation) 2.48 8.66 0.00 0.00 11.78
? RTM-DCU/RTM-GLM 63.57 15.02 17.57 15.08 70.82
RTM-DCU/RTM-GLMd 63.33 12.48 18.70 13.20 71.45
German-English
Baseline (always OK) 67.82 0.00 0.00 0.00 77.60
Baseline (always word order) 1.56 6.96 0.00 0.00 9.23
? RTM-DCU/RTM-GLMd 67.62 3.08 7.19 1.48 74.73
RTM-DCU/RTM-GLM 67.86 2.36 7.55 1.79 75.75
Table 19: Official results for the Multi-class classification part of the WMT14 Quality Evaluation Task 2. The winning
submissions are indicated by a ?. All values are given as percentages.
39
ing seconds). This number has a direct real-world
interpretation: using the order proposed by this
system, a human translator would spend, on av-
erage, about 17 seconds less on a sentence taken
from the top of the ranking compared to a sen-
tence picked randomly from the set.
14
To put this
number into perspective, for this dataset the av-
erage time to complete a sentence post-editing is
39 seconds. As such, one has an immediate inter-
pretation for the usefulness of using such a rank-
ing: translating around 100 sentences taken from
the top of the rankings would take around 36min
(at about 22 seconds/sentence), while translating
the same number of sentences extracted randomly
from the same dataset would take around 1h5min
(at about 39 seconds/sentence). It is in this sense
that we consider post-editing time an interpretable
label.
Another desirable property of label predictions
is usefulness; this property, however, it highly
task-dependent and therefore cannot be judged in
the absence of a specific task. For instance, an in-
terpretable label like post-editing time may not be
that useful in a task the requires one to place the
machine translations into ?ready to publish? and
?not ready to publish? bins. For such an appli-
cation, labels such as the ones used by Task 1.1
are clearly more useful, and also very much inter-
pretable within the scope of the task. Our attempt
at presenting the Quality Prediction task with a va-
riety of prediction labels illustrates a good range
of properties for the proposed labels and enables
one to draw certain conclusions depending on the
needs of the specific task at hand.
For the word-level tasks, different quality labels
equate with using different levels of granularity for
the predictions, which we discuss next.
Exploring word-level quality prediction at
different levels of granularity
Previous work on word-level predictions, e.g. (Bo-
jar et al., 2013) has focused on prediction of auto-
matically derived labels, generally due to practical
considerations as the manual annotation is labour
intensive. While easily applicable, automatic an-
notations, using for example TER alignment be-
tween the machine translation and reference (or
post-edition), face the same problems as automatic
14
Note that the 17.02 seconds figure is a difference in real-
time, not predicted time; what is considered in this variant of
Task 1.3 is only the predicted ranking of data points, not the
absolute values of the predictions.
MT evaluation metrics as they fail to account for
different word choices and lack the ability to re-
liably distinguish meaning preserving reorderings
from those that change the semantics of the out-
put. Furthermore, previous automatic annotation
for word-level quality estimation has focused on
binary labels: correct / incorrect, or at most, the
main edit operations that can be captured by align-
ment metrics like TER: correct, insertion, dele-
tion, substitution.
In this year?s task we were able to provide
manual fine-grained annotations at the word-level
produced by humans irrespective of references or
post-editions. Error categories range from fre-
quent ones, such as unintelligible, mistranslation,
and terminology, to rare ones such as additions or
omissions. For example, only 10 out of more than
3,400 errors in the English-Spanish test set fall
into the latter categories, while over 2,000 words
are marked as unintelligible. By hierarchically
grouping errors into coarser categories we aimed
to find a compromise between data sparsity and
the expressiveness of the labels. What marks a
good compromise depends on the use case, which
we do not specify here, and the quality of the finer
grained predictions: if a system is able to predict
even rare errors these may be grouped later if nec-
essary.
Overall, word-level error prediction seems to re-
main a challenging task as evidenced by the fact
that many submissions were unable to beat a triv-
ial baseline. We hypothesise that this is at least
partially due to a mismatch in loss-functions used
in training and testing. We know from the sys-
tem descriptions that some systems were tuned to
optimise squared error or accuracy, while evalua-
tion was performed using weighted F
1
scores. On
the other hand, even a comparison of just accuracy
shows that systems struggle to obtain a lower error
rates than the ?all-OK? baseline.
Such performance problems are consistent over
the three levels of granularity, contrary to the in-
tuition that binary classification would be easier.
A notable exception is the RTM-DCU/RTM-GLM
system, which is able to beat both the baseline and
all other systems on the Multi-Class variant of the
English-Spanish task ? cf. Table 19 ? with regard
to all metrics. For this and most other submis-
sions we observe that labels are not consistent for
different granularities, i.e. at token marked with a
specific error in the multi-class variant may still
40
carry an ?OK? label in binary annotation. Thus,
additional coarse grained annotations may be de-
rived by automatic means. For example, mapping
the multi-class predictions of the above system to
coarser categories improves the F
1,ERR
score in
Table 17 from 35.08 to 37.02 but does not change
the rank with respect to the other entries.
The fact that coarse grained predictions seem
not to be derived from the fine-grained ones leads
us to believe that most participants treated the
different granularities as independent classifica-
tion tasks. The FBK-UPV-UEDIN team trans-
fers information in the opposite direction by using
their binary predictions as features for Level-1 and
multi-class.
Given the current quality of word-level predic-
tion it remains unclear if these systems can already
be employed in a practical setting, e.g. to focus the
attention of post-editors.
Studying the effects of training and test
datasets with mixed domains, language pairs
and MT systems
This year?s shared task made available datasets for
more than one language pair with the same or dif-
ferent types of annotation, 2-3 multiple MT sys-
tems (plus a human translation) per language pair,
and out-of-domain test data (Tasks 1.1 and 2). In-
stances for each language pair were kept in sep-
arate datasets and thus the ?language pair? vari-
able can be analysed independently. However, for
a given language pair, datasets mix translation sys-
tems (and humans) in Task 1.1, and also text do-
mains in Task 2.
Directly comparing the performance across lan-
guage pairs is not possible, given that their
datasets have different numbers of instances (pro-
duced by 3 or 4 systems) and/or different true
score distributions (see Figure 3). For a relative
comparison (although not all systems submitted
results for all language pairs, which is especially
true in Task 2), we observe in Task 1.1 that for all
language pairs generally at least half of the sys-
tems did better than the baseline. To our surprise,
only one submission combined data for multiple
languages together for Task 1.1: SHEF-lite, treat-
ing each language pair data as a different task in
a multi-task learning setting. However, only for
the ?sparse? variant of the submission significant
gains were reported over modelling each task in-
dependently (with the tasks still sharing the same
data kernel and the same hyperparameters).
The interpretation of the results for Task 2 is
very dependent on the evaluation metric used,
but generally speaking a large variation in per-
formance was found between different languages,
with English-Spanish performing the best, possi-
bly given the much larger number of training in-
stances. Data for Task 2 also presented varied true
score distributions (as shown by the performance
of the baseline (e.g. always ?OK?) in Tables 17-
19.
One of the main goals with Task 1.1 (and Task 2
to some extent) was to test the robustness of mod-
els in a blind setting where multiple MT systems
(and human translations) are put together and their
identifiers are now known. All submissions for
these tasks were therefore translation system ag-
nostic, with no submission attempting to perform
meta-identification of the origins of the transla-
tions. For Task 1.1, data from multiple MT sys-
tems was explicitly used by USHEFF though the
idea of consensus translations. Translations from
all but the system of interest for the same source
segment were used as pseudo-references. The
submission significantly outperformed the base-
line for all language pairs and did particularly well
for Spanish-English and English-Spanish.
An in depth analysis of Task 1.1?s datasets on
the difference in prediction performance between
models built and applied for individual transla-
tion systems and models built and tested for all
translations pooled together is presented in (Shah
and Specia, 2014). Not surprisingly, the former
models perform significantly better, with MAE
scores ranging between 0.35 and 0.5 for differ-
ent language pairs and MT systems, and signifi-
cantly lower scores for models trained and tested
on human translations only (MAE scores between
0.2 and 0.35 for different language pairs), against
MAE scores ranging between 0.5 and 0.65 for
models with pooled data.
For Tasks 1.2 and 1.3, two submissions included
English-Spanish data which had been produced by
yet different MT systems (SHEF-lite and DFKI).
While using these additional instances seemed at-
tractive given the small number of instances avail-
able for these tasks, it is not clear what their contri-
bution was. For example, with a reduced set of in-
stances (only 400) from the combined sets, SHEF-
lite/sparse performed significantly better than its
variant SHEF-lite.
Finally, with respect to out-of-domain (different
41
text domain and MT system) test data, for Task
1.1, none of the papers submitted included experi-
ments. (Shah and Specia, 2014) applied the mod-
els trained on pooled datasets (as explained above)
for each language pair to the out-of-domain test
sets. The results were surprisingly positive, with
average MAE score of 0.5, compared to the 0.5-
0.65 range for in-domain data (see above). Further
analysis is necessary to understand the reasons for
that.
In Task 2, the official training and test sets al-
ready include out-of-domain data because of the
very small amount of in-domain data available,
and thus is is hard to isolate the effect of this data
on the results.
Examining the effectiveness of quality
prediction methods on human translations
Datasets for Tasks 1.1 and 2 contain human trans-
lations, in addition to the automatic translations
from various MT systems. Predicting human
translation quality is an area that has been largely
unexplored. Previous work has looked into dis-
tinguishing human from machine translations (e.g.
(Gamon et al., 2005)), but this problem setting is
somehow artificial, and moreover arguably harder
to solve nowadays given the higher general qual-
ity of current MT systems (Shah and Specia,
2014). Although human translations are obviously
of higher quality in general, many segments are
translated by MT systems with the same or similar
levels of quality as human translation. This is par-
ticularly true for Task 2, since data had been pre-
viously categorised and only ?near misses? were
selected for the word-level annotation, i.e., human
and machine translations that were both nearly
perfect in this case.
While no distinction was made between human
and machine translations in our tasks, we believe
the mix of these two types of translations has had
a negative impact in prediction performance. Intu-
itively, one can expect errors in human translation
to be more subtle, and hence more difficult to cap-
ture via standard quality estimation features. For
example, an incorrect lexical choice (due to, e.g.,
ambiguity) which still fits the context and does not
make the translation ungrammatical is unlikely to
be captured. We hoped that participants would de-
sign features for this particular type of translation,
but although linguistically motivated features have
been exploited, they did not seem appropriate for
human translations.
It is interesting to mention the indirect use of
human translations by USHEFF for Tasks 1.1-1.3:
given a translation for a source segment, all other
translations for the same segment were used as
pseudo-references. Apart from when this transla-
tion was actually the human translation, the hu-
man translation was effectively used as a refer-
ence. While this reference was mixed with 2-
3 other pseudo-references (other machine transla-
tions) for the feature computations, these features
led to significant gains in performance over the
baseline features Scarton and Specia (2014).
We believe that more investigation is needed for
human translation quality prediction. Tasks ded-
icated to this type of data at both sentence- and
word-level in the next editions of this shared task
would be a possible starting point. The acquisi-
tion of such data is however much more costly, as
it is arguably hard to find examples of low quality
human translation, unless specific settings, such as
translation learner corpora, are considered.
5 Medical Translation Task
The Medical Translation Task addresses the prob-
lem of domain-specific and genre-specific ma-
chine translation. The task is split into two sub-
tasks: summary translation, focused on transla-
tion of sentences from summaries of medical ar-
ticles, and query translation, focused on transla-
tion of queries entered by users into medical infor-
mation search engines.
In general, texts of specific domains and gen-
res are characterized by the occurrence of special
vocabulary and syntactic constructions which are
rare or even absent in traditional (general-domain)
training data and therefore difficult for MT. Spe-
cific training data (containing such vocabulary and
syntactic constructions) is usually scarce or not
available at all. Medicine, however, is an exam-
ple of a domain for which in-domain training data
(both parallel and monolingual) is publicly avail-
able in amounts which allow to train a complete
SMT system or to adapt an existing one.
5.1 Task Description
In the Medical Translation Task, we provided links
to various medical-domain training resources and
asked participants to use the data to train or adapt
their systems to translate unseen test sets for both
subtasks between English and Czech (CS), Ger-
man (DE), and French (FR), in both directions.
42
The summary translation test data is domain-
specific, but otherwise can be considered as ordi-
nary sentences. On the other hand, the query trans-
lation test data is also specific for its genre (gen-
eral style) ? it contains short sequences of (more
or less) of independent terms rather than complete
and grammatical sentences, the usual target of cur-
rent MT systems.
Similarly to the standard Translation Task, the
participants of the Medical Translation Task were
allowed to use only the provided resources in the
constrained task (in addition to data allowed in
the constrained standard Translation Task), but
could exploit any additional resources in the un-
constrained task. The submissions were expected
with true letter casing and detokenized. The trans-
lation quality was measured using automatic eval-
uation metrics, manual evaluation was not per-
formed.
5.2 Test and Development Data
The test and development data sets for this task
were provided by the EU FP7 project Khres-
moi.
15
This projects develops a multi-lingual
multi-modal search and access system for biomed-
ical information and documents and its MT com-
ponent allows users to use non-English queries to
search in English documents and see summaries
of retrieved documents in their preferred language
(Czech, German, or French). The statistics of the
data sets are presented in Tables 20 and 21.
For the summary translation subtask, 1,000
and 500 sentences were provided for test devel-
opment purposes, respectively. The sentences
were randomly sampled from automatically gen-
erated summaries (extracts) of English documents
(web pages) containing medical information rel-
evant to 50 topics provided for the CLEF 2013
eHealth Task 3.
16
Out-of-domain and ungram-
matical sentences were manually removed. The
sentences were then translated by medical experts
into Czech, German and French, and the transla-
tions were reviewed. Each sentence was provided
with the corresponding document ID and topic ID.
The set also included a description for each of the
50 topics. The data package (Khresmoi Summary
Translation Test Data 1.1) is now available from
the LINDAT/CLARIN repository
17
and more de-
15
http://khresmoi.eu/
16
https://sites.google.com/site/
shareclefehealth/
17
http://hdl.handle.net/11858/
tails can be found in Zde?nka Ure?sov?a and Pecina
(2014).
For the query translation subtask, the main
test set contains 1,000 queries for test and 508
queries for development purposes. The original
English queries were extracted at random from
real user query logs provided by the Health on the
Net foundation
18
(queries by general public) and
the Trip database
19
(queries by medical experts).
Each query was translated into Czech, German,
and French by medical experts and the transla-
tions were reviewed. The data package (Khresmoi
Query Translation Test Data 1.0) is available from
the LINDAT/CLARIN repository.
20
An additional test set for the query translation
subtask was adopted from the CLEF 2013 eHealth
Task 3 (Pecina et al., 2014). It contains 50 queries
constructed from titles of the test topics (originally
in English) translated into Czech, German, and
French by medical experts. The participants were
asked to translate the queries back to English and
the resulting translations were used in an informa-
tion retrieval (IR) experiment for extrinsic evalua-
tion.
5.3 Training Data
This section reviews the in-domain resources
which were allowed for the constrained Medical
Translation Task in addition to resources for the
constrained standard Translation Task (see Section
2). Most of the corpora are available for direct
download, others can be obtained upon registra-
tion. The corpora usually employ their own, more
or less complex data format. To lower the entry
barrier, we provided a set of easy-to-use scripts to
convert the data to a plain text format suitable for
MT training.
5.3.1 Parallel Training Data
The medical-domain parallel data includes the fol-
lowing corpora (see Table 22 for statistics): The
EMEA corpus (Tiedemann, 2009) contains doc-
uments from the European Medicines Agency,
automatically processed and aligned on sentence
level. It is available for many language pairs, in-
cluding those relevant to this task. UMLS is a
multilingual metathesaurus of health and biomed-
00-097C-0000-0023-866E-1
18
http://www.hon.ch/
19
http://www.tripdatabase.com/
20
http://hdl.handle.net/11858/
00-097C-0000-0022-D9BF-5
43
sents tokens
total Czech German French English
dev 500 9,209 9,924 12,369 10,350
test 1,000 19,191 20,831 26,183 21,423
Table 20: Statistics of summary test data.
queries tokens
total general expert Czech German French English
dev 508 249 259 1,128 1,041 1,335 1,084
test 1,000 500 500 2,121 1,951 2,490 2,067
Table 21: Statistics of query test data.
L1?L2 Czech?English DE?EN FR?EN
data set sents L1 tokens L2 tokens sents L1 tokens L2 tokens sents L1 tokens L2 tokens
EMEA 1,053 13,872 14,378 1,108 13,946 14,953 1,092 17,605 14,786
UMLS 1,441 4,248 5,579 2,001 6,613 8,153 2,171 8,505 8,524
Wiki 3 5 6 10 19 22 8 19 17
MuchMore 29 688 740
PatTr 1,848 102,418 106,727 2,201 127,098 108,665
COPPA 664 49,016 39,933
Table 22: Statistics of the in-domain parallel training data allowed for the constrained task (in thousands).
data set English Czech German French
PatTR 121,592 53,242 54,608
UMLS 7,991 63 24 37
Wiki 26,945 1,784 10,232 8,376
AACT 13,341
DrugBank 953
FMA 884
GENIA 557
GREC 62
PIL 662
Table 23: Sizes of monolingual training data allowed for the
constrained tasks (in thousands of tokens).
ical vocabularies and standards (U.S. National Li-
brary of Medicine, 2009). The UMLS dataset
was constructed by selecting the concepts which
have translations in the respective languages. The
Wiki dataset contains bilingual pairs of titles of
Wikipedia articles belonging to the categories
identified to be medical-domain within the Khres-
moi project. It is available for all three lan-
guage pairs. The MuchMore Springer Corpus
is a German?English parallel corpus of medical
journals abstracts published by Springer (Buitelaar
et al., 2003). PatTR is a parallel corpus extracted
from the MAREC patent collection (W?aschle and
Riezler, 2012). It is available for German?English
and French?English. For the medical domain,
we only consider text from patents indicated to
be from the medicine-related categories (A61,
C12N, C12P). COPPA (Corpus of Parallel Patent
Applications (Pouliquen and Mazenc, 2011) is a
French?English parallel corpus extracted from the
MAREC patent collection (W?aschle and Riezler,
2012). The medical-domain subset is identified by
the same categories as in PatTR.
5.3.2 Monolingual Training Data
The medical-domain monolingual data consists of
the following corpora (statistics are presented in
Table 23): The monolingual UMLS dataset con-
tains concept descriptions in CS, DE, and FR ex-
tracted from the UMLS Metathesaurus (see Sec-
tion 5.3.1). The monolingual Wiki dataset con-
sists of articles belonging to the categories iden-
tified to be medical-domain within the Khresmoi
project. The PatTR dataset contains non-parallel
data extracted from the medical patents included
in the PatTR corpus (see Section 5.3.1). AACT is a
collection of restructured and reformatted English
texts publicly available and downloadable from
ClinicalTrials.gov, containing clinical studies con-
ducted around the world. DrugBank is a bioin-
formatics and cheminformatics resource contain-
ing drug descriptions (Knox et al., 2011). GENIA
is a corpus of biomedical literature compiled and
annotated within the GENIA project (Kim et al.,
2003). FMA stands for the Foundational Model
of Anatomy Ontology, a knowledge source for
biomedical informatics concerned with symbolic
representation of the phenotypic structure of the
human body (Rosse and Mejino Jr., 2008). GREC
(Gene Regulation Event Corpus) is a semantically
annotated English corpus of abstracts of biomedi-
cal papers (Thompson et al., 2009). The PIL cor-
pus is a collection of documents giving instruc-
tions to patients about their medication (Bouayad-
Agha et al., 2000).
5.4 Participants
A total of eight teams participated in the Medical
Translation Task by submitting their systems to at
least one subtask for one or more translation direc-
tions. A list of the participants is given in Table 24;
we provide short descriptions of their systems in
the following.
CUNI was involved in the organization of the task,
and their primary goal was to set up a baseline for
both the subtasks and for all translation directions.
44
ID Participating team
CUNI Charles University in Prague (Du?sek et al., 2014)
DCU-Q Dublin City University (Okita et al., 2014)
DCU-S Dublin City University (Zhang et al., 2014)
LIMSI Laboratoire dInformatique pour la Mecanique et les Sciences de lIng?enieur (P?echeux et al., 2014)
POSTECH Pohang University of Science and Technology (Li et al., 2014a)
UEDIN University of Edinburgh (Durrani et al., 2014a)
UM-DA University of Macau (Wang et al., 2014)
UM-WDA University of Macau (Lu et al., 2014)
Table 24: Participants in the WMT14 Medical Translation Task.
Their systems are based on the Moses phrase-
based toolkit and linear interpolation of in-domain
and out-of-domain language models and phrase ta-
bles. The constrained/unconstrained systems dif-
fer in the training data only. The constrained
ones are built using all allowed training data; the
unconstrained ones take advantage of additional
web-crawled monolingual data used for training of
the language models, and additional parallel non-
medical data from the PatTr and COPPA patent
collections.
DCU-Q submitted a system designed specifically
for terminology translation in the query translation
task for EN?FR and FR?EN. This system supports
six terminology extraction methods and is able to
detect rare word pairs including zero-appearance
word pairs. It uses monotonic decoding with lat-
tice inputs, avoiding unnecessary hypothesis ex-
pansions by the reordering model.
DCU-S submitted a system to the FR?EN sum-
mary translation subtask only. The system is
similar to DCU?s system for patent translation
(phrased-based using Moses) but adapted to trans-
late medical summaries and reports.
LIMSI took part in the summary translation sub-
task for English to French.Their primary submis-
sion uses a combination of two translation sys-
tems: NCODE, based on bilingual n-gram trans-
lation models; and an on-the-fly estimation of
the parameters of Moses along with a vector
space model to perform domain adaptation. A
continuous-space language model is also used in
a post-processing step for each system.
POSTECH submitted a phrase-based SMT sys-
tem and query translation system for the DE?EN
language pair in both subtasks. They analysed
three types of query formation, generated query
translation candidates using term-to-term dictio-
naries and a phrase-based system, and then scored
them using a co-occurrence word frequency mea-
sure to select the best candidate.
UEDIN applied the Moses phrase-based system to
all language pairs and both subtasks. They used
the hierarchical reordering model and the OSM
feature, same as in UEDIN?s news translation sys-
tem, and applied compound splitting to German
input. They used separate language models built
on in-domain and out-of-domain data with linear
interpolation. For all language pairs except CS-
EN and DE-EN, they selected data for the transla-
tion model using modified Moore-Lewis filtering.
For DE-EN and CS-EN, they concatenated all the
supplied parallel training data.
UM-DA submitted systems for all language pairs
in the summary translation subtask based on a
combination of different adaptation steps, namely
domain-specific pre-processing, language model
adaptation, translation model adaptation, numeric
adaptation, and hyphenated word adaptation. Data
for the domain-adapted language and translation
models were selected using various data selection
techniques.
UM-WDA submitted systems for all language
pairs in the summary translation subtask. Their
systems are domain-adapted using web-crawled
in-domain resources: bilingual dictionaries and
monolingual data. The translation model and lan-
guage model trained on the crawled data were in-
terpolated with the best-performing language and
translation model employed in the UM-DA sys-
tems.
5.5 Results
MT quality in the Medical Translation Task
is evaluated using automatic evaluation metrics:
BLEU (Papineni et al., 2002), TER (Snover et al.,
2006), PER (Tillmann et al., 1997), and CDER
(Leusch et al., 2006). BLEU scores are reported as
percentage and all error rates are reported as one
minus the original value, also as percentage, so
that all metrics are in the 0-100 range, and higher
scores indicate better translations.
The main reason for not conducting human
evaluation, as it happens in the standard Trans-
45
original normalized truecased normalized lowercased
ID BLEU BLEU 1-TER 1-PER 1-CDER BLEU 1-TER 1-PER 1-CDER
Czech?English
CUNI 29.64 29.79
?
1.07 47.45
?
1.15 61.64
?
1.06 52.18
?
0.98 31.68
?
1.14 49.84
?
1.10 64.38
?
1.06 54.10
?
0.96
CUNI 22.44 22.57
?
0.95 41.43
?
1.16 55.46
?
1.09 46.42
?
0.96 32.34
?
1.12 50.24
?
1.20 65.07
?
1.10 54.42
?
0.96
UEDIN 36.65 36.87
?
1.23 54.35
?
1.19 67.16
?
1.00 57.61
?
1.01 38.02
?
1.24 56.14
?
1.17 69.24
?
1.01 58.96
?
0.96
UM-DA 37.62 37.79
?
1.26 54.55
?
1.20 68.29
?
0.88 57.28
?
1.03 38.81
?
1.28 56.04
?
1.20 70.06
?
0.82 58.45
?
1.05
CUNI 22.92 23.06
?
0.97 42.49
?
1.10 56.10
?
1.12 47.13
?
0.95 33.18
?
1.15 51.48
?
1.15 66.00
?
1.03 55.30
?
0.96
CUNI 22.69 22.84
?
0.98 42.21
?
1.14 56.01
?
1.11 46.79
?
0.94 32.84
?
1.13 51.10
?
1.11 65.79
?
1.07 54.81
?
0.96
UM-WDA 37.35 37.53
?
1.26 54.39
?
1.19 68.21
?
0.83 57.16
?
1.07 38.61
?
1.27 55.92
?
1.17 70.02
?
0.81 58.36
?
1.07
ONLINE 39.57
?
1.21 58.24
?
1.14 70.16
?
0.78 60.04
?
1.02 40.62
?
1.23 59.72
?
1.11 71.94
?
0.74 61.26
?
1.01
German?English
CUNI 28.20 28.34
?
1.12 46.66
?
1.13 61.53
?
1.03 50.57
?
0.93 30.69
?
1.19 48.91
?
1.16 64.12
?
1.04 52.52
?
0.95
CUNI 28.85 28.99
?
1.15 47.12
?
1.15 61.98
?
1.07 50.72
?
0.98 31.37
?
1.21 49.29
?
1.13 64.53
?
1.05 52.64
?
0.98
POSTECH 25.92 25.99
?
1.06 43.66
?
1.14 59.62
?
0.92 47.13
?
0.90 26.97
?
1.06 45.13
?
1.12 61.53
?
0.89 48.37
?
0.88
UEDIN 37.31 37.53
?
1.19 55.72
?
1.14 68.82
?
0.99 58.35
?
0.95 38.60
?
1.25 57.18
?
1.12 70.46
?
0.98 59.53
?
0.94
UM-DA 35.71 35.81
?
1.23 53.08
?
1.16 66.82
?
0.98 55.91
?
0.96 36.55
?
1.27 54.01
?
1.13 68.05
?
0.97 56.78
?
0.95
CUNI 30.58 30.71
?
1.10 48.68
?
1.09 63.19
?
1.08 52.72
?
0.94 33.14
?
1.19 50.98
?
1.06 65.88
?
1.04 54.74
?
0.94
CUNI 30.22 30.32
?
1.12 47.71
?
1.18 62.20
?
1.10 52.17
?
0.91 32.75
?
1.20 50.00
?
1.14 64.87
?
1.06 54.19
?
0.92
UM-WDA 32.70 32.88
?
1.19 49.60
?
1.18 63.74
?
1.01 53.50
?
0.96 33.95
?
1.23 51.05
?
1.19 65.54
?
0.98 54.73
?
0.96
ONLINE 41.18
?
1.24 59.33
?
1.09 70.95
?
0.92 61.92
?
1.01 42.29
?
1.23 60.76
?
1.08 72.51
?
0.88 63.06
?
0.96
French?English
CUNI 34.42 34.55
?
1.20 52.24
?
1.17 64.52
?
1.03 56.48
?
0.91 36.52
?
1.23 54.35
?
1.12 67.07
?
1.00 58.34
?
0.91
CUNI 33.67 33.59
?
1.16 50.39
?
1.23 61.75
?
1.16 56.74
?
0.97 35.55
?
1.21 52.55
?
1.26 64.45
?
1.13 58.63
?
0.91
DCU-B 44.85 45.01
?
1.24 62.57
?
1.12 74.11
?
0.78 64.33
?
0.99 46.12
?
1.26 64.04
?
1.06 75.84
?
0.74 65.55
?
0.94
UEDIN 46.44 46.68
?
1.26 64.12
?
1.16 74.47
?
0.87 66.40
?
0.96 48.01
?
1.29 65.70
?
1.15 76.30
?
0.86 67.76
?
0.91
UM-DA 47.08 47.22
?
1.33 64.08
?
1.16 75.41
?
0.88 66.15
?
0.96 48.23
?
1.31 65.36
?
1.10 76.95
?
0.89 67.18
?
0.93
CUNI 34.74 34.89
?
1.12 52.39
?
1.16 63.76
?
1.09 57.29
?
0.94 36.84
?
1.17 54.56
?
1.13 66.43
?
1.07 59.14
?
0.90
CUNI 35.04 34.99
?
1.18 52.11
?
1.24 63.24
?
1.09 57.51
?
0.97 37.04
?
1.18 54.38
?
1.17 66.02
?
1.05 59.55
?
0.93
UM-WDA 43.84 44.06
?
1.32 61.14
?
1.18 73.13
?
0.87 63.09
?
1.00 45.17
?
1.36 62.63
?
1.15 74.94
?
0.84 64.37
?
0.99
ONLINE 46.99
?
1.35 64.31
?
1.12 76.07
?
0.78 66.09
?
1.00 47.99
?
1.33 65.65
?
1.07 77.65
?
0.75 67.20
?
0.96
English?Czech
CUNI 17.36 17.65
?
0.96 37.17
?
1.02 49.13
?
0.98 40.31
?
0.95 18.75
?
0.96 38.32
?
1.02 50.82
?
0.91 41.39
?
0.94
CUNI 16.64 16.89
?
0.93 36.57
?
1.05 48.79
?
0.98 39.46
?
0.90 17.94
?
0.96 37.74
?
1.03 50.50
?
0.97 40.59
?
0.91
UEDIN 23.45 23.74
?
1.00 44.20
?
1.10 55.38
?
0.88 46.23
?
0.99 24.20
?
1.00 44.92
?
1.08 56.38
?
0.90 46.78
?
1.00
UM-DA 22.61 22.72
?
0.98 42.73
?
1.16 54.12
?
0.93 44.73
?
1.01 23.12
?
1.01 43.41
?
1.14 55.11
?
0.93 45.32
?
1.02
CUNI 20.56 20.84
?
1.01 39.98
?
1.09 51.98
?
0.99 42.86
?
1.00 22.03
?
1.05 41.19
?
1.08 53.66
?
0.97 43.93
?
1.01
CUNI 19.50 19.72
?
0.97 38.09
?
1.10 50.12
?
1.06 41.50
?
0.96 20.91
?
1.02 39.26
?
1.12 51.79
?
1.04 42.59
?
0.96
UM-WDA 22.14 22.33
?
0.96 42.30
?
1.11 53.89
?
0.92 44.48
?
1.01 22.72
?
0.97 43.02
?
1.09 54.89
?
0.95 45.08
?
0.99
ONLINE 33.45
?
1.28 51.64
?
1.28 61.82
?
1.10 53.97
?
1.18 34.02
?
1.31 52.35
?
1.22 62.84
?
1.08 54.52
?
1.18
English?German
CUNI 12.52 12.64
?
0.77 29.84
?
0.99 45.38
?
1.14 34.69
?
0.81 16.63
?
0.91 33.63
?
1.07 50.03
?
1.24 38.43
?
0.87
CUNI 12.42 12.53
?
0.77 29.02
?
1.05 44.27
?
1.16 34.62
?
0.78 16.41
?
0.91 32.87
?
1.08 48.99
?
1.21 38.37
?
0.86
POSTECH 15.46 15.59
?
0.91 34.41
?
1.01 49.00
?
0.83 37.11
?
0.90 15.98
?
0.92 34.98
?
1.00 49.94
?
0.81 37.60
?
0.87
UEDIN 20.88 21.01
?
1.03 40.03
?
1.08 55.54
?
0.91 42.95
?
0.90 21.40
?
1.03 40.55
?
1.08 56.33
?
0.92 43.41
?
0.90
UM-DA 20.89 21.09
?
1.07 40.76
?
1.03 55.45
?
0.89 43.02
?
0.93 21.52
?
1.08 41.31
?
1.01 56.38
?
0.90 43.58
?
0.91
CUNI 14.29 14.42
?
0.81 31.82
?
1.03 47.01
?
1.13 36.81
?
0.79 18.87
?
0.90 35.76
?
1.11 51.76
?
1.17 40.65
?
0.87
CUNI 13.44 13.58
?
0.75 30.37
?
1.03 45.80
?
1.14 35.80
?
0.76 17.84
?
0.89 34.41
?
1.13 50.75
?
1.18 39.85
?
0.78
UM-WDA 18.77 18.91
?
1.00 37.92
?
1.02 53.59
?
0.85 40.90
?
0.86 19.30
?
1.02 38.42
?
1.01 54.40
?
0.85 41.34
?
0.86
ONLINE 23.92
?
1.06 44.33
?
0.97 57.47
?
0.80 46.35
?
0.91 24.29
?
1.07 44.83
?
0.98 58.20
?
0.80 46.71
?
0.92
English?French
CUNI 30.30 30.67
?
1.11 46.59
?
1.09 59.83
?
1.04 50.51
?
0.93 32.06
?
1.12 48.01
?
1.09 61.66
?
1.00 51.83
?
0.94
CUNI 29.35 29.71
?
1.10 45.84
?
1.07 58.81
?
1.04 50.00
?
0.96 31.02
?
1.10 47.24
?
1.09 60.57
?
1.02 51.31
?
0.94
LIMSI 40.14 43.54
?
1.22 59.70
?
1.04 69.45
?
0.86 61.35
?
0.96 44.04
?
1.22 60.32
?
1.03 70.20
?
0.85 61.90
?
0.94
LIMSI 38.83 42.21
?
1.13 58.88
?
1.01 68.70
?
0.81 60.59
?
0.93 42.69
?
1.12 59.53
?
0.98 69.50
?
0.80 61.17
?
0.91
UEDIN 40.74 44.24
?
1.16 60.66
?
1.07 70.35
?
0.82 62.28
?
0.95 44.85
?
1.17 61.43
?
1.05 71.27
?
0.81 62.94
?
0.91
UM-DA 41.24 41.68
?
1.12 58.72
?
1.06 69.37
?
0.78 60.12
?
0.95 42.16
?
1.11 59.39
?
1.05 70.21
?
0.77 60.71
?
0.92
CUNI 32.23 32.61
?
1.09 48.48
?
1.08 61.13
?
1.01 52.24
?
0.93 34.08
?
1.10 49.93
?
1.11 62.92
?
0.99 53.65
?
0.92
CUNI 32.45 32.84
?
1.06 48.68
?
1.06 61.32
?
0.98 52.35
?
0.94 34.22
?
1.07 50.09
?
1.04 63.04
?
0.96 53.67
?
0.91
UM-WDA 40.78 41.16
?
1.13 58.20
?
0.99 68.93
?
0.84 59.64
?
0.94 41.79
?
1.12 59.10
?
0.96 70.01
?
0.84 60.39
?
0.91
ONLINE 58.63
?
1.26 70.70
?
1.12 78.22
?
0.81 71.89
?
0.96 59.27
?
1.26 71.50
?
1.10 79.16
?
0.81 72.63
?
0.94
Table 25: Official results of translation quality evaluation in the medical summary translation subtask.
46
original normalized truecased normalized lowercased
ID BLEU BLEU 1-TER 1-PER 1-CDER BLEU 1-TER 1-PER 1-CDER
Czech?English
CUNI 10.71 10.57
?
3.42 15.72
?
2.77 23.37
?
3.03 18.68
?
2.42 30.13
?
4.85 53.38
?
3.01 62.53
?
2.84 55.44
?
2.87
CUNI 9.92 9.78
?
3.04 16.84
?
2.84 23.80
?
3.08 19.85
?
2.40 28.21
?
4.56 54.15
?
3.04 62.56
?
2.99 55.91
?
2.79
UEDIN 24.66 24.68
?
4.52 39.88
?
3.05 49.97
?
3.29 41.81
?
2.80 28.25
?
4.94 45.31
?
3.14 55.66
?
3.06 46.67
?
2.77
CUNI 12.00 11.86
?
3.42 18.49
?
2.74 24.67
?
2.85 21.08
?
2.29 31.91
?
4.81 57.61
?
3.13 65.02
?
2.99 59.24
?
2.69
CUNI 10.54 10.39
?
3.48 18.86
?
2.48 26.65
?
2.05 20.53
?
2.08 32.39
?
5.45 56.79
?
3.02 65.52
?
2.26 57.96
?
2.56
ONLINE 28.88
?
4.96 47.31
?
3.35 55.19
?
3.21 49.88
?
2.89 35.33
?
5.20 55.80
?
3.20 64.05
?
2.97 57.94
?
2.85
German?English
CUNI 10.90 10.74
?
3.41 18.89
?
2.39 26.09
?
2.00 20.29
?
2.07 32.15
?
5.23 55.56
?
2.90 63.68
?
2.34 56.45
?
2.62
CUNI 10.71 10.55
?
3.47 18.40
?
2.35 25.45
?
2.04 19.84
?
2.07 32.06
?
5.19 54.85
?
2.91 62.87
?
2.39 55.52
?
2.61
POSTECH 18.06 17.97
?
4.38 28.57
?
3.30 40.38
?
2.77 31.79
?
2.80 21.99
?
4.65 35.76
?
3.35 47.84
?
2.82 38.84
?
2.92
POSTECH 17.99 17.88
?
4.72 29.79
?
3.04 41.15
?
2.48 32.49
?
2.63 24.41
?
4.83 41.72
?
3.19 53.33
?
2.55 44.06
?
2.88
UEDIN 23.33 23.39
?
4.37 38.55
?
3.65 48.21
?
3.43 40.75
?
3.05 27.17
?
4.63 43.87
?
3.52 53.76
?
3.48 45.72
?
3.03
CUNI 10.54 10.39
?
3.48 18.86
?
2.48 26.65
?
2.05 20.53
?
2.08 32.39
?
5.45 56.79
?
3.02 65.52
?
2.26 57.96
?
2.56
CUNI 8.75 8.49
?
3.60 19.10
?
2.27 24.98
?
1.95 19.95
?
2.02 30.00
?
5.59 56.07
?
2.92 62.92
?
2.32 56.27
?
2.56
ONLINE 19.97
?
4.46 37.03
?
3.26 43.91
?
3.22 40.95
?
2.93 33.86
?
4.87 53.28
?
3.28 60.86
?
3.22 56.33
?
2.98
French?English
CUNI 13.90 13.79
?
3.61 18.49
?
2.55 28.35
?
2.81 20.36
?
2.20 34.97
?
5.34 59.54
?
2.94 72.30
?
2.63 58.86
?
2.76
CUNI 12.10 11.95
?
3.41 17.23
?
2.57 27.12
?
2.88 19.15
?
2.28 33.74
?
5.01 58.95
?
2.96 71.25
?
2.76 58.20
?
2.81
DCU-Q 30.85 31.24
?
5.08 58.88
?
2.97 67.94
?
2.62 59.19
?
2.62 36.88
?
5.07 66.38
?
2.85 75.86
?
2.37 66.29
?
2.55
DCU-Q 26.51 26.16
?
4.40 48.02
?
3.72 57.34
?
3.24 53.56
?
2.79 28.61
?
4.52 53.65
?
3.73 63.51
?
3.21 59.07
?
2.79
UEDIN 27.20 27.60
?
3.98 38.54
?
3.22 48.81
?
3.26 39.77
?
2.95 32.23
?
4.27 43.66
?
3.20 54.31
?
3.17 44.53
?
2.79
CUNI 14.03 14.00
?
3.30 20.11
?
2.38 29.00
?
2.71 21.62
?
2.22 38.98
?
5.08 62.90
?
2.87 74.49
?
2.45 62.12
?
2.64
CUNI 13.38 13.16
?
3.52 17.79
?
2.56 28.84
?
2.81 19.17
?
2.23 35.00
?
5.20 59.52
?
2.98 73.08
?
2.57 58.41
?
2.68
ONLINE 32.96
?
5.04 53.68
?
3.21 64.27
?
2.80 54.40
?
2.66 38.09
?
5.52 61.44
?
3.08 72.59
?
2.61 61.60
?
2.78
English?Czech
CUNI 8.37 8.00
?
3.65 17.74
?
2.23 26.46
?
1.96 19.48
?
2.10 19.49
?
4.60 41.53
?
2.94 51.34
?
2.51 42.54
?
2.74
CUNI 9.04 8.75
?
3.64 18.25
?
2.27 26.97
?
1.92 19.69
?
2.11 21.46
?
5.05 42.36
?
3.09 51.99
?
2.40 43.18
?
2.68
UEDIN 12.57 12.40
?
3.61 21.15
?
2.96 33.56
?
2.80 22.30
?
2.67 14.06
?
3.80 24.92
?
2.90 37.85
?
2.72 25.58
?
2.70
UEDIN 6.64 6.21
?
4.73 -2.35
?
3.06 5.95
?
3.48 -0.97
?
3.12 14.35
?
3.52 14.51
?
3.19 24.96
?
3.50 15.11
?
3.10
CUNI 9.06 8.64
?
3.82 19.92
?
2.24 26.97
?
1.94 20.82
?
2.06 22.42
?
5.24 44.89
?
2.94 52.89
?
2.40 45.36
?
2.78
CUNI 8.49 8.01
?
6.05 18.13
?
2.28 25.19
?
1.86 19.19
?
2.01 21.04
?
4.80 42.66
?
2.87 50.34
?
2.47 43.30
?
2.74
ONLINE 21.09
?
4.60 48.56
?
2.82 54.72
?
2.51 48.30
?
2.83 24.37
?
4.80 51.93
?
2.74 58.10
?
2.50 51.62
?
2.80
English?German
CUNI 10.17 10.01
?
3.92 26.48
?
3.24 36.71
?
3.37 29.26
?
2.96 13.02
?
4.17 31.96
?
3.41 42.39
?
3.21 34.61
?
2.95
CUNI 9.98 9.69
?
3.94 26.16
?
3.19 35.50
?
3.23 28.86
?
2.94 12.90
?
4.28 31.75
?
3.33 41.24
?
3.21 34.38
?
3.05
POSTECH 13.43 13.01
?
5.91 26.38
?
3.09 35.75
?
3.16 27.86
?
2.82 15.05
?
5.71 30.45
?
3.10 39.89
?
3.14 31.79
?
3.00
POSTECH 13.41 13.15
?
5.21 22.18
?
3.09 30.89
?
3.31 24.17
?
3.06 14.96
?
5.15 26.13
?
3.19 34.92
?
3.40 27.98
?
3.12
UEDIN 10.45 10.14
?
3.86 23.44
?
3.43 34.55
?
3.34 25.46
?
3.17 11.91
?
4.42 27.91
?
3.45 39.08
?
3.42 29.63
?
3.31
CUNI 8.91 7.72
?
6.48 30.05
?
3.22 40.65
?
2.71 31.91
?
2.88 13.66
?
5.37 35.51
?
3.28 46.12
?
2.74 37.27
?
3.01
CUNI 9.14 8.69
?
6.44 27.66
?
3.31 37.95
?
3.45 31.00
?
2.82 14.03
?
5.92 33.53
?
3.45 44.03
?
3.53 36.73
?
3.00
ONLINE 20.07
?
6.06 41.07
?
3.23 47.41
?
2.86 41.61
?
3.02 21.67
?
6.23 43.78
?
3.23 50.18
?
2.95 44.26
?
3.06
English?French
CUNI 13.12 12.92
?
2.84 21.95
?
2.41 33.19
?
2.09 23.70
?
2.24 28.42
?
3.98 51.43
?
2.90 63.74
?
2.35 52.64
?
2.58
CUNI 12.80 12.65
?
2.81 19.16
?
2.61 31.61
?
2.21 21.91
?
2.32 27.52
?
4.05 47.47
?
3.08 61.43
?
2.37 49.82
?
2.72
DCU-Q 27.69 27.84
?
4.11 48.97
?
3.06 60.90
?
2.55 51.84
?
2.83 28.98
?
4.16 51.73
?
3.10 63.84
?
2.47 54.43
?
2.76
UEDIN 20.16 21.76
?
3.42 31.66
?
4.23 44.37
?
4.13 44.29
?
2.73 23.25
?
3.49 35.38
?
4.19 48.52
?
4.07 47.94
?
2.75
CUNI 13.78 13.57
?
3.00 21.92
?
2.51 33.47
?
2.03 24.16
?
2.32 30.07
?
4.10 51.12
?
3.08 63.61
?
2.45 52.96
?
2.67
CUNI 15.27 15.24
?
3.12 23.58
?
2.54 34.39
?
2.54 25.79
?
2.32 31.40
?
4.15 53.60
?
2.96 65.39
?
2.57 55.47
?
2.69
ONLINE 28.93
?
3.66 49.20
?
3.08 60.85
?
2.69 51.68
?
2.78 30.88
?
3.66 52.25
?
3.08 64.06
?
2.62 54.59
?
2.68
Table 26: Official results of translation quality evaluation in the medical query translation subtask.
source lang. ID P@5 P@10 NDCG@5 NDCG@10 MAP Rprec bpref rel
Czech?English CUNI 0.3280 0.3340 0.2873 0.2936 0.2217 0.2362 0.3473 1461
German?English CUNI 0.2800 0.3000 0.2467 0.2630 0.2057 0.2077 0.3310 1426
French?English CUNI 0.3280 0.3380 0.2811 0.2882 0.2206 0.2284 0.3504 1481
DCU-Q 0.3480 0.3460 0.3060 0.3072 0.2252 0.2358 0.3659 1524
UEDIN 0.4440 0.4300 0.3793 0.3826 0.2843 0.2935 0.3936 1544
English (monolingual) 0.4600 0.4700 0.4091 0.4205 0.3035 0.3198 0.3858 1638
Table 27: Official results of retrieval evaluation in the query translation subtask.
47
lation Task, was the lack of domain expertise of
prospective raters. While in the standard task, the
only requirement for the raters was to be a na-
tive speaker of the target language, in the Med-
ical Translation Task, a very good knowledge of
the domain would be necessary to provide reli-
able judgements and the raters with such an ex-
pertise (medical doctors and native speakers) were
not available.
The complete results of the task are presented
in Table 25 (for summary translation) and Ta-
bles 26 and 27 (for query translation). Partici-
pant IDs given in bold indicate primary submis-
sions, IDs in normal font refer to contrastive sub-
missions. The first section for each translation di-
rection (white background) refers to constrained
submissions and the second one (light-gray back-
ground) to unconstrained submissions. The col-
umn denoted as ?original? contains BLEU scores
as reported by the Matrix submission system ob-
tained on the original submitted translations. Due
to punctuation inconsistency in the original refer-
ence translations, we decided to perform punctu-
ation normalization before calculating the official
scores. The columns denoted as ?normalized true-
cased? contain scores obtained on the submitted
translations after punctuation normalization and
the columns denoted as ?normalized lowercased?
contain scores obtained after punctuation normal-
ization and lowercasing. The normalization script
is available in the package with summary transla-
tion test data. The confidence intervals were ob-
tained by bootstrap resampling with a confidence
level of 95%. Figures in bold denote the best con-
strained system and, if its score is higher, the best
unconstrained system for each translation direc-
tion and each metric. For comparison, we also
present results of a major on-line translation sys-
tem (denoted as ONLINE).
The results of the extrinsic evaluation of query
translation submissions are given in 27. We used
the CLEF 2013 eHealth Task 3 test collection con-
taining about 1 million web pages (in English),
50 test queries (originally in English and trans-
lated to Czech, German, and French), and their
relevance assessments. Some of the participants
of the WMT Medical Task (three teams with five
submissions in total) submitted translations of the
queries (from Czech, German, and French) into
English and these translations were used to query
the CLEF 2013 eHealth Task 3 test collection us-
ing a state-of-the-art system based on a BM25
model, described in Pecina et al. (2014). Origi-
nally, we asked for 10 best translations for each
query, but only the best one were used for the
evaluation. The results are provided in terms of
standard IR evaluation measures: precision at a
cut-off of 5 and 10 documents (P@5, P@10),
normalized discounted cumulative gain (J?arvelin
and Kek?al?ainen, 2002) at 5 and 10 documents
(NDCG@5, NDCG@10), mean average precision
(MAP) (Voorhees and Harman, 2005), precision
reached after R documents retrieved, where R in-
dicates the number of the relevant documents for
each query in the entire collection (Rprec), binary
preference (bpref) (Buckley and Voorhees, 2004),
and number or relevant documents retrieved (rel).
The cross-lingual results are also compared with
the monolingual one (obtained by using the refer-
ence (English) translations of the test topics) to see
how the system would perform if the queries were
translated perfectly.
5.6 Discussion and Conclusion
Both the subtasks turned out to be quite challeng-
ing not only because of the specific domain ? in
summary sentences, we can observe much higher
density of terminology than in ordinary sentences;
the queries, which are also rich in terminology, do
not form sentences at all.
Most submissions were based on systems par-
ticipating in the standard Translation Task and
trained on the provided data or its subsets CUNI
provided baseline systems for all language pairs in
both subtasks, which turned to be relatively strong
for the query translation task, especially in trans-
lation to English, but only in terms of scores ob-
tained on normalized and lowercased translations
since their truecasing component did not perform
well.
In the summary translation subtask, the best
overall results were achieved by the UEDIN team
which won for DE?EN, EN?CS, and EN?FR, fol-
lowed by the UM-DA team, which performed on
par with UEDIN in all other translation.
The unconstrained submissions in almost all
cases did not outperform the results of the con-
strained submissions. Some improvements were
observed in the query translations subtasks by the
CUNI?s unconstrained system with language mod-
els trained on larger in-domain data.
The ONLINE system outperforms all other sub-
48
missions with only two exceptions ? the UM-DA?s
and UEDIN?s systems for the summary translation
in the FR?EN direction, though the score differ-
ences are within the 95% confidence interval.
In the query translation subtask, DCU-Q built
a system designed specifically for terminology
translation between French and English and out-
performed all other participants in translation into
English; however, the confidence intervals in the
query translation task are much wider and most of
the differences in scores of the automatic metrics
are not statistically significant.
The extrinsic evaluation in the cross-lingual in-
formation retrieval was conducted for translations
into English only. CUNI provided the baselines
for all directions, but other submissions were done
for FR?EN only. Here, the winner is UEDIN, who
outperformed both CUNI and DCU-Q, and their
scores are very close to those obtained using the
reference English translations.
Acknowledgments
This work was supported in parts by the
MosesCore, Casmacat, Khresmoi, Matecat and
QTLaunchPad projects funded by the European
Commission (7th Framework Programme), and by
gifts from Yandex.
We would also like to thank our colleagues Ma-
tou?s Mach?a?cek and Martin Popel for detailed dis-
cussions.
References
Avramidis, E. (2014). Efforts on machine learning
over human-mediated translation edit rate. In
Proceedings of the Ninth Workshop on Statisti-
cal Machine Translation, Baltimore, Maryland,
USA. Association for Computational Linguis-
tics.
Beck, D., Shah, K., and Specia, L. (2014). Shef-
lite 2.0: Sparse multi-task gaussian processes
for translation quality estimation. In Proceed-
ings of the Ninth Workshop on Statistical Ma-
chine Translation, Baltimore, Maryland, USA.
Association for Computational Linguistics.
Bic?ici, E. (2013). Referential translation machines
for quality estimation. In Proceedings of the
Eighth Workshop on Statistical Machine Trans-
lation, Sofia, Bulgaria.
Bic?ici, E., Liu, Q., and Way, A. (2014). Parallel
FDA5 for fast deployment of accurate statisti-
cal machine translation systems. In Proceed-
ings of the Ninth Workshop on Statistical Ma-
chine Translation, Baltimore, USA. Association
for Computational Linguistics.
Bicici, E., Liu, Q., and Way, A. (2014). Parallel
fda5 for fast deployment of accurate statistical
machine translation systems. In Proceedings
of the Ninth Workshop on Statistical Machine
Translation, Baltimore, Maryland, USA. Asso-
ciation for Computational Linguistics.
Bicici, E. and Way, A. (2014). Referential transla-
tion machines for predicting translation quality.
In Proceedings of the Ninth Workshop on Sta-
tistical Machine Translation, Baltimore, Mary-
land, USA. Association for Computational Lin-
guistics.
Bojar, O., Buck, C., Callison-Burch, C., Feder-
mann, C., Haddow, B., Koehn, P., Monz, C.,
Post, M., Soricut, R., and Specia, L. (2013).
Findings of the 2013 Workshop on Statistical
Machine Translation. In Proceedings of the
Eighth Workshop on Statistical Machine Trans-
lation, pages 1?42, Sofia, Bulgaria. Association
for Computational Linguistics.
Bojar, O., Diatka, V., Rychl?y, P., Stra?n?ak, P.,
Tamchyna, A., and Zeman, D. (2014). Hindi-
English and Hindi-only Corpus for Machine
Translation. In Proceedings of the Ninth Inter-
national Language Resources and Evaluation
Conference, Reykjavik, Iceland. ELRA.
Bojar, O., Ercegov?cevi?c, M., Popel, M., and
Zaidan, O. (2011). A grain of salt for the WMT
manual evaluation. In Proceedings of the Sixth
Workshop on Statistical Machine Translation,
pages 1?11, Edinburgh, Scotland. Association
for Computational Linguistics.
Borisov, A. and Galinskaya, I. (2014). Yandex
school of data analysis russian-english machine
translation system for wmt14. In Proceedings
of the Ninth Workshop on Statistical Machine
Translation, Baltimore, Maryland, USA. Asso-
ciation for Computational Linguistics.
Bouayad-Agha, N., Scott, D. R., and Power, R.
(2000). Integrating content and style in doc-
uments: A case study of patient information
leaflets. Information Design Journal, 9(2?
3):161?176.
Buckley, C. and Voorhees, E. M. (2004). Re-
trieval evaluation with incomplete information.
49
In Proceedings of the 27th Annual International
ACM SIGIR Conference on Research and De-
velopment in Information Retrieval, pages 25?
32, Sheffield, United Kingdom.
Buitelaar, P., Sacaleanu, B.,
?
Spela Vintar, Stef-
fen, D., Volk, M., Dejean, H., Gaussier, E.,
Widdows, D., Weiser, O., and Frederking, R.
(2003). Multilingual concept hierarchies for
medical information organization and retrieval.
Public deliverable, MuchMore project.
Callison-Burch, C., Fordyce, C., Koehn, P., Monz,
C., and Schroeder, J. (2007). (Meta-) evaluation
of machine translation. In Proceedings of the
Second Workshop on Statistical Machine Trans-
lation (WMT07), Prague, Czech Republic.
Callison-Burch, C., Fordyce, C., Koehn, P., Monz,
C., and Schroeder, J. (2008). Further meta-
evaluation of machine translation. In Proceed-
ings of the Third Workshop on Statistical Ma-
chine Translation (WMT08), Colmbus, Ohio.
Callison-Burch, C., Koehn, P., Monz, C., Pe-
terson, K., Przybocki, M., and Zaidan, O. F.
(2010). Findings of the 2010 joint workshop
on statistical machine translation and metrics
for machine translation. In Proceedings of the
Fourth Workshop on Statistical Machine Trans-
lation (WMT10), Uppsala, Sweden.
Callison-Burch, C., Koehn, P., Monz, C., Post, M.,
Soricut, R., and Specia, L. (2012). Findings of
the 2012 workshop on statistical machine trans-
lation. In Proceedings of the Seventh Workshop
on Statistical Machine Translation, pages 10?
51, Montr?eal, Canada. Association for Compu-
tational Linguistics.
Callison-Burch, C., Koehn, P., Monz, C., and
Schroeder, J. (2009). Findings of the 2009
workshop on statistical machine translation. In
Proceedings of the Fourth Workshop on Sta-
tistical Machine Translation (WMT09), Athens,
Greece.
Callison-Burch, C., Koehn, P., Monz, C., and
Zaidan, O. (2011). Findings of the 2011 work-
shop on statistical machine translation. In Pro-
ceedings of the Sixth Workshop on Statistical
Machine Translation, pages 22?64, Edinburgh,
Scotland.
Camargo de Souza, J. G., Gonz?alez-Rubio, J.,
Buck, C., Turchi, M., and Negri, M. (2014).
Fbk-upv-uedin participation in the wmt14 qual-
ity estimation shared-task. In Proceedings of the
Ninth Workshop on Statistical Machine Trans-
lation, Baltimore, Maryland, USA. Association
for Computational Linguistics.
Cap, F., Weller, M., Ramm, A., and Fraser, A.
(2014). Cims ? the cis and ims joint submis-
sion to wmt 2014 translating from english into
german. In Proceedings of the Ninth Workshop
on Statistical Machine Translation, Baltimore,
Maryland, USA. Association for Computational
Linguistics.
Cohen, J. (1960). A coefficient of agreement for
nominal scales. Educational and Psychological
Measurment, 20(1):37?46.
Cohn, T. and Specia, L. (2013). Modelling an-
notator bias with multi-task gaussian processes:
An application to machine translation quality
estimation. In Proceedings of the 51st An-
nual Meeting of the Association for Compu-
tational Linguistics, ACL-2013, pages 32?42,
Sofia, Bulgaria.
Collins, M. (2002). Discriminative training meth-
ods for hidden markov models: theory and ex-
periments with perceptron algorithms. In Pro-
ceedings of the ACL-02 conference on Empir-
ical methods in natural language processing -
Volume 10, EMNLP ?02, pages 1?8, Strouds-
burg, PA, USA. Association for Computational
Linguistics.
Costa-juss`a, M. R., Gupta, P., Rosso, P., and
Banchs, R. E. (2014). English-to-hindi sys-
tem description for wmt 2014: Deep source-
context features for moses. In Proceedings
of the Ninth Workshop on Statistical Machine
Translation, Baltimore, Maryland, USA. Asso-
ciation for Computational Linguistics.
Do, Q. K., Herrmann, T., Niehues, J., Allauzen,
A., Yvon, F., and Waibel, A. (2014). The
kit-limsi translation system for wmt 2014. In
Proceedings of the Ninth Workshop on Statisti-
cal Machine Translation, Baltimore, Maryland,
USA. Association for Computational Linguis-
tics.
Dungarwal, P., Chatterjee, R., Mishra, A.,
Kunchukuttan, A., Shah, R., and Bhattacharyya,
P. (2014). The iit bombay hindi-english transla-
tion system at wmt 2014. In Proceedings of the
Ninth Workshop on Statistical Machine Trans-
lation, Baltimore, Maryland, USA. Association
for Computational Linguistics.
50
Durrani, N., Haddow, B., Koehn, P., and Heafield,
K. (2014a). Edinburgh?s phrase-based machine
translation systems for wmt-14. In Proceedings
of the ACL 2014 Ninth Workshop of Statistical
Machine Translation, Baltimore, USA.
Durrani, N., Haddow, B., Koehn, P., and Heafield,
K. (2014b). Edinburghs phrase-based machine
translation systems for wmt-14. In Proceedings
of the Ninth Workshop on Statistical Machine
Translation, Baltimore, Maryland, USA. Asso-
ciation for Computational Linguistics.
Du?sek, O., Haji?c, J., Hlav?a?cov?a, J., Nov?ak, M.,
Pecina, P., Rosa, R., Tamchyna, A., Ure?sov?a,
Z., and Zeman, D. (2014). Machine transla-
tion of medical texts in the khresmoi project. In
Proceedings of the ACL 2014 Ninth Workshop
of Statistical Machine Translation, Baltimore,
USA.
Federmann, C. (2012). Appraise: An Open-
Source Toolkit for Manual Evaluation of Ma-
chine Translation Output. The Prague Bulletin
of Mathematical Linguistics (PBML), 98:25?
35.
Foster, J. (2007). Treebanks gone bad: Parser eval-
uation and retraining using a treebank of un-
grammatical sentences. International Journal
on Document Analysis and Recognition, 10(3-
4):129?145.
Freitag, M., Peitz, S., Wuebker, J., Ney, H., Huck,
M., Sennrich, R., Durrani, N., Nadejde, M.,
Williams, P., Koehn, P., Herrmann, T., Cho,
E., and Waibel, A. (2014). Eu-bridge mt:
Combined machine translation. In Proceedings
of the Ninth Workshop on Statistical Machine
Translation, Baltimore, Maryland, USA. Asso-
ciation for Computational Linguistics.
Gamon, M., Aue, A., and Smets, M. (2005).
Sentence-level MT evaluation without reference
translations: beyond language modeling. In
Proceedings of the Annual Conference of the
European Association for Machine Translation,
Budapest.
Geurts, P., Ernst, D., and Wehenkel, L. (2006). Ex-
tremely randomized trees. Machine Learning,
63(1):3?42.
Green, S., Cer, D., and Manning, C. (2014).
Phrasal: A toolkit for new directions in statis-
tical machine translation. In Proceedings of the
Ninth Workshop on Statistical Machine Trans-
lation, Baltimore, Maryland, USA. Association
for Computational Linguistics.
Hardmeier, C., Stymne, S., Tiedemann, J., Smith,
A., and Nivre, J. (2014). Anaphora models and
reordering for phrase-based smt. In Proceed-
ings of the Ninth Workshop on Statistical Ma-
chine Translation, Baltimore, Maryland, USA.
Association for Computational Linguistics.
Herbrich, R., Minka, T., and Graepel, T. (2006).
TrueSkill
TM
: A Bayesian Skill Rating Sys-
tem. In Proceedings of the Twentieth Annual
Conference on Neural Information Processing
Systems, pages 569?576, Vancouver, British
Columbia, Canada. MIT Press.
Herrmann, T., Mediani, M., Cho, E., Ha, T.-L.,
Niehues, J., Slawik, I., Zhang, Y., and Waibel,
A. (2014). The karlsruhe institute of technol-
ogy translation systems for the wmt 2014. In
Proceedings of the Ninth Workshop on Statisti-
cal Machine Translation, Baltimore, Maryland,
USA. Association for Computational Linguis-
tics.
Hokamp, C., Calixto, I., Wagner, J., and Zhang,
J. (2014). Target-centric features for transla-
tion quality estimation. In Proceedings of the
Ninth Workshop on Statistical Machine Trans-
lation, Baltimore, Maryland, USA. Association
for Computational Linguistics.
Hopkins, M. and May, J. (2013). Models of trans-
lation competitions. In Proceedings of the 51st
Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers),
pages 1416?1424, Sofia, Bulgaria.
J?arvelin, K. and Kek?al?ainen, J. (2002). Cumu-
lated gain-based evaluation of ir techniques.
ACM Transactions on Information Systems,
20(4):422?446.
Kim, J.-D., Ohta, T., Tateisi, Y., and Tsujii, J.
(2003). GENIA corpus ? a semantically anno-
tated corpus for bio-textmining. Bioinformatics,
19(suppl 1):i180?i182.
Knox, C., Law, V., Jewison, T., Liu, P., Ly,
S., Frolkis, A., Pon, A., Banco, K., Mak, C.,
Neveu, V., Djoumbou, Y., Eisner, R., Guo,
A. C., and Wishart, D. S. (2011). DrugBank 3.0:
a comprehensive resource for Omics research
on drugs. Nucleic acids research, 39(suppl
1):D1035?D1041.
Koehn, P. (2012a). Simulating human judgment in
51
machine translation evaluation campaigns. In
International Workshop on Spoken Language
Translation (IWSLT).
Koehn, P. (2012b). Simulating Human Judgment
in Machine Translation Evaluation Campaigns.
In Proceedings of the Ninth International Work-
shop on Spoken Language Translation, pages
179?184, Hong Kong, China.
Koehn, P. and Monz, C. (2006). Manual and au-
tomatic evaluation of machine translation be-
tween European languages. In Proceedings of
NAACL 2006 Workshop on Statistical Machine
Translation, New York, New York.
Koppel, M. and Ordan, N. (2011). Translationese
and its dialects. In Proceedings of the 49th An-
nual Meeting of the Association for Computa-
tional Linguistics: Human Language Techolo-
gies, pages 1318?1326, Portland, Oregon.
Landis, J. R. and Koch, G. G. (1977). The mea-
surement of observer agreement for categorical
data. Biometrics, 33:159?174.
Leusch, G., Ueffing, N., and Ney, H. (2006). Cder:
Efficient mt evaluation using block movements.
In Proceedings of the 11th Conference of the
European Chapter of the Association for Com-
putational Linguistics, pages 241?248, Trento,
Italy.
Li, J., Kim, S.-J., Na, H., and Lee, J.-H. (2014a).
Postech?s system description for medical text
translation task. In Proceedings of the ACL
2014 Ninth Workshop of Statistical Machine
Translation, Baltimore, USA.
Li, L., Wu, X., Vaillo, S. C., Xie, J., Way, A., and
Liu, Q. (2014b). The dcu-ictcas mt system at
wmt 2014 on german-english translation task.
In Proceedings of the Ninth Workshop on Sta-
tistical Machine Translation, Baltimore, Mary-
land, USA. Association for Computational Lin-
guistics.
Lopez, A. (2012). Putting Human Assessments of
Machine Translation Systems in Order. In Pro-
ceedings of the Seventh Workshop on Statisti-
cal Machine Translation, pages 1?9, Montr?eal,
Canada. Association for Computational Lin-
guistics.
Lu, Y., Wang, L., Wong, D. F., Chao, L. S., Wang,
Y., and Oliveira, F. (2014). Domain adapta-
tion for medical text translation using web re-
sources. In Proceedings of the ACL 2014 Ninth
Workshop of Statistical Machine Translation,
Baltimore, USA.
Luong, N. Q., Besacier, L., and Lecouteux, B.
(2014). Lig system for word level qe task at
wmt14. In Proceedings of the Ninth Workshop
on Statistical Machine Translation, Baltimore,
Maryland, USA. Association for Computational
Linguistics.
Luong, N. Q., Lecouteux, B., and Besacier, L.
(2013). LIG system for WMT13 QE task: In-
vestigating the usefulness of features in word
confidence estimation for MT. In Proceedings
of the Eighth Workshop on Statistical Machine
Translation, pages 384?389, Sofia, Bulgaria.
Association for Computational Linguistics.
Mach?a?cek, M. and Bojar, O. (2014). Results of
the wmt14 metrics shared task. In Proceedings
of the Ninth Workshop on Statistical Machine
Translation, Baltimore, Maryland, USA. Asso-
ciation for Computational Linguistics.
Matthews, A., Ammar, W., Bhatia, A., Feely, W.,
Hanneman, G., Schlinger, E., Swayamdipta, S.,
Tsvetkov, Y., Lavie, A., and Dyer, C. (2014).
The cmu machine translation systems at wmt
2014. In Proceedings of the Ninth Workshop
on Statistical Machine Translation, Baltimore,
Maryland, USA. Association for Computational
Linguistics.
Neidert, J., Schuster, S., Green, S., Heafield, K.,
and Manning, C. (2014). Stanford universitys
submissions to the wmt 2014 translation task. In
Proceedings of the Ninth Workshop on Statisti-
cal Machine Translation, Baltimore, Maryland,
USA. Association for Computational Linguis-
tics.
Okita, T., Vahid, A. H., Way, A., and Liu, Q.
(2014). Dcu terminology translation system for
medical query subtask at wmt14. In Proceed-
ings of the ACL 2014 Ninth Workshop of Statis-
tical Machine Translation, Baltimore, USA.
Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J.
(2002). BLEU: a method for automatic eval-
uation of machine translation. In Proceedings
of the 40th Annual Meeting of the Association
for Computational Linguistics, pages 311?318,
Philadelphia, PA, USA. Association for Com-
putational Linguistics.
P?echeux, N., Gong, L., Do, Q. K., Marie, B.,
Ivanishcheva, Y., Allauzen, A., Lavergne, T.,
52
Niehues, J., Max, A., and Yvon, Y. (2014).
LIMSI @ WMT?14 Medical Translation Task.
In Proceedings of the Ninth Workshop on Sta-
tistical Machine Translation, Baltimore, USA.
Pecina, P., Du?sek, O., Goeuriot, L., Haji?c, J.,
Hlav?a?cov?a, J., Jones, G., Kelly, L., Leveling, J.,
Mare?cek, D., Nov?ak, M., Popel, M., Rosa, R.,
Tamchyna, A., and Ure?sov?a, Z. (2014). Adapta-
tion of machine translation for multilingual in-
formation retrieval in the medical domain. Arti-
ficial Intelligence in Medicine, (0):?.
Peitz, S., Wuebker, J., Freitag, M., and Ney, H.
(2014). The rwth aachen german-english ma-
chine translation system for wmt 2014. In
Proceedings of the Ninth Workshop on Statisti-
cal Machine Translation, Baltimore, Maryland,
USA. Association for Computational Linguis-
tics.
Pouliquen, B. and Mazenc, C. (2011). COPPA,
CLIR and TAPTA: three tools to assist in over-
coming the patent barrier at WIPO. In Pro-
ceedings of the Thirteenth Machine Translation
Summit, pages 24?30, Xiamen, China. Asia-
Pacific Association for Machine Translation.
Powers, D. M. W. (2011). Evaluation: from preci-
sion, recall and f-measure to roc, informedness,
markedness & correlation. Journal of Machine
Learning Technologies.
Quernheim, D. and Cap, F. (2014). Large-scale ex-
act decoding: The ims-ttt submission to wmt14.
In Proceedings of the Ninth Workshop on Sta-
tistical Machine Translation, Baltimore, Mary-
land, USA. Association for Computational Lin-
guistics.
Rosse, C. and Mejino Jr., J. L. V. (2008). The
foundational model of anatomy ontology. In
Burger, A., Davidson, D., and Baldock, R., ed-
itors, Anatomy Ontologies for Bioinformatics,
volume 6 of Computational Biology, pages 59?
117. Springer London.
Rubino, R., Toral, A., S?anchez-Cartagena, V. M.,
Ferr?andez-Tordera, J., Ortiz Rojas, S., Ram??rez-
S?anchez, G., S?anchez-Mart??nez, F., and Way,
A. (2014). Abu-matran at wmt 2014 transla-
tion task: Two-step data selection and rbmt-
style synthetic rules. In Proceedings of the
Ninth Workshop on Statistical Machine Trans-
lation, Baltimore, Maryland, USA. Association
for Computational Linguistics.
Sakaguchi, K., Post, M., and Van Durme, B.
(2014). Efficient elicitation of annotations for
human evaluation of machine translation. In
Proceedings of the Ninth Workshop on Statisti-
cal Machine Translation, Baltimore, Maryland.
S?anchez-Cartagena, V. M., P?erez-Ortiz, J. A., and
S?anchez-Mart??nez, F. (2014). The ua-prompsit
hybrid machine translation system for the 2014
workshop on statistical machine translation. In
Proceedings of the Ninth Workshop on Statisti-
cal Machine Translation, Baltimore, Maryland,
USA. Association for Computational Linguis-
tics.
Scarton, C. and Specia, L. (2014). Exploring con-
sensus in machine translation for quality esti-
mation. In Proceedings of the Ninth Workshop
on Statistical Machine Translation, Baltimore,
Maryland, USA. Association for Computational
Linguistics.
Schwartz, L., Anderson, T., Gwinnup, J., and
Young, K. (2014). Machine translation and
monolingual postediting: The afrl wmt-14 sys-
tem. In Proceedings of the Ninth Workshop
on Statistical Machine Translation, Baltimore,
Maryland, USA. Association for Computational
Linguistics.
Seginer, Y. (2007). Learning Syntactic Structure.
PhD thesis, University of Amsterdam.
Shah, K., Cohn, T., and Specia, L. (2013). An
investigation on the effectiveness of features for
translation quality estimation. In Proceedings
of the Machine Translation Summit XIV, pages
167?174, Nice, France.
Shah, K. and Specia, L. (2014). Quality estimation
for translation selection. In Proceedings of the
17th Annual Conference of the European As-
sociation for Machine Translation, Dubrovnik,
Croatia.
Snover, M., Dorr, B., Schwartz, R., Micciulla, L.,
and Makhoul, J. (2006). A study of transla-
tion edit rate with targeted human annotation.
In Proceedings of the 7th Biennial Conference
of the Association for Machine Translation in
the Americas (AMTA-2006), Cambridge, Mas-
sachusetts.
Souza, J. G. C. d., Espl-Gomis, M., Turchi, M.,
and Negri, M. (2013). Exploiting qualitative in-
formation from automatic word alignment for
cross-lingual nlp tasks. In The 51st Annual
53
Meeting of the Association for Computational
Linguistics - Short Papers (ACL Short Papers
2013).
Specia, L., Shah, K., de Souza, J. G. C., and Cohn,
T. (2013). QuEst - A Translation Quality Esti-
mation Framework. In Proceedings of the 51th
Conference of the Association for Computa-
tional Linguistics (ACL), Demo Session, Sofia,
Bulgaria.
Tamchyna, A., Popel, M., Rosa, R., and Bojar, O.
(2014). Cuni in wmt14: Chimera still awaits
bellerophon. In Proceedings of the Ninth Work-
shop on Statistical Machine Translation, Balti-
more, Maryland, USA. Association for Compu-
tational Linguistics.
Tan, L. and Pal, S. (2014). Manawi: Using
multi-word expressions and named entities to
improve machine translation. In Proceedings
of the Ninth Workshop on Statistical Machine
Translation, Baltimore, Maryland, USA. Asso-
ciation for Computational Linguistics.
Thompson, P., Iqbal, S., McNaught, J., and Ana-
niadou, S. (2009). Construction of an annotated
corpus to support biomedical information ex-
traction. BMC bioinformatics, 10(1):349.
Tiedemann, J. (2009). News from OPUS ? a
collection of multilingual parallel corpora with
tools and interfaces. In Recent Advances in
Natural Language Processing, volume 5, pages
237?248, Borovets, Bulgaria. John Benjamins.
Tillmann, C., Vogel, S., Ney, H., Zubiaga, A.,
and Sawaf, H. (1997). Accelerated DP based
search for statistical translation. In Kokki-
nakis, G., Fakotakis, N., and Dermatas, E., edi-
tors, Proceedings of the Fifth European Confer-
ence on Speech Communication and Technol-
ogy, pages 2667?2670, Rhodes, Greece. Inter-
national Speech Communication Association.
U.S. National Library of Medicine (2009). UMLS
reference manual. Metathesaurus. Bethesda,
MD, USA.
Voorhees, E. M. and Harman, D. K., editors
(2005). TREC: Experiment and evaluation in
information retrieval, volume 63 of Digital li-
braries and electronic publishing series. MIT
press Cambridge, Cambridge, MA, USA.
Wang, L., Lu, Y., Wong, D. F., Chao, L. S., Wang,
Y., and Oliveira., F. (2014). Combining domain
adaptation approaches for medical text transla-
tion. In Proceedings of the ACL 2014 Ninth
Workshop of Statistical Machine Translation,
Baltimore, USA.
W?aschle, K. and Riezler, S. (2012). Analyz-
ing parallelism and domain similarities in the
MAREC patent corpus. In Salampasis, M. and
Larsen, B., editors, Multidisciplinary Informa-
tion Retrieval, volume 7356 of Lecture Notes
in Computer Science, pages 12?27. Springer
Berlin Heidelberg.
Williams, P., Sennrich, R., Nadejde, M., Huck, M.,
Hasler, E., and Koehn, P. (2014). Edinburghs
syntax-based systems at wmt 2014. In Proceed-
ings of the Ninth Workshop on Statistical Ma-
chine Translation, Baltimore, Maryland, USA.
Association for Computational Linguistics.
Wisniewski, G., P?echeux, N., Allauzen, A., and
Yvon, F. (2014). Limsi submission for wmt?14
qe task. In Proceedings of the Ninth Workshop
on Statistical Machine Translation, Baltimore,
Maryland, USA. Association for Computational
Linguistics.
wu, x., Haque, R., Okita, T., Arora, P., Way, A.,
and Liu, Q. (2014). Dcu-lingo24 participation
in wmt 2014 hindi-english translation task. In
Proceedings of the Ninth Workshop on Statisti-
cal Machine Translation, Baltimore, Maryland,
USA. Association for Computational Linguis-
tics.
Zde?nka Ure?sov?a, Ond?rej Du?sek, J. H. and Pecina,
P. (2014). Multilingual test sets for machine
translation of search queries for cross-lingual
information retrieval in the medical domain. In
To appear in Proceedings of the Ninth Interna-
tional Conference on Language Resources and
Evaluation, Reykjavik, Iceland.
Zhang, J., Wu, X., Calixto, I., Vahid, A. H., Zhang,
X., Way, A., and Liu, Q. (2014). Experiments in
medical translation shared task at wmt 2014. In
Proceedings of the ACL 2014 Ninth Workshop
of Statistical Machine Translation, Baltimore,
USA.
54
A Pairwise System Comparisons by Human Judges
Tables 28?37 show pairwise comparisons between systems for each language pair. The numbers in each
of the tables? cells indicate the percentage of times that the system in that column was judged to be better
than the system in that row, ignoring ties. Bolding indicates the winner of the two systems.
Because there were so many systems and data conditions the significance of each pairwise compar-
ison needs to be quantified. We applied the Sign Test to measure which comparisons indicate genuine
differences (rather than differences that are attributable to chance). In the following tables ? indicates sta-
tistical significance at p ? 0.10, ? indicates statistical significance at p ? 0.05, and ? indicates statistical
significance at p ? 0.01, according to the Sign Test.
Each table contains final rows showing how likely a system would win when paired against a randomly
selected system (the expected win ratio score) and the rank range according the official method used in
Table 8. Gray lines separate clusters based on non-overlapping rank ranges.
O
N
L
I
N
E
-
B
U
E
D
I
N
-
P
H
R
A
S
E
U
E
D
I
N
-
S
Y
N
T
A
X
O
N
L
I
N
E
-
A
C
U
-
M
O
S
E
S
ONLINE-B ? .47? .43? .42? .39?
UEDIN-PHRASE .53? ? .44? .44? .41?
UEDIN-SYNTAX .57? .56? ? .49 .48?
ONLINE-A .58? .56? .51 ? .48?
CU-MOSES .61? .59? .52? .52? ?
score .57 .54 .47 .46 .44
rank 1 2 3-4 3-4 5
Table 28: Head to head comparison, ignoring ties, for Czech-English systems
C
U
-
D
E
P
F
I
X
U
E
D
I
N
-
U
N
C
N
S
T
R
C
U
-
B
O
J
A
R
C
U
-
F
U
N
K
Y
O
N
L
I
N
E
-
B
U
E
D
I
N
-
P
H
R
A
S
E
O
N
L
I
N
E
-
A
C
U
-
T
E
C
T
O
C
O
M
M
E
R
C
I
A
L
1
C
O
M
M
E
R
C
I
A
L
2
CU-DEPFIX ? .50 .42? .48 .44? .43? .41? .35? .30? .24?
UEDIN-UNCNSTR .50 ? .51 .48 .42? .37? .42? .39? .31? .26?
CU-BOJAR .58? .49 ? .49 .45? .44? .40? .36? .32? .24?
CU-FUNKY .52 .52 .51 ? .48 .47? .44? .34? .33? .26?
ONLINE-B .56? .58? .55? .52 ? .48 .47? .41? .31? .26?
UEDIN-PHRASE .57? .63? .56? .53? .52 ? .48 .44? .32? .27?
ONLINE-A .59? .58? .60? .56? .53? .52 ? .45? .37? .30?
CU-TECTO .65? .61? .64? .66? .59? .56? .55? ? .42? .30?
COMMERCIAL1 .70? .69? .68? .67? .69? .68? .63? .58? ? .40?
COMMERCIAL2 .76? .74? .76? .74? .74? .73? .70? .70? .60? ?
score .60 .59 .58 .57 .54 .52 .50 .44 .36 .28
rank 1-3 1-3 1-4 3-4 5-6 5-6 7 8 9 10
Table 29: Head to head comparison, ignoring ties, for English-Czech systems
55
O
N
L
I
N
E
-
B
U
E
D
I
N
-
S
Y
N
T
A
X
O
N
L
I
N
E
-
A
L
I
M
S
I
-
K
I
T
E
U
-
B
R
I
D
G
E
U
E
D
I
N
-
P
H
R
A
S
E
K
I
T
R
W
T
H
D
C
U
-
I
C
T
C
A
S
C
M
U
R
B
M
T
4
R
B
M
T
1
O
N
L
I
N
E
-
C
ONLINE-B ? .46 .40? .41? .35? .42? .38? .35? .40? .31? .33? .32? .22?
UEDIN-SYNTAX .54 ? .51 .47 .47 .45 .45? .39? .36? .38? .35? .34? .27?
ONLINE-A .60? .49 ? .42? .44? .51 .41? .38? .44? .42? .38? .31? .20?
LIMSI-KIT .59? .53 .58? ? .55 .53 .31? .45? .39? .41? .37? .35? .29?
EU-BRIDGE .65? .53 .56? .45 ? .45 .44? .48 .40? .37? .39? .37? .30?
UEDIN-PHRASE .58? .55 .49 .47 .55 ? .48 .39? .34? .45? .40? .40? .34?
KIT .62? .55? .59? .69? .56? .52 ? .45? .41? .45? .47 .40? .31?
RWTH .65? .61? .62? .55? .52 .61? .55? ? .54 .44? .44? .38? .37?
DCU-ICTCAS .60? .64? .56? .61? .60? .66? .59? .46 ? .51 .49 .46? .40?
CMU .69? .62? .58? .59? .63? .55? .55? .56? .49 ? .53 .42? .43?
RBMT4 .67? .65? .62? .63? .61? .60? .53 .56? .51 .47 ? .51 .37?
RBMT1 .68? .66? .69? .65? .63? .60? .60? .62? .54? .58? .49 ? .38?
ONLINE-C .78? .73? .80? .71? .70? .66? .69? .63? .60? .57? .63? .62? ?
score .63 .58 .58 .55 .55 .54 .49 .47 .45 .44 .44 .40 .32
rank 1 2-3 2-3 4-6 4-6 4-6 7-8 7-8 9-11 9-11 9-11 12 13
Table 30: Head to head comparison, ignoring ties, for German-English systems
U
E
D
I
N
-
S
Y
N
T
A
X
O
N
L
I
N
E
-
B
O
N
L
I
N
E
-
A
P
R
O
M
T
-
H
Y
B
R
I
D
P
R
O
M
T
-
R
U
L
E
U
E
D
I
N
-
S
T
A
N
F
O
R
D
E
U
-
B
R
I
D
G
E
R
B
M
T
4
U
E
D
I
N
-
P
H
R
A
S
E
R
B
M
T
1
K
I
T
S
T
A
N
F
O
R
D
-
U
N
C
C
I
M
S
S
T
A
N
F
O
R
D
U
U
O
N
L
I
N
E
-
C
I
M
S
-
T
T
T
U
U
-
D
O
C
E
N
T
UEDIN-SYNTAX ? .55? .46? .45? .46? .44? .41? .45? .43? .41? .38? .38? .36? .33? .38? .30? .30? .25?
ONLINE-B .45? ? .50 .48 .50 .47 .43? .46? .41? .45? .39? .39? .37? .32? .35? .34? .30? .29?
ONLINE-A .54? .50 ? .44? .52 .50 .45? .43? .43? .42? .39? .41? .42? .42? .37? .44? .38? .33?
PROMT-HYBRID .55? .52 .56? ? .45? .47 .47 .46? .50 .44? .42? .40? .41? .38? .39? .39? .33? .34?
PROMT-RULE .54? .50 .48 .55? ? .51 .47 .47 .45? .38? .42? .40? .43? .41? .43? .38? .35? .29?
UEDIN-STANFORD .56? .53 .50 .53 .49 ? .48 .50 .47 .44? .46 .36? .36? .36? .36? .35? .30? .32?
EU-BRIDGE .59? .57? .55? .53 .53 .52 ? .46? .43? .52 .42? .42? .45? .35? .36? .41? .38? .30?
RBMT4 .55? .54? .57? .54? .53 .50 .54? ? .53 .49 .44? .49 .50 .47 .40? .42? .38? .40?
UEDIN-PHRASE .57? .59? .57? .50 .55? .53 .57? .47 ? .50 .55? .47 .45? .44? .43? .42? .37? .34?
RBMT1 .59? .55? .58? .56? .62? .56? .48 .51 .50 ? .47 .47 .45? .47 .43? .42? .38? .41?
KIT .62? .61? .61? .58? .58? .54 .58? .56? .45? .53 ? .47 .49 .46 .43? .48 .34? .37?
STANFORD-UNC .62? .61? .59? .60? .60? .64? .58? .51 .53 .53 .53 ? .48 .47 .45? .45? .39? .41?
CIMS .64? .63? .58? .59? .57? .64? .55? .50 .55? .55? .51 .52 ? .53 .42? .52 .47 .42?
STANFORD .67? .68? .58? .62? .59? .64? .65? .53 .56? .53 .54 .53 .47 ? .53 .42? .39? .48
UU .62? .65? .62? .61? .57? .64? .64? .60? .57? .57? .57? .55? .58? .47 ? .46? .45? .38?
ONLINE-C .70? .66? .56? .61? .62? .65? .59? .58? .58? .58? .52 .55? .48 .58? .54? ? .48 .47
IMS-TTT .70? .70? .62? .67? .65? .70? .62? .62? .63? .62? .66? .61? .53 .61? .55? .52 ? .49
UU-DOCENT .75? .71? .67? .66? .71? .68? .70? .60? .66? .59? .63? .59? .58? .52 .62? .53 .51 ?
score .60 .59 .56 .56 .56 .56 .54 .51 .51 .50 .48 .47 .46 .44 .43 .42 .38 .37
rank 1-2 1-2 3-6 3-6 3-6 3-6 7 8-10 8-10 8-10 11-12 11-13 12-14 13-15 14-16 15-16 17-18 17-18
Table 31: Head to head comparison, ignoring ties, for English-German systems
56
U
E
D
I
N
-
P
H
R
A
S
E
K
I
T
O
N
L
I
N
E
-
B
S
T
A
N
F
O
R
D
O
N
L
I
N
E
-
A
R
B
M
T
1
R
B
M
T
4
O
N
L
I
N
E
-
C
UEDIN-PHRASE ? .48 .48 .45? .43? .28? .28? .19?
KIT .52 ? .54? .48 .44? .31? .29? .21?
ONLINE-B .52 .46? ? .51 .47 .31? .30? .24?
STANFORD .55? .52 .49 ? .46? .34? .30? .23?
ONLINE-A .57? .56? .53 .54? ? .32? .29? .21?
RBMT1 .72? .69? .69? .66? .68? ? .42? .33?
RBMT4 .72? .71? .70? .70? .71? .58? ? .39?
ONLINE-C .81? .79? .76? .77? .79? .67? .61? ?
score .63 .60 .59 .58 .57 .40 .35 .25
rank 1 2-4 2-4 2-4 5 6 7 8
Table 32: Head to head comparison, ignoring ties, for French-English systems
O
N
L
I
N
E
-
B
U
E
D
I
N
-
P
H
R
A
S
E
K
I
T
M
A
T
R
A
N
M
A
T
R
A
N
-
R
U
L
E
S
O
N
L
I
N
E
-
A
U
U
-
D
O
C
E
N
T
P
R
O
M
T
-
H
Y
B
R
I
D
U
A
P
R
O
M
T
-
R
U
L
E
R
B
M
T
1
R
B
M
T
4
O
N
L
I
N
E
-
C
ONLINE-B ? .46? .48 .46? .50 .41? .39? .39? .37? .38? .37? .35? .27?
UEDIN-PHRASE .54? ? .50 .47 .46 .46? .42? .41? .46? .42? .35? .34? .33?
KIT .52 .50 ? .53 .51 .50 .43? .49 .41? .42? .35? .37? .29?
MATRAN .54? .53 .47 ? .49 .50 .43? .43? .38? .48 .40? .34? .32?
MATRAN-RULES .50 .54 .49 .51 ? .53 .40? .45? .46? .42? .44? .40? .34?
ONLINE-A .59? .54? .50 .50 .47 ? .44? .49 .47 .45? .42? .37? .34?
UU-DOCENT .61? .58? .57? .57? .60? .56? ? .43? .52 .46? .39? .44? .33?
PROMT-HYBRID .61? .59? .51 .57? .55? .51 .57? ? .50 .41? .46? .44? .35?
UA .63? .54? .59? .62? .54? .53 .48 .50 ? .49 .46? .43? .34?
PROMT-RULE .62? .58? .58? .52 .58? .55? .54? .59? .51 ? .47 .39? .37?
RBMT1 .63? .65? .65? .60? .56? .58? .61? .54? .54? .53 ? .46? .45?
RBMT4 .65? .66? .63? .66? .60? .63? .56? .56? .57? .61? .54? ? .45?
ONLINE-C .73? .67? .71? .67? .66? .66? .67? .65? .66? .63? .55? .55? ?
score .59 .57 .55 .55 .54 .53 .49 .49 .48 .47 .43 .40 .34
rank 1 2-4 2-5 2-5 4-6 4-6 7-9 7-10 7-10 8-10 11 12 13
Table 33: Head to head comparison, ignoring ties, for English-French systems
O
N
L
I
N
E
-
B
O
N
L
I
N
E
-
A
U
E
D
I
N
-
S
Y
N
T
A
X
C
M
U
U
E
D
I
N
-
P
H
R
A
S
E
A
F
R
L
I
I
T
-
B
O
M
B
A
Y
D
C
U
-
L
I
N
G
O
2
4
I
I
I
T
-
H
Y
D
E
R
A
B
A
D
ONLINE-B ? .36? .33? .37? .31? .21? .20? .14? .00
ONLINE-A .64? ? .48 .47? .44? .31? .30? .24? .12?
UEDIN-SYNTAX .67? .52 ? .47 .46? .33? .29? .24? .12?
CMU .63? .53? .53 ? .47 .37? .31? .26? .11?
UEDIN-PHRASE .69? .56? .54? .53 ? .40? .33? .25? .11?
AFRL .79? .69? .67? .63? .60? ? .53 .40? .16?
IIT-BOMBAY .80? .70? .71? .69? .67? .47 ? .44? .19?
DCU-LINGO24 .86? .76? .76? .74? .75? .60? .56? ? .19?
IIIT-HYDERABAD .94? .88? .88? .89? .89? .84? .81? .81? ?
score .75 .62 .61 .60 .57 .44 .41 .34 .13
rank 1 2-3 2-4 3-4 5 6-7 6-7 8 9
Table 34: Head to head comparison, ignoring ties, for Hindi-English systems
57
O
N
L
I
N
E
-
B
O
N
L
I
N
E
-
A
U
E
D
I
N
-
U
N
C
N
S
T
R
U
E
D
I
N
-
P
H
R
A
S
E
C
U
-
M
O
S
E
S
I
I
T
-
B
O
M
B
A
Y
I
P
N
-
U
P
V
-
C
N
T
X
T
D
C
U
-
L
I
N
G
O
2
4
I
P
N
-
U
P
V
-
N
O
D
E
V
M
A
N
A
W
I
-
H
1
M
A
N
A
W
I
M
A
N
A
W
I
-
R
M
O
O
V
ONLINE-B ? .49 .28? .29? .27? .23? .22? .20? .17? .12? .13? .13?
ONLINE-A .51 ? .31? .29? .27? .25? .20? .20? .21? .19? .16? .15?
UEDIN-UNCNSTR .72? .69? ? .44? .49 .39? .40? .34? .39? .29? .30? .27?
UEDIN-PHRASE .71? .71? .56? ? .48 .45? .44? .39? .37? .31? .31? .32?
CU-MOSES .73? .73? .51 .52 ? .47 .42? .40? .45? .36? .35? .33?
IIT-BOMBAY .77? .75? .61? .55? .53 ? .50 .47 .45? .41? .40? .36?
IPN-UPV-CNTXT .78? .80? .60? .56? .58? .50 ? .51 .41? .40? .40? .37?
DCU-LINGO24 .80? .80? .66? .61? .60? .53 .49 ? .52 .41? .41? .39?
IPN-UPV-NODEV .83? .79? .61? .63? .55? .55? .59? .48 ? .46? .44? .38?
MANAWI-H1 .88? .81? .71? .69? .64? .59? .60? .59? .54? ? .35? .34?
MANAWI .87? .84? .70? .69? .65? .60? .60? .59? .56? .65? ? .39?
MANAWI-RMOOV .87? .85? .73? .68? .67? .64? .63? .61? .62? .66? .61? ?
score .77 .75 .57 .54 .52 .47 .46 .43 .42 .38 .35 .31
rank 1 2 3 4-5 4-5 6-7 6-7 8-9 8-9 10-11 10-11 12
Table 35: Head to head comparison, ignoring ties, for English-Hindi systems
A
F
R
L
-
P
E
O
N
L
I
N
E
-
B
O
N
L
I
N
E
-
A
P
R
O
M
T
-
H
Y
B
R
I
D
P
R
O
M
T
-
R
U
L
E
U
E
D
I
N
-
P
H
R
A
S
E
Y
A
N
D
E
X
O
N
L
I
N
E
-
G
A
F
R
L
U
E
D
I
N
-
S
Y
N
T
A
X
K
A
Z
N
U
R
B
M
T
1
R
B
M
T
4
AFRL-PE ? .42? .40? .39? .39? .41? .35? .39? .28? .26? .26? .29? .21?
ONLINE-B .58? ? .42? .43? .45? .45? .42? .43? .46? .37? .33? .29? .31?
ONLINE-A .60? .58? ? .50 .45? .51 .47 .45? .42? .40? .33? .32? .30?
PROMT-HYBRID .61? .57? .50 ? .47 .45? .49 .44? .43? .44? .39? .31? .27?
PROMT-RULE .61? .55? .55? .53 ? .46? .47 .49 .48 .42? .36? .34? .30?
UEDIN-PHRASE .59? .55? .49 .55? .54? ? .49 .50 .47 .44? .32? .37? .29?
YANDEX .65? .58? .53 .51 .53 .51 ? .48 .50 .43? .34? .36? .34?
ONLINE-G .61? .57? .55? .56? .51 .50 .52 ? .48 .43? .39? .35? .30?
AFRL .72? .54? .58? .57? .52 .53 .50 .52 ? .44? .41? .41? .37?
UEDIN-SYNTAX .74? .63? .60? .56? .58? .56? .57? .57? .56? ? .51 .36? .37?
KAZNU .74? .67? .67? .61? .64? .68? .66? .61? .59? .49 ? .44? .38?
RBMT1 .71? .71? .68? .69? .66? .63? .64? .65? .59? .64? .56? ? .47
RBMT4 .79? .69? .70? .73? .70? .71? .66? .70? .63? .63? .62? .53 ?
score .66 .58 .55 .55 .53 .53 .52 .51 .49 .45 .40 .36 .32
rank 1 2 3-5 3-5 4-7 5-8 5-8 5-8 9 10 11 12 13
Table 36: Head to head comparison, ignoring ties, for Russian-English systems
P
R
O
M
T
-
R
U
L
E
O
N
L
I
N
E
-
B
P
R
O
M
T
-
H
Y
B
R
I
D
U
E
D
I
N
-
U
N
C
N
S
T
R
O
N
L
I
N
E
-
G
O
N
L
I
N
E
-
A
U
E
D
I
N
-
P
H
R
A
S
E
R
B
M
T
4
R
B
M
T
1
PROMT-RULE ? .51 .45? .43? .43? .39? .38? .15? .00
ONLINE-B .49 ? .50 .47? .38? .36? .38? .16? .13?
PROMT-HYBRID .55? .50 ? .49 .47 .39? .40? .18? .15?
UEDIN-UNCNSTR .57? .53? .51 ? .50 .44? .36? .25? .18?
ONLINE-G .57? .62? .53 .50 ? .46? .44? .23? .18?
ONLINE-A .61? .64? .61? .56? .54? ? .49 .24? .18?
UEDIN-PHRASE .62? .62? .60? .64? .56? .51 ? .30? .21?
RBMT4 .85? .84? .82? .75? .77? .76? .70? ? .42?
RBMT1 .91? .87? .85? .82? .82? .82? .79? .58? ?
score .64 .64 .61 .58 .55 .51 .49 .26 .19
rank 1-2 1-2 3 4-5 4-5 6-7 6-7 8 9
Table 37: Head to head comparison, ignoring ties, for English-Russian systems
58
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 195?200,
Baltimore, Maryland USA, June 26?27, 2014.
c
?2014 Association for Computational Linguistics
CUNI in WMT14: Chimera Still Awaits Bellerophon
Ale
?
s Tamchyna, Martin Popel, Rudolf Rosa, Ond
?
rej Bojar
Charles University in Prague, Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics
Malostransk?e n?am?est?? 25, Prague, Czech Republic
surname@ufal.mff.cuni.cz
Abstract
We present our English?Czech and
English?Hindi submissions for this
year?s WMT translation task. For
English?Czech, we build upon last year?s
CHIMERA and evaluate several setups.
English?Hindi is a new language pair for
this year. We experimented with reverse
self-training to acquire more (synthetic)
parallel data and with modeling target-side
morphology.
1 Introduction
In this paper, we describe translation systems sub-
mitted by Charles University (CU or CUNI) to the
Translation task of the Ninth Workshop on Statis-
tical Machine Translation (WMT) 2014.
In ?2, we present our English?Czech systems,
CU-TECTOMT, CU-BOJAR, CU-DEPFIX and CU-
FUNKY. The systems are very similar to our sub-
missions (Bojar et al., 2013) from last year, the
main novelty being our experiments with domain-
specific and document-specific language models.
In ?3, we describe our experiments with
English?Hindi translation, which is a translation
pair new both to us and to WMT. We unsuccess-
fully experimented with reverse self-training and a
morphological-tags-based language model, and so
our final submission, CU-MOSES, is only a basic
instance of Moses.
2 English?Czech
Our submissions for English?Czech build upon
last year?s successful CHIMERA system (Bojar
et al., 2013). We combine several different ap-
proaches:
? factored phrase-based Moses model (?2.1),
? domain-adapted language model (?2.2),
? document-specific language models (?2.3),
? deep-syntactic MT system TectoMT (?2.4),
? automatic post-editing system Depfix (?2.5).
We combined the approaches in several ways
into our four submissions, as made clear by Ta-
ble 1. CU-TECTOMT is the stand-alone TectoMT
translation system, while the other submissions
are Moses-based, using TectoMT indirectly to pro-
vide an additional phrase-table. CU-BOJAR uses
a factored model and a domain-adapted language
model; in CU-DEPFIX, Depfix post-processing is
added; and CU-FUNKY also employs document-
specific language models.
C
U
-
T
E
C
T
O
M
T
C
U
-
B
O
J
A
R
C
U
-
D
E
P
F
I
X
C
U
-
F
U
N
K
Y
TectoMT (?2.4) D D D D
Factored Moses (?2.1) D D D
Adapted LM (?2.2) D D D
Document-specific LMs (?2.3) D
Depfix (?2.5) D D
Table 1: EN?CS systems submitted to WMT.
2.1 Our Baseline Factored Moses System
Our baseline translation system (denoted ?Base-
line? in the following) is similar to last year ? we
trained a factored Moses model on the concatena-
tion of CzEng (Bojar et al., 2012) and Europarl
(Koehn, 2005), see Table 2. We use two fac-
tors: tag, which is the part-of-speech tag, and stc,
which is ?supervised truecasing?, i.e. the surface
form with letter case set according to the lemma;
see (Bojar et al., 2013). Our factored Moses sys-
tem translates from English stc to Czech stc | tag
in one translation step.
Our basic language models are identical to last
year?s submission. We added an adapted language
195
Tokens [M]
Corpus Sents [M] English Czech
CzEng 1.0 14.83 235.67 205.17
Europarl 0.65 17.61 15.00
Table 2: English?Czech parallel data.
Corpus Sents [M] Tokens [M]
CzEng 1.0 14.83 205.17
CWC Articles 36.72 626.86
CNC News 28.08 483.88
CNA 47.00 830.32
Newspapers 64.39 1040.80
News Crawl 24.91 444.84
Total 215.93 3631.87
Table 3: Czech monolingual data.
model which we describe in the following section.
Tables 3 and 4 show basic data about the language
models. Aside from modeling surface forms, our
language models also capture morphological co-
herence to some degree.
2.2 Adapted Language Model
We used the 2013 News Crawl to create a language
model adapted to the domain of the test set (i.e.
news domain) using data selection based on infor-
mation retrieval (Tamchyna et al., 2012). We use
the Baseline system to translate the source sides of
WMT test sets 2012?2014. The translations then
constitute a ?query corpus? for Lucene.
1
For each
sentence in the query corpus, we use Lucene to
retrieve 20 most similar sentences from the 2013
News Crawl. After de-duplication, we obtained a
monolingual corpus of roughly 250 thousand sen-
tences and trained an additional 6-gram language
model on this data.
Domain Factor Order Sents Tokens ARPA.gz Trie
[M] [M] [GB] [GB]
General stc 4 201.31 3430.92 28.2 11.8
General stc 7 24.91 444.84 13.1 8.1
General tag 10 14.83 205.17 7.2 3.0
News stc 6 0.25 4.73 0.2 ?
Table 4: Czech LMs used in CU-BOJAR. The last
small model is described in ?2.2.
1
http://lucene.apache.org
2.3 Document-Specific Language Models
CU-FUNKY further extends the idea described in
?2.2. Taking advantage of document IDs which
are included in WMT development and test data,
we split our dev- (WMT 13) and test-set (WMT
14) into documents. We translate each document
with the Baseline system and use Lucene to re-
trieve 10,000 most similar target-side sentences
from News Crawl 2013 for each document sen-
tence.
Using this procedure, we obtain a corpus for
each document. On average, the corpora con-
tain roughly 208 thousand sentences after de-
duplication. Each corpus then serves as the
training data for the document-specific language
model.
We implemented an alternative to
moses-parallel.perl which splits the
input corpus based on document IDs and runs a
separate Moses instance/job for each document.
Moreover, it allows to modify the Moses config-
uration file according to document ID. We use
this feature to plant the correct document-specific
language model to each job.
In tuning, our technique only adds one weight.
In each split, the weight corresponds to a differ-
ent language model. The optimizer then hope-
fully averages the utility of this document-specific
LM across all documents. The same weight is ap-
plied also in the test set translation, exchanging the
document-specific LM file.
2.4 TectoMT Deep-Syntactic MT System
TectoMT
2
was one of the three key components
in last year?s CHIMERA. It is a linguistically-
motivated tree-to-tree deep-syntactic translation
system with transfer based on Maximum Entropy
context-sensitive translation models (Mare?cek et
al., 2010) and Hidden Tree Markov Models
(
?
Zabokrtsk?y and Popel, 2009). It is trained on
the WMT-provided data: CzEng 1.0 (parallel data)
and News Crawl (2007?2012 Czech monolingual
sets).
We maintain the same approach to combining
TectoMT with Moses as last year ? we translate
WMT test sets from years 2007?2014 and use
them as additional synthetic parallel training data ?
a corpus consisting of the test set source side (En-
glish) and TectoMT output (synthetic Czech). We
then use the standard extraction pipeline to create
2
http://ufal.mff.cuni.cz/tectomt/
196
an additional phrase table from this corpus. The
translated data overlap completely both with our
development and test data for Moses so that tuning
can assign an appropriate weight to the synthetic
phrase table.
2.5 Depfix Automatic Post-Editing
As in the previous years, we used Depfix (Rosa,
2013) to post-process the translations. Depfix is
an automatic post-editing system which is mainly
rule-based and uses various linguistic tools (tag-
gers, parsers, morphological generators, etc.) to
detect and correct errors, especially grammatical
ones. The system was slightly improved since last
year, and a new fixing rule was added for correct-
ing word order in noun clusters translated as geni-
tive constructions.
In English, a noun can behave as an adjective,
as in ?according to the house owners?, while in
Czech, this is not possible, and a genitive construc-
tion has to be used instead, similarly to ?according
to the owners of the house? ? the modifier is in the
genitive morphological case and follows the noun.
However, SMT systems translating into Czech do
not usually focus much on word reordering, which
leads to non-fluent or incomprehensible construc-
tions, such as ?podle domu
gen
vlastn??k?u
gen
? (ac-
cording to-the-house of-the-owners). Fortunately,
such cases are easy to distinguish with the help
of a dependency parser and a morphological tag-
ger ? genitive modifiers usually do not precede the
head but follow it (unless they are parts of named
entities), so we can safely switch the word order
to the correct one: ?podle vlastn??k?u
gen
domu
gen
?
(according to-the-owners of-the-house).
2.6 Results
We report scores of automatic metrics as shown in
the submission system,
3
namely (case-sensitive)
BLEU (Papineni et al., 2002) and TER (Snover
et al., 2006). The results, summarized in Ta-
ble 5, show that CU-FUNKY is the most success-
ful of our systems according to BLEU, while
the simpler CU-DEPFIX wins in TER. The re-
sults of manual evaluation suggest that CU-DEPFIX
(dubbed CHIMERA) remains the best performing
English?Czech system.
In comparison to other English?Czech sys-
tems submitted to WMT 2014, CU-FUNKY ranked
as the second in BLEU, and CU-DEPFIX ranked
3
http://matrix.statmt.org/
as the second in TER; the winning system, ac-
cording to both of these metrics, was UEDIN-
UNCONSTRAINED.
System BLEU TER Manual
CU-DEPFIX 21.1 0.670 0.373
UEDIN-UNCONSTRAINED 21.6 0.667 0.357
CU-BOJAR 20.9 0.674 0.333
CU-FUNKY 21.2 0.675 0.287
GOOGLE TRANSLATE 20.2 0.687 0.168
CU-TECTOMT 15.2 0.716 -0.177
CU-BOJAR +full 2013 news 20.7 0.677 ?
Table 5: Scores of automatic metrics and results of
manual evaluation for our systems. The table also
lists the best system according to automatic met-
rics and Google Translate as the best-performing
commercial system.
Our analysis of CU-FUNKY suggests that it is
not the best performing system on average (de-
spite achieving the highest BLEU scores from our
submissions), but that it is rather the most volatile
system. Some sentences were obviously improved
compared to CU-BOJAR but most got degraded es-
pecially in adequacy. We are well aware of the
many shortcomings our current implementation
has, the most severe of which lie in the sentence
selection by Lucene. For instance, we do not use
any stopwords or keyword detection methods, and
also pretending that each sentence in our monolin-
gual corpus is a ?document? for the information
retrieval system is far from ideal.
We also evaluated a version of CU-BOJAR which
uses not only the adapted LM but also an addi-
tional LM trained on the full 2013 News Crawl
data (see ?CU-BOJAR +full 2013 news? in Table 5)
but found no improvement compared to using just
the adapted model (trained on a subset of the data).
3 English?Hindi
English-Hindi is a new language pair this
year. We submitted an unconstrained system for
English?Hindi translation.
We used HindEnCorp (Bojar et al., 2014) as the
sole source of parallel data (nearly 276 thousand
sentence pairs, around 3.95 million English tokens
and 4.09 million Hindi tokens).
Given that no test set from previous years was
available and that the size of the development set
provided by WMT organizers was only 500 sen-
tence pairs, we held out the first 5000 sentence
pairs of HindEnCorp for this purpose. Our de-
velopment set then consisted of the 500 provided
197
Corpus Sents [M] Tokens [M]
NewsCrawl 1.27 27.27
HindEnCorp 0.28 4.09
HindMonoCorp 43.38 945.43
Total 44.93 976.80
Table 6: Hindi monolingual data.
sentences plus 1500 sentence pairs from HindEn-
Corp. The remaining 3500 sentence pairs taken
from HindEnCorp constituted our test set.
As for monolingual data, we used the News
Crawl corpora provided for the task and the new
monolingual HindMonoCorp, which makes our
submission unconstrained. Table 6 shows statis-
tics of our monolingual data.
We tagged and lemmatized the English data us-
ing Mor?ce (Spoustov?a et al., 2007) and the Hindi
data using Siva Reddy?s POS tagger.
4
3.1 Baseline System
The baseline system was eventually our best-
performing one. Its design is completely straight-
forward ? it uses one phrase table trained on
all parallel data (we translate from ?supervised-
truecased? English into Hindi forms) and one 5-
gram language model trained on all monolingual
data. We used KenLM (Heafield et al., 2013) for
estimating the model as the data was rather large
(see Table 6).
We used GIZA++ (Och and Ney, 2000) as
our word alignment tool. We experimented with
several coarser representations to make the final
alignment more reliable. Table 7 shows the re-
sults. The factor ?stem4? refers to simply taking
the first four characters of each word. For lem-
mas, we used the outputs of the tools mentioned
above. However, lemmas as output by the Hindi
tagger were not much coarser than surface forms
? the ratio between the number of types is merely
1.11 ? so we also tried ?stemming? the lemmas
(lemma4). Of these variants, stem4-stem4 align-
ment worked best and we used it for the rest of our
experiments.
3.2 Reverse Self-Training
Bojar and Tamchyna (2011) showed a simple tech-
nique for improving translation quality in situa-
tions where there is only a small amount of par-
4
http://sivareddy.in/downloads#hindi_
tools
English Hindi BLEU
stem4 stem4 22.96?1.17
lemma lemma4 22.59?1.17
lemma lemma 22.41?1.20
Table 7: Comparison of different factor combina-
tions for word alignment.
allel data available but where there is a sufficient
quantity of target-side monolingual texts. The so-
called ?reverse self-training? uses a factored sys-
tem trained in the opposite direction to translate
the large monolingual data into the source lan-
guage. The translation (in the source language,
i.e. English in our case) and the original target-
side data (Hindi) can be used as additional syn-
thetic parallel data. The authors recommend creat-
ing a separate phrase table from it and combining
the two translation models as alternatives in the
log-linear model (letting tuning weigh their impor-
tance).
The factored setup of the reverse system
(Hindi?English) is essential ? alternative decod-
ing paths with a back-off to a coarser representa-
tion (e.g. stems) on the source side (Hindi) give
the system the ability to generalize beyond surface
forms observed in the training data. The main aim
of this technique is to learn new forms of known
words.
The technique is thus aimed at translating into a
morphologically richer language than the source.
Indeed, the authors showed that if the target lan-
guage has considerably more word types than the
source, the gains achieved by reverse self-training
are higher. In this respect, English?Hindi is not
an ideal candidate given that the ratio we observed
is only 1.2.
The choice of back-off representation is impor-
tant. We measure the vocabulary reduction of
several options and summarize the results in Ta-
ble 8. E.g. for stem4, the vocabulary size is
roughly 30% compared to the number of surface
word forms.
Bojar and Tamchyna (2011) achieved the best
results using ?nosuf3? (?suffix trimming?, i.e. cut-
ting of the last 3 characters of each word); how-
ever, they experimented with European languages
and the highest reduction of vocabulary reported
in the paper is to roughly one half. In our case, the
vocabulary is reduced much more, so we opted for
a more conservative back-off, namely ?nosuf2?.
198
Back-off % of vocab. size
stem4 30.21
lemma4 32.36
nosuf3 36.36
nosuf2 50.76
stem5 53.48
lemma5 57.47
lemma 90.09
Table 8: Options for back-off factors in reverse
self-training and the percentage of their vocabu-
lary size compared to surface forms.
We translated roughly 2 million sentences from
the Hindi monolingual data, focusing on news
to maintain a domain match with the WMT test
set. However, adding the synthetic phrase table
did not bring any improvement and in fact, the
BLEU score dropped to 22.37?1.17 (baseline is
22.96?1.17).
We can attribute the failure of reverse self-
training to the nature of the language pair at hand.
While Hindi has some synthetic properties (e.g.
future tense of verbs or inflection of adjectives are
marked by suffixes), its inflectional morphemes
are realized mainly by post-positions which are
separated from their head-words. Overlooking this
essential property, we attempted to use reverse
self-training but our technique could contribute
only very little.
3.3 Target-Side Morphology
We also experimented with a setup that tradition-
ally works very well for English?Czech trans-
lation: using a high-order language model on
morphological tags to explicitly model target-side
morphological coherence in translation. We used
the same monolingual data as for the baseline lan-
guage model; however, the order of our morpho-
logical language model was set to 10.
This setup also brought no improvement over
the baseline ? in fact, the BLEU score dropped
even further to 22.27?1.14.
4 Conclusion
We presented our contributions to the Translation
task of WMT 2014.
As we have focused on English?Czech trans-
lation for many years, we have developed sev-
eral complex and well-performing systems for it
? an adaptation of the phrase-based Moses sys-
tem, a linguistically-motivated syntax-based Tec-
toMT system, and an automatic post-editing Dep-
fix system. We combine the individual systems
using a very simple yet effective method and the
combined system called CHIMERA confirmed its
state-of-the-art performance.
For English?Hindi translation, which was a
new task for us, we managed to get competitive
results by using a baseline Moses setup, but were
unable to improve upon those by employing ad-
vanced techniques that had proven to be effective
for other translation directions.
Acknowledgments
This research was supported by the grants FP7-
ICT-2013-10-610516 (QTLeap), FP7-ICT-2011-
7-288487 (MosesCore), SVV 260 104. and
GAUK 1572314. This work has been using lan-
guage resources developed, stored and distributed
by the LINDAT/CLARIN project of the Ministry
of Education, Youth and Sports of the Czech Re-
public (project LM2010013).
References
Ond?rej Bojar and Ale?s Tamchyna. 2011. Improving
Translation Model by Monolingual Data. In Proc.
of WMT, pages 330?336. ACL.
Ond?rej Bojar, Zden?ek
?
Zabokrtsk?y, Ond?rej Du?sek, Pe-
tra Galu?s?c?akov?a, Martin Majli?s, David Mare?cek, Ji?r??
Mar?s??k, Michal Nov?ak, Martin Popel, and Ale?s Tam-
chyna. 2012. The Joy of Parallelism with CzEng
1.0. In Proc. of LREC, pages 3921?3928. ELRA.
Ondrej Bojar, Rudolf Rosa, and Ale?s Tamchyna. 2013.
Chimera ? Three Heads for English-to-Czech Trans-
lation. In Proceedings of the Eighth Workshop on
Statistical Machine Translation, pages 90?96.
Ond?rej Bojar, Vojt?ech Diatka, Pavel Rychl?y, Pavel
Stra?n?ak, V??t Suchomel, Ale?s Tamchyna, and Daniel
Zeman. 2014. HindEnCorp ? Hindi-English and
Hindi-only Corpus for Machine Translation. Reyk-
jav??k, Iceland. European Language Resources Asso-
ciation.
Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.
Clark, and Philipp Koehn. 2013. Scalable Modified
Kneser-Ney Language Model Estimation. In Proc.
of ACL.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In Machine Trans-
lation Summit X, pages 79?86.
David Mare?cek, Martin Popel, and Zden?ek
?
Zabokrtsk?y.
2010. Maximum entropy translation model in
199
dependency-based MT framework. In Proc. of WMT
and MetricsMATR, pages 201?206. ACL.
Franz Josef Och and Hermann Ney. 2000. Improved
Statistical Alignment Models. In Proc. of ACL,
pages 440?447, Hong Kong. ACL.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proc. of ACL,
pages 311?318, Stroudsburg, PA, USA. ACL.
Rudolf Rosa. 2013. Automatic post-editing of phrase-
based machine translation outputs. Master?s thesis,
Charles University in Prague, Faculty of Mathemat-
ics and Physics, Praha, Czechia.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study
of translation edit rate with targeted human annota-
tion. In In Proceedings of Association for Machine
Translation in the Americas, pages 223?231.
Drahom??ra Spoustov?a, Jan Haji?c, Jan Votrubec, Pavel
Krbec, and Pavel Kv?eto?n. 2007. The best of two
worlds: Cooperation of statistical and rule-based
taggers for Czech. In Proceedings of the Work-
shop on Balto-Slavonic Natural Language Process-
ing, ACL 2007, pages 67?74, Praha.
Ale?s Tamchyna, Petra Galu?s?c?akov?a, Amir Kamran,
Milo?s Stanojevi?c, and Ond?rej Bojar. 2012. Select-
ing Data for English-to-Czech Machine Translation.
In Proceedings of the Seventh Workshop on Statis-
tical Machine Translation, WMT ?12, pages 374?
381, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Zden?ek
?
Zabokrtsk?y and Martin Popel. 2009. Hidden
Markov Tree Model in Dependency-based Machine
Translation. In Proc. of ACL-IJCNLP Short Papers,
pages 145?148.
200
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 293?301,
Baltimore, Maryland USA, June 26?27, 2014.
c
?2014 Association for Computational Linguistics
Results of the WMT14 Metrics Shared Task
Matou
?
s Mach
?
a
?
cek and Ond
?
rej Bojar
Charles University in Prague, Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics
machacekmatous@gmail.com and bojar@ufal.mff.cuni.cz
Abstract
This paper presents the results of the
WMT14 Metrics Shared Task. We asked
participants of this task to score the
outputs of the MT systems involved in
WMT14 Shared Translation Task. We col-
lected scores of 23 metrics from 12 re-
search groups. In addition to that we com-
puted scores of 6 standard metrics (BLEU,
NIST, WER, PER, TER and CDER) as
baselines. The collected scores were eval-
uated in terms of system level correlation
(how well each metric?s scores correlate
with WMT14 official manual ranking of
systems) and in terms of segment level
correlation (how often a metric agrees with
humans in comparing two translations of a
particular sentence).
1 Introduction
Automatic machine translation metrics play a very
important role in the development of MT systems
and their evaluation. There are many different
metrics of diverse nature and one would like to
assess their quality. For this reason, the Met-
rics Shared Task is held annually at the Workshop
of Statistical Machine Translation
1
, starting with
Koehn and Monz (2006) and following up to Bo-
jar et al. (2014).
In this task, we asked metrics developers to
score the outputs of WMT14 Shared Translation
Task (Bojar et al., 2014). We have collected the
computed metrics? scores and use them to evalu-
ate quality of the metrics.
The systems? outputs, human judgements and
evaluated metrics are described in Section 2. The
quality of the metrics in terms of system level cor-
relation is reported in Section 3. Segment level
correlation with a detailed discussion and a slight
1
http://www.statmt.org/wmt13
change in the calculation compared to the previous
year is reported in Section 4.
2 Data
We used the translations of MT systems involved
in WMT14 Shared Translation Task together with
reference translations as the test set for the Met-
rics Task. This dataset consists of 110 systems?
outputs and 10 reference translations in 10 trans-
lation directions (English from and into Czech,
French, German, Hindi and Russian). For most of
the translation directions each system?s output and
the reference translation contain 3003 sentences.
For more details please see the WMT14 overview
paper (Bojar et al., 2014).
2.1 Manual MT Quality Judgements
During the WMT14 Translation Task, a large scale
manual annotation was conducted to compare the
systems. We used these collected human judge-
ments for the evalution of the automatic metrics.
The participants in the manual annotation were
asked to evaluate system outputs by ranking trans-
lated sentences relative to each other. For each
source segment that was included in the procedure,
the annotator was shown the outputs of five sys-
tems to which he or she was supposed to assign
ranks. Ties were allowed.
These collected rank labels for each five-tuple
of systems were then interpreted as 10 pairwise
comparisons of systems and used to assign each
system a score that reflects how high that system
was usually ranked by the annotators. Please see
the WMT14 overview paper for details on how this
score is computed. You can also find inter- and
intra-annotator agreement estimates there.
2.2 Participants of the Metrics Shared Task
Table 1 lists the participants of WMT14 Shared
Metrics Task, along with their metrics. We have
293
Metric Participant
APAC Hokkai-Gakuen University (Echizen?ya, 2014)
BEER ILLC ? University of Amsterdam (Stanojevic and Sima?an, 2014)
RED-* Dublin City University (Wu and Yu, 2014)
DISCOTK-* Qatar Computing Research Institute (Guzman et al., 2014)
ELEXR University of Tehran (Mahmoudi et al., 2013)
LAYERED Indian Institute of Technology, Bombay (Gautam and Bhattacharyya, 2014)
METEOR Carnegie Mellon University (Denkowski and Lavie, 2014)
AMBER, BLEU-NRC National Research Council of Canada (Chen and Cherry, 2014)
PARMESAN Charles University in Prague (Baran?c??kov?a, 2014)
TBLEU Charles University in Prague (Libovick?y and Pecina, 2014)
UPC-IPA, UPC-STOUT Technical University of Catalunya (Gonz`alez et al., 2014)
VERTA-W, VERTA-EQ University of Barcelona (Comelles and Atserias, 2014)
Table 1: Participants of WMT14 Metrics Shared Task
collected 23 metrics from a total of 12 research
groups.
In addition to that we have computed the fol-
lowing two groups of standard metrics as base-
lines:
? Mteval. The metrics BLEU (Papineni
et al., 2002) and NIST (Dodding-
ton, 2002) were computed using the
script mteval-v13a.pl
2
which is
used in the OpenMT Evaluation Cam-
paign and includes its own tokeniza-
tion. We run mteval with the flag
--international-tokenization
since it performs slightly better (Mach?a?cek
and Bojar, 2013).
? Moses Scorer. The metrics TER (Snover
et al., 2006), WER, PER and CDER (Leusch
et al., 2006) were computed using the Moses
scorer which is used in Moses model opti-
mization. To tokenize the sentences we used
the standard tokenizer script as available in
Moses toolkit.
We have normalized all metrics? scores such
that better translations get higher scores.
3 System-Level Metric Analysis
While the Spearman?s ? correlation coefficient
was used as the main measure of system-level met-
rics? quality in the past, we have decided to use
Pearson correlation coefficient as the main mea-
sure this year. At the end of this section we give
reasons for this change.
We use the following formula to compute the
Pearson?s r for each metric and translation direc-
tion:
2
http://www.itl.nist.gov/iad/mig/
/tools/
r =
?
n
i=1
(H
i
?
?
H)(M
i
?
?
M)
?
?
n
i=1
(H
i
?
?
H)
2
?
?
n
i=1
(M
i
?
?
M)
2
(1)
where H is the vector of human scores of all sys-
tems translating in the given direction, M is the
vector of the corresponding scores as predicted by
the given metric.
?
H and
?
M are their means re-
spectively.
Since we have normalized all metrics such that
better translations get higher score, we consider
metrics with values of Pearson?s r closer to 1 as
better.
You can find the system-level correlations for
translations into English in Table 2 and for trans-
lations out of English in Table 3. Each row in the
tables contains correlations of a metric in each of
the examined translation directions. The metrics
are sorted by average Pearson correlation coeffi-
cient across translation directions. The best results
in each direction are in bold.
The reported empirical confidence intervals of
system level correlations were obtained through
bootstrap resampling of 1000 samples (confidence
level of 95 %).
As in previous years, a lot of metrics outper-
formed BLEU in system level correlation. In
into-English directions, metric DISCOTK-PARTY-
TUNED has the highest correlation in two lan-
guage directions and it is also the best correlated
metric on average according to both Pearson and
Spearman?s coefficients. The second best corre-
lated metric on average (according to Pearson) is
LAYERED which is also the single best metric
in Hindi-to-English direction. Metrics REDSYS
and REDSYSSENT are quite unstable, they win
in French-to-English and Czech-to-English direc-
tions respectively but they perform very poorly in
294
other directions.
Except METEOR, none of the participants took
part in the last year metrics task. We can there-
fore compare current and last year results only
for METEOR and baseline metrics. METEOR, the
last year winner, performs generally well in some
directions but it horribly suffers when evaluating
translations from non-Latin script (Russian and es-
pecially Hindi). For the baseline metrics the re-
sults are quite similar across the years. In both
years BLEU performs best among baseline met-
rics, closely followed by CDER. NIST is in the
middle of the list in both years. The remaining
baseline metrics TER, WER and PER perform
much worse.
The results into German are markedly lower
and have broader confidence intervals than the re-
sults in other directions. This could be explained
by a very high number (18) of participating sys-
tems of similar quality. Both human judgements
and automatic metrics are negatively affected by
these circumstances. To preserve the reliability of
overall metrics? performance across languages, we
decided to exclude English-to-German direction
from the average Pearson and Spearman?s corre-
lation coefficients.
In other out-of-English directions, the best cor-
related metric on average according to Pearson co-
efficient is NIST, even though it does not win in
any single direction. CDER is the second best ac-
cording to Pearson and the best metric according
to Spearman?s. Again it does not win in any single
direction. The metrics PER and WER are quite
unstable. Each of them wins in two directions but
performs very badly in others.
Compared to the last year results, the order of
metrics participating in both years is quite simi-
lar: NIST and CDER performed very well both
years, followed by BLEU. The metrics TER and
WER are again at the end of the list. An interest-
ing change is that PER perform much better this
year.
3.1 Reasons for Pearson correlation
coefficient
In the translation task, there are often similar sys-
tems with human scores very close to each other. It
can therefore easily happen that even a good met-
ric compares two similar systems differently from
humans. We believe that the penalty incurred by
the metric for such a swap should somehow reflect
that the systems were hard to separate.
Since the Spearman?s ? converts both human
and metric scores to ranks and therefore disregards
the absolute differences in the scores, it does ex-
actly what we feel is not fair. The Pearson corre-
lation coefficient does not suffer from this prob-
lem. We are aware of the fact that Pearson cor-
relation coefficient also reflects whether the rela-
tion between manual and automatic scores is lin-
ear (as opposed to e.g. quadratic). We don?t think
this would be negatively affecting any of the met-
rics since overall, the systems are of a comparable
quality and the metrics are likely to behave lin-
early in this small range of scores.
Moreover, the general agreement to adopt Pear-
son instead of Spearman?s correlation coefficient
was already apparent during the WMT12 work-
shop. This change just did not get through for
WMT13.
4 Segment-Level Metric Analysis
We measure the quality of metrics? segment-level
scores using Kendall?s ? rank correlation coeffi-
cient. In this type of evaluation, a metric is ex-
pected to predict the result of the manual pairwise
comparison of two systems. Note that the golden
truth is obtained from a compact annotation of five
systems at once, while an experiment with text-to-
speech evaluation techniques by Vazquez-Alvarez
and Huckvale (2002) suggests that a genuine pair-
wise comparison is likely to lead to more stable
results.
In the past, slightly different variations of
Kendall?s ? computation were used in the Metrics
Tasks. Also some of the participants have noticed
a problem with ties in the WMT13 method. There-
fore, we discuss several possible variants in detail
in this paper.
4.1 Notation for Kendall?s ? computation
The basic formula for Kendall?s ? is:
? =
|Concordant| ? |Discordant|
|Concordant|+ |Discordant|
(2)
where Concordant is the set of all human com-
parisons for which a given metric suggests the
same order andDiscordant is the set of all human
comparisons for which a given metric disagrees.
In the original Kendall?s ? , comparisons with hu-
man or metric ties are considered neither concor-
dant nor discordant. However in the past, Metrics
295
C
o
r
r
e
l
a
t
i
o
n
c
o
e
f
fi
c
i
e
n
t
P
e
a
r
s
o
n
C
o
r
r
e
l
a
t
i
o
n
C
o
e
f
fi
c
i
e
n
t
S
p
e
a
r
m
a
n
?
s
D
i
r
e
c
t
i
o
n
f
r
-
e
n
d
e
-
e
n
h
i
-
e
n
c
s
-
e
n
r
u
-
e
n
A
v
e
r
a
g
e
A
v
e
r
a
g
e
C
o
n
s
i
d
e
r
e
d
S
y
s
t
e
m
s
8
1
3
9
5
1
3
D
I
S
C
O
T
K
-
P
A
R
T
Y
-
T
U
N
E
D
.
9
7
7
?
.
0
0
9
.
9
4
3
?
.
0
2
0
.
9
5
6
?
.
0
0
7
.
9
7
5
?
.
0
3
1
.
8
7
0
?
.
0
2
2
.
9
4
4
?
.
0
1
8
.
9
1
2
?
.
0
4
3
L
A
Y
E
R
E
D
.
9
7
3
?
.
0
0
9
.
8
9
3
?
.
0
2
6
.
9
7
6
?
.
0
0
6
.
9
4
1
?
.
0
4
5
.
8
5
4
?
.
0
2
3
.
9
2
7
?
.
0
2
2
.
8
9
4
?
.
0
4
7
D
I
S
C
O
T
K
-
P
A
R
T
Y
.
9
7
0
?
.
0
1
0
.
9
2
1
?
.
0
2
4
.
8
6
2
?
.
0
1
5
.
9
8
3
?
.
0
2
5
.
8
5
6
?
.
0
2
3
.
9
1
8
?
.
0
1
9
.
8
5
6
?
.
0
4
6
U
P
C
-
S
T
O
U
T
.
9
6
8
?
.
0
1
0
.
9
1
5
?
.
0
2
5
.
8
9
8
?
.
0
1
3
.
9
4
8
?
.
0
4
0
.
8
3
7
?
.
0
2
4
.
9
1
3
?
.
0
2
2
o
.
9
0
1
?
.
0
4
5
V
E
R
T
A
-
W
.
9
5
9
?
.
0
1
1
.
8
6
7
?
.
0
2
9
.
9
2
0
?
.
0
1
1
.
9
3
4
?
.
0
5
0
.
8
4
8
?
.
0
2
4
.
9
0
6
?
.
0
2
5
.
8
6
8
?
.
0
4
5
V
E
R
T
A
-
E
Q
.
9
5
9
?
.
0
1
1
.
8
5
4
?
.
0
3
1
.
9
2
7
?
.
0
1
0
.
9
3
8
?
.
0
4
8
.
8
4
2
?
.
0
2
4
.
9
0
4
?
.
0
2
5
.
8
5
7
?
.
0
4
6
T
B
L
E
U
.
9
5
2
?
.
0
1
2
.
8
3
2
?
.
0
3
4
.
9
5
4
?
.
0
0
7
.
9
5
7
?
.
0
4
0
.
8
0
3
?
.
0
2
7
.
9
0
0
?
.
0
2
4
.
8
4
1
?
.
0
5
6
B
L
E
U
N
R
C
.
9
5
3
?
.
0
1
2
.
8
2
3
?
.
0
3
5
.
9
5
9
?
.
0
0
7
.
9
4
6
?
.
0
4
4
.
7
8
7
?
.
0
2
8
.
8
9
4
?
.
0
2
5
o
.
8
5
5
?
.
0
5
6
B
L
E
U
.
9
5
2
?
.
0
1
2
.
8
3
2
?
.
0
3
4
.
9
5
6
?
.
0
0
7
.
9
0
9
?
.
0
5
4
.
7
8
9
?
.
0
2
7
.
8
8
8
?
.
0
2
7
.
8
3
3
?
.
0
5
8
U
P
C
-
I
P
A
.
9
6
6
?
.
0
1
0
.
8
9
5
?
.
0
2
7
.
9
1
4
?
.
0
1
0
.
8
2
4
?
.
0
7
3
.
8
1
2
?
.
0
2
6
.
8
8
2
?
.
0
2
9
o
.
8
5
8
?
.
0
4
4
C
D
E
R
.
9
5
4
?
.
0
1
2
.
8
2
3
?
.
0
3
4
.
8
2
6
?
.
0
1
6
.
9
6
5
?
.
0
3
5
.
8
0
2
?
.
0
2
7
.
8
7
4
?
.
0
2
5
.
8
0
7
?
.
0
5
0
A
P
A
C
.
9
6
3
?
.
0
1
0
.
8
1
7
?
.
0
3
4
.
7
9
0
?
.
0
1
6
.
9
8
2
?
.
0
2
6
.
8
1
6
?
.
0
2
6
.
8
7
4
?
.
0
2
2
.
8
0
7
?
.
0
4
9
R
E
D
S
Y
S
.
9
8
1
?
.
0
0
8
.
8
9
8
?
.
0
2
6
.
6
7
6
?
.
0
2
2
.
9
8
9
?
.
0
2
1
.
8
1
4
?
.
0
2
6
.
8
7
2
?
.
0
2
1
.
7
8
6
?
.
0
4
7
R
E
D
S
Y
S
S
E
N
T
.
9
8
0
?
.
0
0
8
.
9
1
0
?
.
0
2
4
.
6
4
4
?
.
0
2
3
.
9
9
3
?
.
0
1
8
.
8
0
7
?
.
0
2
7
.
8
6
7
?
.
0
2
0
.
7
7
1
?
.
0
4
3
N
I
S
T
.
9
5
5
?
.
0
1
1
.
8
1
1
?
.
0
3
5
.
7
8
4
?
.
0
1
6
.
9
8
3
?
.
0
2
5
.
8
0
0
?
.
0
2
7
.
8
6
7
?
.
0
2
3
o
.
8
2
4
?
.
0
5
5
D
I
S
C
O
T
K
-
L
I
G
H
T
.
9
6
5
?
.
0
1
1
.
9
3
5
?
.
0
2
2
.
5
5
7
?
.
0
2
5
.
9
5
4
?
.
0
3
8
.
7
9
1
?
.
0
2
7
.
8
4
0
?
.
0
2
4
.
7
7
4
?
.
0
4
6
M
E
T
E
O
R
.
9
7
5
?
.
0
0
9
.
9
2
7
?
.
0
2
2
.
4
5
7
?
.
0
2
7
.
9
8
0
?
.
0
2
9
.
8
0
5
?
.
0
2
6
.
8
2
9
?
.
0
2
3
o
.
7
8
8
?
.
0
4
6
T
E
R
.
9
5
2
?
.
0
1
2
.
7
7
5
?
.
0
3
8
.
6
1
8
?
.
0
2
1
.
9
7
6
?
.
0
3
1
.
8
0
9
?
.
0
2
7
.
8
2
6
?
.
0
2
6
.
7
4
6
?
.
0
5
7
W
E
R
.
9
5
2
?
.
0
1
2
.
7
6
2
?
.
0
3
8
.
6
1
0
?
.
0
2
1
.
9
7
4
?
.
0
3
3
.
8
0
9
?
.
0
2
7
.
8
2
1
?
.
0
2
6
.
7
3
6
?
.
0
5
8
A
M
B
E
R
.
9
4
8
?
.
0
1
2
.
9
1
0
?
.
0
2
6
.
5
0
6
?
.
0
2
6
.
7
4
4
?
.
0
9
5
.
7
9
7
?
.
0
2
7
.
7
8
1
?
.
0
3
7
.
7
2
8
?
.
0
5
1
P
E
R
.
9
4
6
?
.
0
1
3
.
8
6
7
?
.
0
3
1
.
4
1
1
?
.
0
2
5
.
8
8
3
?
.
0
6
3
.
7
9
9
?
.
0
2
8
.
7
8
1
?
.
0
3
2
.
6
9
8
?
.
0
4
7
E
L
E
X
R
.
9
7
1
?
.
0
0
9
.
8
5
7
?
.
0
3
1
.
5
3
5
?
.
0
2
6
.
9
4
5
?
.
0
4
4
?
.
4
0
4
?
.
0
4
5
.
5
8
1
?
.
0
3
1
.
6
5
2
?
.
0
4
6
T
a
b
l
e
2
:
S
y
s
t
e
m
-
l
e
v
e
l
c
o
r
r
e
l
a
t
i
o
n
s
o
f
a
u
t
o
m
a
t
i
c
e
v
a
l
u
a
t
i
o
n
m
e
t
r
i
c
s
a
n
d
t
h
e
o
f
fi
c
i
a
l
W
M
T
h
u
m
a
n
s
c
o
r
e
s
w
h
e
n
t
r
a
n
s
l
a
t
i
n
g
i
n
t
o
E
n
g
l
i
s
h
.
T
h
e
s
y
m
b
o
l
?
o
?
i
n
d
i
c
a
t
e
s
w
h
e
r
e
t
h
e
S
p
e
a
r
m
a
n
?
s
?
a
v
e
r
a
g
e
i
s
o
u
t
o
f
s
e
q
u
e
n
c
e
c
o
m
p
a
r
e
d
t
o
t
h
e
m
a
i
n
P
e
a
r
s
o
n
a
v
e
r
a
g
e
.
296
C
o
r
r
e
l
a
t
i
o
n
c
o
e
f
fi
c
i
e
n
t
P
e
a
r
s
o
n
C
o
r
r
e
l
a
t
i
o
n
C
o
e
f
fi
c
i
e
n
t
S
p
e
a
r
m
a
n
?
s
D
i
r
e
c
t
i
o
n
e
n
-
f
r
e
n
-
h
i
e
n
-
c
s
e
n
-
r
u
A
v
e
r
a
g
e
e
n
-
d
e
A
v
e
r
a
g
e
C
o
n
s
i
d
e
r
e
d
S
y
s
t
e
m
s
1
3
1
2
1
0
9
1
8
(
e
x
c
l
.
e
n
-
d
e
)
N
I
S
T
.
9
4
1
?
.
0
2
2
.
9
8
1
?
.
0
0
6
.
9
8
5
?
.
0
0
6
.
9
2
7
?
.
0
1
2
.
9
5
9
?
.
0
1
2
.
2
0
0
?
.
0
4
6
.
8
5
0
?
.
0
3
0
C
D
E
R
.
9
4
9
?
.
0
2
0
.
9
4
9
?
.
0
1
0
.
9
8
2
?
.
0
0
6
.
9
3
8
?
.
0
1
1
.
9
5
5
?
.
0
1
2
.
2
7
8
?
.
0
4
5
.
8
4
0
?
.
0
3
6
A
M
B
E
R
.
9
2
8
?
.
0
2
3
.
9
9
0
?
.
0
0
4
.
9
7
2
?
.
0
0
8
.
9
2
6
?
.
0
1
2
.
9
5
4
?
.
0
1
2
.
2
4
1
?
.
0
4
5
.
8
1
7
?
.
0
4
1
M
E
T
E
O
R
.
9
4
1
?
.
0
2
1
.
9
7
5
?
.
0
0
7
.
9
7
6
?
.
0
0
7
.
9
2
3
?
.
0
1
3
.
9
5
4
?
.
0
1
2
.
2
6
3
?
.
0
4
5
.
8
0
6
?
.
0
3
9
B
L
E
U
.
9
3
7
?
.
0
2
2
.
9
7
3
?
.
0
0
7
.
9
7
6
?
.
0
0
7
.
9
1
5
?
.
0
1
3
.
9
5
0
?
.
0
1
2
.
2
1
6
?
.
0
4
6
o
.
8
0
9
?
.
0
3
6
P
E
R
.
9
3
6
?
.
0
2
3
.
9
3
1
?
.
0
1
1
.
9
8
8
?
.
0
0
5
.
9
4
1
?
.
0
1
1
.
9
4
9
?
.
0
1
3
.
1
9
0
?
.
0
4
7
o
.
8
2
3
?
.
0
3
7
A
P
A
C
.
9
5
0
?
.
0
2
0
.
9
4
0
?
.
0
1
1
.
9
7
3
?
.
0
0
8
.
9
2
9
?
.
0
1
2
.
9
4
8
?
.
0
1
3
.
3
4
6
?
.
0
4
4
.
7
9
9
?
.
0
4
1
T
B
L
E
U
.
9
3
2
?
.
0
2
3
.
9
6
8
?
.
0
0
8
.
9
7
3
?
.
0
0
8
.
9
1
2
?
.
0
1
3
.
9
4
6
?
.
0
1
3
.
2
3
9
?
.
0
4
6
o
.
8
0
5
?
.
0
3
9
B
L
E
U
N
R
C
.
9
3
3
?
.
0
2
2
.
9
7
1
?
.
0
0
7
.
9
7
4
?
.
0
0
8
.
9
0
1
?
.
0
1
4
.
9
4
5
?
.
0
1
3
.
2
0
5
?
.
0
4
6
o
.
8
0
9
?
.
0
3
9
E
L
E
X
R
.
8
8
5
?
.
0
2
9
.
9
6
2
?
.
0
0
9
.
9
7
9
?
.
0
0
7
.
9
3
8
?
.
0
1
1
.
9
4
1
?
.
0
1
4
.
2
6
0
?
.
0
4
4
.
7
6
8
?
.
0
3
6
T
E
R
.
9
5
4
?
.
0
1
9
.
8
2
9
?
.
0
1
7
.
9
7
8
?
.
0
0
7
.
9
3
1
?
.
0
1
2
.
9
2
3
?
.
0
1
4
.
3
2
4
?
.
0
4
5
.
7
4
5
?
.
0
3
5
W
E
R
.
9
6
0
?
.
0
1
8
.
5
1
6
?
.
0
2
6
.
9
7
6
?
.
0
0
7
.
9
3
2
?
.
0
1
1
.
8
4
6
?
.
0
1
6
.
3
5
7
?
.
0
4
5
.
6
9
6
?
.
0
3
7
P
A
R
M
E
S
A
N
n
/
a
n
/
a
.
9
6
2
?
.
0
0
9
n
/
a
.
9
6
2
?
.
0
0
9
n
/
a
.
9
1
5
?
.
0
4
8
U
P
C
-
I
P
A
.
9
4
0
?
.
0
2
1
n
/
a
.
9
6
9
?
.
0
0
8
.
9
2
1
?
.
0
1
3
.
9
4
3
?
.
0
1
4
.
2
8
5
?
.
0
4
5
.
7
8
5
?
.
0
5
0
R
E
D
S
Y
S
S
E
N
T
.
9
4
1
?
.
0
2
1
n
/
a
n
/
a
n
/
a
.
9
4
1
?
.
0
2
1
.
2
0
8
?
.
0
4
5
o
.
9
6
2
?
.
0
3
8
R
E
D
S
Y
S
.
9
4
0
?
.
0
2
1
n
/
a
n
/
a
n
/
a
.
9
4
0
?
.
0
2
1
.
2
0
8
?
.
0
4
5
.
9
6
2
?
.
0
3
8
U
P
C
-
S
T
O
U
T
.
9
4
0
?
.
0
2
1
n
/
a
.
9
3
8
?
.
0
1
1
.
9
1
9
?
.
0
1
3
.
9
3
3
?
.
0
1
5
.
3
0
1
?
.
0
4
4
.
7
1
3
?
.
0
4
0
T
a
b
l
e
3
:
S
y
s
t
e
m
-
l
e
v
e
l
c
o
r
r
e
l
a
t
i
o
n
s
o
f
a
u
t
o
m
a
t
i
c
e
v
a
l
u
a
t
i
o
n
m
e
t
r
i
c
s
a
n
d
t
h
e
o
f
fi
c
i
a
l
W
M
T
h
u
m
a
n
s
c
o
r
e
s
w
h
e
n
t
r
a
n
s
l
a
t
i
n
g
o
u
t
o
f
E
n
g
l
i
s
h
.
T
h
e
s
y
m
b
o
l
?
o
?
i
n
d
i
c
a
t
e
s
w
h
e
r
e
t
h
e
S
p
e
a
r
m
a
n
?
s
?
a
v
e
r
a
g
e
i
s
o
u
t
o
f
s
e
q
u
e
n
c
e
c
o
m
p
a
r
e
d
t
o
t
h
e
m
a
i
n
P
e
a
r
s
o
n
a
v
e
r
a
g
e
.
297
Tasks (Callison-Burch et al. (2012) and earlier),
comparisons with human ties were considered as
discordant.
To easily see which pairs are counted as concor-
dant and which as discordant, we have developed
the following tabular notation. This is for example
the WMT12 method:
Metric
WMT12 < = >
H
u
m
a
n
< 1 -1 -1
= X X X
> -1 -1 1
Given such a matrix C
h,m
where h,m ? {<,=
, >}
3
and a metric we compute the Kendall?s ? the
following way:
We insert each extracted human pairwise com-
parison into exactly one of the nine sets S
h,m
ac-
cording to human and metric ranks. For example
the set S
<,>
contains all comparisons where the
left-hand system was ranked better than right-hand
system by humans and it was ranked the other way
round by the metric in question.
To compute the numerator of Kendall?s ? , we
take the coefficients from the matrix C
h,m
, use
them to multiply the sizes of the corresponding
sets S
h,m
and then sum them up. We do not in-
clude sets for which the value of C
h,m
is X. To
compute the denominator of Kendall?s ? , we sim-
ply sum the sizes of all the sets S
h,m
except those
where C
h,m
= X. To define it formally:
? =
?
h,m?{<,=,>}
C
h,m
6=X
C
h,m
|S
h,m
|
?
h,m?{<,=,>}
C
h,m
6=X
|S
h,m
|
(3)
4.2 Discussion on Kendall?s ? computation
In 2013, we thought that metric ties should not be
penalized and we decided to excluded them like
the human ties. We will denote this method as
WMT13:
Metric
WMT13 < = >
H
u
m
a
n
< 1 X -1
= X X X
> -1 X 1
It turned out, however, that it was not a good idea:
metrics could game the scoring by avoiding hard
3
Here the relation < always means ?is better than? even
for metrics where the better system receives a higher score.
cases and assigning lots of ties. A natural solution
is to count the metrics ties also in denominator to
avoid the problem. We will denote this variant as
WMT14:
Metric
WMT14 < = >
H
u
m
a
n
< 1 0 -1
= X X X
> -1 0 1
The WMT14 variant does not allow for gaming
the scoring like the WMT13 variant does. Com-
pared to WMT12 method, WMT14 does not pe-
nalize ties.
We were also considering to get human ties in-
volved. The most natural variant would be the fol-
lowing variant denoted as HTIES:
Metric
HTIES < = >
H
u
m
a
n
< 1 0 -1
= 0 1 0
> -1 0 1
Unfortunately this method allows for gaming the
scoring as well. The least risky choice for metrics
in hard cases would be to assign a tie because it
cannot worsen the Kendall?s ? and there is quite a
high chance that the human rank is also a tie. Met-
rics could be therefore tuned to predict ties often
but such metrics are not very useful. For example,
the simplistic metric which assigns the same score
to all candidates (and therefore all pairs would be
tied by the metric) would get the score equal to
the proportion of ties in all human comparisons. It
would become one of the best performing metrics
in WMT13 even though it is not informative at all.
We have decided to use WMT14 variant as the
main evaluation measure this year, however, we
are also reporting average scores computed by
other variants.
4.3 Kendall?s ? results
The final Kendall?s ? results are shown in Table 4
for directions into English and in Table 5 for di-
rections out of English. Each row in the tables
contains correlations of a metric in given direc-
tions. The metrics are sorted by average corre-
lation across translation directions. The highest
correlation in each column is in bold. The ta-
bles also contain average Kendall?s ? computed by
other variants including the variant WMT13 used
last year. Metrics which did not compute scores in
all directions are at the bottom of the tables. The
298
D
i
r
e
c
t
i
o
n
f
r
-
e
n
d
e
-
e
n
h
i
-
e
n
c
s
-
e
n
r
u
-
e
n
A
v
g
A
v
e
r
a
g
e
s
o
f
o
t
h
e
r
v
a
r
i
a
n
t
s
o
f
K
e
n
d
a
l
l
?
s
?
E
x
t
r
a
c
t
e
d
-
p
a
i
r
s
2
6
0
9
0
2
5
2
6
0
2
0
9
0
0
2
1
1
3
0
3
4
4
6
0
W
M
T
1
2
W
M
T
1
3
H
T
I
E
S
D
I
S
C
O
T
K
-
P
A
R
T
Y
-
T
U
N
E
D
.
4
3
3
?
.
0
1
2
.
3
8
0
?
.
0
1
3
.
4
3
4
?
.
0
1
3
.
3
2
8
?
.
0
1
5
.
3
5
5
?
.
0
1
1
.
3
8
6
?
.
0
1
3
.
3
8
6
?
.
0
1
3
.
3
8
6
?
.
0
1
3
.
3
0
6
?
.
0
1
0
B
E
E
R
.
4
1
7
?
.
0
1
3
.
3
3
7
?
.
0
1
4
.
4
3
8
?
.
0
1
3
.
2
8
4
?
.
0
1
6
.
3
3
3
?
.
0
1
1
.
3
6
2
?
.
0
1
3
.
3
5
8
?
.
0
1
3
.
3
6
3
?
.
0
1
3
o
.
3
1
8
?
.
0
1
1
R
E
D
C
O
M
B
S
E
N
T
.
4
0
6
?
.
0
1
2
.
3
3
8
?
.
0
1
4
.
4
1
7
?
.
0
1
3
.
2
8
4
?
.
0
1
5
.
3
3
6
?
.
0
1
1
.
3
5
6
?
.
0
1
3
.
3
4
6
?
.
0
1
3
.
3
6
0
?
.
0
1
3
.
3
1
7
?
.
0
1
1
R
E
D
C
O
M
B
S
Y
S
S
E
N
T
.
4
0
8
?
.
0
1
2
.
3
3
8
?
.
0
1
4
.
4
1
6
?
.
0
1
3
.
2
8
2
?
.
0
1
4
.
3
3
6
?
.
0
1
1
.
3
5
6
?
.
0
1
3
.
3
4
6
?
.
0
1
3
.
3
5
9
?
.
0
1
3
.
3
1
6
?
.
0
1
0
M
E
T
E
O
R
.
4
0
6
?
.
0
1
2
.
3
3
4
?
.
0
1
4
.
4
2
0
?
.
0
1
3
.
2
8
2
?
.
0
1
5
.
3
2
9
?
.
0
1
0
.
3
5
4
?
.
0
1
3
.
3
4
1
?
.
0
1
3
.
3
5
9
?
.
0
1
3
o
.
3
1
7
?
.
0
1
0
R
E
D
S
Y
S
S
E
N
T
.
4
0
4
?
.
0
1
2
.
3
3
8
?
.
0
1
4
.
3
8
6
?
.
0
1
4
.
2
8
3
?
.
0
1
5
.
3
2
1
?
.
0
1
0
.
3
4
6
?
.
0
1
3
.
3
3
5
?
.
0
1
3
.
3
5
0
?
.
0
1
3
.
3
0
9
?
.
0
1
0
R
E
D
S
E
N
T
.
4
0
3
?
.
0
1
2
.
3
3
6
?
.
0
1
4
.
3
8
3
?
.
0
1
4
.
2
8
3
?
.
0
1
5
.
3
2
3
?
.
0
1
1
.
3
4
5
?
.
0
1
3
.
3
3
4
?
.
0
1
3
.
3
4
9
?
.
0
1
3
.
3
0
8
?
.
0
1
0
U
P
C
-
I
P
A
.
4
1
2
?
.
0
1
2
.
3
4
0
?
.
0
1
4
.
3
6
8
?
.
0
1
4
.
2
7
4
?
.
0
1
5
.
3
1
6
?
.
0
1
1
.
3
4
2
?
.
0
1
3
o
.
3
4
0
?
.
0
1
4
.
3
4
3
?
.
0
1
4
.
3
0
0
?
.
0
1
1
U
P
C
-
S
T
O
U
T
.
4
0
3
?
.
0
1
2
.
3
4
5
?
.
0
1
4
.
3
5
2
?
.
0
1
4
.
2
7
5
?
.
0
1
5
.
3
1
7
?
.
0
1
1
.
3
3
8
?
.
0
1
3
.
3
3
6
?
.
0
1
3
.
3
3
9
?
.
0
1
3
.
2
9
4
?
.
0
1
1
V
E
R
T
A
-
W
.
3
9
9
?
.
0
1
3
.
3
2
1
?
.
0
1
5
.
3
8
6
?
.
0
1
4
.
2
6
3
?
.
0
1
5
.
3
1
5
?
.
0
1
1
.
3
3
7
?
.
0
1
4
.
3
2
0
?
.
0
1
4
o
.
3
4
2
?
.
0
1
4
o
.
3
0
4
?
.
0
1
1
V
E
R
T
A
-
E
Q
.
4
0
7
?
.
0
1
3
.
3
1
5
?
.
0
1
4
.
3
8
4
?
.
0
1
3
.
2
6
3
?
.
0
1
5
.
3
1
2
?
.
0
1
1
.
3
3
6
?
.
0
1
3
o
.
3
2
3
?
.
0
1
3
.
3
4
1
?
.
0
1
3
.
3
0
2
?
.
0
1
1
D
I
S
C
O
T
K
-
P
A
R
T
Y
.
3
9
5
?
.
0
1
3
.
3
3
4
?
.
0
1
4
.
3
6
2
?
.
0
1
3
.
2
6
4
?
.
0
1
6
.
3
0
5
?
.
0
1
1
.
3
3
2
?
.
0
1
3
o
.
3
3
2
?
.
0
1
3
.
3
3
2
?
.
0
1
3
.
2
6
3
?
.
0
1
1
A
M
B
E
R
.
3
6
7
?
.
0
1
3
.
3
1
3
?
.
0
1
4
.
3
6
2
?
.
0
1
3
.
2
4
6
?
.
0
1
6
.
2
9
4
?
.
0
1
1
.
3
1
6
?
.
0
1
3
.
3
0
2
?
.
0
1
3
.
3
2
1
?
.
0
1
4
o
.
2
8
6
?
.
0
1
1
B
L
E
U
N
R
C
.
3
8
2
?
.
0
1
3
.
2
7
2
?
.
0
1
4
.
3
2
2
?
.
0
1
4
.
2
2
6
?
.
0
1
6
.
2
6
9
?
.
0
1
1
.
2
9
4
?
.
0
1
3
.
2
6
7
?
.
0
1
4
.
3
0
3
?
.
0
1
4
.
2
7
1
?
.
0
1
1
S
E
N
T
B
L
E
U
.
3
7
8
?
.
0
1
3
.
2
7
1
?
.
0
1
4
.
3
0
0
?
.
0
1
3
.
2
1
3
?
.
0
1
6
.
2
6
3
?
.
0
1
1
.
2
8
5
?
.
0
1
3
.
2
5
8
?
.
0
1
4
.
2
9
3
?
.
0
1
4
.
2
6
4
?
.
0
1
1
A
P
A
C
.
3
6
4
?
.
0
1
2
.
2
7
1
?
.
0
1
4
.
2
8
8
?
.
0
1
4
.
1
9
8
?
.
0
1
6
.
2
7
6
?
.
0
1
1
.
2
7
9
?
.
0
1
3
.
2
4
3
?
.
0
1
4
.
2
9
0
?
.
0
1
4
.
2
6
1
?
.
0
1
1
D
I
S
C
O
T
K
-
L
I
G
H
T
.
3
1
1
?
.
0
1
4
.
2
2
4
?
.
0
1
5
.
2
3
8
?
.
0
1
3
.
1
8
7
?
.
0
1
6
.
2
0
9
?
.
0
1
1
.
2
3
4
?
.
0
1
4
.
2
3
4
?
.
0
1
4
.
2
3
4
?
.
0
1
4
.
1
8
4
?
.
0
1
1
D
I
S
C
O
T
K
-
L
I
G
H
T
-
K
O
O
L
.
0
0
5
?
.
0
0
1
.
0
0
1
?
.
0
0
0
.
0
0
0
?
.
0
0
0
.
0
0
2
?
.
0
0
1
.
0
0
1
?
.
0
0
0
.
0
0
2
?
.
0
0
1
?
.
9
9
6
?
.
0
0
1
o
.
6
7
6
?
.
2
5
6
o
.
2
1
1
?
.
0
0
5
T
a
b
l
e
4
:
S
e
g
m
e
n
t
-
l
e
v
e
l
K
e
n
d
a
l
l
?
s
?
c
o
r
r
e
l
a
t
i
o
n
s
o
f
a
u
t
o
m
a
t
i
c
e
v
a
l
u
a
t
i
o
n
m
e
t
r
i
c
s
a
n
d
t
h
e
o
f
fi
c
i
a
l
W
M
T
h
u
m
a
n
j
u
d
g
e
m
e
n
t
s
w
h
e
n
t
r
a
n
s
l
a
t
i
n
g
i
n
t
o
E
n
g
l
i
s
h
.
T
h
e
l
a
s
t
t
h
r
e
e
c
o
l
u
m
n
s
c
o
n
t
a
i
n
a
v
e
r
a
g
e
K
e
n
d
a
l
l
?
s
?
c
o
m
p
u
t
e
d
b
y
o
t
h
e
r
v
a
r
i
a
n
t
s
.
T
h
e
s
y
m
b
o
l
?
o
?
i
n
d
i
c
a
t
e
s
w
h
e
r
e
t
h
e
a
v
e
r
a
g
e
s
o
f
o
t
h
e
r
v
a
r
i
a
n
t
s
a
r
e
o
u
t
o
f
s
e
q
u
e
n
c
e
c
o
m
p
a
r
e
d
t
o
t
h
e
W
M
T
1
4
v
a
r
i
a
n
t
.
D
i
r
e
c
t
i
o
n
e
n
-
f
r
e
n
-
d
e
e
n
-
h
i
e
n
-
c
s
e
n
-
r
u
A
v
g
A
v
e
r
a
g
e
s
o
f
o
t
h
e
r
v
a
r
i
a
n
t
s
o
f
K
e
n
d
a
l
l
?
s
?
E
x
t
r
a
c
t
e
d
-
p
a
i
r
s
3
3
3
5
0
5
4
6
6
0
2
8
1
2
0
5
5
9
0
0
2
8
9
6
0
W
M
T
1
2
W
M
T
1
3
H
T
I
E
S
B
E
E
R
.
2
9
2
?
.
0
1
2
.
2
6
8
?
.
0
0
9
.
2
5
0
?
.
0
1
3
.
3
4
4
?
.
0
0
9
.
4
4
0
?
.
0
1
3
.
3
1
9
?
.
0
1
1
.
3
1
4
?
.
0
1
1
.
3
2
0
?
.
0
1
1
.
2
7
2
?
.
0
0
9
M
E
T
E
O
R
.
2
8
0
?
.
0
1
2
.
2
3
8
?
.
0
0
9
.
2
6
4
?
.
0
1
2
.
3
1
8
?
.
0
0
9
.
4
2
7
?
.
0
1
2
.
3
0
6
?
.
0
1
1
.
2
8
3
?
.
0
1
1
.
3
1
3
?
.
0
1
1
o
.
2
7
3
?
.
0
0
8
A
M
B
E
R
.
2
6
4
?
.
0
1
2
.
2
2
7
?
.
0
0
9
.
2
8
6
?
.
0
1
2
.
3
0
2
?
.
0
0
9
.
3
9
7
?
.
0
1
3
.
2
9
5
?
.
0
1
1
.
2
6
9
?
.
0
1
1
.
3
0
3
?
.
0
1
1
.
2
6
6
?
.
0
0
9
B
L
E
U
N
R
C
.
2
6
1
?
.
0
1
2
.
2
0
2
?
.
0
0
9
.
2
3
4
?
.
0
1
3
.
2
9
7
?
.
0
0
9
.
3
9
1
?
.
0
1
2
.
2
7
7
?
.
0
1
1
.
2
3
5
?
.
0
1
1
.
2
8
9
?
.
0
1
1
.
2
5
6
?
.
0
0
9
A
P
A
C
.
2
5
3
?
.
0
1
2
.
2
1
0
?
.
0
0
8
.
2
0
3
?
.
0
1
2
.
2
9
2
?
.
0
0
9
.
3
8
8
?
.
0
1
3
.
2
6
9
?
.
0
1
1
.
2
1
7
?
.
0
1
1
.
2
8
5
?
.
0
1
1
.
2
5
2
?
.
0
0
8
S
E
N
T
B
L
E
U
.
2
5
6
?
.
0
1
2
.
1
9
1
?
.
0
0
9
.
2
2
7
?
.
0
1
2
.
2
9
0
?
.
0
0
9
.
3
8
1
?
.
0
1
3
.
2
6
9
?
.
0
1
1
o
.
2
3
2
?
.
0
1
1
.
2
8
0
?
.
0
1
1
.
2
4
6
?
.
0
0
9
U
P
C
-
S
T
O
U
T
.
2
7
9
?
.
0
1
1
.
2
3
4
?
.
0
0
8
n
/
a
.
2
8
2
?
.
0
0
9
.
4
2
5
?
.
0
1
3
.
3
0
5
?
.
0
1
1
.
3
0
0
?
.
0
1
0
.
3
0
6
?
.
0
1
1
.
2
5
6
?
.
0
0
8
U
P
C
-
I
P
A
.
2
6
4
?
.
0
1
2
.
2
2
7
?
.
0
0
9
n
/
a
.
2
9
8
?
.
0
0
9
.
4
2
6
?
.
0
1
3
.
3
0
4
?
.
0
1
1
.
2
9
2
?
.
0
1
1
o
.
3
0
8
?
.
0
1
1
o
.
2
5
9
?
.
0
0
8
R
E
D
S
E
N
T
.
2
9
3
?
.
0
1
2
.
2
4
2
?
.
0
0
9
n
/
a
n
/
a
n
/
a
.
2
6
7
?
.
0
1
0
.
2
4
6
?
.
0
1
0
.
2
7
3
?
.
0
1
1
.
2
5
7
?
.
0
0
8
R
E
D
C
O
M
B
S
Y
S
S
E
N
T
.
2
9
1
?
.
0
1
2
.
2
4
4
?
.
0
0
9
n
/
a
n
/
a
n
/
a
.
2
6
7
?
.
0
1
0
o
.
2
4
9
?
.
0
1
0
.
2
7
2
?
.
0
1
0
.
2
5
6
?
.
0
0
8
R
E
D
C
O
M
B
S
E
N
T
.
2
9
0
?
.
0
1
2
.
2
4
2
?
.
0
0
9
n
/
a
n
/
a
n
/
a
.
2
6
6
?
.
0
1
0
.
2
4
8
?
.
0
1
0
.
2
7
1
?
.
0
1
1
.
2
5
6
?
.
0
0
8
R
E
D
S
Y
S
S
E
N
T
.
2
9
0
?
.
0
1
2
.
2
3
9
?
.
0
0
8
n
/
a
n
/
a
n
/
a
.
2
6
4
?
.
0
1
0
.
2
3
5
?
.
0
1
0
o
.
2
7
3
?
.
0
1
0
o
.
2
5
7
?
.
0
0
8
T
a
b
l
e
5
:
S
e
g
m
e
n
t
-
l
e
v
e
l
K
e
n
d
a
l
l
?
s
?
c
o
r
r
e
l
a
t
i
o
n
s
o
f
a
u
t
o
m
a
t
i
c
e
v
a
l
u
a
t
i
o
n
m
e
t
r
i
c
s
a
n
d
t
h
e
o
f
fi
c
i
a
l
W
M
T
h
u
m
a
n
j
u
d
g
e
m
e
n
t
s
w
h
e
n
t
r
a
n
s
l
a
t
i
n
g
o
u
t
o
f
E
n
g
l
i
s
h
.
T
h
e
l
a
s
t
t
h
r
e
e
c
o
l
u
m
n
s
c
o
n
t
a
i
n
a
v
e
r
a
g
e
K
e
n
d
a
l
l
?
s
?
c
o
m
p
u
t
e
d
b
y
o
t
h
e
r
v
a
r
i
a
n
t
s
.
T
h
e
s
y
m
b
o
l
?
o
?
i
n
d
i
c
a
t
e
s
w
h
e
r
e
t
h
e
a
v
e
r
a
g
e
s
o
f
o
t
h
e
r
v
a
r
i
a
n
t
s
a
r
e
o
u
t
o
f
s
e
q
u
e
n
c
e
c
o
m
p
a
r
e
d
t
o
t
h
e
W
M
T
1
4
v
a
r
i
a
n
t
.
299
possible values of ? range between -1 (a metric al-
ways predicted a different order than humans did)
and 1 (a metric always predicted the same order as
humans). Metrics with a higher ? are better.
We also computed empirical confidence inter-
vals of Kendall?s ? using bootstrap resampling.
We varied the ?golden truth? by sampling from
human judgments. We have generated 1000 new
sets and report the average of the upper and lower
2.5 % empirical bound, which corresponds to the
95 % confidence interval.
In directions into English (Table 4), the
strongest correlated segment-level metric on av-
erage is DISCOTK-PARTY-TUNED followed by
BEER. Unlike the system level correlation, the
results are much more stable here. DISCOTK-
PARTY-TUNED has the highest correlation in 4 of
5 language directions. Generally, the ranking of
metrics is almost the same in each direction.
The only two metrics which also participated
in last year metrics task are METEOR and SENT-
BLEU. In both years, METEOR performed quite
well unlike SENTBLEU which was outperformed
by most of the metrics.
The metric DISCOTK-LIGHT-KOOL is worth
mentioning. It is deliberately designed to assign
the same score for all systems for most of the
segments. It obtained scores very close to zero
(i.e. totally uninformative) in WMT14 variant. In
WMT13 thought it reached the highest score.
In directions out of English (Table 5), the met-
ric with highest correlation on average across all
directions is BEER, followed by METEOR.
5 Conclusion
In this paper, we summarized the results of the
WMT14 Metrics Shared Task, which assesses the
quality of various automatic machine translation
metrics. As in previous years, human judgements
collected in WMT14 serve as the golden truth and
we check how well the metrics predict the judge-
ments at the level of individual sentences as well
as at the level of the whole test set (system-level).
This year, neither the system-level nor the
segment-level scores are directly comparable to
the previous years. The system-level scores are af-
fected by the change of the underlying interpreta-
tion of the collected judgements in the main trans-
lation task evaluation as well as our choice of Pear-
son coefficient instead of Spearman?s rank corre-
lation. The segment-level scores are affected by
the different handling of ties this year. Despite
somewhat sacrificing the year-to-year comparabil-
ity, we believe all changes are towards a fairer
evaluation and thus better in the long term.
As in previous years, segment-level correlations
are much lower than system-level ones, reaching
at most Kendall?s ? of 0.45 for the best performing
metric in its best language pair. So there is quite
some research work to be done. We are happy
to see that many new metrics emerged this year,
which also underlines the importance of the Met-
rics Shared Task.
Acknowledgements
This work was supported by the grant FP7-
ICT-2011-7-288487 (MosesCore) of the European
Union. We are grateful to Jacob Devlin and also
Preslav Nakov for pointing out the issue of reward-
ing ties and for further discussion.
References
Baran?c??kov?a, P. (2014). Parmesan: Improving
Meteor by More Fine-grained Paraphrasing. In
Proceedings of the Ninth Workshop on Statisti-
cal Machine Translation, Baltimore, USA. As-
sociation for Computational Linguistics.
Bojar, O., Buck, C., Federmann, C., Haddow, B.,
Koehn, P., Leveling, J., Monz, C., Pecina, P.,
Post, M., Saint-Amand, H., Soricut, R., Specia,
L., and Tamchyna, A. (2014). Findings of the
2014 workshop on statistical machine transla-
tion. In Proceedings of the Ninth Workshop on
Statistical Machine Translation.
Callison-Burch, C., Koehn, P., Monz, C., Post, M.,
Soricut, R., and Specia, L. (2012). Findings of
the 2012 workshop on statistical machine trans-
lation. In Proceedings of the Seventh Workshop
on Statistical Machine Translation, pages 10?
51, Montr?eal, Canada. Association for Compu-
tational Linguistics.
Chen, B. and Cherry, C. (2014). A System-
atic Comparison of Smoothing Techniques for
Sentence-Level BLEU. In Proceedings of the
Ninth Workshop on Statistical Machine Trans-
lation, Baltimore, USA. Association for Com-
putational Linguistics.
Comelles, E. and Atserias, J. (2014). VERTa par-
ticipation in the WMT14 Metrics Task. In Pro-
ceedings of the Ninth Workshop on Statistical
300
Machine Translation, Baltimore, USA. Associ-
ation for Computational Linguistics.
Denkowski, M. and Lavie, A. (2014). Meteor Uni-
versal: Language Specific Translation Evalua-
tion for Any Target Language. In Proceedings
of the Ninth Workshop on Statistical Machine
Translation, Baltimore, USA. Association for
Computational Linguistics.
Doddington, G. (2002). Automatic evaluation of
machine translation quality using n-gram co-
occurrence statistics. In Proceedings of the sec-
ond international conference on Human Lan-
guage Technology Research, HLT ?02, pages
138?145, San Francisco, CA, USA. Morgan
Kaufmann Publishers Inc.
Echizen?ya, H. (2014). Application of Prize based
on Sentence Length in Chunk-based Automatic
Evaluation of Machine Translation. In Proceed-
ings of the Ninth Workshop on Statistical Ma-
chine Translation, Baltimore, USA. Association
for Computational Linguistics.
Gautam, S. and Bhattacharyya, P. (2014). LAY-
ERED: Description of Metric for Machine
Translation Evaluation in WMT14 Metrics
Task. In Proceedings of the Ninth Workshop
on Statistical Machine Translation, Baltimore,
USA. Association for Computational Linguis-
tics.
Gonz`alez, M., Barr?on-Cede?no, A., and M`arquez,
L. (2014). IPA and STOUT: Leveraging Lin-
guistic and Source-based Features for Machine
Translation Evaluation. In Proceedings of the
Ninth Workshop on Statistical Machine Trans-
lation, Baltimore, USA. Association for Com-
putational Linguistics.
Guzman, F., Joty, S., M`arquez, L., and Nakov, P.
(2014). DiscoTK: Using Discourse Structure
for Machine Translation Evaluation. In Pro-
ceedings of the Ninth Workshop on Statistical
Machine Translation, Baltimore, USA. Associ-
ation for Computational Linguistics.
Koehn, P. and Monz, C. (2006). Manual and au-
tomatic evaluation of machine translation be-
tween european languages. In Proceedings on
the Workshop on Statistical Machine Transla-
tion, pages 102?121, New York City. Associa-
tion for Computational Linguistics.
Leusch, G., Ueffing, N., and Ney, H. (2006).
CDER: Efficient MT Evaluation Using Block
Movements. In In Proceedings of EACL, pages
241?248.
Libovick?y, J. and Pecina, P. (2014). Tolerant
BLEU: a Submission to the WMT14 Metrics
Task. In Proceedings of the Ninth Workshop
on Statistical Machine Translation, Baltimore,
USA. Association for Computational Linguis-
tics.
Mach?a?cek, M. and Bojar, O. (2013). Results of the
WMT13 Metrics Shared Task. In Proceedings
of the Eighth Workshop on Statistical Machine
Translation, pages 45?51, Sofia, Bulgaria. As-
sociation for Computational Linguistics.
Mahmoudi, A., Faili, H., Dehghan, M., and
Maleki, J. (2013). ELEXR: Automatic Evalu-
ation of Machine Translation Using Lexical Re-
lationships. In Castro, F., Gelbukh, A., and
Gonz?alez, M., editors, Advances in Artificial In-
telligence and Its Applications, volume 8265 of
Lecture Notes in Computer Science, pages 394?
405. Springer Berlin Heidelberg.
Papineni, K., Roukos, S., Ward, T., and jing Zhu,
W. (2002). BLEU: a method for automatic eval-
uation of machine translation. pages 311?318.
Snover, M., Dorr, B., Schwartz, R., Micciulla, L.,
and Makhoul, J. (2006). A study of translation
edit rate with targeted human annotation. In In
Proceedings of Association for Machine Trans-
lation in the Americas, pages 223?231.
Stanojevic, M. and Sima?an, K. (2014). BEER:
A Smooth Sentence Level Evaluation Metric
with Rich Ingredients. In Proceedings of the
Ninth Workshop on Statistical Machine Trans-
lation, Baltimore, USA. Association for Com-
putational Linguistics.
Vazquez-Alvarez, Y. and Huckvale, M. (2002).
The reliability of the itu-t p.85 standard for
the evaluation of text-to-speech systems. In
Hansen, J. H. L. and Pellom, B. L., editors, IN-
TERSPEECH. ISCA.
Wu, X. and Yu, H. (2014). RED, The DCU Sub-
mission of Metrics Tasks. In Proceedings of the
Ninth Workshop on Statistical Machine Trans-
lation, Baltimore, USA. Association for Com-
putational Linguistics.
301
Proceedings of the 5th Workshop on South and Southeast Asian NLP, 25th International Conference on Computational Linguistics, pages 37?42,
Dublin, Ireland, August 23-29 2014.
English to Urdu Statistical Machine Translation: Establishing a
Baseline
Bushra Jawaid, Amir Kamran and Ond?ej Bojar
Charles University in Prague
Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics
Malostransk? n?m. 25, Praha 1, CZ-118 00, Czech Republic
jawaid,kamran,bojar@ufal.mff.cuni.cz
Abstract
The aim of this paper is to categorize and present the existence of resources for English-
to-Urdu machine translation (MT) and to establish an empirical baseline for this task.
By doing so, we hope to set up a common ground for MT research with Urdu to allow
for a congruent progress in this field. We build baseline phrase-based MT (PBMT) and
hierarchical MT systems and report the results on 3 official independent test sets. On all
test sets, hierarchial MT significantly outperformed PBMT. The highest single-reference
BLEU score is achieved by the hierarchical system and reaches 21.58% but this figure
depends on the randomly selected test set. Our manual evaluation of 175 sentences
suggests that in 45% of sentences, the hierarchical MT is ranked better than the PBMT
output compared to 21% of sentences where PBMT wins, the rest being equal.
1 Introduction
Statistical Machine Translation (SMT) has always been a challenging task for language pairs with
significant word ordering differences and rich inflectional morphology. The language pair such
as English and Urdu, despite of descending from the same family of Indo-European languages,
differs heavily in syntactic strucure and morphological characteristics. English is relatively
fixed word order language and follows subject-verb-object (SVO) structure whereas Urdu uses
restricted free word order language and most commonly follows the SOV pattern. Urdu word
order is restricted for only few parts of speeches such as adjectives always precede nouns and
postpositions follow nouns. Unlike English, Urdu is a pro-drop language. The morphology of
Urdu is similar to other Indo-European languages, e.g. by having inflectional morphological
system.
To the best of our knowledge, the research on English-to-Urdu machine translation has been
very much fragmented, preventing the authors to build upon the works of others. Our underlying
motivation for this paper is to establish a common ground and provide a concise summary of
available data resources and set up reproducible baseline results of several available test sets.
With this basis, future Urdu MT research should be able to stepwise improve the state of the
art, in contrast with the scattered experiments done so far (Khan et al., 2013; Ali et al., 2013;
Ali and Malik, 2010).
In Section 2, the experimental setup and data processing tools are described. Existing corpora
are introduced in Section 3, automatic results are reported in Section 4 and manual evaluation
is discussed in Section 5.
2 Experimental Setup
This section briefly introduces the selection of SMT models that are used to build the baseline
English-Urdu SMT system and also explains the processing of parallel data before passing it to
the MT system.
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and
proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.
0/
37
2.1 Two Models of SMT
The state-of-the-art MT toolkit Moses1 (Koehn et al., 2007), offers two mainstream models of
SMT: phrase-based (PBMT) and syntax-based (SBMT) that includes the hierarchical model.
The PBMT model operates only on mapping of source phrases (short sequences of words) to
target phrases. For dealing with word order differences, two rather weak models are available:
lexicalized and distance-based. The lexicalized reordering models (Tillmann, 2004) are consid-
ered more advanced as they condition reordering on the actual phrases, whereas the latter model
makes the reordering cost (paid when picking source phrases out of sequence) dependent only
on the length of the jump. The distance-based model is suited well for local reordering but it is
fairly weak in capturing any long distance reorderings.
The syntax-based model (SBMT) builds upon Synchronous Context-Free Grammar (SCFG)
that synchronously generates source and target sentences. The grammar rules can either consist
of linguistically motivated non-terminals such as NP, VP etc. or the generic non-terminal ?X?
in which case the model is called ?hierarchical phrase-based? (Chiang, 2005; Chiang, 2007). In
either case, the model is capable of capturing long-distance reordering much better than the
lexicalized reordering of PBMT.
2.2 Data Processing and MT Training
For the training of our en-ur translation systems, the standard training pipeline of Moses is used
along with the GIZA++ (Och and Ney, 2000) alignment toolkit and a 5-gram SRILM language
model (Stolcke, 2002). The source texts were processed using the Treex platform (Popel and
?abokrtsk?, 2010)2, which included tokenization and lemmatization.
The target side of the corpus is tokenized using a simple tokenization script3 by Dan Zeman and
it is lemmatized using the Urdu Shallow Parser4 developed by Language Technologies Research
Center of IIIT Hyderabad.
The alignments are learnt from the lemmatized version of the corpus. In all other cases,
word forms (i.e. no morphological decomposition) in their true case (i.e. names capitalized but
sentence starts lowercased) are used. The lexicalized reordering model uses the feature set called
?msd-bidirectional-fe?.
3 Dataset
Parallel and monolingual data resources are very scarce for low-resource language pairs such as
English-Urdu. This section highlights the existing parallel and monolingual data resources that
can be utilized for training SMT models. The number of official test sets are also exhibited.
3.1 Parallel Corpus
Our parallel corpus consists of around 79K sentences collected from five different sources. The
collection comes from several domains such as News, Religion, Technology, Language and Culture
etc. 95% of the data is used for training, whereas the rest is evenly split into dev and test sets.
? Emille: EMILLE (Enabling Minority Language Engineering) (Baker et al., 2002) is a col-
lection of monolingual (written and spoken), parallel and annotated corpora of fourteen
South Asian languages which is distributed by the European Language Resources Associa-
tion (ELRA). The Urdu-English part are documents produced by the British Departments
of Health, Social Services, Education and Skills, and Transport, Local Government and the
Regions of British government translated into Urdu.
In this work, the manually sentence aligned version of English-Urdu Emille corpus Jawaid
and Zeman (2011) is used.
1http://statmt.org/moses/
2http://ufal.mff.cuni.cz/treex/
3The tokenization script can be downloaded from: http://hdl.handle.net/11858/00-097C-0000-0023-65A9-5
4http://ltrc.iiit.ac.in/showfile.php?filename=downloads/shallow_parser.php
38
? IPC: The Indic Parallel Corpus (Post et al., 2012)5 is a collection of Wikipedia documents
of six Indian sub-continent languages translated into English through crowdsourcing in the
Amazon Mechanical Turk (MTurk) platform.
The English-Urdu part generally contains four (in some cases three) English translations
for each Urdu sentence. In a separate MTurk task, the Turkers voted which of the English
translations is the best one. The official training, dev and devtest sets is first merged and
afterwards the voting list is used to retrieve only the winning English sentence ignoring
sentences with no votes altogether. The official testset is left unaltered to report our final
results on this data.
? Quran: The publicly available parallel English and Urdu translation of Quranic data6 is
used, which is collected by Jawaid and Zeman (2011) in their work. The data consists of
6K aligned parallel sentences.
? Penn Treebank: Penn Treebank (Marcus et al., 1993) is an annotated corpus of around
4.5 million words originating from Wall Street Journal (WSJ), Brown corpus, Switchboard
and ATIS. The entire treebank in English is released by the Linguistic Data Consortium
(LDC). A subset of the WSJ section whose Urdu translations are provided by Center for
Language Engineering (CLE)7 is used. Out of 2,499 WSJ stories in the Treebank, only 317
are available in Urdu.
? Afrl: Afrl, the largest of the parallel resources we were able to get, is not publicly available.
The corpus originally consists of 87K sentences coming from mix of several domains mainly
news articles. The sentence alignments are manually checked of almost two thirds of the
corpus, around 4K misaligned and 30K duplicate sentences are discarded.
The statistics shown in Table 1 are reported after removing duplicated sentences from each
source. Almost all parallel corpora contained at least tens or hundreds of duplicate sentences.
Afrl on the other hand contained larger chunks of Emille and also smaller subset of Penn Tree-
bank. Around 3K sentences from Afrl that were seen in Emille are discarded but the Penn
Treebank subset of Afrl is left intact because it provides different Urdu translations.
Each parallel corpus is randomly split into train, dev and test sets according to its relative
size.
Corpus Sentences Tokens % of Data Train Dev Test
EN UR
AFRL 50,313 960,683 1,022,563 63.6% 47,769 1,272 1,272
EMILLE 8,629 152,273 199,320 10.9% 8,193 218 218
IPC 7,478 118,644 132,968 9.46% 7,098 190 190
QURAN 6,364 251,387 269,947 8.05% 6,040 162 162
PENN 6,204 158,727 179,457 7.86% 5,888 158 158
TOTAL 78,988 - - 100% 74,988 2,000 2,000
Table 1: Statistics of English-Urdu parallel corpora.
3.2 Monolingual Corpus
Jawaid et al. (2014) release8 a plain and annotated Urdu monolingual corpus of around 95.4
million tokens distributed in around 5.4 million sentences. The monolingual corpus is a mix
5http://joshua-decoder.org/data/indian-parallel-corpora/
6http://ufal.mff.cuni.cz/legacy/umc/005-en-ur/
7http://www.cle.org.pk/software/ling_resources/UrduNepaliEnglishParallelCorpus.htm
8http://hdl.handle.net/11858/00-097C-0000-0023-65A9-5
39
of domains such as News, Religion, Blogs, Literature, Science, Education etc. Only plain text
monolingual data is used to build our language model.
3.3 Official Testsets
In addition to the testset that is created from the parallel corpora resources, results are reported
on three official testsets.
NIST 2008 Open Machine Translation (OpenMT) Evaluation9 has distributed test data from
2 domains: Newswire and Web. The Web data is collected from user forums, discussion groups
and blogs, whereas Newswire data is a mix of newswire stories and data from web. The test data
contain 4 English translations for each Urdu sentence, the first English translation is picked in
all cases. Because the majority of test sets are created in order to faciliatate Urdu-to-English
MT, most of them contain multiple English references against each Urdu source.
Another testset is released with the IPC. Only those sentences are used whose ids are present
in the voting list. The domain of the IPC test set is discussed in Section 3.1.
CLE10 has published small test set from News domain specifically for MT evaluation. The
test data contains 3 Urdu references against each source. All reference translations are used for
the evaluation.
Table 2 shows the number of sentences in each test set that are used for the final evaluation.
We also report the coverage of each test set (calculated on vocabulary size) i.e. how many source
words in a test set were seen in the training data. The notions used in Table 2 to introduce
coverage are explained in Section 4.
NIST 2008 IPC CLE
NewsWire Web Test
Sentences 400 600 544 400
Coverage ALL 84% 91% 90% 87%Except-Afrl 80% 87% 88% 84%
Table 2: Statistics of official English-Urdu test sets.
4 Results
The BLEU metric (Papineni et al., 2002) has been used to evaluate the performance of the
systems. Models are trained on two different datasets: all parallel corpora (referred as ?ALL?)
and parallel data excluding Afrl corpus (referred as ?Except-Afrl?). The latter model is trained
due to the fact that Afrl corpus is publicly not available. The community working on English-
Urdu machine translation can thus have one common baseline that could be used to evaluate
their improved systems in the future. Including Afrl allows us to see the gains in performance
thanks to the additional data.
Table 3 shows the baseline results of phrase-based and hierarchical systems when trained on
both datasets. The results are reported on two test sets: the test set of 2,000 sentences (called
Large in Table 3) as shown in Table 1 and its subset of 728 sentences which excludes 1,272 test
sentences from Afrl (called Small in Table 3).
PBMT performs better when integrated with lexicalized reordering model but Hierarchical
MT outperforms both PBMT setups on both smaller and larger test sets. The absolute BLEU
scores drop by up to 6 points when Afrl is removed from the training data, however they return
back to ?20 when Afrl is also removed from the test set. This highlights the importance of data
overall and the match in domain in particular, as supported by the differences in vocabulary
coverage (see the column ?Coverage? in Table 3).
Table 4 shows the results of the best performing setups (i.e. phrase-based with lexicalized
reordering model and hierarchical model) trained on both training datasets and evaluated on the
9http://catalog.ldc.upenn.edu/LDC2010T21
10http://www.cle.org.pk/software/ling_resources/testingcorpusmt.htm
40
Parallel Corpora Test Set Phrase-based Phrase-based-LexReo Hierarchical Coverage
ALL Large 18.30?0.74 19.19?0.72 21.35?0.84 92%
Except-Afrl Large 12.85?0.74 13.78?0.73 15.11?0.82 78%
Except-Afrl Small 18.41?1.25 19.67?1.27 21.21?1.55 91%
Table 3: Results of Phrase-based, Phrase-based with Lexical Reordering and Hierarchical MT
systems.
official test sets. The BLEU score for CLE test set is reported using all 3 reference translations
at once as well as the average of single-reference BLEUs, taking each reference translation
separately. IPC and NIST2008 results are evaluated on a single reference.
The hierarchical MT performs significantly better than the phrase-based MT on all test sets.
The lowest scores were achieved on the NIST2008 test set but it is difficult to pinpoint any
specific reason (other than some domain difference) because the coverage is comparable to other
test sets (see Table 2). Across all the test sets, Afrl corpus brings about 2 points BLEU absolute.
CLE IPC NIST2008
3 refs 1 ref (avg.) 1 ref 1 ref
ALL Phrase-based-LexReo 18.19?1.19 11.12?1.02 15.82?1.36 15.13?0.95Hierarchical 19.29?1.31 11.81?1.09 18.70?1.64 16.69?1.06
Except-Afrl Phrase-based-LexReo 16.53?1.13 9.92?0.96 13.82?1.20 11.65?0.87Hierarchical 18.48?1.28 11.30?1.03 16.91?1.54 13.01?0.84
Table 4: Results of Phrase-based and Hierarchical systems on official test sets.
5 Manual Evaluation
To manually analyze the output of best performing models sample of 175 sentences is randomly
selected from the large test set translated using both PBMT with lexical reordering and hier-
archical models trained on ?ALL? data sets. QuickJudge11 is used to rank the outputs. The
annotator is shown the source, reference and output from both machine translation systems, the
identity of the MT systems is not known. There are four permitted outcomes of the ranking:
both systems marked as equally good; both systems are equally bad or the output of one of the
systems is better than the other one. Here is the summary of annotation by a single annotator:
? Out of 175 sentences, 41 sentences received equally bad translations from both systems.
? 17 items are marked as equally good.
? In 79 cases, the hierarchical MT is ranked better than the phrase-based MT.
? In the remaining 38 cases, the phrase-based MT is ranked better than the hierarchical MT.
The results from the manual ranking show that the hierarchical systems wins twice more often
than PBMT. The two systems tie in about one third of input sentences, of which about 70 %
are cases where the translations are bad.
6 Conclusion
In this work, a collection of sizeable English-Urdu corpora is summarized for statistical machine
translation. These resources are used to build baseline phrase-based and hierarchical MT systems
for translation into Urdu and the results are reported on 3 independent official test sets. This
11http://ufal.mff.cuni.cz/project/euromatrix/quickjudge/
41
can hopefully serve as a baseline for a wider community of researchers. The output of both
translation models is manually analyzed and it confirms that the hierarchical model is preferred
over phrase-based MT for English-to-Urdu translation.
Acknowledgments
This work has been using language resources developed and/or stored and/or distributed
by the LINDAT-Clarin project of the Ministry of Education of the Czech Republic (project
LM2010013). This work was also supported by the grant FP7-ICT-2011-7-288487 (MosesCore)
of the European Union.
References
Aasim Ali and Muhmmad Kamran Malik. 2010. Development of parallel corpus and english to urdu
statistical machine translation. Int. J. of Engineering & Technology IJET-IJENS, 10:31?33.
Aasim Ali, Arshad Hussain, and Muhammad Kamran Malik. 2013. Model for english-urdu statistical
machine translation. World Applied Sciences, 24:1362?1367.
Paul Baker, Andrew Hardie, Tony McEnery, Hamish Cunningham, and Robert J. Gaizauskas. 2002.
Emille, a 67-million word corpus of indic languages: Data collection, mark-up and harmonisation. In
LREC. European Language Resources Association.
David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proc. of
ACL, pages 263?270.
David Chiang. 2007. Hierarchical phrase-based translation. Comput. Linguist., 33(2):201?228, June.
Bushra Jawaid and Daniel Zeman. 2011. Word-order issues in english-to-urdu statistical machine trans-
lation. Number 95, pages 87?106, Praha, Czechia.
Bushra Jawaid, Amir Kamran, and Ond?ej Bojar. 2014. A Tagged Corpus and a Tagger for Urdu (to
appear). Reykjav?k, Iceland. European Language Resources Association. In print.
Nadeem Khan, Waqas Anwar, Usama Ijaz Bajwa, and Nadir Durrani. 2013. English to urdu hierarchical
phrase-based statistical machine translation. In The 4th Workshop on South and Southeast Asian NLP
(WSSANLP), IJCNLP, pages 72?76, Nagoya, Japan.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ond?ej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open Source Toolkit for Statistical Machine Translation. In
Proc. of ACL Companion Volume, Demo and Poster Sessions, pages 177?180, Prague, Czech Republic.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated
corpus of english: The penn treebank. Comput. Linguist., 19(2):313?330, June.
Franz Josef Och and Hermann Ney. 2000. A Comparison of Alignment Models for Statistical Machine
Translation. In Proc. of COLING, pages 1086?1090. ACL.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: a Method for Automatic
Evaluation of Machine Translation. In Proc. of ACL, pages 311?318.
Martin Popel and Zden?k ?abokrtsk?. 2010. TectoMT: Modular NLP Framework. In Lecture Notes in
Artificial Intelligence, Proceedings of the 7th International Conference on Advances in Natural Language
Processing (IceTAL 2010), volume 6233 of Lecture Notes in Computer Science, pages 293?304. Springer.
Matt Post, Chris Callison-Burch, and Miles Osborne. 2012. Constructing parallel corpora for six indian
languages via crowdsourcing. In Proc. of WMT, ACL, pages 401?409, Montr?al, Canada.
Andreas Stolcke. 2002. Srilm - an extensible language modeling toolkit. In In Proceedings of the 7th
International Conference on Spoken Language Processing (ICSLP) 2002, pages 901?904.
Christoph Tillmann. 2004. A unigram orientation model for statistical machine translation. In Proc. of
HLT-NAACL Short Papers, pages 101?104.
42
Proceedings of the Workshop on Lexical and Grammatical Resources for Language Processing, pages 55?64,
Coling 2014, Dublin, Ireland, August 24 2014.
Comparing Czech and English AMRs
Zde
?
nka Ure
?
sov
?
a Jan Haji
?
c Ond
?
rej Bojar
Charles University in Prague
Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics
Malostransk?e n?am?est?? 25, 11800 Prague 1, Czech Republic
{uresova,hajic,bojar}@ufal.mff.cuni.cz
Abstract
This paper describes in detail the differences between Czech and English annotation us-
ing the Abstract Meaning Representation scheme, which stresses the use of ontologies (and
semantically-oriented verbal lexicons) and relations based on meaning or ontological content
rather than semantics or syntax. The basic ?slogan? of the AMR specification clearly states that
AMR is not an interlingua, yet it is expected that many relations as well as structures constructed
from these relations will be similar or even identical across languages. In our study, we have
investigated 100 sentences in English and their translations into Czech, annotated manually by
AMRs, with the goal to describe the differences and if possible, to classify them into two main
categories: those which are merely convention differences and thus can be unified by changing
such conventions in the AMR annotation guidelines, and those which are so deeply rooted in the
language structure that the level of abstraction which is inherent in the current AMR scheme does
not allow for such unification.
1 Introduction
In this paper, we follow on a previous first exploratory investigation of differences in AMR annotation
among different languages (Xue et al., 2014), which has classified the similarities and differences into
four categories: (a) no difference, (b) local difference only (such as multiword expressions vs. single
word terms), (c) reconcilable difference due to AMR conventions, and (d) deep differences which cannot
be unified in the AMR guidelines. In this paper, we would like to elaborate especially on the (b) and (c)
types, which have been only exemplified in the previous work. In this paper, we would like to not only
go deeper, but also present quantitative comparison on 100 parallel sentences, for all the aforementioned
categories and some of their subtypes.
We will first describe the basic principles of AMR annotation (Banarescu et al., 2013) (Sect. 2, building
also on (Xue et al., 2014)), then present the data (parallel texts) which we have used for this study
(Sect. 3), and describe the quantitative and qualitative comparison between AMR annotation of English
and Czech (Sect. 4). In Sect. 5, we will summarize and discuss further work.
2 Abstract Meaning Representation (AMR)
Syntactic treebanks in several languages (Marcus et al., 1993; Haji?c et al., 2003; Xue et al., 2005)
and related annotated corpora such as PropBank (Palmer et al., 2005), Nombank (Meyers et al., 2004),
TimeBank (Pustejovsky et al., 2003), FactBank (Saur?? and Pustejovsky, 2009), and the Penn Discourse
TreeBank (Prasad et al., 2008), coupled with machine learning techniques, have been used in many NLP
tasks. These annotated resources enabled substantial amounts of research in different areas of semantic
analysis. There had already been tremendous progress in syntactic parsing (Collins, 1999; Charniak,
2000; Petrov and Klein, 2007) and now in Semantic Role Labeling because of the existence of the
PropBank (Gildea and Jurafsky, 2002; Pradhan et al., 2004; Xue and Palmer, 2004; Bohnet et al., 2013)
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings
footer are added by the organizers. License details: http:// creativecommons.org/licenses/by/4.0
55
and similar resources in other languages (Haji?c et al., 2009), and TimeBank has fueled much research in
the area of temporal analysis.
There have been efforts to create a unified representation which would cover at least a whole sentence,
or even a continuous text (Haji?c et al., 2003; Srikumar and Roth, 2013), and currently the Abstract
Meaning Representation represents an attempt to provide a common ground for truly semantic and fully
covering annotation representation.
An Abstract Meaning Representation is a rooted, directional and labeled graph that represents the
meaning of a sentence and it abstracts away from such syntactic notions as word category (verbs and
nouns), word order, morphological variation etc. Instead, it focuses on semantic relations between con-
cepts and makes heavy use of predicate-argument structures as defined in PropBank (for English). As
a result, the word order in the sentence is considered to be of little relevance to the meaning represen-
tation and is not necessarily maintained in the AMR. In addition, many function words (determiners,
prepositions) that do not contribute to meaning and are not explicitly represented in AMR, except for the
semantic relations they express. Readers are referred to Baranescu et al. (2013) for a complete descrip-
tion of AMR.
1
Figure 1: AMR annotation of the sentence ?This infatuation with city living truly baffles me.?
An example of an AMR-annotated sentence can be seen in Fig. 1. The predicate of the sentence
(baffle) becomes the root of the annotation graph, with a reference to the correct sense baffle-01 as
found in PropBank frame files for baffle; PropBank frame files play the role of an ontology of events.
Arguments of predicates, again as described in the PropBank frames, become the substitutes for roles of
the ?who did what to whom? interpretation - in the example sentence, infatuation - marked as ARG0 - is
the thing that baffles someone (the ARG1), i.e. me (the author of the text) in this case. This ?baffling? is
further modified by ?truly?, and marked simply as a modifier, the semantics of which is fully represented
by the word true itself. The agent (infatuation) has to be further restricted - it is the ?infatuation with
city living? which baffles the author - not just any infatuation. This is represented by the relation topic
assigned to the edge between infatuation and live-01 in the AMR graph, and the ?living? (sense
live-01) is further restricted by the location mentioned in the sentence, namely city. Finally,
the modifier this is kept in, since it is needed for reference to previous text, where the ?infatuation?
has been first mentioned.
While the graphical representation in Fig. 1 is simplified in that it does not show the AMR?s crucial
instance-of relations explicitly as edges in the AMR graph, Fig. 2 shows the native underlying
?bracketed? textual representation of the same tree, where the main nodes (i.e. those shown visibly in
Fig. 1) are mentions, and the labels baffle-01, true, live-01, city etc. represent links to
external ontologies. These links are currently represented only by these strings, or by links to PropBank
1
This paragraph as well as the two preceding ones are taken over (and slightly adapted) from the introductory sections of a
previous paper on this topic presented at LREC and co-authored by us (Xue et al., 2014); we share the same AMR formalism
and data.
56
files for events. In the future, these links will be wikified, i.e. for concepts described in an external
ontology, such as Wikipedia, they will be linked to it. The single- or two-letter ?indexes? are in fact the
labels (IDs) of the mentions, and they also serve for (co-)reference purposes; the slash (?/?) is a shortcut
for the instance-of relation.
(b / baffle-01
:ARG0 (i / infatuation
:topic (l / live-01
:location (c / city))
:mod (t / this))
:ARG1 (i2 / i)
:mod (t2 / true))
Figure 2: Textual form of the AMR annotation of the sentence ?This infatuation with city living truly
baffles me.?
In Czech, the event ontology has been approximated by the Czech valency dictionary, PDT-Vallex
(Haji?c et al., 2003), (Ure?sov?a, 2009), (Ure?sov?a and Pajas, 2009), (Ure?sov?a, 2006). No wikification of
non-event nodes has been attempted yet; this is a continuing work, as it is for English.
3 The Data
We have drawn on a blog on Virginia road construction, taken from the WB part of the Penn Treebank.
These sentences have already been annotated using AMRs, and also translated to Czech
2
and subse-
quently AMR-annotated. The English text has 1676 word and punctuation tokens (using the Penn Tree-
bank style tokenization), and its annotated AMR representation contains 1231 nodes (not counting the
instance-of nodes as separate nodes). The Czech version is a result of manually doubly-translated
English original, which has been mutually checked and then one (slightly corrected) translation has been
used for annotation. The Czech text has a total of 1563 tokens and its AMR representation contains 1215
nodes (again, not counting the instance-of nodes as separate nodes).
The data, once annotated, have been converted to a graph and in such a form presented to a linguist
familiar with the AMR style annotation, to study and extract statistics for this comparison study. Fig. 3
shows such a side-by-side graphs for English and Czech AMR for the example parallel sentences.
4 The Comparison
4.1 Quantitative Comparison
In the first pass, we have concentrated on marking and counting the following phenomena:
? structural identity: sentences with identical structure have been marked as being structurally the
same, even if some relation (edge) labels have been different
? structural differences: no. of structural differences have been noted in cases where one or more
(sub)parts of the AMR graph differ between the two languages
? local difference only: out of the above, certain differences have been marked as ?local only? - for
example, if a multiword expression annotated as several nodes in one language corresponds to a
single node in the other language
? relation differences: for each sentence, number of differences in relational labels has been counted
? reference differences: number of different references to an external ontology (or assumed differ-
ences in case no link to such an ontology was actually present in the annotation).
2
...and Chinese, but that was not used for this study.
57
Figure 3: AMR annotation of the sentence ?This infatuation with city living truly baffles me.? and its
translation to Czech (?Tohle/this poblouzn?en??/infatuation bydlen??m/living-INSTR v/in m?estsk?em/city-like
stylu/style m?e/me po?r?ad/still mate/baffles.?)
It is obvious that we could have observed also other types of differences, but at this point, we wanted
to have at least an idea how many differences exist in our approx. 1500-token sample. The resulting
figures are summarized in Table 1.
3
Same
structure
Different
substructures
Local difference
only
Relation
differences
Reference
differences
29 (sents) 193 (subgraphs) 92 (subgraphs) 331 (nodes) 37 (nodes)
of 100 of approx. 800
2
of 193 (all diffs) of 1215 Cz nodes of 1215 Cz nodes
29 % approx. 25 %
2
47.7 % 27.2 % 3.0 %
Table 1: Number and percentages of differences in the annotated data
The number of truly identically annotated sentences (including relation labeling) was only four, two
of which has been interjective ?sentences? at the beginning of the document (?Braaawk!?). On the other
hand, 18 additional sentences would be structurally identical (on top of the 29) if local differences were
disregarded, bringing the (unlabeled sentence identity) total to 47, or almost half of the data (47=29+18).
4.2 Analysis of Differences
The main goal of this study is to analyze differences in the annotation for the two languages, Czech
and English, and determine if a reconciliation of the annotation is possible or not (and for what reason
it is / it is not). Based on the above quantitative analysis, we have concentrated on relation labeling
differences due their high proportion, and on structural differences due to their heterogeneous nature.
The differences in reference annotation are small, but this is due to the lack of full referential annotation
(it has been done for events, but only assumed for other types of entities due to the lack of ontology,
or better to say due to the lack of ?wikification? annotation in both languages), rather than due to high
agreement. We will come back to this once wikification of the annotation is finished.
3
Structural differences are hard to quantify exactly, since the base is difficult to define; it is part of future work.
58
4.3 Differences in Relation Labeling
The differences in function labeling should be taken with a grain of salt. The crucial question is what
should count as a difference in relation labeling if the structure differs - should this be automatically
counted as a difference, or not at all? In the figures summarized in Table 1, we have taken a middle
ground: if the structural difference implied a change in labeling by itself, we have not counted that
difference in order not to ?penalize? the sentence annotation twice.
More detailed inspection of relation labeling differences, which appear to be relatively frequent at
more than 1/4th of all nodes in the annotation, revealed that the by far most frequent mismatch is caused
by different argument labeling for events.
4
While for most purely transitive verbs there is a complete
match, for most other there is a discrepancy due to the attempted semanticization of PDT-Vallex argu-
ment labels ADDR (addressee), EFF (effect) and ORIG (origin), while PropBank simply continues to
number arguments of corresponding verbs consecutively (for example, I thought there is.ARG1 ... vs.
Myslel/I-thought jsem, ?ze/that tam/there je.ARG3?EFF/is ...). The concept of ?shifting? in PDT-Vallex,
which compulsorily fills the first two arguments on syntactic grounds as ACT(ARG0) and PAT(ARG1)
is another source of differences. Furthermore, PropBank leaves out ARG0 e.g. for unaccusative verbs
(for example The window.ARG1 broke vs. Okno.ARG0?ACT/window se/itself rozbilo/broke.). Finally,
some differences are due to some arguments not being considered arguments at all in the other language,
in which case some other AMR label is used instead (for example, We could have spent 400M.ARG3 ...
elsewhere vs. ... mohli/could utratit/spend 400M.extent ... jinde/elsewhere).
These differences could possibly be consolidated (only) by carefully linking the two lexicons (with
AMR guidelines intact). This is in fact being performed in another project (Sindlerova et al., 2014), but
it is a daunting manual task, since the underlying theories behind PropBank and PDT-Vallex/EngVallex
differ. However, one has to ask if it does make sense to do so, because with enough parallel data available,
the mappings can be learned relatively easily: in most cases, no structural differences are involved and
there will be a simple one-to-one mapping between the labels (conditioned on the particular verb sense).
4.4 Structural Differences
Local differences can be safely ignored, since they will be in most cases resolved during the assumed
process of wikification, i.e., linking to an ontology concept. For example, the abbreviation VDOT (Vir-
ginia Department of Transportation), which has to be (and was) translated into Czech in an explanatory
way (otherwise the sentence would become not quite understandable, if only because of the real-world
context). Without wikification, it could not be linked as a whole, and thus a subgraph has been created
with the AMR-appropriate internal semantic relations in the translation (e.g. Virginia.location, etc.).
Certain differences, albeit ?localized? into a small subtree (or subgraph) corresponding to a single
node or another small subtree (subgraph), cannot be resolved by wikification of a different event ontology
(than PropBank or PDT-Vallex). For example, light verb constructions or even certain modal or aspectual
constructions could have a single verb equivalent resulting in two node vs. single node annotation: get
close vs. p?ribl???zit-se, make worse vs. zhor?sit, take position (for sb) vs. zast?avat-se or causing sprawl vs.
roztahuje-se.
Looking at the true structural differences, we have found that there are actually quite a few reasons for
them to appear in the annotation. We will describe them in more detail below.
Non-literal translation is the primary reason for such differences.
5
For example, destination vs.
kam/where-to lid?e/people jezd??/drive (Fig. 4), or job center vs. m??sto/place, kde/where pracuje/work
hodn?e/many lid??/people; these cases cannot be unified neither by changing the translation to a more literal
one (because it would be strongly misleading in the given context, despite the fact that literal translation
of both destination as well as job center does exist in Czech), neither by changing the guidelines, since
the level of abstraction of AMR does not call for a unification of such concepts. Sometimes, non-literal
4
The Czech PDT-Vallex argument labels have been mapped to PropBank labels as follows: ACT? ARG0, PAT? ARG1,
ADDR? ARG2, EFF? ARG3 and ORIG? ARG4.
5
This includes also cases of truly wrong translation, stemming of translator?s misunderstanding of the facts behind the
sentence. This has been found fairly often only after we studied the differences in depth, since a superficial reading and
standard translation revision procedure did not help.
59
translation is forced upon the translation because no word-for-word translation exists, such as in in the
aggregate, which has to be translated using an extra clause z/from celkov?eho/overall pohledu/view to/it
je/is tak/so, ?ze/that ... (Fig. 5).
Figure 4: AMR structural difference: destination vs. kam/where-to lid?e/people jezd??/drive
Phraseological differences and idioms form another large group of differences between the two lan-
guages. The possibility of changing the translation is even more remote than in the above case, even if we
had the chance: the provided translation is actually the correct and perfect one. The reason for different
annotation lies in the AMR scheme, which does not go that far to require ?unified? annotation in such
cases where the idiom or specific phrase cannot be linked to the external ontology as a single unit. For
example, English ?I don?t see any point? is translated as ?nem?a/not-have smysl/purpose?, and despite the
fact that have-purpose-91 is a specific event reference in English (and has been used in the anno-
tation), the verb ?see? still remains annotated as a separate event node, which is not the case in Czech,
since no ?seeing? is expressed in the sentence and it could hardly be asked for in the guidelines to be
inserted. Similarly, I commute back and forth has been translated simply as doj???zd??m/commute, which is
semantically perfectly equivalent but the back and forth has been kept in the English annotation, because
60
Figure 5: AMR structural difference: in the aggregate vs. z/from celkov?eho/overall pohledu/view to/it
je/is tak/so, ?ze/that ...
deleting it was (probably) considered loss of information. It is only the confrontation with the translation
to a different language when one realizes that with just a little more abstraction, the annotation could
have been structurally the same (by keeping only the commute node in in the English annotation).
6
Translation by interpretation is typically discouraged in translation school education, but sometimes
it is necessary to use it for smooth understanding of the translated text. Often, such interpretation results
in different AMR annotation. For example, Virginia centrist has been translated as st?redov?y/centrist
voli?c/voter [z/from Virginie/Virginia], because without the extra word voli?c, the literal translation of
centrist would not be understandable correctly in this context (Fig. 6). Similarly, a 55mph zone vs.
z?ona/zone s/with omezen??m/restriction na/to 55 mph (added word omezen??m/restriction), or traffic vs.
dopravn??/traffic z?acpa/jam.
Convention differences are inherent in many annotation schemes, and we have found them in AMR
6
One could perhaps also argue that adding the equivalent of back and forth to the Czech translation would unify the trans-
lation, too; however, adding its literal equivalent in Czech tam a zp?et would be considered superfluous and unnatural by Czech
speakers.
61
Figure 6: AMR structural difference: Virginia Centrist vs.
st?redov?y/centrist voli?c/voter [z/from Virginie/Virginia]
Figure 7: AMR convention difference:
auditor as a single node vs. person, who
audits
guidelines, too. Often, they were related to the use of ARG-of vs. keeping the nominalization as a
single node. For example, for auditor, translated quite literally as auditor into Czech, has been annotated
as ?a person, who audits? in English while in the Czech AMR structure, there is a single node (Fig. 7)
labeled as auditor (which undoubtedly will be correctly linked to some ontology entry after such link-
age/wikification is complete). These differences might be harder to consolidate, since such conventions
are very difficult to create proper guidelines for, especially across languages. No ontology (whether for
events or objects) will be complete either (to base the decisions on a particular ontology content).
5 Conclusions and Future Work
We have investigated differences in the annotation of parallel texts using the Abstract Meaning Rep-
resentation scheme, on approx. 1500 words of English-Czech corpus (100 sentences). We found and
counted the number of identities and four types of differences (structural, structural local, relational, and
referential), and exemplified them to see if a reconciliation (either by possibly changing the translation,
the guidelines, or the annotation itself) is possible.
This is a work in progress. Substantial amount of work remains. We will have to use larger data,
multiple annotation (interannotator agreement on English was relatively low and we expect to be the
case on Czech, too, once two annotators start annotating the same sentences), and we would also have
to actually suggest changes in the guidelines or their conventions, and to test them also on substantial
amounts of data.
The immediate extension of this work will cover wikification, i.e. the linking of all nodes in the AMR
representation of our dataset to some ontology: events are already covered, internally defined relations
are already annotated, too (such as named entity types, dates, quantities, etc.), but external links remain to
be added. We will not only use Wikipedia (as the term ?wikification? might suggest), but we will extend
this idea also to other sources, such as DBpedia or BabelNet, keeping all links in parallel if possible.
This should allow for deep comparison of the two languages also content-wise. We should then be able
to better answer the question of annotation unification which does depend on these links rather than on
the annotation guidelines themselves.
Parallel AMR-annotated data will be used at the JHU 2014 Summer Workshop, where technology for
AMR-based parsing, generation and possibly also MT will be developed, allowing also technological
insight into the AMR scheme across languages.
Acknowledgements
This work was supported by the grant GP13-03351P of the Grant Agency of the Czech Republic, projects
LH12093 and LM2010013 of the Ministry of Education, Youth and Sports of the Czech Republic, and
the EU FP7 project 610516 ?QTLeap?. It has been using language resources distributed by the LIN-
DAT/CLARIN
7
project of the Ministry of Education, Youth and Sports of the Czech Republic (project
LM2010013).
7
http://lindat.cz, resource used: http://hdl.handle.net/11858/00-097C-0000-0023-4338-F,
also at http://lindat.mff.cuni.cz/services/PDT-Vallex
62
References
L. Banarescu, C. Bonial, S. Cai, M. Georgescu, K. Griffitt, U. Hermjakob, K. Knight, P. Koehn, M. Palmer, and
N. Schneider. 2013. Abstract Meaning Representation for Sembanking. In Proceedings of the 7th Linguistic
Annotation Workshop, Sophia, Bulgaria.
Bernd Bohnet, Joakim Nivre, Igor Boguslavsky, Richard Farkas, Filip Ginter, and Jan Haji?c. 2013. Joint morpho-
logical and syntactic analysis for richly inflected languages. Transactions of the Association for Computational
Linguistics, 1:415?428.
Eugene Charniak. 2000. A maximum-entropy-inspired parser. In Proceedings of the 1st North American chapter
of the Association for Computational Linguistics conference, pages 132?139. Association for Computational
Linguistics.
Michael Collins. 1999. Head-driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of
Pennsylvania.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic labeling of semantic roles. Computational linguistics,
28(3):245?288.
Jan Haji?c, Jarmila Panevov?a, Zde?nka Ure?sov?a, Alevtina B?emov?a, Veronika Kol?a?rov?a, and Petr Pajas. 2003. PDT-
VALLEX: Creating a Large-coverage Valency Lexicon for Treebank Annotation. In Proceedings of The Second
Workshop on Treebanks and Linguistic Theories, volume 9 of Mathematical Modeling in Physics, Engineering
and Cognitive Sciences, pages 57?68. V?axj?o University Press, November 14?15, 2003.
Jan Haji?c, Massimiliano Ciaramita, Richard Johansson, Daisuke Kawahara, Maria Mart??, Llu??s M`arquez, Adam
Meyers, Joakim Nivre, Sebastian Pad?o, Jan
?
St?ep?anek, Pavel Stra?n?ak, Mihai Surdeanu, Nianwen Xue, and
Yi Zhang. 2009. The CoNLL-2009 Shared Task: Syntactic and Semantic Dependencies in Multiple Lan-
guages. In Jan Haji?c, editor, Proceedings of the Thirteenth Conference on Computational Natural Language
Learning (CoNLL): Shared Task, pages 1?18, Boulder, CO, USA. Association for Computational Linguistics.
Jan Haji?c, Alena Bo?hmov?a, Eva Hajicov?a, and Barbora Hladk?a. 2003. The Prague Dependency Treebank: A
Three Level Annotation Scenario. In Anne Abeill?e, editor, Treebanks: Building and Using Annotated Corpora.
Kluwer Academic Publishers.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of
english: The penn treebank. Computational Linguistics, 19(2):313?330.
A. Meyers, R. Reeves, C. Macleod, R. Szekely, V. Zielinska, B. Young, and R. Grishman. 2004. The NomBank
Project: An Interim Report. In Proceedings of the NAACL/HLT Workshop on Frontiers in Corpus Annotation,
pages 24?31, Boston, Massachusetts.
Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005. The Proposition Bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1):71?106.
Slav Petrov and Dan Klein. 2007. Improved inference for unlexicalized parsing. In HLT-NAACL, pages 404?411.
Sameer S Pradhan, Wayne Ward, Kadri Hacioglu, James H Martin, and Daniel Jurafsky. 2004. Shallow semantic
parsing using support vector machines. In HLT-NAACL, pages 233?240.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Miltsakaki, Livio Robaldo, Aravind Joshi, and Bonnie Webber.
2008. The Penn Discourse Treebank 2.0. In Proceedings of the 6th International Conference on Language
Resources and Evaluation (LREC 2008), Marrakech, Morocco.
James Pustejovsky, Patrick Hanks, Roser Saur??, Andrew See, David Day, Lisa Ferro, Robert Gaizauskas, Marcia
Lazo, Andrea Setzer, and Beth Sundheim. 2003. The TimeBank Corpus. Corpus Linguistics, pages 647?656.
Roser Saur?? and James Pustejovsky. 2009. Factbank: a corpus annotated with event factuality. Language resources
and evaluation, 43(3):227?268.
Jana Sindlerova, Zdenka Uresova, and Eva Fucikova. 2014. Resources in Conflict: A Bilingual Valency Lexicon
vs. a Bilingual Treebank vs. a Linguistic Theory. In Nicoletta Calzolari (Conference Chair), Khalid Choukri,
Thierry Declerck, Hrafn Loftsson, Bente Maegaard, Joseph Mariani, Asuncion Moreno, Jan Odijk, and Ste-
lios Piperidis, editors, Proceedings of the Ninth International Conference on Language Resources and Evalua-
tion (LREC?14), pages 2490?2494, Reykjavik, Iceland, May 26-31. European Language Resources Association
(ELRA).
63
Vivek Srikumar and Dan Roth. 2013. Modeling semantic relations expressed by prepositions. Transactions of the
Association for Computational Linguistics, 1:231?242.
Zde?nka Ure?sov?a and Petr Pajas. 2009. Diatheses in the Czech Valency Lexicon PDT-Vallex. In Jana Levick?a
and Radovan Garab??k, editors, Slovko 2009, NLP, Corpus Linguistics, Corpus Based Grammar Research, pages
358?376, Bratislava, Slovakia. Jazykovedn?y ?ustav udov??ta
?
St?ura Slovenskej akad?emie vied, Slovensk?a akad?emia
vied.
Zde?nka Ure?sov?a, 2006. Verbal Valency in the Prague Dependency Treebank from the Annotator?s Viewpoint, pages
93?112. Veda, Bratislava, Bratislava, Slovensko.
Zde?nka Ure?sov?a. 2009. Building the PDT-VALLEX valency lexicon. In Catherine Smith
Michaela Mahlberg, Victorina Gonzalez-Diaz, editor, Proceedings of the Corpus Linguistics Conference),
http://ucrel.lancs.ac.uk/publications/cl2009/100 FullPaper.doc, July 20-23. University of Liverpool, UK.
Nianwen Xue and Martha Palmer. 2004. Calibrating features for semantic role labeling. In EMNLP, pages 88?94.
Nianwen Xue, Fei Xia, Fu dong Chiou, and Martha Palmer. 2005. The Penn Chinese TreeBank: Phrase Structure
Annotation of a Large Corpus. Natural Language Engineering, 11(2):207?238.
Nianwen Xue, Ondrej Bojar, Jan Hajic, Martha Palmer, Zdenka Uresova, and Xiuhong Zhang. 2014. Not an In-
terlingua, But Close: Comparison of English AMRs to Chinese and Czech. In Nicoletta Calzolari (Conference
Chair), Khalid Choukri, Thierry Declerck, Hrafn Loftsson, Bente Maegaard, Joseph Mariani, Asuncion Moreno,
Jan Odijk, and Stelios Piperidis, editors, Proceedings of the Ninth International Conference on Language Re-
sources and Evaluation (LREC?14), pages 1765?1772, Reykjavik, Iceland, May 26-31. European Language
Resources Association (ELRA).
64
