Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 697?704
Manchester, August 2008
Exploiting Constituent Dependencies for Tree Kernel-based Semantic 
Relation Extraction 
Longhua Qian   Guodong Zhou   Fang Kong   Qiaoming Zhu   Peide Qian 
Jiangsu Provincial Key Lab for Computer Information Processing Technology 
School of Computer Science and Technology, Soochow University 
1 Shizi Street, Suzhou, China 215006 
{qianlonghua,gdzhou,kongfang,qmzhu,pdqian}@suda.edu.cn
Abstract
This paper proposes a new approach to 
dynamically determine the tree span for 
tree kernel-based semantic relation ex-
traction. It exploits constituent dependen-
cies to keep the nodes and their head 
children along the path connecting the 
two entities, while removing the noisy in-
formation from the syntactic parse tree, 
eventually leading to a dynamic syntactic 
parse tree. This paper also explores entity 
features and their combined features in a 
unified parse and semantic tree, which in-
tegrates both structured syntactic parse 
information and entity-related semantic 
information. Evaluation on the ACE 
RDC 2004 corpus shows that our dy-
namic syntactic parse tree outperforms all 
previous tree spans, and the composite 
kernel combining this tree kernel with a 
linear state-of-the-art feature-based ker-
nel, achieves the so far best performance. 
1 Introduction 
Information extraction is one of the key tasks in 
natural language processing. It attempts to iden-
tify relevant information from a large amount of 
natural language text documents. Of three sub-
tasks defined by the ACE program1, this paper 
focuses exclusively on Relation Detection and 
Characterization (RDC) task, which detects and 
classifies semantic relationships between prede-
fined types of entities in the ACE corpus. For 
? 2008. Licensed under the Creative Commons Attribution-
Noncommercial-Share Alike 3.0 Unported license 
(http://creativecommons.org/licenses/by-nc-sa/3.0/). Some 
rights reserved. 
1 http://www.ldc.upenn.edu/Projects/ACE/ 
example, the sentence ?Microsoft Corp. is based 
in Redmond, WA? conveys the relation ?GPE-
AFF.Based? between ?Microsoft Corp.? [ORG] 
and ?Redmond? [GPE]. Due to limited accuracy 
in state-of-the-art syntactic and semantic parsing, 
reliably extracting semantic relationships be-
tween named entities in natural language docu-
ments is still a difficult, unresolved problem. 
In the literature, feature-based methods have 
dominated the research in semantic relation ex-
traction. Featured-based methods achieve prom-
ising performance and competitive efficiency by 
transforming a relation example into a set of syn-
tactic and semantic features, such as lexical 
knowledge, entity-related information, syntactic 
parse trees and deep semantic information. How-
ever, detailed research (Zhou et al, 2005) shows 
that it?s difficult to extract new effective features 
to further improve the extraction accuracy. 
Therefore, researchers turn to kernel-based 
methods, which avoids the burden of feature en-
gineering through computing the similarity of 
two discrete objects (e.g. parse trees) directly. 
From prior work (Zelenko et al, 2003; Culotta 
and Sorensen, 2004; Bunescu and Mooney, 2005) 
to current research (Zhang et al, 2006; Zhou et 
al., 2007), kernel methods have been showing 
more and more potential in relation extraction. 
The key problem for kernel methods on rela-
tion extraction is how to represent and capture 
the structured syntactic information inherent in 
relation instances. While kernel methods using 
the dependency tree (Culotta and Sorensen, 2004) 
and the shortest dependency path (Bunescu and 
Mooney, 2005) suffer from low recall perform-
ance, convolution tree kernels (Zhang et al, 2006; 
Zhou et al, 2007) over syntactic parse trees 
achieve comparable or even better performance 
than feature-based methods. 
However, there still exist two problems re-
garding currently widely used tree spans. Zhang 
et al (2006) discover that the Shortest Path-
697
enclosed Tree (SPT) achieves the best perform-
ance. Zhou et al (2007) further extend it to Con-
text-Sensitive Shortest Path-enclosed Tree (CS-
SPT), which dynamically includes necessary 
predicate-linked path information. One problem 
with both SPT and CS-SPT is that they may still 
contain unnecessary information. The other prob-
lem is that a considerable number of useful con-
text-sensitive information is also missing from 
SPT/CS-SPT, although CS-SPT includes some 
contextual information relating to predicate-
linked path. 
This paper proposes a new approach to dy-
namically determine the tree span for relation 
extraction by exploiting constituent dependencies 
to remove the noisy information, as well as keep 
the necessary information in the parse tree. Our 
motivation is to integrate dependency informa-
tion, which has been proven very useful to rela-
tion extraction, with the structured syntactic in-
formation to construct a concise and effective 
tree span specifically targeted for relation extrac-
tion. Moreover, we also explore interesting com-
bined entity features for relation extraction via a 
unified parse and semantic tree. 
The other sections in this paper are organized 
as follows. Previous work is first reviewed in 
Section 2. Then, Section 3 proposes a dynamic 
syntactic parse tree while the entity-related se-
mantic tree is described in Section 4. Evaluation 
on the ACE RDC corpus is given in Section 5. 
Finally, we conclude our work in Section 6. 
2 Related Work 
Due to space limitation, here we only review 
kernel-based methods used in relation extraction. 
For those interested in feature-based methods, 
please refer to Zhou et al (2005) for more details. 
Zelenko et al (2003) described a kernel be-
tween shallow parse trees to extract semantic 
relations, where a relation instance is trans-
formed into the least common sub-tree connect-
ing the two entity nodes. The kernel matches the 
nodes of two corresponding sub-trees from roots 
to leaf nodes recursively layer by layer in a top-
down manner. Their method shows successful 
results on two simple extraction tasks. Culotta 
and Sorensen (2004) proposed a slightly general-
ized version of this kernel between dependency 
trees, in which a successful match of two relation 
instances requires the nodes to be at the same 
layer and in the identical path starting from the 
roots to the current nodes. These strong con-
straints make their kernel yield high precision but 
very low recall on the ACE RDC 2003 corpus. 
Bunescu and Mooney (2005) develop a shortest 
path dependency tree kernel, which simply 
counts the number of common word classes at 
each node in the shortest paths between two enti-
ties in dependency trees. Similar to Culotta and 
Sorensen (2004), this method also suffers from 
high precision but low recall.
Zhang et al (2006) describe a convolution tree 
kernel (CTK, Collins and Duffy, 2001) to inves-
tigate various structured information for relation 
extraction and find that the Shortest Path-
enclosed Tree (SPT) achieves the F-measure of 
67.7 on the 7 relation types of the ACE RDC 
2004 corpus. One problem with SPT is that it 
loses the contextual information outside SPT, 
which is usually critical for relation extraction. 
Zhou et al (2007) point out that both SPT and 
the convolution tree kernel are context-free. They 
expand SPT to CS-SPT by dynamically includ-
ing necessary predicate-linked path information 
and extending the standard CTK to context-
sensitive CTK, obtaining the F-measure of 73.2 
on the 7 relation types of the ACE RDC 2004 
corpus. However, the CS-SPT only recovers part 
of contextual information and may contain noisy 
information as much as SPT. 
In order to fully utilize the advantages of fea-
ture-based methods and kernel-based methods, 
researchers turn to composite kernel methods. 
Zhao and Grishman (2005) define several fea-
ture-based composite kernels to capture diverse 
linguistic knowledge and achieve the F-measure 
of 70.4 on the 7 relation types in the ACE RDC 
2004 corpus. Zhang et al (2006) design a com-
posite kernel consisting of an entity linear kernel 
and a standard CTK, obtaining the F-measure of 
72.1 on the 7 relation types in the ACE RDC 
2004 corpus. Zhou et al (2007) describe a com-
posite kernel to integrate a context-sensitive 
CTK and a state-of-the-art linear kernel. It 
achieves the so far best F-measure of 75.8 on the 
7 relation types in the ACE RDC 2004 corpus. 
In this paper, we will further study how to dy-
namically determine a concise and effective tree 
span for a relation instance by exploiting con-
stituent dependencies inherent in the parse tree 
derivation. We also attempt to fully capture both 
the structured syntactic parse information and 
entity-related semantic information, especially 
combined entity features, via a unified parse and 
semantic tree. Finally, we validate the effective-
ness of a composite kernel for relation extraction, 
which combines a tree kernel and a linear kernel. 
698
3 Dynamic Syntactic Parse Tree 
This section discusses how to generate dynamic 
syntactic parse tree by employing constituent 
dependencies to overcome the problems existing 
in currently used tree spans. 
3.1 Constituent Dependencies in Parse Tree 
Zhang et al (2006) explore five kinds of tree 
spans and find that the Shortest Path-enclosed 
Tree (SPT) achieves the best performance. Zhou 
et al (2007) further propose Context-Sensitive 
SPT (CS-SPT), which can dynamically deter-
mine the tree span by extending the necessary 
predicate-linked path information outside SPT. 
However, the key problem of how to represent 
the structured syntactic parse tree is still partially 
resolved. As we indicate as follows, current tree 
spans suffer from two problems: 
(1) Both SPT and CS-SPT still contain unnec-
essary information. For example, in the sentence 
??bought one of town?s two meat-packing
plants?, the condensed information ?one of 
plants? is sufficient to determine ?DISC? rela-
tionship between the entities ?one? [FAC] and 
?plants? [FAC], while SPT/CS-SPT include the 
redundant underlined part. Therefore more un-
necessary information can be safely removed 
from SPT/CS-SPT. 
(2) CS-SPT only captures part of context-
sensitive information relating to predicate-linked 
structure (Zhou et al, 2007) and still loses much 
context-sensitive information. Let?s take the 
same example sentence ??bought one of town?s
two meat-packing plants?, where indeed there is 
no relationship between the entities ?one? [FAC] 
and ?town? [GPE]. Nevertheless, the information 
contained in SPT/CS-SPT (?one of town?) may 
easily lead to their relationship being misclassi-
fied as ?DISC?, which is beyond our expectation. 
Therefore the underlined part outside SPT/CS-
SPT should be recovered so as to differentiate it 
from positive instances. 
Since dependency plays a key role in many 
NLP problems such as syntactic parsing, seman-
tic role labeling as well as semantic relation ex-
traction, our motivation is to exploit dependency 
knowledge to distinguish the necessary evidence 
from the unnecessary information in the struc-
tured syntactic parse tree.  
On one hand, lexical or word-word depend-
ency indicates the relationship among words 
occurring in the same sentence, e.g. predicate-
argument dependency means that arguments are 
dependent on their target predicates, modifier-
head dependency means that modifiers are de-
pendent on their head words. This dependency 
relationship offers a very condensed representa-
tion of the information needed to assess the rela-
tionship in the forms of the dependency tree (Cu-
lotta and Sorensen, 2004) or the shortest depend-
ency path (Bunescu and Mooney, 2005) that in-
cludes both entities.
On the other hand, when the parse tree corre-
sponding to the sentence is derived using deriva-
tion rules from the bottom to the top, the word-
word dependencies extend upward, making a 
unique head child containing the head word for 
every non-terminal constituent. As indicated as 
follows, each CFG rule has the form: 
P? Ln?L1H R1?Rm
Here, P is the parent node, H is the head child of 
the rule, Ln?L1 and R1?Rm are left and right 
modifiers of H respectively, and both n and m
may be zero. In other words, the parent node P
depends on the head child H, this is what we call 
constituent dependency. Vice versa, we can also 
determine the head child of a constituent in terms 
of constituent dependency. Our hypothesis stipu-
lates that the contribution of the parse tree to es-
tablishing a relationship is almost exclusively 
concentrated in the path connecting the two enti-
ties, as well as the head children of constituent 
nodes along this path. 
3.2 Generation of Dynamic Syntactic Parse 
Tree
Starting from the Minimum Complete Tree 
(MCT, the complete sub-tree rooted by the near-
est common ancestor of the two entities under 
consideration) as the representation of each rela-
tion instance, along the path connecting two enti-
ties, the head child of every node is found ac-
cording to various constituent dependencies. 
Then the path nodes and their head children are 
kept while any other nodes are removed from the 
tree. Eventually we arrive at a tree called Dy-
namic Syntactic Parse Tree (DSPT), which is 
dynamically determined by constituent depend-
encies and only contains necessary information 
as expected. 
There exist a considerable number of constitu-
ent dependencies in CFG as described by Collins 
(2003). However, since our task is to extract the 
relationship between two named entities, our fo-
cus is on how to condense Noun-Phrases (NPs) 
and other useful constituents for relation extrac-
tion. Therefore constituent dependencies can be 
classified according to constituent types of the 
CFG rules: 
699
(1) Modification within base-NPs: base-NPs 
mean that they do not directly dominate an NP
themselves, unless the dominated NP is a posses-
sive NP. The noun phrase right above the entity
headword, whose mention type is nominal or 
name, can be categorized into this type. In this 
case, the entity headword is also the headword of 
the noun phrase, thus all the constituents before 
the headword are dependent on the headword,
and may be removed from the parse tree, while 
the headword and the constituents right after the 
headword remain unchanged. For example, in the 
sentence ??bought one of town?s two meat-
packing plants? as illustrated in Figure 1(a), the
constituents before the headword  ?plants? can 
be removed from the parse tree. In this way the
parse tree ?one of plants? could capture the
?DISC? relationship more concisely and pre-
cisely. Another interesting example is shown in 
Figure 1(b), where the base-NP of the second
entity ?town? is a possessive NP and there is no 
relationship between the entities ?one? and
?town? defined in the ACE corpus. For both SPT
and CS-SPT, this example would be condensed 
to ?one of town? and therefore easily misclassi-
fied as the ?DISC? relationship between the two 
entities. In the contrast, our DSPT can avoid this 
problem by keeping the constituent ??s? and the 
headword ?plants?.
(2) Modification to NPs: except base-NPs,
other modification to NPs can be classified into 
this type. Usually these NPs are recursive, mean-
ing that they contain another NP as their child. 
The CFG rules corresponding to these modifica-
tions may have the following forms:
NP? NP SBAR [relative clause]
NP? NP VP [reduced relative]
NP? NP PP [PP attachment]
Here, the NPs in bold mean that the path con-
necting the two entities passes through them. For
every right hand side, the NP in bold is modified
by the constituent following them. That is, the 
latter is dependent on the former, and may be 
reduced to a single NP. In Figure 1(c) we show a
sentence ?one of about 500 people nominated
for ??, where there exists a ?DISC? relationship
between the entities ?one? and ?people?. Since 
the reduced relative ?nominated for ?? modifies
and is therefore dependent on the ?people?, they 
can be removed from the parse tree, that is, the 
right side (?NP VP?) can be reduced to the left 
hand side, which is exactly a single NP. 
(a) Removal of constituents before the headword in base-NP
(b) Keeping of constituents after the headword in base-NP
NN
one
IN
of
DT
the
NN
town
POS
's
E-FAC
NN
plantstwo
CD NN
one
IN
of
NN
town
POS
's
E-FAC
NN
plantsmeat-packing
JJ
NN
one
PP
IN
of
NP
DT
the
NN
town
POS
's
NN
plantstwo
CD NN
one
IN
of
NN
plantsmeat-packing
JJ NN
one
IN
of
RB
about
QP
CD
500
NNS
people
...
nominated
VBN
for
IN
VP
PP
...
E2-PER
NN
one
IN
of
NNS
people
NN
property
PRP
he
VP
VBZ IN
in
NP
PP
state
NNS
the
NP
JJ
rental
S
owns
DT NN
property
PRP
he
VP
VBZ
owns
governors from connecticut
NNS IN
NP
E-GPE
NNP
,
,
south
NP
E-GPE
NNP
dakota
NNP
,
,
and
CC
montana
NNP
governors from
NNS IN
montana
NNP
(c) Reduction of modification to NP
(d) Removal of arguments to verb
(e) Reduction of conjuncts for NP coordination
E-GPE
NPPP
E1-FAC
NP
E2-FAC
NP
E1-FAC
NP
NP
NP
NP
E2-FAC E1-PER
NP
NP
PP
NP
NP
NP
E1-PER
PP
NP
E2-PER
NP
SBAR
E2-PER
S
NPNP
E1-FAC
PP
NP
NP
E1-PER
NP
E2-GPE
NP
E1-PER
PP
NP
NP
NP
E2-GPE
NP
E1-FAC E2-PER
NP
NP
SBAR
NP
NP
E1-FAC
PP
NP
NP
E2-GPE
NP
NP
E1-PER
PP
NP
NP
E2-GPE
Figure 1. Removal and reduction of constituents using dependencies 
700
(3) Arguments/adjuncts to verbs: this type 
includes the CFG rules in which the left side in-
cludes S, SBAR or VP. An argument represents
the subject or object of a verb, while an adjunct
indicates the location, date/time or way of the
action corresponding to the verb. They depend
on the verb and can be removed if they are not
included in the path connecting the two entities.
However, when the parent tag is S or SBAR, and
its child VP is not included in the path, this VP
should be recovered to indicate the predicate
verb. Figure 1(d) shows a sentence ?? maintain
rental property he owns in the state?, where the
?ART.User-or-Owner? relation holds between 
the entities ?property? and ?he?. While PP can be
removed from the rule  (?VP? VBZ PP?), the 
VP should be kept in the rule (?S? NP VP?).
Consequently, the tree span looks more concise 
and precise for relation extraction. 
(4) Coordination conjunctions: In coordina-
tion constructions, several peer conjuncts may be 
reduced into a single constituent. Although the
first conjunct is always considered as the head-
word (Collins, 2003), actually all the conjuncts
play an equal role in relation extraction. As illus-
trated in Figure 1(e), the NP coordination in the 
sentence (?governors from connecticut, south
dakota, and montana?) can be reduced to a single 
NP (?governors from montana?) by keeping the
conjunct in the path while removing the other 
conjuncts.
(5) Modification to other constituents: ex-
cept for the above four types, other CFG rules 
fall into this type, such as modification to PP,
ADVP and PRN etc. These cases are similar to 
arguments/adjuncts to verbs, but less frequent 
than them, so we will not detail this scenario. 
In fact, SPT (Zhang et al, 2006) can be ar-
rived at by carrying out part of the above re-
moval operations using a single rule (i.e. all the 
constituents outside the linking path should be
removed) and CS-CSPT (Zhou et al, 2007) fur-
ther recovers part of necessary context-sensitive 
information outside SPT, this justifies that SPT
performs well, while CS-SPT outperforms SPT. 
4 Entity-related Semantic Tree 
Entity semantic features, such as entity headword, 
entity type and subtype etc., impose a strong
constraint on relation types in terms of relation
definition by the ACE RDC task. Experiments by
Zhang et al (2006) show that linear kernel using 
only entity features contributes much when com-
bined with the convolution parse tree kernel. 
Qian et al (2007) further indicates that among
these entity features, entity type, subtype, and 
mention type, as well as the base form of predi-
cate verb, contribute most while the contribution
of other features, such as entity class, headword 
and GPE role, can be ignored. 
In order to effectively capture entity-related
semantic features, and their combined features as
well, especially bi-gram or tri-gram features, we 
build an Entity-related Semantic Tree (EST) in 
three ways as illustrated in Figure 2. In the ex-
ample sentence ?they ?re here?, which is ex-
cerpted from the ACE RDC 2004 corpus, there 
exists a relationship ?Physical.Located? between
the entities ?they? [PER] and ?here?
[GPE.Population-Center]. The features are en-
coded as ?TP?, ?ST?, ?MT? and ?PVB?, which
denote type, subtype, mention-type of the two 
entities, and the base form of predicate verb if 
existing (nearest to the 2nd entity along the path 
connecting the two entities) respectively. For 
example, the tag ?TP1? represents the type of the 
1st entity, and the tag ?ST2? represents the sub-
type of the 2nd entity. The three entity-related
semantic tree setups are depicted as follows: 
TP2TP1
(a) Bag Of Features(BOF)
ENT
ST2ST1 MT2MT1 PVB
(c) Entity-Paired Tree(EPT)
ENT
E1 E2
(b) Feature Paired Tree(FPT)
ENT
TP ST MT
ST1TP1 MT1 TP2 ST2 MT2
PVB
TP1 TP2 ST1 ST2 MT1 MT2
PVB
PER null PRO GPE Pop. PRO be
PER null PRO GPE Pop. PRO
be
PER GPE null Pop. PRO PRO
be
Figure 2. Different setups for entity-related se-
mantic tree (EST) 
(a) Bag of Features (BOF, e.g. Fig. 2(a)): all 
feature nodes uniformly hang under the root node,
so the tree kernel simply counts the number of 
common features between two relation instances.
This tree setup is similar to linear entity kernel
explored by Zhang et al (2006). 
(b) Feature-Paired Tree (FPT, e.g. Fig. 2(b)): 
the features of two entities are grouped into dif-
ferent types according to their feature names, e.g.
?TP1? and ?TP2? are grouped to ?TP?. This tree 
setup is aimed to capture the additional similarity
701
of the single feature combined from different 
entities, i.e., the first and the second entities. 
(c) Entity-Paired Tree (EPT, e.g. Fig. 2(c)): all 
the features relating to an entity are grouped to 
nodes ?E1? or ?E2?, thus this tree kernel can fur-
ther explore the equivalence of combined entity 
features only relating to one of the entities be-
tween two relation instances. 
In fact, the BOF only captures the individual 
entity features, while the FPT/EPT can addition-
ally capture the bi-gram/tri-gram features respec-
tively. 
Rather than constructing a composite kernel, 
we incorporate the EST into the DSPT to pro-
duce a Unified Parse and Semantic Tree (UPST) 
to investigate the contribution of the EST to rela-
tion extraction. The entity features can be at-
tached under the top node, the entity nodes, or 
directly combined with the entity nodes as in 
Figure 1. However, detailed evaluation (Qian et 
al., 2007) indicates that the UPST achieves the 
best performance when the feature nodes are at-
tached under the top node. Hence, we also attach 
three kinds of entity-related semantic trees (i.e. 
BOF, FPT and EPT) under the top node of the 
DSPT right after its original children. Thereafter, 
we employ the standard CTK (Collins and Duffy, 
2001) to compute the similarity between two 
UPSTs, since this CTK and its variations are 
successfully applied in syntactic parsing, seman-
tic role labeling (Moschitti, 2004) and relation 
extraction (Zhang et al, 2006; Zhou et al, 2007) 
as well. 
5 Experimentation 
This section will evaluate the effectiveness of the 
DSPT and the contribution of entity-related se-
mantic information through experiments. 
5.1 Experimental Setting  
For evaluation, we use the ACE RDC 2004 cor-
pus as the benchmark data. This data set contains 
451 documents and 5702 relation instances. It 
defines 7 entity types, 7 major relation types and 
23 subtypes. For comparison with previous work, 
evaluation is done on 347 (nwire/bnews) docu-
ments and 4307 relation instances using 5-fold 
cross-validation. Here, the corpus is parsed using 
Charniak?s parser (Charniak, 2001) and relation 
instances are generated by iterating over all pairs 
of entity mentions occurring in the same sentence 
with given ?true? mentions and coreferential in-
formation. In our experimentations, SVMlight
(Joachims, 1998) with the tree kernel function
(Moschitti, 2004) 2  is selected as our classifier. 
For efficiency, we apply the one vs. others
strategy, which builds K classifiers so as to 
separate one class from all others. For 
comparison purposes, the training parameters C 
(SVM) and ? (tree kernel) are also set to 2.4 and 
0.4 respectively. 
5.2 Experimental Results 
Table 1 evaluates the contributions of different 
kinds of constituent dependencies to extraction 
performance on the 7 relation types of the ACE 
RDC 2004 corpus using the convolution parse 
tree kernel as depicted in Figure 1. The MCT 
with only entity-type information is first used as 
the baseline, and various constituent dependen-
cies are then applied sequentially to dynamically 
reshaping the tree in two different modes: 
--[M1] Respective:  every constituent depend-
ency is individually applied on MCT. 
--[M2] Accumulative: every constituent de-
pendency is incrementally applied on the previ-
ously derived tree span, which begins with the 
MCT and eventually gives rise to a Dynamic 
Syntactic Parse Tree (DSPT).  
Dependency types P(%) R(%) F
MCT (baseline) 75.1 53.8 62.7
Modification within 
base-NPs
76.5
(59.8)
59.8
(59.8)
67.1
(67.1)
Modification to NPs 
77.0
(76.2)
63.2
(56.9)
69.4
(65.1)
Arguments/adjuncts to verb 
77.1
(76.1)
63.9
(57.5)
69.9
(65.5)
Coordination conjunctions 
77.3
(77.3)
65.2
(55.1)
70.8
(63.8)
Other modifications 
77.4
(75.0)
65.4
(53.7)
70.9
(62.6)
Table 1. Contribution of constituent dependen-
cies in respective mode (inside parentheses) and 
accumulative mode (outside parentheses) 
The table shows that the final DSPT achieves 
the best performance of 77.4%/65.4%/70.9 in
precision/recall/F-measure respectively after ap-
plying all the dependencies, with the increase of 
F-measure by 8.2 units compared to the baseline 
MCT. This indicates that reshaping the tree by 
exploiting constituent dependencies may signifi-
cantly improve extraction accuracy largely due to 
the increase in recall. It further suggests that con-
stituent dependencies knowledge is very effec-
2 http://ai-nlp.info.uniroma2.it/moschitti/ 
702
tive and can be fully utilized in tree kernel-based 
relation extraction. This table also shows that: 
(1) Both modification within base-NPs and 
modification to NPs contribute much to perform-
ance improvement, acquiring the increase of F-
measure by 4.4/2.4 units in mode M1 and 4.4/2.3 
units in mode M2 respectively. This indicates the 
local characteristic of semantic relations, which 
can be effectively captured by NPs near the two 
involved entities in the DSPT. 
(2) All the other three dependencies show mi-
nor contribution to performance enhancement, 
they improve the F-measure only by 2.8/0.9/-0.1 
units in mode M1 and 0.5/0.9/0.1 units in mode 
M2. This may be due to the reason that these de-
pendencies only remove the nodes far from the 
two entities. 
We compare in Table 2 the performance of 
Unified Parse and Semantic Trees with different 
kinds of Entity Semantic Tree setups using stan-
dard convolution tree kernel, while the SPT and 
DSPT with only entity-type information are 
listed for reference. It shows that: 
(1) All the three unified parse and semantic 
tree kernels significantly outperform the DSPT 
kernel, obtaining an average increase of ~4 units 
in F-measure. This means that they can effec-
tively capture both the structured syntactic in-
formation and the entity-related semantic fea-
tures.
(2) The Unified Parse and Semantic Tree with 
Feature-Paired Tree achieves the best perform-
ance of 80.1/70.7/75.1 in P/R/F respectively, 
with an increase of F-measure by 0.4/0.3 units 
over BOF and EPT respectively. This suggests 
that additional bi-gram entity features captured 
by FPT are more useful than tri-gram entity fea-
tures captured by EPT. 
Tree setups P(%) R(%) F
SPT 76.3 59.8 67.1
DSPT 77.4 65.4 70.9
UPST (BOF) 80.4 69.7 74.7
UPST (FPT) 80.1 70.7 75.1
UPST (EPT) 79.9 70.2 74.8
Table 2. Performance of Unified Parse and 
Semantic Trees (UPSTs) on the 7 relation types 
of the ACE RDC 2004 corpus 
In Table 3 we summarize the improvements of 
different tree setups over SPT. It shows that in a 
similar setting, our DSPT outperforms SPT by 
3.8 units in F-measure, while CS-SPT outper-
forms SPT by 1.3 units in F-measure. This sug-
gests that the DSPT performs best among these 
tree spans. It also shows that the Unified Parse 
and Semantic Tree with Feature-Paired Tree per-
form significantly better than the other two tree 
setups (i.e., CS-SPT and DSPT) by 6.7/4.2 units 
in F-measure respectively. This implies that the 
entity-related semantic information is very useful 
and contributes much when they are incorporated 
into the parse tree for relation extraction. 
Tree setups P(%) R(%) F
CS-SPT over SPT3 1.5   1.1 1.3
DSPT over SPT 1.1   5.6 3.8
UPST (FPT) over SPT 3.8 10.9 8.0
Table 3. Improvements of different tree setups 
over SPT on the ACE RDC 2004 corpus 
Finally, Table 4 compares our system with 
other state-of-the-art kernel-based systems on the 
7 relation types of the ACE RDC 2004 corpus. It 
shows that our UPST outperforms all previous 
tree setups using one single kernel, and even bet-
ter than two previous composite kernels (Zhang 
et al, 2006; Zhao and Grishman, 2005). Fur-
thermore, when the UPST (FPT) kernel is com-
bined with a linear state-of-the-state feature-
based kernel (Zhou et al, 2005) into a composite 
one via polynomial interpolation in a setting 
similar to Zhou et al (2007) (i.e. polynomial de-
gree d=2 and coefficient ?=0.3), we get the so far 
best performance of 77.1 in F-measure for 7 rela-
tion types on the ACE RDC 2004 data set. 
Systems P(%) R(%) F
Ours:
composite kernel 
83.0 72.0 77.1
Zhou et al, (2007):
composite kernel 
82.2 70.2 75.8
Zhang et al, (2006):
composite kernel 
76.1 68.4 72.1
Zhao and Grishman, (2005):4
composite kernel 
69.2 70.5 70.4
Ours:
CTK with UPST 
80.1 70.7 75.1
Zhou et al, (2007): context-
sensitive CTK with CS-SPT 
81.1 66.7 73.2
Zhang et al, (2006):
CTK with SPT 
74.1 62.4 67.7
Table 4. Comparison of different systems on 
the ACE RDC 2004 corpus 
3  We arrive at these values by subtracting P/R/F 
(79.6/5.6/71.9) of Shortest-enclosed Path Tree from P/R/F  
(81.1/6.7/73.2) of Dynamic Context-Sensitive Shortest-
enclosed Path Tree according to Table 2 (Zhou et al, 2007) 
4 There might be some typing errors for the performance 
reported in Zhao and Grishman (2005) since P, R and F do 
not match. 
703
6 Conclusion
This paper further explores the potential of struc-
tured syntactic information for tree kernel-based 
relation extraction, and proposes a new approach 
to dynamically determine the tree span (DSPT) 
for relation instances by exploiting constituent 
dependencies. We also investigate different ways 
of how entity-related semantic features and their 
combined features can be effectively captured in 
a Unified Parse and Semantic Tree (UPST). 
Evaluation on the ACE RDC 2004 corpus shows 
that our DSPT is appropriate for structured repre-
sentation of relation instances. We also find that, 
in addition to individual entity features, com-
bined entity features (especially bi-gram) con-
tribute much when they are combined with a 
DPST into a UPST. And the composite kernel, 
combining the UPST kernel and a linear state-of-
the-art kernel, yields the so far best performance. 
For the future work, we will focus on improv-
ing performance of complex structured parse 
trees, where the path connecting the two entities 
involved in a relationship is too long for current 
kernel methods to take effect. Our preliminary 
experiment of applying certain discourse theory 
exhibits certain positive results.
Acknowledgements 
This research is supported by Project 60673041 
under the National Natural Science Foundation 
of China, Project 2006AA01Z147 under the 
?863? National High-Tech Research and Devel-
opment of China, and the National Research 
Foundation for the Doctoral Program of Higher 
Education of China under Grant No. 
20060285008. We would also like to thank the 
excellent and insightful comments from the three 
anonymous reviewers. 
References
Bunescu, Razvan C. and Raymond J. Mooney. 2005. 
A Shortest Path Dependency Kernel for Relation 
Extraction. In Proceedings of the Human Language 
Technology Conference and Conference on Em-
pirical Methods in Natural Language Processing 
(EMNLP-2005), pages 724-731. Vancover, B.C. 
Charniak, Eugene. 2001. Intermediate-head Parsing 
for Language Models. In Proceedings of the 39th 
Annual Meeting of the Association of Computa-
tional Linguistics (ACL-2001), pages 116-123. 
Collins, Michael. 2003. Head-Driven Statistics Mod-
els for Natural Language Parsing. Computational 
linguistics, 29(4): 589-617. 
Collins, Michael and Nigel Duffy. 2001. Convolution 
Kernels for Natural Language. In Proceedings of 
Neural Information Processing Systems (NIPS-
2001), pages 625-632. Cambridge, MA. 
Culotta, Aron and Jeffrey Sorensen. 2004. Depend-
ency tree kernels for relation extraction. In Pro-
ceedings of the 42nd Annual Meeting of the Asso-
ciation of Computational Linguistics (ACL-2004),
pages 423-439. Barcelona, Spain. 
Joachims, Thorsten. 1998. Text Categorization with 
Support Vector Machine: learning with many rele-
vant features. In Proceedings of the 10th European 
Conference on Machine Learning (ECML-1998),
pages 137-142. Chemnitz, Germany. 
Moschitti, Alessandro. 2004. A Study on Convolution 
Kernels for Shallow Semantic Parsing. In Proceed-
ings of the 42nd Annual Meeting of the Association 
of Computational Linguistics (ACL-2004). Barce-
lona, Spain. 
Qian, Longhua, Guodong Zhou, Qiaoming Zhu and 
Peide Qian. 2007. Relation Extraction using Con-
volution Tree Kernel Expanded with Entity Fea-
tures. In Proceedings of the 21st Pacific Asian 
Conference on Language, Information and Compu-
tation (PACLIC-21), pages 415-421. Seoul, Korea. 
Zelenko, Dmitry, Chinatsu Aone and Anthony Rich-
ardella. 2003. Kernel Methods for Relation Extrac-
tion. Journal of Machine Learning Research, 
3(2003): 1083-1106. 
Zhang, Min, Jie Zhang, Jian Su and Guodong Zhou. 
2006. A Composite Kernel to Extract Relations be-
tween Entities with both Flat and Structured Fea-
tures. In Proceedings of the 21st International 
Conference on Computational Linguistics and the 
44th Annual Meeting of the Association of Compu-
tational Linguistics (COLING/ACL-2006), pages 
825-832. Sydney, Australia. 
Zhao, Shubin and Ralph Grishman. 2005. Extracting 
relations with integrated information using kernel 
methods. In Proceedings of the 43rd Annual Meet-
ing of the Association of Computational Linguistics 
(ACL-2005), pages 419-426. Ann Arbor, USA. 
Zhou, Guodong, Jian Su, Jie Zhang and Min Zhang. 
2005. Exploring various knowledge in relation ex-
traction. In Proceedings of the 43rd Annual Meet-
ing of the Association of Computational Linguistics 
(ACL-2005), pages 427-434. Ann Arbor, USA. 
Zhou, Guodong, Min Zhang, Donghong Ji and 
Qiaoming Zhu. 2007. Tree Kernel-based Relation 
Extraction with Context-Sensitive Structured Parse 
Tree Information. In Proceedings of the 2007 Joint 
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural 
Language Learning (EMNLP/CoNLL-2007), pages 
728-736. Prague, Czech. 
704
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 978?986,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
 
Global Learning of Noun Phrase Anaphoricity in Coreference Resolu-
tion via Label Propagation 
ZHOU GuoDong      KONG Fang 
JiangSu Provincial Key Lab for Computer Information Processing Technology 
School of Computer Science and Technology 
Soochow University. Suzhou, China 215006 
Email:{gdzhou,kongfang}@suda.edu.cn  
 
 
Abstract 
Knowledge of noun phrase anaphoricity might 
be profitably exploited in coreference resolu-
tion to bypass the resolution of non-anaphoric 
noun phrases. However, it is surprising to no-
tice that recent attempts to incorporate auto-
matically acquired anaphoricity information 
into coreference resolution have been some-
what disappointing. This paper employs a 
global learning method in determining the 
anaphoricity of noun phrases via a label 
propagation algorithm to improve learning-
based coreference resolution. In particular, 
two kinds of kernels, i.e. the feature-based 
RBF kernel and the convolution tree kernel, 
are employed to compute the anaphoricity 
similarity between two noun phrases. Experi-
ments on the ACE 2003 corpus demonstrate 
the effectiveness of our method in anaphoric-
ity determination of noun phrases and its ap-
plication in learning-based coreference resolu-
tion. 
1 Introduction 
Coreference resolution, the task of determining 
which noun phrases (NPs) in a text refer to the 
same real-world entity, has long been considered 
an important and difficult problem in natural 
language processing. Identifying the linguistic 
constraints on when two NPs can co-refer re-
mains an active area of research in the commu-
nity. One significant constraint on coreference, 
the anaphoricity constraint, specifies that a non-
anaphoric NP cannot be coreferent with any of 
its preceding NPs in a given text. Therefore, it is 
useful to skip over these non-anaphoric NPs 
rather than attempt an unnecessary search for an 
antecedent for them, only to end up with inaccu-
rate outcomes. Although many existing machine 
learning approaches to coreference resolution 
have performed reasonably well without explicit 
anaphoricity determination (e.g., Soon et al2001; 
Ng and Cardie 2002b; Strube and Muller 2003; 
Yang et al2003, 2008), anaphoricity determina-
tion has been studied fairly extensively in the 
literature, given the potential usefulness of NP 
anaphoricity in coreference resolution. One 
common approach involves the design of heuris-
tic rules to identify specific types of non-
anaphoric NPs, such as pleonastic pronouns (e.g. 
Paice and Husk 1987; Lappin and Leass 1994; 
Kennedy and Boguraev 1996; Denber 1998) and 
existential definite descriptions (e.g., Vieira and 
Poesio 2000). More recently, the problem has 
been tackled using statistics-based (e.g., Bean 
and Riloff 1999; Bergsma et al2008) and learn-
ing-based (e.g. Evans 2001; Ng and Cardie 
2002a; Ng 2004; Yang et al2005; Denis and 
Balbridge 2007) methods. Although there is em-
pirical evidence (e.g. Ng and Cardie 2002a, 
2004) that coreference resolution might be fur-
ther improved with proper anaphoricity informa-
tion, its contribution is still somewhat disap-
pointing and lacks systematic evaluation. 
This paper employs a label propagation (LP) 
algorithm for global learning of NP anaphoricity. 
Given the labeled data and the unlabeled data, 
the LP algorithm first represents labeled and 
unlabeled instances as vertices in a connected 
graph, then propagates the label information 
from any vertex to nearby vertices through 
weighted edges and finally infers the labels of 
unlabeled instances until a global stable stage is 
achieved. Here, the labeled data in this paper 
include all the NPs in the training texts with the 
anaphoricity labeled and the unlabeled data in-
clude all the NPs in a test text with the ana-
phoricity unlabeled. One major advantage of LP-
based anaphoricity determination is that the ana-
phoricity of all the NPs in a text can be deter-
mined together in a global way. Compared with 
previous methods, the LP algorithm can effec-
tively capture the natural clustering structure in 
both the labeled and unlabeled data to smooth 
the labeling function. In particular, two kinds of 
978
 kernels, i.e. the feature-based RBF kernel and 
the convolution tree kernel, are employed to 
compute the anaphoricity similarity between two 
NPs and weigh the edge between them. Experi-
ments on the ACE 2003 corpus show that our 
LP-based anaphoricity determination signifi-
cantly outperforms locally-optimized one, which 
adopts a classifier (e.g. SVM) to determine the 
anaphoricity of NPs in a text individually and 
significantly improves the performance of learn-
ing-based coreference resolution. It also shows 
that, while feature-based anaphoricity determi-
nation contributes much to pronoun resolution, 
its contribution on definite NP resolution can be 
ignored. In comparison, it shows that tree ker-
nel-based anaphoricity resolution contributes 
significantly to the resolution of both pronouns 
and definite NPs due to the inclusion of various 
kinds of syntactic structured information. 
The rest of this paper is organized as follows. 
In Section 2, we review related work in ana-
phoricity determination. Then, the LP algorithm 
is introduced in Section 3 while Section 4 de-
scribes different similarity measurements ex-
plored in the LP algorithm. Section 5 shows the 
experimental results. Finally, we conclude our 
work in Section 6.  
2 Related Work 
Given its potential usefulness in coreference 
resolution, anaphoricity determination has been 
studied fairly extensively in the literature and 
can be classified into three categories: heuristic 
rule-based (e.g. Paice and Husk 1987; Lappin 
and Leass 1994; Kennedy and Boguraev 1996; 
Denber 1998; Vieira and Poesio 2000), statis-
tics-based (e.g., Bean and Riloff 1999; Cherry 
and Bergsma 2005; Bergsma et al2008) and 
learning-based (e.g. Evans 2001; Ng and Cardie 
2002a; Ng 2004; Yang et al2005; Denis and 
Balbridge 2007). 
For the heuristic rule-based approaches, 
Paice and Husk (1987), Lappin and Leass (1994), 
Kennedy and Boguraev (1996), Denber (1998), 
and Cherry and Bergsma (2005) looked for par-
ticular constructions using certain trigger words 
to identify pleonastic pronouns while Vieira and 
Poesio (2000) recognized non-anaphoric definite 
NPs through the use of syntactic cues and case-
sensitive rules and found that nearly 50% of 
definite NPs are non-anaphoric. As a representa-
tive, Lappin and Leass (1994), and Kennedy and 
Boguraev (1996) looked for modal adjectives 
(e.g. ?necessary?) or cognitive verbs (e.g. ?It is 
thought that ? ?) in a set of patterned construc-
tions.  
For the statistics-based approaches, Bean 
and Riloff (1999) developed a statistics-based 
method for automatically identifying existential 
definite NPs which are non-anaphoric. The intui-
tion behind is that many definite NPs are not 
anaphoric since their meanings can be under-
stood from general world knowledge. They 
found that existential NPs account for 63% of all 
definite NPs and 76% of them could be identi-
fied by syntactic or lexical means. Using 1600 
MUC-4 terrorism news documents as the train-
ing data, they achieved 87% in precision and 
78% in recall at identifying non-anaphoric defi-
nite NPs. Cherry and Bergsma (2005) extended 
the work of Lappin and Leass (1994) for large-
scale anaphoricity determination by additionally 
detecting non-anaphoric instances of it using 
Minipar?s pleonastic category Subj. This is done 
by both employing Minipar?s named entity rec-
ognition to identify time expressions, such as ?it 
was midnight? ?, and providing a number of 
other linguistic patterns to match common non-
anaphoric it cases, such as in expressions ?darn 
it? and don?t overdo it?. Bergsma et al(2008) 
proposed a distributional method in detecting 
non-anaphoric pronouns by first extracting the 
surrounding textual context of the pronoun, then 
gathering the distribution of words that occurred 
within that context from a large corpus and fi-
nally learning to classify these distributions as 
representing either anaphoric and non-anaphoric 
pronoun instances. Experiments on  the Science 
News corpus of It-Bank 1  in identifying non-
anaphoric pronoun it show that their distribu-
tional method achieved the performance of 
81.4%, 71.0% and 75.8 in precision, recall and 
F1-measure, respectively, compared with the 
performance of 93.4%, 21.0% and 34.3 in preci-
sion, recall and F1-measure, respectively using 
the rule-based approach as described in Lappin 
and Leass (1994), and  the performance of 
66.4%, 49.7% and 56.9 in precision, recall and 
F1-measure, respectively using the rule-based 
approach as described in Cherry and Bergsma 
(2005).  
Among the learning-based methods, Evans 
(2001) applied a machine learning approach on 
identifying the non-anaphoricity of pronoun it. 
Ng and Cardie (2002a) employed various do-
main-independent features in identifying ana-
phoric NPs and showed how such information 
                                                 
1 www.cs.ualberta.ca/~bergsma/ItBank/ 
979
 can be incorporated into a coreference resolution 
system. Experiments show that their method im-
proves the performance of coreference resolu-
tion by 2.0 and 2.6 to 65.8 and 64.2 in F1-
measure on the MUC-6 and MUC-7 corpora, 
respectively, due to much more gain in precision 
compared with the loss in recall. Ng (2004) ex-
amined the representation and optimization is-
sues in computing and using anaphoricity infor-
mation to improve learning-based coreference 
resolution systems. He used an anaphoricity 
classifier as a filter for coreference resolution. 
Evaluation on the ACE 2003 corpus shows that, 
compared with a baseline coreference resolution 
system of no explicit anaphoricity determination, 
their method improves the performance by 2.8, 
2.2 and 4.5 to 54.5, 64.0 and 60.8 in F1-measure 
(due to the gain in precision) on the NWIRE, 
NPAPER and BNEWS domains, respectively, 
via careful determination of an anaphoricity 
threshold with proper constraint-based represen-
tation and global optimization. However, he did 
not look into the contribution of anaphoricity 
determination on coreference resolution of dif-
ferent NP types, such as pronoun and definite 
NPs. Yang et al(2005) made use of non-
anaphors to create a special class of training in-
stances in the twin-candidate model (Yang et al
2003) and thus equipped it with the non-
anaphoricity determination capability. Experi-
ments show that the proposed method improves 
the performance by 2.9 and 1.6 to 67.3 and 67.2 
in F1-measure on the MUC-6 and MUC-7 cor-
pora, respectively, due to much more gain in 
precision compared with the loss in recall. How-
ever, surprisingly, their experiments also show 
that eliminating non-anaphors using an ana-
phoricity determination module in advance 
harms the performance.  Denis and Balbridge 
(2007) employed an integer linear programming 
(ILP) formulation for coreference resolution 
which models anaphoricity and coreference as a 
joint task, such that each local model informs the 
other for final assignments. Experiments on the 
NWIRE, NPAPER and BNEWS domains of the 
ACE 2003 corpus shows that this joint ana-
phoricity-coreference ILP formulation improves 
the F1-measure by 0.7-1.0 over the coreference-
only ILP formulation. However, their experi-
ments assume true ACE mentions(i.e. all the 
ACE mentions are already known from the an-
notated corpus). Therefore, the actual effect of 
this joint anaphoricity-coreference ILP formula-
tion on fully-automatic coreference resolution is 
still unclear. 
3 Label Propagation  
In the LP algorithm (Zhu and Ghahramani 2002), 
the natural clustering structure in data is repre-
sented as a connected graph. Given the labeled 
data and unlabeled data, the LP algorithm first 
represents labeled and unlabeled instances as 
vertices in a connected graph, then propagates 
the label information from any vertex to nearby 
vertices through weighted edges and finally in-
fers the labels of unlabeled instances until a 
global stable stage is achieved. Figure 1 presents 
the label propagation algorithm. 
___________________________________________
Assume:  
Y : the rn * labeling matrix, where ijy  represents 
the probability of vertex )1( nixi K=  with 
label )1( rjr j K= ; 
LY : the top l  rows of 
0Y . LY corresponds to the 
l  labeled instances; 
UY : the bottom u  rows of 
0Y . UY corresponds to 
the u  unlabeled instances; 
T : a nn *  matrix, with ijt  is the probability 
jumping from vertex ix to vertex jx ; 
 
BEGIN (the algorithm) 
Initialization:  
1)  Set the iteration index 0=t ;  
2)  Let 0Y  be the initial soft labels attached to 
each vertex;  
3)  Let 0LY  be consistent with the labeling in the 
labeled data, where 0ijy = the weight of the 
labeled instance if ix  has the label jr  ;  
4)  Initialize 0UY ; 
REPEAT 
Propagate the labels of any vertex to nearby ver-
tices by tt YTY =+1 ; 
Clamp the labeled data, that is, replace 1+tLY  
with 0LY ; 
UNTIL Y converges(e.g. 1+tLY  converges to 
0
LY ); 
Assign each unlabeled instance with a label: for 
)( nilxi ?p , find its label with 
j
ijymaxarg ; 
END (the algorithm) 
___________________________________________ 
Figure 1: The LP algorithm 
Here, each vertex corresponds to an instance, 
and the edge between any two instances ix  and 
jx  is weighted by ijw  to measure their similar-
ity. In principle, larger edge weights allow labels 
to travel through easier. Thus the closer the in-
stances are, the more likely they have similar 
980
 labels. The algorithm first calculates the weight 
ijw  using a kernel, then transforms it 
to ?
=
=?=
n
k
kjijij wwijpt
1
/)( , which meas-
ures the probability of propagating a label from 
instance jx to instance ix , and finally normal-
izes ijt row by row using ?
=
=
n
k
ikijij ttt
1
/  to maintain 
the class probability interpretation of the label-
ing matrix Y .  
During the label propagation process, the la-
bel distribution of the labeled data is clamped in 
each loop using their initial weights and acts like 
forces to push out labels through the unlabeled 
data. With this push originating from the labeled 
data, the label boundaries will be pushed faster 
along edges with larger weights and settle in 
gaps along those with lower weights. Ideally, we 
can expect that ijw  across different classes 
should be as small as possible and ijw  within the 
same class as big as possible. In this way, label 
propagation tends to happen within the same 
class. This algorithm has been shown to con-
verge to a unique solution (Zhu and Ghahramani 
2002), which can be obtained without iteration 
in theory, and the initialization of YU0 (the unla-
beled data) is not important since YU0 does not 
affect its estimation. However, proper initializa-
tion of YU0 actually helps the algorithm converge 
more rapidly in practice. In this paper, each row 
in YU0 , i.e. the label distribution for each test 
instance, is initialized to the weighted similarity 
of the test instance with the labeled instances. 
4 Kernel-based Similarity  
The key issue in label propagation is how to 
compute the similarity ijw between two in-
stances ix  and jx . This paper examines two 
similarity measures: the feature-based RBF ker-
nel and the convolution tree kernel. 
Feature Type Feature Description 
IsPronoun 1 if current NP is a pronoun, else 0 
IsDefiniteNP 1 if current NP is a define NP, else 0 
IsDemonstrativeNP 1 if current NP is a demonstrative NP,  else 0 
IsArg0 1 if the semantic role of current NP is Arg0/agent, else 0 
IsArg0MainVerb 1 if current NP has the semantic role of Arg0/agent for the 
main predicate of the sentence, else 0 
IsArgs 0 if current NP has no semantic role, else 1 
IsSingularNP 1 if current NP is a singular noun, else 0 
Features  
related with  
current NP itself 
IsMaleFemalePronoun 1 if current NP is a male/female personal pronoun, else 0 
StringMatch 1 if there is a full string match between current NP and one 
of other phrases in the context, else 0 
NameAlias 1 if current NP and one of other phrases in the context is a 
name alias or abbreviation of the other, else 0 
Appositive 1 if current NP and one of other phrases in the context are 
in an appositive structure, else 0 
NPNested 1 if current NP is nested in another NP, else 0 
NPNesting 1 if current NP nests another NP, else 0 
WordSenseAgreement 1 if current NP and one of other phrases in the context agree 
in the WordNet sense, else 0 
IsFirstNPinSentence 1 if current NP is the first NP of this sentence, else 0 
BackwardDistance The distance between current NP and  the nearest backward 
clause, indicated by coordinating words (e.g. that,which). 
Features  
related with  
the local context 
surrounding 
current NP 
ForwardDistance The distance between the nearest forward clause, indicated 
by coordinating words (e.g. that, which), and current NP. 
Table 1: Features in anaphoricity determination of NPs. Note: the semantic role-related features are derived from 
an in-house state-of-the-art semantic role labeling system.
4.1 Feature-based Kernel 
In our feature-based RBF kernel to anaphoricity 
determination, an instance is represented by 17 
lexical, syntactic and semantic features, as 
shown in Table 1, which are specifically de-
signed for distinguishing anaphoric and non-
anaphoric NPs, according to common-sense 
knowledge and linguistic intuitions. Since the 
local context surrounding an NP plays a critical 
role in discriminating whether an NP is ana-
phoric or not, the features in Table 1 can be clas-
sified into two categories: (a) current NP (i.e. the 
NP in anaphoricity consideration) itself, e.g. 
981
 types and semantic roles of  current NP; (b) con-
textual information, e.g.  whether current NP is 
nested in another NP, the distance between cur-
rent NP and a clause structure, indicated by co-
ordinating words (e.g. that, this, which). 
4.2 Tree Kernel 
Given a NP in anaphoricity determination, a 
parse tree represents the local context surround-
ing current NP in a structural way and thus con-
tains much information in determining whether 
current NP is anaphoric or not. For example, the 
commonly used knowledge for anaphoricity de-
termination, such as the grammatical role of cur-
rent NP or whether current NP is nested in other 
NPs, can be directly captured by a parse tree 
structure.  
Given a parse tree and a NP in consideration, 
the problem is how to choose a proper parse tree 
structure to cover syntactic structured informa-
tion well in the tree kernel computation. Gener-
ally, the more a parse tree structure includes, the 
more syntactic structured information would be 
provided, at the expense of more 
noisy/unnecessary information. In this paper, we 
limit the window size to 5  chunks (either NPs or 
non-NPs), including previous two chunks, cur-
rent chunk (i.e. current NP) and following two 
chunks, and prune out the substructures outside 
the window.  Figure 2 shows the full parse tree 
for the sentence ?Mary said the woman in the 
room hit her too?, using the Charniak parser 
(Charniak 2001), and the chunk sequence de-
rived from the parse tree using the Perl script2 
written by Sabine Buchholz from Tilburg Uni-
versity. 
Here, we explore four parse tree structures 
in NP anaphoricity determination: the common 
tree (CT), the shortest path-enclosed tree (SPT), 
the minimum tree (MT) and the dynamically 
extended tree (DET), motivated by Yang et al
(2006) and Zhou et al(2008). Following are the 
examples of the four parse tree structures, corre-
sponding to the full parse tree and the chunk se-
quence, as shown in Figure 2, with the NP chunk 
?(NP (DT the) (NN woman))? in anaphoricity 
determination. 
Common Tree (CT) 
As shown in Figure 3(a), CT is the complete 
sub-tree rooted by the nearest common ancestor 
of the first chunk ?(NP (NNP Mary))? and the 
                                                 
2 http://ilk.kub.nl/~sabine/chunklink/  
last chunk ?(NP (DT the) (NN room))? of the 
five-chunk window.  
Shortest Path-enclosed Tree (SPT) 
As shown in Figure 3(b), SPT is  the smallest 
common sub-tree enclosed by the shortest path 
between the first chunk ?(NP (NNP Mary))? and 
the last chunk ?(NP (DT the) (NN room))? of the 
five-chunk window.  
 
(a) the full parse tree 
(NP (NNP Mary)) (VP (VBD said)) (NP-E (DT the) 
(NN woman)) (PP (IN in)) (NP (DT the) (NN room)) 
(VP (VBD hit)) (NP (PRP her)) (ADVP (RB too)) 
(b) the chunk sequence 
Figure 2: The full parse tree for the sentence ?Mary 
said the woman in the room hit her too?, using the 
Charniak parser, and the corresponding chunk se-
quence derived from it. Here, the label ?E? indicates 
the NP in consideration. 
 
(a) CT: Common Tree 
 
(b) SPT: Shortest Path-enclosed Tree 
982
  
(c) MT: Minimum Tree 
 
(d) DET: Dynamically Extended Tree 
Figure 3: Examples of parse tree structures. 
Minimum Tree (MT) 
As shown in Figure 3(c), MT only keeps the root 
path from the NP in anaphoricity determination 
to the root node of SPT. 
Dynamically Extended Tree (DET),  
The intuitions behind DET are that the informa-
tion related with antecedent candidates (all the  
antecedent candidates compatible3 with current 
NP in anaphoricity consideration), predicates 4 
and right siblings plays a critical role in corefer-
ence resolution. Given a MT, this is done by:  
1)  Attaching all the compatible antecedent can-
didates and their corresponding paths. As 
shown in Figure 3(d), ?Mary? is attached 
while ?the room? is not since the former is 
compatible with the NP ?the woman? and 
the latter is not compatible with the NP ?the 
woman?. In this way, possible coreference 
between current NP and the compatible an-
tecedent candidates can be included in the 
parse tree structure. In some sense, this is a 
natural extension of the twin-candidate 
                                                 
3 With matched number, person and gender agreements. 
4 For simplicity, only verbal predicates are considered in 
this paper. However, this can be extended to nominal predi-
cates with automatic identification of nominal predicates. 
learning method proposed in Yang et al
(2003), which explicitly models the compe-
tition between two antecedent candidates.  
2)  For each node in MT, attaching the path from 
the node to the leaf node of the correspond-
ing predicate, if it is predicate-headed, in the 
sense that such predicate-related information 
is useful in identifying certain kinds of ex-
pressions with non-anaphoric NPs, e.g. the 
non-anaphoric it in ?darn it?. As shown in 
Figure 3(d), ?said? and ?hit? are attached.  
3)  Attaching the path to the head word of the 
first right sibling if the parent of current NP 
is a NP and current NP has one or more right 
siblings. Normally, the NP in anaphoricity 
consideration, NP-E, in the production of 
?NP->NP-E+PP? introduces a new entity 
and thus non-anaphoric. 
4)  Pruning those nodes (except POS nodes) 
with the single in-arc and the single out-arc 
and with its syntactic phrase type same as its 
child node.  
In this paper, the similarity between two 
parse trees is measured using a convolution tree 
kernel, which counts the number of common 
sub-trees as the syntactic structure similarity 
between two parse trees. For details, please refer 
to Collins and Duffy (2001). 
5 Experimentation  
We have systematically evaluated the label 
propagation algorithm on global learning of NP 
anaphoricity determination on the ACE 2003 
corpus, and its application in coreference resolu-
tion. 
5.1 Experimental Setting 
The ACE 2003 corpus contains three domains: 
newswire (NWIRE), newspaper (NPAPER), and 
broadcast news (BNEWS). For each domain, 
there exist two data sets, training and devtest, 
which are used for training and testing respec-
tively.  
As a baseline coreference resolution system, 
a  raw test text is first preprocessed automati-
cally by a pipeline of NLP components, includ-
ing sentence boundary detection, POS tagging, 
named entity recognition and phrase chunking, 
and then a training or test instance is formed by 
a anaphor and one of its antecedent candidates, 
similar to Soon et al(2001). Among them, 
named entity recognition, part-of-speech tagging 
and noun phrase chunking apply the same Hid-
den Markov Model (HMM)-based engine with 
983
 error-driven learning capability (Zhou and Su, 
2000 & 2002). During training, for each anaphor 
encountered, a positive instance is created by 
pairing the anaphor and its closest antecedent 
while a set of negative instances is formed by 
pairing the anaphor with each of the non-
coreferential candidates. Based on the training 
instances, a binary classifier is generated using a 
particular learning algorithm. In this paper, we 
use SVMLight developed by Joachims (1998). 
During resolution, an anaphor is first paired in 
turn with each preceding antecedent candidate to 
form a test instance, which is presented to a 
classifier. The classifier then returns a confi-
dence value indicating the likelihood that the 
candidate is the antecedent. Finally, the candi-
date with the highest confidence value is se-
lected as the antecedent. As a baseline, the NPs 
with mismatched number, person and gender 
agreements are filtered out. On average, an ana-
phor has ~7 antecedent candidates. In particular, 
the test corpus is resolved in document-level, i.e. 
one document by one document. 
For anaphoricity determination, we report 
the performance in Acc+ and Acc-, which meas-
ure the accuracies of identifying anaphoric NPs 
and non-anaphoric NPs, respectively. Obviously, 
higher Acc+ means that more anaphoric NPs 
would be identified correctly, while higher Acc- 
means that more non-anaphoric NPs would be 
filtered out. For coreference resolution, we re-
port the performance in terms of recall, precision, 
and F1-measure using the commonly-used 
model theoretic MUC scoring program (Vilain 
et al, 1995). For separate scoring of different 
NP types, a recognized reference is considered 
correct if the reconized antecedent is in the 
coreferential chain of the anaphor. To see 
whether an improvement is significant, we con-
duct significance testing using paired t-test. In 
this paper, ?>>>?, ?>>? and ?>? denote p-values 
of an improvement smaller than 0.01, in-
between (0.01, 0,05] and bigger than 0.05, 
which mean significantly better, moderately 
better and slightly better, respectively.  
5.2 Experimental Results 
Table 2 shows the performance of LP-based ana-
phoricity determination using the feature-based 
RBF kernel. It shows that our method achieves 
the accuracies of 74.8/84.4, 76.2/81.3 and 
71.8/81.7 on identifying anaphoric/non-
anaphoric NPs in the NWIRE, NPAPER and 
BNEWS domains, respectively. This suggests 
that our approach can effectively filter out about 
82% of non-anaphoric NPs. However, it can 
only keep about 74% of anaphoric NPs. Table 2 
also shows the performance on different NP 
types. Considering the effectiveness of ana-
phoricity determination on indefinite NPs (due 
to that most of anaphoric indefinite NPs are in 
an appositive structure and thus can be easily 
captured by the IsAppositive feature) and that 
most of errors in anaphoricity determination on 
proper nouns are caused by the named entity 
recognition module in the preprocessing), it in-
dicates the difficulty of anaphoricity determina-
tion in filtering out non-anaphoric pronouns and 
identifying anaphoric definite NPs. As a com-
parison, Table 2 also shows the performance of 
locally-optimized anaphoricity determination 
using a classifier (SVM with the feature-based 
RBF kernel, as adopted in this paper) to deter-
mine the NPs in a text individually. It shows that 
the LP-based method systematically outperforms 
(>>>) the SVM-based method. This suggests the 
effectiveness of the LP algorithm in global mod-
eling of the natural clustering structure in ana-
phoricity determination. 
Table 3 shows the performance of LP-based 
anaphoricity determination using the convolu-
tion tree kernel on different parse tree structures. 
It shows that while MT performed worst due to 
its simple structure, DET outperforms MT(>>>), 
SPT(>>>) and CT(>>>) on all the three domains 
due to fine inclusion of necessary structural in-
formation, although inclusion of more informa-
tion in both CT and SPT also improves the per-
formance. It again verifies that LP-based ana-
phoricity determination outperforms (>>>) 
SVM-based one, using the tree kernel. Table 4 
further indicates that all the three kinds of struc-
tural information related with antecedent candi-
dates, predicates and right siblings in DET con-
tribute significantly (>>>). In addition, Table 5 
shows the detailed performance of LP-based 
anaphoricity determination on different anaphor 
types using DET. Compared with the feature-
based RBF kernel as shown in Table 2, it shows 
that the convolution tree kernel significantly 
outperforms (>>>) the feature-based RBF kernel 
in all the three domains, with much contribution 
due to performance improvement on both pro-
nouns and definite NPs, although the tree kernel 
performs moderately worse than the feature-
based RBF kernel due to the effectiveness of 
anaphoricity determination on proper nouns and 
indefinite NPs using the IsNameAlias and IsAp-
positive features respectively. 
 
984
 NWIRE NPAPER BNEWS Anaphor 
Type Acc
+ 
(%) 
Acc-
(%) 
Acc+ 
(%) 
Acc-
(%) 
Acc+ 
(%) 
Acc-
(%) 
Pronoun 88.7 56.2 90.2 58.6 87.4 57.8 
ProperNoun 72.5 85.2 74.6 80.5 70.6 78.8 
DefiniteNP 66.6 83.1 72.1 77.5 65.3 81.5 
InDefiniteNP 95.4 93.7 90.5 95.8 87.2 97.3 
Overall 74.8 84.4 76.2 81.3 71.8 81.7 
Overall(SVM) 71.3 80.2 73.5 79.1 68.4 78.6 
Table 2: The performance of LP-based anaphoric-
ity determination using the feature-based RBF kernel  
Parse Tree structure  
Scheme 
NWIRE
( %)  
NPAPER
( %)  
BNEWS
( %)  
Acc+ 72.6 74.3 74.2 CT Acc- 82.1 80.2 72.3 
Acc+ 72.4 74.1 73.8 SPT Acc- 80.8 79.5 72.5 
Acc+ 71.4 70.5 66.9 MT Acc- 77.2 75.3 78.2 
Acc+ 79.2 81.2 76.5 DET Acc- 87.8 84.5 85.3 
Acc+ 76.5 78.9 74.3 DET(SVM) 
Acc- 82.3 81.6 83.2 
Table 3: The performance of LP-based anaphoric-
ity determination using the convolution tree kernel on 
different parse tree structures 
Performance Change NWIRE( %)  
NPAPER
( %)  
BNEWS
( %)  
Acc+ -4.0 -3.8 -4.3 - antecedent 
candidates Acc- -5.2 -5.3 -4.5 
Acc+ -5.2 -4.8 -5.6 -predicate Acc- -4.3 -3.5 -4.9 
Acc+ -3.6 -4.1 -3.1 -first right 
sibling Acc- -4.8 -5.2 -4.4 
Table 4: The contribution of structural information 
in DET 
 
NWIRE NPAPER BNEWS Anaphor 
Type Acc
+ 
(%) 
Acc-
(%) 
Acc+ 
(%) 
Acc-
(%) 
Acc+ 
(%) 
Acc-
(%) 
Pronoun 90.1 75.6 90.7 79.2 89.2 77.5 
ProperNoun 71.4 83.5 72.8 78.1 68.3 77.2 
DefiniteNP 74.6 89.1 77.3 85.5 75.3 88.7 
InDefiniteNP 93.2 92.1 90.2 94.2 89.4 95.5 
Overall 79.2 87.8 81.2 84.5 76.5 85.3 
Table 5: The performance of LP-based anaphoric-
ity determination using the tree kernel on DET 
Finally, we evaluate the effect of LP-based 
anaphoricity determination on coreference reso-
lution by including it as a preprocessing step to a 
baseline coreference resolution system without 
explicit anaphoricity determination, which em-
ploys the same set of features, as adopted in the 
single-candidate model of Yang et al(2003), 
using a SVM-based classifier and the feature-
based RBF kernel. It shows that anaphoricity 
determination with the feature-based RBF Ker-
nel much improves (>>>) the performance of 
coreference resolution with most of the contribu-
tion due to pronoun resolution while its contri-
bution on definite NPs can be ignored. It indi-
cates the usefulness of anaphoricity determina-
tion in filtering out non-anaphoric pronouns and 
the difficulty in identifying anaphoric definite 
NPs, using the feature-based RBF kernel. It also 
shows that tree kernel-based anaphoricity deter-
mination can not only improve (>>>) the per-
formance on pronoun resolution but also im-
prove (>>>) the performance on definite NP 
resolution due to the much better performance of 
tree kernel-based anaphoricity determination on 
definite NPs. This suggests the necessity of ex-
ploring structural information in identifying 
anaphoric definite NPs. 
NWIRE NPAPER BNEWS System 
R% P% F R% P% F R% P% F 
Pronoun 66.5 61.6 64.0 70.1 64.2 67.0 61.7 63.2 62.4 
DefiniteNP 26.9 80.3 40.2 34.5 62.4 44.4 30.5 71.4 42.9 BaseLine (No Anaphoricity) 
Overall 53.1 67.4 59.4 57.7 67.0 62.1 48.0 65.9 55.5 
Pronoun 64.1 67.9 66.0 67.3 72.4 69.8 59.5 75.7 66.6 
DefiniteNP 26.7 80.6 40.3 34.2 62.5 44.3 30.4 71.9 43.1 +Anaphoricity determination  with the feature-based RBF kernel Overall 50.6 75.4 60.7 54.4 77.1 63.8 45.9 76.9 57.4 
Pronoun 63.5 70.9 67.0 68 74.9 71.3 61.1 77.6 68.3 
DefiniteNP 28.5 82.4 42.1 36.2 65.3 46.1 32.3 73.1 44.2 +Anaphoricity determination with the convolution tree kernel Overall 51.6 77.2 61.8 55.2 78.6 65.2 47.5 80.3 59.6 
Table 6: Employment of anaphoricity determination in coreference resolution 
6 Conclusion  
This paper systematically studies a global learn-
ing method in identifying the anaphoricity of 
noun phrases via a label propagation algorithm 
and the application of an explicit anaphoricity 
determination module in improving learning-
based coreference resolution. In particular, two 
kinds of kernels, i.e. the feature-based RBF ker-
nel and the convolution tree kernel, are em-
ployed to compute the anaphoricity similarity 
985
 between two NPs. Evaluation on the ACE 2003 
corpus indicates that LP-based anaphoricity de-
termination using both the kernels much im-
proves the performance of coreference resolu-
tion. It also shows the usefulness of various 
structural information, related with antecedent 
candidates, predicates and right siblings, in  tree 
kernel-based anaphoricity determination and in 
coreference resolution of both pronouns and 
definite NPs. 
To our knowledge, this is the first system-
atic exploration of both feature-based and tree 
kernel methods in anaphoricity determination 
and the application of an explicit anaphoricity 
determination module in learning coreference 
resolution.  
Acknowledgement  
This research is supported by Project 60873150 
under the National Natural Science Foundation of  
China, project 2006AA01Z147 under the  ?863? 
National High-Tech Research and Development of 
China, project 200802850006 under the National 
Research Foundation for the Doctoral Program of 
Higher Education of China. 
References  
Bean D. and Riloff E. (1999). Corpus-based Identifi-
cation of Non-Anaphoric Noun Phrases. 
ACL?1999:373-380. 
Bergsma S., Lin D.K. and Goebel R. (2008). Distri-
butional Identification of Non-Referential Pro-
nouns. ACL?2008: 10-18. 
Charniak E. (2001). Immediate-head Parsing for Lan-
guage Models. ACL?2001: 129-137.  
Cherry C. and Bergsma S. (2005). An expectation 
maximization approach to pronoun resolution. 
CoNLL?2005:88-95. 
Collins M. and Duffy N. (2001). Convolution kernels 
for natural language. NIPS?2001: 625-632. 
Denber M. (1998). Automatic Resolution of Anaph-
ora in English. Technical Report, Eastman Kodak 
Co. 
Denis P. and Baldridge J. (2007). Joint determination 
of anaphoricity and coreference using integer pro-
gramming. NAACL-HLT?2007:236-243. 
Evans R. (2001). Applying machine learning toward 
an automatic classification of it. Literary and Lin-
guistic Computing, 16(1):45.57. 
Joachims T. (1998). Text Categorization with Sup-
port Vector Machine: learning with many relevant 
features. ECML-1998: 137-142. 
Kennedy C. and Boguraev B. (1996). Anaphora for 
everyone: pronominal anaphora resolution without 
a parser. COLING?1996: 113-118. 
Lappin S. and Leass H.J. (1994). An algorithm for 
pronominal anaphora resolution. Computational 
Linguistics, 20(4):535.561. 
Ng V. and Cardie C. (2002a). Identifying anaphoric 
and non-anaphoric noun phrases to improve 
coreference resolution. COLING?2002:730-736. 
Ng V. and Cardie C. (2002b). Improving machine 
learning approaches to coreference resolution. 
ACL?2002: 104-111 
Ng V. (2004). Learning Noun Phrase Anaphoricity to 
Improve Conference Resolution: Issues in Repre-
sentation and Optimization. ACL?2004: 151-158 
Paice C.D. and Husk G.D. (1987). Towards the auto-
matic recognition of anaphoric features in English 
text: the impersonal pronoun it. Computer Speech 
and Language, 2:109-132. 
Soon W.M., Ng H.T. and Lim D. (2001). A machine 
learning approach to coreference resolution of 
noun phrase. Computational Linguistics, 2001, 
27(4):521-544. 
Strube M. and Muller C. (2003). A machine learning 
approach to pronoun resolution in spoken dialogue. 
ACL?2003: 168-175 
Vieira R. and Poesio M. (2000). An empirically 
based system for processing definite descriptions. 
Computational Linguistics, 27(4): 539?592. 
Vilain M., Burger J., Aberdeen J., Connolly D. and 
Hirschman L. (1995). A model theoretic corefer-
ence scoring scheme. MUC-6: 45?52. 
Yang X.F., Zhou G.D., Su J. and Chew C.L. (2003). 
Coreference Resolution Using Competition Learn-
ing Approach. ACL?2003:177-184 
Yang X.F., Su J. and Tan C.L. (2005). A Twin-
Candidate Model of Coreference Resolution with 
Non-Anaphor Identification Capability. 
IJCNLP?2005:719-730. 
Yang X.F., Su J. and Tan C.L. (2006). Kernel-based 
pronoun resolution with structured syntactic 
knowledge. COLING-ACL?2006: 41-48. 
Yang X.F., Su J., Lang J., Tan C.L., Liu T. and Li S. 
(2008). An Entity-Mention Model for Coreference 
Resolution with Inductive Logic Programming. 
ACL?2008: 843-851. 
Zhou G.D. and Su. J. (2000). Error-driven HMM-
based chunk tagger with context-dependent lexi-
con.  EMNLP-VLC?2000: 71?79 
Zhou G.D. and Su J. (2002). Named Entity recogni-
tion using a HMM-based chunk tagger. In 
ACL?2002:473?480. 
Zhou G.D., Kong F. and Zhu Q.M. (2008). Context-
sensitive convolution tree kernel for pronoun 
resolution . IJCNLP?2008:25-31. 
Zhu X. and Ghahramani Z. (2002). Learning from 
Labeled and Unlabeled Data with Label Propaga-
tion. CMU CALD Technical Report.CMU-CALD-
02-107. 
986
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 987?996,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
 
Employing the Centering Theory in Pronoun Resolution from the Se-
mantic Perspective 
 
KONG Fang     ZHOU GuoDong*    ZHU Qiaoming 
JiangSu Provincial Key Lab for Computer Information Processing Technology 
School of Computer Science and Technology 
Soochow University. Suzhou, China 215006 
Email: {kongfang, gdzhou, qmzhu}@suda.edu.cn 
 
  
 
                                                          
* Corresponding author 
Abstract 
In this paper, we employ the centering the-
ory in pronoun resolution from the seman-
tic perspective. First, diverse semantic role 
features with regard to different predicates 
in a sentence are explored. Moreover, given 
a pronominal anaphor, its relative ranking 
among all the pronouns in a sentence, ac-
cording to relevant semantic role informa-
tion and its surface position, is incorporated. 
In particular, the use of both the semantic 
role features and the relative pronominal 
ranking feature in pronoun resolution is 
guided by extending the centering theory 
from the grammatical level to the semantic 
level in tracking the local discourse focus. 
Finally, detailed pronominal subcategory 
features are incorporated to enhance the 
discriminative power of both the semantic 
role features and the relative pronominal 
ranking feature. Experimental results on the 
ACE 2003 corpus show that the centering-
motivated features contribute much to pro-
noun resolution.  
1 Introduction 
Coreference accounts for cohesion in a text and 
is, in a sense, the hyperlink for a natural lan-
guage. Especially, a coreference instance de-
notes an identity of reference and holds between 
two referring expressions, which can be named 
entities, definite noun phrases, pronouns and so 
on. Coreference resolution is the process of link-
ing together multiple referring expressions of a 
given entity in the world. The key in coreference 
resolution is to determine the antecedent for 
each referring expression in a text. The ability of 
linking referring expressions both within a sen-
tence and across the sentences in a text is critical 
to discourse and language understanding in gen-
eral. For example, coreference resolution is a 
key task in information extraction, machine 
translation, text summarization, and question 
answering. 
There is a long tradition of research on 
coreference resolution within computational lin-
guistics. While earlier knowledge-lean ap-
proaches heavily depend on domain and 
linguistic knowledge (Carter 1987; Carbonell 
and Brown 1988) and have significantly influ-
enced the research, the later approaches usually 
rely on diverse lexical, syntactic and semantic 
properties of referring expressions (Soon et al, 
2001;Ng and Cardie, 2002; Zhou et al, 2004). 
Current research has been focusing on exploiting 
semantic information in coreference resolution. 
For example, Yang et al(2005) proposed a tem-
plate-based statistical approach to compute the 
semantic compatibility between a pronominal 
anaphor and an antecedent candidate, and Yang 
and Su (2007) explored semantic relatedness 
information from automatically discovered pat-
terns, while Ng (2007) automatically induced 
semantic class knowledge from a treebank and 
explored its application in coreference resolution. 
Particularly, this paper focuses on the center-
ing theory (Sidner,1981;Grosz et al,1995; 
Tetreault,2001), which reveals the significant 
impact of the local focus on referring expres-
sions in that the antecedent of a referring expres-
sion usually depends on the center of attention 
throughout the local discourse segment (Mit-
kov,1998). Although the centering theory has 
been considered as a critical theory and the driv-
ing force behind the coreferential phenomena 
since its proposal, its application in coreference 
resolution (in particular pronoun resolution) has 
been somewhat disappointing: it fails to improve 
or even harms the performance of the state-of-
987
 the-art coreference resolution systems in previ-
ous research (e.g. Yang et al 2004). This may be 
due to that centering was originally proposed as 
a model of discourse coherence instead of 
coreference. 
The purpose of this paper is to employ the 
centering theory in pronoun resolution by ex-
tending it from the grammatical level to the se-
mantic level. The intuition behind our approach 
is that, via determining the semantic roles of 
referring expressions in a sentence, such as 
agent and patient, we can derive various center-
ing theory-motivated features in tracking the 
continuity or shift of the local discourse focus, 
thus allowing us to include document-level 
event descriptive information in resolving the 
coreferential relations between referring expres-
sions.  
To the best of our knowledge, this is the first 
research, which successfully applies the center-
ing theory in pronoun resolution from the se-
mantic perspective.  
The rest of this paper is organized as follows. 
Section 2 briefly describes related work in em-
ploying the centering theory and semantic in-
formation in coreference resolution. Then, the 
centering theory is introduced in Section 3 while 
Section 4 details how to employ the centering 
theory from the semantic perspective. Section 5 
reports and discusses the experimental results. 
Finally, we conclude our work in Section 6. 
2 Related Work 
This section briefly overviews the related work 
in coreference resolution from both the centering 
theory and semantic perspectives. 
2.1 Centering Theory 
In the literature, there has been much research in 
the centering theory and its application to 
coreference resolution. 
In the centering theory itself, since the origi-
nal work of Sidner (1979) on immediate focus-
ing of pronouns and the subsequent work of 
Joshi and Weinstein (1981) on centering and 
inferences, much research has been done, in-
cluding centering and linguistic realizations 
(Cote 1993; Prince and Walker 1995), empirical 
and psycholinguistic evaluation of centering 
predictions (Gordon et al 1993,1995; Brennan 
1995; Walker et al1998; Kibble 2001), and the 
cross-linguistic work on centering (Ziv and 
Crosz1994). 
In applications of the centering theory to 
coreference resolution, representative work in-
cludes Brennan et al (1987), Strube (1998), 
Tetreault (1999) and Yang et al (2004). Brennan 
et al (1987) presented a centering theory-based 
formalism in modeling the local focus structure 
in discourse and used it to track the discourse 
context in binding occurring pronouns to corre-
sponding entities. In particular, a BFP (Brennan, 
Friedman and Pollard) algorithm is proposed to 
extend the original centering model to include 
two additional transitions called smooth shift 
and rough shift. Strube (1998) proposed an S-list 
model, assuming that a referring expression pre-
fers a hearer-old discourse entity to other hearer-
new candidates. Tetreault (1999) further ad-
vanced the BFP algorithm by adopting a left-to-
right breadth first walk of the syntactic parse 
trees to rank the antecedent candidates. However, 
the above methods have not been systematically 
evaluated on large annotated corpora, such as 
MUC and ACE. Thus their effects are still un-
clear in real coreference resolution tasks. Yang 
et al(2004) presented a learning-based approach 
by incorporating several S-list model-based fea-
tures to improve the performance in pronoun 
resolution. It shows that, although including S-
list model-based features can slightly boost the 
performance in the ideal case (i.e. given the cor-
rect antecedents of anaphor?s candidates), it de-
teriorates the overall performance in F-measure 
with slightly higher precision but much lower 
recall, in real cases, where the antecedents of 
anaphor?s candidates are determined automati-
cally by a separate coreference resolution mod-
ule.  
2.2 Semantic Information 
It is well known that semantic information plays 
a critical role in coreference resolution. Besides 
the common practice of employing a thesaurus 
(e.g. WordNet) in semantic consistency check-
ing, much research has been done to explore 
various kinds of semantic information, such as 
semantic similarity (Harabagiu et al2000), se-
mantic compatibility (Yang et al2005, 2007), 
and semantic class information (Soon et al2001; 
Ng 2007). Although these methods have been 
proven useful in coreference resolution, their 
contributions are much limited. For example, Ng 
(2007) showed that semantic similarity informa-
tion and semantic agreement information could 
only improve the performance of coreference 
resolution by 0.6 and 0.5 in F-measure respec-
tively, on the ACE 2003 NWIRE corpus.  
988
 3 Centering Theory 
The centering theory is a theory about the local 
discourse structure that models the interaction of 
referential continuity and the salience of dis-
course entities in the internal organization of a 
text. In natural languages, a given entity may be 
referred by different expressions and act as dif-
ferent grammatical roles throughout a text. For 
example, people often use pronouns to refer to 
the main subject of the discourse in focus, which 
can change over different portions of the dis-
course. One main goal of the centering theory is 
to track the focus entities throughout a text.  
The main claims of the centering theory can 
be formalized in terms of Cb (the backward-
looking center), Cf (a list of forward-looking 
centers for each utterance Un) and Cp (the pre-
ferred center, i.e. the most salient candidate for 
subsequent utterances). Given following two 
sentences: 1) Susani gave Betsyj a pet hamsterk; 
2) Shei reminded herj that such hamstersk were 
quite shy. We can have Ub, Uf and Up as follows: 
Ub= ?Susan?; Uf={?Susan?, ?Betsy?, ?a pet 
hamster?}; Up= ?Susan?. 
 Cb(Un)=Cb(Un-1)  or Cb(Un-1) undefined
Cb(Un)?Cb(Un-1)
Cb(Un)=Cp(Un) Continue Smooth Shift
Cb(Un)?Cp(Un) Retain Rough Shift
Table 1: Transitions in the centering theory 
Constraints 
C1. There is precisely one Cb. 
C2. Every element of Cf(Un) must be realized in Un. 
C3. Cb(Un) is the highest-ranked element of Cf(Un-1) 
that is realized in Un. 
Rules 
R1. If some element of Cf(Un-1) is realized as a pro-
noun in Un, then so is Cb(Un). 
R2.Transitions have the descending preference order 
of ?Continue > Retain > Smooth Shift > Rough 
Shift?. 
Table 2: Constraints and rules in the centering theory 
Furthermore, several kinds of focus transi-
tions are defined in terms of two tests: whether 
Cb stays the same (i.e. Cb(Un+1)=Cb(Un)), and 
whether Cb is realized as the most prominent 
referring expression (i.e. Cb(Un=Cp(Un)). We 
refer to the first test as cohesion, and the second 
test as salience. Therefore, there are four possi-
ble combinations, which are displayed in Table 
1 and can result in four kinds of transitions, 
namely Continue, Retain, Smooth Shift, and 
Rough Shift. Obviously, salience, which chooses 
a proper verb form to make Cb prominent within 
a clause or sentence, is an important matter for 
sentence planning, while cohesion, which orders 
propositions in a text to maintain referential con-
tinuity, is an important matter for text planning.  
Finally, the centering theory imposes several 
constraints and rules over Cb/Cf and above tran-
sitions, as shown in Table 2. 
Given the centering theory as described above, 
we can draw the following conclusions: 
1) The centering theory is discourse-related and 
centers are discourse constructs.   
2) The backward-looking center Cb of Un de-
pends only on the expressions that constitute 
the utterance. That is, it is independent of its 
surface position and grammatical roles. 
Moreover, it is not constrained by any previ-
ous utterance in the segment. While the ele-
ments of Cf(Un) are partially ordered to 
reflect relative prominence in Un, grammati-
cal role information is often a major determi-
nant in ranking Cf, e.g. in the descending 
priority order of ?Subject > Object > Others? 
in English (Grosz and Joshi, 2001).  
3) Psychological research (Gordon et al 1993) 
and cross-linguistic research (Kameyama 
1986, 1988; Walker et al 1990,1994) have 
validated that Cb is preferentially realized by 
a pronoun in English.  
4) Frequent rough shifts would lead to a lack of 
local cohesion. To keep local cohesion, peo-
ple tend to plan ahead and minimize the 
number of focus shifts. 
In this paper, we extend the centering theory 
from the grammatical level to the semantic level 
in attempt to better model the continuity or shift 
in the local discourse focus and improve the per-
formance of pronoun resolution via centering-
motivated semantic role features. 
4 Employing Centering Theory from  
Semantic Perspective 
In this section, we discuss how to employ the 
centering theory in pronoun resolution from the 
semantic perspective. In Subsection 4.1, we in-
troduce the semantic roles. In Subsection 4.2, we 
introduce how to employ the centering theory in 
pronoun resolution via semantic role features. 
Finally we compare our method with the previ-
ous work in Subsection 4.3. 
4.1 Semantic Role 
A semantic role is the underlying relationship 
that a participant has with a given predicate in a 
clause, i.e. the actual role a participant plays in 
989
 an event, apart from linguistic encoding of the 
situation. If, in some situation, someone named 
?John? purposely hits someone named ?Bill?, 
then ?John? is the agent and ?Bill? is the patient 
of the hitting event. Therefore, given the predi-
cate ?hit? in both of the following sentences, 
?John? has the same semantic role of agent and 
?Bill? has the same semantic role of patient: 1) 
John hit Bill. 2) Bill was hit by John.  
In the literature, labeling of such semantic 
roles has been well defined by the SRL (Seman-
tic Role Labeling) task, which first identifies the 
arguments of a given predicate and then assigns 
them appropriate semantic roles. During the last 
few years, there has been growing interest in 
SRL. For example, CoNLL 2004 and 2005 have 
made this problem a well-known shared task. 
However, there is still little consensus in the lin-
guistic and NLP communities about what set of 
semantic role labels are most appropriate. Typi-
cal semantic roles include core roles, such as 
agent, patient, instrument, and adjunct roles 
(such as locative, temporal, manner, and cause). 
For core roles, only agent and patient are consis-
tently defined across different predicates, e.g. in 
the popular PropBank (Palmer et al 2005) and 
the derived version evaluated in the CoNLL 
2004 and 2005 shared tasks, as ARG0 and 
ARG1.  
In this paper, we extend the centering theory 
from the grammatical level to the semantic level 
for its better application in pronoun resolution 
via proper semantic role features due to three 
reasons:  
Sentence Grammatical Role Semantic Role
Bob opened the 
door with a key. 
Bob:  
SUBJECT 
Bob:  
AGENT 
The key opened 
the door. 
The key: 
SUBJECT 
The key : 
INSTRUMENT
The door opened. The door: 
SUBJECT 
The door: 
PATIENT 
Table 3: Relationship between grammatical roles and 
semantic roles: an example 
1) Semantic roles are conceptual notions, 
whereas grammatical roles are morph-
syntactic. While the original centering theory 
mainly builds from the grammatical perspec-
tive and grammatical roles do not always cor-
respond directly to semantic roles (Table 3 
shows an example of various semantic roles 
which a subject can play), there is a close re-
lationship between semantic roles and gram-
matical roles. The statistics in the CoNLL 
2004 and 2005 shared tasks (Shen and Lapata, 
2007) shows that the semantic roles of 
ARG0/agent and ARG1/patient account for 
85% of all arguments and most likely act as 
the centers of the local focus structure in dis-
course due to the close relationship between 
subject/object and agent/patient. Therefore, it 
is appropriate to model the centers of an ut-
terance from the semantic perspective via 
semantic roles. 
2) In a sense, semantic roles imply the informa-
tion of grammatical roles, especially for sub-
ject/object. For example, the position of an 
argument and the voice of the predicate verb 
play a central role in SRL. In intuition, an ar-
gument, which occurs before an active verb 
and has the semantic role of Arg0/agent, 
tends to be a subject. That is to say, semantic 
roles (e.g. Arg0/agent and Arg1/patient) can 
be mapped into their corresponding gram-
matical roles (e.g. subject and object), using 
some heuristic rules. Therefore, it would be 
interesting to represent the centers of the ut-
terances and employ the centering theory 
from the semantic perspective. 
3) Semantic role labeling has been well studied 
in the literature and there are good ready-to-
use toolkits available. For example, Pradhan 
(2005) achieved 82.2 in F-measure on the 
CoNLL 2005 version of the Propbank. In 
contrast, the research on grammatical role la-
beling is much less with the much lower 
state-of-the-art performance of 71.2 in F-
measure (Buchholz, 1999). Therefore, it may 
be better to explore the centering theory from 
the semantic perspective. 
4.2 Designing Centering-motivated Fea-
tures from  Semantic Perspective 
In this paper, the centering theory is employed in 
pronoun resolution via three kinds of centering-
motivated features: 
1) Semantic role features. They are achieved by 
checking possible semantic roles of referring 
expressions with regard to various predicates 
in a sentence. Due to the close relationship 
between subject/object and agent/patient, se-
mantic role information should be also a ma-
jor determinant in deciding the center of an 
utterance, which is likely to be the antecedent 
of a referring expression in the descending 
priority order of ?Agent > Patient > Others? 
with regard to their semantic roles, corre-
sponding to the descending priority order of 
?Subject > Object > Others? with regard to 
their grammatical roles. 
990
 2) Relative pronominal ranking feature. Due to 
the predominance of pronouns in tracking the 
local discourse structure1, the relative rank-
ing of a pronoun among all the pronouns in a 
sentence should be useful in pronoun resolu-
tion. This is realized in this paper according 
to its semantic roles (with regard to various 
predicates in a sentence) and surface position 
(in a left-to-right order) by mapping each 
pronoun into 5 levels: a) rank 1 for pronouns 
with semantic role ARG0/agent of the main 
predicate; b) rank 2 for pronouns with seman-
tic role ARG1/patient of the main predicate; c) 
rank 3 for pronouns with semantic role 
ARG0/agent of other predicates; d) rank 4 for 
pronouns with semantic role ARG1/patient of 
other predicates; e) rank 5 for remaining pro-
nouns. Furthermore, for those pronouns with 
the same ranking level, they are ordered ac-
cording to their surface positions in a left-to-
right order, motivated by previous research 
on the centering theory (Grosz et al 1995). 
3) Detailed pronominal subcategory features. 
Given a pronominal expression, its detailed 
pronominal subcategory features, such as 
whether it is a first person pronoun, second 
person pronoun, third person pronoun, neuter 
pronoun or others, are explored to enhance 
the discriminative power of both the semantic 
role features and the relative pronominal 
ranking feature, considering the predominant 
importance of pronouns in tracking the local 
focus structure in discourse.  
4.3 Comparison with Previous Work 
As a representative in explicitly employing se-
mantic role labeling in coreference resolution, 
Ponzetto and Strube (2006) explored two seman-
tic role features to capture the predicate-
argument structure information to benefit 
coreference resolution: I_SEMROLE, the predi-
cate-argument pairs of one referring expression, 
and J_SEMROLE, the predicate-argument pairs 
of another referring expression. Their experi-
ments on the ACE 2003 corpus shows that, 
while the two semantic role features much im-
prove the performance of common noun resolu-
tion by 3.8 and 2.7 in F-measure on the BNEWS 
and NWIRE domains respectively, they only 
                                                          
1 According to the centering theory, the backward-looking 
center Cb is preferentially realized by a pronoun in the sub-
ject position in natural languages, such as English, and 
people tend to plan ahead and minimize the number of 
focus shifts. 
slightly improve the performance of pronoun 
resolution by 0.4 and 0.3 in F-measure on the 
BNEWS and NWIRE domains respectively.  
In comparison, this paper proposes various 
kinds of centering-motivated semantic role fea-
tures in attempt to better model the continuity or 
shift in the local discourse focus by extending 
the centering theory from the grammatical level 
to the semantic level. For example, the 
CAARG0MainVerb feature (as shown in Table 
5) is designed to capture the semantic role of the 
antecedent candidate in the main predicate in 
modeling the discourse center, while, the AN-
PronounRanking feature (as shown in Table 5) is 
designed to determinate the relative priority of 
the pronominal anaphor in retaining the dis-
course center.  
Although both this paper and Ponzetto and 
Strube (2006) employs semantic role features, 
their ways of deriving such features are much 
different due to different driving 
forces/motivations behind. As a result, their con-
tributions on coreference resolution are different: 
while the semantic role features in Ponzette and 
Strube (2006) captures the predicate-argument 
structure information and contributes much to 
common noun resolution and their contribution 
on pronoun resolution can be ignored, the cen-
tering-motivated semantic role features in this 
paper contribute much in pronoun resolution. 
This justifies our attempt to better model the 
continuity or shift of the discourse focus in pro-
noun resolution by extending the centering the-
ory from the grammatical level to the semantic 
level and employing the centering-motivated 
features in pronoun resolution.. 
5 Experimentation and Discussion 
We have evaluated our approach of employing 
the centering theory in pronoun resolution from 
the semantic perspective on the ACE 2003 cor-
pus. 
5.1 Experimental Setting 
The ACE 2003 corpus contains three domains: 
newswire (NWIRE), newspaper (NPAPER), and 
broadcast news (BNEWS). For each domain, 
there exist two data sets, training and devtest, 
which are used for training and testing respec-
tively. Table 4 lists the pronoun distributions 
with coreferential relationships in the training 
data and the test data over pronominal subcate-
gories and sentence distances. Table 4(a) shows 
that third person pronouns occupy most and neu-
991
 tral pronouns occupy second while Table 4(b) 
shows that the antecedents of most pronouns 
occur within the current sentence and the previ-
ous sentence, with a little exception in the test 
data set of BNEWS.  
NWIRE NPAPER BNEWSPronoun  
Subcategory Train Test Train Test Train Test
First Person 263 103 283 120 455 258
Second Person 61 16 29 36 203 68
Third Person 618 179 919 263 736 158
Neuter 395 151 577 190 482 137
Reflexive 23 6 42 12 26 6
Other 0 0 2 0 2 3
(a) Distribution over pronominal subcategories 
NWIRE NPAPER BNEWSDistance 
Train Test Train Test Train Test
0 890 254 1281 347 1149 295
1 447 149 529 197 729 188
2 0 27 0 24 0 41
>2 0 19 0 41 0 100
Total 1337 449 1810 609 1878 624
(b) Distribution over sentence distances 
Table 4: Pronoun statistics on the ACE 2003 corpus 
For preparation, all the documents in the cor-
pus are preprocessed automatically using a pipe-
line of NLP components, including tokenization 
and sentence segmentation, named entity recog-
nition, part-of-speech tagging and noun phrase 
chunking. Among them, named entity recogni-
tion, part-of-speech tagging and noun phrase 
chunking apply the same Hidden Markov Model 
(HMM)-based engine with error-driven learning 
capability (Zhou and Su, 2000 & 2002). In par-
ticular for SRL, we use a state-of-the-art in-
house toolkit, which achieved the precision of 
87.07% for ARG0 identification and the preci-
sion of 78.97% for ARG1 identification, for easy 
integration. In addition, we use the SVM-light2 
toolkit with the radial basis kernel and default 
learning parameters. Finally, we report the per-
formance in terms of recall, precision, and F-
measure, where precision measures the percent-
age of correctly-resolved pronouns (i.e. correctly 
linked with any referring expression in the 
coreferential chain), recall measures the cover-
age of correctly-resolved pronouns, and F-
measure gives an overall figure on equal har-
mony between precision and recall. To see 
whether an improvement is significant, we also 
conduct significance testing using paired t-test. 
In this paper, ?>>>?, ?>>? and ?>? denote p-
values of an improvement smaller than 0.01, in-
between (0.01, 0,05] and bigger than 0.05, 
                                                          
2 http://svmlight.joachims.org/ 
which mean significantly better, moderately 
better and slightly better, respectively. 
5.2 Experimental Results 
Table 5 details various centering-motivated fea-
tures from the semantic perspective, which are 
incorporated in our final system. For example, 
the CAARG0MainVerb feature is designed to 
capture the semantic role of the antecedent can-
didate in the main predicate in modeling the dis-
course center, while the ANPronounRanking 
feature is designed to determinate the relative 
priority of the pronominal anaphor in retaining 
the discourse center. As the baseline, we dupli-
cated the representative system with the same set 
of 12 basic features, as described in Soon et al
(2001). Table 6 shows that our baseline system 
achieves the state-of-the-art performance of 62.3, 
65.3 and 59.0 in F-measure on the NWIRE, 
NPAPER and BNEWS domains, respectively. It 
also shows that the centering-motivated features 
(from the semantic perspective) significantly 
improve the F-measure by 3.6(>>>), 4.5(>>>) 
and 7.7(>>>) on the NWIRE, NPAPER and 
BNEWS domains, respectively. This justifies 
our attempt to model the continuity or shift of 
the discourse focus in pronoun resolution via 
centering-motivated features from the semantic 
perspective. For comparison, we also evaluate 
the performance of our final system from the 
grammatical perspective. This is done by replac-
ing semantic roles with grammatical roles in 
deriving centering-motivated features. Here, la-
beling of grammatical roles is achieved using a 
state-of-the-art toolkit, as described in Buchholz 
(1999). Table 6 shows that properly employing 
the centering theory in pronoun resolution from 
the grammatical perspective can also improve 
the performance. However, the performance im-
provement of employing the centering theory 
from the grammatical perspective is much lower, 
compared with that from the semantic perspec-
tive. This validates our attempt of employing the 
centering theory in pronoun resolution from the 
semantic perspective instead of from the gram-
matical perspective. This also suggests the great 
potential of applying the centering theory in 
pronoun resolution since the centering theory is 
a local coherence theory, which tells how subse-
quent utterances in a text link together.  
Table 7 shows the contribution of the seman-
tic role features and the relative pronominal 
ranking feature in pronoun resolution when the 
detailed pronominal subcategory features are 
included: 
992
  
Feature category Feature Remarks 
CAARG0 1 if the semantic role of the antecedent candidate is ARG0/agent; else 0 
CAARG0MainVerb 1 if the antecedent candidate has the semantic role of ARG0/agent for the main predicate of the sentence; else 0 
Semantic Role-based  Fea-
tures 
ANCASameTarget 1 if the anaphor and the antecedent candidate share the same predicate with regard to their semantic roles; else 0 
Relative Pronominal Rank-
ing Feature ANPronounRanking
Whether the pronominal anaphor is ranked highest among all 
the pronouns in the sentence 
ANPronounType Whether the anaphor is a first person, second person, third person, neuter pronoun or others Detailed Pronominal Sub-
category Features 
CAPronounType Whether the antecedent candidate is a first person, second person, third person, neuter pronoun or others 
Table 5: Centering-motivated features incorporated in our final system  
(with AN indicating the anaphor and CA indicating the antecedent candidate) 
NWIRE NPAPER BNEWS System Variation 
R% P% F R% P% F R% P% F 
Baseline System 57.0 68.6 62.3 61.1 70.1 65.3 49.0 73.9 59.0
Final System 
(from the semantic perspective) 
64.1 67.8 65.9 67.5 72.4 69.8 59.9 75.3 66.7
Final System  
(from the grammatical perspective, for comparison)
63.3 64 63.6 64.7 68.8 66.7 57.1 70.1 63.1
Table 6: Contributions of centering-motivated features in pronoun resolution 
NWIRE NPAPER BNEWS System Variation 
R% P% F R% P% F R% P% F 
Baseline System 57.0 68.6 62.3 61.1 70.1 65.3 49.0 73.9 59.0 
+SR and DC 64.8 67.8 66.3 67.2 72.9 69.9 59.1 75.3 66.3 
+PR and DC 61.5 65.4 63.4 64.9 72.1 68.3 57.4 73.5 64.5 
+SR, PR and DC (Final System) 64.1 67.8 65.9 67.5 72.4 69.8 59.9 75.3 66.7 
Table 7: Contribution of the semantic role features (SR) and the relative pronominal ranking feature (PR) in pro-
noun resolution when the detailed pronominal subcategory features are included 
1) The inclusion of the semantic role features 
improve the performance by 4.0(>>>), 
4.6(>>>) and 7.3(>>>) in F-measure on the 
NWIRE, NPAPER and BNEWS domains, re-
spectively. This suggests the impact of se-
mantic role information in determining the 
local discourse focus.  Since pronouns prefer-
entially occur in the subject position and tend 
to refer to the main subject (Ehrlich 1980; 
Brennan 1995; Walker et al 1998; Cahn 
1995; Gordon and Searce 1995; Kibble et al 
2001), this paper only applies semantic fea-
tures related with the semantic role of 
ARG0/agent, which is closely related with 
the grammatical role of subject, with regard 
to various predicates in a sentence. We have 
also explored features related with other se-
mantic roles. However, our preliminary ex-
perimentation shows that they do not improve 
the performance, even for ARG1/patient, and 
thus are not included in the final system. This 
may be due to that other semantic roles are 
not discriminative enough to make a differ-
ence in deciding the local discourse structure. 
2) It is surprising to notice that further inclusion 
of the relative pronominal ranking feature has 
only slight impact (slight positive impact on 
the BNEWS domain and slight negative im-
pact on the NWIRE and NPAPER domains) 
on the ACE 2003 corpus. This suggests that 
most of information in the relative pronomi-
nal ranking feature has been covered by the 
semantic role features. This is not surprising 
since the semantic role of ARG0/agent, 
which is explored to derive the semantic role 
features, is also applied to decide the relative 
pronominal ranking feature.  
The inclusion of the relative pronominal 
ranking feature improve the performance by 
1.1(>>>), 3.0(>>>) and 5.5(>>>) in F-measure. 
Our further evaluation reveals that the perform-
ance improvement difference among different 
domains of the ACE 2003 corpus is due to the 
distribution of pronouns? antecedents occurring 
over different sentence distances, as shown in 
993
 Table 4. This suggests the usefulness of the rela-
tive pronominal ranking feature in resolving 
pronominal anaphors over longer distance. This 
is consistent with our observation that, as the 
percentage of pronominal anaphors referring to 
more distant antecedents increase, its impact 
turns gradually from negative to positive, when 
further including the relative pronominal ranking 
feature after the semantic role features. The rea-
son that we include the detailed pronominal sub-
category information is due to predominant 
importance of pronouns in tracking the local 
focus structure in discourse and that such de-
tailed pronominal subcategory information is 
discriminative in tracking different subcatego-
ries of pronouns. This suggests the usefulness of 
considering the distribution of the local dis-
course focus over detailed pronominal subcate-
gories. One interesting finding in our 
preliminary experimentation is that the inclusion 
of the detailed pronominal subcategory features 
alone even harms the performance. This may be 
due to the reason that the detailed pronominal 
subcategory features do not have the discrimina-
tive power themselves and that the semantic role 
features and the relative pronominal ranking fea-
ture provide an effective mechanism to explore 
the role of such detailed pronominal subcategory 
features in helping determine the local discourse 
focus. 
 Pronoun  
Subcategory 
NWIRE NPAPER BNEWS
First Person 55.7 55.9 56.6 
Second Person 54.6 60.4 44.0 
Third Person 72.6 80.9 75.7 
Neuter 41.5 50.4 50.2 
Reflexive 85.7 70.0 60.0 
B
as
el
in
e 
Sy
st
em
 
Total 62.3 65.3 59.0 
First Person 64.7 67.0 65.6 
Second Person 78.6 70.0 51.9 
Third Person 80.9 81.8 80.4 
Neuter 48.3 53.0 58.3 
Reflexive 71.4 66.7 80.0 Fi
na
l S
ys
te
m
 
Total 65.9 69.8 66.7 
Table 8: Performance comparison of pronoun resolu-
tion in F-measure over pronoun subcategories 
Table 8 shows the contribution of the center-
ing-motivated features over different pronoun 
subcategories. It shows that the centering-
motivated features contribute much to the reso-
lution of the four major pronoun subcategories 
(i.e. first person, second person, third person and 
neuter) while its negative impact on the minor 
pronoun subcategories (e.g. reflexive) can be 
ignored due to their much less frequent occur-
rence in the corpus.  In particular, the centering-
motivated features improve the performance on 
the major three pronoun subcategories of third 
person / neuter / first person, by 
8.3(>>>)/6.8(>>>)/9.0(>>>), 0.9(>>)/ 2.6 
(>>>)/11.1(>>>) and 4.7(>>>)/8.1(>>>)/9.0 
(>>>), on the NWIRE, NPAPER and BNEWS 
domains of the ACE 2003 corpus, respectively. 
 Distance NWIRE NPAPER BNEWS
<=0 61.6 64.5 68.7 
<=1 60.4 67.5 60.0 
<=2 62.9 67.4 63.7 
B
as
el
in
e 
Sy
st
em
 
Total 62.3 65.3 59.0 
<=0 64.3 70.3 78.7 
<=1 66.8 72.3 72.5 
<=2 66.6 71.8 71.8 
Fi
na
l S
ys
-
te
m
 
Total 65.9 69.8 66.7 
Table 9: Performance comparison of pronoun resolu-
tion in F-measure over sentence distances 
Table 9 shows the contribution of the center-
ing-motivated features over different sentence 
distances. It shows that the centering-motivated 
features improve the performance of pronoun 
resolution on different sentence distances of 
0/1/2, by 2.7(>>>) / 5.8(>>>) / 10.0 (>>>), 
6.4(>>>) / 4.8(>>>) / 12.5(>>>) and 3.7 
(>>>)/4.4(>>>)/8.1(>>>), on the NWIRE, 
NPAPER and BNEWS domains of the ACE 
2003 corpus, respectively. This suggests that the 
centering-motivated features are helpful for both 
intra-sentential and inter-sentential pronoun 
resolution. 
6 Conclusion and Further Work 
This paper extends the centering theory from the 
grammatical level to the semantic level and 
much improves the performance of pronoun 
resolution via centering-motivated features from 
the semantic perspective. This is mainly realized 
by employing various semantic role features 
with regard to various predicates in a sentence, 
in attempt to model the continuity or shift of the 
local discourse focus. Moreover, the relative 
ranking feature of a pronoun among all the pro-
nouns is explored to help determine the relative 
priority of the pronominal anaphor in retaining 
the local discourse focus. Evaluation on the 
ACE 2003 corpus shows that both the centering-
motivated semantic role features and pronominal 
ranking feature much improve the performance 
of pronoun resolution, especially when the de-
tailed pronominal subcategory features of both 
the anaphor and the antecedent candidate are 
included. It is not surprising due to the predomi-
994
 nance of pronouns in tracking the local discourse 
structure in a text.   
To our best knowledge, this paper is the first 
research which successfully applies the center-
ing-motivated features in pronoun resolution 
from the semantic perspective. 
For future work, we will explore more kinds 
of semantic information and structured syntactic 
information in pronoun resolution. In particular, 
we will further employ the centering theory in 
pronoun resolution from both grammatical and 
semantic perspectives on more corpora. 
Acknowledgement  
This research is supported by Project 60673041 
under the National Natural Science Foundation 
of  China, project 2006AA01Z147 under the  
?863? National High-Tech Research and Devel-
opment of China, project 200802850006 under 
the National Research Foundation for the Doc-
toral Program of Higher Education of China, 
project 08KJD520010 under the Natural Science 
Foundation of the Jiangsu Higher Education In-
stitutions of China. 
References 
C. Aone and W.W. Bennett. 1995. Evaluating auto-
mated and manual acquisition of anaphora resolu-
tion strategies. ACL?1995:122-129. 
S.E. Brennan, M.W. Friedman, and C.J. Pollard. 1987. 
A centering approach to pronoun. ACL?1987: 290-
297. 
S.E. Brennan 1995. Centering attention in discourse. 
Language and Cognitive Process, 10/2: 137-67. 
S. Buchholz, J. Veenstra and W. Daelemans. 1999. 
Cascaded Grammatical Relation Assignment. 
EMNLP-VLC?1999: 239-246 
S. Cote. 1983. Ranking and forward-looking centers. 
In Proceedings of the Workshop on the Centering 
Theory in Naturally-Occurring Discourse. 1993. 
D.M. Carter. 1987. Interpreting Anaphors in Natural 
Language Texts. Ellis Horwood, Chichester, UK. 
J. Carbonell and R. Brown. 1988. Anaphora resolu-
tion: a multi-strategy approach. COLING?1988: 
96-101. 
B.J. Grosz, A.K. JoShi and S. Weinstein. 1995. Cen-
tering: a framework for modeling the local coher-
ence of discourse. Computational Linguistics, 
21(2):203-225. 
P.C. Gordon, B.J. Grosz and L.A. Gilliom. 1993. 
Pronouns, names and the centering of attention in 
discourse. Cognitive Science.1993.17(3):311-348 
P.C. Gordon and K. A. Searce. 1995. Pronominaliza-
tion and discourse coherence, discourse structure 
and pronoun interpretation. Memory and Cogni-
tion. 1995. 
S. Harabagiu and S. Maiorano. 2000. Multiligual 
coreference resolution. ANLP-NAACL 2000:142-
149. 
A.K. Joshi and S. Weinstein. 1981. Control of infer-
ence: Role of some aspects of discourse structure-
centering. IJCAI?1981:385-387 
R. Kibble. 2001. A Reformulation of Rule 2 of Cen-
tering. Computational Linguistics, 2001,27(4): 
579-587 
M. Kameyama. 1986. Aproperty-sharing constraint in 
centering. ACL 1986:200-206 
M. Kameyama. 1988. Japanese zero pronominal 
binding, where syntax and discourse meet. In Pro-
ceeding of the Second International Workshop on 
Japanese Syntax. 1988.  
R. Mitkov. 1998. Robust pronoun resolution with 
limited knowledge. COLING-ACL?1998:869-875.  
A. Moschitti and S. Quarteroni. 2008. Kernels on 
linguistics structures for answer entraction. 
ACL?08:113-116 
V. Ng and C. Cardie. 2002. Improving machine 
learning approaches to coreference resolution. 
ACL?2002: 104-111 
V. Ng. 2007. Semantic Class Induction and Corefer-
ence Resolution.  ACL?2007 536-543. 
M. Palmer, D. Gildea and P. Kingsbury. 2005. The 
proposition bank: A corpus annotated with seman-
tic roles. Computational Linguistics, 31(1):71-106. 
S. Pradhan, K. Hacioglu, V. Krugler, W. Ward, J.H. 
Martin, and D. Jurafsky. 2005. Support vector 
learning for semantic argument classification. Ma-
chine Learning, 2005.60:11-39.  
S. P. Ponzetto and M. Strube. 2006. Semantic Role 
Labeling for Coreference Resolution. 
EMNLP?2006 143-146. 
E. F. Prince and M. A. Walker. 1995. A bilateral ap-
porach to givenness: a hearer-status algorithm and 
a centering algorithm. In Proceedings of 4th Inter-
national Pragmatics Conference. 
E. Rich and S. LuperFoy. 1988. An architecture for 
anaphora resolution. In Proceedings of the 2nd 
Conference on Applied Natural Language Proc-
essing. ANLP?1988: 18-24. 
W.M. Soon, H.T. Ng and D. Lim. 2001. A machine 
learning approach to coreference resolution of 
noun phrase. Computational Linguistics, 2001, 
27(4):521-544. 
D. Shen and M. Lapata. 2007. Using semantic roles 
to improve question answering. EMNLP-CoNIL 
2007:12-21 
C. Sidner. 1979. Toward a computation of intrasen-
tential coreference. Technical Report TR-537,MIT. 
Artificial Intelligence Laboratory. 
C. Sidner. 1981. Focusing for interpretation of pro-
nouns. Computational Linguistics,1981.7:217-231 
J. Tetreault. 1999. Analysis of syntax-based pronoun 
resolution methods. ACL 1999:602-605 
J. Tetreault. 2001. A corpus-based evaluation of cen-
tering and pronoun resolution. Computational Lin-
guistics. 2001. 27(4):507-520. 
995
 M. Walker, A. K. Joshi and E. Prince. 1998. Center-
ing in naturally occurring discourse: An overview. 
Clarendon Press:1-28 
X.F. Yang?J. Su?G.D. Zhou and C.L. Tan. 2004. 
Improving pronoun resolution by incorporating 
coreferential information of candidates. 
ACL?2004:127-134. 
X.F. Yang? J. Su and C.L. Tan. 2005. Improving 
Pronoun Resolution Using Statistics - Based Se-
mantic Compatibility Information. ACL?2005:165 
-172. 
X.F. Yang and J. Su. 2007. Coreference Resolution 
Using Semantic Relatedness Information from 
Automatically Discovered Patterns. ACL?2007: 
528-535. 
G.D. Zhou and J. Su. 2004. A high- performance 
coreference resolution system using a multi- agent 
strategy. COLING? 2004:522- 528. 
Y. Ziv and B.J. Grosz. 1994. Right dislocation and 
attentional state. Israel Association of Theoretical 
Linguistics Meetings?1994. 184-199. 
996
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1437?1445,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Semi-Supervised Learning for Semantic Relation Classification using 
Stratified Sampling Strategy 
 
Longhua Qian   Guodong Zhou   Fang Kong   Qiaoming Zhu 
Jiangsu Provincial Key Lab for Computer Information Processing Technology 
School of Computer Science and Technology, Soochow University 
1 Shizi Street, Suzhou, China 215006 
{qianlonghua,gdzhou,kongfang,qmzhu}@suda.edu.cn 
 
 
 
 
Abstract 
 
This paper presents a new approach to 
selecting the initial seed set using stratified 
sampling strategy in bootstrapping-based 
semi-supervised learning for semantic relation 
classification.  First, the training data is 
partitioned into several strata according to 
relation types/subtypes, then relation instances 
are randomly sampled from each stratum to 
form the initial seed set. We also investigate 
different augmentation strategies in iteratively 
adding reliable instances to the labeled set, and 
find that the bootstrapping procedure may stop 
at a reasonable point to significantly decrease 
the training time without degrading too much 
in performance. Experiments on the ACE 
RDC 2003 and 2004 corpora show the 
stratified sampling strategy contributes more 
than the bootstrapping procedure itself. This 
suggests that a proper sampling strategy is 
critical in semi-supervised learning. 
1 Introduction 
With the dramatic increase in the amount of 
textual information available in digital archives 
and the WWW, there has been growing interest 
in techniques for automatically extracting 
information from text documents. Information 
Extraction (IE) is such a technology that IE 
systems are expected to identify relevant 
information (usually of pre-defined types) from 
text documents in a certain domain and put them 
in a structured format. 
According to the scope of the NIST Automatic 
Content Extraction (ACE) program (ACE, 2000-
2007), current research in IE has three main 
objectives: Entity Detection and Tracking (EDT), 
Relation Detection and Characterization (RDC), 
and Event Detection and Characterization (EDC). 
This paper focuses on the ACE RDC subtask, 
where many machine learning methods have 
been proposed, including supervised methods 
(Miller et al, 2000; Zelenko et al, 2002; Culotta 
and Soresen, 2004; Kambhatla, 2004; Zhou et al, 
2005; Zhang et al, 2006; Qian et al, 2008), 
semi-supervised methods (Brin, 1998; Agichtein 
and Gravano, 2000; Zhang, 2004; Chen et al, 
2006; Zhou et al, 2008), and unsupervised 
methods (Hasegawa et al, 2004; Zhang et al, 
2005).  
Current work on semantic relation extraction 
task mainly uses supervised learning methods, 
since it achieves relatively better performance. 
However this method requires a large amount of 
manually labeled relation instances, which is 
both time-consuming and laborious. In the 
contrast, unsupervised methods do not need 
definitions of relation types and hand-tagged data, 
but it is difficult to evaluate their performance 
since there are no criteria for evaluation. 
Therefore, semi-supervised learning has received 
more and more attention, as it can balance the 
advantages and disadvantages between 
supervised and unsupervised methods. With the 
plenitude of unlabeled natural language data at 
hand, semi-supervised learning can significantly 
reduce the need for labeled data with only 
limited sacrifice in performance. Specifically, a 
bootstrapping algorithm chooses the unlabeled 
instances with the highest probability of being 
correctly labeled and use them to augment 
labeled training data iteratively.  
Although previous work (Yarowsky, 1995; 
Blum and Mitchell, 1998; Abney, 2000; Zhang, 
2004) has tackled the bootstrapping approach 
from both the theoretical and practical point of 
view, many key problems still remain unresolved, 
such as the selection of initial seed set. Since the 
size of the initial seed set is usually small (e.g. 
1437
100 instances), the imbalance of relation types or 
manifold structure (cluster structure) in it will 
severely weaken the strength of bootstrapping. 
Therefore, it is critical for a bootstrapping 
approach to select the most appropriate initial 
seed set. However, current systems (Zhang, 2004; 
Chen et al, 2006) use a randomly sampling 
strategy, which fails to explore the affinity nature 
among the training instances. Alternatively, 
Zhou et al (2008) bootstrap a set of weighted 
support vectors from both labeled and unlabeled 
data using SVM. Nevertheless, the initial labeled 
data is still randomly generated only to ensure 
that there are at least 5 instances for every 
relation subtype. 
This paper presents a new approach to 
selecting the initial seed set based on stratified 
sampling strategy in the bootstrapping procedure 
for semi-supervised semantic relation 
classification. The motivation behind the 
stratified sampling is that every relation type 
should be as much as possible represented in the 
initial seed set, thus leading to more instances 
with diverse structures being added to the labeled 
set. In addition, we also explore different 
strategies to augment reliably classified instances 
to the labeled data iteratively, and attempt to find 
a stoppage criterion for the iteration procedure to 
greatly decrease the training time, other than 
using up all the unlabeled set. 
The rest of this paper is organized as follows. 
First, Section 2 reviews related work on semi-
supervised relation extraction. Then we present 
an underlying supervised learner in Section 3. 
Section 4 details various key aspects of the 
bootstrapping procedure, including the stratified 
sampling strategy. Experimental results are 
reported in Section 5. Finally we conclude our 
work in Section 6. 
2 Related Work 
Within the realm of information extraction, 
currently there are several representative semi-
supervised learning systems for extracting 
relations between named entities. 
DIPRE (Dual Iterative Pattern Relation 
Expansion) (Brin, 1998) is a system based on 
bootstrapping that exploits the duality between 
patterns and relations to augment the target 
relation starting from a small sample. However, 
it only extracts simple relations such as (author, 
title) pairs from the WWW. Snowball (Agichtein 
and Gravano, 2000) is another bootstrapping-
based system that extracts relations from 
unstructured text. Snowball shares much in 
common with DIPRE, including the use of both 
the bootstrapping framework and the pattern 
matching approach to extract new unlabeled 
instances. Due to pattern matching techniques, 
their systems are hard to be adapted to the 
general problem of relation extraction. 
Zhang (2004) approaches the relation 
classification problem with bootstrapping on top 
of SVM. He uses various lexical and syntactic 
features in the BootProject algorithm based on 
random feature projection to extract top-level 
relation types in the ACE corpus. Evaluation 
shows that bootstrapping can alleviate the burden 
of hand annotations for supervised learning 
methods to a certain extent.  
Chen et al (2006) investigate a semi-
supervised learning algorithm based on label 
propagation for relation extraction, where labeled 
and unlabeled examples and their distances are 
represented as the nodes and the weights of 
edges respectively in a connected graph, then the 
label information is propagated from any vertex 
to nearby vertices through weighted edges 
iteratively, finally the labels of unlabeled 
examples are inferred after the propagation 
process converges.  
Zhou et al (2008) integrate the advantages of 
SVM bootstrapping in learning critical instances 
and label propagation in capturing the manifold 
structure in both the labeled and unlabeled data, 
by first bootstrapping a moderate number of 
weighted support vectors through a co-training 
procedure from all the available data, and then 
applying label propagation algorithm via the 
bootstrapped support vectors. 
However, in most current systems, the initial 
seed set is selected randomly such that they may 
not adequately represent the inherent structure of 
unseen examples, hence the power of 
bootstrapping may be severely weakened. 
This paper presents a simple yet effective 
approach to generate the initial seed set by 
applying the stratified sampling strategy, 
originated from statistics theory. Furthermore, 
we try to employ the same stratified strategy to 
augment the labeled set. Finally, we attempt to 
find a reasonable criterion to terminate the 
iteration process. 
3 Underlying Supervised Learning 
A semi-supervised learning system usually 
consists of two relevant components: an 
underlying supervised learner and a 
1438
bootstrapping algorithm on top of it. In this 
section we discuss the former, while the latter 
will be described in the following section.  
In this paper, we select Support Vector 
Machines (SVMs) as the underlying supervised 
classifier since it represents the state-of-the-art in 
the machine learning research community, and 
there are good implementations of the algorithm 
available. Specifically, we use LIBSVM (Chang 
et al, 2001), an effective tool for support vector 
classification, since it supports multi-class 
classification and provides probability estimation 
as well. 
For each pair of entity mentions, we extract 
and compute various lexical and syntactic 
features, as employed in a state-of-the-art 
relation extraction system (Zhou et al, 2005). 
(1) Words: According to their positions, four 
categories of words are considered: a) the words 
of both the mentions; b) the words between the 
two mentions; c) the words before M1; and d) 
the words after M2.  
(2) Entity type: This category of features 
concerns about the entity types of both the 
mentions. 
(3) Mention Level: This category of features 
considers the entity level of both the mentions. 
(4) Overlap: This category of features includes 
the number of other mentions and words between 
two mentions. Typically, the overlap features are 
usually combined with other features such as 
entity type and mention level. 
(5) Base phrase chunking: The base phrase 
chunking is proved to play an important role in 
semantic relation extraction. Most of the 
chunking features concern about the headwords 
of the phrases between the two mentions.  
In this paper, we do not employ any deep 
syntactic or semantic features (such as 
dependency tree, full parse tree etc.), since they 
contribute quite limited in relation extraction. 
4 Bootstrapping & Stratified Sampling 
We first present the self-bootstrapping algorithm, 
and then discuss several key problems on 
bootstrapping in the order of initial seed 
selection, augmentation of labeled data and 
stoppage criterion for iteration. 
4.1 Bootstrapping Algorithm 
Following Zhang (2004), we define a basic self-
bootstrapping strategy, which keeps augmenting 
the labeled data set with the models 
straightforwardly trained from previously 
available labeled data as follows: 
Require: labeled seed set L
Require: unlabeled data set U
Require: batch size S
Repeat
    Train a single classifier on L
    Run the classifier on U
    Find at most S instances in U that the classifier has
the highest prediction confidence
    Add them into L
Until: no data points available or the stoppage
condition is reached
Algorithm self-bootstrapping
Figure 1. Self-bootstrapping algorithm 
In order to measure the confidence of the 
classifier?s prediction, we compute the entropy 
of the label probability distribution that the 
classifier assigns to the class label on an example 
(the lower the entropy, the higher the confidence): 
log
n
i i
i
H p p= ??      (1) 
Where n denotes the total number of relation 
classes, and pi denotes the probability of current 
example being classified as the ith class.  
4.2 Stratified Sampling for Initial Seeds  
Normally, the number of available labeled 
instances is quite limited (usually less than 100 
instances) when the iterative bootstrapping 
procedure begins. If the distribution of the initial 
seed set fails to approximate the distribution of 
the test data, the augmented data generated from 
bootstrapping would not capture the essence of 
relation types, and the performance on the test 
set will significantly decrease even only after one 
or two rounds of iterations. Therefore, the 
selection of initial seed set plays an important 
role in bootstrapping-based semantic relation 
extraction. 
Sampling is a part of statistical practice 
concerned with the selection of individual 
observations, which is intended to yield some 
knowledge about a population of interest. When 
dealing with the task of semi-supervised 
semantic relation classification, the population is 
the training set of relation instances from the 
ACE RDC corpora. We compare two practical 
sampling strategies as follows: 
(1) Randomly sampling, which picks the initial 
seeds from the training data using a random 
scheme. Each element thus has an equal 
probability of selection, and the population is not 
1439
subdivided or partitioned. Currently, most work 
on semi-supervised relation extraction employs 
this method. However, since the size of the initial 
seed set is very small, they are not guaranteed to 
capture the statistical properties of the whole 
training data, let alne of the test data. 
(2) Stratified sampling. When the population 
embraces a number of distinct categories, 
stratified sampling (Neyman, 1934) can be 
applied to this case. First, the population can be 
organized by these categories into separate 
"strata", then a sample is selected within each 
"stratum" separately, and randomly. Generally, 
the sample size is normally proportional to the 
relative size of the strata. The main motivation 
for using a stratified sampling design is to ensure 
that particular groups within a population are 
adequately represented in the sample. 
It is well known that the number of the 
instances for each relation type in the ACE RDC 
corpora is greatly unbalanced  (Zhou et al, 2005) 
as shown in Table 1 for the ACE RDC 2004 
corpus. When the relation instances for a specific 
relation type occurs frequently in the initial seed 
set, the classifier will achieve good performance 
on this type, otherwise the classifier can hardly 
recognize them from the test set. In order for 
every type of relations to be properly represented, 
the stratified sampling strategy is applied to the 
seed selection procedure. 
Types Subtypes Train Test
Located 593 145
Near 70 17
PHYS 
Part-Whole 299 79
Business 134 39
Family 101 20
PER-SOC 
Other 44 11
Employ-Executive 388 101
Employ-Staff 427 112
Employ-Undetermined 66 12
Member-of-Group 152 39
Subsidiary 169 37
Partner 10 2
EMP-ORG 
Other 64 16
User-or-Owner 160 40
Inventor-or-Man. 8 1
ART 
Other 1 1
Ethnic 31 8
Ideology 39 9
OTHER-
AFF 
Other 43 11
Citizen-or-Resid. 226 47
Based-In 165 50
GPE-AFF 
Other 31 8
DISC  224 55
Total  3445 860
Table 1. Numbers of relations on the ACE RDC 
2004: break down by relation types and subtypes 
Figure 2 illustrates the stratified sampling 
strategy we use in bootstrapping, where RSET 
denotes the training set, V is the stratification 
variable, and SeedSET denotes the initial seed set. 
First, we divide the relation instances into 
different strata according to available properties, 
such as major relation type (considering reverse 
relations or not) and relation subtype 
(considering reverse relations or not). Then 
within every stratum, a certain number of 
instances are sampled randomly, and this number 
is normally proportional to the size of that 
stratum in the whole population. However, when 
this number is 0 due to the rounding of real 
numbers, it is set to 1. Also it must be ensured 
that the total number of instances being sampled 
is NS. Finally, these instances form the initial 
seed set and can be used as the input to the 
underlying supervised learning for the 
bootstrapping procedure. 
 
Require: RSET ={R1,R2,?,RN} 
Require: V = {v1, v2,?,vK} 
Require: SeedSET with the size of NS (100) 
Initialization: 
SeedSET = NULL 
Steps: 
z Group RSET into K strata according to the 
stratified variable V, i.e.:  
RSET={RSET1,RSET2,?,RSETK} 
z Calculate the class prior probability for each 
stratum i={1,2,?,K} 
)(/)( RSETNUMRSETNUMP ii =  
z Caculate the number of intances being sampled 
for each stratum 
NPN ii ?=  
If Ni =0 then Ni=1 
z Calculate the difference of numbers as follows: 
?
=
? ?=
K
i
iS NNN
1
 
z If N?>0 then add Ni (i=1,2,?,|N?|) by 1 
If N?<0 then subtract 1 from Ni (i=1,2,...,|N?|) 
z For each i from 1 to K 
Select Ni instances from RESTi randomly 
Add them into SeedSET 
 
Figure 2. Stratefied Sampling for initial seeds 
4.3 Augmentation of labeled data 
After each round of iteration, some newly 
classified instances with the highest confidence 
can be augmented to the labeled training data. 
Nevertheless, just like the selection of initial seed 
set, we still wish that every stratum would be 
represented as appropriately as possible in the 
1440
instances added to the labeled set. In this paper, 
we compare two kinds of augmentation strategies 
available: 
(1) Top n method: the classified instances are 
first sorted in the ascending order by their 
entropies (i.e. decreasing confidence), and then 
the top n (usually 100) instances are chosen to be 
added.  
(2) Stratified method: in order to make the 
added instances representative for their stratum, 
we first select m (usually greater than n) 
instances with the highest confidence, then we 
choose n instances from them using the stratified 
strategy. 
4.4 Stoppage of Iterations 
In a self-bootstrapping procedure, as the 
iterations go on, both the reliable and unreliable 
instances are added to the labeled data 
continuously, hence the performance will 
fluctuate in a relatively small range. The key 
question here is how we can know when the 
bootstrapping procedure reaches its best 
performance on the test data. The bootstrapping 
algorithm by Zhang (2004) stops after it runs out 
of all the training instances, which may take a 
relatively long time. In this paper, we present a 
method to determine the stoppage criterion based 
on the mean entropy as follows: 
Hi <= p    (2) 
Where Hi denotes the mean entropy of the 
confidently classified instances being augmented 
to the labeled data in each iteration, and p 
denotes a threshold for the mean entropy, which 
will be fixed through empirical experiments. 
This criterion is based on the assumption that 
when the mean entropy becomes less than or 
equal to a certain threshold, the classifier would 
achieve the most reliable confidence on the 
instances being added to the labeled set, and it 
may be impossible to yield better performance 
since then. Therefore, the iteration may stop at 
that reasonable point.  
5 Experimentation 
This section aims to empirically investigate the 
effectiveness of the bootstrapping-based semi-
supervised learning we discussed above for 
semantic relation classification. In particular, 
different methods for selecting the initial seed set 
and augmenting the labeled data are evaluated. 
5.1 Experimental Setting 
We use the ACE corpora as the benchmark data, 
which are gathered from various newspapers, 
newswire and broadcasts. The ACE 2004 corpus 
contains 451 documents and 5702 positive 
relation instances. It defines 7 relation types and 
23 subtypes between 7 entity types. For easy 
reference with related work in the literature, 
evaluation is also done on 347 documents 
(including nwire and bnews domains) and 4305 
relation instances using 5-fold cross-validation. 
That is, these relation instances are first divided 
into 5 sets, then, one of them (about 860 
instances) is used as the test data set, while the 
others are regarded as the training data set, from 
which the initial seed set is sampled. In the ACE 
2003 corpus, the training set consists of 674 
documents and 9683 positive relation instances 
while the test data consists of 97 documents and 
1386 positive relation instances. The ACE RDC 
2003 task defines 5 relation types and 24 
subtypes between 5 entity types. 
The corpora are first parsed using Collins?s 
parser (Collins, 2003) with the boundaries of all 
the entity mentions kept. Then, the parse trees 
are converted into chunklink format using 
chunklink.pl 1. Finally, various useful lexical and 
syntactic features, as described in Subsection 3.1, 
are extracted and computed accordingly. For the 
purpose of comparison, we define our task as the 
classification of the 5 or 7 major relation types in 
the ACE RDC 2003 and 2004 corpora. 
For LIBSVM parameters, we adopted the 
polynomial kernel, and c is set to 10, g is set to 
0.15. Under this setting, we achieved the best 
classification performance. 
5.2 Experimental Results 
In this subsection, we compare and discuss the 
experimental results using various sampling 
strategies, different augmentation methods, and 
iteration stoppage criterion. 
 
Comparison of sampling strategies in selecting 
the initial seed set 
Table 2 and Table 3 show the initial and the 
highest classification performance of 
Precision/Recall/F-measure for various sampling 
strategies of the initial seed set on 7 major 
relation types of the ACE RDC 2004 corpus 
respectively when the size of initial seed set L is 
100, the batch size S is 100, and the top 100 
                                                 
1 http://ilk.kub.nl/~sabine/chunklink/ 
1441
instances with the highest confidence are added 
at each iteration. Table 2 also lists the number of 
strata for stratified sampling methods from which 
the initial seeds are randomly chosen 
respectively. Table 3 additionally lists the time 
needed to complete the bootstrapping process (on 
a PC with a Pentium IV 3.0G CPU and 1G 
memory). In this paper, we consider the 
following five experimental settings when 
sampling the initial seeds: 
z Randomly Sampling: as described in 
Subsection 4.2. 
z Stratified-M Sampling: the strata are 
grouped in terms of major relation types 
without considering reverse relations. 
z Stratified-MR Sampling: the strata are 
grouped in terms of major relation types, 
including reverse relations. 
z Stratified-S Sampling: the strata are 
grouped in terms of relation subtypes 
without considering reverse subtypes. 
z Stratified-SR Sampling: the strata are 
grouped in terms of relation subtypes, 
including reverse subtypes. 
For each sampling strategies, we performed 20 
trials and computed average scores and the total 
time on the test set over these 20 trials. 
Sampling strategies 
for initial seeds 
# of 
strat. P(%) R(%) F 
Randomly 1 66.1 65.9 65.9
Stratified-M 7 69.1 66.5 67.7
Stratified-MR 13 69.3 67.3 68.2
Stratified-S 30 69.8 67.7 68.7
Stratified-SR 39 69.9 68.5 69.2
Table 2. The initial performance of applying 
various sampling strategies to selecting the initial 
seed set on the ACE RDC 2004 corpus 
Sampling strategies 
for initial seeds 
Time 
(min) P(%) R(%) F 
Randomly 52 68.6 66.2 67.3
Stratified-M 65 71.0 66.9 68.8
Stratified-MR 65 71.6 67.0 69.2
Stratified-S 71 72.7 67.8 70.1
Stratified-SR 77 72.9 68.4 70.6
Table 3. The highest performance of applying 
various sampling strategies in selecting the initial 
seed set on the ACE RDC 2004 corpus 
 
These two tables jointly indicate that the self-
bootstrapping procedure for all sampling 
strategies can moderately improve the 
classification performance by ~1.2 units in F-
score, which is also verified by Zhang (2004). 
Furthermore, they show that: 
z The most improvements in performance 
come from improvements in precision. Actually, 
for some settings the recalls even decrease 
slightly. The reason may be that due to the nature 
of self-bootstrapping, the instances augmented at 
each iteration are always those which are the 
most similar to the initial seed instances, 
therefore the models trained from them would 
exhibit higher precision on the test set, while it 
virtually does no help for recall. 
z All of the four stratified sampling methods 
outperform the randomly sampling method to 
various degrees, both in the initial performance 
and the highest performance. This means that 
sampling of the initial seed set based on 
stratification by major/sub relation types can be 
helpful to relation classification, largely due to 
the performance improvement of the initial seed 
set, which is caused by adequate representation 
of instances for every relation type. 
z Of all the four stratified sampling methods, 
the Stratified-SR sampling achieves the best 
performance of 72.9/68.4/70.6 in P/R/F. 
Moreover, the more the number of strata 
generated by the sampling strategy, the more 
appropriately they would be represented in the 
initial seed set, and the better performance it will 
yield. This also implies that the hierarchy of 
relation types/subtypes in the ACE RDC 2004 
corpus is fairly reasonably defined. 
z An important conclusion, which can be 
draw accordingly, is that the F-score 
improvement of Stratified-SR sampling over 
Randomly sampling in initial performance (3.3 
units) is significantly greater than the F-score 
improvement gained by bootstrapping itself 
using Randomly sampling (1.4 units). This means 
that the sampling strategy of the initial seed set is 
even more important than the bootstrapping 
algorithm itself for relation classification. 
z It is interesting to note that the time needed 
to bootstrap increases with the number of strata. 
The reason may be that due to more diverse 
structures in the labeled data for stratified 
sampling, the SVM needs more time to 
differentiate between instances, i.e. more time to 
learn the models. 
 
Comparison of different augmentation 
strategies of training data 
Figure 3 compares the performance of F-score 
for two augmentation strategies: the Top n 
method and the stratified method, over various 
initial seed sampling strategies on the ACE RDC 
2004 corpus. For each iteration, a variable 
1442
number (m is ranged from 100 to 500) of 
classified instances in the decreasing order of 
confidence are first chosen as the base examples, 
then at most 100 examples are selected from the 
base examples to be augmented to the labeled set. 
Specifically, when m is equal to 100, the whole 
set of the base example is added to the labeled 
data, i.e. degenerated to the Top n augmentation 
strategy. On the other hand, when m is greater 
than 100, we wish we would select examples of 
different major relation types from the base 
examples according to their distribution in the 
training set, in order to achieve the performance 
improvement as much as the stratified sampling 
does in the selection of the initial seed set. 
64
65
66
67
68
69
70
71
72
100 200 300 400 500
# Base examples
F-
sc
or
e
Randomly
Stratified-M 
Stratified-MR
Stratified-S
Stratified-SR
Figure 3. Comparison of two augmentation 
strategies over different sampling strategies in 
selecting the initial seed set. 
This figure shows that, except for randomly 
sampling strategy, the stratified augmentation 
strategies improve the performance. Nevertheless, 
this result is far from our expectation in two 
ways: 
z The performance improvement in F-score is 
trivial, at most 0.4 units on average. The reason 
may be that, although we try to add as many as 
100 classified instances to the labeled data 
according to the distribution of every major 
relation type in the training set, the top m 
instances with the highest confidence are usually 
focused on certain relation types (e.g. PHSY and 
PER-SOC), this leads to the stratified 
augmentation failing to function effectively. 
Hence, all the following experiments will only 
adopt Top n method for augmenting the labeled 
data. 
z With the increase of the number of the base 
examples, the performance fluctuates slightly, 
thus it is relatively difficult to recognize where 
the optima is. We think there are two 
contradictory factors that affect the performance. 
While the reliability of the instances extracted 
from the base examples decreases with the 
increase of the number of base examples, the 
probability of extracting instances of more 
relation types increases with the increase of the 
number of the base examples. These two factors 
inversely interact with each other, leading to the 
fluctuation in performance. 
 
Comparison of different threshold values for 
stoppage criterion 
We compare the performance and 
bootstrapping time (20 trials with the same initial 
seed set) when applying stoppage criterion in 
Formula (2) with different threshold p over 
various sampling strategies on the ACE RDC 
2004 corpus in Figure 4 and Figure 5 
respectively. These two figures jointly show that: 
64
65
66
67
68
69
70
71
0 0.2 0.22 0.24 0.26 0.28 0.3
p
F-
sc
or
e
Randomly
Stratified-M
Stratified-MR
Stratified-S
Stratified-SR
Figure 4. Performance for different p values 
0
10
20
30
40
50
60
70
80
90
0 0.2 0.22 0.24 0.26 0.28 0.3
p
Ti
m
e(
mi
n)
Randomly
Stratified-M
Stratified-MR
Stratified-S
Stratified-SR
Figure 5. Bootstrapping time for different p 
values 
z The performance decreases slowly while the 
bootstrapping time decreases dramatically with 
the increase of p from 0 to 0.3. Specifically, 
when the p equals to 0.3, the bootstrapping time 
tends to be neglected, while the performance is 
almost similar to the initial performance. It 
implies that we can find a reasonable point for 
each sampling strategy, at which the time falls 
greatly while the performance nearly does not 
degrade.  
1443
Bootproject LP-js Stratified Bootstrapping Relation types 
P R F P R F P R F 
ROLE 78.5 69.7 73.8 81.0 74.7 77.7 74.7 86.3 80.1
PART 65.6 34.1 44.9 70.1 41.6 52.2 66.4 47.0 55.0
AT 61.0 84.8 70.9 74.2 79.1 76.6 74.9 66.1 70.2
NEAR - - - 13.7 12.5 13.0 100.0 2.9 5.6
SOC 47.0 57.4 51.7 45.0 59.1 51.0 65.2 79.0 71.4
Average 67.9 67.4 67.6 73.6 69.4 70.9 73.8 73.3 73.5
Table 4. Comparison of semi-supervised relation classification systems on the ACE RDC 2003 corpus 
 
z Clearly, if the performance is the primary 
concern, then p=0.2 may be the best choice in 
that we can get ~30% saving on the time at the 
cost of only ~0.08 loss in F-score on average. If 
the time is a primary concern, then p=0.22 is a 
reasonable threshold in that we get ~50% saving 
on the time at the cost of ~0.25 units loss in F-
score on average. This suggests that our 
proposed stoppage criterion is effective to 
terminate the bootstrapping procedure with 
minor performance loss. 
 
Comparison of Stratified Bootstrapping with 
Bootproject and Label propagation  
Table 4 compares Bootproject (Zhang, 2004), 
Label propagation (Chen et al, 2006) with our 
Stratified Bootstrapping on the 5 major types of 
the ACE RDC 2003 corpus. 
Both Bootproject and Label propagation 
select 100 initial instances randomly, and at each 
iteration, the top 100 instances with the highest 
confidence are added to the labeled data. 
Differently, we choose 100 initial seeds using 
stratified sampling strategy; similarly, the top 
100 instances with the highest confidence are 
augmented to the labeled data at each iteration. 
Due to the lack of comparability followed from 
the different size of the labeled data used in 
(Zhou et al, 2008), we omit their results here. 
This table shows that our stratified 
bootstrapping procedure significantly 
outperforms both Bootproject and Label 
Propagation methods on the ACE RDC corpus, 
with the increase of 5.9/4.1 units in F-score on 
average respectively. Stratified bootstrapping 
consistently outperforms Bootproject in every 
major relation type, while it outperforms Label 
Propagation in three of the major relation types, 
especially SOC type, with the exception of AT 
and NEAR types. The reasons may be follows. 
Although there are many AT relation instances in 
the corpus, they are scattered divergently in 
multi-dimension space so that they tend to be 
relatively difficult to be recognized via SVM. 
For the NEAR relation instances, they occur least 
frequently in the whole corpus, so it is very hard 
for them to be identified via SVM. By contrast, 
even small size of labeled instances can be fully 
utilized to correctly induce the unlabeled 
instances via LP algorithm due to its ability to 
exploit manifold structures of both labeled and 
unlabeled instances (Chen et al, 2006). 
In general, these results again suggest that the 
sampling strategy in selecting the initial seed set 
plays a critical role for relation classification, and 
stratified sampling can significantly improve the 
performance due to proper selection of the initial 
seed set. 
6 Conclusion 
This paper explores several key issues in semi-
supervised learning based on bootstrapping for 
semantic relation classification. The application 
of stratified sampling originated from statistics 
theory to the selection of the initial seed set 
contributes most to the performance 
improvement in the bootstrapping procedure. In 
addition, the more strata the training data is 
divided into, the better performance will be 
achieved. However, the augmentation of the 
labeled data using the stratified strategy fails to 
function effectively largely due to the 
unbalanced distribution of the confidently 
classified instances, rather than the stratified 
sampling strategy itself. Furthermore, we also 
propose a mean entropy-based stoppage criterion 
in the bootstrapping procedure, which can 
significantly decrease the training time with little 
loss in performance. Finally, it also shows that 
our method outperforms other state-of-the-art 
semi-supervised ones. 
 
Acknowledgments 
This research is supported by Project 60673041 
and 60873150 under the National Natural 
Science Foundation of China, Project 
2006AA01Z147 under the ?863? National High-
Tech Research and Development of China, 
1444
Project BK2008160 under the Jiangsu Natural 
Science Foundation of China, and the National 
Research Foundation for the Doctoral Program 
of Higher Education of China under Grant No. 
20060285008. We would also like to thank the 
excellent and insightful comments from the three 
anonymous reviewers. 
References  
S. Abney. Bootstrapping. 2002. In Proceedings of the 
40th Annual Meeting of the Association for 
Computational  Linguistics (ACL 2002). 
ACE 2002-2007. The Automatic Content Extraction 
(ACE) Projects. 2007. http//www.ldc.upenn.edu/ 
Projects/ACE/. 
E. Agichtein and L. Gravano. 2000. Snowball: 
Extracting relations from large plain-text 
collections. In Proceedings of the 5th ACM 
international Conference on Digital Libraries 
(ACMDL 2000). 
A. Blum and T. Mitchell. 1996. Combining labeled 
and unlabeled data with co-training. In COLT: 
Proceedings of the workshop on Computational 
Learning Theory. Morgan Kaufmann Publishers. 
S. Brin. 1998. Extracting patterns and relations from 
the world wide web. In WebDB Workshop at 6th 
International Conference on Extending Database 
Technology (EDBT 98). 
C.C. Chang and C.J. Lin. 2001. LIBSVM: a library 
for support vector machines. http:// 
www.csie.ntu.edu.tw/~cjlin/libsvm. 
M. Collins. 2003. Head-Driven Statistics Models for 
Natural Language Parsing. Computational 
linguistics, 29(4): 589-617. 
J.X. Chen, D.H. Ji, and L.T. Chew. 2006. Relation 
Extraction using Label Propagation Based Semi 
supervised Learning. In Proceedings of the 21st 
International Conference on Computational 
Linguistics and the 44th Annual Meeting of the 
Association of Computational Linguistics 
(COLING/ACL 2006), pages 129-136. July 2006, 
Sydney, Australia.  
A. Culotta and J. Sorensen. 2004. Dependency tree 
kernels for relation extraction. In Proceedings of 
the 42nd Annual Meeting of the Association of 
Computational Linguistics (ACL 2004), pages 423-
439. 21-26 July 2004, Barcelona, Spain. 
T. Hasegawa, S. Sekine, and R. Grishman. 2004. 
Discovering Relations among Named Entities from 
Large Corpora. In Proceedings of the 42nd Annual 
Meeting of the Association of Computational 
Linguistics (ACL 2004). 21-26 July 2004, 
Barcelona, Spain. 
N. Kambhatla. Combining lexical, syntactic and 
semantic features with Maximum Entropy models 
for extracting relations. In Proceedings of the 42nd 
Annual Meeting of the Association of 
Computational Linguistics (ACL 2004)(posters), 
pages 178-181. 21-26 July 2004, Barcelona, Spain. 
S. Miller, H. Fox, L. Ramshaw, and R. Weischedel. 
2000. A novel use of statistical parsing to extract 
information from text. In Proceedings of the 6th 
Applied Natural Language Processing Conference. 
29 April-4 May 2000, Seattle, USA. 
J. Neyman. 1934. On the Two Different Aspects of 
the Representative Method: The Method of 
Stratified Sampling and the Method of Purposive 
Selection. Journal of the Royal Statistical Society, 
97(4): 558-625. 
L.H. Qian, G.D. Zhou, Q.M. Zhu, and P.D Qian. 2008. 
Exploiting constituent dependencies for tree 
kernel-based semantic relation extraction. In 
Proceedings of The 22nd International Conference 
on Computational Linguistics (COLING 2008), 
pages 697-704. 18-22 August 2008, Manchester, 
UK. 
D. Yarowsky. 1995. Unsupervised word sense 
disambiguation rivaling supervised methods. In the 
Proceedings of the 33rd Annual Meeting of the 
Association for Computational Linguistics (ACL 
95), pages 189-196. 26-30 June 1995, MIT, 
Cambridge, Massachusetts, USA. 
D. Zelenko, C. Aone, and A. Richardella. 2003. 
Kernel Methods for Relation Extraction. Journal of 
Machine Learning Research, (2): 1083-1106. 
M. Zhang, J. Zhang, J. Su, and G.D. Zhou. 2006. A 
Composite Kernel to Extract Relations between 
Entities with both Flat and Structured Features. In 
Proceedings of the 21st International Conference 
on Computational Linguistics and the 44th Annual 
Meeting of the Association of Computational 
Linguistics (COLING/ACL 2006), pages 825-832. 
Sydney, Australia. 
M. Zhang, J. Su, D. M. Wang, G. D. Zhou, and C. L. 
Tan. 2005. Discovering Relations between Named 
Entities from a Large Raw Corpus Using Tree 
Similarity-Based Clustering. In Proceedings of the 
2nd international Joint Conference on Natural 
Language Processing (IJCNLP-2005), pages 378-
389. Jeju Island, Korea.  
Z. Zhang. 2004. Weakly-supervised relation 
classification for Information Extraction. In 
Proceedings of ACM 13th conference on 
Information and Knowledge Management (CIKM 
2004). 8-13 Nov 2004, Washington D.C., USA. 
G.D. Zhou, J. Su, J. Zhang, and M. Zhang. 2005. 
Exploring various knowledge in relation extraction. 
In Proceedings of the 43rd Annual Meeting of the 
Association of Computational Linguistics (ACL 
2005), pages 427-434. Ann Arbor, USA. 
G.D. Zhou, J.H. Li, L.H. Qian, and Q.M. Zhu. 2008. 
Semi-Supervised Learning for Relation Extraction. 
In Proceedings of the 3rd International Joint 
Conference on Natural Language Processing 
(IJCNLP-2008), page 32-38. 7-12 January 2008, 
Hyderabad, India. 
 
1445
Context-Sensitive Convolution Tree Kernel 
for Pronoun Resolution 
 
 
ZHOU GuoDong    KONG Fang    ZHU Qiaoming 
JiangSu Provincial Key Lab for Computer Information Processing Technology 
School of Computer Science and Technology 
Soochow Univ.  Suzhou, China 215006 
Email: {gdzhou, kongfang, qmzhu}@suda.edu.cn 
 
 
Abstract 
This paper proposes a context-sensitive convo-
lution tree kernel for pronoun resolution. It re-
solves two critical problems in previous 
researches in two ways. First, given a parse 
tree and a pair of an anaphor and an antecedent 
candidate, it implements a dynamic-expansion 
scheme to automatically determine a proper 
tree span for pronoun resolution by taking 
predicate- and antecedent competitor-related 
information into consideration. Second, it ap-
plies a context-sensitive convolution tree ker-
nel, which enumerates both context-free and 
context-sensitive sub-trees by considering their 
ancestor node paths as their contexts. Evalua-
tion on the ACE 2003 corpus shows that our 
dynamic-expansion tree span scheme can well 
cover necessary structured information in the 
parse tree for pronoun resolution and the con-
text-sensitive tree kernel much outperforms 
previous tree kernels.  
1 Introduction 
It is well known that syntactic structured informa-
tion plays a critical role in many critical NLP ap-
plications, such as parsing, semantic role labeling, 
semantic relation extraction and co-reference reso-
lution. However, it is still an open question on 
what kinds of syntactic structured information are 
effective and how to well incorporate such struc-
tured information in these applications. 
Much research work has been done in this direc-
tion. Prior researches apply feature-based methods 
to select and define a set of flat features, which can 
be mined from the parse trees, to represent particu-
lar structured information in the parse tree, such as 
the grammatical role (e.g. subject or object), ac-
cording to the particular application. Indeed, such 
feature-based methods have been widely applied in 
parsing (Collins 1999; Charniak 2001), semantic 
role labeling (Pradhan et al2005), semantic rela-
tion extraction (Zhou et al2005) and co-reference 
resolution  (Lapin and Leass 1994; Aone and Ben-
nett 1995; Mitkov 1998; Yang et al2004; Luo and 
Zitouni 2005; Bergsma and Lin 2006). The major 
problem with feature-based methods on exploring 
structured information is that they may fail to well 
capture complex structured information, which is 
critical for further performance improvement.  
The current trend is to explore kernel-based 
methods (Haussler, 1999) which can implicitly 
explore features in a high dimensional space by 
employing a kernel to calculate the similarity be-
tween two objects directly. In particular, the ker-
nel-based methods could be very effective at 
reducing the burden of feature engineering for 
structured objects in NLP, e.g. the parse tree struc-
ture in coreference resolution. During recent years, 
various tree kernels, such as the convolution tree 
kernel (Collins and Duffy 2001), the shallow parse 
tree kernel (Zelenko et al2003) and the depend-
ency tree kernel (Culota and Sorensen 2004), have 
been proposed in the literature. Among previous 
tree kernels, the convolution tree kernel represents 
the state-of-the-art and have been successfully ap-
plied by Collins and Duffy (2002) on parsing, Mo-
schitti (2004) on semantic role labeling, Zhang et 
al (2006) on semantic relation extraction  and Yang 
et al(2006) on pronoun resolution.  
However, there exist two problems in Collins 
and Duffy?s kernel. The first is that the sub-trees 
enumerated in the tree kernel are context-free. That 
is, each sub-tree enumerated in the tree kernel does 
not consider the context information outside the 
sub-tree. The second is how to decide a proper tree 
span in the tree kernel computation according to 
the particular application. To resolve above two 
problems, this paper proposes a new tree span 
scheme and applies a new tree kernel and to better 
capture syntactic structured information in pronoun 
25
resolution, whose task is to find the corresponding 
antecedent for a given pronominal anaphor in text. 
The rest of this paper is organized as follows. In 
Section 2, we review related work on exploring 
syntactic structured information in pronoun resolu-
tion and their comparison with our method. Section 
3 first presents a dynamic-expansion tree span 
scheme by automatically expanding the shortest 
path to include necessary structured information, 
such as predicate- and antecedent competitor-
related information. Then it presents a context-
sensitive convolution tree kernel, which not only 
enumerates context-free sub-trees but also context-
sensitive sub-trees by considering their ancestor 
node paths as their contexts. Section 4 shows the 
experimental results. Finally, we conclude our 
work in Section 5.  
2 Related Work 
Related work on exploring syntactic structured 
information in pronoun resolution can be typically 
classified into three categories: parse tree-based 
search algorithms (Hobbs 1978), feature-based  
(Lappin and Leass 1994; Bergsma and Lin 2006) 
and tree kernel-based methods (Yang et al2006).  
As a representative for parse tree-based search 
algorithms, Hobbs (1978) found the antecedent for 
a given pronoun by searching the parse trees of 
current text. It processes one sentence at a time 
from current sentence to the first sentence in text 
until an antecedent is found. For each sentence, it 
searches the corresponding parse tree in a left-to-
right breadth-first way. The first antecedent candi-
date, which satisfies hard constraints (such as gen-
der and number agreement), would be returned as 
the antecedent. Since the search is completely done 
on the parse trees, one problem with the parse tree-
based search algorithms is that the performance 
would heavily rely on the accuracy of the parse 
trees. Another problem is that such algorithms are 
not good enough to capture necessary structured 
information for pronoun resolution. There is still a 
big performance gap even on correct parse trees. 
Similar to other NLP applications, feature-
based methods have been widely applied in pro-
noun resolution to explore syntactic structured in-
formation from the parse trees. Lappin and Leass 
(1994) derived a set of salience measures (e.g. sub-
ject, object or accusative emphasis) with manually 
assigned weights from the syntactic structure out-
put by McCord?s Slot Grammar parser. The candi-
date with the highest salience score would be 
selected as the antecedent. Bergsma and Lin (2006) 
presented an approach to pronoun resolution based 
on syntactic paths. Through a simple bootstrapping 
procedure, highly co-reference paths can be 
learned reliably to handle previously challenging 
instances and robustly address traditional syntactic 
co-reference constraints. Although feature-based 
methods dominate on exploring syntactic struc-
tured information in the literature of pronoun reso-
lution, there still exist two problems with them. 
One problem is that the structured features have to 
be selected and defined manually, usually by lin-
guistic intuition. Another problem is that they may 
fail to effectively capture complex structured parse 
tree information. 
As for tree kernel-based methods, Yang et al
(2006) captured syntactic structured information 
for pronoun resolution by using the convolution 
tree kernel (Collins and Duffy 2001) to measure 
the common sub-trees enumerated from the parse 
trees and achieved quite success on the ACE 2003 
corpus. They also explored different tree span 
schemes and found that the simple-expansion 
scheme performed best. One problem with their 
method is that the sub-trees enumerated in Collins 
and Duffy?s kernel computation are context-free, 
that is, they do not consider the information out-
side the sub-trees. As a result, their ability of ex-
ploring syntactic structured information is much 
limited. Another problem is that, among the three 
explored schemes, there exists no obvious over-
whelming one, which can well cover syntactic 
structured information.  
The above discussion suggests that structured 
information in the parse trees may not be well util-
ized in the previous researches, regardless of fea-
ture-based or tree kernel-based methods. This 
paper follows tree kernel-based methods. Com-
pared with Collins and Duffy?s kernel and its ap-
plication in pronoun resolution (Yang et al2006), 
the context-sensitive convolution tree kernel enu-
merates not only context-free sub-trees but also 
context-sensitive sub-trees by taking their ancestor 
node paths into consideration. Moreover, this paper 
also implements a dynamic-expansion tree span 
scheme by taking predicate- and antecedent com-
petitor-related information into consideration. 
26
3 Context Sensitive Convolution Tree 
Kernel for Pronoun Resolution 
In this section, we first propose an algorithm to 
dynamically determine a proper tree span for pro-
noun resolution and then present a context-
sensitive convolution tree kernel to compute simi-
larity between two tree spans. In this paper, all the 
texts are parsed using the Charniak parser 
(Charniak 2001) based on which the tree span is 
determined. 
3.1 Dynamic-Expansion Tree Span Scheme 
Normally, parsing is done on the sentence level. To 
deal with the cases that an anaphor and an antece-
dent candidate do not occur in the same sentence, 
we construct a pseudo parse tree for an entire text 
by attaching the parse trees of all its sentences to 
an upper ?S? node, similar to Yang et al(2006). 
Given the parse tree of a text, the problem is 
how to choose a proper tree span to well cover syn-
tactic structured information in the tree kernel 
computation. Generally, the more a tree span in-
cludes, the more syntactic structured information 
would be provided, at the expense of more noisy 
information. Figure 2 shows the three tree span 
schemes explored in Yang et al(2006): Min-
Expansion (only including the shortest path con-
necting the anaphor and the antecedent candidate), 
Simple-Expansion (containing not only all the 
nodes in Min-Expansion but also the first level 
children of these nodes) and Full-Expansion (cov-
ering the sub-tree between the anaphor and the 
candidate), such as the sub-trees inside the dash 
circles of Figures 2(a), 2(b) and 2(c) respectively. 
It is found (Yang et al2006) that the simple-
expansion tree span scheme performed best on the 
ACE 2003 corpus in pronoun resolution. This sug-
gests that inclusion of more structured information 
in the tree span may not help in pronoun resolution. 
To better capture structured information in the 
parse tree, this paper presents a dynamic-expansion 
scheme by trying to include necessary structured 
information in a parse tree. The intuition behind 
our scheme is that predicate- and antecedent com-
petitor- (all the other compatible1 antecedent can-
didates between the anaphor and the considered 
antecedent candidate) related information plays a 
critical role in pronoun resolution. Given an ana-
                                                           
1 With matched number, person and gender agreements. 
phor and an antecedent candidate, e.g. ?Mary? and 
?her? as shown in Figure 1, this is done by: 
1) Determining the min-expansion tree span via 
the shortest path, as shown in Figure 1(a). 
2) Attaching all the antecedent competitors along 
the corresponding paths to the shortest path. As 
shown in Figure 1(b), ?the woman? is attached 
while ?the room? is not attached since the for-
mer is compatible with the anaphor and the lat-
ter is not compatible with the anaphor. In this 
way, the competition between the considered 
candidate and other compatible candidates can 
be included in the tree span. In some sense, this 
is a natural extension of the twin-candidate 
learning approach proposed in Yang et al
(2003), which explicitly models the competition 
between two antecedent candidates. 
3) For each node in the tree span, attaching the 
path from the node to the predicate terminal 
node if it is a predicate-headed node. As shown 
in Figure 1(c), ?said? and ?bit? are attached. 
4) Pruning those nodes (except POS nodes) with 
the single in-arc and the single out-arc and with 
its syntactic phrase type same as its child node. 
As shown in Figure 1(d), the left child of the 
?SBAR? node, the ?NP? node, is removed and 
the sub-tree (NP the/DT woman/NN) is at-
tached to the ?SBAR? node directly.  
To show the difference among min-, simple-, 
full- and dynamic-expansion schemes, Figure 2 
compares them for three different sentences, given 
the anaphor ?her/herself? and the antecedent can-
didate ?Mary?. It shows that:  
? Min-, simple- and full-expansion schemes have 
the same tree spans (except the word nodes) for 
the three sentences regardless of the difference 
among the sentences while the dynamic-
expansion scheme can adapt to difference ones. 
? Normally, the min-expansion scheme is too 
simple to cover necessary information (e.g. ?the 
woman? in the 1st sentence is missing).  
? The full-expansion scheme can cover all the 
information at the expense of much noise (e.g. 
?the man in that room? in the 2nd sentence).  
? The simple-expansion scheme can cover some 
necessary predicate-related information (e.g. 
?said? and ?bit? in the sentences). However, it 
may introduce some noise (e.g. the left child of 
27
the ?SBAR? node, the ?NP? node, may not be 
necessary in the 2nd sentence) and ignore neces-
sary antecedent competitor-related information 
(e.g. ?the woman? in the 1st sentence). 
? The dynamic-expansion scheme normally 
works well. It can not only cover predicate-
related information but also structured informa-
tion related with the competitors of the consid-
ered antecedent candidate. In this way, the 
competition between the considered antecedent 
candidate and other compatible candidates can 
be included in the dynamic-expansion scheme. 
 
 
 
Figure 1: Dynamic-Expansion Tree Span Scheme 
 
  
 
Figure 2: Comparison of Min-, Simple-, Full-and Dynamic-Expansions: More Examples 
28
3.2 Context-Sensitive Convolution Tree Kernel 
Given any tree span scheme, e.g. the dynamic-
expansion scheme in the last subsection, we now 
study how to measure the similarity between two 
tree spans using a convolution tree kernel. 
A convolution kernel (Haussler D., 1999) aims 
to capture structured information in terms of sub-
structures. As a specialized convolution kernel, the 
convolution tree kernel, proposed in Collins and 
Duffy (2001), counts the number of common sub-
trees (sub-structures) as the syntactic structure 
similarity between two parse trees. This convolu-
tion tree kernel has been successfully applied by 
Yang et al(2006) in pronoun resolution. However, 
there is one problem with this tree kernel: the sub-
trees involved in the tree kernel computation are 
context-free (That is, they do not consider the in-
formation outside the sub-trees.). This is contrast 
to the tree kernel proposed in Culota and Sorensen 
(2004) which is context-sensitive, that is, it consid-
ers the path from the tree root node to the sub-tree 
root node. In order to integrate the advantages of 
both tree kernels and resolve the problem in 
Collins and Duffy?s kernel, this paper applies the 
same context-sensitive convolution tree kernel, 
proposed by Zhou et al(2007) on relation extrac-
tion. It works by taking ancestral information (i.e. 
the root node path) of sub-trees into consideration: 
? ?
=
?
?
D=
m
i
Nn
Nn
ii
C
ii
ii
nnTTK
1
]2[]2[
]1[]1[
11
11
11
])2[],1[(])2[],1[(  (1) 
where ][1 jN
i is the set of root node paths with 
length i in tree T[j] while the maximal length of a 
root node path is defined by m; and 
])2[],1[( 11
ii nnD  counts the common context-
sensitive sub-trees rooted at root node paths ]1[1
in  
and ]2[1
in . In the tree kernel, a sub-tree becomes 
context-sensitive via the ?root node path? moving 
along the sub-tree root. For more details, please 
refer to Zhou et al(2007). 
4 Experimentation 
This paper focuses on the third-person pronoun 
resolution and, in all our experiments, uses the 
ACE 2003 corpus for evaluation. This ACE corpus 
contains ~3.9k pronouns in the training data and 
~1.0k pronouns in the test data.  
Similar to Soon et al(2001), an input raw text is 
first preprocessed automatically by a pipeline of 
NLP components, including sentence boundary 
detection, POS tagging, named entity recognition 
and phrase chunking, and then a training or test 
instance is formed by a pronoun and one of its an-
tecedent candidates. During training, for each ana-
phor encountered, a positive instance is created by 
pairing the anaphor and its closest antecedent 
while a set of negative instances is formed by pair-
ing the anaphor with each of the non-coreferential 
candidates. Based on the training instances, a bi-
nary classifier is generated using a particular learn-
ing algorithm. In this paper, we use SVMLight 
deleveloped by Joachims (1998). During resolution, 
an anaphor is first paired in turn with each preced-
ing antecedent candidate to form a test instance, 
which is presented to a classifier. The classifier 
then returns a confidence value indicating the like-
lihood that the candidate is the antecedent. Finally, 
the candidate with the highest confidence value is 
selected as the antecedent. In this paper, the NPs 
occurring within the current and previous two sen-
tences are taken as the initial antecedent candidates, 
and those with mismatched number, person and 
gender agreements are filtered out. On average, an 
anaphor has ~7 antecedent candidates. The per-
formance is evaluated using F-measure instead of 
accuracy since evaluation is done on all the pro-
nouns occurring in the data.  
Scheme/m 1 2 3 4 
Min 78.5 79.8 80.8 80.8 
Simple 79.8 81.0 81.7 81.6 
Full 78.3 80.1 81.0 81.1 
Dynamic 80.8 82.3 83.0 82.9 
Table 1: Comparison of different context-sensitive  
convolution tree kernels and tree span schemes 
(with entity type info attached at both the anaphor 
and the antecedent candidate nodes by default) 
In this paper, the m parameter in our context-
sensitive convolution tree kernel as shown in 
Equation (1) indicates the maximal length of root 
node paths and is optimized to 3 using 5-fold cross 
validation on the training data. Table 1 systemati-
cally evaluates the impact of different m in our 
context-sensitive convolution tree kernel and com-
pares our dynamic-expansion tree span scheme 
with the existing three tree span schemes, min-, 
29
simple- and full-expansions as described in Yang 
et al(2006). It also shows that that our tree kernel 
achieves best performance with m = 3 on the test 
data, which outperforms the one with m = 1 by 
~2.2 in F-measure. This suggests that the parent 
and grandparent nodes of a sub-tree  contain much 
information for pronoun resolution while 
considering more ancestral nodes doesnot further 
improve the performance. This may be due to that, 
although our experimentation on the training data 
indicates that  more than 90% (on average) of 
subtrees has a root node path longer than 3 (since 
most of the subtrees are deep from the root node 
and more than 90% of the parsed trees are deeper 
than 6 levels in the ACE 2003 corpus), including a 
root node path longer than 3 may be vulnerable to 
the full parsing errors and have negative impact. It 
also shows that our dynamic-expansion tree span 
scheme outperforms min-expansion, simple-
expansion and full-expansion schemes by ~2.4, 
~1.2 and ~2.1 in F-measure respectively. This 
suggests the usefulness of dynamically expanding 
tree spans to cover necessary structured 
information in pronoun resolution. In all the 
following experiments, we will apply our tree 
kernel with m=3 and the dynamic-expansion tree 
span scheme by default, unless specified. 
We also evaluate the contributions of antecedent 
competitor-related information, predicate-related 
information and pruning in our dynamic-expansion 
tree span scheme by excluding one of them from 
the dynamic-expansion scheme. Table 2 shows that 
1) antecedent competitor-related information con-
tributes much to our scheme; 2) predicate-related 
information contributes moderately; 3) pruning 
only has slight contribution. This suggests the im-
portance of including the competition in the tree 
span and the effect of predicate-argument struc-
tures in pronoun resolution. This also suggests that 
our scheme can well make use of such predicate- 
and antecedent competitor-related information.  
Dynamic Expansion Effect 
- Competitors-related Info 81.1(-1.9) 
- Predicates-related Info 82.2 (-0.8) 
- Pruning 82.8(-0.2)  
All 83.0 
Table 2: Contributions of different factors in our 
dynamic-expansion tree span scheme 
Table 3 compares the performance of different 
tree span schemes for pronouns with antecedents in 
different sentences apart. It shows that our dy-
namic-expansion scheme is much more robust than 
other schemes with the increase of sentences apart. 
Scheme /  
#Sentences Apart 
0 1 2 
Min 86.3 76.7 39.6 
Simple 86.8 77.9 43.8 
Full 86.6 77.4 35.4 
Dynamic 87.6 78.8 54.2 
Table 3: Comparison of tree span schemes with 
antecedents in different sentences apart 
5 Conclusion 
Syntactic structured information holds great poten-
tial in many NLP applications. The purpose of this 
paper is to well capture syntactic structured infor-
mation in pronoun resolution. In this paper, we 
proposes a context-sensitive convolution tree ker-
nel to resolve two critical problems in previous 
researches in pronoun resolution by first automati-
cally determining a dynamic-expansion tree span, 
which effectively covers structured information in 
the parse trees by taking predicate- and antecedent 
competitor-related information into consideration, 
and then applying a context-sensitive convolution 
tree kernel, which enumerates both context-free 
sub-trees and context-sensitive sub-trees. Evalua-
tion on the ACE 2003 corpus shows that our dy-
namic-expansion tree span scheme can better 
capture necessary structured information than the 
existing tree span schemes and our tree kernel can 
better model structured information than the state-
of-the-art Collins and Duffy?s kernel.  
For the future work, we will focus on improving 
the context-sensitive convolution tree kernel by 
better modeling context-sensitive information and 
exploring new tree span schemes by better incor-
porating useful structured information. In the 
meanwhile, a more detailed quantitative evaluation 
and thorough qualitative error analysis will be per-
formed to gain more insights. 
Acknowledgement  
This research is supported by Project 60673041 
under the National Natural Science Foundation of 
China and Project 2006AA01Z147 under the ?863? 
National High-Tech Research and Development of 
China. 
30
References  
Aone C and Bennett W.W. (1995). Evaluating auto-
mated and manual acquisition of anaphora resolu-
tion strategies. ACL?1995:122-129. 
Bergsma S. and Lin D.K.(2006). Bootstrapping path-
based pronoun resolution. COLING-ACL?2006: 33-
40. 
Charniak E. (2001). Immediate-head Parsing for Lan-
guage Models. ACL?2001: 129-137. Toulouse, 
France 
Collins M. (1999) Head-driven statistical models for 
natural language parsing. Ph.D. Thesis. University 
of Pennsylvania. 
Collins M. and Duffy N. (2001). Convolution Ker-
nels for Natural Language. NIPS?2001: 625-632. 
Cambridge, MA 
Culotta A. and Sorensen J. (2004). Dependency tree 
kernels for relation extraction. ACL?2004. 423-429. 
21-26 July 2004. Barcelona, Spain. 
Haussler D. (1999). Convolution Kernels on Discrete 
Structures. Technical Report UCS-CRL-99-10, Uni-
versity of California, Santa Cruz. 
Hobbs J. (1978). Resolving pronoun references. Lin-
gua. 44:339-352. 
Joachims T. (1998). Text Categorization with Sup-
port Vector Machine: learning with many relevant 
features. ECML-1998: 137-142.  Chemnitz, Ger-
many 
Lappin S. and Leass H. (1994). An algorithm for pro-
nominal anaphora resolution. Computational Lin-
guistics. 20(4):526-561. 
Mitkov R. (1998). Robust pronoun resolution with 
limited knowledge. COLING-ACL?1998:869-875. 
Montreal, Canada.  
Moschitti A. (2004). A study on convolution kernels 
for shallow semantic parsing. ACL?2004:335-342. 
Pradhan S., Hacioglu K., Krugler V., Ward W., Mar-
tin J.H. and Jurafsky D. (2005). Support Vector 
Learning for Semantic Argument Classification. 
Machine Learning. 60(1):11-39. 
Soon W. Ng H.T.and Lim D. (2001). A machine 
learning approach to creference resolution of noun 
phrases. Computational Linguistics. 27(4): 521-544. 
Yang X.F., Zhou G.D., Su J. and Tan C.L., Corefer-
ence Resolution Using Competition Learning Ap-
proach, ACL?2003):176-183. Sapporo, Japan, 7-12 
July 2003. 
Yang X.F., Su J. and Tan C.L. (2006). Kernel-based 
pronoun resolution with structured syntactic knowl-
edge. COLING-ACL?2006: 41-48. 
Zelenko D., Aone C. and Richardella. (2003). Kernel 
methods for relation extraction. Journal of Machine 
Learning Research. 3(Feb):1083-1106. 
Zhang M., Zhang J., Su J. and Zhou G.D. (2006). A 
Composite Kernel to Extract Relations between En-
tities with both Flat and Structured Features. 
COLING-ACL-2006: 825-832. Sydney, Australia 
Zhou G.D., Su J. Zhang J. and Zhang M. (2005). Ex-
ploring various knowledge in relation extraction. 
ACL?2005. 427-434. 25-30 June, Ann Arbor, Mich-
gan, USA. 
Zhou G.D., Zhang M., Ji D.H. and Zhu Q.M. (2007). 
Tree Kernel-based Relation Extraction with Con-
text-Sensitive Structured Parse Tree Information. 
EMNLP-CoNLL?2007 
31
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 599?607,
Beijing, August 2010
Dependency-driven Anaphoricity Determination for Coreference 
Resolution
Fang Kong  Guodong Zhou  Longhua Qian  Qiaoming Zhu*
JiangSu Provincial Key Lab for Computer Information Processing Technology 
School of Computer Science and Technology Soochow University 
{kongfang, gdzhou, qianlonghua, qmzhu}@suda.edu.cn 
                                                          
* Corresponding author 
Abstract
This paper proposes a dependency-driven 
scheme to dynamically determine the syn-
tactic parse tree structure for tree ker-
nel-based anaphoricity determination in 
coreference resolution. Given a full syntactic 
parse tree, it keeps the nodes and the paths 
related with current mention based on con-
stituent dependencies from both syntactic 
and semantic perspectives, while removing 
the noisy information, eventually leading to 
a dependency-driven dynamic syntactic 
parse tree (D-DSPT). Evaluation on the ACE 
2003 corpus shows that the D-DSPT out-
performs all previous parse tree structures on 
anaphoricity determination, and that apply-
ing our anaphoricity determination module 
in coreference resolution achieves the so far 
best performance. 
1 Introduction 
Coreference resolution aims to identify which 
noun phrases (NPs, or mentions) refer to the 
same real-world entity in a text. According to 
Webber (1979), coreference resolution can be 
decomposed into two complementary sub-tasks: 
(1) anaphoricity determination, determining 
whether a given NP is anaphoric or not; and (2) 
anaphor resolution, linking together multiple 
mentions of a given entity in the world. Al-
though machine learning approaches have per-
formed reasonably well in coreference resolu-
tion without explicit anaphoricity determina-
tion (e.g. Soon et al 2001; Ng and Cardie 
2002b; Yang et al 2003, 2008; Kong et al 
2009), knowledge of NP anaphoricity is ex-
pected to much improve the performance of a 
coreference resolution system, since a 
non-anaphoric NP does not have an antecedent 
and therefore does not need to be resolved. 
Recently, anaphoricity determination has 
been drawing more and more attention. One 
common approach involves the design of some 
heuristic rules to identify specific types of 
non-anaphoric NPs, such as pleonastic it (e.g. 
Paice and Husk 1987; Lappin and Leass 1994, 
Kennedy and Boguraev 1996; Denber 1998) 
and definite descriptions (e.g. Vieira and Poe-
sio 2000). Alternatively, some studies focus on 
using statistics to tackle this problem (e.g., 
Bean and Riloff 1999; Bergsma et al 2008) 
and others apply machine learning approaches 
(e.g. Evans 2001;Ng and Cardie 2002a, 
2004,2009; Yang et al 2005; Denis and Bal-
bridge 2007; Luo 2007; Finkel and Manning 
2008; Zhou and Kong 2009).  
As a representative, Zhou and Kong (2009) 
directly employ a tree kernel-based method to 
automatically mine the non-anaphoric informa-
tion embedded in the syntactic parse tree. One 
main advantage of the kernel-based methods is 
that they are very effective at reducing the 
burden of feature engineering for structured 
objects. Indeed, the kernel-based methods have 
been successfully applied to mine structured 
information in various NLP applications like 
syntactic parsing (Collins and Duffy, 2001; 
Moschitti, 2004), semantic relation extraction 
(Zelenko et al, 2003; Zhao and Grishman, 
2005; Zhou et al 2007; Qian et al, 2008), se-
mantic role labeling (Moschitti, 2004); corefer-
ence resolution (Yang et al, 2006; Zhou et al, 
2008). One of the key problems for the ker-
nel-based methods is how to effectively capture 
the structured information according to the na-
ture of the structured object in the specific task. 
This paper advances the state-of-the-art per-
formance in anaphoricity determination by ef-
599
fectively capturing the structured syntactic in-
formation via a tree kernel-based method. In 
particular, a dependency-driven scheme is 
proposed to dynamically determine the syntac-
tic parse tree structure for tree kernel-based 
anaphoricity determination by exploiting con-
stituent dependencies from both the syntactic 
and semantic perspectives to keep the neces-
sary information in the parse tree as well as 
remove the noisy information. Our motivation 
is to employ critical dependency information in 
constructing a concise and effective syntactic 
parse tree structure, specifically targeted for 
tree kernel-based anaphoricity determination.  
The rest of this paper is organized as follows. 
Section 2 briefly describes the related work on 
both anaphoricity determination and exploring 
syntactic parse tree structures in related tasks. 
Section 3 presents our dependency-driven 
scheme to determine the syntactic parse tree 
structure. Section 4 reports the experimental 
results. Finally, we conclude our work in Sec-
tion 5. 
2 Related Work 
This section briefly overviews the related work 
on both anaphoricity determination and ex-
ploring syntactic parse tree structures. 
2.1 Anaphoricity Determination 
Previous work on anaphoricity determination 
can be broadly divided into three categories: 
heuristic rule-based (e.g. Paice and Husk 
1987;Lappin and Leass 1994; Kennedy and 
Boguraev 1996; Denber 1998; Vieira and Poe-
sio 2000; Cherry and Bergsma 2005), statis-
tics-based (e.g. Bean and Riloff 1999; Cherry 
and Bergsma 2005; Bergsma et al 2008) and 
learning-based methods (e.g. Evans 2001; Ng 
and Cardie 2002a; Ng 2004; Yang et al 2005; 
Denis and Balbridge 2007; Luo 2007; Finkel 
and Manning 2008; Zhou and Kong 2009; Ng 
2009).  
The heuristic rule-based methods focus on 
designing some heuristic rules to identify spe-
cific types of non-anaphoric NPs. Representa-
tive work includes: Paice and Husk (1987), 
Lappin and Leass (1994) and Kennedy and 
Boguraev (1996). For example, Kennedy and 
Boguraev (1996) looked for modal adjectives 
(e.g. ?necessary?) or cognitive verbs (e.g. ?It is 
thought that?? in a set of patterned construc-
tions) in identifying pleonastic it.
Among the statistics-based methods, Bean 
and Riloff (1999) automatically identified ex-
istential definite NPs which are non-anaphoric.  
The intuition behind is that many definite NPs 
are not anaphoric since their meanings can be 
understood from general world knowledge, e.g. 
?the FBI?. They found that existential NPs ac-
count for 63% of all definite NPs and 76% of 
them could be identified by syntactic or lexical 
means. Cherry and Bergsma (2005) extended 
the work of Lappin and Leass (1994) for 
large-scale anaphoricity determination by addi-
tionally detecting pleonastic it. Bergsma et al 
(2008) proposed a distributional method in de-
tecting non-anaphoric pronouns. They first ex-
tracted the surrounding context of the pronoun 
and gathered the distribution of words that oc-
curred within the context from a large corpus, 
and then identified the pronoun either ana-
phoric or non-anaphoric based on the word dis-
tribution.
Among the learning-based methods, Evans 
(2001) automatically identified the 
non-anaphoricity of pronoun it using various 
kinds of lexical and syntactic features. Ng and 
Cardie (2002a) employed various do-
main-independent features in identifying ana-
phoric NPs. They trained an anaphoricity clas-
sifier to determine whether a NP was anaphoric 
or not, and employed an independently-trained 
coreference resolution system to only resolve 
those mentions which were classified as ana-
phoric. Experiments showed that their method 
improved the performance of coreference 
resolution by 2.0 and 2.6 to 65.8 and 64.2 in 
F1-measure on the MUC-6 and MUC-7 cor-
pora, respectively. Ng (2004) examined the 
representation and optimization issues in com-
puting and using anaphoricity information to 
improve learning-based coreference resolution. 
On the basis, he presented a corpus-based ap-
proach (Ng, 2009) for achieving global opti-
mization by representing anaphoricity as a fea-
ture in coreference resolution. Experiments on 
the ACE 2003 corpus showed that their method 
improved the overall performance by 2.8, 2.2 
and 4.5 to 54.5, 64.0 and 60.8 in F1-measure 
on the NWIRE, NPAPER and BNEWS do-
mains, respectively. However, he did not look 
into the contribution of anaphoricity determi-
600
nation on coreference resolution of different 
NP types. Yang et al (2005) made use of 
non-anaphors to create a special class of train-
ing instances in the twin-candidate model 
(Yang et al 2003) and improved the perform-
ance by 2.9 and 1.6 to 67.3 and 67.2 in 
F1-measure on the MUC-6 and MUC-7 cor-
pora, respectively. However, their experiments 
show that eliminating non-anaphors using an 
anaphoricity determination module in advance 
harms the performance. Denis and Balbridge 
(2007) employed an integer linear program-
ming (ILP) formulation for coreference resolu-
tion which modeled anaphoricity and corefer-
ence as a joint task, such that each local model 
informed the other for the final assignments. 
Experiments on the ACE 2003 corpus showed 
that this joint anaphoricity-coreference ILP 
formulation improved the F1-measure by 
3.7-5.3 on various domains. However, their 
experiments assume true ACE mentions (i.e. all 
the ACE mentions are already known from the 
annotated corpus). Therefore, the actual effect 
of this joint anaphoricity-coreference ILP for-
mulation on fully automatic coreference reso-
lution is still unclear. Luo (2007) proposed a 
twin-model for coreference resolution: a link 
component, which models the coreferential 
relationship between an anaphor and a candi-
date antecedent, and a creation component, 
which models the possibility that a NP was not 
coreferential with any candidate antecedent. 
This method combined the probabilities re-
turned by the creation component (an ana-
phoricity model) with the link component (a 
coreference model) to score a coreference par-
tition, such that a partition was penalized 
whenever an anaphoric mention was resolved. 
Finkel and Manning (2008) showed that transi-
tivity constraints could be incorporated into an 
ILP-based coreference resolution system and 
much improved the performance. Zhou and 
Kong (2009) employed a global learning 
method in determining the anaphoricity of NPs 
via a label propagation algorithm to improve 
learning-based coreference resolution. Experi-
ments on the ACE 2003 corpus demonstrated 
that this method was very effective. It could 
improve the F1-measure by 2.4, 3.1 and 4.1 on 
the NWIRE, NPAPER and BNEWS domains, 
respectively. Ng (2009) presented a novel ap-
proach to the task of anaphoricity determina-
tion based on graph minimum cuts and demon-
strated the effectiveness in improving a learn-
ing-based coreference resolution system. 
In summary, although anaphoricity determi-
nation plays an important role in coreference 
resolution and achieves certain success in im-
proving the overall performance of coreference 
resolution, its contribution is still far from ex-
pectation.
2.2 Syntactic Parse Tree Structures 
For a tree kernel-based method, one key prob-
lem is how to represent and capture the struc-
tured syntactic information. During recent 
years, various tree kernels, such as the convo-
lution tree kernel (Collins and Duffy, 2001), 
the shallow parse tree kernel (Zelenko et al
2003) and the dependency tree kernel (Culota 
and Sorensen, 2004), have been proposed in the 
literature. Among these tree kernels, the con-
volution tree kernel represents the state-of-the 
art and has been successfully applied by 
Collins and Duffy (2002) on syntactic parsing, 
Zhang et al (2006) on semantic relation extrac-
tion and Yang et al (2006) on pronoun resolu-
tion.
Given a tree kernel, the key issue is how to 
generate a syntactic parse tree structure for ef-
fectively capturing the structured syntactic in-
formation. In the literature, various parse tree 
structures have been proposed and successfully 
applied in some NLP applications. As a repre-
sentative, Zhang et al (2006) investigated five 
parse tree structures for semantic relation ex-
traction and found that the Shortest 
Path-enclosed Tree (SPT) achieves the best 
performance on the 7 relation types of the ACE 
RDC 2004 corpus. Yang et al (2006) con-
structed a document-level syntactic parse tree 
for an entire text by attaching the parse trees of 
all its sentences to a new-added upper node and 
examined three possible parse tree structures 
(Min-Expansion, Simple-Expansion and 
Full-Expansion) that contain different sub-
structures of the parse tree for pronoun resolu-
tion. Experiments showed that their method 
achieved certain success on the ACE 2003 
corpus and the simple-expansion scheme per-
forms best. However, among the three explored 
schemes, there exists no obvious overwhelming 
one, which can well cover structured syntactic 
information. One problem of Zhang et al (2006) 
601
and Yang et al (2006) is that their parse tree 
structures are context-free and do not consider 
the information outside the sub-trees. Hence, 
their ability of exploring structured syntactic 
information is much limited. Motivated by 
Zhang et al (2006) and Yang et al (2006), 
Zhou et al (2007) extended the SPT to become 
context-sensitive (CS-SPT) by dynamically 
including necessary predicate-linked path in-
formation. Zhou et al (2008) further proposed 
a dynamic-expansion scheme to automatically 
determine a proper parse tree structure for 
pronoun resolution by taking predicate- and 
antecedent competitor-related information in 
consideration. Evaluation on the ACE 2003 
corpus showed that the dynamic-expansion 
scheme can well cover necessary structured 
information in the parse tree for pronoun reso-
lution. One problem with the above parse tree 
structures is that they may still contain unnec-
essary information and also miss some useful 
context-sensitive information. Qian et al (2008) 
dynamically determined the parse tree structure 
for semantic relation extraction by exploiting 
constituent dependencies to keep the necessary 
information in the parse tree as well as remove 
the noisy information. Evaluation on the ACE 
RDC 2004 corpus showed that their dynamic 
syntactic parse tree structure outperforms all 
previous parse tree structures. However, their 
solution has the limitation in that the depend-
encies were found according to some manu-
ally-written ad-hoc rules and thus may not be 
easily applicable to new domains and applica-
tions.
This paper proposes a new scheme to dy-
namically determine the syntactic parse tree 
structure for anaphoricity determination and 
systematically studies the application of an ex-
plicit anaphoricity determination module in 
improving coreference resolution. 
3 Dependency-driven Dynamic Syn-
tactic Parse Tree 
Given a full syntactic parse tree and a NP in 
consideration, one key issue is how to choose a 
proper syntactic parse tree structure to well 
cover structured syntactic information in the 
tree kernel computation. Generally, the more a 
syntactic parse tree structure includes, the more 
structured syntactic information would be 
available, at the expense of more noisy (or un-
necessary) information.  
It is well known that dependency informa-
tion plays a key role in many NLP problems, 
such as syntactic parsing, semantic role label-
ing as well as semantic relation extraction. Mo-
tivated by Qian et al (2008) and Zhou et al 
(2008), we propose a new scheme to dynami-
cally determine the syntactic parse tree struc-
ture for anaphoricity determination by exploit-
ing constituent dependencies from both the 
syntactic and semantic perspectives to distin-
guish the necessary evidence from the unnec-
essary information in the syntactic parse tree. 
That is, constituent dependencies are explored 
from two aspects: syntactic dependencies and 
semantic dependencies.  
1) Syntactic Dependencies: The Stanford de-
pendency parser1 is employed as our syn-
tactic dependency parser to automatically 
extract various syntactic (i.e. grammatical) 
dependencies between individual words. In 
this paper, only immediate syntactic de-
pendencies with current mention are con-
sidered. The intuition behind is that the im-
mediate syntactic dependencies carry the 
major contextual information of current 
mention.
2) Semantic Dependencies: A state-of-the-art 
semantic role labeling (SRL) toolkit (Li et 
al. 2009) is employed for extracting various 
semantic dependencies related with current 
mention. In this paper, semantic dependen-
cies include all the predicates heading any 
node in the root path from current mention 
to the root node and their compatible argu-
ments (except those overlapping with cur-
rent mention). 
We name our parse tree structure as a depend-
ency-driven dynamic syntactic parse tree 
(D-DSPT). The intuition behind is that the de-
pendency information related with current 
mention in the same sentence plays a critical 
role in anaphoricity determination. Given the 
sentence enclosing the mention under consid-
eration, we can get the D-DSPT as follows: 
(Figure 1 illustrates an example of the D-DSPT 
generation given the sentence ?Mary said the 
woman in the room bit her? with ?woman? as 
current mention.) 
                                                          
1 http://nlp.stanford.edu/software/lex-parser.shtml
602
           
Figure 1:  An example of generating the dependency-driven dynamic syntactic parse tree  
1) Generating the full syntactic parse tree of 
the given sentence using a full syntactic parser. 
In this paper, the Charniak parser (Charniak 
2001) is employed and Figure 1 (a) shows the 
resulting full parse tree. 
2) Keeping only the root path from current 
mention to the root node of the full parse tree. 
Figure 1(b) shows the root path corresponding 
to the current mention ?woman?. In the fol-
lowing steps, we attach the above two types of 
dependency information to the root path.  
3) Extracting all the syntactic dependencies 
in the sentence using a syntactic dependency 
parser, and attaching all the nodes, which have 
immediate dependency relationship with cur-
rent mention, and their corresponding paths to 
the root path. Figure 1(c) illustrates the syntac-
tic dependences extracted from the sentence, 
where the ones in italic mean immediate de-
pendencies with current mention. Figure 1(d) 
shows the parse tree structure after considering 
syntactic dependencies. 
4) Attaching all the predicates heading any 
node in the root path from current mention to 
the root node and their corresponding paths to 
the root path. For the example sentence, there 
are two predicates ?said? and ?bit?, which head 
the ?VP? and ?S? nodes in the root path re-
spectively. Therefore, these two predicates and 
their corresponding paths should be attached to 
the root path as shown in Figure 1(e). Note that 
the predicate ?bit? and its corresponding path 
has already been attached in Stop (3). As a re-
sult, the predicate-related information can be 
attached. According to Zhou and Kong (2009), 
such information is important to definite NP 
resolution.
5) Extracting the semantic dependencies re-
lated with those attached predicates using a 
(shallow) semantic parser, and attaching all the 
compatible arguments (except those overlap-
ping with current mention) and their corre-
sponding paths to the root path. For example, 
as shown in Figure 1(e), since the arguments 
?Mary? and ?her? are compatible with current 
mention ?woman?, these two nodes and their 
corresponding paths are attached while the ar-
gument ?room? is not since its gender does not 
agree with current mention. 
In this paper, the similarity between two 
parse trees is measured using a convolution tree 
kernel, which counts the number of common 
sub-tree as the syntactic structure similarity 
between two parse trees. For details, please 
refer to Collins and Duffy (2001). 
603
4 Experimentation and Discussion 
This section evaluates the performance of de-
pendency-driven anaphoricity determination 
and its application in coreference resolution on 
the ACE 2003 corpus. 
4.1 Experimental Setting 
The ACE 2003 corpus contains three domains: 
newswire (NWIRE), newspaper (NPAPER), 
and broadcast news (BNEWS). For each do-
main, there exist two data sets, training and 
devtest, which are used for training and testing.  
For preparation, all the documents in the 
corpus are preprocessed automatically using a 
pipeline of NLP components, including to-
kenization and sentence segmentation, named 
entity recognition, part-of-speech tagging and 
noun phrase chunking. Among them, named 
entity recognition, part-of-speech tagging and 
noun phrase chunking apply the same 
state-of-the-art HMM-based engine with er-
ror-driven learning capability (Zhou and Su, 
2000 & 2002). Our statistics finds that 62.0%, 
58.5% and 61.4% of entity mentions are pre-
served after preprocessing on the NWIRE, 
NPAPER and BNEWS domains of the ACE 
2003 training data respectively while only 
89.5%, 89.2% and 94% of entity mentions are 
preserved after preprocessing on  the NWIRE, 
NPAPER and BNEWS domains of the ACE 
2003 devtest data. This indicates the difficulty 
of coreference resolution. In addition, the cor-
pus is parsed using the Charniak parser for 
syntactic parsing and the Stanford dependency 
parser for syntactic dependencies while corre-
sponding semantic dependencies are extracted 
using a state-of-the-art semantic role labeling 
toolkit (Li et al 2009). Finally, we use the 
SVM-light2 toolkit with the tree kernel func-
tion as the classifier. For comparison purpose, 
the training parameters C (SVM) and ?(tree
kernel) are set to 2.4 and 0.4 respectively, as 
done in Zhou and Kong (2009).  
For anaphoricity determination, we report 
the performance in Acc+ and Acc-, which 
measure the accuracies of identifying anaphoric 
NPs and non-anaphoric NPs, respectively. Ob-
viously, higher Acc+ means that more ana-
phoric NPs would be identified correctly, while 
                                                          
2 http://svmlight.joachims.org/ 
higher Acc- means that more non-anaphoric 
NPs would be filtered out. For coreference 
resolution, we report the performance in terms 
of recall, precision, and F1-measure using the 
commonly-used model theoretic MUC scoring 
program (Vilain et al 1995). To see whether an 
improvement is significant, we also conduct 
significance testing using paired t-test. In this 
paper, ?***?, ?**? and ?*? denote p-values of an 
improvement smaller than 0.01, in-between 
(0.01, 0,05] and bigger than 0.05, which mean 
significantly better, moderately better and 
slightly better, respectively. 
4.2 Experimental Results 
Performance of anaphoricity determination 
Table 1 presents the performance of anaphoric-
ity determination using the convolution tree 
kernel on D-DSPT. It shows that our method 
achieves the accuracies of 83.27/77.13, 
86.77/80.25 and 90.02/64.24 on identifying 
anaphoric/non-anaphoric NPs in the NWIRE, 
NPAPER and BNEWS domains, respectively.  
This suggests that our approach can effectively 
filter out about 75% of non-anaphoric NPs and 
keep about 85% of anaphoric NPs. In com-
parison, in the three domains Zhou and Kong 
(2009) achieve the accuracies of 76.5/82.3, 
78.9/81.6 and 74.3/83.2, respectively, using the 
tree kernel on a dynamically-extended tree 
(DET). This suggests that their method can fil-
ter out about 82% of non-anaphoric NPs and 
only keep about 76% of anaphoric NPs. In 
comparison, their method outperforms our 
method on filtering out more non-anaphoric 
NPs while our method outperforms their 
method on keeping more anaphoric NPs in 
coreference resolution. While a coreference 
resolution system can detect some 
non-anaphoric NPs (when failing to find the 
antecedent candidate), filtering out anaphoric 
NPs in anaphoricity determination would defi-
nitely cause errors and it is almost impossible 
to recover. Therefore, it is normally more im-
portant to keeping more anaphoric NPs than 
filtering out more non-anaphoric NPs. Table 1 
further presents the performance of anaphoric-
ity determination on different NP types. It 
shows that our method performs best at keep-
ing pronominal NPs and filtering out proper 
NPs.
604
NWIRE NPAPER BNEWS NP Type 
Acc+ Acc- Acc+ Acc- Acc+ Acc-
Pronoun 95.07 50.36 96.40 56.44 98.26 54.03 
Proper NP 84.61 83.17 83.78 79.62 87.61 71.77 
Definite NP 87.17 46.74 82.24 49.18 86.87 53.65 
Indefinite NP 86.01 47.52 80.63 48.45 89.71 47.32 
Over all 83.27 77.13 86.77 80.25 90.02 64.24 
Table 1: Performance of anaphoricity determination using the D-DSPT  
NWIRE NPAPER BNEWS Performance Change 
Acc+ Acc- Acc+ Acc- Acc+ Acc-
D-DSPT 83.27 77.13 86.77 80.25 90.02 64.24 
-Syntactic Dependencies 78.67 72.56 80.14 73.74 87.05 60.20 
-Semantic Dependencies 81.67 76.74 83.47 77.93 89.58 60.67 
Table 2: Contribution of including syntactic and semantic dependencies  
in D-DSPT on anaphoricity determination  
NWIRE NPAPER BNEWS System 
R% P% F R% P% F R% P% F 
Pronoun 70.8 57.9 63.7 76.5 63.5 69.4 70.0 60.3 64.8
Proper NP 80.3 80.1 80.2 81.8 83.6 82.7 76.3 76.8 76.6
Definite NP 35.9 43.4 39.2 43.1 48.5 45.6 47.9 51.9 49.8
Indefinite NP 40.3 26.3 31.8 39.7 22.9 29.0 23.6 10.7 14.7
Without ana-
phoricity de-
termination 
(Baseline)
Over all 55.0 63.8 59.1 62.1 65.0 63.5 53.2 60.5 56.6
Pronoun 65.9 70.2 68.0 72.6 78.7 75.5 67.7 75.8 71.5
Proper NP 80.3 81.0 80.6 81.2 85.1 83.1 76.3 84.4 80.1
Definite NP 32.3  63.1 42.7 38.4 61.7 47.3 42.5 66.4 51.8
Indefinite NP 36.4 55.3 43.9 34.7 50.7 41.2 20.3 45.4 28.1
With D-DSPT 
-based ana-
phoricity de-
termination 
Over all 52.4 79.6 63.2 58.1 80.3 67.4 50.1 79.8 61.6
Pronoun 68.6 71.5 70.1 75.2 80.4 77.7 69.1 77.8 73.5
Proper NP 81.7 89.3 85.3 82.6 90.1 86.2 78.6 88.7 83.3
Definite NP 41.8 85.9 56.2 44.9 85.2 58.8 45.2 87.9 59.7
Indefinite NP 40.3 67.6 50.5 41.2 65.1 50.5 40.9 50.1 45.1
With golden 
anaphoricity
determination 
Over all 54.6 81.7 65.5 60.4 82.1 69.6 51.9 82.1 63.6
Table 3: Performance of anaphoricity determination on coreference resolution 
NWIRE NPAPER BNEWS System 
R% P% F R% P% F R% P% F 
Without anaphoricity determina-
tion (Baseline) 53.1 67.4 59.4 57.7 67.0 62.1 48.0 65.9 55.5Zhou and 
Kong (2009) With Dynamically Extended 
Tree-based anaphoricity determi-
nation
51.6 77.2 61.8 55.2 78.6 65.2 47.5 80.3 59.6
Without anaphoricity determina-
tion (Baseline)
59.1 58. 58.6 60.8 62.6 61.7 57.7 52.6 55.0
Ng (2009) 
With Graph Minimum Cut-based 
anaphoricity determination
54.1 69.0 60.6 57.9 71.2 63.9 53.1 67.5 59.4
Table 4: Performance comparison with other systems 
Table 2 further presents the contribution of 
including syntactic and semantic dependencies 
in the D-DSPT on anaphoricity determination 
by excluding one or both of them. It shows that 
both syntactic dependencies and semantic de-
pendencies contribute significantly (***). 
Performance of coreference resolution 
We have evaluated the effect of our 
D-DSPT-based anaphoricity determination 
module on coreference resolution by including 
it as a preprocessing step to a baseline corefer-
ence resolution system without explicit ana-
phoricity determination, by filtering our those 
non-anaphoric NPs according to the anaphoric-
ity determination module. Here, the baseline 
system employs the same set of features, as 
adopted in the single-candidate model of Yang 
et al (2003) and uses a SVM-based classifier 
with the feature-based RBF kernel. Table 3 
presents the detailed performance of the 
coreference resolution system without ana-
605
phoricity determination, with D-DSPT-based 
anaphoricity determination and. with golden 
anaphoricity determination. Table 3 shows that: 
1) There is a performance gap of 6.4, 6.1 and 
7.0 in F1-measure on the NWIRE, NPAPER 
and BNEWS domain, respectively, between the 
coreference resolution system with golden 
anaphoricity determination and the baseline 
system without anaphoricity determination. 
This suggests the usefulness of proper ana-
phoricity determination in coreference resolu-
tion. This also agrees with Stoyanov et al 
(2009) which measured the impact of golden 
anaphoricity determination on coreference 
resolution using only the annotated anaphors in 
both training and testing.  
2) Compared to the baseline system without 
anaphoricity determination, the D-DSPT-based 
anaphoricity determination module improves 
the performance by 4.1(***), 3.9(***) and 
5.0(***) to 63.2, 67.4 and 61.6 in F1-measure 
on the NWIRE, NPAPER and BNEWS do-
mains, respectively, due to a large gain in pre-
cision and a much smaller drop in recall. In 
addition, D-DSPT-based anaphoricity determi-
nation can not only much improve the per-
formance of coreference resolution on pro-
nominal NPs (***) but also on definite 
NPs(***) and indefinite NPs(***) while the 
improvement on proper NPs can be ignored 
due to the fact that proper NPs can be well ad-
dressed by the simple abbreviation feature in 
the baseline system. 
3) D-DSPT-based anaphoricity determination 
still lags (2.3, 2.2 and 2.0 on the NWIRE, 
NPAPER and BNEWS domains, respectively) 
behind golden anaphoricity determination in 
improving the overall performance of corefer-
ence resolution. This suggests that there exists 
some room in the performance improvement 
for anaphoricity determination. 
Performance comparison with other systems 
Table 4 compares the performance of our sys-
tem with other systems. Here, Zhou and Kong 
(2009) use the same set of features with ours in 
the baseline system and a dynami-
cally-extended tree structure in anaphoricity 
determination. Ng (2009) uses 33 features as 
described in Ng (2007) and a graph minimum 
cut algorithm in anaphoricity determination. It 
shows that the overall performance of our 
baseline system is almost as good as that of 
Zhou and Kong (2009) and a bit better than 
Ng?s (2009).  
For overall performance, our coreference 
resolution system with D-DSPT-based ana-
phoricity determination much outperforms 
Zhou and Kong (2009) in F1-measure by 1.4, 
2.2 and 2.0 on the NWIRE, NPAPER and 
BNEWS domains, respectively, due to the bet-
ter inclusion of dependency information. De-
tailed evaluation shows that such improvement 
comes from coreference resolution on both 
pronominal and definite NPs (Please refer to 
Table 6 in Zhou and Kong, 2009). Compared 
with Zhou and Kong (2009) and Ng (2009), our 
approach achieves the best F1-measure so far 
for each dataset. 
5 Conclusion and Further Work 
This paper systematically studies a depend-
ency-driven dynamic syntactic parse tree 
(DDST) for anaphoricity determination and the 
application of an explicit anaphoricity deter-
mination module in improving learning-based 
coreference resolution. Evaluation on the ACE 
2003 corpus indicates that D-DSPT-based 
anaphoricity determination much improves the 
performance of coreference resolution. 
To our best knowledge, this paper is the first 
research which directly explores constituent 
dependencies in tree kernel-based anaphoricty 
determination from both syntactic and semantic 
perspectives. 
For further work, we will explore more 
structured syntactic information in coreference 
resolution. In addition, we will study the inter-
action between anaphoricity determination and 
coreference resolution and better integrate 
anaphoricity determination with coreference 
resolution.
Acknowledgments 
This research was supported by Projects 
60873150, 60970056, and 90920004 under the 
National Natural Science Foundation of China, 
Project 200802850006 and 20093201110006 
under the Specialized Research Fund for the 
Doctoral Program of Higher Education of 
China.
606
References 
D. Bean and E. Riloff 1999. Corpus-based Identifi-
cation of Non-Anaphoric Noun Phrases. ACL? 
1999 
S. Bergsma, D. Lin and R. Goebel 2008. Distribu-
tional Identification of Non-referential Pronouns. 
ACL?2008 
C. Cherry and S. Bergsma. 2005. An expectation 
maximization approach to pronoun resolution. 
CoNLL?2005 
M. Collins and N. Duffy. 2001. Covolution kernels 
for natural language. NIPS?2001  
M. Denber 1998. Automatic Resolution of Anapho-
ria in English. Technical Report, Eastman Ko-
dakCo. 
P. Denis and J. Baldridge. 2007. Global, joint de-
termination of anaphoricity and coreference 
resolution using integer programming. 
NAACL/HLT?2007
R. Evans 2001. Applying machine learning toward 
an automatic classification of it. Literary and 
Linguistic Computing, 16(1):45-57 
F. Kong, G.D. Zhou and Q.M. Zhu. 2009 Employ-
ing the Centering Theory in Pronoun Resolution 
from the Semantic Perspective. EMNLP?2009 
F. Kong, Y.C. Li, G.D. Zhou and Q.M. Zhu. 2009. 
Exploring Syntactic Features for Pronoun Reso-
lution Using Context-Sensitive Convolution Tree 
Kernel. IALP?2009 
S. Lappin and J. L. Herbert. 1994. An algorithm for 
pronominal anaphora resolution. Computational 
Linguistics, 20(4) 
J.H. Li. G.D. Zhou, H. Zhao, Q.M. Zhu and P.D. 
Qian. Improving nominal SRL in Chinese lan-
guage with verbal SRL information and auto-
matic predicate recognition. EMNLP '2009 
X. Luo. 2007. Coreference or not: A twin model for 
coreference resolution.  NAACL-HLT?2007 
V. Ng and C. Cardie 2002. Identify Anaphoric and 
Non-Anaphoric Noun Phrases to Improve 
Coreference Resolution. COLING?2002 
V. Ng and C. Cardie 2002. Improving machine 
learning approaches to coreference resolution. 
ACL?2002 
V. Ng 2004. Learning Noun Phrase Anaphoricity to 
Improve Coreference Resolution: Issues in Rep-
resentation and Optimization. ACL? 2004 
V. Ng 2009. Graph-cut based anaphoricity determi-
nation for coreference resolution. NAACL?2009 
L.H. Qian, G.D. Zhou, F. Kong, Q.M. Zhu and P.D. 
Qian. 2008. Exploiting constituent dependencies 
for  tree kernel-based semantic relation extrac-
tion. COLING?2008 
W. M. Soon, H. T. Ng and D. Lim  2001. A ma-
chine learning approach to coreference resolution 
of noun phrase. Computational Linguistics, 
27(4):521-544. 
V. Stoyanov, N. Gilbert, C. Cardie and E. Riloff. 
2009. Conundrums in Noun Phrase Coreference 
Resolution: Making Sense of the State-of-the Art. 
ACL?2009 
B. L. Webber. 1979. A Formal Approach to Dis-
course Anaphora. Garland Publishing, Inc. 
X.F. Yang, G.D. Zhou, J. Su and C.L. Chew. 2003. 
Coreference Resolution Using Competition 
Learning Approach. ACL?2003 
X.F. Yang, J. Su and C.L. Chew. 2005. A Twin 
Candidate Model of Coreference Resolution with 
Non-Anaphor Identification Capability. 
IJCNLP?2005 
X.F. Yang, J. Su and C.L. Chew. 2006. Ker-
nel-based pronoun resolution with structured 
syntactic knowledge. COLING-ACL?2006 
X.F. Yang, J. Su and C.L. Tan 2008. A 
Twin-Candidate Model for Learning-Based 
Anaphora Resolution. Computational Linguistics 
34(3):327-356 
M. Zhang, J. Zhang, J. Su and G.D. Zhou. 2006. A 
composite kernel to extract relations between en-
tities with both flat and structured features. 
COLING/ACL?2006 
S. Zhao and R. Grishman. 2005. Extracting relations 
with integered information using kernel methods. 
ACL?2005 
D. Zelenko, A. Chinatsu and R. Anthony. 2003. 
Kernel methods for relation extraction. Machine 
Learning Researching 3(2003):1083-1106 
G.D. Zhou, F. Kong and Q.M. Zhu. 2008. Con-
text-sensitive convolution tree kernel for pronoun 
resolution. IJCNLP?2008 
G.D. Zhou and F. Kong. 2009. Global Learning of 
Noun Phrase Anaphoricity in Coreference Reso-
lution via Label Propagetion. EMNLP?2009 
G.D. Zhou and J. Su. 2002. Named Entity recogni-
tion using a HMM-based chunk tagger. 
ACL?2002 
G.D. Zhou, M. Zhang, D.H. Ji and Q.M. Zhu. 2007. 
Tree kernel-based relation extraction with con-
text-sensitive structured parse tree information. 
EMNLP/CoNLL?2007
607
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 882?891,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
A Tree Kernel-based Unified Framework                                               
for Chinese Zero Anaphora Resolution 
 
Fang Kong  Guodong Zhou*  
JiangSu Provincial Key Lab for Computer Information Processing Technology 
School of Computer Science and Technology Soochow University 
{kongfang, gdzhou}@suda.edu.cn 
 
 
 
                                                          
* Corresponding author 
Abstract 
This paper proposes a unified framework for 
zero anaphora resolution, which can be di-
vided into three sub-tasks: zero anaphor detec-
tion, anaphoricity determination and 
antecedent identification. In particular, all the 
three sub-tasks are addressed using tree ker-
nel-based methods with appropriate syntactic 
parse tree structures. Experimental results on a 
Chinese zero anaphora corpus show that the 
proposed tree kernel-based methods signifi-
cantly outperform the feature-based ones. This 
indicates the critical role of the structural in-
formation in zero anaphora resolution and the 
necessity of tree kernel-based methods in 
modeling such structural information. To our 
best knowledge, this is the first systematic 
work dealing with all the three sub-tasks in 
Chinese zero anaphora resolution via a unified 
framework. Moreover, we release a Chinese 
zero anaphora corpus of 100 documents, 
which adds a layer of annotation to the manu-
ally-parsed sentences in the Chinese Treebank 
(CTB) 6.0.  
1 Introduction 
As one of the most important techniques in dis-
course analysis, anaphora resolution has been a 
focus of research in Natural Language Processing 
(NLP) for decades and achieved much success in 
English recently (e.g. Soon et al 2001; Ng and 
Cardie 2002; Yang et al 2003, 2008; Kong et al 
2009).  
However, there is little work on anaphora reso-
lution in Chinese. A major reason for this phe-
nomenon is that Chinese, unlike English, is a pro-
drop language, whereas in English, definite noun 
phrases (e.g. the company) and overt pronouns (e.g. 
he) are frequently employed as referring expres-
sions, which refer to preceding entities. Kim (2000) 
compared the use of overt subjects in English and 
Chinese. He found that overt subjects occupy over 
96% in English, while this percentage drops to 
only 64% in Chinese. This indicates the prevalence 
of zero anaphors in Chinese and the necessity of 
zero anaphora resolution in Chinese anaphora reso-
lution. Since zero anaphors give little hints (e.g. 
number or gender) about their possible antecedents, 
zero anaphora resolution is much more challenging 
than traditional anaphora resolution. 
Although Chinese zero anaphora has been 
widely studied in the linguistics research (Li and 
Thompson 1979; Li 2004), only a small body of 
prior work in computational linguistics deals with 
Chinese zero anaphora resolution (Converse 2006; 
Zhao and Ng 2007). Moreover, zero anaphor de-
tection, as a critical component for real applica-
tions of zero anaphora resolution, has been largely 
ignored.  
This paper proposes a unified framework for 
Chinese zero anaphora resolution, which can be 
divided into three sub-tasks: zero anaphor detec-
tion, which detects zero anaphors from a text, ana-
phoricity determination, which determines whether 
a zero anaphor is anaphoric or not, and antecedent 
identification, which finds the antecedent for an 
anaphoric zero anaphor. To our best knowledge, 
this is the first systematic work dealing with all the 
three sub-tasks via a unified framework. Moreover, 
we release a Chinese zero anaphora corpus of 100 
documents, which adds a layer of annotation to the 
882
manually-parsed sentences in the Chinese Tree-
bank (CTB) 6.0. This is done by assigning ana-
phoric/non-anaphoric zero anaphora labels to the 
null constituents in a parse tree. Finally, this paper 
illustrates the critical role of the structural informa-
tion in zero anaphora resolution and the necessity 
of tree kernel-based methods in modeling such 
structural information. 
The rest of this paper is organized as follows. 
Section 2 briefly describes the related work on 
both zero anaphora resolution and tree kernel-
based anaphora resolution. Section 3 introduces the 
overwhelming problem of zero anaphora in Chi-
nese and our developed Chinese zero anaphora 
corpus, which is available for research purpose. 
Section 4 presents our tree kernel-based unified 
framework in zero anaphora resolution. Section 5 
reports the experimental results. Finally, we con-
clude our work in Section 6. 
2 Related Work 
This section briefly overviews the related work on 
both zero anaphora resolution and tree kernel-
based anaphora resolution. 
2.1 Zero anaphora resolution 
Although zero anaphors are prevalent in many lan-
guages, such as Chinese, Japanese and Spanish, 
there only have a few works on zero anaphora 
resolution. 
Zero anaphora resolution in Chinese 
Converse (2006) developed a Chinese zero anaph-
ora corpus which only deals with zero anaphora 
category ?-NONE- *pro*? for dropped sub-
jects/objects and ignores other categories, such as 
?-NONE- *PRO*? for non-overt subjects in non-
finite clauses. Besides, Converse (2006) proposed 
a rule-based method to resolve the anaphoric zero 
anaphors only. The method did not consider zero 
anaphor detection and anaphoric identification, and 
performed zero anaphora resolution using the 
Hobbs algorithm (Hobbs, 1978), assuming the 
availability of golden anaphoric zero anaphors and 
golden parse trees.  
Instead, Zhao and Ng (2007) proposed feature-
based methods to zero anaphora resolution on the 
same corpus from Convese (2006). However, they 
only considered zero anaphors with explicit noun 
phrase referents and discarded those with split an-
tecedents or referring to events. Moreover, they 
focused on the sub-tasks of anaphoricity determi-
nation and antecedent identification. For zero ana-
phor detection, a simple heuristic rule was 
employed. Although this rule can recover almost 
all the zero anaphors, it suffers from very low pre-
cision by introducing too many false zero anaphors 
and thus leads to low performance in anaphoricity 
determination, much due to the imbalance between 
positive and negative training examples. 
Zero anaphora resolution in Japanese 
Seki et al (2002) proposed a probabilistic model 
for the sub-tasks of anaphoric identification and 
antecedent identification with the help of a verb 
dictionary. They did not perform zero anaphor de-
tection, assuming the availability of golden zero 
anaphors. Besides, their model needed a large-
scale corpus to estimate the probabilities to prevent 
them from the data sparseness problem.  
Isozaki and Hirao (2003) explored some ranking 
rules and a machine learning method on zero 
anaphora resolution. However, they assumed that 
zero anaphors were already detected and each zero 
anaphor?s grammatical case was already deter-
mined by a zero anaphor detector.  
Iida et al (2006) explored a machine learning 
method for the sub-task of antecedent identifica-
tion using rich syntactic pattern features, assuming 
the availability of golden anaphoric zero anaphors. 
Sasano et al (2008) proposed a fully-lexicalized 
probabilistic model for zero anaphora resolution, 
which estimated case assignments for the overt 
case components and the antecedents of zero ana-
phors simultaneously. However, this model needed 
case frames to detect zero anaphors and a large-
scale corpus to construct these case frames auto-
matically.  
For Japanese zero anaphora, we do not see any 
reports about zero anaphora categories. Moreover, 
all the above related works we can find on Japa-
nese zero anaphora resolution ignore zero anaphor 
detection, focusing on either anaphoricity determi-
nation or antecedent identification. Maybe, it is 
easy to detect zero anaphors in Japanese. However, 
it is out of the scope of our knowledge and this 
paper.  
Zero anaphora resolution in Spanish 
As the only work we can find, Ferrandez and Peral 
(2000) proposed a hand-engineered rule-based 
method for both anaphoricity determination and 
883
antecedent identification. That is, they ignored zero 
anaphor detection. Besides, they only dealt with 
zero anaphors that were in the subject position. 
2.2 Tree kernel-based anaphora resolution 
Although there is no research on tree kernel-based 
zero anaphora resolution in the literature, tree ker-
nel-based methods have been explored in tradi-
tional anaphora resolution to certain extent and 
achieved comparable performance with the domi-
nated feature-based ones. One main advantage of 
kernel-based methods is that they are very effec-
tive at reducing the burden of feature engineering 
for structured objects. Indeed, the kernel-based 
methods have been successfully applied to mine 
structural information in various NLP techniques 
and applications, such as syntactic parsing (Collins 
and Duffy 2001; Moschitti 2004), semantic rela-
tion extraction (Zelenko et al 2003; Zhao and 
Grishman 2005; Zhou et al 2007; Qian et al 2008), 
and semantic role labeling (Moschitti 2004).  
Representative works in tree kernel-based 
anaphora resolution include Yang et al (2006) and 
Zhou et al(2008). Yang et al (2006) employed a 
convolution tree kernel on anaphora resolution of 
pronouns. In particular, a document-level syntactic 
parse tree for an entire text was constructed by at-
taching the parse trees of all its sentences to a new-
added upper node. Examination of three parse tree 
structures using different construction schemes 
(Min-Expansion, Simple-Expansion and Full-
Expansion) on the ACE 2003 corpus showed 
promising results. However, among the three con-
structed parse tree structures, there exists no obvi-
ous overwhelming one, which can well cover 
structured syntactic information. One problem with 
this tree kernel-based method is that all the con-
structed parse tree structures are context-free and 
do not consider the information outside the sub-
trees. To overcome this problem, Zhou et al (2008) 
proposed a dynamic-expansion scheme to auto-
matically construct a proper parse tree structure for 
anaphora resolution of pronouns by taking predi-
cate- and antecedent competitor-related informa-
tion into consideration. Besides, they proposed a 
context-sensitive convolution tree kernel to com-
pute the similarity between the parse tree structures. 
Evaluation on the ACE 2003 corpus showed that 
the dynamic-expansion scheme can well cover 
necessary structural information in the parse tree 
for anaphora resolution of pronouns and the con-
text-sensitive convolution tree kernel much outper-
formed other tree kernels. 
3 Task Definition 
This section introduces the phenomenon of zero 
anaphora in Chinese and our developed Chinese 
zero anaphora corpus. 
3.1 Zero anaphora in Chinese 
A zero anaphor is a gap in a sentence, which refers 
to an entity that supplies the necessary information 
for interpreting the gap. Figure 1 illustrates an ex-
ample sentence from Chinese TreeBank (CTB) 6.0 
(File ID=001, Sentence ID=8). In this example, 
there are four zero anaphors denoted as ?i (i=1, 
2, ?4). Generally, zero anaphors can be under-
stood from the context and do not need to be speci-
fied. 
A zero anaphor can be classified into either ana-
phoric or non-anaphoric, depending on whether it 
has an antecedent in the discourse. Typically, a 
zero anaphor is non-anaphoric when it refers to an 
extra linguistic entity (e.g. the first or second per-
son in a conversion) or its referent is unspecified in 
the context. Among the four anaphors in Figure 1, 
zero anaphors ? 1 and ? 4 are non-anaphoric 
while zero anaphors ?2 and ?3 are anaphoric, 
referring to noun phrase ?????/building ac-
tion? and noun phrase ??????/new district 
managing committee? respectively. 
Chinese zero anaphora resolution is very diffi-
cult due to following reasons: 1) Zero anaphors 
give little hints (e.g. number or gender) about their 
possible antecedents. This makes antecedent iden-
tification much more difficult than traditional 
anaphora resolution. 2) A zero anaphor can be ei-
ther anaphoric or non-anaphoric. In our corpus de-
scribed in Section 3.2, about 60% of zero anaphors 
are non-anaphoric. This indicates the importance 
of anaphoricity determination. 3) Zero anaphors 
are not explicitly marked in a text. This indicates 
the necessity of zero anaphor detection, which has 
been largely ignored in previous research and has 
proved to be difficult in our later experiments. 
 
884
 Figure 1: An example sentence from CTB 6.0, which contains four zero anaphors 
(the example is : ???????????????????????????????????????
???????????????????????/ In order to standardize the building action and prevent the 
inorder phenomenon, the standing committee of new zone annouced a series of files to standardize building market 
based on the related provisions of China and Shanghai in time, and the realities of the development of Pudong are 
considered. ) 
3.2 Zero anaphora corpus in Chinese 
Due to lack of an available zero anaphora corpus 
for research purpose, we develop a Chinese zero 
anaphora corpus of 100 documents from CTB 6.0, 
which adds a layer of annotation to the manually-
parsed sentences. Hoping the public availability of 
this corpus can push the research of zero anaphora 
resolution in Chinese and other languages.  
  Figure 2: An example sentence annotated in CTB 6.0 
ID Cate-gory Description 
AZ
As ZAs
1 -NONE-  *T* 
Used in topicalization and 
object preposing con-
structions 
6 742
2 -NONE-  * 
Used in raising and pas-
sive constructions 1 2 
3 -NONE-  *PRO*
Used in control structures. 
The *PRO* cannot be 
substituted by an overt 
constituent. 
219 399
4 -NONE-  *pro* 
for dropped subject or 
object. 394 449
5 -NONE-  *RNR*
Used for right node rais-
ing (Cataphora) 0 36 
6 Others Other unknown empty categories 92 92 
Total (100 documents, 35089 words) 712 1720
Table 1: Statistics on different categories of  zero 
anaphora (AZA and ZA indicates anaphoric zero ana-
phor and zero anaphor respectively) 
885
Figure 2 illustrates an example sentence anno-
tated in CTB 6.0, where the special tag ?-NONE-? 
represents a null constituent and thus the occur-
rence of a zero anaphor. In our developed corpus, 
we need to annotate anaphoric zero anaphors using 
those null constituents with the special tag of ?-
NONE-?. 
Table 1 gives the statistics on all the six catego-
ries of zero anaphora. Since we do not consider 
zero cataphora in the current version, we simply 
redeem them non-anaphoric. It shows that among 
1720 zero anaphors, only 712 (about 40%) are 
anaphoric. This suggests the importance of ana-
phoricity determination in zero anaphora resolution. 
Table 3 further shows that, among 712 anaphoric 
zero anaphors, 598 (84%) are intra-sentential and 
no anaphoric zero anaphors have their antecedents 
occurring two sentences before. 
Sentence distance AZAs
0 598 
1 114 
>=2 0 
Table 3 Distribution of anaphoric zero anaphors over 
sentence distances 
Figure 3 shows an example in our corpus corre-
sponding to Figure 2. For a non-anaphoric zero 
anaphor, we replace the null constituent with ?E-i 
NZA?, where i indicates the category of zero 
anaphora, with ?1? referring to ?-NONE *T*? 
etc. For an anaphoric zero anaphor, we replace it 
with ?E-x-y-z-i AZA?, where x indicates the sen-
tence id of its antecedent, y indicates the position 
of the first word of its antecedent in the sentence, z 
indicates the position of the last word of its antece-
dent in the sentence, and i indicates the category id 
of the null constituent. 
 Figure 3: an example sentence annotated in our corpus 
4 Tree Kernel-based Framework 
This section presents the tree kernel-based unified 
framework for all the three sub-tasks in zero 
anaphora resolution. For each sub-task, different 
parse tree structures are constructed. In particular, 
the context-sensitive convolution tree kernel, as 
proposed in Zhou et al (2008), is employed to 
compute the similarity between two parse trees via 
the SVM toolkit SVMLight. 
In the tree kernel-based framework, we perform 
the three sub-tasks, zero anaphor detection, ana-
phoricity determination and antecedent identifica-
tion in a pipeline manner. That is, given a zero 
anaphor candidate Z, the zero anaphor detector is 
first called to determine whether Z is a zero ana-
phor or not. If yes, the anaphoricity determiner is 
then invoked to determine whether Z is an ana-
phoric zero anaphor. If yes, the antecedent identi-
fier is finally awaked to determine its antecedent. 
In the future work, we will explore better ways of 
integrating the three sub-tasks (e.g. joint learning). 
4.1 Zero anaphor detection 
At the first glance, it seems that a zero anaphor can 
occur between any two constituents in a parse tree. 
Fortunately, an exploration of our corpus shows 
that a zero anaphor always occurs just before a 
predicate1 phrase node (e.g. VP). This phenome-
non has also been employed in Zhao and Ng (2007) 
in generating zero anaphor candidates. In particular, 
if the predicate phrase node occurs in a coordinate 
structure or is modified by an adverbial node, we 
only need to consider its parent. As shown in Fig-
ure 1, zero anaphors may occur immediately to the 
left of??/guide, ??/avoid, ??/appear, ??
/according to, ?? /combine, ?? /promulgate, 
which cover the four true zero anaphors. Therefore, 
it is simple but reliable in applying above heuristic 
rules to generate zero anaphor candidates. 
Given a zero anaphor candidate, it is critical to 
construct a proper parse tree structure for tree ker-
nel-based zero anaphor detection. The intuition 
behind our parser tree structure for zero anaphor 
detection is to keep the competitive information 
                                                          
1 The predicate in Chinese can be categorized into verb predi-
cate, noun predicate and preposition predicate. In our corpus, 
about 93% of the zero anaphors are driven by verb predicates. 
In this paper, we only explore zero anaphors driven by verb 
predicates. 
886
about the predicate phrase node and the zero ana-
phor candidate as much as possible. In particular, 
the parse tree structure is constructed by first keep-
ing the path from the root node to the predicate 
phrase node and then attaching all the immediate 
verbal phrase nodes and nominal phrase nodes. 
Besides, for the sub-tree rooted by the predicate 
phrase node, we only keep those paths ended with 
verbal leaf nodes and the immediate verbal and 
nominal nodes attached to these paths. Figure 4 
shows an example of the parse tree structure corre-
sponding to Figure 1 with the zero anaphor candi-
date ?2 in consideration. 
During training, if a zero anaphor candidate has 
a counterpart in the same position in the golden 
standard corpus (either anaphoric or non-
anaphoric), a positive instance is generated. Oth-
erwise, a negative instance is generated. During 
testing, each zero anaphor candidate is presented to 
the learned zero anaphor detector to determine 
whether it is a zero anaphor or not. Besides, since a 
zero anaphor candidate is generated when a predi-
cate phrase node appears, there may be two or 
more zero anaphor candidates in the same position. 
However, there is normally one zero anaphor in the 
same position. Therefore, we just select the one 
with maximal confidence as the zero anaphor in 
the position and ignore others, if multiple zero 
anaphor candidates occur in the same position. 
 Figure 4: An example parse tree structure for zero ana-
phor detection with the predicate phrase node and the 
zero anaphor candidate ?2  in black 
4.2 Anaphoricity determination 
To determine whether a zero anaphor is anaphoric 
or not, we limit the parse tree structure between the 
previous predicate phrase node and the following 
predicate phrase node. Besides, we only keep those 
verbal phrase nodes and nominal phrase nodes. 
Figure 5 illustrates an example of the parse tree 
structure for anaphoricity determination, corre-
sponding to Figure 1 with the zero anaphor ?2 in 
consideration.   
VP
IPVV
?? NP-SBJ VP
NN
NP-OBJ
?? NP
??
VV
prevent
appear
phenomenon  
Figure 5: An example parse tree structure for anaphoric-
ity determination with the zero anaphor ?2 in consid-
eration 
4.3 Antecedent identification 
To identify an antecedent for an anaphoric zero 
anaphor, we adopt the Dynamic Expansion Tree, 
as proposed in Zhou et al (2008), which takes 
predicate- and antecedent competitor-related in-
formation into consideration. Figure 6 illustrates an 
example parse tree structure for antecedent identi-
fication, corresponding to Figure 1 with the ana-
phoric zero anaphor ? 2 and the antecedent 
candidate ?????/building action? in consid-
eration.  
 Figure 6: An example parse tree structure for antecedent 
identification with the anaphoric zero anaphor ?2 and 
the antecedent candidate ?????/building action? in 
consideration 
In this paper, we adopt a similar procedure as 
Soon et al (2001) in antecedent identification. Be-
887
sides, since all the anaphoric zero anaphors have 
their antecedents at most one sentence away, we 
only consider antecedent candidates which are at 
most one sentence away. In particular, a document-
level parse tree for an entire document is con-
structed by attaching the parse trees of all its sen-
tences to a new-added upper node, as done in Yang 
et al (2006), to deal with inter-sentential ones. 
5 Experimentation and Discussion 
We have systematically evaluated our tree kernel-
based unified framework on our developed Chi-
nese zero anaphora corpus, as described in Section 
3.2. Besides, in order to focus on zero anaphor 
resolution itself and compare with related work, all 
the experiments are done on golden parse trees 
provided by CTB 6.0. Finally, all the performances 
are achieved using 5-fold cross validation. 
5.1 Experimental results 
Zero anaphor detection 
Table 4 gives the performance of zero anaphor de-
tection, which achieves 70.05%, 83.24% and 76.08 
in precision, recall and F-measure, respectively. 
Here, the lower precision is much due to the simple 
heuristic rules used to generate zero anaphors can-
didates. In fact, the ratio of positive and negative 
instances reaches about 1:12. However, this ratio is 
much better than that (1:30) using the heuristic rule 
as described in Zhao and Ng (2007). It is also 
worth to point out that lower precision higher re-
call is much beneficial than higher precision lower 
recall as higher recall means less filtering of true 
zero anaphors and we can still rely on anaphoricity 
determination to filter out those false zero ana-
phors introduced by lower precision in zero ana-
phor detection. 
P% R% F 
70.05 83.24 76.08 
Table 4: Performance of zero anaphor detection 
Anaphoricity determination 
Table 5 gives the performance of anaphoricity de-
termination. It shows that anaphoricity determina-
tion on golden zero anaphors achieves very good 
performance of 89.83%, 84.21% and 86.93 in pre-
cision, recall and F-measure, respectively, although 
useful information, such as gender and number, is 
not available in anaphoricity determination. This 
indicates the critical role of the structural informa-
tion in anaphoricity determination of zero anaphors. 
It also shows that anaphoricity determination on 
automatic zero anaphor detection achieves 77.96%, 
53.97% and 63.78 in precision, recall and F-
measure, respectively. In comparison with ana-
phoricity determination on golden zero anaphors, 
anaphoricity determination on automatic zero ana-
phor detection lowers the performance by about 23 
in F-measure. This indicates the importance and 
the necessity for further research in zero anaphor 
detection. 
 P% R% F 
golden zero anaphors 89.83 84.21 86.93
zero anaphor detection 77.96 53.97 63.78
Table 5: Performance of anaphoricity determination 
Antecedent identification 
Table 6 gives the performance of antecedent iden-
tification given golden zero anaphors. It shows that 
antecedent identification on golden anaphoric zero 
anaphors achieves 88.93%, 68.36% and 77.29 in 
precision, recall and F-measure, respectively. It 
also shows that antecedent identification on auto-
matic anaphoricity determination achieves 80.38%, 
47.28% and 59.24 in precision, recall and F-
measure, respectively, with a decrease of about 8% 
in precision, about 21% in recall and about 18% in 
F-measure, in comparison with antecedent identifi-
cation on golden anaphoric zero anaphors. This 
indicates the critical role of anaphoricity determi-
nation in antecedent identification.  
 
 P% R% F 
golden anaphoric zero ana-
phors 
88.90 68.36 77.29
anaphoricity determination 80.38 47.28 59.54
Table 6: Performance of antecedent identification given 
golden zero anaphors 
Overall: zero anaphora resolution 
Table 7 gives the performance of overall zero 
anaphora resolution with automatic zero anaphor 
detection, anaphoricity determination and antece-
dent identification. It shows that our tree kernel-
based framework achieves 77.66%, 31.74% and 
45.06 in precision, recall and F-measure. In com-
parison with Table 6, it shows that the errors 
caused by automatic zero anaphor detection de-
crease the performance of overall zero anaphora 
resolution by about 14 in F-measure, in compari-
son with golden zero anaphors. 
888
 
P% R% F 
77.66 31.74 45.06 
Table 7: Performance of zero anaphora resolution 
Figure 7 shows the learning curve of zero 
anaphora resolution with the increase of the num-
ber of the documents in experimentation, with the 
horizontal axis the number of the documents used 
and the vertical axis the F-measure. It shows that 
the F-measure is about 42.5 when 20 documents 
are used in experimentation. This figure increases 
very fast to about 45 when 50 documents are used 
while further increase of documents only slightly 
improves the performance.  
auto ZA and AZA
41
42
43
44
45
46
20 30 40 50 60 70 80 90 100  Figure 7: Learning curve of zero anaphora resolution 
over the number of the documents in experimentation 
Table 8 shows the detailed performance of zero 
anaphora resolution over different sentence dis-
tance between a zero anaphor and its antecedent. It 
is expected that both the precision and the recall of 
intra-sentential resolution are much higher than 
those of inter-sentential resolution, largely due to 
the much more dependency of intra-sentential an-
tecedent identification on the parse tree structures.  
Sentence distance P% R% F 
0 85.12 33.28 47.85
1 46.55 23.64 31.36
2 - - - 
Table 8: Performance of zero anaphora resolution over 
sentence distances 
Table 9 shows the detailed performance of zero 
anaphora resolution over the two major zero 
anaphora categories, ?-NONE- *PRO*? and ?-
NONE- *pro*?. It shows that our tree kernel-based 
framework achieves comparable performance on 
them, both with high precision and low recall. This 
is in agreement with the overall performance. 
ID Category P% R% F 
3 -NONE-  *PRO* 79.37 34.23 47.83
4 -NONE-  *pro* 77.03 30.82 44.03
Table 9: Performance of zero anaphora resolution over 
major zero anaphora categories 
5.2 Comparison with previous work 
As a representative in Chinese zero anaphora reso-
lution, Zhao and Ng (2007) focused on anaphoric-
ity determination and antecedent identification 
using feature-based methods. In this subsection, we 
will compare our tree kernel-based framework with 
theirs in details. 
Corpus 
Zhao and Ng (2007) used a private corpus from 
Converse (2006). Although their corpus contains 
205 documents from CBT 3.0, it only deals with 
the zero anaphors under the zero anaphora cate-
gory of ?-NONE- *pro*? for dropped sub-
jects/objects. Furthermore, Zhao and Ng (2007) 
only considered zero anaphors with explicit noun 
phrase referents and discarded zero anaphors with 
split antecedents (i.e. split into two separate noun 
phrases) or referring to entities. As a result, their 
corpus is only about half of our corpus in the num-
ber of zero anaphors and anaphoric zero anaphors. 
Besides, our corpus deals with all the types of zero 
anaphors and all the categories of zero anaphora 
except zero cataphora. 
Method 
Zhao and Ng (2007) applied feature-based methods 
on anaphoricity determination and antecedent iden-
tification with most of features structural in nature. 
For zero anaphor detection, they used a very sim-
ple heuristic rule to generate zero anaphor candi-
dates. Although this rule can recover almost all the 
zero anaphors, it suffers from very low precision 
by introducing too many false zero anaphors and 
thus may lead to low performance in anaphoricity 
determination, much due to the imbalance between 
positive and negative training examples with the 
ratio up to about 1:30.  
In comparison, we propose a tree kernel-based 
unified framework for all the three sub-tasks in 
zero anaphora resolution. In particular, different 
parse tree structures are constructed for different 
sub-tasks. Besides, a context sensitive convolution 
tree kernel is employed to directly compute the 
similarity between the parse trees. 
For fair comparison with Zhao and Ng (2007), 
we duplicate their system and evaluate it on our 
developed Chinese zero anaphora corpus, using the 
same J48 decision tree learning algorithm in Weka 
and the same feature sets for anaphoricity determi-
nation and antecedent identification.  
889
Table 10 gives the performance of the feature-
based method, as described in Zhao and Ng (2007), 
in anaphoricity determination on our developed 
corpus. In comparison with the tree kernel-based 
method in this paper, the feature-based method 
performs about 16 lower in F-measure, largely due 
to the difference in precision (63.61% vs 89.83%), 
when golden zero anaphors are given. It also 
shows that, when our tree kernel-based zero ana-
phor detector is employed 2 , the feature-based 
method gets much lower precision with a gap of 
about 31%, although it achieves slightly higher 
recall.  
 P% R% F 
golden zero anaphors 63.61 79.71 70.76
zero anaphor detection 46.17 57.69 51.29
Table 10: Performance of the feature-based method 
(Zhao and Ng 2007) in anaphoricity determination on 
our developed corpus 
 P% R% F 
golden anaphoric zero ana-
phors 
77.45 51.97 62.20 
golden zero anaphpors and 
feature-based anaphoricity 
determination 
75.17 29.69 42.57 
overall: tree kernel-based 
zero anaphor detection and 
feature-based anaphoricity 
determination 
70.67 23.64 35.43 
Table 11: Performance of the feature-based method 
(Zhao and Ng 2007) in antecedent identification on our 
developed corpus  
Table 11 gives the performance of the feature-
based method, as described in Zhao and Ng (2007), 
in antecedent identification on our developed cor-
pus. In comparison with our tree kernel-based 
method, it shows that 1) when using golden ana-
phoric zero anaphors, the feature-based method 
performs about 11%, 17% and 15 lower in preci-
sion, recall and F-measure, respectively; 2) when 
golden zero anaphors are given and feature-based 
anaphoricity determination is applied, the feature-
based method performs about 5%, 18% and 17 
lower in precision, recall and F-measure, respec-
tively; and 3) when tree kernel-based zero anaphor 
detection and feature-based anaphoricity determi-
nation are applied, the feature-based method per-
                                                          
2 We do not apply the simple heuristic rule, as adopted in Zhao 
and Ng (2007), in zero anaphor detection, due to its much 
lower performance, for fair comparison on the other two sub-
tsaks.. 
forms about 7%, 8% and 10 lower in precision, 
recall and F-measure, respectively.  
In summary, above comparison indicates the 
critical role of the structural information in zero 
anaphora resolution, given the fact that most of 
features in the feature-based methods in Zhao and 
Ng (2007) are also structural, and the necessity of 
tree kernel methods in modeling such structural 
information, even if more feature engineering in 
the feature-based methods may improve the per-
formance to a certain extent. 
6 Conclusion and Further Work 
This paper proposes a tree kernel-based unified 
framework for zero anaphora resolution, which can 
be divided into three sub-tasks: zero anaphor de-
tection, anaphoricity determination and antecedent 
identification. 
The major contributions of this paper include: 1) 
We release a wide-coverage Chinese zero anaphora 
corpus of 100 documents, which adds a layer of 
annotation to the manually-parsed sentences in the 
Chinese Treebank (CTB) 6.0. 2) To our best 
knowledge, this is the first systematic work dealing 
with all the three sub-tasks in Chinese zero anaph-
ora resolution via a unified framework. 3) Em-
ployment of tree kernel-based methods indicates 
the critical role of the structural information in zero 
anaphora resolution and the necessity of tree kernel 
methods in modeling such structural information.  
In the future work, we will systematically evalu-
ate our framework on automatically-generated 
parse trees, construct more effective parse tree 
structures for different sub-tasks of zero anaphora 
resolution, and explore joint learning among the 
three sub-tasks.  
Besides, we only consider zero anaphors driven 
by a verb predicate phrase node in this paper. In 
the future work, we will consider other situations. 
Actually, among the remaining 7% zero anaphors, 
about 5% are driven by a preposition phrase (PP) 
node, and 2% are driven by a noun phrase (NP) 
node.  However, our preliminary experiments show 
that simple inclusion of those PP-driven and NP-
driven zero anaphors will largely increase the im-
balance between positive and negative instances, 
which significantly decrease the performance.  
Finally, we will devote more on further develop-
ing our corpus, with the ultimate mission of anno-
tating all the documents in CBT 6.0.    
890
Acknowledgments 
This research was supported by Projects 60873150,  
90920004 and 61003153 under the National Natu-
ral Science Foundation of China. 
References  
S. Converse. 2006. Pronominal Anaphora Resolution in 
Chinese. Ph.D. Thesis, Department of Computer and 
Information Science. University of Pennsylvania. 
M. Collins and N. Duffy. 2001. Convolution kernels for 
natural language. NIPS?2001:625-632.  
A. Ferrandez and J. Peral. 2000. A computational ap-
proach to zero-pronouns in Spanish. ACL'2000:166-
172. 
R. Iida, K. Inui, and Y. Matsumoto. 2006. Exploiting 
syntactic patterns as clues in zero-anaphora resolu-
tion. COLING-ACL'2006:625-632 
H. Isozaki and T. Hirao. 2003. Japanese zero pronoun 
resolution based on ranking rules and machine 
learning. EMNLP'2003:184-191 
F. Kong, G.D. Zhou and Q.M. Zhu. 2009 Employing the 
Centering Theory in Pronoun Resolution from the 
Semantic Perspective. EMNLP?2009: 987-996 
C. N. Li and S. A. Thompson. 1979. Third-person pro-
nouns and zero-anaphora in Chinese discourse. Syn-
tax and Semantics, 12:311-335. 
W. Li. 2004. Topic chains in Chinese discourse. Dis-
course Processes, 37(1):25-45. 
A. Moschitti. 2004. A Study on Convolution Kernels for 
Shallow Semantic Parsing, ACL?2004.  
L.H. Qian, G.D. Zhou, F. Kong, Q.M. Zhu and P.D. 
Qian. 2008. Exploiting constituent dependencies for 
tree kernel-based semantic relation extraction. 
COLING?2008:697-704  
K. Seki, A. Fujii, and T. Ishikawa. 2002. A probabilistic 
method for analyzing Japanese anaphora intergrat-
ing zero pronoun detection and resolution. 
COLING'2002:911-917 
R. Sasano. D. Kawahara and S. Kurohashi. 2008. A 
fully-lexicalized probabilistic model for Japanese 
zero anaphora resolution. COLING'2008:769-776 
W.M. Soon, H.T. Ng and D. Lim. 2001. A machine 
learning approach to coreference resolution of noun 
phrase. Computational Linguistics, 2001, 27(4):521-
544. 
V. Ng and C. Cardie 2002. Improving machine learning 
approaches to coreference resolution. ACL?2002: 
104-111 
X.F. Yang, G.D. Zhou, J. Su and C.L. Chew. 2003. 
Coreference Resolution Using Competition Learning 
Approach. ACL?2003:177-184 
X.F. Yang, J. Su and C.L. Tan 2008. A Twin-Candidate 
Model for Learning-Based Anaphora Resolution. 
Computational Linguistics 34(3):327-356 
N. Xue, F. Xia, F.D. Chiou and M. Palmer. 2005. The 
Penn Chinese TreeBank: Phrase structure annotation 
of a large corpus. Natural Language Engineering, 
11(2):207-238. 
X.F. Yang, J. Su and C.L. Tan. 2006. Kernel-based 
pronoun resolution with structured syntactic knowl-
edge. COLING-ACL'2006:41-48. 
D. Zelenko, A. Chinatsu and R. Anthony. 2003. Kernel 
methods for relation extraction. Journal of Machine 
Learning Research, 3(2003):1083-1106  
S. Zhao and H.T. Ng. 2007. Identification and Resolu-
tion of Chinese Zero Pronouns: A Machine Learning 
Approach. EMNLP-CoNLL'2007:541-550. 
S. Zhao and R. Grishman. 2005. Extracting relations 
with integrated information using kernel methods. 
ACL?2005:419-426  
G.D. Zhou, F. Kong and Q.M. Zhu. 2008. Context-
sensitive convolution tree kernel for pronoun resolu-
tion. IJCNLP'2008:25-31 
G.D. Zhou, M. Zhang, D.H. Ji and Q.M. Zhu. 2007. 
Tree kernel-based relation extraction with context-
sensitive structured parse tree information. EMNLP-
CoNLL?2007:728-736  
891
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 278?288,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Exploiting Zero Pronouns to Improve Chinese Coreference Resolution
Fang Kong
Department of Computer Science
National University of Singapore
13 Computing Drive
Singapore 117417
dcskf@nus.edu.sg
Hwee Tou Ng
Department of Computer Science
National University of Singapore
13 Computing Drive
Singapore 117417
nght@comp.nus.edu.sg
Abstract
Coreference resolution plays a critical role
in discourse analysis. This paper focuses
on exploiting zero pronouns to improve Chi-
nese coreference resolution. In particular, a
simplified semantic role labeling framework
is proposed to identify clauses and to detect
zero pronouns effectively, and two effective
methods (refining syntactic parser and refining
learning example generation) are employed to
exploit zero pronouns for Chinese coreference
resolution. Evaluation on the CoNLL-2012
shared task data set shows that zero pronouns
can significantly improve Chinese coreference
resolution.
1 Introduction
As one of the most important tasks in discourse
analysis, coreference resolution aims to link a given
mention (i.e., entity or event) to its co-referring ex-
pression in a text and has been a focus of research in
natural language processing (NLP) for decades.
Over the last decade, various machine learning
techniques have been applied to coreference reso-
lution and have performed reasonably well (Soon
et al, 2001; Ng and Cardie, 2002; Fernandes et al,
2012). Current techniques rely primarily on surface
level features such as string match, syntactic features
such as apposition, and shallow semantic features
such as number, gender, semantic class, etc.
Despite similarities between Chinese and English,
there are differences that have a significant impact
on coreference resolution. In this paper, we focus
on exploiting one of the key characteristics of Chi-
nese text, zero pronouns (ZPs), to improve Chinese
coreference resolution. In particular, a simplified se-
mantic role labeling (SRL) framework is proposed
to identify Chinese clauses and to detect zero pro-
nouns effectively, and two effective methods are em-
ployed to exploit zero pronouns for Chinese corefer-
ence resolution. Experimental results show the ef-
fectiveness of our approach in improving the perfor-
mance of Chinese coreference resolution. Our work
is novel in that it is the first work that incorporates
the use of zero pronouns to significantly improve
Chinese coreference resolution
The rest of this paper is organized as follows.
Section 2 describes our baseline Chinese corefer-
ence resolution system. Section 3 motivates how
the detection of zero pronouns can improve Chinese
coreference resolution, using an illustrating exam-
ple. Section 4 presents our approach to detect zero
pronouns. Section 5 proposes two methods to ex-
ploit zero pronouns to improve Chinese coreference
resolution, based on a corpus study and preliminary
experiments. Section 6 briefly outlines the related
work. Finally, we conclude our work in Section 7.
2 Chinese Coreference Resolution
According to Webber (1978), coreference resolu-
tion can be decomposed into two complementary
subtasks: (1) anaphoricity determination: decid-
ing whether a given noun phrase (NP) is anaphoric
or not; and (2) anaphora resolution: linking to-
gether multiple mentions of a given entity in the
world. Our Chinese coreference resolution system
also contains these two components. Using the train-
ing data set of CoNLL-2012 shared task, we first
train an anaphoricity classifier to determine whether
278
a mention is anaphoric or not, and then employ
an independently-trained coreference resolution sys-
tem to resolve those mentions which are classi-
fied as anaphoric. The lack of gender and number
makes both anaphoricity determination and corefer-
ence resolution in Chinese more difficult.
2.1 Anaphoricity Determination
Since only the mentions that take part in coreference
chains are annotated in the CoNLL-2012 shared
task data set, we first generate a high-recall, low-
precision mention extraction module to extract as
many mentions as possible. The mention extrac-
tion module relies mainly on syntactic parse trees.
We extract all NP nodes, QP (quantifier phrase, i.e.,
complex amount/measure phrase) nodes, and all ter-
minals with part-of-speech tags PN (pronoun) and
NR (proper noun) in parse trees to form a mention
candidate set. Then, we employ some rules to re-
move unlikely mentions, e.g., those which contain
(1) measure words such as ???/one year? and
???/one time?; (2) named entities whose cat-
egories are PERCENT, MONEY, QUANTITY, and
CARDINAL; (3) interrogative pronouns such as ?
??/what? and???/where?.
After pruning, we employ a learning-based
method to train an independent classifier to deter-
mine whether the remaining mentions are anaphoric.
Table 1 lists all the features employed in our
anaphoricity determination system.
2.2 Coreference Resolution
Our Chinese coreference resolution system adopts
the same learning-based model and the same set of
12 features as Soon et al (2001). Considering the
special characteristics of conversation and web texts
(i.e., a large proportion of personal pronouns and the
organization of a text into several parts1) and prepar-
ing for dealing with zero pronouns, we add some
features shown in Table 2.2
1A text in the CoNLL-2012 data set is broken down into
different ?parts?.
2AN denotes anaphor, CA denotes antecedent candidate, IP
denotes a simple clause, and CP denotes a clause headed by a
complementizer. For the feature ANPronounRanking, the rel-
ative ranking of a given pronoun is based on its semantic role
and surface position, and we assign the highest rank to zero pro-
nouns, similar to Kong et al (2009).
R P F
GS 76.32 87.14 81.37
Auto 64.87 78.42 71.00
Table 3: Performance of anaphoricity determination on
the CoNLL-2012 test set
R P F
AM
Mention Detection 65.26 67.20 66.22
MUC 51.64 61.82 56.27
BCUBED 73.40 80.38 76.73
CEAF 53.16 45.66 49.13
Average 60.71
GMB
Mention Detection 82.01 69.58 75.29
MUC 76.21 66.18 70.84
BCUBED 76.15 86.59 81.04
CEAF 59.75 50.52 54.75
Average 68.88
GM
Mention Detection 79.80 100.00 88.77
MUC 80.86 85.48 83.11
BCUBED 73.66 91.94 81.79
CEAF 67.54 64.87 66.18
Average 77.02
Table 4: Performance of our Chinese coreference resolu-
tion system on the CoNLL-2012 test set
2.3 Results and Analysis
All experiments in this section are conducted on the
CoNLL-2012 shared task data set. The SVM-light
toolkit (Joachims, 1999) with radial basis kernel
and default learning parameters is employed in both
anaphoricity determination and coreference resolu-
tion.
Table 3 reports the performance of anaphoricity
determination on the CoNLL-2012 test set using
gold-standard parse trees (GS) and automatic parse
trees (Auto). All performance figures in this paper
are given in percentages. The results show that using
both gold parse trees and automatic parse trees, our
anaphoricity determination system achieves higher
precision than recall. In comparison with using gold
parse trees, precision decreases by about 9% and re-
call 11% on automatic parse trees.
Table 4 reports the performance of our Chinese
coreference resolution system on the CoNLL-2012
test set under three different experimental settings:
with automatic mentions (AM), with gold mention
279
Feature Description
NPType Type of the current mention (pronoun, demonstrative, proper NP).
NPNumber Number of the current mention (singular, plural).
NPGender Gender of the current mention (male, female).
IsHeadWord Whether the current mention is the same as its headword.
StrMatch Whether there is a string match between the current mention and another phrase in the
previous context.
AliasMatch Whether the current mention is a name alias or abbreviation of another phrase in the
previous context.
Appositive Whether the current mention and another phrase in the previous context are in an
appositive relation.
NestIn Whether another NP is nested in the current mention.
NestOut Whether the current mention is nested in another NP.
FirstNP Whether the current mention is the first NP of the sentence.
FrontDistance The number of words between the current mention and the nearest previous clause.
BackDistance The number of words between the current mention and the nearest following clause.
WordSense Whether the current mention and another phrase in the previous context have the same
word sense. Word sense annotation is provided in the CoNLL-2012 data set, based on
the IMS software (Zhong and Ng, 2010).
Table 1: Features employed in our anaphoricity determination system
Feature Description
AN/CAPronounType Whether the anaphor or the antecedent candidate is a zero pronoun, first per-
son, second person, third person, neutral pronoun, or others. In our corefer-
ence resolution system, a zero pronoun is viewed as a kind of special pro-
noun.
AN/CAGrammaticalRole Whether the anaphor or the antecedent candidate is a subject, object, or oth-
ers.
AN/CAOwnerClauseType Whether the anaphor or the antecedent candidate is in a matrix clause, an
independent clause, a subordinate clause, or none of the above.
AN/CARootPath Whether the path of nodes from the anaphor (or the antecedent candidate) to
the root of the parse tree contains NP, IP, CP, or VP.
ANPronounRanking Whether the anaphor is a pronoun and is ranked highest among the pronouns
(including zero pronouns) of the sentence.
AN/CAClosestNP Whether the antecedent candidate is the closest preceding NP of the anaphor.
AN/CAPartDistance This feature captures the distance (in parts) between the antecedent candidate
and the anaphor. If they are in the same part, the value is 0; if they are one
part apart, the value is 1; and so on.
AN/CASameSpeaker Whether the antecedent candidate and the anaphor appear in sentences spo-
ken by the same person.
Table 2: Additional features employed in our Chinese coreference resolution system
280
boundaries (GMB), and with gold mentions (GM).
From the results, we find that:
? Using automatic mentions, our system achieves
56.27, 76.73, and 49.13 in F-measure on MUC,
BCUBED, and CEAF evaluation metrics, re-
spectively.
? Using gold mention boundaries improves the
performance of our system by 14.57, 4.31, and
5.62 in F-measure, due to large gains in both
recall and precision. We also find that using
gold mention boundaries can boost the recall
of mention detection. As described above, our
anaphoricity determination model relies mainly
on the parser. Using gold mention boundaries
can improve the parser performance. Thus
our coreference resolution system can benefit
much from using gold mention boundaries (es-
pecially the recall).
? Employing gold mentions further boosts our
system significantly. In comparison with using
gold mention boundaries, the performance im-
provement is attributed more to an increase in
precision.
In comparison with the three best systems of
CoNLL-2012 in the Chinese closed track (shown in
Table 5), considering average F-measure, we find
that using automatic mentions, our system is only
inferior to that of Chen and Ng (2012); using gold
mention boundaries, our system achieves the best
performance; and using gold mentions, our system is
only a little worse than that of Chen and Ng (2012).
3 Motivation
In order to analyze the impact of zero pronouns on
Chinese coreference resolution, we first use the re-
leased OntoNotes v5.0 data (i.e., the training and de-
velopment portions of the CoNLL-2012 shared task)
in a corpus study.
Statistics show that anaphoric zero pronouns ac-
count for 10.7% of the mentions in coreference
chains in the training data, while in the develop-
ment data, the proportion is 11.3%. The experi-
mental results of our Chinese coreference resolution
system (i.e., the baseline) show that using both gold
mention boundaries and gold mentions significantly
improves system performance, especially for recall,
largely due to improved parser performance. We
then analyze the impact of zero pronouns on Chi-
nese syntactic parsing. As a preliminary exploration,
we integrate Chinese zero pronouns into the Berke-
ley parser (Petrov et al, 2006), experimenting with
gold-standard or automatically determined zero pro-
nouns kept or stripped off (using gold-standard word
segmentation provided in the CoNLL-2012 data).
The results indicate that given gold-standard zero
pronouns, parsing performance improves by 1.8%
in F-measure. Using automatically determined zero
pronouns by our zero pronoun detector to be intro-
duced in Section 4, parsing performance also im-
proves by 1.4% in F-measure.
In order to illustrate the impact of zero pronouns
on parsing performance, consider the following ex-
ample:3
Example (1):
????????????
#?????????#??????
???
...
????????????????
????????????#????
????
(In future, we have a reconstruction
plan.
Divide the park into seven regions, and
bring some more attractions.
. . .
Now we wait for approval of the gov-
ernment before implementing this plan
again. It is expected that work can start
next year.)
Without considering zero pronouns, the parse tree
of the second sentence output by the Berkeley parser
is shown in Figure 1.
Prior to parsing, using our zero pronoun detector
to be introduced in Section 4, the presence of zero
pronouns (denoted by #) can be detected. Figure 2
3In this paper, zero pronouns are denoted by ?#? and men-
tions in the same coreference chain are shown in bold for all
examples.
281
MD MUC BCUBED CEAF Avg
AM
(Chen and Ng, 2012) 71.64 62.21 73.55 50.97 62.24
(Yuan et al, 2012) 68.15 60.33 72.90 48.83 60.69
(Bjo?rkelund and Farkas, 2012) 66.37 58.61 73.10 48.19 59.97
Our baseline system (without ZPs) 66.22 56.27 76.73 49.13 60.71
Our refined system (with auto ZPs) 70.33 59.58 78.15 51.47 63.07
GMB
(Chen and Ng, 2012) 80.45 71.43 77.04 57.17 68.55
(Yuan et al, 2012) 74.02 66.44 75.02 51.81 64.42
(Bjo?rkelund and Farkas, 2012) 71.02 63.56 74.52 50.20 62.76
Our baseline system (without ZPs) 75.29 70.84 81.04 54.75 68.88
Our refined system (with auto ZPs) 75.77 72.62 81.45 58.04 70.70
GM
(Chen and Ng, 2012) 91.73 83.77 81.15 68.38 77.77
(Yuan et al, 2012) 89.95 82.79 79.79 65.58 76.05
(Bjo?rkelund and Farkas, 2012) 83.47 76.85 76.30 56.61 69.92
Our baseline system (without ZPs) 88.77 83.11 81.79 66.18 77.02
Our refined system (with auto ZPs) 91.49 83.46 82.43 65.88 77.26
Table 5: Performance (F-measure) of the three best Chinese coreference resolution systems on the CoNLL-2012 test
set
shows the new parse tree, which includes the de-
tected zero pronouns, output by the Berkeley parser
on the same sentence. Comparing these two parse
trees, we can see that the detected zero pronouns
contribute to better division of clauses and improved
parsing performance, which in turn leads to im-
proved Chinese coreference resolution.
Detecting the presence of zero pronouns also
helps to improve local salience modeling, leading to
improved Chinese coreference resolution. Long sen-
tences containing multiple clauses occur more fre-
quently in Chinese compared to English. Further-
more, a coreference chain can span many sentences.
Zero pronouns can occur not only within one sen-
tence (e.g., the first and second zero pronouns of Ex-
ample (1)), but can also be scattered across multiple
sentences (e.g., the first and third zero pronouns of
Example (1)). The subjects in the second sentence
of Example (1) are omitted.4 Detection of zero pro-
nouns improves local salience modeling, and leads
to the correct identification of all the noun phrases
of the coreference chain in Example (1).
4 Zero Pronoun Detection
Empty elements are those nodes in a parse tree that
do not have corresponding surface words or phrases.
Although empty elements exist in many languages
4In Chinese, pro-dropped subjects account for more than
36% of subjects in sentences (Kim, 2000).
and serve different purposes, they are particularly
important for some languages, such as Chinese,
where subjects and objects are frequently dropped to
keep a discourse concise. Among empty elements,
type *pro*, namely zero pronoun, is either used for
dropped subjects or objects, which can be recovered
from the context (anaphoric), or it is of little interest
for the reader or listener to know (non-anaphoric). In
the Chinese Treebank, type *pro* constitutes about
20% (Yang and Xue, 2010), and more than 85% of
them are anaphoric (Kong and Zhou, 2010). Thus,
zero pronouns are very important in bridging the in-
formation gap in a Chinese text. In this section, we
will introduce our zero pronoun detector.
In Chinese, a zero pronoun always occurs just be-
fore a predicate phrase node (e.g., VP). In particular,
if the predicate phrase node occurs in a coordinate
structure or is modified by an adverbial node, we
only need to consider its parent. A simplified seman-
tic role labeling (SRL) framework (only including
predicate recognition, argument pruning, and argu-
ment identification) is adopted to identify the pred-
icate phrase subtree (Xue, 2008), i.e., the minimal
subtree governed by a predicate and all its argu-
ments.
We carry out zero pronoun detection for every
predicate phrase subtree in an iterative manner from
a parse tree, i.e., determining whether there is a
zero pronoun before the given predicate phrase sub-
282
IP






HH
H
HH
H
HH
H
VP






H
HH
H
HH
H
H
VP


HH
H
VP
H
VV
?
NP
NN
??
VP
 HH
VV
?
NP
 HH
QP
H
CD
?
CLP
M
?
NP
NN
??
PU
?
VP



HH
H
HH
VV
??
NP


HH
H
DNP


HH
H
QP
 HH
ADVP
AD
?
QP
H
CD
?
CLP
M
?
DEG
?
NP
NN
??
PU
?
Figure 1: The parse tree without considering zero pronouns
tree. Viewing the position before the given predi-
cate phrase subtree as a zero pronoun candidate, we
can perform zero pronoun detection using a machine
learning approach.
During training, if a zero pronoun candidate has
a counterpart in the same position in the annotated
training corpus (either anaphoric or non-anaphoric),
a positive example is generated. Otherwise, a nega-
tive example is generated. During testing, each zero
pronoun candidate is presented to the zero pronoun
detector to determine whether it is a zero pronoun.
The features that are employed to detect zero pro-
nouns mainly model the context of the clause itself,
the left and right siblings, and the path of the clause
to the root node. Table 6 lists the features in detail.
4.1 Results and Analysis
We evaluate our zero pronoun detector using gold
parse trees and automatic parse trees produced by
the Berkeley parser. The SVM-light toolkit with ra-
dial basis kernel and default learning parameters is
employed as our learning algorithm.
Table 7 lists the results. From the results, we
R P F
GS 89.32 87.29 88.29
Auto 74.19 77.79 75.95
Table 7: Performance of zero pronoun detection on the
test set using gold and automatic parse trees
find that the performance of our zero pronoun detec-
tor drops about 12% in F-measure when using au-
tomatic parse trees, compared to using gold parse
trees. That is, the performance of zero pronoun de-
tection also depends on the performance of the syn-
tactic parser.
5 Exploiting Zero Pronouns to Improve
Chinese Coreference Resolution
In this section, we will propose two methods, refin-
ing the syntactic parser and refining learning exam-
ple generation, to exploit zero pronouns to improve
Chinese coreference resolution.
283
IP









 
 
 
 
 
 
@
@
@
@
@
@
PP
PP
PP
PP
PP
PP
PP
PP
PP
IP


HH
H
NP
EE
#
VP


HH
H
VP
H
VV
?
NP
NN
??
VP


H
H
VV
?
NP
 HH
QP
H
CD
?
CLP
M
?
NP
NN
??
PU
?
IP


H
HH
NP
EE
#
VP



HH
H
HH
VV
??
NP


HH
H
DNP


HH
H
QP
 HH
ADVP
AD
?
QP
H
CD
?
CLP
M
?
DEG
?
NP
NN
??
PU
?
Figure 2: The parse tree with the detected zero pronouns
5.1 Refining the Syntactic Parser
Similar to our preliminary experiments, we retrain
the Berkeley parser with explicit, automatically de-
tected zero pronouns in the training set and parse
the test set with explicit, automatically detected
zero pronouns using the retrained model. In both
anaphoricity determination and coreference resolu-
tion, the output results of the retrained parser are
employed to generate all features.
5.2 Refining Learning Example Generation
In order to model the salience of all entities, we re-
gard all zero pronouns as a special kind of NPs when
generating the learning examples. Considering the
modest performance of our anaphoricity determina-
tion module, we do not determine the anaphoricity
of zero pronouns. Instead, in the coreference res-
olution stage, all zero pronouns will be considered
during learning example generation (including both
training and test example generation).
For example, consider a coreference chain A1-
A2-Z0-A3-A4 containing one zero pronoun found
in an annotated training document. A1, A2, A3,
and A4 are traditional entity mentions, and Z0 is a
zero pronoun. During training, pairs of mentions in
the chain that are immediately adjacent (i.e., A1-A2,
A2-Z0, Z0-A3, and A3-A4) are used to generate the
positive training examples. Among them, two ex-
amples (i.e., A2-Z0 and Z0-A3) are associated with
a zero pronoun, which can act as both an anaphor
and an antecedent. For each positive pair, e.g., Z0-
A3, we find any noun phrase and zero pronoun oc-
curring between the anaphor A3 and the antecedent
Z0, and pair each of them with A3 to form a nega-
tive example. Similarly, test examples can be gen-
erated except that only the preceding mentions and
zero pronouns in the current and previous two sen-
tences will be paired with an anaphor.
Incorporating zero pronouns models salience of
all entities more accurately. The ratio of positive to
negative examples is also less skewed as a result of
considering zero pronouns ? the ratio changes from
1:7.9 to 1:6.8 after considering zero pronouns.
5.3 Reprocessing
Although in the OntoNotes corpus, dropped subjects
and objects (i.e., zero pronouns) are considered dur-
ing coreference resolution for Chinese, they are not
284
Feature Description
ClauseClass Whether the given clause is a terminal clause or non-terminal clause.
LeftSibling Whether the given clause has a sibling immediately to its left.
LeftSiblingNP Whether the left siblings of the given clause contain an NP.
RightSibling Whether the given clause has a sibling immediately to its right.
RightSiblingVP Whether the right siblings of the given clause contain a VP.
ParentIP/VP Whether the syntactic category of the immediate parent of the given clause is an
IP or VP.
RootPath Whether the path from the given clause to the root of the parse tree contains
an NP or VP or CP. This feature models how the given clause is syntactically
connected to the sentence as a whole, reflecting its function within the sentence.
ClauseType The given clause is an independent clause, a subordinate clause, or others.
Has-Arg0/Arg1 Whether the given clause has an agent or patient argument.
Table 6: Features employed to detect zero pronouns
used in the CoNLL-2012 shared task (i.e., in the
gold evaluation keys, all the links formed by zero
pronouns are removed).
As described in Subsection 5.2, during training
and testing, all links associated with zero pronouns
will be considered in our coreference resolution sys-
tem. That is, we do not distinguish zero pronoun res-
olution from traditional coreference resolution, and
only view zero pronouns as special pronouns. After
generating all the links, zero pronouns are included
in coreference chains. For every coreference chain,
all zero pronouns will be removed before evaluation.
5.4 Experimental Results and Analysis
For fair comparison, all our experiments in this sub-
section have been conducted using the same experi-
mental settings as our baseline system. When com-
pared to our baseline system, all improvements are
statistically significant (p < 0.005).
Table 8 lists the coreference resolution perfor-
mance incorporating automatically detected zero
pronouns. The results show that:
? Using automatically detected zero pronouns
achieves better performance under all experi-
mental settings. In particular, using automatic
mentions, performance improves by 3.31%,
1.42%, and 2.34% in F-measure on the MUC,
BCUBED, and CEAF evaluation metric, re-
spectively. Using gold mention boundaries, au-
tomatic zero pronouns contribute 1.82% in av-
erage F-measure. Using gold mentions, the
R P F
AM
Mention Detection 71.09 69.58 70.33
MUC 55.06 64.91 59.58
BCUBED 76.04 80.38 78.15
CEAF 53.98 49.19 51.47
Average 63.07
GMB
Mention Detection 82.44 70.10 75.77
MUC 75.58 69.89 72.62
BCUBED 76.35 87.27 81.45
CEAF 65.17 52.31 58.04
Average 70.70
GM
Mention Detection 84.31 100.00 91.49
MUC 80.83 86.27 83.46
BCUBED 74.18 92.74 82.43
CEAF 69.91 62.29 65.88
Average 77.26
Table 8: Performance of our Chinese coreference resolu-
tion system incorporating zero pronouns
285
contribution of zero pronouns is only 0.24% in
average F-measure. This is because employing
either gold mention boundaries or gold men-
tions improves parsing performance.
? Our system incorporating zero pronouns out-
performs the three best systems in the CoNLL-
2012 shared task when using automatic men-
tions or gold mention boundaries. Using gold
mentions, our average F-measure is slightly
lower than that of Chen and Ng (2012).5
Table 9 presents the contribution of our two meth-
ods of exploiting zero pronouns and the impact of
gold-standard zero pronouns. We conclude that:
? Both the refined parser and refined example
generation improve performance. While the
refined parser improves the recall of mention
detection and coreference resolution, refined
example generation contributes more to preci-
sion. Combining these two methods further im-
proves coreference resolution.
? There is a performance gap of 6.01%, 4.08%,
and 3.19% in F-measure on the MUC,
BCUBED, and CEAF evaluation metric, re-
spectively, between the coreference resolution
system with gold-standard zero pronouns and
without zero pronouns. This suggests the use-
fulness of zero pronoun detection in Chinese
coreference resolution.
? Our proposed methods incorporating automatic
zero pronouns reduce the performance gap by
about half. This shows the effectiveness of our
proposed methods.
5.5 Discussion
Although the evaluation of the CoNLL-2012 shared
task does not consider zero pronouns, we also eval-
uate the performance of zero pronoun resolution on
the development data set (i.e., extracting all the re-
solved coreference links containing zero pronouns,
acting as anaphor or antecedent, to conduct the eval-
uation independently). The results show that, for
the correct anaphoric zero pronouns, the precision
5Statistical significance testing cannot be conducted since
their output files are not released.
of our system is 94.76%. So viewing zero pronouns
as a special kind of NP, zero pronouns can bridge
salience and contribute to coreference resolution. In
Example (1), the zero pronouns occurring in the sec-
ond sentence help to bridge the coreferential relation
between the mention ?????/this plan? in the
last sentence and the mention ???????/a re-
construction plan? in the first sentence.
6 Related Work
In the last decade, both manual rule-based ap-
proaches (Lee et al, 2011) and statistical ap-
proaches (Soon et al, 2001; Ng and Cardie, 2002;
Fernandes et al, 2012) have been proposed for
coreference resolution. Besides frequently used syn-
tactic and semantic features, more linguistic features
are exploited in recent work (Ponzetto and Strube,
2006; Ng, 2007; Versley, 2007). There is less re-
search on Chinese coreference resolution compared
to English.
Although zero pronouns are prevalent in Chinese,
there is relatively little work on this topic. For Chi-
nese zero pronoun resolution, representative work
includes Converse (2006), Zhao and Ng (2007), and
Kong and Zhou (2010).
For the use of zero pronouns, Chung and Gildea
(2010) applied some extracted patterns to recover
two types of empty elements (*PRO* and *pro*).
Although the performance is still not satisfactory
(e.g., 63.0 and 44.0 in F-measure for *PRO* and
*pro* respectively), it nevertheless improves ma-
chine translation performance by 0.96 in BLEU
score.
7 Conclusion
In this paper, we focus on exploiting one of the key
characteristics of Chinese text, zero pronouns, to im-
prove Chinese coreference resolution. In particu-
lar, a simplified semantic role labeling framework
is proposed to detect zero pronouns effectively, and
two effective methods are employed to incorporate
zero pronouns into Chinese coreference resolution.
Experiments on the CoNLL-2012 shared task show
the effectiveness of our proposed approach. To the
best of our knowledge, this is the first attempt at in-
corporating zero pronouns into Chinese coreference
resolution.
286
MD MUC BCUBED CEAF Avg
R P F R P F R P F R P F
Baseline 65.26 67.20 66.22 51.64 61.82 56.27 73.40 80.38 76.73 53.16 45.66 49.13 60.71
+RP 72.01 66.24 69.00 55.02 61.47 58.07 77.83 78.97 78.40 50.40 49.81 50.10 62.19
+REG 65.92 70.02 67.91 49.98 66.27 56.98 73.64 83.45 78.24 51.12 47.44 49.21 61.48
+AZPs 71.09 69.58 70.33 55.06 64.91 59.58 76.04 80.38 78.15 53.98 49.19 51.47 63.07
+GZPs 72.18 70.59 71.38 58.61 66.45 62.28 78.79 82.94 80.81 54.12 50.63 52.32 65.14
Table 9: Contributions of the two methods of incorporating zero pronouns and the impact of gold zero pronouns
(RP: refining parser using auto zero pronouns, REG: refining example generation using auto zero pronouns, AZPs:
combining both RP and REG using auto zero pronouns, and GZPs: combining both RP and REG using gold zero
pronouns)
Acknowledgments
This research is supported by the Singapore Na-
tional Research Foundation under its International
Research Centre @ Singapore Funding Initiative
and administered by the IDM Programme Office.
References
Anders Bjo?rkelund and Richa?rd Farkas. 2012. Data-
driven multilingual coreference resolution using re-
solver stacking. In Proceedings of the Joint Confer-
ence on EMNLP and CoNLL ? Shared Task, pages 49?
55.
Chen Chen and Vincent Ng. 2012. Combining the best of
two worlds: A hybrid approach to multilingual coref-
erence resolution. In Proceedings of the Joint Con-
ference on EMNLP and CoNLL ? Shared Task, pages
56?63.
Tagyoung Chung and Daniel Gildea. 2010. Effects of
empty categories on machine translation. In Proceed-
ings of the 2010 Conference on Empirical Methods in
Natural Language Processing, pages 636?645.
Susan Converse. 2006. Pronominal Anaphora Resolu-
tion in Chinese. Ph.D. thesis, University of Pennsyl-
vania.
Eraldo Rezende Fernandes, C??cero Nogueira dos Santos,
and Ruy Luiz Milidiu?. 2012. Latent structure percep-
tron with feature induction for unrestricted coreference
resolution. In Proceedings of the Joint Conference on
EMNLP and CoNLL ? Shared Task, pages 41?48.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. In Bernhard Scho?lkopf, Christo-
pher J. C. Burges, and Alexander J. Smola, editors,
Advances in Kernel Methods: Support Vector Learn-
ing. MIT-Press.
Young-Joo Kim. 2000. Subject/object drop in the acqui-
sition of Korean: A cross-linguistic comparison. Jour-
nal of East Asian Linguistics, 9:325?351.
Fang Kong and Guodong Zhou. 2010. A tree kernel-
based unified framework for Chinese zero anaphora
resolution. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Processing,
pages 882?891.
Fang Kong, Guodong Zhou, and Qiaoming Zhu. 2009.
Employing the centering theory in pronoun resolution
from the semantic perspective. In Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing, pages 987?996.
Heeyoung Lee, Yves Peirsman, Angel Chang, Nathanael
Chambers, Mihai Surdeanu, and Dan Jurafsky. 2011.
Stanford?s multi-pass sieve coreference resolution sys-
tem at the CoNLL-2011 shared task. In Proceedings
of the Fifteenth Conference on Computational Natural
Language Learning: Shared Task, pages 28?34.
Vincent Ng and Claire Cardie. 2002. Improving machine
learning approaches to coreference resolution. In Pro-
ceedings of the 40th Annual Meeting of the Association
for Computational Linguistics, pages 104?111.
Vincent Ng. 2007. Semantic class induction and coref-
erence resolution. In Proceedings of the 45th Annual
Meeting of the Association for Computational Linguis-
tics, pages 536?543.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of the 21st
International Conference on Computational Linguis-
tics and 44th Annual Meeting of the Association for
Computational Linguistics, pages 433?440.
Simone Paolo Ponzetto and Michael Strube. 2006.
Exploiting semantic role labeling, WordNet and
Wikipedia for coreference resolution. In Proceedings
of the Human Language Technology Conference of the
NAACL, pages 192?199.
Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong
Lim. 2001. A machine learning approach to corefer-
ence resolution of noun phrases. Computational Lin-
guistics, 27(4):521?544.
287
Yannick Versley. 2007. Antecedent selection techniques
for high-recall coreference resolution. In Proceedings
of the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 496?505.
Bonnie Lynn Webber. 1978. A Formal Approach to Dis-
course Anaphora. Garland Press.
Nianwen Xue. 2008. Labeling Chinese predicates
with semantic roles. Computational Linguistics,
34(2):225?255.
Yaqin Yang and Nianwen Xue. 2010. Chasing the ghost:
Recovering empty categories in the Chinese Treebank.
In Coling 2010: Posters, pages 1382?1390.
Bo Yuan, Qingcai Chen, Yang Xiang, Xiaolong Wang,
Liping Ge, Zengjian Liu, Meng Liao, and Xianbo Si.
2012. A mixed deterministic model for coreference
resolution. In Proceedings of the Joint Conference on
EMNLP and CoNLL ? Shared Task, pages 76?82.
Shanheng Zhao and Hwee Tou Ng. 2007. Identifi-
cation and resolution of Chinese zero pronouns: A
machine learning approach. In Proceedings of the
2007 Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning, pages 541?550.
Zhi Zhong and Hwee Tou Ng. 2010. It Makes Sense:
A wide-coverage word sense disambiguation system
for free text. In Proceedings of the ACL 2010 System
Demonstrations, pages 78?83.
288
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 715?725,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Collective Personal Profile Summarization with Social Networks 
 
 
Zhongqing Wang, Shoushan Li*, Kong Fang, and Guodong Zhou 
Natural Language Processing Lab, School of Computer Science and Technology 
Soochow University, Suzhou, 215006, China  
{wangzq.antony, shoushan.li}@gmail.com,  
{kongfang, gdzhou}@suda.edu.cn 
 
  
 
Abstract 
Personal profile information on social media 
like LinkedIn.com and Facebook.com is at the 
core of many interesting applications, such as 
talent recommendation and contextual advertis-
ing. However, personal profiles usually lack or-
ganization confronted with the large amount of 
available information. Therefore, it is always a 
challenge for people to find desired information 
from them. In this paper, we address the task of 
personal profile summarization by leveraging 
both personal profile textual information and so-
cial networks. Here, using social networks is 
motivated by the intuition that, people with 
similar academic, business or social connections 
(e.g. co-major, co-university, and co-
corporation) tend to have similar experience and 
summaries. To achieve the learning process, we 
propose a collective factor graph (CoFG) model 
to incorporate all these resources of knowledge 
to summarize personal profiles with local textual 
attribute functions and social connection factors. 
Extensive evaluation on a large-scale dataset 
from LinkedIn.com demonstrates the effective-
ness of the proposed approach.* 
1 Introduction 
Web 2.0 has empowered people to actively interact 
with each other, forming social networks around 
mutually interesting information and publishing a 
large amount of useful user-generated content 
(UGC) online (Lappas et al, 2011; Tan et al, 
2011). One popular and important type of UGC is 
the personal profile, where people post detailed 
                                                 
* Corresponding author 
information on online portals about their education, 
experiences and other personal information. Social 
websites like Facebook.com and LinkedIn.com 
have created a viable business as profile portals, 
with the popularity and success partially attributed 
to their comprehensive personal profiles. 
Generally, online personal profiles provide val-
uable resources for businesses, especially for hu-
man resource managers to find talents, and help 
people connect with others of similar backgrounds 
(Yang et al, 2011a; Guy et al, 2010). However, as 
there is always large-scale information of experi-
ence and education fields, it is hardly for us to find 
useful information from the profile. Therefore, it is 
always a challenge for people to find desired in-
formation from them. For this regard, it is highly 
desirable to develop reliable methods to generate a 
summary of a person through his profile automati-
cally.  
To the best of our knowledge, this is the first re-
search that explores automatic summarization of 
personal profiles in social media. A straightfor-
ward approach is to consider personal profile 
summarization as a traditional document summari-
zation problem, which treating each personal pro-
file independently and generate a summary for 
each personal profile individually. For example, 
the well-known extraction and ranking approaches 
(e.g. PageRank, HITS) extract a certain amount of 
important sentences from a document according to 
some ranking measurements to form a summary 
(Wan and Yang, 2008; Wan, 2011).  
However, such straightforward approaches are 
not sufficient to benefit from the carrier of person-
al profiles. As the centroid of social networking, 
people are usually connected to others with similar 
715
background in social media (e.g. co-major, co-
corporation). Therefore, it is reasonable to lever-
age social connection to improve the performance 
of profile summarizing. For example if there are 
co-major, co-university, co-corporation or other 
academic and business relationships between two 
persons, we consider them sharing similar experi-
ence and having similar summaries. 
The remaining challenge is how to incorporate 
both the profile textual information and the con-
nection knowledge in the social networks. In this 
study, we propose a collective factor graph model 
(CoFG) to summarize the text of personal profile 
in social networks with local textual information 
and social connection information. The CoFG 
framework utilizes both the local textual attribute 
functions of an individual person and the social 
connection factor between different persons to col-
lectively summarize personal profile on one person. 
In this study, we treat the profile summarization 
as a supervised learning task. Specifically, we 
model each sentence of the profile as a vector. In 
the training phase, we use the vectors with the so-
cial connection between each person to build the 
CoFG model; while in the testing phase, we per-
form collective inference for the importance of 
each sentence and select a subset of sentences as 
the summary according to the trained model. Eval-
uation on a large-scale data from LinkedIn.com 
indicates that our proposed joint model and social 
connection information improve the performance 
of profile summarization. 
The remainder of our paper is structured as fol-
lows. We go over the related work in Section 2. In 
Section 3, we introduce the data we collected from 
LinkedIn.com and the annotated corpus we con-
structed. In Section 4, we present some motiva-
tional analysis. In Section 5, we explain our pro-
posed model and describe algorithms for parame-
ter estimation and prediction. In Section 6, we pre-
sent our experimental results. We sum up our work 
and discuss future directions in Section 7. 
2 Related Work 
In this section, we will introduce the related work 
on the traditional topic-based summarization, so-
cial-based summarization and factor graph model 
respectively. 
2.1 Topic-based Summarization 
Generally, traditional topic-based summarization 
can be categorized into two categories: extractive 
(Radev et al, 2004) and abstractive (Radev and 
McKeown, 1998) summarization. The former se-
lects a subset of sentences from original docu-
ment(s) to form a summary; the latter reorganizes 
some sentences to form a summary where several 
complex technologies, such as information fusion, 
sentence compression and reformulation are nec-
essarily employed (Wan and Yang, 2008; Celiky-
ilmaz and Hakkani-Tur, 2011; Wang and Zhou, 
2012). This study focuses on extractive summari-
zation.  
Radev et al (2004) proposed a centroid-based 
method to rank the sentences in a document set, 
using various kinds of features, such as the cluster 
centroid, position and TF-IDF features. Ryang and 
Abekawa (2012) proposed a reinforcement learn-
ing approach on text summarization, which models 
the summarization within a reinforcement learn-
ing-based framework.  
Compared to unsupervised approaches, super-
vised learning for summarization is relatively rare. 
A typical work is Shen et al, (2007) which present 
a Conditional Random Fields (CRF) based frame-
work to treat the summarization task as a sequence 
labeling problem. However, different from all ex-
isting studies, our work is the first attempt to con-
sider both textual information and social relation-
ship information for supervised summarization. 
2.2 Social-based Summarization 
As web 2.0 has empowered people to actively in-
teract with each other, studies focusing on social 
media have attracted much attention recently 
(Meeder et al, 2011; Rosenthal and McKeown, 
2011; Yang et al, 2011a). Social-based summari-
zation is exactly a special case of summarization 
where the social connection is employed to help 
obtaining the summarization. Although topic-
based summarization has been extensively studied, 
studies on social-based summarization are relative 
new and rare.  
Hu et al, (2011) proposed an unsupervised Pag-
eRank-based social summarization approach by 
incorporating both document context and user con-
text in the sentence evaluation process. Meng et al, 
(2012) proposed a unified optimization framework 
to produce opinion summaries of tweets through 
716
integrating information from dimensions of topic, 
opinion and insight, as well as other factors (e.g. 
topic relevancy, redundancy and language styles). 
Unlike all the above studies, this paper focuses 
on a novel task, profile summarization. Further-
more, we employ many other kinds of social in-
formation in profiles, such as co-major, and co-
corporation between two people. They are shown 
to be very effective for profile summarization.  
2.3 Factor Graph Model 
As social network has been investigated for sever-
al years (Leskovec et al, 2010; Tan et al, 2011; 
Lu et al, 2010; Guy et al, 2010) and Factor Graph 
Model (FGM) is a popular approach to describe 
the relationship of social network (Tang et al, 
2011a; Zhuang et al, 2012). Factor Graph Model 
builds a graph to represent the relationship of 
nodes on the social networks, and the factor func-
tions are always considered to represent the rela-
tionship of the nodes. 
Tang et al (2011a) and Zhuang et al (2012) 
formalized the problem of social relationship 
learning into a semi-supervised framework, and 
proposed Partially-labeled Pairwise Factor Graph 
Model (PLP-FGM) for learning to infer the type of 
social ties. Dong et al (2012) gave a formal defini-
tion of link recommendation across heterogeneous 
networks, and proposed a ranking factor graph 
model (RFG) for predicting links in social net-
works, which effectively improves the predictive 
performance. Yang et al, (2011b) generated sum-
maries by modeling tweets and social contexts into 
a dual wing factor graph (DWFG), which utilized 
the mutual reinforcement between Web documents 
and their associated social contexts.  
Different from all above researches, this paper 
proposes a pair-wise factor graph model to collec-
tively utilize both textual information and social 
connection factor to generate summary of profile. 
3 Data Collection and Statistics   
The personal profile summarization is a novel task 
and there exists no related data for accessing this 
issue. Therefore, in this study, we collect a data set 
containing personal summaries with the corre-
sponding knowledge, such as the self-introduction 
and personal profiles. In this section, we will in-
troduce this data set in detail. 
3.1 Data Collection  
We collect our data set from LinkedIn.com1 . It 
contains a large number of personal profiles gen-
erated by users, containing various kinds of infor-
mation, such as personal overview, summary, edu-
cation, experience, projects and skills.  
 
John Smith2  
Overview 
Current Applied Researcher at Apple Inc. 
Previous 
Senior Research Scientist at IBM 
? 
Education 
MIT, 
Georgia Institute of Technology,   
? 
Summary 
Machine learning researcher and engineer on 
many fields: 
Query understanding. Automatic Information 
extraction? 
Experience 
Applied Researcher 
Apple Inc., September 2012 ~  
Query recognition and relevance 
? 
Education 
MIT 
Ph.D., Electrical Engineering, 2002 ? 2008 
? 
Figure 1: An example of a profile webpage from 
LinkedIn.com 
 
In this study, the data set is crawled in the fol-
lowing ways. To begin with, 10 random people?s 
public profiles are selected as seed profiles, and 
then the profiles from their ?People Also Viewed? 
field were collected. The data is composed of 
3,182 public profiles3 in total. We do not collect 
personal names in public profiles to protect peo-
ple?s privacy. Figure 1 shows an example of a per-
son?s profile from LinkedIn.com. The profile in-
cludes following fields: 
? Overview: It gives a structure description of a 
person?s general information, such as cur-
rent/previous position and workplace, brief 
                                                 
1 http://www.linkedin.com 
2 The information of the example is a pseudo one. 
3 We collect all the data from LinkedIn.com at Dec 17, 
2012.  
717
education background and general technical 
background.  
? Summary: It summarizes a person?s work, 
experience and education.  
? Experience: It details a person?s work experi-
ence.  
? Education: It details a person?s education 
background.  
Among these fields, the Overview is required 
and the others are optional, such as Project, 
Course and Interest groups. However, compared 
with Overview, Summary, Experience, Education 
fields, they seem to be less important for summari-
zation of personal profiles. Thus, we ignore them 
in our study. 
3.2 Data Statistics of Major Fields 
We collected 3,182 personal profiles from 
LinkedIn.com. Table 1 shows the statistics of ma-
jor fields in our data collection. 
 
Field 
#Non-empty 
fields 
Average 
field 
length 
Overview 3,182 45.1 
Summary 921 25.8 
Experience 3,148 192.1 
Education 2,932 33.6 
Table 1: Statistics of major fields in our data set, i.e. the 
number of non-empty fields and the average length for 
each field 
 
From Table 1, we can see that, 
? The information of each profile is incom-
plete and inconsistent, That is, not all kinds 
of fields are available in each personal?s 
profile.  
? Most people provide their experience and 
education information. However, the Sum-
mary fields are popularly missing (Only 
about 30% of people provide it). This is 
mainly because writing summary is nor-
mally more difficult than other fields. 
Therefore, it is highly desirable to develop 
reliable automatic methods to generate a 
summary of a person through his/her pro-
file. 
? The length of the Experience field is the 
longest one, and work experience always 
could represent general information of 
people.  
3.3 Corpus Construction and Annotation  
Among the 921 profiles that contain the summary, 
we manually select 497 profiles with high quality 
summary to construct the corpus for our research. 
These high-quality summaries are all written by 
the authors themselves. Here, the quality is meas-
ured by manually checking that whether they are 
well capable of summarizing their profiles. That is, 
they are written carefully, and could give an over-
view of a person and represent the education and 
experience information of a person. 
After carefully seeing the profiles, we observe 
that the Experience field contains the most abun-
dant information of a person. Thus, we treat the 
text of Experience field as the source of summary 
for each profile. Besides, we collect social context 
information from Education and Experience field, 
and these social contexts are including by 
LinkedIn explicitly. Table 2 shows the average 
length of summary and experience fields we used 
for evaluating our summarization approach.  
 
Field 
Average 
length 
Summary 
(the summary of the 
profile) 
37.2 
Experience 
(the source text for the 
summarizing) 
372.0 
Table 2: Average length of the high-quality summary  
and corresponding experience fields 
 
From Table 2, we can see that,  
? Compared with the average length of 25.8 
in Table 1, summaries of high quality have 
longer length because they contain more in-
formation of the profiles.  
? The compression ratio of our proposed cor-
pus is 0.1 (37.2/372.0).  
4 Motivation and Analysis 
In this section, we propose the motivation of social 
connection to address the task of personal profile 
summarization. To preliminarily support the moti-
vation, some statistics of the social connection are 
provided. 
718
 Figure 2: An example of personal profile network.  
Red is for female, blue is for male, and the dotted line 
means the social connection between two persons. 
 
We first describe the social connections which 
we used. Figure 2 shows an example of social 
connection between people from the profiles of 
LinkedIn. We find that people are sometimes con-
nected by several social connections. For example, 
John and Lucy are connected by co_unvi relation-
ship, while Lily and Linda are connected by 
co_corp relationship. From LinkedIn, four kinds of 
social relationship between people are extracted 
from the Education field and Experience field. 
They are: 
? co_major denotes that two persons have the 
same major at school 
? co_univ denotes that two persons are graduat-
ed from the same university 
? co_title denotes that two persons have the 
same title at corporation. 
? co_corp denotes that two persons work at the 
same corporation. 
Our basic motivation of using social connection 
lies in the fact that ?connected? people will tend to 
hold related experience and similar summaries.  
We then give the statistics of edges of social 
connection. Table 3 shows basic statistics across 
these edges. From Table 3, we can see that the 
number of users is 497 while the number of social 
connection edges is 14,307. The latter is much 
larger than the former. The number of the edges 
from Education field is similar with the number of 
the edges from Experience filed. Among all the 
relationships, co_unvi is the most common one.  
 
 Numbers 
# users 497 
co_major 1,288 
co_unvi 6,015 
# education field 7,303 
co_title 3,228 
co_corp 3,776 
# experience field 7,004 
# total edges 14,307 
Table 3: The statistic of edges for our main datasets 
5 Collective Factor Graph Model 
In this section, we propose a collective factor 
graph (CoFG) model for learning and summarizing 
the text of personal profile with local textual in-
formation and social connection. 
5.1 Overview of Our Framework 
To generate summaries for profiles, a straightfor-
ward approach is to treat each personal profile in-
dependently and generating a summary for each 
personal profile individually. As we mentioned on 
Section 3.3, we use the sentences of Experience 
field as a text document and consider it as the 
source of summary for each profile. 
Instead, we formalize the problem of personal 
profile summarization in a pair-wise factor graph 
model and propose an approach referred to as 
Loopy Belief Propagation algorithm to learn the 
model for generating the summary of the profile. 
Our basic idea is to define the correlations using 
different types of factor functions. An objective 
function is defined based on the joint probability 
of the factor functions. Thus, the problem of col-
lective personal profile summarization model 
learning is cast as learning model parameters that 
maximizes the joint probability of the input con-
tinuous dynamic network. 
The overview of the proposed method is a su-
pervised framework (as shown in Figure 3).  First, 
we treat each sentence of the training data and test-
ing data as vectors with textual information (local 
textual attribute functions); Second, all the vectors 
are connected by social connection relationships 
(social connection factors) and we model these 
vectors and their relationships into the collective 
factor graph; third, we propose Loopy Belief Prop-
 
John 
Antony 
   Bill 
Lily  
Lucy  
       Linda 
 
 
 
 
 
co_major 
co_univ 
co_corp 
co_corp 
co_title 
co_title 
co_major 
co_univ 
719
agation algorithm to learn the model and predict 
the sentences of testing data; finally, we select a 
subset of sentences of each testing profile as the 
summary according to the models with top-n pre-
diction score. Thus, the core issues of our frame-
work are 1) how to define the collective factor 
graph model to connection profiles with social 
connection; 2) how to learn and predict the pro-
posed CoFG model; 3) how to predict the sentenc-
es from the testing data with the proposed CoFG 
model, and generate the summary by the predict 
scores. We will discuss these issues on the follow-
ing subsections. 
 
 
Figure 3: The overview of our proposed framework 
 
5.2 Model Definition 
Formally, given a network ( , , , )L UG V S S X? , 
each sentence 
is  is associated with an attribute 
vector 
ix  of the profile and a label iy  indicating 
whether the sentence is selected as a summary of 
the profile (The value of 
iy  is binary. 1 means that 
the sentence is selected as a summary sentence, 
whereas 0 stands for the opposite). V denotes the 
authors of the profiles, LS  denotes the labeled 
training data, and US denotes the unlabeled testing 
data. Let { }iX x? and { }iY y? . Then, we have the 
following formulation 
         
? ? ? ? ? ?? ?
, || , ,
P X G Y P YP Y X G P X G?
             (1) 
Here, G denotes all forms of network infor-
mation. This probabilistic formulation indicates 
that labels of skills depend on not only local at-
tributes X, but also the structure of the network G. 
According to Bayes? rule, we have 
         ? ? ? ? ? ?? ?
? ? ? ?
, |
| ,
,
                  | |
P X G Y P Y
P Y X G
P X G
P X Y P Y G
?
?
             (2) 
Where ( | )P Y G represents the probability of labels 
given the structure of the network and ( | )P X Y  
denotes the probability of generating attributes X
associated to their labels Y . We assume that the 
generative probability of attributes given the label 
of each edge is conditionally independent, thus we 
have 
? ? ? ? ? ?| , | |i iiP Y X G P Y G P x y? ?
    (3) 
Where ( | )i iP x y  is the probability of generating 
attributes 
ix given the label iy . Now, the problem 
becomes how to instantiate the probability 
( | )P Y G and ( | )i iP x y . We model them in a Mar-
kov random field, and thus according to the Ham-
mersley-Clifford theorem (Hammersley and 
Clifford, 1971), the two probabilities can be in-
stantiated as follows: 
? ? ? ?
11
1| exp ,
d
i i j j ij i
j
P x y f x yZ ??
? ?? ? ?? ??
       (4) 
? ? ? ?
( )2
1| exp ,
i j NB i
P Y G g i jZ ?
? ?? ? ?? ?? ?
       (5) 
                       
Where 
1 2 and Z Z  are normalization factors. Eq. 4 
indicates that we define an attribute function 
( , )i if x y  for each attribute ijx
 associated with 
sentence
is . j?  is the weight of the j
th attribute. Eq. 
5 represents that we define a set of correlation fac-
tor functions ( , )g i j  over each pair ( , )i j in the 
network. ( )NB i  denotes the set of social relation-
ship neighbors nodes of i.  
 
 
Training  
Set 
 
  
  
Social  
Connection 
Social  
Connection 
Testing  
Set 
  Sentence Scoring 
  Sentence Selection 
 Summarized Profile 
Profiles 
Profiles 
Collective Factor Graph 
Modeling 
  
720
 1 
3 
2 
  
  
 
 
 
  
 
 
 
 
 
 
 
f (v1,y1) 
y
2
 
y
1
 y3 
y
4
 
y
5
 
y
6
 
 S
1
 
 S
2
 
S
3
 
S
4
 
S
5
 
S
6
 
f (v
1
,y
2
) 
f (v
6
,y
6
) 
 
CoFG model 
Nodes of sentences 
with different people 
y1=0 
y
2
=1 
y
3
=1 
y
4
=0 
y
6
=? 
y
5
=? g (y
3
,y
5
) 
Figure 4: Graph representation of CoFG 
The left figure shows the personal profile network. Each dotted line indicates a social connection. Each dotted 
square denotes a person, and the grey square denotes the sentence selected in the summary, and the white square 
denotes a sentence that is not selected as the summary.. 
The right figure shows the CoFG model derived from left figure. Each eclipse denotes a sentence vector of a 
person, and each circle indicates the hidden variable yi. f(vi,yi) indicates the attribute factor function. g(yi,yj) indi-
cates the social connection factor function. 
 
4 
5 
6 
  
  
co_major 
co_corp 
  
Person A 
Person B 
Person C 
We now briefly introduce possible ways to de-
fine the attribute functions{ ( , )}ij i jf x y
, and factor 
function ( , )g i j  .  
Local textual attribute functions{ ( , )}ij i jf x y
: 
It denotes the attribute value associated with each 
sentence i. We define the local textual attribute as 
a feature (Lafferty et al, 2001). We can accumu-
late all the attribute functions and obtain local en-
tropy for a person: 
? ?
1
1 exp ,k k ik i
i k
f x yZ ?
? ?? ?? ???
              (6) 
The textual attributes include following features 
(Shen et al, 2007; Yang et al, 2011b):  
1) BOW: the bag-of-words of each sentence, we 
use unigram features as the basic textual fea-
tures for each sentence.  
2) Length: the number of terms of each sentence. 
3) Topic_words: these are the most frequent 
words in the sentence after the stop words are 
removed. 
4) PageRank_scores: as shown in the related 
work section, a document can be treated as a 
graph and applying a graph-based ranking al-
gorithm (Wan and Yang., 2008). We thus use 
the PageRank score to reflect the importance 
of each sentence. 
Social connection factor function ( , )i jg y y
: 
For the social correlation factor function, we de-
fine it through the pairwise network structure. That 
is, if the person of sentence i and the person of 
sentence j have a social relationship, a factor func-
tion for this social connection is defined (Tang et 
al., 2011a; Tang et al, 2011b), i.e., 
? ? ? ?? ?2, expi j ij i jg y y y y?? ?         (7) 
The person-person social relationships are de-
fined on Section 4, e.g. co_major, co_univ, co_title, 
and co_corp. We define that if two persons have at 
least one social connection edge, they have a so-
cial relationship. In addition, 
ij?  is the weight of 
the function, representing the influence degree of i 
on j. 
To better understand our model, one example of 
factor decomposition is given in Figure 4. In this 
example, there are six sentences from three pro-
files. Among them, four sentences are labeled (two 
are labeled with the category of ?1?, i.e,  1y ?  and 
the other two are labeled with the category of ?0?, 
i.e., 0y ? ) and two sentences are unlabeled (they 
are represented by y=?). We have six attribute 
functions. For example, 
1( , )if v y  denotes the set 
721
of local textual attribute functions of 
iy . We also 
have five pairwise relationships (e.g.,
2 4( , )y y ,
3 5( , )y y ) based on the structure of the input per-
sonal profile social network. For example, 
3 5( , )g y y  denotes social connection between 3y  
and 
5y , while they share the co_major relationship 
on the left figure. 
5.3 Model Learning 
We now address the problem of estimating the free 
parameters. The objective of learning the CoFG 
model is to estimate a parameter configuration 
({ },{ })? ? ??  to maximize the log-likelihood ob-
jective function ( ) log ( | , )L P Y X G?? ? , i.e., 
? ?* argmax L? ??                     (9) 
To solve the objective function, we adopt a gra-
dient descent method. We use ?  (the weight of 
the social connection factor function ( , )i jg y y
) as 
the example to explain how we learn the parame-
ters (the algorithm also applies to tune ?  by simp-
ly replacing ? with? ). Specifically, we first write 
the gradient of each 
k? with regard to the objective 
function (Eq. 9) :  
  ? ? ? ? ? ?( | , ), ,kP Y X G
k
L E g i j E g i j?
?
? ? ? ? ? ? ?? ? ? ?
   (10) 
Where [ ( , )]E g i j is the expectation of factor 
function ( , )g i j  given the data distribution (essen-
tially it can be considered as the average value of 
the factor function ( , )g i j over all pair in the train-
ing data); and 
( | , ) [ ( , )]k Y X GPE g i j?
is the expectation of 
factor function ( , )g i j under the distribution 
( | , )kP Y X G?
given by the estimated model. A 
similar gradient can be derived for parameter
ja
. 
We approximate the marginal distribution
( | , ) [ ( , )]k Y X GPE g i j?
 using LBP (Tang et al, 2011; 
Zhuang et al, 2012). With the marginal probabili-
ties, the gradient can be obtained by summing over 
all triads. It is worth noting that we need to per-
form the LBP process twice for each iteration: one 
is to estimate the marginal distribution of unknown 
variables ?iy ?  and the other is to estimate the 
marginal distribution over all pairs. In this way, 
the algorithm essentially performs a transfer learn-
ing over the complete network. Finally, with the 
obtained gradient, we update each parameter with 
a learning rate? . The learning algorithm is sum-
marized in Figure 5. 
 
Input: Network G , Learning rate ?   
Output: Estimated parameters ?   
Initialize 0? ?   
Repreat 
1) Perform LBP to calculate the 
marginal distribution of unknown 
variables, i.e., ? ?| ,i iP y x G   
2) Perform LBP to calculate the 
marginal distribution of each  
variables, i.e., ? ?( , ), | ,i j i jP y y X G  
3) Calculate the gradient of 
k? ac-
cording to Eq. 10 (for a  with a 
similar formula) 
4) Update parameter ?  with the 
learning rate ?  
               
? ?
new old
L ?? ? ? ?? ?  
Until Convergence 
Figure 5: The Learning Algorithm for CoFG model 
 
5.4 Model Prediction and Summary Gener-
ated 
We can see that in the learning process, the learn-
ing algorithm uses an additional loopy belief prop-
agation to infer the label of unknown relationships. 
With the estimated parameter ? , the summariza-
tion process is to find the most likely configuration 
of Y  for a given profile. This can be obtained by  
? ?* argmax | , ,Y L Y X G ??              (11) 
Finally, we select a subset of sentences of each 
testing profile as the summary according to the 
trained models with top-n prediction scores by *Y   
(Tang et al, 2011b; Dong et al 2012).  
6 Experimentation 
In this section, we describe the settings of our ex-
periment and present the experimental results of 
our proposed CoFG model. 
722
6.1 Experiment Settings 
In the experiment, we use the corpus collected 
from LinkedIn.com that contains 497 profiles (see 
more details in Section 3). The existing summaries 
in these profiles are served as the reference sum-
mary (the standard answers). As discussed in sub-
section 3.3, the average length of summary is 
about 40 words. Thus, we extract 40 words to con-
struct the summary for each profile. We use 200 
personal profiles as the testing data, and the re-
maining ones as the training data. 
We use the ROUGE-1.5.5 (Lin and Hovy, 2004) 
toolkit for evaluation, a popular tool that has been 
widely adopted by several evaluations such as 
DUC and TAC (Wan and Yang, 2008; Wan, 2011). 
We provide four of the ROUGE F-measure scores 
in the experimental results: ROUGE-2 (bigram-
based), ROUGE-L (based on longest common 
subsequences), ROUGE-W (based on weighted 
longest common subsequence, weight=1.2), and 
ROUGE-SU4 (based on skip bigram with a maxi-
mum skip distance of 4).  
6.2 Experimental Results 
We compare the proposed CoFG approach with 
three baselines illustrated as follows: 
? Random: we randomly select sentences of 
each profile to generate the summary for the 
profile. 
? HITS: we employ the HITS algorithm to per-
form profile summarization (Wan and Yang, 
2008). In detail, we first consider the words as 
hubs the sentences as authorities; Then, we 
rank the sentences with the authorities? scores 
for each profile individually; Finally, the 
highest ranked sentences are chosen to consti-
tute the summary. 
? PageRank: we employ the PageRank algo-
rithm to perform profile summarization (Wan 
and Yang, 2008). In detail, we first connect 
the sentences of the profile with cosine text-
based similar measure to construct a graph; 
Then, we apply PageRank algorithm to rank 
the sentence through the graph for each pro-
file individually; Finally, the highest ranked 
sentences are chosen to constitute the sum-
mary.  
?  MaxEnt: as a supervised learning approach, 
maximum entropy uses textual attribute as 
features to train a classification model. Then, 
the classification model is employed to pre-
dict which sentences can be selected to gener-
ate the summary. For the implementation of 
MaxEnt, we employ the tool of mallent 
toolkits4. 
Table 4 shows the comparison results of our ap-
proach (CoFG) and the baseline approaches. From 
Table 4, we can see that 1) either HITS or Pag-
eRank outperforms the approach of  random selec-
tion; 2) The supervised approach i.e. MaxEnt, out-
performs both the HITS algorithm and the Pag-
eRank approach; 3) CoFG model performs best 
and it greatly outperforms both the unsupervised 
and supervised learning baseline approaches in 
terms of the ROUGE-2 F-measure score. This re-
sult verifies the effectiveness of considering the 
social connection between the sentences in differ-
ent profiles, 
Figure 6 shows the performance of our proposed 
CoFG model with different sizes of training data. 
From Figure 6, we can see that CoFG model with 
social connection always performs better than 
MaxEnt, and the performance of our approach de-
scends slowly when the training dataset becomes 
small. Specifically, the performance of CoFG us-
ing only 10% training data achieves better perfor-
mance than MaxEnt using 100% training data. 
 
                                                 
4 http://mallet.cs.umass.edu/ 
 ROUGE-2 ROUGE-L ROUGE-W ROUGE-SU4 
Random 0.0219 0.1363 0.0831 0.0288 
HITS 0.0295 0.1499 0.0905 0.0355 
PageRank 0.0307 0.1574 0.0944 0.0383 
MaxEnt 0.0349 0.1659 0.0995 0.0377 
CoFG 0.0383 0.1696 0.1015 0.0415 
Table 4: Performances of different approaches to profile summarization in terms of different measurements 
723
 
Figure 6:  The performance of CoFG with different 
training data size 
 
Table 5 shows the contribution of the social 
edges with CoFG. Specifically, CoFG is our pro-
posed approach with both education and experi-
ence information, CoFG-edu means that the CoFG 
model considers the social edges of education field 
(co_major, co_univ) only, and CoFG-exp means 
that the CoFG model considers the social edges of 
work experience field (co_title, co_corp) only. 
MaxEnt can be considered as using textual infor-
mation only. 
 
 ROUGE-2 
MaxEnt 0.0349 
CoFG 0.0383 
CoFG-edu 0.0382 
CoFG-exp 0.0381 
Table 5: ROUGE-2 F-Measure score of the contribu-
tion of social edges 
 
From Table 5, we can see that all of our pro-
posed approaches, i.e., CoFG-edu, CoFG-exp, and 
CoFG, outperform the baseline approach, i.e., 
MaxEnt. However, the performance of CoFG-edu, 
CoFG-exp and CoFG are similar. This result is 
mainly due to the fact that the information of so-
cial connection is redundant. For example, two 
persons who are connected by co_major (educa-
tion field) might also be connected by co_corp 
(experience field).  
7 Conclusion and Future Work 
In this paper, we present a novel task named pro-
file summarization and propose a novel approach 
called collective factor graph model to address this 
task. One distinguishing feature of the proposed 
approach lies in its incorporating the social con-
nection. Empirical studies demonstrate that the 
social connection is effective for profile summari-
zation, which enables our approach outperform 
some competitive supervised and unsupervised 
baselines. 
The main contribution of this paper is to explore 
social context information to help generate the 
summary of the profiles, which represents an in-
teresting research direction in social network min-
ing. In the future work, we will explore more kinds 
of social context information and investigate better 
ways of incorporating them into profile summari-
zation and a wider range of social network mining. 
 
Acknowledgments 
 
This research work is supported by the National 
Natural Science Foundation of China 
(No.61273320, No.61272257, No.61331011 and 
No.61375073), and National High-tech Research 
and Development Program of China 
(No.2012AA011102). 
We thank Dr. Jie Tang and Honglei Zhuang for 
providing their software and useful suggestions 
about PGM. We acknowledge Dr. Xinfang Liu, 
Yunxia Xue and Yulai Shen for corpus construc-
tion and insightful comments. We also thank 
anonymous reviewers for their valuable sugges-
tions and comments.  
References  
Baeza-Yates R. and B. Ribeiro-Neto. 1999. Modern 
Information Retrieval. ACM Press and Addison Wes-
ley, 1999 
Celikyilmaz A. and D. Hakkani-Tur. 2011. Discovery 
of Topically Coherent Sentences for Extractive 
Summarization. In Proceeding of ACL-11. 
Dong Y., J. Tang, S. Wu, J. Tian, N. Chawla, J. Rao, 
and H. Cao. 2012. Link Prediction and Recommen-
dation across Heterogeneous Social Networks. In 
Proceedings of ICDM-12. 
Elson D., N. Dames and K. McKeown. 2010. Extracting 
Social Networks from Literary Fiction. In Proceed-
ing of ACL-10. 
Erkan G. and D. Radev. 2004. LexPageRank: Prestige 
in Multi-document Text Summarization. In Proceed-
ings of EMNLP-04. 
Guy I., N. Zwerdling, I.  Ronen, D. Carmel, E. Uziel. 
2010. Social Media Recommendation based on Peo-
ple and Tags. In Proceeding of SIGIR-10. 
0.030
0.032
0.034
0.036
0.038
0.040
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
R
O
U
G
E
-2
 
size of training data 
PageRank MaxEnt CFG
724
Hammersley J. and P. Clifford. 1971. Markov Field on 
Finite Graphs and Lattices, Unpublished manuscript. 
1971. 
Hu P., C. Sun, L. Wu, D. Ji and C. Teng. 1011. Social 
Summarization via Automatically Discovered Social 
Context. In Proceeding of IJCNLP-11. 
Lafferty J, A. McCallum, and F. Pereira. 2001. Condi-
tional Random Fields: Probabilistic Models for Seg-
menting and Labeling Sequence Data. In Proceed-
ings of ICML-01. 
Lappas T., K. Punera and T. Sarlos. 2011. Mining Tags 
Using Social Endorsement Networks. In Proceeding 
of SIGIR-11. 
Leskovec J., D. Huttenlocher and J. Kleinberg. 2010. 
Predicting Positive and Negative Links in Online So-
cial Networks. In Proceedings of WWW-10. 
Lin, C. 2004. ROUGE: a Package for Automatic Evalu-
ation of Summaries. In Proceedings of ACL-04 
Workshop on Text Summarization Branches Out. 
Lu Y., P. Tsaparas, A. Ntoulas and L. Polanyi. 2010. 
Exploiting Social Context for Review Quality Pre-
diction. In Proceeding of WWW-10. 
Meng X?F. Wei? X. Liu? M. Zhou? S. Li and H. 
Wang. 2012. Entity-Centric Topic-Oriented Opinion 
Summarization in Twitter. In Proceeding of KDD-12.  
Murphy K., Y. Weiss, and M. Jordan. 1999. Loopy Be-
lief Propagation for Approximate Inference: An Em-
pirical Study. In Proceedings of UAI-99. 
Radev D. and K. McKeown. 1998. Generating Natural 
Language Summaries from Multiple On-line Sources. 
Computational Linguistics, 24(3):469?500. 
Radev D., H. Jing, M. Stys, and D. Tam. 2004. Cen-
troid-based Summarization of Multiple Documents. 
Information Processing and Management. 40 (2004), 
919-938. 
Rosenthal S. and K. McKeown. 2011. Age Prediction in 
Blogs: A Study of Style, Content, and OnlineBehav-
ior in Pre- and Post-Social Media Generations. In 
Proceeding of ACL-11. 
Ryang S. and T. Abekawa. 2012. Framework of Auto-
matic Text Summarization Using Reinforcement 
Learning. In Proceeding of EMNLP-2012. 
Shen D., J. Sun, H. Li, Q. Yang and Zheng Chen. 2007. 
Document Summarization using Conditional Ran-
dom Fields. In Proceeding of IJCAI-07. 
Tan C., L. Lee, J. Tang, L. Jiang, M. Zhou and P. Li. 
2011. User-Level Sentiment Analysis Incorporating 
Social Networks. In Proceedings of KDD-11. 
Tang W., H. Zhuang, and J. Tang. 2011a. Learning to 
Infer Social Ties in Large Networks. In Proceedings 
of ECML/PKDD-11. 
Tang J., Y. Zhang, J. Sun, J. Rao, W. Yu, Y. Chen, and 
A. Fong. 2011b. Quantitative Study of Individual 
Emotional States in Social Networks. IEEE Transac-
tions on Affective Computing. vol.3(2), Pages 132-
144. 
Wan X. and J. Yang. 2008. Multi-document Summari-
zation using Cluster-based Link Analysis. In Pro-
ceedings of SIGIR-08. 
Wan X. 2011. Using Bilingual Information for Cross-
Language Document Summarization. In Proceedings 
of ACL-11. 
Wang H. and G. Zhou. 2012. Toward a Unified Frame-
work for Standard and Update Multi-Document 
Summarization. ACM Transactions on Asian Lan-
guage Information Processing. vol.11(2). 
Xing E, M. Jordan, and S. Russell. 2003. A Generalized 
Mean Field Algorithm for Variational Inference in 
Exponential Families. In Proceedings of UAI-03. 
Yang S., B. Long, A. Smola, N. Sadagopan, Z. Zheng 
and H. Zha. 2011a. Like like alike ? Joint Friend-
ship and Interest Propagation in Social Networks. In 
Proceeding of WWW-11. 
Yang Z., K. Cai, J. Tang, L. Zhang, Z. Su and J. Li. 
2011b. Social Context Summarization. In Proceed-
ing of SIGIR-11. 
Zhuang H, J. Tang, W. Tang, T. Lou, A. Chin, and X. 
Wang. 2012. Actively Learning to Infer Social Ties. 
In Proceedings of Data Mining and Knowledge Dis-
covery (DMKD-12), vol.25 (2), pages 270-297. 
725
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 68?77,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
A Constituent-Based Approach to Argument Labeling
with Joint Inference in Discourse Parsing
Fang Kong
1?
Hwee Tou Ng
2
Guodong Zhou
1
1
School of Computer Science and Technology, Soochow University, China
2
Department of Computer Science, National University of Singapore
kongfang@suda.edu.cn nght@comp.nus.edu.sg gdzhou@suda.edu.cn
Abstract
Discourse parsing is a challenging task
and plays a critical role in discourse anal-
ysis. In this paper, we focus on label-
ing full argument spans of discourse con-
nectives in the Penn Discourse Treebank
(PDTB). Previous studies cast this task
as a linear tagging or subtree extraction
problem. In this paper, we propose a
novel constituent-based approach to argu-
ment labeling, which integrates the ad-
vantages of both linear tagging and sub-
tree extraction. In particular, the pro-
posed approach unifies intra- and inter-
sentence cases by treating the immediate-
ly preceding sentence as a special con-
stituent. Besides, a joint inference mech-
anism is introduced to incorporate glob-
al information across arguments into our
constituent-based approach via integer lin-
ear programming. Evaluation on PDT-
B shows significant performance improve-
ments of our constituent-based approach
over the best state-of-the-art system. It al-
so shows the effectiveness of our joint in-
ference mechanism in modeling global in-
formation across arguments.
1 Introduction
Discourse parsing determines the internal struc-
ture of a text and identifies the discourse rela-
tions between its text units. It has attracted in-
creasing attention in recent years due to its impor-
tance in text understanding, especially since the
release of the Penn Discourse Treebank (PDTB)
corpus (Prasad et al., 2008), which adds a layer of
discourse annotations on top of the Penn Treebank
?
The research reported in this paper was carried out while
Fang Kong was a research fellow at the National University
of Singapore.
(PTB) corpus (Marcus et al., 1993). As the largest
available discourse corpus, the PDTB corpus has
become the defacto benchmark in recent studies
on discourse parsing.
Compared to connective identification and dis-
course relation classification in discourse parsing,
the task of labeling full argument spans of dis-
course connectives is much harder and thus more
challenging. For connective identification, Lin et
al. (2014) achieved the performance of 95.76%
and 93.62% in F-measure using gold-standard and
automatic parse trees, respectively. For discourse
relation classification, Lin et al. (2014) achieved
the performance of 86.77% in F-measure on clas-
sifying discourse relations into 16 level 2 types.
However, for argument labeling, Lin et al. (2014)
only achieved the performance of 53.85% in F-
measure using gold-standard parse trees and con-
nectives, much lower than the inter-annotation a-
greement of 90.20% (Miltsakaki et al., 2004).
In this paper, we focus on argument labeling in
the PDTB corpus. In particular, we propose a nov-
el constituent-based approach to argument label-
ing which views constituents as candidate argu-
ments. Besides, our approach unifies intra- and
inter-sentence cases by treating the immediately
preceding sentence as a special constituent. Final-
ly, a joint inference mechanism is introduced to
incorporate global information across arguments
via integer linear programming. Evaluation on the
PDTB corpus shows the effectiveness of our ap-
proach.
The rest of this paper is organized as follows.
Section 2 briefly introduces the PDTB corpus.
Related work on argument labeling is reviewed
in Section 3. In Section 4, we describe our
constituent-based approach to argument labeling.
In Section 5, we present our joint inference mech-
anism via integer linear programming (ILP). Sec-
tion 6 gives the experimental results and analysis.
Finally, we conclude in Section 7.
68
2 Penn Discourse Treebank
As the first large-scale annotated corpus that fol-
lows the lexically grounded, predicate-argument
approach in D-LTAG (Lexicalized Tree Adjoin-
ing Grammar for Discourse) (Webber, 2004), the
PDTB regards a connective as the predicate of a
discourse relation which takes exactly two text s-
pans as its arguments. In particular, the text span
that the connective is syntactically attached to is
called Arg2, and the other is called Arg1.
Although discourse relations can be either ex-
plicitly or implicitly expressed in PDTB, this pa-
per focuses only on explicit discourse relations
that are explicitly signaled by discourse connec-
tives. Example (1) shows an explicit discourse re-
lation from the article wsj 2314 with connective
so underlined, Arg1 span italicized, and Arg2 s-
pan bolded.
(1) But its competitors have much broader busi-
ness interests and so are better cushioned
against price swings .
Note that a connective and its arguments can ap-
pear in any relative order, and an argument can be
arbitrarily far away from its corresponding con-
nective. Although the position of Arg2 is fixed
once the connective is located, Arg1 can occur in
the same sentence as the connective (SS), in a sen-
tence preceding that of the connective (PS), or in
a sentence following that of the connective (FS),
with proportions of 60.9%, 39.1%, and less than
0.1% respectively for explicit relations in the PDT-
B corpus (Prasad et al., 2008). Besides, out of
all PS cases where Arg1 occurs in some preced-
ing sentence, 79.9% of them are the exact imme-
diately preceding sentence. As such, in this paper,
we only consider the current sentence containing
the connective and its immediately preceding sen-
tence as the text span where Arg1 occurs, similar
to what was done in (Lin et al., 2014).
3 Related Work
For argument labeling in discourse parsing on the
PDTB corpus, the related work can be classified
into two categories: locating parts of arguments,
and labeling full argument spans.
As a representative on locating parts of argu-
ments, Wellner and Pustejovsky (2007) proposed
several machine learning approaches to identify
the head words of the two arguments for discourse
connectives. Following this work, Elwell and
Baldridge (2008) combined general and connec-
tive specific rankers to improve the performance
of labeling the head words of the two arguments.
Prasad et al. (2010) proposed a set of heuristics to
locate the position of the Arg1 sentences for inter-
sentence cases. The limitation of locating parts of
arguments, such as the positions and head word-
s, is that it is only a partial solution to argument
labeling in discourse parsing.
In comparison, labeling full argument spans can
provide a complete solution to argument labeling
in discourse parsing and has thus attracted increas-
ing attention recently, adopting either a subtree
extraction approach (Dinesh et al. (2005), Lin et
al. (2014)) or a linear tagging approach (Ghosh et
al. (2011)).
As a representative subtree extraction approach,
Dinesh et al. (2005) proposed an automatic tree
subtraction algorithm to locate argument spans for
intra-sentential subordinating connectives. How-
ever, only dealing with intra-sentential subordinat-
ing connectives is not sufficient since they con-
stitute only 40.93% of all cases. Instead, Lin et
al. (2014) proposed a two-step approach. First, an
argument position identifier was employed to lo-
cate the position of Arg1. For the PS case, it di-
rectly selects the immediately preceding sentence
as Arg1. For other cases, an argument node iden-
tifier was employed to locate the Arg1- and Arg2-
nodes. Next, a tree subtraction algorithm was used
to extract the arguments. However, as pointed out
in Dinesh et al. (2005), it is not necessarily the
case that a connective, Arg1, or Arg2 is dominated
by a single node in the parse tree (that is, it can be
dominated by a set of nodes). Figure 1 shows the
gold-standard parse tree corresponding to Exam-
ple (1). It shows that Arg1 includes three nodes:
[
CC
But], [
NP
its competitors], [
V P
have much
broader business interests], and Arg2 includes t-
wo nodes: [
CC
and], [
V P
are better cushioned a-
gainst price swings]. Therefore, such an argumen-
t node identifier has inherent shortcomings in la-
beling arguments. Besides, the errors propagat-
ed from the upstream argument position classifier
may adversely affect the performance of the down-
stream argument node identifier.
As a representative linear tagging approach,
Ghosh et al. (2011) cast argument labeling as a lin-
ear tagging task using conditional random fields.
Ghosh et al. (2012) further improved the perfor-
69
SVP
VP
VP
PP
NP
NNS
swings
NN
price
IN
against
VBN
cushioned
ADVP
RBR
better
VBP
are
RB
so
CC
and
VP
NP
NNS
interests
NN
business
ADJP
JJR
broader
RB
much
VBP
have
NP
NNS
competitors
PRP
its
CC
But
Figure 1: The gold-standard parse tree corresponding to Example (1)
mance with integration of the n-best results.
While the subtree extraction approach locates
argument spans based on the nodes of a parse tree
and is thus capable of using rich syntactic informa-
tion, the linear tagging approach works on the to-
kens in a sentence and is thus capable of capturing
local sequential dependency between tokens. In
this paper, we take advantage of both subtree ex-
traction and linear tagging approaches by propos-
ing a novel constituent-based approach. Further-
more, intra- and inter-sentence cases are unified
by treating the immediately preceding sentence as
a special constituent. Finally, a joint inference
mechanism is proposed to add global information
across arguments.
4 A Constituent-Based Approach to
Argument Labeling
Our constituent-based approach works by first
casting the constituents extracted from a parse tree
as argument candidates, then determining the role
of every constituent as part of Arg1, Arg2, or
NULL, and finally, merging all the constituents
for Arg1 and Arg2 to obtain the Arg1 and Arg2
text spans respectively. Obviously, the key to
the success of our constituent-based approach is
constituent-based argument classification, which
determines the role of every constituent argument
candidate.
As stated above, the PDTB views a connective
as the predicate of a discourse relation. Similar
to semantic role labeling (SRL), for a given con-
nective, the majority of the constituents in a parse
tree may not be its arguments (Xue and Palmer,
2004). This indicates that negative instances (con-
stituents marked NULL) may overwhelm positive
instances. To address this problem, we use a
simple algorithm to prune out these constituents
which are clearly not arguments to the connective
in question.
4.1 Pruning
The pruning algorithm works recursively in pre-
processing, starting from the target connective n-
ode, i.e. the lowest node dominating the connec-
tive. First, all the siblings of the connective node
are collected as candidates, then we move on to
the parent of the connective node and collect it-
s siblings, and so on until we reach the root of
the parse tree. In addition, if the target connec-
tive node does not cover the connective exactly,
the children of the target connective node are also
collected.
For the example shown in Figure 1, we can lo-
cate the target connective node [
RB
so] and return
five constituents ? [
V P
have much broader busi-
ness interests], [
CC
and], [
V P
are better cushioned
against price swings], [
CC
But], and [
NP
its com-
petitors] ? as argument candidates.
It is not surprising that the pruning algorithm
works better on gold parse trees than automatic
parse trees. Using gold parse trees, our pruning al-
gorithm can recall 89.56% and 92.98% (489 out of
546 Arg1s, 808 out of 869 Arg2s in the test data)
of the Arg1 and Arg2 spans respectively and prune
out 81.96% (16284 out of 19869) of the nodes in
the parse trees. In comparison, when automatic
parse trees (based on the Charniak parser (Char-
niak, 2000)) are used, our pruning algorithm can
recall 80.59% and 89.87% of the Arg1 and Arg2
spans respectively and prune out 81.70% (16190
70
Feature Description Example
CON-Str The string of the given connective (case-sensitive) so
CON-LStr The lowercase string of the given connective so
CON-Cat
The syntactic category of the given connective: sub-
ordinating, coordinating, or discourse adverbial
Subordinating
CON-iLSib Number of left siblings of the connective 2
CON-iRSib Number of right siblings of the connective 1
NT-Ctx
The context of the constituent. We use POS combi-
nation of the constituent, its parent, left sibling and
right sibling to represent the context. When there is
no parent or siblings, it is marked NULL.
VP-VP-NULL-CC
CON-NT-Path
The path from the parent node of the connective to
the node of the constituent
RB ? V P ? V P
CON-NT-Position
The position of the constituent relative to the connec-
tive: left, right, or previous
left
CON-NT-Path-iLsib
The path from the parent node of the connective to
the node of the constituent and whether the number
of left siblings of the connective is greater than one
RB ? V P ? V P :>1
Table 1: Features employed in argument classification.
out of 19816) of the nodes in the parse trees.
4.2 Argument Classification
In this paper, a multi-category classifier is em-
ployed to determine the role of an argument can-
didate (i.e., Arg1, Arg2, or NULL). Table 1 lists
the features employed in argument classification,
which reflect the properties of the connective and
the candidate constituent, and the relationship be-
tween them. The third column of Table 1 shows
the features corresponding to Figure 1, consider-
ing [
RB
so] as the given connective and [
V P
have
much broader business interests] as the constituent
in question.
Similar to Lin et al. (2014), we obtained the syn-
tactic category of the connectives from the list pro-
vided in Knott (1996). However, different from
Lin et al. (2014), only the siblings of the root path
nodes (i.e., the nodes occurring in the path of the
connective to root) are collected as the candidate
constituents in the pruning stage, and the value of
the relative position can be left or right, indicat-
ing that the constituent is located on the left- or
right-hand of the root path respectively. Besides,
we view the root of the previous sentence as a spe-
cial candidate constituent. For example, the value
of the feature CON-NT-Position is previous when
the current constituent is the root of the previous
sentence. Finally, we use the part-of-speech (POS)
combination of the constituent itself, its parent n-
ode, left sibling node and right sibling node to rep-
resent the context of the candidate constituent. In-
tuitively, this information can help determine the
role of the constituent.
For the example shown in Figure 1, we first em-
ploy the pruning algorithm to get the candidate
constituents, and then employ our argument clas-
sifier to determine the role for every candidate.
For example, if the five candidates are labeled as
Arg1, Arg2, Arg2, Arg1, and Arg1, respectively,
we merge all the Arg1 constituents to obtain the
Arg1 text span (i.e., But its competitors have much
broader business interests). Similarly, we merge
the two Arg2 constituents to obtain the Arg2 text s-
pan (i.e., and are better cushioned against price
swings).
5 Joint Inference via Integer Linear
Programming
In the above approach, decisions are always made
for each candidate independently, ignoring global
information across candidates in the final output.
For example, although an argument span can be
split into multiple discontinuous segments (e.g.,
the Arg2 span of Example (1) contains two dis-
continuous segments, and, are better cushioned
against price swings), the number of discontinu-
ous segments is always limited. Statistics on the
PDTB corpus shows that the number of discontin-
71
uous segments for both Arg1 and Arg2 is generally
(>= 99%) at most 2. For Example (1), from left
to right, we can obtain the list of constituent can-
didates: [
CC
But], [
NP
its competitors], [
V P
have
much broader business interests], [
CC
and], [
V P
are better cushioned against price swings]. If our
argument classifier wrongly determines the roles
as Arg1, Arg2, Arg1, Arg2, and Arg1 respectively,
we can find that the achieved Arg1 span contains
three discontinuous segments. Such errors may be
corrected from a global perspective.
In this paper, a joint inference mechanism is in-
troduced to incorporate various kinds of knowl-
edge to resolve the inconsistencies in argumen-
t classification to ensure global legitimate predic-
tions. In particular, the joint inference mechanism
is formalized as a constrained optimization prob-
lem, represented as an integer linear programming
(ILP) task. It takes as input the argument classi-
fiers? confidence scores for each constituent can-
didate along with a list of constraints, and outputs
the optimal solution that maximizes the objective
function incorporating the confidence scores, sub-
ject to the constraints that encode various kinds of
knowledge.
In this section, we meet the requirement of ILP
with focus on the definition of variables, the objec-
tive function, and the problem-specific constraints,
along with ILP-based joint inference integrating
multiple systems.
5.1 Definition of Variables
Given an input sentence, the task of argumen-
t labeling is to determine what labels should be
assigned to which constituents corresponding to
which connective. It is therefore natural that en-
coding the output space of argument labeling re-
quires various kinds of information about the con-
nectives, the argument candidates corresponding
to a connective, and their argument labels.
Given an input sentence s, we define following
variables:
(1) P : the set of connectives in a sentence.
(2) p ? P : a connective in P .
(3) C(p): the set of argument candidates corre-
sponding to connective p. (i.e., the parse tree
nodes obtained in the pruning stage).
(4) c ? C(p): an argument candidate.
(5) L: the set of argument labels {Arg1, Arg2,
NULL }.
(6) l ? L: an argument label in L.
In addition, we define the integer variables as
follows:
Z
l
c,p
? {0, 1} (1)
If Z
l
c,p
= 1, the argument candidate c, which
corresponds to connective p, should be assigned
the label l. Otherwise, the argument candidate c is
not assigned this label.
5.2 The Objective Function
The objective of joint inference is to find the best
arguments for all the connectives in one sentence.
For every connective, the pruning algorithm is first
employed to determine the set of corresponding
argument candidates. Then, the argument classifi-
er is used to assign a label to every candidate. For
an individual labeling Z
l
c,p
, we measure the quality
based on the confidence scores, f
l,c,p
, returned by
the argument classifier. Thus, the objective func-
tion can be defined as
max
?
l,c,p
f
l,c,p
Z
l
c,p
(2)
5.3 Constraints
As the key to the success of ILP-based joint infer-
ence, the following constraints are employed:
Constraint 1: The arguments corresponding
to a connective cannot overlap with the connec-
tive. Let c
1
, c
2
..., c
k
be the argument candidates
that correspond to the same connective and over-
lap with the connective in a sentence.
1
Then this
constraint ensures that none of them will be as-
signed as Arg1 or Arg2.
k
?
i=1
Z
NULL
c
i
,p
= k (3)
Constraint 2: There are no overlapping or em-
bedding arguments. Let c
1
, c
2
..., c
k
be the argu-
ment candidates that correspond to the same con-
nective and cover the same word in a sentence.
2
1
Only when the target connective node does not cover the
connective exactly and our pruning algorithm collects all the
children of the target connective node as part of constituent
candidates, such overlap can be introduced.
2
This constraint only works in system combination of
Section 5.4, where additional phantom candidates may intro-
duce such overlap.
72
Then this constraint ensures that at most one of
the constituents can be assigned as Arg1 or Arg2.
That is, at least k ? 1 constituents should be as-
signed the special label NULL.
k
?
i=1
Z
NULL
c
i
,p
? k ? 1 (4)
Constraint 3: For a connective, there is at least
one constituent candidate assigned as Arg2.
?
c
Z
Arg2
c,p
? 1 (5)
Constraint 4: Since we view the previous com-
plete sentence as a special Arg1 constituent candi-
date, denoted as m, there is at least one candidate
assigned as Arg1 for every connective.
?
c
Z
Arg1
c,p
+ Z
Arg1
m,p
? 1 (6)
Constraint 5: The number of discontinuous
constituents assigned as Arg1 or Arg2 should be at
most 2. That is, if argument candidates c
1
, c
2
..., c
k
corresponding to the same connective are discon-
tinuous, this constraint ensures that at most two
of the constituents can be assigned the same label
Arg1 or Arg2.
k
?
i=1
Z
Arg1
c
i
,p
? 2, and
k
?
i=1
Z
Arg2
c
i
,p
? 2 (7)
5.4 System Combination
Previous work shows that the performance of ar-
gument labeling heavily depends on the quality of
the syntactic parser. It is natural that combining
different argument labeling systems on differen-
t parse trees can potentially improve the overall
performance of argument labeling.
To explore this potential, we build two argu-
ment labeling systems ? one using the Berke-
ley parser (Petrov et al., 2006) and the other the
Charniak parser (Charniak, 2000). Previous s-
tudies show that these two syntactic parsers tend
to produce different parse trees for the same sen-
tence (Zhang et al., 2009). For example, our pre-
liminary experiment shows that applying the prun-
ing algorithm on the output of the Charniak parser
produces a list of candidates with recall of 80.59%
and 89.87% for Arg1 and Arg2 respectively, while
achieving recall of 78.6% and 91.1% for Arg1 and
Arg2 respectively on the output of the Berkeley
	






	
 













Figure 2: An example on unifying different candi-
dates.
parser. It also shows that combining these two can-
didate lists significantly improves recall to 85.7%
and 93.0% for Arg1 and Arg2, respectively.
In subsection 5.2, we only consider the con-
fidence scores returned by an argument classifier.
Here, we proceed to combine the probabilities pro-
duced by two argument classifiers. There are two
remaining problems to resolve:
? How do we unify the two candidate lists?
In principle, constituents spanning the same
sequence of words should be viewed as the
same candidate. That is, for different can-
didates, we can unify them by adding phan-
tom candidates. This is similar to the ap-
proach proposed by Punyakanok et al. (2008)
for the semantic role labeling task. For exam-
ple, Figure 2 shows the candidate lists gen-
erated by our pruning algorithm based on t-
wo different parse trees given the segment
?its competitors have much broader business
interests?. Dashed lines are used for phan-
tom candidates and solid lines for true can-
didates. Here, system A produces one can-
didate a1, with two phantom candidates a2
and a3 added. Analogously, phantom can-
didate b3 is added to the candidate list out-
put by System B. In this way, we can get the
unified candidate list: ?its competitors have
much broader business interests?, ?its com-
petitors?, ?have much broader business inter-
ests?.
? How do we compute the confidence score for
every decision? For every candidate in the
unified list, we first determine whether it is
a true candidate based on the specific parse
tree. Then, for a true candidate, we extrac-
t the features from the corresponding parse
73
tree. On this basis, we can determine the
confidence score using our argument classi-
fier. For a phantom candidate, we set the
same prior distribution as the confidence s-
core. In particular, the probability of the
?NULL? class is set to 0.55, following (Pun-
yakanok et al., 2008), and the probabilities of
Arg1 and Arg2 are set to their occurrence fre-
quencies in the training data. For the example
shown in Figure 2, since System A return-
s ?its competitors have much broader busi-
ness interests? as a true candidate, we can ob-
tain its confidence score using our argumen-
t classifier. For the two phantom candidates
? ?its competitors? and ?have much broader
business interests? ? we use the prior dis-
tributions directly. This applies to the candi-
dates for System B. Finally, we simply aver-
age the estimated probabilities to determine
the final probability estimate for every candi-
date in the unified list.
6 Experiments
In this section, we systematically evaluate our
constituent-based approach with a joint inference
mechanism to argument labeling on the PDTB
corpus.
6.1 Experimental settings
All our classifiers are trained using the OpenNLP
maximum entropy package
3
with the default pa-
rameters (i.e. without smoothing and with 100
iterations). As the PDTB corpus is aligned with
the PTB corpus, the gold parse trees and sentence
boundaries are obtained from PTB. Under the au-
tomatic setting, the NIST sentence segmenter
4
and
the Charniak parser
5
are used to segment and parse
the sentences, respectively. lp solve
6
is used for
our joint inference.
This paper focuses on automatically labeling
the full argument spans of discourse connec-
tives. For a fair comparison with start-of-the-
art systems, we use the NUS PDTB-style end-
to-end discourse parser
7
to perform other sub-
tasks of discourse parsing except argument label-
ing, which includes connective identification, non-
3
http://maxent.sourceforge.net/
4
http://duc.nist.gov/duc2004/software/duc2003
.breakSent.tar.gz
5
ftp://ftp.cs.brown.edu/pub/nlparser/
6
http://lpsolve.sourceforge.net/
7
http://wing.comp.nus.edu.sg/ linzihen/parser/
explicit discourse relation identification and clas-
sification.
Finally, we evaluate our system on two aspects:
(1) the dependence on the parse trees (GS/Auto,
using gold standard or automatic parse trees and
sentence boundaries); and (2) the impact of errors
propagated from previous components (noEP/EP,
using gold annotation or automatic results from
previous components). In combination, we have
four different settings: GS+noEP, GS+EP, Au-
to+noEP and Auto+EP. Same as Lin et al. (2014),
we report exact match results under these four set-
tings. Here, exact match means two spans match
identically, except beginning or ending punctua-
tion symbols.
6.2 Experimental results
We first evaluate the effectiveness of our
constituent-based approach by comparing our sys-
tem with the state-of-the-art systems, ignoring
the joint inference mechanism. Then, the con-
tribution of the joint inference mechanism to our
constituent-based approach, and finally the contri-
bution of our argument labeling system to the end-
to-end discourse parser are presented.
Effectiveness of our constituent-based ap-
proach
By comparing with two state-of-the-art argu-
ment labeling approaches, we determine the effec-
tiveness of our constituent-based approach.
Comparison with the linear tagging approach
As a representative linear tagging approach,
Ghosh et al. (2011; 2012; 2012) only reported the
exact match results for Arg1 and Arg2 using the
evaluation script for chunking evaluation
8
under
GS+noEP setting with Section 02?22 of the PDTB
corpus for training, Section 23?24 for testing, and
Section 00?01 for development. It is also worth
mentioning that an argument span can contain
multiple discontinuous segments (i.e., chunks), so
chunking evaluation only shows the exact match
of every argument segment but not the exact match
of every argument span. In order to fairly compare
our system with theirs, we evaluate our system us-
ing both the exact metric and the chunking eval-
uation. Table 2 compares the results of our sys-
tem without joint inference and the results report-
ed by Ghosh et al. (2012) on the same data split.
We can find that our system performs much bet-
8
http://www.cnts.ua.ac.be/conll2000/chunking/
conlleval.txt
74
ter than Ghosh?s on both Arg1 and Arg2, even on
much stricter metrics.
Systems Arg1 Arg2
ours using exact match 65.68 84.50
ours using chunking evaluation 67.48 88.08
reported by Ghosh et al. (2012) 59.39 79.48
Table 2: Performance (F1) comparison of our ar-
gument labeling approach with the linear tagging
approach as adopted in Ghosh et al. (2012)
Comparison with the subtree extracting ap-
proach
For a fair comparison, we also conduct our
experiments on the same data split of Lin et
al. (2014) with Section 02 to 21 for training, Sec-
tion 22 for development, and Section 23 for test-
ing. Table 3 compares our labeling system without
joint inference with Lin et al. (2014), a representa-
tive subtree extracting approach. From the results,
we find that the performance of our argument la-
beling system significantly improves under all set-
tings. This is because Lin et al. (2014) considered
all the internal nodes of the parse trees, whereas
the pruning algorithm in our approach can effec-
tively filter out those unlikely constituents when
determining Arg1 and Arg2.
Setting Arg1 Arg2
Arg1&2
ours
GS+noEP 62.84 84.07 55.69
GS+EP 61.46 81.30 54.31
Auto+EP 56.04 76.53 48.89
Lin?s
GS+noEP 59.15 82.23 53.85
GS+EP 57.64 79.80 52.29
Auto+EP 47.68 70.27 40.37
Table 3: Performance (F1) comparison of our ar-
gument labeling approach with the subtree extrac-
tion approach as adopted in Lin et al. (2014)
As justified above, by integrating the advan-
tages of both linear tagging and subtree extraction,
our constituent-based approach can capture both
rich syntactic information from parse trees and
local sequential dependency between tokens. The
results show that our constituent-based approach
indeed significantly improves the performance
of argument labeling, compared to both linear
tagging and subtree extracting approaches.
Contribution of Joint Inference
Same as Lin et al. (2014), we conduct our ex-
periments using Section 02 to 21 for training, Sec-
tion 22 for development, and Section 23 for test-
ing. Table 4 lists the performance of our argumen-
t labeling system without and with ILP inference
under four different settings, while Table 5 reports
the contribution of system combination. It shows
the following:
? On the performance comparison of Arg1 and
Arg2, the performance on Arg2 is much bet-
ter than that on Arg1 with the performance
gap up to 8% under different settings. This is
due to the fact that the relationship between
Arg2 and the connective is much closer. This
result is also consistent with previous studies
on argument labeling.
? On the impact of error propagation from con-
nective identification, the errors propagated
from connective identification reduce the per-
formance of argument labeling by less than
2% in both Arg1 and Arg2 F-measure under
different settings.
? On the impact of parse trees, using automat-
ic parse trees reduces the performance of ar-
gument labeling by about 5.5% in both Arg1
and Arg2 F-measure under different settings.
In comparison with the impact of error prop-
agation, parse trees have much more impact
on argument labeling.
? On the impact of joint inference, it improves
the performance of argument labeling, espe-
cially on automatic parse trees by about 2%.
9
? On the impact of system combination, the
performance is improved by about 1.5%.
Setting Arg1 Arg2
Arg1&2
without
Joint
Inference
GS+noEP 62.84 84.07 55.69
GS+EP 61.46 81.30 54.31
Auto+noEP 57.75 79.85 50.27
Auto+EP 56.04 76.53 48.89
with
Joint
Inference
GS+noEP 65.76 83.86 58.18
GS+EP 63.96 81.19 56.37
Auto+noEP 60.24 79.74 52.55
Auto+EP 58.10 76.53 50.73
Table 4: Performance (F1) of our argument label-
ing approach.
Contribution to the end-to-end discourse pars-
er
9
Unless otherwise specified, all the improvements in this
paper are significant with p < 0.001.
75
Systems Setting Arg1 Arg2
Arg1&2
Charniak
noEP 60.24 79.74 52.55
EP 58.10 76.53 50.73
Berkeley
noEP 60.78 80.07 52.98
EP 58.80 77.21 51.43
Combined
noEP 61.97 80.61 54.50
EP 59.72 77.55 52.52
Table 5: Contribution of System Combination in
Joint Inference.
Lastly, we focus on the contribution of our ar-
gument labeling approach to the overall perfor-
mance of the end-to-end discourse parser. This
is done by replacing the argument labeling mod-
el of the NUS PDTB-style end-to-end discourse
parser with our argument labeling model. Table 6
shows the results using gold parse trees and auto-
matic parse trees, respectively.
10
From the results,
we find that using gold parse trees, our argument
labeling approach significantly improves the per-
formance of the end-to-end system by about 1.8%
in F-measure, while using automatic parse trees,
the improvement significantly enlarges to 6.7% in
F-measure.
Setting New d-parser Lin et al.?s (2014)
GS 34.80 33.00
Auto 27.39 20.64
Table 6: Performance (F1) of the end-to-end dis-
course parser.
7 Conclusion
In this paper, we focus on the problem of auto-
matically labeling the full argument spans of dis-
course connectives. In particular, we propose a
constituent-based approach to integrate the advan-
tages of both subtree extraction and linear tagging
approaches. Moreover, our proposed approach in-
tegrates inter- and intra-sentence argument label-
ing by viewing the immediately preceding sen-
tence as a special constituent. Finally, a join-
t inference mechanism is introduced to incorpo-
rate global information across arguments into our
10
Further analysis found that the error propagated from
sentence segmentation can reduce the performance of the
end-to-end discourse parser. Retraining the NIST sentence
segmenter using Section 02 to 21 of the PDTB corpus, the
original NUS PDTB-style end-to-end discourse parser can
achieve the performance of 25.25% in F-measure, while the
new version (i.e. replace the argument labeling model with
our argument labeling model) can achieve the performance
of 30.06% in F-measure.
constituent-based approach via integer linear pro-
gramming.
Acknowledgments
This research is supported by the Singapore Na-
tional Research Foundation under its International
Research Centre @ Singapore Funding Initiative
and administered by the IDM Programme Office.
This research is also partially supported by Key
project 61333018 and 61331011 under the Nation-
al Natural Science Foundation of China.
References
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of the First Meeting
of the North American Chapter of the Association
for Computational Linguistics, pages 132?139.
Nikhil Dinesh, Alan Lee, Eleni Miltsakaki, Rashmi
Prasad, Aravind Joshi, and Bonnie Webber. 2005.
Attribution and the (non-)alignment of syntactic and
discourse arguments of connectives. In Proceedings
of the Workshop on Frontiers in Corpus Annotation
II: Pie in the Sky, pages 29?36.
Robert Elwell and Jason Baldridge. 2008. Discourse
connective argument identification with connective
specific rankers. In Second IEEE International Con-
ference on Semantic Computing, pages 198?205.
Sucheta Ghosh, Richard Johansson, Giuseppe Riccar-
di, and Sara Tonelli. 2011. Shallow discourse pars-
ing with conditional random fields. In Proceedings
of the 5th International Joint Conference on Natural
Language Processing, pages 1071?1079.
Sucheta Ghosh, Giuseppe Riccardi, and Richard Jo-
hansson. 2012. Global features for shallow dis-
course parsing. In Proceedings of the 13th Annual
Meeting of the Special Interest Group on Discourse
and Dialogue, pages 150?159.
Sucheta Ghosh. 2012. End-to-End Discourse Parsing
using Cascaded Structured Prediction. Ph.D. thesis,
University of Trento.
Alistair Knott. 1996. A Data-Driven Methodology for
Motivating a Set of Coherence Relations. Ph.D. the-
sis, University of Edinburgh.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2014. A
PDTB-styled end-to-end discourse parser. Natural
Language Engineering, 20(2):151?184.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
76
Eleni Miltsakaki, Rashmi Prasad, Aravind Joshi, and
Bonnie Webber. 2004. The Penn Discourse Tree-
bank. In Proceedings of the Fourth International
Conference on Language Resources and Evaluation,
pages 2237?2240.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and
interpretable tree annotation. In Proceedings of
the 21st International Conference on Computational
Linguistics and the 44th Annual Meeting of the As-
sociation for Computational Linguistics, pages 433?
440.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bonnie
Webber. 2008. The Penn Discourse TreeBank 2.0.
In Proceedings of the LREC 2008 Conference, pages
2961?2968.
Rashmi Prasad, Aravind Joshi, and Bonnie Webber.
2010. Exploiting scope for shallow discourse pars-
ing. In Proceedings of the Seventh International
Conference on Language Resources and Evaluation.
Vasin Punyakanok, Dan Roth, and Wen-tau Yih. 2008.
The important of syntactic parsing and inference in
semantic role labeling. Computational Linguistics,
34(2):257?287.
Bonnie Webber. 2004. D-LTAG: extending lexicalized
TAG to discourse. Cognitive Science, 28(5):751?
779.
Ben Wellner and James Pustejovsky. 2007. Automat-
ically identifying the arguments of discourse con-
nectives. In Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, pages 92?101.
Nianwen Xue and Martha Palmer. 2004. Calibrating
features for semantic role labeling. In Proceedings
of 2004 Conference on Empirical Methods in Natu-
ral Language Processing, pages 88?94.
Hui Zhang, Min Zhang, Chew Lim Tan, and Haizhou
Li. 2009. K-best combination of syntactic parsers.
In Proceedings of the 2009 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1552?1560.
77
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2105?2114,
October 25-29, 2014, Doha, Qatar. c?2014 Association for Computational Linguistics
Building Chinese Discourse Corpus with Connective-driven 
Dependency Tree Structure 
 
Yancui Li1,2 Wenhe Feng2 Jing Sun1 Fang Kong1 Guodong Zhou1 
1Natural Language Processing Lab, School of Computer Science and Technology, Soochow 
University, Suzhou 215006, China 
2Henan Institute of Science and Technology, Xinxiang 453003, China 
{yancuili, wenhefeng}@gmail.com {20104027009, kongfang, gdzhou}@suda.edu.cn 
 
 
 
 
 
Abstract 
In this paper, we propose a Connective-
driven Dependency Tree (CDT) scheme 
to represent the discourse rhetorical 
structure in Chinese language, with ele-
mentary discourse units as leaf nodes 
and connectives as non-leaf nodes, large-
ly motivated by the Penn Discourse 
Treebank and the Rhetorical Structure 
Theory. In particular, connectives are 
employed to directly represent the hier-
archy of the tree structure and the rhetor-
ical relation of a discourse, while the nu-
clei of discourse units are globally de-
termined with reference to the depend-
ency theory. Guided by the CDT scheme, 
we manually annotate a Chinese Dis-
course Treebank (CDTB) of 500 docu-
ments. Preliminary evaluation justifies 
the appropriateness of the CDT scheme 
to Chinese discourse analysis and the 
usefulness of our manually annotated 
CDTB corpus. 
1 Introduction 
It is well-known that interpretation of a text re-
quires understanding of its rhetorical relation 
hierarchy since discourse units rarely exist in 
isolation. Such discourse structure is fundamen-
tal to many text-based applications, such as 
summarization (Marcu, 2000) and question-
answering (Verberne et al., 2007). Due to the 
wide and potential use of discourse structure, 
constructing discourse resources has been at-
tracting more and more attention in recent years. 
In comparison with English, there are much 
fewer discourse resources for Chinese which 
largely restricts the researches in Chinese dis-
course analysis. 
The general notion of discourse structure 
mainly consists of discourse unit, connective, 
structure, relation and nuclearity. However, pre-
vious studies on discourse failed to fully express 
these kinds of information. For example, the 
Rhetorical Structure Theory (RST) (Mann and 
Thompson, 1988) represents a discourse as a 
tree with phrases or clauses as elementary dis-
course units (EDUs). However, RST ignores the 
importance of connectives to a great extent. Fig-
ure 1 gives an example tree structure with four 
EDUs (e1-e4). In comparison, Penn Discourse 
Treebank (PDTB) (Prasad et al., 2008) adopts 
the predicate-argument view of discourse rela-
tion, with discourse connective as predicate and 
two text spans as its arguments. Example (1) 
shows an explicit reason relation signaled by the 
discourse connective ?particularly if? and an 
implicit result relation represented by the insert-
ed discourse connective ?so?, with Arg1 in ital-
ics and Arg2 in bold. However, as a connective 
and its arguments are determined in a local con-
textual window, it is normally difficult to deduce 
a complete discourse structure from such a con-
nective-argument scheme. In this sense, the 
PDTB at best only provides a partial solution to 
the discourse structure. 
[Catching up with commercial competitors in retail 
banking and financial services,] e1 [they argue,] e2 
[will be difficult,] e3 [particularly if market condi-
tions turn sour.]e4 e4
e2
e3
condition
same-unit
e1-e4
e1-e3
attributione1-e2
e1
 
Figure 1: An example of discourse structure in RST 
 
Example (1): An example of the connective-argument 
scheme in PDTB 
A)[Catching up with commercial competitors in retail 
banking and financial services will be difficult ]Arg1, 
they argue, will be difficult, particularly if [market 
2105
 conditions turn sour ] Arg2. (Contingency.Condition. 
Hypothetical) (0616) 
B) So much of the stuff poured into its Austin, Texas, 
offices [that its mail rooms there simply stopped de-
livering it.]Arg1 (Implicit = so)[Now, thousands of 
mailers, catalogs and sales pitches go straight into 
the trash.]Arg2 (Contingency.Cause. Result) (0989) 
Obviously, both RST and PDTB have their 
own advantages and disadvantages in represent-
ing different characteristics of the discourse 
structure. In this paper, we attempt to propose a 
new scheme to Chinese discourse structure, 
adopt advantages of the tree structure from RST 
and connective from PDTB. Meanwhile, the 
special characteristics of Chinese discourse 
structure are well addressed. 
First, it is difficult to define EDU in Chinese 
due to the frequent occurrence of the ellipsis of 
subjects, objects and predicates, and the lack of 
functional marks for EDU. Second, the connec-
tives in Chinese omit much more frequently than 
those in English with about 82.0% vs. 54.5% in 
Zhou and Xue (2012). In Example (2), there are 
even no explicit connectives. Third, previous 
studies have shown the difference in classifying 
Chinese discourse relations from English (Xing, 
2001; Huang and Liao, 2011). This suggests that 
the discourse relations defined for English (both 
RST and PDTB) are not readily suitable for 
Chinese. Finally, the nucleus of a Chinese dis-
course relation is normally not directly related to 
a particular relation type but should be dynami-
cally determined from the global meaning of a 
discourse. 
Example (2): An example of discourse with 4 EDUs 
[       ???                ??        ??     ?      ??     
According to reports,Dongguan Customs  total accept 
?? ??      ??   ???????]e1 [?     ??     
company contract record 8400 plus class, than  pilot 
?    ? ?   ???]e2 [   ??       ??   ???]e3 
before a slight increase, company responses well,  
[??             ??            ???]e4 
generally acknowledge acceptance. 
?[According to reports, Dongguan District Cus-
toms accepted more than 8400 records of company 
contracts,] e1 [a slight increase from before the pi-
lot.]e2 [Companies responded well,]e3 [generally 
acknowledging acceptance.]e4? 
In this paper, we present a Connective-driven 
Dependency Tree (CDT) discourse representa-
tion scheme, which takes advantage of both RST 
and PDTB, with elementary discourse units 
(limited to clauses) as leaf nodes and connec-
tives as non-leaf nodes. Especially, we define 
EDU from three aspects, and employ the con-
nective? level and semantic to indicate the rhe-
torical structure and the discourse relation. Be-
sides, the nuclearity of discourse units in a dis-
course relation is decided on the overall dis-
course meaning. On the basis, we adopt the CDT 
scheme to annotate a certain scale corpus, called 
Chinese Discourse Treebank (CDTB) thereafter 
in this paper. Evaluation shows the appropriate-
ness of the CDT scheme to Chinese discourse 
analysis. 
The rest of this paper is organized as follows. 
Section 2 overviews related work. In Section 3, 
we present the CDT discourse representation 
scheme. In Section 4, we describe the annotation 
of the CDTB corpus. Section 5 compares CDTB 
with other major discourse corpora. Section 6 
gives the experimental results on EDU recogni-
tion, the crucial step for discourse parsing. Final-
ly, conclusion is given in section 7. 
2 Related Work 
In the past decade, several discourse corpora for 
English have emerged, with the Rhetorical 
Structure Theory Discourse Treebank (RST-DT) 
(Carlson et al., 2003) and the Penn Discourse 
Treebank (PDTB) (Prasad et al., 2008) most 
prevalent. 
In the RST framework, a text is represented as 
a discourse tree, with non-overlapping text spans 
(either phrases or clauses) as leaves, and adja-
cent nodes are related through particular rhetori-
cal relations to form a discourse sub-tree, which 
is then related to other adjacent nodes in the tree 
structure. According to RST, there are two types 
of discourse relations, mononuclear and multi-
nuclear. Figure 1 shows an example of discourse 
tree representation, following the notational 
convention of RST. Among the four EDUs (e1-
e4), e1 and e2 are connected by a mononuclear 
relation ?attribution?, where e1 is the nucleus, 
the span (e1-e2) and the EDU e3 are further 
connected by a multi-nuclear relation ?same-
unit?, where they are equally salient. Annotated 
according to the RST framework, the RST-DT 
consists of 385 documents from the Wall Street 
Journal (WSJ). Besides, the original 24 dis-
course relations defined by Mann and Thompson 
(1988) are further divided into a set of 18 rela-
tion classes with 78 finer grained rhetorical rela-
tions in RST-DT. 
As the largest discourse corpus so far, the 
Penn Discourse Treebank (PDTB) contains over 
one million words from WSJ. With EDUs lim-
ited to clauses, the PDTB adopts the predicate-
2106
 argument view of discourse relations, with con-
nective as predicate and two text spans as its 
arguments. Example (1) shows two annotation 
tokens for the connective ?particularly if? and 
?so?. The current version of PDTB 2.0 annotates 
40600 tokens, including 18459 explicit relations 
of 100 distinct types (e.g. ?particularly if? and 
?if? are the same type) and 16224 implicit dis-
course relations of 102 distinct token types. Be-
sides, PDTB provides a three level hierarchy of 
relation tags with the first level consisting of 
four major relation classes (Temporal, Contin-
gency, Comparison, and Expansion), which are 
further divided into 16 types and 23 subtypes. 
In comparison, there are few researches on 
Chinese discourse annotation (Xue, 2005a; Chen, 
2006; Yue, 2008; Huang and Chen, 2011; Zhou 
and Xue, 2012), with no exception employing 
existing RST or PDTB frameworks. For exam-
ple, Zhou and Xue (2012) use the PDTB annota-
tion guidelines to annotate Chinese discourse 
with 98 files from Chinese Treebank (Xue et al., 
2005b) of Xinhua newswire. In particular, they 
adopt a lexically grounded approach and make 
some adaptation based on the linguistic and sta-
tistical characteristics of Chinese text, with Arg1 
and Arg2 defined semantically and the senses of 
discourse relations annotated besides connec-
tives and their lexical alternatives. The agree-
ment on relation types reaches 95.1% and the 
agreement on implicit relations with exact span 
match reaches 76.9%. 
Instead, Chen (2006) and Yue (2008) use RST 
to annotate Chinese discourse. Chen (2006) se-
lects comma as the segmentation signal of EDUs 
(in Example (2), ???(According to reports)? 
will be segmented as an EDU), and finds that 
RST fails to deal with some special features of 
Chinese. Yue (2008) manually annotates a set of 
97 texts according to RST and shows the cross-
lingual transferability of RST to Chinese. How-
ever, it also shows that EDUs in Chinese are 
much different from those in English, and many 
relation types in Chinese have no correspond-
ence to English, and vice versa. 
3 Connective-driven Dependency Tree 
An appropriate representation scheme is funda-
mental to linguistic resource construction. With 
reference to various theories and representation 
scheme on the tree structure and nuclearity of 
RST, the connective, relation and discourse 
structure of Chinese complex sentence (Xing, 
2001), the sentence-group theory (Cao, 1984), 
the connective treatment of PDTB, the conjunc-
tion dependent analysis (Feng and Ji, 2011) and 
the center theory of dependency grammar (Hays, 
1964), we propose a new discourse representa-
tion scheme for Chinese, called Connective-
driven Dependency Tree (CDT), with EDUs as 
leaf nodes and connectives as non-leaf nodes, to 
accommodate the special characteristics of the 
Chinese language in discourse structure. 
For instance, Example (3) consists of 2 sen-
tences, which is part of a paragraph from 
?chtb_0001?, and its corresponding CDT repre-
sentation is shown in Figure 2. Here, the number 
of ?|? in Example (3) stands for the level of 
EDUs in CDT and the numbers marked in Fig-
ure 2 (such as 1, 2 etc.) distinguish EDUs. While 
an arrow points to the main EDU or main dis-
course unit (called nucleus), the combination of 
different EDUs can be considered as EDUs in a 
higher level and the new discourse units can thus 
be combined into higher-level units from bottom 
to up. In this way, the discourse structure can be 
expressed as a tree structure via bottom-up com-
bination of EDUs.  
Obviously, such discourse structure is con-
structed by two kinds of basic units, EDUs (leaf 
nodes) and connectives (non-leaf nodes). On the 
one hand, connectives can represent the dis-
course structure by its hierarchical level in the 
tree. The discourse structure is independent on 
the connective level essentially, rather than the 
reverse. On the other hand, connectives them-
selves can represent the discourse relation. This 
is why we call the scheme ?Connective-driven?. 
As for the abstract discourse relation, we can 
construct a set of discourse relations, mapping a 
connective to discourse relation, according to the 
users? specific requirements. 
Example (3): CDT example from CTB 
1 ??    ??   ??    ?   ??    ??  ????? 
Pudong development open up is a promote Shanghai, construct 
???          ??????   ??       ??   ?     ??? 
modern economy, trade, financial century De cross-century 
???|| 2 (??)     ??             ????    ??      ?? 
project, therefore a large number arisen De previously never 
????   ?   ? ??????|  3 (??)?  ??  {??} 
encounter DE new situation, new problem.To this, Pudong not 
??    ?    ??    ??        ??      ????       ??    ? 
simply DE adopting ?does a period time, wait accumulate Le 
   ??     ?????    ??   ??? ?    ???|| 4{??} 
experience after re-enactment laws regulations De approach,but  
??  ??          ??     ?           ??      ?      ??      ?  
learn developed countries and Shenzhen etc. special zone DE 
??     ???|||| 5<??>??          ???       ???? 
experience lesson, Invite at home and abroad revlant expert 
???|||| 6<??>??????         ??  ?       ?? 
2107
 scholars,               actively, timely DI formulate and issuing 
???  ???||| 7 {?} ??    ??  ??    ?    ???  ? 
statutory file, make these economic activity as soon as appear bei 
 ??      ??  ???  
bring into legality track. 
?1 Pudong's development and opening up is a century-
spanning undertaking for vigorously promoting Shanghai and 
constructing a modern economic, trade, and financial center. || 
2 Because of this, new situations and new questions that have 
not been encountered before are emerging in great numbers. | 3 
In response to this, Pudong is not simply adopting an approach 
of "work for a short time and then draw up laws and regula-
tions only after experience has been accumulated.?|| 4 Instead, 
Pudong is taking advantage of the lessons from experience of 
developed countries and special regions such as Shenzhen, ||||5 
by hiring appropriate domestic and foreign specialists and 
scholars, ||||6 actively and promptly formulating and issuing 
regulatory documents. ||| 7 So these economic activities are 
incorporated into the sphere of influence of the legal system as 
soon as they appear.? 
1 2 3 4 5 6 7
??(therefore?
{can be deleted}
??  (for this) 
{can be deleted}
 ??(and)
 <inserted, bad language sense>
?(cause)
{cann?t be deleted}
??...??(is not... but)
{cann?t be deleted}
 
Figure 2: CDT representation of Example (3) 
3.1 Elementary Discourse Unit 
As the leaf nodes of CDT, EDUs are limited to 
clauses. In principle, EDUs play a crucial role to 
discourse analysis. Since from bottom-up dis-
course combination, EDUs are the start of dis-
course analysis, while from top-down discourse 
segmentation, they are the end of discourse 
analysis. Unfortunately, since there lacks obvi-
ous distinction between Chinese sentence struc-
ture and phrase structure, it is rather difficult to 
define Chinese EDU (clause). Till now, there is 
still no widely accepted definition in the Chinese 
linguistics community (Wang, 2010). Inspired 
by Li et al. (2013a), we give the definition of 
Chinese EDU from three perspectives. First, 
from the syntactic structure perspective, an EDU 
should contain at least one predicate and express 
at least one proposition. Second, from the func-
tional perspective, an EDU should be related to 
other EDUs with some propositional function, 
i.e. not act as a grammatical element of other 
EDUs. Finally, from the morphological perspec-
tive, an EDU should be segmented by some 
punctuation, e.g. comma, semicolon and period. 
We use punctuation because there usually has a 
pause between clauses (EDUs), which can be 
shown in written commas, semicolons etc 
(Huang and Liao, 2011). Normally, it is easy to 
handle complex sentences and special sentence 
patterns (e.g. serial predicate sentences). For 
Example (4), A) is a single sentence with serial 
predicate; B) is complex sentence with two 
EDUs (clauses): 
Example (4): EDU examples 
A) He opened the door and went out. (single sentence, 
serial predicate, one EDU) 
B) 1 He opened the door,| 2 and went out. (complex sen-
tence, two EDUs) 
Take as example, there exist 7 EDUs in Ex-
ample (3), each marked with a number in front. 
According to our definition, the fragment ???
????????? ?(?work for a short 
time?has been accumulated? ) in EDU 3 is not 
segmented as a EDU since: 1) it acts as a gram-
matical element of other EDUs and has no direct 
relationship with other EDUs on propositional 
function; 2) it is marked by a pair of quotation 
marks and does not end with any punctuation. In 
contrast, the fragment ???????????
? ?(?but learn developed?legality track.?) is 
segment as 4 EDUs since it meets the three crite-
ria in our EDU definition. 
3.2 Connective 
As non-leaf nodes in the CDT representation, 
connectives connect EDUs or discourse units. 
Thus, the main criterion of determining whether 
an expression is a connective is to check wheth-
er the two fragments it connects are EDUs (or 
discourse units). In our scheme, the list of ex-
plicit discourse connectives is judged by a data 
driven approach, i.e. with any discourse-like 
word or phrase marked as connective in the an-
notation practice, e.g. ???(therefore)?, ???
(to this)?, ???...??...(is not...but....)?, ??(so 
that), ????(just because)? in Example (3), 
?? ...??(first...then) ?, ????(and at the 
same time)? in Example (5). 
Example (5): Connective examples from CTB 
A) 1<?????>???????| 2??????
?????????|| 3????????????
??(chtb_0001) 
1<If ; As long as>The construction company enters the 
region, |2 first the appropriate bureau delivers these regu-
latory documents,|| 3 Then there is a specialized contin-
gent that carries out a supervisory inspection. 
B) 1?????, 2?? ? ??????????
?????(chtb_0031) 
2108
 1 The processing trade ?, | 2 and at the same time 
is important content in the economic and trade coop-
eration between Guangdong, Hong Kong, Macao and 
Taiwan. 
It is worthy of mention that from the part-of-
speech perspective, connectives are not 
necessarily conjunctions. For example, in 
Example (3) and (5), adverbs ??...??(first? 
then)?, verb phrases ??????(is not?but)?, 
and preposition phrases ??? (to this)? are 
determined as connectives. From the 
morphological perspective, a connective may 
contain more than one word, even discontinuous. 
As a common occurring phenomenon in Chinese 
discourse, there exist many paired Chinese 
connectives, e.g. ?????? (is not?but)? in 
Figure 2. Even in some paired connectives, such 
as ?????? (because?so)?, a word in a 
paired connective can appear independently as a 
connective. Please note that this may not be 
applied to other cases, e.g. ??????  (is 
not?but)? as appeared in Example (3). Moreo-
ver, in many cases whether an expression is a 
connective or not depends on its meaning, e.g., 
??  (in order to)? is a connective, while ?? 
(for)? is not. For the positional distribution, a 
connective may appear anywhere, i.e. in the be-
ginning, middle, or the end of the first or second 
EDU. Example (3) and (5) show some of cases 
in different positions. The above characteristics 
pose special challenges on connective determi-
nation in Chinese language.  
According to the appearance of a connective 
or not, a discourse relation can be either explicit 
or implicit. Previous studies have shown the dif-
ficulty of implicit relation recognition in English 
due to the omission of connectives (Pitler et al., 
2009; Lin et al., 2009). This becomes even 
worse in Chinese since compared with the im-
plicit ratio of 54.5% in English connectives, this 
ratio rises up to about 82% in Chinese (Zhou and 
Xue, 2012). It is worth noting that the majority 
of discourse relations in Chinese are implicit, so 
the insertion of a connective in an implicit posi-
tion can significantly ease the understanding of 
the discourse. That is, a connective driven repre-
sentation scheme is still applicable to a discourse 
with implicit connectives. To help determine 
implicit relations, two special strategies are pro-
posed. 
First, for each explicit connective, a decision 
is made whether or not it can be deleted without 
changing the rhetorical relation of a discourse. It 
should be emphasized that this constraint is 
largely semantic. The motivation behind the re-
moval of explicit connectives is to enlarge im-
plicit instances and help recognize implicit rela-
tions. As shown in Figure 2, we use the paired 
mark ?()? to indicate that a connective can be 
deleted, e.g. connectives ?(?? to this)?, ?(?? 
therefore)?, ?(??? just because)?, and the 
paired mark ?{}? to indicate that a connective 
cannot be deleted, e.g. connectives ?{? so 
that}?, ?{????? is not?but}?. 
Second, since a connective can be inserted to 
represent an implicit relation, our scheme tries to 
insert a connective which can be easily inter-
preted from the semantic perspective with little 
ambiguity into the most appropriate place. Most 
of the connective insertions for implicit relations 
occur between adjacent discourse spans. It is 
worth noting that not all implicit connectives are 
subjective to the language sense. To mark this 
difference, we cluster implicit connectives into 
two categories according to their language sens-
es, either ?good language intuition? or ?bad lan-
guage intuition?. In our scheme, we use the 
paired mark ?<>?to indicate inserted implicit 
connectives, e.g. connectives?<?? e.g.>?, ?<
? but>? with ?good language sense?, connec-
tive ?<?? and>? with ?bad language sense?, as 
shown in Figure 2. 
In some cases, it is possible that there exist 
several insertion options for an implicit connec-
tive due to the ambiguity in a discourse. For ex-
ample, in Example (5A), connectives ??? (if)? 
and ??? (as long as)? are inserted into the first 
level to show the two discourse relation options. 
As far as this happens, connectives are inserted 
and ordered according to annotators? first intui-
tion. 
3.3 Discourse Structure 
In Figure 2, the paragraph is organized as a tree 
structure, in which EDUs appear in the leaf 
nodes and the connectives appear in the non-leaf 
ones. The adoption of tree structure conforms to 
traditional Chinese discourse theories and prac-
tice. For example, a native Chinese speaker 
tends to determine the overall level boundary 
first and then the analysis goes on step by step to 
the individual clauses, when understanding a 
complex sentence. This process naturally forms 
a tree structure. Besides, tree structure is easier 
to formalize, compared with graph.  
More specifically, the hierarchical structure of 
connectives indicates the hierarchical structure 
of discourse units. Apparently, discourse struc-
2109
 ture analysis can be viewed as hierarchical anal-
ysis of connectives, with hierarchical connective 
structure reflecting hierarchical combination of 
discourse units. Essentially, the discourse hierar-
chy indicates the correlation degrees of semantic 
relations in the discourse, the deeper tree level of 
two discourse units, the higher correlation de-
gree of their semantic relation. Therefore, a dis-
course relation is the ultimate factor for the 
choice of hierarchical discourse structure. For a 
reference, please take Sentence 2 in Figure 2 as 
an example. 
3.4 Discourse Relation  
For discourse relation representation, a general 
approach is to assign an abstract relation type to 
a discourse relation directly, such as cause, con-
junction, condition, purpose, etc, as done in 
RST-DT and PDTB. In our CDT scheme, we 
avoid to directly assign an abstract relation type 
to a discourse relation. Instead, we use the con-
nective itself to express the discourse relation, as 
shown in Figure 2. In this way, the difficulty of 
pre-defining a set of acknowledged discourse 
relations and selecting an exact discourse rela-
tion can be avoided during the corpus annotation 
process. Since a Chinese discourse relation is 
largely controlled by connective (Xing, 2001), 
the key to determine a relation is to identify a 
suitable connective. Normally, most of relation 
annotations can easily map from connectives to 
abstract semantic classes of relations, if neces-
sary, with the help of the discourse context. The 
majority of discourse relations in Chinese are 
implicit, but it makes sense to insist on a con-
nective driven representation. With connective 
as a bridge, at least it makes discourse represen-
tation easier. 
For the abstraction of discourse relations, we 
leave it in a later separate stage. Of course, there 
are cases where a connective may represent 
more than one discourse relation. For example, 
connective ??? can denotes the continuous rela-
tion ?? (especially)? and the transitional rela-
tion ?? (however)?. Compared with annotating 
discourse relation directly, annotator's intuition 
is more accurate for specific connective. We 
don't object to label discourse relation, referring 
to the general work and Chinese analysis prac-
tice, give a set of relations (Figure 3), regarding 
it as connective's semantics, and then annotate 
the connective with it. In this way, we can obtain 
a general relation set and resolve the connec-
tive's polysemy problem. We believe that the 
connective itself is the foundation of discourse 
relation, and the relation set can be adjusted dy-
namically according to the application require-
ments. 
Figure 3 shows a three-level set of discourse 
relations example. In the first level, this set con-
tains four relations of causality, coordination, 
transition and explanation, which are further 
clustered into 17 sub-relations in the second lev-
el. For example, relation causality contains 6 
sub-relations, i.e. cause-result, inference, hypo-
thetical, purpose, condition and background. In 
the third level, the connectives are under each 
sub-relation. For example, cause-result relation 
can be represented by ?because?, 'therefore' etc. 
The numbers shown in the parentheses illustrate 
the distributions of different relations in our cor-
pus. For example, there are 1335 causality rela-
tions in the first level, including 686 cause-result 
relations, 38 inference relations, 70 hypothetical 
relations, 335 purpose relations, 72 condition 
relations and 134 background relations. 
causality(1335) coordination(4148)  
cause-result(686) 
because... 
inference(38) 
so that... 
hypothetical(70) 
if... 
purpose(335) 
in order to... 
condition(72) 
only? 
background(134) 
background... 
transition(217) 
transition (200) 
but... 
concessive(17) 
altough... 
coordination(3503) 
and... 
continue(517) 
first...second... 
progressive(59) 
in addition.. 
selectional(10) 
or... 
inverse(59) 
compared with... 
explanation(1617) 
explanation(911) 
which including... 
summary-
elaboration 
in a word... 
(234) 
example(252) 
e.g.... 
evaluation (220) 
evaluation ... 
Figure 3: A three-level set of discourse relations 
3.5 Nucleus and Satellite 
Once discourse units are determined, adjacent 
spans are linked together via connectives to 
build a hierarchical structure. As stated above, 
discourse relations may be either mononuclear 
or multi-nuclear. A mononuclear relation holds 
between a nucleus and a satellite unit. Normally, 
the nucleus usually reflects the intention focus of 
the discourse and is thus more salient in the dis-
course structure, while the satellite usually rep-
resents supportive information for the nucleus. 
In comparison, a multi-nuclear relation usually 
2110
 holds two or more discourse units of equal 
weight in the discourse structure.  
For nucleus determination, we adopt the de-
pendency grammar, and select the unit which 
can stand for the relationship with other dis-
course units in a discourse. As shown in Figure 
2, on the first level, discourse relation ??? (to 
this)? has the latter unit ????????
(Pudong?as soon as they appear.)? as nucleus and 
the former unit ??????? (Pudong ?new 
problem)? as satellite, since the latter unit agrees 
with the main purpose of the discourse, which 
emphasizes some methods for the progress of 
Pudong. Moreover, since the combination of 4, 5 
and 6 has the cause relation with 7, we choose 7 
as nucleus because it can stand for the combina-
tion of 4, 5, 6 and 7, and has the selection rela-
tionship with 3. 
4 Chinese Discourse Treebank 
Given above the CDT scheme, we choose 500 
Xinhua newswire documents from the Chinese 
Treebank (Xue et al., 2005b) in our Chinese 
Discourse Treebank (CDTB) annotation. In par-
ticular, we annotate one discourse tree for each 
paragraph. 
In this section, we address the key issues with 
the CDTB annotation, such as annotator training, 
tagging strategies, corpus quality, along with the 
statistics of the CDTB corpus. 
4.1 Annotator Training 
The annotator team consists of a Ph.D. in Chi-
nese linguistics as the supervisor (senior annota-
tor) and four undergraduate students in Chinese 
linguistics as annotators (two pairs). The annota-
tion is done in four phases. In the first phase, the 
annotators spend 3 months on learning the prin-
ciples of CDT and the use of our developed dis-
course annotation tool. In the second phase, the 
annotators spend 2 months on independently 
annotating the same 50 documents (about 260 
paraphrases), and another 2 months on cross-
checking to resolve the difference and to revise 
the guidelines. In the third phase, the annotators 
spend 9 months on annotating the remaining 450 
documents. In the final phase, the supervisor 
spends 3 months carefully proofread all 500 
documents. 
4.2 Tagging Strategies 
In the CDTB annotation, we employ a top-down 
strategy. That is, we determine the overall level 
first and then the analysis goes on step by step to 
the individual EDUs. This strategy is adopted in 
our annotation tool. The advantages of the top-
down strategy are three folds. First, such a strat-
egy can easily grasp the whole discourse struc-
ture. This conforms to the global nature of dis-
course analysis. Second, due to the lack of clear 
difference between Chinese sentence and phrase 
structure, such a strategy can largely avoid the 
error propagation in Chinese EDU segmentation. 
Since in such a top-down strategy, EDU seg-
mentation becomes an end question, and even if 
an EDU segmentation error happens, its impact 
is localized, i.e. with little impact on the whole 
discourse structure. Our annotation practice 
shows that such strategy is effective. Third, such 
a strategy accords with the cognitive of Chinese 
characteristics, and conforms to the mental pro-
cess of Chinese discourse understanding (Huang 
and Liao, 2011). However, we do not exclude 
the bottom-up strategy. In some cases, on the 
cognitive psychological process, annotator is 
combine top-down and bottom-up strategies. 
Take Example (3) as an example, an annotator 
first finds the first level, with the period at the 
end of sentence 1, and chooses discourse rela-
tion (either explicit or implicit), connective, and 
connective related information (e.g. whether can 
be added, deleted, and the language sense, etc.), 
nuclearity etc. Then, the annotator turns to sen-
tence 1 and marks the second comma as level 2 
with necessary information annotated, and goes 
on to sentence 2, recursively, until all EDUs are 
marked. In this way, a discourse tree with the 
CDT representation is constructed. 
4.3 Quality Assurance 
A number of steps are taken to ensure the quality 
of CDTB. These involve two tasks: checking the 
validity of the trees and tracking inter-annotator 
consistency. 
4.3.1 Tree validation 
We first manually check if a tree has a single 
root node and compare the tree with the docu-
ment to check for missing sentence or fragments 
from the end of text. Then we check the attached 
information such as connectives, relations and 
nuclearity in the tree. We also check the tree 
with a tree traversal program to find the errors 
undetected by the manual validation process. 
Finally, all of the trees work successfully.  
4.3.2 Consistency 
To ensure the quality of CDTB, we adopt the 
inter-annotator consistency using Agreement 
and kappa on 60 documents (chtb0041-chtb 
2111
 0100). Table 1 illustrates the inter-annotator 
consistency in details. 
As shown in Table 1, we measure the agree-
ment of EDU segmentation by determining 
whether punctuation (all period, comma etc. are 
considered) is treated as an EDU boundary. It 
shows that the agreement reaches 91.7% with 
Cohen's kappa value (Cohen, 1960) 0.91. This 
justifies the appropriateness of our EDU defini-
tion. Explicit or Implicit agreement 94.7% is 
calculate by the same EDU boundary (intersec-
tion) of two annotators. For the same explicit 
relation, the connective identification agreement 
is 82.3%, because this is strict measure when 
two annotators choose the same connective word. 
If we relax the measure to contain the same 
word, the agreement can reach 98%. For exam-
ple, one annotate ????(also?and)?, and the 
other annotate ??(and)? is wrong with our strict 
measure. 
 Agreement  Kappa 
EDU segmentation 
Explicit or Implicit 
Explicit connective identifi-
cation 
Implicit connective insertion 
Mononuclear or Multinuclear 
Nuclearity 
Structure 
91.7      0.91 
94.7      0.81 
82.3        -- 
 
74.6        -- 
80.8        -- 
82.4        -- 
77.4        -- 
Table 1: Inter-annotator consistency 
It is not surprising that the agreement on im-
plicit connective insertion with the same posi-
tion and the same connective only reaches 
74.6% since for some discourse relations, there 
may existing several connective alternatives. For 
example, both ?so? and ?therefore? can express 
the same causation relation. If we relax the con-
straint to the compatible connective, the agree-
ment on implicit connective insertion can reach 
up to 84.5%. 
Finally, it shows that the agreement on overall 
discourse structure (with the same connectives 
as non-leaf nodes, the same EDUs as leaf nodes) 
reaches 77.4%. This justifies the appropriateness 
of our CDT scheme, given the inherent ambigui-
ty in Chinese discourse structure. 
4.4 Corpus Statistics 
Currently, the CDTB corpus consists of 500 
newswire articles from Chinese Treebank, which 
are further divided into 2342 paragraphs with a 
CDT representation for one paragraph.  
? For EDUs, CDTB contains 10650 EDUs with 
an average of 4.5 EDUs per tree. On average, 
there are 2 EDUs per sentence and 22 Chi-
nese characters per EDU. 
? For discourse relations, CDTB contains 7310 
relations, of which 1812 are explicit relations 
(24.8%) and 5498 are implicit relations 
(75.2%). This indicates that implicit relations 
occur much more frequently in Chinese than 
in English, e.g. 75.2% in CDTB (Chinese) vs. 
~50% in PDTB (English). 
? With the deepest level of 9, most (98.5%) of 
discourse relations occur in level 1 (2342), 
level 2(2372), level 3(1532), level 4(712), 
and level 5(242). It also shows that 3557 
(48.7%) relations are mononuclear relations 
with 2110 nucleus ahead, while the remain-
ing 3754 relations are multi-nuclear. The 
numbers shown in the parentheses of Figure 
3 illustrate the distributions of different rela-
tions. In comparison with the top 2 most fre-
quently occurring relations in PDTB (Eng-
lish), i.e. the coordination and explanation re-
lations, there exist 3503 (47.9%) and 911 in-
stances respectively, with regard to the ab-
stract relation set as shown in Figure 3. 
? CDTB contains 282 connectives, among 
which 274 (140 can be deleted) appears as 
explicit connectives and 44 can be inserted in 
place of implicit connectives. Table 2 lists 
the top 10 frequent explicit connectives and 
implicit connectives. 
Explicit connectives Implicit connectives 
connectives   frequency connectives   frequency 
?(and)                     208 
??(among them)   154 
?(also)                     131 
?(however)              70 
?(but)                       69 
?(also)                      68 
?(so that)                  56 
?(in order to)            52 
?(in order to)            49 
??(meanwhile)       46 
??(so)                    368 
?(and)                      354 
??(and)                  259 
??(e.g)                   140 
?(in order to)             68 
?(in order to)             61 
??(then)                   55 
??(among them)      48 
?(while)                     47 
??(because)             32 
Table 2: The most frequent connectives in CDTB 
5 Comparison with other Discourse 
Banks 
Table 3 compares the difference of CDTB with 
RST-DT and PDTB from various perspectives, 
such as EDU, connective, relation, structure and 
nuclearity. 
2112
  
 RST-DB PDTB CDTB 
EDU 
Clear defined; start of 
combination; one relation 
has two or more EDUs 
Predicate-argument 
view; one relation 
has two arguments 
Clear defined from three aspects; end of 
top-down segmentation; one relation has 
two or more EDUs 
Connective -- 
Mark explicit con-
nectives and insert 
implicit connectives 
Mark whether an connective can be deleted 
without changing the rhetorical relation; 
insert implicit connective with good intui-
tion and bad intuition differentiated 
Relation 
Abstract set of relation 
types; annotate the rela-
tion types 
Abstract set of rela-
tion types; annotate 
connective and rela-
tion type 
Represent relation by connective; annotate 
connective and it?s attribute; mapping of 
connective to the set of discourse relations 
in a later stage 
Structure Complete tree 
Partial tree, deduced 
by connective and 
it?s argument 
Complete tree; top-down segmentation; 
structure can be represented by the connec-
tive hierarchy 
Nuclearity 
Determined by certain 
rhetorical relation 
-- 
Determined by the global meaning of a 
discourse 
Table 3: The comparison of RST-DT, PDTB and CDTB 
 
6 Preliminary Experimentation 
In order to evaluate the computability of CDTB, 
we give the experimental results on EDU recog-
nition, which is crucial in discourse parsing. Af-
ter excluding sentence end punctuations (such as 
period, question mark, and exclamatory mark), 
which are certainly EDU boundaries, there re-
mains 7625 punctuations as EDU boundaries 
(positive instances) and 4876 punctuations as 
non-EDU boundaries (negative instances). With 
various features as adopted in Xue and Yang 
(2011) and Li et al. (2013b), Table 4 shows the 
performance of EDU recognition on the CDTB 
corpus with 10-fold cross validation.  
 Gold standard parse      Automatic parse 
Accuracy  F1(+)  F1(-)  Accuracy  F1(+)  F1(-) 
MaxEnt 90.6     91.1   90.5 89.0    90.3    87.2 
C45 90.2    90.5    90.1 88.7    90.0    87.7 
NiveBayes 90.2    89.9   88.9 88.0    89.0    86.9 
Table 4: Performance of EDUs recognition 
As shown in Table 4, MaxEnt performs best, 
with accuracy up to 90.6% on gold standard 
parse tree, close to human agreement of 91.7%, 
and with accuracy up to 89% on automatic parse 
tree. This suggests the appropriateness of our 
definition of clause as EDU. Table 4 also gives 
the performance on both positive and negative 
instances. It shows better F1-measure on recog-
nizing positive instances than negative instances. 
7 Conclusions 
In this paper, we propose a Connective-driven 
Dependency Tree (CDT) structure as a represen-
tation scheme for Chinese discourse structure. 
CDT takes advantage of both RST and PDTB, 
and well adapts to the special characteristics of 
Chinese discourse. In particular, we describe 
CDT in detail from various perspectives, such as 
EDU, connective, structure, relation and nucle-
arity. Given the CDT scheme, we annotate 500 
documents in a top-down segmentation process 
to keep consistent with Chinese native?s cogni-
tive habit. Evaluation of the CDTB corpus on 
EDU recognition justifies the appropriateness of 
the CDT scheme to Chinese discourse structure 
and the usefulness of our CDTB corpus. 
In the future work, we will focus on enlarging 
the scale of the corpus annotation and develop-
ing a complete Chinese discourse parser. 
Acknowledgments 
This research is supported by the Project 
2012AA011102 under the National 863 High-
Tech Program of China, by the National Natural 
Science Foundation of China, No.61331011, 
No.61273320. 
Classifier 
2113
 The contact author of this paper, according to 
the meaning given to this role by Soochow Uni-
versity, is Guodong Zhou. The complete corpus 
is available for research purpose upon request. 
Reference 
Zheng Cao.1984. Primary exploration on sentence 
group. Zhejiang Education Press, Hangzhou,CN(in 
Chinese). 
Lynn Carlson, Daniel Marcu, and Mary Ellen Oku-
rowski. 2003. Building a Discourse-tagged Corpus 
in the Framework of Rhetorical Structure Theory. 
Springer Netherlands. 
LiPing Chen. 2006. English and Chinese discourse 
structure dimension theory and practice. Ph.D. 
thesis, Shanghai international studies university 
doctoral dissertation. 
Liping Chen. 2008. Chinese text structure annotation 
theory support?Journal of Nanjing university of 
aeronautics and astronautics, 10(3):69-71(in Chi-
nese). 
Jacob Cohen. 1960. A coefficient of agreement for 
nominal scales. Educational and Psychological 
Measurement, 20 (1): 37?46. 
Wenhe Feng and Donghong Ji. 2011. Parallel struc-
ture analysis of the coordination structure and the 
controller status of connective. Linguistic Sciences, 
2:168-181(in Chinese). 
David G Hays. 1964. Dependency theory: formalism 
and some observations. Language, 40(4):511-525. 
Hen-Hsen Huang and Hsin-Hsi Chen. 2011. Chinese 
Discourse Relation Recognition. In Proceedings of 
5th International Joint Conference on Natural 
Language Process, pages 1442?1446, Chiang Mai, 
Thailand, November 2011. 
Borong Huang and Xudong Liao. 2011. Morden Chi-
nese (volume two, updated 5th edition). Higher 
Education Press. Beijing,CN(in Chinese). 
Yancui Li, Wenhe Feng, and Guodong Zhou. 2013a. 
Elementary discourse unit in Chinese discourse 
structure analysis. In Chinese Lexical Semantics, 
pages 186-198, Wuhan, China, Springer Berlin 
Heidelberg. 
Yancui Li, Wenhe Feng, and Guodong Zhou et al. 
2013b. Research of Chinese Clause Identification 
Based on Comma. Acta Scientiarum Naturalium 
Universitatis Pekinensis, 49(1):7-14 (in Chinese 
with English abstract). 
Ziheng Lin, Min-Yan Kan, and Hwee Tou Ng. 2009. 
Recognizing implicit discourse relations in the 
Penn Discourse Treebank. In Proceedings of the 
2009 Conference on Empirical Methods in Natural 
Language Processing, pages 343?351, Singapore, 
6-7 August 2009. 
Daniel Marcu. 2000. The theory and practice of dis-
course parsing and summarization. MIT Press. 
William Mann and Sandra Thompson. 1988. Rhetori-
cal structure theory: Toward a functional theory of 
text organization. Text, 8(3):243-281. 
Emily Pitler, Annie Louis, and Ani Nenkova. Auto-
matic sense prediction for implicit discourse rela-
tions in text. 2009. In Proceedings of the 47th An-
nual Meeting of the ACL and the 4th IJCNLP of 
the AFNLP, pages 683?691, Suntec, Singapore, 2-
7 August 2009. 
Rashmi Prasad, Nikhil Dinesh, and Lee et al. 2008. 
The Penn Discourse TreeBank 2.0. In Pro-
cessdings of Sixth International Conference on 
Language Resources and Evaluation (LREC), 
pages 2961-2968, Marrakech, Morocco. 
Susan Verberne, Lou Boves, Nelleke Oostdijk, and 
Peter-Arno Coppen. 2007. Discourse-based an-
swering of why-questions. Traitement Automa-
tique des Langues, special issue on Computational 
Approaches to Discourse and Document Pro-
cessing, 47(2):21-41.  
Wenge Wang. 2010. The Current Research Situation 
of the Clause in Modern Chinese. Chinese Lan-
guage Learning, (1): 67?76(in Chinese). 
Fuyi Xing. 2003. Research of Chinese complex sen-
tence. The Commercial Press, Beijing, CN (in 
Chinese). 
Nianwen Xue. 2005a. Annotating the Discourse Con-
nectives in the Chinese Treebank. In Proceedings 
of the ACL Workshop on Frontiers in Corpus An-
notation, pages 84-91, Ann Arbor, Michigan. 
Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha 
Palmer. 2005b. The Penn Chinese Treebank: 
Phrase Structure Annotation of a Large Corpus. 
Natural Language Engineering, 11(2): 207-238. 
Nianwen Xue and Yaqin Yang. 2011. Chinese sen-
tence segmentation as comma classification. In 
Proceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics:Human 
Language Technologies, pages 631-635, Portland, 
Oregon,USA, June 2011. 
Ming Yue. 2008. Rhetorical Structure Annotation of 
Chinese News Commentaries. Journal of Chinese 
Information Processing, 22(4): 19-23(in Chinese 
with English abstract). 
Yuping Zhou and Nianwen Xue. 2012. PDTB-style 
discourse annotation of Chinese text. In Proceed-
ings of the 50th Annual Meeting of the Association 
for Computational Linguistics, pages 69-77, Jeju, 
Republic of Korea, 8-14 July 2012.  
 
2114
