Boost ing Variant Recognit ion with Light Semantics 
C5c i le  Fabre 
ERSS / \]Ddpt de Sciences du Langage 
Univ. Toulouse-Le Mirail 
5 alldes A. Machado 
31058 Toulouse Cedex, France 
c fabreOun iv - t l se2 ,  f r  
Chr i s t ian  Jacquemin 
CNRS-LIMSI  
BP 133 
91403 ORSAY Cedex 
FI'&IICe 
j acquemin@limsi, fr 
Abst ract  
A reasonably simple, domain-independent, 
large-scale approach of lexictd semantics to 
paraphrase recognition is presented in this pa- 
per. It relies on the enrichment of morpho- 
syntactic rules and the addition of fbur boolean 
syntactico-semantic features to a set of 1.,(}23 
words. It results in a significant enhancement 
of precision of 30% with a slight decrease in re- 
call of 10%. 
1 Overv iew 
The recognition ,of paraphrases and variants is 
an important issue in several areas of infornm- 
tion retrieval and text mlderstanding. Merging 
paraphrastic sentences ilnproves ummarization 
by avoiding redundancy (Barzilay et al, 1999). 
Term variant conilation enhances recall in in- 
tbrmation retrieval by pointing at documents 
that contain linguistic variants of (tuery terms 
(Arampatzis et al, 1998). 
In (Jacquemin and Tzoukermann, 1999), a 
technique is proposed for the conflation of 
morpho-syntactic variants that relies solely on 
morphological and low-level syntactic features 
(part-of-speech category, munber agreement, 
morphological relationships, and phrase struc- 
ture). An analysis of these results shows the 
limitation of this approach: correct and incor- 
rect variants cannot be separated satisfactorily 
on a purely morpho-syntactic basis. Sonic addi- 
tional lexical semantics must be taken into con- 
sideration. 
In this study we propose a reasonably sim- 
ple, domain-independent, large-scale approach 
of lexical semantics to noun-to-verb variant 
recognition. It relies on the mere addition of 
two t)oolean syntactic features to 449 verbs and 
two boolean morpho-semantic features to 574 
nouns. It result,; in a significant enhancement 
of precision of 30% with a slight decrease in re- 
call of 10%. This new al)proaeh to semantics-- 
human-based, ettlcient, involving simple linguis- 
tic tbatures --convincingly illustrates the posi- 
tive role of linguistic knowledge in information 
processing. It confirms that verbs and their se- 
mantics play a significant role in document anal- 
ysis (Klavans and Kan, 1998). 
2 Morpho-syntactic Approach to 
Nomino-verba l  Var ia t ion  
In order to illustrate the contribution of se- 
mantics to the detection of paraphrastic struc- 
tures, we focus on a specific type of wtriation: 
the vo.rl)al varbmts of Noun-Preposition-Noun 
terms o1" compounds in French. For example, 
les corttraintes rdsiduellcs darts les coques sont 
anaIysdes (the residual constraints in the shells 
~re mialyzed) is such a vert)al variant of analyse 
de corttraintc (constraint analysis). 
As a baseline tbr the extr~mtion of these vari- 
ants, we use a set of five morpho-syntactic rans- 
tbrmations fbr Noun-Preposition-Noun terms 
reported in (Jacquemin and Tzoukernlann, 
1999) (see Table 1). 1 We use the no- 
tation Ad(Ni)v for |;tie inorphological ink 
between the initial term and the trans- 
ibrmed structure. It rel)resents any verb 
in the same morphological fanfily as Ni. 
For instance, in English, and according 
to the CELEX database, Ad(analysis)v = 
{to analyze, to psychoanalyze}. 
Given a NI P2 N8 structure, these transt'ornm- 
tions are obtained through corpus-based tuning 
1The following symbols are used for syntactic ate- 
gories: N (nouI0, A (adjective), Av (adverb), V (verb), C 
(coordinating conjunction), P (pret}osition), and D (de- 
tcrminer). In the regular exl}ressions, ? denotes option- 
ality and I disjunction. Morphologically related words 
are underlined. 
264 
Tal)le 1: Mort)he-syntactic (MS) Variants of N1 P2 Na Terms 
NheadToV: Ad(N~ )v (Av: (P'? \]) \[P D '?) A:) N:~ 
stabilisation de priz (price sl;al)ilization) ~ stabiliser le'm's priz (stal)ilize their prices) 
NheadToVRev:  Na(A? (PA?N(A(CA)?)?)? (CI)': Av': A'?NA?)'?V':V': Av':)A4(N1)v 
abattage d'arbre (tree cutting) -+ m'bres oat dtd abattus (trees have been cut down) 
Nmodi fToV l :  N1 ((Av ? A (C av '? A)'?) ': V': P) fl4(Na)v 
mdth, ode d'.dvaluatio~ (method of evaluatio~ 0 ~ mdth, ode pour d'valuer (method tbr ewduating) 
NmodifToV2: Nj (A': (V\[ (l) D "? (Av': A) '? N) '?) (Av '? A)': Av':) Ad(Na)v 
zone de ddstabilisation (region of destal)ilization) --+ zone d&tabilisde (destat)ilized region) 
Nmodi fToVRev:  Ad(Ua)v (Av '? (P'? l)) \[ (P D':)A':) N~ 
tcrnpdrat,ure de chau./.\[age (temi)erature fi)r heating_') 
-+ chau/.\]~s a h, aute tempdrat'urc, (heated at high teml)eratures) 
and correspond basically to tbur configurations: 
1. either N1 or Na (re, st)cct;ivcly head and 
modifier of the initial term) is I;r~msformed 
into a morphologically related verb V, 
2. the order of the two content words is re- 
tained or reversed, 
3. the dependency relation 1)etween 1;11(; two 
initial 11011118 is preserved. 
For instance, rule NheadqlbVl/,ev corresl)onds 
to transfi)rmations in whi(:h the, head noun 
is morphologically re,1;~t(;d to the verl) and 
t:h(; ord('r of the two words is re, verse(l; rul(; 
Nmodit'FoV (modifier transfi)rmed, order r(;- 
rained) has been divided into two sul)ruh;s: the 
first one - Nmo(tifYoV1 - re, quires the insertion 
of a pr(;positiou just 1)eft)re th(; verbal form. 
3 The L imi ts  o f  the  
Morpho-syntact i c  A I )p roach  
In the tirst step of this work, we ext)e(:ted th(, 
precision of wu:iant recoglfition to be controlled 
in two ways: firstty, by searching fi)r multi- 
term variants in which the two content words of 
the initial term are foulM, directly or via mor- 
1)hological transformation. Se(:ondly, by dell1> 
ing morpho-syntacti(: pa|;te.rns of variation in 
I;erms of l)art-of-sl)eech strings that are allowed 
to come in 1)e.tween these t;wo COlll;eltt words. 
Yet, the sequences found on su(;h a morl)ho- 
synl;acti(: basis prove to 1)e of wtrying quality 
regarding their at/ility to t)rovide t)arat)hrases 
of the initial l;erm. Consider for instance some 
of l;he vm:imlts del, eeted for the term comparai- 
son de rds'ultat (comparison of results), in which 
only t;11(; \[irst; |;we sequences are good variants: 
compare les rdsultats ((:onlpare the results) 
(rule Nhead~lbV, pattern A4 (N1)vDNa) 
r&ultats ezpdrirnc.ntauz sent compar& (exI)er- 
imental results are compared) (rule Nhead- 
toVRev, 1)atte.rn NaAVA4 (N~)v) 
com, pard.s a'ux 'rds'.,ltats (COmlmred to 
the results) (rule NheadToV, pattern 
Jt4(N1)vPN:{) 
rds'ultd d"unc eomparaison (resulted from ;t 
comparison) (rule NModiiToVRev, pattern 
J~d (Na)vPDN1 )
Such examples show that morpho-syntactie 
patterns ;~re 1;oo coarse-gr~tined to ensure l;hat 
the dependency relation between the two piv- 
ots (results is the object of the prediea?e com- 
parison) is maintained. When trying to detine 
linguistic criteria to ewfluate such w~riants, it; 
apt)ears that the frontier between good and load 
variants lies between those that preserve the ar- 
gument relation 1)etween the two content words 
and those that disrupt it. This means that, in 
the verbal wtriant, the. argument relation be- 
tween the verb and l;he noun must be l;he same 
as t;he relal;io11 between the deverl)al lOUll add 
the othe.r noun in the nominal term. 
None of the five rules ensures that the subcat- 
egorization frame is preserved. For instance, if 
we consider t;11o rule NModifI'oVl{ev, we find se- 
265 
quences that obey this constraint and sequences 
that violate it2: 
cr'itdrc d'&aluation (evaluation criterion) -+ 
dvalv, dselon les crit~r'cs (evahmted according to 
the criteria) 
syst~me d'dvaluation (evaluation system) *--+ 
dvaht6 lc syst&ne (evaluated the system) 
In the second case, the transtbrmation is un- 
acceptable because the instrumental relation ex- 
pressed in the nonfinal term becomes an ob- 
ject relation in the verbal sequence. Even 
when word order is preserved, the relation be- 
tween the pivots can be totally different in the 
term and its transformation, as i.u: contrgIe 
d'installation (installation control) and contrgle 
ccntralisd installd (installed centralized control) 
(rule NModitToV2). 
Our aim was to tbrmulate additional con- 
straints in order to control argument structure 
preservation. We thus had to cope with prob- 
lem of handling nonfinal t)hrases (NP) in which 
one of the elements is morphologically inked to 
a verb. In French, as in English, the seman- 
tics of these nominal phrases is an issue tbr lin- 
guistic description: the two nouns can be linked 
by the whole range of argmnent-predicate rela- 
tions, and very few linguistic elements can be 
used to decide what relation is expressed. Here 
is a brief list of the configurations that are likely 
to appear in such NPs: 
- the second noun is the object of the first; one: 
comparaison de rdsultat (comparison of result) 
- the second noun is the subject of the first 
one: augmentation de I'intcv, sitd (increase in in- 
tensity) 
- the second noun is an adjunct: tr'aitcment g
la chaleur (treating with heat) 
the first noun is an adjunct: taux 
d'augmentation (increase rate) 
Our aim was to find a way to use surface lin- 
guistic knowledge, as required in such an area 
of NLP, to deal with the interpretation of these 
phrases. 
4 Light Semant ics  for 
Nomino-verba l  Variat ions 
Our approach consisted of two steps: firstly, 
defining semantic lues tbr accepting or discard- 
sin what follows~ the symbols --~ and *----> respectively 
indicate correct and incorrect ransformations 
ing variants and, secondly, defining new varia- 
tion patterns based oll these features. 
4.1 F i l ter ing  Cr i te r ia  
First, using linguistic results on the semantics 
of French NPs (Fabre, 1996; Bartning, 1990), 
we identified predicate-argument configurations 
that cannot be matched by a given pattern ('re- 
ject' heuristics in the sense of (Lapata, 1999)). 
For example, when rule NmodifToVRev applies, 
N1 de N3 terms cannot be i)araphrased by ver- 
bal sequences in which N1 is the ol)ject of the 
verb, as in: ezp&iencc d'utilisation (experiment 
of use) *-+ utilisait 'uric ezpdrier~,cc (used an ex- 
periment). In such a configuration, only non-- 
thematic arguments (adjuncts) of the deverbal 
noun may be tbund inside the NP. 
Similarly, when rule NheadToVRev applies, 
N\] de N3 terms cannot be paraphrased by ver- 
bal sequences in which N\] is the subject of a 
transitive verb, as in: utilisation de l'ezp&'icnce 
(use of experiment) *-+ czpdriencc utilisant (ex- 
t)eriment using). 
This configuration provides variants only 
when the verb is intransitive or ergative: erga- 
tive verbs allow tbr alternations of the tbrm: NP 
V (la dcnsitd au.qrncntc) / one V NP (on awl- 
monte la dcnsitd). 
In this case, the tbllowing transtbrmation is 
correct: augmentation de densitd (density in- 
crease) / de,,,,s'itd av,9'mentc (density increases). 
4.2 Enriched Metarules 
Once it has 1)een established wtfich transforma- 
tions should be rejected, we searched tbr sur- 
face linguistic clues that could help us to fil- 
ter out these undesirable variants. It led us to 
the redefinition of the metarules, in two ways: 
putting additional constraints on the part-of  
speech strings that can intervene between the 
two pivots, and defining new features to add lin- 
guistic control upon the application of the rules. 
These t~atures are: the prepositional form, the 
morphological type of the noun, the transitivity 
of the verb, and the voice (active versus pas- 
sive). 
Here are two examples tbr the redefinition of 
the metarules (fllrther details and examples are 
given in table 3): 
ru le Nmodi fToVRev In this case, the 
metarule is transtbrmed into a single 
266 
17elilled rule, in whi('h the ('oml)ilm|:ion 
of parts of sl)eech is mot(; res|;riel;ed: a 
preposition is required to elinfinate object 
rein|ions from the verbal phrase. In ~ul- 
dition, the morphologic~dly comt)lex nora1 
must be ~ \])recessive deverl)M. %'anstbr- 
martens such a.s czpdricnce d',utili.srltion 
*-> ul, ili.sa, it. 'u, ne  c.zpdricncc, a.re filtered 
()u|;. 
rule NheadToVRev Here, the initial 
metarule is refined into three em'iched 
rules, mainly by means of lexical con- 
straints on the verb tbrm. Only N1 P2 Na 
t;(;171118 whe, re 1)2 = dc m:e |;real;ed. \]if the 
v(;rl) is transitive, l;helt the verb forln nlUSI; 
l)e ;t past t)m;l;i(;il)le (rule Nlw, a(tt()Vl/.('v- 
\])ass), so l;ha, t 1;t1(', object relation still hol(ls 
in the vm'iant, if the verl) is intrnnsitive 
or ergative, then the verl) fornl nms|; 
l)e active, st) |;h~l; the sut)je(:t; rel~d;ion 
holds (rule Nhea(ltoVll.ev-A(:tSiml) (resp. 
NheadtoV\]l,ev-ActComp) for simt)le (resl). 
(:omt)h;x) verb fornls). '.l~:allSl'orm~tions 
such as uti l isation dc l'c.zp(;ric'nce *-+ 
czpd'ricncc 'utilisant ~r('~ filtered ()tiC;. 
The r(;iinenlenl; of the mel;m'ul(',s introduced 
four linguistic \]b&i;llr(*,s whi('h had to 1)e encode(t 
in the h',xi('on (see Table 2), nmn(',ty: 
? 1;11(', morl)hologi(:al nature of the noun: th(! 
noun is either non (h:verl)al or devert)~d. In 
the l~d;ter ease, it; may (:orrest)on(t () ;m 
agent deverbal, which reihrs to the agent 
of the verb, e.g. 'utili,sateur (user), or to ~ 
t)rocessive deverbM, whMt reibrs to the a(:- 
|ion (tenoted by the verb, e.g. uti l isat ion 
(.se). 
? the transitivity of the verb: intr;msitive 
mid ergative verbs are marked in the lexi- 
(;Oll. 
This mine|at|on task is not tinm-(:onsuming 
(al)otd; 3 hours for 1.,023 words) and could be 
parl;ly automated: characteristic endings (:ould 
hell) to detect processive mid agent deverb~fls. 
In addition, intrmlsitive mid ergative verbs form 
a sm~fll set of the vert)al lexicon (8% of the 
ver|)s) which is likely to 1)(', l)artly (lom~dn- 
indel)endent. 
5 Exper iments  and  Eva luat ions  
In this section, we ew~luate the variations pro- 
duced fl'om the two preceding sets of metarules: 
initial morpho-synt~mti(" wn'iations (henceforth 
MS) m~d new wn:i~tions enriched through light; 
semantics (henceforl;h MS+S). i 
The wtriald;s ",u:e ot)t~dne(1 Kern a 13.2 million- 
word (:orpus (:omposed of s(:ientiti(: al)stracl;s 
in the agricultural dora;fin (in French) ;rod a 
set of 11,452 terms. :~ The corlms is mlnlyzed 
through SYLEX, a shallow parser l;h~t buihts 
limited 1)hrase structures and associ~tes each 
word with mt unambiguous yntactic (:ategory 
and a, l(;mma. ~Ibrms are acquired from the out- 
|;ltl'es a,l:e sele,(:ted nn(t only terms that occur ~l: 
lea.st three times in the ('ortms m:e retained. 
The nunll)ers of variants exi;r&c|;ed through 
MS nnd MS+S ~u'e reporl;ed in ~l~fl)le el. 
They are re:ranged in su('h ;~ w~y (;hat ('or- 
responding wu'iations are aligned horizontnlly. 
For instance, each of the three MS+S vari- 
ations Nhea(lToV-Conq), NheadToV-SimI) or 
NheadtoV-l)rel ) is a refinenmnt of the MS vari~> 
|ion Nhea(lToV. In other words, the set of wtri- 
ants extracted by these three rich llle|;a, ru\]es is 
in('hl(led into the set of variants exl;ra~cl;ed l)y 
th(', 1)oor met;re'nit. These two sets are not eqmd 
since the rich metm:uh'~s are mnde more sele(:tive 
th;m the origimfl me(mule fl:om whi(:h they m:e 
derived. 
In addition to the oul;tm(; of ri(:h mid poor 
met~mfles, T;fl)le 4 shows, in |;he third col- 
umn, the mnnber of co-occurrences associated 
with these metarules. Co-occurrences m'e the 
least selective filters associated with morpho- 
syn|;~mti(: varimlts; they nre ext/ected to extract 
all the l)Ossible ('orrect nomino-verb:fl variations 
(recall value 1.0). Given a N1 Pu Na term, these 
co-occurrences corresl)ond to a configuration in 
which N1 co-occurs with a verb that is roof  
phologically related to Na or Na co-occurs with 
~r verb related to N~. Co-occurrences are ex- 
tra(:ted from a l l -word window (9 intervening 
words). These co-occurrences are used to eval- 
m~te the recall wflues of the tiltering metarules. 
awe arc grateflfl to Xavier Polanco, Jean Royautd and 
Lmncnt Schmitt (INIST-CNRS) for t)roviding us with 
this s(:icntitic orpus. 
267 
Table 2: Semantically Enriched Lexicon. 
Word Process ive  Deverba l  Agent  Deverba l  Int rans i t ive  Ergat ive 
abaisser - D - A - I - E 
abaissement +D -A  - I  -E  
absorber -D  -A  - I  -E  
absorbe'ar +D +A - I  -E  
accorder - D - A - I - E 
accord +D -A  - I  -E  
accumuler - D - A - I - E 
aceumulateur +D +A - I - E 
accumulation ? D - A - I - E 
accdl&'er -D  -A  - I  +E 
Table 3: Semantically Enriched Morpho-syntactic (MS+S) Variants of N1 P2 N:t Terms 
NheadToV-  Comp: avoir Av '~ 34 (N 1 ) V Av ? D A t N3 
{(N1 d,,ev) = proeessive A P2 =- d,e A (34(NI )v  tense)= pastpartieiple} 
comparaison de rdsultats (comparison of' results) 
--~ a compard les rdsultats (has compared results) 
NheadToV-Simp:  34(N1)v Av ? D A ? N3 
{(N1 dev) = proeess fve  A P2 = de A (34(N1)v tense) ? pastpar t ic ip le}  
dvaluation de risques (ewduntion of risks) --+ (~valv, er les risques (to ewfluate risks) 
NheadtoV-Prep:  34(Nl)v Av ? P2 D A ? N.~ 
{(Nt dev) = process ive} 
exposition d la lumi&'e (exposure to light) --+ ezposdes it la lumidre (exposed to light) 
NheadtoVRev-Pass :  N3 (A ? (P A ? N (A (C A)?)?) ': (C D ? Av ? A ? N A?) ? V ? dtre': Av ?) 2td(N1)v 
{(N3 agreement) = (3.d(N1)v agreement) A P2 = de A (N1 de',,) =-proces,sive A 
(34(N )v tense) = p ,,stp rtie,:ple A (M(N )v = 
r@artit ion de ch, ar.qe (weight distribution) --+ eh, arge @alement r@artie (equally distributed weight) 
NheadtoVRev-ActS imp:  Na(A?(PA?N(A(CA)? )? )? (CD'~Av?A?NA?)? )Ad(N1)v  
{P2 = de A = p,'o essi e A (34(N )v tense)?p stpo, rtie',:ple A 
(Jbl (N1)v valence) = (er.qativelintransitive) } 
chute de tempdrature (drop in temt, erature ) --+ tempdrature ch,'ute (temperature drops) 
NheadtoVRev-ActComp:  Na (n ? (P A t N (A (C A)?)':) ? (C D ? Av': A': N A':)': avoir ? Av ?) 3d(N1 )v 
{P2 = de A (N1 dev} = process ive  A {3d(N1)v tense) = pastpar t ic ip le  A 
{3d (N1)v valence) = (ergat ive l in t rans i t ive)  } 
fermentat ion  de jus (juice fermentatio,t) --+ jus de raisins fermentds (fermented grape juice) 
Prec is ion and Recal l  
In order to calculate the precision and recall of 
the rich and poor metarules and to estimate the 
gains of semantic enrichment, a set of 1,000 co- 
occurrences has been randomly chosen among 
the 159,898 co-occurrences retrieved by the sys- 
tem. They have been divided into three sets: 
S1 (500 co-occurrences) and S2 and S~ (250 co- 
occurrences). S~ has been evaluated indepen- 
dently by the two judges (i.e. the two authors) 
268 
Tabh; 4: Counts of varinnts of NI P2 Na terms 
MS MS+S Co-occurrences 
874 NheadToV-Coml) 
38,693 NheadToV 15,583 NheadToV-Silnp 
7,644 NheadtoV-Prep 
14,24:8 NheadtoVRev-Pass 69,056 NIN2toV1N2 
20,453 Nhead2bVI/.ev 197 Nhea(ltoVI{ev-ActSimp 
26 NheadtoVll.ev-Act Coral) 
6,803 NlnodifPoV1 2,749 NlnoctitWoV1-Ppr 42,882 NIN2toN2V1 
1,160 Nmodif\]}oV2-Infl }
2,588 Nmodif?oV2 0 NmoclitXbV2-Inf2 26,971 N1N2toNIV2 
1 NmodifYoV2-hff3 
~( 9,363 NmoditToVI/.ev 1,892 Nmo(tifPoVIl.ev-Prep 20,989 NIN2toV2N1 
77,900 44,374 159,898 
ill order to test the level of agrcelnent and $2 
and S. 5 have been ewfluated separately by only 
one judge each. Each cooccurreuce has been 
marked as t)ositive (~ correct variation), nega- 
tive (an incorrect variation) or inevaluable. In- 
ewfluable cases correspond either to tagging er- 
rors or to i?l(;orrect erms such ;ts (:oq'ttc de form(. 
(shell of shape) wlfich is an incoml)lete tca'm 
structure \])ec~utse it shouht t0e followed by an 
adjective such as coqu, c dcform, c oval(', (oval- 
shaped shell). Only the cases of ;tgreeul(u,t l)e- 
|;ween the two judges are used for the COml)uta- 
tion of rex:all and t)recision values. 
The achtition of semantics results in an in- 
(:,'ease of precision of 0.29: from 0.499 fbr MS 
to 0.789 for MS+S. The corresl)onding decrease 
of recall is nm(:h smaller: 0.11 from 0.696 for 
MS to 0.586 for MS+S. Pre(:ision and recall can 
t)e (:onfloine(t into a single me,mute such as the 
eifeetiveness measure E~ given by Fonmfla (1) 
in which t~ is a parameter (0 _< a < 1) (van 
Rijsbergen, 1975): 
Ea = 1 - (1) 
E~ varies fl:om 0 to 1.0. Low wflues of Ea cor- 
respond to combined high recall and high preci- 
I in order to assign an equal sion. If we use oe = 
trot)or|ante to precision and recall, the E1 val- 
ues are 0.419 fi)r MS and 0.327 for MS+S. They 
indicate that the addition of semantics has sig- 
nificantly improved the quality of w~riant ex- 
traction. Detailed values of recall and precision 
arc; showll ill Table 5. 
Agreement  on Judgment  
Agreement on ~ classification task can 1)e mea- 
sured through the kappa coefficient (K). It 
ewduato.s the pairwise agreement mnong a set; 
of coders making category .iudgment, correcting 
tbr expected chance agreement (Carletta, 1996). 
In our case the results of the ternary class|It- 
cation task are given by Table 6. The simple 
kappa cecil|(tent is 
Po-  P,! 
K : - -  (2 )  
I -P~.  
7232. in which P0 = E i~ and < = Ei( ,/ ~2~-) (Co- 
hen, 1960). P0 is the proportion of times the 
coders agree and I~, is the proportion of tiines 
we would expect them to agree by chance. The 
value of the kappa coetficient is 0.91 indicating 
a good reliability of the evaluation pertbrmed 
by the two independent .judges. 
6 Conc lus ion  
On a linguistic point of view, this experiment 
demonstrates that NLP applications can pro- 
vide new issues tbr the description of linguis- 
269 
Tab le  5: Precision and recall in variant extract ion for MS and MS+S variations 
PMS PMS+S RMS RMS+S 
0.438 NheadToV 
0.735 NheadToVRev 
0.111 Nmodi f roV1 
0.769 NmodifToV2 
0.448 Nmodi fToVRev 
{ 
{ 
0.875 
0.938 
0.565 
0.902 
1.000 
0.308 
1.000 
O.O00 
NheadToV-Comp 
NheadToV-Sim t)
NheadtoV-Prep 
NheadtoVRev-Pass 0.806 0.664 
NheadtoVRev-ActS imp 
NheadtoVRev-ActComp 
Nmodi fToVl -Pt ) r  0.674 0.578 
Nmodi iToV2-hf f l  } 
NmoditToV2-hff2 0.357 0.214 
NmoditToV2-Inf3 
NmoditToVI{ev-Prep 0.765 0.765 
0.499  0 .789  0 .696  0 .586  
Table 6: Frequencies of t)airwise judgments  for 
the ternary  classification of nomino-verbal w~ri- 
ations ( ,  = inevahmble, + = correct;, - = in- 
correct). 
ni j  * + - hi. 
. 120 9 1 130 
? 1 184 6 191 
- 4 10 165 179 
n.j 125 203 172 500 
tic phenomena.  The problem of linguistic vari- 
at ion in information processing forces the lin- 
guist to reconsider parat)hrase and trm~sf'orma- 
tion mechanisms in a new perspective, based 
on real l inguistic data  and on systematic ort)us 
exploration. The  paraphrase judgment  is eval- 
uated in a new way, from a practical point of 
view: two sequences are said to be a paraphrase 
of each other if the user of an information sys- 
tem considers that  they bring identical or sin> 
ilar informat ion content. Regarding linguistic 
methodology, this work led us to find "l ight" so- 
lutions in terms of lexical encoding to describe 
complex semantic t)henomena. This approach is 
pronfising because it demonstrates that  linguis- 
tic knowledge can really enhance the results of 
term recognit ion beyond the ,norphology level, 
and that  semantics can be taken into account 
to sonm extent. 
References  
A. T. Arampatzis, T. Tsoris, C. H. A. Koster, and 
Tit. P. van der V~reide. 1998. Phrase-based infof 
mation retrieval. Information Proccssin9 '~ Mana.qe- 
ment, 34(6):693 707. 
Inge Bartning. 1990. Los syntagmes l)inoininaux en de - 
les types interprdtatifs subjectifs et agentifs. In Pro- 
cecdings, dixidmc congr~s des romanistcs .scandinavcs. 
Regina Barzilay, Kathleen McKeown, and Michael E1- 
hadad. 1999. hff'ormational fusion in the context 
of multi-document summarization. In Prvccedings of 
ACL'99, pages 55() 557, University of Mawland. 
Jean Carletta. 1996. Asessing agreement on classifica- 
tion tasks: The kappa statistics. Computational Lin- 
guistics, 22(2):249 254. 
J. Cohen. 1960. A coefficient of agreement for nominal 
scales. Educational and P.sychological Measurement, 
20(1):37-46. 
Cdcile Fabre. 1996. Intcrprdtation automatiquc des 
sdquences binominales en fran~:ais et en an.qlais. 
Ph.D. thesis, Universit5 Returns I. 
Christiml Jacquemin and Evelyne Tzoukermmm. 1999. 
NLP ibr term variant extraction: A synergy of mor- 
phology, lexicon, and syntax. In Tomek Strzalkowski, 
editor, Natural Language Information Retrieval, pages 
25-74. Kluwer, Boston, MA. 
Judith Klavans and Min-Yen Kan. 1998. Role of verbs 
in document analysis. In Proceedings of COLING- 
ACL'98, pages 680-686, Universitd e Montrdal, Mort- 
treal, Canada. 
Maria Lapata. 1999. Acquiring lexical generalizations 
from corpora: A case study ibr diathesis alternations. 
In Proceedings of ACL'99, pages 397-404, University 
of Maryland. 
C. J. van Rijsbergen. 1975. In\]ormation Retrieval. But- 
terworth, London. 
270 
J 
In: Proceedings of CoNLL-2000 and LLL-2000, pages 199-208, Lisbon, Portugal, 2000. 
Inductive Logic Programming for 
Corpus-Based Acquisition of Semantic Lexicons 
Pasca le  S4bi l lot  
IRISA - Campus de Beaulieu - 35042 Rennes cedex - France 
sebillot@irisa, fr 
P ier re t te  Bou i l lon  
TIM/ ISSCO - ETI - Universit4 de Gen~ve - 40 Bvd du Pont-d'Arve - 
CH-1205 Geneva-  Switzerland 
Pierrette. Bouillon@issco. unige, ch 
C4ci le Fabre  
ERSS - Universit@ de Toulouse II - 5 all@es A. Machado - 31058 Toulouse cedex - France 
cfabre@univ-tlse2, fr 
Abst ract  
In this paper, we propose an Inductive Logic 
Programming learning method which aims at 
automatically extracting special Noun-Verb (N- 
V) pairs from a corpus in order to build up 
semantic lexicons based on Pustejovsky's Gen- 
erative Lexicon (GL) principles (Pustejovsky, 
1995). In one of the components of this lex- 
ical model, called the qualia structure, words 
are described in terms of semantic roles. For 
example, the relic role indicates the purpose or 
function of an item (cut for knife), the agen- 
tive role its creation mode (build for house), 
etc. The qualia structure of a noun is mainly 
made up of verbal associations, encoding rela- 
tional information. The Inductive Logic Pro- 
gramming learning method that we have devel- 
oped enables us to automatically extract from 
a corpus N-V pairs whose elements axe linked 
by one of the semantic relations defined in the 
qualia structure in GL, and to distinguish them, 
in terms of surrounding categorial context from 
N-V pairs also present in sentences ofthe corpus 
but not relevant. This method has been theoret- 
ically and empirically validated, on a technical 
corpus. The N-V pairs that have been extracted 
will further be used in information retrieval ap- 
plications for index expansion 1. 
1This works is funded by the Agence universi- 
taire de la Francophonie (AUF) (Action de recherche 
partag4e "Acquisition automatique d'dldments du Lex- 
Keywords:  Lexicon learning, Generative 
Lexicon, Inductive Logic Programming, Infor- 
mation indexing. 
1 In t roduct ion  
Information retrieval (IR) systems aim at pro- 
viding a user who asks a query to a database of 
documents with the most relevant exts. The 
quality of these systems is usually measured 
with the help of two criteria: the recall rate, 
which corresponds to the proportion of relevant 
answers that have been given by the system 
compared to the total number of relevant an- 
swers in the database, and the precision rate, 
which denotes the proportion of relevant an- 
swers that are present among the given answers. 
In these IR systems, texts and queries are 
usually represented by indexes, that is, a col- 
lection of some of the words that they contain. 
The quality of the systems therefore highly de- 
pends on the type of indexing language that has 
been chosen. Two kinds of indexes exist: sim- 
ple indexes, which correspond to simple nouns 
(N), verbs (V) and/or adjectives (A) that oc- 
cur in a text or a query 2, and complex indexes, 
which correspond to the compounds (for exam- 
ple, NN compounds) present in the document or 
ique Gdndratif pour amdliorer les performances de 
syst~mes de recherche d'information", r@seau FRAN-  
CIL). 
2All the simple N, V and/or A can be kept as indexes, 
or the most frequent ones for a given text, or those whose 
frequencies in this text are especially high compared to 
their frequencies in the database, etc. 
199 
the question. The solutions that are given for 
a user query are the texts whose indexes better 
match the query index. 
In order to obtain the hightest performances, 
IR systems usually offer some possibilities to 
expand both query and text indexes. Tra- 
ditional index expansion concerns morpho- 
syntactic similarities; for example, the same in- 
dex words in plural and singular forms can be 
matched. Some other systems deal with a kind 
of semantic similarities: if they possess a lin- 
guistic knowledge database, they can, for ex- 
ample, expand a nominal index by following 
synonymy or hyperonymy links. These systems 
are however usually limited to intra-categorial 
expansion, especially N-to-N one. Here we 
deal with a new kind of expansion that has 
been proven particularly useful (Grefenstette, 
1997; Fabre and S~billot, 1999) for document 
database questioning. It concerns N-V links 
and aims at allowing matching between ominal 
and verbal formulations that are semantically 
close. For example, our objective is to permit a 
matching between a query index disk store and 
the text formulation to sell disks, related by the 
typical function of a store. 
N-V index expansion however has to be con- 
trolled in order to ensure that the same con- 
cept is involved in the two formulations. We 
have chosen Pustejovsky's Generative Lexicon 
(GL) framework (Pustejovsky, 1995; Bouillon 
and Busa, 2000) to define what a relevant N- 
V link is, that is, what is a N-V pair in which 
the N and the V are related by a semantic link 
which is close, and which can therefore be used 
to expand indexes. 
In GL formalism, lexical entries consist in 
structured sets of predicates that define a word. 
In one of the components of this lexical model, 
called the qualia structure, words are described 
in terms of semantic roles. The telic role in- 
dicates the purpose or function of an item (for 
example, cut for knife), the agentive role its cre- 
ation mode (build for house), the constitutive 
role its constitutive parts (handle for handcup) 
and the formal role its semantic ategory (con- 
tain (information) for book). The qualia struc- 
ture of a noun is mainly made up of verbal as- 
sociations, encoding relational information. We 
assert hat these N-V links are especially rele- 
vant for index expansion in IR systems (Fabre 
and S~billot, 1999), and what we call a relevant 
N-V pair afterwards in the paper is a pair com- 
posed of a N and a V which are related by one of 
the four semantic relations defined in the qualia 
structure in GL. 
GL is however currently just a formalism; no 
generative l xicons exist that are precise nough 
for every domain and every application (for eg. 
IR), and the cost of a manual construction of 
a lexicon based on GL principles is prohibitive. 
Moreover the real N-V links that are the key- 
point of this formalism cannot be defined a pri- 
ori and have to be acquired from corpora of 
the studied domain. The aim of this paper is 
therefore to present a machine learning method, 
developed in the Inductive Logic Programming 
framework, that enables us to automatically ex- 
tract from a corpus N-V pairs whose elements 
are linked by one of the semantic relations de- 
fined in the qualia structure in GL, and to dis- 
tinguish them, in terms of surrounding cate- 
gorial (Part-of-Speech, POS) context from N- 
V pairs also present in sentences of the corpus 
but not relevant. It will be divided in three 
parts. Section 2 focusses on the motivation of 
this project regarding the use of GL. Section 3 
explains the machine learning method that we 
have developed. Section 4 is dedicated to its 
theoretical nd empirical validations, and to the 
results of its application to a technical corpus. 
2 Mot ivat ion  
As stated in the introduction, our work makes 
two strong claims: firstly N-V associations de- 
fined in GL are relevant for IR and secondly 
this information can be acquired from a corpus 
on the basis of surrounding POS context. These 
presuppositions have to be motivated before ex- 
plaining the learning method: 
1. The aim of GL is to define underspec- 
ified lexical representations that will acquire 
their specifications in context. For example, the 
qualia structure of book indicates that its de- 
fault function is read and that it is created by 
the act of writing. But this information has to 
be enriched in context in order to characterize 
how words are used in specific domains. For 
example, the qualia structure of book will also 
have to indicate that the book can be shelved or 
indexed if this information is necessary to inter- 
pret texts from information science domain. GL 
200 
is therefore a theory of words in context. It can 
also be seen as a way to structure information 
in corpora and, in that sense, the relations it 
defines are therefore privileged information for 
IR. In this perspective, GL has been preferred 
to existing lexical resources uch as WordNet 
(Fellbaum, 1998) for two main reasons: lexical 
relations that we want to exhibit - namely N-V 
links - are unavailable in WordNet, which fo- 
cuses on paradigmatic lexical relations; Word- 
Net is a domain-independent, static resource, 
which can not be used as such to describe lexi- 
cal associations in specific texts, considering the 
great variability of semantic associations from 
one domain to another. 
2. In GL, the qualia structures are not arbi- 
trary repository of information. They contain 
the information ecessary to explain the syn- 
tactic behaviour of the item. We would there- 
fore expect that there are strong connections 
between some specific syntactic phenomena and 
some specific qualia relations. For example, the 
middle construction seems to be only possible if 
a telic relation holds between the N and V (Bas- 
sac and Bouillon, 2000) (for example: ??this 
book writes well vs this book reads well). Sim- 
ilarly, imperative constructions (e.g. open the 
door, follow the links) or adjectival sentences (a 
book difficult to write/read) may also indicate 
a qualia relation. These are some of the con- 
structions that we want to identify primilarly 
in corpora by the learning method. 
3 The  mach ine  learn ing  method 
Trying to infer lexical semantic information 
from corpora is not new: lots of works have 
already been conducted on this subject, espe- 
cially in the statistical learning domain (see 
(Grefenstette, 1994b), for e.g., or (Habert et 
al., 1997) and (Pichon and S~billot, 1997) for 
surveys of this field). Following Harris's frame- 
work (Harris et al, 1989), such research tries to 
extract both syntagmatic and paradigmatic n- 
formation, respectively studying the words that 
appear in the same window-based or syntactic 
contexts as a considered lexical unit (first or- 
der word affinities (Grefenstette, 1994a)), or the 
words that generate the same contexts as the 
key word (second order word affinities). For ex- 
ample, (Briscoe and Carroll, 1997) and (Faure 
and N~dellec, 1999) try to automatically learn 
verbal argument structures and selectional re- 
strictions; (Agarwal, 1995) and (Bouaud et al, 
1997) build semantic classes; (Hearst, 1992) 
and (Morin, 1997) focus on particular lexi- 
cal relations, like hyperonymy. Some of these 
works are concerned with automatically ob- 
taining more complete lexical semantic repre- 
sentations ((Grefenstette, 1994b; Pichon and 
S~billot, 1999). Among these studies, (Puste- 
jovsky et al, 1993) presents a research whose 
aim is to acquire GL nominal qualia structures 
from a corpus; this work is however quite dif- 
ferent from ours because it supposes that the 
qualia structure contents are initialized and are 
only refined with the help of the corpus by using 
the type coercion 3 mechanism. 
In order to automatically acquire N-V pairs 
whose elements are linked by one of the seman- 
tic relations defined in the qualia structure in 
GL, we have decided to use a machine learning 
method. This section is devoted to the expla- 
nation of this choice and to the description of 
the method that we have developed. 
Machine learning aims at automatically 
building programs from examples that are 
known to be positive or negative examples of 
their runnings. According to Mitchell (Mitchell, 
1997), "a computer program is said to learn 
from experience E with respect to some class 
of tasks T and performance measure P, if  its 
performance at tasks in T, as measured by P, 
improve with experience E". 
Among different machine learning techniques, 
we have chosen the Inductive Logic Program- 
ming framework (ILP) (Muggleton and De- 
Raedt, 1994) to learn from a textual corpus N-V 
pairs that are related in terms of one of the re- 
lations defined in the qualia structure in GL. 
Programs that are infered from a set of facts 
and a background knowledge are here logic pro- 
grams, that is, sets of Horn clauses. In the ILP 
framework, the main idea is to obtain a set of 
generalized clauses that is sufficiently generic 
to cover the majority of the positive examples 
(E+), and sufficiently specific to rightly corre- 
spond to the concept we want to learn and to 
cover no (or a few - some noise can be allowed) 
negative xample(s) (E - ) .  For our experiment, 
3A semantic operation that converts an argument to 
the type which is expected by a function, where it would 
otherwise result in a type error. 
201 
we furnish a set of N-V pairs related by one of 
the qualia relations within a POS context (E+), 
and a set of N-V pairs that are not semantically 
linked (E-),  and the method infers general rules 
(clauses) that explain these E +. This particular 
explanatory characteristic of ILP has motivated 
our choice: ILP does not just provide a predic- 
tor (this N-V pair is relevant, this one is not) 
but also a data-based theory. Contrary to some 
statistical methods, it does not just give raw 
results but explains the concept hat is learnt 4. 
We use Progol (Muggleton, 19915) for our 
project, Muggleton's ILP implementation that 
has already been proven well suited to deal with 
a big amount of data in multiple domains, and 
to lead to results comparable to other ILP im- 
plementations (Roberts et al, 1998). 
In this section we briefly describe the corpus 
on which our experiment has been conducted. 
We then explain the elaboration of E + and E -  
for Progol. We finally present he generalized 
clauses that we obtain. The validation of the 
method is detailed in section 4. 
3.1 The corpus 
The French corpus used in this project is 
a 700 kBytes handbook of helicopter main- 
tenance, given to us by MATRA CCR 
A@rospatiale, which contains more than 104000 
word occurrences 5. The MATRA CCR corpus 
has some special characteristics that are espe- 
cially well suited for our task: it is coherent; 
it contains lots of concrete terms (screw, door, 
etc.) that are frequently used in sentences to- 
gether with verbs indicating their telic (screws 
must be tightened, etc.) or agentive roles. 
This corpus has been POS-tagged with the 
help of annotation tools developed in the MUL- 
TEXT project (Armstrong, 1996); sentences and 
words are first segmented with MtSeg; words 
are analyzed and lemmatized with Mmorph (Pe- 
titpierre and Russell, 1998; Bouillon et al, 
1998), and finally disambiguated by the Tatoo 
tool, a Hidden Markov Model tagger (Arm- 
strong et al, 1995). Each word therefore only 
receive one POS-tag, with less than 2% of er- 
4Learning with ILP has already been successfully 
used in natural language processing, for example incor- 
pus POS-tagging (Cussens, 1996) or semantic nterpre- 
tation (Mooney, 1999). 
5104212 word occurrences. 
rors. 
3.2 Example  const ruct ion  
The first task consists in building up E + and 
E -  for Progol, in order for it to infer gener- 
alized clauses that explain what, in the POS 
context of N-V pairs, distinguishes the relevant 
pairs from the not relevant ones. Work has to 
be done to determine what is the most appro- 
priate context for this task. We just present 
here the solution we have finally chosen. Sec- 
tion 4 describes methods and measures to eval- 
uate the "quality" of the learning that enable 
us to choose between the different contextual 
possibilities. Here is our methodology for the 
construction of the examples. 
We first consider all the nouns of the MA- 
TRA CCR corpus. More precisely, we only deal 
with a 81314 word occurrence subcorpus of the 
MATRA CCR corpus, which is formed by all 
the sentences that contain at least one N and 
one V. This subcorpus contains 1489 different 
N (29633 noun occurrences) and 567 different 
V (9522 verb occurrences). For each N of this 
subcorpus, the 10 most strongly associated V, in 
terms of Chi-square, are selected. This first step 
both produces pairs that are really bound by 
one qualia relation ((dcrou, serrer)) 6 and pairs 
that are fully irrelevant ((roue, prescrire)) 7.
Each pair is manually annotated as relevant 
or irrelevant according to Pustejovsky's qualia 
structure principles. A Perl program is then 
used to find the occurrences of these N-V pairs 
in the sentences of the corpus. 
For each occurrence of each pair that is sup- 
posed to be used to build one E +, that is for 
each of the previous pairs that has been glob- 
ally annotated as relevant, a manual control has 
to be done to ensure that the N and the V really 
are in the expected relation within the studied 
sentence. After this control, a second Perl pro- 
gram automatically produces the E +. Here is 
the form of the positive examples: 
POSITiVE(category_before_N, category_after.N, 
category_before_V, V_type, distance, position). 
where V_type indicates if the V is an infinitive 
form, etc., distance corresponds to the number 
6(nut, tighten). 
7(wheel, prescribe) 
202 
of verbs between the N and the V, and position 
is POS (for positive) if the V appears before the 
N in the sentence, NEG if the N appears before 
the V. 
For example, 
POSITIVE(VRBINF, P_DE, VID, VRBINF~ 0, 
POS). 
means that a N-V pair, in which the N is 
surrounded with an infinitive verb on its left 
(VRBINF) and a preposition de s (P.DE) on its 
right, in which the V is preceded by nothing 9
(VID) 1? and is an infinitive one (VRBINF), in 
which no verb exists between the N and the V 
(0), and in which the V appears before the N 
in the sentence (POS), is a relevant pair (for ex- 
ample, in ouvrir la porte de ...). 
The E -  are elaborated in the same way than 
the E +, with the same Perl program. E -  and 
E + forms are identical, except he presence of a 
sign :- before the predicate POSITIVE to denote 
aE- :  
:-POSITIVE (category_before.N, 
category_after_N, category_before_V, V_type, 
distance, position). 
These E -  are automatically built from the 
previous highly correlated N-V pairs that have 
been manually annotated as irrelevant. For ex- 
ample, 
:-POSITIVE(VID, P_PAR, NC, VRBPP, 0, NEG). 
means that a N-V pair, in which the N has noth- 
ing on its left (VID) and a preposition par n 
(P_PAR) on its right, in which the V is preceded 
by a noun (NC) and is a past participle (VRBPP), 
in which no verb exists between the N and the 
V (0), and in which the V appears after the N 
in the sentence (NEG), is an irrelevant pair (for 
example, in freinage par goupilles fendues). 
4031 E + and about 7000 E -  are automati- 
cally produced this way from the corpus. 
sOl. 
9Or by one of the three categories that we do not 
consider for example laboration, that is, determiners, 
adverbs and adjectives. 
1?Empty. 
nBy. 
3.3 Learn ing  w i th  the  he lp  of  P rogo l  
These E + and E -  are then furnish to Progol 
in order for it to try to infer generalized clauses 
that explain the concept "qualia pair" versus 
"not qualia pair". We do not discuss here ei- 
ther parameter setting that concerns the choice 
of the example POS context, or evaluation cri- 
teria; this discussion is postponed to next sec- 
tion; we simply present he learning method and 
the type of generalized clauses that we have ob- 
tained. 
Some information have to be given to Progol 
for it to know what are the categories that can 
undergo a generalization. For example, if two 
E + are identical but possess different locative 
prepositions as second arguments (for eg. sur 12 
and sous13), must Progol produce a generaliza- 
tion corresponding to the same clause except 
that the second argument is replaced by the 
general one: locative-preposition, or by a still 
more general one: preposition? 
The background knowledge used by Progol is 
knowledge on the domain. For example here, it 
contains the fact that a verb can be found in 
the corpus in an infinitive or a conjugated form, 
etc. 
verbe( V ) :- infinitif( V ). 
verbe( V ) :- conjugue( V ). 
and that an infinitive form is denoted by the 
tag VERBINF, and a conjugated form by the tags 
VERB-PL and VER.B-SG, etc. 
infinitif( verbinf ). 
conjugue( verb-pl ). 
conjugue( verb-sg ). 
When Progol is provided with all this knowl- 
edge, learning can begun. The output of Progol 
is of two kinds: some clauses that have not at 
all been generalized (that is, some of the E+), 
and some generalized clauses; we call the set of 
these generalized clauses G, and it is this set G 
that interests us here. Here is an example of one 
of the generalized clauses that we have obtained 
in our experiment: 
POSITIVE(A, C, C, D, E, F) :- 
PREPOSITIONLIEU(A), VIDE(C), VERBINF(D), 
PRES(E). (1) 
12On" 
13Under. 
203 
which means that N-V pairs (i) in which the 
category before the N is a locative preposition 
(PREPOSITIONLIEU(A)), (ii) in which there is 
nothing after the N and before the V (VIDE(C) 
for the second and third arguments), (iii) in 
which the V is an infinitive one (VERBINF(D)), 
and (iv) in which there is no verb between the N 
and the V (proximity denoted by P:aEs(E)14), 
are relevant. No constraint is set on N/V order 
in the sentences. 
This generalized clause covers, for example, 
the following E+: 
POSITIVE(P_SUR, VID, VID, VERBINF, 0, POS). 
which corresponds to the relevant pair (prise, 
brancher) 15 that is detected in the corpus in the 
sentence "Brancher les connecteurs sur les prises 
~lectriques.". 
Some of the generalized clauses in G cover 
lots of E +, others far less. We now present a 
method to detect what the "good" c, lauses are, 
that is, the clauses that explain the concept that 
we want to learn, and a measure of the "quality" 
of the learning that has been conducted. 
4 Learn ing  va l idat ion  and  resu l ts  
This section is dedicated to two aspects of 
the validation of our machine learning method. 
First we define the theoretical validation of the 
learning, that is, we focus on the determination 
of a means to detect what are the "good" gen- 
eralized clauses, and of a measure of the quality 
of the concept learning; this parameter setting 
and evaluation criterion phase explains how we 
have chosen the precise POS context for N-V 
pairs in the E + and E -  (as described in subsec- 
tion 3.2): the six contextual elements in exam- 
ples are the combination that leads to the best 
results in terms of the learning quality measure 
that we have chosen. The second step of the 
validation is the empirical one. We have applied 
the generalized clauses that have been selected 
to the Mat ra  CCR corpus and  haw~ evaluated 
the quality of the results in terms of pairs that 
are indicated relevant or not. Here  are these 
two phases. 
14Close(E). 
l~(plug, to plug in). 
4.1 Theoret ical  val idation 
As we have previously noticed, among the gen- 
eralized clauses produced from our E + and E -  
by Progol (set G), some of them cover a lot of 
E +, others only a few of them. What we want 
is to get a way to automatically find what are 
the generalized clauses that have to be kept in 
order to explain the concept we want to learn. 
We have first defined a measure of the theo- 
retical generality of the clauses 16. The theoreti- 
cal generality of a generalized clause is the num- 
ber of not generalized clauses (E +) that this 
clause can cover. For example, both 
POSITIVE(P_AUTOURDE, VID, VID, VERBINF, 
0, NEG). 
and 
POSITIVE(P_CHEZ, VID, VID, VERBINF, 0, 
POS). 
can be covered by clause (1) (cf. subsec- 
tion 3.3). During the study of, for example, 
the distribution of the number of clauses in G 
on these different heoretical generality values, 
our "hope" is to obtain a gaussian-like graph 
in order to automatically select all the clauses 
present under the gaussian plot, or to calculate 
two thresholds that cover 95% of these clauses 
and to reject the other 5%. This distribution is
however not a gaussian one. 
Our second try has not only concerned the 
theoretical coverage of G clauses but also their 
empirical coverage. This second measure that 
we have defined is the number of E + that are 
really covered by each clause of G. We then con- 
sider the distribution of the empirical coverage 
of G clauses on the theoretical coverages of these 
clauses, that is, we consider the graph in which, 
for each different heoretical measure value for 
G clauses, we draw a line whose length corre- 
sponds to the total number of E + covered by 
the G clauses that have this theoretical cover- 
age value. Here two gaussians clearly appear 
(cf. figure 1), one for rather specific lauses and 
the other for more general ones. We have there- 
fore decided to keep all the generalized clauses 
produced by Progol. 
16We thank J. Nicolas, INRIA researcher at IRISA, for 
his help on this point. 
204 
800 
700 
600 
5OO 
400 
ul 300  
200 
100 
!!iiii~i~ii!ii!i!iiii!iiii!iiii~iiiii!iil 
Theoretical coverage 
Figure 1: Distribution of 
The second point concerns the determination 
of a measure of the quality of the learning for the 
parameter setting. We are especially interested 
in the percentage ofE + that are covered by the 
generalized clauses, and if we permit some noise 
in Progol parameter adjustment to allow more 
generalizations, by the percentage of E -  that 
are rejected by these generalized clauses. The 
measure of the recall and the precision rates of 
the learning method can be summarized in a 
Pearson coefficient: 
Pearson = (TP ,TN) - (FP ,FN)  
x /P rP*PrN*AP*AN 
where A = actual, Pr = predicated, P -- pos- 
itive, N= negative, T= true, F= false; the more 
close to 1 this value is, the better the learning 
is. 
The results for our learning method with a 
rate of Progol noise equal to 0 are the following: 
from the 4031 initial E + and the 6922 initial E- ,  
the 109 generalized clauses produced by Progol 
cover 2485 E + and 0 E-; 1546 E + and 6922 E-  
positive examples on clauses 
are therefore uncovered; the value of the Pear- 
son coefficient is 0.71. (NB: Figure 1 illustrates 
these results). 
We have developed a Perl program whose role 
is to find which Progol noise rate leads to the 
best results. This Progol noise rate is equal to 
37. With this rate, the results are the following: 
from the 4031 initial E + and the 6922 initial E-,  
the 66 generalized clauses produced by Progol 
cover 3547 E + and 348 E-; 484 E + and 6574 E-  
are therefore uncovered; the value of the Pear- 
son coefficient is 0.84. The stability of the set 
of learnt generalized clauses has been tested. 
4.2 Empir ica l  val idat ion 
In order to evaluate the empirical validity of our 
learning method, we have applied the 66 gen- 
eralized clauses to the Matra CCR corpus and 
have studied the appropriateness of the pairs 
that are stated relevant or irrelevant by them. 
Of course, it is impossible to test all the N-V 
combinations present in such a corpus. Our 
evaluation has focussed on some of the signif- 
205 
icant nouns of the domain. 
A Perl program presents to one expert all the 
N-V pairs that appear in one sentence in a part 
of the corpus and include one of the studied 
nouns. The expert manually tags each pair as 
relevant or not. This tagging is then compared 
to the results obtained for these N-V pairs of 
the same part of the corpus by the application 
of the generalized clauses learnt wit\]h Progol. 
The results for seven significant nouns (vis, 
@crou, porte, voyant, prise, capot, bouchon) 17 
are presented in table 1. In the left column, one 
N-V pair is considered as tagged "relevant" by 
the generalized clauses if at least one of them 
covers this pair; in the right one, at least six 
different clauses of G must cover a pair for it 
to be said correctly detected by the generalized 
clauses; the aim of this second test is to reduce 
noise in the results. 
1 occurrence 6 occurrences 
correctly found: 49 correctly found: 23 
incorrectly found: 54 incorrectly found: 4 
not found: 10 not found: 36 
Pearson = 0.5138 Pearson = 0.5209 
Table 1: Empirical validation on Matra CCR 
corpus 
The results are quite promising, especially if 
we compare them to those obtain by Chi-square 
correlation (cf. table 2). This comparison is 
interesting because Chi-square is the first step 
of our selection of N-V couples in the corpus (cf. 
subsection 3.2). 
correctly found: 38 
incorrectly found: 124 
not found: 21 
Pearson = 0.1206 
Table 2: Chi-square results on Matra CCR cor- 
pus 
5 Conc lus ion 
The Inductive Logic Programming learning 
method that we have proposed in order to de- 
fine what is a N-V pair whose elements are 
17(screw, nut, door, indicator signal, plug, cowl, cap). 
bound by one of the qualia relations in Puste- 
jovsky's Generative Lexicon formalism leads to 
very promising results: 83.05% of relevant pairs 
(after one occurrence) are detected for seven sig- 
nificant nouns; these results have to be com- 
pared with the 64% results of Chi-square. It 
is worth noticing that beyond this simple com- 
parison with one of the possible pure statis- 
tics based method is, the interest of using ILP 
learning is its explanatory characteristic; and it 
is this characteristic that have motivated our 
choice: contrary to statistical methods, our ILP 
method does not just extract statistically corre- 
lated pairs but it permits to automatically earn 
rules that distinguish relevant pairs from others. 
The fact that noise has to be used in Progol to 
obtain these results however means that some- 
thing is missing in our E + to fully define the 
concept "qualia pair" versus "not qualia pair"; 
some E -  have to be covered to define it better. 
A piece of information, maybe syntactic and/or 
semantic is missing in our E + to fully character- 
ize it. This fact can be easily illustrated by the 
following example: 'Verbinf det N' structures 
are generally relevant (ouvrir la porte 19, etc.), 
except when the N indicates a collection of ob- 
jects (nettoyer l'ensemble du rdservoir 2?) or a 
part of an object (vider le fond du rdservoir21). 
A simple POS-tagging of the sentences offers 
no difference between them. We are currently 
working on a semantic tagging of the Matra 
CCR corpus in order to improve the results. 
Another future work concerns the automatic 
distinction between the various qualia roles dur- 
ing learning. The last phase of the project will 
deal with the real use of the N-V pairs obtained 
by the machine learning method within one in- 
formation retrieval system and the evaluation of 
the improvement of its performances. 
Re ferences  
Rajeev Agarwal. 1995. Semantic Feature Extraction 
from Technical Texts with Limited Human Inter- 
vention. Ph.D. thesis, Mississippi State Univer- 
sity, USA. 
Susan Armstrong, Pierrette Bouillon, and 
Gilbert Robert. 1995.  Tagger Overview. 
lSThis comparison could be extended to other corpus 
frequency based technics (mutual information, etc.). 
19Open the door. 
2?Clean the whole tank. 
21Empty the tank bottom. 
206 
Technical report, ISSCO, (http://issco- 
www.unige.ch/staff/robert/tatoo/tagger.html). 
Susan Armstrong. 1996. Multext: Multilingual 
Text Tools and Corpora. In H. Feldweg and 
W. Hinrichs, editors, Lexikon und Text, pages 
107-119. Tiibingen: Niemeyer. 
Christian Bassac and Pierrette Bouillon. 2000. The 
Polymorphism of Verbs Exhibiting Middle Tran- 
sitive Alternations in English. Technical report, 
ISSCO. 
Jacques Bouaud, Beno~t Habert, Adeline Nazarenko, 
and Pierre Zweigenbaum. 1997. Regroupe- 
ments issus de d@pendances syntaxiques en cor- 
pus : cat@gorisation et confrontation avec deux 
mod@lisations conceptuelles. In Proceedings of 
Ingdnierie de la Connaissance, Roscoff, France. 
Pierrette Bouillon and Federica Busa. 2000. Gener- 
ativity in the Lexicon. CUP:Cambridge, In Press. 
Pierrette Bouillon, Sabine Lehmann, Sandra 
Manzi, and Dominique Petitpierre. 1998. 
DSveloppement de lexiques ~ grande @chelle. 
In Proceedings of colloque de Tunis 1997 "La 
mgmoire des mots', Tunis, Tunisie. 
Ted Briscoe and John Carroll. 1997. Automatic Ex- 
traction of Subcategorisation from Corpora. In 
Proceedings of 5th ACL conference on Applied 
Natural Language Processing, Washington, USA. 
James Cussens. 1996. Part-of-Speech Disambigua- 
tion using ILP. Technical report, Oxford Univer- 
sity Computing Laboratory. 
C@cile Fabre and Pascale S~billot. 1999. Seman- 
tic Interpretation of Binominal Sequences and In- 
formation Retrieval. In Proceedings of Interna- 
tional ICSC Congress on Computational Intelli- 
gence: Methods and Applications, CIMA '99, Sym- 
posium on Advances in Intelligent Data Analysis 
AIDA '99, Rochester, N.Y., USA. 
David Faure and Claire N@dellec. 1999. Knowledge 
Acquisition of Predicate Argument Structures 
from Technical Texts using Machine Learning: 
the System ASIUM. In Dieter Fensel Rudi Studer, 
editor, Proceedings of 11th European Workshop 
EKAW'99, Dagstuhl, Germany. Springer-Verlag. 
Christiane Fellbaum, editor. 1998. WordNet: An 
Electronic Lexical Database. MIT Press, Cam- 
bridge, MA. 
Gregory Grefenstette. 1994a. Corpus-Derived First, 
Second and Third-Order Word Affinities. In 
Proceedings of EURALEX'9~, Amsterdam, The 
Netherlands. 
Gregory Grefenstette. 1994b. Explorations in Auto- 
matic Thesaurus Discovery. Dordrecht: Kluwer 
Academic Publishers. 
Gregory Grefenstette. 1997. SQLET: Short Query 
Linguistic Expansion Techniques, Palliating One- 
Word Queries by Providing Intermediate Struc- 
ture to Text. In McGill-University, editor, Pro- 
ceedings of Recherche d'Informations Assistde 
par Ordinateur, RIAO'g7, Montr@al, Qu@bec, 
Canada. 
Beno~t Habert, Adeline Nazarenko, and Andr@ 
Salem. 1997. Les linguistiques de corpus. Ar- 
mand Collin/Masson, Paris. 
Zelig Harris, Michael Gottfried, Thomas Ryckman, 
Paul Mattick(Jr), Anne Daladier, Tzvee N. Har- 
ris, and Suzanna Harris. 1989. The Form of 
Information in Science, Analysis of Immunology 
Sublanguage. Kluwer Academic Publisher, Dor- 
drecht. 
Marti A. Hearst. 1992. Automatic Acquisition 
of Hyponyms from Large Text Corpora. In 
Proceedings of 15th International Conference on 
Computational Linguistics, COLING-92, Nantes, 
France. 
Tom M. Mitchell. 1997. Machine Learning. 
McGraw-Hill. 
Raymond Mooney. 1999. Learning for Semantic In- 
terpretation: Scaling Up without Dumbing Down. 
In Proceedings of Learning Language in Logic, 
LLL99, Bled, Slovenia. 
Emmanuel Morin. 1997: Extraction de liens 
s@mantiques entre termes dans des corpus de 
textes techniques : application ~ l'hyponymie. 
In Proceedings of Traitement Automatique des 
Langues Naturelles, TALN'97, Grenoble, France. 
Stephen Muggleton and Luc De-Raedt. 1994. In- 
ductive Logic Programming: Theory and Meth- 
ods. Journal of Logic Programming, 19-20:629- 
679. 
Stephen Muggleton. 1995. Inverse Entailment and 
Progol. New Generation Computing, 13(3-4):245- 
286. 
Dominique Petitpierre and Graham Russell. 1998. 
Mmorph - the Multext Morphology Program. 
Technical report, ISSCO. 
Ronan Pichon and Pascale S~billot. 1997. Acquisi- 
tion automatique d'informations lexicales ~ partir 
de corpus : un brian. Research report n?3321, IN- 
RIA, Rennes. 
Ronan Pichon and Pascale S@billot. 1999. From 
Corpus to Lexicon: from Contexts to Semantic 
Features. In Proceedings of Practical Applications 
in Language Corpora, PALC'99, to appear, Lodz, 
Poland. 
James Pustejovsky, Peter Anick, and Sabine Bergler. 
1993. Lexical Semantic Techniques for Corpus 
Analysis. Computational Linguistics, 19(2). 
James Pustejovsky. 1995. The Generative Lexicon. 
Cambridge:MIT Press. 
Sam Roberts, Wim Van-Laer, Nico Jacobs, Stephen 
Muggleton, and Jeremy Broughton. 1998. A 
Comparison of ILP and Propositional Systems on 
207 
Propositional Data. In Springer-Verlag, editor, 
Proceedings of 8th International Workshop on In- 
ductive Logic Programming, ILP-98, :Berlin, Ger- 
many. LNAI 1446. 
208 
