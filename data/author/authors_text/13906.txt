Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1114?1123, Dublin, Ireland, August 23-29 2014.
Fast Domain Adaptation of SMT models without in-Domain Parallel Data
Prashant Mathur
?
Fondazione Bruno Keseler
Povo - 38100 (IT)
first@fbk.eu
Sriram Venkatapathy
Xerox Research Center Europe
Meylan (FR)
first.last@xrce.xerox.com
Nicola Cancedda
?
Microsoft, London (UK)
first.last@gmail.com
Abstract
We address a challenging problem frequently faced by MT service providers: creating a domain-
specific system based on a purely source-monolingual sample of text from the domain. We solve
this problem by introducing methods for domain adaptation requiring no in-domain parallel data.
Our approach yields results comparable to state-of-the-art systems optimized on an in-domain
parallel set with a drop of as little as 0.5 BLEU points across 4 domains.
1 Introduction
We consider the problem of creating the best possible statistical machine translation (SMT) system for
a specific domain when no parallel sample or training data from such domain is available. We assume
that we have access to a collection of phrase tables (PT) and other models independently created from
now unavailable corpora, and we receive a monolingual source language sample from a text source we
would like to optimize for.
For a MT provider to deliver a SMT system tailored to a customer?s domain, a sample dataset is
requested. In most cases, the customer is able to provide an in-domain mono-lingual sample from his
operations. However, it is generally not feasible for the customer to provide the translations as well
because the customer has to hire professional translators to do that. In such a scenario, the translations has
to be generated by MT service provider itself by hiring human translators thus requiring an investment
upfront. The methods proposed in this paper aim to avoid that by building a good quality pilot SMT
system leveraging only sample mono-lingual source corpus, and previously trained library of models.
This in turn postpones the task of generating in-domain parallel data to a later date when there is a
commitment by the customer.
Unavailability of the raw parallel data could derive from a trading model where data owners share
intermediate-level resources like PTs, Reordering Models (RM) and Language Models (LM), but can
not, or do not want to, share the textual data such resources were derived from. This particular scenario
has been explained in (Cancedda, 2012).
This scenario is similar to the multi-model framework studied in (Sennrich et al., 2013), with the
additional challenge that no parallel development set is available. We build on the linear mixture model
combination of the cited work, extending it to our more challenging environment:
1. We propose a new measure derived from the popular BLEU score (Papineni et al., 2002) to assess
the fitness of a PT to cope with a given monolingual sample S. This measure is computed from
n-gram statistics that can be easily extracted from a PT.
2. We propose a new method for tuning the parameters of a log linear model that does not require
an in-domain parallel development set, and yet achieves results very close to traditional tuning on
parallel in-domain data.
We present our proposed metric BLEU-PT and computation of multi-model in Section 2. The pa-
rameter estimation of log-linear parameters of the SMT system is described in Section 3. We present
experiments and results in Sections 4 and 6 respectively.
?
Major part of the work was performed when the authors were in Xerox Research Center Europe.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1114
2 Building Multi-Model
Given a library of phrase tables, the goal of this step is to generate a domain adapted multi-model. The
challenging aspect in our scenario is the lack of in-domain parallel data, as well as absence of original
parallel corpora corresponding to the library of models. This rules out the possibility of using metrics
such as cross-entropy (Sennrich, 2012b) or LM-perplexity for computing the mixing coefficients. We
present our proposed metric in section 2.1, and interpolation of the phrase tables in section 2.2.
2.1 BLEU-PT
Given a source corpus s, and a set of phrase tables {pt
1
, pt
2
,. . . ,pt
n
}, the goal is to measure the similarity
of each of these tables with s. For measuring the similarity, we use BLEU-PT which is an adaptation of
the popular BLEU score for measuring the similarity between a corpus and a phrase table. The metric
BLEU-PT is measured as described in Equation 1.
BLEU-PT(PT, S) =
(
4
?
n=1
match(n|pt, s)
total(n|s)
)
1/4
(1)
where match(n|pt, s) is the count of n-grams of order n in the source corpus s that exist in the source
side of the phrase table pt. total(n|s) is the number of n-grams of order n in the source corpus.
2.2 Interpolating Models
A state-of-the-art approach for building multi-models is through linear interpolation of component mod-
els, exemplified in Equation 2 for the case of the forward conditional phrase translation model.
h
phr
(s, t) = log
N
?
j=1
?
j
P
phr,j
(t|s) (2)
Various approaches have been suggested for computing the coefficients ? of the interpolated model, the
most recent being perplexity minimization described in (Sennrich, 2012b), where each translation model
feature is optimized separately on the parallel development set. Our work is set in a scenario where no
parallel development set is available for optimizing the interpolation coefficients. We have also observed
that perplexity minimization is computationally intensive, requires aligned parallel development set, and
the optimization time increases rapidly with increasing number of component models (for details, see
Section 4.2).
We propose a simple approach for computation of the mixing coefficients that relies on the similarity
of each model with respect to the test set. The mixing coefficients are obtained by normalizing similarity
values. The similarity between a model (phrase table) and a corpus is computed using the BLEU-PT
metric proposed in the previous section. Another similarity metric that can be used is LM Perplexity.
However, in the current scenario we do not have resources (training data) to build a source side LM for
computing the perplexity.
We empirically compare our method for computing mixing coefficients with the the perplexity min-
imization method. We also experiment with applying the mixing coefficients obtained by using our
method for mixing features of a reordering and language model.
3 Parameter Estimation
The overall quality of translation is strongly impacted by how optimized the weights of the log-linear
combination of various translation features are for a domain of interest. MERT (Och, 2003) and MIRA
(Watanabe et al., 2007) are popular solution to compute an optimal weight vector by minimizing the error
on a held-out parallel development set. BLEU and its approximations are commonly used error metrics.
In this paper we assume lack of a parallel development set, therefore the above methods cannot be used.
Pecina et. al. (2012) showed that the optimized log-linear weight vector
1
of a SMT system does not
depend as much on the actual domain of the development set (on which the system was optimized), as
1
Not to be confused with the mixing coefficients in a linear combination of model components.
1115
on how ?distant? the relevant domain is from the domain of the training corpus used to build the SMT
models. This is an important finding. It means that the weight vector can be modeled as a function of the
distance/similarity between the in-domain development set and the model built from the training set. In
this work, we learn this function from examples of previous parameter optimizations, using our BLEU-
PT as a similarity metric. Once we have retrieved the most relevant PTs (translation and reordering
models) from our library, and we have linearly interpolated them using normalized BLEU-PT, we use
the learned model to estimate the optimal value of the log-linear weights, instead of optimizing them.
In order to learn this mapping, we create a dataset of examples (pairs of the form <BLEU-PT, log-
linear weight vector>, where weight vectors are normalized to ensure comparability across models) by
performing repeated optimizations for out-domain models on a number of parallel development sets (see
section 4 for more details of this data) using a traditional optimization method (MIRA in this work).
Based on this dataset, the function of our interest can therefore be learnt using a supervised approach.
We explore two parametric methods and a non-parametric method. We present these in Section 3.1, and
3.2 respectively. For a mono-lingual source in a new domain, the BLEU-PT can be computed, and then
mapped to the appropriate weight vector using the methods presented below.
3.1 Parametric Methods
We considered two distinct parametric methods for estimating the mapping from model/corpus similarity
into weight vectors. The first one makes the assumption that parameters can be estimated independently
of one another, given the similarity, whereas the second tries to leverage known covariance between
distinct parameters in the vector.
3.1.1 Linear Regression
Motivated by initial experiments highlighting strong correlation between BLEU-PT and optimal feature
weights (see Section 5.1 below), we assumed here a simple linear relation of the form:
?
?
i
= W
i
X + b
i
(3)
where ?
?
i
is the optimal log-linear weight for feature i, X is the feature vector (BLEU-PT vector), W
i
and b
i
are coefficients to be estimated. While a drastic assumption, this has the advantage of limiting
the risk of overfitting in a situation like ours where there is only relatively few datapoints to learn from.
We estimate a
i
and b
i
by simple least squares regression. Once these are available for all features, we
can predict the log linear weights of any model given its BLEU-PT similarity to a monolingual source
sample using Eq. 3.
3.1.2 Multi-Task learning
Optimal log-linear parameters might not be fully independent given BLEU-PT, especially since it is
known that model features can be highly correlated. To account for correlation between parameter
weights, we explore the use of multi-task lasso
2
(Caruana, 1997) where several functions corresponding
to each parameter are learned jointly considering the correlation between their values observed in the
training data. Multi-task lasso consists of a least square loss model trained along with a regularizer and
the objective is to minimize the following:
argmin
w
1
2N
||X ?W ? ?||
2
2
+ ?||W ||
21
where; ||W ||
21
=
M
?
j
?
?
i
w
2
ij
(4)
Here, N is the number of training samples, X is the feature vector(BLEU-PT score vector) ? is the label
vector(log linear weights). ||W ||
21
is the l
21
regularizer (Yang et al., 2011). The problem of prediction
of log linear weights is reduced to prediction of i interlinked tasks where each task has M features
3
.
Coefficients are calculated using coordinate descent algorithm in Multi-Task lasso. Once the coefficients
are calculated we use Eq. 3 to predict the log linear weights.
2
http://scikit-learn.org/
3
In our case we only have 1 feature i.e. BLEU-PT score.
1116
3.2 Non Parametric: Nearest Neighbor
Finally, instead of building a parametric predictor for log linear weights, we experimented with a simple
nearest-neighbor approach:
?
?
i
= ?
i
(M
j
?
) (5)
whereM
j
ranges over the linearly interpolated phrase tables, and ?
i
(M) returns the stored optimal value
for the i
th
log-linear weight, and:
j
?
= argmin
j
min
s
?
(|BLEU-PT(M, s)? BLEU-PT
?
(M
j
, s
?
)|) (6)
where s is the monolingual sample on which we want to calculate the BLEU-PT and s
?
ranges over
the source sides of our available parallel development sets. In other words, a BLEU-PT of a model is
calculated on the source sample to be translated and the log-linear weight is chosen which corresponds
to BLEU-PT
?
, where BLEU-PT
?
is a training data point closest to BLEU-PT. This approach is close to
the cross-domain tuning of Pecina et. al. (2012).
4 Experimental Program
We conducted a number of experiments for English-French language pair, comparing the methods pro-
posed in the previous sections among one another and against state-of-the-art baselines and oracles.
4.1 Datasets
In this section, we present the datasets (EN-FR) that we have used for our experiments and the training
data that was created for the purpose of supervised learning. We collected a set of 12 publicly available
corpora and 1 proprietary corpus, statistics of datasets are provided in Table 1.
Corpus Train Development Test
Commoncrawl 78M 12.4K 12.6K
ECB 4.7M 13.9K 14K
EMEA 13.8M 14K 15.7K
EUconst 133K 8K 8.4K
Europarl 52.8M 13.5K 13.5K
P1 5M 35K 14.5K
KDE4 1.5M 12.8K 5.8K
News Comm. 4M 12.7K 65K
OpenOffice 400K 5.4K 5.6K
OpenSubs 156M 16K 15.7K
PHP 314K 3.5K 4K
TED 2.65M 21K 14.6K
UN 1.92M 21K 21K
Table 1: Statistics of parallel sets (# of source tokens)
500 1000 1500 2000 2500 3000 3500 4000 4500Phrase Table size (MB)0
500
1000
1500
2000
2500
Search
 Time 
(secon
ds)
Calculating time of Metrics v/s Size of Phrase Tablebleu-ptcross entropy
Figure 1: BLEU-PT v/s Cross-Entropy
Commoncrawl (CC) (Smith et al., 2013) and News Commentary (Bojar et al., 2013) corpora were
provided in the 2013 shared translation task organized with workshop on machine translation. TED talks
data was released as a part of IWSLT evaluation task (Cettolo et al., 2012). ECB, EMEA, EUconst,
OpenOffice, OpenSubs 2011, PHP and UN corpora are provided as a part of OPUS parallel corpora
(Tiedemann, 2012). The parallel corpora from OPUS were randomly split into training, development
and testsets. Commoncrawl, News Commentary and TED datasets were used as they were provided in
the evaluation task.
Out of 13 different domain datasets we selected 4 datasets randomly: Commoncrawl, KDE4, TED and
UN (in bold in Table 1), to test our methods.
4.2 BLEU-PT v/s Cross-Entropy
We compared the overheads of calculating BLEU-PT and Cross-Entropy
4
. We are interested in estimat-
ing whether with increasing number of phrase tables the computation of both measures becomes slow or
memory intensive.
4
We used tmcombine.py script that comes along with the moses package to calculate the mixing coefficients.
1117
Another advantage of using BLEU-PT apart from fast retrieval is that we can index the phrase tables
using wFSA based indexing (explanation of indexing the phrase tables is not in the scope of this paper)
and store the FSTs in binarised format on disk. When a source sample comes, we just load the indexed
binaries and calculate the BLEU-PT while this cannot be achieved when we want to calculate cross
entropy because we have to do one pass over all the phrase tables in question.
Experimental results depicted in Figure 1 shows that computation of BLEU-PT is fast (160 seconds)
while computation of cross-entropy is slow (42 minutes) when we combine 12 phrase tables with total
size of 4.2GB.
4.3 Training data for supervised learning and testing
As mentioned earlier, for estimating the parameters we require a training data containing the tuples of
<BLEU-PT, log-linear-weight>. We perform parameter estimation on four of our datasets: Common-
crawl, KDE4, TED and UN. So, for obtaining evaluation results on say, UN, the rest of the resources
are used for generating the training data. Our experimental setup can be explained well using the Venn
diagram shown in Figure 2.
We set one of four domains as the test domain (in this case, UN) whose parallel set is not available to us
and call it setup-UN. The training data tuples obtained from the rest of the 12 datasets are used to estimate
parameters for the UN domain. From these 12 datasets we perform a round-robin experiment where one
by one each dataset is considered as in-domain and the rest as out-domain. In-domain dataset provides the
development set and the rest 11 out-domain models are linearly combined to build translation models.
In figure 2, for example, the development set from the TED domain is taken as the development set
of the multi-model build using the rest (i.e. excluding TED and UN). This multi-model is built by a
weighted linear combination of the out-domain models (11 models). The parameters of this multi-model
are tuned on the in-domain development set using MIRA. Simultaneously, we also calculate the BLEU-
PT of the linear interpolated model on the source side of the in-domain development set (i.e. TED).
This provides us the tuples of BLEU-PT and the log linear weights, which is our training data. So, four
sets of experiments are conducted (one each for four datasets considered for testing), and for each set
of experiments, there are 12 training data points. The final evaluation is done by measuring the BLEU
score obtained on each test set using the predicted parameter estimates.
Reiterating, our optimizing method is fast, and hence, we are not not looking to learn the parameters
apriori for all the domains based on a source side of the development set. The goal is to do a fast
adaptation by predicting the parameters using statistical models for every new test in a particular domain
even in the absence of a parallel development set.
4.4 Prediction
For prediction of parameters for a new domain, the BLEU-PT of the sample source corpus (UN in our
example) is measured with the multi-model built on all the models (all the rest of 12 datasets including
the TED model) and then the supervised predictor is applied. In our experiments, we test both parametric
and non-parametric methods to estimate the parameters based on the training data obtained using the 12
domains.
TEST
In-domain
UN
...
...
EMEA
ECB
KDE
PHP
DEV
In-domain
TED
Figure 2: Cross domain tuning setup
Figure 3: Correlation of log linear weights with BLEU-PT
when indomain sets set to UN and TED
1118
Domain Linear Interpolation
System Train Dev Param. Est. TM(coeff.) RM(coeff.) LM(coeff.)
in-dom-train In In mira N.A N.A N.A
mira-bleupt-tm-rm Out In mira 3 3 7
mira-perp-tm-bleupt-rm Out In mira 3(Perp. Min) 3 7
mira-bleupt-tm-rm-perp-lm Out In mira 3 3 3(LM Perp. Min.)
mira-bleupt-all Out In mira 3 3 3
def-bleupt-all Out 7 def 3 3 3
gen-reg-bleupt-all Out 7 regression 3 3 3
gen-mtl-bleupt-all Out 7 multi-task 3 3 3
gen-nn-bleupt-all Out 7 Near.Neigh. 3 3 3
top5-reg-bleupt-all Out 7 regression 3 3 3
top5-mtl-bleupt-all Out 7 multi-task 3 3 3
top5-nn-bleupt-all Out 7 Near.Neigh. 3 3 3
Table 2: System Description: Each system?s training domain and development set domain along with the optimizer/predictor
is mentioned. def-bleupt-all uses default weights from Moses decoder. Near.Neigh. shows that we used Nearest Neighbor
predictor for optimizing weights. 7 represent log linear interpolation of models while 3 represents linear interpolation. The
mixing coefficients for linear interpolation are calculated by normalizing bleu-pt scores unless mentioned otherwise.
5 Experiments and Results
5.1 Correlation analysis
Before embarking in the actual regression task, we examined the correlation between the similarity values
(BLEU-PT) and the various weights in the training data. If there is good correlation between BLEU-PT
and a particular parameter, then the linear regressor is expected to fit well and then predict an accurate
parameter value for a new domain. For computing the correlation, we use Pearson correlation coefficient
(PCC). Figure 3 shows the PCC between the feature weights and the BLEU-PT scores. The tm?s are the
translation model features, and rm?s are the reordering model features.
We see that there is either a strong positive correlation or a strong negative correlation for most fea-
tures in both the experimental setups shown in the figure 3. This validates our hypothesis that optimal
parameters for a new test domain can indeed be estimated with good reliability. One can also observe
that the correlation level also varies based on the mixture of training models. For example, the correla-
tion is much higher in the training data that excluded UN (setup-UN) than the one that excluded TED
(setup-TED).
In figure 3, one can also see that tm0 (forward phrase conditional probability) and tm2 (backward
phrase conditional probability) which are shown in previous work to be the two most important features
amongst all SMT features (Lopez and Resnik, 2006) in terms of their impact on translation quality, have
a high correlation in setup-UN.
5.2 Systems
All SMT systems were built using the Moses toolkit (Koehn et al., 2007). To automatically align the
parallel corpora we used MGIZA (Gao and Vogel, 2008). Aligned training data in each domain was
then used to create the corresponding component translation models and lexical reordering models. We
created 5-gram language models for every domain using SRILM (Stolcke, 2002) with improved Kneser-
Ney smoothing (Chen and Goodman, 1999) on the target side of the training parallel corpora. Log linear
weights for the systems were optimized using MIRA (Watanabe et al., 2007; Hasler et al., 2011) which
is provided in the Moses toolkit. Performance of the systems are measured in terms of BLEU computed
using the MultEval script (mteval-v13.pl).
We built one in-dom-train system where only in-domain training data is taken into account. This
system shows the importance of in-domain training data in SMT (Haddow and Koehn, 2012). Three
oracle systems are trained on out-domain training corpus and tuned on in-domain development data (in
this case there are four domains we chose to test on: UN, TED, CommonCrawl and KDE4), thus 4
systems for each of the in-domain test sets.
We build another set of SMT systems in which language models are combined by linear interpolation
5
.
5
Linear interpolation of 12 LMs result in one single large LM, thus, one weight. So, a total of 14 weights have to be
optimized or predicted
1119
The systems using linear interpolated LM (mixing coefficients are normalized BLEU-PT scores) are def-
bleupt-all, mira-bleupt-all, gen-reg-bleupt-all, gen-mtl-bleupt-all and gen-nn-bleupt-all. We compare
mira-bleupt-all with mira-bleupt-tm-rm-perp-lm where mixing coefficients for LM interpolation are cal-
culated by standard LM perplexity minimization method over target side of development set.
As mentioned earlier, ideally only a subset of all the models closer to the source sample should be
taken into account for quick adaptation, so we select the top five domains related to the source sample
and interpolate the respective models and address them as top5-* systems. Adding more domains would
unnecesary increase the size of the model and add more noise. Table 2 shows the configuration of
different systems. In the next section we compare the performances of these systems and report the
findings.
6 Results and Discussion
Table 3 presents results of the systems that use an in-domain parallel data. As expected, when an in-
domain corpus is used both for training as well as for optimizing the log-linear parameters, the pefor-
mance is much higher than those systems that do not use in-domain parallel corpus for training (Koehn
and Schroeder, 2007). We also observe that the use of normalized BLEU-PT for computing mixing
coefficients gives comparable performance to using Cross-Entropy. The primary advantage in using
BLEU-PT is that it can be compute much faster than Cross-Entropy (as shown in Figure 1). Evidently,
normalized BLEU-PT scores as mixing coefficients performs at par with mixing coefficients retrieved by
standard perplexity minimization method (Bertoldi and Federico, 2009). One can also use BLEU-PT for
LM interpolation in cases where target side in-domain text is not available.
System UN TED CC KDE
in-dom-train 67.87 29.98 26.62 35.82
mira-bleupt-tm-rm 44.14 31.20 17.43 24.25
mira-perp-tm-bleupt-rm 43.56 31.36 17.54 24.72
mira-bleupt-tm-rm-perp-lm 43.96 31.85 18.45 23.39
mira-bleupt-all 43.66 32.04 18.44 23.09
Table 3: Comparison of In-Domain system versus the estab-
lished Oracles in different setups.
System UN TED CC KDE
gen-reg-bleupt-all 43.27 32.18 17.95 21.05
gen-mtl-bleupt-all 43.35 32.61 18.26 20.67
gen-nn-bleupt-all 42.73 31.04 18.24 21.85
Table 4: Performance of generic systems (gen-*) in all se-
tups.
Table 4 illustrates the impact of phrase table retrieval on the performance of multi-model. All the
systems presented in this table use BLEU-PT for computing mixing coefficients, while the weights
are computed using the three techniques that we explored in this paper. We see that in case of re-
gression, the phrase table retrieval also results in a better MT performance. In the other two cases,
the results are comparable. It shows that retrieval helps in building smaller sized multi-models while
being more accurate on an average. Phrase table retrieval, thus, becomes particularly useful when a
multi-model needs to be built from a library of dozens of pre-trained phrase tables of various domains.
 15.5
 16
 16.5
 17
 17.5
 18
 18.5
 2  4  6  8  10  12
BLEU
 sco
re
Number of Models
Nearest NeighborRegressionMulti-Task Learning
Figure 4: BLEU scores when top k models were
used to evaluate commoncrawl test set where
k ? 1..12.
System UN TED CC KDE
def-bleupt-all 42.03 30.82 17.97 19.66
mira-bleupt-all 43.66 32.04 18.44 23.09
top5-reg-bleupt-all 43.39
N
32.31
N
18.10 21.54
N
top5-mtl-bleupt-all 43.56
N
32.60
N
18.14 20.91
N
top5-nn-bleupt-all 42.96
N
30.89
M
17.79 22.24
N
Table 5: Comparing the baseline system (def-bleupt-all)
and Oracle (mira-bleupt-all) with domain specific multi-model
systems trained on top5 domains.
N
and
M
denotes significantly
better results in comparison with def-bleupt-all system with
p-value < 0.0001 and < 0.05 respectively.
Table 5 compares our approach of computing log-linear weights (in the absence of in-domain develop-
ment set) to the state-of-art weight optimization technique MIRA (which requires an in-domain devel-
opment set). As a baseline, we set default weights to all the parameters, which was shown to a strong
1120
baseline in (Pecina et al., 2012). We see that the methods proposed by us perform significantly bet-
ter than the default weights baseline (improvement of more than 1.5 BLEU score on an average across 4
domains). Among the three approaches for computing weights, the method that uses multi-task lasso per-
forms best (except in setup-KDE where the non-parametric method performs best), along the expected
lines as multi-task lasso considers the correlation between various features. In comparison to MIRA, our
methods result in an average drop of as little as 0.5 BLEU points across 4 domains (see Table 5).
Figure 4 shows BLEU score curve when we vary the k in top-k systems. BLEU score curve is almost
tangential zero when k is between 5 and 6 which essentially means that selection of k = 5 is a good
choice. For CommonCrawl test set, the top five domains used were Europarl, OpenSubs, NewsCom-
mentary, TED and ECB. This is a significant result which indicates that one can build a good system for
a domain even in the absence of the parallel data in the domain of interest.
7 Related Work
Domain adaptation in statistical machine translation has been widely studied and leveraged through
adding more training data (Koehn and Knight, 2001), filtering of out of domain training data (Axelrod
et al., 2011; Koehn and Haddow, 2012), fillup technique (Bisazza et al., 2011), language model adap-
tation by perplexity minimization over in-domain data (Bertoldi and Federico, 2009) and various other
approaches. However, all the above adaptation approaches require either parallel in-domain corpus or
monolingual in-domain target side corpus, thus, not applicable in our scenario.
In this paper we studied mixture modelling of heterogeneous translation models which was first pro-
posed in Foster et. al. (2007). They showed various ways of computing mixing coefficients for linear
interpolation using several distance based metrics borrowed from information theory. However, to cal-
culate any such metrics it was required that one has an access to the source/target training corpus and
source/target development corpus. Other noteable works in mixture modelling in SMT are (Civera and
Juan, 2007; Razmara et al., 2012; Duan et al., 2010).
More recently, Sennrich (2012b) designed an approach to calculate mixing coefficients by minimizing
the perplexity of translation models over an aligned development set for mixture modelling via linear
interpolation or by weighting the corpora. Sennrich et. al. (2012a) clustered of a large heterogeneous
development corpus and tuned a translation system on different clusters. In the decoding phase each
sentence was assigned to a cluster and the translation system tuned on that cluster was used to translate
that sentence.
(Banerjee et al., 2010) build several domain specific translation systems, and trained a classifier to
assign each incoming sentence to a domain and use the domain specific system to translate the corre-
sponding sentence. They assume that each sentence in test set belongs to one of the already existing
domains which means it would fail in the case where the sentence doesn?t belong to any of the existing
domains. In our case we do not make any such assumptions.
Academically, above approaches are well suited for solving the problem of domain adaptation, but
during the deployment of SMT systems in industrial scenario where the client is unable to deliver the
parallel in-domain data these approaches fail to provide a quick solution.
8 Conclusion
We present an approach to multi-model domain adaptation in a particularly challenging setting where
there is no parallel in-domain data. Parameter estimation without in-domain development set is a problem
that, to the best of our knowledge, has not been addressed before. We designed a method for tuning model
parameters without parallel development set and validated it through an experimental program for which
we compared performances against an array of Oracles and Baselines. The effectiveness of the proposed
method empirically supports the findings of (Pecina et al., 2012), who discovered that the log linear
weights largely depend on the distance of training domain from the domain on which the models are
being optimized on. As a side result, we designed in the process a novel similarity metric between a
phrase table and a source sample and implemented it effectively using wFSAs. We empirically showed
the excellent computation speed of BLEU-PT scores as compared to standard Cross-Entropy measure
using standard toolkits.
1121
Acknowledgement
The authors thank the three anonymous reviewers for their comments and suggestions.
References
Amittai Axelrod, Xiaodong He, and Jianfeng Gao. 2011. Domain adaptation via pseudo in-domain data selection.
In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ?11, pages
355?362, Stroudsburg, PA, USA. Association for Computational Linguistics.
Pratyush Banerjee, Jinhua Du, Baoli Li, Sudip Kumar Naskar, Andy Way, and Josef Van Genabith. 2010. Com-
bining multi-domain statistical machine translation models using automatic classifiers. In Proceedings of 9th
Conference of the Association for Machine Translation in the Americas.
Nicola Bertoldi and Marcello Federico. 2009. Domain adaptation for statistical machine translation with monolin-
gual resources. In Proceedings of the Fourth Workshop on Statistical Machine Translation, StatMT ?09, pages
182?189, Stroudsburg, PA, USA. Association for Computational Linguistics.
Arianna Bisazza, Nick Ruiz, and Marcello Federico. 2011. Fill-up versus Interpolation Methods for Phrase-based
SMT Adaptation. In International Workshop on Spoken Language Translation (IWSLT), pages 136?143, San
Francisco, CA.
Ond?rej Bojar, Christian Buck, Chris Callison-Burch, Christian Federmann, Barry Haddow, Philipp Koehn, Christof
Monz, Matt Post, Radu Soricut, and Lucia Specia. 2013. Findings of the 2013 Workshop on Statistical Machine
Translation. In Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 1?44, Sofia,
Bulgaria, August. Association for Computational Linguistics.
Nicola Cancedda. 2012. Private access to phrase tables for statistical machine translation. In ACL (2), pages
23?27.
Rich Caruana. 1997. Multitask learning. Mach. Learn., 28(1):41?75, July.
Mauro Cettolo, Christian Girardi, and Marcello Federico. 2012. Wit
3
: Web inventory of transcribed and translated
talks. In Proceedings of the 16
th
Conference of the European Association for Machine Translation (EAMT),
pages 261?268, Trento, Italy, May.
Stanley F. Chen and Joshua Goodman. 1999. An empirical study of smoothing techniques for language modeling.
Computer Speech and Language, 4(13):359?393.
Jorge Civera and Alfons Juan. 2007. Domain adaptation in statistical machine translation with mixture modelling.
In Proceedings of the Second Workshop on Statistical Machine Translation, pages 177?180, Prague, Czech
Republic, June. Association for Computational Linguistics.
Nan Duan, Mu Li, Dongdong Zhang, and Ming Zhou. 2010. Mixture model-based minimum bayes risk de-
coding using multiple machine translation systems. In Proceedings of the 23rd International Conference on
Computational Linguistics, pages 313?321. Association for Computational Linguistics.
George Foster and Roland Kuhn. 2007. Mixture-model adaptation for smt. In Proceedings of the Second Work-
shop on Statistical Machine Translation, StatMT ?07, pages 128?135, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Qin Gao and Stephan Vogel. 2008. Parallel implementations of word alignment tool. In Software Engineering,
Testing, and Quality Assurance for Natural Language Processing, SETQA-NLP ?08, pages 49?57, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Barry Haddow and Philipp Koehn. 2012. Analysing the effect of out-of-domain data on smt systems. In Pro-
ceedings of the Seventh Workshop on Statistical Machine Translation, pages 422?432, Montreal, Canada, June.
Association for Computational Linguistics.
Eva Hasler, Barry Haddow, and Philipp Koehn. 2011. Margin Infused Relaxed Algorithm for Moses. The Prague
Bulletin of Mathematical Linguistics, 96:69?78.
Philipp Koehn and Barry Haddow. 2012. Towards effective use of training data in statistical machine transla-
tion. In Proceedings of the Seventh Workshop on Statistical Machine Translation, WMT ?12, pages 317?321,
Stroudsburg, PA, USA. Association for Computational Linguistics.
1122
Philipp Koehn and Kevin Knight. 2001. Knowledge sources for word-level translation models. In In Proceedings
of the 2001 Conference on Empirical Methods in Natural Language Processing, pages 27?35.
Philipp Koehn and Josh Schroeder. 2007. Experiments in domain adaptation for statistical machine translation. In
Proceedings of the Second Workshop on Statistical Machine Translation, StatMT ?07, pages 224?227, Strouds-
burg, PA, USA. Association for Computational Linguistics.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran,
R. Zens, C. Dyer, O. Bojar, A. Constantin, and E. Herbst. 2007. Moses: Open Source Toolkit for Statisti-
cal Machine Translation. In Proceedings of the 45th Annual Meeting of the Association for Computational
Linguistics Companion Volume Proceedings of the Demo and Poster Sessions, pages 177?180, Prague, Czech
Republic.
Adam Lopez and Philipp Resnik. 2006. Word-based alignment, phrase-based translation: What?s the link? In In
Proceedings of AMTA, pages 90?99.
Franz Josef Och. 2003. Minimum Error Rate Training in Statistical Machine Translation. In Erhard Hinrichs and
Dan Roth, editors, Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,
pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-jing Zhu. 2002. BLEU : a Method for Automatic Evalua-
tion of Machine Translation. In Computational Linguistics, volume pages, pages 311?318.
Pavel Pecina, Antonio Toral, and Josef van Genabith. 2012. Simple and effective parameter tuning for domain
adaptation of statistical machine translation. In COLING, pages 2209?2224.
Majid Razmara, George Foster, Baskaran Sankaran, and Anoop Sarkar. 2012. Mixing multiple translation models
in statistical machine translation. In Proceedings of the 50th Annual Meeting of the Association for Computa-
tional Linguistics: Long Papers-Volume 1, pages 940?949. Association for Computational Linguistics.
Rico Sennrich, Holger Schwenk, and Walid Aransa. 2013. A multi-domain translation model framework for
statistical machine translation. In Proceedings of the 51st Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 832?840, Sofia, Bulgaria, August. Association for Computational
Linguistics.
Rico Sennrich. 2012a. Mixture-modeling with unsupervised clusters for domain adaptation in statistical machine
translation. In Proceedings of the 16th Annual Conference of the European Association of Machine Translation
(EAMT).
Rico Sennrich. 2012b. Perplexity minimization for translation model domain adaptation in statistical machine
translation. In Proceedings of the 13th Conference of the European Chapter of the Association for Computa-
tional Linguistics, EACL ?12, pages 539?549, Stroudsburg, PA, USA. Association for Computational Linguis-
tics.
Jason R. Smith, Herve Saint-Amand, Magdalena Plamada, Philipp Koehn, Chris Callison-Burch, and Adam Lopez.
2013. Dirt cheap web-scale parallel text from the common crawl. In Proceedings of the 51st Annual Meeting
of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1374?1383, Sofia, Bulgaria,
August. Association for Computational Linguistics.
Andreas Stolcke. 2002. Srilm - an extensible language modeling toolkit. In Proceedings of ICSLP, Denver,
Colorado.
J?org Tiedemann. 2012. Parallel data, tools and interfaces in opus. In Nicoletta Calzolari (Conference Chair),
Khalid Choukri, Thierry Declerck, Mehmet Uur Doan, Bente Maegaard, Joseph Mariani, Jan Odijk, and Stelios
Piperidis, editors, Proceedings of the Eight International Conference on Language Resources and Evaluation
(LREC?12), Istanbul, Turkey, may. European Language Resources Association (ELRA).
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki Isozaki. 2007. Online large-margin training for statistical
machine translation. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language
Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 764?773, Prague, Czech
Republic, June. Association for Computational Linguistics.
Yi Yang, Heng Tao Shen, Zhigang Ma, Zi Huang, and Xiaofang Zhou. 2011. l 2, 1-norm regularized discriminative
feature selection for unsupervised learning. In Proceedings of the Twenty-Second international joint conference
on Artificial Intelligence-Volume Volume Two, pages 1589?1594. AAAI Press.
1123
Proceedings of the NAACL HLT Workshop on Extracting and Using Constructions in Computational Linguistics, pages 32?38,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Syntactic Construct : An Aid for translating English Nominal Compound
into Hindi
Soma Paul
IIIT Hyderabad
soma@iiit.ac.in
Prashant Mathur
IIIT Hyderabad
mathur@research.iiit.ac.in
Sushant Kishore
IIIT Hyderabad
susanta@research.iiit.ac.in
Abstract
This paper illustrates a way of using para-
phrasal interpretation of English nominal
compound for translating them into Hindi.
Input Nominal compound is first para-
phrased automatically with the 8 preposi-
tions as proposed by Lauer (1995) for the
task. English prepositions have one-to-one
mapping to post-position in Hindi. The
English paraphrases are then translated
into Hindi using the mapping schema. We
have got an accuracy of 71% over a set of
gold data of 250 Nominal Compound. The
translation-strategy is motivated by the
following observation: It is only 50% of
the cases that English nominal compound
is translated into nominal compound in
Hindi. In other cases, they are translated
into varied syntactic constructs. Among
them the most frequent construction type
is ?Modifier + Postposition + Head?. The
translation module also attempts to deter-
mine when a compound is translated using
paraphrase and when it is translated into a
Nominal compound.
1 Introduction
Nominal Compounds are syntactically condensed
constructs which have extensively been attempted
to expand in order to unfold the meaning of the
constructions. Currently there exist two different
approaches in Computational Linguistics: (a) La-
beling the semantics of compound with a set of
abstract relations (Moldovan and Girju, 2003) (b)
Paraphrasing the compound in terms of syntac-
tic constructs. Paraphrasing, again, is done in
three ways: (1) with prepositions (?war story?
? ?story about war?) (Lauer 1995) (2) with
verb+preposition nexus (?war story? ? ?story
pertaining to war?, ?noise pollution? ? ?pol-
lution caused by noise?) (Finin 1980) (3) with
Copula (?tuna fish? ? ?fish that is tuna?) (Van-
derwende,1995). Nominal compound (henceforth
NC) is a frequently occurring construct in En-
glish1. A bigram or two word nominal compound,
is a construct of two nouns, the rightmost noun
being the head (H) and the preceding noun the
modifier (M) as found in ?cow milk?, ?road condi-
tion?, ?machine translation? and so on. Rackow et
al. (1992) has rightly observed that the two main
issues in translating the source language NC cor-
rectly into the target language involves (a) correct-
ness in the choice of the appropriate target lex-
eme during lexical substitution and (b) correctness
in the selection of the right target construct type.
The issue stated in (a) involves correct selection of
sense of the component words of NCs followed by
substitution of source language word with that of
target language that best fits for the selected sense
(see Mathur and Paul 2009).
From the perspective of machine translation, the
issue of selecting the right construct of target lan-
guage becomes very significant because English
NCs are translated into varied construct types in
Hindi. This paper motivates the advantage of ex-
panding English nominal compounds into ?para-
phrases with prepositions? for translating them
into Hindi. The English NCs are paraphrased us-
ing Lauer?s (1995) 8 prepositions. In many cases
prepositions are semantically overloaded. For ex-
ample, the NC ?Hindu law? can be paraphrased
as ?law of Hindu?. This paraphrase can mean
?Law made by Hindu? (not for Hindu people alone
though) or ?Law meant for Hindu? (law can be
made by anyone, not by the Hindus necessarily).
Such resolution of meaning is not possible from
?preposition paraphrase?. The paper argues that
this is not an issue from the point of view of trans-
1Kim and Baldwin (2005) reports that the BNC corpus (84
million words: Burnard (2000)) has 2.6% and the Reuters has
(108M words: Rose et al (2002)) 3.9% of bigram nominal
compound.
32
lation at least. It is because the Hindi correspon-
dent of ?of?, which is ?kA?, is equally ambiguous.
The translation of ?Hindu law? is ?hinduoM kA
kAnUn? and the construction can have both afore-
mentioned interpretations. Human users can se-
lect the right interpretation in the given context.
On the other hand, ?paraphrase with preposition?
approach has the following advantages: (a) An-
notation is simpler; (b) Learning is easier and (c)
Data sparseness is less; (d) Most importantly, En-
glish prepositions have one to one Hindi postpo-
sition correspondents most of the times. There-
fore we have chosen the strategy of ?paraphrasing
with prepositions? over other kind of paraphrasal
approach for the task of translation. The pa-
per explores the possibility of maintaining one to
one correspondence of English-Hindi preposition-
postpositions and examines the accuracy of trans-
lation. At this point it is worth mentioning that
translation of English NC as NC as well as differ-
ent syntactic constructs in Hindi is almost equal.
Therefore the task of translating English NCs into
Hindi is divided into two levels: (1) Paraphrases
for an NC are searched in the web corpus, (2) An
algorithm is devised to determine when the para-
phrase is to be ignored and the source language
NC to be translated as NC or transliterated in NC,
and (3) English preposition is replaced by Hindi
corresponding postposition. We have compared
our result with that of google translation system
on 250 that has been manually created.
The next section describes the data in some de-
tail. In section 3, we review earlier works that have
followed similar approaches as the present work.
Our approach is described in section 4. Finally the
result and analysis is discussed in section 5.
2 Data
We made a preliminary study of NCs in English-
Hindi parallel corpora in order to identify the dis-
tribution of various construct types in Hindi which
English NCs are aligned to. We took a paral-
lel corpora of around 50,000 sentences in which
we got 9246 sentences (i.e. 21% cases of the
whole corpus) that have nominal compounds. We
have found that English nominal compound can be
translated into Hindi in the following varied ways:
1. As Nominal Compound
?Hindu texts? ? hindU shAstroM
?milk production? ? dugdha utpAdana
2. M + Postposition + H Construction
?rice husk?? cAvala kI bhUsI,
?room temperature?? kamare ke tApa-
mAna
?wax work?? mom par citroM
?work on wax?
?body pain?? sharIra meM darda
?pain in body?
English NCs are frequently translated into
genitive2 construct in Hindi. In English ?of?
is heavily overloaded(very ambiguous), so
the genitives are in Hindi. The two other
postpositions that we see in the above data
are par ?on? and meM ?in/at? and they refer
to location.
3. As Adjective Noun Construction
?nature cure? ? prAkritika cikitsA
?hill camel? ? pahARI UMta
The words prAkrtik and pahARI being ad-
jectives derived from prakriti and pAhAR re-
spectively.
4. Single Word
?cow dung? ? gobara
The distribution of various translations is given
below:
Construction Type No. of Occurrence
Nominal Compound 3959
Genitive(of-kA/ke/kI) 1976
Purpose (for-ke liye) 22
Location (at/on-par) 34
Location (in-meM) 93
Adjective Noun Phrase 557
Single Word 766
Transliterated NC 1208
Table 1: Distribution of translations of English NC
from English Hindi parallel corpora.
There are 8% cases (see table 1) when an En-
glish NC becomes a single word form in Hindi.
For rest of the cases, they either remain as NC
(translated 43% or transliterated 13%) or corre-
spond to syntactic construct. When NC is trans-
lated as NC, they are mostly technical terms
2?of? corresponds to ?kA/ke/kI?, which are genitive
markers in Hindi.
33
or proper names. Our data shows that there are
around 40% cases when English NC is translated
as various kinds of syntactic constructs such M
+ Postposition + H, Adj + H or longer para-
phrases (?Hand luggage? ? hAth meM le jAne
vAle sAmAn ?luggage to be carried by hand?). Out
of these data, 70% cases are when English NC is
translated into M3 + postposition + H. Thus the
translation of NC into postpositional construction
is very common in Hindi.
For preparation of test data, we extracted nomi-
nal compound from BNC corpus (Burnard et al,
1995). BNC has varied amount of text ranging
from newspaper article to letters, books etc. We
extracted a sample of noun-noun bigrams from the
corpus and manually translated them into Hindi.
In this paper, we propose an algorithm that de-
termines when the syntactic paraphrase of English
NC is to be considered for translation and when it
is left for direct lexical substitution in Hindi.
3 Related Works
There exists no work which has attempted the
approach that we will be discussing here for
translating English NC into Hindi. From that per-
spective, the proposed approach is first of its kind
to be attempted. However, paraphrasing English
NCs is a widely studied issue. Scholars (Levi
1978; Finin 1980) agree there is a limited number
of relations that occur with high frequency in
noun compounds. However, the number and
the level of abstraction of these frequently used
semantic categories are not agreed upon. They
can vary from a few prepositional paraphrases
(Lauer, 1995) to hundreds and even thousands
more specific semantic relations (Finin, 1980).
Lauer (1995), for example, considers eight prepo-
sitional paraphrases as semantic classification
categories: of, for, with, in, on, at, about, and
from. According to this classification, the noun
compound ?bird sanctuary?, for instance, can
be classified both as ?sanctuary of bird? and
?sanctuary for bird?.
The automatic interpretation of noun compounds
is a difficult task for both unsupervised and super-
vised approaches. Currently, the best-performing
NC interpretation methods in computational lin-
guistics focus only on two-word noun compounds
and rely either on rather ad-hoc, domain-specific,
hand-coded semantic taxonomies, or on statistical
3M: Modifier, H: Head
models on large collections of unlabeled data.
The majority of corpus based statistical ap-
proaches to noun compound interpretation
collects statistics on the occurrence frequency
of the noun constituents and uses them in a
probabilistic model (Resnik, 1993; Lauer, 1995;
Lapata and Keller, 2004). Lauer (1995) was the
first to devise and test an unsupervised probabilis-
tic model for noun compound interpretation on
Grolier encyclopedia, an 8 million word corpus,
based on a set of 8 prepositional paraphrases. His
probabilistic model computes the probability of a
preposition p given a noun-noun pair n1-n2 and
finds the most likely prepositional paraphrase
p? = argmaxP (p|n1, n2) (1)
However, as Lauer noticed, this model requires
a very large training corpus to estimate these
probabilities. Lapata and Keller (2004) showed
that simple unsupervised models applied to the
noun compound interpretation task perform sig-
nificantly better when the n-gram frequencies are
obtained from the web (accuracy of 55.71% on Al-
tavista), rather than from a large standard corpus.
Our approach also uses web as a corpus and exam-
ines frequency of various preposition paraphrases
of a given NC. The next section describes our ap-
proach.
4 Approach
This section describes our procedure in details.
The system is comprised of the following stages:
(a) Web search of prepositional paraphrase for En-
glish NC; (b) mapping the English preposition to
corresponding Hindi postposition; (c) Evaluation
of correct paraphrasing on English side as well as
evaluation of translation.
4.1 Paraphrase Selection for Translation
Based on the observation from English-Hindi par-
allel corpus data that we examined as part of this
project, we have designed an algorithm to deter-
mine whether an English NC is to be translated
as an analytic construct or retained as an NC in
Hindi. We used Yahoo search engine to perform
a simple frequency search for ?M Preposition H?
in web corpus for a given input NC. For example,
the paraphrases obtained for the NC ?finance min-
ister? is given in table 2 and frequency of various
paraphrases is shown in the second column:
34
Paraphrase Web Frequency
minister about finance 2
minister from finance 16
minister on finance 34300
minister for finance 1370000
minister with finance 43
minister by finance 20
minister to finance 508
minister in finance 335
minister at finance 64
minister of finance 5420000
Table 2: Frequency of Paraphrases for ?finance
minister? after Web search.
In the table we notice that the distribution is
widely varied. For some paraphrase the count is
very low (minister about finance) while the high-
est count is 5420000 for ?minister of finance?. The
wide distribution is apparent even when the range
is not that high as shown in following table:
Paraphrase Web Frequency
agencies about welfare 1
agencies from welfare 16
agencies on welfare 64
agencies for welfare 707
agencies with welfare 34
agencies in welfare 299
agencies at welfare 0
agencies of welfare 92
Table 3: Frequency of Paraphrases for ?welfare
agencies? after Web search.
During our experiment we have come across
three typical cases: (a) No paraphrase is avail-
able when searched; (b) Frequency counts of some
paraphrases for a given NC is very low and (c) Fre-
quency of a number of paraphrases cross a thresh-
old limit. The threshold is set to be mean of all
the frequencies of paraphrases. Each of such cases
signifies something about the data and we build
our translation heuristics based on these observa-
tions. When no paraphrase is found in web corpus
for a given NC, we consider such NCs very close-
knit constructions and translate them as nominal
compound in Hindi. This generally happens when
the NC is a proper noun or a technical term. Sim-
ilarly when there exists a number of paraphrases
each of those crossing the threshold limit, it indi-
cates that the noun components of such NCs can
occur in various contexts and we select the first
3 paraphrase as probable paraphrase of NCs. For
example, the threshold value for the NC finance
minister is: Threshold = 6825288/8 = 853161.
The two paraphrases considered as probable para-
phrase of this NC is are therefore ?minister of fi-
nance? and ?minister for finance?. The remain-
ing are ignored. When count of a paraphrase is
less than the threshold, they are removed from the
data. We presume that such low frequency does
not convey any significance of paraphrase. On the
contrary, they add to the noise for probability dis-
tribution. For example, all paraphrases of ?ante-
lope species? except ?species of antelope? is very
low as shown in Table 4. They are not therefore
considered as probable paraphrases.
Paraphrase Web Frequency
species about antelope 0
species from antelope 44
species on antelope 98
species for antelope 8
species with antelope 10
species in antelope 9
species at antelope 8
species of antelope 60600
Table 4: Frequency of Paraphrases for antelope
species after Web search.
4.2 Mapping English Preposition to Hindi
Post-position
The strategy of mapping English preposition to
one Hindi post-position is a crucial one for the
present task of translation. The decision is mainly
motivated by a preliminary study of aligned paral-
lel corpora of English and Hindi in which we have
come across the distribution of Lauer?s 8 preposi-
tions as shown in table 5.
The table (Table 5) shows that English preposi-
tions are mostly translated into one Hindi postpo-
sition except for a few cases such as ?at?, ?with?
and ?for?. The probability of ?on? getting trans-
lating into ?ko? and ?of? into ?se? is very less
and therefore we are ignoring them in our map-
ping schema. The preposition ?at? can be trans-
lated into ?meM? and ?para? and both postposi-
tions in Hindi can refer to ?location?. However,
the two prepositions ?with? and ?for? can be trans-
lated into two distinct relations as shown in Ta-
ble 5. From our parallel corpus data, we therefore
35
Prep Post-Pos Sense Prob.
of
kA Possession 0.13
ke Possession 0.574
kI Possession 0.29
se Possession 0.002
from se Source .999
at
meM Location 0.748
par Location .219
with
se Instrument 0.628
ke sAtha Association 0.26
on
par Loc./Theme 0.987
ko Theme 0.007
about ke bAre meM Subj.Matter 0.68
in meM Location .999
for
ke lie Beneficiary 0.72
ke Possession 0.27
Table 5: Mapping of English Preposition to Hindi
postposition from alligned English-Hindi parallel
corpora.
find that these prepositions are semantically over-
loaded from Hindi language perspective. The right
sense and thereafter the right Hindi correspondent
can be selected in the context. In the present task,
we are selecting the mapping with higher prob-
ability. English Prepositions are mapped to one
Hindi Post-position for all cases except for ?at?
and ?about?.
Preposition Postposition
of kA/kI/ke
on para
for ke liye
at para/meM
in meM
from se
with ke sAtha
about
ke bAre meM
ke viSaya meM
ke sambaMdhi
Table 6: Preposition-Postposition Mapping
Post-positions in Hindi can be multi-word as in
?ke bAre meM?, ?ke liye? and so on. In the present
paper we are translating the English preposition to
the mostly probable postposition of Hindi. That
does not mean that the preposition cannot be trans-
lated into any other postposition. However, we are
taking the aforementioned stand as an preliminary
experiment and further refinement in terms of se-
lection of postposition will be done as future work.
For the present study, lexical substitution of head
noun and modifier noun are presumed to be cor-
rect.
5 Result and Analysis
In this section we will describe results of two
steps that are involved in our work: (a) Selection
of English preposition paraphrase for a given En-
glish NC; (b) Translation of English Preposition to
Hindi Post-position.
For a given NC we used a brute force method
to find the paraphrase structure. We used Lauer?s
prepositions (of, in, about, for, with, at, on, from,
to, by) for prepositional paraphrasing. Web search
is done on all paraphrases and frequency counts
are retrieved. Mean frequency (F) is calculated us-
ing all frequencies retrieved. All those paraphrases
that give frequency more than F are selected. We
first tested the algorithm on 250 test data of our
selection. The result of the top three paraphrases
are given below :
Selection Technique Precision
Top 1 61.6%
Top 2 67.20%
Top 3 71.6%
Table 7: Paraphrasing Accuracy
We have also tested the algorithm on Lauer?s
test data (first 218 compounds out 400 of NCs)
and got the following results (Table 8). Each of
the test data was marked with a preposition which
best explained the relationship between two noun
components. Lauer gives X for compounds which
cannot be paraphrased by using prepositions For
eg. tuna fish.
Prep OLauer OCI Percentage
Of 54 37 68.50%
For 42 20 47.62%
In 24 9 37.50%
On 6 2 33.33%
Table 8: Distribution of Preposition on Lauer test
data of 218 NC
OLauer : Number of occurrence of each prepo-
sition in Lauer test data
36
OCI : Number of correctly identified preposition
by our method
In Table 9 we compare our result with that of
Lauer?s on his data. We gave the results with cri-
teria: 1) only ?N prep N? is considered. 2) Non-
Prepositions (X) are also considered.
Case Our Method Lauer?s
N-prep-N 43.67% 39.87%
All 42.2% 28.8%
Table 9: Comparison of our approach with Lauer?s
Approach
Now that we have paraphrased NCs, we attempt
to translate the output into Hindi. We assume that
we have the right lexical substitution. In this pa-
per we have checked for the accuracy of the right
Hindi construction selection.
For a given NC we got the paraphrase as ?H
prep M? or ?MH?. We use English preposition
mapping as described in section 4.2 for translat-
ing NC in Hindi. For MH type compounds di-
rect lexical substitution is tried out. We tested our
approach on the gold data of 250 Nominal Com-
pounds. We translate the same 250 NCs using
google translation system in order to set up a base-
line. Google Translator could translate the data
with 68.8% accuracy.
Google returns only one translation which we
evaluated against our test data. In our case, we
have taken 3 top paraphrases as described in sec-
tion 4.1 and translated them into Hindi by using
the English preposition to Hindi postposition map-
ping schema. The following table presents the
accuracy of the translation of the top three para-
phrases
Case Precision
Top 1 61.6%
Top 2 68.4%
Top 3 70.8%
Table 10: Translation Accuracy
In this work we have not considered the context
of English NC while translating them into Hindi.
Table 11 gives the accuracy of each post-position
as translated from English preposition.
The other prepositions have occurred very less
in number and therefore not given in the table.
Preposition Post Position Accuracy
Of kA/ke/kI 94.3%
For ke liye 72.2%
In meM 42.9%
Table 11: Translation Accuracy for some individ-
ual prepositions
6 Conclusion and Future Work
This paper describes a preliminary approach for
translating English nominal compound into Hindi
using paraphrasing as a method of analysis of
source data. The result of translation is encourag-
ing as a first step towards this kind of work. This
work finds out a useful application for the task
of paraphrasing nominal compound using prepo-
sition. The next step of experiment includes the
following tasks: (a) Designing the test data in such
a way that all correspondents get equal represen-
tation in the data. (b) To examine if there are any
other prepositions (besides Lauer?s 8 preposition)
which can be used for paraphrasing (c) To use con-
text for translation.
References
Gildea. D. and Jurafsky. D. 2002. Automatic labeling
of semantic roles, Computational Linguistics 28 (3),
245-288.
Lapata, M. and Keller, F. 2004. The Web as a baseline:
evaluating the performance of unsupervised Web-
based models for a range of NLP tasks. In: Pro-
ceedings of the Human Language Technology con-
ference (HLT/NAACL), Boston, MA, pp. 121-128.
Lauer, M. 1995 Designing statistical language learn-
ers: experiments on noun compounds, Ph.D. Thesis,
Macquarie University, Australia
Moldovan, D. and Girju, R. 2003 Knowledge discov-
ery from text In: The Tutorial Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics (ACL), Sapporo, Japan.
Mathur, P. and Paul, S. 2009 Automatic Translation of
Nominal Compound into Hindi. In: Proceedings of
International Conference on Natural Language Pro-
cessing (ICON), Hyderabad
Moldovan, D., Girju, R., Tatu, M., and Antohe, D.
2005 On the semantics of noun compounds Com-
puter Speech & Language 19(4): 479-496
Girju, R. 2009 The Syntax and Semantics of Preposi-
tions in the Task of Automatic Interpretation of Nom-
inal Phrases and Compounds: A Cross-Linguistic
Study Computational Linguistics 35(2): 185-228
37
Vanderwende, L. 1995 The analysis of noun sequences
using semantic information extracted from on-line
dictionaries Ph.D. Dissertation, Georgetown Uni-
versity.
Barker, K. and Szpakowicz, S. 1998 Semi-automatic
recognition of noun modifier relationships In Proc.
of the 36th Annual Meeting of the ACL and 17th In-
ternational Conference on Computational Linguis-
tics (COLING/ACL-98) , pages 96-102, Montreal,
Canada.
Finin, T.W. 1980 The semantic interpretation of nom-
inal compounds In Proc. of the 1st Conference on
Artificial Intelligence (AAAI-80), 1980.
Isabelle, P. 1984 Another look at nominal com-
pounds In Proc. of the 10th International Confer-
ence on Computational Linguistics (COLING ?84),
Stanford, USA, 1984.
Kim, S.N. and Baldwin, T. 2005 Automatic Interpreta-
tion of Noun Compounds Using WordNet Similarity
IJCNLP 2005:945-956
Rackow, U., Dagan, I. and Schwall, U. 1992 Auto-
matic translation of noun compounds In Proc. of
the 14th International Conference on Computational
Linguistics (COLING ?92), Nantes, Frances, 1992
38
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 372?378,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
The Uppsala-FBK systems at WMT 2011
Christian Hardmeier
Jo?rg Tiedemann
Uppsala universitet
Inst. fo?r lingvistik och filologi
Uppsala, Sweden
first.last@lingfil.uu.se
Markus Saers
Human Language
Technology Center
Hong Kong Univ. of
Science & Technology
masaers@cs.ust.hk
Marcello Federico
Mathur Prashant
Fondazione Bruno Kessler
Human Language Technologies
Trento, Italy
lastname@fbk.eu
Abstract
This paper presents our submissions to the
shared translation task at WMT 2011. We
created two largely independent systems
for English-to-French and Haitian Creole-to-
English translation to evaluate different fea-
tures and components from our ongoing re-
search on these language pairs. Key features
of our systems include anaphora resolution,
hierarchical lexical reordering, data selection
for language modelling, linear transduction
grammars for word alignment and syntax-
based decoding with monolingual dependency
information.
1 English to French
Our submission to the English-French task was a
phrase-based Statistical Machine Translation based
on the Moses decoder (Koehn et al, 2007). Phrase
tables were separately trained on Europarl, news
commentary and UN data and then linearly inter-
polated with uniform weights. For language mod-
elling, we used 5-gram models trained with the
IRSTLM toolkit (Federico et al, 2008) on the mono-
lingual News corpus and parts of the English-French
109 corpus. More unusual features of our system
included a special component to handle pronomi-
nal anaphora and the hierarchical lexical reordering
model by Galley and Manning (2008). Selected fea-
tures of our system will be discussed in depth in the
following sections.
1.1 Handling pronominal anaphora
Pronominal anaphora is the use of pronominal ex-
pressions to refer to ?something previously men-
tioned in the discourse? (Strube, 2006). It is a very
common phenomenon found in almost all kinds of
texts. Anaphora can be local to a sentence, or it can
cross sentence boundaries. Standard SMT methods
do not handle this phenomenon in a satisfactory way
at present: For sentence-internal anaphora, they de-
pend on the n-gram language model with its lim-
ited history, while cross-sentence anaphora is left
to chance. We therefore added a word-dependency
model (Hardmeier and Federico, 2010) to our sys-
tem to handle anaphora explicitly.
Our processing of anaphoric pronouns follows
the procedure outlined by Hardmeier and Federico
(2010). We use the open-source coreference resolu-
tion system BART (Broscheit et al, 2010) to link
pronouns to their antecedents in the text. Coref-
erence links are handled differently depending on
whether or not they cross sentence boundaries. If
a coreference link points to a previous sentence, we
process the sentence containing the antecedent with
the SMT system and look up the translation of the
antecedent in the translated output. If the corefer-
ence link is sentence-internal, the translation lookup
is done dynamically by the decoder during search.
In either case, the word-dependency model adds a
feature function to the decoder score representing
the probability of a particular pronoun choice given
the translation of the antecedent.
In our English-French system, this model was
only applied to the inanimate pronouns it and they,
which seemed to be the most promising candidates
for improvement since their French equivalents re-
quire gender marking. It was trained on data au-
tomatically annotated for anaphora taken from the
news-commentary corpus, and the vocabulary of the
predicted pronouns was limited to words recognised
as pronouns by the POS tagger.
372
1.2 Hierarchical lexical reordering
The basic word order model of SMT penalises any
divergence between the order of the words in the in-
put sentence and the order of their translation equiv-
alents in the MT output. All reordering must thus be
driven by the language model when no other reorder-
ing model is present. Lexical reordering models
making certain word order choices in the MT out-
put conditional on the identity of the words involved
have been a standard component in SMT for some
years. The lexical reordering model usually em-
ployed in the Moses decoder was implemented by
Koehn et al (2005). Adopting the perspective of the
SMT decoder, which produces the target sentence
from left to right while covering source phrases in
free order, the model distinguishes between three or-
dering classes, monotone, swap and discontinuous,
depending on whether the source phrases giving rise
to the two last target phrases emitted were adjacent
in the same order, adjacent in swapped order or sep-
arated by other source words. Probabilities for each
ordering class given source and target phrase are
estimated from a word-aligned training corpus and
integrated into MT decoding as extra feature func-
tions.
In our submission, we used the hierarchical lexi-
cal reordering model proposed by Galley and Man-
ning (2008) and recently implemented in the Moses
decoder.1 This model uses the same approach of
classifying movements as monotone, swap or dis-
continuous, but unlike the phrase-based model, it
does not require the source language phrases to be
strictly adjacent in order to be counted as monotone
or swap. Instead, a phrase can be recognised as ad-
jacent to, or swapped with, a contiguous block of
source words that has been segmented into multi-
ple phrases. Contiguous phrase blocks are recog-
nised by the decoder with a shift-reduce parsing al-
gorithm. As a result, fewer jumps are labelled with
the uninformative discontinuous class.
1.3 Data selection from the WMT Giga corpus
One of the supplied language resources for this eval-
uation is the French-English WMT Giga corpus,
1The hierarchical lexical reordering model was imple-
mented in Moses during MT Marathon 2010 by Christian Hard-
meier, Gabriele Musillo, Nadi Tomeh, Ankit Srivastava, Sara
Stymne and Marcello Federico.
 60 80 100 120 140 160 180 200 220 240 260 280
 100  150  200  250  300  350  400 60 80 100 120 140 160 180 200 220 240 260 280LM Perplexity LM size (million 5-grams)Data Selection ThresholdThreshold vs PerplexityThreshold vs LM Size
Figure 1: Perplexity and size of language models trained
on data of the WMT Giga corpus that were selected using
different perplexity thresholds.
aka 109 corpus, a large collection of parallel sen-
tences crawled from Canadian and European Union
sources. While this corpus was too large to be used
for model training with the means at our disposal,
we exploited it as a source of parallel data for trans-
lation model training as well as monolingual French
data for the language model by filtering it down to a
manageable size. In order to extract sentences close
to the news translation task, we applied a simple
data selection procedure based on perplexity. Sen-
tence pairs were selected from the WMT Giga cor-
pus if the perplexity of their French part with respect
to a language model (LM) trained on French news
data was below a given threshold. The rationale is
that text sentences which are better predictable by
the LM should be closer to the news domain. The
threshold was set in a way to capture enough novel
n-grams, from one side, but also to avoid adding too
many irrelevant n-grams. It was tuned by training
a 5-gram LM on the selected data and checking its
size and its perplexity on a development set. In fig-
ure 1 we plot perplexity and size of the WMT Giga
LM for different values of the data-selection thresh-
old. Perplexities are computed on the newstest2009
set. As a good perplexity-size trade-off, the thresh-
old 250 was chosen to estimate an additional 5-gram
LM (WMT Giga 250) that was interpolated with
the original News LM. The resulting improvement
in perplexity is reported in table 1. For translation
model data, a perplexity threshold of 159 was ap-
plied.
373
LM Perplexity OOV rate
News 146.84 0.82
News + WMT Giga 250 130.23 0.71
Table 1: Perplexity reduction after interpolating the News
LM with data selected from the 109 corpus.
newstest
2009 2010 2011
Primary submission 0.246 0.286 0.284
w/o Anaphora handling 0.246 0.286 0.284
WMT Giga data
w/o LM 0.244 0.289 0.280
w/o TM 0.247 0.286 0.282
w/o LM and TM 0.247 0.289 0.278
Lexical reordering
phrase-based reo 0.239 0.281 0.275
no lexical reo 0.239 0.281 0.275
with LDC data 0.254 0.293 0.291
Table 2: Ablation test results (case-sensitive BLEU)
1.4 Results and Ablation tests
Owing to time constraints, we were not able to run
thorough tests on our system before submitting it to
the evaluation campaign. We therefore evaluated the
various components included in a post hoc fashion
by running ablation tests. In each test, we left out
one of the system components to identify its effect
on the overall performance. The results of these tests
are reported in table 2.
Performance-wise, the most important particular-
ity of our SMT system was the hierarchical lexical
reordering model, which led to a sizeable improve-
ment of 0.7, 0.5 and 0.9 BLEU points for the 2009,
2010 and 2011 test sets, respectively. We had previ-
ously seen negative results when trying to apply the
same model to English-German SMT, so its perfor-
mance seems to be strongly dependent on the lan-
guage pair it is used with.
Compared to the scores obtained using the full
system, the anaphora handling system did not have
any effect on the BLEU scores. This result is
similar to our result for English-German transla-
tion (Hardmeier and Federico, 2010). Unfortu-
nately, for English-French, the negative results ex-
tends to the pronoun translation scores (not reported
here), where slightly higher recall with the word-
dependency model was overcompensated by de-
graded precision, so the outcome of the experiments
clearly suggests that the anaphora handling proce-
dure is in need of improvement.
The effect of the WMT Giga language model dif-
fers among the test sets. For the 2009 and 2011
test sets, it results in an improvement of 0.2 and 0.4
BLEU points, respectively, while the 2010 test set
fares better without this additional language model.
However, it should be noted that there may be a
problem with the 2010 test set and the News lan-
guage model, which was used as a component in all
our systems. In particular, upgrading the News LM
data from last year?s to this year?s release led to an
improvement of 4 BLEU points on the 2010 test set
and an unrealistically low perplexity of 73 as com-
pared to 130 for the 2009 test set, which makes us
suspect that the latest News LM data may be tainted
with data from the 2010 test corpus. If this is the
case, the 2010 test set should be considered unreli-
able for LM evaluation. The benefit of adding WMT
Giga data to the translation model is less clear. For
the 2009 and 2010 test sets, this leads to a slight
degradation, but for the 2011 corpus, we obtained
a small improvement.
Our shared task submission did not use the French
Gigaword corpus from the Linguistic Data Consor-
tium (LDC2009T28), which is not freely available
to sites without LDC membership. After the sub-
mission, we ran a contrastive experiment including
a 5-gram model trained on this corpus, which led
to a sizeable improvement of 0.7?0.8 BLEU points
across all test sets.
2 Haitian Creole to English
Our experiments with the Haitian Creole-English
data are independent of the system presented for the
English to French task above. We experimented with
both phrase-based SMT and syntax-based SMT. The
main questions we investigated were i) whether we
can improve word alignment and phrase extraction
for phrase-based SMT and ii) whether we can in-
tegrate dependency parsing into a syntax-based ap-
proach. All our experiments were conducted on the
clean data set using Moses for training and decod-
ing. In the following we will first describe the exper-
iments with phrase-based models and linear trans-
374
duction grammars for word alignment and, there-
after, our findings from integrating English depen-
dency parses into a syntax-based approach.
2.1 Phrase-based SMT
The phrase-based system that we used in this series
of experiments uses a rather traditional setup. For
the translations into English we used the news data
provided for the other translations tasks in WMT
2011 to build a large scale-background language
model. The English data from the Haitian Creole
task were used as a separate domain-specific lan-
guage model. For the other translation direction we
only used the in-domain data provided. We used
standard 5-gram models with Witten-Bell discount-
ing and backoff interpolation for all language mod-
els. For the translation model we applied standard
techniques and settings for phrase extraction and
score estimations. However, we applied two differ-
ent systems for word alignment: One is the standard
GIZA++ toolbox implementing the IBM alignment
models (Och and Ney, 2003) and extensions and the
other is based on transduction grammars which will
briefly be introduced in the next section.
2.1.1 Alignment with PLITGs
By making the assumption that the parallel cor-
pus constitutes a linear transduction (Saers, 2011)2
we can induce a grammar that is the most likely to
have generated the observed corpus. The grammar
induced will generate a parse forest for each sen-
tence pair in the corpus, and each parse tree in that
forest will correspond to an alignment between the
two sentences. Following Saers et al (2010), the
alignment corresponding to the best parse can be ex-
tracted and used instead of other word alignment ap-
proaches such as GIZA++. There are several gram-
mar types that generate linear transductions, and in
this work, stochastic bracketing preterminalized lin-
ear inversion transduction grammars (PLITG) were
used (Saers and Wu, 2011). Since we were mainly
interested in the word alignments, we did not induce
phrasal grammars.
Although alignments from PLITGs may not reach
the same level of translation quality as GIZA++,
they make different mistakes, so both complement
2A transduction is a set of pairs of strings, and thus repre-
sents a relation between two languages.
each other. By duplicating the training corpus and
aligning each copy of the corpus with a different
alignment tool, the phrase extractor seems to be able
to pick the best of both worlds, producing a phrase
table that is superior to one produced with either of
the alignments tools used in isolation.
2.1.2 Results
In the following we present our results on the pro-
vided test set3 for translating into both languages
with phrase-based systems trained on different word
alignments. Table 3 summarises the BLEU scores
obtained.
English-Haitian BLEU phrase-table
GIZA++ 0.2567 3,060,486
PLITG 0.2407 5,007,254
GIZA++ & PLITG 0.2572 7,521,754
Haitian-English BLEU phrase-table
GIZA++ 0.3045 3,060,486
PLITG 0.2922 5,049,280
GIZA++ & PLITG 0.3105 7,561,043
Table 3: Phrase-based SMT (pbsmt) on the Haitian
Creole-English test set with different word alignments.
From the table we can see that phrase-based sys-
tems trained on PLITG alignments performs slightly
worse than the ones trained on GIZA++. However
combining both alignments with the simple data du-
plication technique mentioned earlier produces the
overall best scores in both translation directions.
The fact that both alignments lead to complemen-
tary information can be seen in the size of the phrase
tables extracted (see table 3).
2.2 Syntax-based SMT
We used Moses and its syntax-mode for our exper-
iments with hierarchical phrase-based and syntax-
augmented models. Our main interest was to in-
vestigate the influence of monolingual parsing on
the translation performance. In particular, we tried
to integrate English dependency parses created by
MaltParser (Nivre et al, 2007) trained on the Wall
Street Journal section of the Penn Treebank (Mar-
cus et al, 1993) extended with about 4000 questions
3We actually swapped the development set and the test set
by mistake. But, of course, we never mixed development and
test data in any result reported.
375
from the Question Bank (Judge et al, 2006). The
conversion to dependency trees was done using the
Stanford Parser (de Marneffe et al, 2006). Again,
we ran both translation directions to test our settings
in more than just one task. Interesting here is also
the question whether there are significant differences
when integrating monolingual parses on the source
or on the target side.
The motivation for applying dependency parsing
in our experiments is to use the specific information
carried by dependency relations. Dependency struc-
tures encode functional relations between words that
can be seen as an interface to the semantics of a
sentence. This information is usually not avail-
able in phrase-structure representations. We believe
that this type of information can be beneficial for
machine translation. For example, knowing that a
noun acts as the subject of a sentence is more in-
formative than just marking it as part of a noun
phrase. Whether or not this information can be ex-
plored by current syntax-based machine translation
approaches that are optimised for phrase-structure
representations is a question that we liked to inves-
tigate. For comparison we also trained hierarchical
phrase-based models without any additional annota-
tion.
2.2.1 Converting projective dependency trees
First we needed to convert dependency parses to
a tree representation in order to use our data in
the standard models of syntax-based models imple-
mented in Moses. In our experiments, we used
a parser model that creates projective dependency
graphs that can be converted into tree structures of
nested segments. We used the yield of each word
(referring to that word and its transitive dependents)
to define spans of phrases and their dependency rela-
tions are used as span labels. Furthermore, we also
defined pre-terminal nodes that encode the part-of-
speech information of each word. These tags were
obtained using the HunPos tagger (Hala?csy et al,
2007) trained on the Wall Street Journal section of
the Penn Treebank. Figure 2 illustrates the conver-
sion process. Tagging and parsing is done for all En-
glish data without any manual corrections or optimi-
sation of parameters. After the conversion, we were
able to use the standard training procedures imple-
mented in Moses.
-ROOT- andCC howWRB oldJJ isVBZ yourPRP$ nephewNN ?.
advmoddep possnsubjcc
punctnull
<tree label="null">
<tree label="cc">
<tree label="CC">and</tree>
</tree>
<tree label="dep">
<tree label="advmod">
<tree label="WRB">how</tree>
</tree>
<tree label="JJ">old</tree>
</tree>
<tree label="VBZ">is</tree>
<tree label="nsubj">
<tree label="poss">
<tree label="PRP$">your</tree>
</tree>
<tree label="NN">nephew</tree>
</tree>
<tree label="punct">
<tree label=".">?</tree>
</tree>
</tree>
Figure 2: A dependency graph from the training corpus
and its conversion to a nested tree structure. The yield of
each word in the sentence defines a span with the label
taken from the relation of that word to its head. Part-of-
speech tags are used as additional pre-terminal nodes.
2.2.2 Experimental Results
We ran several experiments with slightly differ-
ent settings. We used the same basic setup for
all of them including the same language models
and GIZA++ word alignments that we have used
for the phrase-based models already. Further, we
used Moses for extracting rules of the syntax-based
translation model. We use standard settings for
the baseline system (=hiero) that does not employ
any linguistic markup. For the models that include
dependency-based trees we changed the maximum
span threshold to a high value of 999 (default: 15)
in order to extract as many rules as possible. This
large degree of freedom is possible due to the oth-
erwise strong constraints on rule flexibility imposed
by the monolingual syntactic markup. Rule tables
are dramatically smaller than for the unrestricted hi-
erarchical models (see table 4).
However, rule restriction by linguistic constraints
usually hurts performance due to the decreased cov-
erage of the rule set. One common way of improving
376
reference Are you going to let us die on Ile a` Vaches which is located close the city of Les Cayes. I am ...
pbsmt Do you are letting us die in Ilavach island?s on in Les Cayes. I am ...
hiero do you will let us die in the island Ilavach on the in Les Cayes . I am ...
samt2 Are you going to let us die in the island Ilavach the which is on the Les. My name is ...
reference I?m begging you please help me my situation is very critical.
pbsmt Please help me please. Because my critical situation very much.
hiero please , please help me because my critical situation very much .
samt2 Please help me because my situation very critical.
reference I don?t have money to go and give blood in Port au Prince from La Gonave.
pbsmt I don?t have money, so that I go to give blood Port-au-Prince since lagonave.
hiero I don ?t have any money , for me to go to give blood Port-au-Prince since lagonave .
samt2 I don?t have any money, to be able to go to give blood Port-au-Prince since Gona?ve Island.
Figure 3: Example translations for various models.
English-Haitian BLEU number of rules
hiero 0.2549 34,118,622
malt (source) 0.2180 1,628,496
- binarised 0.2327 9,063,933
- samt1 0.2311 11,691,279
- samt2 0.2366 29,783,694
Haitian-English BLEU number of rules
hiero 0.3034 33,231,535
malt (target) 0.2739 1,922,688
- binarised 0.2857 8,922,343
- samt1 0.2952 11,073,764
- samt2 0.2954 24,554,317
Table 4: Syntax-based SMT on the Haitian Creole-
English test set with (=malt) or without (=hiero) English
parse trees and various parse relaxation strategies. The
final system submitted to WMT11 is malt(target)-samt2.
rule extraction is based on tree manipulation and re-
laxed extraction algorithms. Moses implements sev-
eral algorithms that have been proposed in the lit-
erature. Tree binarisation is one of them. This can
be done in a left-branching and in a right-branching
mode. We used a combination of both in the set-
tings denoted as binarised. The other relaxation al-
gorithms are based on methods proposed for syntax-
augmented machine translation (Zollmann et al,
2008). We used two of them: samt1 combines pairs
of neighbouring children nodes into combined com-
plex nodes and creates additional complex nodes of
all children nodes except the first child and similar
complex nodes for all but the last child. samt2 com-
bines any pair of neighbouring nodes even if they are
not children of the same parent. All of these relax-
ation algorithms lead to increased rule sets (table 4).
In terms of translation performance there seems to
be a strong correlation between rule table size and
translation quality as measured by BLEU. None of
the dependency-based models beats the unrestricted
hierarchical model. Both translation directions be-
have similar with slightly worse performances of
the dependency-based models (relative to the base-
line) when syntax is used on the source language
side. Note also that all syntax-based models (includ-
ing hiero) are below the corresponding phrase-based
SMT systems. Of course, automatic evaluation has
its limits and interesting qualitative differences may
be more visible in manual assessments. The use of
linguistic information certainly has an impact on the
translation hypotheses produced as we can see in the
examples in figure 3. In the future, we plan to inves-
tigate the effect of dependency information on gram-
maticality of translated sentences in more detail.
3 Conclusions
In our English-French and Haitian Creole-English
shared task submissions, we investigated the use
of anaphora resolution, hierarchical lexical reorder-
ing and data selection for language modelling
(English-French) as well as LTG word alignment
and syntax-based decoding with dependency infor-
mation (Haitian Creole-English). While the re-
sults for the systems with anaphora handling were
somewhat disappointing and the effect of data fil-
tering was inconsistent, hierarchical lexical reorder-
ing brought substantial improvements. We also ob-
tained consistent gains by combining information
from different word aligners, and we presented a
simple way of including dependency parses in stan-
dard tree-based decoding.
377
Acknowledgements
Most of the features used in our English-French sys-
tem were originally developed while Christian Hard-
meier was at FBK. Activities at FBK were supported
by the EuroMatrixPlus project (IST-231720) and the
T4ME network of excellence (IST-249119), both
funded by the DG INFSO of the European Commis-
sion through the Seventh Framework Programme.
References
Samuel Broscheit, Massimo Poesio, Simone Paolo
Ponzetto, Kepa Joseba Rodriguez, Lorenza Romano,
Olga Uryupina, Yannick Versley, and Roberto Zanoli.
2010. BART: A multilingual anaphora resolution sys-
tem. In Proceedings of the 5th International Workshop
on Semantic Evaluations (SemEval-2010), Uppsala.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed de-
pendency parses from phrase structure parses. In Pro-
ceedings of LREC.
Marcello Federico, Nicola Bertoldi, and Mauro Cettolo.
2008. IRSTLM: an open source toolkit for handling
large scale language models. In Interspeech 2008,
pages 1618?1621. ISCA.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Processing,
pages 847?855, Honolulu, Hawaii, October. Associa-
tion for Computational Linguistics.
Pe?ter Hala?csy, Andra?s Kornai, and Csaba Oravecz. 2007.
Hunpos: an open source trigram tagger. In Proceed-
ings of the 45th Annual Meeting of the ACL on Inter-
active Poster and Demonstration Sessions, pages 209?
212.
Christian Hardmeier and Marcello Federico. 2010. Mod-
elling Pronominal Anaphora in Statistical Machine
Translation. In Marcello Federico, Ian Lane, Michael
Paul, and Franc?ois Yvon, editors, Proceedings of the
seventh International Workshop on Spoken Language
Translation (IWSLT), pages 283?289.
John Judge, Aoife Cahill, and Josef van Genabith. 2006.
Questionbank: creating a corpus of parse-annotated
questions. In Proceedings of the 21st International
Conference on Computational Linguistics and the 44th
annual meeting of the Association for Computational
Linguistics, pages 497?504.
Philipp Koehn, Amittai Axelrod, Alexandra
Birch Mayne, et al 2005. Edinburgh system
description for the 2005 iwslt speech translation
evaluation. In International workshop on spoken
language translation, Pittsburgh.
Philipp Koehn, Hieu Hoang, Alexandra Birch, et al
2007. Moses: open source toolkit for Statistical Ma-
chine Translation. In Annual meeting of the Associ-
ation for Computational Linguistics: Demonstration
session, pages 177?180, Prague.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated cor-
pus of english: The Penn Treebank. Computational
Linguistics, 19:313?330, June.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,
Gu?lsen Eryigit, Sandra Ku?bler, Svetoslav Marinov,
and Erwin Marsi. 2007. MaltParser: A language-
independent system for data-driven dependency pars-
ing. Natural Language Engineering, 13(2):95?135.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational linguistics, 29:19?51.
Markus Saers and Dekai Wu. 2011. Principled induction
of phrasal bilexica. In Proceedings of the 15th Annual
Conference of the European Association for Machine
Translation, Leuven, Belgium, May.
Markus Saers, Joakim Nivre, and Dekai Wu. 2010. Word
alignment with stochastic bracketing linear inversion
transduction grammar. In Human Language Technolo-
gies: The 2010 Annual Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics, pages 341?344, Los Angeles, California,
June.
Markus Saers. 2011. Translation as Linear Transduc-
tion: Models and Algorithms for Efficient Learning in
Statistical Machine Translation. Ph.D. thesis, Uppsala
University, Department of Linguistics and Philology.
M. Strube. 2006. Anaphora and coreference resolution,
Statistical. In Encyclopedia of language and linguis-
tics, pages 216?222. Elsevier.
Andreas Zollmann, Ashish Venugopal, Franz Och, and
Jay Ponte. 2008. A systematic comparison of phrase-
based, hierarchical and syntax-augmented statistical
mt. In Proceedings of the 22nd International Confer-
ence on Computational Linguistics - Volume 1, pages
1145?1152.
378
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 301?308,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Online Learning Approaches in Computer Assisted Translation
Prashant Mathur??, Mauro Cettolo?, Marcello Federico?
? University of Trento
? FBK - Fondazione Bruno Kessler
Trento, Italy
{prashant, cettolo, federico}@fbk.eu
Abstract
We present a novel online learning ap-
proach for statistical machine translation
tailored to the computer assisted transla-
tion scenario. With the introduction of
a simple online feature, we are able to
adapt the translation model on the fly
to the corrections made by the transla-
tors. Additionally, we do online adaption
of the feature weights with a large mar-
gin algorithm. Our results show that our
online adaptation technique outperforms
the static phrase based statistical machine
translation system by 6 BLEU points abso-
lute, and a standard incremental adaptation
approach by 2 BLEU points absolute.
1 Introduction
The growing needs of the localization and trans-
lation industry have recently boosted research
around computer assisted translation (CAT) tech-
nology. The purpose of CAT is to increase the pro-
ductivity of a human translator. A CAT tool comes
as a package of a Translation Memory (TM), built-
in spell checkers, a dictionary, a terminology list
etc. which help the translator while translating
a sentence. Recent research has led to the in-
tegration of CAT tools with statistical machine
translation (SMT) engines. SMT makes use of a
large available parallel corpus to generate statisti-
cal models for translation. Due to their generaliza-
tion capability, SMT systems are a good fit in this
scenario and a seamless integration of SMT en-
gines in CAT have shown to increase translator?s
productivity (Federico et al, 2012).
Although automatic systems generate reliable
translations they are not accurate enough to be
used directly and need postedition by human trans-
lators. In state-of-the-art CAT tools, the SMT sys-
tems are static in nature and so they cannot adapt
to these corrections. When a SMT system keeps
repeating the same error, productivity of transla-
tors as well as their trust in SMT technology are
negatively affected. As an example, technical doc-
umentation typically contains a lot of repetitions
due to the employed writing style and pervasive
use of terminology. Hence, in order to provide
useful hints, SMT systems are expected to behave
consistently regarding the translation of domain-
specific terms. However, if the user edits the trans-
lation of a technical term in the target text, most
current SMT systems are incapable to learn from
those corrections.
Online learning is a machine learning task
where a predictor iteratively: (1) receives an input
and outputs a label, (2) receives the correct label
from a human and if the two labels do not match, it
learns from the mistake. The task of learning from
user corrections at the sentence level fits well the
online learning scenario, and its expected useful-
ness is clearly related to the amount of repetitions
occurring in the text. The higher the number of
repetititions in a document the more the SMT sys-
tem has chances to translate consistently through
the use of online learning.
In this paper, we implemented two online learn-
ing methods through which a phrase-based SMT
system evolves over time, sentence after sentence,
by taking advantage of the post-edition or transla-
tion of the previous sentence by the user.1
In the first approach, we focus on the translation
model aspect of SMT which is represented by five
conventional features, namely lexical and phrase
translation probabilities in both directed and in-
verted directions, plus a phrase penalty score.
Translation, language and reordering models are
combined in a linear fashion to obtain a score for
1Moses code is available in the github reposi-
tory. https://github.com/mtresearcher/
mosesdecoder/tree/moses_onlinelearning
301
the translation hypothesis as shown in Equation 1.
score(e?, f) = ?i?ihi(e?, f) (1)
where hi(?) are the feature functions representing
the models and ?i are the linear weights. The
highest scored translation is the best hypothesis
e? output by the system. We extend the transla-
tion model with a new feature which provides ex-
tra phrase-pair scores changing according to the
user feedback. The scores of the new feature are
adapted in a discriminative fashion, by reward-
ing phrase-pairs observed in the search space and
in the reference, and penalizing phrase-pairs ob-
served in the search space but not in the reference.
In the second approach, we also adapt the model
weights of the linear combination after each test
sentence by using a margin infused relaxed algo-
rithm (MIRA).
For assessing the robustness of our methods, we
performed experiments on two datasets from dif-
ferent domains and language pairs (?6). More-
over, our online learning approaches are compared
against a static baseline system and against the in-
cremental adaptation approach proposed by Lev-
enberg et. al. (2010) (?5).
2 Related Works
Several online adaptation strategies have been pro-
posed in the past, only a few deal with adaptation
of post-edited/evaluation data while most works
are on adaptation over development data during
tuning of parameters (Och and Ney, 2003).
2.1 Online Adaptation during Tuning
Liang et. al. (2006) improved SMT perfor-
mance by online adaptation of scaling factors (? in
(1)) using averaged perceptron algorithm (Collins,
2002). They presented different strategies to up-
date the SMT models towards reference or oracle
translation: (1) aggressively updating towards ref-
erence, bold update; (2) update towards the ora-
cle translation in N-Best list, local update; (3) a
hybrid approach in which a bold update is per-
formed when the reference is reachable, other-
wise a local update is performed. Liang and Klein
(2009) compared two online EM algorithms, step-
wise online EM (Sato and Ishii, 2000; Cappe? and
Moulines, 2007) and incremental EM (Neal and
Hinton, 1998) which they use to update the align-
ment models (the generative component of SMT)
on the fly. However, stepwise EM is prone to fail-
ure if mini-batch size and stepsize parameters are
not chosen correctly, while incremental EM re-
quires substantial storage costs because it has to
store sufficient statistics for each sample. Other
works on online minimum error rate training in
SMT (Och and Ney, 2003) that deserve mention-
ing are (Hopkins and May, 2011; Hasler et al,
2011).
2.2 Online Adaptation during Decoding
Cesa-Bianchi et. al. (2008) proposed an online
learning approach during decoding. They con-
struct a layer of online weights over the regu-
lar feature weights and update these weights at
sentence level using margin infused relaxed algo-
rithm (Crammer and Singer, 2003); to our knowl-
edge, this is the first work on online adaptation
during decoding. Mart??nez-Go?mez et. al. (2011;
2012) presented a comparison of online adapta-
tion techniques in post editing scenario. They
compared different adaptation strategies on scal-
ing factors and feature functions (respectively, ?
and h(?) in (1)). However, they modified the fea-
ture values during adaptation without any normal-
ization, which disregards the initial assumption of
the feature values being probabilities.
In our approach, the value of the additional on-
line feature can be modified during decoding with-
out changing other feature values (probabilities)
and thus preserving their probability distribution.
3 Feature Adaptation
In the CAT scenario, the user receives a translation
suggestion for each source segment, post-edits it
and finally approves it. From the SMT point of
view, for each source segment the decoder ex-
plores a search space of possible translations and
finally returns the best scoring one (bestHyp) to
the user. The user possibly corrects this suggestion
thus generating the final translation (postedit).
Our online learning procedure is based on the
following idea. For each N-best translation (candi-
date) in the search space, we compute a similarity
score against the postedit using the sentence-level
BLEU metric (Lin and Och, 2004), a smoothed
variant of the popular BLEU metric (Papineni
et al, 2001). We hence compare the similar-
ity score of each candidate against the similar-
ity score achieved by the bestHyp, that was also
computed against the postedit. If the candidate
302
scores better than the bestHyp, then we promote
the building blocks, i.e. phrase-pairs, of candi-
date that were not used in bestHyp and demote the
phrase-pairs used in bestHyp that were not used
for candidate. On the contrary, if the candidate
scores worse than the bestHyp, we promote the
building blocks of bestHyp that are not in candi-
date and demote those of candidate that are not in
bestHyp.
Our promotion/demotion mechanism could be
implemented by updating the features values of
the phrase pairs used in the candidate and bestHyp
translations. However, features in the translation
models are conditional probabilities and perturb-
ing a subset of them by also preserving their nor-
malization constraints can be computationally ex-
pensive. Instead, we propose to introduce an addi-
tional online feature which represents a goodness
score of each phrase-pair in the test set.
We call the set of phrase pairs used to generate
a candidate as candidatePP and the set of phrase
pairs used to generate the bestHyp as bestPP . The
online feature value of each phrase-pair is initial-
ized to a constant and is updated according to the
perceptron update (Rosenblatt, 1958) method. In
particular, the amount by which a current feature
value is rewarded or penalized depends on a learn-
ing rate ? and on the difference between the model
scores (i.e. h ?w) of candidate and bestHyp as cal-
culated by the MT system. A sketch of our online
learning procedure is shown in Algorithm 1.
Algorithm 1: Online Learning
foreach sourceSeg do
bestHyp = Translate(sourceSeg);
postedit = Human(bestHyp);
for i = 1 ? iterations do
N-best=Nbest(source);
foreach candidate ? N-best do
sign = sgn |sBLEU(candidate) -
sBLEU(bestHyp)| ;
foreach phrasePair ? candidatePP do
if phrasePair /? bestPP then
f i = f i?1 + (? ? (?h ? w) ?
sign);
end
end
foreach phrasePair ? bestPP do
if phrasePair /? candidatePP then
f i = f i?1 - (? ? (?h ? w) ?
sign);
end
end
end
end
end
In Algorithm 1, ?h ? w is the above mentioned
score difference as computed by the decoder; mul-
tiplied by ?, it is the margin, that is the value with
which the online feature score (f ) of the phrase
pair under processing is modified. We can observe
that the feature scores are unbounded and could
lead to instability of the algorithm; therefore, we
normalise the scores through the sigmoid function:
f(x) = 21 + exp(x) ? 1 (2)
4 Weight Adaptation
In addition to adapting the online feature values,
we can also apply online adaptation on the fea-
ture weights of the linear combination (eq. 1). In
particular, after translating each sentence we can
adapt the parameters depending on how good the
last translation was. A commonly used algorithm
in this online paradigm for tuning of parameters is
the Margin Infused Relaxed Algorithm (MIRA).
MIRA is an online large margin algorithm that
updates the parameter w? of a given model accord-
ing to the loss that is occurred due to incorrect
classification. In the case of SMT this margin
can be coupled with the loss function, which in
this case is the complement of the sentence level
BLEU(sBLEU). Thus, the loss function can be
formulated as:
l(y?) = sBLEU(y?)? sBLEU(y?) (3)
where y? is the oracle (closest translation to the
reference) and y? is the candidate being processed.
Ideally, this loss should correspond to the differ-
ence between the model scores:
?h ? w? = score(y?)? score(y?) (4)
MIRA is an ultraconservative algorithm, meaning
that the update of the current weight vector is the
smallest possible value satisfying the constraint
that the variation incurred by the objective func-
tion must not be larger than the variation incurred
by the model (plus a non-negative slack variable
?). Formally, weight update at ith iteration is de-
fined as:
wi = argminw
1
2? ||w ? wi?1||
2
? ?? ?
conservative
+ C????
aggressive
?
j
?j
subject to
lj ? ?hj ? w + ?j ?j ? J ? {1 . . . N}
(5)
303
where j ranges over all candidates in the N-
best list, lj is the loss between oracle and the
candidate j, and ?hj ? w is the corresponding
difference in the model scores. C is an aggressive
parameter which controls the size of the update, ?
is the learning rate of the algorithm and ? is usu-
ally a very small value (in our experiments we kept
it as 0.0001). After partial differentiation and lin-
earizing the loss, equation 5 can be rewritten as:
wi = wi?1 + ? ?
?
j
?j ??hj
where
?j = min
{
C, lj ? ?hj ? w||?hj ||2
}
(6)
We solve equation 5, by computing ? with
the optimizer integrated in the Moses toolkit by
(Hasler et al, 2011). Algorithm 2 gives an
overview of the online margin infused relaxed al-
gorithm we implemented in Moses.
Algorithm 2: Online Margin Infused Relaxed
foreach sourceSeg do
bestHyp = Translate(sourceSeg);
postedit = Human(bestHyp);
w0 = w;
for i = 1 ? iterations do
N-best=Nbest(sourceSeg,wi?1);
foreach candidatej ? N-best do
if ?hj ? w + ?j ? lj then
?j = Optimize(lj , hj , w, C);
wi = wi?1 + ? ??j ?j?hj ;
end
end
end
end
In the following section we overview a stream
based adaptation method with which we exper-
imentally compared our two online learning ap-
proaches as it well fits the framework we are work-
ing in.
5 Stream based adaptation
Continuously updating an SMT system to an in-
coming stream of parallel data comes under stream
based adaptation. Levenberg et. al. (2010) pro-
posed an incremental adaptation technique for the
core generative component of the SMT system,
word alignments and language models (Leven-
berg and Osborne, 2009). To get the word align-
ments on the new data they use a Stepwise online
EM algorithm, where old counts (from previous
alignment models) are interpolated with the new
counts.
Since we work at the sentence level, on-the-
fly computation of probabilities of translation and
reordering models is expensive in terms of both
computational and memory requirements. To save
these costs, we prefer using dynamic suffix ar-
ray approach described in (Levenberg et al, 2010;
Callison-Burch et al, 2005; Lopez, 2008). They
are used to efficiently store the source and the tar-
get corpus and alignments in efficient data struc-
ture, namely the suffix array. When a phrase
translation is asked by the decoder, the corpus is
searched, the counts are collected and its probabil-
ities are computed on the fly. However, the current
implementation in Moses of the stream based MT
relying on the suffix arrays is severely limited as
it allows the computation of only three translation
features, namely the two direct translation proba-
bilities and the phrase penalty. This results in a
significant degradation of performance.
6 Experiments
6.1 Datasets
We compared our online learning approaches
(Sections 3 and 4) and the stream based adapta-
tion method (Section 5) on two datasets from dif-
ferent domains, namely Information Technology
(IT) and TED talks, and two different language
pairs. The IT domain dataset is proprietary, it in-
volves the translation of technical documents from
English to Italian and has been used in the field
test carried out under the MateCat project2. Ex-
periments are also conducted on English to French
TED talks dataset (Cettolo et al, 2012) to assess
the robustness of the proposed approaches in a dif-
ferent scenario and to provide results on a publicly
available dataset for the sake of reproducibility.
The training, development (dev2010) and evalu-
ation (tst20103) sets are the same as used in the
last IWSLT last evaluation campaigns. In experi-
ments on TED data, we considered the human ref-
erence translations as post edits, even if they were
2www.matecat.com
3As the size of evaluation set in TED data is too large with
respect to the current implementation of our algorithms, we
performed evaluation on the first 200 sentences only.
304
actually generated from scratch.
In our experiments, the extent of usefulness of
online learning highly depends on the amount of
repetition of text. A reasonable way to measure the
quantity of repetition in each document is through
the repetition rate (Bertoldi et al, 2013). It com-
putes the rate of non-singleton n-grams, n=1...4,
averaging the values over sub-samples S of thou-
sand words from the text, and then combining the
rate of each n-gram to a single score by using the
geometric mean. Equation 7 shows the formula
for calculating the repetition rate of a document,
where dict(n) represents the total number of
different n-grams and nr is the number of different
n-grams occurring exactly r times:
RR =
( 4?
n=1
?
S dict(n)? n1?
S dict(n)
)1/4
(7)
Statistics of the parallel sets and their repetition
rate on both sides are reported in Table 1.
Domain Set #srcTok srcRR #tgtTok tgtRR
ITen?it
Train 57M na 60M na
Dev 3.3k 12.03 3.5k 11.87
Test 3.3k 15.00 3.3k 14.57
TEDen?fr
Train 2.6M na 2.8M na
Dev 20k 3.43 20k 5.27
Test 32k 4.08 34k 3.57
Table 1: Statistics of the parallel data along with
the corresponding repetition rate (RR).
It can be noted that the repetition rates of IT
and TED sets are significantly different, partic-
ularly high in IT documents, much lower in the
TED talks.
6.2 Systems
The SMT systems were built using the Moses
toolkit (Koehn et al, 2007). Training data in each
domain was used to create translation and lexical
reordering models. We created a 5-gram LM for
TED talks and a 6-gram LM for the IT domain
using IRSTLM (Federico et al, 2008) with im-
proved Kneser-Ney smoothing (Chen and Good-
man, 1996) on the target side of the training paral-
lel corpora. The log linear weights for the baseline
systems are optimized using MERT (Och, 2003)
provided in the Moses toolkit. To counter the in-
stability of MERT, we averaged the weights of
three MERT runs in each case. Performance is
measured in terms of BLEU and TER (Snover
et al, 2006) computed using the MultEval script
(Clark et al, 2011). Since the implementations of
standard Giza and of incremental Giza combined
with dynamic suffix arrays are not comparable,
we constructed two baselines, a standard phrase
based SMT system and an incremental Giza base-
line (?5). Details on experimental SMT systems
we built follow.
Baseline This system was built on the parallel
training data for each domain. We run 5 iterations
of model 1, 5 of HMM (Vogel et al, 1996), 3 of
model 3, 3 of model 4 (Brown et al, 1993) us-
ing MGiza (Gao and Vogel, 2008) toolkit to align
the parallel corpus at word level. Translation and
reordering models were built using Moses, while
log-linear weights were optimized with MERT on
the corresponding development sets. The same IT
baseline system was used in the field test of Mate-
Cat and the references in the IT data are actual
postedits of its translation.
IncGiza Baseline We trained alignment models
with incGiza++4 with 5 iterations of model 1 and
10 iterations of the HMM model. To build in-
cremental Giza baselines, we used dynamic suf-
fix arrays as implemented in Moses which allow
the addition of new parallel data during decod-
ing. In the incremental Giza baseline, once a sen-
tence of the test set is translated, the sentence pair
(source and target post-edit/reference) along with
the alignment provided by incGiza are added to
the models.
Online learning systems We developed several
online systems on top of the two aforementioned
baseline systems: (1) +O employ the additional
online feature (Section 3) updated with Algorithm
1; (2) +O+NS as (1) but with the online fea-
ture normalized with the sigmoid function; (3)
+W weights updated (Section 4) with Algorithm
2; (4) +O+W combination of online feature and
weight update; (5) +O+NS+W as system (4) with
normalized online feature score.
In the online learning system we have three ad-
ditional parameters: a weight for the online fea-
ture, a learning rate for features (used in the per-
ceptron update), and a learning rate for feature
weights used by MIRA. These additional param-
eters were optimized by maximizing the BLEU
4http://code.google.com/p/inc-giza-pp/
305
score on the devset and on top of already opti-
mized feature weights. For practical reasons, opti-
mization of the parameters was run with the Sim-
plex algorithm (Nelder and Mead, 1965).
7 Results and Discussion
Tables 2 and 3 collect results by the systems de-
scribed in Section 6.2 on the IT and TED transla-
tion tasks, respectively.
In Table 2, the online system (1st block
?+O+NS+W? system with 10 iterations of online
learning) shows significant improvements, over 6
BLEU points absolute above the baseline. In this
case the online feature can clearly take advantage
of the high repetition rates observed in the IT dev
and test sets (Table 1). Similarly, in the second
block, the online system (2nd block ?+O+NS+W?
with 10 iterations of online learning) outperforms
IncGiza baseline, too. It is interesting to note that
by continuously updating the baseline system af-
ter each translation step, even the plain translation
models are capable to learn from the correction in
the post-edited text.
Figure 1 depicts learning curve of Baseline sys-
tem, ?+O+NS? (referred as +online feature) and
?+O+NS+W? (referred as +MIRA). We plotted in-
cremental BLEU scores after translation of each
sentence, thereby the last point on the plot shows
the corpus level BLEU on the whole test set.
In Table 3, from the first block we can observe
that online learning systems perform only slightly
better than the baseline systems, the main reason
being the low repetition rate observed in the eval-
uation set (as shown in Table 1). The positive re-
sults observed in the second block (?+O+W? with
10 iterations) are probably due to the larger room
for improvement available for translation models
implemented with dynamic suffix arrays, as they
only incorporate 3 features instead of 5. Some-
times, online learning systems show worse results
with higher numbers of iterations, which seems
due to overfitting. It is also interesting to notice
that after optimization the weight value of the on-
line feature was 0.509 for the IT task and 0.072 for
the TED talk task. This confirms the different use
and potential assigned to the online feature by the
SMT systems in the two tasks.
8 Conclusion
We have shown a new way to update the transla-
tion model on the fly without changing the original
probability distribution. We empirically proved
that this method is robust and works for differ-
ent domain datasets be it Information Technology
or TED talks. In addition, if the repetition rate is
high in the text, online learning works much bet-
ter than if the rate is low. We tested both with an
unbounded and a bounded range on the online fea-
ture and found out that bounded values produce
more stable and consistent results. From previous
works, it has been proven that MIRA works well
with sparse features too, so, as for the future plan
we would like to treat each phrase pair as a sparse
feature and tune the sparse weights using MIRA.
From the results, it is evident that we have not used
any sort of stopping criterion for online learning; a
random of 1, 5 and 10 iterations were chosen in a
naive way. Our future plan will extend to working
on finding a stopping criterion for online learning
process.
Acknowledgements
This work was supported by the MateCat project,
which is funded by the EC under the 7th Frame-
work Programme.
References
N. Bertoldi, M. Cettolo, and M. Federico. 2013.
Cache-based online adaptation for machine trans-
lation enhanced computer assisted translation. In
Proc. of MT Summit, Nice, France.
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Compu-
tational Linguistics, 19(2):263?312.
C. Callison-Burch, C. Bannard, and J. Schroeder.
2005. Scaling phrase-based statistical machine
translation to larger corpora and longer phrases. In
Proc. of ACL, pages 255?262, Ann Arbor, US-MI.
O. Cappe? and E. Moulines. 2009. Online EM algo-
rithm for latent data models. Journal of the Royal
Statistical Society Series B (Statistical Methodol-
ogy), 71(3):593?613.
N. Cesa-Bianchi, G. Reverberi, and S. Szedmak. 2008.
Online learning algorithms for computer-assisted
translation. Technical report, SMART project
(www.smart-project.eu).
M. Cettolo, C. Girardi, and M. Federico. 2012. WIT3:
web inventory of transcribed and translated talks. In
Proc. of EAMT, Trento, Italy.
S. F. Chen and J. Goodman. 1996. An empirical study
of smoothing techniques for language modeling. In
Proc. of ACL, pages 310?318, Santa Cruz, US-CA.
306
System Bleu (?) TER (?)
1 Iter 5 Iter 10 Iter 1 Iter 5 Iter 10 Iter
Baseline 38.46(1.79) - - 39.98(1.35) - -
+O 39.88(1.77) 41.22(1.80) 41.16(1.74) 38.69(1.30) 37.78(1.32) 38.37(1.30)
+O+NS 39.91(1.80) 40.54(1.79) 40.71(1.76) 38.67(1.31) 38.21(1.29) 38.17(1.31)
+W 39.76(1.76) 38.16(1.77) 37.57(1.82) 38.58(1.27) 39.53(1.30) 39.93(1.30)
+O+W 41.23(1.66) 40.29(1.54) 29.36(1.45) 37.53(1.26) 38.03(1.24) 49.08(1.25)
+O+NS+W 41.19(1.86) 43.07(1.87) 45.13(1.74) 37.60(1.35) 36.43(1.43) 34.53(1.36)
IncGiza Baseline 28.48(1.50) - - 49.23(1.43) - -
+O 29.34(1.51) 27.80(1.49) 27.52(1.38) 47.86(1.41) 48.20(1.30) 51.01(1.53)
+O+NS 28.69(1.53) 29.68(1.45) 29.36(1.49) 48.21(1.45) 47.51(1.45) 47.92(1.45)
+W 28.25(1.56) 27.68(1.53) 27.57(1.50) 49.05(1.43) 48.74(1.36) 48.10(1.23)
+O+W 29.36(1.61) 29.94(1.64) 25.95(1.25) 47.15(1.41) 46.56(1.31) 50.31(1.15)
+O+NS+W 29.76(1.49) 30.28(1.54) 30.83(1.60) 46.62(1.39) 45.60(1.28) 46.54(1.31)
Table 2: Result on the IT domain task (EN>IT). Baseline is a standard phrase based SMT system, +O
has the online feature, +NS adds normalization of online feature, +W has online weight adaptation.
 20
 25
 30
 35
 40
 45
 50
 0  20  40  60  80  100  120  140  160  180
B
LE
U
 S
co
re
Sentence Number
baseline
+online feature
+MIRA
Figure 1: Incremental BLEU vs. evaluation test size on the information-technology task. Three systems
are tracked: Baseline, +online feature, +MIRA
System Bleu (?) TER (?)
1 Iter 5 Iter 10 Iter 1 Iter 5 Iter 10 Iter
Baseline 22.18(1.23) - - 58.70(1.38) - -
+O 22.17(1.19) 21.85(1.25) 21.51(1.23) 58.75(1.35) 59.22(1.36) 60.48(1.35)
+O+NS 21.97(1.20) 22.37(1.20) 22.24(1.22) 58.86(1.37) 58.75(1.37) 59.09(1.40)
+W 22.39(1.23) 21.44(1.20) 21.00(1.13) 58.96(1.40) 58.73(1.34) 58.71(1.28)
+O+W 22.33(1.21) 22.11(1.22) 21.54(1.20) 58.63(1.37) 58.31(1.38) 58.70(1.36)
+O+NS+W 22.34(1.23) 22.09(1.21) 21.62(1.18) 58.60(1.37) 58.48(1.36) 58.40(1.33)
IncGiza Baseline 15.04(1.08) - - 72.64(1.34) - -
+O 15.30(1.08) 15.47(1.10) 15.86(1.11) 72.33(1.35) 71.68(1.37) 71.09(1.36)
+O+NS 15.21(1.09) 15.48(1.12) 15.48(1.11) 72.19(1.33) 72.06(1.36) 71.65(1.33)
+W 14.81(1.08) 14.61(1.07) 14.73(1.08) 73.03(1.37) 74.69(1.48) 74.28(1.46)
+O+W 15.08(1.08) 15.59(1.09) 16.42(1.11) 72.55(1.33) 70.98(1.32) 70.07(1.27)
+O+NS+W 15.09(1.08) 15.64(1.08) 16.15(1.10) 72.57(1.34) 71.13(1.31) 70.61(1.33)
Table 3: Result on the TED talk task (EN>FR). Baseline is a standard phrase based SMT system, +O
has the online feature, +NS adds normalization of online feature, +W includes online weight adaptation.
307
J. Clark, C. Dyer, A. Lavie, and N. Smith. 2011. Bet-
ter hypothesis testing for statistical machine transla-
tion: Controlling for optimizer instability. In Proc.
of ACL, Portland, US-OR.
M. Collins. 2002. Discriminative training methods
for hidden markov models: Theory and experiments
with perceptron algorithms. In Proc. of EMNLP,
Philadelphia, US-PA.
Koby Crammer and Yoram Singer. 2003. Ultracon-
servative online algorithms for multiclass problems.
Journal of Machine Learning Research, 3:951?991.
M. Federico, N. Bertoldi, and M. Cettolo. 2008.
IRSTLM: an open source toolkit for handling large
scale language models. In Proc. of Interspeech,
pages 1618?1621, Brisbane, Australia.
M. Federico, A. Cattelan, and M. Trombetti. 2012.
Measuring user productivity in machine translation
enhanced computer assisted translation. In Proc. of
AMTA, Bellevue, US-WA.
Q. Gao and S. Vogel. 2008. Parallel implementations
of word alignment tool. In Proc. of SETQA-NLP,
pages 49?57, Columbus, US-OH.
E. Hasler, B. Haddow, and P. Koehn. 2011. Margin
infused relaxed algorithm for Moses. The Prague
Bulletin of Mathematical Linguistics, 96:69?78.
M. Hopkins and J. May. 2011. Tuning as ranking. In
Proc. of EMNLP, pages 1352?1362, Edinburgh, UK.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Con-
stantin, and E. Herbst. 2007. Moses: open source
toolkit for statistical machine translation. In Proc.
of ACL Companion Volume of the Demo and Poster
Sessions, pages 177?180, Prague, Czech Republic.
A. Levenberg and M. Osborne. 2009. Stream-based
randomised language models for SMT. In Proc. of
EMNLP, pages 756?764, Singapore.
A. Levenberg, C. Callison-Burch, and M. Osborne.
2010. Stream-based translation models for statisti-
cal machine translation. In Proc. of HLT-NAACL,
Los Angeles, US-CA.
P. Liang and D. Klein. 2009. Online EM for unsuper-
vised models. In Proc. of NAACL, pages 611?619,
Boulder, US-CO.
P. Liang, A. Bouchard-Co?te?, D. Klein, and B. Taskar.
2006. An end-to-end discriminative approach to ma-
chine translation. In Proc. of ACL, pages 761?768,
Sydney, Australia.
C.-Y. Lin and F. J. Och. 2004. Orange: a method for
evaluating automatic evaluation metrics for machine
translation. In Proc. of COLING, pages 501?507,
Geneva, Switzerland.
A. Lopez. 2008. Tera-scale translation models via pat-
tern matching. In Proc. of COLING, pages 505?512,
Manchester, UK.
P. Mart??nez-Go?mez, G. Sanchis-Trilles, and F. Casacu-
berta. 2011. Online learning via dynamic reranking
for computer assisted translation. In Proc. of CI-
CLing, pages 93?105, Tokyo, Japan.
P. Mart??nez-Go?mez, G. Sanchis-Trilles, and F. Casacu-
berta. 2012. Online adaptation strategies for statis-
tical machine translation in post-editing scenarios.
Pattern Recogn., 45(9):3193?3203.
R. Neal and G. E. Hinton. 1998. A view of the EM al-
gorithm that justifies incremental, sparse, and other
variants. In Learning in Graphical Models, pages
355?368. Kluwer Academic Publishers.
J. A. Nelder and R. Mead. 1965. A simplex method
for function minimization. The Computer Journal,
7(4):308?313.
F.J. Och and H. Ney. 2003. A systematic comparison
of various statistical alignment models. Computa-
tional Linguistics, 29(1):19?51.
F. J. Och. 2003. Minimum error rate training in sta-
tistical machine translation. In Proc. of ACL, pages
160?167, Sapporo, Japan.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2001.
Bleu: a method for automatic evaluation of machine
translation. Research Report RC22176, IBM Re-
search Division, Thomas J. Watson Research Center.
F. Rosenblatt. 1958. The Perceptron: a probabilistic
model for information storage and organization in
the brain. Psychological Review, 65:386?408.
M.-A. Sato and S. Ishii. 2000. On-line EM algorithm
for the normalized Gaussian network. Neural Com-
put., 12(2):407?432.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and J.
Makhoul. 2006. A study of translation edit rate
with targeted human annotation. In Proc. of AMTA,
Boston, US-MA.
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-based
word alignment in statistical translation. In Proc. of
COLING, pages 836?841, Copenhagen, Denmark.
308
