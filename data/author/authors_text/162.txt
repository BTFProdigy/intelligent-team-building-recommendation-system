Attribute-Based and Value-Based Clustering: An Evaluation 
Abdulrahman ALMUHAREB and Massimo POESIO 
Department of Computer Science and Centre for Cognitive Science 
University of Essex 
Colchester, United Kingdom, CO4 3SQ 
aalmuh@essex.ac.uk poesio@essex.ac.uk 
 
Abstract 
In most research on concept acquisition from 
corpora, concepts are modeled as vectors of 
relations extracted from syntactic structures.  
In the case of modifiers, these relations often 
specify values of attributes, as in (attr 
red); this is unlike what typically proposed in 
theories of knowledge representation, where 
concepts are typically defined in terms of their 
attributes (e.g., color).  We compared 
models of concepts based on values with 
models based on attributes, using lexical 
clustering as the basis for comparison. We 
find that attribute-based models work better 
than value-based ones, and result in shorter 
descriptions; but that mixed models including 
both the best attributes and the best values 
work best of all. 
1 Introduction 
In most recent research on concept acquisition 
from corpora (e.g., for lexicon construction), 
concepts are viewed as vectors of relations, or 
properties, extracted from syntactic structures 
(Grefenstette, 1993; Lin, 1998; Curran and Moens, 
2002; Kilgarriff, 2003, and many others).  These 
properties often specify values of attributes such as 
color, shape, or size: for example, the vector used 
by Lin (1998) for the concept dog includes the 
property (dog adj-mod brown). (We will 
use the term values here to refer to any modifier.)  
To our knowledge, however, no attempt has been 
made by computational linguists to use the 
attributes themselves in such vectors: i.e., to learn 
that the description of the concept dog includes 
elements such as (dog color) or (dog 
size).  This is surprising when considering that 
most models of concepts in the AI literature are 
based on such attributes (Brachman and Levesque, 
1985). 
Two problems need to be addressed when trying 
to identify concept attributes.  The first problem is 
that values are easier to extract. We found, 
however, that patterns like the X of the dog, 
already used in (Berland and Charniak, 1999; 
Poesio et al 2002) to find part-of relations (using  
techniques derived from those used in (Hearst, 
1998; Caraballo, 1999) to find hyponymy 
relations) are quite effective at finding attributes.  
A second problem might be that instances of such 
patterns are less frequent than those used to extract 
values, even in large corpora such as the British 
National Corpus (BNC).  But this problem, as well, 
is less serious when using the Web as a corpus 
(Kilgarriff and Schuetze, 2003; Keller and Lapata, 
2003; Markert et al submitted).   
We report on two experiments whose goal was 
to test whether identifying attributes leads to better 
lexical descriptions of concepts. We do this by 
comparing the results obtained by using attributes 
or more general modifiers ? that we will simply 
call values ? as elements of concept vectors used 
to identify concept similarities via clustering.  In 
Section 2, we discuss how Web data were used to 
build attribute- and value- based concept vectors, 
and our clustering and evaluation methods.  In 
Section 3, we discuss a first experiment using the 
set of concepts used in (Lund and Burgess, 1996).  
In Section 4, we discuss a second experiment using 
214 concepts from WordNet (Fellbaum, 1998).  In 
Section 5 we return to the notion of attribute.  
2 Methods 
2.1 Using Text Patterns to Build Concept 
Descriptions 
Our techniques for extracting concept 
descriptions are simpler than those used in other 
work in at least two respects.  First of all, we only 
extracted values expressed as nominal modifiers, 
ignoring properties expressed by verbal 
constructions in which the concept occurred as an 
argument (e.g., Lin?s (dog obj-of have)). 
(We originally made this simplification to 
concentrate on the comparison between attributes 
and values (many verbal relations express more 
complex properties), but found that the resulting 
descriptions were still adequate for clustering.) 
Secondly, our data were not parsed or POS-tagged 
prior to extracting concept properties; our patterns 
are word-based.  Full parsing is essential when 
complete descriptions are built (see below) and 
allows the specification of much more general 
patterns (e.g., matching descriptions modified in a 
variety of ways, see below), but is computationally 
much more expensive, particularly when Web data 
are used, as done here. We also found that when 
using the Web, simple text patterns not requiring 
parsing or POS tagging were sufficient to extract 
large numbers of instances of properties with a 
good degree of precision.  
Our methods for extracting 'values' are 
analogous to those used in the previous literature, 
apart from the two simplifications just mentioned:  
i.e., we just consider every nominal modifier as 
expressing a potential property.  The pattern we 
use to extract values is as follows: 
? "[a|an|the] * C [is|was]" 
where C is a concept, and  the wildcard (*) stands 
for  an unspecified value.  The restriction to 
instances containing is or was to ensure that the C 
actually stands for a concept (i.e., avoiding 
modifiers) proved adequate to ensure precision. An 
example of text matching this pattern is: 
? ? an inexpensive car is ? 
The pattern we use for extracting concept 
attributes is based on linguistic tests for attributes 
already discussed, e.g., in (Woods, 1975).  
According to Woods, A is an attribute of C if we 
can say [V is a/the A of C]: e.g., brown is a color 
of dogs.  If no V can be found which is a value of 
A, then A can not be an attribute for the concept C.  
This test only selects attributes that have values, 
and is designed to exclude other functions defined 
over concepts, such as parts.  But some of these 
functions can be (and have been) viewed as 
defining attributes of concepts as well; so for the 
moment we used more general patterns identifying 
all relational nouns taking a particular concept as 
arguments.  (We return on the issue of the 
characterization of attributes below.)  Our pattern 
for attributes is shown below:  
? "the * of the C [is|was]"  
where again C is a concept, but the wildcard 
denotes an unspecified attribute. Again, is/was is 
used to increase precision.  An example of text 
matching this pattern is: 
? ? the price of the car was ? 
Both of the patterns we use satisfy Hearst's 
desiderata for good patterns (Hearst, 1998): they 
are (i) frequent, (ii) precise, and (iii) easy to 
recognize.  Patterns similar to our attribute pattern 
were used by Berland and Charniak (1999) and 
Poesio et al(2002) to find object parts only; after 
collecting their data, Berland and Charniak filtered 
out words ending with "ness", "ing", and "ity", 
because these express qualities of objects, and used 
a ranking method to rank the remaining words.  
(An accuracy of 55% for the top 50 proposed parts 
was reported.)  We found that these patterns can be 
used to collect other sorts of 'attributes', as well. 
2.2 Web Data Collection through Google 
In recent years there has been growing evidence 
that using the Web as a corpus greatly reduces the 
problem of data sparseness, and its size more than 
compensates the lack of balance (e.g., (Keller and 
Lapata, 2003)).  The benefits from using the Web 
over even large corpora like the BNC for 
extracting semantic relations, particularly when 
using simple text patterns, were informally pointed 
out in (Poesio, 2003) and demonstrated more 
systematically by Markert et al(submitted).  These 
findings were confirmed by our experiments.  A 
comparison of numbers of instances of some 
patterns using the Web and the BNC is shown in 
Table 1. 
Pattern Web BNC 
"the * of the *" 23,100,000 208,155
"the * of the * is" 10,900,000 3,627 
"the * of the car is" 26,400 5 
A
tt
ri
bu
te
 
"the * of the hat is" 2,770 1 
"the fast * is" 38,100 3 
"an electronic * is" 120,000 5 
"the * car is" 84,500 24 V
al
ue
 
"the * hat is" 17,100 1 
Table 1:  Comparison of frequencies of some 
patterns in BNC and the Web.  Web frequency is 
based on Google counts 
We collect our data from the Web using the 
Google search engine, accessed via the freely 
available Google Web API1.  The API only allows 
to retrieve the first 1,000 results per search request; 
to overcome this restriction, we use the daterage 
feature of the Google search request.  This feature 
allows the user to fragment the search space into a 
number of periods, hence retrieving only pages that 
have been updated during a specified period. In the 
two experiments presented here, we aimed to 
collect up to 10,000 matches per search request 
using the daterage feature: we divided the search 
space into 100 days starting from January, 1990 
until mid 2004.  (The procedure we used does not 
guarantee collecting all the instances in the 
accessed periods, because if there are more than 
                                                     
1 Google Web API is available on the Web at 
http://www.google.com/apis/ 
1,000 instances in one period, then only the first 
1,000 instances will be collected.) 2 
Our requests to Google take the general form "s1 
* s2" (including the double quotes), where s1 and s2 
are two strings and the wildcard denotes an 
unspecified single word.  For example, the search 
request "a * car is" catches instances such as: [a 
red car is], [a small car is], and [a sport car is]. It 
is worth mentioning that Google does not pay 
attention to punctuation marks; this is one area in 
which parsing would help. 
When receiving results from Google, we do not 
access the actual Web pages, but instead we 
process the snippets that are returned by Google.3 
2.3 Clustering Methods 
The task that we use to compare concept 
descriptions is lexical acquisition via clustering. 
We experimented with clustering systems such as 
COBWEB (Fisher, 1987) and SUBDUE (Cook and 
Holder, 2000) before settling on CLUTO 2.1 
(Karypis, 2002).  CLUTO is a general-purpose 
clustering tool that implements three different 
clustering algorithms: partitional, agglomerative, 
and graph partitioning algorithms.  CLUTO 
produces both flat and hierarchical clusters.  It uses 
a hard clustering technique, where each concept 
can be assigned to only one cluster.  The software 
allows to choose a similarity metric between a set 
including extended Jaccard and cosine.  CLUTO 
was optimized to cluster data of large sizes in a 
reasonable time.  The software also provides 
analysis and visualization tools. 
In this paper, we use extended Jaccard, which 
was found to produce more accurate results than 
the cosine function in similar tasks (Karypis, 2002; 
Curran and Moens, 2003).  In CLUTO, the 
extended Jaccard function works only with the 
graph partitioning algorithm. 
2.4 Evaluation Measures 
We used two types of measures to evaluate the 
clusters produced by CLUTO using the concept 
descriptions discussed above, both of which 
compare the clusters produced by the system to 
model clusters. Accuracy is computed by dividing 
the number of correctly clustered concepts by the 
total number of concepts.  The number of correctly 
clustered concepts is determined by examining 
                                                     
2 Also, registered users of the API can send up to 1,000 
requests per day, but our daily limit was increased by 
Google to 20,000 requests per day. 
3 Snippets are text excerpts captured from the actual 
web pages with embedded HTML tags.  We process the 
snippets by removing the HTML tags and extracting the 
targeted piece of text that was specified in the request. 
each system cluster, finding the class of each 
concept in the model clusters, and determining the 
majority class. The cluster is then labeled with this 
class;   the concepts belonging to it are taken to be 
correctly clustered, whereas the remaining 
concepts are judged to be incorrectly clustered. 
In the contingency table evaluation (Swets, 
1969; Hatzivassiloglou and McKeown, 1993), the 
clusters are converted into two lists (one for the 
system clusters and one for the model clusters) of 
yes-no answers to the question "Does the pair of 
concepts occur in the same cluster?" for each pair 
of concepts.  A contingency table is then built, 
from which recall (R), precision (P), fallout, and F 
measures can be computed.  For example, if the 
model clusters are: (A, B, C) and (D), and the 
system clusters are: (A, B) and (C, D), the yes-no 
lists are as in Table 2, and the contingency table is 
as in Table 3.   
Question Model Answer 
System 
Answer 
Does the pair (A, B) occur in 
the same cluster? Yes Yes 
Does the pair (A, C) occur in 
the same cluster? Yes No 
Does the pair (A, D) occur in 
the same cluster? No No 
Does the pair (B, C) occur in 
the same cluster? Yes No 
Does the pair (B, D) occur in 
the same cluster? No No 
Does the pair (C, D) occur in 
the same cluster? No Yes 
Table 2: Model and the system answers for the 
co-occurrence question 
Model Answer System Answer 
 Yes  No 
a b Yes 
 
1 
 
1 
c d No 
 
2 
 
2 
Table 3: The contingency table 
33.0
ca
aR ?+=  50.0ba
aP =+=  
33.0
db
bFallout ?+=  40.0PR
PR2F ?+
??=  
3 First Experiment: Using a Set of Concepts 
from Lund and Burgess 
One limitation of using Google is that even with 
an increased daily limit of 20,000, it wouldn?t 
really be feasible to attempt to cluster, say, all of 
WordNet 100,000 noun concepts. For this reason, 
we used much smaller sets of concepts in our two 
experiments. The first set alowed us to compare 
our results with those obtained by Lund and 
Burgess (1996); the second set consisted of a larger 
number of concepts from WordNet. 
Lund and Burgess (1996) used a set of 34 
concepts belonging to 3 different classes (animals, 
body parts, and geographical locations) to evaluate 
their method for acquiring lexical representations, 
HAL (Hyperspace Analogue to Language). Lund 
and Burgess were able to correctly cluster all of the 
concepts except for one body part, tooth, which 
was incorrectly clustered with animals.  In this first 
experiment, we used the 34 Lund and Burgess 
concepts plus Italy, horse, and tongue (37 in total) 
to compare value-based and attribute-based 
description when used for clustering, using concept 
descriptions collected using the methods described 
above.   
The input to clustering is a frequency table with 
concepts as rows and values, attributes, or both 
attributes and values as columns.  Each cell in the 
table contains the frequency of co-occurrence 
between the concept and corresponding value or 
attribute.  Before clustering, the frequencies are 
transformed into weighted values using the t test 
(Manning and Schutze, 1999).  (The t test was 
found by Curran and Moens (2002) to be the best 
weighting method.)  The t test formula we used for 
attributes is shown below: 
2
ji
2
jiji
j,i
N
)attribute,concept(C
N
)attribute(C)concept(C
N
)attribute,concept(C
t
???
?
???
? ??
?
 
(1)
where N is the total number of relations, and C is a 
count function.  The values formula is similar. 
We use the CLUTO vcluster command for 
clustering, with parameters: similarity function = 
Extended Jaccard Coefficient, clustering method = 
Graph Partitioning, no. of clusters = 3. 
Vector Size4 
Used Data 
500 1522 3044 4753 4969 
Values  
Only 64.86% 94.59% - - 94.59%
Attributes 
Only 97.30% 97.30% - 97.30% - 
Attributes1522 
and Values1522 
- - 100.00% - - 
Table 4: Clustering accuracy with  values, 
attributes, and their combination, using different 
vector sizes 
                                                     
4 Here, we choose the top k features by their overall 
frequency. 
Table 4 shows the accuracy of the produced 
clusters when using values, attributes, and the 
combination with different vector sizes.  The 
results show that with concept descriptions of 
length 500, attributes (97.30%) are much more 
accurate than values (64.86%).  With vectors of 
size 1522, the accuracy with attributes remains the 
same, while the accuracy with values improves, 
but is still lower than the accuracy with attributes 
(94.59%).  This indicates that attributes have more 
discriminatory power than values: an attribute 
vector of size 500 is sufficient to produce a more 
accurate result than using a value vector of three 
times the size. But perhaps the most interesting 
result is that even though further  increasing the 
size of pure attribute- and value- descriptions (to 
4753 and 4969, respectively) does not improve 
accuracy, perfect accuracy can be obtained by 
using vectors  of length 3044, including the 1522 
best attributes and the 1522 best values.  This 
suggests that while attributes are a good way of 
generalizing across properties, not all properties of 
concepts can be viewed as attribute/value pairs 
(section 5; also (Poesio and Almuhareb, 
submitted)). 
4 Second Experiment: Using a Set of 
Concepts from WordNet 
In order to get a more realistic evaluation and a 
better comparison with work such as (Lin, 1998; 
Curran and Moens, 2002), we also ran a second 
experiment using a larger set of concepts from the 
WordNet noun hierarchy (Fellbaum, 1998).  We 
chose 214 relatively common concepts from 13 
different classes covering a variety of 
subhierarchies (see Appendix A).  Each class 
contains a set of concepts that share a common 
hypernym in the WordNet hierarchy. 
Model Answer Systems Answer 
Yes No 
Yes 1294 503 Boolean 
No 387 20607 
Yes 1117 950 Frequency No 564 20160 
Table 5: The contingency table based on boolean 
and frequency for the combined attributes and 
values 
The frequencies for attributes and values were 
again collected as in the first experiment.  
However, these data were used in a different way.  
In determining the weight, we performed the t test5 
on boolean values instead of the original 
                                                     
5 We consider only positive values of t. 
frequencies6, treating all positive frequencies as 1 
and everything else as 0.  This eliminates the effect 
of variations in frequencies in the original data, the 
intuition being that frequencies do not add to the 
semantics of concepts: what we are interested in is 
the fact that a concept has a given attribute/value, 
regardless of how many times we have 
encountered this fact.  This approach is similar to 
the approach adopted in (Hearst, 1998); see also 
(Curran and Moens, 2002) for a comparison of 
methods dealing with concept vectors based on 
raw frequencies or boolean values.  The 
transformed table is a binary table that contains 
only zeros and ones in its cells.  Table 5 shows the 
contingency table for clusters produced based on 
boolean and frequency for the combined data of 
attributes and values; it shows that boolean data is 
more accurate in the four cases.  
For clustering, as well, we used CLUTO in a 
different way.  Instead of asking CLUTO to 
compute the similarities between the concepts, we 
computed them ourselves, using the version of the 
extended Jaccard similarity function used by 
Curran and Moens, as this version produces better 
results than the one used in CLUTO.  The two 
versions of the extended Jaccard function are 
shown below: 
 
where tm,i and tn,i are the weighted co-occurrence 
values between concept m and concept n with 
attribute/value i, and computed as in equation (1). 
Measures Used 
Data7 Accuracy Recall Precision Fallout F 
Values 
Only 71.96% 58.48% 52.91% 04.14% 55.55%
Attributes 
Only 64.02% 59.90% 53.54% 04.14% 56.54%
Attributes  
And Values 85.51% 76.98% 72.01% 02.38% 74.41%
Table 6: Clustering evaluation based on values, 
attributes, and the combination 
We compute the similarity between each pair of 
concepts, produce a similarity matrix and send it to 
CLUTO for clustering.  We then call the scluster 
                                                     
6 In equation (1), this will effect only C(concepti, 
attributej), other counts will not be effected. 
7 Here, we use full size vectors that contain all the 
features. 
command of CLUTO with the following 
parameters: clustering method = Graph 
Partitioning, no. of clusters = 13. The results of the 
evaluation are shown in Table 6.   
Value-based concept descriptions resulted in 
better clusters than attribute-based when measured 
using Accuracy (71.96% vs. 64.02%), but the other 
measures all indicate that attributes work slightly 
better than values: e.g., F=55.55% for values, 
56.64% for attributes.  The reason for this 
difference is that the Accuracy measure simply 
evaluates if each concept is assigned to its correct 
cluster, while the remaining measures concern 
about the relation between each pair of concepts 
(i.e., if they were assigned to the same cluster or 
not).  But, just as in Experiment 1, the best results 
by any measure are again obtained when using 
concept descriptions containing the best 'attributes' 
and the best 'values'; this time, however, the 
difference is much more significant: Accuracy is 
85.51%, F is 74.41%.  
Model Cluster 
Sy
st
em
 C
lu
st
er
 
B
ui
ld
in
g 
D
is
ea
se
 
V
eh
ic
le
 
Fe
el
in
g 
B
od
y 
Pa
rt
 
Fr
ui
t 
C
re
at
or
 
Pu
bl
ic
at
io
n 
A
ni
m
al
 
Fu
rn
itu
re
 
C
lo
th
 
F.
 R
el
at
io
n 
T
im
e 
1 0 2 0 11 0 0 0 0 0 0 0 0 0 
2 0 0 13 0 0 0 0 0 0 0 0 0 0 
3 0 0 0 0 0 0 0 0 0 0 0 0 17
4 0 0 0 0 0 0 2 0 18 0 0 6 0 
5 2 0 0 1 0 16 0 0 1 0 0 1 0 
6 1 16 0 0 0 0 0 0 0 0 0 0 1 
7 0 0 0 0 0 0 15 0 0 0 0 0 0 
8 0 0 0 0 0 0 0 15 0 0 0 0 0 
9 0 0 0 0 16 0 0 0 0 4 0 0 1 
10 0 0 0 0 0 0 0 0 1 0 0 9 0 
11 1 0 1 0 0 0 0 0 0 6 0 0 0 
12 0 0 0 0 0 0 0 0 0 2 16 0 0 
13 15 0 0 1 0 0 0 1 0 2 0 0 0 
Table 7: The confusion matrix for the clusters 
produced using both attributes and values 
Table 7 shows the confusion matrix for the 
clusters produced using both attributes and values.  
A close inspection of these clusters reveals that 
'furniture' concepts were the less homogeneous 
because they were scattered among four different 
clusters. There are 14 'furniture' concepts; six of 
them (bookcase, cabinet, couch, cradle, desk and 
wardrobe) were grouped in a separate cluster 
which also contains two more concepts (pickup 
and greenhouse).  Four of the concepts (bed, 
lamp, seat, and table) were clustered with 'body 
part' concepts.  Two of the concepts (dresser and 
sofa) were clustered with 'cloth' concepts, and the 
remaining two concepts (chair and lounge) were 
clustered with 'building' concepts. 
?
?
?
?
??+
?
=
+
?
=
i
i,ni,mi,ni,m
i
i,ni,m
nmCLUTO
i
i,ni,m
i
i,ni,m
nmMoens&Curran
))tt(tt(
)tt(
)concept,concept(sim
)tt(
)tt(
)concept,concept(sim
Two points should be noted about the furniture 
concepts.  First, at least two concepts (seat and 
lounge) have more than one sense in WordNet.  
Seat was clustered with body part concepts, which 
is acceptable if we think of seat as "the fleshy part 
of the human body that you sit on" (WordNet, 
sense 2).  The same for lounge, which was 
clustered with buildings, which is consistent with 
its second sense in WordNet: "a public room (as in 
a hotel or airport) with seating where people can 
wait".  This indicates that techniques for 
differentiating between different senses are needed 
? e.g., using a soft clustering technique as in 
(Pereira et al 1993) instead of a hard clustering 
technique.  Second, furniture concepts may not 
have a common prototype that is shared by all of 
the member concepts.  This is a well known 
problem in the prototype theory of concepts 
(Laurence and Margolis, 1999). 
The greater compactness of attribute-based 
representations vs. value-based ones was more 
evident in this second experiment. We collected 
51,045 distinct values and 8,934 distinct attributes; 
the total number of value-concept relations is 
1,026,335, compared to 422,621 attribute-concept 
relations. 
5 Attributes and Values: A discussion 
Although our results suggest that trying to 
identify attributes is beneficial, the notion of 
'attribute' is not completely clear, and has been 
used in widely different ways in Knowledge 
Representation literature.   An attempt of defining 
the notion has been made by Guarino (1992), who 
classifies attributes into relational and non-
relational attributes.  Relational attributes include 
qualities such as color and position, and relational 
roles such as son and spouse.  Non-relational 
attributes include parts such as wheel and engine.  
The Qualia Structure of the Generative Lexicon 
(Pustejovsky, 1991) is another attempt at 
identifying "the essential attributes of an object as 
defined by the lexical item".  Pustejovsky 
identifies four roles: Constitutive Role (Guarino's 
parts), Formal Role (Guarino's qualities), Agentive 
Role (Guarino's relational roles), and Telic Role 
(not included in Guarino's classification). 
Our analysis of the attribute data shows that the 
attributes we found can be mapped in the four roles 
of the Qualia structure.  Table 8 shows how we 
manually mapped the top 50 attributes of the 
concept car to the Qualia roles and the Guarino's 
classes. This mapping is not trivial (e.g., a path is 
not part of a car, and design can be regarded as a 
quality), but a variety of tests may help: 
Morphological and Ontological Tests:  
Dixon (1991) proposed a semantic classification 
for nouns.  According to Dixon, parts are concrete 
concepts and mostly basic noun roots or rarely 
derived from verbs, while qualities are abstract 
concepts and many of them are basic noun roots or 
derived from adjectives, some derived from stems, 
and few derived from verbs.  Our observations also 
suggest that telic attributes are usually derived 
from verbs. 
Attributes Test:  Since attributes can also be 
viewed as concepts (e.g., in WordNet), they 
themselves should have some shared attributes.  
For example: since parts are concrete objects they 
should share attributes such as size, length, and 
geometry.  Also, since qualities usually can be 
assigned values (e.g. age (25)), then they should 
share attributes such as range and average. 
Question Type Test:  Different types of 
attributes tend to occur with different types of 
questions.  For example, relational role attributes 
tend to occur with who-questions like "Who is the 
driver of the car?" and "Who is the manufacturer of 
the car?" 
Guarino 
Class 
Qualia 
Role Car Attributes 
Part Constitutive Role 
front, rear, interior, inside, side, 
body, trunk, exterior, underside, 
hood, back, nose, roof, engine, 
frame, floor, rest, silhouette, 
backseat, wheelbase, battery, 
chassis, path 
Quality Formal Role 
speed, value, weight, price, 
velocity, color, condition, 
momentum, convenience, 
propulsion, look, inertia, state, 
model, history, balance, motion, 
performance 
Relational 
Role 
Agentive 
Role driver, owner 
- Telic Role8 
handling, use, search, design, 
benefit 
Table 8: The classification of the top 50 
attributes of the concept car 
In future work, we plan to use some of these 
tests to classify attributes, and possibly filter some 
of them; this might improve the discrimination 
power of attributes. Also, concepts may share 
certain Qualia, but differ in other respects: for 
example, the chair concept and the man concept 
share some parts (e.g., arm, back, leg, and seat) 
and even some qualities (e.g., color, size, and 
shape) but differ in other levels (i.e., Agentive 
Role, and Telic Role). 
                                                     
8 Telic roles define purposes, functions, and activities 
that are related to the concept. Some valid telic roles for 
the concept car would be: driving, selling, and buying. 
6 Conclusions 
Simple text patterns were used to automatically 
extract both basic value-based and attribute-based 
concept descriptions for clustering purposes. Our 
preliminary results suggest, first of all, that when 
large amounts of data such as the Web are 
accessed, these simple patterns may be sufficient to 
compute descriptions rich enough to discriminate 
quite well, at least with small sets of concepts 
belonging to clearly distinct classes. Secondly, we 
found that even though attributes are fewer than 
values, attribute-based descriptions need not be as 
long as value-based ones to achieve as good or 
better results. Finally, we found that the best 
descriptions included both attributes and more 
general properties. We plan to extend this work 
both by refining our notion of attribute and by 
using more sophisticated patterns working off the 
output of a parser. 
7 Acknowledgement 
Abdulrahman Almuhareb is supported by King 
Abdulaziz City for Science and Technology 
(KACST), Riyadh, Saudi Arabia. We want to 
thank Google for making their Web API available 
to the research community and George Karypis for 
the CLUTO clustering toolkit. 
References 
M. Berland and E. Charniak. 1999. Finding parts in 
very large corpora. In Proc. of the 37th  ACL, 
pages 57?64, University of Maryland. 
R. J. Brachman and H. J. Levesque, editors. 1985. 
Reading in Knowledge Representation. Morgan 
Kaufmann, California. 
S. A. Caraballo. 1999. Automatic construction of a 
hypernym-labeled noun hierarchy from text. In 
Proc. of  the 37th  ACL. 
D. J. Cook and L. B. Holder. 2000. Graph-based 
data mining. IEEE Intelligent Systems, 15(2), 32-
41. 
J. R. Curran and M. Moens. 2002. Improvements 
in automatic thesaurus extraction. In Proc. of the 
ACL Workshop on Unsupervised Lexical 
Acquisition, pages 59?66.  
R. M. W. Dixon. 1991. A New Approach to 
English Grammar, on Semantic Principles. 
Clarendon Press, Oxford. 
C. Fellbaum, editor. 1998. WordNet: An electronic 
lexical database. The MIT Press. 
D. H. Fisher. 1987. Knowledge acquisition via 
incremental conceptual clustering. Machine 
Learning, 2:139?172. 
G. Grefenstette. 1993. SEXTANT: Extracting 
semantics from raw text implementation details. 
Heuristics: The Journal of Knowledge 
Engineering.  
N. Guarino. 1992. Concepts, attributes and 
arbitrary relations: some linguistic and 
ontological criteria for structuring knowledge 
base. Data and Knowledge Engineering, 8, pages 
249?261. 
V. Hatzivassiloglou and K. McKeown. 1993. 
Towards the automatic identification of 
adjectival scales: clustering adjectives according 
to meaning. In Proc. of the 31st ACL, pages 172?
182. 
M. A. Hearst. 1998. Automated discovery of 
WordNet relations. In C. Fellbaum, editor, 
WordNet: An Electronic Lexical Database. MIT 
Press. 
G. Karypis. 2002. CLUTO: A clustering toolkit. 
Technical Report 02-017, University of 
Minnesota. Available at URL: http://www-
users.cs.umn.edu/~karypis/cluto/. 
F. Keller and M. Lapata. 2003. Using the Web to 
obtain frequencies for unseen bigrams. 
Computational Linguistics, 29(3). 
A. Kilgarriff and H. Schuetze. 2003. Introduction 
to the special issue of Computational Linguistics 
on the web as a corpus. Computational 
Linguistics. 
A. Kilgarriff. 2003. Thesauruses for Natural 
Language Processing. In Proc. of the IEEE 2003 
International Conference on Natural Language 
Processing and Knowledge Engineering (NLP-
KE'03), Beijing. 
S. Laurence and E. Margolis. 1999. Concepts and 
Cognitive Science. In E. Margolis and S. 
Laurence, editors, Concepts: Core Readings. 
Cambridge, MA., Bradford Books/MIT Press, 
pages 3-81. 
D. Lin. 1998. Automatic retrieval and clustering of 
similar words. In Proc. of COLING-ACL, 768-
774. 
K. Lund and C. Burgess. 1996. Producing high-
dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, 
Instrumentation, and Computers, 28,  203-208.   
C. D. Manning and H. Schuetze. 1999. 
Foundations of Statistical NLP. MIT Press. 
K. Markert, M. Nissim, and N. Modjeska. 2004. 
Comparing Knowledge Sources for Nominal 
Anaphora Resolution. Submitted. 
F. Pereira, N. Tishby, and L. Lee. 1993. 
Distributional clustering of English words. In 
Proc. of the 31st ACL, pages 183-190, 
Columbus, Ohio. 
M. Poesio and A. Almuhareb. 2004. Feature-based 
vs. Property-based KR: An Empirical 
Perspective. Submitted. 
M. Poesio, T. Ishikawa, S. Walde, and R. Vieira. 
2002. Acquiring lexical knowledge for anaphora 
resolution. In Proc. of  LREC, Las Palmas, June. 
M. Poesio. 2003. Associative descriptions and 
salience. In Proc. of the EACL Workshop on 
Computational Treatments of Anaphora, 
Budapest. 
J. Pustejovsky. 1991. The generative lexicon. 
Computational Linguistics, 17(4), pages 409-
441. 
J. A. Swets. 1969. Effectiveness of Information 
Retrieval Methods. American Documentation, 
20, pages 72-89. 
W. A. Woods. 1975. What?s in a link: Foundations 
for semantic networks. In Daniel G. Bobrow and 
Alan M. Collins, editors, Representation and 
Understanding: Studies in Cognitive Science, 
pages 35-82. Academic Press, New York. 
Appendix A. The 214 Concepts from the 13 WordNet Classes Used in Experiment 2 
Class Concepts 
Animal bear, bull, camel, cat, cow, deer, dog, elephant, horse, kitten, lion, monkey, mouse, oyster, puppy, rat, sheep, tiger, turtle, zebra 
Building abattoir, center, clubhouse, dormitory, greenhouse, hall, hospital, hotel, house, inn, library, nursery, restaurant, school, skyscraper, tavern, theater, villa, whorehouse 
Cloth pants, blouse, coat, costume, gloves, hat, jacket, jeans, neckpiece, pajamas, robe, scarf, shirt, suit, trousers, uniform 
Creator architect, artist, builder, constructor, craftsman, designer, developer, farmer, inventor, maker, manufacture, musician, originator, painter, photographer, producer, tailor 
Disease acne, anthrax, arthritis, asthma, cancer, cholera, cirrhosis, diabetes, eczema, flu, glaucoma, hepatitis, leukemia, malnutrition, meningitis, plague, rheumatism, smallpox 
Feeling anger, desire, fear, happiness, joy, love, pain, passion, pleasure, sadness, sensitivity, shame, wonder 
Fruit apple, banana, berry, cherry, grape, kiwi, lemon, mango, melon, olive, orange, peach, pear, pineapple, strawberry, watermelon  
Furniture bed, bookcase, cabinet, chair, couch, cradle, desk, dresser, lamp, lounge, seat, sofa, table, wardrobe 
Body Part ankle, arm, ear, eye, face, finger, foot, hand, head, leg, nose, shoulder, toe, tongue, tooth, wrist 
Publication atlas, book, booklet, brochure, catalog, cookbook, dictionary, encyclopedia, handbook, journal, magazine, manual, phonebook, reference, textbook, workbook  
Family 
Relation  
boy, child, cousin, daughter, father, girl, grandchild, grandfather, grandmother, husband, 
kid, mother, offspring, sibling, son, wife  
Time century, decade, era, evening, fall, hour, month, morning, night, overtime, quarter, season, semester, spring, summer, week, weekend, winter, year 
Vehicle aircraft, airplane, automobile, bicycle, boat, car, cruiser, helicopter, motorcycle, pickup, rocket, ship, truck, van 
 
Proceedings of the ACL-SIGLEX Workshop on Deep Lexical Acquisition, pages 18?27,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Identifying Concept Attributes Using a Classifier 
Massimo Poesio  
University of Essex 
Computer Science /  
Language and Computation 
poesio at essex.ac.uk 
Abdulrahman Almuhareb 
University of Essex 
Computer Science /  
Language and Computation  
aalmuh at essex.ac.uk 
 
 
Abstract 
We developed a novel classification of 
concept attributes and two supervised 
classifiers using this classification to iden-
tify concept attributes from candidate at-
tributes extracted from the Web. Our 
binary (attribute / non-attribute) classifier 
achieves an accuracy of 81.82% whereas 
our 5-way classifier achieves 80.35%. 
1 Introduction 
The assumption that concept attributes and, more 
in general, features1 are an important aspect of 
conceptual representation is widespread in all dis-
ciplines involved with conceptual representations, 
from Artificial Intelligence / Knowledge Represen-
tation (starting with at least (Woods, 1975) and 
down to (Baader et al 2003)), Linguistics (e.g., in 
the theories of the lexicon based on typed feature 
structures and/or Pustejovsky?s Generative Lexi-
con theory: (Pustejovsky 1995)) and Psychology 
(Murphy 2002, Vinson et al2003).  This being the 
case, it is surprising how little attention has been 
devoted to this aspect of lexical representation in 
work on large-scale lexical semantics in Computa-
tional Linguistics. The most extensive resource at 
                                                          
1 The term attribute is used informally here to indicate the 
type of relational information about concepts that is expressed 
using so-called roles in Description Logics (Baader et al 
2003)?i.e., excluding IS-A style information (that cars are 
vehicles, for instance).  It is meant to be a more restrictive 
term than the term feature, often used to indicate any property 
of concepts, particularly in Psychology. We are carrying out a 
systematic analysis of the sets of features used in work such as 
(Vinson et al 2003) (see Discussion).  
our disposal, WordNet (Fellbaum, 1998) contains 
very little information that would be considered as 
being about ?attributes??only information about  
parts, not about qualities such as height, or even to 
the values of such attributes in the adjective net-
work?and this information is still very sparse. On 
the other hand, the only work on the extraction of 
lexical semantic relations we are aware of has con-
centrated on the type of relations found in Word-
Net: hyponymy (Hearst, 1998; Caraballo, 1999) 
and meronymy (Berland and Charniak, 1999; Poe-
sio et al 2002).2 
The work discussed here could be perhaps best 
described as an example of empirical ontology: 
using linguistics and philosophical ideas to im-
prove the results of empirical work on lexical / on-
tology acquisition, and vice versa, using findings 
from empirical analysis to question some of the 
assumptions of theoretical work on ontology and 
the lexicon. Specifically, we discuss work on the 
acquisition of (nominal) concept attributes whose 
goal is twofold: on the one hand, to clarify the no-
tion of ?attribute? and its role in lexical semantics, 
if any; on the other, to develop methods to acquire 
such information automatically (e.g., to supple-
ment WordNet).  
The structure of the paper is as follows. After a 
short review of relevant literature on extracting 
semantic relations and on attributes in the lexicon, 
we discuss our classification of attributes, followed 
by the features we used to classify them. We then 
discuss our training methods and the results we 
achieved.  
                                                          
2 In work on the acquisition of lexical information about verbs 
there has been some work on the acquisition of thematic roles, 
(e.g., Merlo and Stevenson, 2001). 
18
2 Background 
2.1 Using Patterns to Extract Semantic Rela-
tions 
The work discussed here belongs to a line of re-
search attempting to acquire information about 
lexical and other semantic relations other than 
similarity / synonymy by identifying syntactic 
constructions that are often (but not always!) used 
to express such relations. The earliest work of this 
type we are aware of is the work by Hearst (1998) 
on acquiring information about hyponymy (= IS-A 
links) by searching for instances of patterns such as  
NP {, NP}* or other NP 
(as in, e.g., bruises ?. broken bones and other 
INJURIES).   A similar approach was used by Ber-
land and Charniak (1999) and Poesio et al(2002) 
to extract information about part-of relations using 
patterns such as 
the N of the N is ?. 
(as in the wheel of the CAR is) and by Girju and 
Moldovan (2002) and Sanchez-Graillet and Poesio 
(2004) to extract causal relations. In previous work 
(Almuhareb and Poesio, 2004) we used this same 
approach to extract attributes, using the pattern  
?the * of the C [is|was]? 
(suggested by, e.g., (Woods, 1975) as a test for 
?attributehood?) to search for attributes of concept 
C in the Web, using the Google API. Although the 
information extracted this way proved a useful ad-
dition to our lexical representations from a cluster-
ing perspective, from the point of view of lexicon 
building this approach results in too many false 
positives, as very few syntactic constructions are 
used to express exclusively one type of semantic 
relation. For example, the ?attributes? of deer ex-
tracted using the text pattern above include ?the 
majority of the deer,? ?the lake of the deer,? and 
?the picture of the deer.? Girju and Moldovan 
(2002) addressed the problem of false positives for 
causal relations by developing WordNet-based fil-
ters to remove unlikely candidates. In this work, 
we developed a semantic filter for attributes based 
on a linguistic theory of attributes which does not 
rely on WordNet except as a source of morpho-
logical information (see below). 
2.2 Two Theories of Attributes 
The earliest attempt to classify attributes and other 
properties of substances we are aware of goes back 
to Aristotle, e.g., in Categories,3 but our classifica-
tion of attributes was inspired primarily by the 
work of Pustejovsky (1995) and Guarino (e.g., 
(1992)). According to Pustejovsky?s Generative 
Lexicon theory (1995), an integral part of a lexical 
entry is its Qualia Structure, which consists of 
four ?roles?:4 the Formal Role, specifying what 
type of object it is: e.g., in the case of a book, that 
it has a shape, a color, etc.; the Constitutive Role, 
specifying the stuff and parts that it consists of 
(e.g., in the case of a book,  that it is made of pa-
per, it has chapters and an index, etc.); the Telic 
Role, specifying the purpose of the object (e.g., in 
the case of a book, reading);  and the Agentive 
Role, specifying how the object was created (e.g., 
in the case of a book, by writing).   
Guarino (1992) argues that there are two types 
of attributes: relational and non-relational. Rela-
tional attributes include qualities such as color and 
position, and relational social roles such as son 
and spouse. Non-relational attributes include parts 
such as wheel and engine. Activities are not 
viewed as attributes in Guarino?s classification. 
3 Attribute Extraction and Classification 
The goal of this work is to identify genuine attrib-
utes by classifying candidate attributes collected 
using text patterns as discussed in (Almuhareb and 
Poesio, 2004) according to a scheme inspired by 
those proposed by Guarino and Pustejovsky.  
The scheme we used to classify the training 
data in the experiment discussed below consists of 
six categories:   
? Qualities: Analogous to Guarino?s qualities 
and Pustejovsky?s formal ?role?. (E.g., ?the 
color of the car?.) 
? Parts: Related to Guarino?s non-relational 
attributes and Pustejovsky?s constitutive 
?roles?. (E.g., ?the hood of the car?). 
? Related-Objects: A new category intro-
duced to cover the numerous physical ob-
jects which are ?related? to an object but are 
not part of it?e.g., ?the track of the deer?. 
                                                          
3 E.g., http://plato.stanford.edu/entries/substance. Thanks to 
one of the referees for drawing our attention to this. 
4 ?Facets? would be perhaps a more appropriate term to avoid 
confusions with the use of the term ?role? in Knowledge Rep-
resentation. 
19
? Activities: These include both the types of 
activities which are part of Pustejovsky?s 
telic ?role? and those which would be in-
cluded in his agentive ?role?. (E.g., ?the re-
pairing of the car?.) 
? Related-Agents: For the activities in which 
the concept in question is acted upon, the 
agent of the activity: e.g., ?the writer of the 
book?, ?the driver of the car?. 
? Non-Attributes: This category covers the 
cases in which the construction ?the N of the 
N? expresses other semantic relations, as in: 
?the last of the deer?, ?the majority of the 
deer,? ?the lake of the deer,? and ?in the 
case of the deer?. 
We will quickly add that (i) we do not view this 
classification as definitive?in fact, we already 
collapsed the classes ?part? and ?related objects?  in 
the  experiments discussed below?and (ii) not all 
of these distinctions are very easy even for human 
judges to do.  For example, design, as an attribute 
of a car, can be judged to be a quality if we think 
of it as taking values such as modern and standard; 
on the other hand, design might also be viewed as 
an activity in other contexts discussing the design-
ing process. Another type of difficulty is that a 
given attribute may express different things for 
different objects. For example, introduction is a 
part of a book, and an activity for a product. An 
additional difficulty results from the strong similar-
ity between parts and related-objects. For example, 
?key? is a related-object to a car but it is not part 
of it. We will return to this issue and to agreement 
on this classification scheme when discussing the 
experiment. 
One difference from previous work is that we 
use additional linguistic constructions to extract 
candidate attributes. The construction ?the X of the 
Y is? used in our previous work is only one exam-
ple of genitive construction. Quirk et al(1985) list 
eight types of genitives in English, four of which 
are useful for our purposes:  
? Possessive Genitive: used to express quali-
ties, parts, related-objects, and related-
agents. 
? Genitive of Measure: used to express quali-
ties. 
? Subjective & Objective Genitives: used to 
express activities. 
We used all of these constructions in the work 
discussed here.  
4 Information Used to Classify Attributes 
Our attribute classifier uses four types of informa-
tion: morphological information, an attribute 
model, a question model, and an attributive-usage 
model. In this section we discuss how this informa-
tion is automatically computed.  
4.1 Morphological Information 
Our use of morphological information is based on 
the noun classification scheme proposed by Dixon 
(1991). According to Dixon, derivational morphol-
ogy provides some information about attribute typ-
e. Parts are concrete objects and almost all of them 
are expressed using basic noun roots (i.e., not de-
rived from adjectives or verbs). Most of qualities 
and properties are either basic noun roots or de-
rived from adjectives. Finally, activities are mostly 
nouns derived from verbs. Although these rules 
only have a heuristic value, we found that morpho-
logically based heuristics did provide useful cues 
when used in combination with the other types of 
information discussed below.  
As we are not aware of any publicly available 
software performing automatic derivational mor-
phology, we developed our own (and very basic) 
heuristic methods. The techniques we used involve 
using information from WordNet, suffix-checking, 
and a POS tagger. 
WordNet was used to find nouns that are de-
rived from verbs and to filter out words that are not 
in the noun database. Nouns in WordNet are linked 
to their derivationally related verbs, but there is no 
indication about which is derived from which. We 
use a heuristic based on length to decide this: the 
system checks if the noun contains more letters 
than the most similar related verb. If this is the 
case, then the noun is judged to be derived from 
the verb. If the same word is used both as a noun 
and as a verb, then we check the usage familiarity 
of the word, which can also be found in WordNet. 
If the word is used more as a verb and the verbal 
usage is not rare, then again the system treats the 
noun as derived from the verb. 
20
To find nouns that are derived from adjectives 
we used simple heuristics based on suffix-
checking. (This was also done by Berland and 
Charniak (1999).) All words that end with ?ity? or 
?ness? are considered to be derived from adjec-
tives. A noun not found to be derived from a verb 
or an adjective is assumed to be a basic noun root. 
In addition to derivational morphology, we used 
the Brill tagger (Brill, 1995) to filter out adjectives 
and other types of words that can occasionally be 
used as nouns such as better, first, and whole be-
fore training. Only nouns, base form verbs, and 
gerund form verbs were kept in the candidate at-
tribute list. 
4.2 Clustering Attributes  
Attributes are themselves concepts, at least in the 
sense that they have their own attributes: for ex-
ample, a part of a car, such as a wheel, has its own 
parts (the tyre) its qualities (weight, diameter) etc.  
This observation suggests that it should be possible 
to find similar attributes in an unsupervised fashion 
by looking at their attributes, just as we did earlier 
for concepts (Almuhareb and Poesio, 2004). In 
order to do this, we used our text patterns for find-
ing attributes to collect from the Web up to 500 
pattern instances for each of the candidate attrib-
utes. The collected data were used to build a vecto-
rial representation of attributes as done in 
(Almuhareb and Poesio, 2004).  We then used 
CLUTO (Karypis, 2002) to cluster attributes using 
these vectorial representations. In a first round of 
experiments we found that the classes ?parts? and 
?related objects? were difficult to differentiate, and 
therefore we merged them. The final model clus-
ters candidate attributes into five classes: activities, 
parts & related-objects, qualities, related-agents, 
and non-attributes. This classification was used as 
one of the input features in our supervised classi-
fier for attributes.  
We also developed a measure to identify par-
ticularly distinctive ?attributes of attributes??
attributes which have a strong tendency to occur 
primarily with attributes (or any concept) of a 
given class?which has proven to work pretty well. 
This measure, which we call Uniqueness, actually 
is the product of two factors: the degree of unique-
ness proper, i.e., the probability P(classi | attrib-
utej) that  an attribute (or, in fact, any other noun) 
will belong to class i given than it has attribute j; 
and a measure of ?definitional power? ?the prob-
ability P(attribute j | classi) that a concept belong-
ing to a given class will have a certain attribute. 
Using MLE to estimate these probabilities, the de-
gree of uniqueness of attributesj of classi is com-
puted as follows: 
 
)(
),( 2
,
ji
ji
ji attributeCn
attributeclassC
Uniqueness ?=  
 
where ni is the number of concepts in classi. C is a 
count function that counts concepts that are associ-
ated with the given attribute. Uniqueness ranges 
from 0 to 1. 
Table 1 shows the 10 most distinctive attributes 
for each of the five attribute classes, as determined 
by the Uniqueness measure just introduced, for the 
1,155 candidate attributes in the training data for 
the experiment discussed below. 
 
Class Top 10 Distinctive Attributes 
Related-Agent 
(0.39) 
identity, hands, duty, consent, 
responsibility, part, attention, 
voice, death, job 
Part &  
Related-Object 
(0.40) 
inside, shape, top, outside, sur-
face, bottom, center, front, size, 
interior 
Activity 
(0.29) 
time, result, process, results, 
timing, date, effect, beginning, 
cause, purpose 
Quality 
(0.23) 
measure, basis, determination, 
question, extent, issue, meas-
urement, light, result, increase 
Non-Attribute 
(0.18) 
content, value, rest, nature, 
meaning, format, interpretation, 
essence, size, source 
Table 1: Top 10 distinctive attributes of the five 
classes of candidate attributes. Average distinct-
iveness (uniqueness) for the top 10 attributes is 
shown between parentheses 
 
 Most of the top 10 attributes of related-agents, 
parts & related-objects, and activities are genuinely 
distinctive attributes for such classes. Thus, attrib-
utes of related-agents reflect the ?intentionality? 
aspect typical of members of this class: identity, 
duty, and responsibility. Attributes of parts are 
common attributes of physical objects (e.g., inside, 
shape). Most attributes of activities have to do with 
temporal properties and causal structure: e.g., be-
ginning, cause. The ?distinctive? attributes of the 
21
quality class are less distinctive, but four such at-
tributes (measure, extent, measurement, and in-
crease) are related to values since many of the 
qualities can have different values (e.g., small and 
large for the quality size). There are however sev-
eral attributes in common between these classes of 
attributes, emphasizing yet again how some of 
these distinctions at least are not completely clear 
cut:  e.g., result, in common between activities and 
qualities (two classes which are sometimes diffi-
cult to distinguish). Finally, as one would expect, 
the attributes of the non-attribute class are not 
really distinctive: their average uniqueness score is 
the lowest. This is because ?non-attribute? is a het-
erogeneous class. 
4.3 The Question Model 
Certain types of attributes can only be used when 
asking certain types of questions. For example, it is 
possible to ask ?What is the color of the car?? but 
not ??When is the color of the car??.  
We created a text pattern for each type of ques-
tion and used these patterns to search the Web and 
collect counts of occurrences of particular ques-
tions. An example of such patterns would be: 
? ?what is|are the A  of the? 
where A is the candidate attribute under investiga-
tion. Patterns for who, when, where, and how are 
similar.  
After collecting occurrence frequencies for all 
the candidate attributes, we transform these counts 
into weights using the t-test weighting function as 
done for all of our counts, using the following for-
mula from Manning and Schuetze (1999): 
 
 
2
2
, ),(
N
)()(),(
N
attributequestionC
attributeCquestionC
N
attributequestionC
t
ji
jiji
ji
??
?
 
where N is the total number of relations, and C is a 
count function. 
Table 2 shows the 10 most frequent attributes 
for each question type. This data was collected us-
ing a more restricted form of the question patterns 
and a varying number of instances for each type of 
questions. The restricted form includes a question 
mark at the end of the phrase and was used to im-
prove the precision. For example, the what-pattern 
would be ?what is the * of the *??. 
Question Top 10 Attributes 
what purpose, name, nature, role, cost, func-tion, significance, size, source, status 
who author, owner, head, leader, president, sponsor, god, lord, father, king 
where rest, location, house, fury, word, edge, center, end, ark, voice 
how 
quality, rest, pace, level, length, mo-
rale, performance, content, organiza-
tion, cleanliness 
when end, day, time, beginning, date, onset, running, birthday, fast, opening 
Table 2: Frequent attributes for each question type 
 
Instances of the what-pattern are frequent in the 
Web: the Google count was more than 2,000,000 
for a query issued in mid 2004. The who-pattern is 
next in terms of occurrence, with about 350,000 
instances. The when-pattern is the most infrequent 
pattern, about 5,300 instances. 
The counts broadly reflected our intuitions 
about the use of such questions. What-questions 
are mainly used with qualities, whereas who-
questions are used with related-agents. Attributes 
occurring with when-questions have some tempo-
ral aspects; attributes occurring with how-questions 
are mostly qualities and activities, and attributes in 
where-questions are of different types but some are 
related to locations. Parts usually do not occur with 
these types of questions. 
4.4 Attributive Use  
Finally, we exploited the fact that certain types of 
attributes are used more in language as concepts 
rather than as attributes. For instance, it is more 
common to encounter the phrase ?the size of the 
?? than ?the ?  of the size?. On the other hand, it is 
more common to encounter the phrase ?the * of 
the window? than ?the window of the *?. Gener-
ally speaking, parts, related-objects, and related-
agents are more likely to have more attributes than 
qualities and activities. We used the two patterns 
?the * of the A? and ?the A of the *? to collect 
Google counts for all of the candidate attributes. 
These counts were also weighted using the t-test as 
in the question model. 
Table 3 illustrates the attributive and conceptual 
usage for each attribute class using a training data 
of 1,155 attributes. The usage averages confirm the 
initial assumption.  
22
Average T-Test Score Attribute Class Conceptual Attributive
Parts & 
Related-Objects 18.81 3.00 
Non-Attributes 13.29 11.07 
Related-Agents 12.15 2.54 
Activities 3.22 5.08 
Qualities 0.23 17.09 
Table 3: Conceptual and attributive usage averages 
for each attribute class 
5 The Experiment 
We trained two classifiers: a 2-way classifier that 
simply classifies candidate attributes into attributes 
and non-attributes, and a 5-way classifier that clas-
sifies candidate attributes into activities, parts & 
related-objects, qualities, related-agents, and non-
attributes. These classifiers were trained using de-
cision trees algorithm (J48) from WEKA (Witten 
and Frank, 1999). 
 
Feature 
el
ec
tio
n 
ab
do
m
en
 
ac
id
ity
 
cr
ea
to
r 
pr
ob
le
m
 
Cluster Id 1 2 4 0 3 
What 0.00 0.00 0.00 0.00 3.80 
When 2.62 0.00 0.00 0.00 0.00 
Where 0.78 0.94 0.00 0.00 0.00 
Who 0.00 0.00 0.00 30.28 0.00 
How 2.05 0.00 1.54 0.00 2.61 
Conceptual 38.16 20.15 0.00 0.00 135.40 
Attributive 0.00 0.00 10.22 1.60 0.00 
Morph DV BN DA DV BN 
Attribute 
Class  
(Output) 
Activity Part Quality RelatedAgent 
Non- 
Attribute
Table 4: Five examples of training instances. The 
values for morph are as follows: DV: derived from 
verb; BN: basic noun; DA: derived from adjective 
 
Our training and testing material was acquired 
as follows. We started from the 24,178 candidate 
attributes collected for the concepts in the balanced 
concept dataset we recently developed (Almuhareb 
and Poesio, 2005). We threw out every candidate 
attribute with a Google frequency less than 20; this 
reduced the number of candidate attributes to 
4,728. We then removed words other than nouns 
and gerunds as discussed above, obtaining 4,296 
candidate attributes.   
The four types of input features for this filtered 
set of candidate attributes were computed as dis-
cussed in the previous section. The best results 
were obtained using all of these features. A train-
ing set of 1,155 candidate attributes was selected 
and hand-classified (see below for agreement fig-
ures). We tried to include enough samples for each 
attribute class in the training set. Table 4 shows the 
input features for five different training examples, 
one for each attribute class. 
6  Evaluation 
For a qualitative idea of the behavior of our classi-
fier, the best attributes for some concepts are listed 
in Appendix A. We concentrate here on quantita-
tive analyses. 
6.1 Classifier Evaluation 1: Cross-Validation 
Our two classifiers were evaluated, first of all, us-
ing 10-fold cross-validation. The 2-way classifier 
correctly classified 81.82% of the candidate attrib-
utes (the baseline accuracy is 80.61%). The 5-way 
classifier correctly classified 80.35% of the attrib-
utes (the baseline accuracy is 23.55%). The preci-
sion / recall results are shown in Table 5. 
 
Attribute Class P R F 
2-Way Classifier 
Attribute 0.854 0.934 0.892
Non-Attribute 0.551 0.335 0.417
5-Way Classifier 
Related-Agent 0.930 0.970 0.950
Part & Related-Object 0.842 0.882 0.862
Activity 0.822 0.878 0.849
Quality 0.799 0.821 0.810
Non-Attribute 0.602 0.487 0.538
Table 5: Cross-validation results for the two  
attribute classifiers 
 
As it can be seen from Table 5, both classifiers 
achieve good F values for all classes except for the 
non-attribute class: F-measures range from 81% to 
95%. With the 2-way classifier, the valid attribute 
class has an F-measure of 89.2%. With the 5-way 
classifier, related-agent is the most accurate class 
(F = 95%) followed by part & related-object, 
activity, and quality (86.2%, 84.9%, and 81.0%, 
23
respectively). With non-attribute, however, we 
find an F of 41.7% in the 2-way classification, and 
53.8% in the 5-way classification. This suggests 
that the best strategy for lexicon building would be 
to use these classifiers to ?find? attributes rather 
than ?filter? non-attributes. 
6.2 Classifier Evaluation 2: Human Judges 
Next, we evaluated the accuracy of the attribute 
classifiers against two human judges (the authors). 
We randomly selected a concept from each of the 
21 classes in the balanced dataset.  Next, we used 
the classifiers to classify the 20 best candidate at-
tributes of each concept, as determined by their t-
test scores. Then, the judges decided if the as-
signed classes are correct or not. For the 5-way 
classifier, the judges also assigned the correct class 
if the automatic assigned class is incorrect.  
After a preliminary examination we decided not 
to consider two troublesome concepts: constructor 
and future. The reason for eliminating constructor 
is that we discovered it is ambiguous: in addition 
to the sense of ?a person who builds things?, we 
discovered that constructor is used widely in the 
Web as a name for a fundamental method in object 
oriented programming languages such as Java. 
Most of the best candidate attributes (e.g., call, 
arguments, code, and version) related to the latter 
sense, that doesn?t exist in WordNet. Our system is 
currently not able to do word sense discrimination, 
but we are currently working on this issue. The 
reason for ignoring the concept future was that this 
word is most commonly used as a modifier in 
phrases such as: ?the car of the future?, and ?the 
office of the future?, and that all of the best candi-
date attributes occurred in this type of construction.  
This reduced the number of evaluated concepts to 
19. 
According to the judges, the 2-way classifier 
was on average able to correctly assign attribute 
classes for 82.57% of the candidate attributes. This 
is very close to its performance in evaluation 1. 
The results using the F-measure reveal similar re-
sults too. Table 6 shows the results of the two clas-
sifiers based on the precision and recall measures. 
According to the judges, the 5-way classifier 
correctly classified 68.72% on average. This per-
formance is good but not as good as its perform-
ance in evaluation 1 (80.35%). The decrease in the 
performance was also shown in the F-measure. 
The F-measure ranges from 0.712 to 0.839 exclud-
ing the non-attribute class. 
  
Attribute Class P R F 
2-Way Classifier 
Attribute 0.928 0.872 0.899
Non-Attribute 0.311 0.459 0.369
5-Way Classifier 
Related-Agent 0.813 0.868 0.839
Part & Related-Object 0.814 0.753 0.781
Activity 0.870 0.602 0.712
Quality 0.821 0.658 0.730
Non-Attribute 0.308 0.632 0.414
Table 6: Evaluation against human judges results 
for the two classifiers 
 
An important question when using human 
judges is the degree of agreement among them. 
The K-statistic was used to measure this agree-
ment. The values of K are shown in Table 7. In the 
2-way classification, the judges agreed on 89.84% 
of the cases. On the other hand, the K-statistic for 
this classification task is 0.452. This indicates that 
part of this strong agreement is because that the 
majority of the candidate attributes are valid attrib-
utes. It also shows the difficulty of identifying non-
attributes even for human judges. In the 5-way 
classification, the two judges have a high level of 
agreement; Kappa statistic is 0.749. The judges 
and the 5-way classifier agreed on 63.71% of the 
cases. 
 
Description 2-Way 5-Way
Human Judges 89.84% 80.69%
Human Judges (Kappa) 0.452 0.749 
Human Judges & Classifier 78.36% 63.71%
Table 7: Level of agreement between the human 
judges and the classifiers 
6.3 Re-Clustering the Balanced Dataset 
Finally, we looked at whether using the classifiers 
results in a better lexical description for the pur-
poses of clustering (Almuhareb and Poesio, 2004). 
In Table 8 we show the results obtained using the 
output of the 2-way classifier to re-cluster the 402 
concepts of our balanced dataset, comparing these 
results with those obtained using all attributes (first 
column) and all attributes that remain after fre-
quency cutoff and POS filtering (column 2). The 
results are based on the CLUTO evaluation meas-
24
ures: Purity (which measures the degree of cohe-
sion of the clusters obtained) and Entropy. The 
purity and entropy formulas are shown in Table 9. 
 
Description 
All 
Candidate 
Attributes
Filtered 
Candidate 
Attributes 
2-Way 
Attributes
Purity 0.657 0.672 0.693 
Entropy 0.335 0.319  0.302  
Vector Size 24,178 4,296 3,824 
Table 8: Results of re-clustering concepts using 
different sets of attributes 
 
Clustering the concepts using only filtered can-
didate attributes improved the clustering purity 
from 0.657 to 0.672. This improvement in purity is 
not significant. However, clustering using only the 
attributes sanctioned by  the 2-way classifier im-
proved the purity further to 0.693, and this im-
provement in purity from the initial purity  was 
significant (t = 2.646, df = 801, p < 0.05). 
 
 Entropy Purity 
Single 
Cluster ?=?=
q
i r
i
r
r
i
r
r n
n
n
n
q
SE
1
log
log
1)(  )(max
1)( iri
r
r nn
SP =
Over-
all )(1
r
k
r
r SE
n
nEntropy ?
=
=  )(
1
r
k
r
r SP
n
nPurity ?
=
=
Table 9: Entropy and Purity in CLUTO. 
Sr is a cluster, nr is the size of the cluster, q is the number of 
classes, nir is the number of concepts from the  ith class that 
were assigned to the rth cluster, n is the number of concepts, 
and k is the number of clusters. 
7 Discussion and Conclusions 
The lexicon does not simply contain information 
about synonymy and hyponymy relations; it also 
contains information about the attributes of the 
concepts expressed by senses, as in Qualia struc-
tures. In previous work, we developed techniques 
for mining candidate attributes from the Web; in 
this paper we presented a method for improving 
the quality of attributes thus extracted, based on a 
classification for attributes derived from work in 
linguistics and philosophy, and a classifier that 
automatically tags candidate attributes with such 
classes. Both the 2-way and the 5-way classifiers 
achieve good precision and recall. Our work also 
reveals, however, that the notion of attribute is not 
fully understood. On the one hand, that attribute 
judgments are not always easy for humans even 
given a scheme; on the other hand, the results for 
certain types of attributes, especially activities and 
qualities, could certainly be improved. We also 
found that whereas attributes of physical objects 
are relatively easy to classify, the attributes of 
other types of concepts are harder ?particularly 
with activities. (See the Appendix for examples.) 
Our longer term goal is thus to further clarify the 
notion of attribute, possibly refining our classifica-
tion scheme, in collaboration with linguists, phi-
losophers, and psycholinguists. One comparison 
we are particularly interested in pursuing at the 
moment is that with feature lists used by psycholo-
gist, for whom knowledge representation is en-
tirely concept-based, and virtually every property 
of a concept counts as an attribute, including prop-
erties that would be viewed as IS-A links and what 
would be considered a value. Is it possible to make 
a principled, yet cognitively based distinction? 
Acknowledgments 
Abdulrahman Almuhareb is supported by King 
Abdulaziz City for Science and Technology 
(KACST), Riyadh, Saudi Arabia. We wish to thank 
the anonymous referees for many helpful sugges-
tions.  
 References 
Almuhareb, A. and Poesio, M. (2004). Attribute-Based 
and Value-Based Clustering: An Evaluation. In Proc. 
of EMNLP. Barcelona, July. 
Almuhareb, A. and Poesio, M. (2005). Concept Learn-
ing and Categorization from the Web. In Proc. of 
CogSci. Italy, July. 
Baader, F., Calvanese, D., McGuinness, D., Nardi, D. 
and Patel-Schneider, P. (Editors). (2003). The De-
scription Logic Handbook. Cambridge University 
Press. 
Berland, M. and Charniak, E. (1999). Finding parts in 
very large corpora. In Proc. of the 37th ACL, (pp. 
57?64). University of Maryland. 
Brill, E. (1995). Transformation-Based Error-Driven 
Learning and Natural Language Processing: A Case 
Study in Part of Speech Tagging. Computational 
Linguistics. 
25
Caraballo, S. A. (1999). Automatic construction of a 
hypernym-labeled noun hierarchy from text. In Proc. 
of  the 37th  ACL. 
Dixon, R. M. W. (1991). A New Approach to English 
Grammar, on Semantic Principles. Clarendon Press, 
Oxford. 
Fellbaum, C. (Editor). (1998). WordNet: An electronic 
lexical database. The MIT Press. 
Girju, R. and Moldovan, D. (2002). Mining answers for 
causal questions. In Proc. AAAI.   
Guarino, N. (1992). Concepts, attributes and arbitrary 
relations: some linguistic and ontological criteria for 
structuring knowledge base. Data and Knowledge 
Engineering, 8, (pp. 249?261). 
Hearst, M. A. (1998). Automated discovery of WordNet 
relations. In Fellbaum, C. (Editor). WordNet: An 
Electronic Lexical Database. MIT Press. 
Karypis, G. (2002). CLUTO: A clustering toolkit. Tech-
nical Report 02-017. University of Minnesota. At 
http://www-users.cs.umn.edu/~karypis/cluto/. 
Manning, C. D. and Schuetze H. (1999). Foundations of 
Statistical NLP. MIT Press. 
Merlo, P. and Stevenson, S. (2001). Automatic Verb 
Classification Based on Statistical Distributions of 
Argument Structure. Computational Linguistics. 27: 
3, 373-408. 
Murphy, G. L. (2002). The Big Book of Concepts. The 
MIT Press. 
Poesio, M., Ishikawa, T., Schulte im Walde, S. and 
Vieira, R. (2002).  Acquiring lexical knowledge for 
anaphora resolution. In Proc. Of LREC.     
Pustejovsky, J. (1995). The generative lexicon. MIT 
Press. 
Quirk, R., Greenbaum, S., Leech, G., and Svartvik, J. 
(1985). A comprehensive grammar of the English 
language. London: Longman. 
Sanchez-Graillet, O. and Poesio, M. (2004). Building 
Bayesian Networks from text. In Proc. of LREC, Lis-
bon, May. 
Vinson, D. P., Vigliocco, G., Cappa, S., and Siri, S. 
(2003). The breakdown of semantic knowledge: in-
sights from a statistical model of meaning representa-
tion. Brain and Language, 86(3), 347-365(19). 
Witten, I. H. and Frank, E. (1999). Data Mining: Prac-
tical Machine Learning Tools and Techniques with 
Java Implementations, Morgan Kaufmann. 
Woods, W. A. (1975). What?s in a link: Foundations for 
semantic networks. In Daniel G. Bobrow and Alan 
M. Collins, editors, Representation and Understand-
ing: Studies in Cognitive Science, (pp. 35-82). Aca-
demic Press, New York. 
 
 
 
Appendix A.   5-Way Automatic Classification of the Best Candidate Attributes of 
Some Concepts 
 
Car 
Class Best Attributes 
Activity acceleration, performance, styling, construction, propulsion, insurance, stance, ride, move-ment 
Part & 
Related-
Object  
front, body, mass, underside, hood, roof, nose, graphics, side, trunk, engine, boot, frame, bot-
tom, backseat, chassis, wheelbase, silhouette, floor, battery, windshield, seat, undercarriage, 
tank, window, steering, drive, finish  
Quality  speed, weight, handling, velocity, color, condition, width, look, colour, feel, momentum, heritage, shape, appearance, ownership, make, convenience, age, quality, reliability 
Related-
Agent  driver, owner, buyer, sponsor, occupant, seller 
Non-
Attribute  
rest, price, design, balance, motion, lure, control, use, future, cost, inertia, model, wheel, 
style, position, setup, sale, supply, safety  
 
26
 
Camel 
Class Best Attributes 
Activity introduction, selling, argument, exhaustion 
Part & 
Related-
Object  
nose, hump, furniture, saddle, hair, flesh, neck, milk, head, reins, foot, eye, hooves, humps, 
ass, feet, hoof, flanks, bones, ears, bag, skin, haunches, stomach, legs, urine, meat, penis, 
load, breast, backside, testicles, rope, corpse, house, nostrils, foam, bell, sight, butt, fur, bod-
ies, toe, hoofs, heads, knees, pancreas, mouth, coat, uterus, necks, chin, udders 
Quality  origins, gait, domestication, usefulness, pace, fleetness, smell, existence, appeal, birth, awk-wardness  
Related-
Agent  ghost 
Non-
Attribute  gift, rhythm, physiology, battle, case, example, dance, manner, description 
 
Cancer 
Class Best Attributes 
Activity 
growth, development, removal, treatment, recurrence, diagnosis, pain, spreading, metastasis, 
detection, eradication, elimination, production, discovery, remission, advance, excision, pre-
vention, evolution, disappearance, anxiety 
Part & 
Related-
Object  
location, site, lump, nature, root, cells, margin, formation, margins, roots, world, region 
Quality  
extent, size, seriousness, progression, severity, aggressiveness, cause, progress, symptoms, 
effects, risk, incidence, staging, biology, onset, characteristics, histology, ability, status, ap-
pearance, thickness, sensitivity, causes, prevalence, responsiveness, ravages, frequency, aeti-
ology, circumstances, rarity, outcome, behavior, genetics 
Related-
Agent  club, patient 
Non-
Attribute  
stage, spread, grade, origin, course, power, return, area, response, presence, type, particulars, 
occurrence, prognosis, pathogenesis, source, news, cure, pathology, properties, genesis, 
boundaries, drama, stages, chapter 
 
Family 
Class Best Attributes 
Activity disintegration, protection, decline, destruction, breakup, abolition, participation, reunifica-tion, reconciliation, dissolution, composition, restoration 
Part & 
Related-
Object  
head, institution, support, flower, core, fabric, culture, dimension, food, lineage, cornerstone, 
community 
Quality  
breakdown, importance, honor, structure, sociology, integrity, unity, sanctity, health, privacy, 
survival, definition, influence, honour, involvement, continuity, stability, size, preservation, 
upbringing, centrality, ancestry, solidarity, hallmark, status, functioning, primacy, autonomy  
Related-
Agent  
father, baby, member, mother, members, patriarch, breadwinner, matriarch, man, foundation, 
founder, heir, daughter 
Non-
Attribute  
rest, role, income, history, concept, welfare, pedigree, genealogy, presence, context, origin, 
bond, tradition, taxonomy, system, wealth, lifestyle, surname, crisis, ideology, rights, eco-
nomics, safety 
 
27
Proceedings of the Second Workshop on Computational Linguistics for Literature, pages 9?16,
Atlanta, Georgia, June 14, 2013. c?2013 Association for Computational Linguistics
Recognition of Classical Arabic Poems 
 
Abdulrahman Almuhareb Ibrahim Alkharashi  Lama AL Saud  Haya Altuwaijri 
Computer Research Institute  
KACST  
Riyadh, Saudi Arabia 
{muhareb, kharashi, lalsaud, htuwaijri}@kacst.edu.sa  
 
 
Abstract 
This work presents a novel method for recog-
nizing and extracting classical Arabic poems 
found in textual sources. The method utilizes 
the basic classical Arabic poem features such 
as structure, rhyme, writing style, and word 
usage. The proposed method achieves a preci-
sion of 96.94% while keeping a high recall 
value at 92.24%. The method was also used to 
build a prototype search engine for classical 
Arabic poems. 
1 Introduction 
Searching for poetry instances on the web, as well 
as identifying and extracting them, is a challenging 
problem. Contributing to the difficulty are the fol-
lowing: creators of web content do not usually fol-
low a fixed standard format when publishing 
poetry content; there is no special HTML tags that 
can be used to identify and format poetry content; 
and finally poetry content is usually intermixed 
with other content published on the web. 
In this paper, a classical Arabic poetry recog-
nition and extraction method has been proposed. 
The method utilizes poem features and writing 
styles to identify and isolate one or more poem text 
bodies in a given piece of text. As an implementa-
tion of the poetry recognition and extraction me-
thod, a prototype Arabic poetry search engine was 
developed.  
The paper is organized as follows. In Section 2, 
the related works are briefly discussed. Section 3 
gives a general overview of Arabic poems features. 
Section 4 discusses the methodology used to iden-
tify and extract poem content from a given text. It 
also presents the used evaluation method. In Sec-
tion 5, we discuss the experimentation including 
the used dataset and results. A prototype imple-
mentation of the method is presented in Section 6 
followed by conclusions and future work plans. 
2 Related Work 
To the best of our knowledge, this work1is the first 
attempt to explore the possibility for building an 
automated system for recognizing and extracting 
Arabic poems from a given piece of text. The most 
similar work related to this effort is the work that 
has been done independently by Tizhoosh and Da-
ra (2006) and Tizhoosh et al (2008). The objective 
of Tizhoosh and his colleagues was to define a me-
thod that can distinguish between poem and non-
poem (prose) documents using text classification 
techniques such as na?ve Bayes, decision trees, and 
neural networks. The classifiers were applied on 
poetic features such as rhyme, shape, rhythm, me-
ter, and meaning. 
Another related work is by Al-Zahrani and El-
shafei (2010) who filed a patent application for 
inventing a system for Arabic poetry meter identi-
fication. Their invention is based on Al-khalil bin 
Ahmed theory on Arabic poetry meters from the 8th 
century. The invented system accepts spoken or 
written Arabic poems to identify and verify their 
poetic meters. The system also can be used to as-
sist the user in interactively producing poems 
based on a chosen meter.  
Work on poem processing has been also con-
ducted on other topics such as poem style and me-
ter classification, rhyme matching, poem 
generation and quality evaluation. For example, Yi 
                                                          
1 Parts of this work are also presented in Patent Application 
No.: US 2012/0290602 A1. 
9
et al (2004) used a technique based on term con-
nection for poetry stylistics analysis. He et al 
(2007) used Support Vector Machines to differen-
tiate bold-and-unconstrained styles from graceful-
and-restrained styles of poetry. Hamidi et al 
(2009) proposed a meter classification system for 
Persian poems based on features that are extracted 
from uttered poems. Reddy and Knight (2011) 
proposed a language-independent method for 
rhyme scheme identification. Manurung (2004) 
and Netzer et al (2009) proposed two poem gener-
ation methods using hill-climbing search and word 
associations norms, respectively. In a recent work, 
Kao and Jurafsky (2012) proposed a method to 
evaluate poem quality for contemporary English 
poetry. Their proposed method computes 16 fea-
tures that describe poem style, imagery, and senti-
ment. Kao and Jurafsky's result showed that 
referencing concrete objects is the primary indica-
tor for professional poetry. 
3 Features of Classical Arabic Poems 
Traditionally, Arabic poems have been used as a 
medium for recording historical events, transfer-
ring messages among tribes, glorifying tribe or 
oneself, or satirizing enemies. Classical Arabic 
poems are characterized by many features. Some 
of these features are common to poems written in 
other languages, and some are specific to Arabic 
poems. Features of classical Arabic poems have 
been established in the pre-Islamic era and re-
mained almost unchanged until now. Variation for 
such features can be noticed in contemporary (Pao-
li, 2001) and Bedouin (Palva, 1993) poems. In this 
section, we describe the Arabic poetic features that 
have been utilized in this work. 
3.1 Presence 
Instances of classical Arabic poems, as well as 
other types of poems, can be found in all sorts of 
printed and electronic documents including books, 
newspapers, magazines, and websites. An instance 
of classical Arabic poems can represent a complete 
poem or a poem portion. A single document can 
contain several classical Arabic poem instances. 
Poems can occur in designated documents by 
themselves or intermixed with normal text. In addi-
tion, poems can be found in non-textual media in-
cluding audios, videos and images. 
In the web, Arabic poem instances can be found 
in designated websites2. Only-poem websites nor-
mally organize poems in categories and adapt a 
unified style format that is maintained for the en-
tire website. Hence, poem instances found in such 
websites are almost carefully written and should 
contain fewer errors.  However, instances found in 
other websites such as forums and blogs are writ-
ten in all sorts of styles and may contain mistakes 
in the content, spelling, and formatting. 
3.2 Structure 
Classical Arabic poems are written as a set of 
verses. There is no limit on the number of verses in 
a poem. However, a typical poem contains be-
tween twenty and a hundred verses (Maling, 1973). 
Arabic poem verses are short in length, compared 
to lines in normal text, and of equivalent length. 
Each verse is divided into two halves called hemis-
tiches which also are equivalent in length. 
3.3 Meter 
The meters of classical Arabic poetry were mod-
eled by Al-Khalil bin Ahmed in the 8th century. Al-
Khalil's system consists of 15 meters (Al-Akhfash, 
a student of Al-Khalil, added the 16th meter later). 
Each meter is described by an ordered set of con-
sonants and vowels. Most classical Arabic poems 
can be encoded using these identified meters and 
those that can't be encoded are considered un-
metrical.   Meters' patterns are applied on the he-
mistich level and each hemistich in the same poem 
must follow the same meter. 
3.4 Rhyme 
Classical Arabic poems follow a very strict but 
simple rhyme model. In this model, the last letter 
of each verse in a given poem must be the same. If 
the last letter in the verse is a vowel, then the 
second last letter of each verse must be the same as 
well. There are three basic vowel sounds in Arabic. 
Each vowel sound has two versions: a long and a 
short version. Short vowels are written as diacriti-
cal marks below or above the letter that precedes 
them while long vowels are written as whole let-
ters. The two versions of each basic vowel are con-
sidered equivalent for rhyme purposes. Table 1 
                                                          
2 adab.com is an example for a dedicated website for Arabic 
poetry. 
10
shows these vowel sets and other equivalent letters. 
These simple matching rules make rhyme detection 
in Arabic a much simpler task compared to English 
where different sets of letter combinations can sig-
nal the same rhyme (Tizhoosh & Dara 2006). On 
the other hand, the fact that, in modern Arabic 
writing, short vowels are ignored adds more chal-
lenges for the rhyme identification process. How-
ever, in poetry typesetting, typists tend not to omit 
short vowels especially for poems written in stan-
dard Arabic. 
 
Table 1: Equivalent vowels and letters 
Equivalent Letters Equivalent Vowels
ta, ta marbutah/a/, /a:/
ha, ta marbutah/u/, /u:/
 /i/, /i:/
 
H1
V
er
se
 1
H2
H1
V
er
se
 2
H2
H1
V
er
se
 3
H2
H1
V
er
se
 4
 H2
Figure 1: An example of classical Arabic poems with 
four verses written in Style 1. H1 and H2 are the first 
and second hemistich. 
3.5 Writing Styles 
There are three predominant writing styles of clas-
sical Arabic poems: (1) the poem is written in a 
single column with each verse in two rows; (2) the 
poem is written in a single column with each verse 
in two rows where the first half of each verse is 
written aligned to the right and the second half of 
each verse is aligned to the left; and (3) the poem 
is written such that each verse is written as two 
halves on the same row and separated by one or 
more punctuation marks or spaces. In some cases, 
this style can also be written without any separa-
tors and the end of the first half and the start of the 
second half have to be guessed by the reader. Fig-
ures 1 to 3 show examples of the three writing 
styles of classical Arabic poems. 
 
H1
V
er
se
 1
H2
H1
V
er
se
 2
H2
H1
V
er
se
 3
H2
H1
V
er
se
 4
 H2
Figure 2: An example of classical Arabic poems with 
four verses written in Style 2. 
 
Hemistich 1Hemistich 2 
Verse 1
Verse 2
Verse 3
 Verse 4
Figure 3: An example of classical Arabic poems with 
four verses written in Style 3. 
3.6 Word Usage 
It is very noticeable that classical Arabic poets tend 
not to use words repetitively in a given poem. To 
evaluate this observation, we analyzed a random 
set of 134 poem instances. We found duplicate 
start words (excluding common stop words) in 
22% of the poems. Duplicate end words were 
found in 31% of the poems. However, the proba-
bility of encountering a verse with a duplicate start 
in the same poem is only 3% and 4% for a dupli-
cate end word. 
4 Method 
The proposed method for standard Arabic poem 
recognition utilizes the poetic features described 
previously including structure, rhyme, writing 
style, and word usage. The meter feature was not 
11
literally used in the proposed method and may be 
used in a future work. The system operation is 
summarized by the flowchart shown in Figure 5 
and described by the following steps: 
1. Read input text line by line accepting only 
lines with reasonable size (e.g., lines of 
size between 2 and 20 words).  
2. Collect consecutive lines that have equiva-
lent length: compute the length of the line 
by counting the characters in the line. 
Lines are considered equivalent in length if 
the length difference is below a certain 
threshold (e.g., 40%, as has been used in 
the experiment discussed below).  
3. Identify lines with separators to process 
Style 3 candidate verses. Separators are 
identified by searching for a set of white 
spaces or punctuations in the middle of the 
line between two halves. If identified, 
transform Style 3 to Style 1 shape for nor-
malization.  
4. Identify candidate rhymes at the end of 
each line.  
5. Identify poems: searching for candidate 
poems in a consecutive list of candidate 
half-verses can produce several solutions 
based on rhyme. Select solution that pro-
duces poems with the maximum possible 
lengths. Figure 4 shows an example for a 
multiple solution case. 
6. Repeat steps 1 to 5 until the end of the text 
body is reached. 
 
 
Figure 4: An example for multiple solutions based on 
rhyme. A list of 10 candidate half-verses indicated by 
their rhymes from A to F. Poem 1 starts at line 2 and 
ends at line 9 with 4 verses and rhyme C. Poem 2 starts 
at line 3 and ends at line 6 with 2 verses and rhyme D. 
The proposed method will select Poem 1 instead of 
Poem 2 since it has more verses. 
Following these steps, the proposed method can 
recognize instances of classical Arabic poems of 
size at least two verses in any plain text. Detecting 
instances of a single verse is not covered in this 
work because the recognition process is only trig-
gered by repetitive patterns that can't occur within 
single verse instances. 
 
 
Figure 5: A flowchart of the proposed system for Arabic 
poems recognition. 
4.1 Handling ill-formed cases 
The proposed method can be applied on plain text 
from any source regardless of formatting and typ-
ing quality. Common formatting and typing mis-
takes and ambiguity are resolved as follows:  
1. Mismatched and false separators: Mis-
matched separators occur when a set of 
candidate verses share the same rhyme but 
with different verse separators. Here, we 
treat the separators as if they were similar 
assuming that the separators were incor-
rectly typed. False separators, on the other 
hand, is identified when a set of candidate 
verses share the same rhyme and one or 
more verses were identified as having se-
parators and the remaining verses have 
not. In this case, we ignore the identified 
separators assuming that these misidenti-
fied separators are just normal punctuation 
12
marks. Figure 6 and 7 show real examples 
from the web for mismatched and false se-
parators, respectively. 
 
  
  
  
   
Figure 6: An example of mismatched separators for a 
poem instance with four verses that share the same 
rhyme. The first two verses share the same separator 
while the third and the forth verses have similar but not 
exact separators. 
 
 
Figure 7: An example of false separators for a poem 
instance with three verses that share the same rhyme. 
The first half of the first and third verses contain dots 
(..) at the middle of the line which can mistakenly be 
identified as separators. 
 
2. Absence of short vowels: To treat missing 
short vowels in rhyme, we, recursively, as-
sume the existence of the vowel if missing 
in a given verse and exists in a neighboring 
verse. Here, the last character in the former 
verse must match the second last character 
in the neighboring verse. Figure 8 shows 
an example of this case. 
 
  
  
  
   
Figure 8: An example of short vowels absence for a 
poem instance with four verses. The first three verses 
neglect the short vowel Kasrah that exists at the end of 
the fourth verse. 
 
3. Absence of separators: This case is trig-
gered when encountering a set of consecu-
tive lines sharing the same rhyme, and 
having line length in words that exceed 
half of the threshold for valid lines, and of 
course have no identifiable separators. The 
proposed remedy is to locate the closest 
whitespace to the center of each line and 
split the lines at those points and generate 
a verse of two hemistiches from each line. 
Figure 9 shows an example of this case. 
 
 
Figure 9: An example of absence of separators for a 
poem instance with five verses. 
4.2 Pruning 
Based on our observations during the development 
phase of the proposed method, it was noticeable 
that the robustness of the method correlates posi-
tively with the number of verses in the candidate 
poem. This is because with each additional verse 
the accumulated evidences are reconfirmed repeti-
tively. This is not the case with few verses candi-
dates. The probability of encountering a false 
matching rhyme for example with two or three 
verses is much higher. To resolve these cases and 
improve the precision of the proposed method, we 
introduce the following pruning tests to be applied 
only to short candidate poems: 
1. Reject short candidate instances with low 
average number of words per half-verses. 
For example, using a threshold of 3 words. 
2. Accept only short candidate instances that 
have at least two letters rhymes. 
3. Reject short candidate instances when 
number of words per half-verse is not 
equivalent. 
4. Reject short candidate instances with dup-
licate starting or ending words that exceed 
a threshold of 20%, for example. 
13
0 2 3 4 4* 
60
65
70
75
80
85
90
95
100
Precision Recall F-measure
Pruning Level
S
c
o
r
e
 
(
%
)
4.3 Evaluation Measure 
To evaluate the proposed method, we applied the 
F-measure (Swets, 1969) based on the precision 
and recall measures. Precision, as shown in Equa-
tion 1, is calculated by dividing the total number of 
the correct lines produced by the method over the 
total number of lines in the output. Given that our 
method processes the input data and generates out-
put as half-verse per line. Recall, as shown in Equ-
ation 2, is computed similarly except that we 
divide over the model total number of correct lines. 
The model resembles the prefect solution for such 
input data. 
 
 
)2(
)1(
Lines Correct of Number Total Model
Lines Correct of Number Total System
=Recall
Lines of Number Total System
Lines Correct of Number Total System
=Precision
?
5 Experiment 
5.1 Dataset 
During the development phase of the method, we 
used several development datasets utilizing data 
drawn from the web. For evaluation purposes, we 
assembled a dataset using text from hundred ran-
domly selected HTML web-pages. The set con-
tains 50 HTML pages with classical Arabic poem 
instances (positive set) and 50 pages without poem 
instances (negative set). To select the positive set, 
we randomly chose 5 poets and searched Google 
and selected the first 10 pages that contain poem 
instances for each poet. The negative set was simi-
larly chosen by selecting the first 50 pages that 
contain no poem instances for an arbitrary query. 
Text from the selected web-pages was converted to 
plain text using the Apache Tika toolkit3 and saved 
in a single large text file. This resulted in a text file 
that contains about 23K non-empty lines including 
161 classical Arabic poem instances having 4,740 
half-verses. 
                                                          
3 The Apache Tika toolkit can be downloaded from 
http://tika.apache.org/ 
5.2 Result 
The poem dataset was used to evaluate the pro-
posed poem recognition method. Figure 10 shows 
the results using five different pruning levels. The 
levels indicate the minimum number of verses for 
the pruning tests to be applied. Level 0 shows the 
performance without applying any of the pruning 
tests. The remaining levels show the results when 
the pruning is applied on candidates with at most 
two, three, and four verses, respectively. Level 4* 
is similar to Level 4 but here the fourth pruning 
test (duplicate words test) is applied on every can-
didate instance instead of only candidates with at 
most four verses. 
 
Figure 10: Evaluation results using five different prun-
ing levels. 
6 A Prototype Poem Search Engine  
In order to assess the performance of the proposed 
poem recognition method in a real-life application, 
a prototype search engine for Arabic poems was 
implemented4. The search engine was built using the 
Apache Nutch web crawler and the Solr search 
engine to provide regular search engine services 
including crawling, parsing, and indexing. The 
HTML parsing plug-in in Nutch was extended us-
ing the proposed method to be able to recognize 
Arabic poems. Using this scenario, the search en-
gine was successfully used to crawl a set of web-
sites, identify all poem and non-poem instances, 
and index poem instances only. Figure 11 shows a 
snapshot of the search engine website. 
 
                                                          
4 The Arabic poem prototype search engine can be accessed at 
http://naba.kacst.edu.sa  
14
 
Figure 11:  A snapshot of the prototype poem search 
engine 
7 Conclusions and Future Work 
In this paper, we proposed a method for classical 
Arabic poem recognition. The proposed method 
was able to identify Arabic poems in any unstruc-
tured text with a very high accuracy. The method 
utilizes the common features of classical Arabic 
poems such as structure, writing style, and rhyme; 
and employs them in the recognition process. A 
specialized search engine for classical Arabic 
poems was implemented as a prototype using the 
proposed method with promising results. For the 
future, we plan to enhance the method by introduc-
ing the well known meter model for classical Arab-
ic poems. We would also like to extend the 
coverage of the method to include other types of 
Arabic poetry, namely contemporary Arabic. For 
the specialized search engine, we plan to add more 
features such as providing different search bounda-
ries, for example, within a poem, a verse, or a he-
mistich. Moreover, we would like to find 
automatic ways to relate a poem to its poet. 
Acknowledgments 
The authors would like to thank Waleed Almutairi 
and Abdulelah Almubarak from KACST for their 
assistance in implementing the prototype poem 
search engine. 
  
15
References 
Al-Zahrani, A.K., Elshafei, M., 2010. Arabic poetry 
meter identification system and method. Patent Ap-
plication US 2010/0185436. 
Hamidi, S., Razzazi, F., Ghaemmaghami, M.P., 2009. 
Automatic Meter Classification in Persian Poetries 
Using Support Vector Machines. Presented at the 
IEEE International Symposium on Signal Processing 
and Information Technology (ISSPIT). 
He, Z.-S., Liang, W.-T., Li, L.-Y., Tian, Y.-F., 2007. 
SVM-Based Classification Method for Poetry Style. 
Presented at the Sixth International Conference on 
Machine Learning and Cybernetics, Hong Kong. 
Kao, J., Jurafsky, D., 2012. A Computational Analysis 
of Style, Affect, and Imagery in Contemporary Poe-
try, in: In Proceedings of the NAACL-HLT 2012 
Workshop on Computational Linguistics for Litera-
ture. Montreal, Canada, pp. 8?17. 
Maling, J., 1973. The theory of classical Arabic metrics 
(dissertation). 
Manurung, H.M., 2004. An Evolutionary Algorithm 
Approach to Poetry Generation (PhD thesis). 
Netzer, Y., Gabay, D., Goldberg, Y., Elhadad, M., 2009. 
Gaiku: Generating Haiku with Word Associations 
Norms. Presented at the Workshop on Computational 
Approaches to Linguistic Creativity  (CALC ?09). 
Palva, H., 1993. Metrical problems of the contemporary 
Bedouin Qasida: A linguistic approach. Asian Fol-
klore Studies 52, 75?92. 
Paoli, B., 2001. Meters and Formulas: The Case of An-
cient Arabic Poetry. Belgian Journal of Linguistics 
15, 113?136. 
Reddy, S., Knight, K., 2011. Unsupervised Discovery of 
Rhyme Schemes. Presented at the 49th Annual Meet-
ing of the Association for Computational Linguistics, 
Portland, Oregon, pp. 77?82. 
Swets, J.A., 1969. Effectiveness of information retrieval 
methods. American Documentation 20, 72?89. 
Tizhoosh, H.R., Dara, R.A., 2006. On Poem Recogni-
tion. Pattern Analysis and Applications, Springer 9, 
325?338. 
Tizhoosh, H.R., Sahba, F., Dara, R., 2008. Poetic Fea-
tures for Poem Recognition: A Comparative Study. 
Journal of Pattern Recognition Research 3. 
Yi, Y., He, Z.-S., Li, L.-Y., Yu, T., 2004. Studies on 
Traditional Chinese Poetry Style Identification. Pre-
sented at the Third International Conference on Ma-
chine Learning and Cybernetics, Shanghai. 
16
