Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 121?130,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
A Rich Feature Vector for Protein-Protein Interaction Extraction from
Multiple Corpora
Makoto Miwa1 Rune S?tre1 Yusuke Miyao1 Jun?ichi Tsujii1,2,3
1Department of Computer Science, the University of Tokyo, Japan
Hongo 7-3-1, Bunkyo-ku, Tokyo, Japan.
2School of Computer Science, University of Manchester, UK
3National Center for Text Mining, UK
{mmiwa,rune.saetre,yusuke,tsujii}@is.s.u-tokyo.ac.jp
Abstract
Because of the importance of protein-
protein interaction (PPI) extraction from
text, many corpora have been proposed
with slightly differing definitions of pro-
teins and PPI. Since no single corpus is
large enough to saturate a machine learn-
ing system, it is necessary to learn from
multiple different corpora. In this paper,
we propose a solution to this challenge.
We designed a rich feature vector, and we
applied a support vector machine modi-
fied for corpus weighting (SVM-CW) to
complete the task of multiple corpora PPI
extraction. The rich feature vector, made
from multiple useful kernels, is used to
express the important information for PPI
extraction, and the system with our fea-
ture vector was shown to be both faster
and more accurate than the original kernel-
based system, even when using just a sin-
gle corpus. SVM-CW learns from one cor-
pus, while using other corpora for support.
SVM-CW is simple, but it is more effec-
tive than other methods that have been suc-
cessfully applied to other NLP tasks ear-
lier. With the feature vector and SVM-
CW, our system achieved the best perfor-
mance among all state-of-the-art PPI ex-
traction systems reported so far.
1 Introduction
The performance of an information extraction pro-
gram is highly dependent on various factors, in-
cluding text types (abstracts, complete articles, re-
ports, etc.), exact definitions of the information to
be extracted, shared sub-topics of the text collec-
tions from which information is to be extracted.
Even if two corpora are annotated in terms of the
same type of information by two groups, the per-
formance of a program trained by one corpus is
unlikely to be reproduced in the other corpus. On
the other hand, from a practical point of view, it is
worth while to effectively use multiple existing an-
notated corpora together, because it is very costly
to make new annotations.
One problem with several different corpora is
protein-protein interaction (PPI) extraction from
text. While PPIs play a critical role in un-
derstanding the working of cells in diverse bio-
logical contexts, the manual construction of PPI
databases such as BIND, DIP, HPRD, IntAct, and
MINT (Mathivanan et al, 2006) is known to be
very time-consuming and labor-intensive. The au-
tomatic extraction of PPI from published papers
has therefore been a major research topic in Natu-
ral Language Processing for Biology (BioNLP).
Among several PPI extraction task settings, the
most common is sentence-based, pair-wise PPI ex-
traction. At least four annotated corpora have been
provided for this setting: AIMed (Bunescu et al,
2005), HPRD50 (Fundel et al, 2006), IEPA (Ding
et al, 2002), and LLL (Ne?dellec, 2005). Each of
these corpora have been used as the standard cor-
pus for training and testing PPI programs. More-
over, several corpora are annotated for more types
of events than just for PPI. Such examples include
BioInfer (Pyysalo et al, 2007), and GENIA (Kim
et al, 2008a), and they can be reorganized into PPI
corpora. Even though all of these corpora were
made for PPI extraction, they were constructed
based on different definitions of proteins and PPI,
which reflect different biological research inter-
ests (Pyysalo et al, 2008).
Research on PPI extraction so far has revealed
that the performance on each of the corpora could
121
benefit from additional examples (Airola et al,
2008). Learning from multiple annotated cor-
pora could lead to better PPI extraction perfor-
mance. Various research paradigms such as induc-
tive transfer learning (ITL) and domain adaptation
(DA) have mainly focused on how to effectively
use corpora annotated by other groups, by reduc-
ing the incompatibilities (Pan and Yang, 2008).
In this paper, we propose the extraction of PPIs
from multiple different corpora. We design a rich
feature vector, and as an ITL method, we ap-
ply a support vector machine (SVM) modified for
corpus weighting (SVM-CW) (Schweikert et al,
2008), in order to evaluate the use of multiple cor-
pora for the PPI extraction task. Our rich feature
vector is made from multiple useful kernels, each
of which is based on multiple parser inputs, pro-
posed by Miwa et al (2008). The system with our
feature vector was better than or at least compa-
rable to the state-of-the-art PPI extraction systems
on every corpus. The system is a good starting
point to use the multiple corpora. Using one of the
corpora as the target corpus, SVM-CW weights
the remaining corpora (we call them the source
corpora) with ?goodness? for training on the tar-
get corpus. While SVM-CW is simple, we show
that SVM-CW can improve the performance of the
system more effectively and more efficiently than
other methods proven to be successful in other
NLP tasks earlier. As a result, SVM-CW with our
feature vector is comprised of a PPI system with
five different models, of which each model is su-
perior to the best model in the original PPI extrac-
tion task, which used only the single corpus.
2 Related Works
While sentence-based, pair-wise PPI extraction
was initially tackled by using simple methods
based on co-occurrences, lately, more sophisti-
cated machine learning systems augmented by
NLP techniques have been applied (Bunescu et al,
2005). The task has been tackled as a classifica-
tion problem. To pull out useful information from
NLP tools including taggers and parsers, several
kernels have been applied to calculate the similar-
ity between PPI pairs. Miwa et al (2008) recently
proposed the use of multiple kernels using multi-
ple parsers. This outperformed other systems on
the AIMed, which is the most frequently used cor-
pus for the PPI extraction task, by a wide margin.
To improve the performance using external
Classification
Result
Training
Data
Feature 
vector
Raw Texts
Parsers
Classifier
Test 
Data
Raw Texts
Model
Pair Information
Pair Information
Label
Figure 1: Overview of our PPI extraction system
training data, many ITL and DA methods have
been proposed. Most of ITL methods assume that
the feature space is same, and that the labels may
be different in only some examples, while most of
DA methods assume that the labels are the same,
and that the feature space is different. Among the
methods, we use adaptive SVM (aSVM) (Yang et
al., 2007), singular value decomposition (SVD)
based alternating structure optimization (SVD-
ASO) (Ando et al, 2005), and transfer AdaBoost
(TrAdaBoost) (Dai et al, 2007) to compare with
SVM-CW. We do not use semi-supervised learn-
ing (SSL) methods, because it would be consid-
erably costly to generate enough clean unlabeled
data needed for SSL (Erkan et al, 2007). aSVM
is seen as a promising DA method among sev-
eral modifications of SVM including SVM-CW.
aSVM tries to find a model that is close to the one
made from other classification problems. SVD-
ASO is one of the most successful SSL, DA, or
multi-task learning methods in NLP. The method
tries to find an additional useful feature space by
solving auxiliary problems that are close to the tar-
get problem. With well-designed auxiliary prob-
lems, the method has been applied to text clas-
sification, text chunking, and word sense disam-
biguation (Ando, 2006). The method was reported
to perform better than or comparable to the best
state-of-the-art systems in all of these tasks. TrAd-
aBoost was proposed as an ITL method. In train-
ing, the method reduces the effect of incompatible
examples by decreasing their weights, and thereby
tries to use useful examples from source corpora.
The method has been applied to text classifica-
tion, and the reported performance was better than
SVM and transductive SVM (Dai et al, 2007).
3 PPI Extraction System
The target task of our system is a sentence-based,
pair-wise PPI extraction. It is formulated as a clas-
sification problem that judges whether a given pair
122
XPGp1 protein interacts with multiple subunits of
TFIIHprot and with CSBp2 protein.
Figure 2: A sentence including an interacting pro-
tein pair (p1, p2). (AIMed PMID 8652557, 9th
sentence, 3rd pair)
BOW
v-walks
e-walks
Graph BOW
v-walks
e-walks
Graph
Normalization
Parsers
KSDEPEnju
a sentence including a pair
feature vector
BOW Graph BOW
v-walks
e-walks
Graph
v-walks
e-walks
Figure 3: Extraction of a feature vector from the
target sentence
of proteins in a sentence is interacting or not. Fig-
ure 2 shows an example of a sentence in which the
given pair (p1 and p2) actually interacts.
Figure 1 shows the overview of the proposed
PPI extraction system. As a classifier using a sin-
gle corpus, we use the 2-norm soft-margin lin-
ear SVM (L2-SVM) classifier, with the dual co-
ordinate decent (DCD) method, by Hsieh et al
(2008). In this section, we explain the two main
features: the feature vector, and the corpus weight-
ing method for multiple corpora.
3.1 Feature Vector
We propose a feature vector with three types of
features, corresponding to the three different ker-
nels, which were each combined with the two
parsers: the Enju 2.3.0, and KSDEP beta 1 (Miyao
et al, 2008); this feature vector is used because the
kernels with these parsers were shown to be effec-
tive for PPI extraction by Miwa et al (2008), and
because it is important to start from a good per-
formance single corpus system. Both parsers were
retrained using the GENIA Treebank corpus pro-
vided by Kim et al (2003). By using our linear
feature vector, we can perform calculations faster
by using fast linear classifiers like L2-SVM, and
we also obtain a more accurate extraction, than by
using the original kernel method.
Figure 3 summarizes the way in which the fea-
ture vector is constructed. The system extracts
Bag-of-Words (BOW), shortest path (SP), and
graph features from the output of two parsers. The
PROT M:1, and M:1, interact M:1, multiple M:1,
of M:1, protein M:1, subunit M:1, with M:2, pro-
tein A:1
Figure 4: Bag-of-Words features of the pair in Fig-
ure 2 with their positions (B:Before, M:in the Mid-
dle of, A:After) and frequencies.
NMOD SBJ
rNMOD
ENTITY1 protein interact ENTITY2protein protein
ENTITY1 protein interacts with multiple and with ENTITY2 protein .
NMOD SBJ
COOD
COORD
NMOD
PMOD
NMOD SBJ
rNMOD
protein interact protein
SBJ rCOOD
rPMOD
V-walks  
E-walks
???
???
???
Figure 5: Vertex walks, edge walks in the upper
shortest path between the proteins in the parse tree
by KSDEP. The walks and their subsets are used
as the shortest path features of the pair in Figure 2.
output is grouped according to the feature-type
and parser, and each group of features is separately
normalized by the L2-norm1. Finally, all values
are put into a single feature vector, and the whole
feature vector is then also normalized by the L2-
norm. The features are constructed by using pred-
icate argument structures (PAS) from Enju, and by
using the dependency trees from KSDEP.
3.1.1 Bag-of-Words (BOW) Features
The BOW feature includes the lemma form of a
word, its relative position to the target pair of pro-
teins (Before, Middle, After), and its frequency in
the target sentence. BOW features form the BOW
kernel in the original kernel method. BOW fea-
tures for the pair in Figure 2 are shown in Figure 4.
3.1.2 Shortest Path (SP) Features
SP features include vertex walks (v-walks), edge
walks (e-walks), and their subsets (Kim et al,
2008b) on the target pair in a parse structure, and
represent the connection between the pair. The
features are the subsets of the tree kernels on the
shortest path (S?tre et al, 2007). Figure 5 illus-
trates the shortest path between the pair in Fig-
ure 2, and its v-walks and e-walks extracted from
the shortest path in the parse tree by KSDEP. A
v-walk includes two lemmas and their link, while
1The vector normalized by the L2-norm is also called a
unit vector.
123
an e-walk includes a lemma and its two links. The
links indicates the predicate argument relations for
PAS, and the dependencies for dependency trees.
3.1.3 Graph Features
Graph features are made from the all-paths graph
kernel proposed by Airola et al (2008). The ker-
nel represents the target pair using graph matrices
based on two subgraphs, and the graph features are
all the non-zero elements in the graph matrices.
The two subgraphs are a parse structure sub-
graph (PSS) and a linear order subgraph (LOS).
Figure 6 describes the subgraphs of the sentence
parsed by KSDEP in Figure 2. PSS represents the
parse structure of a sentence. PSS has word ver-
tices or link vertices. A word vertex contains its
lemma and its part-of-speech (POS), while a link
vertex contains its link. Additionally, both types
of vertices contain their positions relative to the
shortest path. The ?IP?s in the vertices on the
shortest path represent the positions, and the ver-
tices are differentiated from the other vertices like
?P?, ?CC?, and ?and:CC? in Figure 6. LOS repre-
sents the word sequence in the sentence. LOS has
word vertices, each of which contains its lemma,
its relative position to the target pair, and its POS.
Each subgraph is represented by a graph matrix
G as follows:
G = L
T
?
?
n=1
A
n
L, (1)
where L is a N?L label matrix, A is an N?N
edge matrix, N represents the number of vertices,
and L represents the number of labels. The la-
bel of a vertex includes all information described
above (e.g. ?ENTITY1:NN:IP? in Figure 6). If
two vertices have exactly same information, the
labels will be same. G can be calculated effi-
ciently by using the Neumann Series (Airola et al,
2008). The label matrix represents the correspon-
dence between labels and vertices. L
ij
is 1 if the
i-th vertex corresponds to the j-th label, and 0 oth-
erwise. The edge matrix represents the connection
between the pairs of vertices. A
ij
is a weight w
ij
(0.9 or 0.3 in Figure 6 (Airola et al, 2008)) if the
i-th vertex is connected to the j-th vertex, and 0
otherwise. By this calculation, G
ij
represent the
sum of the weights of all paths between the i-th
label and the j-th label.
A B H I L
positive 1,000 2,534 163 335 164
all 5,834 9,653 433 817 330
Table 1: The sizes of used PPI corpora. A:AIMed,
B:BioInfer, H:HPRD50, I:IEPA, and L:LLL.
50
60
70
80
90
100
0 20 40 60 80 100
% examples
AImed (F)
BioInfer (F)
AImed (AUC)
BioInfer (AUC)
Figure 7: Learning curves on two large corpora.
The x-axis is related to the percentage of the ex-
amples in a corpus. The curves are obtained by a
10-fold CV with a random split.
3.2 Corpus Weighting for Mixing Corpora
Table 1 shows the sizes of the PPI corpora that we
used. Their widely-ranged differences including
the sizes were manually analyzed by Pyysalo et
al. (2008). While AIMed, HPRD50, IEPA, and
LLL were all annotated as PPI corpora, BioInfer in
its original form contains much more fine-grained
information than does just the PPI. BioInfer was
transformed into a PPI corpus by a program, so
making it the largest of the five. Among them,
AIMed alone was created by annotating whole ab-
stracts, while the other corpora were made by an-
notating single sentences selected from abstracts.
Figure 7 shows the learning curves on two large
corpora: AIMed and BioInfer. The curves are
obtained by performing a 10-fold cross valida-
tion (CV) on each corpus, with random splits, us-
ing our system. The curves show that the perfor-
mances can benefit from the additional examples.
To get a better PPI extraction system for a chosen
target, we need to draw useful shared information
from external source corpora. We refer to exam-
ples in the source corpora as ?source examples?,
and examples in a target corpus as ?target exam-
ples?. Among the corpora, we assume that the la-
bels in some examples are incompatible, and that
their distributions are also different, but that the
feature space is shared.
In order to draw useful information from the
source corpora to get a better model for the target
124
ENTITY1
NN
IP
protein
NN
IP
interact
VBZ
IP
with
IN
IP
multiple
JJ
subunit
NNS
of
IN
PROT
NN
and
CC
with
IN
IP
ENTITY2
NN
IP
protein
NN
IP
.
.
NMOD
IP
SBJ
IP
COOD
IP
PMOD
NMOD NMOD
PMOD
CC
COORD
IP
NMOD
IP
PMOD
IP
P
ENTITY1
NN
protein
NN
M
interact
VBZ
M
with
IN
M
multiple
JJ
M
subunit
NNS
M
of
IN
M
PROT
NN
M
and
CC
M
with
IN
M
ENTITY2
NN
protein
NN
A
.
.
0.9,            0.3
IP: In shortest Path, B:Before, M:in the Middle of, A:After
Figure 6: Parse structure subgraph and linear order subgraph to extract graph features of the pair in
Figure 2. The parse structure subgraph is from the parse tree by KSDEP.
corpus, we use SVM-CW, which has been used
as a DA method. Given a set of instance-label
pairs (xi, yi), i = 1, . . ., ls + lt, xi?Rn, and
y
i
?{?1,+1}, we solve the following problem:
min
w
1
2
w
T
w + C
s
ls
?
i=1
`
i
+ C
t
ls+lt
?
i=ls+1
`
i
, (2)
where w is a weight vector, ` is a loss function,
and ls and lt are the numbers of source and target
examples respectively. C
s
? 0 and C
t
? 0 are
penalty parameters. We use a squared hinge loss
`
i
= max(0, 1? y
i
w
T
xi)2. Here, the source cor-
pora are treated as one corpus. The problem, ex-
cluding the second term, is equal to L2-SVM. The
problem can be solved using the DCD method.
As an ITL method, SVM-CW weights each cor-
pus, and tries to benefit from the source corpora,
by adjusting the effect of their compatibility and
incompatibility. For the adjustment, these penalty
parameters should be set properly. Since we are
unaware of the widely ranged differences among
the corpora, we empirically estimated them by
performing 10-fold CV on the training data.
4 Evaluation
4.1 Evaluation Settings
We used five corpora for evaluation: AIMed,
BioInfer, HPRD50, IEPA, and LLL. For the com-
parison with other methods, we report the F-
score (%), and the area under the receiver op-
erating characteristic (ROC) curve (AUC) (%)
using (abstract-wise) a 10-fold CV and a one-
answer-per-occurrence criterion. These measures
are commonly used for the PPI extraction tasks.
The F-score is a harmonic mean of Precision and
Recall. The ROC curve is a plot of a true posi-
tive rate (TPR) vs a false positive rate (FPR) for
different thresholds. We tuned the regularization
parameters of all classifiers by performing a 10-
fold CV on the training data using a random split.
The other parameters were fixed, and we report the
highest of the macro-averaged F-scores as our fi-
nal F-score. For 10-fold CV, we split the corpora
as recommended by Airola et al (2008).
4.2 PPI Extraction on a Single Corpus
In this section, we evaluate our system on a single
corpus, in order to evaluate our feature vector and
to justify the use of the following modules: nor-
malization methods and classification methods.
First, we compare our preprocessing method
with other preprocessing methods to confirm how
our preprocessing method improves the perfor-
mance. Our method produced 64.2% in F-score
using L2-SVM on AIMed. Scaling all features in-
dividually to have a maximal absolute value of 1,
produced only 44.2% in the F-score, while nor-
malizing the feature vector by L2-norm produced
61.5% in the F-score. Both methods were inferior
to our method, because the values of features in
the same group should be treated together, and be-
cause the values of features in the different groups
should not have a big discrepancy. Weighting each
125
L2 L1 LR AP CW
F 64.2 64.0 64.2 62.7 63.0
AUC 89.1 88.8 89.0 88.5 87.8
Table 2: Classification performance on AIMed us-
ing five different linear classifiers. The F-score (F)
and Area Under the ROC curve (AUC) are shown.
L2 is L2-SVM, L1 is L1-SVM, LR is logistic re-
gression, AP is averaged perceptron, and CW is
confidence weighted linear classification.
group with different values can produce better re-
sults, as will be explored in our future work.
Next, using our feature vector, we applied
five different linear classifiers to extract PPI
from AIMed: L2-SVM, 1-norm soft-margin
SVM (L1-SVM), logistic regression (LR) (Fan
et al, 2008), averaged perceptron (AP) (Collins,
2002), and confidence weighted linear classifica-
tion (CW) (Dredze et al, 2008). Table 2 indicates
the performance of these classifiers on AIMed.
We employed better settings for the task than did
the original methods for AP and CW. We used a
Widrow-Hoff learning rule (Bishop, 1995) for AP,
and we performed one iteration for CW. L2-SVM
is as good as, if not better, than other classifiers (F-
score and AUC). In the least, L2-SVM is as fast as
these classifiers. AP and CW are worse than the
other three methods, because they require a large
number of examples, and are un-suitable for the
current task. This result indicates that all linear
classifiers, with the exception of AP and CW, per-
form almost equally, when using our feature vec-
tor.
Finally, we implemented the kernel method by
Miwa et al (2008). For a 10-fold CV on AIMed,
the running time was 9,507 seconds, and the per-
formance was 61.5% F-score and 87.1% AUC.
Our system used 4,702 seconds, and the perfor-
mance was 64.2% F-score and 89.1% AUC. This
result displayed that our system, with L2-SVM,
and our new feature vector, is better, and faster,
than the kernel-based system.
4.3 Evaluation of Corpus Weighting
In this section, we first apply each model from a
source corpus to a target corpus, to show how dif-
ferent the corpora are. We then evaluate SVM-CW
by comparing it with three other methods (see Sec-
tion 2) with limited features, and apply it to every
corpus.
0
10
20
30
40
50
60
70
80
90
AIMed BioInfer HPRD50 IEPA LLL
F
Target corpus
AIMed
BioInfer
HPRD50
IEPA
LLL
co-occ
Model
Figure 8: F-score on a target corpus using a model
on a source corpus. For the comparison, we show
the 10-fold CV result on each target corpus and
co-occurrences. The regularization parameter was
fixed to 1.
First, we apply the model from a source corpus
to a target corpus. Figure 8 shows how the model
from a source corpus performs on the target cor-
pus. Interestingly, the model from IEPA performs
better on LLL than the model from LLL itself. All
the results showed that using different corpora (ex-
cept IEPA) is worse than just using the same cor-
pora. However, the cross-corpora scores are still
better than the co-occurrences base-line, which in-
dicates that the corpora share some information,
even though they are not fully compatible.
Next, we compare SVM-CW with three other
methods: aSVM, SVD-ASO, and TrAdaBoost.
For this comparison, we used our feature vec-
tor without including the graph features, because
SVD-ASO and TrAdaBoost require large compu-
tational resources. We applied SVD-ASO and
TrAdaBoost in the following way. As for SVD-
ASO, we made 400 auxiliary problems from the
labels of each corpus by splitting features ran-
domly, and extracted 50 additional features each
for 4 feature groups. In total, we made new 200
additional features from 2,000 auxiliary problems.
As recommended by Ando et al (2005), we re-
moved negative weights, performed SVD to each
feature group, and iterated ASO once. Since Ad-
aBoost easily overfitted with our rich feature vec-
tor, we applied soft margins (Ratsch et al, 2001)
to TrAdaBoost. The update parameter for source
examples was calculated using the update param-
eter on the training data in AdaBoost and the orig-
inal parameter in TrAdaBoost. This ensures that
the parameter would be the same as the original
parameter, when the C value in the soft margin ap-
proaches infinity.
126
aSVM SVD-ASO TrAdaBoost SVM-CW L2-SVM
F AUC F AUC F AUC F AUC F AUC
AIMed 63.6 88.4 62.9 88.3 63.4 88.4 64.0 88.6 63.2 88.4
BioInfer 66.5 85.2 65.7 85.1 66.1 85.2 66.7 85.4 66.2 85.1
HPRD50 71.2 84.3 68.7 80.8 72.6 85.3 72.7 86.4 67.2 80.7
IEPA 73.8 85.4 72.3 83.8 74.3 86.3 75.2 85.9 73.0 84.7
LLL 85.9 89.2 79.3 85.5 86.5 88.8 86.9 90.3 80.3 86.3
Table 3: Comparison of methods on multiple corpora. Our feature vector without graph features is used.
The source corpora with the best F-scores are reported for aSVM, TrAdaBoost, and SVM-CW.
F-score AUC
A B H I L all A B H I L all
A (64.2) 64.0 64.7 65.2 63.7 64.2 (89.1) 89.5 89.2 89.3 89.0 89.4
B 67.9 (67.6) 67.9 67.9 67.7 68.3 86.2 (86.1) 86.2 86.3 86.2 86.4
H 71.3 71.2 (69.7) 74.1 70.8 74.9 84.7 85.0 (82.8) 85.0 83.4 87.9
I 74.4 75.6 73.7 (74.4) 74.4 76.6 86.7 87.1 85.4 (85.6) 86.9 87.8
L 83.2 85.9 82.0 86.7 (80.5) 84.1 86.3 87.1 87.4 90.8 (86.0) 86.2
Table 4: F-score and AUC by SVM-CW. Rows correspond to a target corpus, and columns a source
corpus. A:AIMed, B:BioInfer, H:HPRD50, I:IEPA, and L:LLL corpora. ?all? signifies that all source
corpora are used as one source corpus, ignoring the differences among the corpora. For the comparison,
we show the 10-fold CV result on each target corpus.
In Table 3, we demonstrate the results of the
comparison. SVM-CW improved the classifica-
tion performance at least as much as all the other
methods. The improvement is mainly attributed to
the aggressive use of source examples while learn-
ing the model. Some source examples can be used
as training data, as indicated in Figure 8. SVM-
CW does not set the restriction between C
s
and
C
t
in Equation (2), so it can use source exam-
ples aggressively while learning the model. Since
aSVM transfers a model, and SVD-ASO transfers
an additional feature space, aSVM and SVD-ASO
do not use the source examples while learning the
model. In addition to the difference in the data us-
age, the settings of aSVM and SVD-ASO do not
match the current task. As for aSVM, the DA as-
sumption (that the labels are the same) does not
match the task. In SVD-ASO, the numbers of both
source examples and auxiliary problems are much
smaller than those reported by Ando et al (2005).
TrAdaBoost uses the source examples while learn-
ing the model, but never increases the weight of
the examples, and it attempts to reduce their ef-
fects.
Finally, we apply SVM-CW to all corpora using
all features. Table 4 summarizes the F-score and
AUC by SVM-CW with all features. SVM-CW
is especially effective for small corpora, show-
ing that SVM-CW can adapt source corpora to a
small annotated target corpus. The improvement
on AIMed is small compared to the improvement
on BioInfer, even though these corpora are sim-
ilar in size. One of the reasons for this is that
whole abstracts are annotated in AIMed, therefore
making the examples biased. The difference be-
tween L2-SVM and SVM-CW + IEPA on AIMed
is small, but statistically, it is significant (McNe-
mar test (McNemar, 1947), P = 0.0081). In the
cases of HPRD50 + IEPA, LLL + IEPA, and two
folds in BioInfer + IEPA, C
s
is larger than C
t
in
Equation (2). This is worth noting, because the
source corpus is more weighted than the target cor-
pus, and the prediction performance on the tar-
get corpus is improved. Most methods put more
trust in the target corpus than in the source cor-
pus, and our results show that this setting is not al-
ways effective for mixing corpora. The results also
indicate that IEPA contains more useful informa-
tion for extracting PPI than other corpora, and that
using source examples aggressively is important
for these combinations. We compared the results
of L2-SVM and SVM-CW + IEPA on AIMed,
and found that 38 pairs were described as ?inter-
action? or ?binding? in the sentences among 61
127
SVM-CW L2-SVM Airola et al
F AUC F AUC F AUC
A 65.2 89.3 64.2 89.1 56.4 84.8
B 68.3 86.4 67.6 86.1 61.3 81.9
H 74.9 87.9 69.7 82.8 63.4 79.7
I 76.6 87.8 74.4 85.6 75.1 85.1
L 86.7 90.8 80.5 86.0 76.8 83.4
Table 6: Comparison with the results by Airola
et al (2008). A:AIMed, B:BioInfer, H:HPRD50,
I:IEPA, and L:LLL corpora. The results with the
highest F-score from Table 4 are reported as the
results for SVM-CW.
newly found pairs. This analysis is evidence that
IEPA contains instances to help find such inter-
actions, and that SVM-CW helps to collect gold
pairs that lack enough supporting instances in a
single corpus, by adding instances from other cor-
pora. SVM-CW missed coreferential relations that
were also missed by L2-SVM. This can be at-
tributed to the fact that the coreferential informa-
tion is not stored in our current feature vector; so
we need an even more expressive feature space.
This is left as future work.
SVM-CW is effective on most corpus combi-
nations, and all the models from single corpora
can be improved by adding other source corpora.
This result is impressive, because the baselines by
L2-SVM on just single corpora are already better
than or at least comparable to other state-of-the-art
PPI extraction systems, and also because the vari-
ety of the differences among different corpora is
quite wide depending on various factors including
annotation policies of the corpora (Pyysalo et al,
2008). The results suggest that SVM-CW is useful
as an ITL method.
4.4 Comparison with Other PPI Systems
We compare our system with other previously
published PPI extraction systems. Tables 5 and
6 summarize the comparison. Table 5 summa-
rizes the comparison of several PPI extraction sys-
tems evaluated on the AIMed corpus. As indi-
cated, the performance of the heavy kernel method
is lower than our fast rich feature-vector method.
Our system is, to the extent of our knowledge, the
best performing PPI extraction system evaluated
on the AIMed corpus, both in terms of AUC and
F-scores. Airola et al (2008) first reported results
using all five corpora. We cannot directly com-
pare our result with the F-score results, because
they tuned the threshold, but our system still out-
performs the system by Airola et al (2008) on ev-
ery corpus in AUC values. The results also indi-
cate that our system outperforms other systems on
all PPI corpora, and that both the rich feature vec-
tor and the corpus weighting are effective for the
PPI extraction task.
5 Conclusion
In this paper, we proposed a PPI extraction system
with a rich feature vector, using a corpus weight-
ing method (SVM-CW) for combining the mul-
tiple PPI corpora. The feature vector extracts as
much information as possible from the main train-
ing corpus, and SVM-CW incorporate other exter-
nal source corpora in order to improve the perfor-
mance of the classifier on the main target corpus.
To the extent of our knowledge, this is the first ap-
plication of ITL and DA methods to PPI extrac-
tion. As a result, the system, with SVM-CW and
the feature vector, outperformed all other PPI ex-
traction systems on all of the corpora. The PPI
corpora share some information, and it is shown
to be effective to add other source corpora when
working with a specific target corpus.
The main contributions of this paper are: 1)
conducting experiments in extracting PPI using
multiple corpora, 2) suggesting a rich feature
vector using several previously proposed features
and normalization methods, 3) the combination of
SVM with corpus weighting and the new feature
vector improved results on this task compared with
prior work.
There are many differences among the corpora
that we used, and some of the differences are still
unresolved. For further improvement, it would be
necessary to investigate what is shared and what
is different among the corpora. The SVM-CW
method, and the PPI extraction system, can be ap-
plied generally to other classification tasks, and
to other binary relation extraction tasks, without
the need for modification. There are several other
tasks in which many different corpora, which at
first glance seem compatible, exist. By apply-
ing SVM-CW to such corpora, we will analyze
which differences can be resolved by SVM-CW,
and what differences require a manual resolution.
For the PPI extraction system, we found many
false negatives that need to be resolved. For fur-
ther improvement, we need to analyze the cause
128
positive all P R F AUC
SVM-CW 1,000 5,834 60.0 71.9 65.2 89.3
L2-SVM 1,000 5,834 62.7 66.6 64.2 89.1
(Miwa et al, 2008) 1,005 5,648 60.4 69.3 64.2 (61.5) 87.9 (87.1)
(Miyao et al, 2008) 1,059 5,648 54.9 65.5 59.5
(Airola et al, 2008) 1,000 5,834 52.9 61.8 56.4 84.8
(S?tre et al, 2007) 1,068 5,631 64.3 44.1 52.0
(Erkan et al, 2007) 951 4,020 59.6 60.7 60.0
(Bunescu and Mooney, 2005) 65.0 46.4 54.2
Table 5: Comparison with previous PPI extraction results on the AIMed corpus. The numbers of positive
and all examples, precision (P), recall (R), F-score (F), and AUC are shown. The result with the highest
F-score from Table 4 is reported as the result for SVM-CW. The scores in the parentheses of Miwa et al
(2008) indicate the result using the same 10-fold splits as our result, as indicated in Section 4.2.
of these false negatives more deeply, and design a
more discriminative feature space. This is left as a
future direction of our work.
Acknowledgments
This work was partially supported by Grant-in-Aid
for Specially Promoted Research (MEXT, Japan),
Genome Network Project (MEXT, Japan), and
Scientific Research (C) (General) (MEXT, Japan).
References
Antti Airola, Sampo Pyysalo, Jari Bjo?rne, Tapio
Pahikkala, Filip Ginter, and Tapio Salakoski. 2008.
All-paths graph kernel for protein-protein interac-
tion extraction with evaluation of cross corpus learn-
ing. BMC Bioinformatics.
Rie Kubota Ando, Tong Zhang, and Peter Bartlett.
2005. A framework for learning predictive struc-
tures from multiple tasks and unlabeled data. Jour-
nal of Machine Learning Research, 6:1817?1853.
Rie Kubota Ando. 2006. Applying alternating struc-
ture optimization to word sense disambiguation. In
Proceedings of the Tenth Conference on Compu-
tational Natural Language Learning (CoNLL-X),
pages 77?84, June.
C. M. Bishop. 1995. Neural Networks for Pattern
Recognition. Oxford University Press.
Razvan C. Bunescu and Raymond J. Mooney. 2005.
Subsequence kernels for relation extraction. In
NIPS 2005.
Razvan C. Bunescu, Ruifang Ge, Rohit J. Kate, Ed-
ward M. Marcotte, Raymond J. Mooney, Arun K.
Ramani, and Yuk Wah Wong. 2005. Comparative
experiments on learning information extractors for
proteins and their interactions. Artificial Intelligence
in Medicine, 33(2):139?155.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: theory and experi-
ments with perceptron algorithms. In EMNLP 2002,
pages 1?8.
Wenyuan Dai, Qiang Yang, Gui-Rong Xue, and Yong
Yu. 2007. Boosting for transfer learning. In ICML
2007, pages 193?200.
J. Ding, D. Berleant, D. Nettleton, and E. Wurtele.
2002. Mining medline: abstracts, sentences, or
phrases? Pacific Symposium on Biocomputing,
pages 326?337.
Mark Dredze, Koby Crammer, and Fernando Pereira.
2008. Confidence-weighted linear classification. In
ICML 2008, pages 264?271.
Gunes Erkan, Arzucan Ozgur, and Dragomir R. Radev.
2007. Semi-supervised classification for extract-
ing protein interaction sentences using dependency
parsing. In EMNLP 2007.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification. Journal of
Machine Learning Research, 9:1871?1874.
Katrin Fundel, Robert Ku?ffner, and Ralf Zimmer.
2006. Relex?relation extraction using dependency
parse trees. Bioinformatics, 23(3):365?371.
Cho-Jui Hsieh, Kai-Wei Chang, Chih-Jen Lin,
S. Sathiya Keerthi, and S. Sundararajan. 2008. A
dual coordinate descent method for large-scale lin-
ear SVM. In ICML 2008, pages 408?415.
Jin-Dong Kim, Tomoko Ohta, Yuka Tateisi, and
Jun?ichi Tsujii. 2003. GENIA corpus ? a semanti-
cally annotated corpus for bio-textmining. Bioinfor-
matics, 19:i180?i182.
Jin-Dong Kim, Tomoko Ohta, and Jun?ichi Tsujii.
2008a. Corpus annotation for mining biomedical
events from literature. BMC Bioinformatics, 9:10.
129
Seonho Kim, Juntae Yoon, and Jihoon Yang. 2008b.
Kernel approaches for genic interaction extraction.
Bioinformatics, 24(1):118?126.
Suresh Mathivanan, Balamurugan Periaswamy, TKB
Gandhi, Kumaran Kandasamy, Shubha Suresh, Riaz
Mohmood, YL Ramachandra, and Akhilesh Pandey.
2006. An evaluation of human protein-protein inter-
action data in the public domain. BMC Bioinformat-
ics, 7 Suppl 5:S19.
Quinn McNemar. 1947. Note on the sampling error
of the difference between correlated proportions or
percentages. Psychometrika, 12(2):153?157, June.
Makoto Miwa, Rune S?tre, Yusuke Miyao, Tomoko
Ohta, and Jun?ichi Tsujii. 2008. Combining mul-
tiple layers of syntactic information for protein-
protein interaction extraction. In Proceedings of the
Third International Symposium on Semantic Mining
in Biomedicine (SMBM 2008), pages 101?108.
Yusuke Miyao, Rune S?tre, Kenji Sagae, Takuya
Matsuzaki, and Jun?ichi Tsujii. 2008. Task-
oriented evaluation of syntactic parsers and their
representations. In Proceedings of the 45th Meet-
ing of the Association for Computational Linguistics
(ACL?08:HLT).
Claire Ne?dellec. 2005. Learning language in logic -
genic interaction extraction challenge. In Proceed-
ings of the LLL?05 Workshop.
Sinno Jialin Pan and Qiang Yang. 2008. A survey on
transfer learning. Technical Report HKUST-CS08-
08, Department of Computer Science and Engineer-
ing, Hong Kong University of Science and Technol-
ogy, Hong Kong, China, November.
Sampo Pyysalo, Filip Ginter, Juho Heimonen, Jari
Bjo?rne, Jorma Boberg, Jouni Ja?rvinen, and Tapio
Salakoski. 2007. BioInfer: A corpus for infor-
mation extraction in the biomedical domain. BMC
Bioinformatics, 8:50.
Sampo Pyysalo, Antti Airola, Juho Heimonen, Jari
Bjo?rne, Filip Ginter, and Tapio Salakoski. 2008.
Comparative analysis of five protein-protein inter-
action corpora. In BMC Bioinformatics, volume
9(Suppl 3), page S6.
Gunnar Ratsch, Takashi Onoda, and Klaus-Robert
Muller. 2001. Soft margins for adaboost. Machine
Learning, 42(3):287?320.
Rune S?tre, Kenji Sagae, and Jun?ichi Tsujii. 2007.
Syntactic features for protein-protein interaction ex-
traction. In LBM 2007 short papers.
Gabriele Schweikert, Christian Widmer, Bernhard
Scho?lkopf, and Gunnar Ra?tsch. 2008. An empir-
ical analysis of domain adaptation algorithms for
genomic sequence analysis. In NIPS, pages 1433?
1440.
Jun Yang, Rong Yan, and Alexander G. Hauptmann.
2007. Cross-domain video concept detection using
adaptive SVMs. In MULTIMEDIA ?07: Proceed-
ings of the 15th international conference on Multi-
media, pages 188?197.
130
TOWARDS DATA AND GOAL ORIENTED ANALYSIS:  
TOOL INTER-OPERABILITY AND COMBINATORIAL 
COMPARISON 
Yoshinobu Kano1      Ngan Nguyen1      Rune S?tre1       Kazuhiro Yoshida1 
Keiichiro Fukamachi1      Yusuke Miyao1       Yoshimasa Tsuruoka3   
Sophia Ananiadou2,3        Jun?ichi Tsujii1,2,3 
 
1Department of Computer Science, University of Tokyo 
Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033 Tokyo 
 
2School of Computer Science, University of Manchester 
PO Box 88, Sackville St, MANCHESTER M60 1QD, UK 
 
3NaCTeM (National Centre for Text Mining), Manchester Interdisciplinary Biocentre, 
University of Manchester, 131 Princess St, MANCHESTER M1 7DN, UK 
 
{kano,nltngan,satre,kyoshida,keif,yusuke,tsujii} 
@is.s.u-tokyo.ac.jp 
{yoshimasa.tsuruoka,sophia.ananiadou}@manchester.ac.uk 
 
Abstract 
Recently, NLP researches have advanced 
using F-scores, precisions, and recalls with 
gold standard data as evaluation measures. 
However, such evaluations cannot capture 
the different behaviors of varying NLP 
tools or the different behaviors of a NLP 
tool that depends on the data and domain in 
which it works. Because an increasing 
number of tools are available nowadays, it 
has become increasingly important to grasp 
these behavioral differences, in order to 
select a suitable set of tools, which forms a 
complex workflow for a specific purpose. 
In order to observe such differences, we 
need to integrate available combinations of 
tools into a workflow and to compare the 
combinatorial results. Although generic 
frameworks like UIMA (Unstructured 
Information Management Architecture) 
provide interoperability to solve this 
problem, the solution they provide is only 
partial. In order for truly interoperable 
toolkits to become a reality, we also need 
sharable and comparable type systems with 
an automatic combinatorial comparison 
generator, which would allow systematic 
comparisons of available tools. In this 
paper, we describe such an environment, 
which we developed based on UIMA, and 
we show its feasibility through an example 
of a protein-protein interaction (PPI) 
extraction system. 
1 Introduction 
Recently, an increasing number of TM/NLP tools 
such as part-of-speech (POS) taggers (Tsuruoka et 
al., 2005), named entity recognizers (NERs) 
(Settles, 2005) syntactic parsers (Hara et al, 2005) 
and relation or event extractors (ERs) have been 
developed. Nevertheless, it is still very difficult to 
integrate independently developed tools into an 
aggregated application that achieves a specific 
task. The difficulties are caused not only by 
differences in programming platforms and 
different input/output data formats, but also by the 
lack of higher level interoperability among 
modules developed by different groups.  
859
UIMA, Unstructured Information Management 
Architecture (Lally and Ferrucci, 2004), which was 
originally developed by IBM and has recently 
become an open project in OASIS and Apache, 
provides a promising framework for tool 
integration. Although it has a set of useful 
functionalities, UIMA only provides a generic 
framework, thus it requires a user community to 
develop their own platforms with a set of actual 
software modules. A few attempts have already 
been made to establish platforms, e.g. the CMU 
UIMA component repository 1 , GATE 
(Cunningham et al, 2002) with its UIMA 
interoperability layer, etc.  
However, simply wrapping existing modules to 
be UIMA compliant does not offer a complete 
solution. Most of TM/NLP tasks are composite in 
nature, and can only be solved by combining 
several modules. Users need to test a large number 
of combinations of tools in order to pick the most 
suitable combination for their specific task. 
Although types and type systems are the only 
way to represent meanings in the UIMA 
framework, UIMA does not provide any specific 
types, except for a few purely primitive types. In 
this paper, we propose a way to design sharable 
type systems. A sharable type system designed in 
this way can provide the interoperability between 
independently developed tools with fewer losses in 
information, thus allowing for the combinations of 
tools and comparisons on these combinations. 
We show how our automatic comparison 
generator works based on a type system designed in 
that way. Taking the extraction of protein-protein 
                                                 
1 http://uima.lti.cs.cmu.edu/ 
interaction (PPI) as a typical example of a 
composite task, we illustrate how our platform 
helps users to observe the differences between 
tools and to construct a system for their own needs. 
2 Motivation and Background 
2.1 Goal and Data Oriented Evaluation, 
Module Selection and Inter-operability 
There are standard evaluation metrics for NLP 
modules such as precision, recall and F-value. For 
basic tasks like sentence splitting, POS tagging, 
and named-entity recognition, these metrics can be 
estimated using existing gold-standard test sets.  
Conversely, accuracy measurements based on 
the standard test sets are sometimes deceptive, 
since its accuracy may change significantly in 
practice, depending on the types of text and the 
actual tasks at hand. Because these accuracy 
metrics do not take into account the importance of 
the different types of errors to any particular 
application, the practical utility of two systems 
with seemingly similar levels of accuracy may in 
fact differ significantly. To users and developers 
alike, a detailed examination of how systems 
perform (on the text they would like to process) is 
often more important than standard metrics and 
test sets. Naturally, far greater weight is placed in 
measuring the end-to-end performance of a 
composite system than in measuring the 
performance of the individual components. 
In reality, because the selection of modules 
usually affects the performance of the entire 
system, it is crucial to carefully select modules that 
are appropriate for a given task. This is the main 
reason for having a collection of interoperable 
 
 
TOOL-SPECIFIC TYPES
PennPOS 
Penn verb1 ? ?
POS 
tcas.uima.Annotation 
-begin: int  -end: int 
SyntacticAnnotation SemanticAnnotation 
Sentence Phrase Token NamedEntity Relation 
-ent: FSArray<NamedEntity>
POSToken 
-pos: POS 
RichToken 
uima.jcas.cas.TOP 
UnknownPOS 
-base: String 
-posType: String 
ToolAToken
Verb Noun ?.. 
ToolBPOSToken
Protein 
ToolCProtein
ProteinProteinInteraction
ToolDPPI
Figure 1. Part of our type system 
860
modules. We need to show how the ultimate 
performance will be affected by the selection of 
different modules and show the best combination 
of modules in terms of the performance of the 
whole aggregated system for the task at hand. 
 Since the number of possible combinations of 
component modules is typically large, the system 
has to be able to enumerate and execute them 
semi-automatically. This requires a higher level of 
interoperability of individual modules than just 
wrapping them for UIMA.  
2.2 UIMA 
2.2.1 CAS and Type System 
The UIMA framework uses the ?stand-off 
annotation? style (Ferrucci et al, 2006). The raw 
text in a document is kept unchanged during the 
analysis process, and when the processing of the 
text is performed, the result is added as new stand-
off annotations with references to their positions in 
the raw text. A Common Analysis Structure (CAS) 
maintains a set of these annotations, which in itself 
are objects. The annotation objects in a CAS 
belong to types that are defined separately in a 
hierarchical type system. The features of an 
annotation2  object have values that are typed as 
well. 
2.2.2 Component and Capability 
Each UIMA Component has the capability 
property which describes what types of objects the 
component may take as the input and what types of 
objects it produces as the output. For example, a 
named entity recognizer detects named entities in 
                                                 
tools. Types should be defined in a distinct and 
2 In the UIMA framework, Annotation is a base type which 
has begin and end offset values. In this paper we call any 
objects (any subtype of TOP) as annotations. 
the text and outputs annotation objects of the type 
NamedEntity. 
It is possible to deploy any UIMA component as 
a SOAP web service, so that we can combine a 
remote component on a web service with the local 
component freely inside a UIMA-based system.  
3 Integration Platform and Comparators 
3.1 Sharable and Comparable Type System 
Although UIMA provides a set of useful 
functionalities for an integration platform of 
TM/NLP tools, users still have to develop the 
actual platform by using these functionalities 
effectively. There are several decisions for the 
designer to make an integration platform. 
Determining how to use types in UIMA is a 
crucial decision. Our decision is to keep different 
type systems by individual groups as they are, if 
necessary; we require that individual type systems 
have to be related through a sharable type system, 
which our platform defines. Such a shared type 
system can bridge modules with different type 
systems, though the bridging module may lose 
some information during the translation process.  
Whether such a sharable type system can be 
defined or not is dependent on the nature of each 
problem.  For example, a sharable type system for 
POS tags in English can be defined rather easily, 
since most of POS-related modules (such as POS 
taggers, shallow parsers, etc.) more or less follow 
the well established types defined by the Penn 
Treebank (Marcus et al, 1993) tag set. 
Figure 1 shows a part of our sharable type 
system. We deliberately define a highly organized 
type hierarchy as described above.  
Secondly we should consider that the type 
system may be used to compare a similar sort of 
Comparable Tools 
Sentence 
Detector
Deep 
Parser 
Named  
Entity 
Recognizer 
POS 
Tagger 
PPI 
Extractor 
AImed 
Collection 
Reader 
Comparator 
Evaluator 
Tokenizer 
Figure 2. PPI system workflow  
(conceptual) 
Figure 3.  
Basic example pattern
Comparable Tools
OpenNLP 
Sentence 
Detector 
Enju ABNER 
Stepp 
Tagger
UIMA 
Tokenizer
Figure 4.  
Complex tool example 
Comparable Tools 
GENIA 
Tagger 
OpenNLP 
Sentence 
Detector 
Enju NER 
POS 
Tagger
Tokenizer
Figure 5.  
Branch flow pattern 
Comparable Tools
OpenNLP 
S.D. 
UIMA 
Tokenizer
Enju ABNER 
Stepp 
Tagger
GENIA 
S.D. 
861
hierarchical manner. For example, both tokenizers 
and POS taggers output an object of type Token, 
but their roles are different when we assume a 
cascaded pipeline. We defined Token as a 
supertvpe, POSToken as subtypes of Token. Each 
tool should have an individual type to make clear 
which tool generated which instance, because each 
tool may have a slightly different definition. This 
is important because the capabilities are 
represented by these types, and the capabilities are 
the only attributes which are machine readable. 
3.2 General Combinatorial Comparison 
stem is defined in the previously 
tually shows the workflow of our 
wh
 pattern expansion mechanism which 
ge
cases, a single tool can play two or 
m
                                                
Generator 
Even if the type sy
described way, there are still some issues to 
consider when comparing tools. We illustrate these 
issues using the PPI workflow that we utilized in 
our experiments. 
Figure 2 concep
ole PPI system. If we can prepare two or more 
components for some type of the components in 
the workflow (e.g. two sentence detectors and three 
POS taggers), then we can make combinations of 
these tools to form a multiplied number of 
workflow patterns (2x3 = 6 patterns). See Table 1 
for the details of UIMA components used in our 
experiments. 
We made a
nerates possible workflow patterns automatically 
from a user-defined comparable workflow. A 
comparable workflow is a special workflow that 
explicitly specifies which set of components 
should be compared. Then, users just need to group 
comparable components (e.g. ABNER3 and MedT-
NER as a comparable NER group) without making 
any modifications to the original UIMA 
components. This aggregation of comparable 
components is controlled by our custom workflow 
controller.  
In some 
ore roles (e.g. the GENIA Tagger performs 
tokenization, POS tagging, and NER; see Figure 
4). It may be possible to decompose the original 
tool into single roles, but in most cases it is 
difficult and unnatural to decompose such a 
 
ponent requires two or more input 
ty
4 Experiments and Results 
 using our PPI 
e have several 
co
igure 6 show a part of the 
co
Table 2.   
3 In the example figures, ABNER requires Sentence to 
make the explanation clearer, though ABNER does not 
require it in actual usage. 
complex tool. We designed our comparator to 
detect possible input combinations automatically 
by the types of previously generated annotations, 
and the input capability of each posterior 
component. As described in the previous section, 
the component should have appropriate 
capabilities with proper types in order to permit 
this detection.  
When a com
pes (e.g. our PPI extractor requires outputs of a 
deep parser and a protein NER system), there 
could be different components used in the prior 
flow (e.g. OpenNLP and GENIA sentence 
detectors in Figure 5). Our comparator also 
calculates such cases automatically. 
 OO UO GOO U G A
UU 8 89 8
We have performed experiments
extraction system as an example (Kano et al, 
2008). It is similar to our BioCreative PPI system 
(S?tre et al, 2006) but differs in that we have 
deconstructed the original system into seven 
different components (Figure 2).  
As summarized in Table 1, w
mparable components and the AImed corpus as 
the gold standard data. In this case, possible 
combination workflow patterns are POSToken for 
36, PPI for 589, etc.   
Table 2, 3, 4 and F
mparison result screenshots between these 
patterns on 20 articles from the AImed corpus. In 
the tables, abbreviations like ?OOG? stands for a 
workflow of O(Sentence) -> O(Token) - 
Sentence
comparisons (%). 
Table 3. Part of Token
comparisons, 
precision/recall (%).
OOO UOS GOO 
UUO 87/74 81/68 85/68 
GUG 74/65 73/65 78/65 
GGO 92/95 81/84 97/95 
OGO 100/100 89/88 100/94 
G 0 0 - 85
U
 9/75 /75 8/70
GU 89/75 89/75 88/70
GG 92/95 91/95 97/95
OG 
86 - 0 7
A 6 6 60 -
O - 10 10/100 99/99 00/9481 0 7
Table 4. Part of POSToken comparisons, 
precision/recall (%) 
862
G(POSToken), where O stands for OpenNLP, G 
stands for Genia, U stands for UIMA, etc.  
When neither of the compared results include 
th
e comparison on Sentences 
sh
%  
0 
e gold standard data (AImed in this case), the 
comparison results show a similarity of the tools 
for this specific task and data, rather than an 
evaluation. Even if we lack an annotated corpus, it 
is possible to run the tools and compare the results 
in order to understand the characteristics of the 
tools depending on the corpus and the tool 
combinations.  
Although th
ows low scores of similarities, Tokens are 
almost the same; it means that input sentence 
boundaries do not affect tokenizations so much. 
POSToken similarities drop approximately 0-10
100 
  
                      100
Fi  6  NER (Protein) comp rison di
ences in 
5 Conclusion and Future Work 
ponents, 
 design, which the UIMA 
fra
   0  
gure . a stribution of 
precisions (x-axis, %) and recalls (y-axis, %). 
from the similarities in Token; the differ
Token are mainly apostrophes and punctuations; 
POSTokens are different because each POS 
tagger uses a slightly different set of tags: normal 
Penn tagset for Stepp tagger, BioPenn tagset 
(includes new tags for hyphenation) for GENIA 
tagger, and an original apostrophe tag for 
OpenNLP tagger. 
NLP tasks typically consist of many com
and it is necessary to show which set of tools are 
most suitable for each specific task and data. 
Although UIMA provides a general framework 
with much functionality for interoperability, we 
still need to build an environment that enables the 
combinations and comparisons of tools for a 
specific task.  
The type system
mework does not provide, is one of the most 
critical issues on interoperability. We have thus 
proposed a way to design a sharable and 
comparable type system. Such a type system allows 
for the automatic combinations of any UIMA 
compliant components and for the comparisons of 
these combinations, when the components have 
proper capabilities within the type system. We are 
Sentence Token POSToken RichToken Protein Phrase PPI
GENIA Tagger: Trained on the WSJ, GENIA and PennBioIE corpora (POS). Uses Maximum Entropy (Berger 
et al, 1996) classification, trained on JNLPBA (Kim et al, 2004) (NER). Trained on GENIA corpus (Sentence 
Splitter). 
Enju: HPSG parser with predicate argument structures as well as phrase structures. Although trained with Penn 
Treebank, it can compute accurate analyses of biomedical texts owing to its method for domain adaptation (Hara 
et al, 2005). 
STePP Tagger: Based on probabilistic models, tuned to biomedical text trained by WSJ, GENIA (Kim et al, 
2003)  and PennBioIE corpora. 
MedT-NER: Statistical recognizer trained on the JNLPBA data. 
ABNER: From the University of Wisconsin (Settles, 2005), wrapped by the Center for Computational 
Pharmacology at the University of Colorado.  
Akane++: A new version of the AKANE system (Yakushiji, 2006), trained with SVMlight-TK (Joachims, 1999; 
Bunescu and Mooney, 2006; Moschitti, 2006) and the AImed Corpus. 
UIMA Examples: Provided in the Apache UIMA example. Sentence Splitter and Tokenizer. 
OpenNLP Tools: Part of the OpenNLP project (http://opennlp.sourceforge.net/), from Apache UIMA examples. 
AImed Corpus: 225 Medline abstracts with proteins and PPIs annotated (Bunescu and Mooney, 2006).   
Legend:         Input type(s) required for that tool          Input type(s) required optionally          Output type(s)  
Table 1. List of UIMA Components used in our experiment. 
863
preparing to make a portion of the components and 
services described in this paper publicly available 
(http://www-tsujii.is.s.u-tokyo.ac.jp/uima/). 
The final system shows which combination of 
co
or this work includes 
co
cknowledgments 
e wish to thank Dr. Lawrence Hunter?s text 
References 
Vincent J. Della Pietra, and Stephen 
IT 
 Mooney. 
on." Edited 
tcheva, and V. 
ls and 
m Lally, Daniel Gruhl, and Edward 
RC24122. (2006). 
ilistic disambiguation model of an 
t, 
e 
." MIT Press, (1999): 169-
ls 
ser: a tool comparator, using protein-protein 
i. "Introduction to the Bio-Entity 
d 
ics 
 i180-
le Application with the Unstructured Information 
l 43, 
ng a Large Annotated Corpus of 
ractical 
. (2006). 
oko 
 
cally tagging genes, proteins, and other entity 
rsity 
, 
ust Part-of-
tion 
University of Tokyo, (2006).  
mponents has the best score, and also generates 
comparative results. This helps users to grasp the 
characteristics and differences among tools, which 
cannot be easily observed by the widely used F-
score evaluations only. 
Future directions f
mbining the output of several modules of the 
same kind (such as NERs) to obtain better results, 
collecting other tools developed by other groups 
using the sharable type system, making machine 
learning tools UIMA compliant, and making grid 
computing available with UIMA workflows to 
increase the entire performance without modifying 
the original UIMA components. 
 
A
 
W
mining group at the Center for Computational 
Pharmacology for discussing with us and making 
their tools available for this research. This work 
was partially supported by NaCTeM (the UK 
National Centre for Text Mining), Grant-in-Aid for 
Specially Promoted Research (MEXT, Japan) and 
Genome Network Project (MEXT, Japan). 
NaCTeM is jointly funded by 
JISC/BBSRC/EPSRC. 
Berger, Adam L., 
A. Della Pietra. "A maximum entropy approach to 
natural language processing." Comput. Linguist. (M
Press) 22, no. 1 (1996): 39-71. 
Bunescu, Razvan, and Raymond
"Subsequence Kernels for Relation Extracti
by Weiss Y., Scholkopf B. and Platt J., 171-178. 
Cambridge, MA: MIT Press, (2006). 
Cunningham, H., D. Maynard, K. Bon
Tablan. "GATE: A framework and graphical 
development environment for robust NLP too
applications." Proceedings of the 40th Anniversary 
Meeting of the Association for Computational 
Linguistics. (2002). 
Ferrucci, David, Ada
Epstein. "Towards an Interoperability Standard for Text 
and Multi-Modal Analytics." IBM Research Report, 
Hara, Tadayoshi, Yusuke Miyao, and Jun'ichi Tsujii. 
"Adapting a probab
HPSG parser to a new domain." Edited by Dale Rober
Wong Kam-Fai, Su Jian and Yee Oi. Natural Languag
Processing IJCNLP 2005. Jeju Island, Korea: Springer-
Verlag, (2005). 199-210. 
Joachims, Thorsten. "Making large-scale support vector 
machine learning practical
184. 
Kano, Yoshinobu, et al "Filling the gaps between too
and u
interaction as an example." Proceedings of The Pacific 
Symposium on Biocomputing (PSB). Hawaii, USA, To 
appear, (2008). 
Kim, Jin-Dong, Tomoko Ohta, Yoshimasa Tsuruoka, 
and Yuka Tateis
Recognition Task at JNLPBA." Proceedings of the 
International Workshop on Natural Language 
Processing. Geneva, Switzerland, (2004). 70-75. 
Kim, Jin-Dong, Tomoko Ohta, Yuka Teteisi, an
Jun'ichi Tsujii. "GENIA corpus - a semantically 
annotated corpus for bio-textmining." Bioinformat
(Oxford University Press) 19, no. suppl. 1 (2003):
i182. 
Lally, Adam, and David Ferrucci. "Building an 
Examp
Management Architecture." IBM Systems Journa
no. 3 (2004): 455-475. 
Marcus, Mitchell P., Beatrice Santorini, and Mary Ann 
Marcinkiewicz. "Buildi
English: The Penn Treebank." Computational 
Linguistics 19, no. 2 (1993): 313-330. 
Moschitti, Alessandro. "Making Tree Kernels P
for Natural Language Learning." EACL
S?tre, Rune, Kazuhiro Yoshida, Akane Yakushiji, 
Yusuke Miyao, Yuichiroh Matsubayashi, and Tom
Ohta. "AKANE System: Protein-Protein Interaction
Pairs in BioCreAtIvE2 Challenge." Proceedings of the 
Second BioCreative Challenge Evaluation Workshop. 
(2007). 
Settles, B. "ABNER: an open source tool for 
automati
names in text." Bioinformatics (Oxford Unive
Press) 21, no. 14 (2005): 3191-3192. 
Tsuruoka, Yoshimasa, Yuka Tateishi, Jin-Dong Kim
and Tomoko Ohta. "Developing a Rob
Speech Tagger for Biomedical Text." Advances in 
Informatics - 10th Panhellenic Conference on 
Informatics. Volos, Greece, (2005). 382-392. 
Yakushiji, Akane. "Relation Information Extrac
Using Deep Syntactic Analysis." PhD Thesis, 
864
Combining Multiple Models for Speech Information Retrieval 
Muath Alzghool and Diana Inkpen  
School of Information Technology and Engineering 
University of Ottawa 
{alzghool,diana}@ site.uottawa.ca  
Abstract 
In this article we present a method for combining different information retrieval models in order to increase the retrieval performance 
in a Speech Information Retrieval task. The formulas for combining the models are tuned on training data. Then the system is evaluated 
on test data. The task is particularly difficult because the text collection is automatically transcribed spontaneous speech, with many 
recognition errors. Also, the topics are real information needs, difficult to satisfy. Information Retrieval systems are not able to obtain 
good results on this data set, except for the case when manual summaries are included. 
 
1. Introduction  
Conversational speech such as recordings of interviews or 
teleconferences is difficult to search through. The 
transcripts produced with Automatic Speech Recognition 
(ASR) systems tend to contain many recognition errors, 
leading to low Information Retrieval (IR) performance 
(Oard et al, 2007). 
Previous research has explored the idea of combining 
the results of different retrieval strategies; the motivation is 
that each technique will retrieve different sets of relevant 
documents; therefore combining the results could produce 
a better result than any of the individual techniques. We 
propose new data fusion techniques for combining the 
results of different IR models. We applied our data fusion 
techniques to the Mallach collection (Oard et al, 2007) 
used in the Cross-Language Speech Retrieval (CLSR) task 
at Cross-Language Evaluation Forum (CLEF) 2007. The 
Mallach collection comprises 8104 ?documents? which are 
manually-determined topically-coherent segments taken 
from 272 interviews with Holocaust survivors, witnesses 
and rescuers, totalling 589 hours of speech. Figure 1 shows 
the document structure in CLSR test collection, two ASR 
transcripts are available for this data, in this work we use 
the ASRTEXT2004A field provided by IBM research with 
a word error rate of 38%. Additionally, metadata fields for 
each document include: two sets of 20 automatically 
assigned keywords determined using two different kNN 
classifiers (AK1 and AK2), a set of a varying number of 
manually-assigned keywords (MK), and a manual 
3-sentence summary written by an expert in the field.  A set 
of 63 training topics and 33 test topics were generated for 
this task. The topics provided with the collection were 
created in English from actual user requests. Topics were 
structured using the standard TREC format of Title, 
Description and Narrative fields. To enable CL-SR 
experiments the topics were translated into Czech, German, 
French, and Spanish by native speakers; Figure 2 and 3 
show two examples for English and its translation in 
French respectively. Relevance judgments were generated 
using a search-guided procedure and standard pooling 
methods. See (Oard et al, 2004) for full details of the 
collection design.  
We present results on the automatic transcripts for 
English queries and translated queries (cross-language) 
for two combination methods; we also present results 
when manual summaries and manual keywords are 
indexed. 
 
<DOC> 
<DOCNO>VHF[IntCode]-[SegId].[SequenceNum]</DOCNO\> 
<INTERVIEWDATA>Interviewee name(s) and 
birthdate</INTERVIEWDATA> 
<NAME>Full name of every person mentioned</NAME> 
<MANUALKEYWORD>Thesaurus keywords assigned to the 
segment</MANUALKEYWORD> 
<SUMMARY>3-sentence segment summary</SUMMARY> 
<ASRTEXT2004A>ASR transcript produced in 
2004</ASRTEXT2004A> 
<ASRTEXT2006A>ASR transcript produced in 
2006</ASRTEXT2006A> 
<AUTOKEYWORD2004A1>Thesaurus keywords from a kNN 
classifier</AUTOKEYWORD2004A1> 
<AUTOKEYWORD2004A2>Thesaurus keywords from a second 
kNN classifier</AUTOKEYWORD2004A2> 
</DOC> 
Figure 1. Document structure in CL-SR test collection. 
 
<top>  
<num>1159  
<title>Child survivors in Sweden  
<desc>Describe survival mechanisms of children born 
in 1930-1933 who spend the war in concentration 
camps or in hiding and who presently live in Sweden. 
 <narr>The relevant material should describe the 
circumstances and inner resources of the surviving 
children. The relevant material also describes how 
the wartime experience affected their post-war 
adult life. </top> 
Figure 2. Example for English topic in CL-SR test collection. 
 
<top>  
<num>1159  
<title>Les enfants survivants en Su?de  
<desc>Descriptions des m?canismes de survie des 
enfants n?s entre 1930 et 1933 qui ont pass? la 
guerre en camps de concentration ou cach?s et qui 
vivent actuellement en Su?de.  
<narr>? 
</top>  
Figure 3. Example for French topic in CL-SR test collection. 
2. System Description  
Our Cross-Language Information Retrieval systems 
were built with off-the-shelf components. For the retrieval 
part, the SMART (Buckley, Salton, &Allan, 1992; Salton 
&Buckley, 1988) IR system and the Terrier (Amati &Van 
Rijsbergen, 2002; Ounis et al, 2005) IR system were 
tested with many different weighting schemes for 
indexing the collection and the queries.  
SMART was originally developed at Cornell 
University in the 1960s. SMART is based on the vector 
space model of information retrieval. We use the standard 
notation: weighting scheme for the documents, followed 
by dot, followed by the weighting scheme for the queries, 
each term-weighting scheme is described as a 
combination of term frequency, collection frequency, and 
length normalization components where the schemes are 
abbreviated according to its components variations (n no 
normalization, c cosine, t idf, l log, etc.) We used nnn.ntn, 
ntn.ntn, lnn.ntn, ann.ntn, ltn.ntn, atn.ntn, ntn.nnn , 
nnc.ntc, ntc.ntc, ntc.nnc, lnc.ntc, anc.ntc, ltc.ntc, atc.ntc 
weighting schemes (Buckley, Salton, &Allan, 1992; 
Salton &Buckley, 1988);  lnn.ntn performs very well in 
CLEF-CLSR 2005 and 2006 (Alzghool &Inkpen, 2007; 
Inkpen, Alzghool, &Islam, 2006); lnn.ntn means that lnn 
was used for documents and ntn for queries according to 
the following formulas:  
0.1)ln(nln += tfweight        (1) 
tn
Ntfweight logntn ?=     (2)      
where tf denotes the term frequency of a term t in the 
document or query, N denotes the number of documents 
in the collection, and nt denotes the number of documents 
in which the term t occurs.  
Terrier was originally developed at the University of 
Glasgow. It is based on Divergence from Randomness 
models (DFR) where IR is seen as a probabilistic process 
(Amati &Van Rijsbergen, 2002; Ounis et al, 2005). We 
experimented with the In_expC2 (Inverse Expected 
Document Frequency model with Bernoulli after-effect 
and normalization) weighting model, one of Terrier?s 
DFR-based document weighting models.  
Using the In_expC2 model, the relevance score of a 
document d for a query q is given by the formula: 
                  (3) ?
?
=
qt
dtwqtfqdsim ),(.),(
where qtf is the frequency of term t in the query q, and w(t,d) 
is the relevance score of a document d for the query term t, 
given by: 
)
5.0
1log()
)1(
1(),( 2 +
+??+?
+=
e
e
et n
Ntfn
tfnn
Fdtw   (4) 
where 
-F is the term frequency of t in the whole collection. 
-N is the number of document in the whole collection.  
-nt is the document frequency of t. 
-ne is given by ))
1
(1( Fte N
n
Nn
???=  (5) 
- tfne is the normalized within-document frequency of the 
term t in the document d. It is given by the normalization 2 
(Amati &Van Rijsbergen, 2002; Ounis et al, 2005): 
)_1(log
l
lavgctftfn ee ?+?=     (6) 
where c is a parameter, tf is the within-document 
frequency of the term t in the document d, l is the 
document length, and avg_l is the average document 
length in the whole collection. 
We estimated the parameter c of the Terrier's 
normalization 2 formula by running some experiments on 
the training data, to get the best values for c depending on 
the topic fields used. We obtained the following values: 
c=0.75 for queries using the Title only, c=1 for queries 
using the Title and Description fields, and c=1 for queries 
using the Title, Description, and Narrative fields. We select 
the c value that has a best MAP score according to the 
training data. 
For translating the queries from French and Spanish 
into English, several free online machine translation tools 
were used. The idea behind using multiple translations is 
that they might provide more variety of words and 
phrases, therefore improving the retrieval performance. 
Seven online MT systems (Inkpen, Alzghool, &Islam, 
2006) were used for translating from Spanish and from 
French into English. We combined the outputs of the MT 
systems by simply concatenating all the translations. All 
seven translations of a title made the title of the translated 
query; the same was done for the description and narrative 
fields.  
We propose two methods for combining IR models. We 
use the sum of normalized weighted similarity scores of 15 
different IR schemes as shown in the following formulas: 
 
 ?
?
?+=
schemsIRi
iMAPr NormSimiWiWFusion )]()([1
34      (7) 
?
?
?=
schemsIRi
iMAPr NormSimiWiWFusion )(*)(2
34      (8)                         
where Wr(i) and WMAP(i) are experimentally determined 
weights based on the recall (the number of relevant 
documents retrieved) and precision (MAP score) values for 
each IR scheme computed on the training data. For 
example, suppose that two retrieval runs r1 and r2 give 0.3 
and 0.2 (respectively) as  MAP scores on training data; we 
normalize these scores by dividing them by the maximum 
MAP value: then WMAP(r1) is 1 and WMAP(r2) is 0.66 (then 
we compute the power 3 of these weights, so that one 
weight stays 1 and the other one decreases; we chose power 
3 for MAP score and power 4 for recall, because the MAP 
is more important than the recall). We hope that when we 
multiply the similarity values with the weights and take the 
summation over all the runs, the performance of the 
combined run will improve. NormSimi is the normalized 
similarity for each IR scheme. We did the normalization by 
dividing the similarity by the maximum similarity in the 
run. The normalization is necessary because different 
weighting schemes will generate different range of 
similarity values, so a normalization method should 
applied to each run.  Our method is differed than the work 
done by Fox and Shaw in (1994), and Lee in ( 1995); they 
combined the results by taking the summation of the 
similarity scores without giving any weight to each run. In 
our work we weight each run according to the precision 
and recall on the training data.  
3. Experimental Results 
We applied the data fusion methods described in section 2 
to 14 runs produced by SMART and one run produced by 
Terrier.  Performance results for each single run and fused 
runs are presented in Table 1, in which % change is given 
with respect to the run providing better effectiveness in 
each combination on the training data. The Manual 
English column represents the results when only the 
manual keywords and the manual summaries were used 
for indexing the documents using English topics, the 
Auto-English column represents the results when 
automatic transcripts are indexed from the documents, for 
English topics. For cross-languages experiments the 
results are represented in the columns Auto-French, and 
Auto-Spanish, when using the combined translations 
produced by the seven online MT tools, from French and 
Spanish into English. Since the result of combined 
translation for each language was better than when using 
individual translations from each MT tool on the training 
data (Inkpen, Alzghool, &Islam, 2006), we used only the 
combined translations in our experiments. 
Data fusion helps to improve the performance (MAP 
score) on the test data. The best improvement using data 
fusion (Fusion1) was on the French cross-language 
experiments with 21.7%, which is statistically significant 
while on monolingual the improvement was only 6.5% 
which is not significant. We computed these 
improvements relative to the results of the best 
single-model run, as measured on the training data. This 
supports our claim that data fusion improves the recall by 
bringing some new documents that were not retrieved by 
all the runs. On the training data, the Fusion2 method 
gives better results than Fusion1 for all cases except on 
Manual English, but on the test data Fusion1 is better than 
Fusion2. In general, the data fusion seems to help, 
because the performance on the test data in not always 
good for weighting schemes that obtain good results on 
the training data, but combining models allows the 
best-performing weighting schemes to be taken into 
consideration. 
The retrieval results for the translations from French 
were very close to the monolingual English results, 
especially on the training data, but on the test data the 
difference was significantly worse. For Spanish, the 
difference was significantly worse on the training data, 
but not on the test data.  
Experiments on manual keywords and manual 
summaries available in the test collection showed high 
improvements, the MAP score jumped from 0.0855 to 
0.2761 on the test data. 
4. Conclusion 
We experimented with two different systems: Terrier 
and SMART, with combining the various weighting 
schemes for indexing the document and query terms. We 
proposed two methods to combine different weighting 
scheme from different systems, based on weighted 
summation of normalized similarity measures; the weight 
for each scheme was based on the relative precision and 
recall on the training data. Data fusion helps to improve 
the retrieval significantly for some experiments 
(Auto-French) and for other not significantly (Manual 
English). Our result on automatic transcripts for English 
queries (the required run for the CLSR task at CLEF 
2007), obtained a MAP score of 0.0855. This result was 
significantly better than the other 4 systems that 
participated in the CLSR task at CLEF 2007(Pecina et al, 
2007). 
In future work we plan to investigate more methods of 
data fusion (to apply a normalization scheme scalable to 
unseen data), removing or correcting some of the speech 
recognition errors in the ASR content words, and to use 
speech lattices for indexing.  
5. References 
 
Alzghool, M. & Inkpen, D. (2007). Experiments for the 
cross language speech retrieval task at CLEF 2006. In 
C. Peters, (Ed.), Evaluation of multilingual and 
multi-modal information retrieval (Vol. 4730/2007, 
pp. 778-785). Springer. 
Amati, G. & Van Rijsbergen, C. J. (2002). Probabilistic 
models of information retrieval based on measuring 
the divergence from randomness (Vol. 20). ACM,  
New York. 
Buckley, C., Salton, G., & Allan, J. (1992). Automatic 
retrieval with locality information using smart. In 
Text retrieval conferenc (TREC-1) (pp. 59-72). 
Inkpen, D., Alzghool, M., & Islam, A. (2006). Using 
various indexing schemes and multiple translations in 
the CL-SR task at CLEF 2005. In C. Peters, (Ed.), 
Accessing multilingual information repositories 
(Vol. 4022/2006, pp. 760-768). Springer,  London. 
Lee, J. H. (1995). Combining multiple evidence from 
different properties of weighting schemes, 
Proceedings of the 18th annual international ACM 
SIGIR conference on Research and development in 
information retrieval. ACM, Seattle, Washington, 
United States. 
Oard, D. W., Soergel, D., Doermann, D., Huang, X., 
Murray, G. C., Wang, J., Ramabhadran, B., Franz, 
M., & Gustman, S. (2004). Building an information 
retrieval test collection for spontaneous 
conversational speech, Proceedings of the 27th 
annual international ACM SIGIR conference on 
Research and development in information retrieval. 
ACM, Sheffield, United Kingdom. 
Oard, D. W., Wang, J., Jones, G. J. F., White, R. W., 
Pecina, P., Soergel, D., Huang, X., & Shafran, I. 
(2007). Overview of the CLEF-2006 cross-language 
speech retrieval track. In C. Peters, (Ed.), Evaluation 
of multilingual and multi-modal information 
retrieval (Vol. 4730/2007, pp. 744-758). Springer,  
Heidelberg. 
Ounis, I., Amati, G., Plachouras, V., He, B., Macdonald, 
C., & Johnson, D. (2005). Terrier information 
retrieval platform In Advances in information 
retrieval (Vol. 3408/2005, pp. 517-519). Springer,  
Heidelberg. 
Pecina, P., Hoffmannov?a, P., Jones, G. J. F., Zhang, Y., 
& Oard, D. W. (2007). Overview of the CLEF-2007 
cross language speech retrieval track, Working Notes 
of the CLEF- 2007 Evaluation, . CLEF2007, 
Budapest-Hungary. 
Salton, G. & Buckley, C. (1988). Term weighting 
approaches in automatic text retrieval. Information 
Processing and Management, 24(5): 513-523. 
Shaw, J. A. & Fox, E. A. (1994). Combination of multiple 
searches. In Third text retrieval conference (trec-3) 
(pp. 105-108). National Institute of Standards and 
Technology Special Publication. 
 
 
Manual English Auto-English Auto-French Auto-Spanish Weighting 
scheme Training Test Training Test Training Test Training Test 
nnc.ntc 0.2546 0.2293 0.0888 0.0819 0.0792 0.055 0.0593 0.0614 
ntc.ntc 0.2592 0.2332 0.0892 0.0794 0.0841 0.0519 0.0663 0.0545 
lnc.ntc 0.2710 0.2363 0.0898 0.0791 0.0858 0.0576 0.0652 0.0604 
ntc.nnc 0.2344 0.2172 0.0858 0.0769 0.0745 0.0466 0.0585 0.062 
anc.ntc 0.2759 0.2343 0.0723 0.0623 0.0664 0.0376 0.0518 0.0398 
ltc.ntc 0.2639 0.2273 0.0794 0.0623 0.0754 0.0449 0.0596 0.0428 
atc.ntc 0.2606 0.2184 0.0592 0.0477 0.0525 0.0287 0.0437 0.0304 
nnn.ntn 0.2476 0.2228 0.0900 0.0852 0.0799 0.0503 0.0599 0.061 
ntn.ntn 0.2738 0.2369 0.0933 0.0795 0.0843 0.0507 0.0691 0.0578 
lnn.ntn 0.2858 0.245 0.0969 0.0799 0.0905 0.0566 0.0701 0.0589 
ntn.nnn 0.2476 0.2228 0.0900 0.0852 0.0799 0.0503 0.0599 0.061 
ann.ntn 0.2903 0.2441 0.0750 0.0670 0.0743 0.038 0.057 0.0383 
ltn.ntn 0.2870 0.2435 0.0799 0.0655 0.0871 0.0522 0.0701 0.0501 
atn.ntn 0.2843 0.2364 0.0620 0.0546 0.0722 0.0347 0.0586 0.0355 
In_expC2 0.3177 0.2737 0.0885 0.0744 0.0908 0.0487 0.0747 0.0614 
Fusion 1 0.3208 0.2761 0.0969 0.0855 0.0912 0.0622 0.0731 0.0682 
% change 1.0% 0.9% 0.0% 6.5% 0.4% 21.7% -2.2% 10.0% 
Fusion 2 0.3182 0.2741 0.0975 0.0842 0.0942 0.0602 0.0752 0.0619 
% change 0.2% 0.1% 0.6% 5.1% 3.6% 19.1% 0.7% 0.8% 
Table 1. Results (MAP scores) for 15 weighting schemes using Smart and Terrier (the In_expC2 model), and the results 
for the two Fusions Methods. In bold are the best scores for the 15 single runs on the training data and the corresponding 
results on the test data.  
 
Weighting 
scheme 
Manual English Auto-English Auto- French Auto- Spanish 
 Train. Test Train. Test Train. Test Train. Test 
nnc. ntc 2371 1827 1726 1306 1687 1122 1562 1178 
ntc.ntc 2402 1857 1675 1278 1589 1074 1466 1155 
lnc.ntc 2402 1840 1649 1301 1628 1111 1532 1196 
ntc.nnc 2354 1810 1709 1287 1662 1121 1564 1182 
anc.ntc 2405 1858 1567 1192 1482 1036 1360 1074 
ltc.ntc 2401 1864 1571 1211 1455 1046 1384 1097 
atc.ntc 2387 1858 1435 1081 1361 945 1255 1011 
nnn.ntn 2370 1823 1740 1321 1748 1158 1643 1190 
ntn.ntn 2432 1863 1709 1314 1627 1093 1502 1174 
lnn.ntn 2414 1846 1681 1325 1652 1130 1546 1194 
ntn.nnn 2370 1823 1740 1321 1748 1158 1643 1190 
ann.ntn 2427 1859 1577 1198 1473 1027 1365 1060 
ltn.ntn 2433 1876 1582 1215 1478 1070 1408 1134 
atn.ntn 2442 1859 1455 1101 1390 975 1297 1037 
In_expC2 2638 1823 1624 1286 1676 1061 1631 1172 
Fusion 1 2645 1832 1745 1334 1759 1147 1645 1219 
% change 0.3% 0.5 % 0.3% 1.0% 0.6% -1.0% 0.1% 2.4% 
Fusion 2 2647 1823 1727 1337 1736 1098 1631 1172 
% change 0.3% 0.0% 0.8% 1.2% -0.7% -5.5% -0.7% -1.5% 
Table 2. Results (number of relevant documents retrieved) for 15 weighting schemes using Terrier and SMART, and the 
results for the Fusions Methods. In bold are the best scores for the 15 single runs on training data and the corresponding 
test data. 
Proceedings of ACL-08: HLT, pages 46?54,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Task-oriented Evaluation of Syntactic Parsers and Their Representations
Yusuke Miyao? Rune S?tre? Kenji Sagae? Takuya Matsuzaki? Jun?ichi Tsujii???
?Department of Computer Science, University of Tokyo, Japan
?School of Computer Science, University of Manchester, UK
?National Center for Text Mining, UK
{yusuke,rune.saetre,sagae,matuzaki,tsujii}@is.s.u-tokyo.ac.jp
Abstract
This paper presents a comparative evalua-
tion of several state-of-the-art English parsers
based on different frameworks. Our approach
is to measure the impact of each parser when it
is used as a component of an information ex-
traction system that performs protein-protein
interaction (PPI) identification in biomedical
papers. We evaluate eight parsers (based on
dependency parsing, phrase structure parsing,
or deep parsing) using five different parse rep-
resentations. We run a PPI system with several
combinations of parser and parse representa-
tion, and examine their impact on PPI identi-
fication accuracy. Our experiments show that
the levels of accuracy obtained with these dif-
ferent parsers are similar, but that accuracy
improvements vary when the parsers are re-
trained with domain-specific data.
1 Introduction
Parsing technologies have improved considerably in
the past few years, and high-performance syntactic
parsers are no longer limited to PCFG-based frame-
works (Charniak, 2000; Klein and Manning, 2003;
Charniak and Johnson, 2005; Petrov and Klein,
2007), but also include dependency parsers (Mc-
Donald and Pereira, 2006; Nivre and Nilsson, 2005;
Sagae and Tsujii, 2007) and deep parsers (Kaplan
et al, 2004; Clark and Curran, 2004; Miyao and
Tsujii, 2008). However, efforts to perform extensive
comparisons of syntactic parsers based on different
frameworks have been limited. The most popular
method for parser comparison involves the direct
measurement of the parser output accuracy in terms
of metrics such as bracketing precision and recall, or
dependency accuracy. This assumes the existence of
a gold-standard test corpus, such as the Penn Tree-
bank (Marcus et al, 1994). It is difficult to apply
this method to compare parsers based on different
frameworks, because parse representations are often
framework-specific and differ from parser to parser
(Ringger et al, 2004). The lack of such comparisons
is a serious obstacle for NLP researchers in choosing
an appropriate parser for their purposes.
In this paper, we present a comparative evalua-
tion of syntactic parsers and their output represen-
tations based on different frameworks: dependency
parsing, phrase structure parsing, and deep pars-
ing. Our approach to parser evaluation is to mea-
sure accuracy improvement in the task of identify-
ing protein-protein interaction (PPI) information in
biomedical papers, by incorporating the output of
different parsers as statistical features in a machine
learning classifier (Yakushiji et al, 2005; Katrenko
and Adriaans, 2006; Erkan et al, 2007; S?tre et al,
2007). PPI identification is a reasonable task for
parser evaluation, because it is a typical information
extraction (IE) application, and because recent stud-
ies have shown the effectiveness of syntactic parsing
in this task. Since our evaluation method is applica-
ble to any parser output, and is grounded in a real
application, it allows for a fair comparison of syn-
tactic parsers based on different frameworks.
Parser evaluation in PPI extraction also illu-
minates domain portability. Most state-of-the-art
parsers for English were trained with the Wall Street
Journal (WSJ) portion of the Penn Treebank, and
high accuracy has been reported for WSJ text; how-
ever, these parsers rely on lexical information to at-
tain high accuracy, and it has been criticized that
these parsers may overfit to WSJ text (Gildea, 2001;
46
Klein and Manning, 2003). Another issue for dis-
cussion is the portability of training methods. When
training data in the target domain is available, as
is the case with the GENIA Treebank (Kim et al,
2003) for biomedical papers, a parser can be re-
trained to adapt to the target domain, and larger ac-
curacy improvements are expected, if the training
method is sufficiently general. We will examine
these two aspects of domain portability by compar-
ing the original parsers with the retrained parsers.
2 Syntactic Parsers and Their
Representations
This paper focuses on eight representative parsers
that are classified into three parsing frameworks:
dependency parsing, phrase structure parsing, and
deep parsing. In general, our evaluation methodol-
ogy can be applied to English parsers based on any
framework; however, in this paper, we chose parsers
that were originally developed and trained with the
Penn Treebank or its variants, since such parsers can
be re-trained with GENIA, thus allowing for us to
investigate the effect of domain adaptation.
2.1 Dependency parsing
Because the shared tasks of CoNLL-2006 and
CoNLL-2007 focused on data-driven dependency
parsing, it has recently been extensively studied in
parsing research. The aim of dependency pars-
ing is to compute a tree structure of a sentence
where nodes are words, and edges represent the re-
lations among words. Figure 1 shows a dependency
tree for the sentence ?IL-8 recognizes and activates
CXCR1.? An advantage of dependency parsing is
that dependency trees are a reasonable approxima-
tion of the semantics of sentences, and are readily
usable in NLP applications. Furthermore, the effi-
ciency of popular approaches to dependency pars-
ing compare favorable with those of phrase struc-
ture parsing or deep parsing. While a number of ap-
proaches have been proposed for dependency pars-
ing, this paper focuses on two typical methods.
MST McDonald and Pereira (2006)?s dependency
parser,1 based on the Eisner algorithm for projective
dependency parsing (Eisner, 1996) with the second-
order factorization.
1http://sourceforge.net/projects/mstparser
Figure 1: CoNLL-X dependency tree
Figure 2: Penn Treebank-style phrase structure tree
KSDEP Sagae and Tsujii (2007)?s dependency
parser,2 based on a probabilistic shift-reduce al-
gorithm extended by the pseudo-projective parsing
technique (Nivre and Nilsson, 2005).
2.2 Phrase structure parsing
Owing largely to the Penn Treebank, the mainstream
of data-driven parsing research has been dedicated
to the phrase structure parsing. These parsers output
Penn Treebank-style phrase structure trees, although
function tags and empty categories are stripped off
(Figure 2). While most of the state-of-the-art parsers
are based on probabilistic CFGs, the parameteriza-
tion of the probabilistic model of each parser varies.
In this work, we chose the following four parsers.
NO-RERANK Charniak (2000)?s parser, based on a
lexicalized PCFG model of phrase structure trees.3
The probabilities of CFG rules are parameterized on
carefully hand-tuned extensive information such as
lexical heads and symbols of ancestor/sibling nodes.
RERANK Charniak and Johnson (2005)?s rerank-
ing parser. The reranker of this parser receives n-
best4 parse results from NO-RERANK, and selects
the most likely result by using a maximum entropy
model with manually engineered features.
BERKELEY Berkeley?s parser (Petrov and Klein,
2007).5 The parameterization of this parser is op-
2http://www.cs.cmu.edu/?sagae/parser/
3http://bllip.cs.brown.edu/resources.shtml
4We set n = 50 in this paper.
5http://nlp.cs.berkeley.edu/Main.html#Parsing
47
Figure 3: Predicate argument structure
timized automatically by assigning latent variables
to each nonterminal node and estimating the param-
eters of the latent variables by the EM algorithm
(Matsuzaki et al, 2005).
STANFORD Stanford?s unlexicalized parser (Klein
and Manning, 2003).6 Unlike NO-RERANK, proba-
bilities are not parameterized on lexical heads.
2.3 Deep parsing
Recent research developments have allowed for ef-
ficient and robust deep parsing of real-world texts
(Kaplan et al, 2004; Clark and Curran, 2004; Miyao
and Tsujii, 2008). While deep parsers compute
theory-specific syntactic/semantic structures, pred-
icate argument structures (PAS) are often used in
parser evaluation and applications. PAS is a graph
structure that represents syntactic/semantic relations
among words (Figure 3). The concept is therefore
similar to CoNLL dependencies, though PAS ex-
presses deeper relations, and may include reentrant
structures. In this work, we chose the two versions
of the Enju parser (Miyao and Tsujii, 2008).
ENJU The HPSG parser that consists of an HPSG
grammar extracted from the Penn Treebank, and
a maximum entropy model trained with an HPSG
treebank derived from the Penn Treebank.7
ENJU-GENIA The HPSG parser adapted to
biomedical texts, by the method of Hara et al
(2007). Because this parser is trained with both
WSJ and GENIA, we compare it parsers that are
retrained with GENIA (see section 3.3).
3 Evaluation Methodology
In our approach to parser evaluation, we measure
the accuracy of a PPI extraction system, in which
6http://nlp.stanford.edu/software/lex-parser.
shtml
7http://www-tsujii.is.s.u-tokyo.ac.jp/enju/
This study demonstrates that IL-8 recognizes and
activates CXCR1, CXCR2, and the Duffy antigen
by distinct mechanisms.
The molar ratio of serum retinol-binding protein
(RBP) to transthyretin (TTR) is not useful to as-
sess vitamin A status during infection in hospi-
talised children.
Figure 4: Sentences including protein names
ENTITY1(IL-8) SBJ?? recognizes OBJ?? ENTITY2(CXCR1)
Figure 5: Dependency path
the parser output is embedded as statistical features
of a machine learning classifier. We run a classi-
fier with features of every possible combination of a
parser and a parse representation, by applying con-
versions between representations when necessary.
We also measure the accuracy improvements ob-
tained by parser retraining with GENIA, to examine
the domain portability, and to evaluate the effective-
ness of domain adaptation.
3.1 PPI extraction
PPI extraction is an NLP task to identify protein
pairs that are mentioned as interacting in biomedical
papers. Because the number of biomedical papers is
growing rapidly, it is impossible for biomedical re-
searchers to read all papers relevant to their research;
thus, there is an emerging need for reliable IE tech-
nologies, such as PPI identification.
Figure 4 shows two sentences that include pro-
tein names: the former sentence mentions a protein
interaction, while the latter does not. Given a pro-
tein pair, PPI extraction is a task of binary classi-
fication; for example, ?IL-8, CXCR1? is a positive
example, and ?RBP, TTR? is a negative example.
Recent studies on PPI extraction demonstrated that
dependency relations between target proteins are ef-
fective features for machine learning classifiers (Ka-
trenko and Adriaans, 2006; Erkan et al, 2007; S?tre
et al, 2007). For the protein pair IL-8 and CXCR1
in Figure 4, a dependency parser outputs a depen-
dency tree shown in Figure 1. From this dependency
tree, we can extract a dependency path shown in Fig-
ure 5, which appears to be a strong clue in knowing
that these proteins are mentioned as interacting.
48
(dep_path (SBJ (ENTITY1 recognizes))
(rOBJ (recognizes ENTITY2)))
Figure 6: Tree representation of a dependency path
We follow the PPI extraction method of S?tre et
al. (2007), which is based on SVMs with SubSet
Tree Kernels (Collins and Duffy, 2002; Moschitti,
2006), while using different parsers and parse rep-
resentations. Two types of features are incorporated
in the classifier. The first is bag-of-words features,
which are regarded as a strong baseline for IE sys-
tems. Lemmas of words before, between and after
the pair of target proteins are included, and the linear
kernel is used for these features. These features are
commonly included in all of the models. Filtering
by a stop-word list is not applied because this setting
made the scores higher than S?tre et al (2007)?s set-
ting. The other type of feature is syntactic features.
For dependency-based parse representations, a de-
pendency path is encoded as a flat tree as depicted in
Figure 6 (prefix ?r? denotes reverse relations). Be-
cause a tree kernel measures the similarity of trees
by counting common subtrees, it is expected that the
system finds effective subsequences of dependency
paths. For the PTB representation, we directly en-
code phrase structure trees.
3.2 Conversion of parse representations
It is widely believed that the choice of representa-
tion format for parser output may greatly affect the
performance of applications, although this has not
been extensively investigated. We should therefore
evaluate the parser performance in multiple parse
representations. In this paper, we create multiple
parse representations by converting each parser?s de-
fault output into other representations when possi-
ble. This experiment can also be considered to be
a comparative evaluation of parse representations,
thus providing an indication for selecting an appro-
priate parse representation for similar IE tasks.
Figure 7 shows our scheme for representation
conversion. This paper focuses on five representa-
tions as described below.
CoNLL The dependency tree format used in the
2006 and 2007 CoNLL shared tasks on dependency
parsing. This is a representation format supported by
several data-driven dependency parsers. This repre-
Figure 7: Conversion of parse representations
Figure 8: Head dependencies
sentation is also obtained from Penn Treebank-style
trees by applying constituent-to-dependency conver-
sion8 (Johansson and Nugues, 2007). It should be
noted, however, that this conversion cannot work
perfectly with automatic parsing, because the con-
version program relies on function tags and empty
categories of the original Penn Treebank.
PTB Penn Treebank-style phrase structure trees
without function tags and empty nodes. This is the
default output format for phrase structure parsers.
We also create this representation by converting
ENJU?s output by tree structure matching, although
this conversion is not perfect because forms of PTB
and ENJU?s output are not necessarily compatible.
HD Dependency trees of syntactic heads (Fig-
ure 8). This representation is obtained by convert-
ing PTB trees. We first determine lexical heads of
nonterminal nodes by using Bikel?s implementation
of Collins? head detection algorithm9 (Bikel, 2004;
Collins, 1997). We then convert lexicalized trees
into dependencies between lexical heads.
SD The Stanford dependency format (Figure 9).
This format was originally proposed for extracting
dependency relations useful for practical applica-
tions (de Marneffe et al, 2006). A program to con-
vert PTB is attached to the Stanford parser. Although
the concept looks similar to CoNLL, this representa-
8http://nlp.cs.lth.se/pennconverter/
9http://www.cis.upenn.edu/?dbikel/software.
html
49
Figure 9: Stanford dependencies
tion does not necessarily form a tree structure, and is
designed to express more fine-grained relations such
as apposition. Research groups for biomedical NLP
recently adopted this representation for corpus anno-
tation (Pyysalo et al, 2007a) and parser evaluation
(Clegg and Shepherd, 2007; Pyysalo et al, 2007b).
PAS Predicate-argument structures. This is the de-
fault output format for ENJU and ENJU-GENIA.
Although only CoNLL is available for depen-
dency parsers, we can create four representations for
the phrase structure parsers, and five for the deep
parsers. Dotted arrows in Figure 7 indicate imper-
fect conversion, in which the conversion inherently
introduces errors, and may decrease the accuracy.
We should therefore take caution when comparing
the results obtained by imperfect conversion. We
also measure the accuracy obtained by the ensem-
ble of two parsers/representations. This experiment
indicates the differences and overlaps of information
conveyed by a parser or a parse representation.
3.3 Domain portability and parser retraining
Since the domain of our target text is different from
WSJ, our experiments also highlight the domain
portability of parsers. We run two versions of each
parser in order to investigate the two types of domain
portability. First, we run the original parsers trained
with WSJ10 (39832 sentences). The results in this
setting indicate the domain portability of the original
parsers. Next, we run parsers re-trained with GE-
NIA11 (8127 sentences), which is a Penn Treebank-
style treebank of biomedical paper abstracts. Accu-
racy improvements in this setting indicate the pos-
sibility of domain adaptation, and the portability of
the training methods of the parsers. Since the parsers
listed in Section 2 have programs for the training
10Some of the parser packages include parsing models
trained with extended data, but we used the models trained with
WSJ section 2-21 of the Penn Treebank.
11The domains of GENIA and AImed are not exactly the
same, because they are collected independently.
with a Penn Treebank-style treebank, we use those
programs as-is. Default parameter settings are used
for this parser re-training.
In preliminary experiments, we found that de-
pendency parsers attain higher dependency accuracy
when trained only with GENIA. We therefore only
input GENIA as the training data for the retraining
of dependency parsers. For the other parsers, we in-
put the concatenation of WSJ and GENIA for the
retraining, while the reranker of RERANK was not re-
trained due to its cost. Since the parsers other than
NO-RERANK and RERANK require an external POS
tagger, a WSJ-trained POS tagger is used with WSJ-
trained parsers, and geniatagger (Tsuruoka et al,
2005) is used with GENIA-retrained parsers.
4 Experiments
4.1 Experiment settings
In the following experiments, we used AImed
(Bunescu and Mooney, 2004), which is a popular
corpus for the evaluation of PPI extraction systems.
The corpus consists of 225 biomedical paper ab-
stracts (1970 sentences), which are sentence-split,
tokenized, and annotated with proteins and PPIs.
We use gold protein annotations given in the cor-
pus. Multi-word protein names are concatenated
and treated as single words. The accuracy is mea-
sured by abstract-wise 10-fold cross validation and
the one-answer-per-occurrence criterion (Giuliano
et al, 2006). A threshold for SVMs is moved to
adjust the balance of precision and recall, and the
maximum f-scores are reported for each setting.
4.2 Comparison of accuracy improvements
Tables 1 and 2 show the accuracy obtained by using
the output of each parser in each parse representa-
tion. The row ?baseline? indicates the accuracy ob-
tained with bag-of-words features. Table 3 shows
the time for parsing the entire AImed corpus, and
Table 4 shows the time required for 10-fold cross
validation with GENIA-retrained parsers.
When using the original WSJ-trained parsers (Ta-
ble 1), all parsers achieved almost the same level
of accuracy ? a significantly better result than the
baseline. To the extent of our knowledge, this is
the first result that proves that dependency parsing,
phrase structure parsing, and deep parsing perform
50
CoNLL PTB HD SD PAS
baseline 48.2/54.9/51.1
MST 53.2/56.5/54.6 N/A N/A N/A N/A
KSDEP 49.3/63.0/55.2 N/A N/A N/A N/A
NO-RERANK 50.7/60.9/55.2 45.9/60.5/52.0 50.6/60.9/55.1 49.9/58.2/53.5 N/A
RERANK 53.6/59.2/56.1 47.0/58.9/52.1 48.1/65.8/55.4 50.7/62.7/55.9 N/A
BERKELEY 45.8/67.6/54.5 50.5/57.6/53.7 52.3/58.8/55.1 48.7/62.4/54.5 N/A
STANFORD 50.4/60.6/54.9 50.9/56.1/53.0 50.7/60.7/55.1 51.8/58.1/54.5 N/A
ENJU 52.6/58.0/55.0 48.7/58.8/53.1 57.2/51.9/54.2 52.2/58.1/54.8 48.9/64.1/55.3
Table 1: Accuracy on the PPI task with WSJ-trained parsers (precision/recall/f-score)
CoNLL PTB HD SD PAS
baseline 48.2/54.9/51.1
MST 49.1/65.6/55.9 N/A N/A N/A N/A
KSDEP 51.6/67.5/58.3 N/A N/A N/A N/A
NO-RERANK 53.9/60.3/56.8 51.3/54.9/52.8 53.1/60.2/56.3 54.6/58.1/56.2 N/A
RERANK 52.8/61.5/56.6 48.3/58.0/52.6 52.1/60.3/55.7 53.0/61.1/56.7 N/A
BERKELEY 52.7/60.3/56.0 48.0/59.9/53.1 54.9/54.6/54.6 50.5/63.2/55.9 N/A
STANFORD 49.3/62.8/55.1 44.5/64.7/52.5 49.0/62.0/54.5 54.6/57.5/55.8 N/A
ENJU 54.4/59.7/56.7 48.3/60.6/53.6 56.7/55.6/56.0 54.4/59.3/56.6 52.0/63.8/57.2
ENJU-GENIA 56.4/57.4/56.7 46.5/63.9/53.7 53.4/60.2/56.4 55.2/58.3/56.5 57.5/59.8/58.4
Table 2: Accuracy on the PPI task with GENIA-retrained parsers (precision/recall/f-score)
WSJ-trained GENIA-retrained
MST 613 425
KSDEP 136 111
NO-RERANK 2049 1372
RERANK 2806 2125
BERKELEY 1118 1198
STANFORD 1411 1645
ENJU 1447 727
ENJU-GENIA 821
Table 3: Parsing time (sec.)
equally well in a real application. Among these
parsers, RERANK performed slightly better than the
other parsers, although the difference in the f-score
is small, while it requires much higher parsing cost.
When the parsers are retrained with GENIA (Ta-
ble 2), the accuracy increases significantly, demon-
strating that the WSJ-trained parsers are not suffi-
ciently domain-independent, and that domain adap-
tation is effective. It is an important observation that
the improvements by domain adaptation are larger
than the differences among the parsers in the pre-
vious experiment. Nevertheless, not all parsers had
their performance improved upon retraining. Parser
CoNLL PTB HD SD PAS
baseline 424
MST 809 N/A N/A N/A N/A
KSDEP 864 N/A N/A N/A N/A
NO-RERANK 851 4772 882 795 N/A
RERANK 849 4676 881 778 N/A
BERKELEY 869 4665 895 804 N/A
STANFORD 847 4614 886 799 N/A
ENJU 832 4611 884 789 1005
ENJU-GENIA 874 4624 895 783 1020
Table 4: Evaluation time (sec.)
retraining yielded only slight improvements for
RERANK, BERKELEY, and STANFORD, while larger
improvements were observed for MST, KSDEP, NO-
RERANK, and ENJU. Such results indicate the dif-
ferences in the portability of training methods. A
large improvement from ENJU to ENJU-GENIA shows
the effectiveness of the specifically designed do-
main adaptation method, suggesting that the other
parsers might also benefit from more sophisticated
approaches for domain adaptation.
While the accuracy level of PPI extraction is
the similar for the different parsers, parsing speed
51
RERANK ENJU
CoNLL HD SD CoNLL HD SD PAS
KSDEP CoNLL 58.5 (+0.2) 57.1 (?1.2) 58.4 (+0.1) 58.5 (+0.2) 58.0 (?0.3) 59.1 (+0.8) 59.0 (+0.7)
RERANK CoNLL 56.7 (+0.1) 57.1 (+0.4) 58.3 (+1.6) 57.3 (+0.7) 58.7 (+2.1) 59.5 (+2.3)
HD 56.8 (+0.1) 57.2 (+0.5) 56.5 (+0.5) 56.8 (+0.2) 57.6 (+0.4)
SD 58.3 (+1.6) 58.3 (+1.6) 56.9 (+0.2) 58.6 (+1.4)
ENJU CoNLL 57.0 (+0.3) 57.2 (+0.5) 58.4 (+1.2)
HD 57.1 (+0.5) 58.1 (+0.9)
SD 58.3 (+1.1)
Table 5: Results of parser/representation ensemble (f-score)
differs significantly. The dependency parsers are
much faster than the other parsers, while the phrase
structure parsers are relatively slower, and the deep
parsers are in between. It is noteworthy that the
dependency parsers achieved comparable accuracy
with the other parsers, while they are more efficient.
The experimental results also demonstrate that
PTB is significantly worse than the other represen-
tations with respect to cost for training/testing and
contributions to accuracy improvements. The con-
version from PTB to dependency-based representa-
tions is therefore desirable for this task, although it
is possible that better results might be obtained with
PTB if a different feature extraction mechanism is
used. Dependency-based representations are com-
petitive, while CoNLL seems superior to HD and SD
in spite of the imperfect conversion from PTB to
CoNLL. This might be a reason for the high per-
formances of the dependency parsers that directly
compute CoNLL dependencies. The results for ENJU-
CoNLL and ENJU-PAS show that PAS contributes to a
larger accuracy improvement, although this does not
necessarily mean the superiority of PAS, because two
imperfect conversions, i.e., PAS-to-PTB and PTB-to-
CoNLL, are applied for creating CoNLL.
4.3 Parser ensemble results
Table 5 shows the accuracy obtained with ensembles
of two parsers/representations (except the PTB for-
mat). Bracketed figures denote improvements from
the accuracy with a single parser/representation.
The results show that the task accuracy significantly
improves by parser/representation ensemble. Inter-
estingly, the accuracy improvements are observed
even for ensembles of different representations from
the same parser. This indicates that a single parse
representation is insufficient for expressing the true
Bag-of-words features 48.2/54.9/51.1
Yakushiji et al (2005) 33.7/33.1/33.4
Mitsumori et al (2006) 54.2/42.6/47.7
Giuliano et al (2006) 60.9/57.2/59.0
S?tre et al (2007) 64.3/44.1/52.0
This paper 54.9/65.5/59.5
Table 6: Comparison with previous results on PPI extrac-
tion (precision/recall/f-score)
potential of a parser. Effectiveness of the parser en-
semble is also attested by the fact that it resulted in
larger improvements. Further investigation of the
sources of these improvements will illustrate the ad-
vantages and disadvantages of these parsers and rep-
resentations, leading us to better parsing models and
a better design for parse representations.
4.4 Comparison with previous results on PPI
extraction
PPI extraction experiments on AImed have been re-
ported repeatedly, although the figures cannot be
compared directly because of the differences in data
preprocessing and the number of target protein pairs
(S?tre et al, 2007). Table 6 compares our best re-
sult with previously reported accuracy figures. Giu-
liano et al (2006) and Mitsumori et al (2006) do
not rely on syntactic parsing, while the former ap-
plied SVMs with kernels on surface strings and the
latter is similar to our baseline method. Bunescu and
Mooney (2005) applied SVMs with subsequence
kernels to the same task, although they provided
only a precision-recall graph, and its f-score is
around 50. Since we did not run experiments on
protein-pair-wise cross validation, our system can-
not be compared directly to the results reported
by Erkan et al (2007) and Katrenko and Adriaans
52
(2006), while S?tre et al (2007) presented better re-
sults than theirs in the same evaluation criterion.
5 Related Work
Though the evaluation of syntactic parsers has been
a major concern in the parsing community, and a
couple of works have recently presented the com-
parison of parsers based on different frameworks,
their methods were based on the comparison of the
parsing accuracy in terms of a certain intermediate
parse representation (Ringger et al, 2004; Kaplan
et al, 2004; Briscoe and Carroll, 2006; Clark and
Curran, 2007; Miyao et al, 2007; Clegg and Shep-
herd, 2007; Pyysalo et al, 2007b; Pyysalo et al,
2007a; Sagae et al, 2008). Such evaluation requires
gold standard data in an intermediate representation.
However, it has been argued that the conversion of
parsing results into an intermediate representation is
difficult and far from perfect.
The relationship between parsing accuracy and
task accuracy has been obscure for many years.
Quirk and Corston-Oliver (2006) investigated the
impact of parsing accuracy on statistical MT. How-
ever, this work was only concerned with a single de-
pendency parser, and did not focus on parsers based
on different frameworks.
6 Conclusion and Future Work
We have presented our attempts to evaluate syntac-
tic parsers and their representations that are based on
different frameworks; dependency parsing, phrase
structure parsing, or deep parsing. The basic idea
is to measure the accuracy improvements of the
PPI extraction task by incorporating the parser out-
put as statistical features of a machine learning
classifier. Experiments showed that state-of-the-
art parsers attain accuracy levels that are on par
with each other, while parsing speed differs sig-
nificantly. We also found that accuracy improve-
ments vary when parsers are retrained with domain-
specific data, indicating the importance of domain
adaptation and the differences in the portability of
parser training methods.
Although we restricted ourselves to parsers
trainable with Penn Treebank-style treebanks, our
methodology can be applied to any English parsers.
Candidates include RASP (Briscoe and Carroll,
2006), the C&C parser (Clark and Curran, 2004),
the XLE parser (Kaplan et al, 2004), MINIPAR
(Lin, 1998), and Link Parser (Sleator and Temperley,
1993; Pyysalo et al, 2006), but the domain adapta-
tion of these parsers is not straightforward. It is also
possible to evaluate unsupervised parsers, which is
attractive since evaluation of such parsers with gold-
standard data is extremely problematic.
A major drawback of our methodology is that
the evaluation is indirect and the results depend
on a selected task and its settings. This indicates
that different results might be obtained with other
tasks. Hence, we cannot conclude the superiority of
parsers/representations only with our results. In or-
der to obtain general ideas on parser performance,
experiments on other tasks are indispensable.
Acknowledgments
This work was partially supported by Grant-in-Aid
for Specially Promoted Research (MEXT, Japan),
Genome Network Project (MEXT, Japan), and
Grant-in-Aid for Young Scientists (MEXT, Japan).
References
D. M. Bikel. 2004. Intricacies of Collins? parsing model.
Computational Linguistics, 30(4):479?511.
T. Briscoe and J. Carroll. 2006. Evaluating the accu-
racy of an unlexicalized statistical parser on the PARC
DepBank. In COLING/ACL 2006 Poster Session.
R. Bunescu and R. J. Mooney. 2004. Collective infor-
mation extraction with relational markov networks. In
ACL 2004, pages 439?446.
R. C. Bunescu and R. J. Mooney. 2005. Subsequence
kernels for relation extraction. In NIPS 2005.
E. Charniak and M. Johnson. 2005. Coarse-to-fine n-
best parsing and MaxEnt discriminative reranking. In
ACL 2005.
E. Charniak. 2000. A maximum-entropy-inspired parser.
In NAACL-2000, pages 132?139.
S. Clark and J. R. Curran. 2004. Parsing the WSJ using
CCG and log-linear models. In 42nd ACL.
S. Clark and J. R. Curran. 2007. Formalism-independent
parser evaluation with CCG and DepBank. In ACL
2007.
A. B. Clegg and A. J. Shepherd. 2007. Benchmark-
ing natural-language parsers for biological applica-
tions using dependency graphs. BMC Bioinformatics,
8:24.
53
M. Collins and N. Duffy. 2002. New ranking algorithms
for parsing and tagging: Kernels over discrete struc-
tures, and the voted perceptron. In ACL 2002.
M. Collins. 1997. Three generative, lexicalised models
for statistical parsing. In 35th ACL.
M.-C. de Marneffe, B. MacCartney, and C. D. Man-
ning. 2006. Generating typed dependency parses from
phrase structure parses. In LREC 2006.
J. M. Eisner. 1996. Three new probabilistic models
for dependency parsing: An exploration. In COLING
1996.
G. Erkan, A. Ozgur, and D. R. Radev. 2007. Semi-
supervised classification for extracting protein interac-
tion sentences using dependency parsing. In EMNLP
2007.
D. Gildea. 2001. Corpus variation and parser perfor-
mance. In EMNLP 2001, pages 167?202.
C. Giuliano, A. Lavelli, and L. Romano. 2006. Exploit-
ing shallow linguistic information for relation extrac-
tion from biomedical literature. In EACL 2006.
T. Hara, Y. Miyao, and J. Tsujii. 2007. Evaluating im-
pact of re-training a lexical disambiguation model on
domain adaptation of an HPSG parser. In IWPT 2007.
R. Johansson and P. Nugues. 2007. Extended
constituent-to-dependency conversion for English. In
NODALIDA 2007.
R. M. Kaplan, S. Riezler, T. H. King, J. T. Maxwell, and
A. Vasserman. 2004. Speed and accuracy in shallow
and deep stochastic parsing. In HLT/NAACL?04.
S. Katrenko and P. Adriaans. 2006. Learning relations
from biomedical corpora using dependency trees. In
KDECB, pages 61?80.
J.-D. Kim, T. Ohta, Y. Teteisi, and J. Tsujii. 2003. GE-
NIA corpus ? a semantically annotated corpus for
bio-textmining. Bioinformatics, 19:i180?182.
D. Klein and C. D. Manning. 2003. Accurate unlexical-
ized parsing. In ACL 2003.
D. Lin. 1998. Dependency-based evaluation of MINI-
PAR. In LREC Workshop on the Evaluation of Parsing
Systems.
M. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1994. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguistics,
19(2):313?330.
T. Matsuzaki, Y. Miyao, and J. Tsujii. 2005. Probabilis-
tic CFG with latent annotations. In ACL 2005.
R. McDonald and F. Pereira. 2006. Online learning of
approximate dependency parsing algorithms. In EACL
2006.
T. Mitsumori, M. Murata, Y. Fukuda, K. Doi, and H. Doi.
2006. Extracting protein-protein interaction informa-
tion from biomedical text with SVM. IEICE - Trans.
Inf. Syst., E89-D(8):2464?2466.
Y. Miyao and J. Tsujii. 2008. Feature forest models for
probabilistic HPSG parsing. Computational Linguis-
tics, 34(1):35?80.
Y. Miyao, K. Sagae, and J. Tsujii. 2007. Towards
framework-independent evaluation of deep linguistic
parsers. In Grammar Engineering across Frameworks
2007, pages 238?258.
A. Moschitti. 2006. Making tree kernels practical for
natural language processing. In EACL 2006.
J. Nivre and J. Nilsson. 2005. Pseudo-projective depen-
dency parsing. In ACL 2005.
S. Petrov and D. Klein. 2007. Improved inference for
unlexicalized parsing. In HLT-NAACL 2007.
S. Pyysalo, T. Salakoski, S. Aubin, and A. Nazarenko.
2006. Lexical adaptation of link grammar to the
biomedical sublanguage: a comparative evaluation of
three approaches. BMC Bioinformatics, 7(Suppl. 3).
S. Pyysalo, F. Ginter, J. Heimonen, J. Bjo?rne, J. Boberg,
J. Ja?rvinen, and T. Salakoski. 2007a. BioInfer: a cor-
pus for information extraction in the biomedical do-
main. BMC Bioinformatics, 8(50).
S. Pyysalo, F. Ginter, V. Laippala, K. Haverinen, J. Hei-
monen, and T. Salakoski. 2007b. On the unification of
syntactic annotations under the Stanford dependency
scheme: A case study on BioInfer and GENIA. In
BioNLP 2007, pages 25?32.
C. Quirk and S. Corston-Oliver. 2006. The impact of
parse quality on syntactically-informed statistical ma-
chine translation. In EMNLP 2006.
E. K. Ringger, R. C. Moore, E. Charniak, L. Vander-
wende, and H. Suzuki. 2004. Using the Penn Tree-
bank to evaluate non-treebank parsers. In LREC 2004.
R. S?tre, K. Sagae, and J. Tsujii. 2007. Syntactic
features for protein-protein interaction extraction. In
LBM 2007 short papers.
K. Sagae and J. Tsujii. 2007. Dependency parsing and
domain adaptation with LR models and parser ensem-
bles. In EMNLP-CoNLL 2007.
K. Sagae, Y. Miyao, T. Matsuzaki, and J. Tsujii. 2008.
Challenges in mapping of syntactic representations
for framework-independent parser evaluation. In the
Workshop on Automated Syntatic Annotations for In-
teroperable Language Resources.
D. D. Sleator and D. Temperley. 1993. Parsing English
with a Link Grammar. In 3rd IWPT.
Y. Tsuruoka, Y. Tateishi, J.-D. Kim, T. Ohta, J. Mc-
Naught, S. Ananiadou, and J. Tsujii. 2005. Develop-
ing a robust part-of-speech tagger for biomedical text.
In 10th Panhellenic Conference on Informatics.
A. Yakushiji, Y. Miyao, Y. Tateisi, and J. Tsujii. 2005.
Biomedical information extraction with predicate-
argument structure patterns. In First International
Symposium on Semantic Mining in Biomedicine.
54
Software Engineering, Testing, and Quality Assurance for Natural Language Processing, pages 14?20,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Evaluating the Effects of Treebank Size in a Practical Application for 
Parsing 
Kenji Sagae1, Yusuke Miyao1, Rune S?tre1 and Jun'ichi Tsujii1,2,3 
1Department of Computer Science, Univerisity of Tokyo, Japan 
2School of Computer Science, University of Manchester 
3National Center for Text Mining, Manchester, UK 
{sagae,yusuke,rune.saetre,tsujii@is.s.u-tokyo.ac.jp} 
 
 
 
 
 
Abstract 
Natural language processing modules such as 
part-of-speech taggers, named-entity recog-
nizers and syntactic parsers are commonly 
evaluated in isolation, under the assumption 
that artificial evaluation metrics for individual 
parts are predictive of practical performance 
of more complex language technology sys-
tems that perform practical tasks. Although 
this is an important issue in the design and en-
gineering of systems that use natural language 
input, it is often unclear how the accuracy of 
an end-user application is affected by parame-
ters that affect individual NLP modules.  We 
explore this issue in the context of a specific 
task by examining the relationship between 
the accuracy of a syntactic parser and the 
overall performance of an information extrac-
tion system for biomedical text that includes 
the parser as one of its components.  We 
present an empirical investigation of the rela-
tionship between factors that affect the accu-
racy of syntactic analysis, and how the 
difference in parse accuracy affects the overall 
system.   
1 Introduction 
Software systems that perform practical tasks with 
natural language input often include, in addition to 
task-specific components, a pipeline of basic natu-
ral language processing modules, such as part-of-
speech taggers, named-entity recognizers, syntactic 
parsers and semantic-role labelers.  Although such 
building blocks of larger language technology so-
lutions are usually carefully evaluated in isolation 
using standard test sets, the impact of improve-
ments in each individual module on the overall 
performance of end-to-end systems is less well 
understood.  While the effects of the amount of 
training data, search beam widths and various ma-
chine learning frameworks have been explored in 
detail with respect to speed and accuracy in basic 
natural language processing tasks, how these trade-
offs in individual modules affect the performance 
of the larger systems they compose is an issue that 
has received relatively little attention.  This issue, 
however, is of great practical importance in the 
effective design and engineering of complex soft-
ware systems that deal with natural language.   
In this paper we explore some of these issues 
empirically in an information extraction task in the 
biomedical domain, the identification of protein- 
protein interactions (PPI) mentioned in papers ab-
stracts from MEDLINE, a large database of bio-
medical papers.  Due in large part to the creation of 
biomedical treebanks (Kulick et al, 2004; Tateisi 
et al, 2005) and rapid progress of data-driven 
parsers (Lease and Charniak, 2005; Nivre et al, 
2007), there are now fast, robust and accurate syn-
tactic parsers for text in the biomedical domain.  
Recent research shows that parsing accuracy of 
biomedical corpora is now between 80% and 90% 
(Clegg and Shepherd, 2007; Pyysalo et al, 2007; 
Sagae et al, 2008).  Intuitively, syntactic relation-
ships between words should be valuable in deter-
mining possible interactions between entities 
present in text.  Recent PPI extraction systems 
have confirmed this intuition (Erkan et al, 2007; 
S?tre et al, 2007; Katrenko and Adriaans, 2006).     
While it is now relatively clear that syntactic 
parsing is useful in practical tasks that use natural 
language corpora in bioinformatics, several ques-
14
tions remain as to research issues that affect the 
design and testing of end-user applications, includ-
ing how syntactic analyses should be used in a 
practical setting, whether further improvements in 
parsing technologies will result in further im-
provements in practical systems, whether it is im-
portant to continue the development of treebanks 
and parser adaptation techniques for the biomedi-
cal domain, and how much effort should be spent 
on comparing and benchmarking parsers for bio-
medical data.  We attempt to shed some light on 
these matters by presenting experiments that show 
the relationship of the accuracy of a dependency 
parser and the accuracy of the larger PPI system 
that includes the parser.  We investigate the effects 
of domain-specific treebank size (the amount of 
available manually annotated training data for syn-
tactic parsers) and final system performance, and 
obtain results that should be informative to re-
searchers in bioinformatics who rely on existing 
NLP resources to design information extraction 
systems, as well as to members of the parsing 
community who are interested in the practical im-
pact of parsing research. 
In section 2 we discuss our motivation and re-
lated efforts.  Section 3 describes the system for 
identification of protein-protein interactions used 
in our experiments, and in section 4 describes the 
syntactic parser that provides the analyses for the 
PPI system, and the data used to train the parser.  
We describe our experiments, results and analysis 
in section 5, and conclude in section 6.  
2 Motivation and related work 
While recent work has addressed questions relating 
to the use of different parsers or different types of 
syntactic representations in the PPI extraction task 
(S?tre et al, 2007, Miyao et al, 2008), little con-
crete evidence has been provided for potential ben-
efits of improved parsers or additional resources 
for training syntactic parsers.  In fact, although 
there is increasing interest in parser evaluation in 
the biomedical domain in terms of precision/recall 
of brackets and dependency accuracy (Clegg and 
Shepherd, 2007; Pyysalo et al, 2007; Sagae et al, 
2008), the relationship between these evaluation 
metrics and the performance of practical informa-
tion extraction systems remains unclear.  In the 
parsing community, relatively small accuracy gains 
are often reported as success stories, but again, the 
precise impact of such improvements on practical 
tasks in bioinformatics has not been established. 
One aspect of this issue is the question of do-
main portability and domain adaptation for parsers 
and other NLP modules.  Clegg and Shepherd 
(2007) mention that available statistical parsers 
appear to overfit to the newswire domain, because 
of their extensive use of the Wall Street Journal 
portion of the Penn Treebank (Marcus et al, 1994) 
during development and training.  While this claim 
is supported by convincing evaluations that show 
that parsers trained on the WSJ Penn Treebank 
alone perform poorly on biomedical text in terms 
of accuracy of dependencies or bracketing of 
phrase structure, the benefits of using domain-
specific data in terms of practical system perfor-
mance have not been quantified.  These expected 
benefits drive the development of domain-specific 
resources, such as the GENIA treebank (Tateisi et 
al., 2005), and parser domain adaption (Hara et al, 
2007), which are of clear importance in parsing 
research, but of largely unconfirmed impact on 
practical systems. 
Quirk and Corston-Oliver (2006) examine a 
similar issue, the relationship between parser accu-
racy and overall system accuracy in syntax-
informed machine translation.  Their research is 
similar to the work presented here, but they fo-
cused on the use of varying amounts of out-of-
domain training data for the parser, measuring how 
a translation system for technical text performed 
when its syntactic parser was trained with varying 
amounts of Wall Street Journal text.  Our work, in 
contrast, investigates the use of domain-specific 
training material in parsers for biomedical text, a 
domain where significant amounts of effort are 
allocated for development of domain-specific NLP 
resources in hope that such resources will result in 
better overall performance in practical systems.  
3 A PPI extraction system based on syn-
tactic parsing 
PPI extraction is an NLP task to identify protein 
pairs that are mentioned as interacting in biomedi-
cal papers.  Figure 2 shows two sentences that in-
clude protein names: the former sentence mentions 
a protein interaction, while the latter does not.  
Given a protein pair, PPI extraction is a task of 
binary classification; for example, <IL-8, CXCR1> 
15
is a positive example, and <RBP, TTR> is a ne-
gative example. 
Following recent work on using dependency 
parsing in systems that identify protein interactions 
in biomedical text (Erkan et al, 2007; S?tre et al, 
2007; Katrenko and Adriaans, 2006), we have built 
a system for PPI extraction that uses dependency 
relations as features. As exemplified, for the pro-
tein pair IL-8 and CXCR1 in the first sentence of 
Figure 2, a dependency parser outputs a dependen-
cy tree shown in Figure 1.  From this dependency 
tree, we can extract a dependency path between 
IL-8 and CXCR1 (Figure 3), which appears to be 
a strong clue in knowing that these proteins are 
mentioned as interacting. 
The system we use in this paper is similar to the 
one described in S?tre et al (2007), except that it 
uses syntactic dependency paths obtained with a 
dependency parser, but not predicate-argument 
paths based on deep-parsing.  This method is based 
on SVM with SubSet Tree Kernels (Collins, 2002; 
Moschitti, 2006).  A dependency path is encoded 
as a flat tree as depicted in Figure 4. Because a tree 
kernel measures the similarity of trees by counting 
common subtrees, it is expected that the system 
finds effective subsequences of dependency paths.   
In addition to syntactic dependency features, we 
incorporate bag-of-words features, which are re-
garded as a strong baseline for IE systems.  We use 
lemmas of words before, between and after the pair 
of target proteins. 
In this paper, we use Aimed (Bunescu and 
Mooney, 2004), which is a popular benchmark for 
the evaluation of PPI extraction systems.  The 
Aimed corpus consists of 225 biomedical paper 
abstracts (1970 sentences), which are sentence-
split, tokenized, and annotated with proteins and 
PPIs.  
4 A data-driven dependency parser for 
biomedical text 
The parser we used as component of our PPI ex-
traction system was a shift-reduce dependency 
parser that uses maximum entropy models to de-
termine the parser?s actions.  Our overall parsing 
approach uses a best-first probabilistic shift-reduce 
algorithm, working left-to right to find labeled de-
pendencies one at a time. The algorithm is essen-
tially a dependency version of the constituent 
parsing algorithm for probabilistic parsing with 
LR-like data-driven models described by Sagae 
and Lavie (2006).  This dependency parser has 
been shown to have state-of-the-art accuracy in the 
CoNLL shared tasks on dependency parsing 
(Buchholz and Marsi, 2006; Nivre, 2007). Sagae 
and Tsujii (2007) present a detailed description of 
the parsing approach used in our work, including 
the parsing algorithm and the features used to clas-
sify parser actions.  In summary, the parser uses an 
algorithm similar to the LR parsing algorithm 
(Knuth, 1965), keeping a stack of partially built 
syntactic structures, and a queue of remaining in-
put tokens.  At each step in the parsing process, the 
parser can apply a shift action (remove a token 
from the front of the queue and place it on top of 
the stack), or a reduce action (pop the two topmost 
This study demonstrates that IL-8 recognizes 
and activates CXCR1, CXCR2, and the Duf-
fy antigen by distinct mechanisms. 
 
The molar ratio of serum retinol-binding pro-
tein (RBP) to transthyretin (TTR) is not 
useful to assess vitamin A status during infec-
tion in hospitalized children. 
Figure 2: Example sentences with protein names 
Figure 1: A dependency tree 
ROOT  IL-8  recognizes  and  activates  CXCR1 
ROOT 
SBJ 
OBJ 
COORD 
CC 
ENTITY1(IL-8)    recognizes   ENTITY2(CXCR1) 
Figure 3: A dependency path between protein names 
SBJ OBJ 
16
stack items, and push a new item composed of the 
two popped items combined in a single structure). 
This parsing approach is very similar to the one 
used successfully by Nivre et al (2006), but we 
use a maximum entropy classifier (Berger et al, 
1996) to determine parser actions, which makes 
parsing considerably faster. In addition, our pars-
ing approach performs a search over the space of 
possible parser actions, while Nivre et al?s ap-
proach is deterministic. 
The parser was trained using 8,000 sentences 
from the GENIA Treebank (Tateisi et al, 2005), 
which contains abstracts of papers taken from 
MEDLINE, annotated with syntactic structures.  
To determine the effects of training set size on the 
parser, and consequently on the PPI extraction sys-
tem, we trained several parsing models with differ-
ent amounts of GENIA Treebank data.  We started 
with 100 sentences, and increased the training set 
by 100 sentence increments, up to 1,000 sentences.  
From that point, we increased the training set by 
1,000 sentence increments.  Figure 5 shows the 
labeled dependency accuracy for the varying sizes 
of training sets.  The accuracy was measured on a 
portion of the GENIA Treebank reserved as devel-
opment data.  The result clearly demonstrates that 
the increase in the size of the training set contri-
butes to increasing parse accuracy.  Training the 
parser with only 100 sentences results in parse ac-
curacy of about 72.5%.  Accuracy rises sharply 
with additional training data until the size of the 
training set reaches about 1,000 sentences (about 
82.5% accuracy).  From there, accuracy climbs 
consistently, but slowly, until 85.6% accuracy is 
reached with 8,000 sentences of training data. 
It should be noted that parser accuracy on the 
Aimed data used in our PPI extraction experiments 
may be slightly lower, since the domain of the 
GENIA Treebank is not exactly the same as the 
Aimed corpus.  Both of them were extracted from 
MEDLINE, but the criteria for data selection were 
not the same in the two corpora, creating possible 
differences in sub-domains.  We also note that the 
accuracy of a parser trained with more than 40,000 
sentences from the Wall Street Journal portion of 
the Penn Treebank is under 79%, a level equivalent 
to that obtained by training the parser with only 
500 sentences of GENIA data. 
 
 
Figure 5: Data size vs. parse accuracy 
 
5 Experiments and Results 
In this section we present our PPI extraction expe-
riments applying the dependency parsers trained 
with the different amounts of the GENIA Treebank 
in our PPI system.  As we mentioned, the GENIA 
Treebank is used for training the parser, while the 
Aimed is used for training and evaluation of PPI 
extraction.  A part-of-speech tagger trained with 
GENIA and PennBioIE was used.  We do not ap-
ply automatic protein name detection, and instead 
use the gold-standard protein annotations in the 
Aimed corpus.  Before running a parser, multiword 
protein names are concatenated and treated as sin-
gle words. As described in Section 3, bag-of-words 
and syntactic dependency paths are fed as features 
to the PPI classifier. The accuracy of PPI extrac-
tion is measured by the abstract-wise 10-fold cross 
validation (S?tre et al 2007). 
When we use the part-of-speech tagger and the 
dependency parser trained with WSJ, the accuracy 
(F-score) of PPI extraction on this data set is 55.2.  
The accuracy increases to 56.9 when we train the 
part-of-speech tagger with GENIA and Penn BioIE, 
while using the WSJ-trained parser.  This confirms 
the claims by Lease and Charniak (2005) that sub-
sentential lexical analysis alone is helpful in adapt-
ing WSJ parsers to the biomedical domain.  While 
Lease and Charniak looked only at parse accuracy, 
70
75
80
85
90
0 2000 4000 6000 8000
Figure 4: A tree kernel representation of the dependency 
path 
(dep_path (SBJ (ENTITY1 ecognizes)) 
(rOBJ (recognizes ENTITY2))) 
17
our result shows that the increase in parse accuracy 
is, as expected, beneficial in practice. 
Figure 6 shows the relationship between the 
amount of parser training data and the F-score for 
the PPI extraction.  The result shows that the accu-
racy of PPI extraction increases with the use of 
more sentences to train the parser.    The best accu-
racy was obtained when using 4,000 sentences, 
where parsing accuracy is around 84.3.  Although 
it may appear that further increasing the training 
data for the parser may not improve the PPI extrac-
tion accuracy (since only small and inconsistent 
variations in F-score are observed in Figure 6), 
when we plot the curves shown in Figures 5 and 6 
in a single graph (Figure 7), we see that the two 
curves match each other to a large extent.  This is 
supported by the strong correlation between parse 
accuracy and PPI accuracy observed in Figure 8.  
While this suggests that training the parser with a 
larger treebank may result in improved accuracy in 
PPI extraction, we observe that a 1% absolute im-
provement in parser accuracy corresponds roughly 
to a 0.25 improvement in PPI extraction F-score.  
Figure 5 indicates that to obtain even a 1% im-
provement in parser accuracy by using more train-
ing data, the size of the treebank would have to 
increase significantly. 
Although the results presented so far seem to 
suggest the need for a large data annotation effort 
to achieve a meaningful improvement in PPI ex-
traction accuracy, there are other ways to improve 
the overall accuracy of the system without an im-
provement in parser accuracy.  One obvious alter-
native is to increase the size of the PPI-annotated 
corpus (which is distinct from the treebank used to 
train the parser).  As mentioned in section 3, our 
system is trained using the Aimed corpus, which 
contains 225 abstracts from biomedical papers with 
manual annotations indicating interactions between 
proteins.  Pairs of proteins with no interaction de-
scribed in the text are used as negative examples, 
and pairs of proteins described as interacting are 
used as positive examples.  The corpus contains a 
total of roughly 9,000 examples.  Figure 9 shows 
how the overall system accuracy varies when dif-
ferent amounts of training data (varying amounts 
of training examples) are used to train the PPI sys-
tem (keeping the parse accuracy constant, using all 
of the available training data in the GENIA tree-
bank to train the parser).  While Figure 5 indicates 
that a significant improvement in parse accuracy 
requires a large increase in the treebank used to 
train the parser, and Figure 7 shows that improve-
ments in PPI extraction accuracy may require a 
sizable improvement in parse accuracy, Figure 9 
suggests that even a relatively small increase in the 
PPI corpus may lead to a significant improvement 
in PPI extraction accuracy. 
 
Figure 6: Parser training data size vs. PPI extraction 
accuracy 
 
 
 
Figure 7: Parser training data size vs. parser accuracy 
and PPI extraction accuracy 
 
 
 
Figure 8: Parse accuracy vs. PPI extraction accuracy 
 
53
54
55
56
57
58
0 2000 4000 6000 8000
53
54
55
56
57
58
68
72
76
80
84
88
0 5000 10000
Parser
PPI F-score
53
54
55
56
57
58
70 75 80 85 90
18
 
 
Figure 9: Number of PPI training examples vs. PPI ex-
traction accuracy 
 
While some of the conclusions that can be 
drawn from these results may be somewhat sur-
prising, most are entirely expected.  However, even 
in these straightforward cases, our experiments 
provide some empirical evidence and concrete 
quantitative analysis to complement intuition.  We 
see that using domain-specific training data for the 
parsing component for the PPI extraction system 
produces superior results, compared to using train-
ing data from the WSJ Penn Treebank.  When the 
parser trained on WSJ sentences is used, PPI ex-
traction accuracy is about 55, compared to over 57 
when sentences from biomedical papers are used.  
This corresponds fairly closely to the differences in 
parser accuracy: the accuracy of the parser trained 
on 500 sentences from GENIA is about the same 
as the accuracy of the parser trained on the entire 
WSJ Penn Treebank, and when these parsers are 
used in the PPI extraction system, they result in 
similar overall task accuracy.  However, the results 
obtained when a domain-specific POS tagger is 
combined with a parser trained with out-of-domain 
data, overall PPI results are nearly at the same lev-
el as those obtained with domain-specific training 
data (just below 57 with a domain-specific POS 
tagger and out-of-domain parser, and just above 57 
for domain-specific POS tagger and parser).  At 
the same time, the argument against annotating 
domain-specific data for parsers in new domains is 
not a strong one, since higher accuracy levels (for 
both the parser and the overall system) can be ob-
tained with a relatively small amount of domain-
specific data. 
Figures 5, 6 and 7 also suggest that additional 
efforts in improving parser accuracy (through the 
use of feature engineering, other machine learning 
techniques, or an increase in the size of its training 
set) could improve PPI extraction accuracy, but a 
large improvement in parser accuracy may be re-
quired.  When we combine these results with the 
findings obtained by Miyao et al (2008), they sug-
gest that a better way to improve the overall sys-
tem is to spend more effort in designing a specific 
syntactic representation that addresses the needs of 
the system, instead of using a generic representa-
tion designed for measuring parser accuracy.  
Another potentially fruitful course of action is to 
design more sophisticated and effective ways for 
information extraction systems to use NLP tools, 
rather than simply extracting features that corres-
pond to small fragments of syntactic trees.  Of 
course, making proper use of natural language 
analysis is a considerable challenge, but one that 
should be kept in mind through the design of prac-
tical systems that use NLP components. 
6 Conclusion 
This paper presented empirical results on the rela-
tionship between the amount of training data used 
to create a dependency parser, and the accuracy of 
a system that performs identification of protein-
protein interactions using the dependency parser.  
We trained a dependency parser with different 
amounts of data from the GENIA Treebank to es-
tablish how the improvement in parse accuracy 
corresponds to improvement in practical task per-
formance in this information extraction task.  
While parsing accuracy clearly increased with 
larger amounts of data, and is likely to continue 
increasing with additional annotation of data for 
the GENIA Treebank, the trend in the accuracy of 
PPI extraction indicates that a sizable improvement 
in parse accuracy may be necessary for improved 
detection of protein interactions. 
When combined with recent findings by Miyao 
et al (2008), our results indicate that further work 
in designing PPI extraction systems that use syn-
tactic dependency features would benefit from 
more adequate syntactic representations or more 
sophisticated use of NLP than simple extraction of 
syntactic subtrees.  Furthermore, to improve accu-
racy in this task, efforts on data annotation should 
focus on task-specific data (manual annotation of 
40
45
50
55
60
65
0 5000 10000 15000
19
protein interactions in biomedical papers), rather 
than on additional training data for syntactic pars-
ers.  While annotation of parser training data might 
seems like a cost-effective choice, since improved 
parser results might be beneficial in a number of 
systems where the parser can be used, our results 
show that, in this particular task, efforts should be 
focused elsewhere, such as the annotation of addi-
tion PPI data.  
Acknowledgements 
We thank the anonymous reviewers for their in-
sightful comments.  This work was partially sup-
ported by Grant-in-Aid for Specially Promoted 
Research (MEXT, Japan), Genome Network 
Project (MEXT, Japan), and Grant-in-Aid for 
Young Scientists (MEXT, Japan). 
References 
 
Berger, A., S. A. Della Pietra, and V. J. Della Pietra. 
1996. A maximum entropy approach to natural lan-
guage processing. Computational Linguistics, 
22(1):39?71. 
Clegg, A. and Shepherd, A. 2007. Benchmarking natu-
ral-language parsers for biological applications using 
dependency graphs. BMC Bioinformatics, 8:24. 
Erkan, G., A. Ozgur, and D. R. Radev. 2007. Semisu-
pervised classification for extracting protein interac-
tion sentences using dependency parsing. In 
Proceedings of CoNLL-EMNLP 2007. 
Hara, T., Miyao, Y and Tsujii, J. 2007. Evaluating Im-
pact of Re-training a Lexical Disambiguation Model 
on Domain Adaptation of an HPSG Parser. In Pro-
ceedings of the International Conference on Parsing 
Technologies (IWPT). 
Katrenko, S. and P. W. Adriaans. 2006. Learning rela-
tions from biomedical corpora using dependency 
trees. In Proceedings of the first workshop on Know-
ledge Discovery and Emergent Complexity in BioIn-
formatics (KDECB), pages 61?80. 
Kulick, S., A. Bies, M. Liberman, M. Mandel, R. 
McDonald, M. Palmer, A. Schein and L. Ungar. 2004. 
Integrated Annotation for Biomedical Information 
Extraction. In Proceedings of Biolink 2004: Linking 
Biological Literature, Ontologies and Databases 
(HLT-NAACL workshop). 
Lease, M. and Charniak, E. 2005. Parsing Biomedical 
Literature. In R. Dale, K.-F. Wong, J. Su, and O. 
Kwong, editors, Proceedings of the 2nd International 
Joint Conference on Natural Language Processing 
(IJCNLP'05), volume 3651 of Lecture Notes in 
Computer Science, pages 58 ? 69. 
Miyao, Y., S?tre, R., Sagae, K., Matsuzaki, T. and Tsu-
jii, J. 2008. Task-Oriented Evaluation of Syntactic 
Parsers and Their Representations.  In Proceedings of 
the 46th Annual Meeting of the Association for Com-
putational Linguistics. 
Nivre, J., Hall, J., Kubler, S., McDonald, R., Nilsson, J., 
Riedel, S. and Yuret, D. 2007. The CoNLL 2007 
Shared Task on Dependency Parsing. In Proceedings 
the CoNLL 2007 Shared Task in EMNLP-CoNLL. 
Nivre, Joakim, Johan Hall, Jens Nilsson, Gulsen Eryi-
git,and Svetoslav Marinov. 2006. Labeled pseudo-
projective dependency parsing with support vector 
machines. In Proceedings of the Tenth Conference on 
Computational Natural Language Learning, shared 
task session. 
Pyysalo S., Ginter F., Haverinen K., Heimonen J., Sala-
koski T. and Laippala V. 2007. On the unification of 
syntactic annotations under the Stanford dependency 
scheme: A case study on BioInfer and GENIA.  In 
Proceedings of BioNLP 2007: Biological, Transla-
tional and Clinical Language Processing. 
Quirk, C. and Corston-Oliver S. 2006. The impact of 
parse quality on syntactically-informed statistical 
machine translation. In Proceedings of EMNLP 2007. 
S?tre, R., Sagae, K., and Tsujii, J. 2007. Syntactic fea-
tures for protein-protein interaction extraction.  In 
Proceedings of the International Symposium on Lan-
guages in Biology and Medicine (LBM short oral 
presentations). 
Sagae, K. and Lavie, A. 2006. A best-first probabilistic 
shift-reduce parser. In Proceedings of the 
COLING/ACL 2006 Main Conference Poster Ses-
sions, pages 691?698, Sydney, Australia, July. Asso-
ciation for Computational Linguistics. 
Sagae, K., Miyao, Y. and Tsujii, J. 2008. Challenges in 
Mapping of Syntactic Representations for Frame-
work-Independent Parser Evaluation. In Proceedings 
of the Workshop on Automated Syntatic Annotations 
for Interoperable Language Resources at the First 
International Conference on Global Interoperability 
for Language Resources (ICGL'08). 
Tateisi, Y., Yakushiji, A., Ohta, T., and Tsujii, J. 2005. 
Syntax annotation for the GENIA corpus. In Pro-
ceedings Second International Joint Conference on 
Natural Language Processing: Companion Volume 
including Posters/Demos and tutorial abstracts. 
20
BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 118?119,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Raising the Compatibility of Heterogeneous Annotations:
A Case Study on Protein Mention Recognition
Yue Wang? Kazuhiro Yoshida? Jin-Dong Kim? Rune S?tre? Jun?ichi Tsujii???
?Department of Computer Science, University of Tokyo
?School of Informatics, University of Manchester
?National Center for Text Mining
Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033 JAPAN
{wangyue, kyoshida, jdkim, rune.saetre, tsujii}@is.s.u-tokyo.ac.jp
Abstract
While there are several corpora which claim
to have annotations for protein references,
the heterogeneity between the annotations is
recognized as an obstacle to develop expen-
sive resources in a synergistic way. Here we
present a series of experimental results which
show the differences of protein mention an-
notations made to two corpora, GENIA and
AImed.
1 Introduction
There are several well-known corpora with protein
mention annotations. It is a natural request to bene-
fit from the existing annotations, but the heterogene-
ity of the annotations remains an obstacle. The het-
erogeneity is caused by different definitions of ?pro-
tein?, annotation conventions, and so on.
It is clear that by raising the compatibility of an-
notations, we can reduce the performance degrada-
tion caused by the heterogeneity of annotations.
In this work, we design several experiments to
observe the effect of removing or relaxing the het-
erogeneity between the annotations in two corpora.
The experimental results show that if we understand
where the difference is, we can raise the compati-
bility of the heterogeneous annotations by removing
the difference.
2 Corpora and protein mention recognizer
We used two corpora: the GENIA corpus (Kim
et al, 2003), and the AImed corpus (Bunescu and
Mooney, 2006). There are 2,000 MEDLINE ab-
stracts and 93,293 entities in the GENIA corpus.
?
??
??
??
??
??
??
??
??
??
?? ?? ?? ?? ??? ??? ??? ???????
???????????????????
???
????
Proceedings of the Workshop on BioNLP: Shared Task, pages 103?106,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
From Protein-Protein Interaction to Molecular Event Extraction
Rune S?tre?, Makoto Miwa?, Kazuhiro Yoshida? and Jun?ichi Tsujii?
{rune.saetre,mmiwa,kyoshida,tsujii}@is.s.u-tokyo.ac.jp
?Department of Computer Science
?Information Technology Center
University of Tokyo
Hongo 7-3-1, Bunkyo-ku, Tokyo, Japan
Abstract
This document describes the methods and re-
sults for our participation in the BioNLP?09
Shared Task #1 on Event Extraction. It also
contains some error analysis and a brief dis-
cussion of the results. Previous shared tasks in
the BioNLP community have focused on ex-
tracting gene and protein names, and on find-
ing (direct) protein-protein interactions (PPI).
This year?s task was slightly different, since
the protein names were already manually an-
notated in the text. The new challenge was
to extract biological events involving these
given gene and gene products. We modi-
fied a publicly available system (AkanePPI)
to apply it to this new, but similar, protein
interaction task. AkanePPI has previously
achieved state-of-the-art performance on all
existing public PPI corpora, and only small
changes were needed to achieve competitive
results on this event extraction task. Our of-
ficial result was an F-score of 36.9%, which
was ranked as number six among submissions
from 24 different groups. We later balanced
the recall/precision by including more predic-
tions than just the most confident one in am-
biguous cases, and this raised the F-score on
the test-set to 42.6%. The new Akane program
can be used freely for academic purposes.
1 Introduction
With the increasing number of publications report-
ing on protein interactions, there is also a steadily
increasing interest in extracting information from
Biomedical articles by using Natural Language Pro-
cessing (BioNLP). There has been several shared
tasks arranged by the BioNLP community to com-
pare different ways of doing such Information Ex-
traction (IE), as reviewed in Krallinger et al(2008).
Earlier shared tasks have dealt with Protein-
Protein Interaction (PPI) in general, but this
task focuses on more specific molecular events,
such as Gene expression, Transcription, Pro-
tein catabolism, Localization and Binding, plus
(Positive or Negative) Regulation of proteins or
other events. Most of these events are related to PPI,
so our hypothesis was that one of the best perform-
ing PPI systems would perform well also on this
new event extraction task. We decided to modify a
publicly available system with flexible configuration
scripting (Miwa et al, 2008). Some adjustments had
to be made to the existing system, like adding new
types of Named Entities (NE) to represent the events
mentioned above. The modified AkaneRE (for Re-
lation Extraction) can be freely used in academia1.
2 Material and Methods
The event extraction system is implemented in a
pipeline fashion (Fig. 1).
2.1 Tokenization and Sentence Boundary
Detection
The text was split into single sentences by a sim-
ple sentence detection program, and then each sen-
tence was split into words (tokens). The tokeniza-
tion was done by using white-space as the token-
separator, but since all protein names are known dur-
ing both training and testing, some extra tokeniza-
tion rules were applied. For example, the protein
1http://www-tsujii.is.s.u-tokyo.ac.jp/?satre/akane/
103
Recursive Template  
Output
POS tagging
Parsing
(Enju & GDep)
Event Clueword 
Recognition
Event Template 
Extraction
Machine 
Learning (ML)
Training Data
ML Filtering
POS tagging
Event Clueword 
Recognition
Event Template 
Filling
Test Data
Models with 
Templates
Parsing
(Enju & GDep)
Tokenization Tokenization
Figure 1: System Overview
name ?T cell factor 1? is treated as a single token,
?T cell factor 1?, and composite tokens including a
protein name, like ?(T cell factor 1)?, are split into
several tokens, like ?(?, ?T cell factor 1? and ?)?, by
adding space around all given protein names. Also,
punctuation (commas, periods etc.) were treated as
separate tokens.
2.2 POS-tagging and Parsing
We used Enju2 and GDep3 to parse the text. These
parsers have their own built-in Part-of-Speech (POS)
taggers, and Enju also provides a normalized lemma
form for each token.
2.3 Event Clue-word tagging
Event clue-word detection was performed by a Ma-
chine Learning (ML) sequence labeling program.
This named-entity tagger program is based on a first
order Maximum Entropy Markov Model (MEMM)
and is described in Yoshida and Tsujii (2007). The
clue-word annotation of the shared-task training set
was converted into BIO format, and used to train the
2http://www-tsujii.is.s.u-tokyo.ac.jp/enju/
3http://www.cs.cmu.edu/?sagae/parser/gdep/
MEMM model. The features used in the MEMM
model was extracted from surface strings and POS
information of the words corresponding to (or ad-
jacent to) the target BIO tags. The clue-word tag-
ger was applied to the development and test sets to
obtain the marginal probability that each word is a
clue-word of a certain category. The probabilities
were obtained by marginalizing the n-best output of
the MEMM tagger. We later also created clue-word
probability annotation of the training set, to enable
the template extraction program to access clue-word
probability information in the training phase.
2.4 Event Template Extraction
The training data was used to determine which
events to extract. As input to the system, a list of
Named Entity (NE) types and the Roles they can
play were provided. The roles can be thought of as
slots for arguments in event-frames, and in this task
the roles were Event (clue), Theme and Cause. In
the original AkanePPI (based on the AIMed corpus),
the only NE type was Protein, and the only role was
Theme (p1 and p2). All the (PPI) events were pair-
wise interactions, and there was no explicit event-
clue role. This means that all the events could be
represented with the single template shown first in
Table 1.
The BioNLP shared task used eight other NE
types, in addition to manually annotated Proteins,
namely Binding, Gene expression, Localization,
Protein catabolism, Transcription, Regulation, Pos-
itive Regulation and Negative Regulation. The first
five events have only Theme slots, which can only
be filled by Proteins, while the last three regulation
events are very diverse. They also have one Theme
slot, but they can have a Cause slot as well, and each
role/slot can be filled with either Proteins, or other
Events. See the first half of Table 1.
148 templates were extracted and clustered into
nine homogeneous groups which were classified
as nine separate sub-problems. The grouping was
based on whether the templates had an Event or a
Protein in the same role-positions. This way of orga-
nizing the groups was motivated by the fact that the
Proteins are 100% certain, while the accuracy of the
clue-word recognizer is only around 50% (estimated
on the training data). The bottom of Table 1 shows
the resulting nine general interaction templates.
104
2.5 Machine Learning with Maximum Entropy
Models
We integrated Maximum Entropy (ME) modeling,
also known as Logistic Regression, into AkaneRE.
This was done by using LIBLINEAR4, which han-
dles multi-class learning and prediction. Gold tem-
plates were extracted during training, and each tem-
plate was matched with all legal combinations of
Named Entities (including gold proteins/clue-words
and other recognized clue-word candidates) in each
sentence. The positive training examples were la-
beled as gold members of the template, and all other
combinations matching a given template were la-
beled as negative examples within that specific tem-
plate class. The templates were grouped into the
nine general templates shown in the bottom of Ta-
ble 1. Using one-vs-rest logistic regression, we
trained one multi-class classifier for each of the nine
groups individually. The ML features are shown in
Table 2.
In the test-phase, we extracted and labeled all re-
lation candidates matching all the templates from the
training-phase. The ML component was automati-
cally run independently for each of the nine groups
listed in the bottom of Table 1. Each time, all the
candidate template-instances in the current group
were assigned a confidence score by the classifier for
that group. This score is the probability that a can-
didate is a true relation, and a value above a certain
threshold means that the extracted relation will be
predicted as a true member of its specific template.
LIBLINEAR?s C-value parameter and the prediction
threshold were selected by hand to produce a good
F-score (according to the strict matching criterion)
on the development-test set.
2.6 Filtering and recursive output of the most
confident template instances
After machine learning, all the template instances
were filtered based on their confidence score. Af-
ter tuning the threshold to the development test-set,
we ended up using 1 as our C-value, and 3.5% as
our confidence threshold. Because the prediction
of Regulation Events were done independent from
the sub-events (or proteins) affected by that event,
some sub-events had to be included for complete-
4http://www.csie.ntu.edu.tw/?cjlin/liblinear/
ness, even if their confidence score was below the
threshold.
3 Results and Discussion
Our final official result was an F-score of 36.9%,
which was ranked as number six among the sub-
missions from 24 different groups. This means that
the AkanePPI system can achieve good results when
used on other PPI-related relation-extraction tasks,
such as this first BioNLP event recognition shared
task. The most common error was in predicting reg-
ulation events with other events as Theme or Cause.
The problem is that these events involve more than
one occurrence of event-trigger words, so the perfor-
mance is more negatively affected by our imperfect
clue-word detection system.
Since the recall was much lower on the test-set
than on the development test-set, we later allowed
the system to predict multiple confident alternatives
for a single event-word, and this raised our score on
the test-set from 36.9% to 42.6%. In hindsight, this
is obvious since there are many such examples in
the training data: E.g. ?over-express? is both posi-
tive regulation and Gene expression. The new sys-
tem, named AkaneRE (for Relation Extraction), can
be used freely for academic purposes.
As future work, we believe a closer integration
between the clue-word recognition and the template
prediction modules can lead to better performance.
Acknowledgments
?Grant-in-Aid for Specially Promoted Research?
and ?Genome Network Project?, MEXT, Japan.
References
Martin Krallinger et al 2008. Evaluation of text-mining
systems for biology: overview of the second biocre-
ative community challenge. Genome Biology, 9(S2).
Makoto Miwa, Rune S?tre, Yusuke Miyao, Tomoko
Ohta, and Jun?ichi Tsujii. 2008. Combining multi-
ple layers of syntactic information for protein-protein
interaction extraction. In Proceedings of SMBM 2008,
pages 101?108, Turku, Finland, September.
Kazuhiro Yoshida and Jun?ichi Tsujii. 2007. Reranking
for biomedical named-entity recognition. In Proceed-
ings of the Workshop on BioNLP 2007, June. Prague,
Czech Republic.
105
Freq Event Theme1 Theme2 Theme3 Theme4 Cause
- PPI Protein Protein
613 Binding Protein
213 Binding Protein Protein
3 Binding Protein Protein Protein
2 Binding Protein Protein Protein Protein
217 Regulation Protein Protein
12 Regulation Binding Protein
48 +Regulation Transcription Protein
4 +Regulation Phosphorylation Binding
5 -Regulation +Regulation Protein
... ... ... ...
Total 148 Templates
Count General Templates Theme1 Theme2 Theme3 Theme4 Cause
9 event templates Protein
1 event template Protein Protein
1 event template Protein Protein Protein
1 event template Protein Protein Protein Protein
3 event templates Protein Protein
12 event templates Protein Event
27 event templates Event
26 event templates Event Protein
68 event templates Event Event
Table 1: Interaction Templates from the training-set. Classic PPI at the top, compared to Binding and Regulation
events in the middle. 148 different templates were automatically extracted from the training data by AkaneRE. At
the bottom, the Generalized Interaction Templates are shown, with proteins distinguished from other Named Entities
(Events)
Feature Example
Text The binding of the most prominent factor, named TCF-1 ( T cell factor 1 ),
is correlated with the proto-enhancer activity of TCEd.
BOW B The
BOW M0 -comma- -lparen- factor most named of prominent PROTEIN the
BOW A -comma- -rparen- activity correlated is of proto-enhancer the TCEd with
Enju PATH (ENTITY1) (<prep arg12arg1) (of) (prep arg12arg2>) (factor)
(<verb arg123arg2) (name) (verb arg123arg3>) (ENTITY2)
pairs (ENTITY1 <prep arg12arg1) (<prep arg12arg1 of) (of prep arg12arg2>) ...
triples (ENTITY1 <prep arg12arg1 of) (<prep arg12arg1 of prep arg12arg2>) ...
GDep PATH (ENTITY1) (<NMOD) (name) (<VMOD) (ENTITY2)
pairs/triples (ENTITY1 <NMOD) (<NMOD name) ... (ENTITY1 <NMOD name) ...
Vector BOW B BOW M0...BOW M4 BOW A Enju PATH GDep PATH
Table 2: Bag-Of-Words (BOW) and shortest-path features for the machine learning. Several BOW feature groups were
created for each template, based on the position of the words in the sentence, relative to the position of the template?s
Named Entities (NE). Specifically, BOW B was made by the words from the beginning of the sentence to the first NE,
BOW A by the words between the last NE and the end of the sentence, and BOW M0 to BOW M4 was made by the
words between the main event clue-word and the NE in slot 0 through 4 respectively. The path features are made from
one, two or three neighbor nodes. We also included certain specific words, like ?binding?, as features.
106
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 788?796,
Beijing, August 2010
Entity-Focused Sentence Simplification for Relation Extraction
Makoto Miwa1 Rune S?tre1 Yusuke Miyao2 Jun?ichi Tsujii1,3,4
1Department of Computer Science, The University of Tokyo
2National Institute of Informatics
3School of Computer Science, University of Manchester
4National Center for Text Mining
mmiwa@is.s.u-tokyo.ac.jp,rune.saetre@is.s.u-tokyo.ac.jp,
yusuke@nii.ac.jp,tsujii@is.s.u-tokyo.ac.jp
Abstract
Relations between entities in text have
been widely researched in the natu-
ral language processing and information-
extraction communities. The region con-
necting a pair of entities (in a parsed
sentence) is often used to construct ker-
nels or feature vectors that can recognize
and extract interesting relations. Such re-
gions are useful, but they can also incor-
porate unnecessary distracting informa-
tion. In this paper, we propose a rule-
based method to remove the information
that is unnecessary for relation extraction.
Protein?protein interaction (PPI) is used
as an example relation extraction problem.
A dozen simple rules are defined on out-
put from a deep parser. Each rule specif-
ically examines the entities in one target
interaction pair. These simple rules were
tested using several PPI corpora. The PPI
extraction performance was improved on
all the PPI corpora.
1 Introduction
Relation extraction (RE) is the task of finding a
relevant semantic relation between two given tar-
get entities in a sentence (Sarawagi, 2008). Some
example relation types are person?organization
relations (Doddington et al, 2004), protein?
protein interactions (PPI), and disease?gene as-
sociations (DGA) (Chun et al, 2006). Among
the possible RE tasks, we chose the PPI extrac-
tion problem. PPI extraction is a major RE task;
around 10 corpora have been published for train-
ing and evaluation of PPI extraction systems.
Recently, machine-learning methods, boosted
by NLP techniques, have proved to be effec-
tive for RE. These methods are usually intended
to highlight or select the relation-related regions
in parsed sentences using feature vectors or ker-
nels. The shortest paths between a pair of enti-
ties (Bunescu and Mooney, 2005) or pair-enclosed
trees (Zhang et al, 2006) are widely used as focus
regions. These regions are useful, but they can in-
clude unnecessary sub-paths such as appositions,
which cause noisy features.
In this paper, we propose a method to remove
information that is deemed unnecessary for RE.
Instead of selecting the whole region between
a target pair, the target sentence is simplified
into simpler, pair-related, sentences using general,
task-independent, rules. By addressing particu-
larly the target entities, the rules do not affect im-
portant relation-related expressions between the
target entities. We show how rules of two groups
can be easily defined using the analytical capabil-
ity of a deep parser with specific examination of
the target entities. Rules of the first group can re-
place a sentence with a simpler sentence, still in-
cluding the two target entities. The other group of
rules can replace a large region (phrase) represent-
ing one target entity, with just a simple mention of
that target entity. With only a dozen simple rules,
we show that we can solve several simple well-
known problems in RE, and that we can improve
the performance of RE on all corpora in our PPI
test-set.
788
2 Related Works
The general paths, such as the shortest path or
pair-enclosed trees (Section 1), can only cover
a part of the necessary information for relation
extraction. Recent machine-learning methods
specifically examine how to extract the missing
information without adding too much noise. To
find more representative regions, some informa-
tion from outside the original regions must be
included. Several tree kernels have been pro-
posed to extract such regions from the parse
structure (Zhang et al, 2006). Also the graph
kernel method emphasizes internal paths with-
out ignoring outside information (Airola et al,
2008). Composite kernels have been used to com-
bine original information with outside informa-
tion (Zhang et al, 2006; Miwa et al, 2009).
The approaches described above are useful,
but they can include unnecessary information that
distracts learning. Jonnalagadda and Gonzalez
(2009) applied bioSimplify to relation extraction.
BioSimplify is developed to improve their link
grammar parser by simplifying the target sentence
in a general manner, so their method might re-
move important information for a given target re-
lation. For example, they might accidentally sim-
plify a noun phrase that is needed to extract the
relation. Still, they improved overall PPI extrac-
tion recall using such simplifications.
To remove unnecessary information from a sen-
tence, some works have addressed sentence sim-
plification by iteratively removing unnecessary
phrases. Most of this work is not task-specific;
it is intended to compress all information in a tar-
get sentence into a few words (Dorr et al, 2003;
Vanderwende et al, 2007). Among them, Vickrey
and Koller (2008) applied sentence simplification
to semantic role labeling. With retaining all argu-
ments of a verb, Vickrey simplified the sentence
by removing some information outside of the verb
and arguments.
3 Entity-Focused Sentence
Simplification
We simplify a target sentence using simple rules
applicable to the output of a deep parser called
Mogura (Matsuzaki et al, 2007), to remove noisy
information for relation extraction. Our method
relies on the deep parser; the rules depend on the
Head-driven Phrase Structure Grammar (HPSG)
used by Mogura, and all the rules are written for
the parser Enju XML output format. The deep
parser can produce deep syntactic and semantic
information, so we can define generally applica-
ble comprehensive rules on HPSG with specific
examination of the entities.
For sentence simplification in relation extrac-
tion, the meaning of the target sentence itself is
less important than maintaining the truth-value of
the relation (interact or not). For that purpose,
we define rules of two groups: clause-selection
rules and entity-phrase rules. A clause-selection
rule constructs a simpler sentence (still includ-
ing both target entities) by removing noisy infor-
mation before and after the relevant clause. An
entity-phrase rule simplifies an entity-containing
region without changing the truth-value of the re-
lation. By addressing the target entities particu-
larly, we can define rules for many applications,
and we can simplify target sentences with less
danger of losing relation-related mentions. The
rules are summarized in Table 1.
Our method is different from the sentence sim-
plification in other systems (ref. Section 2). First,
our method relies on the parser, while bioSimplify
by Jonnalagadda and Gonzalez (2009) is devel-
oped for the improvement of their parser. Second,
our method tries to keep only the relation-related
regions, unlike other general systems including
bioSimplify which tried to keep all information in
a sentence. Third, our entity-phrase rules modify
only the entity-containing phrases, while Vickrey
and Koller (2008) tries to remove all information
outside of the target verb and arguments.
3.1 Clause-selection Rules
In compound or complex sentences, it is natural
to assume that one clause includes both the target
entities and the relation-related information. It can
also be assumed that the remaining sentence parts,
outside the clause, contain less related (or noisy)
information. The clause-selection rules simplify a
sentence by retaining only the clause that includes
the target entities (and by discarding the remain-
der of the sentence). We define three types of
789
Rule Group Rule Type # Example (original? simplified )
Sentence Clause 1 We show that A interacts with B.? A interacts with B.
Clause Selection Relative Clause 2 ... A that interacts with B.? A interacts with B.
Copula 1 A is a protein that interacts with B.? A interacts with B.
Apposition 2 a protein, A? A
Entity Phrase Exemplification 4 proteins, such as A? AParentheses 2 a protein (A)? A
Coordination 3 protein and A? A
Table 1: Rules for Sentence Simplification. (# is the rule count. A and B are the target entities.)
(a) S
bbbbbbb \\\\\\\
... VP
bbbbbbb \\\\\\\
N*
ccccc [[[[[
Vcc
77
(copular) ...
bbbbbbb \\\\\\\
... ENTITY ... N* S-REL
bbbbbbb \\\\\\\
NP-REL
NN
...
ccccc [[[[[
... ENTITY ...
A is a protein that interacts with B .
(b) S
bbbbbbb \\\\\\\
N*
ccccc [[[[[
...
ccccc [[[[[
... ENTITY ... ... ENTITY ...
A interacts with B .
Figure 1: Copula Rule. (a) is simplified to (b).
The arrows represent predicate?argument rela-
tions.
(a) N*
bbbbbbb \\\\\\\
N* ...
bbbbbbb ]]]]]]]]]]]]]
PN
RR
55(apposition) N*
ccccc [[[[[
... ENTITY ...
protein , A
(b) N*
ccccc [[[[[
... ENTITY ...
A
Figure 2: Apposition Rule.
clause-selection rules for sentence clauses, rela-
tive clauses, and copula. The sentence clause rule
finds the (smallest) clause that includes both tar-
get entities. It then replaces the original sentence
with the clause. The relative clause rules con-
struct a simple sentence from a relative clause and
the antecedent. If this simple sentence includes
the target entities, it is used instead of the orig-
inal sentence. We define two rules for the case
where the antecedent is the subject of the relative
clause. One rule is used when the relative clause
includes both the target entities. The other rule is
used when the antecedent includes one target en-
tity and the relative clause includes the other tar-
get entity. The copula rule is for sentences that
include copular verbs (e.g. be, is, become, etc).
The rule constructs a simple sentence from a rel-
ative clause with the subject of the copular verb
as the antecedent subject of the clause. The rule
replaces the target sentence with the constructed
sentence, if the relative clause includes one target
entity and the subject of a copular verb includes
the other target entity, as shown in Figure 1.
3.2 Entity-phrase Rules
Even the simple clauses (or paths between two
target entities) include redundant or noisy expres-
sions that can distract relation extraction. Some
of these expressions are related to the target enti-
ties, but because they do not affect the truth-value
of the relation, they can be deleted to make the
path simple and clear. The target problem affects
which expressions can be removed. We define
four types of rules for appositions, exemplifica-
tions, parentheses, and coordinations. Two appo-
sition rules are defined to select the correct ele-
ment from an appositional expression. One ele-
ment modifies or defines the other element in ap-
position, but the two elements represent the same
information from the viewpoint of PPI. If the tar-
get entity is in one of these elements, removing the
other element does not affect the truth-value of the
interaction. Many of these apposition expressions
are identified by the deep parser. The rule to se-
lect the last element is presented in Figure 2. Four
exemplification rules are defined for the two ma-
jor types of expressions using the phrases ?includ-
ing? or ?such as?. Exemplification is represented
by hyponymy or hypernymy. As for appositions,
the truth-value of the interaction does not change
whether we use the specific mention or the hyper-
class that the mention represents. Two parenthe-
ses rules are defined. Parentheses are useful for
synonyms, hyponyms, or hypernyms (ref. the two
790
1: S ? input sentence
2: repeat
3: reset rules {apply all the rules again}
4: P ? parse S
5: repeat
6: r ? next rule {null if no more rules}
7: if r is applicable to P then
8: P ? apply r to P
9: S ? sentence extracted from P
10: break (Goto 3)
11: end if
12: until r is null
13: until r is null
14: return S
Figure 3: Pseudo-code for sentence simplifica-
tion.
former rules). Three coordination rules are de-
fined. Removing other phrases from coordinated
expressions that include a target entity does not
affect the truth-value of the target relation. Two
rules are defined for simple coordination between
two phrases (e.g. select left or right phrase), and
one rule is defined to (recursively) remove one
element from lists of more than two coordinated
phrases (while maintaining the coordinating con-
junction, e.g. ?and?).
3.3 Sentence Simplification
To simplify a sentence, we apply rules repeatedly
until no more applications are possible as pre-
sented in Figure 3. After one application of one
rule, the simplified sentence is re-parsed before
attempting to apply all the rules again. This is be-
cause we require a consistent parse tree as a start-
ing point for additional applications of the rules,
and because a parser can produce more reliable
output for a partly simplified sentence than for the
original sentence. Using this method, we can also
backtrack and seek out conversion errors by exam-
ining the cascade of partly simplified sentences.
4 Evaluation
To elucidate the effect of the sentence simplifi-
cation, we applied the rules to five PPI corpora
and evaluated the PPI extraction performance. We
then analyzed the errors. The evaluation settings
will be explained in Section 4.1. The results of the
PPI extraction will be explained in Section 4.2. Fi-
nally, the deeper analysis results will be presented
in Section 4.3.
4.1 Experimental Settings
The state-of-the-art PPI extraction system
AkaneRE by Miwa et al (2009) was used to
evaluate our approach. The system uses a com-
bination of three feature vectors: bag-of-words
(BOW), shortest path (SP), and graph features.
Classification models are trained with a support
vector machine (SVM), and AkaneRE (with
Mogura) is used with default parameter settings.
The following two systems are used for a state-
of-the-art comparison: AkaneRE with multiple
parsers and corpora (Miwa et al, 2009), and
Airola et al (2008) single-parser, single-corpus
system.
The rules were evaluated on the BioIn-
fer (Pyysalo et al, 2007), AIMed (Bunescu et al,
2005), IEPA (Ding et al, 2002), HPRD50 (Fun-
del et al, 2006), and LLL (Ne?dellec, 2005) cor-
pora1. Table 2 shows the number of positive (in-
teracting) vs. all pairs. One duplicated abstract in
the AIMed corpus was removed.
These corpora have several differences in their
definition of entities and relations (Pyysalo et al,
2008). In fact, BioInfer and AIMed target al oc-
curring entities related to the corpora (proteins,
genes, etc). On the other hand, IEPA, HPRD50,
and LLL only use limited named entities, based
either on a list of entity names or on a named en-
tity recognizer. Only BioInfer is annotated for
other event types in addition to PPI, including
static relations such as protein family member-
ship. The sentence lengths are also different. The
duplicated pair-containing sentences contain the
following numbers of words on average: 35.8 in
BioInfer, 31.3 in AIMed, 31.8 in IEPA, 26.5 in
HPRD50, and 33.4 in LLL.
For BioInfer, AIMed, and IEPA, each corpus is
split into training, development, and test datasets2.
The training dataset from AIMed was the only
training dataset used for validating the rules. The
development datasets are used for error analysis.
The evaluation was done on the test dataset, with
models trained using training and development
1http://mars.cs.utu.fi/PPICorpora/
GraphKernel.html
2This split method will be made public later.
791
BioInfer AIMed IEPA HPRD50 LLL
pos all pos all pos all pos all pos all
training 1,848 7,108 684 4,072 256 630 - - - -
development 256 928 102 608 23 51 - - - -
test 425 1,618 194 1,095 56 136 - - - -
all 2,534 9,653 980 5,775 335 817 163 433 164 330
Table 2: Number of positive (pos) vs. all possible sentence pairs in used PPI corpora.
BioInfer AIMed IEPA
Rule Applied F AUC Applied F AUC Applied F AUC
No Application 0 62.5 83.0 0 61.2 87.9 0 73.4 82.5
Clause Selection 4,313 63.5 83.9 2,569 62.5 88.2 307 75.0 83.7
Entity Phrase 22,066 60.5 80.9 7,784 61.2 86.1 1,031 72.7 83.3
ALL 26,281 62.9 82.1 10,783 60.2 85.7 1,343 75.4 85.7
Table 3: Performance of PPI Extraction on test datasets. ?Applied? represents the number of times the
rules are applied on the corpus. ?No Application? means PPI extraction without sentence simplification.
ALL is the case all rules are used. The top scores for each corpus are shown in bold.
datasets). Ten-fold cross-validation (CV) was
done to facilitate comparison with other existing
systems. For HPRD50 and LLL, there are insuf-
ficient examples to split the data, so we use these
corpora only for comparing the scores and statis-
tics. We split the corpora for the CV, and mea-
sured the F -score (%) and area under the receiver
operating characteristic (ROC) curve (AUC) as
recommended in (Airola et al, 2008). We count
each occurrence as one example because the cor-
rect interactions must be extracted for each occur-
rence if the same protein name occurs multiple
times in a sentence.
In the experiments, the rules are applied in the
following order: sentence?clause, exemplifica-
tion, apposition, parentheses, coordination, cop-
ula, and relative-clause rules. Furthermore, if the
same rule is applicable in different parts of the
parse tree, then the rule is first applied closest to
the leaf-nodes (deepest first). The order of the
rules is arbitrary; changing it does not affect the
results much. We conducted five experiments us-
ing the training and development dataset in IEPA,
each time with a random shuffling of the order of
the rules; the results were 77.8?0.26 in F -score
and 85.9?0.55 in AUC.
4.2 Performance of PPI Extraction
The performance after rule application was bet-
ter than the baseline (no application) on all the
corpora, and most rules could be frequently ap-
plied. We show the PPI extraction performance on
Rule Applied F AUC
No Application 0 72.9 84.5
Sentence Clause 145 71.6 83.8
Relative Clause 7 73.3 84.1
Copula 0 72.9 84.5
Clause Selection 152 71.4 83.4
Apposition 64 73.2 84.6
Exemplification 33 72.9 84.7
Parentheses 90 72.9 85.1
Coordination 417 73.6 85.4
Entity Phrase 605 74.1 86.6
ALL 763 75.0 86.6
Table 4: Performance of PPI Extraction on
HPRD50.
Rule Applied F AUC
No Application 0 79.0 84.6
Sentence Clause 135 81.3 85.2
Relative Clause 42 78.8 84.6
Copula 0 79.0 84.6
Clause Selection 178 81.0 85.6
Apposition 197 79.6 83.9
Exemplification 0 79.0 84.6
Parentheses 56 79.5 85.8
Coordination 322 84.2 89.4
Entity Phrase 602 83.8 90.1
ALL 761 82.9 90.5
Table 5: Performance of PPI Extraction on LLL.
BioInfer, AIMed, and IEPA with rules of different
groups in Table 3. The effect of using rules of
different types for PPI extraction from HPRD50
and LLL is reported in Table 4 and Table 5. Ta-
ble 6 shows the number of times each rule was
applied in an ?apply all-rules? experiment. The
usability of the rules depends on the corpus, and
different combinations of rules produce different
792
Rule B AIMed IEPA H LLL
S. Cl. 3,960 2,346 300 150 135
R. Cl. 287 212 17 5 24
Copula 60 57 1 0 0
Cl. Sel. 4,307 2,615 318 155 159
Appos. 3,845 1,100 99 69 198
Exempl. 383 127 11 33 0
Paren. 2,721 2,158 235 91 88
Coord. 15,025 4,783 680 415 316
E. Foc. 21,974 8,168 1,025 608 602
Sum 26,281 10,783 1,343 763 761
Table 6: Distribution of the number of rules ap-
plied when all rules are applied. B:BioInfer, and
H:HPRD50 corpora.
Rules Miwa et al Airola et al
F AUC F AUC F AUC
B 60.0 79.8 68.3 86.4 61.3 81.9
A 54.9 83.7 65.2 89.3 56.4 84.8
I 77.8 88.7 76.6 87.8 75.1 85.1
H 75.0 86.6 74.9 87.9 63.4 79.7
L 82.9 90.5 86.7 90.8 76.8 83.4
Table 7: Comparison with the results by Miwa et
al. (2009) and Airola et al (2008). The results
with all rules are reported.
results. For the clause-selection rules, the per-
formance was as good as or better than the base-
line for all corpora, except for HPRD50, which
indicates that the pair-containing clauses also in-
clude most of the important information for PPI
extraction. Clause selection rules alone could im-
prove the overall performance for the BioInfer and
AIMed corpora. Entity-phrase rules greatly im-
proved the performance on the IEPA, HPRD50,
and LLL corpora, although these rules degraded
the performance on the BioInfer and AIMed cor-
pora. These phenomena hold even if we use small
parts of the two corpora, so this is not because of
the size of the corpora.
We compare our results with the results by
Miwa et al (2009) and Airola et al (2008) in Ta-
ble 7. On three of five corpora, our method pro-
vides better results than the state-of-the-art system
by Airola et al (2008), and also provides com-
parable results to those obtained using multiple
parsers and corpora (Miwa et al, 2009) despite
the fact that our method uses one parser and one
corpus at a time. We cannot directly compare our
result with Jonnalagadda and Gonzalez (2009) be-
cause the evaluation scheme, the baseline system,
[FP?TN][Sentence, Parenthesis, Coordination] To
characterize the AAV functions mediating this effect,
cloned AAV type 2 wild-type or mutant genomes were
transfected into simian virus 40 (SV40)-transformed
hamster cells together with the six HSV replication genes
(encoding UL5, UL8, major DNA-binding protein, DNA
polymerase, UL42 , and UL52) which together are
necessary and sufficient for the induction of SV40 DNA
amplification (R. Heilbronn and H. zur Hausen, J. Virol.
63:3683-3692, 1989). (BioInfer.d760.s0)
[TP?FN][Coordination] Both the GT155-calnexin and
the GT155-CAP-60 interactions were dependent on the
presence of a correctly modified oligosaccharide group
on GT155, a characteristic of many calnexin interactions.
(AIMed.d167.s1408)
[TN?TN][Coordination, Parenthesis] Leptin may act as
a negative feedback signal to the hypothalamic control of
appetite through suppression of neuropeptide Y (NPY)
secretion and stimulation of cocaine and amphetamine
regulated transcript (CART) . (IEPA.d190.s454)
Figure 4: A rule-related error, a critical error, and
a parser-related error. Regions removed by the
rules are underlined, and target proteins are shown
in bold. Predictions, applied rules, and sentence
IDs are shown.
[FN?TP][Sentence, Coordination] WASp contains a
binding motif for the Rho GTPase CDC42Hs as well as
verprolin / cofilin-like actin-regulatory domains , but no
specific actin structure regulated by CDC42Hs-WASp has
been identified. (BioInfer.d795.s0)
[FN?TP][Parenthesis, Apposition] The protein Raf-1 , a
key mediator of mitogenesis and differentiation, associates
with p21ras (refs 1-3) . (AIMed.d124.s1055)
[FN?TP][Sentence, Parenthesis] On the basis of
far-Western blot and plasmon resonance (BIAcore)
experiments, we show here that recombinant bovine
prion protein (bPrP) (25-242) strongly interacts with the
catalytic alpha/alpha? subunits of protein kinase CK2
(also termed ?casein kinase 2?) (IEPA.d197.s479)
Figure 5: Correctly simplified cases. The first
sentence is a difficult (not PPI) relation, which is
typed as ?Similar? in the BioInfer corpus.
and test parts differ.
4.3 Analysis
We trained models using the training datasets
and classified the examples in the development
datasets. Two types of analysis were performed
based on these results: simplification-based and
classification-based analysis.
For the simplification-based analysis, we com-
pared positive (interacting) and negative pair sen-
tences that produce the exact same (inconsistent)
sentence after protein names normalization and
793
BioInfer AIMed IEPA
Before simplification FN FP TP TN FN FP TP TN FN FP TP TN Not AffectedAfter simplification TP TN FN FP TP TN FN FP TP TN FN FP
No Error 18 2 3 35 14 21 21 8 3 2 0 4 32
No Application 3 2 0 3 0 7 8 0 0 1 0 1 7
Number of Errors 0 2 0 32 4 2 1 4 0 0 0 0 1
Number of Pairs 21 6 3 70 18 30 30 12 3 3 0 5 40
Coordination 0 0 0 20 4 2 1 0 0 0 0 0 1
Sentence 0 2 0 4 0 0 0 4 0 0 0 0 0
Parenthesis 0 0 0 5 0 0 0 0 0 0 0 0 0
Exemplification 0 0 0 2 0 0 0 0 0 0 0 0 0
Apposition 0 0 0 1 0 0 0 0 0 0 0 0 0
Table 8: Distribution of sentence simplification errors compared to unsimplified predictions with their
types (on the three development datasets). TP, True Positive; TN, True Negative; FN, False Negative;
FP, False Positive. ?No Error? means that simplification was correct; ?No Application? means that no
rule could be applied; Other rule names mean that an error resulted from that rule application. ?Not
Affected? means that the prediction outcome did not change.
simplification in the training dataset. The numbers
of such inconsistent sentences are 7 for BioIn-
fer, 78 for AIMed, and 1 for IEPA. The few in-
consistencies in BioInfer and IEPA are from er-
rors by the rules, mainly triggered by parse errors.
The frequent inconsistencies in AIMed are mostly
from inconsistent annotations. For example, even
if all coordinated proteins are either interacting or
not, only the first protein mention is annotated as
interacting.
For the classification-based analysis, we
specifically examine simplified pairs that were
predicted differently before and after the simplifi-
cation. Pairs predicted differently before and after
rule application were selected: 100 random pairs
from BioInfer and all 90 pairs from AIMed. For
IEPA, all 51 pairs are reported. Simplified results
are classified as errors when the rules affect a re-
gion unrelated to the entities in the smallest sen-
tence clause. The results of analysis are shown in
Table 8. There were 34 errors in BioInfer, and 11
errors in AIMed. Among the errors, there were
five critical errors (in two sentences, in AIMed).
Critical errors mean that the pairs lost relation-
related mentions, and the errors are the only er-
rors which caused the changes in the truth-value
of the relation. There was also a rule-related er-
ror (in BioInfer), which means that rules with cor-
rect parse results affect a region unrelated to the
entities, and parse errors (parser-related errors).
Figure 4 shows the rule-related error in BioInfer,
one critical error in AIMed, and one parser-related
error in IEPA.
5 Discussion
Our end goal is to provide consistent relation
extraction for real tasks. Here we discuss the
?safety? of applying our simplification rules, the
difficulties in the BioInfer and AIMed corpora, the
reduction of errors, and the requirements for such
a general (PPI) extraction system.
Our rules are applicable to sentences, with little
danger of changing the relation-related mentions.
Figure 5 shows three successfully simplified cases
(?No Error? cases from Table 8). The sentence
simplification leaves sufficient information to de-
termine the value of the relation in these exam-
ples. Relation-related mentions remained for most
of the simplification error cases. There were only
five critical errors, which changed the truth-value
of the relation, out of 46 errors in 241 pairs shown
in Table 8. Please note that some rules can be
dangerous for other relation extraction tasks. For
example, the sentence clause rule could remove
modality information (negation, speculation, etc.)
modifying the clause, but there are few such cases
in the PPI corpora (see Table 8). Also, the task of
hedge detection (Morante and Daelemans, 2009)
can be solved separately, in the original sentences,
after the interacting pairs have been found. For
example, in the BioNLP shared task challenge
and the BioInfer corpus, interaction detection and
modality are treated as two different tasks. Once
other NLP tasks, like static relation (Pyysalo et
794
al., 2009) or coreference resolution, become good
enough, they can supplement or even substitute
some of the proposed rules.
There are different difficulties in the BioInfer
and AIMed corpora. BioInfer includes more com-
plicated sentences and problems than the other
corpora do, because 1) the apposition, coordi-
nation, and exemplification rules are more fre-
quently used in the BioInfer corpus than in the
other corpora (shown in Table 6), 2) there were
more errors in the BioInfer corpus than in other
corpora among the simplified sentences (shown
in Table 8), and 3) BioInfer has more words per
sentence and more relation types than the other
corpora. AIMed contains several annotation in-
consistencies as explained in Section 4.3. These
inconsistencies must be removed to properly eval-
uate the effect of our method.
Simplification errors are mostly caused by
parse errors. Our rule specifically examines a part
of parser output; a probability is attached to the
part. The probability is useful for defining the or-
der of rule applications, and the n-best results by
the parser are useful to fix major errors such as co-
ordination errors. By using these modifications of
rule applications and by continuous improvement
in parsing technology for the biomedical domain,
the performance on the BioInfer and AIMed cor-
pora will be improved also for the all rules case.
The PPI extraction system lost the ability to
capture some of the relation-related expressions
left by the simplification rules. This indicates
that the system used to extract some relations (be-
fore simplification) by using back-off features like
bag-of-words. The system can reduce bad effects
caused by parse errors, but it also captures the an-
notation inconsistencies in AIMed. Our simpli-
fication (without errors) can capture more general
expressions needed for relation extraction. To pro-
vide consistent PPI relation extraction in a general
setting (e.g. for multiple corpora or for other pub-
lic text collections), the parse errors must be dealt
with, and a relation extraction system that can cap-
ture (only) general relation-related expressions is
needed.
6 Conclusion
We proposed a method to simplify sentences, par-
ticularly addressing the target entities for relation
extraction. Using a few simple rules applicable
to the output of a deep parser called Mogura,
we showed that sentence simplification is effec-
tive for relation extraction. Applying all the rules
improved the performance on three of the five
corpora, while applying only the clause-selection
rules raised the performance for the remaining two
corpora as well. We analyzed the simplification
results, and showed that the simple rules are ap-
plicable with little danger of changing the truth-
values of the interactions.
The main contributions of this paper are: 1) ex-
planation of general sentence simplification rules
using HPSG for relation extraction, 2) presenting
evidence that application of the rules improve re-
lation extraction performance, and 3) presentation
of an error analysis from two viewpoints: simpli-
fication and classification results.
As future work, we are planning to refine and
complete the current set of rules, and to cover
the shortcomings of the deep parser. Using these
rules, we can then make better use of the parser?s
capabilities. We will also attempt to apply our
simplification rules to other relation extraction
problems than those of PPI.
Acknowledgments
This work was partially supported by Grant-in-
Aid for Specially Promoted Research (MEXT,
Japan), Genome Network Project (MEXT, Japan),
and Scientific Research (C) (General) (MEXT,
Japan).
795
References
Airola, Antti, Sampo Pyysalo, Jari Bjo?rne, Tapio
Pahikkala, Filip Ginter, and Tapio Salakoski. 2008.
A graph kernel for protein-protein interaction ex-
traction. In Proceedings of the BioNLP 2008 work-
shop.
Bunescu, Razvan C. and Raymond J. Mooney. 2005.
A shortest path dependency kernel for relation ex-
traction. In HLT ?05: Proceedings of the confer-
ence on Human Language Technology and Empiri-
cal Methods in Natural Language Processing, pages
724?731.
Bunescu, Razvan C., Ruifang Ge, Rohit J. Kate, Ed-
ward M. Marcotte, Raymond J. Mooney, Arun K.
Ramani, and Yuk Wah Wong. 2005. Compara-
tive experiments on learning information extractors
for proteins and their interactions. Artificial Intelli-
gence in Medicine, 33(2):139?155.
Chun, Hong-Woo, Yoshimasa Tsuruoka, Jin-Dong
Kim, Rie Shiba, Naoki Nagata, Teruyoshi Hishiki,
and Jun?ichi Tsujii. 2006. Extraction of gene-
disease relations from medline using domain dictio-
naries and machine learning. In The Pacific Sympo-
sium on Biocomputing (PSB), pages 4?15.
Ding, J., D. Berleant, D. Nettleton, and E. Wurtele.
2002. Mining medline: abstracts, sentences, or
phrases? Pacific Symposium on Biocomputing,
pages 326?337.
Doddington, George, Alexis Mitchell, Mark Przy-
bocki, Lance Ramshaw, Stephanie Strassel, and
Ralph Weischedel. 2004. The automatic content
extraction (ACE) program: Tasks, data, and evalua-
tion. In Proceedings of LREC?04, pages 837?840.
Dorr, Bonnie, David Zajic, and Richard Schwartz.
2003. Hedge trimmer: A parse-and-trim approach
to headline generation. In in Proceedings of Work-
shop on Automatic Summarization, pages 1?8.
Fundel, Katrin, Robert Ku?ffner, and Ralf Zimmer.
2006. Relex?relation extraction using dependency
parse trees. Bioinformatics, 23(3):365?371.
Jonnalagadda, Siddhartha and Graciela Gonzalez.
2009. Sentence simplification aids protein-protein
interaction extraction. In Proceedings of the 3rd
International Symposium on Languages in Biology
and Medicine, pages 109?114, November.
Matsuzaki, Takuya, Yusuke Miyao, and Jun?ichi Tsu-
jii. 2007. Efficient HPSG parsing with supertag-
ging and cfg-filtering. In IJCAI?07: Proceedings of
the 20th international joint conference on Artifical
intelligence, pages 1671?1676, San Francisco, CA,
USA. Morgan Kaufmann Publishers Inc.
Miwa, Makoto, Rune S?tre, Yusuke Miyao, and
Jun?ichi Tsujii. 2009. Protein-protein interac-
tion extraction by leveraging multiple kernels and
parsers. International Journal of Medical Informat-
ics, June.
Morante, Roser and Walter Daelemans. 2009. Learn-
ing the scope of hedge cues in biomedical texts. In
Proceedings of the BioNLP 2009 Workshop, pages
28?36, Boulder, Colorado, June. Association for
Computational Linguistics.
Ne?dellec, Claire. 2005. Learning language in logic -
genic interaction extraction challenge. In Proceed-
ings of the LLL?05 Workshop.
Pyysalo, Sampo, Filip Ginter, Juho Heimonen, Jari
Bjo?rne, Jorma Boberg, Jouni Ja?rvinen, and Tapio
Salakoski. 2007. BioInfer: A corpus for infor-
mation extraction in the biomedical domain. BMC
Bioinformatics, 8:50.
Pyysalo, Sampo, Antti Airola, Juho Heimonen, Jari
Bjo?rne, Filip Ginter, and Tapio Salakoski. 2008.
Comparative analysis of five protein-protein inter-
action corpora. In BMC Bioinformatics, volume
9(Suppl 3), page S6.
Pyysalo, Sampo, Tomoko Ohta, Jin-Dong Kim, and
Jun?ichi Tsujii. 2009. Static relations: a piece
in the biomedical information extraction puzzle.
In BioNLP ?09: Proceedings of the Workshop on
BioNLP, pages 1?9, Morristown, NJ, USA. Asso-
ciation for Computational Linguistics.
Sarawagi, Sunita. 2008. Information extraction.
Foundations and Trends in Databases, 1(3):261?
377.
Vanderwende, Lucy, Hisami Suzuki, Chris Brockett,
and Ani Nenkova. 2007. Beyond sumbasic: Task-
focused summarization with sentence simplifica-
tion and lexical expansion. Inf. Process. Manage.,
43(6):1606?1618.
Vickrey, David and Daphne Koller. 2008. Sentence
simplification for semantic role labeling. In Pro-
ceedings of ACL-08: HLT, pages 344?352, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
Zhang, Min, Jie Zhang, Jian Su, and Guodong Zhou.
2006. A composite kernel to extract relations be-
tween entities with both flat and structured features.
In ACL-44: Proceedings of the 21st International
Conference on Computational Linguistics and the
44th annual meeting of the Association for Compu-
tational Linguistics, pages 825?832. Association for
Computational Linguistics.
796
