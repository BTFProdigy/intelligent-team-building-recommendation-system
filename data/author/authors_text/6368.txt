275
276
277
278
279
280
281
282
283
284
285
286
287
288
289
290
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 117?120,
New York, June 2006. c?2006 Association for Computational Linguistics
Quantitative Methods for Classifying Writing Systems
Gerald Penn
University of Toronto
10 King?s College Rd.
Toronto M5S 3G4, Canada
gpenn@cs.toronto.edu
Travis Choma
Cognitive Science Center Amsterdam
Sarphatistraat 104
1018 GV Amsterdam, Netherlands
travischoma@gmail.com
Abstract
We describe work in progress on using
quantitative methods to classify writing
systems according to Sproat?s (2000) clas-
sification grid using unannotated data. We
specifically propose two quantitative tests
for determining the type of phonography
in a writing system, and its degree of lo-
gography, respectively.
1 Background
If you understood all of the world?s languages, you
would still not be able to read many of the texts
that you find on the world wide web, because they
are written in non-Roman scripts that have been ar-
bitrarily encoded for electronic transmission in the
absence of an accepted standard. This very mod-
ern nuisance reflects a dilemma as ancient as writ-
ing itself: the association between a language as
it is spoken and the language as it is written has a
sort of internal logic to it that we can comprehend,
but the conventions are different in every individ-
ual case ? even among languages that use the same
script, or between scripts used by the same language.
This conventional association between language and
script, called a writing system, is indeed reminis-
cent of the Saussurean conception of language itself,
a conventional association of meaning and sound,
upon which modern linguistic theory is based.
Despite linguists? necessary reliance upon writ-
ing to present and preserve linguistic data, how-
ever, writing systems were a largely neglected cor-
ner of linguistics until the 1960s, when Gelb (1963)
presented the first classification of writing systems.
Now known as the Gelb teleology, this classification
viewed the variation we see among writing systems,
particularly in the size of linguistic ?chunks? rep-
resented by an individual character or unit of writ-
ing (for simplicity, referred to here as a grapheme),
along a linear, evolutionary progression, beginning
with the pictographic forerunners of writing, pro-
ceeding through ?primitive? writing systems such as
Chinese and Egyptian hieroglyphics, and culminat-
ing in alphabetic Greek and Latin.
While the linear and evolutionary aspects of
Gelb?s teleology have been rejected by more recent
work on the classification of writing systems, the ad-
mission that more than one dimension may be nec-
essary to characterize the world?s writing systems
has not come easily. The ongoing polemic between
Sampson (1985) and DeFrancis (1989), for exam-
ple, while addressing some very important issues in
the study of writing systems,1 has been confined ex-
clusively to a debate over which of several arboreal
classifications of writing is more adequate.
Sproat (2000)?s classification was the first multi-
dimensional one. While acknowledging that other
dimensions may exist, Sproat (2000) arranges writ-
ing systems along the two principal dimensions of
Type of Phonography and Amount of Logography,
both of which will be elaborated upon below. This
is the departure point for our present study.
Our goal is to identify quantitative methods that
1These include what, if anything, separates true writing sys-
tems from other more limited written forms of communication,
and the psychological reality of our classifications in the minds
of native readers.
117
Type of Phonography
Consonantal Polyconsonantal Alphabetic Core Syllabic Syllabic
W. Semitic English, PahawhHmong Linear B Modern YiGreek,
Korean,
Devanagari
?
?
A
m
o
u
n
to
fL
o
go
gr
ap
hy
Perso-Aramaic
Chinese
Egyptian Sumerian,
Mayan,
Japanese
Figure 1: Sproat?s writing system classification grid (Sproat, 2000, p. 142).
can assist in the classification of writing systems. On
the one hand, these methods would serve to verify
or refute proposals such as Sproat?s (2000, p. 142)
placement of several specific writing systems within
his grid (Figure 1) and to properly place additional
writing systems, but they could also be used, at least
corroboratively, to argue for the existence of more
appropriate or additional dimensions in such grids,
through the demonstration of a pattern being con-
sistently observed or violated by observed writing
systems. The holy grail in this area would be a tool
that could classify entirely unknown writing systems
to assist in attempts at archaeological decipherment,
but more realistic applications do exist, particularly
in the realm of managing on-line document collec-
tions in heterogeneous scripts or writing systems.
No previous work exactly addresses this topic.
None of the numerous descriptive accounts that cat-
alogue the world?s writing systems, culminating in
Daniels and Bright?s (1996) outstanding reference
on the subject, count as quantitative. The one com-
putational approach that at least claims to consider
archaeological decipherment (Knight and Yamada,
1999), curiously enough, assumes an alphabetic and
purely phonographic mapping of graphemes at the
outset, and applies an EM-style algorithm to what
is probably better described as an interesting varia-
tion on learning the ?letter-to-sound? mappings that
one normally finds in text analysis for text-to-speech
synthesizers. The cryptographic work in the great
wars of the early 20th century applied statistical rea-
soning to military communications, although this
too is very different in character from deciphering
a naturally developed writing system.
2 Type of Phonography
Type of phonography, as it is expressed in Sproat?s
grid, is not a continuous dimension but a dis-
crete choice by graphemes among several differ-
ent phonographic encodings. These characterize
not only the size of the phonological ?chunks? en-
coded by a single grapheme (progressing left-to-
right in Figure 1 roughly from small to large),
but also whether vowels are explicitly encoded
(poly/consonantal vs. the rest), and, in the case of
vocalic syllabaries, whether codas as well as onsets
are encoded (core syllabic vs. syllabic). While we
cannot yet discriminate between all of these phono-
graphic aspects (arguably, they are different dimen-
sions in that a writing system may select a value
from each one independently), size itself can be reli-
ably estimated from the number of graphemes in the
underlying script, or from this number in combina-
tion with the tails of grapheme distributions in repre-
sentative documents. Figure 2, for example, graphs
the frequencies of the grapheme types witnessed
among the first 500 grapheme tokens of one docu-
ment sampled from an on-line newspaper website in
each of 8 different writing systems plus an Egyp-
tian hieroglyphic document from an on-line reposi-
tory. From left to right, we see the alphabetic and
consonantal (small chunks) scripts, followed by the
polyconsonantal Egyptian hieroglyphics, followed
by core syllabic Japanese, and then syllabic Chinese.
Korean was classified near Japanese because its Uni-
code representation atomically encodes the multi-
segment syllabic complexes that characterize most
Hangul writing. A segmental encoding would ap-
pear closer to English.
3 Amount of Logography
Amount of logography is rather more difficult.
Roughly, logography is the capacity of a writing
system to associate the symbols of a script directly
118
with the meanings of specific words rather than in-
directly through their pronunciations. No one to
our knowledge has proposed any justification for
whether logography should be viewed continuously
or discretely. Sproat (2000) believes that it is contin-
uous, but acknowledges that this belief is more im-
pressionistic than factual. In addition, it appears, ac-
cording to Sproat?s (2000) discussion that amount or
degree of logography, whatever it is, says something
about the relative frequency with which graphemic
tokens are used semantically, rather than about the
properties of individual graphemes in isolation. En-
glish, for example, has a very low degree of lo-
gography, but it does have logographic graphemes
and graphemes that can be used in a logographic
aspect. These include numerals (with or without
phonographic complements as in ?3rd,? which dis-
tinguishes ?3? as ?three? from ?3? as ?third?), dol-
lar signs, and arguably some common abbreviations
as ?etc.? By contrast, type of phonography predicts
a property that holds of every individual grapheme
? with few exceptions (such as symbols for word-
initial vowels in CV syllabaries), graphemes in the
same writing system are marching to the same drum
in their phonographic dimension.
Another reason that amount of logography is dif-
ficult to measure is that it is not entirely indepen-
dent of the type of phonography. As the size of the
phonological units encoded by graphemes increases,
at some point a threshold is crossed wherein the
unit is about the size of a word or another meaning-
bearing unit, such as a bound morpheme. When
this happens, the distinction between phonographic
and logographic uses of such graphemes becomes
a far more intensional one than in alphabetic writ-
ing systems such as English, where the boundary is
quite clear. Egyptian hieroglyphics are well known
for their use of rebus signs, for example, in which
highly pictographic graphemes are used not for the
concepts denoted by the pictures, but for concepts
with words pronounced like the word for the de-
picted concept. There are very few writing systems
indeed where the size of the phonological unit is
word-sized and yet the writing system is still mostly
phonographic;2 it could be argued that the distinc-
2Modern Yi (Figure 1) is one such example, although the
history of Modern Yi is more akin to that of a planned language
than a naturally evolved semiotic system.
tion simply does not exist (see Section 4).
0
10
20
30
40
50
60
0 50 100 150 200 250
fre
qu
en
cy
symbol
"Egyptian"
"English"
"Greek"
"Hebrew"
"Japanese"
"Korean"
"Mandarin"
"Spanish"
"Russian"
Figure 2: Grapheme distributions in 9 writing sys-
tems. The symbols are ordered by inverse frequency
to separate the heads of the distributions better. The
left-to-right order of the heads is as shown in the key.
Nevertheless, one can distinguish pervasive se-
mantical use from pervasive phonographic use. We
do not have access to electronically encoded Mod-
ern Yi text, so to demonstrate the principle, we will
use English text re-encoded so that each ?grapheme?
in the new encoding represents three consecutive
graphemes (breaking at word boundaries) in the un-
derlying natural text. We call this trigraph English,
and it has no (intensional) logography. The princi-
ple is that, if graphemes are pervasively used in their
semantical respect, then they will ?clump? seman-
tically just like words do. To measure this clump-
ing, we use sample correlation coefficients. Given
two random variables, X and Y , their correlation is
given by their covariance, normalized by their sam-
ple standard deviations:
corr(X,Y ) = cov(X,Y )s(X)?s(Y )
cov(X,Y ) = 1n?1?0?i,j?n(xi ? ?i)(yj ? ?j)
s(X) =
?
1
n?1?0?i?n(xi ? ?)2
For our purposes, each grapheme type is treated as
a variable, and each document represents an obser-
vation. Each cell of the matrix of correlation co-
efficients then tells us the strength of the correla-
tion between two grapheme types. For trigraph En-
glish, part of the correlation matrix is shown in Fig-
ure 3. Part of the correlation matrix for Mandarin
119
Figure 3: Part of the trigraph-English correlation
matrix.
Chinese, which has a very high degree of logogra-
phy, is shown in Figure 4. For both of the plots in
Figure 4: Part of the Mandarin Chinese correlation
matrix.
our example, counts for 2500 grapheme types were
obtained from 1.63 million tokens of text (for En-
glish, trigraphed Brown corpus text, for Chinese,
GB5-encoded text from an on-line newspaper).
By adding the absolute values of the correla-
tions over these matrices (normalized for number of
graphemes), we obtain a measure of the extent of
the correlation. Pervasive semantic clumping, which
would be indicative of a high degree of logography,
corresponds to a small extent of correlation ? in
other words the correlation is pinpointed at semanti-
cally related logograms, rather than smeared over se-
mantically orthogonal phonograms. In our example,
these sums were repeated for several 2500-type sam-
ples from among the approximately 35,000 types
in the trigraph English data, and the approximately
4,500 types in the Mandarin data. The average sum
for trigraph English was 302,750 whereas for Man-
darin Chinese it was 98,700. Visually, this differ-
ence is apparent in that the trigraph English matrix
is ?brighter? than the Mandarin one. From this we
should conclude that Mandarin Chinese has a higher
degree of logography than trigraph English.
4 Conclusion
We have proposed methods for independently mea-
suring the type of phonography and degree of logog-
raphy from unannotated data as a means of classify-
ing writing systems. There is more to understand-
ing how a writing system works than these two di-
mensions. Crucially, the direction in which texts
should be read, the so-called macroscopic organi-
zation of typical documents, is just as important as
determining the functional characteristics of individ-
ual graphemes.
Our experiments with quantitative methods for
classification, furthermore, have led us to a new un-
derstanding of the differences between Sproat?s clas-
sification grid and earlier linear attempts. While we
do not accept Gelb?s teleological interpretation, we
conjecture that there is a linear variation in how in-
dividual writing systems behave, even if they can be
classified according to multiple dimensions. Mod-
ern Yi stands as a single, but questionable, coun-
terexample to this observation, and for it to be vis-
ible in Sproat?s grid (with writing systems arranged
along only the diagonal), one would need an objec-
tive and verifiable means of discriminating between
consonantal and vocalic scripts. This remains a topic
for future consideration.
References
P. Daniels and W. Bright. 1996. The World?s Writing
Systems. Oxford.
J. DeFrancis. 1989. Visible Speech: The Diverse One-
ness of Writing Systems. University of Hawaii.
I. Gelb. 1963. A Study of Writing. Chicago, 2nd ed.
K. Knight and K. Yamada. 1999. A computational ap-
proach to deciphering unknown scripts. In Proc. of
ACL Workshop on Unsupervised Learning in NLP.
G. Sampson. 1985. Writing Systems. Stanford.
R. Sproat. 2000. A Computational Theory of Writing
Systems. Cambridge University Press.
120
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 197?200,
New York, June 2006. c?2006 Association for Computational Linguistics
Comparing the roles of textual, acoustic and spoken-language features 
on spontaneous-conversation summarization  
Xiaodan Zhu Gerald Penn 
Department of Computer Science, University of Toronto 
10 Kings College Rd., Toronto, Canada 
{xzhu, gpenn} @cs.toronto.edu
 
Abstract 
This paper is concerned with the 
summarization of spontaneous 
conversations. Compared with broadcast 
news, which has received intensive study, 
spontaneous conversations have been less 
addressed in the literature.  Previous 
work has focused on textual features 
extracted from transcripts. This paper 
explores and compares the effectiveness 
of both textual features and speech-
related features. The experiments show 
that these features incrementally improve 
summarization performance. We also find 
that speech disfluencies, which  have been 
removed as noise in previous work, help 
identify important utterances, while the 
structural feature is less effective than it 
is in broadcast news. 
1 Introduction 
Spontaneous conversations are a very important 
type of speech data. Distilling important 
information from them has commercial and other 
importance. Compared with broadcast news, which 
has received the most intensive studies (Hori and 
Furui, 2003; Christensen et al 2004; Maskey and 
Hirschberg, 2005), spontaneous conversations have 
been less addressed in the literature.  
Spontaneous conversations are different from 
broadcast news in several aspects: (1) spontaneous 
conversations are often less well formed 
linguistically, e.g., containing more speech 
disfluencies and false starts; (2) the distribution of 
important utterances in spontaneous conversations 
could be different from that in broadcast news, e.g., 
the beginning part of news often contains 
important information, but in conversations, 
information may be more evenly distributed; (3) 
conversations often contain discourse clues, e.g., 
question-answer pairs and speakers? information, 
which can be utilized to keep the summary 
coherent; (4) word error rates (WERs) from speech 
recognition are usually much higher in 
spontaneous conversations.  
Previous work on spontaneous-conversation 
summarization has mainly focused on textual 
features (Zechner, 2001; Gurevych and Strube, 
2004), while speech-related features have not been 
explored for this type of speech source. This paper 
explores and compares the effectiveness of both 
textual features and speech-related features.  The 
experiments show that these features incrementally 
improve summarization performance. We also 
discuss problems (1) and (2) mentioned above. For 
(1), Zechner (2001) proposes to detect and remove 
false starts and speech disfluencies from transcripts, 
in order to make the text-format summary concise 
and more readable. Nevertheless, it is not always 
necessary to remove them. One reason is that 
original utterances are often more desired to ensure 
comprehensibility and naturalness if the summaries 
are to be delivered as excerpts of audio (see section 
2), in order to avoid the impact of WER. Second, 
disfluencies are not necessarily noise; instead, they 
show regularities in a number of dimensions 
(Shriberg, 1994), and correlate with many factors 
including topic difficulty (Bortfeld et al 2001). 
Rather than removing them, we explore the effects 
of disfluencies on summarization, which, to our 
knowledge, has not yet been addressed in the 
literature. Our experiments show that they improve 
summarization performance.   
To discuss problem (2), we explore and compare 
both textual features and speech-related features, 
as they are explored in broadcast news (Maskey 
and Hirschberg, 2005).  The experiments show that 
the structural feature (e.g. utterance position) is 
less effective for summarizing spontaneous 
conversations than it is in broadcast news. MMR 
197
and lexical features are the best. Speech-related 
features follow. The structural feature is least 
effective. We do not discuss problem (3) and (4) in 
this paper. For problem (3), a similar idea has been 
proposed to summarize online blogs and 
discussions. Problem (4) has been partially 
addressed by (Zechner & Waibel, 2000); but it has 
not been studied together with acoustic features. 
2 Utterance-extraction-based 
summarization 
Still at its early stage, current research on speech 
summarization targets a less ambitious goal: 
conducting extractive, single-document, generic, 
and surface-level-feature-based summarization.  
The pieces to be extracted could correspond to 
words (Koumpis, 2002; Hori and Furui, 2003). The 
extracts could be utterances, too. Utterance 
selection is useful. First, it could be a preliminary 
stage applied before word extraction, as proposed 
by Kikuchi et al (2003) in their two-stage 
summarizer. Second, with utterance-level extracts, 
one can play the corresponding audio to users, as 
with the speech-to-speech summarizer discussed in 
Furui et al (2003). The advantage of outputting 
audio segments rather than transcripts is that it 
avoids the impact of WERs caused by automatic 
speech recognition (ASR). We will focus on 
utterance-level extraction, which at present appears 
to be the only way to ensure comprehensibility and 
naturalness if the summaries are to be delivered as 
excerpts of audio themselves.  
Previous work on spontaneous conversations 
mainly focuses on using textual features. Gurevych 
& Strube (2004) develop a shallow knowledge-
based approach. The noun portion of WordNet is 
used as a knowledge source. The noun senses were 
manually disambiguated rather than automatically. 
Zechner (2001) applies maximum marginal 
relevance (MMR) to select utterances for 
spontaneous conversation transcripts. 
3 Classification based utterance 
extraction  
Spontaneous conversations contain more 
information than textual features. To utilize these 
features, we reformulate the utterance selection 
task as a binary classification problem, an 
utterance is either labeled as ?1? (in-summary) or 
?0? (not-in-summary). Two state-of-the-art 
classifiers, support vector machine (SVM) and 
logistic regression (LR), are used. SVM seeks an 
optimal separating hyperplane, where the margin is 
maximal. In our experiments, we use the OSU-
SVM package. Logistic regression (LR) is indeed a 
softmax linear regression, which models the 
posterior probabilities of the class label with the 
softmax of linear functions of feature vectors. For 
the binary classification that we require in our 
experiments, the model format is simple.  
 
3.1 Features 
The features explored in this paper include: 
(1) MMR score: the score calculated with MMR 
(Zechner, 2001) for each utterance. 
(2) Lexicon features: number of named entities, 
and utterance length (number of words). The 
number of named entities includes: person-
name number, location-name number, 
organization-name number, and the total 
number. Named entities are annotated 
automatically with a dictionary. 
(3) Structural features: a value is assigned to 
indicate whether a given utterance is in the first, 
middle, or last one-third of the conversation. 
Another Boolean value is assigned to indicate 
whether this utterance is adjacent to a speaker 
turn or not.  
(4) Prosodic features: we use basic prosody: the 
maximum, minimum, average and range of 
energy, as well as those of fundamental 
frequency, normalized by speakers.  All these 
features are automatically extracted. 
(5) Spoken-language features: the spoken-language 
features include number of repetitions, filled 
pauses, and the total number of them. 
Disfluencies adjacent to a speaker turn are not 
counted, because they are normally used to 
coordinate interaction among speakers. 
Repetitions and pauses are detected in the same 
way as described in Zechner (2001). 
4 Experimental results 
4.1 Experiment settings 
The data used for our experiments come from 
SWITCHBOARD. We randomly select 27 
conversations, containing around 3660 utterances. 
The important utterances of each conversation are 
198
manually annotated. We use f-score and the 
ROUGE score as evaluation metrics. Ten-fold 
cross validation is applied to obtain the results 
presented in this section. 
4.2 Summarization performance 
4.2.1 F-score 
Table-1 shows the f-score of logistic regression 
(LR) based summarizers, under different 
compression ratios, and with incremental features 
used. 
 10% 15% 20% 25% 30%
(1)  MMR .246 .309 .346 .355 .368
(2) (1) +lexicon .293 .338 .373 .380 .394
(3) (2)+structure .334 .366 .400 .409 .404
(4) (3)+acoustic .336 .364 .388 .410 .415
(5) (4)+spoken language .333 .376 .410 .431 .422 
Table 1. f-score of LR summarizers using incremental features 
Below is the f-score of SVM-based summarizer: 
 10% 15% 20% 25% 30%
(1) MMR .246 .309 .346 .355 .368
(2) (1) +lexicon .281 .338 .354 .358 .377
(3) (2)+structural .326 .371 .401 .409 .408
(4) (3)+acoustic .337 .380 .400 .422 .418
(5) (4)+spoken language .353 .380 .416 .424 .423 
Table 2. f-score of SVM summarizers using incremental features 
Both tables show that the performance of 
summarizers improved, in general, with more 
features used. The use of lexicon and structural 
features outperforms MMR, and the speech-related 
features, acoustic features and spoken language 
features produce additional improvements.  
4.2.2 ROUGE 
The following tables provide the ROUGE-1 scores: 
 10% 15% 20% 25% 30%
(1) MMR .585 .563 .523 .492 .467
(2) (1) +lexicon .602 .579 .543 .506 .476
(3) (2)+structure .621 .591 .553 .516 .482
(4) (3)+acoustic .619 .594 .554 .519 .485
(5) (4)+spoken language .619 .600 .566 .530 .492 
Table 3. ROUGE-1 of LR summarizers using incremental features 
 
 10% 15% 20% 25% 30%
(1) MMR .585 .563 .523 .492 .467
(2) (1) +lexicon .604 .581 .542 .504 .577
(3) (2)+structure .617 .600 .563 .523 .490
(4) (3)+acoustic .629 .610 .573 .533 .496
(5)(4)+spoken language .628 .611 .576 .535 .502 
Table 4. ROUGE-1 of SVM summarizers using incremental features 
The ROUGE-1 scores show similar tendencies to 
the f-scores: the rich features improve 
summarization performance over the baseline 
MMR summarizers. Other ROUGE scores like 
ROUGE-L show the same tendency, but are not 
presented here due to the space limit.  
Both the f-score and ROUGE indicate that, in 
general, rich features incrementally improve 
summarization performance.  
4.3 Comparison of features 
To study the effectiveness of individual features, 
the receiver operating characteristic (ROC) curves 
of these features are presented in Figure-1 below. 
The larger the area under a curve is, the better the 
performance of this feature is. To be more exact, 
the definition for the y-coordinate (sensitivity) and 
the x-coordinate (1-specificity) is: 
ratenegtivetrue
FPTN
TN
yspecificit
ratepositivetrue
FNTP
TP
ysensitivit
=+=
=+=  
where TP, FN, TN and FP are true positive, false 
negative, true negative, and false positive, 
respectively. 
 
Figure-1. ROC curves for individual features 
Lexicon and MMR features are the best two 
individual features, followed by spoken-language 
and acoustic features. The structural feature is least 
effective.   
Let us first revisit the problem (2) discussed 
above in the introduction. The effectiveness of the 
structural feature is less significant than it is in 
broadcast news. According to the ROC curves 
presented in Christensen et al (2004), the 
structural feature (utterance position) is one of the 
best features for summarizing read news stories, 
and is less effective when news stories contain 
spontaneous speech. Both their ROC curves cover 
larger area than the structural feature here in figure 
1, that is, the structure feature is less effective for 
summarizing spontaneous conversation than it is in 
broadcast news. This reflects, to some extent, that 
199
information is more evenly distributed in 
spontaneous conversations.  
Now let us turn to the role of speech disfluencies, 
which are very common in spontaneous 
conversations. Previous work detects and removes 
disfluencies as noise. Indeed, disfluencies show 
regularities in a number of dimensions (Shriberg, 
1994). They correlate with many factors including 
the topic difficulty (Bortfeld et al 2001). Tables 1-
4 above show that they improve summarization 
performance when added upon other features. 
Figure-1 shows that when used individually, they 
are better than the structural feature, and also better 
than acoustic features at the left 1/3 part of the 
figure, where the summary contains relatively 
fewer utterances. Disfluencies, e.g., pauses, are 
often inserted when speakers have word-searching 
problem, e.g., a problem finding topic-specific 
keywords:  
Speaker A: with all the uh sulfur and all that other 
stuff they're dumping out into the atmosphere. 
The above example is taken from a conversation 
that discusses pollution. The speaker inserts a filled 
pause uh in front of the word sulfur. Pauses are not 
randomly inserted. To show this, we remove them 
from transcripts. Section-2 of SWITCHBOARD 
(about 870 dialogues and 189,000 utterances) is 
used for this experiment. Then we insert these 
pauses back randomly, or insert them back at their 
original places, and compare the difference. For 
both cases, we consider a window with 4 words 
after each filled pause. We average the tf.idf scores 
of the words in each of these windows. Then, for 
all speaker-inserted pauses, we obtain a set of 
averaged tf.idf scores. And for all randomly-
inserted pauses, we have another set. The mean of 
the former set (5.79 in table 5) is statistically 
higher than that of the latter set (5.70 in table 5). 
We can adjust the window size to 3, 2 and 1, and 
then get the following table. 
Window size 1 2 3 4 
Insert Randomly 5.69 5.69 5.70 5.70Mean of 
tf.idf score Insert by speaker  5.72 5.82 5.81 5.79
Difference is significant? (t-test, p<0.05) Yes Yes Yes Yes 
Table 5.  Average tf.idf scores of words following filled pauses. 
The above table shows that instead of randomly 
inserting pauses, real speakers insert them in front 
of words with higher tf.idf scores. This helps 
explain why disfluencies work. 
5 Conclusions 
Previous work on summarizing spontaneous 
conversations has mainly focused on textual 
features. This paper explores and compares both 
textual and speech-related features. The 
experiments show that these features incrementally 
improve summarization performance. We also find 
that speech disfluencies, which are removed as 
noise in previous work, help identify important 
utterances, while the structural feature is less 
effective than it is in broadcast news.  
6 References 
Bortfeld, H., Leon, S.D., Bloom, J.E., Schober, M.F., & 
Brennan, S.E. 2001. Disfluency Rates in Conversation: 
Effects of Age, Relationship, Topic Role, and Gender. 
Language and Speech, 44(2): 123-147  
Christensen, H., Kolluru, B., Gotoh, Y., Renals, S., 2004. 
From text summarisation to style-specific 
summarisation for broadcast news. Proc. ECIR-2004. 
Furui, S., Kikuichi T. Shinnaka Y., and Hori C. 2003. 
Speech-to-speech and speech to text summarization,. 
First International workshop on Language 
Understanding and Agents for Real World Interaction, 
2003. 
Gurevych I. and Strube M. 2004. Semantic Similarity 
Applied to Spoken Dialogue Summarization. COLING-
2004. 
Hori C. and Furui S., 2003. A New Approach to Automatic 
Speech Summarization IEEE Transactions on 
Multimedia, Vol. 5, NO. 3, September 2003,  
Kikuchi T., Furui S. and Hori C., 2003. Automatic Speech 
Summarization Based on Sentence Extraction and 
Compaction, Proc. ICASSP-2003. 
Koumpis K., 2002. Automatic Voicemail Summarisation 
for Mobile Messaging Ph.D. Thesis, University of 
Sheffield, UK, 2002. 
Maskey, S.R., Hirschberg, J. "Comparing Lexial, 
Acoustic/Prosodic, Discourse and Structural Features 
for Speech Summarization", Eurospeech 2005. 
Shriberg, E.E. (1994). Preliminaries to a Theory of Speech 
Disfluencies. Ph.D. thesis, University of California at 
Berkeley. 
Zechner K. and Waibel A., 2000. Minimizing word error 
rate in textual summaries of spoken language. NAACL-
2000. 
Zechner K., 2001. Automatic Summarization of Spoken 
Dialogues in Unrestricted Domains. Ph.D. thesis, 
Carnegie Mellon University, November 2001. 
200
Tractability and Structural Closures in Attribute Logic Type Signatures
Gerald Penn
Department of Computer Science
University of Toronto
10 King?s College Rd.
Toronto M5S 3G4, Canada
gpenn@cs.toronto.edu
Abstract
This paper considers three assumptions
conventionally made about signatures
in typed feature logic that are in po-
tential disagreement with current prac-
tice among grammar developers and
linguists working within feature-based
frameworks such as HPSG: meet-semi-
latticehood, unique feature introduc-
tion, and the absence of subtype cover-
ing. It also discusses the conditions un-
der which each of these can be tractably
restored in realistic grammar signatures
where they do not already exist.
1 Introduction
The logic of typed feature structures (LTFS, Car-
penter 1992) and, in particular, its implementa-
tion in the Attribute Logic Engine (ALE, Car-
penter and Penn 1996), have been widely used
as a means of formalising and developing gram-
mars of natural languages that support computa-
tionally efficient parsing and SLD resolution, no-
tably grammars within the framework of Head-
driven Phrase Structure Grammar (HPSG, Pollard
and Sag 1994). These grammars are formulated
using a vocabulary provided by a finite partially
ordered set of types and a set of features that must
be specified for each grammar, and feature struc-
tures in these grammars must respect certain con-
straints that are also specified. These include ap-
propriateness conditions, which specify, for each
type, all and only the features that take values
in feature structures of that type, and with which
types of values (value restrictions). There are also
more general implicational constraints of the form
  
, where   is a type, and  is an expres-
sion from LTFS?s description language. In LTFS
and ALE, these four components, a partial order
of types, a set of features, appropriateness declara-
tions and type-antecedent constraints can be taken
as the signature of a grammar, relative to which
descriptions can be interpreted.
LTFS and ALE also make several assump-
tions about the structure and interpretation of this
partial order of types and about appropriateness,
some for the sake of generality, others for the
sake of efficiency or simplicity. Appropriate-
ness is generally accepted as a good thing, from
the standpoints of both efficiency and representa-
tional accuracy, and while many have advocated
the need for implicational constraints that are even
more general, type-antecedent constraints at the
very least are also accepted as being necessary and
convenient. Not all of the other assumptions are
universally observed by formal linguists or gram-
mar developers, however.
This paper addresses the three most contentious
assumptions that LTFS and ALE make, and how
to deal with their absence in a tractable manner.
They are:
1. Meet-semi-latticehood: every partial order
of types must be a meet semi-lattice. This
implies that every consistent pair of types has
a least upper bound.
2. Unique feature introduction: for every fea-
ture, F, there is a unique most general type to
which F is appropriate.
3. No subtype covering: there can be feature
structures of a non-maximally-specific type
that are not typable as any of its maximally
specific subtypes. When subtype covering
is not assumed, feature structures themselves
can be partially ordered and taken to repre-
sent partial information states about some set
of objects. When subtype covering is as-
sumed, feature structures are discretely or-
dered and totally informative, and can be
taken to represent objects in the (linguistic)
world themselves. The latter interpretation is
subscribed to by Pollard and Sag (1994), for
example.
All three of these conditions have been claimed
elsewhere to be either intractable or impossible
to restore in grammar signatures where they do
not already exist. It will be argued here that: (1)
restoring meet-semi-latticehood is theoretically
intractable, for which the worst case bears a dis-
quieting resemblance to actual practice in current
large-scale grammar signatures, but nevertheless
can be efficiently compilable in practice due to the
sparseness of consistent types; (2) unique feature
introduction can always be restored to a signature
in low-degree polynomial time, and (3) while type
inferencing when subtype covering is assumed is
intractable in the worst case, a very elegant con-
straint logic programming solution combined with
a special compilation method exists that can re-
store tractability in many practical contexts. Some
simple completion algorithms and a corrected NP-
completeness proof for non-disjunctive type infer-
encing with subtype covering are also provided.
2 Meet-semi-latticehood
In LTFS and ALE, partial orders of types are as-
sumed to be meet semi-lattices:
Definition 1 A partial order, 
	 , is a meet
semi-lattice iff for any  ,  .
 is the binary greatest lower bound, or meet op-
eration, and is the dual of the join operation,  ,
which corresponds to unification, or least upper
bounds (in the orientation where Generalized Encoding of Description Spaces and its Application to Typed
Feature Structures
Gerald Penn
Department of Computer Science
University of Toronto
10 King's College Rd.
Toronto M5S 3G4, Canada
Abstract
This paper presents a new formalization of
a unification- or join-preserving encoding
of partially ordered sets that more essen-
tially captures what it means for an en-
coding to preserve joins, generalizing the
standard definition in AI research. It then
shows that every statically typable ontol-
ogy in the logic of typed feature struc-
tures can be encoded in a data structure
of fixed size without the need for resizing
or additional union-find operations. This
is important for any grammar implemen-
tation or development system based on
typed feature structures, as it significantly
reduces the overhead of memory manage-
ment and reference-pointer-chasing dur-
ing unification.
1 Motivation
The logic of typed feature structures (Carpenter,
1992) has been widely used as a means of formaliz-
ing and developing natural language grammars that
support computationally efficient parsing, genera-
tion and SLD resolution, notably grammars within
the Head-driven Phrase Structure Grammar (HPSG)
framework, as evidenced by the recent successful
development of the LinGO reference grammar for
English (LinGO, 1999). These grammars are for-
mulated over a finite vocabulary of features and par-
tially ordered types, in respect of constraints called
appropriateness conditions. Appropriateness speci-
fies, for each type, all and only the features that take
values in feature structures of that type, along with
adj noun
CASE:case
nom acc plus minus subst
case bool head
MOD:bool
PRD:bool
 
Figure 1: A sample type system with appropriate-
ness conditions.
the types of values (value restrictions) those feature
values must have. In Figure 1,1 for example, all
head-typed TFSs must have bool-typed values for
the features MOD and PRD, and no values for any
other feature.
Relative to data structures like arrays or logical
terms, typed feature structures (TFSs) can be re-
garded as an expressive refinement in two different
ways. First, they are typed, and the type system al-
lows for subtyping chains of unbounded depth. Fig-
ure 1 has a chain of length  from   to noun. Point-
ers to arrays and logical terms can only monoton-
ically ?refine? their (syntactic) type from unbound
(for logical terms, variables) to bound. Second, al-
though all the TFSs of a given type have the same
features because of appropriateness, a TFS may ac-
quire more features when it promotes to a subtype. If
a head-typed TFS promotes to noun in the type sys-
tem above, for example, it acquires one extra case-
valued feature, CASE. When a subtype has two or
1In this paper, Carpenter's (1992) convention of using  as
the most general type, and depicting subtypes above their su-
pertypes is used.
                  Computational Linguistics (ACL), Philadelphia, July 2002, pp. 64-71.
                         Proceedings of the 40th Annual Meeting of the Association for
more incomparable supertypes, a TFS can also mul-
tiply inherit features from other supertypes when it
promotes.
The overwhelmingly most prevalent operation
when working with TFS-based grammars is unifica-
tion, which corresponds mathematically to finding
a least upper bound or join. The most common in-
stance of unification is the special case in which a
TFS is unified with the most general TFS that satis-
fies a description stated in the grammar. This special
case can be decomposed at compile-time into more
atomic operations that (1) promote a type to a sub-
type, (2) bind a variable, or (3) traverse a feature
path, according to the structure of the description.
TFSs actually possess most of the properties of
fixed-arity terms when it comes to unification, due
to appropriateness. Nevertheless, unbounded sub-
typing chains and acquiring new features conspire
to force most internal representations of TFSs to per-
form extra work when promoting a type to a subtype
to earn the expressive power they confer. Upon be-
ing repeatedly promoted to new subtypes, they must
be repeatedly resized or repeatedly referenced with
a pointer to newly allocated representations, both of
which compromise locality of reference in memory
and/or involve pointer-chasing. These costs are sig-
nificant.
Because appropriateness involves value restric-
tions, simply padding a representation with some ex-
tra space for future features at the outset must guar-
antee a proper means of filling that extra space with
the right value when it is used. Internal representa-
tions that lazily fill in structure must also be wary
of the common practice in description languages of
binding a variable to a feature value with a scope
larger than a single TFS ? for example, in sharing
structure between a daughter category and a mother
category in a phrase structure rule. In this case, the
representation of a feature's value must also be in-
terpretable independent of its context, because two
separate TFSs may refer to that variable.
These problems are artifacts of not using a rep-
resentation which possesses what in knowledge rep-
resentation is known as a join-preserving encoding
of a grammar's TFSs ? in other words, a repre-
sentation with an operation that naturally behaves
like TFS-unification. The next section presents the
standard definition of join-preserving encodings and
provides a generalization that more essentially cap-
tures what it means for an encoding to preserve
joins. Section 3 formalizes some of the defining
characteristics of TFSs as they are used in com-
putational linguistics. Section 4 shows that these
characteristics quite fortuitously agree with what
is required to guarantee the existence of a join-
preserving encoding of TFSs that needs no resizing
or extra referencing during type promotion. Sec-
tion 5 then shows that a generalized encoding exists
in which variable-binding scope can be larger than a
single TFS ? a property no classical encoding has.
Earlier work on graph unification has focussed on
labelled graphs with no appropriateness, so the cen-
tral concern was simply to minimize structure copy-
ing. While this is clearly germane to TFSs, appro-
priateness creates a tradeoff among copying, the po-
tential for more compact representations, and other
memory management issues such as locality of ref-
erence that can only be optimized empirically and
relative to a given grammar and corpus (a recent ex-
ample of which can be found in Callmeier (2001)).
While the present work is a more theoretical consid-
eration of how unification in one domain can sim-
ulate unification in another, the data structure de-
scribed here is very much motivated by the encod-
ing of TFSs as Prolog terms allocated on a contigu-
ous WAM-style heap. In that context, the emphasis
on fixed arity is really an attempt to avoid copying,
and lazily filling in structure is an attempt to make
encodings compact, but only to the extent that join
preservation is not disturbed. While this compro-
mise solution must eventually be tested on larger and
more diverse grammars, it has been shown to reduce
the total parsing time of a large corpus on the ALE
HPSG benchmark grammar of English (Penn, 1993)
by a factor of about 4 (Penn, 1999).
2 Join-Preserving Encodings
We may begin with a familiar definition from dis-
crete mathematics:
Definition 1 Given two partial orders  
	 and
 	 , a function    is an order-
embedding iff, for every A Tabulation-Based Parsing Method that Reduces Copying
Gerald Penn and Cosmin Munteanu
Department of Computer Science
University of Toronto
Toronto M5S 3G4, Canada
 
gpenn,mcosmin  @cs.toronto.edu
Abstract
This paper presents a new bottom-up chart
parsing algorithm for Prolog along with
a compilation procedure that reduces the
amount of copying at run-time to a con-
stant number (2) per edge. It has ap-
plications to unification-based grammars
with very large partially ordered cate-
gories, in which copying is expensive,
and can facilitate the use of more so-
phisticated indexing strategies for retriev-
ing such categories that may otherwise be
overwhelmed by the cost of such copy-
ing. It also provides a new perspective
on ?quick-checking? and related heuris-
tics, which seems to confirm that forcing
an early failure (as opposed to seeking
an early guarantee of success) is in fact
the best approach to use. A preliminary
empirical evaluation of its performance is
also provided.
1 Introduction
This paper addresses the cost of copying edges
in memoization-based, all-paths parsers for phrase-
structure grammars. While there have been great ad-
vances in probabilistic parsing methods in the last
five years, which find one or a few most probable
parses for a string relative to some grammar, all-
paths parsing is still widely used in grammar devel-
opment, and as a means of verifying the accuracy of
syntactically more precise grammars, given a corpus
or test suite.
Most if not all efficient all-paths phrase-structure-
based parsers for natural language are chart-based
because of the inherent ambiguity that exists in
large-scale natural language grammars. Within
WAM-based Prolog, memoization can be a fairly
costly operation because, in addition to the cost of
copying an edge into the memoization table, there
is the additional cost of copying an edge out of the
table onto the heap in order to be used as a premise
in further deductions (phrase structure rule applica-
tions). All textbook bottom-up Prolog parsers copy
edges out: once for every attempt to match an edge
to a daughter category, based on a matching end-
point node, which is usually the first-argument on
which the memoization predicate is indexed. De-
pending on the grammar and the empirical distri-
bution of matching mother/lexical and daughter de-
scriptions, this number could approach  copies
for an edge added early to the chart, where  is the
length of the input to be parsed.
For classical context-free grammars, the category
information that must be copied is normally quite
small in size. For feature-structure-based grammars
and other highly lexicalized grammars with large
categories, however, which have become consider-
ably more popular since the advent of the standard
parsing algorithms, it becomes quite significant. The
ALE system (Carpenter and Penn, 1996) attempts
to reduce this by using an algorithm due to Carpen-
ter that traverses the string breadth-first, right-to-left,
but matches rule daughters rule depth-first, left-to-
right in a failure-driven loop, which eliminates the
need for active edges and keeps the sizes of the heap
and call stack small. It still copies a candidate edge
every time it tries to match it to a daughter descrip-
tion, however, which can approach      because
of its lack of active edges. The OVIS system (van
Noord, 1997) employs selective memoization, which
tabulates only maximal projections in a head-corner
parser ? partial projections of a head are still re-
computed.
A chart parser with zero copying overhead has
yet to be discovered, of course. This paper presents
one that reduces this worst case to two copies per
non-empty edge, regardless of the length of the in-
put string or when the edge was added to the chart.
Since textbook chart parsers require at least two
copies per edge as well (assertion and potentially
matching the next lexical edge to the left/right), this
algorithm always achieves the best-case number of
copies attainable by them on non-empty edges. It is
thus of some theoretical interest in that it proves that
at least a constant bound is attainable within a Prolog
setting. It does so by invoking a new kind of gram-
mar transformation, called EFD-closure, which en-
sures that a grammar need not match an empty cat-
egory to the leftmost daughter of any rule. This
transformation is similar to many of the myriad of
earlier transformations proposed for exploring the
decidability of recognition under various parsing
control strategies, but the property it establishes is
more conservative than brute-force epsilon elimi-
nation for unification-based grammars (Dymetman,
1994). It also still treats empty categories distinctly
from non-empty ones, unlike the linking tables pro-
posed for treating leftmost daughters in left-corner
parsing (Pereira and Shieber, 1987). Its motivation,
the practical consideration of copying overhead, is
also rather different, of course.
The algorithm will be presented as an improved
version of ALE?s parser, although other standard
bottom-up parsers can be similarly adapted.
2 Why Prolog?
Apology! This paper is not an attempt to show that
a Prolog-based parser could be as fast as a phrase-
structure parser implemented in an imperative pro-
gramming language such as C. Indeed, if the cat-
egories of a grammar are discretely ordered, chart
edges can be used for further parsing in situ, i.e.,
with no copying out of the table, in an impera-
tive programming language. Nevertheless, when the
categories are partially ordered, as in unification-
based grammars, there are certain breadth-first pars-
ing control strategies that require even imperatively
implemented parsers to copy edges out of their ta-
bles.
What is more important is the tradeoff at stake
between efficiency and expressiveness. By improv-
ing the performance of Prolog-based parsing, the
computational cost of its extra available expres-
sive devices is effectively reduced. The alterna-
tive, simple phrase-structure parsing, or extended
phrase-structure-based parsing with categories such
as typed feature structures, is extremely cumber-
some for large-scale grammar design. Even in
the handful of instances in which it does seem to
have been successful, which includes the recent
HPSG English Resource Grammar and a handful of
Lexical-Functional Grammars, the results are by no
means graceful, not at all modular, and arguably not
reusable by anyone except their designers.
The particular interest in Prolog?s expressiveness
arises, of course, from the interest in generalized
context-free parsing beginning with definite clause
grammars (Pereira and Shieber, 1987), as an in-
stance of a logic programming control strategy. The
connection between logic programming and parsing
is well-known and has also been a very fruitful one
for parsing, particularly with respect to the appli-
cation of logic programming transformations (Sta-
bler, 1993) and constraint logic programming tech-
niques to more recent constraint-based grammati-
cal theories. Relational predicates also make gram-
mars more modular and readable than pure phrase-
structure-based grammars.
Commercial Prolog implementations are quite
difficult to beat with imperative implementations
when it is general logic programming that is re-
quired. This is no less true with respect to more re-
cent data structures in lexicalized grammatical theo-
ries. A recent comparison (Penn, 2000) of a version
between ALE (which is written in Prolog) that re-
duces typed feature structures to Prolog term encod-
ings, and LiLFeS (Makino et al, 1998), the fastest
imperative re-implementation of an ALE-like lan-
guage, showed that ALE was slightly over 10 times
faster on large-scale parses with its HPSG reference
grammar than LiLFeS was with a slightly more effi-
cient version of that grammar.
3 Empirical Efficiency
Whether this algorithm will outperform standard
Prolog parsers is also largely empirical, because:
1. one of the two copies is kept on the heap itself
and not released until the end of the parse. For
large parses over large data structures, that can
increase the size of the heap significantly, and
will result in a greater number of cache misses
and page swaps.
2. the new algorithm also requires an off-line par-
tial evaluation of the grammar rules that in-
creases the number of rules that must be it-
erated through at run-time during depth-first
closure. This can result in redundant opera-
tions being performed among rules and their
partially evaluated instances to match daughter
categories, unless those rules and their partial
evaluations are folded together with local dis-
junctions to share as much compiled code as
possible.
A preliminary empirical evaluation is presented in
Section 8.
Oepen and Carroll (2000), by far the most com-
prehensive attempt to profile and optimize the per-
formance of feature-structure-based grammars, also
found copying to be a significant issue in their im-
perative implementations of several HPSG parsers
? to the extent that it even warranted recomput-
ing unifications in places, and modifying the man-
ner in which active edges are used in their fastest
attempt (called hyper-active parsing). The results of
the present study can only cautiously be compared to
theirs so far, because of our lack of access to the suc-
cessive stages of their implementations and the lack
of a common grammar ported to all of the systems
involved. Some parallels can be drawn, however,
particularly with respect to the utility of indexing
and the maintenance of active edges, which suggest
that the algorithm presented below makes Prolog be-
have in a more ?C-like? manner on parsing tasks.
4 Theoretical Benefits
The principal benefits of this algorithm are that:
1. it reduces copying, as mentioned above.
2. it does not suffer from a problem that text-
book algorithms suffer from when running un-
der non-ISO-compatible Prologs (which is to
say most of them). On such Prologs, asserted
empty category edges that can match leftmost
daughter descriptions of rules are not able to
combine with the outputs of those rules.
3. keeping a copy of the chart on the heap allows
for more sophisticated indexing strategies to
apply to memoized categories that would oth-
erwise be overwhelmed by the cost of copying
an edge before matching it against an index.
Indexing is also briefly considered in Section 8. In-
dexing is not the same thing as filtering (Torisawa
and Tsuji, 1995), which extracts an approximation
grammar to parse with first, in order to increase the
likelihood of early unification failure. If the filter
parse succeeds, the system then proceeds to perform
the entire unification operation, as if the approxima-
tion had never been applied. Malouf et al (2000)
cite an improvement of 35?45% using a ?quick-
check? algorithm that they appear to believe finds
the optimal selection of  feature paths for quick-
checking. It is in fact only a greedy approxima-
tion ? the optimization problem is exponential in
the number of feature paths used for the check.
Penn (1999) cites an improvement of 15-40% sim-
ply by re-ordering the sister features of only two
types in the signature of the ALE HPSG grammar
during normal unification.
True indexing re-orders required operations with-
out repeating them. Penn and Popescu (1997) build
an automaton-based index for surface realization
with large lexica, and suggest an extension to statis-
tically trained decision trees. Ninomiya et al (2002)
take a more computationally brute-force approach to
index very large databases of feature structures for
some kind of information retrieval application. Nei-
ther of these is suitable for indexing chart edges dur-
ing parsing, because the edges are discarded after
every sentence, before the expense of building the
index can be satisfactorily amortized. There is a fair
amount of relevant work in the database and pro-
gramming language communities, but many of the
results are negative (Graf, 1996) ? very little time
can be spent on constructing the index.
A moment?s thought reveals that the very notion
of an active edge, tabulating the well-formed pre-
fixes of rule right-hand-sides, presumes that copy-
ing is not a significant enough issue to merit the
overhead of more specialized indexing. While the
present paper proceeds from Carpenter?s algorithm,
in which no active edges are used, it will become
clear from our evaluation that active edges or their
equivalent within a more sophisticated indexing
strategy are an issue that should be re-investigated
now that the cost of copying can provably be re-
duced in Prolog.
5 The Algorithm
In this section, it will be assumed that the phrase-
structure grammar to be parsed with obeys the fol-
lowing property:
Definition 1 An (extended) context-free grammar,
 
, is empty-first-daughter-closed (EFD-closed) iff,
for every production rule,  
			 in   ,
  and there are no empty productions (empty
categories) derivable from non-terminal   .
The next section will show how to transform any
phrase-structure grammar into an EFD-closed gram-
mar.
This algorithm, like Carpenter?s algorithm, pro-
ceeds breadth-first, right-to-left through the string,
at each step applying the grammar rules depth-
first, matching daughter categories left-to-right.
The first step is then to reverse the input
string, and compute its length (performed by
reverse count/5) and initialize the chart:
rec(Ws,FS) :-
retractall(edge(_,_,_)),
reverse_count(Ws,[],WsRev,0,Length),
CLength is Length - 1,
functor(Chart,chart,CLength),
build(WsRev,Length,Chart),
edge(0,Length,FS).
Two copies of the chart are used in this
presentation. One is represented by a term
chart(E1,...,EL), where the  th argument
holds the list of edges whose left node is  . Edges at
the beginning of the chart (left node 0) do not need
to be stored in this copy, nor do edges beginning at
the end of the chart (specifically, empty categories
with left node and right node Length). This will
be called the term copy of the chart. The other copy
is kept in a dynamic predicate, edge/3, as a text-
book Prolog chart parser would. This will be called
the asserted copy of the chart.
Neither copy of the chart stores empty categories.
These are assumed to be available in a separate pred-
icate, empty cat/1. Since the grammar is EFD-
closed, no grammar rule can produce a new empty
category. Lexical items are assumed to be available
in the predicate lex/2.
The predicate, build/3, actually builds the
chart:
build([W|Ws],R,Chart):-
RMinus1 is R - 1,
(lex(W,FS),
add_edge(RMinus1,R,FS,Chart)
; ( RMinus1 =:= 0 -> true
; rebuild_edges(RMinus1,Edges),
arg(RMinus1,Chart,Edges),
build(Ws,RMinus1,Chart)
)
).
build([],_,_).
The precondition upon each call to
build(Ws,R,Chart) is that Chart con-
tains the complete term copy of the non-loop edges
of the parsing chart from node R to the end, while
Ws contains the (reversed) input string from node
R to the beginning. Each pass through the first
clause of build/3 then decrements Right, and
seeds the chart with every category for the lexical
item that spans from R-1 to R. The predicate,
add edge/4 actually adds the lexical edge to the
asserted copy of the chart, and then closes the chart
depth-first under rule applications in a failure-driven
loop. When it has finished, if Ws is not empty
(RMinus1 is not 0), then build/3 retracts all of
the new edges from the asserted copy of the chart
(with rebuild edges/2, described below) and
adds them to the R-1st argument of the term copy
before continuing to the next word.
add edge/4matches non-leftmost daughter de-
scriptions from either the term copy of the chart,
thus eliminating the need for additional copying of
non-empty edges, or from empty cat/1. When-
ever it adds an edge, however, it adds it to the as-
serted copy of the chart. This is necessary because
add edge/4 works in a failure-driven loop, and
any edges added to the term copy of the chart would
be removed during backtracking:
add_edge(Left,Right,FS,Chart):-
assert(edge(Left,Right,FS)),
rule(FS,Left,Right,Chart).
rule(FS,L,R,Chart) :-
(Mother ===> [FS|DtrsRest]), % PS rule
match_rest(DtrsRest,R,Chart,Mother,L).
match_rest([],R,Chart,Mother,L) :-
% all Dtrs matched
add_edge(L,R,Mother,Chart).
match_rest([Dtr|Dtrs],R,Chart,Mother,L) :-
arg(R,Chart,Edges),
member(edge(Dtr,NewR),Edges),
match_rest(Dtrs,NewR,Chart,Mother,L)
; empty_cat(Dtr),
match_rest(Dtrs,R,Chart,Mother,L).
Note that we never need to be concerned with up-
dating the term copy of the chart during the opera-
tion of add edge/4 because EFD-closure guaran-
tees that all non-leftmost daughters must have left
nodes strictly greater than the Left passed as the
first argument to add edge/4.
Moving new edges from the asserted copy to
the term copy is straightforwardly achieved by re-
build edges/2:
rebuild_edges(Left,Edges) :-
retract(edge(Left,R,FS))
-> Edges = [edge(FS,R)|EdgesRest],
rebuild_edges(Left,EdgesRest)
; Edges = [].
The two copies required by this algorithm are
thus: 1) copying a new edge to the asserted copy
of the chart by add edge/4, and 2) copying new
edges from the asserted copy of the chart to the term
copy of the chart by rebuild edges/2. The as-
serted copy is only being used to protect the term
copy from being unwound by backtracking.
Asymptotically, this parsing algorithm has the
same cubic complexity as standard chart parsers ?
only its memory consumption and copying behavior
are different.
6 EFD-closure
To convert an (extended) context-free grammar to
one in which EFD-closure holds, we must partially
evaluate those rules for which empty categories
could be the first daughter over the available empty
categories. If all daughters can be empty categories
in some rule, then that rule may create new empty
categories, over which rules must be partially evalu-
ated again, and so on. The closure algorithm is pre-
sented in Figure 1 in pseudo-code and assumes the
existence of six auxiliary lists:
  Es? a list of empty categories over which par-
tial evaluation is to occur,
  Rs ? a list of rules to be used in partial evalu-
ation,
  NEs ? new empty categories, created by
partial evaluation (when all daughters have
matched empty categories),
  NRs? new rules, created by partial evaluation
(consisting of a rule to the leftmost daughter of
which an empty category has applied, with only
its non-leftmost daughters remaining),
  EAs ? an accumulator of empty categories al-
ready partially evaluated once on Rs, and
  RAs? an accumulator of rules already used in
partial evaluation once on Es.
Initialize Es to empty cats of grammar;
initialize Rs to rules of input grammar;
initialize the other four lists to [];
loop:
while Es =/= [] do
for each E in Es do
for each R in Rs do
unify E with the leftmost unmatched
category description of R;
if it does not match, continue;
if the leftmost category was rightmost
(unary rule),
then add the new empty category to NEs
otherwise, add the new rule (with leftmost
category marked as matched) to NRs;
od
od;
EAs := append(Es,EAs); Rs := append(Rs,RAs);
RAs := []; Es := NEs; NEs := [];
od;
if NRs = [],
then end: EAs are the closed empty cats,
Rs are the closed rules
else
Es := EAs; EAs := []; RAs := Rs;
Rs := NRs; NRs := []
go to loop
Figure 1: The off-line EFD-closure algorithm.
Each pass through the while-loop attempts to
match the empty categories in Es against the left-
most daughter description of every rule in Rs. If
new empty categories are created in the process
(because some rule in Rs is unary and its daugh-
ter matches), they are also attempted ? EAs holds
the others until they are done. Every time a rule?s
leftmost daughter matches an empty category, this
effectively creates a new rule consisting only of
the non-leftmost daughters of the old rule. In a
unification-based setting, these non-leftmost daugh-
ters could also have some of their variables instan-
tiated to information from the matching empty cate-
gory.
If the while-loop terminates (see the next section),
then the rules of Rs are stored in an accumulator,
RAs, until the new rules, NRs, have had a chance
to match their leftmost daughters against all of the
empty categories that Rs has. Partial evaluation with
NRs may create new empty categories that Rs have
never seen and therefore must be applied to. This is
taken care of within the while-loop when RAs are
added back to Rs for second and subsequent passes
through the loop.
7 Termination Properties
The parsing algorithm itself always terminates be-
cause the leftmost daughter always consumes input.
Off-line EFD-closure may not terminate when in-
finitely many new empty categories can be produced
by the production rules.
We say that an extended context-free grammar, by
which classical CFGs as well as unification-based
phrase-structure grammars are implied, is   -offline-
parseable (   -OP) iff the empty string is not infinitely
ambiguous in the grammar. Every   -OP grammar
can be converted to a weakly equivalent grammar
which has the EFD-closure property. The proof of
this statement, which establishes the correctness of
the algorithm, is omitted for brevity.
EFD-closure bears some resemblance in its inten-
tions to Greibach Normal Form, but: (1) it is far
more conservative in the number of extra rules it
must create; (2) it is linked directly to the deriv-
able empty categories of the grammar, whereas GNF
conversion proceeds from an already   -eliminated
grammar (EFD-closure of any   -free grammar, in
fact, is the grammar itself); (3) GNF is rather more
difficult to define in the case of unification-based
grammars than with classical CFGs, and in the one
generalization we are aware of (Dymetman, 1992),
EFD-closure is actually not guaranteed by it; and
Dymetman?s generalization only works for classi-
cally offline-parseable grammars.
In the case of non-   -OP grammars, a standard
bottom-up parser without EFD-closure would not
terminate at run-time either. Our new algorithm is
thus neither better nor worse than a textbook bottom-
up parser with respect to termination. A remain-
ing topic for consideration is the adaptation of this
method to strategies with better termination proper-
ties than the pure bottom-up strategy.
8 Empirical Evaluation
The details of how to integrate an indexing strategy
for unification-based grammars into the EFD-based
parsing algorithm are too numerous to present here,
but a few empirical observations can be made. First,
EFD-based parsing is faster than Carpenter?s algo-
rithm even with atomic, CFG-like categories, where
the cost of copying is at a minimum, even with no in-
dexing. We defined several sizes of CFG by extract-
ing local trees from successively increasing portions
of the Penn Treebank II, as shown in Table 1, and
WSJ Number of Lexicon Number of
directories WSJ files size Rules
00 4 131 77
00 5 188 124
00 6 274 168
00 8 456 282
00 10 756 473
00 15 1167 736
00 20 1880 1151
00 25 2129 1263
00 30 2335 1369
00 35 2627 1589
00 40 3781 2170
00 50 5645 3196
00?01 100 8948 5246
00?01 129 11242 6853
00?02 200 13164 7984
00?02 250 14730 9008
00?03 300 17555 10834
00?03 350 18861 11750
00?04 400 20359 12696
00?05 481 20037 13159
00?07 700 27404 17682
00?09 901 32422 20999
Table 1: The grammars extracted from the Wall
Street Journal directories of the PTB II.
then computed the average time to parse a corpus of
sentences (5 times each) drawn from the initial sec-
tion. All of the parsers were written in SICStus Pro-
log. These average times are shown in Figure 2 as a
function of the number of rules. Storing active edges
is always the worst option, followed by Carpenter?s
algorithm, followed by the EFD-based algorithm. In
this atomic case, indexing simply takes on the form
of a hash by phrase structure category. This can be
implemented on top of EFD because the overhead of
copying has been reduced. This fourth option is the
fastest by a factor of approximately 2.18 on average
over EFD without indexing.
One may also refer to Table 2, in which the num-
0.001
0.01
0.1
1
10
100
1000
10000
100000
0 5000 10000 15000 20000 25000
Ti
m
e 
[lo
g(s
ec
)]
Number of rules
Average parsing times
Active
Carpenter
EFD
EFD-index
Figure 2: Parsing times for simple CFGs.
Number Successful Failed Success
of rules unifications unifications rate (%)
124 104 1,766 5.56
473 968 51,216 1.85
736 2,904 189,528 1.51
1369 7,152 714,202 0.99
3196 25,416 3,574,138 0.71
5246 78,414 14,644,615 0.53
6853 133,205 30,743,123 0.43
7984 158,352 40,479,293 0.39
9008 195,382 56,998,866 0.34
10834 357,319 119,808,018 0.30
11750 441,332 151,226,016 0.29
12696 479,612 171,137,168 0.28
14193 655,403 250,918,711 0.26
17682 911,480 387,453,422 0.23
20999 1,863,523 847,204,674 0.21
Table 2: Successful unification rate for the (non-
indexing) EFD parser.
ber of successful and failed unifications (matches)
was counted over the test suite for each rule set.
Asymptotically, the success rate does not decrease
by very much from rule set to rule set. There are so
many more failures early on, however, that the sheer
quantity of failed unifications makes it more impor-
tant to dispense with these quickly.
Of the grammars to which we have access that use
larger categories, this ranking of parsing algorithms
is generally preserved, although we have found no
correlation between category size and the factor of
improvement. John Carroll?s Prolog port of the
Alvey grammar of English (Figure 3), for example,
is EFD-closed, but the improvement of EFD over
Carpenter?s algorithm is much smaller, presumably
because there are so few edges when compared to
the CFGs extracted from the Penn Treebank. EFD-
index is also slower than EFD without indexing be-
cause of our poor choice of index for that gram-
mar. With subsumption testing (Figure 4), the ac-
tive edge algorithm and Carpenter?s algorithm are
at an even greater disadvantage because edges must
be copied to be compared for subsumption. On a
pre-release version of MERGE (Figure 5),1 a modi-
fication of the English Resource Grammar that uses
more macros and fewer types, the sheer size of the
categories combined with a scarcity of edges seems
to cost EFD due to the loss of locality of reference,
although that loss is more than compensated for by
indexing.
100
1000
10000
0 20 40 60 80 100 120 140 160 180 200
Ti
m
e 
[lo
g(m
sec
)]
Test cases
Parsing times over Alvey grammar - no subsumption
Active
Carp
EFD-Index
EFD
Figure 3: Alvey grammar with no subsumption.
100
1000
10000
0 20 40 60 80 100 120 140 160 180 200
Ti
m
e 
[lo
g(m
sec
)]
Test cases
Parsing times over Alvey grammar - with subsumption
Active
Carp
EFD
EFD-index
Figure 4: Alvey grammar with subsumption testing.
1We are indebted to Kordula DeKuthy and Detmar Meurers
of Ohio State University, for making this pre-release version
available to us.
100
1000
0 5 10 15 20
Ti
m
e 
[lo
g(m
sec
)]
Test cases
Parsing times over Merge grammar
Active
EFD
Carp
EFD-index
Figure 5: MERGE on the CSLI test-set.
9 Conclusion
This paper has presented a bottom-up parsing algo-
rithm for Prolog that reduces the copying of edges
from either linear or quadratic to a constant num-
ber of two per non-empty edge. Its termination
properties and asymptotic complexity are the same
as a standard bottom-up chart parser, but in prac-
tice it performs better. Further optimizations can be
incorporated by compiling rules in a way that lo-
calizes the disjunctions that are implicit in the cre-
ation of extra rules in the compile-time EFD-closure
step, and by integrating automaton- or decision-tree-
based indexing with this algorithm. With copying
now being unnecessary for matching a daughter cat-
egory description, these two areas should result in
a substantial improvement to parse times for highly
lexicalized grammars. The adaptation of this algo-
rithm to active edges, other control strategies, and to
scheduling concerns such as finding the first parse as
quickly as possible remain interesting areas of fur-
ther extension.
Apart from this empirical issue, this algorithm is
of theoretical interest in that it proves that a con-
stant number of edge copies can be attained by an
all-paths parser, even in the presence of partially or-
dered categories.
References
B. Carpenter and G. Penn. 1996. Compiling typed
attribute-value logic grammars. In H. Bunt and
M. Tomita, editors, Recent Advances in Parsing Tech-
nologies, pages 145?168. Kluwer.
M. Dymetman. 1992. A generalized greibach normal
form for definite clause grammars. In Proceedings of
the International Conference on Computational Lin-
guistics.
M. Dymetman. 1994. A simple transformation for
offline-parsable gramamrs and its termination proper-
ties. In Proceedings of the International Conference
on Computational Linguistics.
P. Graf. 1996. Term Indexing. Springer Verlag.
T. Makino, K. Torisawa, and J. Tsuji. 1998. LiL-
FeS ? practical unification-based programming sys-
tem for typed feature structures. In Proceedings of
COLING/ACL-98, volume 2, pages 807?811.
R. Malouf, J. Carroll, and A. Copestake. 2000. Efficient
feature structure operations without compilation. Nat-
ural Language Engineering, 6(1):29?46.
T. Ninomiya, T. Makino, and J. Tsuji. 2002. An indexing
scheme for typed feature structures. In Proceedings of
the 19th International Conference on Computational
Linguistics (COLING-02).
S. Oepen and J. Carroll. 2000. Parser engineering and
performance profiling. Natural Language Engineer-
ing.
G. Penn and O. Popescu. 1997. Head-driven genera-
tion and indexing in ALE. In Proceedings of the EN-
VGRAM workshop; ACL/EACL-97.
G. Penn. 1999. Optimising don?t-care non-determinism
with statistical information. Technical Report 140,
Sonderforschungsbereich 340, Tu?bingen.
G. Penn. 2000. The Algebraic Structure of Attributed
Type Signatures. Ph.D. thesis, Carnegie Mellon Uni-
versity.
F. C. N. Pereira and S. M. Shieber. 1987. Prolog and
Natural-Language Analysis, volume 10 of CSLI Lec-
ture Notes. University of Chicago Press.
E. Stabler. 1993. The Logical Approach to Syntax: Foun-
dations, Specifications, and implementations of Theo-
ries of Government and Binding. MIT Press.
K. Torisawa and J. Tsuji. 1995. Compiling HPSG-style
grammar to object-oriented language. In Proceedings
of NLPRS-1995, pages 568?573.
G. van Noord. 1997. An efficient implementation of the
head-corner parser. Computational Linguistics.
Optimizing Typed Feature Structure Grammar Parsing through
Non-Statistical Indexing
Cosmin Munteanu and Gerald Penn
University of Toronto
10 King?s College Rd.
Toronto M5S 3G4
Canada
 
mcosmin,gpenn  @cs.toronto.edu
Abstract
This paper introduces an indexing method based on
static analysis of grammar rules and type signatures
for typed feature structure grammars (TFSGs). The
static analysis tries to predict at compile-time which
feature paths will cause unification failure during
parsing at run-time. To support the static analysis,
we introduce a new classification of the instances
of variables used in TFSGs, based on what type of
structure sharing they create. The indexing actions
that can be performed during parsing are also enu-
merated. Non-statistical indexing has the advan-
tage of not requiring training, and, as the evalua-
tion using large-scale HPSGs demonstrates, the im-
provements are comparable with those of statistical
optimizations. Such statistical optimizations rely
on data collected during training, and their perfor-
mance does not always compensate for the training
costs.
1 Introduction
Developing efficient all-paths parsers has been a
long-standing goal of research in computational lin-
guistics. One particular class still in need of pars-
ing time improvements is that of TFSGs. While
simpler formalisms such as context-free grammars
(CFGs) also face slow all-paths parsing times when
the size of the grammar increases significantly, TF-
SGs (which generally have fewer rules than large-
scale CFGs) become slow as a result of the com-
plex structures used to describe the grammatical cat-
egories. In HPSGs (Pollard and Sag, 1994), one cat-
egory description could contain hundreds of feature
values. This has been a barrier in transferring CFG-
successful techniques to TFSG parsing.
For TFSG chart parsers, one of the most time-
consuming operations is the retrieval of categories
from the chart during rule completion (closing of
constituents in the chart under a grammar rule).
Looking in the chart for a matching edge for a
daughter is accomplished by attempting unifications
with edges stored in the chart, resulting in many
failed unifications. The large and complex structure
of TFS descriptions (Carpenter, 1992) leads to slow
unification times, affecting the parsing times. Thus,
failing unifications must be avoided during retrieval
from the chart.
To our knowledge, there have been only four
methods proposed for improving the retrieval com-
ponent of TFSG parsing. One (Penn and Munteanu,
2003) addresses only the cost of copying large cate-
gories, and was found to reduce parsing times by an
average of 25% on a large-scale TFSG (MERGE).
The second, a statistical method known as quick-
check (Malouf et al, 2000), determines the paths
that are likely to cause unification failure by pro-
filing a large sequence of parses over representa-
tive input, and then filters unifications at run-time
by first testing these paths for type consistency.
This was measured as providing up to a 50% im-
provement in parse times on the English Resource
Grammar (Flickinger, 1999, ERG). The third (Penn,
1999b) is a similar but more conservative approach
that uses the profile to re-order sister feature values
in the internal data structure. This was found to im-
prove parse times on the ALE HPSG by up to 33%.
The problem with these statistical methods is that
the improvements in parsing times may not jus-
tify the time spent on profiling, particularly during
grammar development. The static analysis method
introduced here does not use profiling, although it
does not preclude it either. Indeed, an evaluation of
statistical methods would be more relevant if mea-
sured on top of an adequate extent of non-statistical
optimizations. Although quick-check is thought to
produce parsing time improvements, its evaluation
used a parser with only a superficial static analysis
of chart indexing.
That analysis, rule filtering (Kiefer et al, 1999),
reduces parse times by filtering out mother-daughter
unifications that can be determined to fail at
compile-time. True indexing organizes the data
(in this case, chart edges) to avoid unnecessary re-
trievals altogether, does not require the operations
that it performs to be repeated once full unification
is deemed necessary, and offers the support for eas-
ily adding information extracted from further static
analysis of the grammar rules, while maintaining
the same indexing strategy. Flexibility is one of the
reasons for the successful employment of indexing
in databases (Elmasri and Navathe, 2000) and auto-
mated reasoning (Ramakrishnan et al, 2001).
In this paper, we present a general scheme for in-
dexing TFS categories during parsing (Section 3).
We then present a specific method for statically an-
alyzing TFSGs based on the type signature and the
structure of category descriptions in the grammar
rules, and prove its soundness and completeness
(Section 4.2.1). We describe a specific indexing
strategy based on this analysis (Section 4), and eval-
uate it on two large-scale TFSGs (Section 5). The
result is a purely non-statistical method that is com-
petitive with the improvements gained by statistical
optimizations, and is still compatible with further
statistical improvements.
2 TFSG Terminology
TFSs are used as formal representatives of rich
grammatical categories. In this paper, the formal-
ism from (Carpenter, 1992) will be used. A TFSG
is defined relative to a fixed set of types and set of
features, along with constraints, called appropriate-
ness conditions. These are collectively known as
the type signature (Figure 3). For each type, ap-
propriateness specifies all and only the features that
must have values defined in TFSs of that type. It
also specifies the types of the values that those fea-
tures can take. The set of types is partially ordered,
and has a unique most general type (   ? ?bottom?).
This order is called subsumption (  ): more specific
(higher) types inherit appropriate features from their
more general (lower) supertypes. Two types t1 and
t2 unify (t1  t2  ) iff they have a least upper bound
in the hierarchy. Besides a type signature, TFSGs
contain a set of grammar (phrase) rules and lexical
descriptions. A simple example of a lexical descrip-
tion is: john  SYNSEM :  SYN : np  SEM : j 	 , while
an example of a phrase rule is given in Figure 1.
 SYN : s  SEM :  V PSem 
 AGENT : NPSem 		
 SYN : np  AGR : Agr  SEM : NPSem 	 ,
 SYN : vp  AGR : Agr  SEM : VPSem 	 .
Figure 1: A phrase rule stating that the syntactic category
s can be combined from np and vp if their values for
agr are the same. The semantics of s is that of the verb
phrase, while the semantics of the noun phrase serves as
agent.
2.1 Typed Feature Structures
A TFS (Figure 2) is like a recursively defined record
in a programming language: it has a type and fea-
tures with values that can be TFSs, all obeying
the appropriateness conditions of the type signature.
TFSs can also be seen as rooted graphs, where arcs
correspond to features and nodes to substructures. A
node typing function ?  q 	 associates a type to every
node q in a TFS. Every TFS F has a unique starting
or root node, qF . For a given TFS, the feature value
partial function ?  f 
 q 	 specifies the node reachable
from q by feature f when one exists. The path value
partial function ?  pi 
 q 	 specifies the node reachable
from q by following a path of features pi when one
exists. TFSs can be unified as well. The result repre-
sents the most general consistent combination of the
information from two TFSs. That information in-
cludes typing (by unifying the types), feature values
(by recursive unification), and structure sharing (by
an equivalence closure taken over the nodes of the
arguments). For large TFSs, unification is compu-
tationally expensive, since all the nodes of the two
TFSs are visited. In this process, many nodes are
collapsed into equivalence classes because of struc-
ture sharing. A node x in a TFS F with root qF and
a node x  in a TFS F  with root qF  are equivalent
(  ) with respect to F

F  iff x  qF and x  qF

,
or if there is a path pi such that ?F  F   pi 
 qF 	 x and
?F  F   pi 
 qF  	 x  .
NUMBER:
PERSON:
GENDER: masculine
third
[1]singular
NUMBER:
PERSON:
GENDER:
third
neuter
[1]
throwing
THROWER: index
THROWN: index
Figure 2: A TFS. Features are written in uppercase,
while types are written with bold-face lowercase. Struc-
ture sharing is indicated by numerical tags, such as [1].
THROWER:
THROWN:
index
index
masculine  feminine  neuter  singular   plural first  second  third
numgend pers
PERSON:
GENDER:
NUMBER:
pers
num
gend
throwing index
Figure 3: A type signature. For each type, appropriate-
ness declares the features that must be defined on TFSs
of that type, along with the type restrictions applying to
their values.
2.2 Structure Sharing in Descriptions
TFSGs are typically specified using descriptions,
which logically denote sets of TFSs. Descriptions
can be more terse because they can assume all of
the information about their TFSs that can be in-
ferred from appropriateness. Each non-disjunctive
description can be associated with a unique most
general feature structure in its denotation called a
most general satisfier (MGSat). While a formal
presentation can be found in (Carpenter, 1992), we
limit ourselves to an intuitive example: the TFS
from Figure 2 is the MGSat of the description:
throwing  THROWER :  PERSON : third  NUMBER :
 singular  Nr 	 
 GENDER : masculine 	  THROWN :
 PERSON : third  NUMBER : Nr
 GENDER : neuter 	 .
Descriptions can also contain variables, such as Nr.
Structure sharing is enforced in descriptions
through the use of variables. In TFSGs, the scope
of a variable extends beyond a single description, re-
sulting in structure sharing between different TFSs.
In phrase structure rules (Figure 1), this sharing
can occur between different daughter categories in
a rule, or between a mother and a daughter. Unless
the term description is explicitly used, we will use
?mother? and ?daughter? to refer to the MGSat of a
mother or daughter description.
We can classify instances of variables based on
what type of structure sharing they create. Inter-
nal variables are the variables that represent inter-
nal structure sharing (such as in Figure 2). The oc-
currences of such variables are limited to a single
category in a phrase structure rule. External vari-
ables are the variables used to share structure be-
tween categories. If a variable is used for struc-
ture sharing both inside a category and across cat-
egories, then it is also considered an external vari-
able. For a specific category, two kinds of external
variable instances can be distinguished, depending
on their occurrence relative to the parsing control
strategy: active external variables and inactive ex-
ternal variables. Active external variables are in-
stances of external variables that are shared between
the description of a category D and one or more de-
scriptions of categories in the same rule as D vis-
ited by the parser before D as the rule is extended
(completed). Inactive external variables are the ex-
ternal variable instances that are not active. For ex-
ample, in bottom-up left-to-right parsing, all of a
mother?s external variable instances would be active
because, being external, they also occur in one of
the daughter descriptions. Similarly, all of the left-
most daughter?s external variable instances would
be inactive because this is the first description used
by the parser. In Figure 1, Agr is an active external
variable in the second daughter, but it is inactive in
the first daughter.
The active external variable instances are im-
portant for path indexing (Section 4.2), because
they represent the points at which the parser must
copy structure between TFSs. They are therefore
substructures that must be provided to a rule by
the parsing chart if these unifications could poten-
tially fail. They also represent shared nodes in the
MGSats of a rule?s category descriptions. In our
definitions, we assume without loss of generality
that parsing proceeds bottom-up, with left-to-right
of rule daughters. This is the ALE system?s (Car-
penter and Penn, 1996) parsing strategy.
Definition 1. If D1 
  
 Dn are daughter de-
scriptions in a rule and the rules are extended
from left to right, then Ext  MGSat  Di 		 is the
set of nodes shared between MGSat  Di 	 and
MGSat  D1 	  MGSat  Di  1 	 . For a mother de-
scription M, Ext  MGSat  M 		 is the set of nodes
shared with any daughter in the same rule.
Because the completion of TFSG rules can cause
the categories to change in structure (due to exter-
nal variable sharing), we need some extra notation
to refer to a phrase structure rule?s categories at dif-
ferent times during a single application of that rule.
By

M we symbolize the mother M after M?s rule is
completed (all of the rule?s daughters are matched
with edges in the chart).

D symbolizes the daugh-
ter D after all daughters to D?s left in D?s rule were
unified with edges from the chart. An important re-
lation exists between M and

M: if qM is M?s root and

qM is

M?s root, then

x  M 



x 

M such that  pi for
which ?  pi 
 qM 	  x and ?  pi 


qM 	 

x, ?  x 	
	 ? 

x 	 .
In other words, extending the rule extends the in-
formation states of its categories monotonically. A
similar relation exists between D and

D. The set of
all nodes x in M such that  pi for which ?  pi 
 qM 	  x
and ?  pi 


qM 	 

x will be denoted by 

x 
 1 (and like-
wise for nodes in D). There may be more than one
node in 

x 
 1 because of unifications that occur dur-
ing the extension of M to

M.
3 The Indexing Timeline
Indexing can be applied at several moments dur-
ing parsing. We introduce a general strategy for in-
dexed parsing, with respect to what actions should
be taken at each stage.
Three main stages can be identified. The first
one consists of indexing actions that can be taken
off-line (along with other optimizations that can be
performed at compile-time). The second and third
stages refer to actions performed at run time.
Stage 1. In the off-line phase, a static analysis
of grammar rules can be performed. The complete
content of mothers and daughters may not be ac-
cessible, due to variables that will be instantiated
during parsing, but various sources of information,
such as the type signature, appropriateness specifi-
cations, and the types and features of mother and
daughter descriptions, can be analyzed and an ap-
propriate indexing scheme can be specified. This
phase of indexing may include determining: (1a)
which daughters in which rules will certainly not
unify with a specific mother, and (1b) what informa-
tion can be extracted from categories during parsing
that can constitute indexing keys. It is desirable to
perform as much analysis as possible off-line, since
the cost of any action taken during run time pro-
longs the parsing time.
Stage 2. During parsing, after a rule has been
completed, all variables in the mother have been ex-
tended as far as they can be before insertion into
the chart. This offers the possibility of further in-
vestigating the mother?s content and extracting sup-
plemental information from the mother that con-
tributes to the indexing keys. However, the choice
of such investigative actions must be carefully stud-
ied, since it might burden the parsing process.
Stage 3. While completing a rule, for each
daughter a matching edge is searched in the chart.
At this moment, the daughter?s active external vari-
ables have been extended as far as they can be be-
fore unification with a chart edge. The information
identified in stage (1b) can be extracted and unified
as a precursor to the remaining steps involved in cat-
egory unification. These steps also take place at this
stage.
4 TFSG Indexing
To reduce the time spent on failures when search-
ing for an edge in the chart, each edge (edge?s cat-
egory) has an associated index key which uniquely
identifies the set of daughter categories that can po-
tentially match it. When completing a rule, edges
unifying with a specific daughter are searched for in
the chart. Instead of visiting all edges in the chart,
the daughter?s index key selects a restricted number
of edges for traversal, thus reducing the number of
unification attempts.
The passive edges added to the chart represent
specializations of rules? mothers. When a rule is
completed, its mother M is added to the chart ac-
cording to M?s indexing scheme, which is the set of
index keys of daughters that might possibly unify
with M. The index is implemented as a hash, where
the hash function applied to a daughter yields the
daughter?s index key (a selection of chart edges).
For a passive edge representing M, M?s index-
ing scheme provides the collection of hash entries
where it will be added.
Each daughter is associated with a unique index
key. During parsing, a specific daughter is searched
for in the chart by visiting only those edges that have
a matching key, thus reducing the time needed for
traversing the chart. The index keys can be com-
puted off-line (when daughters are indexed by posi-
tion), or during parsing.
4.1 Positional Indexing
In positional indexing, the index key for
each daughter is represented by its position
(rule number and daughter position in the
rule). The structure of the index can be de-
termined at compile-time (first stage). For
each mother M in the grammar, a collection
L  M 	    Ri 
 D j 	 daughters that can match M  is
created (M?s indexing scheme), where each element
of L  M 	 represents the rule number Ri and daughter
position D j inside rule Ri (1  j  arity  Ri 	 ) of a
category that can match with M.
For TFSGs it is not possible to compute off-line
the exact list of mother-daughter matching pairs, but
it is possible to rule out certain non-unifiable pairs
before parsing ? a compromise that pays off with a
very low index management time.
During parsing, each time an edge (representing
a rule?s mother M) is added to the chart, it is in-
serted into the hash entries associated with the po-
sitions  Ri 
 D j 	 from the list L  M 	 (the number of
entries where M is inserted is L  M 	 ). The entry
associated with the key  Ri 
 D j 	 will contain only
categories that can possibly unify with the daughter
at position  Ri 
 D j 	 in the grammar.
Because our parsing algorithm closes categories
depth-first under leftmost daughter matching, only
daughters Di with i  2 are searched for in the
chart (and consequently, indexed). We used the
EFD-based modification of this algorithm (Penn
and Munteanu, 2003), which needs no active edges,
and requires a constant two copies per edges, rather
than the standard one copy per retrieval found in
Prolog parsers. Without this, the cost of copying
TFS categories would have overwhelmed the bene-
fit of the index.
4.2 Path Indexing
Path indexing is an extension of positional index-
ing. Although it shares the same underlying prin-
ciple as the path indexing used in automated rea-
soning (Ramakrishnan et al, 2001), its functionality
is related to quick check: extract a vector of types
from a mother (which will become an edge) and a
daughter, and test the unification of the two vectors
before attempting to unify the edge and the daugh-
ter. Path indexing differs from quick-check in that
it identifies these paths by a static analysis of gram-
mar rules, performed off-line and with no training
required. Path indexing is also built on top of po-
sitional indexing, therefore the vector of types can
be different for each potentially unifiable mother-
daughter pair.
4.2.1 Static Analysis of Grammar Rules
Similar to the abstract interpretation used in pro-
gram verification (Cousot and Cousot, 1992),
the static analysis tries to predict a run-time
phenomenon (specifically, unification failures) at
compile-time. It tries to identify nodes in a mother
that carry no relevant information with respect to
unification with a particular daughter. For a mother
M unifiable with a daughter D, these nodes will
be grouped in a set StaticCut  M 
 D 	 . Intuitively,
these nodes can be left out or ignored while com-
puting the unification of

M and

D. The StaticCut
can be divided into two subsets: StaticCut  M 
 D 	 
RigidCut  M 
 D 	  VariableCut  M 
 D 	 
The RigidCut represents nodes that can be left out
because neither they, nor one of their ?pi-ancestors,
can have their type values changed by means of ex-
ternal variable sharing. The VariableCut represents
nodes that are either externally shared, or have an
externally shared ancestor, but still can be left out.
Definition 2. RigidCut  M 
 D 	 is the largest subset
of nodes x  M such that,  y  D for which x  y:
1. x  Ext  M 	 , y  Ext  D 	 ,
2.

x   M s.t.  pi s.t. ?  pi 
 x  	  x, x   Ext  M 	 , and
3.

y   D s.t.  pi s.t. ?  pi 
 y  	  y, y   Ext  D 	 .
Definition 3. VariableCut is the largest subset of
nodes x  M such that:
1. x  RigidCut  M 
 D 	 , and
2.

y  D for which x  y,  s  ?  x 	 
  t  ?  y 	 ,
s

t exists.
In words, a node can be left out even if it is ex-
ternally shared (or has an externally shared ances-
tor) if all possible types this node can have unify
with all possible types its corresponding nodes in
D can have. Due to structure sharing, the types of
nodes in M and D can change during parsing, by
being specialized to one of their subtypes. Condi-
tion 2 ensures that the types of these nodes will re-
main compatible (have a least upper bound), even if
they specialize during rule completion. An intuitive
example (real-life examples cannot be reproduced
here ? a category in a typical TFSG can have hun-
dreds of nodes) is presented in Figure 4.
y2
y1
y3 y5
t1
t6
t6
y4 t1
t5
F:
G:
H:
G:
K:
Dx1
x2
x3
x4
F: H:
G:
I:
t7 t7
t3
t1
G:t1
H:t6
F:t6
K:t1
I:t3
t1
t5 t3
G:t5 t4
t2
J:t5
t7
t6
t0
T
t8
M
Figure 4: Given the above type signature, mother M and
daughter D (externally shared nodes are pointed to by
dashed arrows), nodes x1  x2  and x3 from M can be left
out when unifying M with D during parsing. x1 and x3
 RigidCut  M

D  , while x2
 VariableCut  M

D  (?  y2 
can promote only to t7, thus x2 and y2 will always be
compatible). x4 is not included in the StaticCut, because
if ?  y5  promotes to t5, then ?  y4  will promote to t5 (not
unifiable with t3).
When computing the unification between a
mother and a daughter during parsing, the same out-
come (success or failure) will be reached by using
a reduced representation of the mother (

MsD), with
nodes in StaticCut  M 
 D 	 removed from

M.
Proposition 1. For a mother M and a daughter D,
if M

D

before parsing, and

M (as an edge in the
chart) and

D exist, then during parsing: (1)

MsD


D



M


D

, (2)

MsD


D  

M


D  .
Proof. The second part (

MsD


D  

M


D  )
of Proposition 1 has a straightforward proof: if

MsD


D  , then 

z 

MsD  

D such that 	  t for
which


x  

z 
 
 t  ? 

x 	 . Since

MsD 

M, 

z 

M  

D such that 	  t for which


x  

z  
 
 t  ? 

x 	 ,
and therefore,

M


D  .
The first part of the proposition will be proven by
showing that


z 

M  

D, a consistent type can be
assigned to 

z 
 , where 

z 
 is the set of nodes in

M
and

D equivalent to

z with respect to the unification
of

M and

D.1
Three lemmata need to be formulated:
Lemma 1. If x 

M and x  

x 
 1
, then ? 

x 	 ?  x 	 .
Similarly, for y 

D, y  

y   1, ? 

y 	 ?  y 	 .
Lemma 2. If types t0 
 t1 
  
 tn are such that  t 0 
t0 


i   1 
  
 n 	 , t 0  ti

, then  t  t0 such that

i 
 1 
  
 n 	 , t  ti.
1Because we do not assume inequated TFSs (Carpenter,
1992) here, unification failure must result from type inconsis-
tency.
Lemma 3. If x 

M and

y 

D for which x  y, then
 x  

x 
 1

  y  

y   1 such that x  y.
In proving the first part of Proposition 1, four
cases are identified: Case A:  

z  
 


M   1 and
 

z  
 


D  1, Case B:  

z 



M   1 and  

z 



D  1, Case C:  

z 
 


M  1 and  

z 



D   1,
Case D:  

z 



M   1 and  

z 



D   1. Case A
is trivial, and D is a generalization of B and C.
Case B. It will be shown that  t  Type such that


y  

z  
 


D and for  

x   

z 



M, t  ? 

y 	 and
t  ? 

x 	 .
Subcase B.i:

x 

M 


x 

MsD .


y  

z  



D,

y 

x. Therefore, according to Lemma 3,  x 


x 
 1

  y  

y   1 such that x  y. Thus, according
to Condition 2 of Definition 3,

s  ?  y 	 
  t  ?  x 	 ,
s

t

. But according to Lemma 1, ? 

y 	  ?  y 	 and
? 

x 	  ?  x 	 . Therefore, 

y  

z  



D,

s  ? 

y 	 ,

t  ? 

x 	 , s

t

, and hence, 

y  

z  



D 


t 
? 

x 	 
 t

? 

y 	

. Thus, according to Lemma 2,  t 
? 

x 	 



y  

z  
 


D, t  ? 

y 	 .
Subcase B.ii:

x 

M 


x 

MsD . Since

MsD


D

,
 t  ? 

x 	 such that


y  

z  



D, t  ? 

y 	 .
Case C. It will be shown that  t  ? 

y 	 such
that


x  

z 
 , t  ? 

x 	 . Let  

y   

z 
 


D. The
set 

z 



M can be divided into two subsets: Sii 
 

x  

z  



M 

x 

MsD  , and Si   

x  

z  



M 

x 

M 


x 

MsD , and x  VariableCut  M 
 D 	  . If x
were in RigidCut  M 
 D 	 , then necessarily  

z  



M 
would be 1. Since Sii 

MsD and

MsD


D

, then
 t   ? 

y 	 such that


x  Sii 
 t   ? 

x 	 (*). How-
ever,


x  Sii,

x 

y. Therefore, according to
Lemma 3,


x  Sii 
  x  

x 
 1

  y  

y   1 such that
x  y. Thus, since x  VariableCut  M 
 D 	 , Condi-
tion 2 of Definition 3 holds, and therefore, accord-
ing to Lemma 1,

s1  ? 

x 	 


s2  ? 

y 	 
 s1  s2

.
More than this, since t   ? 

y 	 (for the type t  from
(*)),  s1  ? 

x 	 


s 2  t  
 s1  s 2

, and hence,

s 2 
t  
 s 2  ? 

x 	

. Thus, according to Lemma 2 and to
(*),  t  t   ?  y 	 such that  x  Sii 
 t  ? 

x 	  Thus,
 t such that


x  

z  
 , t  ? 

x 	 .
While Proposition 1 could possibly be used by
grammar developers to simplify TFSGs themselves
at the source-code level, here we only exploit it for
internally identifying index keys for more efficient
chart parsing with the existing grammar. There may
be better static analyses, and better uses of this static
analysis. In particular, future work will focus on us-
ing static analysis to determine smaller representa-
tions (by cutting nodes in Static Cuts) of the chart
edges themselves.
4.2.2 Building the Path Index
The indexing schemes used in path indexing are
built on the same principles as those in positional
indexing. The main difference is the content of the
indexing keys, which now includes a third element.
Each mother M has its indexing scheme defined as:
L  M 	    Ri 
 D j 
 Vi  j 	  . The pair  Ri 
 D j 	 is the po-
sitional index key (as in positional indexing), while
Vi  j is the path index vector containing type values
extracted from M. A different set of types is ex-
tracted for each mother-daughter pair. So, path in-
dexing uses a two-layer indexing method: the po-
sitional key for daughters, and types extracted from
the typed feature structure. Each daughter?s index
key is now given by L  D j 	     Ri 
 Vi  j 	  , where Ri
is the rule number of a potentially matching mother,
and Vi  j is the path index vector containing types ex-
tracted from D j.
The types extracted for the indexing vectors
are those of nodes found at the end of indexing
paths. A path pi is an indexing path for a mother-
daughter pair  M 
 D 	 iff: (1) pi is defined for both M
and D, (2)  x  StaticCut  M 
 D 	 
  f s.t. ?  f 
 x 	 
?  pi 
 qM 	 (qM is M?s root), and (3) ?  pi 
 qM 	 
StaticCut  M 
 D 	 . Indexing paths are the ?frontiers?
of the non-statically-cut nodes of M.
A similar key extraction could be performed dur-
ing Stage 2 of indexing (as outlined in Section 3),
using

M rather than M. We have found that this on-
line path discovery is generally too expensive to be
performed during parsing, however.
As stated in Proposition 1, the nodes in
StaticCut  M 
 D 	 do not affect the success/failure
of

M


D. Therefore, the types of first nodes
not included in StaticCut  M 
 D 	 along each path
pi that stems from the root of M and D are in-
cluded in the indexing key, since these nodes might
contribute to the success/failure of the unifica-
tion. It should be mentioned that the vectors Vi  j
are filled with values extracted from

M after M?s
rule is completed, and from

D after all daugh-
ters to the left of D are unified with edges in the
chart. As an example, assuming that the index-
ing paths are THROWER:PERSON, THROWN, and
THROWN:GENDER, the path index vector for the
TFS shown in Figure 2 is  third 
 index 
 neuter 	 .
4.2.3 Using the Path Index
Inserting and retrieving edges from the chart using
path indexing is similar to the general method pre-
sented at the beginning of this section. The first
layer of the index is used to insert a mother as
an edge into appropriate chart entries, according to
the positional keys for the daughters it can match.
Along with the mother, its path index vector is in-
serted into the chart.
When searching for a matching edge for a daugh-
ter, the search is restricted by the first indexing layer
to a single entry in the chart (labeled with the posi-
tional index key for the daughter). The second layer
restricts searches to the edges that have a compati-
ble path index vector. The compatibility is defined
as type unification: the type pointed to by the el-
ement Vi  j  n 	 of an edge?s vector Vi  j should unify
with the type pointed to by the element Vi  j  n 	 of the
path index vector Vi  j of the daughter on position D j
in a rule Ri.
5 Experimental Evaluation
Two TFSGs were used to evaluate the performance
of indexing: a pre-release version of the MERGE
grammar, and the ALE port of the ERG (in its final
form). MERGE is an adaptation of the ERG which
uses types more conservatively in favour of rela-
tions, macros and complex-antecedent constraints.
This pre-release version has 17 rules, 136 lexical
items, 1157 types, and 144 introduced features. The
ERG port has 45 rules, 1314 lexical entries, 4305
types and 155 features. MERGE was tested on 550
sentences of lengths between 6 and 16 words, ex-
tracted from the Wall Street Journal annotated parse
trees (where phrases not covered by MERGE?s vo-
cabulary were replaced by lexical entries having the
same parts of speech), and from MERGE?s own
test corpus. ERG was tested on 1030 sentences of
lengths between 6 and 22 words, extracted from the
Brown Corpus and from the Wall Street Journal an-
notated parse trees.
Rather than use the current version of ALE, TFSs
were encoded as Prolog terms as prescribed in
(Penn, 1999a), where the number of argument po-
sitions is the number of colours needed to colour
the feature graph. This was extended to allow for
the enforcement of type constraints during TFS uni-
fication. Types were encoded as attributed variables
in SICStus Prolog (Swedish Institute of Computer
Science, 2004).
5.1 Positional and path indexing evaluation
The average and best improvements in parsing times
of positional and path indexing over the same EFD-
based parser without indexing are presented in Ta-
ble 1. The parsers were implemented in SICStus
3.10.1 for Solaris 8, running on a Sun Server with 16
GB of memory and 4 UltraSparc v.9 processors at
1281 MHz. For MERGE, parsing times range from
10 milliseconds to 1.3 seconds. For ERG, parsing
times vary between 60 milliseconds and 29.2 sec-
onds.
Positional Index Path Index
average best average best
MERGE 1.3% 50% 1.3% 53.7%
ERG 13.9% 36.5% 12% 41.6%
Table 1: Parsing time improvements of positional and
path indexing over the non-indexed EFD parser.
5.2 Comparison with statistical optimizations
Non-statistical optimizations can be seen as a first
step toward a highly efficient parser, while statistical
optimization can be applied as a second step. How-
ever, one of the purposes of non-statistical index-
ing is to eliminate the burden of training while of-
fering comparable improvements in parsing times.
A quick-check parser was also built and evaluated
and the set-up times for the indexed parsers and
the quick-check parser were compared (Table 2).
Quick-check was trained on a 300-sentence training
corpus, as prescribed in (Malouf et al, 2000). The
training corpus included 150 sentences also used in
testing. The number of paths in path indexing is dif-
ferent for each mother-daughter pair, ranging from
1 to 43 over the two grammars.
Positional Path Quick
Index Index Check
Compiling grammar 6?30?
Compiling index 2? 1?33? -
Training - - 3h28?14?
Total set-up time: 6?32? 8?3? 3h34?44?
Table 2: The set-up times for non-statistically indexed
parsers and statistically optimized parsers for MERGE.
As seen in Table 3, quick-check alone surpasses
positional and path indexing for the ERG. How-
ever, it is outperformed by them on the MERGE,
recording slower times than even the baseline. But
the combination of quick-check and path indexing
is faster than quick-check alone on both grammars.
Path indexing at best provided no decrease in per-
formance over positional indexing alone in these ex-
periments, attesting to the difficulty of maintaining
efficient index keys in an implementation.
Positional Path Quick Quick +
Indexing Indexing Check Path
MERGE 1.3% 1.3% -4.5% -4.3%
ERG 13.9% 12% 19.8% 22%
Table 3: Comparison of average improvements over non-
indexed parsing among all parsers.
The quick-check evaluation presented in (Malouf
et al, 2000) uses only sentences with a length of
at most 10 words, and the authors do not report the
set-up times. Quick-check has an additional advan-
tage in the present comparison, because half of the
training sentences were included in the test corpus.
While quick-check improvements on the ERG
confirm other reports on this method, it must be
Grammar Successful Failed unifications Failure rate reduction (vs. no index)
unifications EFD Positional Path Quick Positional Path Quick
non-indexed Index Index Check Index Index Check
MERGE 159 755 699 552 370 7.4% 26.8% 50.9%
ERG 1078 215083 109080 108610 18040 49.2% 49.5% 91.6%
Table 4: The number of successful and failed unifications for the non-indexed, positional indexing, path indexing, and
quick-check parsers, over MERGE and ERG (collected on the slowest sentence in the corresponding test sets.)
noted that quick-check appears to be parochially
very well-suited to the ERG (indeed quick-check
was developed alongside testing on the ERG). Al-
though the recommended first 30 most probable
failure-causing paths account for a large part of
the failures recorded in training on both grammars
(94% for ERG and 97% for MERGE), only 51 paths
caused failures at all for MERGE during training,
compared to 216 for the ERG. Further training with
quick-check for determining a better vector length
for MERGE did not improve its performance.
This discrepancy in the number of failure-causing
paths could be resulting in an overfitted quick-check
vector, or, perhaps the 30 paths chosen for MERGE
really are not the best 30 (quick-check uses a greedy
approximation). In addition, as shown in Table 4,
the improvements made by quick-check on the ERG
are explained by the drastic reduction of (chart look-
up) unification failures during parsing relative to the
other methods. It appears that nothing short of a
drastic reduction is necessary to justify the overhead
of maintaining the index, which is the largest for
quick-check because some of its paths must be tra-
versed at run-time ? path indexing only uses paths
available at compile-time in the grammar source.
Note that path indexing outperforms quick-check on
MERGE in spite of its lower failure reduction rate,
because of its smaller overhead.
6 Conclusions and Future Work
The indexing method proposed here is suitable for
several classes of unification-based grammars. The
index keys are determined statically and are based
on an a priori analysis of grammar rules. A ma-
jor advantage of such indexing methods is the elim-
ination of the lengthy training processes needed
by statistical methods. Our experimental evalu-
ation demonstrates that indexing by static analy-
sis is a promising alternative to optimizing parsing
with TFSGs, although the time consumed by on-line
maintenance of the index is a significant concern ?
echoes of an observation that has been made in ap-
plications of term indexing to databases and pro-
gramming languages (Graf, 1996). Further work
on efficient implementations and data structures is
therefore required. Indexing by static analysis of
grammar rules combined with statistical methods
also can provide a higher aggregate benefit.
The current static analysis of grammar rules used
as a basis for indexing does not consider the effect
of the universally quantified constraints that typi-
cally augment the signature and grammar rules. Fu-
ture work will investigate this extension as well.
References
B. Carpenter and G. Penn. 1996. Compiling typed
attribute-value logic grammars. In H. Bunt and
M. Tomita, editors, Recent Advances in Parsing
Technologies, pages 145?168. Kluwer.
B. Carpenter. 1992. The Logic of Typed Feature
Structures. Cambridge University Press.
P. Cousot and R. Cousot. 1992. Abstract interpre-
tation and application to logic programs. Journal
of Logic Programming, 13(2?3).
R. Elmasri and S. Navathe. 2000. Fundamentals of
database systems. Addison-Wesley.
D. Flickinger. 1999. The English Resource Gram-
mar. http://lingo.stanford.edu/erg.html.
P. Graf. 1996. Term Indexing. Springer.
B. Kiefer, H.U. Krieger, J. Carroll, and R. Malouf.
1999. A bag of useful techniques for efficient and
robust parsing. In Proceedings of the 37th An-
nual Meeting of the ACL.
R. Malouf, J. Carrol, and A. Copestake. 2000. Effi-
cient feature structure operations without compi-
lation. Natural Language Engineering, 6(1).
G. Penn and C. Munteanu. 2003. A tabulation-
based parsing method that reduces copying. In
Proceedings of the 41st Annual Meeting of the
ACL, Sapporo, Japan.
G. Penn. 1999a. An optimised Prolog encoding of
typed feature structures. Technical Report 138,
SFB 340, Tu?bingen.
G. Penn. 1999b. Optimising don?t-care non-
determinism with statistical information. Techni-
cal Report 140, SFB 340, Tu?bingen.
C. Pollard and I. Sag. 1994. Head-driven Phrase
Structure Grammar. The University of Chicago
Press.
I.V. Ramakrishnan, R. Sekar, and A. Voronkov.
2001. Term indexing. In Handbook of Auto-
mated Reasoning, volume II, chapter 26. Elsevier
Science.
Swedish Institute of Computer Science. 2004. SIC-
Stus Prolog 3.11.0. http://www.sics.se/sicstus.
Head-Driven Parsing for Word Lattices
Christopher Collins
Department of Computer Science
University of Toronto
Toronto, ON, Canada
ccollins@cs.utoronto.ca
Bob Carpenter
Alias I, Inc.
Brooklyn, NY, USA
carp@alias-i.com
Gerald Penn
Department of Computer Science
University of Toronto
Toronto, ON, Canada
gpenn@cs.utoronto.ca
Abstract
We present the first application of the head-driven
statistical parsing model of Collins (1999) as a si-
multaneous language model and parser for large-
vocabulary speech recognition. The model is
adapted to an online left to right chart-parser for
word lattices, integrating acoustic, n-gram, and
parser probabilities. The parser uses structural
and lexical dependencies not considered by n-
gram models, conditioning recognition on more
linguistically-grounded relationships. Experiments
on the Wall Street Journal treebank and lattice cor-
pora show word error rates competitive with the
standard n-gram language model while extracting
additional structural information useful for speech
understanding.
1 Introduction
The question of how to integrate high-level knowl-
edge representations of language with automatic
speech recognition (ASR) is becoming more impor-
tant as (1) speech recognition technology matures,
(2) the rate of improvement of recognition accu-
racy decreases, and (3) the need for additional in-
formation (beyond simple transcriptions) becomes
evident. Most of the currently best ASR systems use
an n-gram language model of the type pioneered by
Bahl et al (1983). Recently, research has begun to
show progress towards application of new and bet-
ter models of spoken language (Hall and Johnson,
2003; Roark, 2001; Chelba and Jelinek, 2000).
Our goal is integration of head-driven lexical-
ized parsing with acoustic and n-gram models for
speech recognition, extracting high-level structure
from speech, while simultaneously selecting the
best path in a word lattice. Parse trees generated
by this process will be useful for automated speech
understanding, such as in higher semantic parsing
(Ng and Zelle, 1997).
Collins (1999) presents three lexicalized models
which consider long-distance dependencies within a
sentence. Grammar productions are conditioned on
headwords. The conditioning context is thus more
focused than that of a large n-gram covering the
same span, so the sparse data problems arising from
the sheer size of the parameter space are less press-
ing. However, sparse data problems arising from
the limited availability of annotated training data be-
come a problem.
We test the head-driven statistical lattice parser
with word lattices from the NIST HUB-1 corpus,
which has been used by others in related work (Hall
and Johnson, 2003; Roark, 2001; Chelba and Je-
linek, 2000). Parse accuracy and word error rates
are reported. We present an analysis of the ef-
fects of pruning and heuristic search on efficiency
and accuracy and note several simplifying assump-
tions common to other reported experiments in this
area, which present challenges for scaling up to real-
world applications.
This work shows the importance of careful al-
gorithm and data structure design and choice of
dynamic programming constraints to the efficiency
and accuracy of a head-driven probabilistic parser
for speech. We find that the parsing model of
Collins (1999) can be successfully adapted as a lan-
guage model for speech recognition.
In the following section, we present a review of
recent works in high-level language modelling for
speech recognition. We describe the word lattice
parser developed in this work in Section 3. Sec-
tion 4 is a description of current evaluation metrics,
and suggestions for new metrics. Experiments on
strings and word lattices are reported in Section 5,
and conclusions and opportunities for future work
are outlined in Section 6.
2 Previous Work
The largest improvements in word error rate (WER)
have been seen with n-best list rescoring. The best
n hypotheses of a simple speech recognizer are pro-
cessed by a more sophisticated language model and
re-ranked. This method is algorithmically simpler
than parsing lattices, as one can use a model de-
veloped for strings, which need not operate strictly
left to right. However, we confirm the observa-
tion of (Ravishankar, 1997; Hall and Johnson, 2003)
that parsing word lattices saves computation time by
only parsing common substrings once.
Chelba (2000) reports WER reduction by rescor-
ing word lattices with scores of a structured lan-
guage model (Chelba and Jelinek, 2000), interpo-
lated with trigram scores. Word predictions of the
structured language model are conditioned on the
two previous phrasal heads not yet contained in a
bigger constituent. This is a computationally inten-
sive process, as the dependencies considered can be
of arbitrarily long distances. All possible sentence
prefixes are considered at each extension step.
Roark (2001) reports on the use of a lexical-
ized probabilistic top-down parser for word lattices,
evaluated both on parse accuracy and WER. Our
work is different from Roark (2001) in that we use
a bottom-up parsing algorithm with dynamic pro-
gramming based on the parsing model II of Collins
(1999).
Bottom-up chart parsing, through various forms
of extensions to the CKY algorithm, has been ap-
plied to word lattices for speech recognition (Hall
and Johnson, 2003; Chappelier and Rajman, 1998;
Chelba and Jelinek, 2000). Full acoustic and n-best
lattices filtered by trigram scores have been parsed.
Hall and Johnson (2003) use a best-first probabilis-
tic context free grammar (PCFG) to parse the input
lattice, pruning to a set of local trees (candidate par-
tial parse trees), which are then passed to a version
of the parser of Charniak (2001) for more refined
parsing. Unlike (Roark, 2001; Chelba, 2000), Hall
and Johnson (2003) achieve improvement in WER
over the trigram model without interpolating its lat-
tice parser probabilities directly with trigram prob-
abilities.
3 Word Lattice Parser
Parsing models based on headword dependency re-
lationships have been reported, such as the struc-
tured language model of Chelba and Jelinek (2000).
These models use much less conditioning informa-
tion than the parsing models of Collins (1999), and
do not provide Penn Treebank format parse trees as
output. In this section we outline the adaptation of
the Collins (1999) parsing model to word lattices.
The intended action of the parser is illustrated
in Figure 1, which shows parse trees built directly
upon a word lattice.
3.1 Parameterization
The parameterization of model II of Collins (1999)
is used in our word lattice parser. Parameters are
* tokyo was the couldthatspeculation
unit
yen
the
rise
arise
NN NNP INAUX DT MD VBNNIN
and
in
CC
S
NP
S*
NP VP
*
Figure 1: Example of a partially-parsed word lat-
tice. Different paths through the lattice are simul-
taneously parsed. The example shows two final
parses, one of low probability (S   ) and one of high
probability (S).
maximum likelihood estimates of conditional prob-
abilities ? the probability of some event of inter-
est (e.g., a left-modifier attachment) given a con-
text (e.g., parent non-terminal, distance, headword).
One notable difference between the word lattice
parser and the original implementation of Collins
(1999) is the handling of part-of-speech (POS) tag-
ging of unknown words (words seen fewer than 5
times in training). The conditioning context of the
parsing model parameters includes POS tagging.
Collins (1999) falls back to the POS tagging of Rat-
naparkhi (1996) for words seen fewer than 5 times
in the training corpus. As the tagger of Ratnaparkhi
(1996) cannot tag a word lattice, we cannot back off
to this tagging. We rely on the tag assigned by the
parsing model in all cases.
Edges created by the bottom-up parsing are as-
signed a score which is the product of the inside and
outside probabilities of the Collins (1999) model.
3.2 Parsing Algorithm
The algorithm is a variation of probabilistic
online, bottom-up, left-to-right Cocke-Kasami-
Younger parsing similar to Chappelier and Rajman
(1998).
Our parser produces trees (bottom-up) in a right-
branching manner, using unary extension and binary
adjunction. Starting with a proposed headword, left
modifiers are added first using right-branching, then
right modifiers using left-branching.
Word lattice edges are iteratively added to the
agenda. Complete closure is carried out, and the
next word edge is added to the agenda. This process
is repeated until all word edges are read from the
lattice, and at least one complete parse is found.
Edges are each assigned a score, used to rank
parse candidates. For parsing of strings, the score
for a chart edge is the product of the scores of any
child edges and the score for the creation of the new
edge, as given by the model parameters. This score,
defined solely by the parsing model, will be referred
to as the parser score. The total score for chart
edges for the lattice parsing task is a combination
of the parser score, an acoustic model score, and a
trigram model score. Scaling factors follow those of
(Chelba and Jelinek, 2000; Roark, 2001).
3.3 Smoothing and Pruning
The parameter estimation techniques (smoothing
and back-off) of Collins (1999) are reimplemented.
Additional techniques are required to prune the
search space of possible parses, due to the com-
plexity of the parsing algorithm and the size of the
word lattices. The main technique we employ is a
variation of the beam search of Collins (1999) to
restrict the chart size by excluding low probability
edges. The total score (combined acoustic and lan-
guage model scores) of candidate edges are com-
pared against edge with the same span and cate-
gory. Proposed edges with score outside the beam
are not added to the chart. The drawback to this
process is that we can no longer guarantee that a
model-optimal solution will be found. In practice,
these heuristics have a negative effect on parse accu-
racy, but the amount of pruning can be tuned to bal-
ance relative time and space savings against preci-
sion and recall degradation (Collins, 1999). Collins
(1999) uses a fixed size beam (10   000). We exper-
iment with several variable beam (?b) sizes, where
the beam is some function of a base beam (b) and
the edge width (the number of terminals dominated
by an edge). The base beam starts at a low beam
size and increases iteratively by a specified incre-
ment if no parse is found. This allows parsing to
operate quickly (with a minimal number of edges
added to the chart). However, if many iterations
are required to obtain a parse, the utility of starting
with a low beam and iterating becomes questionable
(Goodman, 1997). The base beam is limited to con-
trol the increase in the chart size. The selection of
the base beam, beam increment, and variable beam
function is governed by the familiar speed/accuracy
trade-off.1 The variable beam function found to al-
low fast convergence with minimal loss of accuracy
is:
?b  blog

w  2  2 
(1)
1Details of the optimization can be found in Collins (2004).
Charniak et al (1998) introduce overparsing as a
technique to improve parse accuracy by continuing
parsing after the first complete parse tree is found.
The technique is employed by Hall and Johnson
(2003) to ensure that early stages of parsing do not
strongly bias later stages. We adapt this idea to
a single stage process. Due to the restrictions of
beam search and thresholds, the first parse found by
the model may not be the model optimal parse (i.e.,
we cannot guarantee best-first search). We there-
fore employ a form of overparsing ? once a com-
plete parse tree is found, we further extend the base
beam by the beam increment and parse again. We
continue this process as long as extending the beam
results in an improved best parse score.
4 Expanding the Measures of Success
Given the task of simply generating a transcription
of speech, WER is a useful and direct way to mea-
sure language model quality for ASR. WER is the
count of incorrect words in hypothesis ?W per word
in the true string W . For measurement, we must as-
sume prior knowledge of W and the best alignment
of the reference and hypothesis strings.2 Errors are
categorized as insertions, deletions, or substitutions.
Word Error Rate 
100 Insertions  Substitutions  DeletionsTotal Words in Correct Transcript (2)
It is important to note that most models ? Mangu
et al (2000) is an innovative exception ? minimize
sentence error. Sentence error rate is the percent-
age of sentences for which the proposed utterance
has at least one error. Models (such as ours) which
optimize prediction of test sentences Wt , generated
by the source, minimize the sentence error. Thus
even though WER is useful practically, it is formally
not the appropriate measure for the commonly used
language models. Unfortunately, as a practical mea-
sure, sentence error rate is not as useful ? it is not
as fine-grained as WER.
Perplexity is another measure of language model
quality, measurable independent of ASR perfor-
mance (Jelinek, 1997). Perplexity is related to the
entropy of the source model which the language
model attempts to estimate.
These measures, while informative, do not cap-
ture success of extraction of high-level information
from speech. Task-specific measures should be used
in tandem with extensional measures such as per-
plexity and WER. Roark (2002), when reviewing
2SCLITE (http://www.nist.gov/speech/
tools/) by NIST is the most commonly used alignment tool.
parsing for speech recognition, discusses a mod-
elling trade-off between producing parse trees and
producing strings. Most models are evaluated ei-
ther with measures of success for parsing or for
word recognition, but rarely both. Parsing mod-
els are difficult to implement as word-predictive
language models due to their complexity. Gener-
ative random sampling is equally challenging, so
the parsing correlate of perplexity is not easy to
measure. Traditional (i.e., n-gram) language mod-
els do not produce parse trees, so parsing metrics
are not useful. However, Roark (2001) argues for
using parsing metrics, such as labelled precision
and recall,3 along with WER, for parsing applica-
tions in ASR. Weighted WER (Weber et al, 1997)
is also a useful measurement, as the most often
ill-recognized words are short, closed-class words,
which are not as important to speech understanding
as phrasal head words. We will adopt the testing
strategy of Roark (2001), but find that measurement
of parse accuracy and WER on the same data set is
not possible given currently available corpora. Use
of weighted WER and development of methods to
simultaneously measure WER and parse accuracy
remain a topic for future research.
5 Experiments
The word lattice parser was evaluated with sev-
eral metrics ? WER, labelled precision and recall,
crossing brackets, and time and space resource us-
age. Following Roark (2001), we conducted evalu-
ations using two experimental sets ? strings and
word lattices. We optimized settings (thresholds,
variable beam function, base beam value) for pars-
ing using development test data consisting of strings
for which we have annotated parse trees.
The parsing accuracy for parsing word lattices
was not directly evaluated as we did not have an-
notated parse trees for comparison. Furthermore,
standard parsing measures such as labelled preci-
sion and recall are not directly applicable in cases
where the number of words differs between the pro-
posed parse tree and the gold standard. Results
show scores for parsing strings which are lower than
the original implementation of Collins (1999). The
WER scores for this, the first application of the
Collins (1999) model to parsing word lattices, are
comparable to other recent work in syntactic lan-
guage modelling, and better than a simple trigram
model trained on the same data.
3Parse trees are commonly scored with the PARSEVAL set
of metrics (Black et al, 1991).
5.1 Parsing Strings
The lattice parser can parse strings by creating a
single-path lattice from the input (all word transi-
tions are assigned an input score of 1.0). The lat-
tice parser was trained on sections 02-21 of the Wall
Street Journal portion of the Penn Treebank (Tay-
lor et al, 2003) Development testing was carried
out on section 23 in order to select model thresh-
olds and variable beam functions. Final testing was
carried out on section 00, and the PARSEVAL mea-
sures (Black et al, 1991) were used to evaluate the
performance.
The scores for our experiments are lower than the
scores of the original implementation of model II
(Collins, 1999). This difference is likely due in part
to differences in POS tagging. Tag accuracy for our
model was 93.2%, whereas for the original imple-
mentation of Collins (1999), model II achieved tag
accuracy of 96.75%. In addition to different tagging
strategies for unknown words, mentioned above, we
restrict the tag-set considered by the parser for each
word to those suggested by a simple first-stage tag-
ger.4 By reducing the tag-set considered by the pars-
ing model, we reduce the search space and increase
the speed. However, the simple tagger used to nar-
row the search also introduces tagging error.
The utility of the overparsing extension can be
seen in Table 1. Each of the PARSEVAL measures
improves when overparsing is used.
5.2 Parsing Lattices
The success of the parsing model as a language
model for speech recognition was measured both
by parsing accuracy (parsing strings with annotated
reference parses), and by WER. WER is measured
by parsing word lattices and comparing the sentence
yield of the highest scoring parse tree to the refer-
ence transcription (using NIST SCLITE for align-
ment and error calculation).5 We assume the pars-
ing performance achieved by parsing strings carries
over approximately to parsing word lattices.
Two different corpora were used in training the
parsing model on word lattices:
  sections 02-21 of the WSJ Penn Treebank (the
same sections as used to train the model for
parsing strings) [1 million words]
4The original implementation (Collins, 1999) of this model
considered all tags for all words.
5To properly model language using a parser, one should sum
parse tree scores for each sentence hypothesis, and choose the
sentence with the best sum of parse tree scores. We choose the
yield of the parse tree with the highest score. Summation is too
computationally expensive given the model ? we do not even
generate all possible parse trees, but instead restrict generation
using dynamic programming.
Exp. OP LP (%) LR (%) CB 0 CB (%)   2 CB (%)
Ref N 88.7 89.0 0.95 65.7 85.6
1 N 79.4 80.6 1.89 46.2 74.5
2 Y 80.8 81.4 1.70 44.3 80.4
Table 1: Results for parsing section 0 (   40 words) of the WSJ Penn Treebank: OP = overparsing, LP/LR
= labelled precision/recall. CB is the average number of crossing brackets per sentence. 0 CB,   2 CB are
the percentage of sentences with 0 or   2 crossing brackets respectively. Ref is model II of (Collins, 1999).
  section ?1987? of the BLLIP corpus (Charniak
et al, 1999) [20 million words]
The BLLIP corpus is a collection of Penn
Treebank-style parses of the three-year (1987-1989)
Wall Street Journal collection from the ACL/DCI
corpus (approximately 30 million words).6 The
parses were automatically produced by the parser
of Charniak (2001). As the memory usage of our
model corresponds directly to the amount of train-
ing data used, we were restricted by available mem-
ory to use only one section (1987) of the total cor-
pus. Using the BLLIP corpus, we expected to get
lower quality parse results due to the higher parse
error of the corpus, when compared to the manually
annotated Penn Treebank. The WER was expected
to improve, as the BLLIP corpus has much greater
lexical coverage.
The training corpora were modified using a utility
by Brian Roark to convert newspaper text to speech-
like text, before being used as training input to the
model. Specifically, all numbers were converted to
words (60  sixty) and all punctuation was re-
moved.
We tested the performance of our parser on the
word lattices from the NIST HUB-1 evaluation task
of 1993. The lattices are derived from a set of
utterances produced from Wall Street Journal text
? the same domain as the Penn Treebank and the
BLLIP training data. The word lattices were previ-
ously pruned to the 50-best paths by Brian Roark,
using the A* decoding of Chelba (2000). The word
lattices of the HUB-1 corpus are directed acyclic
graphs in the HTK Standard Lattice Format (SLF),
consisting of a set of vertices and a set of edges.
Vertices, or nodes, are defined by a time-stamp and
labelled with a word. The set of labelled, weighted
edges, represents the word utterances. A word w is
hypothesized over edge e if e ends at a vertex v la-
belled w. Edges are associated with transition prob-
abilities and are labelled with an acoustic score and
a language model score. The lattices of the HUB-
6The sentences of the HUB-1 corpus are a subset of those
in BLLIP. We removed all HUB-1 sentences from the BLLIP
corpus used in training.
1 corpus are annotated with trigram scores trained
using a 20 thousand word vocabulary and 40 mil-
lion word training sample. The word lattices have a
unique start and end point, and each complete path
through a lattice represents an utterance hypothesis.
As the parser operates in a left-to-right manner, and
closure is performed at each node, the input lattice
edges must be processed in topological order. Input
lattices were sorted before parsing. This corpus has
been used in other work on syntactic language mod-
elling (Chelba, 2000; Roark, 2001; Hall and John-
son, 2003).
The word lattices of the HUB-1 corpus are anno-
tated with an acoustic score, a, and a trigram proba-
bility, lm, for each edge. The input edge score stored
in the word lattice is:
log

Pinput   ? log

a   ? log  lm  (3)
where a is the acoustic score and lm is the trigram
score stored in the lattice. The total edge weight in
the parser is a scaled combination of these scores
with the parser score derived with the model param-
eters:
log

w   ? log

a   ? log  lm   s (4)
where w is the edge weight, and s is the score as-
signed by the parameters of the parsing model. We
optimized performance on a development subset of
test data, yielding ?  1  16 and ?  1.
There is an important difference in the tokeniza-
tion of the HUB-1 corpus and the Penn Treebank
format. Clitics (i.e., he?s, wasn?t) are split
from their hosts in the Penn Treebank (i.e., he ?s,
was n?t), but not in the word lattices. The Tree-
bank format cannot easily be converted into the lat-
tice format, as often the two parts fall into different
parse constituents. We used the lattices modified by
Chelba (2000) in dealing with this problem ? con-
tracted words are split into two parts and the edge
scores redistributed. We followed Hall and John-
son (2003) and used the Treebank tokenization for
measuring the WER. The model was tested with and
without overparsing.
We see from Table 2 that overparsing has little
effect on the WER. The word sequence most easily
parsed by the model (i.e., generating the first com-
plete parse tree) is likely also the word sequence
found by overparsing. Although overparsing may
have little effect on WER, we know from the exper-
iments on strings that overparsing increases parse
accuracy. This introduces a speed-accuracy trade-
off: depending on what type of output is required
from the model (parse trees or strings), the addi-
tional time and resource requirements of overpars-
ing may or may not be warranted.
5.3 Parsing N-Best Lattices vs. N-Best Lists
The application of the model to 50-best word lat-
tices was compared to rescoring the 50-best paths
individually (50-best list parsing). The results are
presented in Table 2.
The cumulative number of edges added to the
chart per word for n-best lists is an order of mag-
nitude larger than for corresponding n-best lattices,
in all cases. As the WERs are similar, we conclude
that parsing n-best lists requires more work than
parsing n-best lattices, for the same result. There-
fore, parsing lattices is more efficient. This is be-
cause common substrings are only considered once
per lattice. The amount of computational savings is
dependent on the density of the lattices ? for very
dense lattices, the equivalent n-best list parsing will
parse common substrings up to n times. In the limit
of lowest density, a lattice may have paths without
overlap, and the number of edges per word would
be the same for the lattice and lists.
5.4 Time and Space Requirements
The algorithms and data structures were designed to
minimize parameter lookup times and memory us-
age by the chart and parameter set (Collins, 2004).
To increase parameter lookup speed, all parameter
values are calculated for all levels of back-off at
training time. By contrast, (Collins, 1999) calcu-
lates parameter values by looking up event counts
at run-time. The implementation was then opti-
mized using a memory and processor profiler and
debugger. Parsing the complete set of HUB-1 lat-
tices (213 sentences, a total of 3,446 words) on av-
erage takes approximately 8 hours, on an Intel Pen-
tium 4 (1.6GHz) Linux system, using 1GB memory.
Memory requirements for parsing lattices is vastly
greater than equivalent parsing of a single sentence,
as chart size increases with the number of divergent
paths in a lattice. Additional analysis of resource
issues can be found in Collins (2004).
5.5 Comparison to Previous Work
The results of our best experiments for lattice- and
list-parsing are compared with previous results in
Table 3. The oracle WER7 for the HUB-1 corpus
is 3.4%. For the pruned 50-best lattices, the oracle
WER is 7.8%. We see that by pruning the lattices
using the trigram model, we already introduce addi-
tional error. Because of the memory usage and time
required for parsing word lattices, we were unable
to test our model on the original ?acoustic? HUB-1
lattices, and are thus limited by the oracle WER of
the 50-best lattices, and the bias introduced by prun-
ing using a trigram model. Where available, we also
present comparative scores of the sentence error rate
(SER) ? the percentage of sentences in the test set
for which there was at least one recognition error.
Note that due to the small (213 samples) size of the
HUB-1 corpus, the differences seen in SER may not
be significant.
We see an improvement in WER for our pars-
ing model alone (?  ?  0) trained on 1 million
words of the Penn Treebank compared to a trigram
model trained on the same data ? the ?Treebank
Trigram? noted in Table 3. This indicates that the
larger context considered by our model allows for
performance improvements over the trigram model
alone. Further improvement is seen with the com-
bination of acoustic, parsing, and trigram scores
(?  1  16   ?  1). However, the combination of
the parsing model (trained on 1M words) with the
lattice trigram (trained on 40M words) resulted in
a higher WER than the lattice trigram alone. This
indicates that our 1M word training set is not suf-
ficient to permit effective combination with the lat-
tice trigram. When the training of the head-driven
parsing model was extended to the BLLIP 1987
corpus (20M words), the combination of models
(?  1  16   ?  1) achieved additional improvement
in WER over the lattice trigram alone.
The current best-performing models, in terms of
WER, for the HUB-1 corpus, are the models of
Roark (2001), Charniak (2001) (applied to n-best
lists by Hall and Johnson (2003)), and the SLM of
Chelba and Jelinek (2000) (applied to n-best lists by
Xu et al (2002)). However, n-best list parsing, as
seen in our evaluation, requires repeated analysis of
common subsequences, a less efficient process than
directly parsing the word lattice.
The reported results of (Roark, 2001) and
(Chelba, 2000) are for parsing models interpolated
with the lattice trigram probabilities. Hall and John-
7The WER of the hypothesis which best matches the true
utterance, i.e., the lowest WER possible given the hypotheses
set.
Training Size Lattice/List OP WER Number of EdgesS D I T (per word)
1M Lattice N 10.4 3.3 1.5 15.2 1788
1M List N 10.4 3.2 1.4 15.0 10211
1M Lattice Y 10.3 3.2 1.4 14.9 2855
1M List Y 10.2 3.2 1.4 14.8 16821
20M Lattice N 9.0 3.1 1.0 13.1 1735
20M List N 9.0 3.1 1.0 13.1 9999
20M Lattice Y 9.0 3.1 1.0 13.1 2801
20M List Y 9.0 3.3 0.9 13.3 16030
Table 2: Results for parsing HUB-1 n-best word lattices and lists: OP = overparsing, S = substutitions (%),
D = deletions (%), I = insertions (%), T = total WER (%). Variable beam function: ?b  b  log  w  2  2  .
Training corpora: 1M = Penn Treebank sections 02-21; 20M = BLLIP section 1987.
Model n-best List/Lattice Training Size WER (%) SER (%)
Oracle (50-best lattice) Lattice 7.8
Charniak (2001) List 40M 11.9
Xu (2002) List 20M 12.3
Roark (2001) (with EM) List 2M 12.7
Hall (2003) Lattice 30M 13.0
Chelba (2000) Lattice 20M 13.0
Current (?  1  16   ?  1) List 20M 13.1 71.0
Current (?  1  16   ?  1) Lattice 20M 13.1 70.4
Roark (2001) (no EM) List 1M 13.4
Lattice Trigram Lattice 40M 13.7 69.0
Current (?  1  16   ?  1) List 1M 14.8 74.3
Current (?  1  16   ?  1) Lattice 1M 14.9 74.0
Current (?  ?  0) Lattice 1M 16.0 75.5
Treebank Trigram Lattice 1M 16.5 79.8
No language model Lattice 16.8 84.0
Table 3: Comparison of WER for parsing HUB-1 words lattices with best results of other works. SER =
sentence error rate. WER = word error rate. ?Speech-like? transformations were applied to all training
corpora. Xu (2002) is an implementation of the model of Chelba (2000) for n-best list parsing. Hall (2003)
is a lattice-parser related to Charniak (2001).
son (2003) does not use the lattice trigram scores
directly. However, as in other works, the lattice
trigram is used to prune the acoustic lattice to the
50 best paths. The difference in WER between
our parser and those of Charniak (2001) and Roark
(2001) applied to word lists may be due in part to the
lower PARSEVAL scores of our system. Xu et al
(2002) report inverse correlation between labelled
precision/recall and WER. We achieve 73.2/76.5%
LP/LR on section 23 of the Penn Treebank, com-
pared to 82.9/82.4% LP/LR of Roark (2001) and
90.1/90.1% LP/LR of Charniak (2000). Another
contributing factor to the accuracy of Charniak
(2001) is the size of the training set ? 20M words
larger than that used in this work. The low WER
of Roark (2001), a top-down probabilistic parsing
model, was achieved by training the model on 1 mil-
lion words of the Penn Treebank, then performing a
single pass of Expectation Maximization (EM) on a
further 1.2 million words.
6 Conclusions
In this work we present an adaptation of the parsing
model of Collins (1999) for application to ASR. The
system was evaluated over two sets of data: strings
and word lattices. As PARSEVAL measures are not
applicable to word lattices, we measured the pars-
ing accuracy using string input. The resulting scores
were lower than that original implementation of the
model. Despite this, the model was successful as a
language model for speech recognition, as measured
by WER and ability to extract high-level informa-
tion. Here, the system performs better than a simple
n-gram model trained on the same data, while si-
multaneously providing syntactic information in the
form of parse trees. WER scores are comparable to
related works in this area.
The large size of the parameter set of this parsing
model necessarily restricts the size of training data
that may be used. In addition, the resource require-
ments currently present a challenge for scaling up
from the relatively sparse word lattices of the NIST
HUB-1 corpus (created in a lab setting by profes-
sional readers) to lattices created with spontaneous
speech in non-ideal conditions. An investigation
into the relevant importance of each parameter for
the speech recognition task may allow a reduction in
the size of the parameter space, with minimal loss of
recognition accuracy. A speedup may be achieved,
and additional training data could be used. Tun-
ing of parameters using EM has lead to improved
WER for other models. We encourage investigation
of this technique for lexicalized head-driven lattice
parsing.
Acknowledgements
This research was funded in part by the Natural Sci-
ences and Engineering Research Council (NSERC)
of Canada. Advice on training and test data was
provided by Keith Hall of Brown University.
References
L. R. Bahl, F. Jelinek, and R. L. Mercer. 1983. A maxi-
mum likelihood approach to continuous speech recog-
nition. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 5:179?190.
E. Black, S. Abney, D. Flickenger, C. Gdaniec, R. Gr-
ishman, P. Harrison, D. Hindle, R. Ingria, F. Jelinek,
J. Klavans, M. Liberman, M. Marcus, S. Roukos,
B. Santorini, and T. Strzalkowski. 1991. A procedure
for quantitatively comparing the syntactic coverage of
English grammars. In Proceedings of Fourth DARPA
Speech and Natural Language Workshop, pages 306?
311.
J.-C. Chappelier and M. Rajman. 1998. A practical
bottom-up algorithm for on-line parsing with stochas-
tic context-free grammars. Technical Report 98-284,
Swiss Federal Institute of Technology, July.
Eugene Charniak, Sharon Goldwater, and Mark John-
son. 1998. Edge-Based Best-First Chart Parsing. In
6th Annual Workshop for Very Large Corpora, pages
127?133.
Eugene Charniak, Don Blaheta, Niyu Ge, Keith Hall,
John Hale, and Mark Johnson. 1999. BLLIP 1987-89
WSJ Corpus Release 1. Linguistic Data Consortium.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the 2000 Conference
of the North American Chapter of the Association
for Computational Linguistics, pages 132?129, New
Brunswick, U.S.A.
Eugene Charniak. 2001. Immediate-head parsing for
language models. In Proceedings of the 39th Annual
Meeting of the ACL.
Ciprian Chelba and Frederick Jelinek. 2000. Structured
language modeling. Computer Speech and Language,
14:283?332.
Ciprian Chelba. 2000. Exploiting Syntactic Structure
for Natural Language Modeling. Ph.D. thesis, Johns
Hopkins University.
Christopher Collins. 2004. Head-Driven Probabilistic
Parsing for Word Lattices. M.Sc. thesis, University of
Toronto.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
Joshua Goodman. 1997. Global thresholding and
multiple-pass parsing. In Proceedings of the 2nd Con-
ference on Empirical Methods in Natural Language
Processing.
Keith Hall and Mark Johnson. 2003. Language mod-
eling using efficient best-first bottom-up parsing. In
Proceedings of the IEEE Automatic Speech Recogni-
tion and Understanding Workshop.
Frederick Jelinek. 1997. Information Extraction From
Speech And Text. MIT Press.
Lidia Mangu, Eric Brill, and Andreas Stolcke. 2000.
Finding consensus in speech recognition: Word error
minimization and other applications of confusion net-
works. Computer Speech and Language, 14(4):373?
400.
Hwee Tou Ng and John Zelle. 1997. Corpus-based
approaches to semantic interpretation in natural lan-
guage processing. AI Magazine, 18:45?54.
A. Ratnaparkhi. 1996. A maximum entropy model for
part-of-speech tagging. In Conference on Empirical
Methods in Natural Language Processing, May.
Mosur K. Ravishankar. 1997. Some results on search
complexity vs accuracy. In DARPA Speech Recogni-
tion Workshop, pages 104?107, February.
Brian Roark. 2001. Robust Probabilistic Predictive Syn-
tactic Processing: Motivations, Models, and Applica-
tions. Ph.D. thesis, Brown University.
Brian Roark. 2002. Markov parsing: Lattice rescoring
with a statistical parser. In Proceedings of the 40th
Annual Meeting of the ACL, pages 287?294.
Ann Taylor, Mitchell Marcus, and Beatrice Santorini,
2003. The Penn TreeBank: An Overview, chapter 1.
Kluwer, Dordrecht, The Netherlands.
Hans Weber, Jo?rg Spilker, and Gu?nther Go?rz. 1997.
Parsing n best trees from a word lattice. Kunstliche
Intelligenz, pages 279?288.
Peng Xu, Ciprian Chelba, and Frederick Jelinek. 2002.
A study on richer syntactic dependencies in structured
language modeling. In Proceedings of the 40th An-
nual Meeting of the ACL, pages 191?198.
Balancing Clarity and Efficiency in Typed Feature Logic through Delaying
Gerald Penn
University of Toronto
10 King?s College Rd.
Toronto M5S 3G4
Canada
gpenn@cs.toronto.edu
Abstract
The purpose of this paper is to re-examine the bal-
ance between clarity and efficiency in HPSG design,
with particular reference to the design decisions
made in the English Resource Grammar (LinGO,
1999, ERG). It is argued that a simple generaliza-
tion of the conventional delay statements used in
logic programming is sufficient to restore much of
the functionality and concomitant benefit that the
ERG elected to forego, with an acceptable although
still perceptible computational cost.
1 Motivation
By convention, current HPSGs consist, at the very
least, of a deductive backbone of extended phrase
structure rules, in which each category is a descrip-
tion of a typed feature structure (TFS), augmented
with constraints that enforce the principles of gram-
mar. These principles typically take the form of
statements, ?for all TFSs, ? holds,? where ? is
usually an implication. Historically, HPSG used
a much richer set of formal descriptive devices,
however, mostly on analogy to developments in
the use of types and description logics in program-
ming language theory (A??t-Kac?i, 1984), which had
served as the impetus for HPSG?s invention (Pol-
lard, 1998). This included logic-programming-style
relations (Ho?hfeld and Smolka, 1988), a powerful
description language in which expressions could de-
note sets of TFSs through the use of an explicit
disjunction operator, and the full expressive power
of implications, in which antecedents of the above-
mentioned ? principles could be arbitrarily com-
plex.
Early HPSG-based natural language processing
systems faithfully supported large chunks of this
richer functionality, in spite of their inability to han-
dle it efficiently ? so much so that when the de-
signers of the ERG set out to select formal descrip-
tive devices for their implementation with the aim
of ?balancing clarity and efficiency,? (Flickinger,
2000), they chose to include none of these ameni-
ties. The ERG uses only phrase-structure rules and
type-antecedent constraints, pushing all would-be
description-level disjunctions into its type system or
rules. In one respect, this choice was successful, be-
cause it did at least achieve a respectable level of
efficiency. But the ERG?s selection of functionality
has acquired an almost liturgical status within the
HPSG community in the intervening seven years.
Keeping this particular faith, moreover, comes at a
considerable cost in clarity, as will be argued below.
This paper identifies what it is precisely about
this extra functionality that we miss (modularity,
Section 2), determines what it would take at a mini-
mum computationally to get it back (delaying, Sec-
tion 3), and attempts to measure exactly how much
that minimal computational overhead would cost
(about 4 ?s per delay, Section 4). This study has
not been undertaken before; the ERG designers?
decision was based on largely anecdotal accounts
of performance relative to then-current implemen-
tations that had not been designed with the inten-
tion of minimizing this extra cost (indeed, the ERG
baseline had not yet been devised).
2 Modularity: the cost in clarity
Semantic types and inheritance serve to organize
the constraints and overall structure of an HPSG
grammar. This is certainly a familiar, albeit vague
justification from programming languages research,
but the comparison between HPSG and modern
programming languages essentially ends with this
statement.
Programming languages with inclusional poly-
morphism (subtyping) invariably provide functions
or relations and allow these to be reified as meth-
ods within user-defined subclasses/subtypes. In
HPSG, however, values of features must necessar-
ily be TFSs themselves, and the only method (im-
plicitly) provided by the type signature to act on
these values is unification. In the absence of other
methods and in the absence of an explicit disjunc-
tion operator, the type signature itself has the re-
sponsibility of not only declaring definitional sub-
fin-wh-fill-rel-clinf-wh-fill-rel-cl red-rel-cl simp-inf-rel-cl
fin-hd-fill-ph inf-hd-fill-ph
wh-rel-cl non-wh-rel-cl hd-fill-ph hd-comp-ph
inter-cl rel-cl hd-adj-ph hd-nexus-ph
clause non-hd-ph hd-ph
headed phrase
phrase
Figure 1: Relative clauses in the ERG (partial).
class relationships, but expressing all other non-
definitional disjunctions in the grammar (as subtyp-
ing relationships). It must also encode the neces-
sary accoutrements for implementing all other nec-
essary means of combination as unification, such as
difference lists for appending lists, or the so-called
qeq constraints of Minimal Recursion Semantics
(Copestake et al, 2003) to encode semantic embed-
ding constraints.
Unification, furthermore, is an inherently non-
modular, global operation because it can only be
defined relative to the structure of the entire par-
tial order of types (as a least upper bound). Of
course, some partial orders are more modularizable
than others, but legislating the global form that type
signatures must take on is not an easy property to
enforce without more local guidance.
The conventional wisdom in programming lan-
guages research is indeed that types are responsi-
ble for mediating the communication between mod-
ules. A simple type system such as HPSG?s can thus
only mediate very simple communication. Modern
programming languages incorporate some degree of
parametric polymorphism, in addition to subtyping,
in order to accommodate more complex communi-
cation. To date, HPSG?s use of parametric types has
been rather limited, although there have been some
recent attempts to apply them to the ERG (Penn and
Hoetmer, 2003). Without this, one obtains type sig-
natures such as Figure 1 (a portion of the ERG?s for
relative clauses), in which both the semantics of the
subtyping links themselves (normally, subset inclu-
sion) and the multi-dimensionality of the empirical
domain?s analysis erode into a collection of arbi-
trary naming conventions that are difficult to vali-
date or modify.
A more avant-garde view of typing in program-
ming languages research, inspired by the Curry-
Howard isomorphism, is that types are equivalent
to relations, which is to say that a relation can me-
diate communication between modules through its
arguments, just as a parametric type can through its
parameters. The fact that we witness some of these
mediators as types and others as relations is sim-
ply an intensional reflection of how the grammar
writer thinks of them. In classical HPSG, relations
were generally used as goals in some proof resolu-
tion strategy (such as Prolog?s SLD resolution), but
even this has a parallel in the world of typing. Using
the type signature and principles of Figure 2, for ex-
appendbase appendrec
Arg1: e list Arg1:ne list
Junk:append
append
Arg1: list
Arg2: list
Arg3: list
?
appendbase =? Arg2 : L ? Arg3 : L.
appendrec =? Arg1 : [H |L1] ?
Arg2 : L2 ? Arg3 : [H |L3] ?
Junk : (append ? A1 : L1 ?
A2 : L2 ? Arg3 : L3).
Figure 2: Implementing SLD resolution over the ap-
pend relation as sort resolution.
ample, we can perform proof resolution by attempt-
ing to sort resolve every TFS to a maximally spe-
cific type. This is actually consistent with HPSG?s
use of feature logic, although most TFS-based NLP
systems do not sort resolve because type inference
under sort resolution is NP-complete (Penn, 2001).
Phrase structure rules, on the other hand, while
they can be encoded inside a logic programming re-
lation, are more naturally viewed as algebraic gen-
erators. In this respect, they are more similar to
the immediate subtyping declarations that grammar
writers use to specify type signatures ? both chart
parsing and transitive closure are instances of all-
source shortest-path problems on the same kind of
algebraic structure, called a closed semi-ring. The
only notion of modularity ever proven to hold of
phrase structure rule systems (Wintner, 2002), fur-
thermore, is an algebraic one.
3 Delaying: the missing link of
functionality
If relations are used in the absence of recursive data
structures, a grammar could be specified using rela-
tions, and the relations could then be unfolded off-
line into relation-free descriptions. In this usage,
relations are just macros, and not at all inefficient.
Early HPSG implementations, however, used quite
a lot of recursive structure where it did not need to
be, and the structures they used, such as lists, buried
important data deep inside substructures that made
parsing much slower. Provided that grammar writ-
ers use more parsimonious structures, which is a
good idea even in the absence of relations, there is
nothing wrong with the speed of logic programming
relations (Van Roy, 1990).
Recursive datatypes are also prone to non-
termination problems, however. This can happen
when partially instantiated and potentially recur-
sive data structures are submitted to a proof reso-
lution procedure which explores the further instan-
tiations of these structures too aggressively. Al-
though this problem has received significant atten-
tion over the last fifteen years in the constraint logic
programming (CLP) community, no true CLP im-
plementation yet exists for the logic of typed fea-
ture structures (Carpenter, 1992, LTFS). Some as-
pects of general solution strategies, including in-
cremental entailment simplification (A??t-Kaci et al,
1992), deterministic goal expansion (Doerre, 1993),
and guard statements for relations (Doerre et al,
1996) have found their way into the less restrictive
sorted feature constraint systems from which LTFS
descended. The CUF implementation (Doerre et al,
1996), notably, allowed for delay statements to be
attached to relation definitions, which would wait
until each argument was at least as specific as some
variable-free, disjunction-free description before re-
solving.
In the remainder of this section, a method is
presented for reducing delays on any inequation-
free description, including variables and disjunc-
tions, to the SICStus Prolog when/2 primitive
(Sections 3.4). This method takes full advan-
tage of the restrictions inherent to LTFS (Sec-
tion 3.1) to maximize run-time efficiency. In ad-
dition, by delaying calls to subgoals individually
rather than the (universally quantified) relation defi-
nitions themselves,1 we can also use delays to post-
pone non-deterministic search on disjunctive de-
scriptions (Section 3.3) and to implement complex-
antecedent constraints (Section 3.2). As a result,
this single method restores all of the functionality
we were missing.
For simplicity, it will be assumed that the target
language of our compiler is Prolog itself. This is in-
consequential to the general proposal, although im-
plementing logic programs in Prolog certainly in-
volves less effort.
1Delaying relational definitions is a subcase of this func-
tionality, which can be made more accessible through some ex-
tra syntactic sugar.
3.1 Restrictions inherent to LTFS
LTFS is distinguished by its possession of appro-
priateness conditions that mediate the occurrence of
features and types in these records. Appropriateness
conditions stipulate, for every type, a finite set of
features that can and must have values in TFSs of
that type. This effectively forces TFSs to be finite-
branching terms with named attributes. Appropri-
ateness conditions also specify a type to which the
value of an appropriate feature is restricted (a value
restriction). These conditions make LTFS very con-
venient for linguistic purposes because the combi-
nation of typing with named attributes allows for a
very terse description language that can easily make
reference to a sparse amount of information in what
are usually extremely large structures/records:
Definition: Given a finite meet semi-lattice of types,
Type, a fixed finite set of features, Feat, and a count-
able set of variables, Var, ? is the least set of de-
scriptions that contains:
? v, v ? Var ,
? ?, ? ? Type ,
? F : ?, F ? Feat , ? ? ?,
? ?1 ? ?2, ?1, ?2 ? ?, and
? ?1 ? ?2, ?1, ?2 ? ?.
A nice property of this description language is
that every non-disjunctive description with a non-
empty denotation has a unique most general TFS in
its denotation. This is called its most general satis-
fier.
We will assume that appropriateness guarantees
that there is a unique most general type, Intro(F)
to which a given feature, F, is appropriate. This is
called unique feature introduction. Where unique
feature introduction is not assumed, it can be added
automatically in O(F ?T ) time, where F is the num-
ber of features and T is the number of types (Penn,
2001). Meet semi-latticehood can also be restored
automatically, although this involves adding expo-
nentially many new types in the worst case.
3.2 Complex Antecedent Constraints
It will be assumed here that all complex-antecedent
constraints are implicitly universally quantified, and
are of the form:
? =? (? ? ?)
where ?, ? are descriptions from the core descrip-
tion language, ?, and ? is drawn from a definite
clause language of relations, whose arguments are
also descriptions from ?. As mentioned above, the
ERG uses the same form, but where ? can only be a
type description, ? , and ? is the trivial goal, true.
The approach taken here is to allow for arbitrary
antecedents, ?, but still to interpret the implica-
tions of principles using subsumption by ?, i.e., for
every TFS (the implicit universal quantification is
still there), either the consequent holds, or the TFS
is not subsumed by the most general satisfier of
?. The subsumption convention dates back to the
TDL (Krieger and Scha?fer, 1994) and ALE (Car-
penter and Penn, 1996) systems, and has earlier an-
tecedents in work that applied lexical rules by sub-
sumption (Krieger and Nerbone, 1991). The Con-
Troll constraint solver (Goetz and Meurers, 1997)
attempted to handle complex antecedents, but used
a classical interpretation of implication and no de-
ductive phrase-structure backbone, which created a
very large search space with severe non-termination
problems.
Within CLP more broadly, there is some related
work on guarded constraints (Smolka, 1994) and on
inferring guards automatically by residuation of im-
plicational rules (Smolka, 1991), but implicit uni-
versal quantification of all constraints seems to be
unique to linguistics. In most CLP, constraints on a
class of terms or objects must be explicitly posted to
a store for each member of that class. If a constraint
is not posted for a particular term, then it does not
apply to that term.
The subsumption-based approach is sound with
respect to the classical interpretation of implication
for those principles where the classical interpreta-
tion really is the correct one. For completeness,
some additional resolution method (in the form of
a logic program with relations) must be used. As is
normally the case in CLP, deductive search is used
alongside constraint resolution.
Under such assumptions, our principles can be
converted to:
trigger(?) =? v ? whenfs((v = ?), ((v = ?)??))
Thus, with an implementation of type-antecedent
constraints and an implementation of whenfs/2
(Section 3.3), which delays the goal in its second
argument until v is subsumed by (one of) the most
general satisfier(s) of description ?, all that remains
is a method for finding the trigger, the most effi-
cient type antecedent to use, i.e., the most general
one that will not violate soundness. trigger(?) can
be defined as follows:
? trigger(v) = ?,
? trigger(?) = ? ,
? trigger(F : ?) = Intro(F),
? trigger(?1??2) = trigger(?1)ttrigger(?2),
and
? trigger(?1??2) = trigger(?1)utrigger(?2),
where t and u are respectively unification and gen-
eralization in the type semi-lattice.
In this and the next two subsections, we can use
Figure 3 as a running example of the various stages
of compilation of a typical complex-antecedent con-
straint, namely the Finiteness Marking Principle for
German (1). This constraint is stated relative to the
signature shown in Figure 4. The description to the
left of the arrow in Figure 3 (1) selects TFSs whose
substructure on the path SYNSEM:LOC:CAT satisfies
two requirements: its HEAD value has type verb,
and its MARKING value has type fin. The princi-
ple says that every TFS that satisfies that descrip-
tion must also have a SYNSEM: LOC: CAT: HEAD:
VFORM value of type bse.
To find the trigger in Figure 3 (1), we can observe
that the antecedent is a feature value description
(F:?), so the trigger is Intro(SYNSEM), the unique
introducer of the SYNSEM feature, which happens
to be the type sign. We can then transform this con-
straint as above (Figure 3 (2)). The cons and goal
operators in (2)?(5) are ALE syntax, used respec-
tively to separate the type antecedent of a constraint
from the description component of the consequent
(in this case, just the variable, X), and to separate
the description component of the consequent from
its relational attachment. We know that any TFS
subsumed by the original antecedent will also be
subsumed by the most general TFS of type sign, be-
cause sign introduces SYNSEM.
3.3 Reducing Complex Conditionals
Let us now implement our delay predicate,
whenfs(V=Desc,Goal). Without loss of
generality, it can be assumed that the first argument
is actually drawn from a more general conditional
language, including those of the form Vi = Desci
closed under conjunction and disjunction. It can
also be assumed that the variables of each Desc i are
distinct. Such a complex conditional can easily be
converted into a normal form in which each atomic
conditional contains a non-disjunctive description.
Conjunction and disjunction of atomic conditionals
then reduce as follows (using the Prolog convention
of comma for AND and semi-colon for OR):
whenfs((VD1,VD2),Goal) :-
whenfs(VD1,whenfs(VD2,Goal)).
whenfs((VD1;VD2),Goal) :-
whenfs(VD1,(Trigger = 0 -> Goal
; true)),
whenfs(VD2,(Trigger = 1 -> Goal
; true)).
The binding of the variable Trigger is necessary
to ensure that Goal is only resolved once in case the
(1) synsem:loc:cat:(head:verb,marking:fin) =? synsem:loc:cat:head:vform:bse.
(2) sign cons X goal
whenfs((X=synsem:loc:cat:(head:verb,marking:fin)),
(X=synsem:loc:cat:head:vform:bse)).
(3) sign cons X goal
whentype(sign,X,(farg(synsem,X,SynVal),
whentype(synsem,SynVal,(farg(loc,SynVal,LocVal),
whentype(local,LocVal,(farg(cat,LocVal,CatVal),
whenfs((CatVal=(head:verb,marking:fin)),
(X=synsem:loc:cat:head:vform:bse)))))))).
(4) sign cons X goal
(whentype(sign,X,(farg(synsem,X,SynVal),
whentype(synsem,SynVal,(farg(loc,SynVal,LocVal),
whentype(local,LocVal,(farg(cat,LocVal,CatVal),
whentype(category,CatVal,(farg(head,CatVal,HdVal),
whentype(verb,HdVal,
whentype(category,CatVal,(farg(marking,CatVal,MkVal),
whentype(fin,MkVal,
(X=synsem:loc:cat:head:vform:bse)))))))))))))).
(5) sign cons X goal
(farg(synsem,X,SynVal),
farg(loc,SynVal,LocVal),
farg(cat,LocVal,CatVal),
farg(head,CatVal,HdVal),
whentype(verb,HdVal,(farg(marking,CatVal,MkVal),
whentype(fin,MkVal,
(X=synsem:loc:cat:head:vform:bse))))).
(6) sign(e list( ),e list( ),SynVal,DelayVar)
(7) whentype(Type,FS,Goal) :-
functor(FS,CurrentType,Arity),
(sub type(Type,CurrentType) -> call(Goal)
; arg(Arity,FS,DelayVar), whentype(Type,DelayVar,Goal)).
Figure 3: Reduction stages for the Finiteness Marking Principle.
bse ind fin inf verb noun
vform marking head
VFORM:vform
sign
QRETR:list
QSTORE:list
SYNSEM:synsem
synsem
LOC:local
category
HEAD:head
MARKING:marking
local
CAT:category
?
Figure 4: Part of the signature underlying the constraint in Figure 3.
goals for both conditionals eventually unsuspend.
For atomic conditionals, we must thread two
extra arguments, VsIn, and VsOut, which track
which variables have been seen so far. Delaying
on atomic type conditionals is implemented by a
special whentype/3 primitive (Section 3.4), and
feature descriptions reduce using unique feature
introduction:
whenfs(V=T,Goal,Vs,Vs) :-
type(T) -> whentype(T,V,Goal).
whenfs(V=(F:Desc),Goal,VsIn,VsOut):-
unique introducer(F,Intro),
whentype(Intro,V,
(farg(F,V,FVal),
whenfs(FVal=Desc,Goal,VsIn,
VsOut))).
farg(F,V,FVal) binds FVal to the argument
position of V that corresponds to the feature F once
V has been instantiated to a type for which F is
appropriate.
In the variable case, whenfs/4 simply binds the
variable when it first encounters it, but subsequent
occurrences of that variable create a suspension
using Prolog when/2, checking for identity with
the previous occurrences. This implements a
primitive delay on structure sharing (Section 3.4):
whenfs(V=X,Goal,VsIn,VsOut) :-
var(X),
(select(VsIn,X,VsOut)
-> % not first X - wait
when(?=(V,X),
((V==X) -> call(Goal) ; true))
; % first X - bind
VsOut=VsIn,V=X,call(Goal)).
In practice, whenfs/2 can be partially evalu-
ated by a compiler. In the running example, Fig-
ure 3, we can compile the whenfs/2 subgoal in
(2) into simpler whentype/2 subgoals, that delay
until X reaches a particular type. The second case of
whenfs/4 tells us that this can be achieved by suc-
cessively waiting for the types that introduce each
of the features, SYNSEM, LOC, and CAT. As shown
in Figure 4, those types are sign, synsem and local,
respectively (Figure 3 (3)).
The description that CatVal is suspended on is
a conjunction, so we successively suspend on each
conjunct. The type that introduces both HEAD and
MARKING is category (4). In practice, static anal-
ysis can greatly reduce the complexity of the re-
sulting relational goals. In this case, static analy-
sis of the type system tells us that all four of these
whentype/2 calls can be eliminated (5), since X
must be a sign in this context, synsem is the least
appropriate type of any SYNSEM value, local is the
least appropriate type of any LOC value, and cate-
gory is the least appropriate type of any CAT value.
3.4 Primitive delay statements
The two fundamental primitives typically provided
for Prolog terms, e.g., by SICStus Prolog when/2,
are: (1) suspending until a variable is instantiated,
and (2) suspending until two variables are equated
or inequated. The latter corresponds exactly to
structure-sharing in TFSs, and to shared variables
in descriptions; its implementation was already dis-
cussed in the previous section. The former, if car-
ried over directly, would correspond to delaying un-
til a variable is promoted to a type more specific
than ?, the most general type in the type semi-
lattice. There are degrees of instantiation in LTFS,
however, corresponding to long subtyping chains
that terminate in ?. A more general and useful
primitive in a typed language with such chains is
suspending until a variable is promoted to a partic-
ular type. whentype(Type,X,Goal), i.e., de-
laying subgoal Goal until variable X reaches Type,
is then the non-universally-quantified cousin of the
type-antecedent constraints that are already used in
the ERG.
How whentype(Type,X,Goal) is imple-
mented depends on the data structure used for TFSs,
but in Prolog they invariably use the underlying Pro-
log implementation of when/2. In ALE, for ex-
ample, TFSs are represented with reference chains
that extend every time their type changes. One
can simply wait for a variable position at the end
of this chain to be instantiated, and then com-
pare the new type to Type. Figure 3 (6) shows
a schematic representation of a sign-typed TFS
with SYNSEM value SynVal, and two other ap-
propriate feature values. Acting upon this as its
second argument, the corresponding definition of
whentype(Type,X,Goal) in Figure 3 (7) de-
lays on the variable in the extra, fourth argument
position. This variable will be instantiated to a sim-
ilar term when this TFS promotes to a subtype of
sign.
As described above, delaying until the antecedent
of the principle in Figure 3 (1) is true or false ul-
timately reduces to delaying until various feature
values attain certain types using whentype/3. A
TFS may not have substructures that are specific
enough to determine whether an antecedent holds
or not. In this case, we must wait until it is known
whether the antecedent is true or false before ap-
plying the consequent. If we reach a deadlock,
where several constraints are suspended on their
antecedents, then we must use another resolution
method to begin testing more specific extensions of
the TFS in turn. The choice of these other methods
characterizes a true CLP solution for LTFS, all of
which are enabled by the method presented in this
paper. In the case of the signature in Figure 4, one
of these methods may test whether a marking-typed
substructure is consistent with either fin or inf. If it
is consistent with fin, then this branch of the search
may unsuspend the Finiteness Marking Principle on
a sign-typed TFS that contains this substructure.
4 Measuring the cost of delaying
How much of a cost do we pay for using delay-
ing? In order to answer this question definitively,
we would need to reimplement a large-scale gram-
mar which was substantially identical in every way
to the ERG but for its use of delay statements. The
construction of such a grammar is outside the scope
of this research programme, but we do have access
to MERGE,2 which was designed to have the same
extensional coverage of English as the ERG. Inter-
nally, the MERGE is quite unlike the ERG. Its TFSs
are far larger because each TFS category carries in-
side it the phrase structure daughters of the rule that
created it. It also has far fewer types, more fea-
ture values, a heavy reliance on lists, about a third
as many phrase structure rules with daughter cate-
gories that are an average of 32% larger, and many
more constraints. Because of these differences, this
version of MERGE runs on average about 300 times
slower than the ERG.
On the other hand, MERGE uses delaying for all
three of the purposes that have been discussed in this
paper: complex antecedents, explicit whenfs/2
calls to avoid non-termination problems, and ex-
plicit whenfs/2 calls to avoid expensive non-
deterministic searches. While there is currently no
delay-free grammar to compare it to, we can pop
open the hood on our implementation and mea-
sure delaying relative to other system functions on
MERGE with its test suite. The results are shown in
Figure 5. These results show that while the per call
per sent.
avg. avg. %
Function ?s avg. parse
/ call # calls time
PS rules 1458 410 0.41
Chart access 13.3 13426 0.12
Relations 4.0 1380288 1.88
Delays 2.6 3633406 6.38
Path compression 2.0 955391 1.31
Constraints 1.6 1530779 1.62
Unification 1.5 37187128 38.77
Dereferencing 0.5 116731777 38.44
Add type MGSat 0.3 5131391 0.97
Retrieve feat. val. 0.02 19617973 0.21
Figure 5: Run-time allocation of functionality in
MERGE. Times were measured on an HP Omni-
book XE3 laptop with an 850MHz Pentium II pro-
cessor and 512MB of RAM, running SICStus Pro-
log 3.11.0 on Windows 98 SE.
cost of delaying is on a par with other system func-
tions such as constraint enforcement and relational
goal resolution, delaying takes between three and
five times more of the percentage of sentence parse
2The author sincerely thanks Kordula DeKuthy and Det-
mar Meurers for their assistance in providing the version of
MERGE (0.9.6) and its test suite (1347 sentences, average word
length 6.3, average chart size 410 edges) for this evaluation.
MERGE is still under development.
time because it is called so often. This reflects, in
part, design decisions of the MERGE grammar writ-
ers, but it also underscores the importance of having
an efficient implementation of delaying for large-
scale use. Even if delaying could be eliminated en-
tirely from this grammar at no cost, however, a 6%
reduction in parsing speed would not, in the present
author?s view, warrant the loss of modularity in a
grammar of this size.
5 Conclusion
It has been shown that a simple generalization of
conventional delay statements to LTFS, combined
with a subsumption-based interpretation of impli-
cational constraints and unique feature introduction
are sufficient to restore much of the functionality
and concomitant benefit that has been routinely sac-
rificed in HPSG in the name of parsing efficiency.
While a definitive measurement of the computa-
tional cost of this functionality has yet to emerge,
there is at least no apparent indication from the
experiments that we can conduct that disjunction,
complex antecedents and/or a judicious use of recur-
sion pose a significant obstacle to tractable grammar
design when the right control strategy (CLP with
subsumption testing) is adopted.
References
H. A??t-Kaci, A. Podelski, and G. Smolka. 1992.
A feature-based constraint system for logic pro-
gramming with entailment. In Proceedings of
the International Conference on Fifth Generation
Computer Systems.
H. A??t-Kac?i. 1984. A Lattice-theoretic Approach to
Computation based on a Calculus of Partially Or-
dered Type Structures. Ph.D. thesis, University of
Pennsylvania.
B. Carpenter and G. Penn. 1996. Compiling typed
attribute-value logic grammars. In H. Bunt and
M. Tomita, editors, Recent Advances in Parsing
Technologies, pages 145?168. Kluwer.
B. Carpenter. 1992. The Logic of Typed Feature
Structures. Cambridge.
A. Copestake, D. Flickinger, C. Pollard, and I. Sag.
2003. Minimal Recursion Semantics: An intro-
duction. Journal submission, November 2003.
J. Doerre, M. Dorna, J. Junger, and K. Schneider,
1996. The CUF User?s Manual. IMS Stuttgart,
2.0 edition.
J. Doerre. 1993. Generalizing Earley deduction
for constraint-based grammars. Technical Report
R1.2.A, DYANA Deliverable.
D. Flickinger. 2000. On building a more efficient
grammar by exploiting types. Natural Language
Engineering, 6(1):15?28.
T. Goetz and W.D. Meurers. 1997. Interleav-
ing universal principles and relational constraints
over typed feature logic. In Proceedings of the
35th ACL / 8th EACL, pages 1?8.
M. Ho?hfeld and G. Smolka. 1988. Definite re-
lations over constraint languages. LILOG Re-
port 53, IBM Deutschland.
H.-U. Krieger and J. Nerbone. 1991. Feature-based
inheritance networks for computational lexicons.
In Proceedings of the ACQUILEX Workshop on
Default Inheritance in the Lexicon, number 238
in University of Cambridge, Computer Labora-
tory Technical Report.
H.-U. Krieger and U. Scha?fer. 1994. TDL ?
a type description language for HPSG part
1: Overview. Technical Report RR-94-37,
Deutsches Forschungszentrum fu?r Ku?nstliche In-
telligenz (DFKI), November.
LinGO. 1999. The LinGO grammar and lexicon.
Available on-line at http://lingo.stanford.edu.
G. Penn and K. Hoetmer. 2003. In search of epis-
temic primitives in the english resource grammar.
In Proceedings of the 10th International Confer-
ence on Head-driven Phrase Structure Grammar,
pages 318?337.
G. Penn. 2001. Tractability and structural closures
in attribute logic signatures. In Proceedings of
the 39th ACL, pages 410?417.
C. J. Pollard. 1998. Personal communiciation to the
author.
G. Smolka. 1991. Residuation and guarded rules
for constraint logic programming. Technical Re-
port RR-91-13, DFKI.
G. Smolka. 1994. A calculus for higher-order
concurrent constraint programming with deep
guards. Technical Report RR-94-03, DFKI.
P. Van Roy. 1990. Can Logic Programming Exe-
cute as Fast as Imperative Programming? Ph.D.
thesis, University of California, Berkeley.
S. Wintner. 2002. Modular context-free grammars.
Grammars, 5(1):41?63.
Proceedings of ACL-08: HLT, pages 470?478,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
A Critical Reassessment of Evaluation Baselines for Speech Summarization
Gerald Penn and Xiaodan Zhu
University of Toronto
10 King?s College Rd.
Toronto M5S 3G4 CANADA
 
gpenn,xzhu  @cs.toronto.edu
Abstract
We assess the current state of the art in speech
summarization, by comparing a typical sum-
marizer on two different domains: lecture data
and the SWITCHBOARD corpus. Our re-
sults cast significant doubt on the merits of this
area?s accepted evaluation standards in terms
of: baselines chosen, the correspondence of
results to our intuition of what ?summaries?
should be, and the value of adding speech-
related features to summarizers that already
use transcripts from automatic speech recog-
nition (ASR) systems.
1 Problem definition and related literature
Speech is arguably the most basic, most natural form
of human communication. The consistent demand
for and increasing availability of spoken audio con-
tent on web pages and other digital media should
therefore come as no surprise. Along with this avail-
ability comes a demand for ways to better navigate
through speech, which is inherently more linear or
sequential than text in its traditional delivery.
Navigation connotes a number of specific tasks,
including search, but also browsing (Hirschberg et
al., 1999) and skimming, which can involve far
more analysis and manipulation of content than the
spoken document retrieval tasks of recent NIST
fame (1997 2000). These would include time com-
pression of the speech signal and/or ?dichotic? pre-
sentations of speech, in which a different audio track
is presented to either ear (Cherry and Taylor, 1954;
Ranjan et al, 2006). Time compression of speech,
on the other hand, excises small slices of digitized
speech data out of the signal so that the voices speak
all of the content but more quickly. The excision
can either be fixed rate, for which there have been
a number of experiments to detect comprehension
limits, or variable rate, where the rate is determined
by pause detection and shortening (Arons, 1992),
pitch (Arons, 1994) or longer-term measures of lin-
guistic salience (Tucker and Whittaker, 2006). A
very short-term measure based on spectral entropy
can also be used (Ajmal et al, 2007), which has
the advantage that listeners cannot detect the vari-
ation in rate, but they nevertheless comprehend bet-
ter than fixed-rate baselines that preserve pitch pe-
riods. With or without variable rates, listeners can
easily withstand a factor of two speed-up, but Likert
response tests definitively show that they absolutely
hate doing it (Tucker and Whittaker, 2006) relative
to word-level or utterance-level excisive methods,
which would include the summarization-based strat-
egy that we pursue in this paper.
The strategy we focus on here is summariza-
tion, in its more familiar construal from compu-
tational linguistics and information retrieval. We
view it as an extension of the text summarization
problem in which we use automatically prepared,
imperfect textual transcripts to summarize speech.
Other details are provided in Section 2.2. Early
work on speech summarization was either domain-
restricted (Kameyama and Arima, 1994), or prided
itself on not using ASR at all, because of its unreli-
ability in open domains (Chen and Withgott, 1992).
Summaries of speech, however, can still be delivered
audially (Kikuchi et al, 2003), even when (noisy)
transcripts are used.
470
The purpose of this paper is not so much to in-
troduce a new way of summarizing speech, as to
critically reappraise how well the current state of
the art really works. The earliest work to con-
sider open-domain speech summarization seriously
from the standpoint of text summarization technol-
ogy (Valenza et al, 1999; Zechner and Waibel,
2000) approached the task as one of speech tran-
scription followed by text summarization of the re-
sulting transcript (weighted by confidence scores
from the ASR system), with the very interesting re-
sult that transcription and summarization errors in
such systems tend to offset one another in overall
performance. In the years following this work, how-
ever, some research by others on speech summa-
rization (Maskey and Hirschberg, 2005; Murray et
al., 2005; Murray et al, 2006, inter alia) has fo-
cussed de rigueur on striving for and measuring the
improvements attainable over the transcribe-then-
summarize baseline with features available from
non-transcriptional sources (e.g., pitch and energy
of the acoustic signal) or those, while evident in tex-
tual transcripts, not germane to texts other than spo-
ken language transcripts (e.g., speaker changes or
question-answer pair boundaries).
These ?novel? features do indeed seem to help,
but not by nearly as much as some of this recent
literature would suggest. The experiments and the
choice of baselines have largely been framed to il-
luminate the value of various knowledge sources
(?prosodic features,? ?named entity features? etc.),
rather than to optimize performance per se ? al-
though the large-dimensional pattern recognition al-
gorithms and classifiers that they use are inappropri-
ate for descriptive hypothesis testing.
First, most of the benefit attained by these novel
sources can be captured simply by measuring the
lengths of candidate utterances. Only one paper we
are aware of (Christensen et al, 2004) has presented
the performance of length on its own, although the
objective there was to use length, position and other
simple textual feature baselines (no acoustics) to
distinguish the properties of various genres of spo-
ken audio content, a topic that we will return to in
Section 2.1.1 Second, maximal marginal relevance
1Length features are often mentioned in the text of other
work as the most beneficial single features in more hetero-
(MMR) has also fallen by the wayside, although it
too performs very well. Again, only one paper that
we are aware of (Murray et al, 2005) provides an
MMR baseline, and there MMR significantly out-
performs an approach trained on a richer collection
of features, including acoustic features. MMR was
the method of choice for utterance selection in Zech-
ner and Waibel (2000) and their later work, but it
is often eschewed perhaps because textbook MMR
does not directly provide a means to incorporate
other features. There is a simple means of doing so
(Section 2.3), and it is furthermore very resilient to
low word-error rates (WERs, Section 3.3).
Third, as inappropriate uses of optimization meth-
ods go, the one comparison that has not made it
into print yet is that of the more traditional ?what-is-
said? features (MMR, length in words and named-
entity features) vs. the avant-garde ?how-it-is-said?
features (structural, acoustic/prosodic and spoken-
language features). Maskey & Hirschberg (2005)
divide their features into these categories, but only
to compute a correlation coefficient between them
(0.74). The former in aggregate still performs sig-
nificantly better than the latter in aggregate, even if
certain members of the latter do outperform certain
members of the former. This is perhaps the most re-
assuring comparison we can offer to text summariza-
tion and ASR enthusiasts, because it corroborates
the important role that ASR still plays in speech
summarization in spite of its imperfections.
Finally, and perhaps most disconcertingly, we
can show that current speech summarization per-
forms just as well, and in some respects even bet-
ter, with SWITCHBOARD dialogues as it does with
more coherent spoken-language content, such as lec-
tures. This is not a failing of automated systems
themselves ? even humans exhibit the same ten-
dency under the experimental conditions that most
researchers have used to prepare evaluation gold
standards. What this means is that, while speech
summarization systems may arguably be useful and
are indeed consistent with whatever it is that humans
are doing when they are enlisted to rank utterances,
this evaluation regime simply does not reflect how
well the ?summaries? capture the goal-orientation or
geneous systems, but without indicating their performance on
their own.
471
higher-level purpose of the data that they are trained
on. As a community, we have been optimizing an
utterance excerpting task, we have been moderately
successful at it, but this task in at least one impor-
tant respect bears no resemblance to what we could
convincingly call speech summarization.
These four results provide us with valuable insight
into the current state of the art in speech summariza-
tion: it is not summarization, the aspiration to mea-
sure the relative merits of knowledge sources has
masked the prominence of some very simple base-
lines, and the Zechner & Waibel pipe-ASR-output-
into-text-summarizer model is still very competitive
? what seems to matter more than having access
to the raw spoken data is simply knowing that it is
spoken data, so that the most relevant, still textu-
ally available features can be used. Section 2 de-
scribes the background and further details of the ex-
periments that we conducted to arrive at these con-
clusions. Section 3 presents the results that we ob-
tained. Section 4 concludes by outlining an ecologi-
cally valid alternative for evaluating real summariza-
tion in light of these results.
2 Setting of the experiment
2.1 Provenance of the data
Speech summarizers are generally trained to sum-
marize either broadcast news or meetings. With
the exception of one paper that aspires to compare
the ?styles? of spoken and written language ceteris
paribus (Christensen et al, 2004), the choice of
broadcast news as a source of data in more recent
work is rather curious. Broadcast news, while open
in principle in its range of topics, typically has a
range of closely parallel, written sources on those
same topics, which can either be substituted for spo-
ken source material outright, or at the very least
be used corroboratively alongside them. Broadcast
news is also read by professional news readers, using
high quality microphones and studio equipment, and
as a result has very lower WER ? some even call
ASR a solved problem on this data source. Broad-
cast news is also very text-like at a deeper level. Rel-
ative position within a news story or dialogue, the
dreaded baseline of text summarization, works ex-
tremely well in spoken broadcast news summariza-
tion, too. Within the operating region of the receiver
operating characteristics (ROC) curve most relevant
to summarizers (0.1?0.3), Christensen et al (2004)
showed that position was by far the best feature in
a read broadcast news system with high WER, and
that position and length of the extracted utterance
were the two best with low WER. Christensen et
al. (2004) also distinguished read news from ?spon-
taneous news,? broadcasts that contain interviews
and/or man-in-the-field reports, and showed that in
the latter variety position is not at all prominent
at any level of WER, but length is. Maskey &
Hirschberg?s (2005) broadcast news is a combina-
tion of read news and spontaneous news.
Spontaneous speech, in our view, particularly in
the lecture domain, is our best representative of what
needs to be summarized. Here, the positional base-
line performs quite poorly (although length does ex-
tremely well, as discussed below), and ASR per-
formance is far from perfect. In the case of lec-
tures, there are rarely exact transcripts available, but
there are bulleted lines from presentation slides, re-
lated research papers on the speaker?s web page and
monographs on the same topic that can be used to
improve the language models for speech recogni-
tion systems. Lectures have just the right amount of
props for realistic ASR, but still very open domain
vocabularies and enough spontaneity to make this a
problem worth solving. As discussed further in Sec-
tion 4, the classroom lecture genre also provides us
with a task that we hope to use to conduct a better
grounded evaluation of real summarization quality.
To this end, we use a corpus of lectures recorded
at the University of Toronto to train and test our sum-
marizer. Only the lecturer is recorded, using a head-
worn microphone, and each lecture lasts 50 minutes.
The lectures in our experiments are all undergradu-
ate computer science lectures. The results reported
in this paper used four different lectures, each from
a different course and spoken by a different lecturer.
We used a leave-one-out cross-validation approach
by iteratively training on three lectures worth of ma-
terial and testing on the one remaining. We combine
these iterations by averaging. The lectures were di-
vided at random into 8?15 minute intervals, how-
ever, in order to provide a better comparison with
the SWITCHBOARD dialogues. Each interval was
treated as a separate document and was summarized
separately. So the four lectures together actually
472
provide 16 SWITCHBOARD-sized samples of ma-
terial, and our cross-validation leaves on average
four of them out in a turn.
We also use part of the SWITCHBOARD cor-
pus in one of our comparisons. SWITCHBOARD
is a collection of telephone conversations, in which
two participants have been told to speak on a cer-
tain topic, but with no objective or constructive
goal to proceed towards. While the conversations
are locally coherent, this lack of goal-orientation is
acutely apparent in all of them ? they may be as
close as any speech recording can come to being
about nothing.2 We randomly selected 27 conver-
sations, containing a total of 3665 utterances (iden-
tified by pause length), and had three human anno-
tators manually label each utterance as in- or out-
of-summary. Interestingly, the interannotator agree-
ment on SWITCHBOARD (   		 ) is higher
than on the lecture corpus (0.372) and higher than
the   -score reported by Galley (2006) for the ICSI
meeting data used by Murray et al (2005; 2006),
in spite of the fact that Murray et al (2005) primed
their annotators with a set of questions to consider
when annotating the data.3 This does not mean that
the SWITCHBOARD summaries are qualitatively
better, but rather that annotators are apt to agree
more on which utterances to include in them.
2.2 Summarization task
As with most work in speech summarization, our
strategy involves considering the problem as one
of utterance extraction, which means that we are
not synthesizing new text or speech to include in
summaries, nor are we attempting to extract small
phrases to sew together with new prosodic contours.
Candidate utterances are identified through pause-
length detection, and the length of these pauses has
been experimentally calibrated to 200 msec, which
results in roughly sentence-sized utterances. Sum-
marization then consists of choosing the best N% of
these utterances for the summary, where N is typ-
2It should be noted that the meandering style of SWITCH-
BOARD conversations does have correlates in text processing,
particularly in the genres of web blogs and newsgroup- or wiki-
based technical discussions.
3Although we did define what a summary was to each anno-
tator beforehand, we did not provide questions or suggestions
on content for either corpus.
ically between 10 and 30. We will provide ROC
curves to indicate performance as a function over all
N. An ROC is plotted along an x-axis of specificity
(true-negative-rate) and a y-axis of sensitivity (true-
positive-rate). A larger area under the ROC corre-
sponds to better performance.
2.3 Utterance isolation
The framework for our extractive summarization ex-
periments is depicted in Figure 1. With the excep-
tion of disfluency removal, it is very similar in its
overall structure to that of Zechner?s (2001). The
summarizer takes as input either manual or auto-
matic transcripts together with an audio file, and
has three modules to process disfluencies and extract
features important to identifying sentences.
Figure 1: Experimental framework for summarizing
spontaneous conversations.
During sentence boundary detection, words that
are likely to be adjacent to an utterance boundary
are determined. We call these words trigger words.
False starts are very common in spontaneous
speech. According to Zechner?s (2001) statistics on
the SWITCHBOARD corpus, they occur in 10-15%
of all utterances. A decision tree (C4.5, Release
8) is used to detect false starts, trained on the POS
tags and trigger-word status of the first and last four
words of sentences from a training set. Once false
starts are detected, these are removed.
We also identify repetitions as a sequence of be-
tween 1 and 4 words which is consecutively re-
473
peated in spontaneous speech. Generally, repetitions
are discarded. Repetitions of greater length are ex-
tremely rare statistically and are therefore ignored.
Question-answer pairs are also detected and
linked. Question-answer detection is a two-stage
process. The system first identifies the questions and
then finds the corresponding answer. For (both WH-
and Yes/No) question identification, another C4.5
classifier was trained on 2,000 manually annotated
sentences using utterance length, POS bigram oc-
currences, and the POS tags and trigger-word status
of the first and last five words of an utterance. After
a question is identified, the immediately following
sentence is labelled as the answer.
2.4 Utterance selection
To obtain a trainable utterance selection module that
can utilize and compare rich features, we formu-
lated utterance selection as a standard binary clas-
sification problem, and experimented with several
state-of-the-art classifiers, including linear discrim-
inant analysis LDA, support vector machines with
a radial basis kernel (SVM), and logistic regression
(LR), as shown in Figure 2 (computed on SWITCH-
BOARD data). MMR, Zechner?s (2001) choice, is
provided as a baseline. MMR linearly interpolates
a relevance component and a redundancy compo-
nent that balances the need for new vs. salient in-
formation. These two components can just as well
be mixed through LR, which admits the possibility
of adding more features and the benefit of using LR
over held-out estimation.
0 0.2 0.4 0.6 0.8 10
0.2
0.4
0.6
0.8
1
R
ec
al
l
Precision
 
 
LR?full?fea
LDA?full?fea
SVM?full?fea
LR?MMR?fea
MMR
Figure 2: Precision-recall curve for several classifiers on
the utterance selection task.
As Figure 2 indicates, there is essentially no dif-
ference in performance among the three classifiers
we tried, nor between MMR and LR restricted to
the two MMR components. This is important, since
we will be comparing MMR to LR-trained classi-
fiers based on other combinations of features below.
The ROC curves in the remainder of this paper have
been prepared using the LR classifier.
2.5 Features extracted
While there is very little difference realized across
pattern recognition methods, there is much more at
stake with respect to which features the methods use
to characterize their input. We can extract and use
the features in Figure 3, arranged there according to
their knowledge source.
We detect disfluencies in the same manner as
Zechner (2001)). Taking ASR transcripts as input,
we use the Brill tagger (Brill, 1995) to assign POS
tags to each word. There are 42 tags: Brill?s 38 plus
four which identify filled-pause disfluencies:
  empty coordinating conjunctions (CO),
  lexicalized filled pauses (DM),
  editing terms (ET), and
  non-lexicalized filled pauses (UH).
Our disfluency features include the number of each
of these, their total, and also the number of repeti-
tions. Disfluencies adjacent to a speaker turn are ig-
nored, however, because they occur as a normal part
of turn coordination between speakers.
Our preliminary experiments suggest that speaker
meta-data do not improve on the quality of summa-
rization, and so this feature is not included.
We indicate with bold type the features that indi-
cate some quantity of length, and we will consider
these as members of another class called ?length,?
in addition to their given class above. In all of the
data on which we have measured, the correlation be-
tween time duration and number of words is nearly
1.00 (although pause length is not).
2.6 Evaluation of summary quality
We plot receiver operating characteristic (ROC)
curves along a range of possible compression pa-
rameters, and in one case, ROUGE scores. ROUGE
474
1. Lexical features
  MMR score4,
  utterance length (in words),
2. Named entity features ? number of:
  person names,
  location names
  organization names
  the sum of these
3. Structural features
  utterance position, labelled as first, middle, or
last one-third of the conversation
  a Boolean feature indicating whether an utter-
ance is adjacent to a speaker turn
1. Acoustic features ? min, max and avg. of:5
  pitch
  energy
  speaking rate
  (unfilled) pause length
  time duration (in msec)
2. ?Spoken language? features
  disfluencies
  given/new information
  question/answer pair identification
Figure 3: Features available for utterance selection by knowledge source. Features in bold type quantify length. In our
experiments, we exclude these from their knowledge sources, and study them as a separate length category.
and F-measure are both widely used in speech sum-
marization, and they have been shown by others
to be broadly consistent on speech summarization
tasks (Zhu and Penn, 2005).
3 Results and analysis
3.1 Lecture corpus
The results of our evaluation on the lecture data ap-
pear in Figure 4. As is evident, there is very little
difference among the combinations of features with
this data source, apart from the positional baseline,
?lead,? which simply chooses the first N% of the
utterances. This performs quite poorly. The best
performance is achieved by using all of the features
together, but the length baseline, which uses only
those features in bold type from Figure 3, is very
close (no statistically significant difference), as is
MMR.6
4When evaluated on its own, the MMR interpolating param-
eter is set through experimentation on a held-out dataset, as in
Zechner (2001). When combined with other features, its rele-
vance and redundancy components are provided to the classifier
separately.
5All of these features are calculated on the word level and
normalized by speaker.
6We conducted the same evaluation without splitting the lec-
tures into 8?15 minute segments (so that the summaries sum-
marize an entire lecture), and although space here precludes
the presentation of the ROC curves, they are nearly identical
Figure 4: ROC curve for utterance selection with the lec-
ture corpus with several feature combinations.
3.2 SWITCHBOARD corpus
The corresponding results on SWITCHBOARD are
shown in Figure 5. Again, length and MMR are
very close to the best alternative, which is again all
of features combined. The difference with respect
to either of these baselines is statistically significant
within the popular 10?30% compression range, as
is the classifier trained on all features but acoustic
to those on the segments shown here.
475
Figure 5: ROC curve for SWITCHBOARD utterance se-
lection with several feature combinations.
(not shown). The classifier trained on all features
but spoken language features (not shown) is not sig-
nificantly better, so it is the spoken language fea-
tures that make the difference, not the acoustic fea-
tures. The best score is also significantly better than
on the lecture data, however, particularly in the 10?
30% range. Our analysis of the difference suggests
that the much greater variance in utterance length in
SWITCHBOARD is what accounts for the overall
better performance of the automated system as well
as the higher human interannotator agreement. This
also goes a long way to explaining why the length
baseline is so good.
Still another perspective is to classify features as
either ?what-is-said? (MMR, length and NE fea-
tures) or ?how-it-is-said? (structural, acoustic and
spoken-language features), as shown in Figure 6.
What-is-said features are better, but only barely so
within the usual operating region of summarizers.
3.3 Impact of WER
Word error rates (WERs) arising from speech recog-
nition are usually much higher in spontaneous con-
versations than in read news. Having trained ASR
models on SWITCHBOARD section 2 data with
our sample of 27 conversations removed, the WER
on that sample is 46%. We then train a language
model on SWITCHBOARD section 2 without re-
moving the 27-conversation sample so as to delib-
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Se
ns
itiv
ity
1?Specificity
 
 
all
what?is?said
how?it?is?said
Figure 6: ROC curves for textual and non-textual fea-
tures.
erately overfit the model. This pseudo-WER is then
39%. We might be able to get less WER by tuning
the ASR models or by using more training data, but
that is not the focus here. Summarizing the auto-
matic transcripts generated from both of these sys-
tems using our LR-based classifier with all features,
as well as manual (perfect) transcripts, we obtain the
ROUGE?1 scores in Table 1.
WER 10% 15% 20% 25% 30%
0.46 .615 .591 .556 .519 .489
0.39 .615 .591 .557 .526 .491
0 .619 .600 .566 .530 .492
Table 1: ROUGE?1 of LR system with all features under
different WERs.
Table 1 shows that WERs do not impact summa-
rization performance significantly. One reason is
that the acoustic and structural features are not af-
fected by word errors, although WERs can affect
the MMR, spoken language, length and NE features.
Figures 7 and 8 present the ROC curves of the MMR
and spoken language features, respectively, under
different WERs. MMR is particularly resilient,
even on SWITCHBOARD. Keywords are still often
correctly recognized, even in the presence of high
WER, although possibly because the same topic is
discussed in many SWITCHBOARD conversations.
476
Figure 7: ROC curves for the effectiveness of MMR
scores on transcripts under different WERs.
Figure 8: ROC curves for the effectiveness of spoken lan-
guage features on transcripts under different WERs.
When some keywords are misrecognized (e.g. hat),
furthermore, related words (e.g. dress, wear) still
may identify important utterances. As a result, a
high WER does not necessarily mean a worse tran-
script for bag-of-keywords applications like sum-
marization and classification, regardless of the data
source. Utterance length does not change very much
when WERs vary, and in addition, it is often a la-
tent variable that underlies some other features? role,
e.g., a long utterance often has a higher MMR score
than a short utterance, even when the WER changes.
Note that the effectiveness of spoken language
features varies most between manually and automat-
ically generated transcripts just at around the typi-
cal operating region of most summarization systems.
The features of this category that respond most to
WER are disfluencies. Disfluency detection is also
at its most effective in this same range with respect
to any transcription method.
4 Future Work
In terms of future work in light of these results,
clearly the most important challenge is to formu-
late an experimental alternative to measuring against
a subjectively classified gold standard in which an-
notators are forced to commit to relative salience
judgements with no attention to goal orientation and
no requirement to synthesize the meanings of larger
units of structure into a coherent message. It is here
that using the lecture domain offers us some addi-
tional assistance. Once these data have been tran-
scribed and outlined, we will be able to formulate
examinations for students that test their knowledge
of the topics being lectured upon: both their higher-
level understanding of goals and conceptual themes,
as well as factoid questions on particular details. A
group of students can be provided with access to a
collection of entire lectures to establish a theoreti-
cal limit. Experimental and control groups can then
be provided with access only to summaries of those
lectures, prepared using different sets of features, or
different modes of delivery (text vs. speech), for ex-
ample. This task-based protocol involves quite a bit
more work, and at our university, at least, there are
regulations that preclude us placing a group of stu-
dents in a class at a disadvantage with respect to an
examination for credit that need to be dealt with. It
is, however, a far better means of assessing the qual-
ity of summaries in an ecologically valid context.
It is entirely possible that, within this protocol, the
baselines that have performed so well in our experi-
ments, such as length or, in read news, position, will
utterly fail, and that less traditional acoustic or spo-
ken language features will genuinely, and with sta-
tistical significance, add value to a purely transcript-
based text summarization system. To date, how-
ever, that case has not been made. He et al (1999)
conducted a study very similar to the one suggested
above and found no significant difference between
using pitch and using slide transition boundaries. No
ASR transcripts or length features were used.
477
References
M. Ajmal, A. Kushki, and K. N. Plataniotis. 2007. Time-
compression of speech in informational talks using
spectral entropy. In Proceedings of the 8th Interna-
tional Workshop on Image Analysis for Multimedia In-
teractive Services (WIAMIS-07).
B Arons. 1992. Techniques, perception, and applications
of time-compressed speech. In American Voice I/O
Society Conference, pages 169?177.
B. Arons. 1994. Speech Skimmer: Interactively Skim-
ming Recorded Speech. Ph.D. thesis, MIT Media Lab.
E. Brill. 1995. Transformation-based error-driven learn-
ing and natural language processing: A case study
in part-of-speech tagging. Computational Linguistics,
21(4):543?565.
F. Chen and M. Withgott. 1992. The use of emphasis
to automatically summarize a spoken discourse. In
Proceedings of the IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP),
volume 1, pages 229?232.
E. Cherry and W. Taylor. 1954. Some further exper-
iments on the recognition of speech, with one and
two ears. Journal of the Acoustic Society of America,
26:554?559.
H. Christensen, B. Kolluru, Y. Gotoh, and S. Renals.
2004. From text summarisation to style-specific sum-
marisation for broadcast news. In Proceedings of the
26th European Conference on Information Retrieval
(ECIR-2004), pages 223?237.
M. Galley. 2006. A skip-chain conditional random field
for ranking meeting utterances by importance. In Pro-
ceedings of the 2006 Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP 2006).
L. He, E. Sanocki, A. Gupta, and J. Grudin. 1999. Auto-
summarization of audio-video presentations. In MUL-
TIMEDIA ?99: Proceedings of the seventh ACM in-
ternational conference on Multimedia (Part 1), pages
489?498.
J. Hirschberg, S. Whittaker, D. Hindle, F. Pereira, and
A. Singhal. 1999. Finding information in audio: A
new paradigm for audio browsing and retrieval. In
Proceedings of the ESCA/ETRW Workshop on Access-
ing Information in Spoken Audio, pages 117?122.
M. Kameyama and I. Arima. 1994. Coping with about-
ness complexity in information extraction from spo-
ken dialogues. In Proceedings of the 3rd International
Conference on Spoken Language Processing (ICSLP),
pages 87?90.
T. Kikuchi, S. Furui, and C. Hori. 2003. Two-stage au-
tomatic speech summarization by sentence extraction
and compaction. In Proceedings of the ISCA/IEEE
Workshop on Spontaneous Speech Processing and
Recognition (SSPR), pages 207?210.
S. Maskey and J. Hirschberg. 2005. Comparing lex-
ial, acoustic/prosodic, discourse and structural features
for speech summarization. In Proceedings of the 9th
European Conference on Speech Communication and
Technology (Eurospeech), pages 621?624.
G. Murray, S. Renals, and J. Carletta. 2005. Extractive
summarization of meeting recordings. In Proceedings
of the 9th European Conference on Speech Communi-
cation and Technology (Eurospeech), pages 593?596.
G. Murray, S. Renals, J. Moore, and J. Carletta. 2006. In-
corporating speaker and discourse features into speech
summarization. In Proceedings of the Human Lan-
guage Technology Conference - Annual Meeting of the
North American Chapter of the Association for Com-
putational Linguistics (HLT-NAACL), pages 367?374.
National Institute of Standards. 1997?2000. Pro-
ceedings of the Text REtrieval Conferences.
http://trec.nist.gov/pubs.html.
Abhishek Ranjan, Ravin Balakrishnan, and Mark
Chignell. 2006. Searching in audio: the utility of tran-
scripts, dichotic presentation, and time-compression.
In CHI ?06: Proceedings of the SIGCHI conference on
Human Factors in computing systems, pages 721?730,
New York, NY, USA. ACM Press.
S. Tucker and S. Whittaker. 2006. Time is of the essence:
an evaluation of temporal compression algorithms. In
CHI ?06: Proceedings of the SIGCHI conference on
Human Factors in computing systems, pages 329?338,
New York, NY, USA. ACM Press.
R. Valenza, T. Robinson, M. Hickey, and R. Tucker.
1999. Summarization of spoken audio through infor-
mation extraction. In Proceedings of the ESCA/ETRW
Workshop on Accessing Information in Spoken Audio,
pages 111?116.
K. Zechner and A. Waibel. 2000. Minimizing word er-
ror rate in textual summaries of spoken language. In
Proceedings of the 6th Applied Natural Language Pro-
cessing Conference and the 1st Meeting of the North
American Chapter of the Association for Computa-
tional Linguistics (ANLP/NAACL), pages 186?193.
K. Zechner. 2001. Automatic Summarization of Spo-
ken Dialogues in Unrestricted Domains. Ph.D. thesis,
Carnegie Mellon University.
X. Zhu and G. Penn. 2005. Evaluation of sentence selec-
tion for speech summarization. In Proceedings of the
RANLP workshop on Crossing Barriers in Text Sum-
marization Research, pages 39?45.
478
Tutorial Abstracts of ACL-08: HLT, page 6,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Interactive Visualization for Computational Linguistics
Christopher Collins and Gerald Penn
Department of Computer Science
University of Toronto
10 King?s College Road
Toronto, Ontario, Canada
{ccollins,gpenn}@cs.utoronto.ca
Sheelagh Carpendale
Department of Computer Science
University of Calgary
2500 University Dr. NW
Calgary, Canada
sheelagh@ucalgary.ca
Interactive information visualization is an emerg-
ing and powerful research technique that can be used
to understand models of language and their abstract
representations. Much of what computational lin-
guists fall back upon to improve NLP applications
and to model language ?understanding? is structure
that has, at best, only an indirect attestation in ob-
servable data. An important part of our research
progress thus depends on our ability to fully investi-
gate, explain, and explore these structures, both em-
pirically and relative to accepted linguistic theory.
The sheer complexity of these abstract structures,
and the observable patterns on which they are based,
usually limits their accessibility ? often even to the
researchers creating or attempting to learn them.
To aid in this understanding, visual ?externaliza-
tions? are used for presentation and explanation ?
traditional statistical graphs and custom-designed il-
lustrations fill the pages of ACL papers. These vi-
sualizations provide post hoc insight into the repre-
sentations and algorithms designed by researchers,
but visualization can also assist in the process of re-
search itself. There are special statistical methods,
falling under the rubric of ?exploratory data analy-
sis,? and visualization techniques just for this pur-
pose, in fact, but these are not widely used or even
known in CL. These techniques offer the potential
for revealing structure and detail in data, before any-
one else has noticed them.
When observing natural language engineers at
work, we also notice that, even without a formal vi-
sualization background, they often create sketches
to aid in their understanding and communication of
complex structures. These are ad hoc visualizations,
but they, too, can be extended by taking advantage
of current information visualization research.
This tutorial will enable members of the ACL
community to leverage information visualization
theory into exploratory data analysis, algorithm de-
sign, and data presentation techniques for their own
research. We draw on fundamental studies in cog-
nitive psychology to introduce ?visual variables? ?
visual dimensions on which data can be encoded.
We also discuss the use of interaction and animation
to enhance the usability and usefulness of visualiza-
tions.
Topics covered in this tutorial include a review of
information visualization techniques that are appli-
cable to CL, pointers to existing visualization tools
and programming toolkits, and new directions in vi-
sualizing CL data and results. We also discuss the
challenges of evaluating visualizations, noting dif-
ferences from the evaluation methods traditionally
used in CL, and discuss some heuristic approaches
and techniques used for measuring insight. Informa-
tion visualizations in CL research can also be mea-
sured by the impact they have on algorithm and data
structure design.
Information visualization is also filled with op-
portunities to make more creative visualizations that
benefit from the CL community?s deeper collective
understanding of natural language. Given that most
visualizations of language are created by researchers
with little or no linguistic expertise, we?ll cover
some open and very ripe possibilities for improving
the state of the art in text-based visualizations.
6
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 64?72,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Topological Field Parsing of German
Jackie Chi Kit Cheung
Department of Computer Science
University of Toronto
Toronto, ON, M5S 3G4, Canada
jcheung@cs.toronto.edu
Gerald Penn
Department of Computer Science
University of Toronto
Toronto, ON, M5S 3G4, Canada
gpenn@cs.toronto.edu
Abstract
Freer-word-order languages such as Ger-
man exhibit linguistic phenomena that
present unique challenges to traditional
CFG parsing. Such phenomena produce
discontinuous constituents, which are not
naturally modelled by projective phrase
structure trees. In this paper, we exam-
ine topological field parsing, a shallow
form of parsing which identifies the ma-
jor sections of a sentence in relation to
the clausal main verb and the subordinat-
ing heads. We report the results of topo-
logical field parsing of German using the
unlexicalized, latent variable-based Berke-
ley parser (Petrov et al, 2006) Without
any language- or model-dependent adapta-
tion, we achieve state-of-the-art results on
the Tu?Ba-D/Z corpus, and a modified NE-
GRA corpus that has been automatically
annotated with topological fields (Becker
and Frank, 2002). We also perform a qual-
itative error analysis of the parser output,
and discuss strategies to further improve
the parsing results.
1 Introduction
Freer-word-order languages such as German ex-
hibit linguistic phenomena that present unique
challenges to traditional CFG parsing. Topic focus
ordering and word order constraints that are sen-
sitive to phenomena other than grammatical func-
tion produce discontinuous constituents, which are
not naturally modelled by projective (i.e., with-
out crossing branches) phrase structure trees. In
this paper, we examine topological field parsing, a
shallow form of parsing which identifies the ma-
jor sections of a sentence in relation to the clausal
main verb and subordinating heads, when present.
We report the results of parsing German using
the unlexicalized, latent variable-based Berkeley
parser (Petrov et al, 2006). Without any language-
or model-dependent adaptation, we achieve state-
of-the-art results on the Tu?Ba-D/Z corpus (Telljo-
hann et al, 2004), with a F1-measure of 95.15%
using gold POS tags. A further reranking of
the parser output based on a constraint involv-
ing paired punctuation produces a slight additional
performance gain. To facilitate comparison with
previous work, we also conducted experiments on
a modified NEGRA corpus that has been automat-
ically annotated with topological fields (Becker
and Frank, 2002), and found that the Berkeley
parser outperforms the method described in that
work. Finally, we perform a qualitative error anal-
ysis of the parser output on the Tu?Ba-D/Z corpus,
and discuss strategies to further improve the pars-
ing results.
German syntax and parsing have been studied
using a variety of grammar formalisms. Hocken-
maier (2006) has translated the German TIGER
corpus (Brants et al, 2002) into a CCG-based
treebank to model word order variations in Ger-
man. Foth et al (2004) consider a version of de-
pendency grammars known as weighted constraint
dependency grammars for parsing German sen-
tences. On the NEGRA corpus (Skut et al, 1998),
they achieve an accuracy of 89.0% on parsing de-
pendency edges. In Callmeier (2000), a platform
for efficient HPSG parsing is developed. This
parser is later extended by Frank et al (2003)
with a topological field parser for more efficient
parsing of German. The system by Rohrer and
Forst (2006) produces LFG parses using a manu-
ally designed grammar and a stochastic parse dis-
ambiguation process. They test on the TIGER cor-
pus and achieve an F1-measure of 84.20%. In
Dubey and Keller (2003), PCFG parsing of NE-
GRA is improved by using sister-head dependen-
cies, which outperforms standard head lexicaliza-
tion as well as an unlexicalized model. The best
64
performing model with gold tags achieve an F1
of 75.60%. Sister-head dependencies are useful in
this case because of the flat structure of NEGRA?s
trees.
In contrast to the deeper approaches to parsing
described above, topological field parsing identi-
fies the major sections of a sentence in relation
to the clausal main verb and subordinating heads,
when present. Like other forms of shallow pars-
ing, topological field parsing is useful as the first
stage to further processing and eventual seman-
tic analysis. As mentioned above, the output of
a topological field parser is used as a guide to
the search space of a HPSG parsing algorithm in
Frank et al (2003). In Neumann et al (2000),
topological field parsing is part of a divide-and-
conquer strategy for shallow analysis of German
text with the goal of improving an information ex-
traction system.
Existing work in identifying topological fields
can be divided into chunkers, which identify the
lowest-level non-recursive topological fields, and
parsers, which also identify sentence and clausal
structure.
Veenstra et al (2002) compare three approaches
to topological field chunking based on finite state
transducers, memory-based learning, and PCFGs
respectively. It is found that the three techniques
perform about equally well, with F1 of 94.1% us-
ing POS tags from the TnT tagger, and 98.4% with
gold tags. In Liepert (2003), a topological field
chunker is implemented using a multi-class ex-
tension to the canonically two-class support vec-
tor machine (SVM) machine learning framework.
Parameters to the machine learning algorithm are
fine-tuned by a genetic search algorithm, with a
resulting F1-measure of 92.25%. Training the pa-
rameters to SVM does not have a large effect on
performance, increasing the F1-measure in the test
set by only 0.11%.
The corpus-based, stochastic topological field
parser of Becker and Frank (2002) is based on
a standard treebank PCFG model, in which rule
probabilities are estimated by frequency counts.
This model includes several enhancements, which
are also found in the Berkeley parser. First,
they use parameterized categories, splitting non-
terminals according to linguistically based intu-
itions, such as splitting different clause types (they
do not distinguish different clause types as basic
categories, unlike Tu?Ba-D/Z). Second, they take
into account punctuation, which may help iden-
tify clause boundaries. They also binarize the very
flat topological tree structures, and prune rules
that only occur once. They test their parser on a
version of the NEGRA corpus, which has been
annotated with topological fields using a semi-
automatic method.
Ule (2003) proposes a process termed Directed
Treebank Refinement (DTR). The goal of DTR is
to refine a corpus to improve parsing performance.
DTR is comparable to the idea of latent variable
grammars on which the Berkeley parser is based,
in that both consider the observed treebank to be
less than ideal and both attempt to refine it by split-
ting and merging nonterminals. In this work, split-
ting and merging nonterminals are done by consid-
ering the nonterminals? contexts (i.e., their parent
nodes) and the distribution of their productions.
Unlike in the Berkeley parser, splitting and merg-
ing are distinct stages, rather than parts of a sin-
gle iteration. Multiple splits are found first, then
multiple rounds of merging are performed. No
smoothing is done. As an evaluation, DTR is ap-
plied to topological field parsing of the Tu?Ba-D/Z
corpus. We discuss the performance of these topo-
logical field parsers in more detail below.
All of the topological parsing proposals pre-
date the advent of the Berkeley parser. The exper-
iments of this paper demonstrate that the Berke-
ley parser outperforms previous methods, many of
which are specialized for the task of topological
field chunking or parsing.
2 Topological Field Model of German
Topological fields are high-level linear fields in
an enclosing syntactic region, such as a clause
(Ho?hle, 1983). These fields may have constraints
on the number of words or phrases they contain,
and do not necessarily form a semantically co-
herent constituent. Although it has been argued
that a few languages have no word-order con-
straints whatsoever, most ?free word-order? lan-
guages (even Warlpiri) have at the very least some
sort of sentence- or clause-initial topic field fol-
lowed by a second position that is occupied by
clitics, a finite verb or certain complementizers
and subordinating conjunctions. In a few Ger-
manic languages, including German, the topology
is far richer than that, serving to identify all of
the components of the verbal head of a clause,
except for some cases of long-distance dependen-
65
cies. Topological fields are useful, because while
Germanic word order is relatively free with respect
to grammatical functions, the order of the topolog-
ical fields is strict and unvarying.
Type Fields
VL (KOORD) (C) (MF) VC (NF)
V1 (KOORD) (LV) LK (MF) (VC) (NF)
V2 (KOORD) (LV) VF LK (MF) (VC) (NF)
Table 1: Topological field model of German.
Simplified from Tu?Ba-D/Z corpus?s annotation
schema (Telljohann et al, 2006).
In the German topological field model, clauses
belong to one of three types: verb-last (VL), verb-
second (V2), and verb-first (V1), each with a spe-
cific sequence of topological fields (Table 1). VL
clauses include finite and non-finite subordinate
clauses, V2 sentences are typically declarative
sentences and WH-questions in matrix clauses,
and V1 sentences include yes-no questions, and
certain conditional subordinate clauses. Below,
we give brief descriptions of the most common
topological fields.
? VF (Vorfeld or ?pre-field?) is the first con-
stituent in sentences of the V2 type. This is
often the topic of the sentence, though as an
anonymous reviewer pointed out, this posi-
tion does not correspond to a single function
with respect to information structure. (e.g.,
the reviewer suggested this case, where VF
contains the focus: ?Wer kommt zur Party?
?Peter kommt zur Party. ?Who is coming to
the Party? ?Peter is coming to the party.)
? LK (Linke Klammer or ?left bracket?) is the
position for finite verbs in V1 and V2 sen-
tences. It is replaced by a complementizer
with the field label C in VL sentences.
? MF (Mittelfeld or ?middle field?) is an op-
tional field bounded on the left by LK and
on the right by the verbal complex VC or
by NF. Most verb arguments, adverbs, and
prepositional phrases are found here, unless
they have been fronted and put in the VF, or
are prosodically heavy and postposed to the
NF field.
? VC is the verbal complex field. It includes
infinite verbs, as well as finite verbs in VL
sentences.
? NF (Nachfeld or ?post-field?) contains
prosodically heavy elements such as post-
posed prepositional phrases or relative
clauses.
? KOORD1 (Koordinationsfeld or ?coordina-
tion field?) is a field for clause-level conjunc-
tions.
? LV (Linksversetzung or ?left dislocation?) is
used for resumptive constructions involving
left dislocation. For a detailed linguistic
treatment, see (Frey, 2004).
Exceptions to the topological field model as de-
scribed above do exist. For instance, parenthetical
constructions exist as a mostly syntactically inde-
pendent clause inside another sentence. In our cor-
pus, they are attached directly underneath a clausal
node without any intervening topological field, as
in the following example. In this example, the par-
enthetical construction is highlighted in bold print.
Some clause and topological field labels under the
NF field are omitted for clarity.
(1) (a) (SIMPX ?(VF Man) (LK mu?) (VC verstehen) ?
, (SIMPX sagte er), ? (NF da? diese
Minderheiten seit langer Zeit massiv von den
Nazis bedroht werden)). ?
(b) Translation: ?One must understand,? he said,
?that these minorities have been massively
threatened by the Nazis for a long time.?
3 A Latent Variable Parser
For our experiments, we used the latent variable-
based Berkeley parser (Petrov et al, 2006). La-
tent variable parsing assumes that an observed
treebank represents a coarse approximation of
an underlying, optimally refined grammar which
makes more fine-grained distinctions in the syn-
tactic categories. For example, the noun phrase
category NP in a treebank could be viewed as a
coarse approximation of two noun phrase cate-
gories corresponding to subjects and object, NP?S,
and NP?VP.
The Berkeley parser automates the process of
finding such distinctions. It starts with a simple bi-
narized X-bar grammar style backbone, and goes
through iterations of splitting and merging non-
terminals, in order to maximize the likelihood of
the training set treebank. In the splitting stage,
1The Tu?Ba-D/Z corpus distinguishes coordinating and
non-coordinating particles, as well as clausal and field co-
ordination. These distinctions need not concern us for this
explanation.
66
Figure 1: ?I could never have done that just for aesthetic reasons.? Sample Tu?Ba-D/Z tree, with topolog-
ical field annotations and edge labels. Topological field layer in bold.
an Expectation-Maximization algorithm is used to
find a good split for each nonterminal. In the
merging stage, categories that have been over-
split are merged together to keep the grammar size
tractable and reduce sparsity. Finally, a smoothing
stage occurs, where the probabilities of rules for
each nonterminal are smoothed toward the prob-
abilities of the other nonterminals split from the
same syntactic category.
The Berkeley parser has been applied to the
Tu?BaD/Z corpus in the constituent parsing shared
task of the ACL-2008 Workshop on Parsing Ger-
man (Petrov and Klein, 2008), achieving an F1-
measure of 85.10% and 83.18% with and without
gold standard POS tags respectively2. We chose
the Berkeley parser for topological field parsing
because it is known to be robust across languages,
and because it is an unlexicalized parser. Lexi-
calization has been shown to be useful in more
general parsing applications due to lexical depen-
dencies in constituent parsing (e.g. (Ku?bler et al,
2006; Dubey and Keller, 2003) in the case of Ger-
man). However, topological fields explain a higher
level of structure pertaining to clause-level word
order, and we hypothesize that lexicalization is un-
likely to be helpful.
4 Experiments
4.1 Data
For our experiments, we primarily used the Tu?Ba-
D/Z (Tu?binger Baumbank des Deutschen / Schrift-
sprache) corpus, consisting of 26116 sentences
(20894 training, 2611 development, 2089 test,
with a further 522 sentences held out for future ex-
2This evaluation considered grammatical functions as
well as the syntactic category.
periments)3 taken from the German newspaper die
tageszeitung. The corpus consists of four levels
of annotation: clausal, topological, phrasal (other
than clausal), and lexical. We define the task of
topological field parsing to be recovering the first
two levels of annotation, following Ule (2003).
We also tested the parser on a version of the NE-
GRA corpus derived by Becker and Frank (2002),
in which syntax trees have been made projec-
tive and topological fields have been automatically
added through a series of linguistically informed
tree modifications. All internal phrasal structure
nodes have also been removed. The corpus con-
sists of 20596 sentences, which we split into sub-
sets of the same size as described by Becker and
Frank (2002)4. The set of topological fields in
this corpus differs slightly from the one used in
Tu?Ba-D/Z, making no distinction between clause
types, nor consistently marking field or clause
conjunctions. Because of the automatic anno-
tation of topological fields, this corpus contains
numerous annotation errors. Becker and Frank
(2002) manually corrected their test set and eval-
uated the automatic annotation process, reporting
labelled precision and recall of 93.0% and 93.6%
compared to their manual annotations. There are
also punctuation-related errors, including miss-
ing punctuation, sentences ending in commas, and
sentences composed of single punctuation marks.
We test on this data in order to provide a bet-
ter comparison with previous work. Although we
could have trained the model in Becker and Frank
(2002) on the Tu?Ba-D/Z corpus, it would not have
3These are the same splits into training, development, and
test sets as in the ACL-08 Parsing German workshop. This
corpus does not include sentences of length greater than 40.
416476 training sentences, 1000 development, 1058 test-
ing, and 2062 as held-out data. We were unable to obtain
the exact subsets used by Becker and Frank (2002). We will
discuss the ramifications of this on our evaluation procedure.
67
Gold tags Edge labels LP% LR% F1% CB CB0% CB ? 2% EXACT%
- - 93.53 93.17 93.35 0.08 94.59 99.43 79.50
+ - 95.26 95.04 95.15 0.07 95.35 99.52 83.86
- + 92.38 92.67 92.52 0.11 92.82 99.19 77.79
+ + 92.36 92.60 92.48 0.11 92.82 99.19 77.64
Table 2: Parsing results for topological fields and clausal constituents on the Tu?Ba-D/Z corpus.
been a fair comparison, as the parser depends quite
heavily on NEGRA?s annotation scheme. For ex-
ample, Tu?Ba-D/Z does not contain an equiva-
lent of the modified NEGRA?s parameterized cat-
egories; there exist edge labels in Tu?BaD/Z, but
they are used to mark head-dependency relation-
ships, not subtypes of syntactic categories.
4.2 Results
We first report the results of our experiments on
the Tu?Ba-D/Z corpus. For the Tu?Ba-D/Z corpus,
we trained the Berkeley parser using the default
parameter settings. The grammar trainer attempts
six iterations of splitting, merging, and smoothing
before returning the final grammar. Intermediate
grammars after each step are also saved. There
were training and test sentences without clausal
constituents or topological fields, which were ig-
nored by the parser and by the evaluation. As
part of our experiment design, we investigated the
effect of providing gold POS tags to the parser,
and the effect of incorporating edge labels into the
nonterminal labels for training and parsing. In all
cases, gold annotations which include gold POS
tags were used when training the parser.
We report the standard PARSEVAL measures
of parser performance in Table 2, obtained by the
evalb program by Satoshi Sekine and Michael
Collins. This table shows the results after five it-
erations of grammar modification, parameterized
over whether we provide gold POS tags for pars-
ing, and edge labels for training and parsing. The
number of iterations was determined by experi-
ments on the development set. In the evaluation,
we do not consider edge labels in determining
correctness, but do consider punctuation, as Ule
(2003) did. If we ignore punctuation in our evalu-
ation, we obtain an F1-measure of 95.42% on the
best model (+ Gold tags, - Edge labels).
Whether supplying gold POS tags improves
performance depends on whether edge labels are
considered in the grammar. Without edge labels,
gold POS tags improve performance by almost
two points, corresponding to a relative error reduc-
tion of 33%. In contrast, performance is negatively
affected when edge labels are used and gold POS
tags are supplied (i.e., + Gold tags, + Edge la-
bels), making the performance worse than not sup-
plying gold tags. Incorporating edge label infor-
mation does not appear to improve performance,
possibly because it oversplits the initial treebank
and interferes with the parser?s ability to determine
optimal splits for refining the grammar.
Parser LP% LR% F1%
Tu?Ba-D/Z
This work 95.26 95.04 95.15
Ule unknown unknown 91.98
NEGRA - from Becker and Frank (2002)
BF02 (len. ? 40) 92.1 91.6 91.8
NEGRA - our experiments
This work (len. ? 40) 90.74 90.87 90.81
BF02 (len. ? 40) 89.54 88.14 88.83
This work (all) 90.29 90.51 90.40
BF02 (all) 89.07 87.80 88.43
Table 3: BF02 = (Becker and Frank, 2002). Pars-
ing results for topological fields and clausal con-
stituents. Results from Ule (2003) and our results
were obtained using different training and test sets.
The first row of results of Becker and Frank (2002)
are from that paper; the rest were obtained by our
own experiments using that parser. All results con-
sider punctuation in evaluation.
To facilitate a more direct comparison with pre-
vious work, we also performed experiments on the
modified NEGRA corpus. In this corpus, topo-
logical fields are parameterized, meaning that they
are labelled with further syntactic and semantic in-
formation. For example, VF is split into VF-REL
for relative clauses, and VF-TOPIC for those con-
taining topics in a verb-second sentence, among
others. All productions in the corpus have also
been binarized. Tuning the parameter settings on
the development set, we found that parameterized
categories, binarization, and including punctua-
tion gave the best F1 performance. First-order
horizontal and zeroth order vertical markoviza-
68
tion after six iterations of splitting, merging, and
smoothing gave the best F1 result of 91.78%. We
parsed the corpus with both the Berkeley parser
and the best performing model of Becker and
Frank (2002).
The results of these experiments on the test set
for sentences of length 40 or less and for all sen-
tences are shown in Table 3. We also show other
results from previous work for reference. We
find that we achieve results that are better than
the model in Becker and Frank (2002) on the test
set. The difference is statistically significant (p =
0.0029, Wilcoxon signed-rank).
The results we obtain using the parser of Becker
and Frank (2002) are worse than the results de-
scribed in that paper. We suggest the following
reasons for this discrepancy. While the test set
used in the paper was manually corrected for eval-
uation, we did not correct our test set, because it
would be difficult to ensure that we adhered to the
same correction guidelines. No details of the cor-
rection process were provided in the paper, and de-
scriptive grammars of German provide insufficient
guidance on many of the examples in NEGRA on
issues such as ellipses, short infinitival clauses,
and expanded participial constructions modifying
nouns. Also, because we could not obtain the ex-
act sets used for training, development, and test-
ing, we had to recreate the sets by randomly split-
ting the corpus.
4.3 Category Specific Results
We now return to the Tu?Ba-D/Z corpus for a
more detailed analysis, and examine the category-
specific results for our best performing model (+
Gold tags, - Edge labels). Overall, Table 4 shows
that the best performing topological field cate-
gories are those that have constraints on the type
of word that is allowed to fill it (finite verbs in
LK, verbs in VC, complementizers and subordi-
nating conjunctions in C). VF, in which only one
constituent may appear, also performs relatively
well. Topological fields that can contain a vari-
able number of heterogeneous constituents, on the
other hand, have poorer F1-measure results. MF,
which is basically defined relative to the positions
of fields on either side of it, is parsed several points
below LK, C, and VC in accuracy. NF, which
contains different kinds of extraposed elements, is
parsed at a substantially worse level.
Poorly parsed categories tend to occur infre-
quently, including LV, which marks a rare re-
sumptive construction; FKOORD, which marks
topological field coordination; and the discourse
marker DM. The other clause-level constituents
(PSIMPX for clauses in paratactic constructions,
RSIMPX for relative clauses, and SIMPX for
other clauses) also perform below average.
Topological Fields
Category # LP% LR% F1%
PARORD 20 100.00 100.00 100.00
VCE 3 100.00 100.00 100.00
LK 2186 99.68 99.82 99.75
C 642 99.53 98.44 98.98
VC 1777 98.98 98.14 98.56
VF 2044 96.84 97.55 97.20
KOORD 99 96.91 94.95 95.92
MF 2931 94.80 95.19 94.99
NF 643 83.52 81.96 82.73
FKOORD 156 75.16 73.72 74.43
LV 17 10.00 5.88 7.41
Clausal Constituents
Category # LP% LR% F1%
SIMPX 2839 92.46 91.97 92.21
RSIMPX 225 91.23 92.44 91.83
PSIMPX 6 100.00 66.67 80.00
DM 28 59.26 57.14 58.18
Table 4: Category-specific results using grammar
with no edge labels and passing in gold POS tags.
4.4 Reranking for Paired Punctuation
While experimenting with the development set
of Tu?Ba-D/Z, we noticed that the parser some-
times returns parses, in which paired punctuation
(e.g. quotation marks, parentheses, brackets) is
not placed in the same clause?a linguistically im-
plausible situation. In these cases, the high-level
information provided by the paired punctuation is
overridden by the overall likelihood of the parse
tree. To rectify this problem, we performed a sim-
ple post-hoc reranking of the 50-best parses pro-
duced by the best parameter settings (+ Gold tags,
- Edge labels), selecting the first parse that places
paired punctuation in the same clause, or return-
ing the best parse if none of the 50 parses satisfy
the constraint. This procedure improved the F1-
measure to 95.24% (LP = 95.39%, LR = 95.09%).
Overall, 38 sentences were parsed with paired
punctuation in different clauses, of which 16 were
reranked. Of the 38 sentences, reranking improved
performance in 12 sentences, did not affect perfor-
mance in 23 sentences (of which 10 already had a
perfect parse), and hurt performance in three sen-
tences. A two-tailed sign test suggests that rerank-
69
ing improves performance (p = 0.0352). We dis-
cuss below why sentences with paired punctuation
in different clauses can have perfect parse results.
To investigate the upper-bound in performance
that this form of reranking is able to achieve, we
calculated some statistics on our (+ Gold tags, -
Edge labels) 50-best list. We found that the aver-
age rank of the best scoring parse by F1-measure
is 2.61, and the perfect parse is present for 1649
of the 2088 sentences at an average rank of 1.90.
The oracle F1-measure is 98.12%, indicating that
a more comprehensive reranking procedure might
allow further performance gains.
4.5 Qualitative Error Analysis
As a further analysis, we extracted the worst scor-
ing fifty sentences by F1-measure from the parsed
test set (+ Gold tags, - Edge labels), and compared
them against the gold standard trees, noting the
cause of the error. We analyze the parses before
reranking, to see how frequently the paired punc-
tuation problem described above severely affects a
parse. The major mistakes made by the parser are
summarized in Table 5.
Problem Freq.
Misidentification of Parentheticals 19
Coordination problems 13
Too few SIMPX 10
Paired punctuation problem 9
Other clause boundary errors 7
Other 6
Too many SIMPX 3
Clause type misidentification 2
MF/NF boundary 2
LV 2
VF/MF boundary 2
Table 5: Types and frequency of parser errors in
the fifty worst scoring parses by F1-measure, us-
ing parameters (+ Gold tags, - Edge labels).
Misidentification of Parentheticals Parentheti-
cal constructions do not have any dependencies on
the rest of the sentence, and exist as a mostly syn-
tactically independent clause inside another sen-
tence. They can occur at the beginning, end, or
in the middle of sentences, and are often set off
orthographically by punctuation. The parser has
problems identifying parenthetical constructions,
often positing a parenthetical construction when
that constituent is actually attached to a topolog-
ical field in a neighbouring clause. The follow-
ing example shows one such misidentification in
bracket notation. Clause internal topological fields
are omitted for clarity.
(2) (a) Tu?Ba-D/Z: (SIMPX Weder das Ausma? der
Scho?nheit noch der fru?here oder spa?tere
Zeitpunkt der Geburt macht einen der Zwillinge
fu?r eine Mutter mehr oder weniger echt /
authentisch / u?berlegen).
(b) Parser: (SIMPX Weder das Ausma? der
Scho?nheit noch der fru?here oder spa?tere
Zeitpunkt der Geburt macht einen der Zwillinge
fu?r eine Mutter mehr oder weniger echt)
(PARENTHETICAL / authentisch /
u?berlegen.)
(c) Translation: ?Neither the degree of beauty nor
the earlier or later time of birth makes one of the
twins any more or less real/authentic/superior to
a mother.?
We hypothesized earlier that lexicalization is
unlikely to give us much improvement in perfor-
mance, because topological fields work on a do-
main that is higher than that of lexical dependen-
cies such as subcategorization frames. However,
given the locally independent nature of legitimate
parentheticals, a limited form of lexicalization or
some other form of stronger contextual informa-
tion might be needed to improve identification per-
formance.
Coordination Problems The second most com-
mon type of error involves field and clause coordi-
nations. This category includes missing or incor-
rect FKOORD fields, and conjunctions of clauses
that are misidentified. In the following example,
the conjoined MFs and following NF in the cor-
rect parse tree are identified as a single long MF.
(3) (a) Tu?Ba-D/Z: Auf dem europa?ischen Kontinent
aber hat (FKOORD (MF kein Land und keine
Macht ein derartiges Interesse an guten
Beziehungen zu Ru?land) und (MF auch kein
Land solche Erfahrungen im Umgang mit
Ru?land)) (NF wie Deutschland).
(b) Parser: Auf dem europa?ischen Kontinent aber
hat (MF kein Land und keine Macht ein
derartiges Interesse an guten Beziehungen zu
Ru?land und auch kein Land solche
Erfahrungen im Umgang mit Ru?land wie
Deutschland).
(c) Translation: ?On the European continent,
however, no land and no power has such an
interest in good relations with Russia (as
Germany), and also no land (has) such
experience in dealing with Russia as Germany.?
Other Clause Errors Other clause-level errors
include the parser predicting too few or too many
clauses, or misidentifying the clause type. Clauses
are sometimes confused with NFs, and there is one
case of a relative clause being misidentified as a
70
main clause with an intransitive verb, as the finite
verb appears at the end of the clause in both cases.
Some clause errors are tied to incorrect treatment
of elliptical constructions, in which an element
that is inferable from context is missing.
Paired Punctuation Problems with paired
punctuation are the fourth most common type of
error. Punctuation is often a marker of clause
or phrase boundaries. Thus, predicting paired
punctuation incorrectly can lead to incorrect
parses, as in the following example.
(4) (a) ? Auch (SIMPX wenn der Krieg heute ein
Mobilisierungsfaktor ist) ? , so Pau , ? (SIMPX
die Leute sehen , da? man fu?r die Arbeit wieder
auf die Stra?e gehen mu?) . ?
(b) Parser: (SIMPX ? (LV Auch (SIMPX wenn der
Krieg heute ein Mobilisierungsfaktor ist)) ? , so
Pau , ? (SIMPX die Leute sehen , da? man fu?r
die Arbeit wieder auf die Stra?e gehen mu?)) . ?
(c) Translation: ?Even if the war is a factor for
mobilization,? said Pau, ?the people see, that
one must go to the street for employment again.?
Here, the parser predicts a spurious SIMPX
clause spanning the text of the entire sentence, but
this causes the second pair of quotation marks to
be parsed as belonging to two different clauses.
The parser also predicts an incorrect LV field. Us-
ing the paired punctuation constraint, our rerank-
ing procedure was able to correct these errors.
Surprisingly, there are cases in which paired
punctuation does not belong inside the same
clause in the gold parses. These cases are ei-
ther extended quotations, in which each of the
quotation mark pair occurs in a different sen-
tence altogether, or cases where the second of the
quotation mark pair must be positioned outside
of other sentence-final punctuation due to ortho-
graphic conventions. Sentence-final punctuation
is typically placed outside a clause in this version
of Tu?Ba-D/Z.
Other Issues Other incorrect parses generated
by the parser include problems with the infre-
quently occurring topological fields like LV and
DM, inability to determine the boundary between
MF and NF in clauses without a VC field sepa-
rating the two, and misidentifying appositive con-
structions. Another issue is that although the
parser output may disagree with the gold stan-
dard tree in Tu?Ba-D/Z, the parser output may be
a well-formed topological field parse for the same
sentence with a different interpretation, for ex-
ample because of attachment ambiguity. Each of
the authors independently checked the fifty worst-
scoring parses, and determined whether each parse
produced by the Berkeley parser could be a well-
formed topological parse. Where there was dis-
agreement, we discussed our judgments until we
came to a consensus. Of the fifty parses, we de-
termined that nine, or 18%, could be legitimate
parses. Another five, or 10%, differ from the gold
standard parse only in the placement of punctua-
tion. Thus, the F1-measures we presented above
may be underestimating the parser?s performance.
5 Conclusion and Future Work
In this paper, we examined applying the latent-
variable Berkeley parser to the task of topological
field parsing of German, which aims to identify the
high-level surface structure of sentences. Without
any language or model-dependent adaptation, we
obtained results which compare favourably to pre-
vious work in topological field parsing. We further
examined the results of doing a simple reranking
process, constraining the output parse to put paired
punctuation in the same clause. This reranking
was found to result in a minor performance gain.
Overall, the parser performs extremely well in
identifying the traditional left and right brackets
of the topological field model; that is, the fields
C, LK, and VC. The parser achieves basically per-
fect results on these fields in the Tu?Ba-D/Z corpus,
with F1-measure scores for each at over 98.5%.
These scores are higher than previous work in the
simpler task of topological field chunking. The fo-
cus of future research should thus be on correctly
identifying the infrequently occuring fields and
constructions, with parenthetical constructions be-
ing a particular concern. Possible avenues of fu-
ture research include doing a more comprehensive
discriminative reranking of the parser output. In-
corporating more contextual information might be
helpful to identify discourse-related constructions
such as parentheses, and the DM and LV topolog-
ical fields.
Acknowledgements
We are grateful to Markus Becker, Anette Frank,
Sandra Kuebler, and Slav Petrov for their invalu-
able help in gathering the resources necessary for
our experiments. This work is supported in part
by the Natural Sciences and Engineering Research
Council of Canada.
71
References
M. Becker and A. Frank. 2002. A stochastic topo-
logical parser for German. In Proceedings of the
19th International Conference on Computational
Linguistics, pages 71?77.
S. Brants, S. Dipper, S. Hansen, W. Lezius, and
G. Smith. 2002. The TIGER Treebank. In Proceed-
ings of the Workshop on Treebanks and Linguistic
Theories, pages 24?41.
U. Callmeier. 2000. PET?a platform for experimen-
tation with efficient HPSG processing techniques.
Natural Language Engineering, 6(01):99?107.
A. Dubey and F. Keller. 2003. Probabilistic parsing
for German using sister-head dependencies. In Pro-
ceedings of the 41st Annual Meeting of the Associa-
tion for Computational Linguistics, pages 96?103.
K.A. Foth, M. Daum, and W. Menzel. 2004. A
broad-coverage parser for German based on defea-
sible constraints. Constraint Solving and Language
Processing.
A. Frank, M. Becker, B. Crysmann, B. Kiefer, and
U. Schaefer. 2003. Integrated shallow and deep
parsing: TopP meets HPSG. In Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics, pages 104?111.
W. Frey. 2004. Notes on the syntax and the pragmatics
of German Left Dislocation. In H. Lohnstein and
S. Trissler, editors, The Syntax and Semantics of the
Left Periphery, pages 203?233. Mouton de Gruyter,
Berlin.
J. Hockenmaier. 2006. Creating a CCGbank and a
Wide-Coverage CCG Lexicon for German. In Pro-
ceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 505?512.
T.N. Ho?hle. 1983. Topologische Felder. Ph.D. thesis,
Ko?ln.
S. Ku?bler, E.W. Hinrichs, and W. Maier. 2006. Is it re-
ally that difficult to parse German? In Proceedings
of EMNLP.
M. Liepert. 2003. Topological Fields Chunking for
German with SVM?s: Optimizing SVM-parameters
with GA?s. In Proceedings of the International Con-
ference on Recent Advances in Natural Language
Processing (RANLP), Bulgaria.
G. Neumann, C. Braun, and J. Piskorski. 2000. A
Divide-and-Conquer Strategy for Shallow Parsing
of German Free Texts. In Proceedings of the sixth
conference on Applied natural language processing,
pages 239?246. Morgan Kaufmann Publishers Inc.
San Francisco, CA, USA.
S. Petrov and D. Klein. 2008. Parsing German with
Latent Variable Grammars. In Proceedings of the
ACL-08: HLT Workshop on Parsing German (PaGe-
08), pages 33?39.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree
annotation. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
44th Annual Meeting of the Association for Compu-
tational Linguistics, pages 433?440, Sydney, Aus-
tralia, July. Association for Computational Linguis-
tics.
C. Rohrer and M. Forst. 2006. Improving coverage
and parsing quality of a large-scale LFG for Ger-
man. In Proceedings of the Language Resources
and Evaluation Conference (LREC-2006), Genoa,
Italy.
W. Skut, T. Brants, B. Krenn, and H. Uszkoreit.
1998. A Linguistically Interpreted Corpus of Ger-
man Newspaper Text. Proceedings of the ESSLLI
Workshop on Recent Advances in Corpus Annota-
tion.
H. Telljohann, E. Hinrichs, and S. Kubler. 2004.
The Tu?Ba-D/Z treebank: Annotating German with a
context-free backbone. In Proceedings of the Fourth
International Conference on Language Resources
and Evaluation (LREC 2004), pages 2229?2235.
H. Telljohann, E.W. Hinrichs, S. Kubler, and H. Zins-
meister. 2006. Stylebook for the Tubingen Tree-
bank of Written German (Tu?Ba-D/Z). Seminar fur
Sprachwissenschaft, Universitat Tubingen, Tubin-
gen, Germany.
T. Ule. 2003. Directed Treebank Refinement for PCFG
Parsing. In Proceedings of Workshop on Treebanks
and Linguistic Theories (TLT) 2003, pages 177?188.
J. Veenstra, F.H. Mu?ller, and T. Ule. 2002. Topolog-
ical field chunking for German. In Proceedings of
the Sixth Conference on Natural Language Learn-
ing, pages 56?62.
72
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 549?557,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Summarizing multiple spoken documents: finding evidence from
untranscribed audio
Xiaodan Zhu, Gerald Penn and Frank Rudzicz
University of Toronto
10 King?s College Rd.,
Toronto, M5S 3G4, ON, Canada
{xzhu,gpenn,frank}@cs.toronto.edu
Abstract
This paper presents a model for summa-
rizing multiple untranscribed spoken doc-
uments. Without assuming the availabil-
ity of transcripts, the model modifies a
recently proposed unsupervised algorithm
to detect re-occurring acoustic patterns in
speech and uses them to estimate similari-
ties between utterances, which are in turn
used to identify salient utterances and re-
move redundancies. This model is of in-
terest due to its independence from spo-
ken language transcription, an error-prone
and resource-intensive process, its abil-
ity to integrate multiple sources of infor-
mation on the same topic, and its novel
use of acoustic patterns that extends pre-
vious work on low-level prosodic feature
detection. We compare the performance of
this model with that achieved using man-
ual and automatic transcripts, and find that
this new approach is roughly equivalent
to having access to ASR transcripts with
word error rates in the 33?37% range with-
out actually having to do the ASR, plus
it better handles utterances with out-of-
vocabulary words.
1 Introduction
Summarizing spoken documents has been exten-
sively studied over the past several years (Penn
and Zhu, 2008; Maskey and Hirschberg, 2005;
Murray et al, 2005; Christensen et al, 2004;
Zechner, 2001). Conventionally called speech
summarization, although speech connotes more
than spoken documents themselves, it is motivated
by the demand for better ways to navigate spoken
content and the natural difficulty in doing so ?
speech is inherently more linear or sequential than
text in its traditional delivery.
Previous research on speech summarization has
addressed several important problems in this field
(see Section 2.1). All of this work, however,
has focused on single-document summarization
and the integration of fairly simplistic acoustic
features, inspired by work in descriptive linguis-
tics. The issues of navigating speech content are
magnified when dealing with larger collections ?
multiple spoken documents on the same topic. For
example, when one is browsing news broadcasts
covering the same events or call-centre record-
ings related to the same type of customer ques-
tions, content redundancy is a prominent issue.
Multi-document summarization on written docu-
ments has been studied for more than a decade
(see Section 2.2). Unfortunately, no such effort
has been made on audio documents yet.
An obvious way to summarize multiple spo-
ken documents is to adopt the transcribe-and-
summarize approach, in which automatic speech
recognition (ASR) is first employed to acquire
written transcripts. Speech summarization is ac-
cordingly reduced to a text summarization task
conducted on error-prone transcripts.
Such an approach, however, encounters several
problems. First, assuming the availability of ASR
is not always valid for many languages other than
English that one may want to summarize. Even
when it is, transcription quality is often an issue?
training ASR models requires collecting and an-
notating corpora on specific languages, dialects,
or even different domains. Although recognition
errors do not significantly impair extractive sum-
marizers (Christensen et al, 2004; Zhu and Penn,
2006), error-laden transcripts are not necessarily
browseable if recognition errors are higher than
certain thresholds (Munteanu et al, 2006). In
such situations, audio summaries are an alterna-
tive when salient content can be identified directly
from untranscribed audio. Third, the underlying
paradigm of most ASR models aims to solve a
549
classification problem, in which speech is seg-
mented and classified into pre-existing categories
(words). Words not in the predefined dictionary
are certain to be misrecognized without excep-
tion. This out-of-vocabulary (OOV) problem is
unavoidable in the regular ASR framework, al-
though it is more likely to happen on salient words
such as named entities or domain-specific terms.
Our approach uses acoustic evidence from the
untranscribed audio stream. Consider text sum-
marization first: many well-known models such
as MMR (Carbonell and Goldstein, 1998) and
MEAD (Radev et al, 2004) rely on the reoccur-
rence statistics of words. That is, if we switch
any word w1 with another word w2 across an
entire corpus, the ranking of extracts (often sen-
tences) will be unaffected, because no word-
specific knowledge is involved. These mod-
els have achieved state-of-the-art performance in
transcript-based speech summarization (Zechner,
2001; Penn and Zhu, 2008). For spoken docu-
ments, such reoccurrence statistics are available
directly from the speech signal. In recent years, a
variant of dynamic time warping (DTW) has been
proposed to find reoccurring patterns in the speech
signal (Park and Glass, 2008). This method has
been successfully applied to tasks such as word
detection (Park and Glass, 2006) and topic bound-
ary detection (Malioutov et al, 2007).
Motivated by the work above, this paper ex-
plores the approach to summarizing multiple spo-
ken documents directly over an untranscribed au-
dio stream. Such a model is of interest because of
its independence from ASR. It is directly applica-
ble to audio recordings in languages or domains
when ASR is not possible or transcription quality
is low. In principle, this approach is free from the
OOV problem inherent to ASR. The premise of
this approach, however, is to reliably find reoccur-
ing acoustic patterns in audio, which is challeng-
ing because of noise and pronunciation variance
existing in the speech signal, as well as the dif-
ficulty of finding alignments with proper lengths
corresponding to words well. Therefore, our pri-
mary goal in this paper is to empirically determine
the extent to which acoustic information alone can
effectively replace conventional speech recogni-
tion with or without simple prosodic feature de-
tection within the multi-document speech summa-
rization task. As shown below, a modification of
the Park-Glass approach amounts to the efficacy
of a 33-37% WER ASR engine in the domain
of multiple spoken document summarization, and
also has better treatment of OOV items. Park-
Glass similarity scores by themselves can attribute
a high score to distorted paths that, in our context,
ultimately leads to too many false-alarm align-
ments, even after applying the distortion thresh-
old. We introduce additional distortion penalty
and subpath length constraints on their scoring to
discourage this possibility.
2 Related work
2.1 Speech summarization
Although abstractive summarization is more de-
sirable, the state-of-the-art research on speech
summarization has been less ambitious, focus-
ing primarily on extractive summarization, which
presents the most important N% of words,
phrases, utterances, or speaker turns of a spo-
ken document. The presentation can be in tran-
scripts (Zechner, 2001), edited speech data (Fu-
rui et al, 2003), or a combination of these (He
et al, 2000). Audio data amenable to summa-
rization include meeting recordings (Murray et al,
2005), telephone conversations (Zhu and Penn,
2006; Zechner, 2001), news broadcasts (Maskey
and Hirschberg, 2005; Christensen et al, 2004),
presentations (He et al, 2000; Zhang et al, 2007;
Penn and Zhu, 2008), etc.
Although extractive summarization is not as
ideal as abstractive summarization, it outperforms
several comparable alternatives. Tucker and Whit-
taker (2008) have shown that extractive summa-
rization is generally preferable to time compres-
sion, which speeds up the playback of audio doc-
uments with either fixed or variable rates. He et
al. (2000) have shown that either playing back im-
portant audio-video segments or just highlighting
the corresponding transcripts is significantly bet-
ter than providing users with full transcripts, elec-
tronic slides, or both for browsing presentation
recordings.
Given the limitations associated with ASR, it is
no surprise that previous work (He et al, 1999;
Maskey and Hirschberg, 2005; Murray et al,
2005; Zhu and Penn, 2006) has studied features
available in audio. The focus, however, is pri-
marily limited to prosody. The assumption is that
prosodic effects such as stress can indicate salient
information. Since a direct modeling of compli-
cated compound prosodic effects like stress is dif-
550
ficult, they have used basic features of prosody in-
stead, such as pitch, energy, duration, and pauses.
The usefulness of prosody was found to be very
limited by itself, if the effect of utterance length is
not considered (Penn and Zhu, 2008). In multiple-
spoken-document summarization, it is unlikely
that prosody will be more useful in predicating
salience than in single document summarization.
Furthermore, prosody is also unlikely to be appli-
cable to detecting or handling redundancy, which
is prominent in the multiple-document setting.
All of the work above has been conducted on
single-document summarization. In this paper
we are interested in summarizing multiple spo-
ken documents by using reoccurrence statistics of
acoustic patterns.
2.2 Multiple-document summarization
Multi-document summarization on written text
has been studied for over a decade. Compared
with the single-document task, it needs to remove
more content, cope with prominent redundancy,
and organize content from different sources prop-
erly. This field has been pioneered by early work
such as the SUMMONS architecture (Mckeown
and Radev, 1995; Radev and McKeown, 1998).
Several well-known models have been proposed,
i.e., MMR (Carbonell and Goldstein, 1998), multi-
Gen (Barzilay et al, 1999), and MEAD (Radev
et al, 2004). Multi-document summarization has
received intensive study at DUC. 1 Unfortunately,
no such efforts have been extended to summarize
multiple spoken documents yet.
Abstractive approaches have been studied since
the beginning. A famous effort in this direction
is the information fusion approach proposed in
Barzilay et al (1999). However, for error-prone
transcripts of spoken documents, an abstractive
method still seems to be too ambitious for the time
being. As in single-spoken-document summariza-
tion, this paper focuses on the extractive approach.
Among the extractive models, MMR (Carbonell
and Goldstein, 1998) and MEAD (Radev et al,
2004), are possibly the most widely known. Both
of them are linear models that balance salience and
redundancy. Although in principle, these mod-
els allow for any estimates of salience and re-
dundancy, they themselves calculate these scores
with word reoccurrence statistics, e.g., tf.idf,
and yield state-of-the-art performance. MMR it-
1http://duc.nist.gov/
eratively selects sentences that are similar to the
entire documents, but dissimilar to the previously
selected sentences to avoid redundancy. Its de-
tails will be revisited below. MEAD uses a redun-
dancy removal mechanism similar to MMR, but
to decide the salience of a sentence to the whole
topic, MEAD uses not only its similarity score
but also sentence position, e.g., the first sentence
of each new story is considered important. Our
work adopts the general framework of MMR and
MEAD to study the effectiveness of the acoustic
pattern evidence found in untranscribed audio.
3 An acoustics-based approach
The acoustics-based summarization technique
proposed in this paper consists of three consecu-
tive components. First, we detect acoustic patterns
that recur between pairs of utterances in a set of
documents that discuss a common topic. The as-
sumption here is that lemmata, words, or phrases
that are shared between utterances are more likely
to be acoustically similar. The next step is to com-
pute a relatedness score between each pair of ut-
terances, given the matching patterns found in the
first step. This yields a symmetric relatedness ma-
trix for the entire document set. Finally, the relat-
edness matrix is incorporated into a general sum-
marization model, where it is used for utterance
selection.
3.1 Finding common acoustic patterns
Our goal is to identify subsequences within acous-
tic sequences that appear highly similar to regions
within other sequences, where each sequence con-
sists of a progression of overlapping 20ms vec-
tors (frames). In order to find those shared pat-
terns, we apply a modification of the segmen-
tal dynamic time warping (SDTW) algorithm to
pairs of audio sequences. This method is similar
to standard DTW, except that it computes multi-
ple constrained alignments, each within predeter-
mined bands of the similarity matrix (Park and
Glass, 2008).2 SDTW has been successfully ap-
plied to problems such as topic boundary detec-
tion (Malioutov et al, 2007) and word detection
(Park and Glass, 2006). An example application
of SDTW is shown in Figure 1, which shows the
results of two utterances from the TDT-4 English
dataset:
2Park and Glass (2008) used Euclidean distance. We used
cosine distance instead, which was found to be better on our
held-out dataset.
551
I: the explosion in aden harbor killed seven-
teen u.s. sailors and injured other thirty
nine last month.
II: seventeen sailors were killed.
These two utterances share three words: killed,
seventeen, and sailors, though in different orders.
The upper panel of Figure 1 shows a matrix of
frame-level similarity scores between these two
utterances where lighter grey represents higher
similarity. The lower panel shows the four most
similar shared subpaths, three of which corre-
spond to the common words, as determined by the
approach detailed below.
Figure 1: Using segmental dynamic time warping
to find matching acoustic patterns between two ut-
terances.
Calculating MFCC
The first step of SDTW is to represent each utter-
ance as sequences of Mel-frequency cepstral coef-
ficient (MFCC) vectors, a commonly used repre-
sentation of the spectral characteristics of speech
acoustics. First, conventional short-time Fourier
transforms are applied to overlapping 20ms Ham-
ming windows of the speech amplitude signal.
The resulting spectral energy is then weighted
by filters on the Mel-scale and converted to 39-
dimensional feature vectors, each consisting of 12
MFCCs, one normalized log-energy term, as well
as the first and second derivatives of these 13 com-
ponents over time. The MFCC features used in
the acoustics-based approach are the same as those
used below in the ASR systems.
As in (Park and Glass, 2008), an additional
whitening step is taken to normalize the variances
on each of these 39 dimensions. The similarities
between frames are then estimated using cosine
distance. All similarity scores are then normalized
to the range of [0, 1], which yields similarity ma-
trices exemplified in the upper panel of Figure 1.
Finding optimal paths
For each similarity matrix obtained above, local
alignments of matching patterns need to be found,
as shown in the lower panel of Figure 1. A sin-
gle global DTW alignment is not adequate, since
words or phrases held in common between utter-
ances may occur in any order. For example, in Fig-
ure 1 killed occurs before all other shared words in
one document and after all of these in the other, so
a single alignment path that monotonically seeks
the lower right-hand corner of the similarity ma-
trix could not possibly match all common words.
Instead, multiple DTWs are applied, each starting
from different points on the left or top edges of the
similarity matrix, and ending at different points on
the bottom or right edges, respectively. The width
of this diagonal band is proportional to the esti-
mated number of words per sequence.
Given an M -by-N matrix of frame-level simi-
larity scores, the top-left corner is considered the
origin, and the bottom-right corner represents an
alignment of the last frames in each sequence. For
each of the multiple starting points p0 = (x0, y0)
where either x0 = 0 or y0 = 0, but not neces-
sarily both, we apply DTW to find paths P =
p0, p1, ..., pK that maximize
?
0? i? K sim(pi),
where sim(pi) is the cosine similarity score of
point pi = (xi, yi) in the matrix. Each point on the
path, pi, is subject to the constraint |xi ? yi| < T ,
where T limits the distortion of the path, as we
determine experimentally. The ending points are
pK = (xK , yK) with either xK = N or yK =
M . For considerations of efficiency, the multi-
ple DTW processes do not start from every point
on the left or top edges. Instead, they skip every
T such starting points, which still guarantees that
there will be no blind-spot in the matrices that are
inaccessible to all DTW search paths.
Finding optimal subpaths
After the multiple DTW paths are calculated, the
optimal subpath on each is then detected in or-
der to find the local alignments where the simi-
larity is maximal, which is where we expect ac-
tual matched phrases to occur. For a given path
P = p0, p2, ..., pK , the optimal subpath is defined
to be a continuous subpath, P ? = pm, pm+1..., pn
552
that maximizes
?
m?i?n sim(pi)
n?m+1 , 0 ? n ? m ? k,
and m ? n + 1 ? L. That is, the subpath is at
least as long as L and has the maximal average
similarity. L is used to avoid short alignments that
correspond to subword segments or short function
words. The value of L is determined on a devel-
opment set.
The version of SDTW employed by (Malioutov
et al, 2007) and Park and Glass (2008) employed
an algorithm of complexity O(Klog(L)) from
(Lin et al, 2002) to find subpaths. Lin et al (2002)
have also proven that the length of the optimal sub-
path is between L and 2L? 1, inclusively. There-
fore, our version uses a very simple algorithm?
just search and find the maximum of average simi-
larities among all possible subpaths with lengths
between L and 2L ? 1. Although the theoreti-
cal upper bound for this algorithm is O(KL), in
practice we have found no significant increase in
computation time compared with the O(Klog(L))
algorithm?L is actually a constant for both Park
and Glass (2008) and us, it is much smaller than
K, and the O(Klog(L)) algorithm has (constant)
overhead of calculating right-skew partitions.
In our implementation, since most of the time is
spent on calculating the average similarity scores
on candidate subpaths, all average scores are
therefore pre-calculated incrementally and saved.
We have also parallelized the computation of sim-
ilarities by topics over several computer clusters.
A detailed comparison of different parallelization
techniques has been conducted by Gajjar et al
(2008). In addition, comparing time efficiency
between the acoustics-based approach and ASR-
based summarizers is interesting but not straight-
forward since a great deal of comparable program-
ming optimization needs to be additionally consid-
ered in the present approach.
3.2 Estimating utterance-level similarity
In the previous stage, we calculated frame-level
similarities between utterance pairs and used these
to find potential matching patterns between the
utterances. With this information, we estimate
utterance-level similarities by estimating the num-
bers of true subpath alignments between two utter-
ances, which are in turn determined by combining
the following features associated with subpaths:
Similarity of subpath
We compute similarity features on each subpath.
We have obtained the average similarity score of
each subpath as discussed in Section 3.1. Based
on this, we calculate relative similarity scores,
which are computed by dividing the original sim-
ilarity of a given subpath by the average similar-
ity of its surrounding background. The motivation
for capturing the relative similarity is to punish
subpaths that cannot distinguish themselves from
their background, e.g., those found in a block of
high-similarity regions caused by certain acoustic
noise.
Distortion score
Warped subpaths are less likely to correspond to
valid matching patterns than straighter ones. In
addition to removing very distorted subpaths by
applying a distortion threshold as in (Park and
Glass, 2008), we also quantitatively measured the
remaining ones. We fit each of them with least-
square linear regression and estimate the residue
scores. As discussed above, each point on a sub-
path satisfies |xi ? yi| < T , so the residue cannot
be bigger than T . We used this to normalize the
distortion scores to the range of [0,1].
Subpath length
Given two subpaths with nearly identical average
similarity scores, we suggest that the longer of the
two is more likely to refer to content of interest
that is shared between two speech utterances, e.g.,
named entities. Longer subpaths may in this sense
therefore be more useful in identifying similarities
and redundancies within a speech summarization
system. As discussed above, since the length of a
subpath len(P ?) has been proven to fall between
L and 2L ? 1, i.e., L ? len(P ?) ? 2L ? 1,
given a parameter L, we normalize the path length
to (len(P ?) ? L)/L, corresponding to the range
[0,1).
The similarity scores of subpaths can vary widely
over different spoken documents. We do not use
the raw similarity score of a subpath, but rather
its rank. For example, given an utterance pair, the
top-1 subpath is more likely to be a true alignment
than the rest, even if its distortion score may be
higher. The similarity ranks are combined with
distortion scores and subpath lengths simply as
follows. We divide subpaths into the top 1, 3, 5,
and 10 by their raw similarity scores. For sub-
paths in each group, we check whether their dis-
tortion scores are below and lengths are above
553
some thresholds. If they are, in any group, then
the corresponding subpaths are selected as ?true?
alignments for the purposes of building utterance-
level similarity matrix. The numbers of true align-
ments are used to measure the similarity between
two utterances. We therefore have 8 threshold pa-
rameters to estimate, and subpaths with similarity
scores outside the top 10 are ignored. The rank
groups are checked one after another in a decision
list. Powell?s algorithm (Press et al, 2007) is used
to find the optimal parameters that directly mini-
mize summarization errors made by the acoustics-
based model relative to utterances selected from
manual transcripts.
3.3 Extractive summarization
Once the similarity matrix between sentences in a
topic is acquired, we can conduct extractive sum-
marization by using the matrix to estimate both
similarity and redundancy. As discussed above,
we take the general framework of MMR and
MEAD, i.e., a linear model combining salience
and redundancy. In practice, we used MMR in our
experiments, since the original MEAD considers
also sentence positions 3 , which can always been
added later as in (Penn and Zhu, 2008).
To facilitate our discussion below, we briefly re-
visit MMR here. MMR (Carbonell and Goldstein,
1998) iteratively augments the summary with ut-
terances that are most similar to the document
set under consideration, but most dissimilar to the
previously selected utterances in that summary, as
shown in the equation below. Here, the sim1 term
represents the similarity between a sentence and
the document set it belongs to. The assumption is
that a sentence having a higher sim1 would better
represent the content of the documents. The sim2
term represents the similarity between a candidate
sentence and sentences already in the summary. It
is used to control redundancy. For the transcript-
based systems, the sim1 and sim2 scores in this
paper are measured by the number of words shared
between a sentence and a sentence/document set
mentioned above, weighted by the idf scores of
these words, which is similar to the calculation of
sentence centroid values by Radev et al (2004).
3The usefulness of position varies significantly in differ-
ent genres (Penn and Zhu, 2008). Even in the news domain,
the style of broadcast news differs from written news, for
example, the first sentence often serves to attract audiences
(Christensen et al, 2004) and is hence less important as in
written news. Without consideration of position, MEAD is
more similar to MMR.
Note that the acoustics-based approach estimates
this by using the method discussed above in Sec-
tion 3.2.
Nextsent = argmax
tnr,j
(? sim1(doc, tnr,j)
?(1 ? ?)maxtr,ksim2(tnr,j, tr,k))
4 Experimental setup
We use the TDT-4 dataset for our evaluation,
which consists of annotated news broadcasts
grouped into common topics. Since our aim in this
paper is to study the achievable performance of the
audio-based model, we grouped together news sto-
ries by their news anchors for each topic. Then we
selected the largest 20 groups for our experiments.
Each of these contained between 5 and 20 articles.
We compare our acoustics-only approach
against transcripts produced automatically from
two ASR systems. The first set of transcripts
was obtained directly from the TDT-4 database.
These transcripts contain a word error rate of
12.6%, which is comparable to the best accura-
cies obtained in the literature on this data set.
We also run a custom ASR system designed to
produce transcripts at various degrees of accu-
racy in order to simulate the type of performance
one might expect given languages with sparser
training corpora. These custom acoustic mod-
els consist of context-dependent tri-phone units
trained on HUB-4 broadcast news data by se-
quential Viterbi forced alignment. During each
round of forced alignment, the maximum likeli-
hood linear regression (MLLR) transform is used
on gender-dependent models to improve the align-
ment quality. Language models are also trained on
HUB-4 data.
Our aim in this paper is to study the achievable
performance of the audio-based model. Instead
of evaluating the result against human generated
summaries, we directly compare the performance
against the summaries obtained by using manual
transcripts, which we take as an upper bound to
the audio-based system?s performance. This ob-
viously does not preclude using the audio-based
system together with other features such as utter-
ance position, length, speaker?s roles, and most
others used in the literature (Penn and Zhu, 2008).
Here, we do not want our results to be affected by
them with the hope of observing the difference ac-
curately. As such, we quantify success based on
ROUGE (Lin, 2004) scores. Our goal is to evalu-
554
ate whether the relatedness of spoken documents
can reasonably be gleaned solely from the surface
acoustic information.
5 Experimental results
We aim to empirically determine the extent to
which acoustic information alone can effectively
replace conventional speech recognition within the
multi-document speech summarization task. Since
ASR performance can vary greatly as we dis-
cussed above, we compare our system against
automatic transcripts having word error rates of
12.6%, 20.9%, 29.2%, and 35.5% on the same
speech source. We changed our language mod-
els by restricting the training data so as to obtain
the worst WER and then interpolated the corre-
sponding transcripts with the TDT-4 original au-
tomatic transcripts to obtain the rest. Figure 2
shows ROUGE scores for our acoustics-only sys-
tem, as depicted by horizontal lines, as well as
those for the extractive summaries given automatic
transcripts having different WERs, as depicted
by points. Dotted lines represent the 95% con-
fidence intervals of the transcript-based models.
Figure 2 reveals that, typically, as the WERs of au-
tomatic transcripts increase to around 33%-37%,
the difference between the transcript-based and the
acoustics-based models is no longer significant.
These observations are consistent across sum-
maries with different fixed lengths, namely 10%,
20%, and 30% of the lengths of the source docu-
ments for the top, middle, and bottom rows of Fig-
ure 2, respectively. The consistency of this trend is
shown across both ROUGE-2 and ROUGE-SU4,
which are the official measures used in the DUC
evaluation. We also varied the MMR parameter ?
within a typical range of 0.4?1, which yielded the
same observation.
Since the acoustics-based approach can be ap-
plied to any data domain and to any language
in principle, this would be of special interest
when those situations yield relatively high WER
with conventional ASR. Figure 2 also shows the
ROUGE scores achievable by selecting utterances
uniformly at random for extractive summarization,
which are significantly lower than all other pre-
sented methods and corroborate the usefulness of
acoustic information.
Although our acoustics-based method performs
similarly to automatic transcripts with 33-37%
WER, the errors observed are not the same, which
0 0.1 0.2 0.3 0.4 0.5
0.7
0.75
0.8
0.85
0.9
0.95
1
Len=10% Rand=0.197
R
O
UG
E?
SU
4
Word error rate
0 0.1 0.2 0.3 0.4 0.5
0.7
0.75
0.8
0.85
0.9
0.95
1
Len=20%, Rand=0.340
R
O
UG
E?
SU
4
Word error rate
0 0.1 0.2 0.3 0.4 0.5
0.7
0.75
0.8
0.85
0.9
0.95
1
Len=30%, Rand=0.402
R
O
UG
E?
SU
4
Word error rate
0 0.1 0.2 0.3 0.4 0.5
0.7
0.75
0.8
0.85
0.9
0.95
1
Len=10%, Rand=0.176
R
O
UG
E?
2
Word error rate
0 0.1 0.2 0.3 0.4 0.5
0.7
0.75
0.8
0.85
0.9
0.95
1
Len=20%, Rand=0.324
R
O
UG
E?
2
Word error rate
0 0.1 0.2 0.3 0.4 0.5
0.7
0.75
0.8
0.85
0.9
0.95
1
Len=30%, Rand=0.389
R
O
UG
E?
2
Word error rate
Figure 2: ROUGE scores and 95% confidence in-
tervals for the MMR-based extractive summaries
produced from our acoustics-only approach (hori-
zontal lines), and from ASR-generated transcripts
having varying WER (points). The top, middle,
and bottom rows of subfigures correspond to sum-
maries whose lengths are fixed at 10%, 20%, and
30% the sizes of the source text, respectively. ? in
MMR takes 1, 0.7, and 0.4 in these rows, respec-
tively.
we attribute to fundamental differences between
these two methods. Table 1 presents the number
of different utterances correctly selected by the
acoustics-based and ASR-based methods across
three categories, namely those sentences that are
correctly selected by both methods, those ap-
pearing only in the acoustics-based summaries,
and those appearing only in the ASR-based sum-
maries. These are shown for summaries having
different proportional lengths relative to the source
documents and at different WERs. Again, correct-
ness here means that the utterance is also selected
when using a manual transcript, since that is our
defined topline.
A manual analysis of the corpus shows that
utterances correctly included in summaries by
555
Summ. Both ASR Aco.-
length only only
WER=12.6%
10% 85 37 8
20% 185 62 12
30% 297 87 20
WER=20.9%
10% 83 36 10
20% 178 65 19
30% 293 79 24
WER=29.2%
10% 77 34 16
20% 172 58 25
30% 286 64 31
WER=35.5%
10% 75 33 18
20% 164 54 33
30% 272 67 45
Table 1: Utterances correctly selected by both
the ASR-based models and acoustics-based ap-
proach, or by either of them, under different
WERs (12.6%, 20.9%, 29.2%, and 35.5%) and
summary lengths (10%, 20%, and 30% utterances
of the original documents)
the acoustics-based method often contain out-of-
vocabulary errors in the corresponding ASR tran-
scripts. For example, given the news topic of the
bombing of the U.S. destroyer ship Cole in Yemen,
the ASR-based method always mistook the word
Cole, which was not in the vocabulary, for cold,
khol, and called. Although named entities and
domain-specific terms are often highly relevant
to the documents in which they are referenced,
these types of words are often not included in
ASR vocabularies, due to their relative global rar-
ity. Importantly, an unsupervised acoustics-based
approach such as ours does not suffer from this
fundamental discord. At the very least, these find-
ings suggest that ASR-based summarization sys-
tems augmented with our type of approach might
be more robust against out-of-vocabulary errors.
It is, however, very encouraging that an acoustics-
based approach can perform to within a typical
WER range within non-broadcast-news domains,
although those domains can likewise be more
challenging for the acoustics-based approach. Fur-
ther experimentation is necessary. It is also of sci-
entific interest to be able to quantify this WER as
an acoustics-only baseline for further research on
ASR-based spoken document summarizers.
6 Conclusions and future work
In text summarization, statistics based on word
counts have traditionally served as the foundation
of state-of-the-art models. In this paper, the simi-
larity of utterances is estimated directly from re-
curring acoustic patterns in untranscribed audio
sequences. These relatedness scores are then in-
tegrated into a maximum marginal relevance lin-
ear model to estimate the salience and redundancy
of those utterance for extractive summarization.
Our empirical results show that the summarization
performance given acoustic information alone is
statistically indistinguishable from that of modern
ASR on broadcast news in cases where the WER
of the latter approaches 33%-37%. This is an en-
couraging result in cases where summarization is
required, but ASR is not available or speech recog-
nition performance is degraded. Additional anal-
ysis suggests that the acoustics-based approach
is useful in overcoming situations where out-of-
vocabulary error may be more prevalent, and we
suggest that a hybrid approach of traditional ASR
with acoustics-based pattern matching may be the
most desirable future direction of research.
One limitation of the current analysis is that
summaries are extracted only for collections of
spoken documents from among similar speakers.
Namely, none of the topics under analysis consists
of a mix of male and female speakers. We are cur-
rently investigating supervised methods to learn
joint probabilistic models relating the acoustics of
groups of speakers in order to normalize acoustic
similarity matrices (Toda et al, 2001). We sug-
gest that if a stochastic transfer function between
male and female voices can be estimated, then the
somewhat disparate acoustics of these groups of
speakers may be more easily compared.
References
R. Barzilay, K. McKeown, and M. Elhadad. 1999. In-
formation fusion in the context of multi-document
summarization. In Proc. of the 37th Association for
Computational Linguistics, pages 550?557.
J. G. Carbonell and J. Goldstein. 1998. The use of
mmr, diversity-based reranking for reordering doc-
uments and producing summaries. In Proceedings
of the 21st annual international ACM SIGIR con-
ference on research and development in information
retrieval, pages 335?336.
H. Christensen, B. Kolluru, Y. Gotoh, and S. Renals.
2004. From text summarisation to style-specific
556
summarisation for broadcast news. In Proceedings
of the 26th European Conference on Information Re-
trieval (ECIR-2004), pages 223?237.
S. Furui, T. Kikuichi, Y. Shinnaka, and C. Hori. 2003.
Speech-to-speech and speech to text summarization.
In First International workshop on Language Un-
derstanding and Agents for Real World Interaction.
M. Gajjar, R. Govindarajan, and T. V. Sreenivas. 2008.
Online unsupervised pattern discovery in speech us-
ing parallelization. In Proc. Interspeech, pages
2458?2461.
L. He, E. Sanocki, A. Gupta, and J. Grudin. 1999.
Auto-summarization of audio-video presentations.
In Proceedings of the seventh ACM international
conference on Multimedia, pages 489?498.
L. He, E. Sanocki, A. Gupta, and J. Grudin. 2000.
Comparing presentation summaries: Slides vs. read-
ing vs. listening. In Proceedings of ACM CHI, pages
177?184.
Y. Lin, T. Jiang, and Chao. K. 2002. Efficient al-
gorithms for locating the length-constrained heavi-
est segments with applications to biomolecular se-
quence analysis. J. Computer and System Science,
63(3):570?586.
C. Lin. 2004. Rouge: a package for automatic
evaluation of summaries. In Proceedings of the
42st Annual Meeting of the Association for Com-
putational Linguistics (ACL), Text Summarization
Branches Out Workshop, pages 74?81.
I Malioutov, A. Park, B. Barzilay, and J. Glass. 2007.
Making sense of sound: Unsupervised topic seg-
mentation over acoustic input. In Proc. ACL, pages
504?511.
S. Maskey and J. Hirschberg. 2005. Comparing lexial,
acoustic/prosodic, discourse and structural features
for speech summarization. In Proceedings of the
9th European Conference on Speech Communica-
tion and Technology (Eurospeech), pages 621?624.
K. Mckeown and D.R. Radev. 1995. Generating sum-
maries of multiple news articles. In Proc. of SIGIR,
pages 72?82.
C. Munteanu, R. Baecker, G Penn, E. Toms, and
E. James. 2006. Effect of speech recognition ac-
curacy rates on the usefulness and usability of we-
bcast archives. In Proceedings of SIGCHI, pages
493?502.
G. Murray, S. Renals, and J. Carletta. 2005.
Extractive summarization of meeting recordings.
In Proceedings of the 9th European Conference
on Speech Communication and Technology (Eu-
rospeech), pages 593?596.
A. Park and J. Glass. 2006. Unsupervised word ac-
quisition from speech using pattern discovery. Proc.
ICASSP, pages 409?412.
A. Park and J. Glass. 2008. Unsupervised pattern dis-
covery in speech. IEEE Trans. ASLP, 16(1):186?
197.
G. Penn and X. Zhu. 2008. A critical reassessment of
evaluation baselines for speech summarization. In
Proc. of the 46th Association for Computational Lin-
guistics, pages 407?478.
W.H. Press, S.A. Teukolsky, W.T. Vetterling, and B.P.
Flannery. 2007. Numerical recipes: The art of sci-
ence computing.
D. Radev and K. McKeown. 1998. Generating natural
language summaries from multiple on-line sources.
In Computational Linguistics, pages 469?500.
D. Radev, H. Jing, M. Stys, and D. Tam. 2004.
Centroid-based summarization of multiple docu-
ments. Information Processing and Management,
40:919?938.
T. Toda, H. Saruwatari, and K. Shikano. 2001. Voice
conversion algorithm based on gaussian mixture
model with dynamic frequency warping of straight
spectrum. In Proc. ICASPP, pages 841?844.
S. Tucker and S. Whittaker. 2008. Temporal compres-
sion of speech: an evaluation. IEEE Transactions
on Audio, Speech and Language Processing, pages
790?796.
K. Zechner. 2001. Automatic Summarization of Spo-
ken Dialogues in Unrestricted Domains. Ph.D. the-
sis, Carnegie Mellon University.
J. Zhang, H. Chan, P. Fung, and L Cao. 2007. Compar-
ative study on speech summarization of broadcast
news and lecture speech. In Proc. of Interspeech,
pages 2781?2784.
X. Zhu and G. Penn. 2006. Summarization of spon-
taneous conversations. In Proceedings of the 9th
International Conference on Spoken Language Pro-
cessing, pages 1531?1534.
557
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 764?772,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Improving Automatic Speech Recognition for Lectures through
Transformation-based Rules Learned from Minimal Data
Cosmin Munteanu??
?National Research Council Canada
46 Dineen Drive
Fredericton E3B 9W4, CANADA
Cosmin.Munteanu@nrc.gc.ca
Gerald Penn?
?University of Toronto
Dept. of Computer Science
Toronto M5S 3G4, CANADA
{gpenn,xzhu}@cs.toronto.edu
Xiaodan Zhu?
Abstract
We demonstrate that transformation-based
learning can be used to correct noisy
speech recognition transcripts in the lec-
ture domain with an average word error
rate reduction of 12.9%. Our method is
distinguished from earlier related work by
its robustness to small amounts of training
data, and its resulting efficiency, in spite of
its use of true word error rate computations
as a rule scoring function.
1 Introduction
Improving access to archives of recorded lectures
is a task that, by its very nature, requires research
efforts common to both Automatic Speech Recog-
nition (ASR) and Human-Computer Interaction
(HCI). One of the main challenges to integrating
text transcripts into archives of webcast lectures is
the poor performance of ASR systems on lecture
transcription. This is in part caused by the mis-
match between the language used in a lecture and
the predictive language models employed by most
ASR systems. Most ASR systems achieve Word
Error Rates (WERs) of about 40-45% in realis-
tic and uncontrolled lecture conditions (Leeuwis
et al, 2003; Hsu and Glass, 2006).
Progress in ASR for this genre requires both
better acoustic modelling (Park et al, 2005;
Fu?gen et al, 2006) and better language modelling
(Leeuwis et al, 2003; Kato et al, 2000; Munteanu
et al, 2007). In contrast to some unsupervised ap-
proaches to language modelling that require large
amounts of manual transcription, either from the
same instructor or on the same topic (Nanjo and
Kawahara, 2003; Niesler and Willett, 2002), the
solution proposed by Glass et al (2007) uses half
of the lectures in a semester course to train an
ASR system for the other half or for when the
course is next offered, and still results in signifi-
cant WER reductions. And yet even in this sce-
nario, the business case for manually transcrib-
ing half of the lecture material in every recorded
course is difficult to make, to say the least. Manu-
ally transcribing a one-hour recorded lecture re-
quires at least 5 hours in the hands of qualified
transcribers (Hazen, 2006) and roughly 10 hours
by students enrolled in the course (Munteanu et
al., 2008). As argued by Hazen (2006), any ASR
improvements that rely on manual transcripts need
to offer a balance between the cost of producing
those transcripts and the amount of improvement
(i.e. WER reductions).
There is some work that specializes in adap-
tive language modelling with extremely limited
amounts of manual transcripts. Klakow (2000)
filters the corpus on which language models are
trained in order to retain the parts that are more
similar to the correct transcripts on a particular
topic. This technique resulted in relative WER
reductions of between 7% and 10%. Munteanu
et al (2007) use an information retrieval tech-
nique that exploits lecture presentation slides, au-
tomatically mining the World Wide Web for doc-
uments related to the topic as attested by text
on the slides, and using these to build a better-
matching language model. This yields about an
11% relative WER reduction for lecture-specific
language models. Following upon other applica-
tions of computer-supported collaborative work to
address shortcomings of other systems in artificial
intelligence (von Ahn and Dabbish, 2004), a wiki-
based technique for collaboratively editing lecture
transcripts has been shown to produce entirely cor-
764
rected transcripts, given the proper motivation for
students to participate (Munteanu et al, 2008).
Another approach is active learning, where the
goal is to select or generate a subset of the avail-
able data that would be the best candidate for ASR
adaptation or training (Riccardi and Hakkani-Tur,
2005; Huo and Li, 2007).1 Even with all of these,
however, there remains a significant gap between
this WER and the threshold of 25%, at which lec-
ture transcripts have been shown with statistical
significance to improve student performance on
a typical lecture browsing task (Munteanu et al,
2006).
People have also tried to correct ASR output in
a second pass. Ringger and Allen (1996) treated
ASR errors as noise produced by an auxiliary
noisy channel, and tried to decode back to the per-
fect transcript. This reduced WER from 41% to
35% on a corpus of train dispatch dialogues. Oth-
ers combine the transcripts or word lattices (from
which transcripts are extracted) of two comple-
mentary ASR systems, a technique first proposed
in the context of NIST?s ROVER system (Fiscus,
1997) with a 12% relative error reduction (RER),
and subsequently widely employed in many ASR
systems.
This paper tries to correct ASR output using
transformation-based learning (TBL). This, too,
has been attempted, although on a professional
dictation corpus with a 35% initial WER (Peters
and Drexel, 2004). They had access to a very large
amount of manually transcribed data ? so large,
in fact, that the computation of true WER in the
TBL rule selection loop was computationally in-
feasible, and so they used a set of faster heuristics
instead. Mangu and Padmanabhan (2001) used
TBL to improve the word lattices from which the
transcripts are decoded, but this method also has
efficiency problems (it begins with a reduction of
the lattice to a confusion network), is poorly suited
to word lattices that have already been heavily
domain-adapted because of the language model?s
low perplexity, and even with higher perplexity
models (the SWITCHBOARD corpus using a lan-
1This work generally measures progress by reduction in
the size of training data rather than relative WER reduction.
Riccardi and Hakkani-Tur (2005) achieved a 30% WER with
68% less training data than their baseline. Huo and Li (2007)
worked on a small-vocabulary name-selection task that com-
bined active learning with acoustic model adaptation. They
reduced the WER from 15% to 3% with 70 syllables of acous-
tic adaptation, relative to a baseline that reduced the WER to
3% with 300 syllables of acoustic adaptation.
guage model trained over a diverse range of broad-
cast news and telephone conversation transcripts),
was reported to produce only a 5% WER reduc-
tion.
What we show in this paper is that a true WER
calculation is so valuable that a manual transcrip-
tion of only about 10 minutes of a one-hour lecture
is necessary to learn the TBL rules, and that this
smaller amount of transcribed data in turn makes
the true WER calculation computationally feasi-
ble. With this combination, we achieve a greater
average relative error reduction (12.9%) than that
reported by Peters and Drexel (2004) on their dic-
tation corpus (9.6%), and an RER over three times
greater than that of our reimplementation of their
heuristics on our lecture data (3.6%). This is on
top of the average 11% RER from language model
adaptation on the same data. We also achieve
the RER from TBL without the obligatory round
of development-set parameter tuning required by
their heuristics, and in a manner that is robust to
perplexity. Less is more.
Section 2 briefly introduces Transformation-
Based Learning (TBL), a method used in various
Natural Language Processing tasks to correct the
output of a stochastic model, and then introduces
a TBL-based solution for improving ASR tran-
scripts for lectures. Section 3 describes our exper-
imental setup, and Section 4 analyses its results.
2 Transformation-Based Learning
Brill?s tagger introduced the concept of
Transformation-Based Learning (TBL) (Brill,
1992). The fundamental principle of TBL is
to employ a set of rules to correct the output
of a stochastic model. In contrast to traditional
rule-based approaches where rules are manually
developed, TBL rules are automatically learned
from training data. The training data consist of
sample output from the stochastic model, aligned
with the correct instances. For example, in Brill?s
tagger, the system assigns POSs to words in a text,
which are later corrected by TBL rules. These
rules are learned from manually-tagged sentences
that are aligned with the same sentences tagged
by the system. Typically, rules take the form of
context-dependent transformations, for example
?change the tag from verb to noun if one of the
two preceding words is tagged as a determiner.?
An important aspect of TBL is rule scor-
ing/ranking. While the training data may suggest
765
a certain transformation rule, there is no guarantee
that the rule will indeed improve the system?s ac-
curacy. So a scoring function is used to rank rules.
From all the rules learned during training, only
those scoring higher than a certain threshold are
retained. For a particular task, the scoring func-
tion ideally reflects an objective quality function.
Since Brill?s tagger was first introduced, TBL
has been used for other NLP applications, includ-
ing ASR transcript correction (Peters and Drexel,
2004). A graphical illustration of this task is pre-
sented in Figure 1. Here, the rules consist of
Figure 1: General TBL algorithm. Transformation
rules are learned from the alignment of manually-
transcribed text (T ) with automatically-generated
transcripts (TASR) of training data, ranked accord-
ing to a scoring function (S) and applied to the
ASR output (T ?ASR) of test data.
word-level transformations that correct n-gram se-
quences. A typical challenge for TBL is the heavy
computational requirements of the rule scoring
function (Roche and Schabes, 1995; Ngai and
Florian, 2001). This is no less true in large-
vocabulary ASR correction, where large training
corpora are often needed to learn good rules over
a much larger space (larger than POS tagging, for
example). The training and development sets are
typically up to five times larger than the evaluation
test set, and all three sets must be sampled from the
same cohesive corpus.
While the objective function for improving the
ASR transcript is WER reduction, the use of this
for scoring TBL rules can be computationally pro-
hibitive over large data-sets. Peters and Drexel
(2004) address this problem by using an heuris-
tic approximation to WER instead, and it appears
that their approximation is indeed adequate when
large amounts of training data are available. Our
approach stands at the opposite side of this trade-
off ? restrict the amount of training data to a bare
minimum so that true WER can be used in the
rule scoring function. As it happens, the mini-
mum amount of data is so small that we can au-
tomatically develop highly domain-specific lan-
guage models for single 1-hour lectures. We show
below that the rules selected by this function lead
to a significant WER reduction for individual lec-
tures even if a little less than the first ten minutes of
the lecture are manually transcribed. This combi-
nation of domain-specificity with true WER leads
to the superior performance of the present method,
at least in the lecture domain (we have not experi-
mented with a dictation corpus).
Another alternative would be to change the
scope over which TBL rules are ranked and eval-
uated, but it is well known that globally-scoped
ranking over the entire training set at once is so
useful to TBL-based approaches that this is not
a feasible option ? one must either choose an
heuristic approach, such as that of Peters and
Drexel (2004) or reduce the amount of training
data to learn sufficiently robust rules.
2.1 Algorithm and Rule Discovery
As our proposed TBL adaptation operates di-
rectly on ASR transcripts, we employ an adapta-
tion of the specific algorithm proposed by Peters
and Drexel (2004), which is schematically repre-
sented in Figure 1. This in turn was adapted from
the general-purpose algorithm introduced by Brill
(1992).
The transformation rules are contextual word-
replacement rules to be applied to ASR tran-
scripts, and are learned by performing a word-
level alignment between corresponding utterances
in the manual and ASR transcripts of training
data, and then extracting the mismatched word
sequences, anchored by matching words. The
matching words serve as contexts for the rules?
application. The rule discovery algorithm is out-
lined in Figure 2; it is applied to every mismatch-
ing word sequence between the utterance-aligned
manual and ASR transcripts.
For every mismatching sequence of words, a set
766
? for every sequence of words c0w1 . . . wnc1 in the
ASR output that is deemed to be aligned with a
corresponding sequence c0w?1 . . . w?mc1 in the
manual transcript:
? add the following contextual replacements to the
set of discovered rules:
/ c0w1 . . . wnc1 / c0w?1 . . . w?mc1 /
/ c0w1 . . . wn / c0w?1 . . . w?m /
/ w1 . . . wnc1 / w?1 . . . w?mc1 /
/ w1 . . . wn / w?1 . . . w?m /
? for each i such that 1 ? i < min(n, m), add
the following contextual replacements to the set of
discovered rules:
/ c0w1 . . . wi / c0w?1 . . . w?a(i) /
/ wi+1 . . . wnc1 / w?a(i+1) . . . w?mc1 /
/ w1 . . . wi / w?1 . . . w?a(i) /
/ wi+1 . . . wn / w?a(i+1) . . . w?m /
Figure 2: The discovery of transformation rules.
of contextual replacement rules is generated. The
set contains the mismatched pair, by themselves
and together with three contexts formed from the
left, right, and both anchor context words. In
addition, all possible splices of the mismatched
pair and the surrounding context words are also
considered.2 Rules are shown here as replace-
ment expressions in a sed-like syntax. Given the
rule r = /w1 . . . wn/w?1 . . . w?m/, every instance
of the n-gram w1 . . . wn appearing in the current
transcript is replaced with the n-gram w?1 . . . w?m.
Rules cannot apply to their own output. Rules that
would result in arbitrary insertions of single words
(e.g. / /w1/) are discarded. An example of a rule
learned from transcripts is presented in Figure 3.
2.2 Scoring Function and Rule Application
The scoring function that ranks rules is the main
component of any TBL algorithm. Assuming a
relatively small size for the available training data,
a TBL scoring function that directly correlates
with WER can be conducted globally over the en-
tire training set. In keeping with TBL tradition,
however, rule selection itself is still greedily ap-
proximated. Our scoring function is defined as:
SWER(r, TASR, T ) = WER(TASR, T )
?WER(?(r, TASR), T ),
2The splicing preserves the original order of the word-
level utterance alignment, i.e., the output of a typical dynamic
programming implementation of the edit distance algorithm
(Gusfield, 1997). For this, word insertion and deletion oper-
ations are treated as insertions of blanks in either the manual
or ASR transcript.
Utterance-align ASR output and correct transcripts:
ASR: the okay one and you come and get your seats
Correct: ok why don?t you come and get your seats
?
Insert sentence delimiters (to serve as possible
anchors for the rules):
ASR: <s> the okay one and you come and get your seats </s>
Correct: <s> ok why don?t you come and get your seats </s>
?
Extract the mismatching sequence, enclosed by
matching anchors:
ASR: <s> the okay one and you
Correct: <s> ok why don?t you
?
Output all rules for replacing the incorrect ASR
sequence with the correct text, using the entire
sequence (a) or splices (b), with or without
surrounding anchors:
(a) the okay one and / ok why don?t
(a) the okay one and you / ok why don?t you
(a) <s> the okay one and / <s> ok why don?t
(a) <s> the okay one and you / <s> ok why don?t you
(b) the okay / ok
(b) <s> the okay / <s> ok
(b) one and / why don?t
(b) one and you / why don?t you
(b) the okay one / ok why
(b) <s> the okay one / <s> ok why
(b) and / don?t
(b) and you / don?t you
Figure 3: An example of rule discovery.
where ?(r, TASR) is the result of applying rule r
on text TASR.
As outlined in Figure 1, rules that occur in the
training sample more often than an established
threshold are ranked according to the scoring func-
tion. The ranking process is iterative: in each iter-
ation, the highest-scoring rule rbest is selected. In
subsequent iterations, the training data TASR are
replaced with the result of applying the selected
rule on them (TASR ? ?(rbest, TASR)) and the re-
maining rules are scored on the transformed train-
ing text. This ensures that the scoring and ranking
of remaining rules takes into account the changes
brought by the application of the previously se-
lected rules. The iterations stop when the scoring
function reaches zero: none of the remaining rules
improves the WER on the training data.
On testing data, rules are applied to ASR tran-
767
scripts in the same order in which they were se-
lected.
3 Experimental Design
Several combinations of TBL parameters were
tested with no tuning or modifications between
tests. As the proposed method was not refined dur-
ing the experiments, and since one of the goals of
our proposed approach is to eliminate the need for
developmental data sets, the available data were
partitioned only into training and test sets, with
one additional hour set aside for code development
and debugging.
It can be assumed that a one-hour lecture given
by the same instructor will exhibit a strong cohe-
sion, both in topic and in speaking style, between
its parts. Therefore, in contrast to typical TBL
solutions, we have evaluated our TBL-based ap-
proach by partitioning each 50 minute lecture into
a training and a test set, where the training set is
smaller than the test set. As mentioned in the intro-
duction, it is feasible to obtain manual transcripts
for the first 10 to 15 minutes of a lecture. As such,
the evaluation was carried out with two values for
the training size: the first fifth (TS = 20%) and
the first third (TS = 33%) of the lecture being
manually transcribed.
Besides the training size parameter, during all
experimental tests a second parameter was also
considered: the rule pruning threshold (RT ). As
described in Section 2.2, of all the rules learned
during the rule discovery step, only those that oc-
cur more often than the threshold are scored and
ranked. This parameter can be set as low as 1 (con-
sider all rules) or 2 (consider all rules that occur
at least twice over the training set). For larger-
scale tasks, the threshold serves as a pruning al-
ternative to the computational burden of scoring
several thousand rules. A large threshold could
potentially lead to discrediting low-frequency but
high-scoring rules. Due to the intentionally small
size of our training data for lecture TBL, the low-
est threshold was set to RT = 2. When a de-
velopment set is available, several values for the
RT parameter could be tested and the optimal one
chosen for the evaluation task. Since we used no
development set, we tested two more values for the
rule pruning threshold: RT = 5 and RT = 10.
Since our TBL solution is an extension of the
solution proposed in Peters and Drexel (2004),
their heuristic is our baseline. Their scoring func-
tion is the expected error reduction:
XER = ErrLen ? (GoodCnt?BadCnt),
a WER approximation computed over all instances
of rules applicable to the training set which reflects
the difference between true positives (the number
of times a rule is correctly applied to errorful tran-
scripts ? GoodCnt) and false positives (the in-
stances of correct text being unnecessarily ?cor-
rected? by a rule ? BadCnt). These are weighted
by the length in words (ErrLen) of the text area
that matches the left-hand side of the replacement.
3.1 Acoustic Model
The experiments were conducted using the
SONIC toolkit (Pellom, 2001). We used the
acoustic model distributed with the toolkit, which
was trained on 30 hours of data from 283 speak-
ers from the WSJ0 and WSJ1 subsets of the
1992 development set of the Wall Street Jour-
nal (WSJ) Dictation Corpus. Our own lectures
consist of eleven lectures of approximately 50
minutes each, recorded in three separate courses,
each taught by a different instructor. For each
course, the recordings were performed in different
weeks of the same term. They were collected in
a large, amphitheatre-style, 200-seat lecture hall
using the AKG C420 head-mounted directional
microphone. The recordings were not intrusive,
and no alterations to the lecture environment or
proceedings were made. The 1-channel record-
ings were digitized using a TASCAM US-122 au-
dio interface as uncompressed audio files with a
16KHz sampling rate and 16-bit samples. The au-
dio recordings were segmented at pauses longer
than 200ms, manually for one instructor and au-
tomatically for the other two, using the silence
detection algorithm described in Placeway et al
(1997). Our implementation was manually fine-
tuned for every instructor in order to detect all
pauses longer than 200ms while allowing a maxi-
mum of 20 seconds in between pauses.
The evaluation data are described in Table 1.
Four evaluations tasks were carried out; for in-
structor R, two separate evaluation sessions, R-1
and R-2, were conducted, using two different lan-
guage models.
The pronunciation dictionary was custom-built
to include all words appearing in the corpus on
which the language model was trained. Pronunci-
ations were extracted from the 5K-word WSJ dic-
tionary included with the SONIC toolkit and from
768
Evaluation
task name R-1 R-2 G-1 K-1
Instructor R. G. K.
Gender Male Male Female
Age Early 60s Mid 40s Early 40s
Segmentation manual automatic automatic
# lectures 4 3 4
Lecture topic Interactive Software Unix pro-
media design design gramming
Language model WSJ-5K WEB ICSISWB WSJ-5K
Table 1: The evaluation data.
the 100K-word CMU pronunciation dictionary.
For all models, we allowed one non-dictionary
word per utterance, but only for lines longer than
four words. For allowable non-dictionary words,
SONIC?s sspell lexicon access tool was used to
generate pronunciations using letter-to-sound pre-
dictions. The language models were trained us-
ing the CMU-CAM Language Modelling Toolkit
(Clarkson and R., 1997) with a training vocabu-
lary size of 40K words.
3.2 Language Models
The four evaluations were carried out using the
language models given in Table 1, either custom-
built for a particular topic or the baseline models
included in the SONIC toolkit, as follows:
WSJ-5K is the baseline model of the SONIC
toolkit. It is a 5K-word model built using the same
corpus as the base acoustic model included in the
toolkit.
ICSISWB is a 40K-word model created
through the interpolation of language models built
on the entire transcripts of the ICSI Meeting cor-
pus and the Switchboard corpus. The ICSI Meet-
ing corpus consists of recordings of university-
based multi-speaker research meetings, totaling
about 72 hours from 75 meetings (Janin et al,
2003). The Switchboard (SWB) corpus (Godfrey
et al, 1992) is a large collection of about 2500
scripted telephone conversations between approx-
imately 500 English-native speakers, suitable for
the conversational style of lectures, as also sug-
gested in (Park et al, 2005).
WEB is a language model built for each par-
ticular lecture, using information retrieval tech-
niques that exploit the lecture slides to automat-
ically mine the World Wide Web for documents
related to the presented topic. WEB adapts IC-
SISWB using these documents to build a language
model that better matches the lecture topic. It is
also a 40K-word model built on training corpora
with an average file size of approximately 200 MB
per lecture, and an average of 35 million word to-
kens per lecture.
It is appropriate to take the difference between
ICSISWB and WSJ-5K to be one of greater genre
specificity, whereas the difference between WEB
and ICSISWB is one of greater topic-specificity.
Our experiments on these three models (Munteanu
et al, 2007) shows that the topic adaptation pro-
vides nearly all of the benefit.
4 Results
Tables 2, 3 and 43 present the evaluation results
ICSISWB Lecture 1 Lecture 2 Lecture 3
TS = % 20 33 20 33 20 33
Initial WER 50.93 50.75 54.10 53.93 48.79 49.35
XER RT = 10 46.63 49.38 49.93 48.61 49.52 50.43
RT = 5 48.34 49.75 49.32 48.81 49.58 49.26
RT = 2 54.05 56.84 52.01 49.11 50.37 51.66
XER-NoS RT = 10 49.54 49.38 54.10 53.93 48.79 48.24
RT = 5 49.54 49.31 56.70 55.50 48.51 48.42
RT = 2 59.00 59.28 57.61 55.03 50.41 52.67
SWER RT = 10 46.63 46.53 49.80 48.44 45.83 45.42
RT = 5 46.63 45.60 47.75 47.23 44.76 44.44
RT = 2 44.48 44.30 47.46 47.02 43.60 44.13
Table 4: Experimental evaluation: WER values for
instructor G using the ICSISWB language model.
for instructors R and G. The transcripts were ob-
tained through ASR runs using three different lan-
guage models. The TBL implementation with our
scoring function SWER brings relative WER re-
ductions ranging from 10.5% to 14.9%, with an
average of 12.9%.
These WER reductions are greater than those
produced by the XER baseline approach. It is not
possible to provide confidence intervals since the
proposed method does not tune parameters from
sampled data (which we regard as a very positive
quality for such a method to have). Our specu-
lative experimentation with several values for TS
and RT , however, leads us to conclude that this
method is significantly less sensitive to variations
in both the training size TS and the rule pruning
threshold RT than earlier work, making it suitable
for application to tasks with limited training data
? a result somewhat expected since rules are vali-
dated through direct WER reductions over the en-
tire training set.
3Although WSJ-5K and ICSISWB exhibited nearly the
same WER in our earlier experiments on all lecturers, we
did find upon inspection of the transcripts in question that
ICSISWB was better interpretable on speakers that had more
casual speaking styles, whereas WSJ-5K was better on speak-
ers with more rehearsed styles. We have used whichever of
these baselines was the best interpretable in our experiments
here (WSJ-5K for R and K, ICSISWB for G).
769
WSJ-5K Lecture 1 Lecture 2 Lecture 3 Lecture 4
TS = % 20 33 20 33 20 33 20 33
Initial WER 50.48 50.93 51.31 51.90 50.28 49.23 54.39 54.04
XER RT = 10 49.97 49.82 49.27 49.77 46.85 48.08 52.17 50.58
RT = 5 50.01 50.07 49.99 51.13 48.39 47.37 50.91 49.62
RT = 2 49.87 51.75 49.52 51.13 47.13 47.31 52.70 50.56
XER-NoS RT = 10 47.25 46.82 49.98 48.72 48.44 45.21 51.37 49.73
RT = 5 49.03 48.78 47.37 51.25 47.84 44.07 49.54 48.97
RT = 2 52.21 53.47 49.31 52.29 50.85 49.41 50.63 51.81
SWER RT = 10 45.18 44.58 49.06 45.97 46.49 45.30 49.60 47.95
RT = 5 44.82 43.82 46.73 45.52 45.64 43.18 47.79 46.74
RT = 2 44.04 43.99 45.81 45.16 44.35 41.49 46.89 44.28
Table 2: Experimental evaluation: WER values for instructor R using the WSJ-5K language model.
WEB Lecture 1 Lecture 2 Lecture 3 Lecture 4
TS = % 20 33 20 33 20 33 20 33
Initial WER 45.54 45.85 43.36 43.87 46.69 47.14 49.78 49.38
XER RT = 10 42.91 43.90 42.44 43.81 46.78 45.35 46.92 49.65
RT = 5 43.45 43.81 42.65 44.37 46.90 42.12 47.34 46.04
RT = 2 43.26 45.46 44.19 44.66 43.77 45.12 61.54 60.40
XER-NoS RT = 10 43.51 42.97 42.11 41.98 44.66 46.59 47.24 46.30
RT = 5 44.96 42.98 40.01 40.52 44.66 41.74 47.23 44.35
RT = 2 46.72 48.16 44.79 45.87 40.44 44.32 61.84 64.40
SWER RT = 10 41.98 41.44 42.11 40.75 44.66 45.27 47.24 45.85
RT = 5 40.97 40.56 38.85 39.08 44.66 40.84 45.27 42.39
RT = 2 40.67 40.47 38.00 38.07 40.00 40.08 43.31 41.52
Table 3: Experimental evaluation: WER values for instructor R using the WEB language models.
As for how the transcripts improve, words with
lower information content (e.g., a lower tf.idf
score) are corrected more often and with more
improvement than words with higher information
content. The topic-specific language model adap-
tation that the TBL follows upon benefits words
with higher information content more. It is possi-
ble that the favour observed in TBL with SWER
towards lower information content is a bias pro-
duced by the preceding round of language model
adaptation, but regardless, it provides a much-
needed complementary effect. This can be ob-
served in Tables 2 and 3, in which TBL produces
nearly the same RER in either table for any lecture.
We have also extensively experimented with the
usability of lecture transcripts on human subjects
(Munteanu et al, 2006), and have found that task-
based usability varies in linear relation to WER.
An analysis of the rules selected by both TBL
implementations revealed that using the XER ap-
proximation leads to several single-word rules be-
ing selected, such as rules removing all instances
of frequent stop-words such as ?the? and ?for? or
pronouns such as ?he.? Therefore, an empirical
improvement (XER ?NoS) of the baseline was
implemented that, beside pruning rules below the
RT threshold, omits such single-word rules from
being selected. As shown in Tables 2, 3 and 4,
this restriction slightly improves the performance
of the approximation-based TBL for some values
of the RT and TS parameters, although it still
does not consistently match the WER reductions
of our scoring function.
Although the experimental evaluation shows
positive improvements in transcript quality
through TBL, in particular when using the SWER
scoring function, an exception is illustrated in
Table 5. The recordings for this evaluation were
collected from a course on Unix programming,
and lectures were highly interactive. Instructor
K used numerous examples of C or Shell code,
many of them being developed and tested in
class. While the keywords from a programming
language can be easily added to the ASR lexicon,
the pronunciation of such abbreviated forms (es-
pecially for Shell programming) and of mostly all
variable and custom function names proved to be
a significant difficulty for the ASR system. This,
combined with a high speaking rate and often
inconsistently truncated words, led to few TBL
rules occurring even above the lowest RT = 2
threshold (despite many TBL rules being initially
discovered).
As previously mentioned, one of the drawbacks
of global TBL rule scoring is the heavy compu-
tational burden. The experiments conducted here,
however, showed an average learning time of one
hour per one-hour lecture, reaching at most three
770
WSJ-5K Lecture 1 Lecture 2 Lecture 3 Lecture 4
TS = % 20 33 20 33 20 33 20 33
Initial WER 44.31 44.06 46.12 45.80 51.10 51.19 53.92 54.89
XER RT = 10 44.31 44.06 46.12 46.55 51.10 51.19 53.92 54.89
RT = 5 44.31 44.87 46.82 47.47 51.10 51.19 53.96 55.56
RT = 2 47.46 55.21 50.54 51.01 52.60 54.93 57.48 60.46
XER-NoS RT = 10 44.31 44.06 46.12 46.55 51.10 51.19 53.92 54.89
RT = 5 44.31 44.87 46.82 47.47 51.10 51.19 53.96 55.56
RT = 2 46.43 54.41 50.54 51.01 53.01 55.02 57.47 60.02
SWER RT = 10 44.31 44.06 46.12 45.80 51.10 51.19 53.92 54.89
RT = 5 44.31 44.05 46.11 45.88 51.10 51.19 53.92 54.89
RT = 2 44.34 44.07 46.03 45.89 50.96 50.93 54.01 55.16
Table 5: Experimental evaluation: WER values for instructor K using the WSJ-5K language model.
hours4 for a threshold of 2 when training over tran-
scripts for one third of a lecture. Therefore, it can
be concluded that, despite being computationally
more intensive than a heuristic approximation (for
which the learning time is on the order of just a
few minutes), a TBL system using a global, WER-
correlated scoring function not only produces bet-
ter transcripts, but also produces them in a feasible
amount of time with only a small amount of man-
ual transcription for each lecture.
5 Summary and Discussion
One of the challenges to reducing the WER of
ASR transcriptions of lecture recordings is the
lack of manual transcripts on which to train var-
ious ASR improvements. In particular, for one-
hour lectures given by different lecturers (such as,
for example, invited presentations), it is often im-
practical to manually transcribe parts of the lecture
that would be useful as training or development
data. However, transcripts for the first 10-15 min-
utes of a particular lecture can be easily obtained.
In this paper, we presented a solution that im-
proves the quality of ASR transcripts for lectures.
WER is reduced by 10% to 14%, with an average
reduction of 12.9%, relative to initial values. This
is achieved by making use of manual transcripts
from as little as the first 10 minutes of a one-hour
lecture. The proposed solution learns word-level
transformation-based rules that attempt to replace
parts of the ASR transcript with possible correc-
tions. The experimental evaluation carried out
over eleven lectures from three different courses
and instructors shows that this amount of manual
transcription can be sufficient to further improve a
lecture-specific ASR system.
4It should be noted that, in order to preserve compatibil-
ity with other software tools, the code developed for these
experiments was not optimized for speed. It is expected that
a dedicated implementation would result in even lower run-
times.
In particular, we demonstrated that a true WER-
based scoring function for the TBL algorithm is
both feasible and effective with a limited amount
of training data and no development data. The pro-
posed function assigns scores to TBL rules that di-
rectly correlate with reductions in the WER of the
entire training set, leading to a better performance
than that of a heuristic approximation. Further-
more, a scoring function that directly optimizes
for WER reductions is more robust to variations
in training size as well as to the value of the rule
pruning threshold. As little as a value of 2 can be
used for the threshold (scoring all rules that occur
at least twice), with limited impact on the com-
putational burden of learning the transformation
rules.
References
E. Brill. 1992. A simple rule-based part of speech
tagger. In Proc. 3rd Conf. on Applied NLP (ANLP),
pages 152 ? 155.
P.R. Clarkson and Rosenfeld R. 1997. Statistical lan-
guage modeling using the CMU-Cambridge Toolkit.
In Proc. Eurospeech, volume 1, pages 2707?2710.
J.G. Fiscus. 1997. A post-processing system to yield
reduced word error rates: Recognizer output voting
error reduction (ROVER). In Proc. IEEE Workshop
on Automatic Speech Recognition and Understand-
ing (ASRU), pages 347?354.
C. Fu?gen, M. Kolss, D. Bernreuther, M. Paulik,
S. Stu?ker, S. Vogel, and A. Waibel. 2006. Open
domain speech recognition & translation: Lectures
and speeches. In Proc. IEEE Conf. on Acoustics,
Speech, and Signal Processing (ICASSP), volume 1,
pages 569?572.
J. Glass, T.J. Hazen, S. Cyphers, I. Malioutov,
D. Huynh, and R. Barzilay. 2007. Recent progress
in the MIT spoken lecture processing project. In
Proc. 10th EuroSpeech / 8th InterSpeech, pages
2553?2556.
771
J. J. Godfrey, E. C. Holliman, and J. McDaniel. 1992.
SWITCHBOARD: Telephone speech corpus for re-
search and development. In Proc. IEEE Conf.
Acoustics, Speech, and Signal Processing (ICASSP),
pages 517?520.
D. Gusfield. 1997. Algorithms on Strings, Trees, and
Sequences. Cambridge University Press.
T.J. Hazen. 2006. Automatic alignment and error
correction of human generated transcripts for long
speech recordings. In Proc. 9th Intl. Conf. on Spo-
ken Language Processing (ICSLP) / InterSpeech,
pages 1606?1609.
B-J. Hsu and J. Glass. 2006. Style & topic lan-
guage model adaptation using HMM-LDA. In Proc.
ACL Conf. on Empirical Methods in NLP (EMNLP),
pages 373?381.
Q. Huo and W. Li. 2007. An active approach
to speaker and task adaptation based on automatic
analysis of vocabulary confusability. In Proc. 10th
EuroSpeech / 8th InterSpeech, pages 1569?1572.
A. Janin, Baron D., J. Edwards, D. Ellis, D. Gelbart,
N. Morgan, B. Peskin, T. Pfau, E. Shriberg, A. Stol-
cke, and C. Wooters. 2003. The ICSI meeting cor-
pus. In Proc. IEEE Conf. on Acoustics, Speech, and
Signal Processing (ICASSP), pages 364?367.
K. Kato, H. Nanjo, and T. Kawahara. 2000. Au-
tomatic transcription of lecture speech using topic-
independent language modeling. In Proc. Intl. Conf.
on Spoken Language Processing (ICSLP), volume 1,
pages 162?165.
D. Klakow. 2000. Selecting articles from the language
model training corpus. In Proc. IEEE Conf. on
Acoustics, Speech, and Signal Processing (ICASSP),
pages 1695?1698.
E. Leeuwis, M. Federico, and M. Cettolo. 2003. Lan-
guage modeling and transcription of the TED corpus
lectures. In Proc. Intl. Conf. on Acoustics, Speech,
and Signal Processing (ICASSP), volume 1, pages
232?235.
L. Mangu and M. Padmanabhan. 2001. Error correc-
tive mechanisms for speech recognition. In Proc.
IEEE Conf. on Acoustics, Speech, and Signal Pro-
cessing (ICASSP), pages 29?32.
C. Munteanu, R. Baecker, and G. Penn. 2008. Collab-
orative editing for improved usefulness and usabil-
ity of transcript-enhanced webcasts. In Proc. ACM
SIGCHI Conf. (CHI), pages 373?382.
C. Munteanu, R. Baecker, G. Penn, E. Toms, and
D. James. 2006. The effect of speech recognition
accuracy rates on the usefulness and usability of we-
bcast archives. In Proc. ACM SIGCHI Conf. (CHI),
pages 493?502.
C. Munteanu, G. Penn, and R. Baecker. 2007. Web-
based language modelling for automatic lecture tran-
scription. In Proc. 10th EuroSpeech / 8th Inter-
Speech, pages 2353?2356.
H. Nanjo and T. Kawahara. 2003. Unsupervised lan-
guage model adaptation for lecture speech recogni-
tion. In Proc. ISCA / IEEE Workshop on Sponta-
neous Speech Processing and Recognition (SSPR).
G. Ngai and R. Florian. 2001. Transformation-based
learning in the fast lane. In Proc. 2nd NAACL, pages
1?8.
T. Niesler and D. Willett. 2002. Unsupervised lan-
guage model adaptation for lecture speech transcrip-
tion. In Proc. Intl. Conf. on Spoken Language Pro-
cessing (ICSLP/Interspeech), pages 1413?1416.
A. Park, T. J. Hazen, and J. R. Glass. 2005. Auto-
matic processing of audio lectures for information
retrieval: Vocabulary selection and language model-
ing. In Proc. IEEE Conf. on Acoustics, Speech, and
Signal Processing (ICASSP).
B. L. Pellom. 2001. SONIC: The university of col-
orado continuous speech recognizer. Technical Re-
port #TR-CSLR-2001-01, University of Colorado.
J. Peters and C. Drexel. 2004. Transformation-based
error correction for speech-to-text systems. In Proc.
Intl. Conf. on Spoken Language Processing (IC-
SLP/Interspeech), pages 1449?1452.
P. Placeway, S. Chen, M. Eskenazi, U. Jain, V. Parikh,
B. Raj, M. Ravishankar, R. Rosenfeld, K. Seymore,
and M. Siegler. 1997. The 1996 HUB-4 Sphinx-3
system. In Proc. DARPA Speech Recognition Work-
shop.
G. Riccardi and D. Hakkani-Tur. 2005. Active learn-
ing: Theory and applications to automatic speech
recognition. IEEE Trans. Speech and Audio Pro-
cessing, 13(4):504?511.
E. K. Ringger and J. F. Allen. 1996. Error correction
via a post-processor for continuous speech recogni-
tion. In Proc. IEEE Conf. on Acoustics, Speech, and
Signal Processing (ICASSP), pages 427?430.
E. Roche and Y. Schabes. 1995. Deterministic part-of-
speech tagging with finite-state transducers. Com-
putational Linguistics, 21(2):227?253.
L. von Ahn and L. Dabbish. 2004. Labeling images
with a computer game. In Proc. ACM SIGCHI Conf.
(CHI), pages 319?326.
772
A Web-based Instructional Platform for Constraint-Based Grammar
Formalisms and Parsing
W. Detmar Meurers
Dept. of Linguistics
Ohio State University
dm@ling.osu.edu
Gerald Penn
Dept. of Computer Science
University of Toronto
gpenn@cs.toronto.edu
Frank Richter
Seminar fu?r Sprachwissenschaft
Universita?t Tu?bingen
fr@sfs.uni-tuebingen.de
Abstract
We propose the creation of a web-based
training framework comprising a set of
topics that revolve around the use of fea-
ture structures as the core data structure
in linguistic theory, its formal foundations,
and its use in syntactic processing.
1 Introduction
Feature structures have been used prolifically at ev-
ery level of linguistic theory, and they form the
mathematical foundation of our most comprehen-
sive and rigorous schools of syntactic theory, includ-
ing Lexical-Functional Grammar and Head-driven
Phrase Structure Grammar. This data structure is
popular because it shares many properties with the
first-order terms of classical logic, and in addi-
tion provides named access to substructures through
paths of features. Often it also includes a type sys-
tem reminiscent of the taxonomical classification
systems that are widely used in knowledge represen-
tation, psychology and the natural sciences.
For teaching a subject like computational linguis-
tics, which draws on a broad curriculum from many
traditional disciplines to audiences with mixed back-
grounds themselves, feature-structure-based theo-
retical and computational linguistics have three im-
portant properties. First, they are a mature disci-
pline, in which a great deal of accomplishments have
been made over the last 20 years, spanning from em-
pirical and conceptual advances in linguistic theory
to its mathematical and computational foundations,
to grammar development and efficient processing.
Second, they are pervasive as an already existing
representation standard for many levels of linguistic
study. Third, they are transparent, reducing com-
plex theories of grammar to a basic collection of
mathematical concepts and algorithms for answer-
ing formal questions about those theories. One can
address the distinction between descriptions of ob-
jects and the objects themselves, the difference be-
tween consistency and truth, and what it means for a
syntactic theory to be not only elegant but correct in
a precise and provable sense.
The purpose of this paper is to discuss how these
three properties can be cast into an instructional set-
ting to arrive at a framework for teaching computa-
tional linguistics that highlights the integrated nature
and precision with which work in this very hetero-
geneous discipline can be presented. In principle,
the framework we are proposing is open-ended, in
the sense that additional modules should be added
by students and other researchers, subject to the de-
sign principles given in Section 3. We are currently
designing three of the core modules for this frame-
work: formal foundations, constraint-based gram-
mar implementation, and parsing.
2 Problems of seminar-style courses
The contents of our core modules are based on a
series of previous seminar-style courses, in partic-
ular on constraint-based grammar implementation,
which also started integrating interactive compo-
nents and web-based materials into traditional face-
to-face teaching. These are described in detail in
Section 5. The traditional seminar-style teaching
method underlying the courses mentioned therein
                     July 2002, pp. 19-26.  Association for Computational Linguistics.
              Natural Language Processing and Computational Linguistics, Philadelphia,
         Proceedings of the Workshop on Effective Tools and Methodologies for Teaching
has a number of inherent problems, however. These
problems become particularly pressing when topics
as diverse as linguistic theory, grammar implemen-
tation, parsing, mathematical foundations of linguis-
tic theory and feature logics are combined in a single
course that is addressed to a mixed audience with
varying backgrounds in computer science, knowl-
edge representation, artificial intelligence and lin-
guistics, in any combination of these subjects.
First, the seminar-style teaching format as used in
those grammar implementation courses presupposes
a fairly coherent audience of linguists with a shared
background of linguistic knowledge. Second, since
computers are only used as a medium to implement
grammars and since the implementation platform is
not optimized for web-based training, it is neces-
sary that there be a relatively low number of stu-
dents per teacher. Third, the theoretical material is
in the form of overheads and research papers, which
are in electronic form but not easily accessible with-
out the accompanying lecture as part of a seminar-
style course. Fourth, the background lectures of the
courses lack the support of the kind of graphical,
interactive visualization that teaching software can
in principle offer. Finally, the courses follow a sin-
gle path through the materials as determined by the
teacher, which the student cannot change according
to their specific interests and their prior knowledge.
We believe that these shortcomings can be over-
come by shifting from a seminar-style to a web-
based training format in a way that preserves the
positive aspects of successful hands-on courses. On
the other hand, to successfully shift from seminar-
style to web-based training we believe it is essential
to do this based on a scientific understanding of the
nature and possibilities of web-based learning. In
the next section we therefore embed our work in the
context of education and collaborate learning tech-
nology research.
3 Education and collaborative learning
technology research
Our perspective on web-based training draws its in-
spiration primarily from work in building ?learn-
ing communities? in education research (Lin et al,
1995; Nonaka, 1994), in which:
1. a precise context is established to introduce
tacit knowledge and experience, in this case
on subjects in computational linguistics and the
traditional disciplines it draws from,
2. conflicting perspectives are shared, concepts
are objectified and submitted to a process of
justification and arbitration, and
3. the concepts are then integrated into the knowl-
edge base as modules upon which further in-
structional material or grammar implementa-
tions can be constructed.
We thus intend to provide an environment that
teaches students by actively encouraging them to
participate in research that extends our collective
knowledge in this area. In principle, there are no
boundaries to the material that could be included in
the evolving framework. We intend to make it avail-
able as an open-source standard for grammar de-
velopment and instruction in the hope that this will
encourage researchers and educators to contribute
modules to it, and to use a feature-structure based
approach for their own research and courses.
Scardamalia and Bereiter (1993) identify seven
global characteristics that technologies must have to
support this kind of participation:
Balance: a distinction between public and private
and between individual and group knowledge pro-
cesses. That includes free access to others? work, in-
cluding implementations of concepts as algorithms
or grammars, and opportunities to borrow ideas into
their own work that would be prohibitively time-
consuming or otherwise advanced to formulate on
their own. Such technologies must also encour-
age time for personal ?reflection and refinement?
and anonymous public or private contribution to the
knowledge space. The present framework achieves
this by providing an open-source setting combined
with a web-based instructional tool for self-paced
learning and individual design of both the contents
and order of the curriculum.
Contribution and notification: to prevent ideas
from being presented in an insulated structure that
discourages questioning, debate, or revision. As dis-
cussed in Section 4.2, this is achieved by providing
extensive linking and annotation of resources using
web-compatible metalanguages for integrating mod-
ules at the implementational, formal and instruc-
tional levels.
Source referencing: a means of preserving the
boundaries of a contributor?s idea and its credit as
well as a history of prior accounts and antecedents
to the idea. In the present framework, this is pro-
vided by means of a requirements analysis compo-
nent that requires contributed modules to identify
the contribution by new concepts or resources pro-
vided, existing concepts or resources imported for it
to work, and an account of existing alternatives with
a description of its distinction from them.
Storage and retrieval: which places contribu-
tions in a ?communal context? of related contribu-
tions by others to encourage joint work between con-
tributors working on problems with significant over-
lap. The present framework must organize the pre-
sentation of existing modules along several thematic
dimensions to accomplish this.
Multiple points of entry: for stu-
dents/contributors with different backgrounds
and levels of experience. Material is made acces-
sible in more basic or fundamental modules by
projecting the formal content of the subject into a
graphically based common-sense domain at which
it can be grasped more intuitively (see Section 4.3).
Accessibility in more advanced modules is provided
by links specified in the requirements analysis
component to more basic modules that the former
rely upon.
Coherence-producing mechanisms: feedback
to contributors and framework moderators of mod-
ules that are ?fading? for lack of attention or further
development. These can either be reinstated or refor-
mulated, moved to a private space of more periph-
eral modules, or deleted outright. This is a way of
encouraging activity that is productive, and restrict-
ing the chance of confusion or information overload.
Such a coherence mechanism must exist within this
framework.
Links to external resources: to situate the justifi-
cation and discussion of contributions in a wide con-
text. We make use of the web-based training plat-
form ILIAS1 which is available as open source soft-
ware and offers a high degree of flexibility in terms
of the integration of internal and external resources.
1http://www.ilias.uni-koeln.de/ios/index-e.html
4 Integration of the framework
The goal of our current work is to transform previ-
ous, seminar-style courses and new input into teach-
ing materials that are fit for web-based training in the
general framework outlined in the previous section.
This clearly involves much more than simply refor-
matting old teaching materials into web-compatible
formats. Instead, it requires an analysis of the con-
tents of the courses, the interleaving and hyperlink-
ing of the textual materials, and the development
of graphical, interactive solutions for presenting and
interacting with the content of the material. Since
the nature of the textual material as such is familiar
(instructional notes, reference guides to major sec-
tions with indices, system documentation, annotated
system source code, and annotated grammar source
code), we use the limited space in this paper to high-
light the integrated nature of the approach as well as
the web-based training specific issues of hyperlink-
ing and visualization.
4.1 Integration of linguistic and computational
aspects
Our approach is distinguished by its integration of
grammars, the parsers that use them and the on-
line instructional materials. Compared to the LKB
system2, which as mentioned in Section 5.2 has
also been used successfully in teaching grammar
development, the greater range of formal expres-
sive devices available to our parsing system, called
TRALE, allows for more readable and compact
grammars, which we believe to be of central impor-
tance in a teaching context. To illustrate this, we
are currently porting the LinGO3 English Resource
Grammar (ERG) from the LKB (on which the ERG
was designed) to the TRALE system.
Given the scope of our web-based training frame-
work as including an integrated module on parsing,
it is also relevant that the TRALE system itself can
be relatively compact and transparent at the source-
code level since it exploits its close affinity to the
underlying Prolog on which it is implemented. This
contrasts with the perspective of Copestake et al
(2001), who concede that the LKB is unsuitable for
teaching parsing.
2http://www-csli.stanford.edu/?aac/lkb.html
3http://lingo.stanford.edu/csli/
4.2 The use of hyperlinks
Several different varieties of links are distinguished
within the course material, giving a first-class repre-
sentation to the transfer of knowledge between the
linguistic, computational and mathematical sources
that inform this interdisciplinary area. We intend to
distinguish the following kinds of links:
Conceptual/taxonomical: connecting instances
of key concepts and terms used throughout the
course material with their definitions and prove-
nience;
Empirical context: connecting instances of de-
sign decisions, algorithms and formal definitions to
encyclopedic discussions of their linguistic motiva-
tion and empirical significance;
Denotational: connecting instances of construc-
tional terms and issues within linguistics as well as
correctness conditions of algorithms to the mathe-
matical definitions that formalize them within the
foundations of constraint-based linguistics;
Operational: connecting mathematical defini-
tions and instances of related linguistic discussions
to computational instructional material describing
the algorithms used to construct, refute or transform
the formal objects representing them in a practical
system;
Implementational: connecting discussions of al-
gorithms to the actual annotated system source code
in the TRALE system used to implement them, and
mathematical definitions and discussions of linguis-
tic constructions to the actual annotated grammar
source code used to represent them in a typical im-
plementation.
The idea behind this classification is that when
more course material is added to the web-based
training framework we are proposing, the new mate-
rial will take into account these distinctions to obtain
a conceptually coherent use of hyperlinks through-
out the framework.
4.3 Visualization
Our three core modules make use of a number of
graphical user interfaces: a tool for interleaved vi-
sualization and interaction with trees and attribute
value matrices, one for the presentation of lexical
rules and their interaction, an Emacs-based source-
level debugger, and a program for the graphical ex-
ploration of the formal foundations of typed feature
logic. The first two are extensions of tools we al-
ready used for our previous courses, and the third is
an extension of the ALE source-level debugger, so
we here focus on the last, new development.
The main goal of the MorphMoulder (MoMo) is
to project the formality of its subject, the formal
foundations of constraint languages over typed fea-
ture structures, onto a graphical level at which it can
be grasped more intuitively.4 The transparency of
this level is essential for providing multiple points
of entry (Section 3) to this fundamentally impor-
tant module. The MoMo tool allows the user to
explore the relationship between the two levels of
the formal architecture: the descriptions and the el-
ements described. To this end, the user works with
a graphical interface on a whiteboard. Labeled di-
rected graphs representing feature structures can be
constructed on the whiteboard from their basic com-
ponents, nodes and arcs. The nodes are depicted
as colored balls, which are assigned types, and the
arcs are depicted as arrows that may be labeled by
feature names. Once a feature structure has been
constructed, the user may examine its logical prop-
erties. The three main functions of the MoMo tool
allow one to check (1) whether a feature structure
complies with a given signature, (2) whether a well-
formed feature structure satisfies a description or a
set of descriptions, and (3) whether a well-formed
feature structure is a model of a description or a set
of descriptions. In the context of the course, the
functions of MoMo thus lead the user from under-
standing the well-formedness of feature structures
with respect to a signature to an understanding of
feature structures in their role as a logical model of
a theory. If a student has chosen course modules that
include a focus on formal foundations of feature log-
ics or feature logics based linguistic theory, the first
introduction to the subject by MoMo can easily be
followed up by a course module with rigorous math-
ematical definitions.
In constraint-based frameworks, the user declares
the primitives of the empirical domain in terms of
a type hierarchy with appropriate attributes and at-
tribute values. Consider a signature that licenses
lists of various birds, which may then be classified
according to certain properties. First of all, the sig-
4MoMo is written by Ekaterina Ovchinnikova, U. Tu?bingen.
nature needs to comprise a type hierarchy and fea-
ture appropriateness conditions for lists. Let type list
be an immediate supertype of the types non-empty-
list and empty-list in the type hierarchy (henceforth
abbreviated as nelist and elist). Let the appropri-
ateness conditions declare the attributes HEAD and
TAIL appropriate for (objects of) type nelist, the val-
ues of TAIL at nelist be of type list, and the values
of HEAD at type nelist be of type bird (for lists of
birds). Finally no attributes are appropriate for the
type elist. A typical choice for the interpretation of
that kind of signature in constraint-based formalisms
is the collection of totally well-typed and sort re-
solved feature structures. All nodes of totally well-
typed and sort resolved feature structures are of a
maximally specific type (types with no subtypes);
and they have outgoing arcs for all and only those
features that are appropriate to their type, with the
feature values again obeying appropriateness. Our
signature for lists thus declares an ontology of fea-
ture structures with nodes of type nelist or elist (but
never of type list), where the former must bear the
outgoing arcs HEAD and TAIL, and the latter have no
outgoing arcs. They signal the end of the list. The
HEAD values of non-empty lists must be in the de-
notation of the type bird.
Figure 1 illustrates how the MoMo tool can be
used to study the relationship between signatures
and the feature structures they license by letting
the user construct feature structures and interac-
tively explore whether particular feature structures
are well-formed according to the signature. To the
left of the whiteboard there are two clickable graph-
ics consoles of possible nodes and arcs from which
the user may choose to draw feature structures. The
consoles offer nodes of all maximally specific types
and arcs of all attributes that are declared in the
signature. In the present example, parrot, wood-
pecker, and canary are the maximally specific sub-
types of bird.
Each color of edge represents a different attribute,
and each color of node represents a different type.
The grayed outlines on edges and nodes indicate that
all of the respective edges and nodes in this partic-
ular example are licensed by the signature that was
provided. The HEAD arc originating at the node of
type elist, however, violates the appropriateness con-
ditions of the signature. The feature structure de-
Figure 1: Graphically evaluating well-typedness of
feature structures.
picted here, therefore, is not well-formed. The sig-
nature check thus fails on the given feature structure,
as indicated by the red light in the upper function
console to the right of the whiteboard.
Similarly, MoMo can graphically depict satisfia-
bility and modellability of a single description or set
of descriptions. To this end, the user may be asked to
construct a description that a given feature structure
satisfies or models; or she may be asked to construct
feature structures that satisfy or model a given de-
scription (or set of descriptions). The system will
give systematic feedback on the correct or incorrect
usage of the syntax of the description language as
well as on to which extent a feature structure satis-
fies or models descriptions, systematically guiding
the user to correct solutions.
Figure 2 shows a successful satisfiability check of
a well-formed feature structure. The feature struc-
ture is derived from the one in Figure 1 by re-
moving the incorrect HEAD arc and its substructure
from the elist node. The query, asked in a sepa-
rate window, is whether the feature structure satis-
fies the constraint (nelist, head:(parrot,
color:green), tail:nelist). Since this
is the case, the green light on the function console to
the right is signaling succeed. If we were to perform
model checking of the same feature structure against
the same constraint, checking would fail, and MoMo
would indicate the nodes of the feature structure that
do not satisfy the given constraint.
Figure 2: Graphically evaluating constraint satisfac-
tion of feature structures.
MoMo?s descriptions are a syntactic parallel to
TRALE?s descriptions, thus introducing the student
not only to the syntax and semantics of constraint
languages but also to the language that will be used
for the implementation of grammars later in the
course. The close relationship of description lan-
guages also facilitates a comparison of their model-
theoretic semantics and the truth conditions of gram-
mars with the structure and semantics of algorithms
that use descriptions for constraint resolution and in
parsing. Finally, their common structure allows for a
tight network of hyperlinks across the boundaries of
different course modules and course topics, linking
them to a common source of mathematical, imple-
mentational and linguistic indices, which explain the
usage of common mathematical concepts across the
different areas of application of typed feature struc-
tures.
5 From seminar-style courses to
web-based training
Having discussed the ideas driving the web-based
teaching platform and exemplified one of the tools,
we now return to the courses which have informed
our work on the three core modules currently being
developed in terms of their content and the use of a
web- and implementation environment they make.
5.1 Grammar implementation in ALE
ALE5 (Carpenter and Penn, 1996) is a conserva-
tive extension of Prolog based on typed feature
structures, with a built-in parser and semantic-head-
driven generator. The demand for such a utility
was so great when it was beta-released in 1992
that it immediately became the subject of early
work in graphical front-end development for large
constraint-based grammars: first with the Pleuk sys-
tem (Calder, 1993), then as one of several systems
supported by Gertjan van Noord?s HDrug6, followed
by an ALE-mode Emacs user interface (Laurens,
1995). It also provided the computational support
for one of the very first web-based computational
linguistics courses, Colin Matheson?s widely used
HPSG Development in ALE7. A follow-up course on
computational morphology8, also by Colin Mathe-
son, was based on ALE-RA9, a morphological ex-
tension of ALE by Tomaz Erjavec.
Our current web-based training module is sup-
ported by an extension of ALE, called TRALE,
that uses a slightly different interpretation of typing
found in many linguistic theories and an enhanced
constraint language that supports constraints with
complex antecedents (Penn, 2000).
5http://www.cs.toronto.edu/?gpenn/ale.html
6http://grid.let.rug.nl/?vannoord/hdrug/
7http://www.ltg.hcrc.ed.ac.uk/projects/ledtools/ale-hpsg/
8http://www.ltg.ed.ac.uk/projects/ledtools/ale-ra/
9http://nl.ijs.si/et/Thesis/ALE-RA/
5.2 Constraint-based grammar
implementation
Over the past five years, we have held another course
on Constraint-Based Grammar Implementation in
a variety of settings, from summer schools to reg-
ular curriculum courses.10 It offers hands-on ex-
perience to linguists interested in the formalization
of linguistic knowledge in a constraint-based gram-
mar formalism. The course is taught in an interac-
tive fashion in a computer laboratory and combines
background lectures with practical exercises on how
to specify grammars in ConTroll11 (Go?tz and Meur-
ers, 1997), a processing system for constraint-based
grammars intended to process with HPSG theories
directly from the form in which they are constructed
by linguists.
The background lectures of the Constraint-based
grammar implementation courses introduce the rel-
evant mathematical and computational knowledge
and focus on the main ingredients of constraint-
based grammars: highly structured lexical represen-
tations, constituent structures, and the encoding of
well-formedness constraints on grammatical repre-
sentations. In the lab, students work on exercises
exploring the theoretical concepts covered in the lec-
tures. In a later part of the course, they are given
the opportunity to undertake individualized gram-
mar projects for modeling theoretically and empir-
ically significant syntactic constructions of their na-
tive language.
This course was the first hands-on computational
syntax course at the European Summer School
in Language, Logic, and Information (ESSLLI,
1997: Aix-en-Provence), and was also offered at the
LSA Linguistic Institute (1999: University of Illi-
nois, Urbana-Champaign)12 and the Computational
Linguistics and Represented Knowledge (CLaRK)
Summer School (1999: Eberhard-Karls Universita?t,
Tu?bingen)13. Generally regarded as a highly suc-
cessful course and teaching method, every subse-
quent ESSLLI summer school has offered at least
one similar course: Practical HPSG Grammar Engi-
neering (1998: Ann Copestake, Dan Flickinger, and
10The courses were taught by E. Hinrichs and D. Meurers.
11http://www.sfs.uni-tuebingen.de/controll/
12http://ling.osu.edu/?dm/lehre/lsa99/
13http://ling.osu.edu/?dm/lehre/clark99/
Stephan Oepen)14, Development of large scale LFG
grammars: Linguistics, Engineering and Resources
(1999: Miriam Butt, Annette Frank, and Jonas
Kuhn)15, Grammatical Resources: Logic, Struc-
ture, Control (1999: Michael Moortgat and Richard
T. Oehrle)16, An Introduction to Grammar Engi-
neering using HPSG (2000: Ann Copestake, Rob
Malouf)17, Advanced Grammar Engineering using
HPSG (2000: Dan Flickinger, Stephan Oepen)18,
and An Introduction to Stochastic Attribute-Value
Grammars (2001: Rob Malouf, Miles Osborne)19.
5.3 Introduction to theory-driven CL
A further source of material for the core modules
of our web-based training framework is the graduate
level Introduction to Theory-driven Computational
Linguistics at the Ohio State University.20 It covers
the basic issues of the following topics: finite state
automata and transducers, formal language theory,
computability and complexity, recognizers/parsers
for context free grammars, memoization, and pars-
ing with complex categories.
The theoretical material is combined with prac-
tical exercises in Prolog implementing different as-
pects of parsers. At the end of the course, students
complete a project consisting of building and testing
a grammar fragment for a short English text of their
choice. The traditional one-quarter course includes
weekly exercises, extensive web-based course mate-
rial for students, and a course workbook21 as a guide
through the theoretical material.
5.4 Model-theoretic introduction to Syntax
Our approach to teaching the fundamentals of math-
ematical theories through graphical metaphors in
the context of syntax derives from our experience
with this method in teaching Syntax I (HPSG) at
the Eberhard-Karls Universita?t Tu?bingen in 1998,
14http://www.coli.uni-sb.de/esslli/Seiten/Oepen.html
15http://www.let.uu.nl/esslli/Courses/butt.html
16http://www.let.uu.nl/esslli/Courses/moortgat-oehrle.html
17http://www.cs.bham.ac.uk/?esslli/notes/copestake.html
18http://www.cs.bham.ac.uk/?esslli/notes/oepen.html
19http://odur.let.rug.nl/?malouf/esslli01/
20The course was taught by D. Meurers; see http://ling.osu.
edu/?dm/2001/winter/684.01/
21This workbook is based, with kind permission from the
authors, on the module workbook for ?Techniques in Natural
Language Processing 1? by Chris Mellish, Pete Whitelock and
Graeme Ritchie, 1994, Dept. of AI, University of Edinburgh.
1999 and 2001.22 In these seminars, which did not
presuppose any prior knowledge of model-theoretic
methods in logic, the mathematical foundations of
feature logic were introduced by intuitive means but
with as much precision as possible without strict for-
malization. An introduction to a standardized ver-
sion of the logical description language of HPSG
was accompanied with problem sets that required
the students to construct three-dimensional feature
structure models (made of styrofoam and wires) of
descriptions and sets of descriptions. The informal
but very concrete understanding of the relationship
between a theory cast in a constraint language and its
feature structure models had a very positive result on
students? ability to grasp and build working analyses
of unseen constructions compared to the results of
the more traditional method of teaching constraint-
based syntax used in previous years. At the same
time, the teaching method successfully used an ap-
peal to prior world knowledge rather than unfamiliar
mathematical notation in order to make the students
familiar with the basic concepts of constraint satis-
faction and truth in feature logics.
6 Summary and Outlook
The interdisciplinary nature of computational lin-
guistics and the diverse backgrounds of the student
audience makes it particularly attractive to teach a
subject like constraint-based grammar formalisms
and parsing using a web-based instructional plat-
form which integrates formal and computational
foundations, linguistic theory, and grammar im-
plementation. We discussed several seminar-style
courses which have informed our proposal in terms
of content, highlighted the problems of the tradi-
tional face-to-face teaching, and described our en-
vironment of web-based teaching materials plus im-
plementational support. We argued that a web-based
training framework for the topic can be organized
around feature structures as a central data structure
in formal foundations, linguistics and implementa-
tion. We outlined the educational and collaborative
learning background in which an informed proposal
on web-based training must be embedded and used
the newly developed tool MoMo as an illustration
22The courses were taught by F. Richter and M. Sailer; see
http://www.sfs.uni-tuebingen.de/?fr/teaching/
of how we envisage projecting the formal content of
the subject into a graphically based common-sense
domain in which it can be grasped more intuitively.
The three core modules on formal founda-
tions, constraint-based grammar implementation,
and parsing will be completed and made publicly
available at the end of 2003. The joint project
is funded by the German Federal Ministry for Re-
search Technology (BMBF) as part of the consor-
tium Media-intensive teaching modules in the com-
putational linguistics curriculum (MiLCA).23
References
J. Calder. 1993. Graphical interaction with constraint-
based grammars. In Proceedings of PACLING ?93,
pages 160?168, Vancouver, British Columbia.
B. Carpenter and G. Penn. 1996. Compiling typed
attribute-value logic grammars. In H. Bunt and
M. Tomita, editors, Recent Advances in Parsing Tech-
nologies, pages 145?168. Kluwer, Dordrecht.
A. Copestake, J. Carroll, D. Flickinger, R. Malouf, and
S. Oepen. 2001. Using an open-source unification-
based system for CL/NLP teaching. In Proceedings
of the EACL/ACL Workshop on Sharing Tools and Re-
sources for Research and Education, pages 35?38.
T. Go?tz and W. D. Meurers. 1997. The ConTroll system
as large grammar development platform. In Proceed-
ings of the EACL/ACL Workshop on Computational
Environments for Grammar Development and Linguis-
tic Engineering, pages 38?45. http://ling.osu.edu/?dm/
papers/envgram.html.
O. Laurens. 1995. An Emacs user interface for ALE.
Technical Report CSS-IS TR 95-07, School of Com-
puting Science, Simon Fraser University.
X. Lin, J.D. Bransford, and C.E. Hmelo. 1995. Instruc-
tional design and development of learning communi-
ties: an invitation to dialogue. Educational Technol-
ogy, 35(5):53?63.
I. Nonaka. 1994. A dynamic theory of organizational
knowledge creation. Organizational Science, 5(1).
G. Penn. 2000. Applying Constraint Handling Rules
to HPSG. In Proceedings of the Workshop on Rule-
Based Constraint Reasoning and Programming, CL
2000.
M. Scardamalia and C. Bereiter. 1993. Technologies
for knowledge-building discourse. Communications
of the ACM, 36(5):37?41.
23http://milca.sfs.uni-tuebingen.de/A4/HomePage/top.html
Proceedings of the 3rd Workshop on Constraints and Language Processing (CSLP-06), pages 9?16,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Control Strategies for Parsing with Freer Word-Order Languages
Gerald Penn
Dept. of Computer Science
University of Toronto
Toronto M5S 3G4, Canada
Stefan Banjevic
Dept. of Mathematics
University of Toronto
Toronto M5S 2E4, Canada
fgpenn,banjevic,mpademkog@cs.toronto.edu
Michael Demko
Dept. of Computer Science
University of Toronto
Toronto M5S 3G4, Canada
Abstract
We provide two different methods for
bounding search when parsing with freer
word-order languages. Both of these can
be thought of as exploiting alternative
sources of constraints not commonly used
in CFGs, in order to make up for the lack
of more rigid word-order and the standard
algorithms that use the assumption of rigid
word-order implicitly. This work is pre-
liminary in that it has not yet been evalu-
ated on a large-scale grammar/corpus for a
freer word-order language.
1 Introduction
This paper describes two contributions to the
area of parsing over freer word-order (FWO) lan-
guages, i.e., languages that do not readily admit a
semantically transparent context-free analysis, be-
cause of a looser connection between grammati-
cal function assignment and linear constituent or-
der than one finds in English. This is a partic-
ularly ripe area for constraint-based methods be-
cause such a large number of linguistic partial
knowledge sources must be brought to bear on
FWO parsing in order to restrict its search space to
a size comparable to that of standard CFG-based
parsing.
The first addresses the indexation of tabled sub-
strings in generalized chart parsers for FWO lan-
guages. While chart parsing can famously be cast
as deduction (Pereira and Warren, 1983), what
chart parsing really is is an algebraic closure over
the rules of a phrase structure grammar, which is
most naturally expressed inside a constraint solver
such as CHR (Morawietz, 2000). Ideally, we
would like to use standard chart parsers for FWO
languages, but because of the constituent ordering
constraints that are implicit in the right-hand-sides
(RHSs) of CFG rules, this is not possible without
effectively converting a FWO grammar into a CFG
by expanding its rule system exponentially into all
possible RHS orders (Barton et al, 1987). FWO
grammar rules generally cannot be used as they
stand in a chart parser because tabled substrings
record a non-terminal category C derived over a
contiguous subspan of the input string from word
i to word j. FWO languages have many phrasal
categories that are not contiguous substrings.
Johnson (1985), Reape (1991) and others have
suggested using bit vectors to index chart edges
as an alternative to substring spans in the case of
parsing over FWO languages, but that is really
only half of the story. We still need a control strat-
egy to tell us where we should be searching for
some constituent at any point in a derivation. This
paper provides such a control strategy, using this
data structure, for doing search more effectively
with a FWO grammar.
The second contribution addresses another
source of constraints on the search space: the
length of the input. While this number is not a
constant across parses, it is constant within a sin-
gle parse, and there are functions that can be pre-
computed for a fixed grammar which relate tight
upper and lower bounds on the length of the in-
put to both the height of a parse tree and other
variables (defined below) whose values bound the
recursion of the fixed phrase structure rule sys-
tem. Iteratively computing and caching the val-
ues of these functions as needed allows us to in-
vert them efficiently, and bound the depth of the
search. This can be thought of as a partial substi-
tute for the resource-bounded control that bottom-
up parsing generally provides, Goal-directedness
9
is maintained, because ? with the use of con-
straint programming ? it can still be used inside
a top-down strategy. In principle, this could be
worthwhile to compute for some CFGs as well, al-
though the much larger search space covered by a
na??ve bottom-up parser in the case of FWO gram-
mars (all possible subsequences, rather than all
possible contiguous subsequences), makes it con-
siderably more valuable in the present setting.
In the worst case, a binary-branching immediate
dominance grammar (i.e., no linear precedence)
could specify that every word belongs to the same
category, W , and that phrases can be formed from
every pair of words or phrases. A complete pars-
ing chart in this case would have exponentially
many edges, so nothing in this paper (or in the
aforementioned work on bit vectors) actually im-
proves the asymptotic complexity of the recogni-
tion task. Natural languages do not behave like
this, however. In practice, one can expect more
polymorphy in the part-of-speech/category sys-
tem, more restrictions in the allowable combina-
tions of words and phrases (specified in the imme-
diate dominance components of a phrase structure
rule system), and more restrictions in the allow-
able orders and discontinuities with which those
argument categories can occur (specified in the
linear precedence components of a phrase struc-
ture rule system).
These restrictions engender a system of con-
straints that, when considered as a whole, admit
certain very useful, language-dependent strategies
for resolving the (respectively, don?t-care) nonde-
terministic choice points that a (resp., all-paths)
parser must face, specifically: (1) which lexical
categories to use (or, resp., in which order), given
the input words, (2) which phrase structure rules
to apply (resp., in which order), and (3) given a
particular choice of phrase structure rule, in which
order to search for the argument categories on its
right-hand side (this one is don?t-care nondeter-
ministic even if the parser is looking for only the
best/first parse). These heuristics are generally ob-
tained either through the use of a parameter esti-
mation method over a large amount of annotated
data, or, in the case of a manually constructed
grammar, simply through some implicit conven-
tion, such as the textual order in which the lexicon,
rule system, or RHS categories are stated.1
1In the case of the lexicon and rule system, there is a very
long-standing tradition in logic programming of using this
This paper does not address how to find these
heuristics. We assume that they exist, and instead
address the problem of adapting a chart parser
to their efficient use. To ignore this would in-
volve conducting an enormous number of deriva-
tions, only to look in the chart at the end and
discover that we have already derived the current
bit-vector/category pair. In the case of standard
CFG-based parsing, one generally avoids this by
tabling so-called active edges, which record the
subspaces on which a search has already been ini-
tiated. This works well because the only existen-
tially quantified variables in the tabled entry are
the interior nodes in the span which demarcate
where one right-hand-side category ends and an-
other adjacent one begins. To indicate that one is
attempting to complete the rule, S ! NP V P ,
for example, one must only table the search from
i to j for some k, such that NP is derivable from
i to k and V P is derivable from k to j. Our first
contribution can be thought of as a generalization
of these active edges to the case of bit vectors.
2 FWO Parsing as Search within a
Powerset Lattice
A standard chart-parser views constituents as ex-
tending over spans, contiguous intervals of a lin-
ear string. In FWO parsing, constituents partition
the input into not necessarily contiguous subse-
quences, which can be thought of as bit vectors
whose AND is 0 and whose OR is 2n   1, given an
initial n-length input string. For readability, and
to avoid making an arbitrary choice as to whether
the leftmost word should correspond to the most
significant or least significant bit, we will refer
to these constituents as subsets of f1 : : : ng rather
than as n-length bit vectors. For simplicity and
because of our heightened awareness of the im-
portance of goal-directedness to FWO parsing (see
the discussion in the previous section), we will
only outline the strictly top-down variant of our
strategy, although natural analogues do exist for
the other orientations.
2.1 State
State is: hN;CanBV;ReqBVi.
The returned result is: UsedBV or failure.
convention. To our knowledge, the first to apply it to the order
of RHS categories, which only makes sense once one drops
the implicit linear ordering implied by the RHSs of context-
free grammar rules, was Daniels and Meurers (2002).
10
Following Penn and Haji-Abdolhosseini
(2003), we can characterize a search state under
these assumptions using one non-terminal, N , and
two subsets/bit vectors, the CanBV and ReqBV.2
CanBV is the set of all words that can be used
to build an N , and ReqBV is the set of all words
that must be used while building the N . CanBV
always contains ReqBV, and what it additionally
contains are optional words that may or may not
be used. If search from this state is successful,
i.e., N is found using ReqBV and nothing that
is not in CanBV, then it returns a UsedBV, the
subset of words that were actually used. We will
assume here that our FWO grammars are not so
free that one word can be used in the derivation of
two or more sibling constituents, although there is
clearly a generalization to this case.
2.2 Process
Search(hN;C;Ri) can then be defined in the
constraint solver as follows:
2.2.1 Initialization
A top-down parse of an n-length string be-
gins with the state consisting of the distinguished
category, S, of the grammar, and CanBV =
ReqBV = f1 : : : ng.
2.2.2 Active Edge Subsumption
The first step is to check the current state against
states that have already been considered. For ex-
pository reasons, this will be presented below. Let
us assume for now that this step always fails to
produce a matching edge. We must then predict
using the rules of the FWO grammar.
2.2.3 Initial Prediction
hN;C;Ri =) hN
1
; C; i, where:
1. N
0
! N
1
: : : N
k
,
2. k > 1, and
3. N tN
0
#.
As outlined in Penn and Haji-Abdolhosseini
(2003), the predictive step from a state consisting
of hN;C;Ri using an immediate dominance rule,
N
0
! N
1
: : : N
k
, with k > 1 and no linear prece-
dence constraints transits to a state hN
1
; C; i pro-
vided that N is compatible with N
0
. In the case
of a classical set of atomic non-terminals, com-
patibility should be interpreted as equality. In the
2Actually, Penn and Haji-Abdolhosseini (2003) use
CanBV and OptBV, which can be defined as CanBV \
ReqBV.
case of Prolog terms, as in definite clause gram-
mars, or typed feature structures, as in head-driven
phrase structure grammar, compatibility can be in-
terpreted as either unifiability or the asymmetric
subsumption of N by N
0
. Without loss of gener-
ality, we will assume unifiability here.
This initial predictive step says that there are,
in general, no restrictions on which word must be
consumed (ReqBV = ). Depending on the lan-
guage chosen for expressing linear precedence re-
strictions, this set may be non-empty, and in fact,
the definition of state used here may need to be
generalized to something more complicated than
a single set to express the required consumption
constraints.
2.2.4 Subsequent Prediction
hN;C;Ri =) hN
j+1
; C
j
; i, where:
1. N
0
! N
1
: : : N
k
,
2. N tN
0
#,
3. hN
1
; C; i succeeded with U
1
,
.
.
.
hN
j
; C
j 1
; i succeeded with U
j
,
4. k > 1 and 1  j < k   1, and
5. C
j
= C \ U
1
\ : : : \ U
j
.
Regardless of these generalizations, however,
each subsequent predictive step, having recog-
nized N
1
: : : N
j
, for 1  j < k   1, computes the
next CanBV C
j
by removing the consumed words
U
j
from the previous CanBV C
j 1
, and then tran-
sits to state hN
j+1
; C
j
; i. Removing the Used-
BVs is the result of our assumption that no word
can be used by two or more sibling constituents.
2.2.5 Completion
hN;C;Ri =) hN
k
; C
k 1
; R
k 1
i, where:
1. N
0
! N
1
: : : N
k
,
2. N tN
0
#,
3. hN
1
; C; i succeeded with U
1
,
.
.
.
hN
k 1
; C
k 2
; i succeeded with U
k 1
,
4. C
k 1
= C \ U
1
\ : : : \ U
k 1
, and
5. R
k 1
= R \ U
1
\ : : : \ U
k 1
.
The completion step then involves recognizing
the last RHS category (although this is no longer
rightmost in terms of linear precedence). Here,
the major difference from subsequent prediction is
that there is now a potentially non-empty ReqBV.
Only with the last RHS category are we actually
in a position to enforce R from the source state.
If hN
k
; C
k 1
; R
k 1
i succeeds with U
k
, then
hN;C;Ri succeeds with U
1
[ : : : [ U
k
.
11
2.3 Active Edge Subsumption Revisited
So far, this is very similar to the strategy out-
lined in Penn and Haji-Abdolhosseini (2003). If
we were to add active edges in a manner simi-
lar to standard chart parsing, we would tabulate
states like hN
a
; C
a
; R
a
i and then compare them
in step 2.2.2 to current states hN;C;Ri by deter-
mining whether (classically) N = N
a
, C = C
a
,
and R = R
a
. This might catch some redundant
search, but just as we can do better in the case of
non-atomic categories by checking for subsump-
tion (N
a
v N ) or unifiability (N t N
a
#), we can
do better on C and R as well because these are sets
that come with a natural notion of containment.
Figure 1 shows an example of how this contain-
ment can be used. Rather than comparing edges
annotated with linear subspans, as in the case of
CFG chart parsing, here we are comparing edges
annotated with sublattices of the powerset lattice
on n elements, each of which has a top element (its
CanBV) and a bottom element (its ReqBV). Ev-
erything in between this top and bottom is a sub-
set of words that has been (or will be) tried if that
combination has been tabled as an active edge.
Figure 1 assumes that n = 6, and that we have
tabled an active edge (dashed lines) with C
a
=
f1; 2; 4; 5; 6g, and R
a
= f1; 2g. Now suppose
later that we decide to search for the same cate-
gory in C = f1; 2; 3; 4; 5; 6g, R = f1; 2g (dotted
lines). Here, C 6= C
a
, so an equality-based com-
parison would fail, but a better strategy would be
to reallocate the one extra bit in C (3) to R, and
then search C 0 = f1; 2; 3; 4; 5; 6g, R0 = f1; 2; 3g
(solid lines). As shown in Figure 1, this solid re-
gion fills in all and only the region left unsearched
by the active edge.
This is actually just one of five possible cases
that can arise during the comparison. The com-
plete algorithm is given in Figure 2. This algo-
rithm works as a filter, which either blocks the
current state from further exploration, allows it to
be further explored, or breaks it into several other
states that can be concurrently explored. Step 1(a)
deals with category unifiability. If the current cat-
egory, N , is unifiable with the tabled active cat-
egory, N
a
, then 1(a) breaks N into more specific
pieces that are either incompatible with N
a
or sub-
sumed by N
a
. By the time we get to 1(b), we know
we are dealing with a piece that is subsumed by
N
a
. O stands for ?optional,? CanBV bits that are
not required.
Check(hN;C;Ri):
 For each active edge, a, with hN
a
; C
a
; R
a
i,
1. If N tN
a
#, then:
(a) For each minimal category N 0 such
that N v N 0 and N 0 tN
a
", concur-
rently:
? Let N := N 0, and continue [to
next active edge].
(b) Let N := N tN
a
, O := C \R and
O
a
:= C
a
\R
a
.
(c) If C
a
\ O
a
\ C 6= , then continue
[to next active edge].
(d) If C\O\C
a
6= , then continue [to
next active edge].
(e) If (Z :=)O \ C
a
6= , then:
i. Let O := O \ Z,
ii. Concurrently:
A. continue [to next active
edge], and
B. (1) Let C := C \ Z,
(2) goto (1) [to reconsider
this active edge].
(f) If (Z :=)C
a
\O
a
\O 6= , then:
i. Let O := O \ Z, C := C \ Z ,
ii. continue [to next active edge].
(g) Fail ? this state is subsumed by an
active edge.
2. else continue [to next active edge].
Figure 2: Active edge checking algorithm.
Only one of 1(g) or the bodies of 1(c), 1(d), 1(e)
or 1(f) is ever executed in a single pass through the
loop. These are the five cases that can arise dur-
ing subset/bit vector comparison, and they must
be tried in the order given. Viewing the current
state?s CanBV and ReqBV as a modification of the
active edge?s, the first four cases correspond to:
the removal of required words (1(c)), the addition
of required words (1(d)), the addition of optional
(non-required) words (1(e)), and the reallocation
of required words to optional words (1(f)). Unless
one of these four cases has happened, the current
sublattice has already been searched in its entirety
(1(g)).
2.4 Linear Precedence Constraints
The elaboration above has assumed the absence
of any linear precedence constraints. This is the
12
f1,2,3,4,5,6g
f1,2,3,4,5g f1,2,3,5,6g f1,2,3,4,6g f1,2,4,5,6g
f1,2,3,4g f1,2,3,5g f1,2,3,6g f1,2,4,5g f1,2,4,6g f1,2,5,6g
f1,2,3g f1,2,4g f1,2,5g f1,2,6g
f1,2g
Figure 1: A powerset lattice representation of active edge checking with CanBV and ReqBV.
worst case, from a complexity perspective. The
propagation rules of section 2.2 can remain un-
changed in a concurrent constraint-based frame-
work in which other linear precedence constraints
observe the resulting algebraic closure and fail
when violated, but it is possible to integrate these
into the propagators for efficiency. In either case,
the active edge subsumption procedure remains
unchanged.
For lack of space, we do not consider the char-
acterization of linear precedence constraints in
terms of CanBV and ReqBV further here.
3 Category Graphs and Iteratively
Computed Yields
Whereas in the last section we trivialized linear
precedence, the constraints of this section sim-
ply do not use them. Given a FWO grammar, G,
with immediate dominance rules, R, over a set of
non-terminals, N , we define the category graph
of G to be the smallest directed bipartite graph,
C(G) = hV;Ei, such that:
 V = N [R [ fLex;Emptyg,
 (X; r) 2 E if non-terminal X appears on the
RHS of rule r,
 (r;X) 2 E if the LHS non-terminal of r is
X ,
 (Lex; r) 2 E if there is a terminal on the
RHS of rule r, and
 (Empty; r) 2 E if r is an empty production
rule.
We will call the vertices of C(G) either category
nodes or rule nodes. Lex and Empty are consid-
ered category nodes. The category graph of the
grammar in Figure 3, for example, is shown in
S ! VP NP VP
1
! V NP
NP
1
! N? S VP
2
! V
NP
2
! N? N ! fboy, girlg
N?
1
! N Det Det ! fa, the, thisg
N?
2
! N V ! fsees, callsg
Figure 3: A sample CFG-like grammar.
Figure 4. By convention, we draw category nodes
with circles, and rule nodes with boxes, and we la-
bel rule nodes by the LHS categories of the rules
they correspond to plus an index. For brevity, we
will assume a normal form for our grammars here,
in which the RHS of every rule is either a string of
non-terminals or a single terminal.
Category graphs are a minor variation of the
?grammar graphs? of Moencke and Wilhelm
(1982), but we will use them for a very differ-
ent purpose. For brevity, we will consider only
atomic non-terminals in the remainder of this sec-
tion. Category graphs can be constructed for par-
tially ordered sets of non-terminals, but in this
case, they can only be used to approximate the val-
ues of the functions that they exactly compute in
the atomic case.
13
SS
NP VP
NP
1
NP
2
VP
1
VP
2
N?
N?
2
N?
1
N Det V
N Det V
Lex Empty
Figure 4: The category graph for the grammar in
Figure 3.
Restricting search to unexplored sublattices
helps us with recursion in a grammar in that it
stops redundant search, but in some cases, recur-
sion can be additionally bounded (above and be-
low) not because it is redundant but because it can-
not possibly yield a string as short or long as the
current input string. Inputs are unbounded in size
across parses, but within a single parse, the input
is fixed to a constant size. Category graphs can be
used to calculate bounds as a function of this size.
We will refer below to the length of an input string
below a particular non-terminal in a parse tree as
the yield of that non-terminal instance. The height
of a non-terminal instance in a parse tree is 1 if it
is pre-terminal, and 1 plus the maximum height of
any of its daughter non-terminals otherwise. Non-
terminal categories can have a range of possible
yields and heights.
3.1 Parse Tree Height
Given a non-terminal, X , let Xmax(h) be the
maximum yield that a non-terminal instance of X
at height h in any parse tree can produce, given
the fixed grammar G. Likewise, let Xmin(h) be
the minimum yield that such an instance must pro-
duce. Also, as an abuse of functional notation, let:
X
max
( h) = max
0jh
X
max
(j)
X
min
( h) = min
0jh
X
min
(j)
Now, using these, we can come back and define
X
max
(h) and Xmin(h):
Lex
max
(h) =
Lex
min
(h) =
(
1 h = 0
undefined otherwise
Empty
max
(h) =
Empty
min
(h) =
(
0 h = 0
undefined otherwise
and for all other category nodes, X:
X
max
(1) =
X
min
(1) =
8
>
<
>
:
0 X !  2 R
1 X ! t 2 R
undefined otherwise
and for h > 1:
X
max
(h) = max
X!X
1
:::X
k
2R

max
1ik
X
max
i
(h  1)
+
k
P
j=1;j 6=i
X
max
j
( h  1)
!
X
min
(h) = min
X!X
1
:::X
k
2R

min
1ik
X
min
i
(h  1)
+
k
P
j=1;j 6=i
X
min
j
( h  1)
!
:
For example, in Figure 3, there is only one rule
with S as a LHS category, so:
Smax(h) = max
Coling 2010: Poster Volume, pages 1550?1557,
Beijing, August 2010
Imposing Hierarchical Browsing Structures onto Spoken Documents
Xiaodan Zhu & Colin Cherry
Institute for Information Technology
National Research Council Canada
{Xiaodan.Zhu,Colin.Cherry}@nrc-cnrc.gc.ca
Gerald Penn
Department of Computer Science
University of Toronto
gpenn@cs.toronto.edu
Abstract
This paper studies the problem of im-
posing a known hierarchical structure
onto an unstructured spoken document,
aiming to help browse such archives.
We formulate our solutions within a
dynamic-programming-based alignment
framework and use minimum error-
rate training to combine a number of
global and hierarchical constraints. This
pragmatic approach is computationally
efficient. Results show that it outperforms
a baseline that ignores the hierarchical
and global features and the improvement
is consistent on transcripts with different
WERs. Directly imposing such hierar-
chical structures onto raw speech without
using transcripts yields competitive
results.
1 Introduction
Though speech has long served as a basic method
of human communication, revisiting and brows-
ing speech content had never been a possibility
before human can record their own voice. Re-
cent technological advances in recording, com-
pressing, and distributing such archives have led
to the consistently increasing availability of spo-
ken content.
Along with this availability comes a demand for
better ways to browse such archives, which is in-
herently more difficult than browsing text. In re-
lying on human beings? ability to browse text, a
solution is therefore to reduce the speech brows-
ing problem to a text browsing task through tech-
nologies that can automatically convert speech to
text, i.e., the automatic speech recognition (ASR).
Research along this line has implicitly changed
the traditional speaking-for-hearing and writing-
for-reading construals: now speech can be read
through its transcripts, though it was not originally
intended for this purpose, which in turn raises a
new set of problems.
The efficiency and convenience of reading spo-
ken documents are affected by at least two facts.
First, the quality of transcripts can impair brows-
ing efficiency, e.g., as shown in (Stark et al, 2000;
Munteanu et al, 2006), though if the goal is only
to browse salient excerpts, recognition errors on
the extracts can be reduced by considering the
confidence scores assigned by ASR (Zechner and
Waibel, 2000; Hori and Furui, 2003).
Even if transcription quality is not a problem,
browsing transcripts is not straightforward. When
intended to be read, written documents are al-
most always presented as more than uninterrupted
strings of text. Consider that for many writ-
ten documents, e.g., books, indicative structures
such as section/subsection headings and tables-of-
contents are standard constituents created manu-
ally to help readers. Structures of this kind, how-
ever, are rarely aligned with spoken documents.
In this paper, we are interested in addressing
the second issue: adding hierarchical browsable
structures to speech transcripts. We define a hi-
erarchical browsable structure as a set of nested
labelled bracketing which, when placed in text,
partition the document into labeled segments. Ex-
amples include the sequence of numbered sec-
tion headings in this paper, or the hierarchical
slide/bullet structure in the slides of a presenta-
tion.
1550
An ideal solution to this task would directly in-
fer both the hierarchical structure and the labels
from unstructured spoken documents. However,
this is a very complex task, involving the analysis
of not only local but also high-level discourse over
large spans of transcribed speech. Specifically for
spoken documents, spoken-language characteris-
tics as well as the lack of formality and thematic
boundaries in transcripts violate many conditions
that a reliable algorithm (Marcu, 2000) relies on
and therefore make the task even harder.
In this paper, we aim at a less ambitious but
naturally occurring problem: imposing a known
hierarchical structure, e.g., presentation slides,
onto the corresponding document, e.g., presenta-
tion transcripts. Given an ordered, nested set of
topic labels, we must place the labels so as to
correctly segment the document into appropriate
units. Such an alignment would provide a useful
tool for presentation browsing, where a user could
easily navigate through a presentation by clicking
on bullets in the presentation slides. The solution
to this task should also provide insights and tech-
niques that will be useful in the harder structure-
inference task, where hierarchies and labels are
not given.
We present a dynamic-programming-based
alignment framework that considers global docu-
ment features and local hierarchical features. This
pragmatic approach is computationally efficient
and outperforms a baseline alignment that ignores
the hierarchical structure of bullets within slides.
We also explore the impact of speech recognition
errors on this task. Furthermore, we study the
feasibility of directly aligning a structure to raw
speech, as opposed to a transcript.
2 Related work
Topic/slide boundary detection The previous
work most directly related to ours is research that
attempts to find flat structures of spoken docu-
ments, such as topic and slide boundaries. For
example, the work of (Chen and Heng, 2003;
Ruddarraju, 2006; Zhu et al, 2008) aims to find
slide boundaries in the corresponding lecture tran-
scripts. Malioutov et al (2007) developed an ap-
proach to detecting topic boundaries of lecture
recordings by finding repeated acoustic patterns.
None of this work, however, has involved hierar-
chical structures that exist at different levels of a
document.
In addition, researchers have also analyzed
other multimedia channels, e.g., video (Liu et al,
2002; Wang et al, 2003; Fan et al, 2006), to de-
tect slide transitions. Such approaches, however,
are unlikely to find semantic structures that are
more detailed than slide transitions, e.g., the bullet
hierarchical structures that we are interested in.
Building tables-of-contents on written text A
notable effort going further than topic segmenta-
tion is the work by Branavan et al (2007), which
aims at the ultimate goal of building tables-of-
contents for written texts. However, the authors
assumed the availability of the hierarchical struc-
tures and the corresponding text spans. Therefore,
their problem was restricted to generating titles for
each span. Our work here can be thought of as the
inverse problem, in which the title of each section
is known, but the corresponding segments in the
spoken documents are unknown. Once the corre-
spondence is found, an existing hierarchical struc-
ture along with its indicative titles is automatically
imposed on the speech recordings. Moreover, this
paper studies spoken documents instead of writ-
ten text. We believe it is more attractive not only
because of the necessity of browsing spoken con-
tent in a more efficient way but also the general
absence of helpful browsing structures that are of-
ten available in written text, as we have already
discussed above.
Rhetoric analysis In general, analyzing dis-
course structures can provide thematic skeletons
(often represented as trees) of a document as well
as relationship between the nodes in the trees. Ex-
amples include the widely known discourse pars-
ing work by Marcu (2000). However, when the
task involves the understanding of high-level dis-
course, it becomes more challenging than just
finding local discourse conveyed on small spans of
text; e.g., the latter is more likely to benefit from
the presence of discourse markers. Specifically
for spoken documents, spoken-language charac-
teristics as well as the absence of formality and
thematic boundaries in transcripts pose additional
1551
difficulty. For example, the boundaries of sen-
tences, paragraphs, and larger text blocks like sec-
tions are often missing. Together with speech
recognition errors as well as other speech charac-
teristics such as speech disfluences, they will im-
pair the conditions on which an effective and reli-
able algorithm of discourse analysis is often built.
3 Problem formulation
We are given a speech sequence U =
u1, u2, ..., um, where ui is an utterance. De-
pending on the application, ui can either stand
for the audio or transcript of the utterance. We
are also given a corresponding hierarchical struc-
ture. In our work, this is a sequence of lecture
slides containing a set of slide titles and bullets,
B = {b1, b2, ..., bn}, organized in a tree structure
T (?,?,?), where ? is the root of the tree that
concatenates all slides of a lecture; i.e., each slide
is a child of the root ? and each slide?s bullets
form a subtree. In the rest of this paper, the word
bullet means both the title of a slide (if any) and
any bullet in it. ? is the set of nodes of the tree
(both terminal and non-terminals, excluding the
root ?), each corresponding to a bullet bi in the
slides. ? is the edge set. With the definitions, our
task is herein to find the triple (bi, uk, ul), denot-
ing that a bullet bi starts from the kth utterance
uk and ends at the lth. Constrained by the tree
structure, the text span corresponding to an an-
cestor bullet contains those corresponding to its
descendants; i.e., if a bullet bi is the ancestor of
another bullet bj in the tree, the acquired bound-
ary triples (bi, uk1, ul1) and (bj , uk2, ul2) should
satisfy uk1 ? uk2 and ul1 ? ul2. In implemen-
tation, we only need to find the starting point of a
bullet, i.e., a pair (bi, uk), since we know the tree
structure in advance and therefore we know that
the starting position of the next sibling bullet is
the ending boundary for the current bullet.
4 Our approaches
Our task is to find the correspondence between
slide bullets and a speech sequence or its tran-
scripts. Research on finding correspondences be-
tween parallel texts pervades natural language
processing. For example, aligning bilingual sen-
tence pairs is an essential step in training ma-
chine translation models. In text summarization,
the correspondence between human-written sum-
maries and their original texts has been identified
(Jing, 2002), too. In speech recognition, forced
alignment is applied to align speech and tran-
scripts. In this paper, we keep the general frame-
work of alignment in solving our problem.
Our solution, however, should be flexible to
consider multiple constraints such as those con-
veyed in hierarchical bullet structures and global
word distribution. Accordingly, the model pro-
posed in this paper depends on two orthogonal
strategies to ensure efficiency and richness of the
model. First of all, we formulate all our solutions
within a classic dynamic programming framework
to enforce computational efficiency (section 4.1).
On the other hand, we explore the approach to in-
corporating hierarchical and global features into
the alignment framework (Section 4.2). The as-
sociated parameters are then optimized with Pow-
ell?s algorithm (Section 4.3).
4.1 A pre-order walk of bullet trees
We formulate our solutions within the classic
dynamic-programming-based alignment frame-
work, dynamic time warping (DTW). To this end,
we need to sequentialize the given hierarchies,
i.e., bullet trees. We propose to do so through a
pre-order walk of a bullet tree; i.e., at any step
of a recursive traversal of the tree, the alignment
model always visits the root first, followed by its
children in a left-to-right order. This sequential-
ization actually corresponds to a reasonable as-
sumption: words appearing earlier on a given slide
are spoken earlier by the speaker. The pre-order
walk is also used by (Branavan et al, 2007) to
reduce the search space of their discriminative
table-of-contents generation. Our sequentializa-
tion strategy can be intuitively thought of as re-
moving indentations that lead each bullet. As
shown in Figure 1, the right panel is a bullet array
resulting from a pre-walk of the slide in the left
panel. In our baseline model, the resulted bullet
array is directly aligned with lecture utterances.
Other orders of bullet traversal could also be
considered, e.g., when speech does not strictly fol-
low bullet orders. In general, one can regard our
1552
task here as a tagging problem to allow further
flexibility on bullet-utterance correspondence, in
which bullets are thought of as tags. However,
considering the fact that bullets are created to or-
ganize speech and in most cases they correspond
to the development of speech content monotoni-
cally, this paper focuses on addressing the prob-
lem in the alignment framework.
Figure 1: A pre-order walk of a bullet tree.
4.2 Incorporating hierarchical and global
features
Our models should be flexible enough to consider
constraints that could be helpful, e.g., the hierar-
chical bullet structures and global word distribu-
tion. We propose to consider all these constraints
in the phase of estimating similarity matrices. To
this end, we use two levels of similarity matrices
to capture local tree constraints and global word
distributions, respectively.
First of all, information conveyed in the hierar-
chies of bullet trees should be considered, such as
the potentially discriminative nature between two
sibling bullets (Branavan et al, 2007) and the re-
lationships between ancestor and descendant bul-
lets. We incorporate them in the bullet-utterance
similarity matrices. Specifically, when estimating
the similarity between a bullet bi and an utterance
uj , we consider local tree constraints based on
where the node bi is located on the slide. We do
so by accounting for first and second-order tree
features. Given a bullet, bi, we first represent it
as multiple vectors, one for each of the following:
its own words, the words appearing in its parent
bullet, grandparent, children, grandchildren, and
the bullets immediately adjacent to bi. That is, bi
is now represented as 6 vectors of words (we do
not discriminate between its left and right siblings
and put these words in the same vector). Simi-
larity between the bullet bi and an utterance uj is
calculated by taking a weighted average over the
similarities between each of the 6 vectors and the
utterance uj . A linear combination is used and the
weights are optimized on a development set.
Global property of word distributions could be
helpful, too. A general term often has less dis-
criminative power in the alignment framework
than a word that is localized to a subsection of
the document and is related to specific subtopics.
For example, in a lecture that teaches introductory
computer science topics, aligning a general term
?computer? should receive a smaller weight than
aligning some topic-specific terms such as ?au-
tomaton.? The latter word is more likely to appear
in a more narrow text span. It is not straightfor-
ward to directly calculate idf scores unless a lec-
ture is split into smaller segments in some way.
Instead, in our models, the distribution property
of a word is considered in word-level similarity
matrices with the following formula.
sim(wi, wj) =
{
0 : i 6= j
1? ? var(wi)maxk(var(wk)) : i = j
Aligning different words receives no bonus,
while matching the same word between bullets
and utterances receives a score of 1 minus a dis-
tribution penalty, as shown in the formula above.
The function var(wi) calculates the standard vari-
ance of the positions where the word wi appears.
Divided by the maximal standard variance of word
positions in the same lecture, the score is normal-
ized to [0,1]. This distribution penalty is weighted
by ?, which is tuned in a development set. Again,
a general term is expected to have a larger posi-
tional variance.
Once a word-level matrix is acquired, it is com-
bined with the bullet-utterance level matrix dis-
cussed above. Specifically, when measuring the
similarity between a word vector (one of the 6
vectors) and the transcripts of an utterance, we
sum up the word-level similarity scores of all
matching words between them, normalize the re-
sulted score by the length of the vector and ut-
terance, and then renormalize it to the range
1553
[0, 1] within the same spoken document. The
final bullet-utterance similarity matrix is incor-
porated into the pre-order-walk suquentialization
discussed above, when alignment is conducted.
4.3 Parameter optimization
Powell?s algorithm (Press et al, 2007) is used to
find the optimal weights for the constraints we in-
corporated above, to directly minimize the objec-
tive function, i.e., the Pk and WindowDiff scores
that we will discuss later. As a summary, we have
7 weights to tune: a weight for each of the fol-
lowing: parent bullet, grandparent, adjacent sib-
lings, children, grandchildren, and the current bul-
let, plus the word distribution penalty ?. The val-
ues of these weights are determined on a develop-
ment set.
Note that the model we propose here does not
exclude the use of further features; instead, many
other features, such as smoothed word similarity
scores, can be easily added to this model. We
are conservative on our model complexity here,
in terms of number of weights need to be tuned,
for the consideration of the size of data that we
can used to estimate these weights. Finally, with
all the 7 weights being determined, we apply the
standard dynamic time warping (DTW).
5 Experimental set-up
5.1 Data
We use a corpus of lectures recorded at a large
research university. The correspondence between
bullets and speech utterances are manually an-
notated in a subset of this lecture corpus, which
contains approximately 30,000 word tokens in
its manual transcripts. Intuitively, this roughly
equals a 120-page double-spaced essay in length.
The lecturer?s voice was recorded with a head-
mounted microphone with a 16kHz sampling rate
and 16-bit samples. Students? comments and
questions were not recorded. The speech is split
into utterances by pauses longer than 200ms, re-
sulting in around 4000 utterances. There are 119
slides that are composed of 921 bullets. A sub-
set containing around 25% consecutive slides and
their corresponding speech/transcripts are used as
our development set to tune the parameters dis-
cussed earlier; the rest data are used as our test
set.
5.2 Evaluation metric
We evaluate our systems according to how well
the segmentation implied by the inferred bullet
alignment matches that of the manually anno-
tated gold-standard bullet algnment. Though one
may consider that different bullets may be of dif-
ferent importance, in this paper we do not use
any heuristics to judge this and we treat all bul-
lets equally in our evaluation. We evaluate our
systems with the Pk and WindowDiff metrics
(Malioutov et al, 2007; Beeferman et al, 1999;
Pevsner and Hearst, 2002). Note that for both
metrics, the lower a score is, the better the per-
formance of a system is. The Pk score computes
the probability of a randomly chosen pair of words
being inconsistently separated. The WindowDiff
is a variant of Pk; it penalizes false positives and
near misses equally.
6 Experimental results
6.1 Alignment performance
Table 1 presents the results on automatic tran-
scripts with a 39% WER, a typical WER in realis-
tic and uncontrolled lecture conditions (Leeuwis
et al, 2003; Hsu and Glass, 2006). The tran-
scripts were generated with the SONIC toolkit
(Pellom, 2001). The acoustic model was trained
on the Wall Street Journal dictation corpus. The
language model was trained on corpora obtained
from the Web through searching the words ap-
pearing on slides as suggested by (Munteanu et
al., 2007).
Pk WindowDiff
UNI 0.481 0.545
TT 0.469 0.534
B-ALN 0.283 0.376
HG-ALN 0.266 0.359
Table 1: The Pk and WindowDiff scores of uni-
form segmentation (UNI), TextTiling (TT), base-
line alignment (B-ALN), and alignment with hier-
archical and global information (HG-ALN).
From Table 1, we can see that the model that
1554
utilizes the hierarchical structures of slides and
global distribution of words, i.e., the HG-ALN
model, reduces both Pk and WindowDiff scores
over the baseline model, B-ALN. As discussed
earlier, the baseline is a re-implementation of
standard dynamic time warping based only on a
pre-order walk of the slides, while the HG-ALN
model incorporates also hierarchical bullet con-
straints and global word distribution.
Table 1 also presents the performance of a
typical topic segmentation algorithm, TextTiling
(Hearst, 1997). Note that similar to (Malioutov et
al., 2007), we force the number of predicted topic
segments to be the target number, i.e., in our task,
the number of bullets. The results show that both
the Pk and WindowDiff scores of TextTiling are
significantly higher than those of the alignment al-
gorithms. Our manual analysis suggests that many
segments are as short as several utterances and the
difference between two consecutive segments is
too subtle to be captured by a lexical cohesion-
based method such as TextTiling. For compari-
son, We also present the results of uniform seg-
mentation (UNI), which simply splits the tran-
script of each lecture evenly into segments with
same numbers of words.
6.2 Performance under different WERs
Speech recognition errors within reasonable
ranges often have very small impact on many spo-
ken language processing tasks such as spoken lan-
guage retrieval (Garofolo et al, 2000) and speech
summarization (Christensen et al, 2004; Maskey,
2008; Murray, 2008; Zhu, 2010). To study the
impact of speech recognition errors on our task
here, we experimented with the alignment mod-
els on manual transcripts as well as on automatic
transcripts with different WERs, including a 39%
and a 46% WER produced by two real recogni-
tion systems. To increase the spectrum of our ob-
servation, we also overfit our ASR models to ob-
tain smaller WERs at the levels of 11%, 19%, and
30%.
From Figure 2, we can see that at all levels of
these different WERs, the HG-ALN model con-
sistently outperforms the B-ALN system (the AU-
DIO model will be discussed below). The Pk
and WindowDiff curves also show that the align-
0 0.1 0.2 0.3 0.4
0.24
0.26
0.28
0.3
0.32
Pk under different WERs
Pk
Word error rate
 
 
B?ALN
HG?ALN
AUDIO
0 0.1 0.2 0.3 0.4
0.34
0.36
0.38
0.4
WindowDiff under different WERs
W
in
do
wD
iff
Word error rate
 
 
B?ALN
HG?ALN
AUDIO
Figure 2: The impact of different WERs on the
alignment models. The performance of an audio-
based model (AUDIO) is also presented.
ment performance is sensitive to recognition er-
rors, particularly when the WER is in the range of
30%?45%, suggesting that the problem we study
here can benefit from the improvement of current
ASR systems in this range, e.g., the recent ad-
vance achieved in (Glass et al, 2007).
6.3 Imposing hierarchical structures onto
raw speech
We can actually impose hierarchical structures di-
rectly onto raw speech, through estimating the
similarity between bullets and speech. This en-
ables navigation through the raw speech by using
slides; e.g., one can hear different parts of speech
by clicking a bullet. We apply keyword spotting to
solve this problem, which detects the occurrences
of each bullet word in the corresponding lecture
audio.
1555
In this paper, we use a token-passing based al-
gorithm provided in the ASR toolkit SONIC (Pel-
lom, 2001). Since the slides are given in advance,
we manually add into the pronunciation dictio-
nary the words that appear in slides but not in
the pronunciation dictionary. To estimate sim-
ilarity between a word vector (discussed earlier
in Section 4.2) and an utterance, we sum up all
keyword-spotting confidence scores assigned be-
tween them, normalize the resulted score by the
length of the vector and the duration of the utter-
ance, and then renormalize it to the range [0, 1]
within the same spoken lecture.
We present the performance of our bullet-audio
alignment model (AUDIO) in Figure 2 so that one
can compare its effectiveness with the transcrip-
tion based methods. The figure shows that the
performance of the AUDIO model is comparable
to the baseline transcription-based model, i.e., B-
ALN, when the WERs of the transcripts are in the
range of 37%?39%. The performance is compara-
ble to the HG-ALN model when WERs are in the
range of 42%?44%. Also, this suggests that incor-
porating hierarchical and global features compen-
sates for the performance degradation of speech
recognition in this range when the WER is 4%-
6% higher.
Note that we did not observe that the perfor-
mance is different when incorporating hierarchi-
cal information and global word distributions into
the AUDIO model, so the AUDIO results in Fig-
ure 2 are the performance of both types of meth-
ods. The current keyword spotting component
yields a high false-positive rate; e.g., it incorrectly
reports many words that are acoustically similar to
parts of other words that really appear in an utter-
ance. This happened even when a high threshold
is set. The noise impairs the benefit of hierarchical
and distribution features.
7 Conclusions and discussions
This paper investigates the problem of imposing
a known hierarchical structure onto an unstruc-
tured spoken document. Results show that incor-
porating local hierarchical constraints and global
word distributions in the efficient dynamic pro-
gramming framework yields a better performance
over the baseline. Further experiments on a wide
range of WERs confirm that the improvement is
consistent, and show that both types of models
are sensitive to speech recognition errors, partic-
ularly when WER increases to 30% and above.
Moreover, directly imposing hierarchical struc-
tures onto raw speech through keyword spotting
achieves competitive performance.
References
Beeferman, D., A. Berger, and J. Lafferty. 1999.
Statistical models for text segmentation. Machine
Learning, 34(1-3):177?210.
Branavan, S., Deshpande P., and Barzilay R. 2007.
Generating a table-of-contents: A hierarchical dis-
criminative approach. In Proc. of Annual Meeting
of the Association for Computational Linguistics.
Chen, Y. and W. J. Heng. 2003. Automatic synchro-
nization of speech transcript and slides in presenta-
tion. In Proc. International Symposium on Circuits
and Systems.
Christensen, H., B. Kolluru, Y. Gotoh, and S. Re-
nals. 2004. From text summarisation to style-
specific summarisation for broadcast news. In Proc.
of the 26th European Conference on Information
Retrieval, pages 223?237.
Fan, Q., K. Barnard, A. Amir, A. Efrat, and M. Lin.
2006. Matching slides to presentation videos using
sift and scene background. In Proc. of ACM Inter-
national Workshop on Multimedia Information Re-
trieval, pages 239?248.
Garofolo, J., G. Auzanne, and E. Voorhees. 2000.
The trec spoken document retrieval track: A success
story. In Proc. of Text Retrieval Conference, pages
16?19.
Glass, J., T. Hazen, S. Cyphers, I. Malioutov,
D. Huynh, and R. Barzilay. 2007. Recent progress
in the mit spoken lecture processing project. Proc.
of Annual Conference of the International Speech
Communication Association, pages 2553?2556.
Hearst, M. 1997. Texttiling: Segmenting text into
multi-paragraph subtopic passages. Computational
Linguistics, 23(1):33?64.
Hori, C. and S. Furui. 2003. A new approach to au-
tomatic speech summarization. IEEE Transactions
on Multimedia, 5(3):368?378.
Hsu, B. and J. Glass. 2006. Style and topic language
model adaptation using hmm-lda. In Proc. of Con-
ference on Empirical Methods in Natural Language
Processing.
1556
Jing, H. 2002. Using hidden markov modeling to
decompose human-written summaries. Computa-
tional Linguistics, 28(4):527?543.
Leeuwis, E., M. Federico, and M. Cettolo. 2003. Lan-
guage modeling and transcription of the ted corpus
lectures. In Proc. of IEEE International Conference
on Acoustics, Speech and Signal Processing.
Liu, T., R. Hjelsvold, and J. R. Kender. 2002. Analysis
and enhancement of videos of electronic slide pre-
sentations. In Proc. IEEE International Conference
on Multimedia and Expo.
Malioutov, I., A. Park, B. Barzilay, and J. Glass. 2007.
Making sense of sound: Unsupervised topic seg-
mentation over acoustic input. In Proc. of Annual
Meeting of the Association for Computational Lin-
guistics, pages 504?511.
Marcu, D. 2000. The theory and practice of discourse
parsing and summarization. The MIT Press.
Maskey, S. 2008. Automatic Broadcast News Speech
Summarization. Ph.D. thesis, Columbia University.
Munteanu, C., R. Baecker, G. Penn, E. Toms, and
E. James. 2006. Effect of speech recognition accu-
racy rates on the usefulness and usability of webcast
archives. In Proc. of ACM Conference on Human
Factors in Computing Systems, pages 493?502.
Munteanu, C., G. Penn, and R. Baecker. 2007.
Web-based language modelling for automatic lec-
ture transcription. In Proc. of Annual Conference
of the International Speech Communication Associ-
ation.
Murray, G. 2008. Using Speech-Specific Character-
istics for Automatic Speech Summarization. Ph.D.
thesis, University of Edinburgh.
Pellom, B. L. 2001. Sonic: The university of col-
orado continuous speech recognizer. Tech. Rep. TR-
CSLR-2001-01, University of Colorado.
Pevsner, L. and M. Hearst. 2002. A critique and im-
provement of an evaluation metric for text segmen-
tation. Computational Linguistics, 28:19?36.
Press, W.H., S.A. Teukolsky, W.T. Vetterling, and B.P.
Flannery. 2007. Numerical recipes: The art of sci-
ence computing. Cambridge University Press.
Ruddarraju, R. 2006. Indexing Presentations Using
Multiple Media Streams. Ph.D. thesis, Georgia In-
stitute of Technology. M.S. Thesis.
Stark, L., S. Whittaker, and J. Hirschberg. 2000. Find-
ing information in audio: A new paradigm for au-
dio browsing and retrieval. In Proc. of International
Conference on Spoken Language Processing.
Wang, F., C. W. Ngo, and T. C. Pong. 2003. Synchro-
nization of lecture videos and electronic slides by
video text analysis. In Proc. of ACM International
Conference on Multimedia.
Zechner, K. and A. Waibel. 2000. Minimizing word
error rate in textual summaries of spoken language.
In Proc. of Applied Natural Language Processing
Conference and Meeting of the North American
Chapter of the Association for Computational Lin-
guistics, pages 186?193.
Zhu, X., X. He, C. Munteanu, and G. Penn. 2008. Us-
ing latent dirichlet alocation to incorporate domain
knowledge for topic transition detection. In Proc.
of Annual Conference of the International Speech
Communication Association.
Zhu, X. 2010. Summarizing Spoken Documents
Through Utterance Selection. Ph.D. thesis, Univer-
sity of Toronto.
1557
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 23?33,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Utilizing Extra-sentential Context for Parsing
Jackie Chi Kit Cheung and Gerald Penn
Department of Computer Science
University of Toronto
Toronto, ON, M5S 3G4, Canada
{jcheung,gpenn}@cs.toronto.edu
Abstract
Syntactic consistency is the preference to
reuse a syntactic construction shortly after its
appearance in a discourse. We present an anal-
ysis of the WSJ portion of the Penn Tree-
bank, and show that syntactic consistency is
pervasive across productions with various left-
hand side nonterminals. Then, we implement
a reranking constituent parser that makes use
of extra-sentential context in its feature set.
Using a linear-chain conditional random field,
we improve parsing accuracy over the gen-
erative baseline parser on the Penn Treebank
WSJ corpus, rivalling a similar model that
does not make use of context. We show that
the context-aware and the context-ignorant
rerankers perform well on different subsets of
the evaluation data, suggesting a combined ap-
proach would provide further improvement.
We also compare parses made by models, and
suggest that context can be useful for parsing
by capturing structural dependencies between
sentences as opposed to lexically governed de-
pendencies.
1 Introduction
Recent corpus linguistics work has produced ev-
idence of syntactic consistency, the preference to
reuse a syntactic construction shortly after its ap-
pearance in a discourse (Gries, 2005; Dubey et al,
2005; Reitter, 2008). In addition, experimental stud-
ies have confirmed the existence of syntactic prim-
ing, the psycholinguistic phenomenon of syntactic
consistency1. Both types of studies, however, have
1Whether or not corpus-based studies of consistency have
any bearing on syntactic priming as a reality in the human mind
limited the constructions that are examined to partic-
ular syntactic constructions and alternations. For in-
stance, Bock (1986) and Gries (2005) examine spe-
cific constructions such as the passive voice, dative
alternation and particle placement in phrasal verbs,
and Dubey et al (2005) deal with the internal struc-
ture of noun phrases. In this work, we extend these
results and present an analysis of the distribution of
all syntactic productions in the Penn Treebank WSJ
corpus. We provide evidence that syntactic consis-
tency is a widespread phenomenon across produc-
tions of various types of LHS nonterminals, includ-
ing all of the commonly occurring ones.
Despite this growing evidence that the probability
of syntactic constructions is not independent of the
extra-sentential context, current high-performance
statistical parsers (e.g. (Petrov and Klein, 2007; Mc-
Closky et al, 2006; Finkel et al, 2008)) rely solely
on intra-sentential features, considering the partic-
ular grammatical constructions and lexical items
within the sentence being parsed. We address this
by implementing a reranking parser which takes ad-
vantage of features based on the context surrounding
the sentence. The reranker outperforms the genera-
tive baseline parser, and rivals a similar model that
does not make use of context. We show that the
context-aware and the context-ignorant models per-
form well on different subsets of the evaluation data,
suggesting a feature set that combines the two mod-
els would provide further improvement. Analysis of
the rerankings made provides cases where contex-
tual information has clearly improved parsing per-
is a subject of debate. See (Pickering and Branigan, 1999) and
(Gries, 2005) for opposing viewpoints.
23
prior denominator
prior numerator pos_adapt numerator
pos_adapt denominator
fp,tf?p,t
f?p,?t fp,?t
?p p
?t
t
Figure 1: Visual representation of calculation of prior and
positive adaptation probabilities. t represents the pres-
ence of a construction in the target set. p represents the
presence of the construction in the prime set.
formance, indicating the potential of extra-sentential
contextual information to aid parsing, especially for
structural dependencies between sentences, such as
parallelism effects.
2 Syntactic Consistency in the Penn
Treebank WSJ
Syntactic consistency has been examined by Dubey
et al (2005) for several English corpora, including
the WSJ, Brown, and Switchboard corpora. They
have provided evidence that syntactic consistency
exists not only within coordinate structures, but also
in a variety of other contexts, such as within sen-
tences, between sentences, within documents, and
between speaker turns in the Switchboard corpus.
However, their analysis rests on a selected number
of constructions concerning the internal structure of
noun phrases. We extend their result here to arbi-
trary syntactic productions.
There have also been studies into syntactic con-
sistency that consider all syntactic productions in
dialogue corpora (Reitter, 2008; Buch and Pietsch,
2010). These studies find an inverse correlation be-
tween the probability of the appearance of a syn-
less frequent        Production-type deciles        more frequent
Pr
op
or
tio
n 
of
 c
on
si
st
en
t p
ro
du
ct
io
n-
ty
pe
s
Figure 2: Production-types (singletons removed) catego-
rized into deciles by frequency and the proportion of the
production-types in that bin that is consistent to a signifi-
cant degree.
tactic structure and the distance since its last occur-
rence, which indicates syntactic consistency. These
studies, however, do not provide consistency results
on subsets of production-types, such as by produc-
tion LHS as our study does, so the implications that
can be drawn from them for improving parsing are
less apparent.
We adopt the measure used by Dubey et al (2005)
to quantify syntactic consistency, adaptation prob-
ability. This measure originates in work on lexical
priming (Church, 2000), and quantifies the probabil-
ity of a target word or construction w appearing in a
?primed? context. Specifically, four frequencies are
calculated, based on whether the target construction
appears in the previous context (the prime set), and
whether the construction appears after this context
(the target set):
fp,?t(w) = # of times w in prime set only
f?p,t(w) = # of times w in target set only
f?p,?t(w) = # of times w in neither set
fp,t(w) = # of times w in both sets
We also define N to be the sum of the four fre-
24
LHS prior pos adapt ratio + > prior sig. insig. + < prior sig.
ADJP 0.03 0.05 1.96 26 251 0
ADVP 0.21 0.24 1.15 26 122 0
NP 0.17 0.22 1.27 281 2284 0
PP 0.56 0.58 1.04 32 125 0
PRN 0.01 0.03 4.60 12 82 0
PRT 0.06 0.08 1.40 3 3 0
QP 0.03 0.18 5.41 24 147 0
S 0.30 0.34 1.13 42 689 1
SBAR 0.15 0.20 1.31 13 68 0
SINV 0.01 0.01 1.00 3 77 0
VP 0.08 0.12 1.56 148 1459 0
WHADVP 0.04 0.08 1.84 2 8 0
WHNP 0.07 0.10 1.39 3 47 0
WHPP 0.01 0.02 2.65 1 1 0
Table 1: Weighted average by production frequency among non-singleton production-types of prior and positive adap-
tation probabilities, and the ratio between them. The columns on the right show the number of production-types
for which the positive adaptation probability is significantly greater than, not different from, or less than the prior
probability. We exclude LHSs with a weighted average prior of less than 0.005, due to the small sample size.
quencies. Then, we define the prior and the positive
adaptation probability of a construction as follows
(See also Figure 1):
prior(w) = fp,t(w) + f?p,t(w)
N
pos adapt(w) = fp,t(w)
fp,t(w) + fp,?t(w)
A positive adaptation probability that is greater
than the prior probability would be interpreted as
evidence for syntactic consistency for that construc-
tion. We conduct ?2 tests for statistical signif-
icance testing. We analyze the Penn Treebank
WSJ corpus according this schema for all produc-
tions that occur in sections 2 to 22. These are the
standard training and development sets for train-
ing parsers. We did not analyze section 23 in or-
der not to use its characteristics in designing our
reranking parser so that we can use this section as
our evaluation test set. Our analysis focuses on the
consistency of rules between sentences, so we take
the previous sentence within the same article as the
prime set, and the current sentence as the target set
in calculating the probabilities given above. The
raw data from which we produced our analysis are
available at http://www.cs.toronto.edu/
?
jcheung/wsj_parallelism_data.txt.
We first present results for consistency in all the
production-types2, grouped by the LHS of the pro-
duction. Table 1 shows the weighted average prior
and positive adaptation probabilities for productions
by LHS, where the weighting is done by the num-
ber of occurrence of that production. Production-
types that only occur once are removed. It also
shows the number of production-types in which the
positive adaptation probability is statistically signif-
icantly greater than, not significantly different from,
and significantly lower than the prior probability.
Quite remarkably, very few production-types are
significantly less likely to reoccur compared to the
prior probability. Also note the wide variety of LHSs
for which there is a large number of production-
types that are consistent to a significant degree.
While a large number of production-types appears
not to be significantly more likely to occur in a
primed context, this is due to the large number of
production-types which only appear a few times.
Frequently occurring production-types mostly ex-
hibit syntactic consistency.
We show this in Figure 2, in which we put
non-singleton production-types into ten bins by fre-
2That is, all occurrences of a production with a particular
LHS and RHS.
25
Ten most frequent production-types
production f?p,t fp,t fp,?t prior pos adapt ratio
PP ? IN NP 5624 26224 5793 0.80 0.82 1.02
NP ? NP PP 9033 12451 9388 0.54 0.57 1.05
NP ? DT NN 9198 10585 9172 0.50 0.54 1.07
S ? NP VP 8745 9897 9033 0.47 0.52 1.11
S ? NP VP . 8576 8501 8888 0.43 0.49 1.13
S ? VP 8717 7867 9042 0.42 0.47 1.11
NP ? PRP 7208 5309 7285 0.32 0.42 1.33
ADVP ? RB 7986 3949 7905 0.30 0.33 1.10
NP ? NN 7630 3390 7568 0.28 0.31 1.11
VP ? TO VP 7039 3552 7250 0.27 0.33 1.23
Ten most consistent among 10% most frequent production-types
production f?p,t fp,t fp,?t prior pos adapt ratio
QP ? # CD CD 51 18 45 0.00 0.29 163.85
NP ? JJ NNPS 52 7 53 0.00 0.12 78.25
NP ? NP , ADVP 109 24 99 0.00 0.20 58.05
NP ? DT JJ CD NN 63 6 67 0.00 0.08 47.14
PP ? IN NP NP 83 10 87 0.00 0.10 43.86
QP ? IN $ CD 51 3 49 0.00 0.06 42.28
NP ? NP : NP . 237 128 216 0.01 0.37 40.34
INTJ ? UH 59 4 60 0.00 0.06 39.26
ADVP ? IN NP 108 11 83 0.00 0.12 38.91
NP ? CD CD 133 21 128 0.00 0.14 36.21
Table 2: Some instances of consistency effects of productions. All productions? pos adapt probability is significantly
greater than its prior probability at p < 10?6.
quency and calculated the proportion of production-
types in that bin for which the positive adaptation
probability is significantly greater than the prior. It is
clear that the most frequently occurring production-
types are also the ones most likely to exhibit evi-
dence of syntactic consistency.
Table 2 shows the breakdown of the prior and
positive adaptation calculation components for the
ten most frequent production-types and the ten most
consistent (by the ratio pos adapt / prior) produc-
tions among the top decile of production-types. Note
that all of these production-types are consistent to a
statistically significant degree. Interestingly, many
of the most consistent production-types have NP as
the LHS, but overall, productions with many differ-
ent LHS parents exhibit consistency.
3 A Context-Aware Reranker
Having established evidence for widespread syntac-
tic consistency in the WSJ corpus, we now investi-
gate incorporating extra-sentential context into a sta-
tistical parser. The first decision to make is whether
to incorporate the context into a generative or a dis-
criminative parsing model.
Employing a generative model would allow us to
train the parser in one step, and one such parser
which incorporates the previous context has been
implemented by Dubey et al (2006). They imple-
ment a PCFG, learning the production probabilities
by a variant of standard PCFG-MLE probability es-
timation that conditions on whether a rule has re-
cently occurred in the context or not:
P (RHS|LHS,Prime) = c(LHS ? RHS,Prime)
c(LHS,Prime)
LHS and RHS represent the left-hand side and
26
right-hand side of a production, respectively. Prime
is a binary variable which is True if and only if
the current production has occurred in the prime set
(the previous sentence). c represents the frequency
count.
The drawback of such a system is that it doubles
the state space of the model, and hence likely in-
creases the amount of data needed to train the parser
to a comparable level of performance as a more com-
pact model, or would require elaborate smoothing.
Dubey et al (2006) find that this system performs
worse than the baseline PCFG-MLE model, drop-
ping F1 from 73.3% to 71.6%3.
We instead opt to incorporate the extra-sentential
context into a discriminative reranking parser, which
naturally allows additional features to be incorpo-
rated into the statistical model. Many discriminative
models of constituent parsing have been proposed in
recent literature. They can be divided into two broad
categories?those that rerank the N-best outputs of a
generative parser, and those that make all parsing de-
cisions using the discriminative model. We choose
to implement an N-best reranking parser so that we
can utilize state-of-the-art generative parsers to en-
sure a good selection of candidate parses to feed
into our reranking module. Also, fully discrimina-
tive models tend to suffer from efficiency problems,
though recent models have started to overcome this
problem (Finkel et al, 2008).
Our approach is similar to N-best reranking
parsers such as Charniak and Johnson (2005)
and Collins and Koo (2005), which implement a va-
riety of features to capture within-sentence lexical
and structural dependencies. It is also similar to
work which focuses on coordinate noun phrase pars-
ing (e.g. (Hogan, 2007; Ku?bler et al, 2009)) in that
we also attempt to exploit syntactic parallelism, but
in a between-sentence setting rather than in a within-
sentence setting that only considers coordination.
As evidence of the potential of an N-best rerank-
ing approach with respect to extra-sentential con-
text, we considered the 50-best parses in the devel-
opment set produced by the generative parser, and
categorized each into one of nine bins depending
on whether this candidate parse exhibits more, less,
3A similar model which conditions on whether productions
have previously occurred within the same sentence, however,
improves F1 to 73.6%.
Overlap
less equal more
worse F1 32519 7224 17280(81.8%) (69.3%) (75.4%)
equal F1 1023 1674 540(2.6%) (16.1%) (2.4%)
better F1 6224 1527 5106(15.7%) (14.6%) (22.3%)
Table 3: Correlation between rule overlap and F1 com-
pared to the generative baseline for the 50-best parses in
the development set.
or the same amount of rule overlap with the previ-
ous correct parse than the generative baseline, and
whether the candidate parse has a better, worse, or
the same F1 measure than the generative baseline
(Table 3). We find that a larger percentage of candi-
date parses which share more productions with the
previous parse are better than the generative base-
line parse than for the other categories, and this dif-
ference is statistically significant (?2 test).
3.1 Conditional Random Fields
For our statistical reranker, we implement a linear-
chain conditional random field (CRF). CRFs are a
very flexible class of graphical models which have
been used for various sequence and relational la-
belling tasks (Lafferty et al, 2001). They have been
used for tree labelling, in XML tree labelling (Jousse
et al, 2006) and semantic role labelling tasks (Cohn
and Blunsom, 2005). They have also been used for
shallow parsing (Sha and Pereira, 2003), and full
constituent parsing (Finkel et al, 2008; Tsuruoka et
al., 2009). We exploit the flexibility of CRFs by in-
corporating features that depend on extra-sentential
context.
In a linear-chain CRF, the conditional probabil-
ity of a sequence of labels y = y{t=1...T} given a se-
quence of observed output x = x{t=1...T} and weight
vector ? = ?{k=1...K} is given as follows:
P (y|x) = 1
Z
exp(
T
?
t=1
?
k
?kfk(yt?1, yt, x, t))
27
where Z is the partition function. The feature func-
tions fk(yt?1, yt, x, t) can depend on two neighbour-
ing parses, the sentences in the sequence, and the
position of the sentence in the sequence. Since our
feature functions do not depend on the words or
the time-step within the sequence, however, we will
write fk(yt?1, yt) from now on.
We treat each document in the corpus as one CRF
sequence, and each sentence as one time-step in
the sequence. The label sequence then is the se-
quence of parses, and the outputs are the sentences
in the document. Since there is a large number of
parses possible for each sentence and correspond-
ingly many possible states for each label variable,
we restrict the possible label state-space by extract-
ing the N-best parses from a generative parser, and
rerank over the sequences of candidate parses thus
provided. We use the generative parser of Petrov
and Klein (2007), a state-splitting parser that uses an
EM algorithm to find splits in the nonterminal sym-
bols to maximize training data likelihood. We use
the 20-best parses, with an oracle F1 of 94.96% on
section 23.
To learn the weight vector, we employ a stochastic
gradient ascent method on the conditional log like-
lihood, which has been shown to perform well for
parsing tasks (Finkel et al, 2008). In standard gra-
dient ascent, the conditional log likelihood with a L2
regularization term for a Gaussian prior for a train-
ing corpus of N sequences is
L(?) =
N
?
i=1
?
t,k
?kfk(y(i)t?1, y
(i)
t )
?
N
?
i=1
log Z(i) ?
?
k
?2k
2?2
And the partial derivatives with respect to the
weights are
?L
??k
=
N
?
i=1
?
t
fk(y(i)t?1, y
(i)
t )
?
N
?
i=1
?
t
?
y,y?
fk(y, y?)P (y, y?|x(i))
?
?
k
?k
?2
The first term is the feature counts in the train-
ing data, and the second term is the feature expecta-
tions according to the current weight vector. The
third term corresponds to the penalty to non-zero
weight values imposed by regularization. The prob-
abilities in the second term can be efficiently calcu-
lated by the CRF-version of the forward-backward
algorithm.
In standard gradient ascent, we update the weight
vector after iterating through the whole training cor-
pus. Because this is computationally expensive, we
instead use stochastic gradient ascent, which ap-
proximates the true gradient by the gradient calcu-
lated from a single sample from the training corpus.
We thus do not have to sum over the training set in
the above expressions. We also employ a learning
rate multiplier on the gradient. Thus, the weight up-
date for the ith encountered training sequence during
training is
? = ? + ?i?Lstochastic(?)
?i = ?
? ?N
? ?N + i
The learning rate function is modelled on the one
used by Finkel et al (2008). It is designed such that
?i is halved after ? passes through the training set.
We train the model by iterating through the train-
ing set in a randomly permuted order, updating the
weight vector after each sequence. The parameters
?, ? , and ? are tuned to the development set. The fi-
nal settings we use are ? = 0.08, ? = 5, and ? = 50.
We use sections 2?21 of the Penn Treebank WSJ for
training, 22 for development, and 23 for testing. We
conduct 20-fold cross validation to generate the N-
best parses for the training set, as is standard for N-
best rerankers.
To rerank, we do inference with the linear-chain
CRF for the most likely sequence of parses using
the Viterbi algorithm.
3.2 Feature Functions
We experiment with various feature functions that
depend on the syntactic and lexical parallelism be-
tween yt?1 and yt. We use the occurrence of a rule
in yt that occurred in yt?1 as a feature. Based on the
results of the corpus analysis, the first representation
28
(1) (S (NP (DT NN)) (VP (VBD)))
(2) (S (NP (NNS)) (VP (VBD)))
Phrasal features:
Template: (parent, childL, childR, repeated)
(S, edge, NP, +), (S, NP, VP, +), (S, VP, edge, +), (NP, edge,
NNS,?), (NP, NNS, edge,?), (VP, edge, VBD, +), (VP, VBD,
edge, +)
Lexical features:
Template: (parent, POSL, POSR, repeated)
(S, edge, NNS, ?), (S, NNS, VBD, ?), (S, VBD, edge, +),
(NP, edge, NNS, ?), (NP, NNS, edge, ?), (VP, edge, VBD,
+), (VP, VBD, edge, +)
Figure 3: Example of features extracted from a parse se-
quence specified down to the POS level.
we tried was to simply enumerate the (non-lexical)
productions in yt along with whether that production
is found in yt?1. However, we found that our most
successful feature function is to consider overlaps in
partial structures of productions.
Specifically, we decompose a tree into all of the
nonlexical vertically and horizontally markovized
subtrees. Each of the subtrees in yt marked by
whether that same subtree occurs in the previous
tree is a feature. The simple production represen-
tation corresponds to a vertical markovization of 1
and a horizontal markovization of infinite. We found
that a vertical markovization of 1 and a horizontal
markovization of 2 produced the best results on our
data. We will call this model the phrasal model.
This schema so far only considers local substruc-
tures of parse trees, without being informed by the
lexical information found in the leaves of the tree.
We try another schema which considers the POS tag
sequences found in each subtree. A feature then is
the node label of the root of the subtree with the POS
tag sequence it dominates, again decomposed into
sequences of length 2 by markovization. We will
call this model the lexical model.
To extract features from this sequence, we con-
sider the substructures in the second parse, and mark
whether they are found in the first parse as well. We
add edge markers to mark the beginning and end of
constituents. See Figure 3 for an example of features
Method F1 (%)
Model-averaged 90.47
Combined, jointly trained ?Context 90.33
Combined, jointly trained 90.31
Model-averaged ?Context 90.22
lexical ?Context 90.21
lexical 90.20
phrasal 90.12
phrasal ?Context 89.74
Generative 89.70
Table 4: Development set (section 22) results of various
models that we trained. Italicized are the models we use
for the test set.
extracted by the two models.
We will consider various ways of combining the
two schemata above in the next section. In addition,
we also add a feature corresponding to the scaled log
probability of a parse tree derived from the genera-
tive parsing baseline. Scaling is necessary because
of the large differences in the magnitude of the log
probability for different sentences. The scaling for-
mula that we found to work best is to scale the max-
imum log probability among the N-best candidate
parses to be 1.0 and the minimum to be 0.0.
3.3 Results
We train the two models which make use of extra-
sentential context described in the previous section,
and use the model to parse the development and
test set. We also trained a model which combines
both sets of features, but we found that we get better
performance by training the two models separately,
then averaging the models by computing the respec-
tive averages of their features? weights. Thus, we
use the model-averaged version of the models that
consider context in the test set experiments. The
generative parser forms the first baseline method
to which we compare our results. We also train a
reranker which makes use of the same features as we
described above, but without marking whether each
substructure occurs in the previous sentence. This is
thus a reranking method which does not make use
of the previous context. Again, we tried model aver-
aging, but this produces less accurate parses on the
29
LP LR F1 Exact CB 0CB LP LR F1 Exact CB 0CB
development set ? length ? 40 development set ? all sentences
Generative 90.33 90.20 90.27 39.92 0.68 71.99 89.64 89.75 89.70 37.76 0.82 68.65
+Context 91.25 90.71 90.98 41.25 0.61 73.45 90.62 90.33 90.47 38.88 0.74 70.47
?Context 90.85 90.78 90.82 40.62 0.62 73.00 90.28 90.38 90.22 38.24 0.74 70.00
Table 5: Parsing results on the development set (section 22) of the Penn Treebank WSJ (%, except for CB). Generative
is the generative baseline of Petrov and Klein (2007), +Context is the best performing reranking model using previous
context (model-averaged phrasal and lexical), ?Context is the best performing reranking model not using
previous context (jointly trained phrasal and lexical).
LP LR F1 Exact CB 0CB LP LR F1 Exact CB 0CB
test set ? length ? 40 test set ? all sentences
Generative 90.04 89.84 89.94 38.31 0.80 68.33 89.60 89.35 89.47 36.05 0.94 65.81
+Context 90.63 90.11 90.37 39.02 0.73 69.40 90.17 89.64 89.91 36.84 0.87 67.09
?Context 90.64 90.43 90.54 38.62 0.72 69.84 90.20 89.97 90.08 36.47 0.85 67.55
Table 6: Parsing results on the test set (section 23) of the Penn Treebank WSJ (%, except for CB)
development set, so we use the jointly trained model
on the test set. We will refer to this model as the
context-ignorant or ?Context model, as opposed to
the previous context-aware or +Context model. The
results of these experiments on the development set
are shown in Table 4.
PARSEVAL results4 on the development and test
set are presented in Tables 5 and 6. We see that
the reranked models outperform the generative base-
line model in terms of F1, and that the reranked
model that uses extra-sentential context outperforms
the version that does not use extra-sentential context
in the development set, but not in the test set. Us-
ing Bikel?s randomized parsing evaluation compara-
tor5, we find that both reranking models outperform
the baseline generative model to statistical signifi-
cance for recall and precision. The context-ignorant
reranker outperforms the context-aware reranker on
recall (p < 0.01), but not on precision (p = 0.42).
However, the context-aware model has the highest
exact match scores in both the development and the
test set.
The F1 result suggests two possibilities?either the
context-aware model captures the same information
as the context-ignorant model, but less effectively, or
the two models capture different information about
4This evaluation ignores punctuation and corresponds to the
new.prm parameter setting on evalb.
5http://www.cis.upenn.edu/
?
dbikel/
software.html
Sec. ?Context better same +Context better
22 157 1357 186
23 258 1904 254
Table 7: Context-aware vs. context-ignorant reranking
results, by sentential F1.
the parses. Two pieces of evidence point to the
latter possibility. First, if the context-aware model
were truly inferior, then we would expect it to out-
perform the context-ignorant model on almost no
sentences. Otherwise, we would expect them to
do well on different sentences. Table 7 shows that
the context-aware model outperforms the context-
ignorant model on nearly as many trees in the test
section as the reverse. Second, if we hypotheti-
cally had an oracle that could determine whether the
context-ignorant or the context-aware model would
be more accurate on a sentence and if the two models
were complementary to each other, we would expect
to achieve a gain in F1 over the generative baseline
which is roughly the sum of the gain achieved by
each model separately. This is indeed the case, as
we are able to achieve F1s of 91.23% and 90.89%
on sections 22 and 23 respectively, roughly twice the
improvement that the individual models obtain.
To put our results in perspective, we now compare
the magnitude of the improvement in F1 our context-
30
System Baseline Best Imp. (rel.)
Dubey et al (2006) 73.3 73.6 0.3 (1.1%)
Hogan (2007) 89.4 89.6 0.2 (1.9%)
This work 89.5 89.9 0.4 (3.8%)
Table 8: A comparison of parsers specialized to exploit
intra- or extra-sentential syntactic parallelism on section
23 in terms of the generative baseline they compare them-
selves against, the best F1 their non-baseline models
achieve, and the absolute and relative improvements.
aware model achieves over the generative baseline
to that of other systems specialized to exploit intra-
or extra-sentential parallelism. We achieve a greater
improvement despite the fact that our generative
baseline provides a higher level of performance, and
is presumably thus more difficult to improve upon
(Table 8). These systems do not compare themselves
against a reranked model that does not use paral-
lelism as we do in this work.
During inference, the Viterbi algorithm recov-
ers the most probable sequence of parses, and this
means that we are relying on the generative parser to
provide the context (i.e. the previous parses) when
analyzing any given sentence. We do another type of
oracle analysis in which we provide the parser with
the correct, manually annotated parse tree of the
previous sentence when extracting features for the
current sentence during training and parsing. This
?perfect context? model achieves F1s of 90.42% and
90.00% on sections 22 and 23 respectively, which is
comparable to the best results of our reranking mod-
els. This indicates that the lack of perfect contextual
information is not a major obstacle to further im-
proving parsing performance.
3.4 Analysis
We now analyze several specific cases in the devel-
opment set in which the reranker makes correct use
of contextual information. They concretely illustrate
how context can improve parsing performance, and
confirm our initial intuition that extra-sentential con-
text can be useful for parsing. The sentence in (3)
and (4) is one such case.
(3) Generative/Context-ignorant: (S (S A BMA
spokesman said ?runaway medical costs? have
made health insurance ?a significant
challenge) ,? and (S margins also have been
pinched ...) (. .))
(4) Context-aware: (S (NP A BMA spokesman)
(VP said ?runaway medical costs? have made
health insurance ?a significant challenge,? and
margins also have been pinched ...) (. .))
The baseline and the context-ignorant models
parse the sentence as a conjunction of two S clauses,
misanalyzing the scope of what is said by the BMA
spokesman to the first part of the conjunct. By an-
alyzing the features and feature weight values ex-
tracted from the parse sequence, we determined that
the context-aware reranker is able to correct the
analysis of the scoping due to a parallelism in the
syntactic structure. Specifically, the substructure
S ? V P. is present in both this sentence and the
previous sentence of the reranked sequence, which
also contains a reporting verb.
(5) (S (NP BMA Corp., Kansas City, Mo.,) (VP
said it?s weighing ?strategic alternatives? ...
and is contacting possible buyers ...) (. .))
As a second example, consider the following sen-
tence.
(6) Generative/Context-ignorant: To achieve
maximum liquidity and minimize price
volatility, (NP either all markets) (VP should
be open to trading or none).
(7) Context-aware: To achieve maximum liquidity
and minimize price volatility, (CC either) (S
(NP all markets) should be open to trading or
none).
The original generative and context-ignorant
parses posit that ?either all markets? is a noun
phrase, which is incorrect. Syntactic parallelism cor-
rects this for two reasons. First, the reranker prefers
a determiner to start an NP in a consistent context,
as both surround sentences also contain this sub-
structure. Also, the previous sentence also contains
a conjunction CC followed by a S node under a S
node, which the reranker prefers.
While these examples show contextual features to
be useful for parsing coordinations, we also found
31
context-awareness to be useful for other types of
structural ambiguity such as PP attachment ambi-
guity. Notice that the method we employ to cor-
rect coordination errors is different from previous
approaches which usually rely on lexical or syntac-
tic similarity between conjuncts rather than between
sentences. Our approach can thus broaden the range
of sentences that can be usefully reranked. For ex-
ample, there is little similarity between conjuncts to
avail of in the second example (Sentences 6 and 7).
Based on these analyses, it appears that con-
text awareness provides a source of information for
parsing which is not available to context-ignorant
parsers. We should thus consider integrating both
types of features into the reranking parser to build
on the advantages of each. Specifically, within-
sentence features are most appropriate for lexi-
cal dependencies and some structural dependencies.
Extra-sentential features, on the other hand, are ap-
propriate for capturing the syntactic consistency ef-
fects as we have demonstrated in this paper.
4 Conclusions
In this paper, we have examined evidence for syn-
tactic consistency between neighbouring sentences.
First, we conducted a corpus analysis of the Penn
Treebank WSJ, and shown that parallelism exists
between sentences for productions with a variety
of LHS types, generalizing previous results for
noun phrase structure. Then, we explored a novel
source of features for parsing informed by the extra-
sentential context. We improved on the parsing ac-
curacy over a generative baseline parser, and rival a
similar reranking model that does not rely on extra-
sentential context. By examining the subsets of
the evaluation data on which each model performs
best and also individual cases, we argue that con-
text allows a type of structural ambiguity resolution
not available to parsers which only rely on intra-
sentential context.
Acknowledgments
We would like to thank the anonymous reviewers
and Timothy Fowler for their comments. This work
is supported in part by the Natural Sciences and En-
gineering Research Council of Canada.
References
J.K. Bock. 1986. Syntactic persistence in language pro-
duction. Cognitive Psychology, 18(3):355?387.
A. Buch and C. Pietsch. 2010. Measuring syntactic
priming in dialog corpora. In Proceedings of the Con-
ference on Linguistic Evidence 2010: Empirical, The-
oretical and Computational Perspectives.
E. Charniak and M. Johnson. 2005. Coarse-to-fine n-best
parsing and MaxEnt discriminative reranking. In Pro-
ceedings of the 43rd ACL, pages 173?180. Association
for Computational Linguistics.
K.W. Church. 2000. Empirical estimates of adaptation:
the chance of two Noriegas is closer to p/2 than p2. In
Proceedings of 18th COLING, pages 180?186. Asso-
ciation for Computational Linguistics.
T. Cohn and P. Blunsom. 2005. Semantic role labelling
with tree conditional random fields. In Ninth Confer-
ence on Computational Natural Language Learning,
pages 169?172.
M. Collins and T. Koo. 2005. Discriminative reranking
for natural language parsing. Computational Linguis-
tics, 31(1):25?70.
A. Dubey, P. Sturt, and F. Keller. 2005. Parallelism in
coordination as an instance of syntactic priming: Evi-
dence from corpus-based modeling. In Proceedings of
HLT/EMNLP 2005, pages 827?834.
A. Dubey, F. Keller, and P. Sturt. 2006. Integrating syn-
tactic priming into an incremental probabilistic parser,
with an application to psycholinguistic modeling. In
Proceedings of the 21st COLING and the 44th ACL,
pages 417?424. Association for Computational Lin-
guistics.
J.R. Finkel, A. Kleeman, and C.D. Manning. 2008. Effi-
cient, feature-based, conditional random field parsing.
Proceedings of ACL-08: HLT, pages 959?967.
S.T. Gries. 2005. Syntactic priming: A corpus-based
approach. Journal of Psycholinguistic Research,
34(4):365?399.
D. Hogan. 2007. Coordinate noun phrase disambigua-
tion in a generative parsing model. In Proceedings of
45th ACL, volume 45, pages 680?687.
F. Jousse, R. Gilleron, I. Tellier, and M. Tommasi. 2006.
Conditional random fields for XML trees. In ECML
Workshop on Mining and Learning in Graphs.
S. Ku?bler, W. Maier, E. Hinrichs, and E. Klett. 2009.
Parsing coordinations. In Proceedings of the 12th
EACL, pages 406?414. Association for Computational
Linguistics.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In International
Conference on Machine Learning, pages 282?289.
32
D. McClosky, E. Charniak, and M. Johnson. 2006. Ef-
fective self-training for parsing. In Proceedings of
HLT-NAACL 2006.
S. Petrov and D. Klein. 2007. Improved inference for
unlexicalized parsing. In Proceedings of HLT-NAACL
2007, pages 404?411. Association for Computational
Linguistics.
M.J. Pickering and H.P. Branigan. 1999. Syntactic prim-
ing in language production. Trends in Cognitive Sci-
ences, 3(4):136?141.
D. Reitter. 2008. Context Effects in Language Produc-
tion: Models of Syntactic Priming in Dialogue Cor-
pora. Ph.D. thesis, University of Edinburgh.
F. Sha and F. Pereira. 2003. Shallow parsing with condi-
tional random fields. In Proceedings of HLT-NAACL,
pages 213?220.
Y. Tsuruoka, J. Tsujii, and S. Ananiadou. 2009. Fast full
parsing by linear-chain conditional random fields. In
Proceedings of the 12th EACL, pages 790?798. Asso-
ciation for Computational Linguistics.
33
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 775?786,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Unsupervised Sentence Enhancement for Automatic Summarization
Jackie Chi Kit Cheung
University of Toronto
10 King?s College Rd., Room 3302
Toronto, ON, Canada M5S 3G4
jcheung@cs.toronto.edu
Gerald Penn
University of Toronto
10 King?s College Rd., Room 3302
Toronto, ON, Canada M5S 3G4
gpenn@cs.toronto.edu
Abstract
We present sentence enhancement as a
novel technique for text-to-text genera-
tion in abstractive summarization. Com-
pared to extraction or previous approaches
to sentence fusion, sentence enhancement
increases the range of possible summary
sentences by allowing the combination of
dependency subtrees from any sentence
from the source text. Our experiments in-
dicate that our approach yields summary
sentences that are competitive with a sen-
tence fusion baseline in terms of con-
tent quality, but better in terms of gram-
maticality, and that the benefit of sen-
tence enhancement relies crucially on an
event coreference resolution algorithm us-
ing distributional semantics. We also
consider how text-to-text generation ap-
proaches to summarization can be ex-
tended beyond the source text by exam-
ining how human summary writers incor-
porate source-text-external elements into
their summary sentences.
1 Introduction
Sentence fusion is the technique of merging sev-
eral input sentences into one output sentence
while retaining the important content (Barzilay
and McKeown, 2005; Filippova and Strube, 2008;
Thadani and McKeown, 2013). For example, the
input sentences in Figure 1 may be fused into one
output sentence.
As a text-to-text generation technique, sentence
fusion is attractive because it provides an avenue
for moving beyond sentence extraction in auto-
matic summarization, while not requiring deep se-
Input: Bil Mar Foods Co., a meat processor
owned by Sara Lee, announced a recall of
certain lots of hot dogs and packaged meat.
Input: The outbreak led to the recall on Tues-
day of 15 million pounds of hot dogs and cold
cuts produced at the Bil Mar Foods plant.
Output: The outbreak led to the recall on Tues-
day of lots of hot dogs and packaged meats
produced at the Bil Mar Foods plant.
Figure 1: An example of fusing two input sen-
tences into an output sentence. The sections of the
input sentences that are retained in the output are
shown in bold.
mantic analysis beyond, say, a dependency parser
and lexical semantic resources.
The overall trajectory pursued in the field can
be characterized as a move away from local con-
texts relying heavily on the original source text to-
wards more global contexts involving reformula-
tion of the text. Whereas sentence extraction and
sentence compression (Knight and Marcu, 2000,
for example) involve taking one sentence and per-
haps removing parts of it, traditional sentence fu-
sion involves reformulating a small number of rel-
atively similar sentences in order to take the union
or intersection of the information present therein.
In this paper, we move further along this path
in the following ways. First, we present sen-
tence enhancement as a novel technique which
extends sentence fusion by combining the subtrees
of many sentences into the output sentence, rather
than just a few. Doing so allows relevant informa-
tion from sentences that are not similar to the orig-
inal input sentences to be added during fusion. As
775
Source text: This fact has been underscored in
the last few months by two unexpected out-
breaks of food-borne illness.
Output: The outbreak of food-borne illness led
to the recall on Tuesday of lots of hot dogs
and meats produced at the Bil Mar Foods
plant.
Figure 2: An example of sentence enhancement,
in which parts of dissimilar sentences are incorpo-
rated into the output sentence.
shown in Figure 2, the phrase of food-borne illness
can be added to the previous output sentence, de-
spite originating in a source text sentence that is
quite different overall.
Elsner and Santhanam (2011) proposed a super-
vised method to fuse disparate sentences, which
takes as input a small number of sentences with
compatible information that have been manually
identified by editors of articles. By contrast, our
algorithm is unsupervised, and tackles the prob-
lem of identifying compatible event mergers in the
entire source text using an event coreference mod-
ule. Our method outperforms a previous syntax-
based sentence fusion baseline on measures of
summary content quality and grammaticality.
Second, we analyze how text-to-text genera-
tion systems may make use of text that is not in
the source text itself, but in articles on a related
topic in the same domain. By examining the parts
of human-written summaries that are not found
in the source text, we find that using in-domain
text allows summary writers to more precisely ex-
press some target semantic content, but that more
sophisticated computational semantic techniques
will be required to enable automatic systems to
likewise do so.
A more general argument of this paper is that
the apparent dichotomy between text-to-text gen-
eration and semantics-to-text generation can be
resolved by viewing them simply as having dif-
ferent starting points towards the same end goal
of precise and wide-coverage NLG. The statisti-
cal generation techniques developed by the text-
to-text generation community have been success-
ful in many domains. Yet the results of our ex-
periments and studies demonstrate the following:
as text-to-text generation techniques move beyond
using local contexts towards more dramatic refor-
mulations of the kind that human writers perform,
more semantic analysis will be needed in order to
ensure that the reformulations preserve the infer-
ences that can be drawn from the input text.
2 Related Work
A relatively large body of work exists in sentence
compression (Knight and Marcu, 2000; McDon-
ald, 2006; Galley and McKeown, 2007; Cohn
and Lapata, 2008; Clarke and Lapata, 2008, in-
ter alia), and sentence fusion (Barzilay and McK-
eown, 2005; Marsi and Krahmer, 2005; Filippova
and Strube, 2008; Filippova, 2010; Thadani and
McKeown, 2013). Unlike this work, our sentence
enhancement algorithm considers the entire source
text and is not limited to the initial input sentences.
Few previous papers focus on combining the con-
tent of diverse sentences into one output sentence.
Wan et al. (2008) propose sentence augmentation
by identifying ?seed? words in a single original
sentence, then adding information from auxiliary
sentences based on word co-occurrence counts.
Elsner and Santhanam (2011) investigate the idea
of fusing disparate sentences with a supervised al-
gorithm, as discussed above.
Previous studies on cut-and-paste summariza-
tion (Jing and McKeown, 2000; Saggion and La-
palme, 2002) investigate the operations that hu-
man summarizers perform on the source text in
order to produce the summary text. Our previ-
ous work argued that current extractive systems
rely too heavily on notions of information central-
ity (Cheung and Penn, 2013). This paper extends
this work by identifying specific linguistic factors
correlated with the use of source-text-external ele-
ments.
3 A Sentence Enhancement Algorithm
The basic steps in our sentence expansion algo-
rithm are as follows: (1) clustering to identify ini-
tial input sentences, (2) sentence graph creation,
(3) sentence graph expansion, (4) tree generation,
and (5) linearization.
At a high level, our method for sentence en-
hancement is inspired by the syntactic sentence
fusion approach of Filippova and Strube (2008)
(henceforth, F&S) originally developed for Ger-
man, in that it operates over the dependency parses
of a small number of input sentences to produce
an output sentence which fuses parts of the in-
776
put sentences. We adopt the same assumption as
F&S that these initial core sentences have a high
degree of similarity with each other, and should
form the core of a new sentence to be generated
(Step 1). While fusion from highly disparate in-
put sentences is possible, Elsner and Santhanam
(2011) showed how difficult it is to do so cor-
rectly, even where such cases are manually iden-
tified. We thus aim for a more targeted type of
fusion initially. Next, the dependency trees of
the core sentences are fused into an intermediate
sentence graph (Step 2), a directed acyclic graph
from which the final sentence will be generated
(Steps 4 and 5). We will compare against our im-
plementation of F&S, adapted to English.
However, unlike F&S or other previous ap-
proaches to sentence fusion, the sentence enhance-
ment algorithm may also avail itself of the de-
pendency parses of all of the other sentences in
the source text, which expands the range of pos-
sible sentences that may be produced. This is ac-
complished by expanding the sentence graph with
parts of these sentences (Step 3). One important
issue here is that the expansion must be modulated
by an event coreference component to ensure that
the merging of information from different points
in the source text is valid and does not result in
incorrect or nonsensical inferences.
3.1 Core sentence identification
To generate the core sentence clusters, we first
identify clusters of similar sentences, then rank the
clusters according to their salience. The top clus-
ter in the source text is then selected to be the input
to the sentence fusion algorithms.
Sentence alignment is performed by complete-
link agglomerative clustering, which requires a
measure of similarity between sentences and a
stopping criterion. We define the similarity be-
tween two sentences to be the standard cosine
similarity between the lemmata of the sentences,
weighted by IDF and excluding stopwords, and
clustering is run until a similarity threshold of
0.5 is reached. Since complete-link clustering
prefers small coherent clusters and we select the
top-scoring cluster in each document collection,
the method is somewhat robust to different choices
of the stopping threshold.
The clusters are scored according to the signa-
ture term method of Lin and Hovy (2000), which
assigns an importance score to each term accord-
BMFoods    announce    recall    certain lots...
outbreak    led    recall    Tuesday    15M pounds...
nsubj dobj
nsubj dobj
prep_of
prep_of
prep_on
(a) Abbreviated dependency trees.
BMFoods    announce                 certain lots...
outbreak    led                 Tuesday    15M pounds...
nsubj dobj
nsubj
dobj
prep_of
prep_of
prep_onrecall
food-borne illness
prep_of
(b) Sentence graph after merging the nodes with lemma recall
(in bold), and expanding the node outbreak (dashed outgoing
edge).
Figure 3: An example of the input dependency
trees for sentence graph creation and expansion,
using the input sentences of Figure 1.
ing to how much more often it appears in the
source text compared to some irrelevant back-
ground text using a log likelihood ratio. Specifi-
cally, the score of a cluster is equal to the sum of
the importance scores of the set of lemmata in the
cluster.
3.2 Sentence graph creation
After core sentence identification, the next step
is to align the nodes of the dependency trees of
the core input sentences in order to create the ini-
tial sentence graph. The input to this step is the
collapsed dependency tree representations of the
core sentences produced by the Stanford parser
1
.
In this representation, preposition nodes are col-
lapsed into the label of the dependency edge be-
tween the functor of the prepositional phrase and
the prepositional object. Chains of conjuncts are
also split, and each argument is attached to the
parent. In addition, auxiliary verbs, negation par-
ticles, and noun-phrase-internal elements
2
are col-
lapsed into their parent nodes. Figure 3a shows
the abbreviated dependency representations of the
input sentences from Figure 1.
Then, a sentence graph is created by merging
nodes that share a common lemma and part-of-
1
As part of the CoreNLP suite: http://nlp.
stanford.edu/software/corenlp.shtml
2
As indicated by the dependency edge label nn.
777
speech tag. In addition, we allow synonyms to
be merged, defined as being in the same Word-
Net synset. Merging is blocked if the word is a
stop word, which includes function words as well
as a number of very common verbs (e.g., be, have,
do). Throughout the sentence graph creation and
expansion process, the algorithm disallows the ad-
dition of edges that would result in a cycle in the
graph.
3.3 Sentence graph expansion
The initial sentence graph is expanded by merg-
ing in subtrees from dependency parses of non-
core sentences drawn from the source text. First,
expansion candidates are identified for each node
in the sentence graph by finding all of the depen-
dency edges in the source text from non-core sen-
tences in which the governor of the edge shares
the same lemma and POS tag as the node in the
sentence graph.
Then, these candidate edges are pruned accord-
ing to two heuristics. The first is to keep only one
candidate edge of each dependency relation type
according to the edge that has the highest informa-
tiveness score (Section 3.4.1), with ties being bro-
ken according to which edge has a subtree with a
fewer number of nodes. The second is to perform
event coreference in order to prune away those
candidate edges which are unlikely to be describ-
ing the same event as the core sentences, as ex-
plained in the next section. Finally, any remaining
candidate edges are fused into the sentence graph,
and the subtree rooted at the dependent of the can-
didate edge is added to the sentence graph as well.
See Figure 3b for an example of sentence graph
creation and expansion.
3.3.1 Event coreference
One problem of sentence fusion is that the differ-
ent inputs of the fusion may not refer to the same
event, resulting in an incorrect merging of infor-
mation, as would be the case in the following ex-
ample:
S1: Officers pled not guilty but risked 25 years to
life.
S2: Officers recklessly engaged in conduct which
seriously risked the lives of others.
Here, the first usage of risk refers to the potential
sentence imposed if the officers are convicted in
a trial, whereas the second refers to the potential
harm caused by the officer.
Context 1: Officers ... risked 25 years to life...
(nsubj, officers)   (dobj, life)
(nsubj, conduct)   (advmod, seriously)   (dobj, life)
sim1((risk, dobj), (risk, dobj))
    ? sim2(life, life) = 1.0
sim1((risk, nsubj), (risk, nsubj))
  ? sim2(officer, conduct) = 0.38
Context 2: ...conduct seriously risked the lives...
Figure 4: Event coreference resolution as a
maximum-weight bipartite graph matching prob-
lem. All the nodes share the predicate risk.
In order to ensure that sentence enhancement
does not lead to the merging of such incompati-
ble events, we designed a simple method to ap-
proximate event coreference resolution that does
not require event coreference labels. This method
is based on the intuition that different mentions of
an event should contain many of the same partic-
ipants. Thus, by measuring the similarity of the
arguments and the syntactic contexts between the
node in the sentence graph and the candidate edge,
we can have a measure of the likelihood that they
refer to the same event.
We would be interested in integrating existing
event coreference resolution systems into this step
in the future, such as the unsupervised method
of Bejan and Harabagiu (2010). Existing event
coreference systems tend to focus on cases with
different heads (e.g., X kicked Y, then Y was in-
jured), which could increase the possibilities for
sentence enhancement, if the event coreference
module is sufficiently accurate. However, since
our method currently only merges identical heads,
we require a more fine-grained method based on
distributional measures of similarity.
We measure the similarity of these syntactic
contexts by aligning the arguments in the syn-
tactic contexts and computing the similarity of
the aligned arguments. These problems can be
jointly solved as a maximum-weight bipartite
graph matching problem (Figure 4). Formally, let
a syntactic context be a list of dependency triples
(h, r, a), consisting of a governor or head node h
and a dependent argument a in the dependency re-
lation r, where head node h is fixed across each
778
element of the list. Then, each of the two in-
put syntactic contexts forms one of the two dis-
joint sets in a complete weighted bipartite graph
where each node corresponds to one dependency
triple. We define the edge weights according to
the similarities of the edge?s incident nodes; i.e.,
between two dependency triples (h
1
, r
1
, a
1
) and
(h
2
, r
2
, a
2
). We also decompose the similarity
into the similarities between the head and relation
types ((h
1
, r
1
) and (h
2
, r
2
)), and between the ar-
guments (a
1
and a
2
). The edge weight function is
thus:
sim((h
1
, r
1
, a
1
), (h
2
, r
2
, a
2
)) = (1)
sim
1
((h
1
, r
1
), (h
2
, r
2
))? sim
2
(a
1
, a
2
),
where sim
1
and sim
2
are binary functions that rep-
resent the similarities between governor-relation
pairs and dependents, respectively. We train mod-
els of distributional semantics using a large back-
ground corpus; namely, the Annotated Gigaword
corpus (Napoles et al., 2012). For sim
1
, we cre-
ate a vector of counts of the arguments that are
seen filling each (h, r) pair, and define the similar-
ity between two such pairs to be the cosine simi-
larity between their argument vectors. For sim
2
,
we create a basic vector-space representation of
a word d according to words that are found in
the context of word d within a five-word context
window, and likewise compute the cosine simi-
larity between the word vectors. These methods
of computing distributional similarity are well at-
tested in lexical semantics for measuring the re-
latedness of words and syntactic structures (Tur-
ney and Pantel, 2010), and similar methods have
been applied in text-to-text generation by Ganitke-
vitch et al. (2012), though the focus of that work is
to use paraphrase information thus learned to im-
prove sentence compression.
The resulting graph matching problem is solved
using the NetworkX package for Python
3
. The fi-
nal similarity score is an average of the similarity
scores from Equation 1 that participate in the se-
lected matching, weighted by the product of the
IDF scores of the dependent nodes of each edge.
This final score is used as a threshold that candi-
date contexts from the source text must meet in
order to be eligible for being merged into the sen-
tence graph. This threshold was tuned by cross-
validation, and can remain constant, although re-
3
http://networkx.github.io/
tuning to different domains (a weakly supervised
alternative) is likely to be beneficial.
3.4 Tree generation
The next major step of the algorithm is to extract
an output dependency tree from the expanded sen-
tence graph. We formulate this as an integer linear
program, in which variables correspond to edges
of the sentence graph, and a solution to the linear
program determines the structure of an output de-
pendency tree. We use ILOG CPLEX to solve all
of the integer linear programs in our experiments.
A good dependency tree must at once express
the salient or important information present in the
input text as well as be grammatically correct and
of a manageable length. These desiderata are en-
coded into the linear program as constraints or as
part of the objective function.
3.4.1 Objective function
We designed an objective function that considers
the importance of the words and syntactic rela-
tions that are selected as well as accounts for re-
dundancy in the output sentence. Let X be the set
of variables in the program, and let each variable
in X take the form x
h,r,a
, a binary variable that
represents whether an edge in the sentence graph
from a head node with lemma h to an argument
with lemma a in relation r is selected. For a lexi-
con ?, our objective function is:
max
?
w??
max
x
h,r,a
?Xs.t.a=w
(x
h,r,w
? P (r|h) ? I(w)),
(2)
where P (r|h) is the probability that head h
projects the dependency relation r, and I(w) is
the informativeness score for word w as defined
by Clarke and Lapata (2008). This formulation
encourages the selection of words that are infor-
mative according to I(w) and syntactic relations
that are probable. The inner max function for each
w in the lexicon encourages non-redundancy, as
each word may only contribute once to the objec-
tive value. This function can be rewritten into a
form compatible with a standard linear program by
the addition of auxiliary variables and constraints.
For more details of how this and other aspects of
the linear program are implemented, see the sup-
plementary document.
3.4.2 Constraints
Well-formedness constraints, taken directly from
F&S, ensure that the set of selected edges pro-
779
duces a tree. Another constraint limits the number
of content nodes in the tree to 11, which corre-
sponds to the average number of content nodes in
human-written summary sentences in the data set.
Syntactic constraints aim to ensure grammatical-
ity of the output sentence. In addition to the con-
straint proposed by F&S regarding subordinating
conjunctions, we propose two other ones. The first
ensures that a nominal or adjectival predicate must
be selected with a copular construction at the top
level of a non-finite clause. The second ensures
that transitive verbs retain both of their comple-
ments in the output
4
. Semantic constraints ensure
that only noun phrases of sufficiently high simi-
larity which are not in a hyperonym-hyponym or
holonym-meronym relation with each other may
be joined by coordination.
3.5 Linearization
The final step of our method is to linearize the de-
pendency tree from the previous step into the final
sequence of words. We implemented our own lin-
earization method to take advantage of the order-
ing information can be inferred from the original
source text sentences.
Our linearization algorithm proceeds top-down
from the root of the dependency tree to the leaves.
At each node of the tree, linearization consists of
realizing the previously collapsed elements such
as prepositions, determiners and noun compound
elements, then ordering the dependent nodes with
respect to the root node and each other. Restoring
the collapsed elements is accomplished by simple
heuristics. For example, prepositions and deter-
miners precede their accompanying noun phrase.
The dependent nodes are ordered by a sort-
ing algorithm, where the order between two syn-
tactic relations and dependent nodes (r
1
, a
1
) and
(r
2
, a
2
) is determined as follows. First, if a
1
and
a
2
originated from the same source text sentence,
then they are ordered according to their order of
appearance in the source text. Otherwise, we con-
sider the probability P (r
1
precedes r
2
), and order
a
1
before a
2
iff P (r
1
precedes r
2
) > 0.5. This
distribution, P (r
1
precedes r
2
), is estimated by
counting and normalizing the order of the relation
types in the source text corpus. For the purposes
of ordering, the governor node is treated as if it
4
We did not experiment with changing the grammatical
voice in the output tree, such as introducing a passive con-
struction if only a direct object is selected, but this is one
possible extension of the algorithm.
were a dependent node with a special syntactic re-
lation label self. This algorithm always produces
an output ordering with a projective dependency
tree, which is a reasonable assumption for English.
4 Experiments
4.1 Method
Recent approaches to sentence fusion have of-
ten been evaluated as isolated components. For
example, F&S evaluate the output sentences by
asking human judges to rate the sentences? in-
formativeness and grammaticality according to a
1?5 Likert scale rating. Thadani and McKe-
own (2013) combine grammaticality ratings with
an automatic evaluation which compares the sys-
tem output against gold-standard sentences drawn
from summarization data sets. However, this eval-
uation setting still does not reflect the utility of
sentence fusion in summarization, because the
input sentences come from human-written sum-
maries rather than the original source text.
We adopt a more realistic setting of using sen-
tence fusion in automatic summarization by draw-
ing the input or core sentences automatically from
the source text, then evaluating the output of the
fusion and expansion algorithm directly as one-
sentence summaries according to standard sum-
marization evaluation measures of content quality.
Data preparation. Our experiments are con-
ducted on the TAC 2010 and TAC 2011 Guided
Summarization corpus (Owczarzak and Dang,
2010), on the initial summarization task. Each
document cluster is summarized by one sentence,
generated from an initial cluster of core sentences
as described in Section 3.1.
Evaluation measures. We evaluate summary
content quality using the word-overlap measures
ROUGE-1 and ROUGE-2, as is standard in the
summarization community. We also measure the
quality of sentences at a syntactic or shallow se-
mantic level that operates at the level of depen-
dency triples by a measure that we call Pyra-
mid BE. Specifically, we extract all of the depen-
dency triples of the form t = (h, r, a) from the
sentence under evaluation and the gold-standard
summaries, where h and a are the lemmata of
the head and the argument, and r is the syntac-
tic relation, normalized for grammatical voice and
excluding the collapsed edges which are mostly
noun-phrase-internal elements and grammatical
780
Method Pyramid BE ROUGE-1 ROUGE-2 Log Likelihood Oracle Pyramid BE
Fusion (F&S) 10.61 10.07 2.15 -159.31 28.00
Expansion 8.82 9.41 1.82 -157.46 52.97
+Event coref 11.00 9.76 1.93 -156.20 40.30
Table 1: Results of the sentence enhancement and fusion experiments.
particles. Then, we perform a matching between
the set of triples in the sentence under evalua-
tion and in a reference summary following the
Transformed BE method of Tratz and Hovy (2008)
with the total weighting scheme. This match-
ing is performed between the sentence and ev-
ery gold-standard summary, and the maximum of
these scores is taken. This score is then divided
by the maximum score that is achievable using the
number of triples present in the input sentence, as
inspired by the Pyramid method. This denom-
inator is more appropriate than the one used in
Transformed BE, which is designed for the case
where the evaluated summary and the reference
summaries are of comparable length.
For grammaticality, we parse the output sen-
tences using the Stanford parser
5
, and use the log
likelihood of the most likely parse of the sentence
as a coarse estimate of grammaticality. Parse log
likelihoods have been shown to be useful in deter-
mining grammaticality (Wagner et al., 2009), and
many of the problems associated with using it do
not apply in our evaluation, because our sentences
have a fixed number of content nodes, and contain
similar content. While we could have conducted
a user study to elicit Likert-scale grammaticality
judgements, such results are difficult to interpret
and the scores depend heavily on the set of judges
and the precise evaluation setting, as is the case for
sentence compression (Napoles et al., 2011).
4.2 Results and discussion
As shown in Table 1, sentence enhancement with
coreference outperforms the sentence fusion algo-
rithm of F&S in terms of the Pyramid BE measure
and the baseline expansion algorithm, though only
the latter difference is statistically significant (p =
0.019
6
). In terms of the ROUGE word overlap
5
The likelihoods are obtained by the PCFG model of
CoreNLP version 1.3.2. We experimented with the Berke-
ley parser (Petrov et al., 2006) as well, with similar results
that favour the sentence enhancement with event coreference
method, but because the parser failed to parse a number of
cases, we do not report those results here.
6
All statistical significance results in this section are for
Wilcoxon signed-rank tests.
measures, fusion achieves a better performance,
but it only outperforms the expansion baseline
significantly (ROUGE-1: p = 0.021, ROUGE-
2: p = 0.012). Note that the ROUGE scores
are low because they involve comparing a one-
sentence summary against a paragraph-long gold
standard. The average log likelihood result sug-
gests that sentence enhancement with event coref-
erence produces sentences that are more grammat-
ical than traditional fusion does, and this differ-
ence is statistically significant (p = 0.044). These
results show that sentence enhancement with event
coreference is competitive with a strong previous
sentence fusion method in terms of content, de-
spite having to combine information from more
diverse sentences. This does not come at the ex-
pense of grammaticality; in fact, it seems that hav-
ing a greater possible range of output sentences
may even improve the grammaticality of the out-
put sentences.
Oracle score. To examine the potential of sen-
tence enhancement, we computed an oracle score
that provides an upper bound to the best possi-
ble sentence that may be extracted from the sen-
tence graph. First, we ranked all of dependency
triples found in each gold-standard summary by
their score (i.e., the number of gold-standard sum-
maries they appear in). Then, we took the high-
est scoring triples from this ranking that are found
in the sentence graph until the length limit was
reached, and divided by the Pyramid-based de-
nominator as above
7
. The oracle score is the max-
imum of these scores over the gold-standard sum-
maries. The resulting oracle scores are shown
in the rightmost column of Table 1. While it
is no surprise that the oracle score improves af-
ter the sentence graph is expanded, the large in-
crease in the oracle score indicates the potential of
sentence enhancement for generating high-quality
summary sentences.
7
There is no guarantee that these dependency triples form
a tree structure. Hence, this is an upper bound.
781
Grammaticality. There is still room for im-
provement in the grammaticality of the generated
sentences, which will require modelling contexts
larger than individual predicates and their argu-
ments. Consider the following output of the sen-
tence enhancement with event coreference system:
(3) The government has launched an
investigation into Soeharto?s wealth by the
Attorney General?s office on the wealth of
former government officials.
This sentence suffers from coherence problems
because two pieces of information are duplicated.
The first is the subject of the investigation, which
is expressed by two prepositional objects of in-
vestigation with the prepositions into and on.
The second, more subtle incoherence concerns
the body that is responsible for the investigation,
which is expressed both by the subject of launch
(The government has launched an investigation),
and the by-prepositional object of investigation (an
investigation ... by the Attorney General?s office).
Clearly, a model that makes fewer independence
assumptions about the relation between different
edges in the sentence graph is needed.
5 A Study of Source-External Elements
The sentence enhancement algorithm presented
above demonstrates that it is possible to use the
entire source text to produce an informative sen-
tence. Yet it is still limited by the particular pred-
icates and dependency relations that are found
in the source. The next step towards develop-
ing abstractive systems that exhibit human-like be-
haviour is to try to incorporate elements into the
summary that are not found in the source text at
all.
Despite its apparent difficulty, there is reason to
be hopeful for text-to-text generation techniques
even in such a scenario. In particular, we showed
in earlier work that almost all of the caseframes,
or pairs of governors and relations, in human-
written summaries can be found in the source text
or in a small set of additional related articles that
belong to the same domain as the source text (e.g.,
natural disasters) (Cheung and Penn, 2013). What
that study lacks, however, is a detailed analysis
of the factors surrounding why human summary
writers use non-source-text elements in their sum-
maries, and how these may be automatically iden-
tified in the in-domain text. In this section, we
supply such an analysis and provide evidence that
human summary writers actually do incorporate
elements external to the source text for a reason,
namely, that these elements are more specific to
the semantic content that they wish to convey. We
also identify a number of features that may be use-
ful for automatically determining the appropriate-
ness of these in-domain elements in a summary.
5.1 Method
We performed our analysis on the predicates
present in text, such as kill and computer. We also
analyzed predicate-relation pairs (PR pairs) such
as (kill, nsubj) or (computer, amod). This choice
is similar to the caseframes used by Cheung and
Penn (2013), and we similarly apply transforma-
tions to normalize for grammatical voice and other
syntactic alternations, but we consider PR pairs of
all relation types, unlike caseframes, which only
consider verb complements and prepositional ob-
jects. PR pairs are extracted from the prepro-
cessed corpus. We use the TAC 2010 Guided
Summarization data set for our analyses, which
we organize into two sub-studies. In the prove-
nance study, we divide the PR pairs in human-
written summaries according to whether they are
found in the source text (source-internal) or not
(source-external). In the domain study, we divide
in-domain but source-external predicate-relation
pairs according to whether they are used in a
human-written summary (gold-standard) or not
(non-gold-standard).
5.2 Provenance Study
In the first study, we compare the characteristics
of gold-standard predicates and PR pairs accord-
ing to their provenance; that is, are they found in
the source text itself? The question that we try to
answer is why human summarizers need to look
beyond the source text at all when writing their
summaries. We will provide evidence that they do
so because they can find predicates that are more
appropriate to the content that is being expressed
according to two quantitative measures.
Predicate provenance. Source-external PR
pairs may be external to the source text for two
reasons. Either the predicate (i.e., the actual word)
is found in the source text, but the dependency
relation (i.e., the semantic predication that holds
between the predicate and its arguments) is
not found with that particular predicate, or the
782
Average freq (millions)
Source-internal 1.77 (1.57, 2.08)
Source-external 1.15 (0.99, 1.50)
(a) The average predicate frequency of source-internal vs.
source-external gold-standard predicates in an external corpus.
Arg entropy
Source-internal 7.94 (7.90, 7.97)
Source-external 7.42 (7.37, 7.48)
(b) The average argument entropy of source-internal vs. source-
external PR pairs in bits.
Table 2: Results of the provenance study. 95%
confidence intervals are estimated by the bootstrap
method and indicated in parentheses.
predicate itself may be external to the source text
altogether. If the former is true, then a generalized
version of the sentence enhancement algorithm
presented in this paper could in principle capture
these PR-pairs. We thus compute the proportion
of source-external PR pairs where the predicate
already exists in the source text.
We find that 2413 of the 4745 source-external
PR pairs, or 51% have a predicate that can be
found in the source text. This indicates that an
extension of the sentence enhancement with event
coreference approach presented in this paper could
capture a substantial portion of the source-external
PR pairs in its hypothesis space already.
Predicate frequency. What factors then can ac-
count for the remaining predicates that are not
found in the source text at all? The first such fac-
tor we identify is the frequency of the predicates.
Here, we take frequency to be the number of oc-
currences of the predicate in an external corpus;
namely the Annotated Gigaword, which gives us
a proxy for the specificity or informativeness of a
word. In this comparison, we take the set of pred-
icates in human-written summaries, divide them
according to whether they are found in the source
text or not, and then look up their frequency of ap-
pearance in the Annotated Gigaword corpus.
As Table 2a shows, the predicates that are not
found in the source text consist of significantly less
frequent words on average (Wilcoxon rank-sums
test, p < 10
?17
). This suggests that human sum-
mary writers are motivated to use source-external
predicates, because they are able to find a more in-
formative or apposite predicate than the ones that
are available in the source text.
Entropy of argument distribution. Another
measure of the informativeness or appropriateness
of a predicate is to examine the range of arguments
that it tends to take. A more generic word would
be expected to take a wider range of arguments,
whereas a more particular word would take a nar-
rower range of arguments, for example those of
a specific entity type. We formalize this notion
by measuring the entropy of the distribution of ar-
guments that a predicate-relation pair takes as ob-
served in Annotated Gigaword. Given frequency
statistics f(h, r, a) of predicate head h taking an
argument word a in relation r, we define the argu-
ment distribution of predicate-relation pair (h, r)
as:
P (a|h, r) = f(h, r, a)/
?
a
?
f(h, r, a
?
) (4)
We then compute the entropy of P (a|h, r) for the
gold-standard predicate-relation pairs, and com-
pare the average argument entropies of the source-
internal and the source-external subsets.
Table 2b shows the result of this comparison.
Source-external PR pairs exhibit a lower average
argument entropy, taking a narrower range of pos-
sible arguments. Together these two findings indi-
cate that human summary writers look beyond the
source text not just for the sake of diversity or to
avoid copying the source text; they do so because
they can find predicates that more specifically con-
vey some desired semantic content.
5.3 Domain study
The second study examines how to distinguish
those source-external predicates and PR pairs in
in-domain articles that are used in a summary from
those that are not. For this study, we rely on the
topic category divisions in the TAC 2010 data set,
and define the in-domain text to be the documents
that belong to the same topic category as the target
document cluster (but not including the target doc-
ument cluster itself). This study demonstrates the
importance of better semantic understanding for
developing a text-to-text generation system that
uses in-domain text, and identifies potentially use-
ful features for training such a system.
Nearest neighbour similarity. In the event-
coreference step of the sentence enhancement al-
gorithm, we relied on distributional semantics to
783
N NN sim
GS 2202 0.493 (0.486, 0.501)
Non-GS 789K 0.443 (0.442, 0.443)
(a) Average similarity of gold-standard (GS) and
non-gold-standard (non-GS) PR pairs to the near-
est neighbour in the source text.
N Freq. (millions) Fecundity
GS 1568 2.44 (2.05, 2.94) 21.6 (20.8, 22.5)
non-GS 268K 0.85 (0.83, 0.87) 6.43 (6.41, 6.47)
(b) Average frequency and fecundity of GS and non-GS predicates in
an external corpus. The differences are statistically significant (p <
10
?10
).
Table 3: Results of the domain study. 95% confidence intervals are given in parentheses.
measure the similarity of arguments. Here, we
examine how well distributional similarity deter-
mines the appropriateness of a source-external PR
pair in a summary. Specifically, we measure its
similarity to the nearest PR pair in the source text.
To determine the similarity between two PR pairs,
we compute the cosine similarity between their
vector representations. The vector representation
of a PR pair is the concatenation of a context vec-
tor for the predicate itself and a selectional pref-
erences vector for the PR pair; that is, the vector
of counts with elements f(h, r, a) for fixed h and
r. These vectors are trained from the Annotated
Gigaword corpus.
The average nearest-neighbour similarities of
PR pairs are shown in Table 3a. While the dif-
ference between the gold-standard and non-gold-
standard PR pairs is indeed statistically signifi-
cant, the magnitude of the difference is not large.
This illustrates the challenge of mining source-
external text for abstractive summarization, and
demonstrates the need for a more structured or
detailed semantic representation in order to deter-
mine the PR pairs that would be appropriate. In
other words, the kind of simple event coreference
method based solely on distributional semantics
that we used in Section 3.3.1 is unlikely to be suf-
ficient when moving beyond the source text.
Frequency and fecundity. We also explore sev-
eral features that would be relevant to identifying
predicates in in-domain text that are used in the
automatic summary. This is a difficult problem, as
less than 0.6% of such predicates are actually used
in the source text. As a first step, we consider sev-
eral simple measures of the frequency and charac-
teristics of the predicates.
The first measure that we compute is the aver-
age predicate frequency of the gold-standard and
non-gold-standard predicates in an external cor-
pus, as in Section 5.2. A second, related mea-
sure is to compute the number of possible relations
that may occur with a given predicate. We call
this measure the fecundity of a predicate. Both
of these are computed with respect to the external
Annotated Gigaword corpus, as before.
As shown in Table 3b, there is a dramatic dif-
ference in both measures between gold-standard
and non-gold-standard predicates in in-domain ar-
ticles. Gold-standard predicates tend to be more
common words compared to non-gold-standard
ones. This result is not in conflict with the re-
sult in the provenance study that source-external
predicates are less common words. Rather, it is
a reminder that the background frequencies of the
predicates matter, and must be considered together
with the semantic appropriateness of the candidate
word.
6 Conclusions
This paper introduced sentence enhancement as
a method to incorporate information from multi-
ple points in the source text into one output sen-
tence in a fashion that is more flexible than previ-
ous sentence fusion algorithms. Our results show
that sentence enhancement improves the content
and grammaticality of summary sentences com-
pared to previous syntax-based sentence fusion ap-
proaches. Then, we presented studies on the com-
ponents of human-written summaries that are ex-
ternal to the source text. Our analyses suggest that
human summary writers look beyond the source
text to find predicates and relations that more pre-
cisely express some target semantic content, and
that more sophisticated semantic techniques are
needed in order to exploit in-domain articles for
text-to-text generation in summarization.
Acknowledgments
We would like to thank the anonymous reviewers
for valuable suggestions. The first author was sup-
ported by a Facebook PhD Fellowship during the
completion of this research.
784
References
Regina Barzilay and Kathleen R. McKeown. 2005.
Sentence fusion for multidocument news summa-
rization. Computational Linguistics, 31(3):297?
328.
Cosmin A. Bejan and Sanda Harabagiu. 2010. Unsu-
pervised event coreference resolution with rich lin-
guistic features. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics, pages 1412?1422. Association for Compu-
tational Linguistics.
Jackie Chi Kit Cheung and Gerald Penn. 2013. To-
wards robust abstractive multi-document summa-
rization: A caseframe analysis of centrality and do-
main. In Proceedings of the 51st Annual Meeting
of the Association for Computational Linguistics,
pages 1233?1242, August.
James Clarke and Mirella Lapata. 2008. Global in-
ference for sentence compression: An integer linear
programming approach. Journal of Artificial Intelli-
gence Research(JAIR), 31:399?429.
Trevor Cohn and Mirella Lapata. 2008. Sentence
compression beyond word deletion. In Proceedings
of the 22nd International Conference on Compu-
tational Linguistics (Coling 2008), pages 137?144,
Manchester, UK, August. Coling 2008 Organizing
Committee.
Micha Elsner and Deepak Santhanam. 2011. Learn-
ing to fuse disparate sentences. In Proceedings of
the Workshop on Monolingual Text-To-Text Gener-
ation, pages 54?63. Association for Computational
Linguistics.
Katja Filippova and Michael Strube. 2008. Sentence
fusion via dependency graph compression. In Pro-
ceedings of the 2008 Conference on Empirical Meth-
ods in Natural Language Processing, pages 177?
185, Honolulu, Hawaii, October. Association for
Computational Linguistics.
Katja Filippova. 2010. Multi-sentence compression:
Finding shortest paths in word graphs. In Proceed-
ings of the 23rd International Conference on Com-
putational Linguistics (Coling 2010), pages 322?
330, Beijing, China, August. Coling 2010 Organiz-
ing Committee.
Michel Galley and Kathleen McKeown. 2007. Lex-
icalized Markov grammars for sentence compres-
sion. In Human Language Technologies 2007:
The Conference of the North American Chapter of
the Association for Computational Linguistics; Pro-
ceedings of the Main Conference, pages 180?187.
Juri Ganitkevitch, Benjamin Van Durme, and Chris
Callison-Burch. 2012. Monolingual distributional
similarity for text-to-text generation. In Proceedings
of *SEM 2012: The First Joint Conference on Lex-
ical and Computational Semantics, pages 256?264,
Montr?eal, Canada, June. Association for Computa-
tional Linguistics.
Hongyan Jing and Kathleen R. McKeown. 2000. Cut
and paste based text summarization. In Proceed-
ings of the 1st North American Chapter of the As-
sociation for Computational Linguistics Conference,
pages 178?185.
Kevin Knight and Daniel Marcu. 2000. Statistics-
based summarization?step one: Sentence compres-
sion. In Proceedings of the National Conference on
Artificial Intelligence, pages 703?710.
Chin-Yew Lin and Eduard Hovy. 2000. The automated
acquisition of topic signatures for text summariza-
tion. In COLING 2000 Volume 1: The 18th In-
ternational Conference on Computational Linguis-
tics, COLING ?00, pages 495?501, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Erwin Marsi and Emiel Krahmer. 2005. Explorations
in sentence fusion. In Proceedings of the European
Workshop on Natural Language Generation, pages
109?117.
Ryan T. McDonald. 2006. Discriminative sentence
compression with soft syntactic evidence. In 11th
Conference of the European Chapter of the Associa-
tion for Computational Linguistics.
Courtney Napoles, Benjamin Van Durme, and Chris
Callison-Burch. 2011. Evaluating sentence com-
pression: Pitfalls and suggested remedies. In Pro-
ceedings of the Workshop on Monolingual Text-To-
Text Generation, pages 91?97. Association for Com-
putational Linguistics.
Courtney Napoles, Matthew Gormley, and Benjamin
Van Durme. 2012. Annotated Gigaword. In Pro-
ceedings of the NAACL-HLT Joint Workshop on Au-
tomatic Knowledge Base Construction & Web-scale
Knowledge Extraction (AKBC-WEKEX), pages 95?
100.
Karolina Owczarzak and Hoa T. Dang. 2010. TAC
2010 guided summarization task guidelines.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and
interpretable tree annotation. In Proceedings of
the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associ-
ation for Computational Linguistics.
Horacio Saggion and Guy Lapalme. 2002. Generat-
ing indicative-informative summaries with SumUM.
Computational Linguistics, 28(4):497?526.
Kapil Thadani and Kathleen McKeown. 2013. Super-
vised sentence fusion with single-stage inference. In
Proceedings of the Sixth International Joint Confer-
ence on Natural Language Processing, pages 1410?
1418, Nagoya, Japan, October. Asian Federation of
Natural Language Processing.
Stephen Tratz and Eduard Hovy. 2008. Summariza-
tion evaluation using transformed Basic Elements.
In Proceedings of the First Text Analysis Conference
(TAC).
785
Peter D. Turney and Patrick Pantel. 2010. From
frequency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Research,
37:141?188.
Joachim Wagner, Jennifer Foster, and Josef van Gen-
abith. 2009. Judging grammaticality: Experi-
ments in sentence classification. CALICO Journal,
26(3):474?490.
Stephen Wan, Robert Dale, Mark Dras, and Cecile
Paris. 2008. Seed and grow: Augmenting statisti-
cally generated summary sentences using schematic
word patterns. In Proceedings of the 2008 Con-
ference on Empirical Methods in Natural Language
Processing, pages 543?552, Honolulu, Hawaii, Oc-
tober. Association for Computational Linguistics.
786
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 33?43,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Evaluating Distributional Models of Semantics for Syntactically
Invariant Inference
Jackie CK Cheung and Gerald Penn
Department of Computer Science
University of Toronto
Toronto, ON, M5S 3G4, Canada
{jcheung,gpenn}@cs.toronto.edu
Abstract
A major focus of current work in distri-
butional models of semantics is to con-
struct phrase representations composition-
ally from word representations. However,
the syntactic contexts which are modelled
are usually severely limited, a fact which
is reflected in the lexical-level WSD-like
evaluation methods used. In this paper, we
broaden the scope of these models to build
sentence-level representations, and argue
that phrase representations are best eval-
uated in terms of the inference decisions
that they support, invariant to the partic-
ular syntactic constructions used to guide
composition. We propose two evaluation
methods in relation classification and QA
which reflect these goals, and apply several
recent compositional distributional models
to the tasks. We find that the models out-
perform a simple lemma overlap baseline
slightly, demonstrating that distributional
approaches can already be useful for tasks
requiring deeper inference.
1 Introduction
A number of unsupervised semantic models
(Mitchell and Lapata, 2008, for example) have re-
cently been proposed which are inspired at least
in part by the distributional hypothesis (Harris,
1954)?that a word?s meaning can be character-
ized by the contexts in which it appears. Such
models represent word meaning as one or more
high-dimensional vectors which capture the lex-
ical and syntactic contexts of the word?s occur-
rences in a training corpus.
Much of the recent work in this area has, fol-
lowing Mitchell and Lapata (2008), focused on
the notion of compositionality as the litmus test of
a truly semantic model. Compositionality is a nat-
ural way to construct representations of linguistic
units larger than a word, and it has a long history
in Montagovian semantics for dealing with argu-
ment structure and assembling rich semantical ex-
pressions of the kind found in predicate logic.
While compositionality may thus provide a
convenient recipe for producing representations
of propositionally typed phrases, it is not a nec-
essary condition for a semantic representation.
Rather, that distinction still belongs to the crucial
ability to support inference. It is not the inten-
tion of this paper to argue for or against composi-
tionality in semantic representations. Rather, our
interest is in evaluating semantic models in order
to determine their suitability for inference tasks.
In particular, we contend that it is desirable and
arguably necessary for a compositional semantic
representation to support inference invariantly, in
the sense that the particular syntactic construction
that guided the composition should not matter rel-
ative to the representations of syntactically differ-
ent phrases with the same meanings. For example,
we can assert that John threw the ball and The ball
was thrown by John have the same meaning for
the purposes of inference, even though they differ
syntactically.
An analogy can be drawn to research in image
processing, in which it is widely regarded as im-
portant for the representations of images to be in-
variant to rotation and scaling. What we should
want is a representation of sentence meaning that
is invariant to diathesis, other regular syntactic al-
ternations in the assignment of argument struc-
ture, and, ideally, even invariant to other meaning-
preserving or near-preserving paraphrases.
33
Existing evaluations of distributional semantic
models fall short of measuring this. One evalua-
tion approach consists of lexical-level word sub-
stitution tasks which primarily evaluate a sys-
tem?s ability to disambiguate word senses within a
controlled syntactic environment (McCarthy and
Navigli, 2009, for example). Another approach is
to evaluate parsing accuracy (Socher et al 2010,
for example), which is really a formalism-specific
approximation to argument structure analysis.
These evaluations may certainly be relevant to
specific components of, for example, machine
translation or natural language generation sys-
tems, but they tell us little about a semantic
model?s ability to support inference.
In this paper, we propose a general framework
for evaluating distributional semantic models that
build sentence representations, and suggest two
evaluation methods that test the notion of struc-
turally invariant inference directly. Both rely on
determining whether sentences express the same
semantic relation between entities, a crucial step
in solving a wide variety of inference tasks like
recognizing textual entailment, information re-
trieval, question answering, and summarization.
The first evaluation is a relation classification
task, where a semantic model is tested on its abil-
ity to recognize whether a pair of sentences both
contain a particular semantic relation, such as
Company X acquires Company Y. The second task
is a question answering task, the goal of which is
to locate the sentence in a document that contains
the answer. Here, the semantic model must match
the question, which expresses a proposition with a
missing argument, to the answer-bearing sentence
which contains the full proposition.
We apply these new evaluation protocols to
several recent distributional models, extending
several of them to build sentence representa-
tions. We find that the models outperform a sim-
ple lemma overlap model only slightly, but that
combining these models with the lemma overlap
model can improve performance. This result is
likely due to weaknesses in current models? abil-
ity to deal with issues such as named entities,
coreference, and negation, which are not empha-
sized by existing evaluation methods, but it does
suggest that distributional models of semantics
can play a more central role in systems that re-
quire deep, precise inference.
2 Compositionality and Distributional
Semantics
The idea of compositionality has been central to
understanding contemporary natural language se-
mantics from an historiographic perspective. The
idea is often credited to Frege, although in fact
Frege had very little to say about compositional-
ity that had not already been repeated since the
time of Aristotle (Hodges, 2005). Our modern
notion of compositionality took shape primarily
with the work of Tarski (1956), who was actu-
ally arguing that a central difference between for-
mal languages and natural languages is that nat-
ural language is not compositional. This in turn
was the ?the contention that an important theo-
retical difference exists between formal and nat-
ural languages,? that Richard Montague so fa-
mously rejected (Montague, 1974). Composi-
tionality also features prominently in Fodor and
Pylyshyn?s (1988) rejection of early connection-
ist representations of natural language semantics,
which seems to have influenced Mitchell and La-
pata (2008) as well.
Logic-based forms of compositional semantics
have long strived for syntactic invariance in mean-
ing representations, which is known as the doc-
trine of the canonical form. The traditional justifi-
cation for canonical forms is that they allow easy
access to a knowledge base to retrieve some de-
sired information, which amounts to a form of in-
ference. Our work can be seen as an extension of
this notion to distributional semantic models with
a more general notion of representational similar-
ity and inference.
There are many regular alternations that seman-
tics models have tried to account for such as pas-
sive or dative alternations. There are also many
lexical paraphrases which can take drastically dif-
ferent syntactic forms. Take the following exam-
ple from Poon and Domingos (2009), in which the
same semantic relation can be expressed by a tran-
sitive verb or an attributive prepositional phrase:
(1) Utah borders Idaho.
Utah is next to Idaho.
In distributional semantics, the original sen-
tence similarity test proposed by Kintsch (2001)
served as the inspiration for the evaluation per-
formed by Mitchell and Lapata (2008) and most
later work in the area. Intransitive verbs are given
34
in the context of their syntactic subject, and can-
didate synonyms are ranked for their appropri-
ateness. This method targets the fact that a syn-
onym is appropriate for only some of the verb?s
senses, and the intended verb sense depends on
the surrounding context. For example, burn and
beam are both synonyms of glow, but given a par-
ticular subject, one of the synonyms (called the
High similarity landmark) may be a more appro-
priate substitution than the other (the Low similar-
ity landmark). So, if the fire is the subject, glowed
is the High similarity landmark, and beamed the
Low similarity landmark.
Fundamentally, this method was designed as
a demonstration that compositionality in com-
puting phrasal semantic representations does not
interfere with the ability of a representation to
synthesize non-compositional collocation effects
that contribute to the disambiguation of homo-
graphs. Here, word-sense disambiguation is im-
plicitly viewed as a very restricted, highly lexi-
calized case of inference for selecting the appro-
priate disjunct in the representation of a word?s
meaning.
Kintsch (2001) was interested in sentence sim-
ilarity, but he only conducted his evaluation on
a few hand-selected examples. Mitchell and La-
pata (2008) conducted theirs on a much larger
scale, but chose to focus only on this single case
of syntactic combination, intransitive verbs and
their subjects, in order to ?factor out inessential
degrees of freedom? to compare their various al-
ternative models more equitably. This was not
necessary?using the same, sufficiently large, un-
biased but syntactically heterogeneous sample of
evaluation sentences would have served as an ade-
quate control?and this decision furthermore pre-
vents the evaluation from testing the desired in-
variance of the semantic representation.
Other lexical evaluations suffer from the same
problem. One uses the WordSim-353 dataset
(Finkelstein et al 2002), which contains hu-
man word pair similarity judgments that seman-
tic models should reproduce. However, the word
pairs are given without context, and homography
is unaddressed. Also, it is unclear how reliable
the similarity scores are, as different annotators
may interpret the integer scale of similarity scores
differently. Recent work uses this dataset mostly
for parameter tuning. Another is the lexical para-
phrase task of McCarthy and Navigli (2009), in
which words are given in the context of the sur-
rounding sentence, and the task is to rank a given
list of proposed substitutions for that word. The
list of substitutions as well as the correct rankings
are elicited from annotators. This task was origi-
nally conceived as an applied evaluation of WSD
systems, not an evaluation of phrase representa-
tions.
Parsing accuracy has been used as a prelimi-
nary evaluation of semantic models that produce
syntactic structure (Socher et al 2010; Wu and
Schuler, 2011). However, syntax does not always
reflect semantic content, and we are specifically
interested in supporting syntactic invariance when
doing semantic inference. Also, this type of eval-
uation is tied to a particular grammar formalism.
The existing evaluations that are most similar in
spirit to what we propose are paraphrase detection
tasks that do not assume a restricted syntactic con-
text. Washtell (2011) collected human judgments
on the general meaning similarity of candidate
phrase pairs. Unfortunately, no additional guid-
ance on the definition of ?most similar in mean-
ing? was provided, and it appears likely that sub-
jects conflated lexical, syntactic, and semantic re-
latedness. Dolan and Brockett (2005) define para-
phrase detection as identifying sentences that are
in a bidirectional entailment relation. While such
sentences do support exactly the same inferences,
we are also interested in the inferences that can
be made from similar sentences that are not para-
phrases according to this strict definition ? a sit-
uation that is more often encountered in end ap-
plications. Thus, we adopt a less restricted notion
of paraphrasis.
3 An Evaluation Framework
We now describe a simple, general framework
for evaluating semantic models. Our framework
consists of the following components: a seman-
tic model to be evaluated, pairs of sentences that
are considered to have high similarity, and pairs
of sentences that are considered to have low simi-
larity.
In particular, the semantic model is a binary
function, s = M(x, x?), which returns a real-
valued similarity score, s, given a pair of arbitrary
linguistic units (that is, words, phrases, sentences,
etc.), x and x?. Note that this formulation of the
semantic model is agnostic to whether the models
use compositionality to build a phrase represen-
35
tation from constituent representations, and even
to the actual representation used. The model is
tested by applying it to each element in the fol-
lowing two sets:
H = {(h, h?)|h and h? are linguistic units (2)
with high similarity}
L = {(l, l?)|l and l? are linguistic units (3)
with low similarity}
The resulting sets of similarity scores are:
SH =
{
M(h, h?)|(h, h?) ? H
} (4)
SL =
{
M(l, l?)|(l, l?) ? L
} (5)
The semantic model is evaluated according to
its ability to separate SH and SL. We will de-
fine specific measures of separation for the tasks
that we propose shortly. While the particular def-
initions of ?high similarity? and ?low similarity?
depend on the task, at the crux of both our evalu-
ations is that two sentences are similar if they ex-
press the same semantic relation between a given
entity pair, and dissimilar otherwise. This thresh-
old for similarity is closely tied to the argument
structure of the sentence, and allows considerable
flexibility in the other semantic content that may
be contained in the sentence, unlike the bidirec-
tional paraphrase detection task. Yet it ensures
that a consistent and useful distinction for infer-
ence is being detected, unlike unconstrained sim-
ilarity judgments.
Also, compared to word similarity assessments
or paraphrase elicitation, determining whether a
sentence expresses a semantic relation is a much
easier task cognitively for human judges. This bi-
nary judgment does not involve interpreting a nu-
merical scale or coming up with an open-ended
set of alternative paraphrases. It is thus easier to
get reliable annotated data.
Below, we present two tasks that instantiate
this evaluation framework and choice of similar-
ity threshold. They differ in that the first is tar-
geted towards recognizing declarative sentences
or phrases, while the second is targeted towards a
question answering scenario, where one argument
in the semantic relation is queried.
3.1 Task 1: Relation Classification
The first task is a relation classification task. Rela-
tion extraction and recognition are central to a va-
riety of other tasks, such as information retrieval,
ontology construction, recognizing textual entail-
ment and question answering.
In this task, the high and the low similarity sen-
tence pairs are constructed in the following man-
ner. First, a target semantic relation, such as Com-
pany X acquires Company Y is chosen, and enti-
ties are chosen for each slot in the relation, such as
Company X=Pfizer and Company Y=Rinat Neu-
roscience. Then, sentences containing these enti-
ties are extracted and divided into two subsets. In
one of them, E, the entities are in the target se-
mantic relation, while in the other, NE, they are
not. The evaluation sets H and L are then con-
structed as follows:
H = E ? E \ {(e, e)|e ? E} (6)
L = E ?NE (7)
In other words, the high similarity sentence
pairs are all the pairs where both express the tar-
get semantic relation, except the pairs between a
sentence and itself, while the low similarity pairs
are all the pairs where exactly one of the two sen-
tences expresses the target relation.
Several sentences expressing the relation Pfizer
acquires Rinat Neuroscience are shown in Exam-
ples 8 to 10. These sentences illustrate the amount
of syntactic and lexical variation that the semantic
model must recognize as expressing the same se-
mantic relation. In particular, besides recognizing
synonymy or near-synonymy at the lexical level,
models must also account for subcategorization
differences, extra arguments or adjuncts, and part-
of-speech differences due to nominalization.
(8) Pfizer buys Rinat Neuroscience to extend
neuroscience research and in doing so
acquires a product candidate for OA.
(lexical difference)
(9) A month earlier, Pfizer paid an estimated
several hundred million dollars for biotech
firm Rinat Neuroscience. (extra argument,
subcategorization)
(10) Pfizer to Expand Neuroscience Research
With Acquisition of Biotech Company Rinat
Neuroscience (nominalization)
Since our interest is to measure the models?
ability to separate SH and SL in an unsuper-
vised setting, standard supervised classification
accuracy is not applicable. Instead, we employ
36
the area under a ROC curve (AUC), which does
not depend on choosing an arbitrary classification
threshold. A ROC curve is a plot of the true pos-
itive versus false positive rate of a binary classi-
fier as the classification threshold is varied. The
area under a ROC curve can thus be seen as the
performance of linear classifiers over the scores
produced by the semantic model. The AUC can
also be interpreted as the probability that a ran-
domly chosen positive instance will have a higher
similarity score than a randomly chosen negative
instance. A random classifier is expected to have
an AUC of 0.5.
3.2 Task 2: Restricted QA
The second task that we propose is a restricted
form of question answering. In this task, the sys-
tem is given a question q and a document D con-
sisting of a list of sentences, in which one of the
sentences contains the answer to the question. We
define:
H = {(q, d)|d ? D and d answers q} (11)
L = {(q, d)|d ? D and d does not answer q}
(12)
In other words, the sentences are divided into two
subsets; those that contain the answer to q should
be similar to q, while those that do not should be
dissimilar. We also assume that only one sentence
in each document contains the answer, so H con-
tains only one sentence.
Unrestricted question answering is a difficult
problem that forces a semantic representation to
deal sensibly with a number of other semantic is-
sues such as coreference and information aggre-
gation which still seem to be out of reach for
contemporary distributional models of meaning.
Since our focus in this work is on argument struc-
ture semantics, we restrict the question-answer
pairs to those that only require dealing with para-
phrases of this type.
To do so, we semi-automatically restrict the
question-answer pairs by using the output of an
unsupervised clustering semantic parser (Poon
and Domingos, 2009). The semantic parser clus-
ters semantic sub-expressions derived from a de-
pendency parse of the sentence, so that those sub-
expressions that express the same semantic re-
lations are clustered. The parser is used to an-
swer questions, and the output of the parser is
manually checked. We use only those cases that
have thus been determined to be correct question-
answer pairs. As a result of this restriction, this
task is rather more like Task 1 in how it tests a
model?s ability to recognize lexical and syntac-
tic paraphrases. This task also involves recog-
nizing voicing alternations, which were automati-
cally extracted by the semantic parser.
An example of a question-answer pair involv-
ing a voicing alternation that is used in this task is
presented in Example 13.
(13) Q: What does il-2 activate?
A: PI3K
Sentence: Phosphatidyl inositol 3-kinase
(PI3K) is activated by IL-2.
Since there is only one element in H and hence
SH for each question and document, we measure
the separation between SH and SL using the rank
of the score of answer-bearing sentence among
the scores of all the sentences in the document.
We normalize the rank so that it is between 0
(ranked least similar) and 1 (ranked most simi-
lar). Where ties occur, the sentence is ranked as
if it were in the median position among the tied
sentences. If the question-answer pairs are zero-
indexed by i, answer(i) is the index of the sen-
tence containing the answer for the ith pair, and
length(i) is the number of sentences in the doc-
ument, then the mean normalized rank score of a
system is:
norm rank = E
i
[
1? answer(i)length(i) ? 1
]
(14)
4 Experiments
We drew a number of recent distributional seman-
tic models to compare in this paper. We first de-
scribe the models and our reimplementation of
them, before describing the tasks and the datasets
used in detail and the results.
4.1 Distributional Semantic Models
We tested four recent distributional models and a
lemma overlap baseline, which we now describe.
We extended several of the models to compo-
sitionally construct phrase representations using
component-wise vector addition and multiplica-
tion, as we note below. Since the focus of this pa-
per is on evaluation methods for such models, we
did not experiment with other compositionality
37
operators. We do note, however, that component-
wise operators have been popular in recent liter-
ature, and have been applied across unrestricted
syntactic contexts (Mitchell and Lapata, 2009),
so there is value in evaluating the performance of
these operators in itself. The models were trained
on the Gigaword corpus (2nd ed., ~2.3B words).
All models use cosine similarity to measure the
similarity between representations, except for the
baseline model.
Lemma Overlap This baseline simply repre-
sents a sentence as the counts of each lemma
present in the sentence after removing stop
words. Let a sentence x consist of lemma-tokens
m1, . . . ,m|x|. The similarity between two sen-
tences is then defined as
M(x, x?) = #In(x, x?) + #In(x?, x) (15)
#In(x, x?) =
|x|
?
i=1
1x?(mi) (16)
where 1x?(mi) is an indicator function that returns
1 if mi ? x?, and 0 otherwise. This definition
accounts for multiple occurrences of a lemma.
M&L Mitchell and Lapata (2008) propose a
framework for compositional distributional se-
mantics using a standard term-context vector
space word representation. A phrase is repre-
sented as a vector of context-word counts (actu-
ally, pmi-scaled values), which is derived compo-
sitionally by a function over constituent vectors,
such as component-wise addition or multiplica-
tion. This model ignores syntactic relations and
is insensitive to word-order.
E&P Erk and Pado? (2008) introduce a struc-
tured vector space model which uses syntactic de-
pendencies to model the selectional preferences
of words. The vector representation of a word in
context depends on the inverse selectional prefer-
ences of its dependents, and the selectional pref-
erences of its head. For example, suppose catch
occurs with a dependent ball in a direct object
relation. The vector for catch would then be in-
fluenced by the inverse direct object preferences
of ball (e.g. throw, organize), and the vector for
ball would be influenced by the selectional pref-
erences of catch (e.g. cold, drift). More formally,
given words a and b in a dependency relation r,
a distributional representation of a, va, the repre-
sentation of a in context, a?, is given by
a? = va ?Rb(r?1) (17)
Rb(r) =
?
c:f(c,r,b)>?
f(c, r, b) ? vc, (18)
where Rb(r) is the vector describing the selec-
tional preference of word b in relation r, f(c, r, b)
is the frequency of this dependency triple, ? is a
frequency threshold to weed out uncommon de-
pendency triples (10 in our experiments), and ?
is a vector combination operator, here component-
wise multiplication. We extend the model to com-
pute sentence representations from the contextu-
alized word vectors using component-wise addi-
tion and multiplication.
TFP Thater et al(2010)?s model is also sensi-
tive to selectional preferences, but to two degrees.
For example, the vector for catch might contain
a dimension labelled (OBJ,OBJ-1,throw),
which indicates the strength of connection be-
tween the two verbs through all of the co-
occurring direct objects which they share. Unlike
E&P, TFP?s model encodes the selectional prefer-
ences in a single vector using frequency counts.
We extend the model to the sentence level with
component-wise addition and multiplication, and
word vectors are contextualized by the depen-
dency neighbours. We use a frequency threshold
of 10 and a pmi threshold of 2 to prune infrequent
word and dependencies.
D&L Dinu and Lapata (2010) (D&L) assume
a global set of latent senses for all words, and
models each word as a mixture over these latent
senses. The vector for a word ti in the context of
a word cj is modelled by
v(ti, cj) = P (z1|ti, cj), ...P (zK |ti, cj) (19)
where z1...K are the latent senses. By mak-
ing independence assumptions and decomposing
probabilities, training becomes a matter of esti-
mating the probability distributions P (zk|ti) and
P (cj |zk) from data. While Dinu and Lapata
(2010) describe two methods to do so, based
on non-negative matrix factorization and latent
Dirichlet alcation, the performances are similar,
so we tested only the latent Dirichlet alcation
method. Like the two previous models, we ex-
tend the model to build sentence representations
38
Pfizer/Rinat N. Yahoo/Inktomi Besson/Paris Antoinette/Vienna Average
Overlap 0.7393 0.6007 0.7395 0.8914 0.7427
Models trained on the entire GigaWord
M&L add 0.6196 0.5387 0.5259 0.7275 0.6029
M&L mult 0.9036 0.6099 0.6443 0.8467 0.7511
D&L add 0.9214 0.8168 0.6989 0.8932 0.8326
D&L mult 0.7732 0.6734 0.6527 0.7659 0.7163
Models trained on the AFP section
E&P add 0.7536 0.4933 0.2780 0.6408 0.5414
E&P mult 0.5268 0.5328 0.5252 0.8421 0.6067
TFP add 0.4357 0.5325 0.8725 0.7183 0.6398
TFP mult 0.5554 0.5524 0.7283 0.6917 0.6320
M&L add 0.5643 0.5504 0.4594 0.7640 0.5845
M&L mult 0.8679 0.6324 0.4356 0.8258 0.6904
D&L add 0.8143 0.9062 0.6373 0.8664 0.8061
D&L mult 0.8429 0.7461 0.645 0.5948 0.7072
Table 1: Task 1 results in AUC scores. The values in bold indicate the best performing model for a particular
training corpus. The expected random baseline performance is 0.5.
Entities: {X, Y} + N
Relation: acquires
{Pfizer, Rinat Neuroscience} 41 50
{Yahoo, Inktomi} 115 433
Relation: was born in
{Luc Besson, Paris} 6 126
{Marie Antoinette, Vienna} 39 105
Table 2: Task 1 dataset characteristics. N is the total
number of sentences. + is the number of sentences
that express the relation.
from the contextualized representations. We set
the number of latent senses to 1200, and train for
600 Gibbs sampling iterations.
4.2 Training and Parameter Settings
We reimplemented these four models, following
the parameter settings described by previous work
where possible, though we also aimed for consis-
tency in parameter settings between models (for
example, in the number of context words). For the
non-baseline models, we followed previous work
and model only the 30000 most frequent lemmata.
Context vectors are constructed using a symmet-
ric window of 5 words, and their dimensions rep-
resent the 3000 most frequent lemmatized context
words excluding stop words. Due to resource lim-
itations, we trained the syntactic models over the
AFP subset of Gigaword (~338M words). We also
trained the other two models on just the AFP por-
tion for comparison. Note that the AFP portion
of Gigaword is three times larger than the BNC
corpus (~100M words), on which several previ-
ous syntactic models were trained. Because our
main goal is to test the general performance of the
models and to demonstrate the feasibility of our
evaluation methods, we did not further tune the
parameter settings to each of the tasks, as doing
so would likely only yield minor improvements.
4.3 Task 1
We used the dataset by Bunescu and Mooney
(2007), which we selected because it contains
multiple realizations of an entity pair in a target
semantic relation, unlike similar datasets such as
the one by Roth and Yih (2002). Controlling for
the target entity pair in this manner makes the task
more difficult, because the semantic model cannot
make use of distributional information about the
entity pair in inference. The dataset is separated
into subsets depending on the target binary rela-
tion (Company X acquires Company Y or Person
X was born in Place Y) and the entity pair (e.g.,
Yahoo and Inktomi) (Table 2).
The dataset was constructed semi-
automatically using a Google search for the
two entities in order with up to seven content
words in between. Then, the extracted sentences
were hand-labelled with whether they express the
target relation. Because the order of the entities
has been fixed, passive alternations do not appear
39
Pure models Mixed models
All Subset All Subset
Overlap 0.8770 0.7291 0.8770 0.7291
Models trained on the entire GigaWord
M&L add 0.7467 0.6106 0.8782 0.7523
M&L mult 0.5331 0.5690 0.8841 0.7678
D&L add 0.6552 0.5716 0.8791 0.7539
D&L mult 0.5488 0.5255 0.8841 0.7466
Models trained on the AFP section
E&P add 0.4589 0.4516 0.8748 0.7375
E&P mult 0.5201 0.5584 0.8882 0.7719
TFP add 0.6887 0.6443 0.8940 0.7871
TFP mult 0.5210 0.5199 0.8785 0.7432
M&L add 0.7588 0.6206 0.8710 0.7371
M&L mult 0.5710 0.5540 0.8801 0.7540
D&L add 0.6358 0.5402 0.8713 0.7305
D&L mult 0.5647 0.5461 0.8856 0.7683
Table 3: Task 2 results, in normalized rank scores.
Subset is the cases where lemma overlap does not
achieve a perfect score. The two columns on the right
indicate performance using the sum of the scores from
the lemma overlap and the semantic model. The ex-
pected random baseline performance is 0.5.
in this dataset.
The results for Task 1 indicate that the D&L ad-
dition model performs the best (Table 1), though
the lemma overlap model presents a surprisingly
strong baseline. The syntax-modulated E&P and
TFP models perform poorly on this task, even
when compared to the other models trained on the
AFP subset. The M&L multiplication model out-
performs the addition model, a result which cor-
roborates previous findings on the lexical substi-
tution task. The same does not hold in the D&L
latent sense space. Overall, some of the datasets
(Yahoo and Antoinette) appear to be easier for the
models than others (Pfizer and Besson), but more
entity pairs and relations would be needed to in-
vestigate the models? variance across datasets.
4.4 Task 2
We used the question-answer pairs extracted by
the Poon and Domingos (2009) semantic parser
from the GENIA biomedical corpus that have
been manually checked to be correct (295 pairs).
Because our models were trained on newspaper
text, they required adaptation to this specialized
domain. Thus, we also trained the M&L, E&P
and TFP models on the GENIA corpus, back-
ing off to word vectors from the GENIA corpus
when a word vector could not be found in the
Gigaword-trained model. We could not do this
for the D&L model, since the global latent senses
that are found by latent Dirichlet alcation train-
ing do not have any absolute meaning that holds
across multiple runs. Instead, we found the 5
words in the Gigaword-trained D&L model that
were closest to each novel word in the GENIA
corpus according to cosine similarity over the co-
occurrence vectors of the words in the GENIA
corpus, and took their average latent sense distri-
butions as the vector for that word.
Unlike in Task 1, there is no control for the
named entities in a sentence, because one of the
entities in the semantic relation is missing. Also,
distributional models have problems in dealing
with named entities which are common in this
corpus, such as the names of genes and proteins.
To address these issues, we tested hybrid models
where the similarity score from a semantic model
is added to the similarity score from the lemma
overlap model.
The results are presented in Table 3. Lemma
overlap again presents a strong baseline, but the
hybridized models are able to outperform simple
lemma overlap. Unlike in Task 1, the E&P and
TFP models are comparable to the D&L model,
and the mixed TFP addition model achieves the
best result, likely due to the need to more pre-
cisely distinguish syntactic roles in this task. The
D&L addition model, which achieved the best
performance in Task 1, does not perform as well
in this task. This could be due to the domain adap-
tation procedure for the D&L model, which could
not be reasonably trained on such a small, special-
ized corpus.
5 Related Work
Turney and Pantel (2010) survey various types of
vector space models and applications thereof in
computational linguistics. We summarize below
a number of other word- or phrase-level distribu-
tional models.
Several approaches are specialized to deal with
homography. The top-down multi-prototype ap-
proach determines a number of senses for each
word, and then clusters the occurrences of the
word (Reisinger and Mooney, 2010) into these
senses. A prototype vector is created for each
of these sense clusters. When a new occurrence
40
of a word is encountered, it is represented as a
combination of the prototype vectors, with the de-
gree of influence from each prototype determined
by the similarity of the new context to the exist-
ing sense contexts. In contrast, the bottom-up ex-
emplar-based approach assumes that each occur-
rence of a word expresses a different sense of the
word. The most similar senses of the word are ac-
tivated when a new occurrence of it is encountered
and combined, for example with a kNN algorithm
(Erk and Pado?, 2010).
The models we compared and the above work
assume each dimension in the feature vector cor-
responds to a context word. In contrast, Washtell
(2011) uses potential paraphrases directly as di-
mensions in his expectation vectors. Unfortu-
nately, this approach does not outperform vari-
ous context word-based approaches in two phrase
similarity tasks.
In terms of the vector composition function,
component-wise addition and multiplication are
the most popular in recent work, but there ex-
ist a number of other operators such as tensor
product and convolution product, which are re-
viewed by Widdows (2008). Instead of vector
space representations, one could also use a matrix
space representation with its much more expres-
sive matrix operators (Rudolph and Giesbrecht,
2010). So far, however, this has only been ap-
plied to specific syntactic contexts (Baroni and
Zamparelli, 2010; Guevara, 2010; Grefenstette
and Sadrzadeh, 2011), or tasks (Yessenalina and
Cardie, 2011).
Neural networks have been used to learn both
phrase structure and representations. In Socher et
al. (2010), word representations learned by neu-
ral network models such as (Bengio et al 2006;
Collobert and Weston, 2008) are fed as input into
a recursive neural network whose nodes represent
syntactic constituents. Each node models both the
probability of the input forming a constituent and
the phrase representation resulting from composi-
tion.
6 Conclusions
We have proposed an evaluation framework for
distributional models of semantics which build
phrase- and sentence-level representations, and
instantiated two evaluation tasks which test for
the crucial ability to recognize whether sen-
tences express the same semantic relation. Our
results demonstrate that compositional distribu-
tional models of semantics already have some
utility in the context of more empirically complex
semantic tasks than WSD-like lexical substitution
tasks, in which compositional invariance is a req-
uisite property. Simply computing lemma over-
lap, however, is a very competitive baseline, due
to issues in these protocols with named entities
and domain adaptivity. The better performance
of the mixture models in Task 2 shows that such
weaknesses can be addressed by hybrid seman-
tic models. Future work should investigate more
refined versions of such hybridization, as well as
extend this idea to other semantic phenomena like
coreference, negation and modality.
We also observe that no single model or com-
position operator performs best for all tasks and
datasets. The latent sense mixture model of Dinu
and Lapata (2010) performs well in recognizing
semantic relations in general web text. Because
of the difficulty of adapting it to a specialized
domain, however, it does less well in biomedi-
cal question answering, where the syntax-based
model of Thater et al(2010) performs the best.
A more thorough investigation of the factors that
can predict the performance and/or invariance of
a given composition operator is warranted.
In the future, we would like to evaluate other
models of compositional semantics that have been
recently proposed. We would also like to collect
more comprehensive test data, to increase the ex-
ternal validity of our evaluations.
Acknowledgments
We would like to thank Georgiana Dinu and Ste-
fan Thater for help with reimplementing their
models. Saif Mohammad, Peter Turney, and
the anonymous reviewers provided valuable com-
ments on drafts of this paper. This project was
supported by the Natural Sciences and Engineer-
ing Research Council of Canada.
References
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages
1183?1193.
Yoshua Bengio, Holger Schwenk, Jean-Se?bastien
Sene?cal, Fre?deric Morin, and Jean-Luc Gauvain.
41
2006. Neural probabilistic language models. In-
novations in Machine Learning, pages 137?186.
Razvan C. Bunescu and Raymond J. Mooney. 2007.
Learning to extract relations from the web using
minimal supervision. In Proceedings of the 45th
Annual Meeting of the Association for Computa-
tional Linguistics, pages 576?583.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of the 25th International Conference on
Machine Learning, page 160?167.
Georgiana Dinu and Mirella Lapata. 2010. Measuring
distributional similarity in context. In Proceedings
of the 2010 Conference on Empirical Methods in
Natural Language Processing, pages 1162?1172.
William B. Dolan and Chris Brockett. 2005. Auto-
matically constructing a corpus of sentential para-
phrases. In Proceedings of the Third International
Workshop on Paraphrasing, pages 9?16.
Katrin Erk and Sebastian Pado?. 2008. A structured
vector space model for word meaning in context. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, pages 897?
906.
Katrin Erk and Sebastian Pado?. 2010. Exemplar-
based models for word meaning in context. In Pro-
ceedings of the ACL 2010 Conference Short Papers,
pages 92?97.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. 2002. Placing search in context: The
concept revisited. ACM Transactions on Informa-
tion Systems, 20(1):116?131.
Jerry A. Fodor and Zenon W. Pylyshyn. 1988. Con-
nectionism and cognitive architecture: A critical
analysis. Cognition, 28:3?71.
Edward Grefenstette and Mehrnoosh Sadrzadeh.
2011. Experimental support for a categorical com-
positional distributional model of meaning. In
Proceedings of the 2011 Conference on Empirical
Methods in Natural Language Processing, pages
1394?1404.
Emiliano Guevara. 2010. A regression model
of adjective-noun compositionality in distributional
semantics. In Proceedings of the 2010 Workshop on
GEometrical Models of Natural Language Seman-
tics, pages 33?37.
Zeller S. Harris. 1954. Distributional structure. Word,
10(23):146?162.
Wilfred Hodges. 2005. The interplay of fact and the-
ory in separating syntax from meaning. In Work-
shop on Empirical Challenges and Analytical Al-
ternatives to Strict Compositionality.
Walter Kintsch. 2001. Predication. Cognitive Sci-
ence, 25(2):173?202.
Diana McCarthy and Roberto Navigli. 2009. The en-
glish lexical substitution task. Language Resources
and Evaluation, 43(2):139?159.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL-08: HLT, pages 236?244.
Jeff Mitchell and Mirella Lapata. 2009. Language
models based on semantic composition. In Pro-
ceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing, pages
430?439.
Richard Montague. 1974. English as a formal lan-
guage. Formal Philosophy, pages 188?221.
Hoifung Poon and Pedro Domingos. 2009. Unsuper-
vised semantic parsing. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1?10.
Joseph Reisinger and Raymond J. Mooney. 2010.
Multi-prototype vector-space models of word
meaning. In Human Language Technologies: The
2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics.
Dan Roth and Wen-tau Yih. 2002. Probabilistic rea-
soning for entity & relation recognition. In Pro-
ceedings of the 19th International Conference on
Computational Linguistics, pages 835?841.
Sebastian Rudolph and Eugenie Giesbrecht. 2010.
Compositional matrix-space models of language.
In Proceedings of the 48th Annual Meeting of the
Association for Computational Linguistics, pages
907?916.
Richard Socher, Christopher D. Manning, and An-
drew Y. Ng. 2010. Learning continuous phrase
representations and syntactic parsing with recursive
neural networks. Proceedings of the Deep Learn-
ing and Unsupervised Feature Learning Workshop
of NIPS 2010, pages 1?9.
Alfred Tarski. 1956. The concept of truth in formal-
ized languages. Logic, Semantics, Metamathemat-
ics, pages 152?278.
Stefan Thater, Hagen Fu?rstenau, and Manfred Pinkal.
2010. Contextualizing semantic representations us-
ing syntactically enriched vector models. In Pro-
ceedings of the 48th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 948?957.
Peter D. Turney and Patrick Pantel. 2010. From
frequency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Research,
37:141?188.
Justin Washtell. 2011. Compositional expectation:
A purely distributional model of compositional se-
mantics. In Proceedings of the Ninth International
Conference on Computational Semantics (IWCS
2011), pages 285?294.
Dominic Widdows. 2008. Semantic vector products:
Some initial investigations. In Second AAAI Sym-
posium on Quantum Interaction.
42
Stephen Wu and William Schuler. 2011. Structured
composition of semantic vectors. In Proceedings
of the Ninth International Conference on Computa-
tional Semantics (IWCS 2011), pages 295?304.
Ainur Yessenalina and Claire Cardie. 2011. Com-
positional matrix-space models for sentiment analy-
sis. In Proceedings of the 2011 Conference on Em-
pirical Methods in Natural Language Processing,
pages 172?182.
43
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 696?705,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Unsupervised Detection of Downward-Entailing Operators By
Maximizing Classification Certainty
Jackie CK Cheung and Gerald Penn
Department of Computer Science
University of Toronto
Toronto, ON, M5S 3G4, Canada
{jcheung,gpenn}@cs.toronto.edu
Abstract
We propose an unsupervised, iterative
method for detecting downward-entailing
operators (DEOs), which are important for
deducing entailment relations between sen-
tences. Like the distillation algorithm of
Danescu-Niculescu-Mizil et al(2009), the
initialization of our method depends on the
correlation between DEOs and negative po-
larity items (NPIs). However, our method
trusts the initialization more and aggres-
sively separates likely DEOs from spuri-
ous distractors and other words, unlike dis-
tillation, which we show to be equivalent
to one iteration of EM prior re-estimation.
Our method is also amenable to a bootstrap-
ping method that co-learns DEOs and NPIs,
and achieves the best results in identifying
DEOs in two corpora.
1 Introduction
Reasoning about text has been a long-standing
challenge in NLP, and there has been consider-
able debate both on what constitutes inference and
what techniques should be used to support infer-
ence. One task involving inference that has re-
cently received much attention is that of recog-
nizing textual entailment (RTE), in which the goal
is to determine whether a hypothesis sentence can
be entailed from a piece of source text (Bentivogli
et al 2010, for example).
An important consideration in RTE is whether
a sentence or context produces an entailment re-
lation for events that are a superset or subset of
the original sentence (MacCartney and Manning,
2008). By default, contexts are upward-entailing,
allowing reasoning from a set of events to a su-
perset of events as seen in (1). In the scope of
a downward-entailing operator (DEO), however,
this entailment relation is reversed, such as in
the scope of the classical DEO not (2). There
are also operators which are neither upward- nor
downward entailing, such as the expression ex-
actly three (3).
(1) She sang in French. ? She sang.
(upward-entailing)
(2) She did not sing in French. ? She did not
sing. (downward-entailing)
(3) Exactly three students sang. 6? Exactly
three students sang in French. (neither
upward- nor downward-entailing)
Danescu-Niculescu-Mizil et al(2009) (hence-
forth DLD09) proposed the first computational
methods for detecting DEOs from a corpus. They
proposed two unsupervised algorithms which rely
on the correlation between DEOs and negative
polarity items (NPIs), which by the definition of
Ladusaw (1980) must appear in the context of
DEOs. An example of an NPI is yet, as in the
sentence This project is not complete yet. The
first baseline method proposed by DLD09 sim-
ply calculates a ratio of the relative frequencies
of a word in NPI contexts versus in a general
corpus, and the second is a distillation method
which appears to refine the baseline ratios using a
task-specific heuristic. Danescu-Niculescu-Mizil
and Lee (2010) (henceforth DL10) extend this ap-
proach to Romanian, where a comprehensive list
of NPIs is not available, by proposing a bootstrap-
ping approach to co-learn DEOs and NPIs.
DLD09 are to be commended for having iden-
tified a crucial component of inference that nev-
ertheless lends itself to a classification-based ap-
696
proach, as we will show. However, as noted
by DL10, the performance of the distillation
method is mixed across languages and in the
semi-supervised bootstrapping setting, and there
is no mathematical grounding of the heuristic to
explain why it works and whether the approach
can be refined or extended. This paper supplies
the missing mathematical basis for distillation and
shows that, while its intentions are fundamentally
sound, the formulation of distillation neglects an
important requirement that the method not be
easily distracted by other word co-occurrences
in NPI contexts. We call our alternative cer-
tainty, which uses an unusual posterior classifica-
tion confidence score (based on the max function)
to favour single, definite assignments of DEO-
hood within every NPI context. DLD09 actually
speculated on the use of max as an alternative,
but within the context of an EM-like optimization
procedure that throws away its initial parameter
settings too willingly. Certainty iteratively and
directly boosts the scores of the currently best-
ranked DEO candidates relative to the alternatives
in a Na??ve Bayes model, which thus pays more re-
spect to the initial weights, constructively build-
ing on top of what the model already knows. This
method proves to perform better on two corpora
than distillation, and is more amenable to the co-
learning of NPIs and DEOs. In fact, the best
results are obtained by co-learning the NPIs and
DEOs in conjunction with our method.
2 Related work
There is a large body of literature in linguis-
tic theory on downward entailment and polar-
ity items1, of which we will only mention the
most relevant work here. The connection between
downward-entailing contexts and negative polar-
ity items was noticed by Ladusaw (1980), who
stated the hypothesis that NPIs must be gram-
matically licensed by a DEO. However, DEOs
are not the sole licensors of NPIs, as NPIs can
also be found in the scope of questions, certain
numeric expressions (i.e., non-monotone quanti-
fiers), comparatives, and conditionals, among oth-
ers. Giannakidou (2002) proposes that the prop-
erty shared by these constructions and downward
entailment is non-veridicality. If F is a propo-
1See van der Wouden (1997) for a comprehensive refer-
ence.
sitional operator for proposition p, then an oper-
ator is non-veridical if Fp 6? p. Positive opera-
tors such as past tense adverbials are veridical (4),
whereas questions, negation and other DEOs are
non-veridical (5, 6).
(4) She sang yesterday. ? She sang.
(5) She denied singing. 6? She sang.
(6) Did she sing? 6? She sang.
While Ladusaw?s hypothesis is thus accepted
to be insufficient from a linguistic perspective, it
is nevertheless a useful starting point for compu-
tational methods for detecting NPIs and DEOs,
and has inspired successful techniques to detect
DEOs, like the work by DLD09, DL10, and also
this work. In addition to this hypothesis, we fur-
ther assume that there should only be one plausi-
ble DEO candidate per NPI context. While there
are counterexamples, this assumption is in prac-
tice very robust, and is a useful constraint for our
learning algorithm. An analogy can be drawn to
the one sense per discourse assumption in word
sense disambiguation (Gale et al 1992).
The related?and as we will argue, more
difficult?problem of detecting NPIs has also
been studied, and in fact predates the work on
DEO detection. Hoeksema (1997) performed the
first corpus-based study of NPIs, predominantly
for Dutch, and there has also been work on de-
tecting NPIs in German which assumes linguistic
knowledge of licensing contexts for NPIs (Lichte
and Soehn, 2007). Richter et al(2010) make
this assumption as well as use syntactic structure
to extract NPIs that are multi-word expressions.
Parse information is an especially important con-
sideration in freer-word-order languages like Ger-
man where a MWE may not appear as a contigu-
ous string. In this paper, we explicitly do not as-
sume detailed linguistic knowledge about licens-
ing contexts for NPIs and do not assume that a
parser is available, since neither of these are guar-
anteed when extending this technique to resource-
poor languages.
3 Distillation as EM Prior Re-estimation
Let us first review the baseline and distillation
methods proposed by DLD09, then show that dis-
tillation is equivalent to one iteration of EM prior
697
re-estimation in a Na??ve Bayes generative proba-
bilistic model up to constant rescaling. The base-
line method assigns a score to each word-type
based on the ratio of its relative frequency within
NPI contexts to its relative frequency within a
general corpus. Suppose we are given a corpus C
with extracted NPI contexts N and they contain
tokens(C) and tokens(N ) tokens respectively.
Let y be a candidate DEO, countC(y) be the uni-
gram frequency of y in a corpus, and countN (y)
be the unigram frequency of y in N . Then, we
define S(y) to be the ratio between the relative
frequencies of y within NPI contexts and in the
entire corpus2:
S(y) = count
N (y)/tokens(N )
countC(y)/tokens(C) . (7)
The scores are then used as a ranking to de-
termine word-types that are likely to be DEOs.
This method approximately captures Ladusaw?s
hypothesis by highly ranking words that appear
in NPI contexts more often than would be ex-
pected by chance. However, the problem with
this approach is that DEOs are not the only words
that co-occur with NPIs. In particular, there exist
many piggybackers, which, as defined by DLD09,
collocate with DEOs due to semantic relatedness
or chance, and would thus incorrectly receive a
high S(y) score.
Examples of piggybackers found by DLD09 in-
clude the proper noun Milken, and the adverb vig-
orously, which collocate with DEOs like deny in
the corpus they used. DLD09?s solution to the
piggybacker problem is a method that they term
distillation. Let Ny be the NPI contexts that con-
tain word y; i.e., Ny = {c ? N|c ? y}. In dis-
tillation, each word-type is given a distilled score
according to the following equation:
Sd(y) =
1
|Ny|
?
p?Ny
S(y)
?
y??p S(y?)
. (8)
where p indexes the set of NPI contexts which
contain y3, and the denominator is the number of
2DLD09 actually use the number of NPI contexts con-
taining y rather than countN (y), but we find that using the
raw count works better in our experiments.
3In DLD09, the corresponding equation does not indicate
that p should be the contexts that include y, but it is clear
from the surrounding text that our version is the intended
meaning. If all the NPI contexts were included in the sum-
mation, Sd(y) would reduce to inverse relative frequency.
Y
L
DEO
Context wordsX
Figure 1: Na??ve Bayes formulation of DEO detection.
NPI contexts which contain y.
DLD09 find that distillation seems to improve
the performance of DEO detection in BLLIP.
Later work by DL10, however, shows that distil-
lation does not seem to improve performance over
the baseline method in Romanian, and the authors
also note that distillation does not improve perfor-
mance in their experiments on co-learning NPIs
and DEOs via bootstrapping.
A better mathematical grounding of the distilla-
tion method?s apparent heuristic in terms of exist-
ing probabilistic models sheds light on the mixed
performance of distillation across languages and
experimental settings. In particular, it turns out
that the distillation method of DLD09 is equiva-
lent to one iteration of EM prior re-estimation in
a Na??ve Bayes model. Given a lexicon L of L
words, let each NPI context be one sample gen-
erated by the model. One sample consists of a
latent categorical (i.e., a multinomial with one
trial) variable Y whose values range over L, cor-
responding to the DEO that licenses the context,
and observed Bernoulli variables ~X = Xi=1...L
which indicate whether a word appears in the NPI
context (Figure 1). This method does not attempt
to model the order of the observed words, nor the
number of times each word appears. Formally, a
Na??ve Bayes model is given by the following ex-
pression:
P ( ~X, Y ) =
L
?
i=1
P (Xi|Y )P (Y ). (9)
The probability of a DEO given a particular
NPI context is
P (Y | ~X) ?
L
?
i=1
P (Xi|Y )P (Y ). (10)
698
The probability of a set of observed NPI con-
texts N is the product of the probabilities for each
sample:
P (N ) =
?
~X?N
P ( ~X) (11)
P ( ~X) =
?
y?L
P ( ~X, y). (12)
We first instantiate the baseline method of
DLD09 by initializing the parameters to the
model, P (Xi = 1|y) and P (Y = y), such that
P (Y = y) is proportional to S(y). Recall that this
initialization utilizes domain knowledge about the
correlation between NPIs and DEOs, inspired by
Ladusaw?s hypothesis:
P (Y = y) = S(y)/
?
y?
S(y?) (13)
P (Xi = 1|y) =
{
1 if Xi corresponds to y
0.5 otherwise.
(14)
This initialization of P (Xi = 1|y) ensures that
the the value of y corresponds to one of the words
in the NPI context, and the initialization of P (Y )
is simply a normalization of S(y).
Since we are working in an unsupervised set-
ting, there are no labels for Y available. A com-
mon and reasonable assumption about learning
the parameter settings in this case is to find the pa-
rameters that maximize the likelihood of the ob-
served training data; i.e., the NPI contexts:
?? = argmax
?
P (N ; ?). (15)
The EM algorithm is a well-known iterative al-
gorithm for performing this optimization. Assum-
ing that the prior P (Y = y) is a categorical distri-
bution, the M-step estimate of these parameters
after one iteration through the corpus is as fol-
lows:
P t+1(Y = y) =
?
~X?N
P t(y| ~X)
?
y? P t(y?| ~X)
(16)
We do not re-estimate P (Xi = 1|y) because
their role is simply to ensure that the DEO re-
sponsible for an NPI context exists in the context.
Estimating these parameters would exacerbate the
problems with EM for this task which we will dis-
cuss shortly.
P (Y ) gives a prior probability that a certain
word-type y is a DEO in an NPI context, without
normalizing for the frequency of y in NPI con-
texts. Since we are interested in estimating the
context-independent probability that y is a DEO,
we must calculate the probability that a word is
a DEO given that it appears in an NPI context.
Let Xy be the observed variable corresponding to
y. Then, the expression we are interested in is
P (y|Xy = 1). We now show that P (y|Xy =
1) = P (y)/P (Xy = 1), and that this expression
is equivalent to (8).
P (y|Xy = 1) =
P (y,Xy = 1)
P (Xy = 1)
(17)
Recall that P (y,Xy = 0) = 0 because of the
assumption that a DEO appears in the NPI context
that it generates. Thus,
P (y,Xy = 1) = P (y,Xy = 1) + P (y,Xy = 0)
= P (y) (18)
One iteration of EM to calculate this proba-
bility is equivalent to the distillation method of
DLD09. In particular, the numerator of (17),
which we just showed to be equal to the estimate
of P (Y ) given by (16), is exactly the sum of the
responsibilities for a particular y, and is propor-
tional to the summation in (8) modulo normaliza-
tion, because P ( ~X |y) is constant for all y in the
context. The denominator P (Xy = 1) is simply
the proportion of contexts containing y, which is
proportional to |Ny|. Since both the numerator
and denominator are equivalent up to a constant
factor, an identical ranking is produced by distil-
lation and EM prior re-estimation.
Unfortunately, the EM algorithm does not pro-
vide good results on this task. In fact, as more
iterations of EM are run, the performance drops
drastically, even though the corpus likelihood
is increasing. The reason is that unsupervised
EM learning is not constrained or biased towards
learning a good set of DEOs. Rather, a higher data
likelihood can be achieved simply by assigning
high prior probabilities to frequent word-types.
This can be seen qualitatively by consider-
ing the top-ranking DEOs after several itera-
tions of EM/distillation (Figure 2). The top-
ranking words are simply function words or other
words common in the corpus, which have noth-
ing to do with downward entailment. In effect,
699
1 iteration 2 iterations 3 iterations
denies the the
denied to to
unaware denied that
longest than than
hardly that and
lacking if has
deny has if
nobody denies of
opposes and denied
highest but denies
Figure 2: Top 10 DEOs after iterations of EM on
BLLIP.
EM/distillation overrides the initialization based
on Ladusaw?s hypothesis and finds another solu-
tion with a higher data likelihood. We will also
provide a quantitative analysis of the effects of
EM/distillation in Section 5.
4 Alternative to EM: Maximizing the
Posterior Classification Certainty
We have seen that in trying to solve the piggy-
backer problem, EM/distillation too readily aban-
dons the initialization based on Ladusaw?s hy-
pothesis, leading to an incorrect solution. Instead
of optimizing the data likelihood, what we need is
a measure of the number of plausible DEO candi-
dates there are in an NPI context, and a method
that refines the scores towards having only one
such plausible candidate per context. To this end,
we define the classification certainty to be the
product of the maximum posterior classification
probabilities over the DEO candidates. For a set
of hidden variables yN for NPI contexts N , this
is the expression:
Certainty(yN |N ) =
?
~X?N
max
y
P (y| ~X). (19)
To increase this certainty score, we propose
a novel iterative heuristic method for refining
the baseline initializations of P (Y ). Unlike
EM/distillation, our method biases learning to-
wards trusting the initialization, but refines the
scores towards having only one plausible DEO
per context in the training corpus. This is accom-
plished by treating the problem as a DEO classi-
fication problem, and then maximizing an objec-
tive ratio that favours one DEO per context. Our
method is not guaranteed to increase classification
certainty between iterations, but we will show that
it does increase certainty very quickly in practice.
The key observation that allows us to resolve
the tension between trusting the initialization and
enforcing one DEO per NPI context is that the
distributions of words that co-occur with DEOs
and piggybackers are different, and that this dif-
ference follows from Ladusaw?s hypothesis. In
particular, while DEOs may appear with or with-
out piggybackers in NPI contexts, piggybackers
do not appear without DEOs in NPI contexts, be-
cause Ladusaw?s hypothesis stipulates that a DEO
is required to license the NPI in the first place.
Thus, the presence of a high-scoring DEO candi-
date among otherwise low-scoring words is strong
evidence that the high-scoring word is not a pig-
gybacker and its high score from the initialization
is deserved. Conversely, a DEO candidate which
always appears in the presence of other strong
DEO candidates is likely a piggybacker whose
initial high score should be discounted.
We now describe our heuristic method that is
based on this intuition. For clarity, we use scores
rather than probabilities in the following explana-
tion, though it is equally applicable to either. As
in EM/distillation, the method is initialized with
the baseline S(y) scores. One iteration of the
method proceeds as follows. Let the score of the
strongest DEO candidate in an NPI context p be:
M(p) = max
y?p
Sth(y), (20)
where Sth(y) is the score of candidate y at the tth
iteration according to this heuristic method.
Then, for each word-type y in each context p,
we compare the current score of y to the scores of
the other words in p. If y is currently the strongest
DEO candidate in p, then we give y credit equal
to the proportional change to M(p) if y were re-
moved (Context p without y is denoted p \ y). A
large change means that y is the only plausible
DEO candidate in p, while a small change means
that there are other plausible DEO candidates. If
y is not currently the strongest DEO candidate, it
receives no credit:
cred(p, y) =
{
M(p)?M(p\y)
M(p) if Sth(y) = M(p)
0 otherwise.
(21)
700
NPI contexts
A B C,B C,B C,D C
Original scores
S(A) = 5, S(B) = 4, S(C) = 1, S(D) = 2
Updated scores
Sh(A) = 5? (5? 4)/5 = 1
Sh(B) = 4? (0 + 2? (4? 1)/4)/3 = 2
Sh(C) = 1? (0 + 0 + 0) = 0
Sh(D) = 2? (2? 1)/2 = 1
Figure 3: Example of one iteration of the certainty-
based heuristic on four NPI contexts with four words
in the lexicon.
Then, the average credit received by each y is
a measure of how much we should trust the cur-
rent score for y. The updated score for each DEO
candidate is the original score multiplied by this
average:
St+1h (y) =
Sth(y)
|Ny|
?
?
p?Ny
cred(p, y). (22)
The probability P t+1(Y = y) is then simply
St+1h (y) normalized:
P t+1(Y = y) = S
t+1
h (y)
?
y??L
St+1h (y?)
. (23)
We iteratively reduce the scores in this fashion
to get better estimates of the relative suitability of
word-types as DEOs.
An example of this method and how it solves
the piggybacker problem is given in Figure 3. In
this example, we would like to learn that B and
D are DEOs, A is a piggybacker, and C is a fre-
quent word-type, such as a stop word. Using the
original scores, piggybacker A would appear to
be the most likely word to be a DEO. However,
by noticing that it never occurs on its own with
words that are unlikely to be DEOs (in the exam-
ple, word C), our heuristic penalizes A more than
B, and ranks B higher after one iteration. EM
prior re-estimation would not correctly solve this
example, as it would converge on a solution where
C receives all of the probability mass because it
appears in all of the contexts, even though it is
unlikely to be a DEO according to the initializa-
tion.
5 Experiments
We evaluate the performance of these methods on
the BLLIP corpus (?30M words) and the AFP
portion of the Gigaword corpus (?338M words).
Following DLD09, we define an NPI context to
be all the words to the left of an NPI, up to the
closest comma or semi-colon, and removed NPI
contexts which contain the most common DEOs
like not. We further removed all empty NPI con-
texts or those which only contain other punctua-
tion. After this filtering, there were 26696 NPI
contexts in BLLIP and 211041 NPI contexts in
AFP, using the same list of 26 NPIs defined by
DLD09.
We first define an automatic measure of per-
formance that is common in information retrieval.
We use average precision to quantify how well a
system separates DEOs from non-DEOs. Given a
list of known DEOs, G, and non-DEOs, the aver-
age precision of a ranked list of items, X, is de-
fined by the following equation:
AP (X) =
?n
k=1 P (X1...k)? 1(xk ? G)
|G| ,
(24)
where P (X1...k) is the precision of the first k
items and 1(xk ? G) is an indicator function
which is 1 if x is in the gold standard list of DEOs
and 0 otherwise.
DLD09 simply evaluated the top 150 output
DEO candidates by their systems, and qualita-
tively judged the precision of the top-k candidates
at various values of k up to 150. Average preci-
sion can be seen as a generalization of this evalu-
ation procedure that is sensitive to the ranking of
DEOs and non-DEOs. For development purposes,
we use the list of 150 annotations by DLD09. Of
these, 90 were DEOs, 30 were not, and 30 were
classified as ?other? (they were either difficult to
classify, or were other types of non-veridical oper-
ators like comparatives or conditionals). We dis-
carded the 30 ?other? items and ignored all items
not in the remaining 120 items when evaluating a
ranked list of DEO candidates. We call this mea-
sure AP120.
In addition, we annotated DEO candidates from
the top-150 rankings produced by our certainty-
701
absolve, abstain, banish, bereft, boycott, cau-
tion, clear, coy, delay, denial, desist, devoid,
disavow, discount, dispel, disqualify, down-
play, exempt, exonerate, foil, forbid, forego,
impossible, inconceivable, irrespective, limit,
mitigate, nip, noone, omit, outweigh, pre-
condition, pre-empt, prerequisite, refute, re-
move5, repel, repulse, scarcely, scotch, scuttle,
seldom, sensitive, shy, sidestep, snuff, thwart,
waive, zero-tolerance
Figure 4: Lemmata of DEOs identified in this work not
found by DLD09.
based heuristic on BLLIP and also by the dis-
tillation and heuristic methods on AFP, in order
to better evaluate the final output of the meth-
ods. This produced an additional 68 DEOs (nar-
rowly defined) (Figure 4), 58 non-DEOs, and 31
?other? items4. Adding the DEOs and non-DEOs
we found to the 120 items from above, we have
an expanded list of 246 items to rank, and a corre-
sponding average precision which we call AP246.
We employ the frequency cut-offs used by
DLD09 for sparsity reasons. A word-type must
appear at least 10 times in an NPI context and
150 times in the corpus overall to be considered.
We treat BLLIP as a development corpus and use
AP120 on AFP to determine the number of itera-
tions to run our heuristic (5 iterations for BLLIP
and 13 iterations for AFP). We run EM/distillation
for one iteration in development and testing, be-
cause more iterations hurt performance, as ex-
plained in Section 3.
We first report the AP120 results of our ex-
periments on the BLLIP corpus (Table 1 sec-
ond column). Our method outperforms both
EM/distillation and the baseline method. These
results are replicated on the final test set from
AFP using the full set of annotations AP246 (Ta-
ble 1 third column). Note that the scores are lower
when using all the annotations because there are
more non-DEOs relative to DEOs in this list, mak-
ing the ranking task more challenging.
A better understanding of the algorithms can
4The complete list will be made publicly available.
5We disagree with DLD09 that remove is not downward-
entailing; e.g., The detergent removed stains from his cloth-
ing. ? The detergent removed stains from his shirts.
Method BLLIP AP120 AFP AP246
Baseline .879 .734
Distillation .946 .785
This work .955 .809
Table 1: Average precision results on the BLLIP and
AFP corpora.
be obtained by examining the data likelihood and
the classification certainty at each iteration of the
algorithms (Figure 5). Whereas EM/distillation
maximizes the former expression, the certainty-
based heuristic method actually decreases data
likelihood for the first couple of iterations before
increasing it again. In terms of classification cer-
tainty, EM/distillation converges to a lower classi-
fication certainty score compared to our heuristic
method. Thus, our method better captures the as-
sumption of one DEO per NPI context.
6 Bootstrapping to Co-Learn NPIs and
DEOs
The above experiments show that the heuristic
method outperforms the EM/distillation method
given a list of NPIs. We would like to extend
this result to novel domains, corpora, and lan-
guages. DLD09 and DL10 proposed the follow-
ing bootstrapping algorithm for co-learning NPIs
and DEOs given a much smaller list of NPIs as a
seed set.
1. Begin with a small set of seed NPIs
2. Iterate:
(a) Use the current list of NPIs to learn a
list of DEOs
(b) Use the current list of DEOs to learn a
list of NPIs
Interestingly, DL10 report that while this
method works in Romanian data, it does not work
in the English BLLIP corpus. They speculate that
the reason might be due to the nature of the En-
glish DEO any, which can occur in all classes of
DE contexts according to an analysis by Haspel-
math (1997). Further, they find that in Romanian,
distillation does not perform better than the base-
line method during Step (2a). While this linguis-
tic explanation may certainly be a factor, we raise
702
0 1 2 3 4 5 6 7 8 9 10
-2.5
-2
-1.5
-1
-0.5
0
x 106
Iterations
Lo
g 
pr
ob
ab
ilit
y
(a) Data log likelihood.
0 1 2 3 4 5 6 7 8 9 10
-2.5
-2
-1.5
-1
-0.5
0
x 105
Iterations
Lo
g 
pr
ob
ab
ilit
y
(b) Log classification certainty probabilities.
Figure 5: Log likelihood and classification certainty probabilities of NPI contexts in two corpora. Thinner lines
near the top are for BLLIP; thicker lines for AFP. Blue dotted: baseline; red dashed: distillation; green solid:
our certainty-based heuristic method. P ( ~X|y) probabilities are not included since they would only result in a
constant offset in the log domain.
a second possibility that the distillation algorithm
itself may be responsible for these results. As ev-
idence, we show that the heuristic algorithm is
able to work in English with just the single seed
NPI any, and in fact the bootstrapping approach in
conjunction with our heuristic even outperforms
the above approaches when using a static list of
NPIs.
In particular, we use the methods described in
the previous sections for Step (2a), and the follow-
ing ratio to rank NPI candidates in Step (2b), cor-
responding to the baseline method to detect DEOs
in reverse:
T (x) = count
D(x)/tokens(D)
countC(x)/tokens(C) . (25)
Here, countD(x) refers to the number of oc-
currences of NPI candidate x in DEO contexts
D, defined to be the words to the right of a DEO
operator up to a comma or semi-colon. We do
not use the EM/distillation or heuristic methods in
Step (2b). Learning NPIs from DEOs is a much
harder problem than learning DEOs from NPIs.
Because DEOs (and other non-veridical opera-
tors) license NPIs, the majority of occurrences of
NPIs will be in the context of a DEO, modulo am-
biguity of DEOs such as the free-choice any and
other spurious correlations such as piggybackers
as discussed earlier. In the other direction, it is
not the case that DEOs always or nearly always
appear in the context of an NPI. Rather, the most
common collocations of DEOs are the selectional
preferences of the DEO, such as common argu-
ments to verbal DEOs, prepositions that are part
of the subcategorization of the DEO, and words
that together with the surface form of the DEO
comprise an idiomatic expression or multi-word
expression. Further, NPIs are more likely to be
composed of multiple words, while many DEOs
are single words, possibly with PP subcategoriza-
tion requirements which can be filled in post hoc.
Because of these issues, we cannot trust the ini-
tialization to learn NPIs nearly as much as with
DEOs, and cannot use the distillation or certainty
methods for this step. Rather, the hope is that
learning a noisy list of ?pseudo-NPIs?, which of-
ten occur in negative contexts but may not actu-
ally be NPIs, can still improve the performance of
DEO detection.
There are a number of parameters to the method
which we tuned to the BLLIP corpus using
AP120. At the end of Step (2a), we use the cur-
rent top 25 DEOs plus 5 per iteration as the DEO
list for the next step. To the initial seed NPI of
703
Method BLLIP AP120 AFP AP246
Baseline .889 (+.010) .739 (?.005)
Distillation .930 (?.016) .804 (+.019)
This work .962 (+.007) .821 (+.012)
Table 2: Average precision results with bootstrapping
on the BLLIP and AFP corpora. Absolute gain in av-
erage precision compared to using a fixed list of NPIs
given in brackets.
anymore, anything, anytime, avail, bother,
bothered, budge, budged, countenance, faze,
fazed, inkling, iota, jibe, mince, nor, whatso-
ever, whit
Figure 6: Probable NPIs found by bootstrapping using
the certainty-based heuristic method.
any, we add the top 5 ranking NPI candidates at
the end of Step (2b) in each subsequent iteration.
We ran the bootstrapping algorithm for 11 itera-
tions for all three algorithms. The final evaluation
was done on AFP using AP246.
The results show that bootstrapping can indeed
improve performance, even in English (Table 2).
Using bootstrapping to co-learn NPIs and DEOs
actually results in better performance than spec-
ifying a static list of NPIs. The certainty-based
heuristic in particular achieves gains with boot-
strapping in both corpora, in contrast to the base-
line and distillation methods. Another factor that
we found to be important is to add a sufficient
number of NPIs to the NPI list each iteration, as
adding too few NPIs results in only a small change
in the NPI contexts available for DEO detection.
DL10 only added one NPI per iteration, which
may explain why they did not find any improve-
ment with bootstrapping in English. It also ap-
pears that learning the pseudo-NPIs does not hurt
performance in detecting DEO, and further, that
a number of true NPIs are learned by our method
(Figure 6).
7 Conclusion
We have proposed a novel unsupervised method
for discovering downward-entailing operators
from raw text based on their co-occurrence with
negative polarity items. Unlike the distilla-
tion method of DLD09, which we show to
be an instance of EM prior re-estimation, our
method directly addresses the issue of piggyback-
ers which spuriously correlate with NPIs but are
not downward-entailing. This is achieved by
maximizing the posterior classification certainty
of the corpus in a way that respects the initializa-
tion, rather than maximizing the data likelihood
as in EM/distillation. Our method outperforms
distillation and a baseline method on two corpora
as well as in a bootstrapping setting where NPIs
and DEOs are jointly learned. It achieves the best
performance in the bootstrapping setting, rather
than when using a fixed list of NPIs. The perfor-
mance of our algorithm suggests that it is suitable
for other corpora and languages.
Interesting future research directions include
detecting DEOs of more than one word as well as
distinguishing the particular word sense and sub-
categorization that is downward-entailing. An-
other problem that should be addressed is the
scope of the downward entailment, generalizing
work being done in detecting the scope of nega-
tion (Councill et al 2010, for example).
Acknowledgments
We would like to thank Cristian Danescu-
Niculescu-Mizil for his help with replicating his
results on the BLLIP corpus. This project was
supported by the Natural Sciences and Engineer-
ing Research Council of Canada.
References
Luisa Bentivogli, Peter Clark, Ido Dagan, Hoa T.
Dang, and Danilo Giampiccolo. 2010. The sixth
pascal recognizing textual entailment challenge. In
The Text Analysis Conference (TAC 2010).
Isaac G. Councill, Ryan McDonald, and Leonid Ve-
likovich. 2010. What?s great and what?s not:
Learning to classify the scope of negation for im-
proved sentiment analysis. In Proceedings of the
Workshop on Negation and Speculation in Natural
Language Processing, pages 51?59. Association for
Computational Linguistics.
Cristian Danescu-Niculescu-Mizil and Lillian Lee.
2010. Don?t ?have a clue??: Unsupervised co-
learning of downward-entailing operators. In Pro-
ceedings of the ACL 2010 Conference Short Papers,
pages 247?252. Association for Computational Lin-
guistics.
Cristian Danescu-Niculescu-Mizil, Lillian Lee, and
Richard Ducott. 2009. Without a ?doubt??: Un-
supervised discovery of downward-entailing oper-
704
ators. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics.
William A. Gale, Kenneth W. Church, and David
Yarowsky. 1992. One sense per discourse. In Pro-
ceedings of the Workshop on Speech and Natural
Language, pages 233?237. Association for Compu-
tational Linguistics.
Anastasia Giannakidou. 2002. Licensing and sensitiv-
ity in polarity items: from downward entailment to
nonveridicality. CLS, 38:29?53.
Martin Haspelmath. 1997. Indefinite pronouns. Ox-
ford University Press.
Jack Hoeksema. 1997. Corpus study of negative po-
larity items. IV-V Jornades de corpus linguistics
1996?1997.
William A. Ladusaw. 1980. On the notion ?affective?
in the analysis of negative-polarity items. Journal
of Linguistic Research, 1(2):1?16.
Timm Lichte and Jan-Philipp Soehn. 2007. The re-
trieval and classification of negative polarity items
using statistical profiles. Roots: Linguistics in
Search of Its Evidential Base, pages 249?266.
Bill MacCartney and Christopher D. Manning. 2008.
Modeling semantic containment and exclusion in
natural language inference. In Proceedings of the
22nd International Conference on Computational
Linguistics.
Frank Richter, Fabienne Fritzinger, and Marion Weller.
2010. Who can see the forest for the trees? ex-
tracting multiword negative polarity items from
dependency-parsed text. Journal for Language
Technology and Computational Linguistics, 25:83?
110.
Ton van der Wouden. 1997. Negative Contexts: Col-
location, Polarity and Multiple Negation. Rout-
ledge.
705
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 186?195,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Entity-based local coherence modelling using topological fields
Jackie Chi Kit Cheung and Gerald Penn
Department of Computer Science
University of Toronto
Toronto, ON, M5S 3G4, Canada
{jcheung,gpenn}@cs.toronto.edu
Abstract
One goal of natural language generation is
to produce coherent text that presents in-
formation in a logical order. In this pa-
per, we show that topological fields, which
model high-level clausal structure, are an
important component of local coherence
in German. First, we show in a sen-
tence ordering experiment that topologi-
cal field information improves the entity
grid model of Barzilay and Lapata (2008)
more than grammatical role and simple
clausal order information do, particularly
when manual annotations of this informa-
tion are not available. Then, we incor-
porate the model enhanced with topolog-
ical fields into a natural language gen-
eration system that generates constituent
orders for German text, and show that
the added coherence component improves
performance slightly, though not statisti-
cally significantly.
1 Introduction
One type of coherence modelling that has captured
recent research interest is local coherence mod-
elling, which measures the coherence of a docu-
ment by examining the similarity between neigh-
bouring text spans. The entity-based approach,
in particular, considers the occurrences of noun
phrase entities in a document (Barzilay and Lap-
ata, 2008). Local coherence modelling has been
shown to be useful for tasks like natural language
generation and summarization, (Barzilay and Lee,
2004) and genre classification (Barzilay and Lap-
ata, 2008).
Previous work on English, a language with rel-
atively fixed word order, has identified factors that
contribute to local coherence, such as the gram-
matical roles associated with the entities. There is
good reason to believe that the importance of these
factors vary across languages. For instance, freer-
word-order languages exhibit word order patterns
which are dependent on discourse factors relating
to information structure, in addition to the gram-
matical roles of nominal arguments of the main
verb. We thus expect word order information to be
particularly important in these languages in dis-
course analysis, which includes coherence mod-
elling.
For example, Strube and Hahn (1999) introduce
Functional Centering, a variant of Centering The-
ory which utilizes information status distinctions
between hearer-old and hearer-new entities. They
apply their model to pronominal anaphora reso-
lution, identifying potential antecedents of sub-
sequent anaphora by considering syntactic and
word order information, classifying constituents
by their familiarity to the reader. They find that
their approach correctly resolves more pronomi-
nal anaphora than a grammatical role-based ap-
proach which ignores word order, and the differ-
ence between the two approaches is larger in Ger-
man corpora than in English ones. Unfortunately,
their criteria for ranking potential antecedents re-
quire complex syntactic information in order to
classify whether proper names are known to the
hearer, which makes their algorithm hard to auto-
mate. Indeed, all evaluation is done manually.
We instead use topological fields, a model of
clausal structure which is indicative of information
structure in German, but shallow enough to be au-
tomatically parsed at high accuracy. We test the
hypothesis that they would provide a good com-
plement or alternative to grammatical roles in lo-
cal coherence modelling. We show that they are
superior to grammatical roles in a sentence or-
dering experiment, and in fact outperforms sim-
ple word-order information as well. We further
show that these differences are particularly large
when manual syntactic and grammatical role an-
186
Millionen von Mark   verschwendet   der Senat jeden Monat,   weil   er   sparen will. 
LK MF VCVF LK MF
S
NF
S
?The senate wastes millions of marks each month, because it wants to save.?
Figure 1: The clausal and topological field structure of a German sentence. Notice that the subordinate
clause receives its own topology.
notations are not available.
We then embed these topological field annota-
tions into a natural language generation system to
show the utility of local coherence information in
an applied setting. We add contextual features
using topological field transitions to the model
of Filippova and Strube (2007b) and achieve a
slight improvement over their model in a con-
stituent ordering task, though not statistically sig-
nificantly. We conclude by discussing possible
reasons for the utility of topological fields in lo-
cal coherence modelling.
2 Background and Related Work
2.1 German Topological Field Parsing
Topological fields are sequences of one or more
contiguous phrases found in an enclosing syntac-
tic region, which is the clause in the case of the
German topological field model (Ho?hle, 1983).
These fields may have constraints on the number
of words or phrases they contain, and do not nec-
essarily form a semantically coherent constituent.
In German, the topology serves to identify all of
the components of the verbal head of a clause, as
well as clause-level structure such as complemen-
tizers and subordinating conjunctions. Topologi-
cal fields are a useful abstraction of word order,
because while Germanic word order is relatively
free with respect to grammatical functions, the or-
der of the topological fields is strict and unvarying.
A German clause can be considered to be an-
chored by two ?brackets? which contain modals,
verbs and complementizers. The left bracket (linke
Klammer, LK) may contain a complementizer,
subordinating conjunction, or a finite verb, de-
pending on the clause type, and the right bracket
contains the verbal complex (VC). The other topo-
logical fields are defined in relation to these two
brackets, and contain all other parts of the clause
such as verbal arguments, adjuncts, and discourse
cues.
The VF (Vorfeld or ?pre-field?) is so-named be-
cause it occurs before the left bracket. As the first
constituent of most matrix clauses in declarative
sentences, it has special significance for the coher-
ence of a passage, which we will further discuss
below. The MF (Mittelfeld or ?middle field?) is
the field bounded by the two brackets. Most verb
arguments, adverbs, and prepositional phrases are
found here, unless they have been fronted and put
in the VF, or are prosodically heavy and postposed
to the NF field. The NF (Nachfeld or ?post-field?)
contains prosodically heavy elements such as post-
posed prepositional phrases or relative clauses,
and occasionally postposed noun phrases.
2.2 The Role of the Vorfeld
One of the reasons that we use topological fields
for local coherence modelling is the role that the
VF plays in signalling the information structure of
German clauses, as it often contains the topic of
the sentence.
In fact, its role is much more complex than be-
ing simply the topic position. Dipper and Zins-
meister (2009) distinguish multiple uses of the VF
depending on whether it contains an element re-
lated to the surrounding discourse. They find that
45.1% of VFs are clearly related to the previous
context by a reference or discourse relation, and a
further 21.9% are deictic and refer to the situation
described in the passage in a corpus study. They
also run a sentence insertion experiment where
subjects are asked to place an extracted sentence
in its original location in a passage. The authors
remark that extracted sentences with VFs that are
referentially related to previous context (e.g., they
contain a coreferential noun phrase or a discourse
relation like ?therefore?) are reinserted at higher
accuracies.
187
a)
# Original Sentence and Translation
1
Einen Zufluchtsort fu?r Frauen, die von ihren Ma?nnern mi?handelt werden, gibt es nunmehr auch
in Treptow.
?There is now a sanctuary for women who are mistreated by their husbands in Treptow as well.?
2
Das Bezirksamt bietet Frauen (auch mit Kindern) in derartigen Notsituationen voru?bergehend
eine Unterkunft.
?The district office offers women (even with children) in this type of emergency temporary
accommodation.?
3
Zugleich werden die Betroffenen der Regelung des Unterhalts, bei Beho?rdenga?ngen und auch
bei der Wohnungssuche unterstu?tzt.
?At the same time, the affected are supported with provisions of necessities, in dealing with
authorities, and also in the search for new accommodations.?
b)
DE Zufluchtsort Frauen Ma?nnern Treptow Kindern
EN sanctuary women husbands Treptow children
1 acc oth oth oth ?
2 ? oth ? ? oth
3 ? nom ? ? ?
c)
? ? ? nom ? acc ? oth nom ? nom nom nom acc nom oth
0.3 0.0 0.0 0.1 0.0 0.0 0.0 0.0
acc ? acc nom acc acc acc oth oth ? oth nom oth acc oth oth
0.1 0.0 0.0 0.0 0.3 0.1 0.0 0.1
Table 1: a) An example of a document from Tu?Ba-D/Z, b) an abbreviated entity grid representation of
it, and c) the feature vector representation of the abbreviated entity grid for transitions of length two.
Mentions of the entity Frauen are underlined. nom: nominative, acc: accusative, oth: dative, oblique,
and other arguments
Filippova and Strube (2007c) also examine the
role of the VF in local coherence and natural lan-
guage generation, focusing on the correlation be-
tween VFs and sentential topics. They follow Ja-
cobs (2001) in distinguishing the topic of addres-
sation, which is the constituent for which the
proposition holds, and frame-setting topics, which
is the domain in which the proposition holds, such
as a temporal expression. They show in a user
study that frame-setting topics are preferred to top-
ics of addressation in the VF, except when a con-
stituent needs to be established as the topic of ad-
dressation.
2.3 Using Entity Grids to Model Local
Coherence
Barzilay and Lapata (2008) introduce the entity
grid as a method of representing the coherence of a
document. Entity grids indicate the location of the
occurrences of an entity in a document, which is
important for coherence modelling because men-
tions of an entity tend to appear in clusters of
neighbouring or nearby sentences in coherent doc-
uments. This last assumption is adapted from Cen-
tering Theory approaches to discourse modelling.
In Barzilay and Lapata (2008), an entity grid is
constructed for each document, and is represented
as a matrix in which each row represents a sen-
tence, and each column represents an entity. Thus,
a cell in the matrix contains information about an
entity in a sentence. The cell is marked by the
presence or absence of the entity, and can also be
augmented with other information about the en-
tity in this sentence, such as the grammatical role
of the noun phrase representing that entity in that
sentence, or the topological field in which the noun
phrase appears.
Consider the document in Table 1. An entity
grid representation which incorporates the syntac-
tic role of the noun phrase in which the entity ap-
188
pears is also shown (not all entities are listed for
brevity). We tabulate the transitions of entities be-
tween different syntactic positions (or their non-
occurrence) in sentences, and convert the frequen-
cies of transitions into a feature vector representa-
tion of transition probabilities in the document.
To calculate transition probabilities, we divide
the frequency of a particular transition by the total
number of transitions of that length.
This model of local coherence was investigated
for German by Filippova and Strube (2007a). The
main focus of that work, however, was to adapt
the model for use in a low-resource situation when
perfect coreference information is not available.
This is particularly useful in natural language un-
derstanding tasks. They employ a semantic clus-
tering model to relate entities. In contrast, our
work focuses on improving performance by anno-
tating entities with additional linguistic informa-
tion, such as topological fields, and is geared to-
wards natural language generation systems where
perfect information is available.
Similar models of local coherence include vari-
ous Centering Theory accounts of local coherence
((Kibble and Power, 2004; Poesio et al, 2004)
inter alia). The model of Elsner and Charniak
(2007) uses syntactic cues to model the discourse-
newness of noun phrases. There are also more
global content models of topic shifts between sen-
tences like Barzilay and Lee (2004).
3 Sentence Ordering Experiments
3.1 Method
We test a version of the entity grid representa-
tion augmented with topological fields in a sen-
tence ordering experiment corresponding to Ex-
periment 1 of Barzilay and Lapata (2008). The
task is a binary classification task to identify the
original version of a document from another ver-
sion which contains the sentences in a randomly
permuted order, which is taken to be incoherent.
We solve this problem in a supervised machine
learning setting, where the input is the feature vec-
tor representations of the two versions of the doc-
ument, and the output is a binary value indicating
the document with the original sentence ordering.
We use SVMlight?s ranking module for classifi-
cation (Joachims, 2002).
The corpus in our experiments consists of the
last 480 documents of Tu?Ba-D/Z version 4 (Telljo-
hann et al, 2004), which contains manual corefer-
ence, grammatical role and topological field infor-
mation. This set is larger than the set that was used
in Experiment 1 of Barzilay and Lapata (2008),
which consists of 400 documents in two English
subcorpora on earthquakes and accidents respec-
tively. The average document length in the Tu?Ba-
D/Z subcorpus is also greater, at 19.2 sentences
compared to about 11 for the two subcorpora. Up
to 20 random permutations of sentences were gen-
erated from each document, with duplicates re-
moved.
There are 216 documents and 4126 original-
permutation pairs in the training set, and 24 docu-
ments and 465 pairs in the development set. The
remaining 240 documents are in the final test set
(4243 pairs). The entity-based model is parame-
terized as follows.
Transition length ? the maximum length of the
transitions used in the feature vector representa-
tion of a document.
Representation ? when marking the presence of
an entity in a sentence, what information about
the entity is marked (topological field, grammat-
ical role, or none). We will describe the represen-
tations that we try in section 3.2.
Salience ? whether to set a threshold for the fre-
quency of occurrence of entities. If this is set, all
entities below a certain frequency are treated sep-
arately from those reaching this frequency thresh-
old when calculating transition probabilities. In
the example in Table 1, with a salience thresh-
old of 2, Frauen would be treated separately from
Ma?nnern or Kindern.
Transition length, salience, and a regularization
parameter are tuned on the development set. We
only report results using the setting of transition
length ? 4, and no salience threshold, because
they give the best performance on the development
set. This is in contrast to the findings of Barzi-
lay and Lapata (2008), who report that transition
length ? 3 and a salience threshold of 2 perform
best on their data.
3.2 Entity Representations
The main goal of this study is to compare word
order, grammatical role and topological field in-
formation, which is encoded into the entity grid at
each occurrence of an entity. Here, we describe
the variants of the entity representations that we
compare.
189
Baseline Representations We implement sev-
eral baseline representations against which we test
our topological field-enhanced model. The sim-
plest baseline representation marks the mere ap-
pearance of an entity without any additional infor-
mation, which we refer to as default.
Another class of baseline representations mark
the order in which entities appear in the clause.
The correlation between word order and informa-
tion structure is well known, and has formed the
basis of some theories of syntax such as the Prague
School?s (Sgall et al, 1986). The two versions
of clausal order we tried are order 1/2/3+,
which marks a noun phrase as the first, the sec-
ond, or the third or later to appear in a clause, and
order 1/2+, which marks a noun phrase as the
first, or the second or later to appear in a clause.
Since noun phrases can be embedded in other
noun phrases, overlaps can occur. In this case, the
dominating noun phrase takes the smallest order
number among its dominated noun phrases.
The third class of baseline representations we
employ mark an entity by its grammatical role
in the clause. Barzilay and Lapata (2008) found
that grammatical role improves performance in
this task for an English corpus. Because Ger-
man distinguishes more grammatical roles mor-
phologically than English, we experiment with
various granularities of role labelling. In particu-
lar, subj/obj distinguishes the subject position,
the object position, and another category for all
other positions. cases distinguishes five types of
entities corresponding to the four morphological
cases of German in addition to another category
for noun phrases which are not complements of
the main verb.
Topological Field-Based These representations
mark the topological field in which an entity ap-
pears. Some versions mark entities which are
prepositional objects separately. We try versions
which distinguish VF from non-VF, as well as
more general versions that make use of a greater
set of topological fields. vfmarks the noun phrase
as belonging to a VF (and not in a PP) or not.
vfpp is the same as above, but allows preposi-
tional objects inside the VF to be marked as VF.
topf/pp distinguishes entities in the topological
fields VF, MF, and NF, contains a separate cat-
egory for PP, and a category for all other noun
phrases. topf distinguishes between VF, MF, and
NF, on the one hand, and everything else on the
other. Prepositional objects are treated the same
as other noun phrases here.
Combined We tried a representation which
combines grammatical role and topological field
into a single representation, subj/obj?vf,
which takes the Cartesian product of subj/obj
and vf above.
Topological fields do not map directly to topic-
focus distinctions. For example, besides the topic
of the sentence, the Vorfeld may contain discourse
cues, expletive pronouns, or the informational or
contrastive focus. Furthermore, there are addi-
tional constraints on constituent order related to
pronominalization. Thus, we devised additional
entity representations to account for these aspects
of German.
topic attempts to identify the sentential topic
of a clause. A noun phrase is marked as TOPIC
if it is in VF as in vfpp, or if it is the first
noun phrase in MF and also the first NP in the
clause. Other noun phrases in MF are marked
as NONTOPIC. Categories for NF and miscella-
neous noun phrases also exist. While this repre-
sentation may appear to be very similar to sim-
ply distinguishing the first entity in a clause as for
order 1/2+ in that TOPIC would correspond
to the first entity in the clause, they are in fact dis-
tinct. Due to issues related to coordination, appos-
itive constructions, and fragments which do not
receive a topology of fields, the first entity in a
clause is labelled the TOPIC only 80.8% of the
time in the corpus. This representation also distin-
guishes NFs, which clausal order does not.
topic+pron refines the above by taking into
account a word order restriction in German that
pronouns appear before full noun phrases in the
MF field. The following set of decisions repre-
sents how a noun phrase is marked: If the first NP
in the clause is a pronoun in an MF field and is the
subject, we mark it as TOPIC. If it is not the sub-
ject, we mark it as NONTOPIC. For other NPs, we
follow the topic representation.
3.3 Automatic annotations
While it is reasonable to assume perfect annota-
tions of topological fields and grammatical roles in
many NLG contexts, this assumption may be less
appropriate in other applications involving text-to-
text generation where the input to the system is
text such as paraphrasing or machine translation.
Thus, we test the robustness of the entity repre-
190
Representation Manual Automatic
topf/pp 94.44 94.89
topic 94.13 94.53
topic+pron 94.08 94.51
topf 93.87 93.11
subj/obj 93.831 91.7++
cases 93.312 90.93++
order 1/2+ 92.51++ 92.1+
subj/obj?vf 92.32++ 90.74++
default 91.42++ 91.42++
vfpp 91.37++ 91.68++
vf 91.21++ 91.16++
order 1/2/3+ 91.16++ 90.71++
Table 2: Accuracy (%) of the permutation de-
tection experiment with various entity represen-
tations using manual and automatic annotations
of topological fields and grammatical roles. The
baseline without any additional annotation is un-
derlined. Two-tailed sign tests were calculated for
each result against the best performing model in
each column (1: p = 0.101; 2: p = 0.053; +: statis-
tically significant, p < 0.05; ++: very statistically
significant, p < 0.01 ).
sentations to automatic extraction in the absence
of manual annotations. We employ the following
two systems for extracting topological fields and
grammatical roles.
To parse topological fields, we use the Berke-
ley parser of Petrov and Klein (2007), which has
been shown to perform well at this task (Cheung
and Penn, 2009). The parser is trained on sections
of Tu?Ba-D/Z which do not overlap with the sec-
tion from which the documents for this experiment
were drawn, and obtains an overall parsing per-
formance of 93.35% F1 on topological fields and
clausal nodes without gold POS tags on the section
of Tu?Ba-D/Z it was tested on.
We tried two methods to obtain grammatical
roles. First, we tried extracting grammatical roles
from the parse trees which we obtained from the
Berkeley parser, as this information is present in
the edge labels that can be recovered from the
parse. However, we found that we achieved bet-
ter accuracy by using RFTagger (Schmid and
Laws, 2008), which tags nouns with their morpho-
logical case. Morphological case is distinct from
grammatical role, as noun phrases can function as
adjuncts in possessive constructions and preposi-
Annotation Accuracy (%)
Grammatical role 83.6
Topological field (+PP) 93.8
Topological field (?PP) 95.7
Clausal order 90.8
Table 3: Accuracy of automatic annotations of
noun phrases with coreferents. +PP means that
prepositional objects are treated as a separate cate-
gory from topological fields. ?PP means they are
treated as other noun phrases.
tional phrases. However, we can approximate the
grammatical role of an entity using the morpho-
logical case. We follow the annotation conven-
tions of Tu?Ba-D/Z in not assigning a grammati-
cal role when the noun phrase is a prepositional
object. We also do not assign a grammatical role
when the noun phrase is in the genitive case, as
genitive objects are very rare in German and are
far outnumbered by the possessive genitive con-
struction.
3.4 Results
Table 2 shows the results of the sentence ordering
permutation detection experiment. The top four
performing entity representations are all topologi-
cal field-based, and they outperform grammatical
role-based and simple clausal order-based mod-
els. These results indicate that the information
that topological fields provide about clause struc-
ture, appositives, right dislocation, etc. which is
not captured by simple clausal order is important
for coherence modelling. The representations in-
corporating linguistics-based heuristics do not out-
perform purely topological field-based models.
Surprisingly, the VF-based models fare quite
poorly, performing worse than not adding any an-
notations, despite the fact that topological field-
based models in general perform well. This result
may be a result of the heterogeneous uses of the
VF.
The automatic topological field annotations are
more accurate than the automatic grammatical role
annotations (Table 3), which may partly explain
why grammatical role-based models suffer more
when using automatic annotations. Note, how-
ever, that the models based on automatic topolog-
ical field annotations outperform even the gram-
matical role-based models using manual annota-
tion (at marginal significance, p < 0.1). The topo-
191
logical field annotations are accurate enough that
automatic annotations produce no decrease in per-
formance.
These results show the upper bound of entity-
based local coherence modelling with perfect
coreference information. The results we obtain
are higher than the results for the English cor-
pora of Barzilay and Lapata (2008) (87.2% on the
Earthquakes corpus and 90.4% on the Accidents
corpus), but this is probably due to corpus differ-
ences as well as the availability of perfect corefer-
ence information in our experiments1.
Due to the high performance we obtained, we
calculated Kendall tau coefficients (Lapata, 2006)
over the sentence orderings of the cases in which
our best performing model is incorrect, to deter-
mine whether the remaining errors are instances
where the permuted ordering is nearly identical to
the original ordering. We obtained a ? of 0.0456
in these cases, compared to a ? of ?0.0084 for all
the pairs, indicating that this is not the case.
To facilitate comparison to the results of Filip-
pova and Strube (2007a), we rerun this experiment
on the same subsections of the corpus as in that
work for training and testing. The first 100 arti-
cles of Tu?Ba-D/Z are used for testing, while the
next 200 are used for training and development.
Unlike the previous experiments, we do not do
parameter tuning on this set of data. Instead, we
follow Filippova and Strube (2007a) in using tran-
sition lengths of up to three. We do not put in
a salience threshold. We see that our results are
much better than the ones reported in that work,
even for the default representation. The main
reason for this discrepancy is probably the way
that entities are created from the corpus. In our
experiments, we create an entity for every single
noun phrase node that we encounter, then merge
the entities that are linked by coreference. Filip-
pova and Strube (2007a) convert the annotations
of Tu?Ba-D/Z into a dependency format, then ex-
tract entities from the noun phrases found there.
They may thus annotate fewer entities, as there
1Barzilay and Lapata (2008) use the coreference sys-
tem of Ng and Cardie (2002) to obtain coreference anno-
tations. We are not aware of similarly well-tested, pub-
licly available coreference resolution systems that handle all
types of anaphora for German. We considered adapting the
BART coreference resolution toolkit (Versley et al, 2008) to
German, but a number of language-dependent decisions re-
garding preprocessing, feature engineering, and the learning
paradigm would need to be made in order to achieve rea-
sonable performance comparable to state-of-the-art English
coreference resolution systems.
Representation Accuracy (%)
topf/pp 93.83
topic 93.31
topic+pron 93.31
topf 92.49
subj/obj 88.99
order 1/2+ 88.89
order 1/2/3+ 88.84
cases 88.63
vf 87.60
vfpp 88.17
default 87.55
subj/obj?vf 87.71
(Filippova and Strube, 2007) 75
Table 4: Accuracy (%) of permutation detection
experiment with various entity representations us-
ing manual and automatic annotations of topolog-
ical fields and grammatical roles on subset of cor-
pus used by Filippova and Strube (2007a).
may be nested NP nodes in the original corpus.
There may also be noise in the dependency con-
version process.
The relative rankings of different entity repre-
sentations in this experiment are similar to the
rankings of the previous experiment, with topolog-
ical field-based models outperforming grammati-
cal role and clausal order models.
4 Local Coherence for Natural Language
Generation
One of the motivations of the entity grid-based
model is to improve surface realization decisions
in NLG systems. A typical experimental design
would pass the contents of the test section of a
corpus as input to the NLG system with the order-
ing information stripped away. The task is then to
regenerate the ordering of the information found
in the original corpus. Various coherence models
have been tested in corpus-based NLG settings.
For example, Karamanis et al (2009) compare
several versions of Centering Theory-based met-
rics of coherence on corpora by examining how
highly the original ordering found in the corpus
is ranked compared to other possible orderings of
propositions. A metric performs well if it ranks
the original ordering better than the alternative or-
derings.
In our next experiment, we incorporate local co-
192
herence information into the system of Filippova
and Strube (2007b). We embed entity topologi-
cal field transitions into their probabilistic model,
and show that the added coherence component
slightly improves the performance of the baseline
NLG system in generating constituent orderings in
a German corpus, though not to a statistically sig-
nificant degree.
4.1 Method
We use the WikiBiography corpus2 for our exper-
iments. The corpus consists of more than 1100 bi-
ographies taken from the German Wikipedia, and
contains automatic annotations of morphological,
syntactic, and semantic information. Each article
also contains the coreference chain of the subject
of the biography (the biographee). The first 100
articles are used for testing, the next 200 for de-
velopment, and the rest for training.
The baseline generation system already incor-
porates topological field information into the con-
stituent ordering process. The system operates in
two steps. First, in main clauses, one constituent
is selected as the Vorfeld (VF). This is done us-
ing a maximum entropy model (call it MAXENT).
Then, the remaining constituents are ordered using
a second maximum entropy model (MAXENT2).
Significantly, Filippova and Strube (2007b) found
that selecting the VF first, and then ordering the
remaining constituents results in a 9% absolute
improvement over the corresponding model where
the selection is performed in one step by the sort-
ing algorithm alone.
The maximum entropy model for both steps rely
on the following features:
? features on the voice, valency, and identity of
the main verb of the clause
? features on the morphological and syntactic
status of the constituent to be ordered
? whether the constituent occurs in the preced-
ing sentence
? features for whether the constituent contains
a determiner, an anaphoric pronoun, or a rel-
ative clause
? the size of the constituent in number of mod-
ifiers, in depth, and in number of words
2http://www.eml-research.de/english/
research/nlp/download/wikibiography.php
? the semantic class of the constituent (per-
son, temporal, location, etc.) The biographee,
in particular, is marked by its own semantic
class.
In the first VF selection step, MAXENT simply
produces a probability of each constituent being a
VF, and the constituent with the highest probabil-
ity is selected. In the second step, MAXENT2 takes
the featural representation of two constituents, and
produces an output probability of the first con-
stituent preceding the second constituent. The fi-
nal ordering is achieved by first randomizing the
order of the constituents in a clause (besides the
first one, which is selected to be the VF), then
sorting them according to the precedence proba-
bilities. Specifically, a constituent A is put before
a constituent B if MAXENT2(A,B) > 0.5. Because
this precedence relation is not antisymmetric (i.e.,
MAXENT2(A,B) > 0.5 and MAXENT2(B,A) >
0.5 may be simultaneously true or simultaneously
false), different initializations of the order pro-
duce different sorted results. In our experiments,
we correct this by defining the precedence rela-
tion to be A precedes B iff MAXENT2(A,B) >
MAXENT2(B,A). This change does not greatly im-
pact the performance, and removes the random-
ized element of the algorithm.
The baseline system does not directly model the
context when ordering constituents. All of the
features but one in the original maximum entropy
models rely on local properties of the clause. We
incorporate local coherence information into the
model by adding entity transition features which
we found to be useful in the sentence ordering ex-
periment in Section 3 above.
Specifically, we add features indicating the
topological fields in which entities occur in the
previous sentences. We found that looking back
up to two sentences produces the best results (by
tuning on the development set). Because this cor-
pus does not come with general coreference in-
formation except for the coreference chain of the
biographee, we use the semantic classes instead.
So, all constituents in the same semantic class are
treated as one coreference chain. An example of a
feature may be biog-last2, which takes on a value
such as ?v??, meaning that this constituent refers
to the biographee, and the biographee occurs in
the VF two clauses ago (v), but does not appear in
the previous clause (?). For a constituent which is
not the biographee, this feature would be marked
193
Method VF Acc (%) Acc (%) Tau
Baseline 68.7 60.9 0.72
+Coherence 69.2 61.5 0.72
Table 5: Results of adding coherence features into
a natural language generation system. VF Acc%
is the accuracy of selecting the first constituent in
main clauses. Acc % is the percentage of per-
fectly ordered clauses, tau is Kendall?s ? on the
constituent ordering. The test set contains 2246
clauses, of which 1662 are main clauses.
?na? (not applicable).
4.2 Results
Table 5 shows the results of adding these contex-
tual features into the maximum entropy models.
We see that we obtain a small improvement in the
accuracy of VF selection, and in the accuracy of
correctly ordering the entire clause. These im-
provements are not statistically significant by Mc-
Nemar?s test. We suggest that the lack of coref-
erence information for all entities in the article
may have reduced the benefit of the coherence
component. Also, the topline of performance is
substantially lower than 100%, as multiple order-
ings are possible and equally valid. Human judge-
ments on information structuring for both inter-
and intra-sentential units are known to have low
agreement (Barzilay et al, 2002; Filippova and
Strube, 2007c; Lapata, 2003; Chen et al, 2007).
Thus, the relative error reduction is higher than the
absolute reduction might suggest.
5 Conclusions
We have shown that topological fields are a use-
ful source of information for local coherence mod-
elling. In a sentence-order permutation detection
task, models which use topological field infor-
mation outperform both grammatical role-based
models and models based on simple clausal or-
der, with the best performing model achieving a
relative error reduction of 40.4% over the original
baseline without any additional annotation. Ap-
plying our local coherence model in another set-
ting, we have embedded topological field transi-
tions of entities into an NLG system which orders
constituents in German clauses. We find that the
coherence-enhanced model slightly outperforms
the baseline system, but this was not statistically
significant.
We suggest that the utility of topological fields
in local coherence modelling comes from the in-
teraction between word order and information
structure in freer-word-order languages. Crucially,
topological fields take into account issues such
as coordination, appositives, sentential fragments
and differences in clause types, which word or-
der alone does not. They are also shallow enough
to be accurately parsed automatically for use in
resource-poor applications.
Further refinement of the topological field an-
notations to take advantage of the fact that they
do not correspond neatly to any single information
status such as topic or focus could provide addi-
tional performance gains. The model also shows
promise for other discourse-related tasks such as
coreference resolution and discourse parsing.
Acknowledgements
We are grateful to Katja Filippova for providing us
with source code for the experiments in Section 4
and for answering related questions, and to Tim-
othy Fowler for useful discussions and comments
on a draft of the paper. This work is supported in
part by the Natural Sciences and Engineering Re-
search Council of Canada.
References
R. Barzilay and M. Lapata. 2008. Modeling local co-
herence: An entity-based approach. Computational
Linguistics, 34(1):1?34.
R. Barzilay and L. Lee. 2004. Catching the drift: Prob-
abilistic content models, with applications to gen-
eration and summarization. In Proc. HLT-NAACL
2004, pages 113?120.
R. Barzilay, N. Elhadad, and K. McKeown. 2002. In-
ferring strategies for sentence ordering in multidoc-
ument news summarization. Journal of Artificial In-
telligence Research, 17:35?55.
E. Chen, B. Snyder, and R. Barzilay. 2007. Incremen-
tal text structuring with online hierarchical ranking.
In Proceedings of EMNLP, pages 83?91.
J.C.K. Cheung and G. Penn. 2009. Topological Field
Parsing of German. In Proc. 47th ACL and 4th IJC-
NLP, pages 64?72. Association for Computational
Linguistics.
S. Dipper and H. Zinsmeister. 2009. The Role of
the German Vorfeld for Local Coherence: A Pi-
lot Study. In Proceedings of the Conference of the
German Society for Computational Linguistics and
Language Technology (GSCL), pages 69?79. Gunter
Narr.
194
M. Elsner and E. Charniak. 2007. A generative
discourse-new model for text coherence. Technical
report, Technical Report CS-07-04, Brown Univer-
sity.
K. Filippova and M. Strube. 2007a. Extending the
entity-grid coherence model to semantically related
entities. In Proceedings of the Eleventh European
Workshop on Natural Language Generation, pages
139?142. Association for Computational Linguis-
tics.
K. Filippova and M. Strube. 2007b. Generating con-
stituent order in German clauses. In Proc. 45th ACL,
pages 320?327.
K. Filippova and M. Strube. 2007c. The German Vor-
feld and Local Coherence. Journal of Logic, Lan-
guage and Information, 16(4):465?485.
T.N. Ho?hle. 1983. Topologische Felder. Ph.D. thesis,
Ko?ln.
J. Jacobs. 2001. The dimensions of topiccomment.
Linguistics, 39(4):641?681.
T. Joachims. 2002. Learning to Classify Text Using
Support Vector Machines. Kluwer.
N. Karamanis, C. Mellish, M. Poesio, and J. Oberlan-
der. 2009. Evaluating centering for information or-
dering using corpora. Computational Linguistics,
35(1):29?46.
R. Kibble and R. Power. 2004. Optimizing referential
coherence in text generation. Computational Lin-
guistics, 30(4):401?416.
M. Lapata. 2003. Probabilistic text structuring: Exper-
iments with sentence ordering. In Proc. 41st ACL,
pages 545?552.
M. Lapata. 2006. Automatic evaluation of information
ordering: Kendall?s tau. Computational Linguistics,
32(4):471?484.
V. Ng and C. Cardie. 2002. Improving machine learn-
ing approaches to coreference resolution. In Proc.
40th ACL, pages 104?111.
S. Petrov and D. Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of NAACL
HLT 2007, pages 404?411.
M. Poesio, R. Stevenson, B.D. Eugenio, and J. Hitze-
man. 2004. Centering: A parametric theory
and its instantiations. Computational Linguistics,
30(3):309?363.
H. Schmid and F. Laws. 2008. Estimation of condi-
tional probabilities with decision trees and an appli-
cation to fine-grained POS tagging. In Proc. 22nd
COLING, pages 777?784. Association for Compu-
tational Linguistics.
P. Sgall, E. Hajic?ova?, J. Panevova?, and J. Mey. 1986.
The meaning of the sentence in its semantic and
pragmatic aspects. Springer.
M. Strube and U. Hahn. 1999. Functional center-
ing: Grounding referential coherence in information
structure. Computational Linguistics, 25(3):309?
344.
H. Telljohann, E. Hinrichs, and S. Kubler. 2004.
The Tu?Ba-D/Z treebank: Annotating German with
a context-free backbone. In Proc. Fourth Interna-
tional Conference on Language Resources and Eval-
uation (LREC 2004), pages 2229?2235.
Y. Versley, S.P. Ponzetto, M. Poesio, V. Eidelman,
A. Jern, J. Smith, X. Yang, and A. Moschitti. 2008.
BART: A modular toolkit for coreference resolution.
In Proc. 46th ACL-HLT Demo Session, pages 9?12.
Association for Computational Linguistics.
195
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 335?344,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Accurate Context-Free Parsing with Combinatory Categorial Grammar
Timothy A. D. Fowler and Gerald Penn
Department of Computer Science, University of Toronto
Toronto, ON, M5S 3G4, Canada
{tfowler, gpenn}@cs.toronto.edu
Abstract
The definition of combinatory categorial
grammar (CCG) in the literature varies
quite a bit from author to author. How-
ever, the differences between the defini-
tions are important in terms of the lan-
guage classes of each CCG. We prove
that a wide range of CCGs are strongly
context-free, including the CCG of CCG-
bank and of the parser of Clark and Cur-
ran (2007). In light of these new results,
we train the PCFG parser of Petrov and
Klein (2007) on CCGbank and achieve
state of the art results in supertagging ac-
curacy, PARSEVAL measures and depen-
dency accuracy.
1 Introduction
Combinatory categorial grammar (CCG) is a vari-
ant of categorial grammar which has attracted in-
terest for both theoretical and practical reasons.
On the theoretical side, we know that it is mildly
context-sensitive (Vijay-Shanker and Weir, 1994)
and that it can elegantly analyze a wide range of
linguistic phenomena (Steedman, 2000). On the
practical side, we have corpora with CCG deriva-
tions for each sentence (Hockenmaier and Steed-
man, 2007), a wide-coverage parser trained on that
corpus (Clark and Curran, 2007) and a system for
converting CCG derivations into semantic repre-
sentations (Bos et al, 2004).
However, despite being treated as a single uni-
fied grammar formalism, each of these authors use
variations of CCG which differ primarily on which
combinators are included in the grammar and the
restrictions that are put on them. These differences
are important because they affect whether the
mild context-sensitivity proof of Vijay-Shanker
and Weir (1994) applies. We will provide a gen-
eralized framework for CCG within which the full
variation of CCG seen in the literature can be de-
fined. Then, we prove that for a wide range of
CCGs there is a context-free grammar (CFG) that
has exactly the same derivations. Included in this
class of strongly context-free CCGs are a grammar
including all the derivations in CCGbank and the
grammar used in the Clark and Curran parser.
Due to this insight, we investigate the potential
of using tools from the probabilistic CFG com-
munity to improve CCG parsing results. The
Petrov parser (Petrov and Klein, 2007) uses la-
tent variables to refine the grammar extracted from
a corpus to improve accuracy, originally used
to improve parsing results on the Penn treebank
(PTB). We train the Petrov parser on CCGbank
and achieve the best results to date on sentences
from section 23 in terms of supertagging accuracy,
PARSEVAL measures and dependency accuracy.
These results should not be interpreted as proof
that grammars extracted from the Penn treebank
and from CCGbank are equivalent. Bos?s system
for building semantic representations from CCG
derivations is only possible due to the categorial
nature of CCG. Furthermore, the long distance de-
pendencies involved in extraction and coordina-
tion phenomena have a more natural representa-
tion in CCG.
2 The Language Classes of Combinatory
Categorial Grammars
A categorial grammar is a grammatical system
consisting of a finite set of words, a set of cate-
gories, a finite set of sentential categories, a finite
lexicon mapping words to categories and a rule
system dictating how the categories can be com-
bined. The set of categories are constructed from a
finite set of atoms A (e.g. A = {S,NP,N,PP})
and a finite set of binary connectives B (e.g.
B = {/, \}) to build an infinite set of categories
C(A,B) (e.g. C(A,B) = {S, S\NP, (S\NP )/
NP, . . .}). For a category C , its size |C| is the
335
number of atom occurrences it contains. When not
specified, connectives are left associative.
According to the literature, combinatory cate-
gorial grammar has been defined to have a vari-
ety of rule systems. These rule systems vary from
a small rule set, motivated theoretically (Vijay-
Shanker and Weir, 1994), to a larger rule set,
motivated linguistically, (Steedman, 2000) to a
very large rule set, motivated by practical cover-
age (Hockenmaier and Steedman, 2007; Clark and
Curran, 2007). We provide a definition general
enough to incorporate these four main variants of
CCG, as well as others.
A combinatory categorial grammar (CCG) is a
categorial grammar whose rule system consists of
rule schemata where the left side is a sequence of
categories and the right side is a single category
where the categories may include variables over
both categories and connectives. In addition, rule
schemata may specify a sequence of categories
and connectives using the . . . convention1 . When
. . . appears in a rule, it matches any sequence of
categories and connectives according to the con-
nectives adjacent to the . . .. For example, the rule
schema for forward composition is:
X/Y, Y/Z ? X/Z
and the rule schema for generalized forward
crossed composition is:
X/Y, Y |1Z1|2 . . . |nZn ? X|1Z1|2 . . . |nZn
where X, Y and Zi for 1 ? i ? n are variables
over categories and |i for 1 ? i ? n are variables
over connectives. Figure 1 shows a CCG deriva-
tion from CCGbank.
A well-known categorial grammar which is not
a CCG is Lambek categorial grammar (Lambek,
1958) whose introduction rules cannot be charac-
terized as combinatory rules (Zielonka, 1981).
2.1 Classes for defining CCG
We define a number of schema classes general
enough that the important variants of CCG can be
defined by selecting some subset of the classes. In
addition to the schema classes, we also define two
restriction classes which define ways in which the
rule schemata from the schema classes can be re-
stricted. We define the following schema classes:
1The . . . convention (Vijay-Shanker and Weir, 1994) is
essentially identical to the $ convention of Steedman (2000).
(1) Application
? X/Y, Y ? X
? Y,X\Y ? X
(2) Composition
? X/Y, Y/Z ? X/Z
? Y \Z,X\Y ? X\Z
(3) Crossed Composition
? X/Y, Y \Z ? X\Z
? Y/Z,X\Y ? X/Z
(4) Generalized Composition
? X/Y, Y/Z1/ . . . /Zn ? X/Z1/ . . . /Zn
? Y \Z1\ . . . \Zn,X\Y ? X\Z1\ . . . \Zn
(5) Generalized Crossed Composition
? X/Y, Y |1Z1|2 . . . |nZn
? X|1Z1|2 . . . |nZn
? Y |1Z1|2 . . . |nZn,X\Y
? X|1Z1|2 . . . |nZn
(6) Reducing Generalized Crossed Composition
Generalized Composition or Generalized
Crossed Composition where |X| ? |Y |.
(7) Substitution
? (X/Y )|1Z, Y |1Z ? X|1Z
? Y |1Z, (X\Y )|1Z ? X|1Z
(8) D Combinator2
? X/(Y |1Z), Y |2W ? X|2(W |1Z)
? Y |2W,X\(Y |1Z) ? X|2(W |1Z)
(9) Type-Raising
? X ? T/(T\X)
? X ? T\(T/X)
(10) Finitely Restricted Type-Raising
? X ? T/(T\X) where ?X,T ? ? S for fi-
nite S
? X ? T\(T/X) where ?X,T ? ? S for fi-
nite S
(11) Finite Unrestricted Variable-Free Rules
? ~X ? Y where ? ~X, Y ? ? S for finite S
2Hoyt and Baldridge (2008) argue for the inclusion of the
D Combinator in CCG.
336
Mr. Vinken is chairman of Elsevier N.V. , the Dutch publishing group .
N/N N S[dcl]\NP/NP N NP\NP/NP N/N N , NP [nb]/N N/N N/N N .
N
N
NP
NP [conj]
N
NP
NP
NP\NP
NP
NP
S[dcl]\NP
N
NP
S[dcl]
S[dcl]
Figure 1: A CCG derivation from section 00 of CCGbank.
We define the following restriction classes:
(A) Rule Restriction to a Finite Set
The rule schemata in the schema classes of a
CCG are limited to a finite number of instan-
tiations.
(B) Rule Restrictions to Certain Categories 3
The rule schemata in the schema classes of a
CCG are limited to a finite number of instan-
tiations although variables are allowed in the
instantiations.
Vijay-Shanker and Weir (1994) define CCG to
be schema class (4) with restriction class (B).
Steedman (2000) defines CCG to be schema
classes (1-5), (6), (10) with restriction class (B).
2.2 Strongly Context-Free CCGs
Proposition 1. The set of atoms in any derivation
of any CCG consisting of a subset of the schema
classes (1-8) and (10-11) is finite.
Proof. A finite lexicon can introduce only a finite
number of atoms in lexical categories.
Any rule corresponding to a schema in the
schema classes (1-8) has only those atoms on the
right that occur somewhere on the left. Rules in
classes (10-11) can each introduce a finite number
of atoms, but there can be only a finite number of
3Baldridge (2002) introduced a variant of CCG where
modalities are added to the connectives / and \ along with
variants of the combinatory rules based on these modalities.
Our proofs about restriction class (B) are essentially identical
to proofs regarding the multi-modal variant.
such rules, limiting the new atoms to a finite num-
ber.
Definition 1. The subcategories for a category c
are c1 and c2 if c = c1 ? c2 for ? ? B and c if c is
atomic. Its second subcategories are the subcate-
gories of its subcategories.
Proposition 2. Any CCG consisting of a subset
of the rule schemata (1-3), (6-8) and (10-11) has
derivations consisting of only a finite number of
categories.
Proof. We first prove the proposition excluding
schema class (8). We will use structural induction
on the derivations to prove that there is a bound on
the size of the subcategories of any category in the
derivation. The base case is the assignment of a
lexical category to a word and the inductive step is
the use of a rule from schema classes (1-4), (6-7)
and (10-11).
Given that the lexicon is finite, there is a bound
k on the size of the subcategories of lexical cate-
gories. Furthermore, there is a bound l on the size
of the subcategories of categories on the right side
of any rule in (10) and (11). Let m = max(k, l).
For rules from schema class (1), the category
on the right is a subcategory of the first category
on the left, so the subcategories on the right are
bound by m. For rules from schema classes (2-3),
the category on the right has subcategories X and
Z each of which is bound in size by m since they
occur as subcategories of categories on the left.
For rules from schema class (6), since reduc-
ing generalized composition is a special case of re-
337
ducing generalized crossing composition, we need
only consider the latter. The category on the right
has subcategories X|1Z1|2 . . . |n?1|Zn?1 and Zn.
Zn is bound in size by m because it occurs as
a subcategory of the second category on the left.
Then, the size of Y |1Z1|2 . . . |n?1|Zn?1 must be
bound by m and since |X| ? |Y |, the size of
X|1Z1|2 . . . |n?1|Zn?1 must also be bound by m.
For rules from schema class (7), the category on
the right has subcategories X and Z . The size of
Z is bound by m because it is a subcategory of a
category on the left. The size of X is bound by
m because it is a second subcategory of a category
on the left.
Finally, the use of rules in schema classes (10-
11) have categories on the right that are bounded
by l, which is, in turn, bounded by m. Then, by
proposition 1, there must only be a finite number
of categories in any derivation in a CCG consisting
of a subset of rule schemata (1-3), (6-7) and (10-
11).
The proof including schema class (8) is essen-
tially identical except that k must be defined in
terms of the size of the second subcategories.
Definition 2. A grammar is strongly context-free
if there exists a CFG such that the derivations of
the two grammars are identical.
Proposition 3. Any CCG consisting of a subset
of the schema classes (1-3), (6-8) and (10-11) is
strongly context-free.
Proof. Since the CCG generates derivations
whose categories are finite in number let C be that
set of categories. Let S(C,X) be the subset of C
matching category X (which may have variables).
Then, for each rule schema C1, C2 ? C3 in (1-3)
and (6-8), we construct a context-free rule C ?3 ?
C ?1, C ?2 for each C ?i in S(C,Ci) for 1 ? i ? 3.
Similarly, for each rule schema C1 ? C2 in (10),
we construct a context-free rule C ?2 ? C ?1 which
results in a finite number of such rules. Finally, for
each rule schema ~X ? Z in (11) we construct a
context-free rule Z ? ~X. Then, for each entry in
the lexicon w ? C , we construct a context-free
rule C ? w.
The constructed CFG has precisely the same
rules as the CCG restricted to the categories in C
except that the left and right sides have been re-
versed. Thus, by proposition 2, the CFG has ex-
actly the same derivations as the CCG.
Proposition 4. Any CCG consisting of a subset of
the schema classes (1-3), (6-8) and (10-11) along
with restriction class (B) is strongly context-free.
Proof. If a CCG is allowed to restrict the use of
its rules to certain categories as in schema class
(B), then when we construct the context-free rules
by enumerating only those categories in the set C
allowed by the restriction.
Proposition 5. Any CCG that includes restriction
class (A) is strongly context-free.
Proof. We construct a context-free grammar with
exactly those rules in the finite set of instantiations
of the CCG rule schemata along with context-
free rules corresponding to the lexicon. This
CFG generates exactly the same derivations as the
CCG.
We have thus proved that of a wide range of the
rule schemata used to define CCGs are context-
free.
2.3 Combinatory Categorial Grammars in
Practice
CCGbank (Hockenmaier and Steedman, 2007)
is a corpus of CCG derivations that was semi-
automatically converted from the Wall Street Jour-
nal section of the Penn treebank. Figure 2 shows
a categorization of the rules used in CCGbank ac-
cording to the schema classes defined in the pre-
ceding section where a rule is placed into the least
general class to which it belongs. In addition to
having no generalized composition other than the
reducing variant, it should also be noted that in all
generalized composition rules, X = Y implying
that the reducing class of generalized composition
is a very natural schema class for CCGbank.
If we assume that type-raising is restricted to
those instances occurring in CCGbank4, then a
CCG consisting of schema classes (1-3), (6-7) and
(10-11) can generate all the derivations in CCG-
bank. By proposition 3, such a CCG is strongly
context-free. One could also observe that since
CCGbank is finite, its grammar is not only a
context-free grammar but can produce only a finite
number of derivations. However, our statement is
much stronger because this CCG can generate all
of the derivations in CCGbank given only the lex-
icon, the finite set of unrestricted rules and the fi-
nite number of type-raising rules.
4Without such an assumption, parsing is intractable.
338
Schema Class Rules Instances
Application 519 902176
Composition 102 7189
Crossed Composition 64 14114
Reducing Generalized 50 612
Crossed Composition
Generalized Composition 0 0
Generalized Crossed 0 0
Composition
Substitution 3 4
Type-Raising 27 3996
Unrestricted Rules 642 335011
Total 1407 1263102
Figure 2: The rules of CCGbank by schema class.
The Clark and Curran CCG Parser (Clark and
Curran, 2007) is a CCG parser which uses CCG-
bank as a training corpus. Despite the fact that
there is a strongly context-free CCG which gener-
ates all of the derivations in CCGbank, it is still
possible that the grammar learned by the Clark
and Curran parser is not a context-free grammar.
However, in addition to rule schemata (1-6) and
(10-11) they also include restriction class (A) by
restricting rules to only those found in the train-
ing data5. Thus, by proposition 5, the Clark and
Curran parser is a context-free parser.
3 A Latent Variable CCG Parser
The context-freeness of a number of CCGs should
not be considered evidence that there is no ad-
vantage to CCG as a grammar formalism. Unlike
the context-free grammars extracted from the Penn
treebank, these allow for the categorial semantics
that accompanies any categorial parse and for a
more elegant analysis of linguistic structures such
as extraction and coordination. However, because
we now know that the CCG defined by CCGbank
is strongly context-free, we can use tools from the
CFG parsing community to improve CCG parsing.
To illustrate this point, we train the Petrov
parser (Petrov and Klein, 2007) on CCGbank.
The Petrov parser uses latent variables to refine
a coarse-grained grammar extracted from a train-
ing corpus to a grammar which makes much more
fine-grained syntactic distinctions. For example,
5The Clark and Curran parser has an option, which is dis-
abled by default, for not restricting the rules to those that ap-
pear in the training data. However, they find that this restric-
tion is ?detrimental to neither parser accuracy or coverage?
(Clark and Curran, 2007).
in Petrov?s experiments on the Penn treebank, the
syntactic category NP was refined to the more
fine-grained NP 1 and NP 2 roughly correspond-
ing to NP s in subject and object positions. Rather
than requiring such distinctions to be made in the
corpus, the Petrov parser hypothesizes these splits
automatically.
The Petrov parser operates by performing a
fixed number of iterations of splitting, merging
and smoothing. The splitting process is done
by performing Expectation-Maximization to de-
termine a likely potential split for each syntactic
category. Then, during the merging process some
of the splits are undone to reduce grammar size
and avoid overfitting according to the likelihood
of the split against the training data.
The Petrov parser was chosen for our experi-
ments because it refines the grammar in a mathe-
matically principled way without altering the na-
ture of the derivations that are output. This is
important because the input to the semantic back-
end and the system that converts CCG derivations
to dependencies requires CCG derivations as they
appear in CCGbank.
3.1 Experiments
Our experiments use CCGbank as the corpus and
we use sections 02-21 for training (39603 sen-
tences), 00 for development (1913 sentences) and
23 for testing (2407 sentences).
CCGbank, in addition to the basic atoms S, N ,
NP and PP , also differentiates both the S and
NP atoms with features allowing more subtle dis-
tinctions. For example, declarative sentences are
S[dcl], wh-questions are S[wq] and sentence frag-
ments are S[frg] (Hockenmaier and Steedman,
2007). These features allow finer control of the use
of combinatory rules in the resulting grammars.
However, this fine-grained control is exactly what
the Petrov parser does automatically. Therefore,
we trained the Petrov parser twice, once on the
original version of CCGbank (denoted ?Petrov?)
and once on a version of CCGbank without these
features (denoted ?Petrov no feats?). Furthermore,
we will evaluate the parsers obtained after 0, 4, 5
and 6 training iterations (denoted I-0, I-4, I-5 and
I-6). When we evaluate on sets of sentences for
which not all parsers return an analysis, we report
the coverage (denoted ?Cover?).
We use the evalb package for PARSEVAL
evaluation and a modified version of Clark and
339
Parser Accuracy % No feats %
C&C Normal Form 92.92 93.38
C&C Hybrid 93.06 93.52
Petrov I-5 93.18 93.73
Petrov no feats I-6 - 93.74
Figure 3: Supertagging accuracy on the sentences
in section 00 that receive derivations from the four
parsers shown.
Parser Accuracy % No feats %
C&C Hybrid 92.98 93.43
Petrov I-5 93.10 93.59
Petrov no feats I-6 - 93.62
Figure 4: Supertagging accuracy on the sentences
in section 23 that receive derivations from the
three parsers shown.
Curran?s evaluate script for dependency eval-
uation. To determine statistical significance, we
obtain p-values from Bikel?s randomized parsing
evaluation comparator6, modified for use with tag-
ging accuracy, F-score and dependency accuracy.
3.2 Supertag Evaluation
Before evaluating the parse trees as a whole, we
evaluate the categories assigned to words. In the
supertagging literature, POS tagging and supertag-
ging are distinguished ? POS tags are the tradi-
tional Penn treebank tags (e.g. NN, VBZ and DT)
and supertags are CCG categories. However, be-
cause the Petrov parser trained on CCGbank has
no notion of Penn treebank POS tags, we can only
evaluate the accuracy of the supertags.
The results are shown in figures 3 and 4 where
the ?Accuracy? column shows accuracy of the su-
pertags against the CCGbank categories and the
?No feats? column shows accuracy when features
are ignored. Despite the lack of POS tags in the
Petrov parser, we can see that it performs slightly
better than the Clark and Curran parser. The dif-
ference in accuracy is only statistically significant
between Clark and Curran?s Normal Form model
ignoring features and the Petrov parser trained on
CCGbank without features (p-value = 0.013).
3.3 Constituent Evaluation
In this section we evaluate the parsers using the
traditional PARSEVAL measures which measure
recall, precision and F-score on constituents in
6http://www.cis.upenn.edu/ dbikel/software.html
both labeled and unlabeled versions. In addition,
we report a variant of the labeled PARSEVAL
measures where we ignore the features on the cat-
egories. For reasons of brevity, we report the PAR-
SEVAL measures for all sentences in sections 00
and 23, rather than for sentences of length is less
than 40 or less than 100. The results are essentially
identical for those two sets of sentences.
Figure 5 gives the PARSEVAL measures on sec-
tion 00 for Clark and Curran?s two best models
and the Petrov parser trained on the original CCG-
bank and the version without features after various
numbers of training iterations. Figure 7 gives the
accuracies on section 23.
In the case of Clark and Curran?s hybrid model,
the poor accuracy relative to the Petrov parsers can
be attributed to the fact that this model chooses
derivations based on the associated dependencies
at the expense of constituent accuracy (see section
3.4). In the case of Clark and Curran?s normal
form model, the large difference between labeled
and unlabeled accuracy is primarily due to the mis-
labeling of a small number of features (specifi-
cally, NP[nb] and NP[num]). The labeled accu-
racies without features gives the results when fea-
tures are disregarded.
Due to the similarity of the accuracies and the
difference in the coverage between I-5 of the
Petrov parser on CCGbank and I-6 of the Petrov
parser on CCGbank without features, we reevalu-
ate their results on only those sentences for which
they both return derivations in figures 6 and 8.
These results show that the features in CCGbank
actually inhibit accuracy (to a statistically signifi-
cant degree in the case of unlabeled accuracy on
section 00) when used as training data for the
Petrov parser.
Figure 9 gives a comparison between the Petrov
parser trained on the Penn treebank and on CCG-
bank. These numbers should not be directly com-
pared, but the similarity of the unlabeled measures
indicates that the difference between the structure
of the Penn treebank and CCGbank is not large.7
3.4 Dependency Evaluation
The constituent-based PARSEVAL measures are
simple to calculate from the output of the Petrov
parser but the relationship of the PARSEVAL
7Because punctuation in CCG can have grammatical
function, we include it in our accuracy calculations result-
ing in lower scores for the Petrov parser trained on the Penn
treebank than those reported in Petrov and Klein (2007).
340
Labeled % Labeled no feats % Unlabeled %
Parser R P F R P F R P F Cover
C&C Normal Form 71.14 70.76 70.95 80.66 80.24 80.45 86.16 85.71 85.94 98.95
C&C Hybrid 50.08 49.47 49.77 58.13 57.43 57.78 61.27 60.53 60.90 98.95
Petrov I-0 74.19 74.27 74.23 74.66 74.74 74.70 78.65 78.73 78.69 99.95
Petrov I-4 85.86 85.78 85.82 86.36 86.29 86.32 89.96 89.88 89.92 99.90
Petrov I-5 86.30 86.16 86.23 86.84 86.70 86.77 90.28 90.13 90.21 99.90
Petrov I-6 85.95 85.68 85.81 86.51 86.23 86.37 90.22 89.93 90.08 99.22
Petrov no feats I-0 - - - 72.16 72.59 72.37 76.52 76.97 76.74 99.95
Petrov no feats I-5 - - - 86.67 86.57 86.62 90.30 90.20 90.25 99.90
Petrov no feats I-6 - - - 87.45 87.37 87.41 90.99 90.91 90.95 99.84
Figure 5: Constituent accuracy on all sentences from section 00.
Labeled % Labeled no feats % Unlabeled %
Parser R P F R P F R P F
Petrov I-5 86.56 86.46 86.51 87.10 87.01 87.05 90.43 90.33 90.38
Petrov no feats I-6 - - - 87.45 87.37 87.41 90.99 90.91 90.95
p-value - - - 0.089 0.090 0.088 0.006 0.008 0.007
Figure 6: Constituent accuracy on the sentences in section 00 that receive a derivation from both parsers.
Labeled % Labeled no feats % Unlabeled %
Parser R P F R P F R P F Cover
C&C Normal Form 71.15 70.79 70.97 80.73 80.32 80.53 86.31 85.88 86.10 99.58
Petrov I-5 86.94 86.80 86.87 87.47 87.32 87.39 90.75 90.59 90.67 99.83
Petrov no feats I-6 - - - 87.49 87.49 87.49 90.81 90.82 90.81 99.96
Figure 7: Constituent accuracy on all sentences from section 23.
Labeled % Labeled no feats % Unlabeled %
Parser R P F R P F R P F
Petrov I-5 86.94 86.80 86.87 87.47 87.32 87.39 90.75 90.59 90.67
Petrov no feats I-6 - - - 87.48 87.49 87.49 90.81 90.82 90.81
p-value - - - 0.463 0.215 0.327 0.364 0.122 0.222
Figure 8: Constituent accuracy on the sentences in section 23 that receive a derivation from both parsers.
Labeled % Unlabeled %
Parser R P F R P F Cover
Petrov on PTB I-6 89.65 89.97 89.81 90.80 91.13 90.96 100.00
Petrov on CCGbank I-5 86.94 86.80 86.87 90.75 90.59 90.67 99.83
Petrov on CCGbank no feats I-6 87.49 87.49 87.49 90.81 90.82 90.81 99.96
Figure 9: Constituent accuracy for the Petrov parser on the corpora on all sentences from Section 23.
Mr. Vinken is chairman of Elsevier N.V. , the Dutch publishing group .
N/N N S[dcl]\NP/NP N NP\NP/NP N/N N , NP [nb]/N N/N N/N N .
Figure 10: The argument-functor relations for the CCG derivation in figure 1.
341
Mr. Vinken is chairman of Elsevier N.V. , the Dutch publishing group .
N/N N S[dcl]\NP/NP N NP\NP/NP N/N N , NP [nb]/N N/N N/N N .
Figure 11: The set of dependencies obtained by reorienting the argument-functor edges in figure 10.
Labeled % Unlabeled %
Parser R P F R P F Cover
C&C Normal Form 84.39 85.28 84.83 90.93 91.89 91.41 98.95
C&C Hybrid 84.53 86.20 85.36 90.84 92.63 91.73 98.95
Petrov I-0 79.87 78.81 79.34 87.68 86.53 87.10 96.45
Petrov I-4 84.76 85.27 85.02 91.69 92.25 91.97 96.81
Petrov I-5 85.30 85.87 85.58 92.00 92.61 92.31 96.65
Petrov I-6 84.86 85.46 85.16 91.79 92.44 92.11 96.65
Figure 12: Dependency accuracy on CCGbank dependencies on all sentences from section 00.
Labeled % Unlabeled %
Parser R P F R P F
C&C Hybrid 84.71 86.35 85.52 90.96 92.72 91.83
Petrov I-5 85.50 86.08 85.79 92.12 92.75 92.44
p-value 0.005 0.189 0.187 < 0.001 0.437 0.001
Figure 13: Dependency accuracy on the section 00 sentences that receive an analysis from both parsers.
Labeled % Unlabeled %
Parser R P F R P F
C&C Hybrid 85.11 86.46 85.78 91.15 92.60 91.87
Petrov I-5 85.73 86.29 86.01 92.04 92.64 92.34
p-value 0.013 0.278 0.197 < 0.001 0.404 0.005
Figure 14: Dependency accuracy on the section 23 sentences that receive an analysis from both parsers.
Training Time Parsing Time Training RAM
Parser in CPU minutes in CPU minutes in gigabytes
Clark and Curran Normal Form Model 1152 2 28
Clark and Curran Hybrid Model 2672 4 37
Petrov on PTB I-0 1 5 2
Petrov on PTB I-5 180 20 8
Petrov on PTB I-6 660 21 16
Petrov on CCGbank I-0 1 5 2
Petrov on CCGbank I-4 103 70 8
Petrov on CCGbank I-5 410 600 14
Petrov on CCGbank I-6 2760 2880 24
Petrov on CCGbank no feats I-0 1 5 2
Petrov on CCGbank no feats I-5 360 240 7
Petrov on CCGbank no feats I-6 1980 390 13
Figure 15: Time and space usage when training on sections 02-21 and parsing on section 00.
342
scores to the quality of a parse is not entirely clear.
For this reason, the word to word dependencies
of categorial grammar parsers are often evaluated.
This evaluation is aided by the fact that in addition
to the CCG derivation for each sentence, CCG-
bank also includes a set of dependencies. Fur-
thermore, extracting dependencies from a CCG
derivation is well-established (Clark et al, 2002).
A CCG derivation can be converted into de-
pendencies by, first, determining which arguments
go with which functors as specified by the CCG
derivation. This can be represented as in figure
10. Although this is not difficult, some care must
be taken with respect to punctuation and the con-
junction rules. Next, we reorient some of the
edges according to information in the lexical cat-
egories. A language for specifying these instruc-
tions using variables and indices is given in Clark
et al (2002). This process is shown in figures 1,
10 and 11 with the directions of the dependencies
reversed from Clark et al (2002).
We used the CCG derivation to dependency
converter generate included in the C&C tools
package to convert the output of the Petrov parser
to dependencies. Other than a CCG derivation,
their system requires only the lexicon of edge re-
orientation instructions and methods for convert-
ing the unrestricted rules of CCGbank into the
argument-functor relations. Important for the pur-
pose of comparison, this system does not depend
on their parser.
An unlabeled dependency is correct if the or-
dered pair of words is correct. A labeled depen-
dency is correct if the ordered pair of words is cor-
rect, the head word has the correct category and
the position of the category that is the source of
that edge is correct. Figure 12 shows accuracies
from the Petrov parser trained on CCGbank along
with accuracies for the Clark and Curran parser.
We only show accuracies for the Petrov parser
trained on the original version of CCGbank be-
cause the dependency converter cannot currently
generate dependencies for featureless derivations.
The relatively poor coverage of the Petrov
parser is due to the failure of the dependency con-
verter to output dependencies from valid CCG
derivations. However, the coverage of the depen-
dency converter is actually lower when run on the
gold standard derivations indicating that this cov-
erage problem is not indicative of inaccuracies in
the Petrov parser. Due to the difference in cover-
age, we again evaluate the top two parsers on only
those sentences that they both generate dependen-
cies for and report those results in figures 13 and
14. The Petrov parser has better results by a sta-
tistically significant margin for both labeled and
unlabeled recall and unlabeled F-score.
3.5 Time and Space Evaluation
As a final evaluation, we compare the resources
that are required to both train and parse with the
Petrov parser on the Penn Treebank, the Petrov
parser on the original version of CCGbank, the
Petrov parser on CCGbank without features and
the Clark and Curran parser using the two mod-
els. All training and parsing was done on a 64-bit
machine with 8 dual core 2.8 Ghz Opteron 8220
CPUs and 64GB of RAM. Our training times are
much larger than those reported in Clark and Cur-
ran (2007) because we report the cumulative time
spent on all CPUs rather than the maximum time
spent on a CPU. Figure 15 shows the results.
As can be seen, the Clark and Curran parser
has similar training times, although signifi-
cantly greater RAM requirements than the Petrov
parsers. In contrast, the Clark and Curran parser is
significantly faster than the Petrov parsers, which
we hypothesize to be attributed to the degree
to which Clark and Curran have optimized their
code, their use of C++ as opposed to Java and
their use of a supertagger to prune the lexicon.
4 Conclusion
We have provided a number of theoretical results
proving that CCGbank contains no non-context-
free structure and that the Clark and Curran parser
is actually a context-free parser. Based on these
results, we trained the Petrov parser on CCGbank
and achieved state of the art results in terms of
supertagging accuracy, PARSEVAL measures and
dependency accuracy.
This demonstrates the following. First, the abil-
ity to extract semantic representations from CCG
derivations is not dependent on the language class
of a CCG. Second, using a dedicated supertagger,
as opposed to simply using a general purpose tag-
ger, is not necessary to accurately parse with CCG.
Acknowledgments
We would like to thank Stephen Clark, James Cur-
ran, Jackie C. K. Cheung and our three anonymous
reviewers for their insightful comments.
343
References
J. Baldridge. 2002. Lexically Specified Deriva-
tional Control in Combinatory Categorial Gram-
mar. Ph.D. thesis, University of Edinburgh.
J. Bos, S. Clark, M. Steedman, J. R Curran, and
J. Hockenmaier. 2004. Wide-coverage semantic
representations from a CCG parser. In Proceedings
of COLING, volume 4, page 1240?1246.
S. Clark and J. R. Curran. 2007. Wide-Coverage ef-
ficient statistical parsing with CCG and Log-Linear
models. Computational Linguistics, 33(4):493?552.
S. Clark, J. Hockenmaier, and M. Steedman. 2002.
Building deep dependency structures with a wide-
coverage CCG parser. In Proceedings of the 40th
Meeting of the ACL, page 327?334.
J. Hockenmaier and M. Steedman. 2007. CCGbank:
a corpus of CCG derivations and dependency struc-
tures extracted from the penn treebank. Computa-
tional Linguistics, 33(3):355?396.
F. Hoyt and J. Baldridge. 2008. A logical basis for
the d combinator and normal form in CCG. In Pro-
ceedings of ACL-08: HLT, page 326?334, Colum-
bus, Ohio. Association for Computational Linguis-
tics.
J. Lambek. 1958. The mathematics of sen-
tence structure. American Mathematical Monthly,
65(3):154?170.
S. Petrov and D. Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of NAACL
HLT 2007, page 404?411.
M. Steedman. 2000. The syntactic process. MIT
Press.
K. Vijay-Shanker and D. Weir. 1994. The equivalence
of four extensions of context-free grammars. Math-
ematical Systems Theory, 27(6):511?546.
W. Zielonka. 1981. Axiomatizability of Ajdukiewicz-
Lambek calculus by means of cancellation schemes.
Zeitschrift fur Mathematische Logik und Grundla-
gen der Mathematik, 27:215?224.
344
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1040?1047,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
An Exact A* Method for Deciphering Letter-Substitution Ciphers
Eric Corlett and Gerald Penn
Department of Computer Science
University of Toronto
{ecorlett,gpenn}@cs.toronto.edu
Abstract
Letter-substitution ciphers encode a docu-
ment from a known or hypothesized lan-
guage into an unknown writing system or
an unknown encoding of a known writing
system. It is a problem that can occur in
a number of practical applications, such as
in the problem of determining the encod-
ings of electronic documents in which the
language is known, but the encoding stan-
dard is not. It has also been used in rela-
tion to OCR applications. In this paper, we
introduce an exact method for decipher-
ing messages using a generalization of the
Viterbi algorithm. We test this model on a
set of ciphers developed from various web
sites, and find that our algorithm has the
potential to be a viable, practical method
for efficiently solving decipherment prob-
lems.
1 Introduction
Letter-substitution ciphers encode a document
from a known language into an unknown writ-
ing system or an unknown encoding of a known
writing system. This problem has practical sig-
nificance in a number of areas, such as in reading
electronic documents that may use one of many
different standards to encode text. While this is not
a problem in languages like English and Chinese,
which have a small set of well known standard en-
codings such as ASCII, Big5 and Unicode, there
are other languages such as Hindi in which there
is no dominant encoding standard for the writing
system. In these languages, we would like to be
able to automatically retrieve and display the in-
formation in electronic documents which use un-
known encodings when we find them. We also
want to use these documents for information re-
trieval and data mining, in which case it is impor-
tant to be able to read through them automatically,
without resorting to a human annotator. The holy
grail in this area would be an application to ar-
chaeological decipherment, in which the underly-
ing language?s identity is only hypothesized, and
must be tested. The purpose of this paper, then,
is to simplify the problem of reading documents
in unknown encodings by presenting a new algo-
rithm to be used in their decipherment. Our algo-
rithm operates by running a search over the n-gram
probabilities of possible solutions to the cipher, us-
ing a generalization of the Viterbi algorithm that
is wrapped in an A* search, which determines at
each step which partial solutions to expand. It
is guaranteed to converge on the language-model-
optimal solution, and does not require restarts or
risk falling into local optima. We specifically con-
sider the problem of finding decodings of elec-
tronic documents drawn from the internet, and
we test our algorithm on ciphers drawn from ran-
domly selected pages of Wikipedia. Our testing
indicates that our algorithm will be effective in this
domain.
It may seem at first that automatically decoding
(as opposed to deciphering) a document is a sim-
ple matter, but studies have shown that simple al-
gorithms such as letter frequency counting do not
always produce optimal solutions (Bauer, 2007).
If the text from which a language model is trained
is of a different genre than the plaintext of a cipher,
the unigraph letter frequencies may differ substan-
tially from those of the language model, and so
frequency counting will be misleading. Because
of the perceived simplicity of the problem, how-
ever, little work was performed to understand its
computational properties until Peleg and Rosen-
feld (1979), who developed a method that repeat-
edly swaps letters in a cipher to find a maximum
probability solution. Since then, several different
approaches to this problem have been suggested,
some of which use word counts in the language
to arrive at a solution (Hart, 1994), and some of
1040
which treat the problem as an expectation max-
imization problem (Knight et al, 2006; Knight,
1999). These later algorithms are, however, highly
dependent on their initial states, and require a
number of restarts in order to find the globally op-
timal solution. A further contribution was made by
(Ravi and Knight, 2008), which, though published
earlier, was inspired in part by the method pre-
sented here, first discovered in 2007. Unlike the
present method, however, Ravi and Knight (2008)
treat the decipherment of letter-substitution ci-
phers as an integer programming problem. Clever
though this constraint-based encoding is, their pa-
per does not quantify the massive running times
required to decode even very short documents
with this sort of approach. Such inefficiency indi-
cates that integer programming may simply be the
wrong tool for the job, possibly because language
model probabilities computed from empirical data
are not smoothly distributed enough over the space
in which a cutting-plane method would attempt to
compute a linear relaxation of this problem. In
any case, an exact method is available with a much
more efficient A* search that is linear-time in the
length of the cipher (though still horribly exponen-
tial in the size of the cipher and plain text alpha-
bets), and has the additional advantage of being
massively parallelizable. (Ravi and Knight, 2008)
also seem to believe that short cipher texts are
somehow inherently more difficult to solve than
long cipher texts. This difference in difficulty,
while real, is not inherent, but rather an artefact of
the character-level n-gram language models that
they (and we) use, in which preponderant evidence
of differences in short character sequences is nec-
essary for the model to clearly favour one letter-
substitution mapping over another. Uniform char-
acter models equivocate regardless of the length of
the cipher, and sharp character models with many
zeroes can quickly converge even on short ciphers
of only a few characters. In the present method,
the role of the language model can be acutely per-
ceived; both the time complexity of the algorithm
and the accuracy of the results depend crucially on
this characteristic of the language model. In fact,
we must use add-one smoothing to decipher texts
of even modest lengths because even one unseen
plain-text letter sequence is enough to knock out
the correct solution. It is likely that the method
of (Ravi and Knight, 2008) is sensitive to this as
well, but their experiments were apparently fixed
on a single, well-trained model.
Applications of decipherment are also explored
by (Nagy et al, 1987), who uses it in the con-
text of optical character recognition (OCR). The
problem we consider here is cosmetically related
to the ?L2P? (letter-to-phoneme) mapping prob-
lem of text-to-speech synthesis, which also fea-
tures a prominent constraint-based approach (van
den Bosch and Canisius, 2006), but the constraints
in L2P are very different: two different instances
of the same written letter may legitimately map to
two different phonemes. This is not the case in
letter-substitution maps.
2 Terminology
Substitution ciphers are ciphers that are defined
by some permutation of a plaintext alphabet. Ev-
ery character of a plaintext string is consistently
mapped to a single character of an output string
using this permutation. For example, if we took
the string ?hello world? to be the plaintext, then
the string ?ifmmp xpsme? would be a cipher
that maps e to f , l to m, and so on. It is easy
to extend this kind of cipher so that the plaintext
alphabet is different from the ciphertext alphabet,
but still stands in a one to one correspondence to
it. Given a ciphertext C, we say that the set of
characters used inC is the ciphertext alphabet ?C ,
and that its size is nC . Similarly, the entire possi-
ble plaintext alphabet is ?P , and its size is is nP .
Since nC is the number of letters actually used
in the cipher, rather than the entire alphabet it is
sampled from, we may find that nC < nP even
when the two alphabets are the same. We refer to
the length of the cipher string C as clen. In the
above example, ?P is { , a, . . . z} and nP = 27,
while ?C = { , e, f, i,m, p, s, x}, clen = 11 and
nC = 8.
Given the ciphertext C, we say that a partial
solution of size k is a map ? = {p1 : c1, . . . pk :
ck}, where c1, . . . , ck ? ?C and are distinct, and
p1, . . . , pk ? ?P and are distinct, and where k ?
nC . If for a partial solution ??, we have that ? ?
??, then we say that ?? extends ?. If the size of ?? is
k+1 and ? is size k, we say that ?? is an immediate
extension of ?. A full solution is a partial solution
of size nC . In the above example, ?1 = { : , d :
e} would be a partial solution of size 2, and ?2 =
{ : , d : e, g : m} would be a partial solution
of size 3 that immediately extends ?1. A partial
solution ?T { : , d : e, e : f, h : i, l : m, o :
1041
p, r : s, w : x} would be both a full solution and
the correct one. The full solution ?T extends ?1
but not ?2.
Every possible full solution to a cipher C will
produce a plaintext string with some associated
language model probability, and we will consider
the best possible solution to be the one that gives
the highest probability. For the sake of concrete-
ness, we will assume here that the language model
is a character-level trigram model. This plain-
text can be found by treating all of the length clen
strings S as being the output of different charac-
ter mappings from C. A string S that results from
such a mapping is consistent with a partial solu-
tion ? iff, for every pi : ci ? ?, the character posi-
tions of C that map to pi are exactly the character
positions with ci in C.
In our above example, we had C =
?ifmmp xpsme?, in which case we had
clen = 11. So mappings from C to
?hhhhh hhhhh? or ? hhhhhhhhhh? would
be consistent with a partial solution of size 0,
while ?hhhhh hhhhn? would be consistent with
the size 2 partial solution ? = { : , n : e}.
3 The Algorithm
In order to efficiently search for the most likely so-
lution for a ciphertext C, we conduct a search of
the partial solutions using their trigram probabil-
ities as a heuristic, where the trigram probability
of a partial solution ? of length k is the maximum
trigram probability over all strings consistent with
it, meaning, in particular, that ciphertext letters not
in its range can be mapped to any plaintext letter,
and do not even need to be consistently mapped to
the same plaintext letter in every instance. Given
a partial solution ? of length n, we can extend ?
by choosing a ciphertext letter c not in the range
of ?, and then use our generalization of the Viterbi
algorithm to find, for each p not in the domain of
?, a score to rank the choice of p for c, namely the
trigram probability of the extension ?p of ?. If we
start with an empty solution and iteratively choose
the most likely remaining partial solution in this
way, storing the extensions obtained in a priority
heap as we go, we will eventually reach a solution
of size nC . Every extension of ? has a probabil-
ity that is, at best, equal to that of ?, and every
partial solution receives, at worst, a score equal
to its best extension, because the score is poten-
tially based on an inconsistent mapping that does
not qualify as an extension. These two observa-
tions taken together mean that one minus the score
assigned by our method constitutes a cost function
over which this score is an admissible heuristic in
the A* sense. Thus the first solution of size nC
will be the best solution of size nC .
The order by which we add the letters c to par-
tial solutions is the order of the distinct cipher-
text letters in right-to-left order of their final oc-
currence in C. Other orderings for the c, such as
most frequent first, are also possible though less
elegant.1
Algorithm 1 Search Algorithm
Order the letters c1 . . . cnC by rightmost occur-
rence in C, rnC < . . . < r1.
Create a priority queue Q for partial solutions,
ordered by highest probability.
Push the empty solution ?0 = {} onto the
queue.
while Q is not empty do
Pop the best partial solution ? from Q.
s = |?|.
if s = nC then
return ?
else
For all p not in the range of ?, push the
immediate extension ?p onto Q with the
score assigned to table cell G(rs+1, p, p)
by GVit(?, cs+1, rs+1) if it is non-zero.
end if
end while
Return ?Solution Infeasible?.
Our generalization of the Viterbi algorithm, de-
picted in Figure 1, uses dynamic programming to
score every immediate extension of a given partial
solution in tandem, by finding, in a manner con-
sistent with the real Viterbi algorithm, the most
probable input string given a set of output sym-
bols, which in this case is the cipher C. Unlike the
real Viterbi algorithm, we must also observe the
constraints of the input partial solution?s mapping.
1We have experimented with the most frequent first regi-
men as well, and it performs worse than the one reported here.
Our hypothesis is that this is due to the fact that the most fre-
quent character tends to appear in many high-frequency tri-
grams, and so our priority queue becomes very long because
of a lack of low-probability trigrams to knock the scores of
partial solutions below the scores of the extensions of their
better scoring but same-length peers. A least frequent first
regimen has the opposite problem, in which their rare oc-
currence in the ciphertext provides too few opportunities to
potentially reduce the score of a candidate.
1042
A typical decipherment involves multiple runs of
this algorithm, each of which scores all of the im-
mediate extensions, both tightening and lowering
their scores relative to the score of the input par-
tial solution. A call GVit(?, c, r) manages this by
filling in a table G such that for all 1 ? i ? r, and
l, k ? ?P , G(i, l, k) is the maximum probability
over every plaintext string S for which:
? len(S) = i,
? S[i] = l,
? for every p in the domain of ?, every 1 ? j ?
i, if C[j] = ?(p) then S[j] = p, and
? for every position 1 ? j ? i, if C[j] = c,
then S[j] = k.
The real Viterbi algorithm lacks these final two
constraints, and would only store a single cell at
G(i, l). There, G is called a trellis. Ours is larger,
so so we will refer to G as a greenhouse.
The table is completed by filling in the columns
from i = 1 to clen in order. In every column i,
we will iterate over the values of l and over the
values of k such that k : c and l : are consistent
with ?. Because we are using a trigram character
model, the cells in the first and second columns
must be primed with unigram and bigram proba-
bilities. The remaining probabilities are calculated
by searching through the cells from the previous
two columns, using the entry at the earlier column
to indicate the probability of the best string up to
that point, and searching through the trigram prob-
abilities over two additional letters. Backpointers
are necessary to reference one of the two language
model probabilities. Cells that would produce in-
consistencies are left at zero, and these as well as
cells that the language model assigns zero to can
only produce zero entries in later columns.
In order to decrease the search space, we add the
further restriction that the solutions of every three
character sequence must be consistent: if the ci-
phertext indicates that two adjacent letters are the
same, then only the plaintext strings that map the
same letter to each will be considered. The num-
ber of letters that are forced to be consistent is
three because consistency is enforced by remov-
ing inconsistent strings from consideration during
trigram model evaluation.
Because every partial solution is only obtained
by extending a solution of size one less, and ex-
tensions are only made in a predetermined order
of cipher alphabet letters, every partial solution is
only considered / extended once.
GVit is highly parallelizable. The nP ?nP cells
of every column i do not depend on each other ?
only on the cells of the previous two columns i?1
and i?2, as well as the language model. In our im-
plementation of the algorithm, we have written the
underlying program in C/C++, and we have used
the CUDA library developed for NVIDIA graphics
cards to in order to implement the parallel sections
of the code.
4 Experiment
The above algorithm is designed for application to
the transliteration of electronic documents, specif-
ically, the transliteration of websites, and it has
been tested with this in mind. In order to gain re-
alistic test data, we have operated on the assump-
tion that Wikipedia is a good approximation of the
type of language that will be found in most inter-
net articles. We sampled a sequence of English-
language articles from Wikipedia using their ran-
dom page selector, and these were used to create
a set of reference pages. In order to minimize the
common material used in each page, only the text
enclosed by the paragraph tags of the main body of
the pages were used. A rough search over internet
articles has shown that a length of 1000 to 11000
characters is a realistic length for many articles, al-
though this can vary according to the genre of the
page. Wikipedia, for example, does have entries
that are one sentence in length. We have run two
groups of tests for our algorithm. In the first set
of tests, we chose the mean of the above lengths
to be our sample size, and we created and decoded
10 ciphers of this size (i.e., different texts, same
size). We made these cipher texts by appending
the contents of randomly chosen Wikipedia pages
until they contained at least 6000 characters, and
then using the first 6000 characters of the result-
ing files as the plaintexts of the cipher. The text
length was rounded up to the nearest word where
needed. In the second set of tests, we used a single
long ciphertext, and measured the time required
for the algorithm to finish a number of prefixes of
it (i.e., same text, different sizes). The plaintext for
this set of tests was developed in the same way as
the first set, and the input ciphertext lengths con-
sidered were 1000, 3500, 6000, 8500, 11000, and
13500 characters.
1043
Greenhouse Array
(a) (b) (c) (d)
...
l
m
n
...
z
l w ? ? ? y t g ? ? ? g u
? ? ? e f g ? ? ? z
Figure 1: Filling the Greenhouse Table. Each cell in the greenhouse is indexed by a plaintext letter and
a character from the cipher. Each cell consists of a smaller array. The cells in the array give the best
probabilities of any path passing through the greenhouse cell, given that the index character of the array
maps to the character in column c, where c is the next ciphertext character to be fixed in the solution. The
probability is set to zero if no path can pass through the cell. This is the case, for example, in (b) and (c),
where the knowledge that ? ? maps to ? ? would tell us that the cells indicated in gray are unreachable.
The cell at (d) is filled using the trigram probabilities and the probability of the path at starting at (a).
In all of the data considered, the frequency of
spaces was far higher than that of any other char-
acter, and so in any real application the character
corresponding to the space can likely be guessed
without difficulty. The ciphers we have consid-
ered have therefore been simplified by allowing
the knowledge of which character corresponds to
the space. It appears that Ravi and Knight (2008)
did this as well. Our algorithm will still work with-
out this assumption, but would take longer. In the
event that a trigram or bigram would be found in
the plaintext that was not counted in the language
model, add one smoothing was used.
Our character-level language model used was
developed from the first 1.5 million characters of
the Wall Street Journal section of the Penn Tree-
bank corpus. The characters used in the lan-
guage model were the upper and lower case let-
ters, spaces, and full stops; other characters were
skipped when counting the frequencies. Further-
more, the number of sequential spaces allowed
was limited to one in order to maximize context
and to eliminate any long stretches of white space.
As discussed in the previous paragraph, the space
character is assumed to be known.
When testing our algorithm, we judged the time
complexity of our algorithm by measuring the ac-
tual time taken by the algorithm to complete its
runs, as well as the number of partial solutions
placed onto the queue (?enqueued?), the number
popped off the queue (?expanded?), and the num-
ber of zero-probability partial solutions not en-
queued (?zeros?) during these runs. These latter
numbers give us insight into the quality of trigram
probabilities as a heuristic for the A* search.
We judged the quality of the decoding by mea-
suring the percentage of characters in the cipher
alphabet that were correctly guessed, and also the
word error rate of the plaintext generated by our
solution. The second metric is useful because a
low probability character in the ciphertext may be
guessed wrong without changing as much of the
actual plaintext. Counting the actual number of
word errors is meant as an estimate of how useful
or readable the plaintext will be. We did not count
the accuracy or word error rate for unfinished ci-
phers.
We would have liked to compare our results
with those of Ravi and Knight (2008), but the
method presented there was simply not feasible
1044
Algorithm 2 Generalized Viterbi Algorithm
GVit(?, c, r)
Input: partial solution ?, ciphertext character c,
and index r into C.
Output: greenhouse G.
Initialize G to 0.
i = 1
for All (l, k) such that ? ? {k : c, l : Ci} is
consistent do
G(i, l, k) = P (l).
end for
i = 2
for All (l, k) such that ? ? {k : c, l : Ci} is
consistent do
for j such that ? ? {k : c, l : Ci, j : Ci?1} is
consistent do
G(i, l, k) = max(G(i, l, k), G(0, j, k)?
P (l|j))
end for
end for
i = 3
for (l, k) such that ? ? {k : c, l : Ci} is consis-
tent do
for j1, j2 such that ??{k : c, j2 : C[i?2], j1 :
C[i? 1], l : Ci} is consistent do
G(i, l, k) = max(G(i, l, k), G(i?2, j2, k)
? P (j1|j2)? P (l|j2j1)).
end for
end for
for i = 4 to r do
for (l, k) such that ? ? {k : c, l : Ci} is con-
sistent do
for j1, j2 such that ? ? {k : c, j2 :
C[i?2], j1 : C[i?1], l : Ci} is consistent
do
G(i, l, k) = max(G(i, l, k),
G(i?2, j2, k)?P (j1|j2j2(back))
? P (l|j2j1)).
end for
end for
end for
on texts and (case-sensitive) alphabets of this size
with the computing hardware at our disposal.
5 Results
In our first set of tests, we measured the time con-
sumption and accuracy of our algorithm over 10
ciphers taken from random texts that were 6000
characters long. The time values in these tables are
given in the format of (H)H:MM:SS. For this set
of tests, in the event that a test took more than 12
hours, we terminated it and listed it as unfinished.
This cutoff was set in advance of the runs based
upon our armchair speculation about how long one
might at most be reasonably expected to wait for
a web-page to be transliterated (an overnight run).
The results from this run appear in Table 1. All
running times reported in this section were ob-
tained on a computer running Ubuntu Linux 8.04
with 4 GB of RAM and 8 ? 2.5 GHz CPU cores.
Column-level subcomputations in the greenhouse
were dispatched to an NVIDIA Quadro FX 1700
GPU card that is attached through a 16-lane PCI
Express adapter. The card has 512 MB of cache
memory, a 460 MHz core processor and 32 shader
processors operating in parallel at 920 MHz each.
In our second set of tests, we measured the time
consumption and accuracy of our algorithm over
several prefixes of different lengths of a single
13500-character ciphertext. The results of this run
are given in Table 2.
The first thing to note in this data is that the ac-
curacy of this algorithm is above 90 % for all of
the test data, and 100% on all but the smallest 2
ciphers. We can also observe that even when there
are errors (e.g., in the size 1000 cipher), the word
error rate is very small. This is a Zipf?s Law effect
? misclassified characters come from poorly at-
tested character trigrams, which are in turn found
only in longer, rarer words. The overall high ac-
curacy is probably due to the large size of the
texts relative to the uniticity distance of an En-
glish letter-substitution cipher (Bauer, 2007). The
results do show, however, that character trigram
probabilities are an effective indicator of the most
likely solution, even when the language model and
test data are from very different genres (here, the
Wall Street Journal and Wikipedia, respectively).
These results also show that our algorithm is ef-
fective as a way of decoding simple ciphers. 80%
of our runs finished before the 12 hour cutoff in
the first experiment.
1045
Cipher Time Enqueued Expanded Zeros Accuracy Word Error Rate
1 2:03:06 964 964 44157 100% 0%
2 0:13:00 132 132 5197 100% 0%
3 0:05:42 91 91 3080 100% 0%
4 Unfinished N/A N/A N/A N/A N/A
5 Unfinished N/A N/A N/A N/A N/A
6 5:33:50 2521 2521 114283 100% 0%
7 6:02:41 2626 2626 116392 100% 0%
8 3:19:17 1483 1483 66070 100% 0%
9 9:22:54 4814 4814 215086 100% 0%
10 1:23:21 950 950 42107 100% 0%
Table 1: Time consumption and accuracy on a sample of 10 6000-character texts.
Size Time Enqueued Expanded Zeros Accuracy Word Error Rate
1000 40:06:05 119759 119755 5172631 92.59% 1.89%
3500 0:38:02 615 614 26865 96.30% 0.17%
6000 0:12:34 147 147 5709 100% 0%
8500 8:52:25 1302 1302 60978 100% 0%
11000 1:03:58 210 210 8868 100% 0%
13500 0:54:30 219 219 9277 100% 0%
Table 2: Time consumption and accuracy on prefixes of a single 13500-character ciphertext.
As far as the running time of the algorithm goes,
we see a substantial variance: from a few minutes
to several hours for most of the longer ciphers, and
that there are some that take longer than the thresh-
old we gave in the experiment. Specifically, there
is substantial variability in the the running times
seen.
Desiring to reduce the variance of the running
time, we look at the second set of tests for possible
causes. In the second test set, there is a general
decrease in both the running time and the number
of solutions expanded as the length of the ciphers
increases. Running time correlates very well with
A* queue size.
Asymptotically, the time required for each
sweep of the Viterbi algorithm increases, but this
is more than offset by the decrease in the number
of required sweeps.
The results, however, do not show that running
time monotonically decreases with length. In par-
ticular, the length 8500 cipher generates more so-
lutions than the length 3500 or 6000 ones. Recall
that the ciphers in this section are all prefixes of
the same string. Because the algorithm fixes char-
acters starting from the end of the cipher, these
prefixes have very different character orderings,
c1, . . . , cnC , and thus a very different order of par-
tial solutions. The running time of our algorithm
depends very crucially on these initial conditions.
Perhaps most interestingly, we note that the
number of enqueued partial solutions is in ev-
ery case identical or nearly identical to the num-
ber of partial solutions expanded. From a the-
oretical perspective, we must also remember the
zero-probability solutions, which should in a sense
count when judging the effectiveness of our A*
heuristic. Naturally, these are ignored by our im-
plementation because they are so badly scored
that they could never be considered. Neverthe-
less, what these numbers show is that scores based
on character-level trigrams, while theoretically ad-
missible, are really not all that clever when it
comes to navigating through the search space of
all possible letter substitution ciphers, apart from
their very keen ability at assigning zeros to a
large number of partial solutions. A more com-
plex heuristic that can additionally rank non-zero
probability solutions with more prescience would
likely make a very great difference to the running
time of this method.
1046
6 Conclusions
In the above paper, we have presented an algo-
rithm for solving letter-substitution ciphers, with
an eye towards discovering unknown encoding
standards in electronic documents on the fly. In
a test of our algorithm over ciphers drawn from
Wikipedia, we found its accuracy to be 100% on
the ciphers that it solved within a threshold of 12
hours, this being 80% of the total attempted. We
found that the running time of our algorithm is
highly variable depending on the order of char-
acters attempted, and, due to the linear-time the-
oretical complexity of this method, that running
times tend to decrease with larger ciphertexts due
to our character-level language model?s facility at
eliminating highly improbable solutions. There is,
however, a great deal of room for improvement in
the trigram model?s ability to rank partial solutions
that are not eliminated outright.
Perhaps the most valuable insight gleaned from
this study has been on the role of the language
model. This algorithm?s asymptotic runtime com-
plexity is actually a function of entropic aspects of
the character-level language model that it uses ?
more uniform models provide less prominent sep-
arations between candidate partial solutions, and
this leads to badly ordered queues, in which ex-
tended partial solutions can never compete with
partial solutions that have smaller domains, lead-
ing to a blind search. We believe that there is a
great deal of promise in characterizing natural lan-
guage processing algorithms in this way, due to the
prevalence of Bayesian methods that use language
models as priors.
Our approach makes no explicit attempt to ac-
count for noisy ciphers, in which characters are
erroneously mapped, nor any attempt to account
for more general substitution ciphers in which a
single plaintext (resp. ciphertext) letter can map to
multiple ciphertext (resp. plaintext) letters, nor for
ciphers in which ciphertext units corresponds to
larger units of plaintext such syllables or words.
Extensions in these directions are all very worth-
while to explore.
References
Friedrich L. Bauer. 2007. Decrypted Secrets.
Springer-Verlag, Berlin Heidelberg.
George W. Hart. 1994. To Decode Short Cryptograms.
Communications of the ACM, 37(9): 102?108.
Kevin Knight. 1999. Decoding Complexity in Word-
Replacement Translation Models. Computational
Linguistics, 25(4):607?615.
Kevin Knight, Anish Nair, Nishit Rathod, Kenji Ya-
mada. Unsupervised Analysis for Decipherment
Problems. Proceedings of the COLING/ACL 2006,
2006, 499?506.
George Nagy, Sharad Seth, Kent Einspahr. 1987.
Decoding Substitution Ciphers by Means of Word
Matching with Application to OCR. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence,
9(5):710?715.
Shmuel Peleg and Azriel Rosenfeld. 1979. Breaking
Substitution Ciphers Using a Relaxation Algorithm.
Communications of the ACM, 22(11):589?605.
Sujith Ravi, Kevin Knight. 2008. Attacking Decipher-
ment Problems Optimally with Low-Order N-gram
Models Proceedings of the ACL 2008, 812?819.
Antal van den Bosch, Sander Canisius. 2006. Im-
proved Morpho-phonological Sequence Processing
with Constraint Satisfaction Inference Proceedings
of the Eighth Meeting of the ACL Special Interest
Group on Computational Phonology at HLT-NAACL
2006, 41?49.
1047
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1512?1521,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
A Generalized-Zero-Preserving Method for Compact Encoding of
Concept Lattices
Matthew Skala
School of Computer Science
University of Waterloo
mskala@cs.toronto.edu
Victoria Krakovna
Ja?nos Krama?r
Dept. of Mathematics
University of Toronto
{vkrakovna,jkramar}@gmail.com
Gerald Penn
Dept. of Computer Science
University of Toronto
gpenn@cs.toronto.edu
Abstract
Constructing an encoding of a concept lat-
tice using short bit vectors allows for ef-
ficient computation of join operations on
the lattice. Join is the central operation
any unification-based parser must support.
We extend the traditional bit vector encod-
ing, which represents join failure using the
zero vector, to count any vector with less
than a fixed number of one bits as failure.
This allows non-joinable elements to share
bits, resulting in a smaller vector size. A
constraint solver is used to construct the
encoding, and a variety of techniques are
employed to find near-optimal solutions
and handle timeouts. An evaluation is pro-
vided comparing the extended representa-
tion of failure with traditional bit vector
techniques.
1 Introduction
The use of bit vectors is almost as old as HPSG
parsing itself. Since they were first suggested in
the programming languages literature (A??t-Kaci et
al., 1989) as a method for computing the unifica-
tion of two types without table lookup, bit vectors
have been attractive because of three speed advan-
tages:
? The classical bit vector encoding uses bitwise
AND to calculate type unification. This is
hard to beat.
? Hash tables, the most common alternative,
involve computing the Dedekind-MacNeille
completion (DMC) at compile time if the in-
put type hierarchy is not a bounded-complete
partial order. That is exponential time in the
worst case; most bit vector methods avoid ex-
plicitly computing it.
? With large type signatures, the table that in-
dexes unifiable pairs of types may be so large
that it pushes working parsing memory into
swap. This loss of locality of reference costs
time.
Why isn?t everyone using bit vectors? For the
most part, the reason is their size. The classical
encoding given by A??t-Kaci et al (1989) is at least
as large as the number of meet-irreducible types,
which in the parlance of HPSG type signatures
is the number of unary-branching types plus the
number of maximally specific types. For the En-
glish Resource Grammar (ERG) (Copestake and
Flickinger, 2000), these are 314 and 2474 respec-
tively. While some systems use them nonetheless
(PET (Callmeier, 2000) does, as a very notable ex-
ception), it is clear that the size of these codes is a
source of concern.
Again, it has been so since the very beginning:
A??t-Kaci et al (1989) devoted several pages to
a discussion of how to ?modularize? type codes,
which typically achieves a smaller code in ex-
change for a larger-time operation than bitwise
AND as the implementation of type unification.
However, in this and later work on the subject
(e.g. (Fall, 1996)), one constant has been that we
know our unification has failed when the imple-
mentation returns the zero vector. Zero preserva-
tion (Mellish, 1991; Mellish, 1992), i.e., detect-
ing a type unification failure, is just as important
as obtaining the right answer quickly when it suc-
ceeds.
The approach of the present paper borrows
from recent statistical machine translation re-
search, which addresses the problem of efficiently
representing large-scale language models using a
mathematical construction called a Bloom filter
(Talbot and Osborne, 2007). The approach is best
combined with modularization in order to further
reduce the size of the codes, but its novelty lies in
1512
the observation that counting the number of one
bits in an integer is implemented in the basic in-
struction sets of many CPUs. The question then
arises whether smaller codes would be obtained
by relaxing zero preservation so that any resulting
vector with at most ? bits is interpreted as failure,
with ? ? 1.
Penn (2002) generalized join-preserving encod-
ings of partial orders to the case where more than
one code can be used to represent the same ob-
ject, but the focus there was on codes arising from
successful unifications; there was still only one
representative for failure. To our knowledge, the
present paper is the first generalization of zero
preservation in CL or any other application do-
main of partial order encodings.
We note at the outset that we are not using
Bloom filters as such, but rather a derandomized
encoding scheme that shares with Bloom filters
the essential insight that ? can be greater than zero
without adverse consequences for the required al-
gebraic properties of the encoding. Deterministic
variants of Bloom filters may in turn prove to be
of some value in language modelling.
1.1 Notation and definitions
A partial order ?X,v? consists of a set X and a
reflexive, antisymmetric, and transitive binary re-
lation v. We use u unionsq v to denote the unique least
upper bound or join of u, v ? X , if one exists, and
u u v for the greatest lower bound or meet. If we
need a second partial order, we use  for its order
relation and g for its join operation. We are espe-
cially interested in a class of partial orders called
meet semilattices, in which every pair of elements
has a unique meet. In a meet semilattice, the join
of two elements is unique when it exists at all, and
there is a unique globally least element ? (?bot-
tom?).
A successor of an element u ? X is an element
v 6= u ? X such that u v v and there is now ? X
with w 6= u,w 6= v, and u v w v v, i.e., v fol-
lows u in X with no other elements in between. A
maximal element has no successor. A meet irre-
ducible element is an element u ? X such that for
any v, w ? X , if u = v uw then u = v or u = w.
A meet irreducible has at most one successor.
Given two partial orders ?X,v? and ?Y,?, an
embedding of X into Y is a pair of functions
f : X ? Y and g : (Y ? Y ) ? {0, 1}, which
may have some of the following properties for all
u, v ? X:
u v v ? f(u)  f(v) (1)
defined(u unionsq v)? g(f(u), f(v)) = 1 (2)
?defined(u unionsq v)? g(f(u), f(v)) = 0 (3)
u unionsq v = w ? f(u) g f(v) = f(w) (4)
With property (1), the embedding is said to pre-
serve order; with property (2), it preserves suc-
cess; with property (3), it preserves failure; and
with property (4), it preserves joins.
2 Bit-vector encoding
Intuitively, taking the join of two types in a type hi-
erarchy is like taking the intersection of two sets.
Types often represent sets of possible values, and
the type represented by the join really does repre-
sent the intersection of the sets that formed the in-
put. So it seems natural to embed a partial order of
types ?X,v? into a partial order (in fact, a lattice)
of sets ?Y,?, where Y is the power set of some
set Z, and  is the superset relation ?. Then join
g is simply set intersection ?. The embedding
function g, which indicates whether a join exists,
can be naturally defined by g(f(u), f(v)) = 0 if
and only if f(u) ? f(v) = ?. It remains to choose
the underlying set Z and embedding function f .
A??t-Kaci et al (1989) developed what has be-
come the standard technique of this type. They
set Z to be the set of all meet irreducible elements
in X; and f(u) = {v ? Z|v w u}, that is, the
meet irreducible elements greater than or equal to
u. The resulting embedding preserves order, suc-
cess, failure, and joins. If Z is chosen to be the
maximal elements of X instead, then join preser-
vation is lost but the embedding still preserves or-
der, success, and failure. The sets can be repre-
sented efficiently by vectors of bits. We hope to
minimize the size of the largest set f(?), which
determines the vector length.
It follows from the work of Markowsky (1980)
that the construction of A??t-Kaci et al is optimal
among encodings that use sets with intersection
for meet and empty set for failure: with Y defined
as the power set of some setZ,v as?, unionsq as?, and
g(f(u), f(v)) = 0 if and only if f(u)? f(v) = ?,
then the smallest Z that will preserve order, suc-
cess, failure, and joins is the set of all meet irre-
ducible elements of X . No shorter bit vectors are
possible.
We construct shorter bit vectors by modifying
the definition of g, so that the minimality results
1513
no longer apply. In the following discussion we
present first an intuitive and then a technical de-
scription of our approach.
2.1 Intuition from Bloom filters
Vectors generated by the above construction tend
to be quite sparse, or if not sparse, at least bor-
ing. Consider a meet semilattice containing only
the bottom element ? and n maximal elements all
incomparable to each other. Then each bit vector
would consist of either all ones, or all zeroes ex-
cept for a single one. We would thus be spending
n bits to represent a choice among n + 1 alterna-
tives, which should fit into a logarithmic number
of bits. The meet semilattices that occur in prac-
tice are more complicated than this example, but
they tend to contain things like it as a substruc-
ture. With the traditional bit vector construction,
each of the maximal elements consumes its own
bit, even though those bits are highly correlated.
The well-known technique called Bloom fil-
tering (Bloom, 1970) addresses a similar issue.
There, it is desired to store a large array of bits
subject to two considerations. First, most of the
bits are zeroes. Second, we are willing to accept
a small proportion of one-sided errors, where ev-
ery query that should correctly return one does so,
but some queries that should correctly return zero
might actually return one instead.
The solution proposed by Bloom and widely
used in the decades since is to map the entries in
the large bit array pseudorandomly (by means of
a hash function) into the entries of a small bit ar-
ray. To store a one bit we find its hashed location
and store it there. If we query a bit for which the
answer should be zero but it happens to have the
same hashed location as another query with the an-
swer one, then we return a one and that is one of
our tolerated errors.
To reduce the error rate we can elaborate the
construction further: with some fixed k, we use
k hash functions to map each bit in the large array
to several locations in the small one. Figure 1 il-
lustrates the technique with k = 3. Each bit has
three hashed locations. On a query, we check all
three; they must all contain ones for the query to
return a one. There will be many collisions of indi-
vidual hashed locations, as shown; but the chances
are good that when we query a bit we did not in-
tend to store in the filter, at least one of its hashed
locations will still be empty, and so the query will
1 1 1 1 1
1 1 1
1
?1
Figure 1: A Bloom filter
return zero. Bloom describes how to calculate the
optimal value of k, and the necessary length of
the hashed array, to achieve any desired bound on
the error rate. In general, the hashed array can
be much smaller than the original unhashed ar-
ray (Bloom, 1970).
Classical Bloom filtering applied to the sparse
vectors of the embedding would create some per-
centage of incorrect join results, which would then
have to be handled by other techniques. Our work
described here combines the idea of using k hash
functions to reduce the error rate, with perfect
hashes designed in a precomputation step to bring
the error rate to zero.
2.2 Modified failure detection
In the traditional bit vector construction, types
map to sets, join is computed by intersection of
sets, and the empty set corresponds to failure
(where no join exists). Following the lead of
Bloom filters, we change the embedding function
g(f(u), f(v)) to be 0 if and only if |f(u)?f(v)| ?
? for some constant ?. With ? = 0 this is the same
as before. Choosing greater values of ? allows us
to re-use set elements in different parts of the type
hierarchy while still avoiding collisions.
Figure 2 shows an example meet semilattice. In
the traditional construction, to preserve joins we
must assign one bit to each of the meet-irreducible
elements {d, e, f, g, h, i, j, k, l,m}, for a total of
ten bits. But we can use eight bits and still pre-
serve joins by setting g(f(u), f(v)) = 0 if and
only if |f(u) ? f(v)| ? ? = 1, and f as follows.
f(?) = {1, 2, 3, 4, 5, 6, 7, 8}
f(a) = {1, 2, 3, 4, 5}
f(b) = {1, 6, 7, 8} f(c) = {1, 2, 3}
f(d) = {2, 3, 4, 5} f(e) = {1, 6}
f(f) = {1, 7} f(g) = {1, 8}
f(h) = {6, 7} f(i) = {6, 8}
f(j) = {1, 2} f(k) = {1, 3}
f(l) = {2, 3} f(m) = {2, 3, 4}
(5)
1514
ac d
b
e f g h i
j k l
m
Figure 2: An example meet semilattice; ? is the
most general type.
As a more general example, consider the very
simple meet semilattice consisting of just a least
element ? with n maximal elements incompara-
ble to each other. For a given ? we can represent
this in b bits by choosing the smallest b such that
( b
?+1
)
? n and assigning each maximal element a
distinct choice of the bits. With optimal choice of
?, b is logarithmic in n.
2.3 Modules
As A??t-Kaci et al (1989) described, partial or-
ders encountered in practice often resemble trees.
Both their technique and ours are at a disadvantage
when applied to large trees; in particular, if the
bottom of the partial order has successors which
are not joinable with each other, then those will be
assigned large sets with little overlap, and bits in
the vectors will tend to be wasted.
To avoid wasting bits, we examine the partial
order X in a precomputation step to find the mod-
ules, which are the smallest upward-closed sub-
sets of X such that for any x ? X , if x has at
least two joinable successors, then x is in a mod-
ule. This is similar to ALE?s definition of mod-
ule (Penn, 1999), but not the same. The definition
of A??t-Kaci et al (1989) also differs from ours.
Under our definition, every module has a unique
least element, and not every type is in a module.
For instance, in Figure 2, the only module has a
as its least element. In the ERG?s type hierarchy,
there are 11 modules, with sizes ranging from 10
to 1998 types.
To find the join of two types in the same mod-
ule, we find the intersection of their encodings and
check whether it is of size greater than ?. If the
types belong to two distinct modules, there is no
join. For the remaining cases, where at least one of
the types lacks a module, we observe that the mod-
ule bottoms and non-module types form a tree, and
the join can be computed in that tree. If x is a type
in the module whose bottom is y, and z has no
module, then x unionsq z = y unionsq z unless y unionsq z = y
in which case x unionsq z = x; so it only remains to
compute joins within the tree. Our implementa-
tion does that by table lookup. More sophisticated
approaches could be appropriate on larger trees.
3 Set programming
Ideally, we would like to have an efficient algo-
rithm for finding the best possible encoding of any
given meet semilattice. The encoding can be rep-
resented as a collection of sets of integers (repre-
senting bit indices that contain ones), and an opti-
mal encoding is the collection of sets whose over-
all union is smallest subject to the constraint that
the collection forms an encoding at all. This com-
binatorial optimization problem is a form of set
programming; and set programming problems are
widely studied. We begin by defining the form of
set programming we will use.
Definition 1 Choose set variables S1, S2, . . . , Sn
to minimize b = |
?n
i=1 Si| subject to some con-
straints of the forms |Si| ? ri, Si ? Sj , Si + Sj ,
|Si ? Sj | ? ?, and Si ? Sj = Sk. The constant
? is the same for all constraints. Set elements may
be arbitrary, but we generally assume they are the
integers {1 . . . b} for convenience.
The reduction of partial order representation to
set programming is clear: we create a set variable
for every type, force the maximal types? sets to
contain at least ? + 1 elements, and then use sub-
set to enforce that every type is a superset of all
its successors (preserving order and success). We
limit the maximum intersection of incomparable
types to preserve failure. To preserve joins, if that
property is desired, we add a constraint Si + Sj
for every pair of types xi 6v xj and one of the
form Si ? Sj = Sk for every xi, xj , xk such that
xi unionsq xj = xk..
Given a constraint satisfaction problem like this
one, we can ask two questions: is there a feasi-
ble solution, assigning values to the variables so
all constraints are satisfied; and if so what is the
optimal solution, producing the best value of the
objective while remaining feasible? In our prob-
lem, there is always a feasible solution we can
find by the generalized A??t-Kaci et al construc-
tion (GAK), which consists of assigning ? bits
1515
shared among all types; adding enough unshared
new bits to maximal elements to satisfy cardinal-
ity constraints; adding one new bit to each non-
maximal meet irreducible type; and propagating
all the bits down the hierarchy to satisfy the subset
constraints. Since the GAK solution is feasible, it
provides a useful upper bound on the result of the
set programming.
Ongoing research on set programming has pro-
duced a variety of software tools for solving these
problems. However, at first blush our instances are
much too large for readily-available set program-
ming tools. Grammars like ERG contain thou-
sands of types. We use binary constraints be-
tween every pair of types, for a total of millions
of constraints?and these are variables and con-
straints over a domain of sets, not integers or re-
als. General-purpose set programming software
cannot handle such instances.
3.1 Simplifying the instances
First of all, we only use minimum cardinality con-
straints |Si| ? ri for maximal types; and every
ri ? ? + 1. Given a feasible bit assignment for a
maximal type with more than ri elements in its set
Si, we can always remove elements until it has ex-
actly ri elements, without violating the other con-
straints. As a result, instead of using constraints
|Si| ? ri we can use constraints |Si| = ri. Doing
so reduces the search space.
Subset is transitive; so if we have constraints
Si ? Sj and Sj ? Sk, then Si ? Sk is implied
and we need not specify it as a constraint. Simi-
larly, if we have Si ? Sj and Si * Sk, then we
have Sj * Sk. Furthermore, if Si and Sj have
maximum intersection ?, then any subset of Si
also has maximum intersection ? with any subset
of Sk, and we need not specify those constraints
either.
Now, let a choke-vertex in the partial order
?X,v? be an element u ? X such that for ev-
ery v, w ? X where v is a successor of w and
u v v, we have u v w. That is, any chain of suc-
cessors from elements not after u to elements after
u, must pass through u. Figure 2 shows choke-
vertices as squares. We call these choke-vertices
by analogy with the graph theoretic concept of
cut-vertices in the Hasse diagram of the partial or-
der; but note that some vertices (like j and k) can
be choke-vertices without being cut-vertices, and
some vertices (like c) can be cut-vertices without
being choke-vertices. Maximal and minimal ele-
ments are always choke-vertices.
Choke-vertices are important because the op-
timal bit assignment for elements after a choke-
vertex u is almost independent of the bit assign-
ment elsewhere in the partial order. Removing
the redundant constraints means there are no con-
straints between elements after u and elements
before, or incomparable with, u. All constraints
across u must involve u directly. As a result, we
can solve a smaller instance consisting of u and
everything after it, to find the minimal number of
bits ru for representing u. Then we solve the rest
of the problem with a constraint |Su| = ru, ex-
cluding all partial order elements after u, and then
combine the two solutions with any arbitrary bi-
jection between the set elements assigned to u in
each solution. Assuming optimal solutions to both
sub-problems, the result is an optimal solution to
the original problem.
3.2 Splitting into components
If we cut the partial order at every choke-vertex,
we reduce the huge and impractical encoding
problem to a collection of smaller ones. The cut-
ting expresses the original partial order as a tree
of components, each of which corresponds to a set
programming instance. Components are shown by
the dashed lines in Figure 2. We can find an op-
timal encoding for the entire partial order by opti-
mally encoding the components, starting with the
leaves of that tree and working our way back to the
root.
The division into components creates a collec-
tion of set programming instances with a wide
range of sizes and difficulty; we examine each in-
stance and choose appropriate techniques for each
one. Table 1 summarizes the rules used to solve an
instance, and shows the number of times each rule
was applied in a typical run with the modules ex-
tracted from ERG, a ten-minute timeout, and each
? from 0 to 10.
In many simple cases, GAK is provably opti-
mal. These include when ? = 0 regardless of the
structure of the component; when the component
consists of a bottom and zero, one, or two non-
joinable successors; and when there is one element
(a top) greater than all other elements in the com-
ponent. We can easily recognize these cases and
apply GAK to them.
Another important special case is when the
1516
Condition Succ. Fail. Method
? = 0 216 GAK (optimal)
? top 510 GAK (optimal)
2 successors 850 GAK (optimal)
3 or 4
successors
70 exponential
variable
only ULs 420 b-choose-(?+1)
special case
before UL
removal
251 59 ic_sets
after UL
removal
9 50 ic_sets
remaining 50 GAK
Table 1: Rules for solving an instance in the ERG
component consists of a bottom and some num-
ber k of pairwise non-joinable successors, and the
successors all have required cardinality ? + 1.
Then the optimal encoding comes from finding the
smallest b such that
( b
?+1
)
is at least k, and giving
each successor a distinct combination of the b bits.
3.3 Removing unary leaves
For components that do not have one of the spe-
cial forms described above, it becomes necessary
to solve the set programming problem. Some of
our instances are small enough to apply constraint
solving software directly; but for larger instances,
we have one more technique to bring them into the
tractable range.
Definition 2 A unary leaf (UL) is an element x in
a partial order ?X,v? such that x is maximal and
x is the successor of exactly one other element.
ULs are special because their set programming
constraints always take a particular form: if x is a
UL and a successor of y, then the constraints on
its set Sx are exactly that |Sx| = ? + 1, Sx ? Sy,
and Sx has intersection of size at most ? with the
set for any other successor of y. Other constraints
disappear by the simplifications described earlier.
Furthermore, ULs occur frequently in the par-
tial orders we consider in practice; and by increas-
ing the number of sets in an instance, they have
a disproportionate effect on the difficulty of solv-
ing the set programming problem. We therefore
implement a special solution process for instances
containing ULs: we remove them all, solve the re-
sulting instance, and then add them back one at a
time while attempting to increase the overall num-
ber of elements as little as possible.
This process of removing ULs, solving, and
adding them back in, may in general produce sub-
optimal solutions, so we use it only when the
solver cannot find a solution on the full-sized prob-
lem. In practical experiments, the solver gener-
ally either produces an optimal or very nearly op-
timal solution within a time limit on the order of
ten minutes; or fails to produce a feasible solu-
tion at all, even with a much longer limit. Testing
whether it finds a solution is then a useful way to
determine whether UL removal is worthwhile.
Recall that in an instance consisting of k ULs
and a bottom, an optimal solution consists of find-
ing the smallest b such that
( b
?+1
)
is at least k; that
is the number of bits for the bottom, and we can
choose any k distinct subsets of size ?+ 1 for the
ULs. Augmenting an existing solution to include
additional ULs involves a similar calculation.
To add a UL x as the successor of an element
y without increasing the total number of bits, we
must find a choice of ? + 1 of the bits already as-
signed to y, sharing at most ? bits with any of y?s
other successors. Those successors are in general
sets of arbitrary size, but all that matters for as-
signing x is how many subsets of size ? + 1 they
already cover. The UL can use any such subset
not covered by an existing successor of y. Our al-
gorithm counts the subsets already covered, and
compares that with the number of choices of ?+1
bits from the bits assigned to y. If enough choices
remain, we use them; otherwise, we add bits until
there are enough choices.
3.4 Solving
For instances with a small number of sets and rela-
tively large number of elements in the sets, we use
an exponential variable solver. This encodes the
set programming problem into integer program-
ming. For each element x ? {1, 2, . . . , b}, let
c(x) = {i|x ? Si}; that is, c(x) represents the
indices of all the sets in the problem that contain
the element x. There are 2n ? 1 possible values
of c(x), because each element must be in at least
one set. We create an integer variable for each of
those values. Each element is counted once, so the
sum of the integer variables is b. The constraints
translate into simple inequalities on sums of the
variables; and the system of constraints can be
solved with standard integer programming tech-
niques. After solving the integer programming
problem we can then assign elements arbitrarily
1517
to the appropriate combinations of sets.
Where applicable, the exponential variable ap-
proach works well, because it breaks all the sym-
metries between set elements. It also continues to
function well even when the sets are large, since
nothing in the problem directly grows when we
increase b. The wide domains of the variables
may be advantageous for some integer program-
ming solvers as well. However, it creates an in-
teger programming problem of size exponential in
the number of sets. As a result, it is only applica-
ble to instances with a very few set variables.
For more general set programming instances,
we feed the instance directly into a solver de-
signed for such problems. We used the ECLiPSe
logic programming system (Cisco Systems, 2008),
which offers several set programming solvers as
libraries, and settled on the ic sets library. This
is a straightforward set programming solver based
on containment bounds. We extended the solver
by adding a lightweight not-subset constraint, and
customized heuristics for variable and value selec-
tion designed to guide the solver to a feasible so-
lution as soon as possible. We choose variables
near the top of the instance first, and prefer to as-
sign values that share exactly ? bits with exist-
ing assigned values. We also do limited symme-
try breaking, in that whenever we assign a bit not
shared with any current assignment, the choice of
bit is arbitrary so we assume it must be the lowest-
index bit. That symmetry breaking speeds up the
search significantly.
The present work is primarily on the benefits
of nonzero ?, and so a detailed study of gen-
eral set programming techniques would be inap-
propriate; but we made informal tests of several
other set-programming solvers. We had hoped that
a solver using containment-lexicographic hybrid
bounds as described by Sadler and Gervet (Sadler
and Gervet, 2008) would offer good performance,
and chose the ECLiPSe framework partly to gain
access to its ic hybrid sets implementation of such
bounds. In practice, however, ic hybrid sets gave
consistently worse performance than ic sets (typi-
cally by an approximate factor of two). It appears
that in intuitive terms, the lexicographic bounds
rarely narrowed the domains of variables much un-
til the variables were almost entirely labelled any-
way, at which point containment bounds were al-
most as good; and meanwhile the increased over-
head of maintaining the extra bounds slowed down
the entire process to more than compensate for
the improved propagation. We also evaluated the
Cardinal solver included in ECLiPSe, which of-
fers stronger propagation of cardinality informa-
tion; it lacked other needed features and seemed
no more efficient than ic sets. Among these
three solvers, the improvements associated with
our custom variable and value heuristics greatly
outweighed the baseline differences between the
solvers; and the differences were in optimization
time rather than quality of the returned solutions.
Solvers with available source code were pre-
ferred for ease of customization, and free solvers
were preferred for economy, but a license for
ILOG CPLEX (IBM, 2008) was available and we
tried using it with the natural encoding of sets as
vectors of binary variables. It solved small in-
stances to optimality in time comparable to that
of ECLiPSe. However, for medium to large in-
stances, CPLEX proved impractical. An instance
with n sets of up to b bits, dense with pairwise
constraints like subset and maximum intersection,
requires ?(n2b) variables when encoded into in-
teger programming in the natural way. CPLEX
stores a copy of the relaxed problem, with signifi-
cant bookkeeping information per variable, for ev-
ery node in the search tree. It is capable of storing
most of the tree in compressed form on disk, but in
our larger instances even a single node is too large;
CPLEX exhausts memory while loading its input.
The ECLiPSe solver also stores each set variable
in a data structure that increases linearly with the
number of elements, so that the size of the prob-
lem as stored by ECLiPSe is also ?(n2b); but the
constant for ECLiPSe appears to be much smaller,
and its search algorithm stores only incremental
updates (with nodes per set instead of per element)
on a stack as it explores the tree. As a result, the
ECLiPSe solver can process much larger instances
than CPLEX without exhausting memory.
Encoding into SAT would allow use of the so-
phisticated solvers available for that problem. Un-
fortunately, cardinality constraints are notoriously
difficult to encode in Boolean logic. The obvi-
ous encoding of our problem into CNFSAT would
require O(n2b?) clauses and variables. Encod-
ings into Boolean variables with richer constraints
than CNFSAT (we tried, for instance, the SICS-
tus Prolog clp(FD) implementation (Carlsson et
al., 1997)) generally exhausted memory on much
smaller instances than those handled by the set-
1518
Module n b0 ? b?
mrs_min 10 7 0 7
conj 13 8 1 7
list 27 15 1 11
local_min 27 21 1 10
cat_min 30 17 1 14
individual 33 15 0 15
head_min 247 55 0 55
*sort* 247 129 3 107
synsem_min 612 255 0 255
sign_min 1025 489 3 357
mod_relation 1998 1749 6 284
entire ERG 4305 2788 140 985
Table 2: Best encodings of the ERG and its mod-
ules: n is number of types, b0 is vector length with
? = 0, and ? is parameter that gives the shortest
vector length b?.
variable solvers, while offering no improvement
in speed.
4 Evaluation
Table 2 shows the size of our smallest encodings
to date for the entire ERG without modularization,
and for each of its modules. These were found
by running the optimization process of the previ-
ous section on Intel Xeon servers with a timeout
of 30 minutes for each invocation of the solver
(which may occur several times per module). Un-
der those conditions, some modules take a long
time to optimize?as much as two hours per tested
value of ? for sign_min. The Xeon?s hyper-
threading feature makes reproducibility of timing
results difficult, but we found that results almost
never improved with additional time allowance be-
yond the first few seconds in any case, so the prac-
tical effect of the timing variations should be min-
imal.
These results show some significant improve-
ments in vector length for the larger modules.
However, they do not reveal the entire story. In
particular, the apparent superiority of ? = 0 for
the synsem_min module should not be taken
as indicating that no higher ? could be better:
rather, that module includes a very difficult set
programming instance on which the solver failed
and fell back to GAK. For the even larger modules,
nonzero ? proved helpful despite solver failures,
because of the bits saved by UL removal. UL re-
moval is clearly a significant advantage, but only
Encoding length time space
Lookup table n/a 140 72496
Modular, best ? 0?357 321 203
Modular, ? = 0 0?1749 747 579
Non-mod, ? = 0 2788 4651 1530
Non-mod, ? = 1 1243 2224 706
Non-mod, ? = 2 1140 2008 656
Non-mod, ? = 9 1069 1981 622
Non-mod, ? = 140 985 3018 572
Table 3: Query performance. Vector length in bits,
time in milliseconds, space in Kbytes.
for the modules where the solver is failing any-
way. One important lesson seems to be that further
work on set programming solvers would be bene-
ficial: any future more capable set programming
solver could be applied to the unsolved instances
and would be expected to save more bits.
Table 3 and Figure 3 show the performance of
the join query with various encodings. These re-
sults are from a simple implementation in C that
tests all ordered pairs of types for joinability. As
well as testing the non-modular ERG encoding for
different values of ?, we tested the modularized
encoding with ? = 0 for all modules (to show the
effect of modularization alone) and with ? cho-
sen per-module to give the shortest vectors. For
comparison, we also tested a simple lookup table.
The same implementation sufficed for all these
tests, by means of putting all types in one mod-
ule for the non-modular bit vectors or no types
in any module for the pure lookup table. The
times shown are milliseconds of user CPU time
to test all join tests (roughly 18.5 million of them),
on a non-hyperthreading Intel Pentium 4 with a
clock speed of 2.66GHz and 1G of RAM, run-
ning Linux. Space consumption shown is the total
amount of dynamically-allocated memory used to
store the vectors and lookup table.
The non-modular encoding with ? = 0 is the
basic encoding of A??t-Kaci et al (1989). As Ta-
ble 3 shows, we achieved more than a factor of
two improvement from that, in both time and vec-
tor length, just by setting ? = 1. Larger values
offered further small improvements in length up to
? = 140, which gave the minimum vector length
of 985. That is a shallow minimum; both ? = 120
and ? = 160 gave vector lengths of 986, and the
length slowly increased with greater ?.
However, the fastest bit-count on this architec-
1519
 1500
 2000
 2500
 3000
 3500
 4000
 4500
 5000
 0  50  100  150  200
u
se
r 
CP
U
 ti
m
e 
(m
s)
lambda (bits)
Figure 3: Query performance for the ERG without modularization.
ture, using a technique first published by Weg-
ner (1960), requires time increasing with the num-
ber of nonzero bits it counts; and a similar effect
would appear on a word-by-word basis even if we
used a constant-time per-word count. As a result,
there is a time cost associated with using larger ?,
so that the fastest value is not necessarily the one
that gives the shortest vectors. In our experiments,
? = 9 gave the fastest joins for the non-modular
encoding of the ERG. As shown in Figure 3, all
small nonzero ? gave very similar times.
Modularization helps a lot, both with ? = 0,
and when we choose the optimal ? per module.
Here, too, the use of optimal ? improves both time
and space by more than a factor of two. Our best
bit-vector encoding, the modularized one with per-
module optimal ?, is only a little less than half
the speed of the lookup table; and this test favours
the lookup table by giving it a full word for every
entry (no time spent shifting and masking bits) and
testing the pairs in a simple two-level loop (almost
purely sequential access).
5 Conclusion
We have described a generalization of conven-
tional bit vector concept lattice encoding tech-
niques to the case where all vectors with ? or fewer
one bits represent failure; traditional encodings are
the case ? = 0. Increasing ? can reduce the over-
all storage space and improve speed.
A good encoding requires a kind of perfect
hash, the design of which maps naturally to con-
straint programming over sets of integers. We
have described a practical framework for solving
the instances of constraint programming thus cre-
ated, in which we can apply existing or future con-
straint solvers to the subproblems for which they
are best suited; and a technique for modularizing
practical type hierarchies to get better value from
the bit vector encodings. We have evaluated the re-
sulting encodings on the ERG?s type system, and
examined the performance of the associated unifi-
cation test. Modularization, and the use of nonzero
?, each independently provide significant savings
in both time and vector length.
The modified failure detection concept suggests
several directions for future work, including eval-
uation of the new encodings in the context of a
large-scale HPSG parser; incorporation of further
developments in constraint solvers; and the possi-
bility of approximate encodings that would permit
one-sided errors as in traditional Bloom filtering.
References
Hassan A??t-Kaci, Robert S. Boyer, Patrick Lincoln, and
Roger Nasr. 1989. Efficient implementation of lat-
tice operations. ACM Transactions on Programming
Languages and Systems, 11(1):115?146, January.
1520
Burton H. Bloom. 1970. Space/time trade-offs in hash
coding with allowable errors. Communications of
the ACM, 13(7):422?426, July.
Ulrich Callmeier. 2000. PET ? a platform for ex-
perimentation with efficient HPSG processing tech-
niques. Natural Language Engineering, 6(1):99?
107.
Mats Carlsson, Greger Ottosson, and Bjo?rn Carlson.
1997. An open-ended finite domain constraint
solver. In H. Glaser, P. Hartel, and H. Kucken, ed-
itors, Programming Languages: Implementations,
Logics, and Programming, volume 1292 of Lec-
ture Notes in Computer Science, pages 191?206.
Springer-Verlag, September.
Cisco Systems. 2008. ECLiPSe 6.0. Computer soft-
ware. Online http://eclipse-clp.org/.
Ann Copestake and Dan Flickinger. 2000. An
open-source grammar development environment
and broad-coverage English grammar using HPSG.
In Proceedings of the Second Conference on Lan-
guage Resources and Evaluation (LREC 2000).
Andrew Fall. 1996. Reasoning with Taxonomies.
Ph.D. thesis, Simon Fraser University.
IBM. 2008. ILOG CPLEX 11. Computer software.
George Markowsky. 1980. The representation of
posets and lattices by sets. Algebra Universalis,
11(1):173?192.
Chris Mellish. 1991. Graph-encodable description
spaces. Technical report, University of Edinburgh
Department of Artificial Intelligence. DYANA De-
liverable R3.2B.
Chris Mellish. 1992. Term-encodable description
spaces. In D.R. Brough, editor, Logic Program-
ming: New Frontiers, pages 189?207. Kluwer.
Gerald Penn. 1999. An optimized prolog encoding of
typed feature structures. In D. De Schreye, editor,
Logic programming: proceedings of the 1999 Inter-
national Conference on Logic Programming (ICLP),
pages 124?138.
Gerald Penn. 2002. Generalized encoding of descrip-
tion spaces and its application to typed feature struc-
tures. In Proceedings of the 40th Annual Meeting of
the Association for Computational Linguistics (ACL
2002), pages 64?71.
Andrew Sadler and Carmen Gervet. 2008. Enhanc-
ing set constraint solvers with lexicographic bounds.
Journal of Heuristics, 14(1).
David Talbot and Miles Osborne. 2007. Smoothed
Bloom filter language models: Tera-scale LMs on
the cheap. In Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL), pages 468?476.
Peter Wegner. 1960. A technique for counting ones
in a binary computer. Communications of the ACM,
3(5):322.
1521
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 392?401,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Probabilistic Domain Modelling With Contextualized Distributional
Semantic Vectors
Jackie Chi Kit Cheung
University of Toronto
10 King?s College Rd., Room 3302
Toronto, ON, Canada M5S 3G4
jcheung@cs.toronto.edu
Gerald Penn
University of Toronto
10 King?s College Rd., Room 3302
Toronto, ON, Canada M5S 3G4
gpenn@cs.toronto.edu
Abstract
Generative probabilistic models have been
used for content modelling and template
induction, and are typically trained on
small corpora in the target domain. In
contrast, vector space models of distribu-
tional semantics are trained on large cor-
pora, but are typically applied to domain-
general lexical disambiguation tasks. We
introduce Distributional Semantic Hidden
Markov Models, a novel variant of a hid-
den Markov model that integrates these
two approaches by incorporating contex-
tualized distributional semantic vectors
into a generative model as observed emis-
sions. Experiments in slot induction show
that our approach yields improvements in
learning coherent entity clusters in a do-
main. In a subsequent extrinsic evalua-
tion, we show that these improvements are
also reflected in multi-document summa-
rization.
1 Introduction
Detailed domain knowledge is crucial to many
NLP tasks, either as an input for language un-
derstanding, or as the goal itself, to acquire such
knowledge. For example, in information extrac-
tion, a list of slots in the target domain is given
to the system, and in natural language generation,
content models are trained to learn the content
structure of texts in the target domain for infor-
mation structuring and automatic summarization.
Generative probabilistic models have been one
popular approach to content modelling. An impor-
tant advantage of this approach is that the structure
of the model can be adapted to fit the assumptions
about the structure of the domain and the nature
of the end task. As this field has progressed, the
formal structures that are assumed to represent a
domain have increased in complexity and become
more hierarchical. Earlier work assumes a flat set
of topics (Barzilay and Lee, 2004), which are ex-
pressed as states of a latent random variable in the
model. Later work organizes topics into a hierar-
chy from general to specific (Haghighi and Van-
derwende, 2009; Celikyilmaz and Hakkani-Tur,
2010). Recently, Cheung et al (2013) formalized
a domain as a set of frames consisting of proto-
typical sequences of events, slots, and slot fillers
or entities, inspired by classical AI work such as
Schank and Abelson?s (1977) scripts. We adopt
much of this terminology in this work. For exam-
ple, in the CRIMINAL INVESTIGATIONS domain,
there may be events such as a murder, an investi-
gation of the crime, an arrest, and a trial. These
would be indicated by event heads such as kill, ar-
rest, charge, plead. Relevant slots would include
VICTIM, SUSPECT, AUTHORITIES, PLEA, etc.
One problem faced by this line of work is that,
by their nature, these models are typically trained
on a small corpus from the target domain, on the
order of hundreds of documents. The small size of
the training corpus makes it difficult to estimate re-
liable statistics, especially for more powerful fea-
tures such as higher-order N-gram features or syn-
tactic features.
By contrast, distributional semantic models are
trained on large, domain-general corpora. These
methods model word meaning using the contexts
in the training corpus in which the word appears.
The most popular approach today is a vector space
representation, in which each dimension corre-
sponds to some context word, and the value at that
dimension corresponds to the strength of the as-
sociation between the context word and the target
word being modelled. A notion of word similarity
arises naturally from these models by comparing
the similarity of the word vectors, for example by
using a cosine measure. Recently, these models
have been extended by considering how distribu-
392
tional representations can be modified depending
on the specific context in which the word appears
(Mitchell and Lapata, 2008, for example). Con-
textualization has been found to improve perfor-
mance in tasks like lexical substitution and word
sense disambiguation (Thater et al, 2011).
In this paper, we propose to inject contextual-
ized distributional semantic vectors into genera-
tive probabilistic models, in order to combine their
complementary strengths for domain modelling.
There are a number of potential advantages that
distributional semantic models offer. First, they
provide domain-general representations of word
meaning that cannot be reliably estimated from the
small target-domain corpora on which probabilis-
tic models are trained. Second, the contextualiza-
tion process allows the semantic vectors to implic-
itly encode disambiguated word sense and syntac-
tic information, without further adding to the com-
plexity of the generative model.
Our model, the Distributional Semantic Hidden
Markov Model (DSHMM), incorporates contextu-
alized distributional semantic vectors into a gen-
erative probabilistic model as observed emissions.
We demonstrate the effectiveness of our model in
two domain modelling tasks. First, we apply it to
slot induction on guided summarization data over
five different domains. We show that our model
outperforms a baseline version of our method that
does not use distributional semantic vectors, as
well as a recent state-of-the-art template induction
method. Then, we perform an extrinsic evaluation
using multi-document summarization, wherein we
show that our model is able to learn event and slot
topics that are appropriate to include in a sum-
mary. From a modelling perspective, these results
show that probabilistic models for content mod-
elling and template induction benefit from distri-
butional semantics trained on a much larger cor-
pus. From the perspective of distributional seman-
tics, this work broadens the variety of problems to
which distributional semantics can be applied, and
proposes methods to perform inference in a prob-
abilistic setting beyond geometric measures such
as cosine similarity.
2 Related Work
Probabilistic content models were proposed by
Barzilay and Lee (2004), and related models have
since become popular for summarization (Fung
and Ngai, 2006; Haghighi and Vanderwende,
2009), and information ordering (Elsner et al,
2007; Louis and Nenkova, 2012). Other related
generative models include topic models and struc-
tured versions thereof (Blei et al, 2003; Gruber
et al, 2007; Wallach, 2008). In terms of domain
learning in the form of template induction, heuris-
tic methods involving multiple clustering steps
have been proposed (Filatova et al, 2006; Cham-
bers and Jurafsky, 2011). Most recently, Cheung
et al (2013) propose PROFINDER, a probabilis-
tic model for frame induction inspired by content
models. Our work is similar in that we assume
much of the same structure within a domain and
consequently in the model as well (Section 3), but
whereas PROFINDER focuses on finding the ?cor-
rect? number of frames, events, and slots with a
nonparametric method, this work focuses on in-
tegrating global knowledge in the form of distri-
butional semantics into a probabilistic model. We
adopt one of their evaluation procedures and use it
to compare with PROFINDER in Section 5.
Vector space models form the basis of modern
information retrieval (Salton et al, 1975), but only
recently have distributional models been proposed
that are compositional (Mitchell and Lapata, 2008;
Clark et al, 2008; Grefenstette and Sadrzadeh,
2011, inter alia), or that contextualize the meaning
of a word using other words in the same phrase
(co-compositionality) (Erk and Pado?, 2008; Dinu
and Lapata, 2010; Thater et al, 2011). We re-
cently showed how such models can be evaluated
for their ability to support semantic inference for
use in complex NLP tasks like question answering
or automatic summarization (Cheung and Penn,
2012).
Combining distributional information and prob-
abilistic models has actually been explored in pre-
vious work. Usually, an ad-hoc clustering step
precedes training and is used to bias the initializa-
tion of the probabilistic model (Barzilay and Lee,
2004; Louis and Nenkova, 2012), or the clustering
is interleaved with iterations of training (Fung et
al., 2003). By contrast, our method better modu-
larizes the two, and provides a principled way to
train the model. More importantly, previous ad-
hoc clustering methods only use distributional in-
formation derived from the target domain itself;
initializing based on domain-general distributional
information can be problematic because it can bias
training towards a local optimum that is inappro-
priate for the target domain, leading to poor per-
393
?1 
?1 
?1 
?1 
?1 
? 
?? 
?? 
?? 
?? 
?? 
. . .  
?? ????
?  ??????  ??  ????
?  ??????  
Figure 1: Graphical representation of our model.
Distributions that generate the latent variables and
hyperparameters are omitted for clarity.
formance.
3 Distributional Semantic Hidden
Markov Models
We now describe the DSHMM model. This model
can be thought of as an HMM with two layers
of latent variables, representing events and slots
in the domain. Given a document consisting of
a sequence of T clauses headed by propositional
heads ~H (verbs or event nouns), and argument
noun phrases ~A, a DSHMM models the joint prob-
ability of observations ~H , ~A, and latent random
variables ~E and ~S representing domain events and
slots respectively; i.e., P ( ~H, ~A, ~E, ~S).
The basic structure of our model is similar to
PROFINDER. Each timestep in the model gener-
ates one clause in the document. More specifi-
cally, it generates the event heads and arguments
which are crucial in identifying events and slots.
We assume that event heads are verbs or event
nouns, while arguments are the head words of their
syntactically dependent noun phrases. We also as-
sume that the sequence of clauses and the clause-
internal syntactic structure are fixed, for example
by applying a dependency parser. Within each
clause, a hierarchy of latent and observed variables
maps to corresponding elements in the clause (Ta-
ble 1), as follows:
Event Variables At the top-level, a categorical
latent variable Et with NE possible states repre-
sents the event that is described by clause t. Its
value is conditioned on the previous time step?s
event variable, following the standard, first-order
Markov assumption (PE(Et|Et?1), or PEinit(E1)
Node Component Textual unit
Et Event Clause
Sta Slot Noun phrase
Ht Event head Verb/event noun
Ata Event argument Noun phrase
Table 1: The correspondence between nodes in our
graphical model, the domain components that they
model, and the related elements in the clause.
for the first clause). The internal structure of the
clause is generated by conditioning on the state of
Et, including the head of the clause, and the slots
for each argument in the clause.
Slot Variables Categorical latent variables with
NS possible states represent the slot that an argu-
ment fills, and are conditioned on the event vari-
able in the clause, Et (i.e., PS(Sta|Et), for the
ath slot variable). The state of Sta is then used to
generate an argument Ata.
Head and Argument Emissions The head of
the clause Ht is conditionally dependent on Et,
and each argument Ata is likewise conditioned on
its slot variable Sta. Unlike in most applications of
HMMs in text processing, in which the represen-
tation of a token is simply its word or lemma iden-
tity, tokens in DSHMM are also associated with a
vector representation of their meaning in context
according to a distributional semantic model (Sec-
tion 3.1). Thus, the emissions can be decomposed
into pairs Ht = (lemma(Ht), sem(Ht)) and
Ata = (lemma(Ata), sem(Ata)), where lemma
and sem are functions that return the lemma iden-
tity and the semantic vector respectively. The
probability of the head of a clause is thus:
PH(Ht|Et) = PHlemm(lemma(Ht)|Et) (1)
? PHsem(sem(Ht)|Et),
and the probability of a clausal argument is like-
wise:
PA(Ata|Sta) = PAlemm(lemma(Ata)|Sta) (2)
? PAsem(sem(Ata)|Sta).
All categorical distributions are smoothed using
add-? smoothing (i.e., uniform Dirichlet priors).
Based on the independence assumptions described
above, the joint probability distribution can be fac-
394
tored into:
P ( ~H, ~A, ~E, ~S) = PEinit(E1) (3)
?
T?
t=2
PE(Et|Et?1)
T?
t=1
PH(Ht|Et)
?
T?
t=1
Ct?
a=1
PS(Sta|Et)PA(Ata|Sta).
3.1 Vector Space Models of Semantics
In this section, we describe several methods for
producing the semantic vectors associated with
each event head or argument; i.e., the function
sem. We chose several simple, but widely studied
models, to investigate whether they can be effec-
tively integrated into DSHMM. We start with a de-
scription of the training of a basic model without
any contextualization, then describe several con-
textualized models based on recent work.
Simple Vector Space Model In the basic ver-
sion of the model (SIMPLE), we train a term-
context matrix, where rows correspond to target
words, and columns correspond to context words.
Training begins by counting context words that ap-
pear within five words of the target word, ignor-
ing stopwords. We then convert the raw counts
to positive pointwise mutual information scores,
which has been shown to improve word similarity
correlation results (Turney and Pantel, 2010). We
set thresholds on the frequencies of words for in-
clusion as target and context words (given in Sec-
tion 4). Target words which fall below the thresh-
old are modelled as UNK. All the methods below
start from this basic vector representation.
Component-wise Operators Mitchell and Lap-
ata (2008) investigate using component-wise op-
erators to combine the vectors of verbs and their
intransitive subjects. We use component-wise op-
erators to contextualize our vectors, but by com-
bining with all of the arguments, and regardless
of the event head?s category. Let event head h
be the syntactic head of a number of arguments
a1, a2, ...am, and ~vh, ~va1 , ~va2 , ...~vam be their re-
spective vector representations according to the
SIMPLE method. Then, their contextualized vec-
tors ~cM&Lh ,~cM&La1 , ...~cM&Lam would be:
~cM&Lh = ~vh  (
m?
i=1
~vam) (4)
~cM&Lai = ~vai  ~vh,?i = 1...m, (5)
where  represents a component-wise operator,
addition or multiplication, and ? represents its
repeated application. We tested component-wise
addition (M&L+) and multiplication (M&L?).
Selectional Preferences Erk and Pado? (2008)
(E&P) incorporate inverse selectional preferences
into their contextualization function. The intu-
ition is that a word should be contextualized such
that its vector representation becomes more sim-
ilar to the vectors of other words that its depen-
dency neighbours often take in the same syntactic
position. For example, suppose catch is the head
of the noun ball, in the relation of a direct object.
Then, the vector for ball would be contextualized
to become similar to the vectors for other frequent
direct objects of catch, such as baseball, or cold.
Likewise, the vector for catch would be contextu-
alized to become similar to the vectors for throw,
hit, etc. Formally, let h take a as its argument in
relation r. Then:
~cE&Ph = ~vh ?
m?
i=1
?
w?L
freq(w, r, ai) ? ~vw, (6)
~cE&Pa = ~va ?
?
w?L
freq(h, r, w) ? ~vw, (7)
where freq(h, r, a) is the frequency of h occur-
ring as the head of a in relation r in the train-
ing corpus, L is the lexicon, and ? represents
component-wise multiplication.
Dimensionality Reduction and Vector Emission
After contextualization, we apply singular value
decomposition (SVD) for dimensionality reduc-
tion to reduce the number of model parameters,
keeping the k most significant singular values and
vectors. In particular, we apply SVD to the m-by-
n term-context matrix M produced by the SIM-
PLE method, resulting in the truncated matrices
M ? Uk?kV Tk , where Uk is a m-by-k matrix, ?k
is k-by-k, and Vk is n-by-k. This takes place af-
ter contextualization, so the component-wise op-
erators apply in the original semantic space. Af-
terwards, the contextualized vector in the original
space, ~c, can be transformed into a vector in the
reduced space, ~cR, by ~cR = ??1k V Tk ~c.Distributional semantic vectors are traditionally
compared by measures which ignore vector mag-
nitudes, such as cosine similarity, but a multivari-
ate Gaussian is sensitive to magnitudes. Thus, the
final step is to normalize ~cR into a unit vector by
dividing it by its L2 norm, ||~cR||.
395
We model the emission of these contextualized
vectors in DSHMM as multivariate Gaussian dis-
tributions, so the semantic vector emissions can be
written as PHsem, PAsem ? N (?,?), where ? ? Rk
is the mean and ? ? Rk?k is the covariance
matrix. To avoid overfitting, we regularize the
covariance using its conjugate prior, the Inverse-
Wishart distribution. We follow the ?neutral? set-
ting of hyperparameters given by Ormoneit and
Tresp (1995), so that the MAP estimate for the co-
variance matrix for (event or slot) state i becomes:
?i =
?
j rij(xj ? ?i)(xj ? ?i)T + ?I?
j rij + 1
, (8)
where j indexes all the relevant semantic vectors
xj in the training set, rij is the posterior respon-
sibility of state i for vector xj , and ? is the re-
maining hyperparameter that we tune to adjust the
amount of regularization. To further reduce model
complexity, we set the off-diagonal entries of the
resulting covariance matrix to zero.
3.2 Training and Inference
Inference in DSHMM is accomplished by the stan-
dard Inside-Outside and tree-Viterbi algorithms,
except that the tree structure is fixed, so there
is no need to sum over all possible subtrees.
Model parameters are learned by the Expectation-
Maximization (EM) algorithm. We tune the hy-
perparameters (NE , NS , ?, ?, k) and the number
of EM iterations by two-fold cross-validation1.
3.3 Summary and Generative Process
In summary, the following steps are applied to
train a DSHMM:
1. Train a distributional semantic model on a
large, domain-general corpus.
2. Preprocess and generate contextualized vec-
tors of event heads and arguments in the
small corpus in the target domain.
3. Train the DSHMM using the EM algorithm.
The formal generative process is as follows:
1. Draw categorical distributions PEinit;
PE , PS , PHlemm (one per event state);
PAlemm (one per slot state) from Dirichlet
priors.
2. Draw multivariate Gaussians PHsem, PAsem for
each event and slot state, respectively.
1The topic cluster splits and the hyperparameter set-
tings are available at http://www.cs.toronto.edu/
?jcheung/dshmm/dshmm.html.
3. Generate the documents, clause by clause.
Generating a clause at position t consists of
these steps:
1. Generate the event state Et ? PE (or PEinit).
2. Generate the event head components
lemm(Ht) ? PHlemm, sem(Ht) ? PHsem.
3. Generate a number of slot states Sta ? PS .
4. For each slot, generate the argument compo-
nents lemm(Ata) ? PAlemm, sem(Ata) ?
PAsem.
4 Experiments
We trained the distributional semantic models us-
ing the Annotated Gigaword corpus (Napoles et
al., 2012), which has been automatically prepro-
cessed and is based on Gigaword 5th edition. This
corpus contains almost ten million news articles
and more than 4 billion tokens. We used those ar-
ticles marked as ?stories? ? the vast majority of
them. We modelled the 50,000 most common lem-
mata as target words, and the 3,000 most common
lemmata as context words.
We then trained DSHMM and conducted our
evaluations on the TAC 2010 guided summa-
rization data set (Owczarzak and Dang, 2010).
Lemmatization and extraction of event heads and
arguments are done by preprocessing with the
Stanford CoreNLP tool suite (Toutanova et al,
2003; de Marneffe et al, 2006). This data set con-
tains 46 topic clusters of 20 articles each, grouped
into five topic categories or domains. For exam-
ple, one topic cluster in the ATTACK category is
about the Columbine Massacre. Each topic cluster
contains eight human-written ?model? summaries
(?model? here meaning a gold standard). Half of
the articles and model summaries in a topic cluster
are used in the guided summarization task, and the
rest are used in the update summarization task.
We chose this data set because it allows us
to conduct various domain-modelling evaluations.
First, templates for the domains are provided, and
the model summaries are annotated with slots
from the template, allowing for an intrinsic eval-
uation of slot induction (Section 5). Second, it
contains multiple domain instances for each of the
domains, and each domain instance comes anno-
tated with eight model summaries, allowing for an
extrinsic evaluation of our system (Section 6).
396
5 Guided Summarization Slot Induction
We first evaluated our models on their ability to
produce coherent clusters of entities belonging to
the same slot, adopting the experimental proce-
dure of Cheung et al (2013).
As part of the official TAC evaluation proce-
dure, model summaries were manually segmented
into contributors, and labelled with the slot in
the TAC template that the contributor expresses.
For example, a summary fragment such as On 20
April 1999, a massacre occurred at Columbine
High School is segmented into the contributors:
(On 20 April 1999, WHEN); (a massacre oc-
curred, WHAT); and (at Columbine High School,
WHERE).
In the slot induction evaluation, this annotation
is used as follows. First, the maximal noun phrases
are extracted from the contributors and clustered
based on the TAC slot of the contributor. These
clusters of noun phrases then become the gold
standard clusters against which automatic systems
are compared. Noun phrases are considered to be
matched if the lemmata of their head words are the
same and they are extracted from the same sum-
mary. This accounts for the fact that human an-
notators often only label the first occurrence of a
word that belongs to a slot in a summary, and fol-
lows the standard evaluation procedure in previ-
ous information extraction tasks, such as MUC-4.
Pronouns and demonstratives are ignored. This
extraction process is noisy, because the meaning
of some contributors depends on an entire verb
phrase, but we keep this representation to allow
a direct comparison to previous work.
Because we are evaluating unsupervised sys-
tems, the clusters produced by the systems are not
labelled, and must be matched to the gold stan-
dard clusters. This matching is performed by map-
ping to each gold cluster the best system cluster
according to F1. The same system cluster may be
mapped multiple times, because several TAC slots
can overlap. For example, in the NATURAL DIS-
ASTERS domain, an earthquake may fit both the
WHAT slot as well as the CAUSE slot, because it
generated a tsunami.
We trained a DSHMM separately for each of the
five domains with different semantic models, tun-
ing hyperparameters by two-fold cross-validation.
We then extracted noun phrase clusters from the
model summaries according to the slot labels pro-
duced by running the Viterbi algorithm on them.
Method P R F1
HMM w/o semantics 13.8 64.1 22.6*
DSHMM w/ SIMPLE 20.9 27.5 23.7
DSHMM w/ E&P 20.7 27.9 23.8
PROFINDER 23.7 25.0 24.3
DSHMM w/ M&L+ 19.7 36.3 25.6*
DSHMM w/ M&L? 22.1 33.2 26.5*
Table 2: Slot induction results on the TAC guided
summarization data set. Asterisks (*) indicate
that the model is statistically significantly differ-
ent from PROFINDER in terms of F1 at p < 0.05.
Results We compared DSHMM to two base-
lines. Our first baseline is PROFINDER, a state-
of-the-art template inducer which Cheung et al
(2013) showed to outperform the previous heuris-
tic clustering method of Chambers and Jurafsky
(2011). Our second baseline is our DSHMM
model, without the semantic vector component,
(HMM w/o semantics). To calculate statistical
significance, we use the paired bootstrap method,
which can accommodate complex evaluation met-
rics like F1 (Berg-Kirkpatrick et al, 2012).
Table 2 shows that performance of the mod-
els. Overall, PROFINDER significantly outper-
forms the HMM baseline, but not any of the
DSHMM models by F1. DSHMM with contextu-
alized semantic vectors achieves the highest F1s,
and are significantly better than PROFINDER. All
of the differences in precision and recall between
PROFINDER and the other models are significant.
The baseline HMM model has highly imbalanced
precision and recall. We think this is because the
model is unable to successfully produce coher-
ent clusters, so the best-case mapping procedure
during evaluation picked large clusters that have
high recall. PROFINDER has slightly higher preci-
sion, which may be due to its non-parametric split-
merge heuristic. We plan to investigate whether
this learning method could improve DSHMM?s
performance further. Importantly, the contextual-
ization of the vectors seems to be beneficial, at
least with the M&L component-wise operators.
In the next section, we show that the improve-
ment from contextualization transfers to multi-
document summarization results.
397
6 Multi-document Summarization: An
Extrinsic Evaluation
We next evaluated our models extrinsically in the
setting of extractive, multi-document summariza-
tion. To use the trained DSHMM for extractive
summarization, we need a decoding procedure for
selecting sentences in the source text to include in
the summary. Inspired by the KLSUM and HI-
ERSUM methods of Haghighi and Vanderwende
(2009), we develop a criterion based on Kullback-
Leibler (KL) divergence between distributions es-
timated from the source text, and those estimated
from the summary. The assumption here is that
these distributions should match in a good sum-
mary. We describe two methods to use this crite-
rion: a basic unsupervised method (Section 6.1),
and a supervised variant that makes use of in-
domain summaries to learn the salient slots and
events in the domain (Section 6.2).
6.1 A KL-based Criterion
There are four main component distributions from
our model that should be considered during extrac-
tion: (1) the distribution of events, (2) the distri-
bution of slots, (3) the distribution of event heads,
and (4) the distribution of arguments. We estimate
(1) as the context-independent probability of being
in a certain event state, which can be calculated
using the Inside-Outside algorithm. Given a col-
lection of documents D which make up the source
text, the distribution of event topics P?E(E) is es-
timated as:
P?E(E = e) = 1Z
?
d?D
?
t
Int(e)Outt(e)
P (d) , (9)
where Int(e) and Outt(e) are the values of the
inside and outside trellises at timestep t for some
event state e, and Z is a normalization constant.
The distribution for a set of sentences in a can-
didate summary, Q?E(E), is identical, except the
summation is over the clauses in the candidate
summary. Slot distributions P?S(S) and Q?S(S) (2)
are defined analogously, where the summation oc-
curs along all the slot variables.
For (3) and (4), we simply use the MLE es-
timates of the lemma emissions, where the esti-
mates are made over the source text and the can-
didate summary instead of over the entire train-
ing set. All of the candidate summary distribu-
tions (i.e., the ?Q? distributions?) are smoothed by
a small amount, so that the KL-divergence is al-
ways finite. Our KL criterion combines the above
components linearly, weighting the lemma distri-
butions by the probability of their respective event
or slot state:
KLScore = (10)
DKL(P?E ||Q?E) +DKL(P?S ||Q?S)
+
NE?
e=1
P?E(e)DKL(P?H(H|e)||Q?H(H|e))
+
NS?
s=1
P?S(s)DKL(P?A(A|s)||Q?A(A|s))
To produce a summary, sentences from the
source text are greedily added such thatKLScore
is minimized at each step, until the desired sum-
mary length is reached, discarding sentences with
fewer than five words.
6.2 Supervised Learning
The above unsupervised method results in sum-
maries that closely mirror the source text in terms
of the event and slot distributions, but this ig-
nores the fact that not all such topics should be
included in a summary. It also ignores genre-
specific, stylistic considerations about character-
istics of good summary sentences. For example,
Woodsend and Lapata (2012) find several factors
that indicate sentences should not be included in
an extractive summary, such as the presence of
personal pronouns. Thus, we implemented a sec-
ond method, in which we modify the KL criterion
above by estimating P?E and P?S from other model
summaries that are drawn from the same domain
(i.e. topic category), except for those summaries
that are written for the specific topic cluster to be
used for evaluation.
6.3 Method and Results
We used the best performing models from the slot
induction task and the above unsupervised and su-
pervised methods based on KL-divergence to pro-
duce 100-word summaries of the guided summa-
rization source text clusters. We did not com-
pare against PROFINDER, as its structure is dif-
ferent and would have required a different proce-
dure than the KL-criterion we developed above.
As shown in the previous evaluation, however, the
HMM baseline without semantics and DSHMM
with SIMPLE perform similarly in terms of F1,
398
Method ROUGE-1 ROUGE-2 ROUGE-SU4
unsup. sup. unsup. sup. unsup. sup.
Leading baseline 28.0 ? 5.39 ? 8.6 ?
HMM w/o semantics 32.3 32.7 6.45 6.49 10.1 10.2
DSHMM w/ SIMPLE 32.1 32.7 5.81 6.50 9.8 10.2
DSHMM w/ M&L+ 32.1 33.4 6.27 6.82 10.0 10.6
DSHMM w/ M&L? 32.4 34.3* 6.35 7.11? 10.2 11.0*
DSHMM w/ E&P 32.8 33.8* 6.38 7.31* 10.3 10.8*
Table 3: TAC 2010 summarization results by three settings of ROUGE. Asterisks (*) indicate that the
model is statistically significantly better than the HMM model without semantics at a 95% confidence
interval, a caret ? indicates that the value is marginally so.
so we consider these competitive baselines. We
did not evaluate with the update summarization
task, because our method has not been adapted to
it. For the evaluation measure, we used the stan-
dard ROUGE suite of automatic evaluation mea-
sures (Lin, 2004). Note that the evaluation con-
ditions of TAC 2010 are different, and thus those
results are not directly comparable to ours. For in-
stance, top performing systems in TAC 2010 make
use of manually constructed lists of entities known
to fit the slots in the provided templates and sam-
ple topic statements, which our method automat-
ically learns. We include the leading baseline re-
sults from the competition as a point of reference,
as it is a well-known and non-trivial one for news
articles. This baseline summary consists of the
leading sentences from the most recent document
in the source text cluster up to the word length
limit.
Table 3 shows the summarization results for the
three most widely-used settings of ROUGE. All
of our models outperform the leading baseline by
a large margin, demonstrating the effective of the
KL-criterion. In terms of unsupervised perfor-
mance, all of our models perform similarly. Be-
cause the unsupervised method mimics the distri-
butions in the source text at all levels, the method
may negate the benefit of learning and simply pro-
duce summaries that match the source text in the
word distributions, thus being an approximation
of KLSUM. Looking at the supervised results,
however, the semantic vector models show clear
gains in ROUGE, whereas the baseline method
does not obtain much benefit from supervision. As
in the previous evaluation, the models with con-
textualized semantic vectors provide the best per-
formance. M&L? performs very well, as in slot
induction, but E&P also performs well, unlike in
the previous evaluation. This result reinforces the
importance of the contextualization procedure for
distributional semantic models.
Analysis To better understand what is gained by
supervision using in-domain summaries, we ana-
lyzed the best performing M&L? model?s output
summaries for one document cluster from each
domain. For each event state, we calculated the
ratio P?Esumm(e)/P?Esource(e), for the probability of
an event state e as estimated from the training
summaries and the the source text respectively.
Likewise, we calculated P?Ssumm(s)/P?Ssource(s) for
the slot states. This ratio indicates the change in
state?s probability after supervision; the greater the
ratio, the more preferred that state becomes after
training. We selected the most preferred and dis-
preferred event and slot for each document clus-
ter, and took the three most probable lemmata
from the associated lemma distribution (Table 4).
It seems that supervision is beneficial because it
picks out important event heads and arguments in
the domain, such as charge, trial, and murder in
the TRIALS domain. It also helps the summarizer
avoid semantically generic words (be or have),
pronouns, quotatives, and common but irrelevant
words (home, city, restaurant in TRIALS).
7 Conclusion
We have shown that contextualized distributional
semantic vectors can be successfully integrated
into a generative probabilistic model for domain
modelling, as demonstrated by improvements in
slot induction and multi-document summariza-
tion. The effectiveness of our model stems from
the use of a large domain-general corpus to train
the distributional semantic vectors, and the im-
plicit syntactic and word sense information pro-
399
Domain Event Heads Slot Arguments+ ? + ?
ATTACKS say
2, cause,
doctor say
2, be, have attack, hostage,troops he, it, they
TRIALS charge, trial,accuse say, be, have
prison, murder,
charge
home, city, restau-
rant
RESOURCES reduce, increase,university say, be, have
government,
effort, program he, they, it
DISASTERS flood, strengthen,engulf say, be, have
production,
statoil, barrel he, it, they
HEALTH be, department,have say, do, make
food, product,
meat she, people, way
Table 4: Analysis of the most probable event heads and arguments in the most preferred (+) and dispre-
ferred (?) events and slots after supervised training.
vided by the contextualization process. Our ap-
proach is modular, and allows principled train-
ing of the probabilistic model using standard tech-
niques. While we have focused on the overall clus-
tering of entities and the distribution of event and
slot topics in this work, we would also like to in-
vestigate discourse modelling and content struc-
turing. Finally, our work shows that the applica-
tion of distributional semantics to NLP tasks need
not be confined to lexical disambiguation. We
would like to see modern distributional semantic
methods incorporated into an even greater variety
of applications.
Acknowledgments
This work is supported by the Natural Sciences
and Engineering Research Council of Canada.
References
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models, with applications
to generation and summarization. In Proceedings
of the Human Language Technology Conference of
the North American Chapter of the Association for
Computational Linguistics: HLT-NAACL 2004.
Taylor Berg-Kirkpatrick, David Burkett, and Dan
Klein. 2012. An empirical investigation of statisti-
cal significance in NLP. In Proceedings of the 2012
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning, Jeju Island, Korea, July. Asso-
ciation for Computational Linguistics.
2The event head say happens to appear in both the most
preferred and dispreferred events in the ATTACKS domain.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. The Journal of
Machine Learning Research, 3.
Asli Celikyilmaz and Dilek Hakkani-Tur. 2010. A hy-
brid hierarchical model for multi-document summa-
rization. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 815?824, Uppsala, Sweden, July. Asso-
ciation for Computational Linguistics.
Nathanael Chambers and Dan Jurafsky. 2011.
Template-based information extraction without the
templates. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies, pages 976?
986, Portland, Oregon, USA, June. Association for
Computational Linguistics.
Jackie Chi Kit Cheung and Gerald Penn. 2012. Evalu-
ating distributional models of semantics for syntacti-
cally invariant inference. In Proceedings of the 13th
Conference of the European Chapter of the Associ-
ation for Computational Linguistics, pages 33?43,
Avignon, France, April. Association for Computa-
tional Linguistics.
Jackie Chi Kit Cheung, Hoifung Poon, and Lucy Van-
derwende. 2013. Probabilistic frame induction. In
Proceedings of the 2013 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies.
Stephen Clark, Bob Coecke, and Mehrnoosh
Sadrzadeh. 2008. A compositional distribu-
tional model of meaning. In Proceedings of the
Second Quantum Interaction Symposium (QI-2008).
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
In LREC 2006.
Georgiana Dinu and Mirella Lapata. 2010. Measur-
ing distributional similarity in context. In Proceed-
400
ings of the 2010 Conference on Empirical Methods
in Natural Language Processing, pages 1162?1172.
Micha Elsner, Joseph Austerweil, and Eugene Char-
niak. 2007. A unified local and global model for
discourse coherence. In Human Language Tech-
nologies 2007: The Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics; Proceedings of the Main Conference,
Rochester, New York, April. Association for Com-
putational Linguistics.
Katrin Erk and Sebastian Pado?. 2008. A structured
vector space model for word meaning in context. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, pages 897?
906.
Elena Filatova, Vasileios Hatzivassiloglou, and Kath-
leen McKeown. 2006. Automatic creation of
domain templates. In Proceedings of the COL-
ING/ACL 2006 Main Conference Poster Sessions,
pages 207?214, Sydney, Australia, July. Association
for Computational Linguistics.
Pascale Fung and Grace Ngai. 2006. One story, one
flow: Hidden markov story models for multilin-
gual multidocument summarization. ACM Transac-
tions on Speech and Language Processing (TSLP),
3(2):1?16.
Pascale Fung, Grace Ngai, and Chi-Shun Cheung.
2003. Combining optimal clustering and hidden
markov models for extractive summarization. In
Proceedings of the ACL 2003 Workshop on Multilin-
gual Summarization and Question Answering, pages
21?28, Sapporo, Japan, July. Association for Com-
putational Linguistics.
Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011.
Experimental support for a categorical composi-
tional distributional model of meaning. In Proceed-
ings of the 2011 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1394?
1404, Edinburgh, Scotland, UK., July. Association
for Computational Linguistics.
Amit Gruber, Michael Rosen-Zvi, and Yair Weiss.
2007. Hidden topic markov models. Artificial In-
telligence and Statistics (AISTATS).
Aria Haghighi and Lucy Vanderwende. 2009. Ex-
ploring content models for multi-document summa-
rization. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Compu-
tational Linguistics, pages 362?370, Boulder, Col-
orado, June. Association for Computational Linguis-
tics.
Chin Y. Lin. 2004. ROUGE: A package for automatic
evaluation of summaries. In Stan Szpakowicz and
Marie-Francine Moens, editors, Text Summarization
Branches Out: Proceedings of the ACL-04 Work-
shop, pages 74?81, Barcelona, Spain, July. Associa-
tion for Computational Linguistics.
Annie Louis and Ani Nenkova. 2012. A coherence
model based on syntactic patterns. In Proceedings
of the 2012 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Computa-
tional Natural Language Learning, Jeju Island, Ko-
rea, July. Association for Computational Linguistics.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL-08: HLT, pages 236?244.
1992. Proceedings of the Fourth Message Understand-
ing Conference (MUC-4). Morgan Kaufmann.
Courtney Napoles, Matthew Gormley, and Benjamin
Van Durme. 2012. Annotated gigaword. In Pro-
ceedings of the NAACL-HLT Joint Workshop on Au-
tomatic Knowledge Base Construction & Web-scale
Knowledge Extraction (AKBC-WEKEX), pages 95?
100.
Dirk Ormoneit and Volker Tresp. 1995. Improved
gaussian mixture density estimates using bayesian
penalty terms and network averaging. In Advances
in Neural Information Processing, pages 542?548.
Karolina Owczarzak and Hoa T. Dang. 2010. TAC
2010 guided summarization task guidelines.
Gerard Salton, Anita Wong, and Chung-Shu Yang.
1975. A vector space model for automatic indexing.
Communications of the ACM, 18(11):613?620.
Roger C. Schank and Robert P. Abelson. 1977. Scripts,
Plans, Goals, and Understanding: An Inquiry Into
Human Knowledge Structures. Lawrence Erlbaum,
July.
Stefan Thater, Hagen Fu?rstenau, and Manfred Pinkal.
2011. Word meaning in context: A simple and ef-
fective vector model. In Proceedings of IJCNLP.
Kristina Toutanova, Dan Klein, Christoper D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1, page 180.
Peter D. Turney and Patrick Pantel. 2010. From
frequency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Research,
37:141?188.
Hanna M. Wallach. 2008. Structured topic models for
language. Doctoral dissertation, University of Cam-
bridge.
Kristian Woodsend and Mirella Lapata. 2012. Mul-
tiple aspect summarization using integer linear pro-
gramming. In Proceedings of the 2012 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, Jeju Island, Korea, July. Association for
Computational Linguistics.
401
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1233?1242,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Towards Robust Abstractive Multi-Document Summarization: A
Caseframe Analysis of Centrality and Domain
Jackie Chi Kit Cheung
University of Toronto
10 King?s College Rd., Room 3302
Toronto, ON, Canada M5S 3G4
jcheung@cs.toronto.edu
Gerald Penn
University of Toronto
10 King?s College Rd., Room 3302
Toronto, ON, Canada M5S 3G4
gpenn@cs.toronto.edu
Abstract
In automatic summarization, centrality is
the notion that a summary should contain
the core parts of the source text. Cur-
rent systems use centrality, along with re-
dundancy avoidance and some sentence
compression, to produce mostly extrac-
tive summaries. In this paper, we investi-
gate how summarization can advance past
this paradigm towards robust abstraction
by making greater use of the domain of
the source text. We conduct a series of
studies comparing human-written model
summaries to system summaries at the se-
mantic level of caseframes. We show that
model summaries (1) are more abstrac-
tive and make use of more sentence aggre-
gation, (2) do not contain as many topi-
cal caseframes as system summaries, and
(3) cannot be reconstructed solely from
the source text, but can be if texts from
in-domain documents are added. These
results suggest that substantial improve-
ments are unlikely to result from better
optimizing centrality-based criteria, but
rather more domain knowledge is needed.
1 Introduction
In automatic summarization, centrality has been
one of the guiding principles for content selection
in extractive systems. We define centrality to be
the idea that a summary should contain the parts
of the source text that are most similar or repre-
sentative of the source text. This is most trans-
parently illustrated by the Maximal Marginal Rel-
evance (MMR) system of Carbonell and Goldstein
(1998), which defines the summarization objective
to be a linear combination of a centrality term and
a non-redundancy term.
Since MMR, much progress has been made on
more sophisticated methods of measuring central-
ity and integrating it with non-redundancy (See
Nenkova and McKeown (2011) for a recent sur-
vey). For example, term weighting methods such
as the signature term method of Lin and Hovy
(2000) pick out salient terms that occur more often
than would be expected in the source text based on
frequencies in a background corpus. This method
is a core component of the most successful sum-
marization methods (Conroy et al, 2006).
While extractive methods based on centrality
have thus achieved success, there has long been
recognition that abstractive methods are ultimately
more desirable. One line of work is in text simpli-
fication and sentence fusion, which focus on the
ability of abstraction to achieve a higher compres-
sion ratio (Knight and Marcu, 2000; Barzilay and
McKeown, 2005). A less examined issue is that of
aggregation and information synthesis. A key part
of the usefulness of summaries is that they provide
some synthesis or analysis of the source text and
make a more general statement that is of direct rel-
evance to the user. For example, a series of related
events can be aggregated and expressed as a trend.
The position of this paper is that centrality is
not enough to make substantial progress towards
abstractive summarization that is capable of this
type of semantic inference. Instead, summariza-
tion systems need to make more use of domain
knowledge. We provide evidence for this in a se-
ries of studies on the TAC 2010 guided summa-
rization data set that examines how the behaviour
of automatic summarizers can or cannot be dis-
tinguished from human summarizers. First, we
confirm that abstraction is a desirable goal, and
1233
provide a quantitative measure of the degree of
sentence aggregation in a summarization system.
Second, we show that centrality-based measures
are unlikely to lead to substantial progress towards
abstractive summarization, because current top-
performing systems already produce summaries
that are more ?central? than humans do. Third, we
consider how domain knowledge may be useful as
a resource for an abstractive system, by showing
that key parts of model summaries can be recon-
structed from the source plus related in-domain
documents.
Our contributions are novel in the following re-
spects. First, our analyses are performed at the
level of caseframes, rather at the level of words or
syntactic dependencies as in previous work. Case-
frames are shallow approximations of semantic
roles which are well suited to characterizing a do-
main by its slots. Furthermore, we take a devel-
opmental rather than evaluative perspective?our
goal is not to develop a new evaluation measure as
defined by correlation with human responsiveness
judgments. Instead, our studies reveal useful cri-
teria with which to distinguish human-written and
system summaries, helping to guide the develop-
ment of future summarization systems.
2 Related Work
Domain-dependent template-based summariza-
tion systems have been an alternative to extractive
systems which make use of rich knowledge about
a domain and information extraction techniques to
generate a summary, possibly using a natural lan-
guage generation system (Radev and McKeown,
1998; White et al, 2001; McKeown et al, 2002).
This paper can be seen as a first step towards
reconciling the advantages of domain knowledge
with the resource-lean extraction approaches pop-
ular today.
As noted above, Lin and Hovy?s (2000) sig-
nature terms have been successful in discovering
terms that are specific to the source text. These
terms are identified by a log-likelihood ratio test
based on their relative frequencies in relevant and
irrelevant documents. They were originally pro-
posed in the context of single-document summa-
rization, where they were calculated using in-
domain (relevant) vs. out-of-domain (irrelevant)
text. In multi-document summarization, the in-
domain text has been replaced by the source text
cluster (Conroy et al, 2006), thus they are now
used as a form of centrality-based features. In
this paper, we use guided summarization data as
an opportunity to reopen the investigation into the
effect of domain, because multiple document clus-
ters from the same domain are available.
Summarization evaluation is typically done by
comparing system output to human-written model
summaries, and are validated by their correlation
with user responsiveness judgments. The compar-
ison can be done at the word level, as in ROUGE
(Lin, 2004), at the syntactic level, as in Basic
Elements (Hovy et al, 2006), or at the level of
summary content units, as in the Pyramid method
(Nenkova and Passonneau, 2004). There are also
automatic measures which do not require model
summaries, but compare against the source text in-
stead (Louis and Nenkova, 2009; Saggion et al,
2010).
Several studies complement this paper by ex-
amining the best possible extractive system us-
ing current evaluation measures, such as ROUGE
(Lin and Hovy, 2003; Conroy et al, 2006). They
find that the best possible extractive systems score
higher or as highly than human summarizers, but
it is unclear whether this means the oracle sum-
maries are actually as useful as human ones in
an extrinsic setting. Genest et al (2009) ask hu-
mans to create extractive summaries, and find that
they score in between current automatic systems
and human-written abstracts on responsiveness,
linguistic quality, and Pyramid score. In the lec-
ture domain, He et al (1999; 2000) find that
lecture transcripts that have been manually high-
lighted with key points improve students? quiz
scores more than when using automated summa-
rization techniques or when providing only the
lecture transcript or slides.
Jing and McKeown (2000) manually analyzed
30 human-written summaries, and find that 19%
of sentences cannot be explained by cut-and-paste
operations from the source text. Saggion and La-
palme (2002) similarly define a list of transfor-
mations necessary to convert source text to sum-
mary text, and manually analyzed their frequen-
cies. Copeck and Szpakowicz (2004) find that
at most 55% of vocabulary items found in model
summaries occur in the source text, but they do
not investigate where the other vocabulary items
might be found.
1234
Sentence:
At one point, two bomb squad trucks sped to
the school after a backpack scare.
Dependencies:
num(point, one) prep at(sped, point)
num(trucks, two) nn(trucks, bomb)
nn(trucks, squad) nsubj(sped, trucks)
root(ROOT, sped) det(school, the)
prep to(sped, school) det(scare, a)
nn(scare, backpack) prep after(sped, scare)
Caseframes:
(speed, prep at) (speed, nsubj)
(speed, prep to) (speed, prep after)
Table 1: A sentence decomposed into its depen-
dency edges, and the caseframes derived from
those edges that we consider (in black).
3 Theoretical basis of our analysis
Many existing summarization evaluation methods
rely on word or N-gram overlap measures, but
these measures are not appropriate for our anal-
ysis. Word overlap can occur due to shared proper
nouns or entity mentions. Good summaries should
certainly contain the salient entities in the source
text, but when assessing the effect of the domain,
different domain instances (i.e., different docu-
ment clusters in the same domain) would be ex-
pected to contain different salient entities. Also,
the realization of entities as noun phrases depends
strongly on context, which would confound our
analysis if we do not also correctly resolve corefer-
ence, a difficult problem in its own right. We leave
such issues to other work (Nenkova and McKe-
own, 2003, e.g.).
Domains would rather be expected to share slots
(a.k.a. aspects), which require a more semantic
level of analysis that can account for the various
ways in which a particular slot can be expressed.
Another consideration is that the structures to be
analyzed should be extracted automatically. Based
on these criteria, we selected caseframes to be the
appropriate unit of analysis. A caseframe is a shal-
low approximation of the semantic role structure
of a proposition-bearing unit like a verb, and are
derived from the dependency parse of a sentence1.
1Note that caseframes are distinct from (though directly
Relation Caseframe Pair Sim.
Degree (kill, dobj) (wound, dobj) 0.82
Causal (kill, dobj) (die, nsubj) 0.80
Type (rise, dobj) (drop, prep to) 0.81
Figure 1: Sample pairs of similar caseframes by
relation type, and the similarity score assigned to
them by our distributional model.
In particular, they are (gov, role) pairs, where gov
is a proposition-bearing element, and role is an
approximation of a semantic role with gov as its
head (See Figure 1 for examples). Caseframes do
not consider the dependents of the semantic role
approximations.
The use of caseframes is well grounded in a va-
riety of NLP tasks relevant to summarization such
as coreference resolution (Bean and Riloff, 2004),
and information extraction (Chambers and Juraf-
sky, 2011), where they serve the central unit of se-
mantic analysis. Related semantic representations
are popular in Case Grammar and its derivative
formalisms such as frame semantics (Fillmore,
1982).
We use the following algorithm to extract case-
frames from dependency parses. First, we extract
those dependency edges with a relation type of
subject, direct object, indirect object, or prepo-
sitional object (with the preposition indicated),
along with their governors. The governor must be
a verb, event noun (as defined by the hyponyms
of the WordNet EVENT synset), or nominal or ad-
jectival predicate. Then, a series of deterministic
transformations are applied to the syntactic rela-
tions to account for voicing alternations, control,
raising, and copular constructions.
3.1 Caseframe Similarity
Direct caseframe matches account for some vari-
ation in the expression of slots, such as voicing
alternations, but there are other reasons different
caseframes may indicate the same slot (Figure 1).
For example, (kill, dobj) and (wound, dobj) both
indicate the victim of an attack, but differ by
the degree of injury to the victim. (kill, dobj)
and (die, nsubj) also refer to a victim, but are
linked by a causal relation. (rise, dobj) and
inspired by) the similarly named case frames of Case Gram-
mar (Fillmore, 1968).
1235
(drop, prep to) on the other hand simply share a
named entity type (in this case, numbers). To ac-
count for these issues, we measure caseframe sim-
ilarity based on their distributional similarity in a
large training corpus.
First, we construct vector representations of
each caseframe, where the dimensions of the vec-
tor correspond to the lemma of the head word that
fills the caseframe in the training corpus. For ex-
ample, kicked the ball would result in a count of
1 added to the caseframe (kick, dobj) for the con-
text word ball. Then, we rescale the counts into
pointwise mutual information values, which has
been shown to be more effective than raw counts
at detecting semantic relatedness (Turney, 2001).
Similarity between caseframes can then be com-
pared by cosine similarity between the their vector
representations.
For training, we use the AFP portion of the
Gigaword corpus (Graff et al, 2005), which we
parsed using the Stanford parser?s typed depen-
dency tree representation with collapsed conjunc-
tions (de Marneffe et al, 2006). For reasons of
sparsity, we only considered caseframes that ap-
pear at least five times in the guided summariza-
tion corpus, and only the 3000 most common lem-
mata in Gigaword as context words.
3.2 An Example
To illustrate how caseframes indicate the slots in a
summary, we provide the following fragment of a
model summary from TAC about the Unabomber
trial:
(1) In Sacramento, Theodore Kaczynski faces a
10-count federal indictment for 4 of the 16
mail bomb attacks attributed to the
Unabomber in which two people were killed.
If found guilty, he faces a death penalty. ...
He has pleaded innocent to all charges. U.S.
District Judge Garland Burrell Jr. presides
in Sacramento.
All of the slots provided by TAC for the Inves-
tigations and Trials domain can be identified by
one or more caseframes. The DEFENDANT can be
identified by (face, nsubj), and (plead, nsubj);
the CHARGES by (face, dobj); the REASON
by (indictment, prep for); the SENTENCE by
(face, dobj); the PLEAD by (plead, dobj); and
the INVESTIGATOR by (preside, nsubj).
4 Experiments
We conducted our experiments on the data and re-
sults of the TAC 2010 summarization workshop.
This data set contains 920 newspaper articles in
46 topics of 20 documents each. Ten are used in
an initial guided summarization task, and ten are
used in an update summarization task, in which
a summary must be produced assuming that the
original ten documents had already been read. All
summaries have a word length limit of 100 words.
We analyzed the results of the two summarization
tasks separately in our experiments.
The 46 topics belong to five different cate-
gories or domains: Accidents and natural dis-
asters, Criminal or terrorist attacks, Health and
safety, Endangered resources, and Investigations
and trials. Each domain is associated with a tem-
plate specifying the type of information that is ex-
pected in the domain, such as the participants in
the event or the time that the event occurred.
In our study, we compared the characteristics of
summaries generated by the eight human summa-
rizers with those generated by the peer summaries,
which are basically extractive systems. There
are 43 peer summarization systems, including two
baselines defined by NIST. We refer to systems
by their ID given by NIST, which are alphabetical
for the human summarizers (A to H), and numeric
for the peer summarizers (1 to 43). We removed
two peer systems (systems 29 and 43) which did
not generate any summary text in the workshop,
presumably due to software problems. For each
measure that we consider, we compare the average
among the human-written summaries to the three
individual peer systems, which we chose in order
to provide a representative sample of the average
and best performance of the automatic systems ac-
cording to current evaluation methods. These sys-
tems are all primarily extractive, like most of the
systems in the workshop:
Peer average The average of the measure
among the 41 peer summarizers.
Peer 16 This system scored the highest in re-
sponsiveness scores on the original summarization
task and in ROUGE-2, responsiveness, and Pyra-
mid score in the update task.
Peer 22 This system scored the highest in
ROUGE-2 and Pyramid score in the original sum-
marization task.
1236
1
4
28
25
12
10
15
31
36
18
42
8
14
30
37
33
13
19
24
16
21
40
41
3
27
38
17
35
23
39
34
22
7
6
20
26
11
32
5
9
G
2
F
B
E
A
D
C
H
System IDs
0.0
0.5
1.0
1.5
2.0
N
u
m
be
r
o
fs
en
te
n
ce
s
(a) Initial guided summarization task
28
1
31
18
42
4
10
15
36
24
25
12
13
16
30
34
27
3
9
8
14
37
33
40
7
11
39
38
19
35
26
23
17
22
21
6
32
41
5
20
F
G
2
E
C
B
A
H
D
System IDs
0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
1.6
1.8
N
u
m
be
r
o
fs
en
te
n
ce
s
(b) Update summarization task
Figure 2: Average sentence cover size: the average number of sentences needed to generate the case-
frames in a summary sentence (Study 1). Model summaries are shown in darker bars. Peer system
numbers that we focus on are in bold.
Condition Initial Update
Model average 1.58 1.57
Peer average 1.06 1.06
Peer 1 1.00 1.00
Peer 16 1.04 1.04
Peer 22 1.08 1.09
Table 2: The average number of source text sen-
tences needed to cover a summary sentence. The
model average is statistically significantly differ-
ent from all the other conditions p < 10?7
(Study 1).
Peer 1 The NIST-defined baseline, which is the
leading sentence baseline from the most recent
document in the source text cluster. This system
scored the highest on linguistic quality in both
tasks.
4.1 Study 1: Sentence aggregation
We first confirm that human summarizers are more
prone to sentence aggregation than system sum-
marizers, showing that abstraction is indeed a de-
sirable goal. To do so, we propose a measure to
quantify the degree of sentence aggregation exhib-
ited by a summarizer, which we call average sen-
tence cover size. This is defined to be the min-
imum number of sentences from the source text
needed to cover all of the caseframes found in a
summary sentence (for those caseframes that can
be found in the source text at all), averaged over all
of the summary sentences. Purely extractive sys-
tems would thus be expected to score 1.0, as would
systems that perform text compression by remov-
ing constituents of a source text sentence. Human
summarizers would be expected to score higher, if
they actually aggregate information from multiple
points in the source text.
To illustrate, suppose we assign arbitrary in-
dices to caseframes, a summary sentence con-
tains caseframes {1, 2, 3, 4, 5}, and the source
text contains three sentences with caseframes,
which can be represented as a nested set
{{1, 3, 4}, {2, 5, 6}, {1, 4, 7}}. Then, the sum-
mary sentence can be covered by two sentences
from the source text, namely {{1, 3, 4}, {2, 5, 6}}.
This problem is actually an instance of the min-
imum set cover problem, in which sentences are
sets, and caseframes are set elements. Minimum
set cover is NP-hard in general, but the standard
integer programming formulation of set cover suf-
ficed for our data set; we used ILOG CPLEX
12.4?s mixed integer programming mode to solve
all the set cover problems optimally.
Results Figure 2 shows the ranking of the sum-
marizers by this measure. Most peer systems have
a low average sentence cover size of close to 1,
which reflects the fact that they are purely or al-
most purely extractive. Human model summariz-
ers show a higher degree of aggregation in their
summaries. The averages of the tested condi-
tions are shown in Table 2, and are statistically
significant. Peer 2 shows a relatively high level
of aggregation despite being an extractive system.
Upon inspection of its summaries, it appears that
Peer 2 tends to select many datelines, and without
punctuation to separate them from the rest of the
summary, our automatic analysis tools incorrectly
merged many sentences together, resulting in in-
correct parses and novel caseframes not found in
1237
A
32
B
12
42
27
37
33
G
1
5
28
7
39
2
E
F
H
35
26
15
C
D
11
20
9
36
14
19
40
13
16
8
30
4
6
10
3
18
41
21
34
24
17
25
31
22
23
38
System IDs
0.00
0.02
0.04
0.06
0.08
0.10
0.12
Pe
r
w
o
rd
de
n
si
ty
(a) Initial guided summarization task
E
A
G
B
37
1
33
C
12
27
26
42
39
11
H
28
F
15
2
D
32
20
35
5
40
7
4
10
8
19
14
30
36
41
18
3
9
21
24
34
13
22
25
16
31
17
6
23
38
System IDs
0.00
0.02
0.04
0.06
0.08
0.10
Pe
r
w
o
rd
de
n
si
ty
(b) Update summarization task
Figure 3: Density of signature caseframes (Study 2).
Topic: Unabomber trial
(charge, dobj), (kill, dobj),
(trial, prep of), (bombing, prep in)
Topic: Mangrove forests
(beach, prep of), (save, dobj)
(development, prep of), (recover, nsubj)
Topic: Bird Flu
(infect, prep with), (die, nsubj)
(contact, dobj), (import, prep from)
Figure 4: Examples of signature caseframes found
in Study 2.
the source text.
4.2 Study 2: Signature caseframe density
Study 1 shows that human summarizers are more
abstractive in that they aggregate information from
multiple sentences in the source text, but how is
this aggregation performed? One possibility is
that human summary writers are able to pack a
greater number of salient caseframes into their
summaries. That is, humans are fundamentally re-
lying on centrality just as automatic summarizers
do, and are simply able to achieve higher compres-
sion ratios by being more succinct. If this is true,
then sentence fusion methods over the source text
alone might be able to solve the problem. Unfor-
tunately, we show that this is false and that system
summaries are actually more central than model
ones.
To extract topical caseframes, we use Lin and
Hovy?s (2000) method of calculating signature
terms, but extend the method to apply it at the
caseframe rather than the word level. We fol-
low Lin and Hovy (2000) in using a significance
Condition Initial Update
Model average 0.065 0.052
Peer average 0.080? 0.072?
Peer 1 0.066 0.050
Peer 16 0.083? 0.085?
Peer 22 0.101? 0.084?
Table 3: Signature caseframe densities for differ-
ent sets of summarizers, for the initial and update
guided summarization tasks (Study 2). ?: p <
0.005.
threshold of 0.001 to determine signature case-
frames2. Figure 4 shows examples of signature
caseframes for several topics. Then, we calculate
the signature caseframe density of each of the
summarization systems. This is defined to be the
number of signature caseframes in the set of sum-
maries divided by the number of words in that set
of summaries.
Results Figure 3 shows the density for all of the
summarizers, in ascending order of density. As
can be seen, the human abstractors actually tend to
use fewer signature caseframes in their summaries
than automatic systems. Only the leading baseline
is indistinguishable from the model average. Ta-
ble 3 shows the densities for the conditions that
we described earlier. The differences in density
between the human average and the non-baseline
conditions are highly statistically significant, ac-
cording to paired two-tailed Wilcoxon signed-rank
tests for the statistic calculated for each topic clus-
ter.
These results show that human abstractors do
2We tried various other thresholds, but the results were
much the same.
1238
Threshold 0.9 0.8
Condition Init. Up. Init. Up.
Model average 0.066 0.052 0.062 0.047
Peer average 0.080 0.071 0.071 0.063
Peer 1 0.068 0.050 0.060 0.044
Peer 16 0.083 0.086 0.072 0.077
Peer 22 0.100 0.086 0.084 0.075
Table 4: Density of signature caseframes after
merging to various threshold for the initial (Init.)
and update (Up.) summarization tasks (Study 2).
not merely repeat the caseframes that are indica-
tive of a topic cluster or use minor grammatical
alternations in their summaries. Rather, a genuine
sort of abstraction or distillation has taken place,
either through paraphrasing or semantic inference,
to transform the source text into the final informa-
tive summary.
Merging Caseframes We next investigate
whether simple paraphrasing could account for
the above results; it may be the case that human
summarizers simply replace words in the source
text with synonyms, which can be detected with
distributional similarity. Thus, we merged similar
caseframes into clusters according to the distribu-
tional semantic similarity defined in Section 3.1,
and then repeated the previous experiment. We
chose two relatively high levels of similarity (0.8
and 0.9), and used complete-link agglomerative
(i.e., bottom-up) clustering to merge similar
caseframes. That is, each caseframe begins as a
separate cluster, and the two most similar clusters
are merged at each step until the desired similarity
threshold is reached. Cluster similarity is defined
to be the minimum similarity (or equivalently,
maximum distance) between elements in the
two clusters; that is, maxc?C1,c??C2 ?sim(c, c?).
Complete-link agglomerative clustering tends to
form coherent clusters where the similarity be-
tween any pair within a cluster is high (Manning
et al, 2008).
Cluster Results Table 4 shows the results after
the clustering step, with similarity thresholds of
0.9 and 0.8. Once again, model summaries contain
a lower density of signature caseframes. The sta-
tistical significance results are unchanged. This in-
dicates that simple paraphrasing alone cannot ac-
count for the difference in the signature caseframe
densities, and that some deeper abstraction or se-
mantic inference has occurred.
Note that we are not claiming that a lower den-
sity of signature caseframes necessarily correlates
with a more informative summary. For example,
some automatic summarizers are comparable to
the human abstractors in their relatively low den-
sity of signature caseframes, but these turn out to
be the lowest performing summarization systems
by all measures in the workshop, and they are un-
likely to rival human abstractors in any reasonable
evaluation of summary informativeness. It does,
however, appear that further optimizing centrality-
based measures alone is unlikely to produce bet-
ter informative summaries, even if we analyze the
summary at a syntactic/semantic rather than lexi-
cal level.
4.3 Study 3: Summary Reconstruction
The above studies show that the higher degree
of abstraction in model summaries cannot be ex-
plained by better compression of topically salient
caseframes alone. We now switch perspectives to
ask how model summaries might be automatically
generated at all. We will show that they cannot
be reconstructed solely from the source text, ex-
tending Copeck and Szpakowicz (2004)?s result to
caseframes. However, we also show that if articles
from the same domain are added, reconstruction
then becomes possible. Our measure of whether
a model summary can be reconstructed is case-
frame coverage. We define this to be the propor-
tion of caseframes in a summary that is contained
by some reference set. This is thus a score be-
tween 0 and 1. Unlike in the previous study, we
use the full set of caseframes, not just signature
caseframes, because our goal now to create a hy-
pothesis space from which it is in principle possi-
ble to generate the model summaries.
Results We first calculated caseframe coverage
with respect to the source text alone (Figure 5).
As expected, automatic systems show close to per-
fect coverage, because of their basically extractive
nature, while model summaries show much lower
coverage. These statistics are summarized by Ta-
ble 5. These results present a fundamental limit
to extractive systems, and also text simplification
and sentence fusion methods based solely on the
source text.
The Impact of Domain Knowledge How might
automatic summarizers be able to acquire these
1239
A
G
E
B
H
F
C
D
38
17
2
32
20
6
39
40
5
9
34
14
23
35
19
7
33
41
12
11
37
26
42
21
27
3
24
28
10
4
8
13
16
31
30
25
22
1
15
18
36
System IDs
0.0
0.2
0.4
0.6
0.8
1.0
Co
ve
ra
ge
(a) Initial guided summarization task
G
A
B
E
H
C
F
D
2
38
17
32
11
41
39
20
35
19
26
21
5
23
14
37
40
27
42
12
25
4
6
33
7
8
30
22
31
10
24
13
34
15
28
1
3
9
16
18
36
System IDs
0.0
0.2
0.4
0.6
0.8
1.0
Co
ve
ra
ge
(b) Update summarization task
Figure 5: Coverage of summary text caseframes in source text (Study 3).
Condition Initial Update
Model average 0.77 0.75
Peer average 0.99 0.99
Peer 1 1.00 1.00
Peer 16 1.00 1.00
Peer 22 1.00 1.00
Table 5: Coverage of caseframes in summaries
with respect to the source text. The model aver-
age is statistically significantly different from all
the other conditions p < 10?8 (Study 3).
caseframes from other sources? Traditional sys-
tems that perform semantic inference do so from a
set of known facts about the domain in the form of
a knowledge base, but as we have seen, most ex-
tractive summarization systems do not make much
use of in-domain corpora. We examine adding
in-domain text to the source text to see how this
would affect coverage.
Recall that the 46 topics in TAC 2010 are cat-
egorized into five domains. To calculate the im-
pact of domain knowledge, we add all the docu-
ments that belong in the same domain to the source
text to calculate coverage. To ensure that coverage
does not increase simply due to increasing the size
of the reference set, we compare to the baseline of
adding the same number of documents that belong
to another domain. As shown in Table 6, the ef-
fect of adding more in-domain text on caseframe
coverage is substantial, and noticeably more than
using out-of-domain text. In fact, nearly all case-
frames can be found in the expanded set of arti-
cles. The implication of this result is that it may
be possible to generate better summaries by min-
ing in-domain text for relevant caseframes.
Reference corpus Initial Update
Source text only 0.77 0.75
+out-of-domain 0.91 0.91
+in-domain 0.98 0.97
Table 6: The effect on caseframe coverage of
adding in-domain and out-of-domain documents.
The difference between adding in-domain and out-
of-domain text is significant p < 10?3 (Study 3).
5 Conclusion
We have presented a series of studies to distin-
guish human-written informative summaries from
the summaries produced by current systems. Our
studies are performed at the level of caseframes,
which are able to characterize a domain in terms of
its slots. First, we confirm that model summaries
are more abstractive and aggregate information
from multiple source text sentences. Then, we
show that this is not simply due to summary writ-
ers packing together source text sentences contain-
ing topical caseframes to achieve a higher com-
pression ratio, even if paraphrasing is taken into
account. Indeed, model summaries cannot be re-
constructed from the source text alone. How-
ever, our results are also positive in that we find
that nearly all model summary caseframes can be
found in the source text together with some in-
domain documents.
Current summarization systems have been
heavily optimized towards centrality and lexical-
semantical reasoning, but we are nearing the bot-
tom of the barrel. Domain inference, on the other
hand, and a greater use of in-domain documents
as a knowledge source for domain inference, are
very promising indeed. Mining useful caseframes
1240
for a sentence fusion-based approach has the po-
tential, as our experiments have shown, to deliver
results in just the areas where current approaches
are weakest.
Acknowledgements
This work is supported by the Natural Sciences
and Engineering Research Council of Canada.
References
Regina Barzilay and Kathleen R. McKeown. 2005.
Sentence fusion for multidocument news summa-
rization. Computational Linguistics, 31(3):297?
328.
David Bean and Ellen Riloff. 2004. Unsupervised
learning of contextual role knowledge for corefer-
ence resolution. In Proceedings of the Human Lan-
guage Technology Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics: HLT-NAACL 2004.
Jaime Carbonell and Jade Goldstein. 1998. The use of
MMR, diversity-based reranking for reordering doc-
uments and producing summaries. In Proceedings
of the 21st Annual International ACM SIGIR Con-
ference on Research and Development in Informa-
tion Retrieval, pages 335?336. ACM.
Nathanael Chambers and Dan Jurafsky. 2011.
Template-based information extraction without the
templates. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies, pages 976?
986, Portland, Oregon, USA, June. Association for
Computational Linguistics.
John M. Conroy, Judith D. Schlesinger, and Dianne P.
O?Leary. 2006. Topic-focused multi-document
summarization using an approximate oracle score.
In Proceedings of the COLING/ACL 2006 Main
Conference Poster Sessions, pages 152?159, Syd-
ney, Australia, July. Association for Computational
Linguistics.
Terry Copeck and Stan Szpakowicz. 2004. Vocabu-
lary agreement among model summaries and source
documents. In Proceedings of the 2004 Document
Understanding Conference (DUC).
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
In LREC 2006.
Charles Fillmore. 1968. The case for case. In E. Bach
and R. T. Harms, editors, Universals in Linguistic
Theory, pages 1?88. Holt, Reinhart, and Winston,
New York.
Charles J. Fillmore. 1982. Frame semantics. Linguis-
tics in the Morning Calm, pages 111?137.
Pierre-Etienne Genest, Guy Lapalme, and Mehdi
Yousfi-Monod. 2009. Hextac: the creation of a
manual extractive run. In Proceedings of the Second
Text Analysis Conference, Gaithersburg, Maryland,
USA. National Institute of Standards and Technol-
ogy.
David Graff, Junbo Kong, Ke Chen, and Kazuaki
Maeda. 2005. English gigaword second edition.
Linguistic Data Consortium, Philadelphia.
Liwei He, Elizabeth Sanocki, Anoop Gupta, and
Jonathan Grudin. 1999. Auto-summarization of
audio-video presentations. In Proceedings of the
Seventh ACM International Conference on Multime-
dia. ACM.
Liwei He, Elizabeth Sanocki, Anoop Gupta, and
Jonathan Grudin. 2000. Comparing presentation
summaries: slides vs. reading vs. listening. In Pro-
ceedings of the SIGCHI Conference on Human Fac-
tors in Computing Systems, CHI ?00, pages 177?
184, New York, NY, USA. ACM.
Eduard Hovy, Chin-Yew Lin, Liang Zhou, and Junichi
Fukumoto. 2006. Automated summarization evalu-
ation with Basic Elements. In Proceedings of the 5th
International Conference on Language Resources
and Evaluation (LREC), pages 899?902.
IBM. IBM ILOG CPLEX Optimization Studio V12.4.
Hongyan Jing and Kathleen R. McKeown. 2000. Cut
and paste based text summarization. In Proceed-
ings of the 1st North American Chapter of the As-
sociation for Computational Linguistics Conference,
pages 178?185.
Kevin Knight and Daniel Marcu. 2000. Statistics-
based summarization-step one: Sentence compres-
sion. In Proceedings of the National Conference on
Artificial Intelligence.
Chin-Yew Lin and Eduard Hovy. 2000. The auto-
mated acquisition of topic signatures for text sum-
marization. In Proceedings of the 18th Conference
on Computational Linguistics - Volume 1, COLING
?00, pages 495?501, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Chin-Yew Lin and Eduard Hovy. 2003. The potential
and limitations of automatic sentence extraction for
summarization. In Proceedings of the HLT-NAACL
03 on Text Summarization Workshop. Association
for Computational Linguistics.
Chin Y. Lin. 2004. ROUGE: A package for automatic
evaluation of summaries. In Stan Szpakowicz and
Marie-Francine Moens, editors, Text Summarization
Branches Out: Proceedings of the ACL-04 Work-
shop, pages 74?81, Barcelona, Spain, July. Associa-
tion for Computational Linguistics.
Annie Louis and Ani Nenkova. 2009. Automatically
evaluating content selection in summarization with-
out human models. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
1241
Processing. Association for Computational Linguis-
tics.
Christopher D. Manning, Prabhakar Raghavan, and
Hinrich Schu?tze, 2008. Introduction to Information
Retrieval, chapter 17. Cambridge University Press.
Kathleen R. McKeown, Regina Barzilay, David Evans,
Vasileios Hatzivassiloglou, Judith L. Klavans, Ani
Nenkova, Carl Sable, Barry Schiffman, and Sergey
Sigelman. 2002. Tracking and summarizing news
on a daily basis with Columbia?s Newsblaster. In
Proceedings of the Second International Conference
on Human Language Technology Research, pages
280?285. Morgan Kaufmann Publishers Inc.
Ani Nenkova and Kathleen McKeown. 2003. Refer-
ences to named entities: a corpus study. In Com-
panion Volume of the Proceedings of HLT-NAACL
2003 - Short Papers. Association for Computational
Linguistics.
Ani Nenkova and Kathleen McKeown. 2011. Auto-
matic summarization. Foundations and Trends in
Information Retrieval, 5(2):103?233.
Ani Nenkova and Rebecca Passonneau. 2004. Evalu-
ating content selection in summarization: The pyra-
mid method. In Proceedings of the Human Lan-
guage Technology Conference of the North Ameri-
can Chapter of the Association for Computational
Linguistics: HLT-NAACL 2004, volume 2004, pages
145?152.
Dragomir R. Radev and Kathleen R. McKeown. 1998.
Generating natural language summaries from mul-
tiple on-line sources. Computational Linguistics,
24(3):470?500.
Horacio Saggion and Guy Lapalme. 2002. Generat-
ing indicative-informative summaries with SumUM.
Computational linguistics, 28(4):497?526.
Horacio Saggion, Juan-Manuel Torres-Moreno, Iria
Cunha, and Eric SanJuan. 2010. Multilingual sum-
marization evaluation without human models. In
Proceedings of the 23rd International Conference
on Computational Linguistics: Posters, pages 1059?
1067. Association for Computational Linguistics.
Peter Turney. 2001. Mining the web for synonyms:
PMI-IR versus LSA on TOEFL. In Proceedings of
the Twelth European Conference on Machine Learn-
ing (ECML-2001), pages 491?502.
Michael White, Tanya Korelsky, Claire Cardie, Vincent
Ng, David Pierce, and Kiri Wagstaff. 2001. Mul-
tidocument summarization via information extrac-
tion. In Proceedings of the First International Con-
ference on Human Language Technology Research.
Association for Computational Linguistics.
1242
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 11?13,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
The mathematics of language learning
Andra?s Kornai
Computer and Automation Research Institute
Hungarian Academy of Sciences
andras@kornai.com
Gerald Penn
Department of Computer Science
University of Toronto
gpenn@cs.utoronto.edu
James Rogers
Computer Science Department
Earlham College
jrogers@cs.earlham.edu
Anssi Yli-Jyra?
Department of Modern Languages
University of Helsinki
anssi.yli-jyra@helsinki.fi
Over the past decade, attention has gradu-
ally shifted from the estimation of parameters to
the learning of linguistic structure (for a survey
see Smith 2011). The Mathematics of Language
(MOL) SIG put together this tutorial, composed of
three lectures, to highlight some alternative learn-
ing paradigms in speech, syntax, and semantics in
the hopes of accelerating this trend.
Compounding the enormous variety of formal
models one may consider is the bewildering range
of ML techniques one may bring to bear. In addi-
tion to the surprisingly useful classical techniques
inherited from multivariate statistics such as Prin-
cipal Component Analysis (PCA, Pearson 1901)
and Linear Discriminant Analysis (LDA, Fisher
1936), computational linguists have experimented
with a broad range of neural net, nearest neighbor,
maxent, genetic/evolutionary, decision tree, max
margin, boost, simulated annealing, and graphical
model learners. While many of these learners be-
came standard in various domains of ML, within
CL the basic HMM approach proved surprisingly
resilient, and it is only very recently that deep
learning techniques from neural computing are be-
coming competitive not just in speech, but also
in OCR, paraphrase, sentiment analysis, parsing
and vector-based semantic representations. The
first lecture will provide a mathematical introduc-
tion to some of the fundamental techniques that
lie beneath these linguistic applications of neural
networks, such as: BFGS optimization, finite dif-
ference approximations of Hessians and Hessian-
free optimization, contrastive divergence and vari-
ational inference.
Lecture 1: The mathematics of
neural computing ? Penn
Recent results in acoustic modeling, OCR, para-
phrase, sentiment analysis, parsing and vector-
based semantic representations have shown that
natural language processing, like so many other
corners of artificial intelligence, needs to pay more
attention to neural computing.
I Gaussian Mixture Models
? Lagrange?s theorem
? Stochastic gradient descent
? typical acoustic models using GMMs and
HMMs
II Optimization theory
? Hessian matrices
? Broyden-Fletcher-Goldfarb-Shanno theory
? finite difference approximations of Hessians
? Hessian-free optimization
? Krylov methods
III Application: Product models
? products of Gaussians vs. GMMs
? products of ?experts?
? Gibbs sampling and Markov-chain Monte
Carlo
? contrastive divergence
IV Experimentation: Deep NNs for acoustic
modeling
? intersecting product models with Boltzmann
machines
? ?generative pre-training?
? acoustic modeling with Deep Belief Networks
? why DBNs work well
V Variational inference
? variational Bayes for HMMs
In spite of the enormous progress brought by
ML techniques, there remains a rather significant
range of tasks where automated learners cannot
yet get near human performance. One such is the
unsupervised learning of word structure addressed
by MorphoChallenge, another is the textual entail-
ment task addressed by RTE.
The second lecture recasts these and similar
problems in terms of learning weighted edges in a
sparse graph, and presents learning techniques that
seem to have some potential to better find spare fi-
nite state and near-FS models than EM. We will
provide a mathematical introduction to the Min-
imum Description Length (MDL) paradigm and
11
spectral learning, and relate these to the better-
known techniques based on (convex) optimization
and (data-oriented) memorization.
Lecture 2: Lexical structure
detection ? Kornai
While modern syntactic theory focuses almost en-
tirely on productive, rule-like regularities with
compositional semantics, the vast bulk of the infor-
mation conveyed by natural language, over 85%,
is encoded by improductive, irregular, and non-
compositional means, primarily by lexical mean-
ing. Morphology and the lexicon provide a rich
testing ground for comparing structure learning
techniques, especially as inferences need to be
based on very few examples, often just one.
I Motivation
? Why study structure?
? Why study lexical structure?
II Lexical structure
? Function words, content words
? Basic vocabulary (Ogden 1930, Swadesh 1950,
Yasseri et al2012)
? Estimation style
III Formal models of lexical semantics
? Associative (Findler 1979, Dumais 2005, CVS
models)
? Combinatorial (FrameNet)
? Algebraic (Kornai 2010)
IV Spectral learning
? Case frames and valency
? Spectral learning as data cleaning (Ng 2001)
? Brew and Schulte im Walde 2002 (German),
Nemeskey et al(Hungarian)
? Optionality in case frames
V Models with zeros
? Relating ungrammaticality and low probabil-
ity (Pereira 2000, Stefanowitsch 2006)
? Estimation errors, language distances (Kornai
1998, 2011)
? Quantization error
VI Minimum description length
? Kolmogorov complexity and universal gram-
mar (Clark 1994)
? MDL in morphology (Goldsmith 2000, Creutz
and Lagus 2002, 2005,...)
? MDL for weighted languages
? Ambiguity
? Discarding data ? yes, you can!
? Collapsing categories
VII New directions
? Spectral learning of HMMs (Hsu et al2009,
2012)
? of weighted automata (Balle and Mohri 2012)
? Feature selection, LASSO (Pajkossy 2013)
? Long Short-Term Memory (Monner and Reg-
gia 2012)
? Representation learning (Bengio et al2013)
Given the broad range of competing formal
models such as templates in speech, PCFGs and
various MCS models in syntax, logic-based and
association-based models in semantics, it is some-
what surprising that the bulk of the applied work
is still performed by HMMs. A particularly signifi-
cant case in point is provided by PCFGs, which
have not proved competitive with straight tri-
gram models. Undergirding the practical failure
of PCFGs is a more subtle theoretical problem,
that the nonterminals in better PCFGs cannot be
identified with the kind of nonterminal labels that
grammarians assume, and conversely, PCFGs em-
bodying some form of grammatical knowledge tend
not to outperform flatly initialized models that
make no use of such knowledge. A natural response
to this outcome is to retrench and use less power-
ful formal models, and the last lecture will be spent
in the subregular space of formal models even less
powerful than finite state automata.
Lecture 3: Subregular Languages
and Their Linguistic Relevance ?
Rogers and Yli-Jyra?
The difficulty of learning a regular or context-free
language in the limit from positive data gives a
motivation for studying non-Chomskyan language
classes. The lecture gives an overview of the tax-
onomy of the most important subregular classes of
languages and motivate their linguistic relevance
in phonology and syntax.
I Motivation
? Some classes of (sub)regular languages
? Learning (finite descriptions of) languages
? Identification in the limit from positive data
? Lattice leaners
II Finer subregular language classes
? The dot-depth hierarchy and the local and
piecewise hierarchies
? k-Local and k-Piecewise Sets
III Relevance to phonology
? Stress patterns
? Classifying subregular constraints
IV Probabilistic models of language
? Strictly Piecewise Distributions (Heinz and
Rogers 2010)
V Relevance to syntax
? Beyond the inadequate right-linear grammars
? Parsing via intersection and inverse morphism
12
? Subregular constraints on the structure anno-
tations
? Notions of (parameterized) locality in syntax.
The relevance of some parameterized subregular
language classes is shown through machine learn-
ing and typological arguments. Typological results
on a large set of languages (Heinz 2007, Heinz et al
2011) relate language types to the theory of sub-
regular language classes.
There are finite-state approaches to syn-
tax showing subregular properties. Although
structure-assigning syntax differs from phonotac-
tical constraints, the inadequacy of right-linear
grammars does not generalize to all finite-state
representations of syntax. The linguistic relevance
and descriptive adequacy are discussed, in particu-
lar, in the context of intersection parsing and con-
junctive representations of syntax.
Instructors
Anssi Yli-Jyra? is Academy Research Fellow of the
Academy of Finland and Visiting Fellow at Clare
Hall, Cambridge. His research focuses on finite-
state technology in phonology, morphology and
syntax. He is interested in weighted logic, depen-
dency complexity and machine learning.
James Rogers is Professor of Computer Science at
Earlham College, currently on sabbatical at the
Department of Linguistics and Cognitive Science,
University of Delaware. His primary research in-
terests are in formal models of language and for-
mal language theory, particularly model-theoretic
approaches to these, and in cognitive science.
Gerald Penn teaches computer science at the Uni-
versity of Toronto, and is a Senior Member of
the IEEE. His research interests are in spoken
language processing, human-computer interaction,
and mathematical linguistics.
Andra?s Kornai teaches at the Algebra Depart-
ment of the Budapest Institute of Technology,
and leads the HLT group at the Computer and
Automation Research Institute of the Hungarian
Academy of Sciences. He is interested in ev-
erything in the intersection of mathematics and
linguistics. For a list of his publications see
http://kornai.com/pub.html.
Online resources
Slides for the tutorial:
http://molweb.org/acl13tutorial.pdf
Bibliography:
http://molweb.org/acl13refs.pdf
Software:
http://molweb.org/acl13sw.pdf
13
Proceedings of the Workshop on Evaluation Metrics and System Comparison for Automatic Summarization, pages 28?35,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Ecological Validity and the Evaluation of Speech Summarization Quality
Anthony McCallum Cosmin Munteanu
University of Toronto National Research Council Canada
40 St. George Street 46 Dineen Drive
Toronto, ON, Canada Fredericton, NB, Canada
mccallum@cs.toronto.edu cosmin.munteanu@nrc-cnrc.gc.ca
Gerald Penn Xiaodan Zhu
University of Toronto National Research Council Canada
40 St. George Street 1200 Montreal Road
Toronto, ON, Canada Ottawa, ON, Canada
gpenn@cs.toronto.edu xiaodan.zhu@nrc-cnrc.gc.ca
Abstract
There is little evidence of widespread adoption of 
speech summarization systems. This may be due in 
part to the fact that the natural language heuristics 
used  to  generate  summaries  are  often  optimized 
with respect to a class of evaluation measures that, 
while  computationally  and  experimentally  inex-
pensive,  rely on subjectively selected gold stand-
ards  against  which  automatically  generated  sum-
maries  are  scored.  This  evaluation  protocol  does 
not take into account the usefulness of a summary 
in assisting the listener in achieving his or her goal.
     In this paper we study how current measures 
and methods for evaluating summarization systems 
compare to human-centric evaluation criteria. For 
this, we have designed and conducted an ecologic-
ally valid evaluation that determines the value of a 
summary  when  embedded  in  a  task,  rather  than 
how closely a summary resembles a gold standard. 
The results of our evaluation demonstrate  that  in 
the  domain  of  lecture  summarization,  the  well-
known  baseline  of  maximal  marginal  relevance 
(Carbonell and Goldstein, 1998) is statistically sig-
nificantly  worse  than  human-generated  extractive 
summaries,  and even worse than having no sum-
mary at  all in  a simple quiz-taking task. Priming 
seems to have no statistically significant effect on 
the usefulness  of the human summaries.  In addi-
tion, ROUGE scores and, in particular, the context-
free annotations that are often supplied to ROUGE 
as references, may not always be reliable as inex-
pensive proxies for ecologically valid evaluations. 
In fact, under some conditions, relying exclusively 
on ROUGE may even lead to scoring human-gen-
erated summaries that are inconsistent in their use-
fulness relative to using no summaries very favour-
ably.
1 Background and Motivation
Summarization  maintains  a  representation  of  an 
entire spoken document, focusing on those utter-
ances (sentence-like units) that are most important 
and therefore does not require the user to process 
everything that has been said. Our work focuses on 
extractive summarization where a selection of ut-
terances is chosen from the original spoken docu-
ment in order to make up a summary.
Current  speech  summarization  research  has 
made extensive use  of intrinsic  evaluation meas-
ures  such  as  F-measure,  Relative  Utility,  and 
ROUGE  (Lin,  2004),  which  score  summaries 
against  subjectively  selected  gold  standard  sum-
maries obtained using human annotators. These an-
notators are asked to arbitrarily select (in or out) or 
rank utterances, and in doing so commit to relative 
salience judgements with no attention to goal ori-
entation  and  no  requirement  to  synthesize  the 
meanings of larger units of structure into a coher-
ent message.
28
Given this  subjectivity,  current  intrinsic evalu-
ation measures are unable to properly judge which 
summaries  are  useful  for real-world applications. 
For  example,  intrinsic  evaluations  have  failed  to 
show that summaries created by algorithms based 
on complex linguistic and acoustic features are bet-
ter  than  baseline  summaries  created  by  simply 
choosing the positionally first utterances or longest 
utterances in a spoken document (Penn and Zhu, 
2008).  What  is  needed  is  an  ecologically  valid 
evaluation  that  determines  how valuable  a  sum-
mary is when embedded in a task, rather than how 
closely  a  summary  matches  the  subjective  utter-
ance level scores assigned by annotators.
   Ecological validity is "the ability of experiments 
to tell us how real people operate in the real world" 
(Cohen, 1995). This is often obtained by using hu-
man judges, but it is important to realize that the 
mere use of human subjects provides no guarantee 
as  to the ecological  validity  of their  judgements. 
When utterances are merely ranked with numerical 
scores  out  of  context,  for  example,  the  human 
judges who perform this task are not performing a 
task that they generally perform in their daily lives, 
nor does the task correspond to how they would 
create or use a good summary if they did have a 
need for one. In fact, there may not even be a guar-
antee that they  understand the task --- the notions 
of ?importance,? ?salience? and the like, when de-
fining the criterion by which utterances are selec-
ted, are not easy to circumscribe. Judgements ob-
tained in this fashion are no better than those of the 
generative linguists who leaned back in their arm-
chairs in the 1980s to introspect on the grammatic-
ality  of  natural  language  sentences.  The  field  of 
computational linguistics could only advance when 
corpora became electronically available to invest-
igate language that was written in an ecologically 
valid context.
   Ours is not the first ecologically valid experiment 
to be run in the context of speech summarization, 
however.  He et al (1999; 2000) conducted a very 
thorough  and  illuminating  study  of  speech  sum-
marization in the lecture domain that  showed (1) 
speech summaries are indeed very useful to have 
around, if they are done properly, and (2) abstract-
ive summaries do not seem to add any statistically 
significant advantage to the quality of a summary 
over  what  topline  extractive  summaries  can 
provide. This is very good news; extractive sum-
maries are worth creating. Our study extends this 
work by attempting to evaluate the relative quality 
of  extractive  summaries.  We  conjecture  that  it 
would be very difficult for this field to progress un-
less we have a means of accurately measuring ex-
tractive summarization quality. Even if the meas-
ure comes at great expense, it is important to do.
   Another noteworthy paper is that of Liu and Liu 
(2010), who, in addition to collecting human sum-
maries of six meetings, conducted a subjective as-
sessment of the quality of those summaries  with 
numerically  scored  questionnaires.  These  are 
known as Likert scales, and they form an important 
component of any human-subject study in the field 
of human-computer interaction. Liu and Liu (2010) 
cast  considerable doubt on the value of ROUGE 
relative to these questionnaires. We will focus here 
on an objective, task-based measure that typically 
complements those subjective assessments.
2 Spontaneous Speech
Spontaneous speech is often not linguistically well-
formed,  and  contains  disfluencies,  such  as  false 
starts,  filled pauses,  and repetitions.  Additionally, 
spontaneous speech is more vulnerable to automat-
ic speech recognition (ASR) errors, resulting in a 
higher  word  error  rate  (WER).  As  such,  speech 
summarization has the most potential for domains 
consisting  of  spontaneous  speech  (e.g.  lectures, 
meeting recordings). Unfortunately, these domains 
are not easy to evaluate compared to highly struc-
tured  domains  such  as  broadcast  news.  Further-
more,  in  broadcast  news,  nearly  perfect  studio 
acoustic  conditions  and  professionally  trained 
readers  results  in  low  ASR WER,  making  it  an 
easy domain to summarize. The result is that most 
research has been conducted in this domain. How-
ever,  a positional  baseline performs very well  in 
summarizing broadcast news (Christensen, 2004), 
meaning that simply taking the first  N utterances 
provides a very challenging baseline, questioning 
the value of summarizing this domain. In addition, 
the widespread availability  of written  sources  on 
the same topics means that there is not a strong use 
case for speech summarization over simply sum-
marizing the equivalent  textual  articles on which 
the news  broadcasts  were  based.   This  makes  it 
even more difficult to preserve ecological validity.
University lectures present a much more relev-
ant domain, with less than ideal acoustic conditions 
but  structured  presentations  in  which  deviation 
29
from written sources (e.g., textbooks) is common-
place.  Here,  a  positional  baseline  performs  very 
poorly. The lecture domain also lends itself well to 
a  task-based  evaluation  measure;  namely  univer-
sity level quizzes or exams. This constitutes a real-
world problem in a domain that is also representat-
ive of other spontaneous speech domains that can 
benefit from speech summarization.
3 Ecologically Valid Evaluation
As pointed out by Penn and Zhu (2008), current 
speech summarizers have been optimized to per-
form an utterance selection task that may not ne-
cessarily reflect how a summarizer is able to cap-
ture the goal orientation or purpose of the speech 
data. In our study, we follow methodologies estab-
lished in the field of Human-Computer Interaction 
(HCI) for evaluating an algorithm or system ? that 
is, determining the benefits a system brings to its 
users, namely usefulness, usability, or utility, in al-
lowing a user to reach a specific goal. Increasingly, 
such user-centric evaluations are carried out within 
various  natural  language  processing  applications 
(Munteanu et  al.,  2006).  The  prevailing  trend in 
HCI  is  for  conducting  extrinsic  summary  evalu-
ations (He et al, 2000; Murray et al, 2008; Tucker 
et al, 2010), where the value of a summary is de-
termined by how well the summary can be used to 
perform a specific task rather than comparing the 
content of a summary to an artificially created gold 
standard. We have conducted an ecologically valid 
evaluation of speech summarization that has evalu-
ated summaries  under real-world conditions, in a 
task-based manner.
The university lecture domain is an example of a 
domain where summaries are an especially suitable 
tool  for  navigation.  Simply  performing  a  search 
will not result in the type of understanding required 
of students in their lectures. Lectures have topics, 
and there is a clear communicative goal. For these 
reasons, we have chosen this domain for our evalu-
ation. By using actual university lectures as well as 
university students representative of the users who 
would make use of a speech summarization system 
in this domain, all results obtained are ecologically 
valid.
3.1Experimental Overview
We conducted a within-subject experiment where 
participants  were  provided  with  first  year  soci-
ology university lectures on a lecture browser sys-
tem installed on a desktop computer. For each lec-
ture, the browser made accessible the audio, manu-
al transcripts, and an optional summary. Evaluation 
of a summary was based on how well the user of 
the summary was able to complete a quiz based on 
the content of the original lecture material.
It is important to note that not all extrinsic eval-
uation is ecologically valid.  To ensure ecological 
validity in this study, great care was taken to en-
sure that human subjects were placed under condi-
tions that result in behavior that would be expected 
in actual real-world tasks.
3.2Evaluation
Each quiz consisted of 12 questions, and were de-
signed to be representative of what students were 
expected to learn in the class, incorporating factual 
questions  only,  to  ensure  that  variation  in  parti-
cipant  intelligence had a minimal impact  on res-
ults.  In  addition,  questions  involved  information 
that was distributed equally throughout the lecture, 
but at the same time not linearly in the transcript or 
audio  slider,  which  would  have  allowed  parti-
cipants to predict where the next answer might be 
located. Finally, questions were designed to avoid 
content that was thought to be common knowledge 
in  order  to  minimize  the  chance  of  participants 
having previous knowledge of the answers.
All  questions  were  short  answer  or  fill-in-the-
blank. Each quiz consisted of an equal number of 
four distinct  types  of questions,  designed so that 
performing a simple search would not be effective, 
though  no  search  functionality  was  provided. 
Question types do not appear in any particular or-
der on the quiz and were not grouped together.
Type  1: These  questions  can  be  answered 
simply  by  looking  at  the  slides.  As  such,  these 
questions  could  be  answered  correctly  with  or 
without a summary as slides were available in all 
conditions.
Type 2:  Slides provide an indication of where 
the content required to answer these questions are 
located. Access to the corresponding utterances is 
still required to find the answer to the questions.
30
Type 3: Answers to these questions can only be 
found  in  the  transcript  and  audio.  The  slides 
provide no hint as to where the relevant content is 
located.
Type 4: These questions are more complicated 
and require a certain level of topic comprehension. 
These questions often require connecting concepts 
from various portions of the lecture. These ques-
tions are more difficult and were included to min-
imize  the chance  that  participants  would already 
know the answer to questions without watching the 
lecture.
A teaching assistant for the sociology class from 
which  our  lectures  were  obtained  generated  the 
quizzes  used in the evaluation.  This teaching as-
sistant had significant experience in the course, but 
was not involved in the design of this study and did 
not have any knowledge relating to our hypotheses 
or  the  topic  of  extractive  summarization.  These 
quizzes provided an ecologically valid quantitative 
measure of whether a given summary was useful. 
Having this evaluation metric in place, automated 
summaries  were  compared  to  manual  summaries 
created by each participant in a previous session.
3.3Participants
Subjects  were  recruited  from  a  large  university 
campus,  and  were  limited  to  undergraduate  stu-
dents  who  had  at  least  two  terms  of  university 
studies,  to  ensure  familiarity  with  the  format  of 
university-level lectures and quizzes. Students who 
had taken the first year sociology course we drew 
lectures  from  were  not  permitted  to  participate. 
The study was conducted with 48 participants over 
the  course  of  approximately  one  academic 
semester.
3.4Method
Each evaluation session began by having a parti-
cipant perform a short warm-up with a portion of 
lecture content, allowing the participant to become 
familiar with the lecture browser interface. Follow-
ing  this,  the  participant  completed  four  quizzes, 
one  for  each  of  four  lecture-condition  combina-
tions. There were a total of four lectures and four 
conditions.  Twelve  minutes  were  given  for  each 
quiz. During this time, the participant was able to 
browse the audio, slides, and summary. Each lec-
ture was about forty minutes in length, establishing 
a time constraint. Lectures and conditions were ro-
tated using a Latin square for counter balancing. 
All participants completed each of the four condi-
tions.
One week prior to his or her evaluation session, 
each participant was brought in and asked to listen 
to  and  summarize  the  lectures  beforehand.  This 
resulted  in  the  evaluation  simulating  a  scenario 
where  someone  has  heard  a  lecture  at  least  one 
week in the past and may or may not remember the 
content during an exam or quiz. This is similar to 
conditions most university students experience.
3.5Conditions
The lecture audio recordings were manually tran-
scribed and segmented into utterances, determined 
by 200 millisecond pauses,  resulting in segments 
that  correspond  to  natural  sentences  or  phrases. 
The task of summarization consisted of choosing a 
set of utterances for inclusion in a summary (ex-
tractive summarization), where the total summary 
length was bounded by 17-23% of the words in the 
lecture;  a percentage typical  to most  summariza-
tion scoring tasks. All participants were asked to 
make use of the browser interface for four lectures, 
one for each of the following conditions:  no sum-
mary,  generic  manual summary,  primed manual  
summary, and automatic summary.
No  summary: This condition  served  as  a 
baseline where no summary was provided, but par-
ticipants  had  access  to  the  audio  and  transcript. 
While  all  lecture  material  was  provided,  the 
twelve-minute time constraint made it impossible 
to listen to the lecture in its entirety.
Generic  manual  summary: I  this  condition, 
each  participant  was  provided  with  a  manually 
generated summary. Each summary was created by 
the participant him or herself in a previous session. 
Only  audio  and text  from the  in-summary  utter-
ances  were  available  for  use.  This  condition 
demonstrates how a manually created summary is 
able to aid in the task of taking a quiz on the sub-
ject matter.
Primed manual summary: Similar to above, in 
this condition, a summary was created manually by 
selecting a set of utterances from the lecture tran-
script.  For  primed  summaries,  full  access  to  a 
priming quiz, containing all of the questions in the 
evaluation quiz as well as several additional ques-
tions, was available  at the time of summary cre-
31
ation. This determines the value of creating sum-
maries with a particular task in mind, as opposed to 
simply choosing utterances that are felt to be most 
important or salient.
Automatic  summary: The  procedure  for  this 
condition was identical to the generic manual sum-
mary condition from the point of view of the parti-
cipant.  However, during the evaluation phase, an 
automatically generated summary was provided in-
stead of the summary that the participant created 
him or herself. The algorithm used to generate this 
summary was an implementation of  MMR  (Car-
bonell and Goldstein, 1998). Cosine similarity with 
tf-idf term weighting was used to calculate similar-
ity. Although the redundancy component of MMR 
makes  it  especially  suitable  for  multi-document 
summarization,  there  is  no  negative  effect  if  re-
dundancy is not an issue. It is worth noting that our 
lectures are longer than material typically summar-
ized, and lectures in general are more likely to con-
tain  redundant  material  than  a  domain  such  as 
broadcast news. There was only one MMR sum-
mary generated for each lecture, meaning that mul-
tiple participants made use of identical summaries. 
The automatic summary was created by adding the 
highest scoring utterances one at a time until the 
sum of the length of all of the selected utterances 
reached 20% of the number of words in the origin-
al  lecture.  MMR was  chosen  as  it  is  commonly 
used  in  summarization.  MMR  is  a  competitive 
baseline,  even  among  state-of-art  summarization 
algorithms, which tend to correlate well with it.
What  this  protocol  does  not  do  is  pit  several 
strategies  for  automatic  summary  generation 
against  each  other.   That  study,  where  more  ad-
vanced summarization algorithms will also be ex-
amined, is forthcoming.  The present experiments 
have the collateral benefit  of  serving as a means 
for collecting ecologically valid human references 
for that study.
3.6Results
Quizzes were scored by a teaching assistant for the 
sociology  course  from  which  the  lectures  were 
taken. Quizzes were marked as they would be in 
the  actual  course  and  each  question  was  graded 
with equal  weight  out  of  two marks.  The scores 
were then converted to a percentage. The resulting 
scores (Table 1) are 49.3+-17.3% for the  no sum-
mary condition,  48.0+-16.2%  for  the  generic  
manual  summary  condition,  49.1+-15.2% for  the 
primed summary  condition,  and 41.0+-16.9% for 
MMR. These scores are lower than averages expec-
ted in a typical university course. This can be par-
tially  attributed  to  the  existence  of  a  time  con-
straint.
Condition Average Quiz Score
no summary 49.3+-17.3%
generic manual summary 48.0+-16.2%
primed manual summary 49.1+-15.2%
automatic summary (MMR) 41.0+-16.9%
Table 1. Average Quiz Scores
Execution  of  the  Shapiro-Wilk Test  confirmed 
the scores are normally distributed and Mauchly's 
Test of Sphericity indicates that the sphericity as-
sumption holds. Skewness and Kurtosis tests were 
also  employed  to  confirm  normality.  A repeated 
measures  ANOVA determined  that  scores  varied 
significantly between conditions (F(3,141)=5.947, 
P=0.001). Post-hoc tests using the Bonferroni cor-
rection  indicate  that  the  no  summary,  generic  
manual  summary,  and  primed  manual  summary 
conditions all  resulted  in  higher  scores  than  the 
automatic (MMR) summary condition. The differ-
ence  is  significant  at  P=0.007,  P=0.014 and 
P=0.012 respectively. Although normality was as-
sured, the Friedman Test further confirms a signi-
ficant  difference  between  conditions 
(?2(3)=11.684, P=0.009).
4 F-measure
F-measure is an evaluation metric that balances 
precision and recall which has been used to evalu-
ate summarization. Utterance level F-measure 
scores were calculated using the same summaries 
used in our human evaluation. In addition, three 
annotators were asked to create conventional gold 
standard summaries using binary selection. Annot-
ators were not primed in any sense, did not watch 
the lecture videos, and had no sense of the higher 
level purpose of their annotations. We refer to the 
resulting summaries as context-free as they were 
not created under ecologically valid conditions. F-
measure was also calculated with reference to 
these.
The F-measure results (Table 2) point out a few 
interesting phenomena. Firstly, when evaluating a 
32
given  peer  summary  type  with  the  same  model 
type,  the  generic-generic  scores  are  higher  than 
both  the  primed-primed and  context-free-con-
text-free summaries. This means that generic sum-
maries tend to share more utterances with each oth-
er, than primed summaries do, which are more var-
ied. This seems unintuitive at first, but could po-
tentially be explained by the possibility that differ-
ent participants focused on different aspects of the 
priming quiz, due to either perceived importance, 
or lack of time (or summary space) to address all 
of the priming questions.
Peer Type Model Type Average F-measure
generic generic 0.388 
primed generic 0.365 
MMR generic 0.214 
generic primed 0.365 
primed primed 0.374 
MMR primed 0.209 
generic context-free 0.371 
primed context-free 0.351 
MMR context-free 0.243 
context-free context-free 0.374 
Table 2. Average F-measure
We  also  observe  that  generic  summaries  are 
more similar to conventionally annotated (context-
free) summaries than either primed or MMR are. 
This  makes  sense  and  also  confirms  that  even 
though primed summaries do not significantly out-
perform generic summaries in the quiz taking task, 
they are inherently distinguishable from each other.
Furthermore,  when  evaluating  MMR using  F-
measure,  we  see that  MMR summaries  are  most 
similar to the context-free summaries, whose utter-
ance selections can be considered somewhat arbit-
rary.  Our  quiz  results  confirm MMR is  signific-
antly worse  than  generic  and primed summaries. 
This casts doubt on the practice of using similarly 
annotated  summaries  as  gold  standards  for  sum-
marization evaluation using ROUGE.
5 ROUGE Evaluation
More  common  than  F-measure,  ROUGE  (Lin, 
2004) is often used to evaluate summarization. Al-
though Lin (2004) claimed to have demonstrated 
that ROUGE correlates well with human summar-
ies,  both  Murray  et  al.  (2005),  and Liu  and Liu 
(2010) have cast doubt upon this.  It is important to 
acknowledge, however, that ROUGE is actually a 
family of measures, distinguished not only by the 
manner  in  which  overlap  is  measured  (1-grams, 
longest  common  subsequences,  etc.),  but  by  the 
provenience of the summaries that are provided to 
it as references.  If these are not ecologically valid, 
there is no sense in holding ROUGE accountable 
for an erratic result.
   To examine how ROUGE fairs under ecologic-
ally  valid  conditions,  we  calculated  ROUGE-1, 
ROUGE-2,  ROUGE-L, and ROUGE-SU4 on our 
data using the standard options outlined in previ-
ous DUC evaluations. ROUGE scores were calcu-
lated  for  each  of  the  generic  manual  summary, 
primed manual summary, and automatic summary 
conditions.  Each  summary  in  a  given  condition 
was  evaluated  once  against  the  generic  manual  
summaries  and  once  using  the  primed  manual 
summaries.  Similar  to  Liu  and  Liu  (2010), 
ROUGE  evaluation  was  conducted  using  leave-
one-out on the model summary type and averaging 
the results.
In addition to calculating ROUGE on the sum-
maries from our ecologically valid evaluation, we 
also followed  more  conventional  ROUGE evalu-
ation  and  used  the  same  context-free  annotator 
summaries as were used in our F-measure calcula-
tions above. Using these context-free summaries, 
the original  generic  manual,  primed manual,  and 
automatic  summaries  were  evaluated  using 
ROUGE.  The  result  of  these  evaluations  are 
presented in Table 3.
Looking at the ROUGE scores, we can see that 
when evaluated by each type of model summary, 
MMR  performs  worse  than  either  generic  or 
primed manual summaries. This is consistent with 
our quiz results, and perhaps shows that ROUGE 
may be able to distinguish human summaries from 
MMR.  Looking  at  the  generic-generic,  primed-
primed,  and  context-free-context-free scores,  we 
can get a sense of how much agreement there was 
between summaries. It is not surprising that con-
text-free  annotator  summaries  showed  the  least 
agreement,  as  these  summaries  were  generated 
with no higher purpose in mind. This suggests that 
using annotators to generate gold standards in such 
a manner is not ideal.  In addition, real world ap-
plications  for  summarization  would  conceivably 
33
rarely consist of a situation where a summary was 
created for no apparent reason. More interesting is 
the observation that, when measured by ROUGE, 
primed summaries have less in common with each 
other than generic summaries do. The difference, 
however,  is  less  pronounced  when  measured  by 
ROUGE than by F-measure. This is likely due to 
the fact that ROUGE can account for semantically 
similar utterances.
Peer 
type
Model type R-1 R-2 R-L R-SU4
generic generic 0.75461 0.48439 0.75151 0.51547 
primed generic 0.74408 0.46390 0.74097 0.49806 
MMR generic 0.71659 0.40176 0.71226 0.44838 
generic primed 0.74457 0.46432 0.74091 0.49844 
primed primed 0.74693 0.46977 0.74344 0.50254 
MMR primed 0.70773 0.38874 0.70298 0.43802 
generic context-free 0.72735 0.46421 0.72432 0.49573 
primed context-free 0.71793 0.44325 0.71472 0.47805 
MMR context-free 0.69233 0.37600 0.68813 0.42413 
context-
free
context-free 0.70707 0.44897 0.70365 0.48019 
Table 3. Average ROUGE Scores
5.1Correlation with Quiz Scores
In order to assess the ability of ROUGE to predict 
quiz scores, we measured the correlation between 
ROUGE scores and quiz scores on a per participant 
basis. Similar to Murray et al (2005), and Liu and 
Liu (2010), we used Spearman?s rank coefficient 
(rho) to measure the correlation between ROUGE 
and our human evaluation. Correlation was meas-
ured both by calculating Spearman's rho on all data 
points (?all? in Table 4) and by performing the cal-
culation separately for each lecture and averaging 
the results (?avg?). Significant rho values (p-value 
less than 0.05) are shown in bold.
Note that there are not many bolded values, in-
dicating  that  there  are  few  (anti-)correlations 
between quiz scores and ROUGE. The rho values 
reported by Liu and Liu (2010) correspond to the 
?all?  row of  our  generic-context-free  scores  (Liu 
and Liu (2010) did not report ROUGE-L), and we 
obtained  roughly  the  same  scores  as  they
did.  In  contrast  to  this,  our  "all"  generic-generic 
correlations are very low. It is possible that the lec-
tures condition the parameters of the correlation to 
such an extent that fitting all of the quiz-ROUGE
pairs to the same correlation across lectures is un-
reasonable. It may therefore be more useful to look 
at rho  values computed by lecture. For these val-
ues, our R-SU4 scores are not as high relative to R-
1 and R-2 as those reported by Liu and Liu (2010). 
It is also worth noting that the use of context-free 
binary selections as a reference results in increased 
correlation for generic summaries, but substantially 
decreases correlation for primed summaries.
With the exception that generic references prefer 
generic  summaries  and  primed  references  prefer 
primed  summaries,  all  other  values  indicate  that 
both generic and primed summaries are better than 
MMR.  However,  instead  of  ranking  summary 
types,  what  is  important  here  is  the  ecologically 
valid quiz scores.  Our data provides no evidence 
that ROUGE scores accurately predict quiz scores. 
6 Conclusions
We have presented an investigation into how cur-
rent  measures  and  methodologies  for  evaluating 
summarization systems compare to human-centric 
evaluation  criteria.  An  ecologically-valid  evalu-
ation was conducted that determines the value of a 
summary  when  embedded  in  a  task,  rather  than 
how closely a summary resembles a gold standard. 
The  resulting  quiz  scores  indicate  that  manual 
summaries  are  significantly  better  than  MMR. 
ROUGE scores were calculated using the summar-
ies created in the study. In addition, more conven-
tional context-free annotator summaries were also 
used in ROUGE evaluation. Spearman's rho indic-
ated  no  correlation  between  ROUGE scores  and 
our ecologically valid quiz scores. The results offer 
evidence that ROUGE scores and particularly con-
text-free  annotator-generated  summaries  as  gold 
standards may not always be reliably used in place 
of an ecologically valid evaluation.
34
Peer type Model type R-1 R-2 R-L R-SU4
generic generic all 0.017 0.066 0.005 0.058 
lec1 0.236 0.208 0.229 0.208 
lec2 0.276 0.28 0.251 0.092 
lec3 0.307 0.636 0.269 0.428 
lec4 0.193 -0.011 0.175 0.018 
avg 0.253 0.278 0.231 0.187 
primed generic all -0.097 -0.209 -0.090 -0.192 
lec1 -0.239 -0.458 -0.194 -0.458 
lec2 -0.306 -0.281 -0.306 -0.316 
lec3 0.191 0.142 0.116 0.255 
lec4 -0.734 -0.78 -0.769 -0.78 
avg -0.272 -0.344 -0.288 -0.325 
generic primed all 0.009 0.158 -0.004 0.133 
lec1 0.367 0.247 0.367 0.162 
lec2 0.648 0.425 0.634 0.304 
lec3 0.078 0.417 0.028 0.382 
lec4 0.129 0.079 0.115 0.025 
avg 0.306 0.292 0.286 0.218 
primed primed all 0.161 0.042 0.161 0.045 
lec1 0.042 -0.081 0.042 -0.194 
lec2 0.238 0.284 0.259 0.284 
lec3 0.205 0.12 0.205 0.12 
lec4 0.226 0.423 0.314 0.423 
avg 0.178 0.187 0.205 0.158 
generic con-
text-free
all 0.282 0.306 0.265 0.347 
lec1 -0.067 0.296 -0.004 0.325 
lec2 0.414 0.414 0.438 0.319 
lec3 0.41 0.555 0.41 0.555 
lec4 0.136 0.007 0.136 0.054 
avg 0.223 0.318 0.245 0.313 
primed con-
text-free
all -0.146 -0.282 -0.151 -0.305 
lec1 0.151 -0.275 0.151 -0.299 
lec2 -0.366 -0.611 -0.366 -0.636 
lec3 0.273 0.212 0.273 0.202 
lec4 -0.815 -0.677 -0.825 -0.755 
avg -0.189 -0.338 -0.192 -0.372 
Table 4. Correlation (Spearman's rho) between Quiz 
Scores and ROUGE
7 References 
J. Carbonell and J. Goldstein. 1998. The use of mmr, di-
versity-based reranking for reordering documents and 
producing summaries. In Proceedings of the 21st an-
nual  international  ACM SIGIR  conference  on  Re-
search  and  development  in  information  retrieval, 
335-336, ACM.
P. R. Cohen. 1995.  Empirical methods for artificial in-
telligence.  Volume 55. MIT press Cambridge, Mas-
sachusetts.
H.  Christensen,  B.  Kolluru,  Y.  Gotoh,  and S.  Renals. 
2004. From text summarisation to style-specific sum-
marisation for broadcast news. Advances in Informa-
tion Retrieval, 223-237.
L. He, E. Sanocki, A. Gupta, and J. Grudin. 1999. Auto-
summarization of audio-video presentations. In  Pro-
ceedings  of  the  seventh  ACM international  confer-
ence on Multimedia (Part 1), 489-498. ACM.
L. He, E. Sanocki, A. Gupta, and J. Grudin. 2000. Com-
paring presentation summaries: slides vs. reading vs. 
listening. In Proc. of the SIGCHI, 177-184, ACM.
C. Lin.  2004.  Rouge:  a  package for  automatic  evalu-
ation of summaries. In Proc. of ACL, Text Summariz-
ation Branches Out Workshop, 74?81.
F. Liu and Y. Liu. 2010. Exploring correlation between 
rouge and human evaluation on meeting summaries. 
Audio,  Speech,  and  Language  Processing,  IEEE  
Transactions on, 18(1):187-196.
C. Munteanu,  R.  Baecker,  G. Penn,  E.  Toms,  and  D. 
James. 2006. The effect of speech recognition accur-
acy rates on the usefulness and usability of webcast 
archives. In  Proceedings of the SIGCHI conference 
on Human Factors in  computing systems,  493-502, 
ACM.
G. Murray, S. Renals, J. Carletta, and J. Moore. 2005. 
Evaluating automatic summaries of meeting record-
ings. In Proc. of the ACL 2005 MTSE Workshop, Ann 
Arbor, MI, USA, 33-40.
G.  Murray,  T.  Kleinbauer,  P.  Poller,  S.  Renals,  J. 
Kilgour, and T. Becker. 2008. Extrinsic summariza-
tion  evaluation:  A  decision  audit  task.  Machine 
Learning for Multimodal Interaction, 349-361.
G. Penn and X. Zhu. 2008. A critical reassessment of 
evaluation baselines for speech summarization. Proc.  
of ACL-HLT.
S. Tucker, O. Bergman, A. Ramamoorthy, and S. Whit-
taker.  2010.  Catchup:  a  useful  application  of  time-
travel in meetings. In Proc. of CSCW, 99-102, ACM.
S. Tucker and S. Whittaker.  2006.  Time is  of  the  es-
sence:  an  evaluation  of  temporal  compression  al-
gorithms. In Proc. of the SIGCHI, 329-338, ACM.
35
Proceedings of the 13th Meeting on the Mathematics of Language (MoL 13), pages 83?92,
Sofia, Bulgaria, August 9, 2013. c?2013 Association for Computational Linguistics
Why Letter Substitution Puzzles are Not Hard to Solve: A Case Study in
Entropy and Probabilistic Search-Complexity
Eric Corlett
University of Toronto
10 King?s College Rd., Room 3302
Toronto, ON, Canada M5S 3G4
ecorlett@cs.toronto.edu
Gerald Penn
University of Toronto
10 King?s College Rd., Room 3302
Toronto, ON, Canada M5S 3G4
gpenn@cs.toronto.edu
Abstract
In this paper we investigate the theoretical
causes of the disparity between the theo-
retical and practical running times for the
A? algorithm proposed in Corlett and Penn
(2010) for deciphering letter-substitution
ciphers. We argue that the difference seen
is due to the relatively low entropies of the
probability distributions of character tran-
sitions seen in natural language, and we
develop a principled way of incorporat-
ing entropy into our complexity analysis.
Specifically, we find that the low entropy
of natural languages can allow us, with
high probability, to bound the depth of the
heuristic values expanded in the search.
This leads to a novel probabilistic bound
on search depth in these tasks.
1 Introduction
When working in NLP, we can find ourselves
using algorithms whose worst-case running time
bounds do not accurately describe their empiri-
cally determined running times. Specifically, we
can often find that the algorithms that we are us-
ing can be made to run efficiently on real-world
instances of their problems despite having theo-
retically high running times. Thus, we have an ap-
parent disparity between the theoretical and prac-
tical running times of these algorithms, and so we
must ask why these algorithms can provide results
in a reasonable time frame. We must also ask to
what extent we can expect our algorithms to re-
main practical as we change the downstream do-
mains from which we draw problem instances.
At a high level, the reason such algorithms can
work well in the real world is that the real world
applications from which we draw our inputs do
not tend to include the high complexity inputs. In
other words, our problem space either does not
cover all possible inputs to the algorithm, or it
does, but with a probability distribution that gives
a vanishingly small likelihood to the ?hard? inputs.
Thus, it would be beneficial to incorporate into our
running time analysis the fact that our possible in-
puts are restricted, even if only restricted in rela-
tive frequency rather than in absolute terms.
This means that any running time that we ob-
serve must be considered to be dependent on the
distribution of inputs that we expect to sample
from. It probably does not come as a surprise that
any empirical analysis of running time carries with
it the assumption that the data on which the tests
were run are typical of the data which we expect
to see in practice. Yet the received wisdom on the
asymptotic complexity of algorithms in computa-
tional linguistics (generally what one might see
in an advanced undergraduate algorithms curricu-
lum) has been content to consider input only in
terms of its size or length, and not the distribution
from which it was sampled. Indeed, many algo-
rithms in NLP actually take entire distributions as
input, such as language models. Without a more
mature theoretical understanding of time complex-
ity, it is not clear exactly what any empirical run-
ning time results would mean. A worst-case com-
plexity result gives a guarantee that an algorithm
will take no more than a certain number of steps
to complete. An average-case result gives the ex-
pected number of steps to complete. But an empir-
ical running time found by sampling from a distri-
bution that is potentially different from what the
algorithm was designed for is only a lesson in how
truly different the distribution is.
It is also common for the theoretical study of
asymptotic time complexity in NLP to focus on
the worst-case complexity of a problem or algo-
rithm rather than an expected complexity, in spite
of the existence for now over 20 years of methods
for average-case analysis of an algorithm. Even
these, however, often assume a uniform distribu-
83
tion over input, when in fact the true expectation
must consider the probability distribution that we
will draw the inputs from. Uniform distributions
are only common because we may not know what
the distribution is beforehand.
Ideally, we should want to characterize the run-
ning time of an algorithm using some known prop-
erties of its input distribution, even if the precise
distribution is not known. Previous work that at-
tempts this does exist. In particular, there is a vari-
ant of analysis referred to as smoothed analysis
which gives a bound on the average-case running
time of an algorithm under the assumption that all
inputs are sampled with Gaussian measurement er-
ror. As we will argue in Section 2, however, this
approach is of limited use to us.
We instead approach the disparity of theoretical
and practical running time by making use of statis-
tics such as entropy, which are taken from the in-
put probability distributions, as eligible factors in
our analysis of the running time complexity. This
is a reasonable approach to the problem, in view of
the numerous entropic studies of word and charac-
ter distributions dating back to Shannon.
Specifically, we analyze the running time of the
A? search algorithm described in Corlett and Penn
(2010). This algorithm deciphers text that has
been enciphered using a consistent letter substitu-
tion, and its running time is linear in the length of
the text being deciphered, but theoretically expo-
nential in the size of the input and output alpha-
bets. This na??ve theoretical analysis assumes that
characters are uniformly distributed, however. A
far more informative bound is attainable by mak-
ing reference to the entropy of the input. Be-
cause the algorithm takes a language model as one
of its inputs (the algorithm is guaranteed to find
the model-optimal letter substitution over a given
text), there are actually two input distributions: the
distribution assumed by the input language model,
and the distribution from which the text to be de-
ciphered was sampled. Another way to view this
problem is as a search for a permutation of letters
as the outcomes of one distribution such that the
two distributions are maximally similar. So our
informative bound is attained through reference to
the cross-entropy of these two distributions.
We first formalize our innate assumption that
these two distributions are similar, and build an
upper bound for the algorithm?s complexity that
incorporates the cross-entropy between the two
distributions. The analysis concludes that, rather
than being exponential in the length of the input or
in the size of the alphabets, it is merely exponen-
tial in the cross-entropy of these two distributions,
thus exposing the importance of their similarity.
Essentially, our bound acts as a probability distri-
bution over the necessary search depth.
2 Related Work
The closest previous work to the analysis pre-
sented here is the use of smoothed analysis to ex-
plain the tractable real-world running time of a
number of algorithms with an exponential worst-
case complexity. These algorithms include the
simplex algorithm, as described by Spielman and
Teng (2004), the k-means clustering algorithm, as
described by Arthur et al (2009) and others. As
in our current approach, smoothed analysis works
by running a general average-case analysis of the
algorithms without direct knowledge of the distri-
bution from which the problem inputs have been
drawn. The assumption made in smoothed anal-
ysis is that every input has been read with some
Gaussian measurement error. That is, in a typi-
cal worst-case analysis, we may have an adversary
choose any input for our algorithm, after which we
must calculate how bad the resulting running time
might be, but in a smoothed analysis, the adver-
sary gives us input by placing it into the real world
so that we may measure it, and this measurement
adds a small error drawn from a Gaussian dis-
tribution to the problem instance. The point of
smoothed analysis is to find the worst average-case
running time, under these conditions, that the ad-
versary can subject us to. Thus the analysis is an
average case, subject to this error, of worst cases.
In the papers cited above, this method of analysis
was able to drop running times from exponential
to polynomial.
It is unfortunate that this approach does not
readily apply to many of the algorithms that we
use in NLP. To see why this is, simply note that
we can only add a small Gaussian error to our in-
puts if our inputs themselves are numerical. If the
inputs to our algorithms are discrete, say, in the
form of strings, then Gaussian errors are not mean-
ingful. Rather, we must ask what sort of error we
can expect to see in our inputs, and to what extent
these errors contribute to the running time of our
algorithms. In the case of decipherment, ?error?
is committed by substituting one character for an-
84
other consistently.
The strongest known result on the search com-
plexity of A? is given in Pearl (1984). This work
found that, under certain assumptions, a bound on
the absolute error between the heuristic used and
the true best cost to reach the goal yields a polyno-
mial worst-case depth for the search. This happens
when the bound is constant across search instances
of different sizes. On the other hand, if the relative
error does not have this constant bound, the search
complexity can still be exponential. This analy-
sis assumes that the relative errors in the heuristic
are independent between nodes of the search tree.
It is also often very difficult even to calculate the
value of a heuristic that possesses such a bound,
as it might involve calculating the true best cost,
which can be as difficult as completely solving a
search problem instance (Korf et al, 2001). Thus,
most practical heuristics still give rise to theoreti-
cally exponential search complexities in this view.
In Korf and Reid (1998) and Korf et al (2001),
on the other hand, several practical problems are
treated, such as random k-SAT, Rubik?s cubes, or
sliding tile puzzles, which are not wholly unlike
deciphering letter substitution puzzles in that they
calculate permutations, and therefore can assume,
as we do, that overall time complexity directly cor-
responds to the number of nodes visited at differ-
ent depths in the search tree that have a heuris-
tic low enough to guarantee node expansion. But
their analysis assumes that it is possible to both es-
timate and use a probability distribution of heuris-
tic values on different nodes of the search graph,
whereas in our task, this distribution is very dif-
ficult to sample because almost every node in the
search graph has a worse heuristic score than the
goal does, and would therefore never be expanded.
Without an accurate idea of what the distribution
of the heuristic is, we cannot accurately estimate
the complexity of the algorithm. On the other
hand, their analysis makes no use of any estimates
of the cost of reaching the goal, because the prac-
tical problems that they consider do not allow for
particularly accurate estimates. In our treatment,
we find that the cost to reach the goal can be esti-
mated with high probability, and that this estimate
is much less than the cost of most nodes in the
search graph. These different characteristics allow
us to formulate a different sort of bound on the
search complexity for the decipherment problem.
3 The Algorithm
We now turn to the algorithm given in Corlett and
Penn (2010) which we will investigate, and we ex-
plain the model we use to find our bound.
The purpose of the algorithm is to allow us to
read a given ciphertext C which is assumed to
be generated by putting an unknown plaintext P
through an unknown monoalphabetic cipher.
We will denote the ciphertext alphabet as ?c
and the plaintext alphabet as ?p. Given any string
T , we will denote n(T ) as the length of T . Fur-
thermore, we assume that the plaintext P is drawn
from some string distribution q. We do not assume
q to be a trigram distribution, but we do require it
to be a distribution from which trigrams can be
calculated (e.g, a 5-gram corpus will in general
have probabilities that cannot be predicted using
the associated trigrams, but the associated trigram
corpus can be recovered from the 5-grams).
It is important to realize in the algorithm de-
scription and analysis that q may also not be
known exactly, but we only assume that it exists,
and that we can approximate it with a known tri-
gram distribution p. In Corlett and Penn (2010),
for example, p is the trigram distribution found us-
ing the Penn treebank. It is assumed that this is a
good approximation for the distribution q, which
in Corlett and Penn (2010) is the text in Wikipedia
from which ciphers are drawn. As is common
when dealing with probability distributions over
natural languages, we assume that both p and q
are stationary and ergodic, and we furthermore as-
sume that p is smooth enough that any trigram that
can be found in any string generated by q occurs in
p (i.e., we assume that the cross entropyH(p, q) is
finite).
The algorithm works in a model in which, for
any run of the algorithm, the plaintext string P
is drawn according to the distribution q. We do
not directly observe P , but instead its encoding
using the cipher key, which we will call piT . We
observe the ciphertext C = pi?1T (P ). We note that
piT is unknown, but that it does not change as new
ciphertexts are drawn.
Now, the way that the algorithm in Corlett and
Penn (2010) works is by searching over the pos-
sible keys to the cipher to find the one that maxi-
mizes the probability of the plaintext according to
the distribution p. It does so as follows.
In addition to the possible keys to the cipher,
85
weakened cipher keys called partial solutions are
added to the search space. A partial solution of
size k (denoted as pik) is a section of a possible full
cipher key which is only defined on k character
types in the cipher. We consider the character
types to be fixed according to some preset order,
and so the k fixed letters in pik do not change
between different partial solutions of size k.
Given a partial solution pik, a string pi
n(C)
k (C)
is defined whose probability we use as an upper
bound for the probability of the plaintext when-
ever the true solution to the cipher contains pik
as a subset. The string pin(C)k (C) is the most
likely string that we can find that is consistent
with C on the letters fixed by pik. That is, we
define the set ?k so that S ? ?k iff whenever
si and ci are the characters at index i in S and
C, then si = pik(ci) if ci is fixed in pik. Note
that if ck is not fixed in pik, we let si take any
value. We extend the partial character function
to the full string function pin(C)k on ?
n(C)
c so that
pin(C)k (C) = argmax(S??k)probp(S).
In Corlett and Penn (2010), the value pin(C)k (C)
is efficiently computed by running it through
the Viterbi algorithm. That is, given C, p and
pik, a run of the Viterbi algorithm is set up in
which the letter transition probabilities are those
that are given in p. In order to describe the
emission probabilities, suppose that we partition
the ciphertext alphabet ?c into two sets ?1 and
?2, where ?1 is the set of ciphertext letters fixed
by pik. For any plaintext letter y ? ?p, if there
is a ciphertext letter x ? ?1 such that y ? x is
a rule in pik, then the emission probability that y
will be seen as x is set to 1, and the probability
that y will be seen as any other letter is set to 0.
On the other hand, if there is no rule y ? x in
pik for any ciphertext letter x, then the emission
probability associated with y is uniform over the
letters x ? ?2 and 0 for the letters x ? ?1.
The search algorithm described in Corlett and
Penn (2010) uses the probability of the string
pin(C)k (C), or more precisely, the log probabil-
ity ?logprobp(pi
n(C)
k (C)), as an A
? heuristic over
the partial solutions pik. In this search, an edge
is added from a size k partial solution pik to a
size k + 1 partial solution pik+1 if pik agrees with
pik+1 wherever it is defined. The score of a node
pik is the log probability of its associated string:
?logprobp(pi
n(C)
k (C)). We can see that if pik has
an edge leading to pik+1, then ?k+1 ? ?k, so that
?logprobp(pi
n(C)
k+1 (C)) ? ?logprobp(pi
n(C)
k (C)).
Thus, the heuristic is nondecreasing. Moreover,
by applying the same statement inductively we can
see that any full solution to the cipher that has pik
as a subset must have a score at least as great as
that of pik. This means that the score never over-
estimates the cost of completing a solution, and
therefore that the heuristic is admissible.
4 Analysis
The bound that we will prove is that for any k > 0
and for any ?, ? > 0, there exists an n ? N such
that if the length n(C) of the cipher C is at least
n, then with probability at least 1 ? ?, the search
for the key to the cipher C requires no more than
2n?(H(p,q)+?) expansions of any partial solution of
size k to complete. Applying the same bound over
every size k of partial solution will then give us
that for any ?, ? > 0, there exists a n0 > 0 such
that if the length n(C) of the cipher C is at least
n, then with probability at least 1 ? ?, the search
for the key to the cipher C requires no more than
2n(H(p,q)+?) expansions of any partial solution of
size greater than 0 to complete (note that there is
only one partial solution of size 0).
Let pi? be the solution that is found by the
search. This solution has the property that it is the
full solution that induces the most probable plain-
text from the cipher, and so it produces a plaintext
that is at least as likely as that of the true solution
P . Thus, we have that ?logprobp(pi?n(C)(C)) ?
?logprobp(pi
n(C)
T (C)) = ?logprobp(P ).
We find our bound by making use of the fact that
an A? search never expands a node whose score
is greater than that of the goal node pi?. Thus, a
partial solution pik is expanded only if
?logprobp(pi
n(C)
k (C)) ? ?logprobp(pi
?n(C)(C)).
Since
?logprobp(pi
?n(C)(C)) ? ?logprobp(P ),
we have that pik is expanded only if
?logprobp(pi
n(C)
k (C)) ? ?logprobp(P ).
So we would like to count the number of solutions
satisfying this inequality.
86
We would first like to approximate the value of
?logprobp(P ), then. But, since P is drawn from
an ergodic stationary distribution q, this value
will approach the cross entropy H(p, q) with high
probability: for any ?1, ?1 > 0, there exists an
n1 > 0 such that if n(C) = n(P ) > N1, then
| ? logprobp(P )/n(C)?H(p, q)| < ?1
with probability at least 1 ? ?1. In this case, we
have that ?logprobp(P ) < n(C)(H(p, q) + ?1).
Now, if k is fixed, and if pik and pi?k are two dif-
ferent size k partial solutions, then pik and pi?k must
disagree on at least one letter assignment. Thus,
the sets ?k and ??k must be disjoint. But then we
also have that pin(C)k (C) 6= pi
n(C)?
k (C). Therefore,
if we can find an upper bound for the size of the
set
{S ? ?n(C)p |S = pi
n(C)
k (C) for some pik},
we will have an upper bound on the number of
times the search will expand any partial solution
of size k. We note that under the previous assump-
tions, and with probability at least 1? ?1, none of
these strings can have a log probability larger than
n(C)(H(p, q) + ?1).
For any plaintext string C drawn from q, we let
aPb be the substring of P between the indices a
and b. Similarly, we let aSb be the substring of
S = pin(C)k (C) between the indices a and b.
We now turn to the proof of our bound: Let
?, ? > 0 be given. We give the following three
bounds on n:
(a) As stated above, we can choose n1 so that for
any string P drawn from q with length at least
n1,
| ? logprobp(P )/n(P )?H(p, q)| < ?1/2
with probability at least 1? ?/3.
(b) We have noted that if k is fixed then any two
size k partial solutions must disagree on at
least one of the letters that they fix. So if we
have a substring aPb of P with an instance of
every letter type fixed by the partial solutions
of size k, then the substrings aSb of S must
be distinct for every S ? {S ? ?n(C)p |S =
pin(C)k (C) for some pik}. Since q is ergodic,
we can find an n2 such that for any string P
drawn from q with length at least n2, every
letter fixed in pik can be found in some length
n2 substring P2 of P , with probability at least
1? ?/3.
(c) By the Lemma below, there exists an n? > 0
such that for all partial solutions pik, there ex-
ists a trigram distribution rk on the alphabet
?p such that if S = pi
n(C)
k (C) and b ? a =
n > n?, then
?
?
?
?
?logprob(aSb)
n
?H(p, rk)
?
?
?
? < ?/4
with a probability of at least 1? ?/3.
Let n = max(n1, n2, n?). Then, the probability
of any single one of the properties in (a), (b) or (c)
failing in a string of length at least n is at most ?/3,
and so the probability of any of them failing is at
most ?. Thus, with a probability of at least 1??, all
three of the properties hold for any string P drawn
from q with length at least n. Let P be drawn from
q, and suppose n(P ) > n. Let aPb be a length n
substring of P containing a token of every letter
type fixed by the size k partial solutions.
Suppose that pik is a partial solution such that
?logprobp(pi
n(C)
k (C)) ? n(P )(H(p, q) + ?/2).
Then, letting S = pin(C)k (C), we have that if
?
?
?
?
?logprob(S)
n(P )
?H(p, rk)
?
?
?
? < ?/4
and
?
?
?
?
?logprob(aSb)
n
?H(p, rk)
?
?
?
? < ?/4
it follows that
?
?
?
?
?logprob(S)
n(P )
+
logprob(aSb)
n
?
?
?
?
?
?
?
?
?
?logprob(S)
n(P )
?H(p, rk)
?
?
?
?
+
?
?
?
??H(p, rk)?
logprob(aSb)
n
?
?
?
?
? ?/4 + ?/4 = ?/2
But then,
?
logprob(aSb)
n
<
?logprob(S)
n(P )
+ ?/2
?
n(P )(H(p, q) + ?/2)
n(P )
+ ?/2
= H(p, q) + ?.
87
So, for our bound we will simply need to find the
number of substrings aSb such that
? log probp(aSb) < n(H(p, q) + ?).
Letting IH(aSb) = 1 if ?logprobp(aSb) <
n(H(p, q) + ?) and 0 otherwise, the number of
strings we need becomes
X
aSb??
n(C)
p
IH(aSb) = 2
n?(H(p,q)+?)
X
aSb??
n(C)
p
IH(aSb)2
?n?(H(p,q)+?)
<2n?(H(p,q)+?)
X
aSb??
n(C)
p
IH(aSb)probp(aSb)
(since ? log probp(aSb) < n(H(p, q) + ?)
implies probp(aSb) > 2?n?(H(p,q)+?))
? 2n?(H(p,q)+?)
X
aSb??
n(C)
p
probp(aSb)
= 2n?(H(p,q)+?)
Thus, we have a bound of 2n?(H(p,q)+?) on
the number of substrings of length n satisfying
? log probp(aSb) < n(H(p, q) + ?). Since we
know that with probability at least 1? ?, these are
the only strings that need be considered, we have
proven our bound. 
4.1 Lemma:
We now show that for any fixed k > 0
and ??, ?? > 0, there exists some n? > 0
such that for all partial solutions pik, there
exists a trigram distribution rk on the al-
phabet ?p such that if S = pi
n(C)
k (C) and
b ? a = n > n?, |?logprob(aSb)n ? H(p, rk)| < ?
?
with a probability of at least 1? ??.
Proof of Lemma: Given any partial solution pik,
it will be useful in this section to consider the
strings S = pin(C)k (C) as functions of the plain-
text P rather than the ciphertext C. Since C =
pi?1T (P ), then, we will compose pi
n(C)
k and pi
?1
T
to get pin(C)?k (P ) = pi
n(C)
k (pi
?1
T (P )). Now, since
piT is derived from a character bijection between
?c and ?p, and since pi
n(C)
k fixes the k character
types in ?c that are defined in pik, we have that
pin(C)?k fixes k character types in ?p. Let ?P1 be
the set of k character types in ?p that are fixed by
pin(C)?k , and let ?P2 = ?p \?P1 . We note that ?P1
and ?P2 do not depend on which pik we use, but
only on k.
Now, any string P which is drawn from q
can be decomposed into overlapping substrings
by splitting it whenever it has see two adjacent
characters from ?P1 . When we see a bigram in
P of this form, say, y1y2, we split P so that both
the end of the initial string and the beginning of
the new string are y1y2. Note that when we have
more than two adjacent characters from ?P1 we
will split the string more than once, so that we get
a series of three-character substrings of P in our
decomposition. As a matter of bookkeeping we
will consider the initial segment to begin with two
start characters s with indices corresponding to 0
and ?1 in P . As an example, consider the string
P = friends, romans, countrymen, lend me
your ears
Where ?P1 = {? ?, ?, ?, ?a?, ?y?}. In this case,
we would decompose P into the strings ?ssfriends,
?, ?, romans, ?, ?, countrymen, ?, ?, lend me ?, ?e y?,
? your e? and ? ears?.
Let M be the set of all substrings that can be
generated in this way by decomposing strings P
which are drawn from q. Since the end of any
string m ?M contains two adjacent characters in
?P1 and since the presence of two adjacent char-
acters in ?P1 signals a position at which a string
will be decomposed into segments, we have that
the set M is prefix-free. Every string m ? M
is a string in ?p, and so they will have probabili-
ties probq(m) in q. It should be noted that for any
m ? M the probability probq(m) may be differ-
ent from the trigram probabilities predicted by q,
but will instead be the overall probability in q of
seeing the string m.
For any pair T, P of strings, let #(T, P ) be the
number of times T occurs in P . Since we as-
sume that the strings drawn from q converge to
the distribution q, we have that for any ?3, ?3 >
0 and any n4 > 0, there exists an n3 > 0
such that for any substring P3 of P of length
at least n3, where P is drawn from q, and for
any m ? M of length at most n4, the number
|#(m,P )/len(P3) ? probq(m)| < ?3 with prob-
ability greater than 1? ?3.
Now suppose that for some P drawn from q
we have a substring aPb of P such that aPb =
m,m ? M . If S = pin(C)?k (P ), consider the sub-
string aSb of S. Recall that the string function
pin(C)?k can map the characters in P to S in one
of two ways: if a character xi ? ?P1 is found at
index i in P , then the corresponding character in S
88
is pik(xi). Otherwise, xi is mapped to whichever
character yi in ?P maximizes the probability in p
of S given pin(C)?k (xi?2)pi
n(C)?
k (xi?1)yi. Since the
values of pin(C)?k (xi?2), pi
n(C)?
k (xi?1) and yi are in-
terdependent, and since pin(C)?k (xi?2) is dependent
on its previous two neighbors, the value that yi
takes may be dependent on the values taken by
pin(C)?k (xj) for indices j quite far from i. How-
ever, we see that no dependencies can cross over
a substring in P containing two adjacent charac-
ters in ?P1 , since these characters are not trans-
formed by pin(C)?k in a way that depends on their
neighbors. Thus, if aPb = m ? M , the endpoints
of aPb are made up of two adjacent characters in
?P1 , and so the substring aSb of S depends only
on the substring aPb of P . Specifically, we see that
aSb = pi
n(C)?
k (aPb).
Since we can decompose any P into overlap-
ping substrings m1,m2, . . . ,mt in M , then, we
can carry over this decomposition into S to break
S into pin(C)?k (m1), pi
n(C)?
k (m2), . . . , pi
n(C)?
k (mt).
Note that the score generated by S in
the A? search algorithm is the sum
?
1?i? logprobp(yi?2yi?1yi), where yi is
the ith character in S. Also note that ev-
ery three-character sequence yi?2yi?1yi
occurs exactly once in the decomposition
pin(C)?k (m1), pi
n(C)?
k (m2), . . . , pi
n(C)?
k (mt). Since
for anym the number of occurrences of pin(C)?k (m)
in S under this decomposition will be equal to the
number of occurrences of m in P , we have that
?logprobp(S) =
X
1?i?n(P )
logprobp(yi?2yi?1yi)
=
X
m?M
#(m,P ) ? (?logprobp(pi
n(C)?
k (m))).
Having finished these definitions, we can
now define the distribution rk. In princi-
ple, this distribution should be the limit of
the frequency of trigram counts of the strings
S = pin(C)?k (P ), where n(P ) approaches infin-
ity. Given a string S = pin(C)?k (P ), where P
is drawn from q, and given any trigram y1y2y3
of characters in ?p, this frequency count is
#(y1y2y3,S)
n(P ) . Breaking S into its component sub-
strings pin(C)?k (m1), pi
n(C)?
k (m2), . . . , pi
n(C)?
k (mt),
as we have done above, we see that any instance
of the trigram y1y2y3 in S occurs in exactly one of
the substrings pin(C)?k (mi), 1 ? i ? t. Grouping
together similar mis, we find
#(y1y2y3, S)
n(P )
=
tP
i=1
#(y1y2y3, pi
n(C)?
k (mi))
n(P )
=
P
m?M
#(y1y2y3, pi
n(C)?
k (m)) ?#(m,P )
n(P )
As n(P ) approaches infinity, we find that #(m,P )n(P )
approaches probq(m), and so we can write
probrk (y1y2y3) =
X
m?M
#(y1y2y3, pi
n(C)?
k (m))probq(m).
Since 0 ?
?
m?M #(y1y2y3, pi
n(C)?
k (m))probq(m)
when P is sampled from q we have that
X
y1y2y3
probrk (y1y2y3)
=
X
y1y2y3
X
m?M
#(y1y2y3, pi
n(C)?
k (m))probq(m)
= lim
n(P )??
X
y1y2y3
X
m?M
#(y1y2y3, pi
n(C)?
k (m))
#(m,P )
n(P )
= lim
n(P )??
X
m?M
X
y1y2y3
#(y1y2y3, pi
n(C)?
k (m))
#(m,P )
n(P )
= lim
n(P )??
X
m?M
(n(pin(C)?k (m))? 2)#(m,P )
n(P )
= lim
n(P )??
X
m?M
(n(m)? 2)#(m,P )
n(P )
= lim
n(P )??
n(P )
n(P )
= 1,
so we have that probrk is a valid probability distri-
bution. In the above calculation we can rearrange
the terms, so convergence implies absolute conver-
gence. The sum
?
y1y2y3 #(y1y2y3, pi
n(C)?
k (m))
gives (n(pin(C)?k (m)) ? 2) because there is one
trigram for every character in pin(C)?k (m), less two
to compensate for the endpoints. However, since
the different m overlap by two in a decomposition
from P , the sum (n(m) ? 2)#(m,P ) just gives
back the length n(P ), allowing for the fact that
the initial m has two extra start characters.
Having defined rk, we can now find the value of
H(p, rk). By definition, this term will be
89
Xy1y2y3
?logprobp(y1y2y3)probrk (y1y2y3)
=
X
y1y2y3
?logprobp(y1y2y3)
X
m?M
#(y1y2y3, pi
n(C)?
k (m))probq(m)
=
X
m?M
X
y1y2y3
?logprobp(y1y2y3)#(y1y2y3, pi
n(C)?
k (m))probq(m)
=
X
m?M
?logprobp(m)probq(m).
Now, we can finish the proof of the Lemma.
Holding k fixed, let ??, ?? > 0 be given. Since we
have assumed that p does not assign a zero proba-
bility to any trigram generated by q, we can find a
trigram x1x2x3 generated by q whose probability
in p is minimal. Let X = ?logprobp(x1x2x3),
and note that probp(x1x2x3) > 0 implies
X < ?. Since we know by the argu-
ment above that when P is sampled from q,
limn(P )??(
?
m?M
(npin(C)?k (m)?2)?#(m,P )
n(P ) ) = 1,
we have that
?
m?M
(npin(C)?k (m)? 2)probq(m) = 1.
Thus, we can choose n4 so that
?
m?M,n(m)?n4
(npin(C)?k (m)? 2)probq(m)
> 1? ??/4X.
Let Y = |{m ? M,n(m) ? n4}|, and choose
n? such that if P is sampled from q and aPb is a
substring of P with length greater than n?, then
with probability at least 1 ? ??, for every m ? M
we will have that
?
?
?
?
#(m, aPb)
n(aPb)
? probq(m)
?
?
?
? < ?
?/4XY (n4 ? 2).
Let pik be any partial solution of length k, and let
rk be the trigram probability distribution described
above. Then let P be sampled from q, and let S =
pin(C)k (C) = pi
n(C)?
k (P ), and let a, b be indices of
S such that b ? a = n > n?. Finally, we will
partition the set M as follows: we let M ? be the
set {m ?M |n(n) ? n4} andM ?? be the set {m ?
M |n(m) > n4}. Thus, we have that
?
?
?
?
?logprob(aSb)
n
?H(p, rk)
?
?
?
?
=
?
?
?
?
?
P
m?M #(m, aPb)(?logprobp(pi
n(C)?
k (m))
n
?
X
m?M
probq(m) ? (?logprobp(pi
n(C)?
k (m))
?
?
?
?
?
.
Grouping the terms of these sums into the index
sets M ? and M ??, we find that this value is at most
?
?
?
?
?
X
m?M?
?
#(m, aPb)
n
? probq(m)
?
(?logprobp(pi
n(C)?
k (m))
?
?
?
?
?
+
?
?
?
?
?
X
m?M??
?
#(m, aPb)
n
? probq(m)
?
(?logprobp(pi
n(C)?
k (m))
?
?
?
?
?
Furthermore, we can break up the sum over the
index M ?? to bound this value by
?
?
?
?
?
X
m?M?
?
#(m, aPb)
n
? probq(m)
?
(?logprobp(pi
n(C)?
k (m))
?
?
?
?
?
+
?
?
?
?
?
X
m?M??
#(m, aPb)
n
(?logprobp(pi
n(C)?
k (m))
?
?
?
?
?
+
?
?
?
?
?
X
m?M??
probq(m)(?logprobp(pi
n(C)?
k (m))
?
?
?
?
?
Now, for any m ? M , we have that
the score ?logprobp(pi
n(C)?
k (m) equals?
1?i?n(m)?2?logprobp(yiyi+1yi+2), where yi
is the character at the index i in pin(C)?k (m).
Taking the maximum possible values for
?logprobp(yiyi+1yi+2), we find that this sum is
at most (n(m)? 2)X . Applying this bound to the
previous formula, we find that it is at most
?
?
?
?
?
X
m?M?
?
#(m, aPb)
n
? probq(m)
?
(n(m)? 2)X
?
?
?
?
?
+
?
?
?
?
?
X
m?M??
#(m, aPb)
n
(n(m)? 2)X
?
?
?
?
?
+
?
?
?
?
?
X
m?M??
probq(m) ? (n(m)? 2)X
?
?
?
?
?
.
We can bound each of these three terms separately.
Looking at the first sum in this series, we find that
with probability at least 1? ??,
90
?
?
?
?
?
X
m?M?
?
#(m, aPb)
n
? probq(m)
?
(n(m)? 2)X
?
?
?
?
?
(*)
?
X
m?M?
?
?
?
?
#(m, aPb)
n
? probq(m)
?
?
?
? (n(m)? 2)X
?
X
m?M?
?
?
?
?
??
4(n4 ? 2)XY
?
?
?
? ? (n(m)? 2)X
?
X
m?M?
?
?
?
?
??
4Y
?
?
?
?
=
??
4Y
X
m?M?
1 =
??
4Y
Y = ?/4.
In order to bound the second sum, we make use
of the fact that
?
m?M #(m, aPb)(n(m) ? 2) =
n(aPb) = n to find that once again, with probabil-
ity greater than 1? ??,
?
?
?
?
?
X
m?M??
#(m, aPb)
n
(n(m)? 2)X
?
?
?
?
?
?
X
m?M??
?
?
?
?
#(m, aPb)
n
(n(m)? 2)X
?
?
?
? .
Since M ?? = M ?M ?, this value is
X
m?M
?
?
?
?
#(m, aPb)
n
(n(m)? 2)X
?
?
?
?
?
X
m?M?
?
?
?
?
#(m, aPb)
n
(n(m)? 2)X
?
?
?
?
=X ?
X
m?M?
?
?
?
?
#(m, aPb)
n
(n(m)? 2)X
?
?
?
? .
This value can further be split into
=X?
X
m?M?
?
?
?
?
?
#(m, aPb)
n
+(1?1)probq(m)
?
(n(m)?2)X
?
?
?
?
?X ?
 
X
m?M?
|probq(m)(n(m)? 2)X|
?
X
m?M?
?
?
?
?
#(m, aPb)
n
? probq(m)
?
?
?
? (n(m)? 2)X
!
Using our value for the sum in (*), we find that
this is
=X ?
X
m?M?
|probq(m)(n(m)? 2)X|
+
X
m?M?
?
?
?
?
#(m, aPb)
n
? probq(m)
?
?
?
? (n(m)? 2)X
?X ?
X
m?M?
|probq(m)(n(m)? 2)X|+
??
4
,
Using our definition of n4, we can further bound
this value by
=X
 
1?
X
m?M?
probq(m)(n(m)? 2)
!
+
??
4
<X
?
1?
?
1?
??
4X
??
+
??
4
=X
??
4X
+
??
4
=
??
2
.
Finally, we once again make use of the definition
of n4 to find that the last sum is
?
?
?
?
?
X
m?M??
probq(m) ? (n(m)? 2)X
?
?
?
?
?
=
X
m?M??
probq(m) ? (n(m)? 2)X
= X
X
m?M??
probq(m) ? (n(m)? 2)
< X
??
4X
=
??
4
.
Adding these three sums together, we get
??
4
+
??
2
+
??
4
= ??.
Thus,
?
?
?
?logprob(aSb)
n ?H(p, rk)
?
?
? < ?? with prob-
ability greater than 1? ??, as required. 
5 Conclusion
In this paper, we discussed a discrepancy between
the theoretical and practical running times of cer-
tain algorithms that are sensitive to the entropies
of their input, or the entropies of the distributions
from which their inputs are sampled. We then
used the algorithm from Corlett and Penn (2010)
as a subject to allow us to investigate ways to
talk about average-case complexity in light of
this discrepancy. Our analysis was sufficient
to give us a bound on the search complexity
of this algorithm which is exponential in the
cross-entropy between the training distribution
and the input distribution. Our method in effect
yields a probabilistic bound on the depth of the
search heuristic used. This leads to an exponen-
tially smaller search space for the overall problem.
We must note, however, that our analysis does
not fully reconcile the discrepancy between the
91
theoretical and practical running time for this
algorithm. In particular, our bound still does not
explain why the number of search nodes expanded
by this algorithm tends to converge on one per
partial solution size as the length of the string
grows very large. As such, we are interested in
further studies as to how to explain the running
time of this algorithm. It is our opinion that this
can be done by refining our description of the sets
?k to exclude strings which cannot be considered
by the algorithm. Not only would this allow us
to reduce the overall number of strings we would
have to count when determining the bound, but
we would also have to consider fewer strings
when determining the value of n?. Both changes
would reduce the overall complexity of our bound.
This general strategy may have the potential to
illuminate the practical time complexities of ap-
proximate search algorithms as well.
References
David Arthur, Bodo Manthey, and Heiko Ro?glin.
k-means has polynomial smoothed complex-
ity. In The 50th Annual Symposium on Foun-
dations of Computer Science. IEEE Computer
Society Technical Committee on Mathematical
Foundations of Computing, 2009. URL http:
//arxiv.org/abs/0904.1113.
Eric Corlett and Gerald Penn. An exact A? method
for deciphering letter-substitution ciphers. In
Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics,
pages 1040?1047, 2010.
Richard E Korf and Michael Reid. Complexity
analysis of admissible heuristic search. In Pro-
ceedings of the Fifteenth National Conference
on Artificial Intelligence, 1998.
Richard E Korf, Michael Reid, and Stefan
Edelkamp. Time complexity of iterative-
deepening-A?. Artificial Intelligence, 129(1?2):
199?218, 2001.
Judea Pearl. Heuristics: Intelligent Search Strate-
gies for Computer Problem Solving. Addison-
Wesley, 1984.
Daniel A Spielman and Shang-Hua Teng.
Smoothed analysis of algorithms: Why the
simplex algorithm usually takes polynomial
time. Journal of the ACM, 51(3):385?463,
2004.
92
Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 119?127,
Baltimore, Maryland, USA. June 27, 2014.
c?2014 Association for Computational Linguistics
Evaluating Sentiment Analysis Evaluation: A Case Study in Securities
Trading
Siavash Kazemian Shunan Zhao
Department of Computer Science
University of Toronto
{kazemian,szhao,gpenn}@cs.toronto.edu
Gerald Penn
Abstract
There are numerous studies suggesting
that published news stories have an im-
portant effect on the direction of the stock
market, its volatility, the volume of trades,
and the value of individual stocks men-
tioned in the news. There is even some
published research suggesting that auto-
mated sentiment analysis of news doc-
uments, quarterly reports, blogs and/or
Twitter data can be productively used as
part of a trading strategy. This paper
presents just such a family of trading
strategies, and then uses this application to
re-examine some of the tacit assumptions
behind how sentiment analyzers are gen-
erally evaluated, in spite of the contexts of
their application. This discrepancy comes
at a cost.
1 Introduction
Amidst the vast amount of user-generated and
professionally-produced textual data, analysts
from different fields are turning to the natural lan-
guage processing community to sift through these
large corpora and make sense of them. Interna-
tional collaborative projects such as the Digging
into Data Challenge (2012) or the Big Data Con-
ference sponsored by the Marketing Science In-
stitute (2012) are some recent examples of these
initiatives.
The proliferation of opinion-rich text on the
World Wide Web, which includes anything from
product reviews to political blog posts, led to the
growth of sentiment analysis as a research field
more than a decade ago. The market need to quan-
tify opinions expressed in social media and the
blogosphere has provided a great opportunity for
sentiment analysis technology to make an impact
in many sectors, including the financial industry,
in which interest in automatically detecting news
sentiment in order to inform trading strategies ex-
tends back at least 10 years. In this case, senti-
ment takes on a slightly different meaning; posi-
tive sentiment is not the emotional and subjective
use of laudatory language. Rather, a news article
that contains positive sentiment is optimistic about
the future financial prospects of a company.
Zhang and Skiena (2010) have shown that news
sentiment can effectively inform simple market
neutral trading algorithms, producing a maximum
yearly return of around 30%, and even more when
using sentiment from blogs and Twitter data. They
did so, however, without an appropriate baseline,
making it very difficult to appreciate the signif-
icance of this number. Using a very standard
sentiment analyzer, we are able to garner annual-
ized returns over twice that percentage (70.1%),
and in a manner that highlights some of the bet-
ter design decisions that Zhang and Skiena (2010)
made, viz., their decision to use raw SVM scores
rather than discrete positive or negative senti-
ment classes, and their decision to go long (resp.,
short) in the n best- (worst-) ranking securities
rather than to treat all positive (negative) securi-
ties equally. We trade based upon the raw SVM
score itself, rather than its relative rank within a
basket of other securities, and tune a threshold for
that score that determines whether to go long, neu-
tral or short. We sample our stocks for both train-
ing and evaluation with and without survivor bias,
the tendency for long positions in stocks that are
publicly traded as of the date of the experiment to
pay better using historical trading data than long
positions in random stocks sampled on the trad-
ing days themselves. Most of the evaluations of
sentiment-based trading either unwittingly adopt
this bias, or do not need to address it because their
returns are computed over historical periods so
brief. We also provide appropriate trading base-
lines as well as Sharpe ratios to attempt to quan-
119
tify the relative risk inherent to our experimen-
tal strategies. As tacitly assumed by most of the
work on this subject, our trading strategy is not
portfolio-limited, and our returns are calculated on
a percentage basis with theoretical, commission-
free trades.
Our motivation for undertaking this study has
been to reappraise the evaluation standards for
sentiment analyzers. It is not at all uncommon
within the sentiment analysis community to eval-
uate a sentiment analyzer with a variety of classi-
fication accuracy or hypothesis testing scores such
as F-measures, kappas or Krippendorff alphas de-
rived from human-subject annotations, even when
more extensional measures are available. In secu-
rities trading, this would of course include actual
market returns from historical data. With Holly-
wood films, another popular domain for automatic
sentiment analysis, one might refer to box-office
returns or the number of award nominations that
a film receives rather than to its star-rankings on
review websites where pile-on and confirmation
biases are widely known to be rampant. Are the
opinions of human judges, paid or unpaid, a suf-
ficient proxy for the business cases that actually
drive the demand for sentiment analyzers?
We regret to report that they are not. We have
even found a particular modification to our stan-
dard financial sentiment analyzer that, when eval-
uated against an evaluation test set sampled from
the same pool of human-subject annotations as
the analyzer?s training data, returns significantly
poorer performance, but when evaluated against
actual market returns, yields significantly better
performance. This should worry researchers who
rely on classification accuracies and hypothesis
tests relative to human-subject data, because the
improvements that they report, whether based on
better feature selection or different pattern recog-
nition algorithms, may in fact not be improve-
ments at all.
The good news, however, is that, based upon our
experience within this particular domain, training
on human-subject annotations and then tuning on
more extensional data, in cases where the latter
are less abundant, seems to suffice for bringing
the evaluation back to reality. A likely machine-
learning explanation for this is that whenever two
unbiased estimators are pitted against each other,
they often result in an improved combined perfor-
mance because each acts as a regularizer against
the other. If true, this merely attests to the relative
independence of task-based and human-annotated
knowledge sources. A more HCI-oriented view
would argue that direct human-subject annotations
are highly problematic unless the annotations have
been elicited in manner that is ecologically valid.
When human subjects are paid to annotate quar-
terly reports or business news, they are paid re-
gardless of the quality of their annotations, the
quality of their training, or even their degree of
comprehension of what they are supposed to be
doing. When human subjects post film reviews on
web-sites, they are participating in a cultural activ-
ity in which the quality of the film under consider-
ation is only one factor. These sources of annota-
tion have not been properly controlled.
2 Related Work in Financial Sentiment
Analysis
Studies confirming the relationship between me-
dia and market performance date back to at
least Niederhoffer (1971), who looked at NY
Times headlines and determined that large market
changes were more likely following world events
than on random days. Conversely, Tetlock (2007)
looked at media pessimism and concluded that
high media pessimism predicts downward prices.
Tetlock (2007) also developed a trading strategy,
achieving modest annualized returns of 7.3%. En-
gle and Ng (1993) looked at the effects of news on
volatility, showing that bad news introduces more
volatility than good news. Chan (2003) claimed
that prices are slow to reflect bad news and stocks
with news exhibit momentum. Antweiler and
Frank (2004) showed that there is a significant, but
negative correlation between the number of mes-
sages on financial discussion boards about a stock
and its returns, but that this trend is economically
insignificant. Aside from Tetlock (2007), none of
this work evaluated the effectiveness of an actual
sentiment-based trading strategy.
There is, of course, a great deal of work on
automated sentiment analysis as well; see Pang
and Lee (2008) for a survey. More recent de-
velopments that are germane to our work include
the use of different information retrieval weighting
schemes (Paltoglou and Thelwall, 2010) and the
utilization of Latent Dirichlet Allocation (LDA)
in a joint sentiment/topic framework (Lin and He,
2009).
There has also been some work that analyzes the
120
sentiment of financial documents without actually
using those results in trading strategies (Koppel
and Shtrimberg, 2004; Ahmad et al., 2006; Fu et
al., 2008; O?Hare et al., 2009; Devitt and Ahmad,
2007; Drury and Almeida, 2011). As to the rela-
tionship between sentiment and stock price, Das
and Chen (2007) performed sentiment analysis on
discussion board posts. Using this analysis, they
built a ?sentiment index? that computed the time-
varying sentiment of the 24 stocks in the Morgan
Stanley High-Tech Index (MSH), and tracked how
well their index followed the aggregate price of the
MSH itself. Their sentiment analyzer was based
upon a voting algorithm, although they also dis-
cussed a vector distance algorithm that performed
better. Their baseline, the Rainbow algorithm, also
came within 1 percentage point of their reported
accuracy. This is one of the very few studies that
has evaluated sentiment analysis itself (as opposed
to a sentiment-based trading strategy) against mar-
ket returns (versus gold-standard sentiment anno-
tations). Das and Chen (2007) focused exclusively
on discussion board messages and their evaluation
was limited to the stocks on the MSH, whereas
we focus on Reuters newswire and evaluate over
a wide range of NYSE-listed stocks and market
capitalization levels.
Butler and Keselj (2009) try to determine sen-
timent from corporate annual reports using both
character n-gram profiles and readability scores.
They also developed a sentiment-based trading
strategy with high returns, but do not report how
the strategy works or how they computed the re-
turns, making the results difficult to compare to
ours. Basing a trading strategy upon annual re-
ports also calls into question the frequency with
which the trading strategy could be exercised.
The work that is most similar to ours is that
of Zhang and Skiena (2010). They look at both
financial blog posts and financial news, forming
a market-neutral trading strategy whereby each
day, companies are ranked by their reported sen-
timent. The strategy then goes long and short on
equal numbers of positive- and negative-sentiment
stocks, respectively. They conduct their trading
evaluation over the period from 2005 to 2009, and
report a yearly return of roughly 30% when us-
ing news data, and yearly returns of up to 80%
when they use Twitter and blog data. Further-
more, they trade based upon sentiment ranking
rather than pure sentiment analysis, i.e., instead of
trading based on the raw sentiment score of the
document, they first rank the documents and trade
based on this relative ranking.
Zhang and Skiena (2010) compare their strat-
egy to two strategies which they term Worst-
sentiment Strategy and Random-selection Strat-
egy. The Worst-sentiment Strategy trades the op-
posite of their strategy, going short on positive sen-
timent stocks and going long on negative senti-
ment stocks. The Random-selection Strategy ran-
domly picks stocks to go long and short in. As
trading strategies, these baselines set a very low
standard. Our evaluation compares our strategy to
standard trading benchmarks such as momentum
trading and holding the S&P, as well as to oracle
trading strategies over the same trading days.
3 Method and Materials
3.1 News Data
Our dataset consists of a combination of two col-
lections of Reuters news documents. The first was
obtained for a roughly evenly weighted collec-
tion of 22 small-, mid- and large-cap companies,
randomly sampled from the list of all companies
traded on the NYSE as of 10
th
March, 1997. The
second was obtained for a collection of 20 com-
panies randomly sampled from those companies
that were publicly traded in March, 1997 and still
listed on 10
th
March, 2013. For both collections
of companies, we collected every chronologically
third Reuters news document about them from the
period March, 1997 to March, 2013. The news
articles prior to 10
th
March, 2005 were used as
training data, and the news articles on or after 10
th
March, 2005 were reserved as testing data. We
chose to split the dataset at a fixed date rather than
randomly in order not to incorporate future news
into the classifier through lexical choice.
In total, there were 1256 financial news docu-
ments. Each was labelled by two human annota-
tors as being one of negative, positive, or neutral
sentiment. The annotators were instructed to de-
termine the state of the author?s belief about the
company, rather than to make a personal assess-
ment of the company?s prospects. Of the 1256,
only the 991 documents that were labelled twice
as negative or positive were used for training and
evaluation.
121
Representation Accuracy
bm25 freq 81.143%
term presence 80.164%
bm25 freq with sw 79.827%
freq with sw 75.564%
freq 79.276%
Table 1: Average 10-fold cross validation ac-
curacy of the sentiment classifier using different
term-frequency weighting schemes. The same
folds were used in all feature sets.
3.2 Sentiment Analysis and Intrinsic
Evaluation
For each selected document, we first filter out all
punctuation characters and the most common 429
stop words. Our sentiment analyzer is a support-
vector machine with a linear kernel function im-
plemented using SVM
light
(Joachims, 1999). We
have experimented with raw term frequencies, bi-
nary term-presence features, and term frequen-
cies weighted by the BM25 scheme, which had
the most resilience in the study of information-
retrieval weighting schemes for sentiment analysis
by Paltoglou and Thelwall (2010). We performed
10 fold cross-validation on the training data, con-
structing our folds so that each contains an approx-
imately equal number of negative and positive ex-
amples. This ensures that we do not accidentally
bias a fold.
Pang et al. (2002) use word presence features
with no stop list, instead excluding all words with
frequencies of 3 or less. Pang et al. (2002) nor-
malize their word presence feature vectors, rather
than term weighting with an IR-based scheme like
BM25, which also involves a normalization step.
Pang et al. (2002) also use an SVM with a linear
kernel on their features, but they train and com-
pute sentiment values on film reviews rather than
financial texts, and their human judges also clas-
sified the training films on a scale from 1 to 5,
whereas ours used a scale that can be viewed as
being from -1 to 1, with specific qualitative inter-
pretations assigned to each number. Antweiler and
Frank (2004) use SVMs with a polynomial kernel
(of unstated degree) to train on word frequencies
relative to a three-valued classification, but they
only count frequencies for the 1000 words with
the highest mutual information scores relative to
the classification labels. Butler and Keselj (2009)
also use an SVM trained upon a very different set
of features, and with a polynomial kernel of degree
3.
As a sanity check, we measured the accuracy of
our sentiment analyzer on film reviews by training
and evaluating on Pang and Lee?s (Pang and Lee,
2004) film reviews dataset, which contains 1000
positively and 1000 negatively labelled reviews.
Pang and Lee conveniently labelled the folds that
they used when they ran their experiments. Using
these same folds, we obtain an average accuracy
of 86.85%, which is comparable to Pang and Lee?s
86.4% score for subjectivity extraction.
Table 1 shows the performance of SVM with
BM25 weighting on our Reuters evaluation set
versus several baselines. All baselines are iden-
tical except for the term weighting schemes used,
and whether stop words were removed. As can be
observed, SVM-BM25 has the highest sentiment
classification accuracy: 80.164% on average over
the 10 folds. This compares favourably with pre-
vious reports of 70.3% average accuracy over 10
folds on financial news documents (Koppel and
Shtrimberg, 2004). We will nevertheless adhere
to normalized term presence for now, in order to
stay close to Pang and Lee?s (Pang and Lee, 2004)
implementation.
4 Task-based Evaluation
In our second evaluation protocol, we evaluate the
accuracy of the sentiment analyzer by embedding
the analyzer inside a simple trading strategy, and
then trading with it.
Our trading strategy is simple: going long when
the classifier reports positive sentiment in a news
article about a company, and short when the classi-
fier reports negative sentiment. In section 4.1, we
use the discrete polarity returned by the classifier
to decide whether go long/abstain/short a stock. In
section 4.2 we instead use the raw SVM score that
reports the distance of the current document from
the classifier?s decision boundary.
In section 4.3, we hold the trading strategy con-
stant, and instead vary the document representa-
tion features in the underlying sentiment analyzer.
Here, we measure both market return and classifier
accuracy to determine whether they agree.
In all three experiments, we compare the per-
position returns of trading strategies with the fol-
lowing four standards, where the number of days
for which a position is held remains constant:
1. The momentum strategy computes the price
122
of the stock h days ago, where h is the hold-
ing period. Then, it goes long for h days if
the previous price is lower than the current
price. It goes short otherwise.
2. The S&P strategy simply goes long on the
S&P 500 for the holding period. This strat-
egy completely ignores the stock in question
and the news about it.
3. The oracle S&P strategy computes the value
of the S&P 500 index h days into the future.
If the future value is greater than the current
day?s value, then it goes long on the S&P 500
index. Otherwise, it goes short.
4. The oracle strategy computes the value of the
stock h days into the future. If the future
value is greater than the current day?s value,
then it goes long on the stock. Otherwise, it
goes short.
The oracle and oracle S&P strategies are included
as toplines to determine how close the experimen-
tal strategies come to ones with perfect knowledge
of the future. ?Market-trained? is the same as ?ex-
perimental? at test time, but trains the sentiment
analyzer on the market return of the stock in ques-
tion for h days following a training article?s publi-
cation, rather than the article?s annotation.
4.1 Experiment One: Utilizing Sentiment
Labels in the Trading Strategy
Given a news document for a publicly traded com-
pany, the trading agent first computes the senti-
ment class of the document. If the sentiment is
positive, the agent goes long on the stock on the
date the news is released. If the sentiment is neg-
ative, it goes short. All trades are made based on
the adjusted closing price on this date. We evalu-
ate the performance of this strategy using four dif-
ferent holding periods: 30, 5, 3, and 1 day(s).
The returns and Sharpe ratios are presented in
Table 2 for the four different holding periods and
the five different trading strategies. The Sharpe
ratio can be viewed as a return to risk ratio. A
high Sharpe ratio indicates good return for rela-
tively low risk. The Sharpe ratio is calculated as
follows:
S =
E[R
a
?R
b
]
?
var(R
a
?R
b
)
,
where R
a
is the return of a single asset and R
b
is
the return of a risk-free asset, such as a 10-year
U.S. Treasury note.
Strategy Period Return S. Ratio
Experimental
30 days -0.037% -0.002
5 days 0.763% 0.094
3 days 0.742% 0.100
1 day 0.716% 0.108
Momentum
30 days 1.176% 0.066
5 days 0.366% 0.045
3 days 0.713% 0.096
1 day 0.017% -0.002
S&P
30 days 0.318% 0.059
5 days -0.038% -0.016
3 days -0.035% -0.017
1 day 0.046% 0.036
Oracle S&P
30 days 3.765% 0.959
5 days 1.617% 0.974
3 days 1.390% 0.949
1 day 0.860% 0.909
Oracle
30 days 11.680% 0.874
5 days 5.143% 0.809
3 days 4.524% 0.761
1 day 3.542% 0.630
Market-trained
30 days 0.286% 0.016
5 days 0.447% 0.054
3 days 0.358% 0.048
1 day 0.533% 0.080
Table 2: Returns and Sharpe ratios for the Experi-
mental, baseline and topline trading strategies over
30, 5, 3, and 1 day(s) holding periods.
The returns from this experimental trading sys-
tem are fairly low, although they do beat the base-
lines. A one-way ANOVA test between the ex-
perimental strategy, momentum strategy, and S&P
strategy using the percent returns from the indi-
vidual trades yields p values of 0.06493, 0.08162,
0.1792, and 0.4164, respectively, thus failing to
reject the null hypothesis that the returns are not
significantly higher. Furthermore, the means and
medians of all three trading strategies are approx-
imately the same and centred around 0. The stan-
dard deviations of the experimental strategy and
the momentum strategy are nearly identical, dif-
fering only in the thousandths digit. The standard
deviations for the S&P strategy differ from the
other two strategies due to the fact that the strat-
egy buys and sells the entire S&P 500 index and
not the individual stocks described in the news ar-
ticles. There is, in fact, no convincing evidence
that discrete sentiment class leads to an improved
trading strategy from this or any other study with
123
Figure 1: Percent returns for 1 day holding period
versus market capitalization of the traded stocks.
which we are familiar, based on the details that
they publish. One may note, however, that the re-
turns from the experimental strategy have slightly
higher Sharpe ratios than either of the baselines.
One may also note that using a sentiment ana-
lyzer mostly beats training directly on market data,
which to an extent vindicates the use of sentiment
annotation as a separate component.
Figure 1 shows the market capitalizations of
the companies for each individual trade plotted
against the percent return for the 1 day holding pe-
riod. The correlation between the two variables is
not significant. The graphs for the other holding
periods are similar.
Figure 2 shows the percent change in share
value plotted against the raw SVM score for the
different holding periods. We can see a weak cor-
relation between the two. For the 30 days, 5 days,
3 days, and 1 day holding periods, the correlations
are 0.017, 0.16, 0.16, and 0.16, respectively. The
line of best fit is shown.
This prompts us to conduct our next experiment.
4.2 Experiment Two: Utilizing SVM scores
in Trading Strategy
4.2.1 Variable Single Threshold
Previously, we would label a document as positive
(negative) if the score is above (below) 0, because
0 is the decision boundary. However, 0 might not
be the best threshold for providing high returns.
To examine this hypothesis, we took the evaluation
dataset, i.e. the dataset with news articles dated on
or after March 10, 2005, and divided it into two
folds where each fold has an equal number of doc-
uments with positive and negative sentiment. We
used the first fold to determine an optimal thresh-
old value ? and trade using the data from the sec-
ond fold and that threshold. For every news article,
if the SVM score for that article is above (below)
?, then we go long (short) on the appropriate stock
on the day the article was released. A separate
theta was determined for each holding period. We
varied ? from ?1 to 1 in increments of 0.1.
Using this method, we were able to obtain much
higher returns. In order of 30, 5, 3, and 1 day hold-
ing periods, the returns were 0.057%, 1.107%,
1.238%, and 0.745%. This is a large improvement
over the previous returns, as they are average per-
position figures.
1
4.2.2 Safety Zones
For every news item classified, SVM outputs a
score. For a binary SVM with a linear kernel func-
tion f , given some feature vector x, f(x) can be
viewed as the signed distance of x from the de-
cision boundary (Boser et al., 1992). It is then
possibly justified to interpret raw SVM scores as
degrees to which an article is positive or negative.
As in the previous section, we separate the eval-
uation set into the same two folds, only now we
use two thresholds, ? > ?. We will go long when
the SVM score is above ?, abstain when the SVM
score is between ? and ?, and go short when the
SVM score is below ?. This is a strict generaliza-
tion of the above experiment, in which ? = ?.
For convenience, we will assume in this section
that ? = ??, leaving us again with one parameter
to estimate. We again vary ? from 0 to 1 in in-
crements of 0.1. Figure 3 shows the returns as a
function of ? for each holding period on the devel-
opment dataset. If we increased the upper bound
on ? to be greater than 1, then there would be too
few trading examples (less than 10) to reliably cal-
culate the Sharpe ratio. Using this method with
? = 1, we were able to obtain even higher returns:
3.843%, 1.851%, 1.691, and 2.251% for the 30,
5, 3, and 1 day holding periods, versus 0.057%,
1.107%, 1.238%, and 0.745% in the second task-
based experiment.
4.3 Experiment Three: Feature Selection
Let us now hold the trading strategy fixed (at the
final one, with safety zones) and turn to the un-
derlying sentiment analyzer. With a good trading
1
Training directly on market data, by comparison, yields
-0.258%, -0.282%, -0.036% and -0.388%, respectively.
124
Figure 2: Percent change of trade returns plotted against SVM values for the 1, 3, 5, and 30 day holding
periods in Exp. 1. Graphs are cropped to zoom in.
Figure 3: Returns for the different thresholds on the development dataset for 30, 5, 3, and 1 day holding
periods in Exp. 2 with safety zone.
125
Representation Accuracy pi ? ? 30 days 5 days 3 days 1 day
term presence 80.164% 0.589 0.59 0.589 3.843% 1.851% 1.691% 2.251%
bm25 freq 81.143% 0.609 0.61 0.609 1.110% 1.770% 1.781% 0.814%
bm25 freq d n copular 62.094% 0.012 0.153 0.013 3.458% 2.834% 2.813% 2.586%
bm25 freq with sw 79.827% 0.581 0.583 0.581 0.390% 1.685% 1.581% 1.250%
freq 79.276% 0.56 0.566 0.561 1.596% 1.221% 1.344% 1.330%
freq with sw 75.564% 0.47 0.482 0.47 1.752% 0.638% 1.056% 2.205%
Table 3: Sentiment classification accuracy (average 10-fold cross-validation), Scott?s pi, Krippendorff?s
?, Cohen?s ? and trade returns of different feature sets and term frequency weighting schemes in Exp. 3.
The same folds were used for the different representations. The non-annualized returns are presented in
columns 3-6.
strategy in place, it is clearly possible to vary some
aspect of the sentiment analyzer in order to deter-
mine its best setting in this context. Is classifier ac-
curacy a suitable proxy for this? Indeed, we may
hope that classifier accuracy will be more portable
to other possible tasks, but then it must at least
correlate well with task-based performance.
We tried another feature representation for doc-
uments. In addition to evaluating those attempted
earlier, we now hypothesize that the passive voice
may be useful to emphasize in our representations,
as the existential passive can be used to evade re-
sponsibility. So we add to the BM25 weighted
vector the counts of word tokens ending in ?n? or
?d? as well as the total count of every conjugated
form of the copular verb: ?be?, ?is?, ?am?, ?are?,
?were?, ?was?, and ?been?. These three features
are superficial indicators of the passive voice.
Table 3 presents the returns obtained from
these 6 feature representations. The feature set
with BM25-weighted term frequencies plus the
number of copulars and tokens ending in ?n?,
?d? (bm25 freq d n copular) yields higher returns
than any other representation attempted on the 5,
3, and 1 day holding periods, and the second-
highest on the 30 days holding period, But it has
the worst classification accuracy by far: a full 18
percentage points below term presence. This is a
very compelling illustration of how misleading an
intrinsic evaluation can be. Other agreement mea-
sures likewise point in the opposite direction.
5 Conclusion
In this paper, we examined the application of senti-
ment analysis in stock trading strategies. We built
a binary sentiment classifier that achieves high ac-
curacy when tested on movie data and financial
news data from Reuters. In three task-based ex-
periments, we evaluated the usefulness of senti-
ment analysis in simple trading strategies. Al-
though high annual returns can be achieved by
simply utilizing sentiment labels in a trading strat-
egy, they can be improved by incorporating the
output of the SVM?s decision function. We have
observed that classification accuracy alone is not
always an accurate predictor of task-based perfor-
mance. This calls into question the benefit of using
intrinsic sentiment classification accuracy, partic-
ularly when the relative cost of a task-based eval-
uation may be comparably low. We have also de-
termined that training on human-annotated senti-
ment does in fact perform better than training on
market returns themselves. So sentiment analysis
is an important component, but it must be tuned
against task data.
As for future work, we plan to explore other
ways of deriving sentiment labels for supervised
training. It would be interesting to infer the senti-
ment of published news from stock price fluctua-
tions instead of the reverse. Given that many fac-
tors that affect stock price fluctuations and further
considering the drift that is present in stock prices
as a result of bad published news (Chan, 2003),
this mode of inference is not simple and requires
careful consideration and design.
Furthermore, we would like to study how senti-
ment is defined in the financial world. In particu-
lar, we want to examine the relationship between
the precise definition of news sentiment and trad-
ing strategy returns. This study has used a rather
general definition of news sentiment. We are in-
terested in exploring if there is a more precise def-
inition that can improve trading performance.
Our current price data only includes adjusted
opening and closing prices. Most of our news data
contain only the date of the article, not the specific
time. It is possible that a much shorter-term trad-
ing strategy than we can currently test would be
even more successful.
126
References
Khurshid Ahmad, David Cheng, and Yousif Almas.
2006. Multi-lingual sentiment analysis of financial
news streams. In Proceedings of the 1st Interna-
tional Conference on Grid in Finance.
Werner Antweiler and Murray Z Frank. 2004. Is all
that talk just noise? the information content of inter-
net stock message boards. The Journal of Finance,
59(3):1259?1294.
Bernhard E. Boser, Isabelle M. Guyon, and
Vladimir N. Vapnik. 1992. A training algo-
rithm for optimal margin classifiers. In Proceedings
of the fifth annual workshop on Computational
learning theory, COLT ?92, pages 144?152, New
York, NY, USA. ACM.
Matthew Butler and Vlado Keselj. 2009. Finan-
cial forecasting using character n-gram analysis and
readability scores of annual reports. In Proceedings
of Canadian AI?2009, Kelowna, BC, Canada, May.
Wesley S. Chan. 2003. Stock price reaction to news
and no-news: Drift and reversal after headlines.
Journal of Financial Economics, 70(2):223?260.
Sanjiv R. Das and Mike Y. Chen. 2007. Yahoo! for
amazon: Sentiment extraction from small talk on the
web. Management Science, 53(9):1375?1388.
Joseph Davies-Gavin, Clarence Lee, and Lingling
Zhang. 2012. Conference summary. In Marketing
Science Institute Conference on Big Data, Decem-
ber.
Ann Devitt and Khurshid Ahmad. 2007. Sentiment
polarity identification in financial news: A cohesion-
based approach. In Proceedings of the ACL.
Brett Drury and J. J. Almeida. 2011. Identification
of fine grained feature based event and sentiment
phrases from business news stories. In Proceedings
of the International Conference on Web Intelligence,
Mining and Semantics, WIMS ?11, pages 27:1?27:7,
New York, NY, USA. ACM.
Robert F. Engle and Victor K. Ng. 1993. Measuring
and testing the impact of news on volatility. The
Journal of Finance, 48(5):1749?1778.
Tak-Chung Fu, Ka ki Lee, Donahue C. M. Sze, Fu-Lai
Chung, Chak man Ng, and Chak man Ng. 2008.
Discovering the correlation between stock time se-
ries and financial news. In Web Intelligence, pages
880?883.
Thorsten Joachims. 1999. Making large-scale svm
learning practical. advances in kernel methods-
support vector learning, b. sch?olkopf and c. burges
and a. smola.
Moshe Koppel and Itai Shtrimberg. 2004. Good news
or bad news? let the market decide. In AAAI Spring
Symposium on Exploring Attitude and Affect in Text,
pages 86?88. Press.
Chenghua Lin and Yulan He. 2009. Joint senti-
ment/topic model for sentiment analysis. In Pro-
ceedings of the 18th ACM conference on Informa-
tion and knowledge management, CIKM ?09, pages
375?384, New York, NY, USA. ACM.
Victor Niederhoffer. 1971. The analysis of world
events and stock prices. Journal of Business, pages
193?219.
Neil O?Hare, Michael Davy, Adam Bermingham,
Paul Ferguson, P?araic Sheridan, Cathal Gurrin, and
Alan F. Smeaton. 2009. Topic-dependent senti-
ment analysis of financial blogs. In Proceedings
of the 1st international CIKM workshop on Topic-
sentiment analysis for mass opinion measurement.
Georgios Paltoglou and Mike Thelwall. 2010. A study
of information retrieval weighting schemes for sen-
timent analysis. In Proceedings of the ACL, pages
1386?1395. Association for Computational Linguis-
tics.
Bo Pang and Lillian Lee. 2004. A sentimental educa-
tion: Sentiment analysis using subjectivity summa-
rization based on minimum cuts. In Proceedings of
the ACL, pages 271?278.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and trends in infor-
mation retrieval, 2(1-2):1?135.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using
machine learning techniques. In Proceedings of the
ACL-02 conference on Empirical methods in natu-
ral language processing - Volume 10, EMNLP ?02,
pages 79?86, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Paul C. Tetlock. 2007. Giving content to investor sen-
timent: The role of media in the stock market. The
Journal of Finance, 62(3):1139?1168.
Christa Williford, Charles Henry, and Amy Friedlan-
der. 2012. One culture: Computationally inten-
sive research in the humanities and social sciences.
Technical report, Council on Library and Informa-
tion Resources, June.
Wenbin Zhang and Steven Skiena. 2010. Trading
strategies to exploit blog and news sentiment. In The
4th International AAAI Conference on Weblogs and
Social Media.
127
