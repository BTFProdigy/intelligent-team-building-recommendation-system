Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 107?115,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
Sparse Approximate Dynamic Programming for Dialog Management
Senthilkumar Chandramohan, Matthieu Geist, Olivier Pietquin
SUPELEC - IMS Research Group, Metz - France.
{senthilkumar.chandramohan, matthieu.geist, olivier.pietquin}@supelec.fr
Abstract
Spoken dialogue management strategy op-
timization by means of Reinforcement
Learning (RL) is now part of the state of
the art. Yet, there is still a clear mis-
match between the complexity implied by
the required naturalness of dialogue sys-
tems and the inability of standard RL al-
gorithms to scale up. Another issue is the
sparsity of the data available for training in
the dialogue domain which can not ensure
convergence of most of RL algorithms.
In this paper, we propose to combine a
sample-efficient generalization framework
for RL with a feature selection algorithm
for the learning of an optimal spoken dia-
logue management strategy.
1 Introduction
Optimization of dialogue management strategies
by means of Reinforcement Learning (RL) (Sut-
ton and Barto, 1998) is now part of the state of
the art in the research area of Spoken Dialogue
Systems (SDS) (Levin and Pieraccini, 1998; Singh
et al, 1999; Pietquin and Dutoit, 2006; Williams
and Young, 2007). It consists in casting the dia-
logue management problem into the Markov Deci-
sion Processes (MDP) paradigm (Bellman, 1957)
and solving the associated optimization problem.
Yet, there is still a clear mismatch between the
complexity implied by the required naturalness of
the dialogue systems and the inability of standard
RL algorithms to scale up. Another issue is the
sparsity of the data available for training in the
dialogue domain because collecting and annotat-
ing data is very time consuming. Yet, RL algo-
rithms are very data demanding and low amounts
of data can not ensure convergence of most of
RL algorithms. This latter problem has been ex-
tensively studied in the recent years and is ad-
dressed by simulating new dialogues thanks to
a statistical model of human-machine interaction
(Pietquin, 2005) and user modeling (Eckert et al,
1997; Pietquin and Dutoit, 2006; Schatzmann et
al., 2006). However, this results in a variability of
the learned strategy depending on the user model-
ing method (Schatzmann et al, 2005) and no com-
mon agreement exists on the best user model.
The former problem, that is dealing with com-
plex dialogue systems within the RL framework,
has received much less attention. Although some
works can be found in the SDS literature it is far
from taking advantage of the large amount of ma-
chine learning literature devoted to this problem.
In (Williams and Young, 2005), the authors reduce
the complexity of the problem (which is actually a
Partially Observable MDP) by automatically con-
densing the continuous state space in a so-called
summary space. This results in a clustering of the
state space in a discrete set of states on which stan-
dard RL algorithms are applied. In (Henderson et
al., 2008), the authors use a linear approximation
scheme and apply the SARSA(?) algorithm (Sut-
ton and Barto, 1998) in a batch setting (from data
and not from interactions or simulations). This al-
gorithm was actually designed for online learning
and is known to converge very slowly. It there-
fore requires a lot of data and especially in large
state spaces. Moreover, the choice of the features
used for the linear approximation is particularly
simple since features are the state variables them-
selves. The approximated function can therefore
not be more complex than an hyper-plane in the
state variables space. This drawback is shared by
the approach of (Li et al, 2009) where a batch al-
gorithm (Least Square Policy Iteration or LSPI) is
combined to a pruning method to only keep the
most meaningful features. In addition the com-
plexity of LSPI is O(p3).
In the machine learning community, this issue
is actually addressed by function approximation
accompanied with dimensionality reduction. The
107
data sparsity problem is also widely addressed in
this literature, and sample-efficiency is one main
trend of research in this field. In this paper, we
propose to combine a sample-efficient batch RL
algorithm (namely the Fitted Value Iteration (FVI)
algorithm) with a feature selection method in a
novel manner and to apply this original combi-
nation to the learning of an optimal spoken dia-
logue strategy. Although the algorithm uses a lin-
ear combination of features (or basis functions),
these features are much richer in their ability of
representing complex functions.
The ultimate goal of this research is to provide
a way of learning optimal dialogue policies for a
large set of situations from a small and fixed set of
annotated data in a tractable way.
The rest of this paper is structured as follows.
Section 2 gives a formal insight of MDP and
briefly reminds the casting of the dialogue prob-
lem into the MDP framework. Section 3.2 pro-
vides a description of approximate Dynamic Pro-
gramming along with LSPI and FVI algorithms.
Section 4 provides an overview on how LSPI and
FVI can be combined with a feature selection
scheme (which is employed to learn the represen-
tation of theQ-function from the dialogue corpus).
Our experimental set-up, results and a comparison
with state-of-the-art methods are presented in Sec-
tion 5. Eventually, Section 6 concludes.
2 Markov Decision Processes
The MDP (Puterman, 1994) framework is used
to describe and solve sequential decision mak-
ing problems or equivalently optimal control prob-
lems in the case of stochastic dynamic systems.
AnMDP is formally a tuple {S,A, P,R, ?}where
S is the (finite) state space, A the (finite) action
space, P ? P(S)S?A the family of Markovian
transition probabilities1, R ? RS?A?S the reward
function and ? the discounting factor (0 ? ? ? 1).
According to this formalism, a system to be con-
trolled steps from state to state (s ? S) according
to transition probabilities P as a consequence of
the controller?s actions (a ? A). After each tran-
sition, the system generates an immediate reward
(r) according to its reward function R. How the
system is controlled is modeled with a so-called
policy pi ? AS mapping states to actions. The
quality of a policy is quantified by the so-called
value function which maps each state to the ex-
1Notation f ? AB is equivalent to f : B ? A
pected discounted cumulative reward given that
the agent starts in this state and follows the policy
pi: V pi(s) = E[
??
i=0 ?
iri|s0 = s, pi]. An optimal
policy pi? maximizes this function for each state:
pi? = argmaxpi V
pi. Suppose that we are given the
optimal value function V ? (that is the value func-
tion associated to an optimal policy), deriving the
associated policy would require to know the transi-
tion probabilities P . Yet, this is usually unknown.
This is why the state-action value (or Q-) function
is introduced. It adds a degree of freedom on the
choice of the first action:
Qpi(s, a) = E[
??
i=0
?iri|s0 = s, a0 = a, pi] (1)
The optimal policy is noted pi? and the related
Q-function Q?(s, a). An action-selection strategy
that is greedy according to this function (pi(s) =
argmaxa Q
?(s, a)) provides an optimal policy.
2.1 Dialogue as an MDP
The casting of the spoken dialogue management
problem into the MDP framework (MDP-SDS)
comes from the equivalence of this problem to
a sequential decision making problem. Indeed,
the role of the dialogue manager (or the decision
maker) is to select and perform dialogue acts (ac-
tions in the MDP paradigm) when it reaches a
given dialogue turn (state in the MDP paradigm)
while interacting with a human user. There can
be several types of system dialogue acts. For
example, in the case of a restaurant information
system, possible acts are request(cuisine type),
provide(address), confirm(price range), close etc.
The dialogue state is usually represented effi-
ciently by the Information State paradigm (Lars-
son and Traum, 2000). In this paradigm, the di-
alogue state contains a compact representation of
the history of the dialogue in terms of system acts
and its subsequent user responses (user acts). It
summarizes the information exchanged between
the user and the system until the considered state
is reached.
A dialogue management strategy is thus a map-
ping between dialogue states and dialogue acts.
Still following the MDP?s definitions, the optimal
strategy is the one that maximizes some cumula-
tive function of rewards collected all along the in-
teraction. A common choice for the immediate
reward is the contribution of each action to user
satisfaction (Singh et al, 1999). This subjective
108
reward is usually approximated by a linear com-
bination of objective measures like dialogue dura-
tion, number of ASR errors, task completion etc.
(Walker et al, 1997).
3 Solving MDPs
3.1 Dynamic Programming
Dynamic programming (DP) (Bellman, 1957)
aims at computing the optimal policy pi? if the
transition probabilities and the reward function are
known.
First, the policy iteration algorithm computes
the optimal policy in an iterative way. The ini-
tial policy is arbitrary set to pi0. At iteration k, the
policy pik?1 is evaluated, that is the associated Q-
function Qpik?1(s, a) is computed. To do so, the
Markovian property of the transition probabilities
is used to rewrite Equation (1) as :
Qpi(s, a) = Es?|s,a[R(s, a, s
?) + ?Qpi(s?, pi(s?))]
= T piQpi(s, a) (2)
This is the so-called Bellman evaluation equa-
tion and T pi is the Bellman evaluation opera-
tor. T pi is linear and therefore this defines a lin-
ear system that can be solved by standard meth-
ods or by an iterative method using the fact
that Qpi is the unique fixed-point of the Bell-
man evaluation operator (T pi being a contrac-
tion): Q?pii = T
piQ?pii?1, ?Q?
pi
0 limi?? Q?
pi
i =
Qpi. Then the policy is improved, that is
pik is greedy respectively to Qpik?1 : pik(s) =
argmaxa?A Q
pik?1(s, a). Evaluation and im-
provement steps are iterated until convergence of
pik to pi? (which can be demonstrated to happen in
a finite number of iterations when pik = pik?1).
The value iteration algorithm aims at estimat-
ing directly the optimal state-action value function
Q? which is the solution of the Bellman optimality
equation (or equivalently the unique fixed-point of
the Bellman optimality operator T ?):
Q?(s, a) = Es?|s,a[R(s, a, s
?) + ? max
b?A
Q?(s?, b)]
= T ?Q?(s, a) (3)
The T ? operator is not linear, therefore comput-
ingQ? via standard system-solving methods is not
possible. However, it can be shown that T ? is
also a contraction (Puterman, 1994). Therefore,
according to Banach fixed-point theorem, Q? can
be estimated using the following iterative way:
Q??i = T
?Q??i?1, ?Q?
?
0 limi??
Q??i = Q
? (4)
However, the convergence takes an infinite num-
ber of iterations. Practically speaking, iterations
are stopped when some criterion is met, classi-
cally a small difference between two iterations:
?Q??i ? Q?
?
i?1? < ?. The estimated optimal pol-
icy (which is what we are ultimately interested in)
is greedy respectively to the estimated optimal Q-
function: p?i?(s) = argmaxa?A Q?
?(s, a).
3.2 Approximate Dynamic Programming
DP-based approaches have two drawbacks. First,
they assume the transition probabilities and the re-
ward function to be known. Practically, it is rarely
true and especially in the case of spoken dialogue
systems. Most often, only examples of dialogues
are available which are actually trajectories in the
state-action space. Second, it assumes that the Q-
function can be exactly represented. However, in
real world dialogue management problems, state
and action spaces are often too large (even contin-
uous) for such an assumption to hold. Approxi-
mate Dynamic Programming (ADP) aims at esti-
mating the optimal policy from trajectories when
the state space is too large for a tabular representa-
tion. It assumes that theQ-function can be approx-
imated by some parameterized function Q??(s, a).
In this paper, a linear approximation of the Q-
function will be assumed: Q??(s, a) = ?T?(s, a).
where ? ? Rp is the parameter vector and ?(s, a)
is the set of p basis functions. All functions ex-
pressed in this way define a so-called hypothesis
space H = {Q??|? ? Rp}. Any function Q can be
projected onto this hypothesis space by the opera-
tor ? defined as
?Q = argmin
Q???H
?Q? Q???
2. (5)
The goal of the ADP algorithms explained in the
subsequent sections is to compute the best set of
parameters ? given the basis functions.
3.2.1 Least-Squares Policy Iteration
The least-squares policy iteration (LSPI) algo-
rithm has been introduced by Lagoudakis and Parr
(2003). The underlying idea is exactly the same
as for policy iteration: interleaving evaluation and
improvement steps. The improvement steps are
same as before, but the evaluation step should
learn an approximate representation of the Q-
function using samples. In LSPI, this is done using
the Least-Squares Temporal Differences (LSTD)
algorithm of Bradtke and Barto (1996).
109
LSTD aims at minimizing the distance between
the approximated Q-function Q?? and the projec-
tion onto the hypothesis space of its image through
the Bellman evaluation operator ?T piQ??: ?pi =
argmin??Rp ?Q?? ? ?T
piQ???2. This can be in-
terpreted as trying to minimize the difference be-
tween the two sides of the Bellman equation (1)
(which should ideally be zero) in the hypothesis
space. Because of the approximation, this differ-
ence is most likely to be non-zero.
Practically, T pi is not known, but a set of
N transitions {(sj , aj , rj , s?j)1?j?N} is available.
LSTD therefore solves the following optimiza-
tion problem: ?pi = argmin?
?N
j=1 C
N
j (?) where
CNj (?) = (rj +?Q??pi(s
?
j , pi(s
?
j))??Q??(sj , aj))
2.
Notice that ?pi appears in both sides of the equa-
tion, which renders this problem difficult to solve.
However, thanks to the linear parametrization, it
admits an analytical solution, which defines the
LSTD algorithm:
?pi = (
N?
j=1
?j??
pi
j )
?1
N?
j=1
?jrj (6)
with ?j = ?(sj , aj) and ??pij = ?(sj , aj) ?
??(s?j , pi(s
?
j)).
LSPI is initialized with a policy pi0. Then, at
iteration k, the Q-function of policy pik?1 is esti-
mated using LSTD, and pik is greedy respectively
to this estimated state-action value function. Itera-
tions are stopped when some stopping criterion is
met (e.g., small differences between consecutive
policies or associated Q-functions).
3.2.2 Least-Squares Fitted Value Iteration
The Fitted Value Iteration (FVI) class of algo-
rithms (Bellman and Dreyfus, 1959; Gordon,
1995; Ernst et al, 2005) generalizes value iter-
ation to model-free and large state space prob-
lems. The T ? operator (eq. (3)) being a con-
traction, a straightforward idea would be to apply
it iteratively to the approximation similarly to eq.
(4): Q??k = T
?Q??k?1 . However, T
?Q?? does not
necessarily lie in H, it should thus be projected
again onto the hypothesis space H. By consider-
ing the same projection operator ? as before, this
leads to finding the parameter vector ? satisfying:
Q??? = ?T
?Q???. The fitted-Q algorithm (a spe-
cial case of FVI) assumes that the composed ?T ?
operator is a contraction and therefore admits an
unique fixed point, which is searched for through
the classic iterative scheme: Q??k = ?T
?Q??k?1 .
However, the model (transition probabilities and
the reward function) is usually not known, there-
fore a sampled Bellman optimality operator T? ?
is considered instead. For a transition sample
(sj , aj , rj , s?j), it is defined as: T?
?Q(sj , aj) =
rj + ? maxa?A Q(s?j , a). This defines the general
fitted-Q algorithm (?0 being chosen by the user):
Q??k = ?T?
?Q??k?1 . Fitted-Q can then be special-
ized by choosing how T? ?Q??k?1 is projected onto
the hypothesis space, that is the supervised learn-
ing algorithm that solves the projection problem
of eq. (5). The least squares algorithm is chosen
here.
The parametrization being linear, and a train-
ing base {(sj , aj , rj , s?j)1?j?N} being available,
the least-squares fitted-Q (LSFQ for short) is de-
rived as follows (we note ?(sj , aj) = ?j):
?k = argmin
??Rp
NX
j=1
(T? ?Q??k?1(sj , aj)? Q??(sj , aj))
2 (7)
= (
NX
j=1
?j?
T
j )
?1
NX
j=1
?j(rj + ? max
a?A
(?Tk?1?(s
?
j , a)))
Equation (7) defines an iteration of the proposed
linear least-squares-based fitted-Q algorithm. An
initial parameter vector ?0 should be chosen, and
iterations are stopped when some criterion is met
(maximum number of iterations or small differ-
ence between two consecutive parameter vector
estimates). Assuming that there are M itera-
tions, the optimal policy is estimated as p?i?(s) =
argmaxa?A Q??M (s, a).
4 Learning a sparse parametrization
LSPI and LSFQ (FVI) assume that the basis func-
tions are chosen beforehand. However, this is dif-
ficult and problem-dependent. Thus, we propose
to combine these algorithms with a scheme which
learns the representation from dialogue corpora.
Let?s place ourselves in a general context. We
want to learn a parametric representation for an
approximated function f?(z) = ?T?(z) from
samples {z1, . . . , zN}. A classical choice is to
choose a kernel-based representation (Scholkopf
and Smola, 2001). Formally, a kernel K(z, z?i)
is a continuous, positive and semi-definite func-
tion (e.g., Gaussian or polynomial kernels) cen-
tered on z?i. The feature vector ?(z) is therefore
of the form: ?(z) =
(
K(z, z?1) . . . K(z, z?p)
)
.
The question this section answers is the following:
given the training basis {z1, . . . , zN} and a kernel
110
K, how to choose the number p of basis functions
and the associated kernel centers (z?1, . . . , z?p)?
An important result about kernels is the Mer-
cer theorem, which states that for each kernel
K there exists a mapping ? : z ? Z ?
?(z) ? F such that ?z1, z2 ? Z, K(z1, z2) =
??(z1), ?(z2)? (in short, K defines a dot prod-
uct in F). The space F is called the feature
space, and it can be of infinite dimension (e.g.,
Gaussian kernel), therefore ? cannot always be
explicitly built. Given this result and from the
bilinearity of the dot product, f? can be rewrit-
ten as follows: f?(z) =
?p
i=1 ?iK(z, z?i) =
??(z),
?p
i=1 ?i?(z?i)?. Therefore, a kernel-based
parametrization corresponds to a linear approx-
imation in the feature space, the weight vector
being
?p
i=1 ?i?(z?i). This is called the kernel
trick. Consequently, kernel centers (z?1, . . . , z?p)
should be chosen such that (?(z?1), . . . , ?(z?p)) are
linearly independent in order to avoid using re-
dundant basis functions. Moreover, kernel cen-
ters should be chosen among the training samples.
To sum up, learning such a parametrization re-
duces to finding a dictionary D = (z?1, . . . , z?p) ?
{z1, . . . , zN} such that (?(z?1), . . . , ?(z?p)) are lin-
early independent and such that they span the
same subspace as (?(z1), . . . , ?(zN )). Engel et
al. (2004) provides a dictionary method to solve
this problem, briefly sketched here.
The training base is sequentially processed, and
the dictionary is initiated with the first sample:
D1 = {z1}. At iteration k, a dictionary Dk?1
computed from {z1, . . . , zk?1} is available and the
kth sample zk is considered. If ?(zk) is linearly
independent of ?(Dk?1), then it is added to the
dictionary: Dk = Dk?1 ? {zk}. Otherwise, the
dictionary remains unchanged: Dk = Dk?1. Lin-
ear dependency can be checked by solving the
following optimization problem (pk?1 being the
size of Dk?1): ? = argminw?Rpk?1 ??(zk) ??pk?1
i=1 wi?(z?i)?
2. Thanks to the kernel trick (that
is the fact that ??(zk), ?(z?i)? = K(zk, z?i)) and to
the bilinearity of the dot product, this optimization
problem can be solved analytically and without
computing explicitly ?. Formally, linear depen-
dency is satisfied if ? = 0. However, an approxi-
mate linear dependency is allowed, and ?(zk) will
be considered as linearly dependent of ?(Dk?1) if
? < ?, where ? is the so-called sparsification fac-
tor. This allows controlling the trade-off between
quality of the representation and its sparsity. See
Engel et al (2004) for details as well as an efficient
implementation of this dictionary approach.
4.1 Resulting algorithms
We propose to combine LSPI and LSFQ with the
sparsification approach exposed in the previous
section: a kernel is chosen, the dictionary is com-
puted and then LSPI or LSFQ is applied using the
learnt basis functions. For LSPI, this scheme has
been proposed before by Xu et al (2007) (with
the difference that they generate new trajectories
at each iteration whereas we use the same for all
iterations). The proposed sparse LSFQ algorithm
is a novel contribution of this paper.
We start with the sparse LSFQ algorithm. In or-
der to train the dictionary, the inputs are needed
(state-action couples in this case), but not the out-
puts (reward are not used). For LSFQ, the input
space remains the same over iterations, therefore
the dictionary can be computed in a preprocessing
step from {(sj , aj)1?j?N}. Notice that the matrix
(
?N
j=1 ?j?
T
j )
?1 remains also the same over itera-
tions, therefore it can be computed in a preprocess-
ing step too. The proposed sparse LSFQ algorithm
is summarized in appendix Algorithm 1.
For the sparse LSPI algorithm, things are
different. This time, the inputs depend on
the iteration. More precisely, at iteration k,
the input is composed of state-action couples
(sj , aj) but also of transiting state-action cou-
ples (s?j , pik?1(s
?
j)). Therefore the dictionary
has to be computed at each iteration from
{(sj , aj)1?j?N , (s?j , pik?1(s
?
j))1?j?N}. This de-
fines the parametrization which is considered for
the Q-function evaluation. The rest of the algo-
rithm is as for the classic LSPI and it is summa-
rized in appendix Algorithm 2.
Notice that sparse LSFQ has a lower computa-
tional complexity than the sparse LSPI. For sparse
LSFQ, dictionary and the matrix P?1 are com-
puted in a preprocessing step, therefore the com-
plexity per iteration is in O(p2), with p being
the number of basis functions computed using the
dictionary method. For LSPI, the inverse matrix
depends on the iteration, as well as the dictio-
nary, therefore the computational complexity is in
O(p3k) per iteration, where pk is the size of the dic-
tionary computed at the kth iteration.
111
5 Experimental set-up and results
5.1 Dialogue task and RL parameters
The experimental setup is a form-filling dialogue
system in the tourist information domain similar to
the one studied in (Lemon et al, 2006). The sys-
tem aims to give information about restaurants in
the city based on specific user preferences. Three
slots are considered: (i) location, (ii) cuisine and
(iii) price-range of the restaurant. The dialogue
state has three continuous components ranging
from 0 to 1, each representing the average of filling
and confirmation confidence of the corresponding
slots. The MDP SDS has 13 actions: Ask-slot
(3 actions), Explicit-confirm (3 actions), Implicit-
confirm and Ask-slot value (6 actions) and Close-
dialogue (1 action). The ? parameter was set to
0.95 in order to encourage delayed rewards and
also to induce an implicit penalty for the length of
the dialogue episode. The reward function R is
presented as follows: every correct slot filling is
awarded 25, every incorrect slot filling is awarded
-75 and every empty slot filling is awarded -300.
The reward is awarded at the end of the dialogue.
5.2 Dialogue corpora for policy optimization
So as to perform sparse LSFQ or sparse LSPI, a di-
alogue corpus which represents the problem space
is needed. As for any batch learning method, the
samples used for learning should be chosen (if
they can be chosen) to span across the problem
space. In this experiment, a user simulation tech-
nique was used to generate the data corpora. This
way, the sensibility of the method to the size of
the training data-set could be analyzed (available
human-dialogue corpora are limited in size). The
user simulator was plugged to the DIPPER (Lemon
et al, 2006) dialogue management system to gen-
erate dialogue samples. To generate data, the dia-
logue manager strategy was jointly based on a sim-
ple hand-coded policy (which aims only to fill all
the slots before closing the dialogue episode irre-
spective of slot confidence score i.e.,) and random
action selection.
Randomly selected system acts are used with
probability ? and hand-coded policy selected sys-
tem acts are used with probability (1-?). During
our data generation process the ? value was set to
0.9. Rather than using a fully random policy we
used an ?-greedy policy to ensure that the prob-
lem space is well sampled and in the same time at
least few episodes have successful completion of
task compared to a totally random policy. We ran
56,485 episodes between the policy learner and
an unigram user simulation, using the ?-greedy
policy (of which 65% are successful task com-
pletion episodes) and collected 393,896 dialogue
turns (state transitions). The maximum episode
length is set as 100 dialogue turns. The dialogue
turns (samples) are then divided into eight differ-
ent training sets each with 5.104 samples.
5.3 Linear representation of Q-function
Two different linear representations of the Q-
function were used. First, a set of basis functions
computed using the dictionary method outlined in
Section 4 is used. A Gaussian kernel is used for
the dictionary computation (? = 0.25). The num-
ber of elements present in the dictionary varied
based on the number of samples used for computa-
tion and the sparsification factor. It was observed
during the experiments that including a constant
term to the Q-function representation (value set
to 1) in addition to features selected by the dic-
tionary method avoided weight divergence. Our
second representation of Q-function used a set of
hand-picked features presented as a set of Gaus-
sian functions, centered in ?i and with the same
standard deviation ?i = ?). Our RBF network
had 3 Gaussians for each dimension in the state
vector and considering that we have 13 actions, in
total we used 351 (i.e, 33 ? 13) features for ap-
proximating theQ-function. This allows consider-
ing that each state variable contributes to the value
function differently according to its value contrar-
ily to similar work (Li et al, 2009; Henderson et
al., 2008) that considers linear contribution of each
state variable. Gaussians were centered at ?i = 0.0,
0.5, 1.0 in every dimension with a standard devi-
ation ?i = ? = 0.25. Our stopping criteria was
based on comparison between L1 norm of suc-
ceeding weights and a threshold ? which was set to
10?2 i.e, convergence if
?
i
(
|?ni ? ?
n?1
i |
1
)
< ?,
where n is the iteration number. For sparse LSPI
since the dictionary is computed during each iter-
ation, stopping criteria based on ? is not feasible
thus the learning was stopped after 30 iterations.
5.4 Evaluation of learned policy
We ran a set of learning iterations using two differ-
ent representations of Q-function and with differ-
ent numbers of training samples (one sample is a
dialogue turn, that is a state transition {s, a, r, s?}).
The number of samples used for training ranged
112
 
0
 
10
 
20
 
30
 
40
 
50
 
60
 
70
 
5000
 
10000
 
20000
 
30000
 
40000
 
50000
Average discounted sum of rewards
Numbe
r of sam
ples us
ed for 
trainin
g
Hand c
rafted FittedQ LSPI
Sparse
 Fitted
Q (Nu=0
.7)
Sparse
 Fitted
Q (Nu=0
.8)
Figure 1: FittedQ policy evaluation statistics
from 1.103 to 50.103 samples (no convergence of
weights was observed with fewer samples than
1.103). The training is repeated for each of the 8
training data sets. Dictionary computed using dif-
ferent number of training samples and with ?=0.7
and 0.8 had a maximum of 367 and 306 elements
respectively (with lower values of ? the number
of features is higher than the hand-selected ver-
sion). The policies learned were then tested us-
ing a unigram user simulation and the DIPPER di-
alogue management framework. Figures 1 and 2
show the average discounted sum of rewards of
policies tested over 8?25 dialogue episodes.
5.5 Analysis of evaluation results
Our experimental results show that the dialogue
policies learned using sparse SLFQ and LSPI with
the two different Q-function representations per-
form significantly better than the hand-coded pol-
icy. Most importantly it can be observed from
Figure 1 and 2 that the performance of sparse
LSFQ and sparse LSPI (which uses the dictionary
method for feature selection) are nearly as good
as LSFQ and LSPI (which employs more numer-
ous hand-selected basis functions). This shows the
effectiveness of using the dictionary method for
learning the representation of the Q-function from
the dialogue corpora. For this specific problem
the set of hand selected features seem to perform
better than sparse LSPI and sparse LSFQ, but this
may not be always the case. For complex dialogue
management problems feature selection methods
such as the one studied here will be handy since
the option of manually selecting a good set of fea-
tures will cease to exist.
Secondly it can be concluded that, similar to
LSFQ and LSPI, the sparse LSFQ and sparse LSPI
based dialogue management are also sample effi-
 
0
 
10
 
20
 
30
 
40
 
50
 
60
 
70
 
5000
 
10000
 
20000
 
30000
 
40000
 
50000
Average discounted sum of rewards
Numbe
r of sam
ples us
ed for 
trainin
g
Hand c
rafted LSPI
Sparse
-LSPI 
(Nu=0.7
)
Sparse
-LSPI 
(Nu=0.8
)
Figure 2: LSPI policy evaluation statistics
cient and needs only few thousand samples (recall
that a sample is a dialogue turn and not a dialogue
episode) to learn fairly good policies, thus exhibit-
ing a possibility to learn a good policy directly
from very limited amount of dialogue examples.
We believe this is a significant improvement when
compared to the corpora requirement for dialogue
management using other RL algorithms such as
SARSA. However, sparse LSPI seems to result in
poorer performance compared to sparse LSFQ.
One key advantage of using the dictionary
method is that only mandatory basis functions are
selected to be part of the dictionary. This results
in fewer feature weights ensuring faster conver-
gence during training. From Figure 1 it can also
be observed that the performance of both LSFQ
and LSPI (using hand selected features) are nearly
identical. From a computational complexity point
of view, LSFQ and LSPI roughly need the same
number of iterations before the stopping criterion
is met. However, reminding that the proposed
LSFQ complexity is O(p)2 per iteration whereas
LSPI complexity is O(p3) per iteration, LSFQ is
computationally less intensive.
6 Discussion and Conclusion
In this paper, we proposed two sample-efficient
generalization techniques to learn optimal dia-
logue policies from limited amounts of dialogue
examples (namely sparse LSFQ and LSPI). Par-
ticularly, a novel sparse LSFQ method has been
proposed and was demonstrated to out-perform
handcrafted and LSPI-based policies while using
a limited number of features. By using a kernel-
based approximation scheme, the power of repre-
sentation of the state-action value function (or Q-
function) is increased with comparison to state-of-
113
the-art algorithms (such as (Li et al, 2009; Hen-
derson et al, 2008)). Yet the number of features is
also increased. Using a sparsification algorithm,
this number is reduced while policy performances
are kept. In the future, more compact representa-
tion of the state-action value function will be in-
vestigated such as neural networks.
Acknowledgments
The work presented here is part of an ongoing re-
search for CLASSiC project (Grant No. 216594,
www.classic-project.org) funded by the European
Commission?s 7th Framework Programme (FP7).
References
Richard Bellman and Stuart Dreyfus. 1959. Functional
approximation and dynamic programming. Math-
ematical Tables and Other Aids to Computation,
13:247?251.
Richard Bellman. 1957. Dynamic Programming.
Dover Publications, sixth edition.
Steven J. Bradtke and Andrew G. Barto. 1996. Lin-
ear Least-Squares algorithms for temporal differ-
ence learning. Machine Learning, 22(1-3):33?57.
Wieland Eckert, Esther Levin, and Roberto Pieraccini.
1997. User Modeling for Spoken Dialogue System
Evaluation. In ASRU?97, pages 80?87.
Yaakov Engel, Shie Mannor, and Ron Meir. 2004. The
Kernel Recursive Least Squares Algorithm. IEEE
Transactions on Signal Processing, 52:2275?2285.
Damien Ernst, Pierre Geurts, and Louis Wehenkel.
2005. Tree-Based Batch Mode Reinforcement
Learning. Journal of Machine Learning Research,
6:503?556.
Geoffrey Gordon. 1995. Stable Function Approxima-
tion in Dynamic Programming. In ICML?95.
James Henderson, Oliver Lemon, and Kallirroi
Georgila. 2008. Hybrid reinforcement/supervised
learning of dialogue policies from fixed data sets.
Computational Linguistics, vol. 34(4), pp 487-511.
Michail G. Lagoudakis and Ronald Parr. 2003. Least-
squares policy iteration. Journal of Machine Learn-
ing Research, 4:1107?1149.
Staffan Larsson and David R. Traum. 2000. Informa-
tion state and dialogue management in the TRINDI
dialogue move engine toolkit. Natural Language
Engineering, vol. 6, pp 323?340.
Oliver Lemon, Kallirroi Georgila, James Henderson,
and Matthew Stuttle. 2006. An ISU dialogue sys-
tem exhibiting reinforcement learning of dialogue
policies: generic slot-filling in the TALK in-car sys-
tem. In EACL?06, Morristown, NJ, USA.
Esther Levin and Roberto Pieraccini. 1998. Us-
ing markov decision process for learning dialogue
strategies. In ICASSP?98.
Lihong Li, Suhrid Balakrishnan, and Jason Williams.
2009. Reinforcement Learning for Dialog Man-
agement using Least-Squares Policy Iteration and
Fast Feature Selection. In InterSpeech?09, Brighton
(UK).
Olivier Pietquin and Thierry Dutoit. 2006. A prob-
abilistic framework for dialog simulation and opti-
mal strategy learning. IEEE Transactions on Audio,
Speech & Language Processing, 14(2): 589-599.
Olivier Pietquin. 2005. A probabilistic descrip-
tion of man-machine spoken communication. In
ICME?05, pages 410?413, Amsterdam (The Nether-
lands), July.
Martin L. Puterman. 1994. Markov Decision Pro-
cesses: Discrete Stochastic Dynamic Programming.
Wiley-Interscience, April.
Jost Schatzmann, Matthew N. Stuttle, Karl Weilham-
mer, and Steve Young. 2005. Effects of the
user model on simulation-based learning of dialogue
strategies. In ASRU?05, December.
Jost Schatzmann, Karl Weilhammer, Matt Stuttle, and
Steve Young. 2006. A survey of statistical user sim-
ulation techniques for reinforcement-learning of dia-
logue management strategies. Knowledge Engineer-
ing Review, vol. 21(2), pp. 97?126.
Bernhard Scholkopf and Alexander J. Smola. 2001.
Learning with Kernels: Support Vector Machines,
Regularization, Optimization, and Beyond. MIT
Press, Cambridge, MA, USA.
Satinder Singh, Michael Kearns, Diane Litman, and
Marilyn Walker. 1999. Reinforcement learning for
spoken dialogue systems. In NIPS?99. Springer.
Richard S. Sutton and Andrew G. Barto. 1998. Re-
inforcement Learning: An Introduction (Adaptive
Computation and Machine Learning). The MIT
Press, 3rd edition, March.
Marilyn A. Walker, Diane J. Litman, Candace A.
Kamm, and Alicia Abella. 1997. PARADISE: A
framework for evaluating spoken dialogue agents.
In ACL?97, pages 271?280, Madrid (Spain).
Jason Williams and Steve Young. 2005. Scaling up
pomdps for dialogue management: the summary
pomdp method. In ASRU?05.
Jason D. Williams and Steve Young. 2007. Partially
observable markov decision processes for spoken di-
alog systems. Computer Speech and Language, vol.
21(2), pp. 393?422.
Xin Xu, Dewen Hu, and Xicheng Lu. 2007. Kernel-
based least squares policy iteration for reinforce-
ment learning. IEEE Transactions on Neural Net-
works, 18(4):973?992, July.
114
Appendix
This appendix provides pseudo code for the algo-
rithms described in the paper.
Algorithm 1: Sparse LSFQ.
Initialization;
Initialize vector ?0, choose a kernel K and a
sparsification factor ?;
Compute the dictionary;
D = {(s?j , a?j)1?j?p} from {(sj , aj)1?j?N};
Define the parametrization;
Q?(s, a) = ?T?(s, a) with ?(s, a) =
(K((s, a), (s?1, a?1)), . . . ,K((s, a), (s?p, a?p)))T ;
Compute P?1;
P?1 = (
?N
j=1 ?j?
T
j )
?1;
for k = 1, 2, . . . ,M do
Compute ?k, see Eq. (7);
end
p?i?M (s) = argmaxa?A Q??M (s, a);
Algorithm 2: Sparse LSPI.
Initialization;
Initialize policy pi0, choose a kernel K and a
sparsification factor ?;
for k = 1, 2, . . . do
Compute the dictionary;
D = {(s?j , a?j)1?j?pk} from
{(sj , aj)1?j?N , (s?j , pik?1(s
?
j))1?j?N};
Define the parametrization;
Q?(s, a) = ?T?(s, a) with ?(s, a) =
(K((s, a), (s?1, a?1)), . . . ,K((s, a), (s?pk , a?pk)))
T ;
Compute ?k?1, see Eq. (6);
Compute pik;
pik(s) = argmaxa?A Q??k?1(s, a);
end
115
NAACL-HLT 2012 Workshop on Future directions and needs in the Spoken Dialog Community: Tools and Data, pages 9?10,
Montre?al, Canada, June 7, 2012. c?2012 Association for Computational Linguistics
Statistical User Simulation for Spoken Dialogue Systems:
What for, Which Data, Which Future? ?
Olivier Pietquin
SUPELEC - UMI 2958 (GeorgiaTech - CNRS)
2 rue Edouard Belin
57070 Metz - France
olivier.pietquin@supelec.fr
Abstract
There has been a lot of interest for user sim-
ulation in the field of spoken dialogue sys-
tems during the last decades. User simulation
was first proposed to assess the performance
of SDS before a public release. Since the late
90?s, user simulation is also used for dialogue
management optimisation. In this position pa-
per, we focus on statistical methods for user
simulation, their main advantages and draw-
backs. We initiate a reflection about the util-
ity of such methods and give some insights of
what their future should be.
1 Introduction
User simulation for Spoken Dialogue Systems
(SDS) aims at generating artificial interactions sup-
posed to be representative of what would be an ac-
tual dialogue between a human user and a given
dialogue system. User simulation is thus different
from user modeling which is often included into the
systems to infer user goals from observable clues
(user?s utterances, intonations etc.) (Zukerman and
Albrecht, 2001). In this paper we focus on statistical
methods for user simulation, that is methods purely
based on data and statistical models and not cogni-
tive models. Also, we only address user simulations
working at the intention level, that is generating dia-
log acts and not speech or natural language (Schatz-
mann et al, 2006). User modeling, used to infer user
intentions in dialogue systems is not addressed.
?This work as been partially funded by the INTERREG IVa
project ALLEGRO and the Re?gion Lorraine
The aim of user simulation was initially to as-
sess the performance of a SDS before a public re-
lease (Eckert et al, 1997). Given a performance
metric and a simulation method, the natural idea of
automatically optimizing SDS (using reinforcement
learning RL) appeared in the literature in the late
90?s (Levin et al, 2000).
2 Is user simulation useful?
Initially, SDS optimisation required a lot of data be-
cause of inefficiency of RL algorithms, justifying
the use of simulation. In recent years, sample effi-
cient RL methods were applied to SDS optimization.
This allows learning optimal dialogue strategies di-
rectly from batches of data collected between sub-
optimal systems and actual users (Li et al, 2009;
Pietquin et al, 2011b) but also from online interac-
tions (Pietquin et al, 2011a; Gasic et al, 2011). Do
we have to conclude that user simulation is useless?
3 Do we need to train models?
It is commonly admitted that learning parameters of
user simulation models is hard because most of vari-
ables are hidden (user goal, mental states etc.) and
tricky to annotate. This is why current user simula-
tors are trainable but rarely trained (Pietquin, 2006;
Schatzmann et al, 2007). Do we really need to train
user simulation models? If so, which data and anno-
tation schemes do we need?
4 Does simulation reach the target?
User simulation aims at reproducing plausible inter-
actions but in contexts that were not seen in the data
9
collected to train the model. It is generally hard to
assess the quality of such models. Especially, it is
hard to find a single metric to assess user simulation
performances (Pietquin and Hastie, 2011). Also, it
has been shown that user simulation affects a lot the
result of SDS strategy optimisation (Schatzmann et
al., 2005). What should be assessed? Statistical
consistency, ability to generalize, ability to generate
sequences of interactions similar to real dialogues,
ability to produce optimal strategies by RL? If one
wants to learn an optimal simulation model, there is
a need for a single optimality criterion.
5 What?s the future of user simulation for
SDS?
Whatever the use one wants to make of user simula-
tion (learning or assessment for SDS), the future of
this research field relies probably on a redefinition of
the role of user simulation. So far, user simulation
is seen as a generative systems, generating dialog
acts according to the context. Current user simula-
tion models are therefore based on a large amount of
conditional probabilities which are hard to learn, and
the training (if there is one) requires a lot of prior
knowledge, the introduction of smoothing parame-
ters etc.
We believe that user simulation should be rede-
fined as a sequential decision making problem in
which a user tries to reach a goal in a natural and ef-
ficient way, helped by an artificial agent (the SDS).
One major difference between this vision and the
common probabilistic one is that it takes into ac-
count the fact that human users adapt their behav-
ior to the performances and the strategy of the SDS.
This can be called ?co-adaptation? between human
users and artificial systems and justifies that user
simulation should still be studied.
Recently, user simulation models based on inverse
reinforcement learning have been proposed (Chan-
dramohan et al, 2011). In this framework, a user
is modeled as optimizing it?s behavior according
to some unknown reward which is inferred from
recorded data. This might be an answer to the co-
adaptation problem. Yet, is user simulation still use-
ful in this framework? Knowing the reward of the
user, do we still need simulation or is it possible to
compute directly an optimal dialogue strategy?
References
S. Chandramohan, M. Geist, F. Lefe`vre, and O. Pietquin.
2011. User Simulation in Dialogue Systems using In-
verse Reinforcement Learning. In Proc. of Interspeech
2011, Florence (Italy).
W. Eckert, E. Levin, and R. Pieraccini. 1997. User
Modeling for Spoken Dialogue System Evaluation. In
Proc. of ASRU?97, Santa Barbara (USA).
M. Gasic, F. Jurcicek, B. Thomson, K. Yu, and S. Young.
2011. On-line policy optimisation of spoken dialogue
systems via live interaction with human subjects?. In
Proc. of ASRU 2011, Hawaii (USA).
E. Levin, R. Pieraccini, and W. Eckert. 2000. A Stochas-
tic Model of Human-Machine Interaction for learning
dialog Strategies. IEEE Transactions on Speech and
Audio Processing, 8:11?23.
L. Li, S. Balakrishnan, and J. Williams. 2009. Reinforce-
ment Learning for Dialog Management using Least-
Squares Policy Iteration and Fast Feature Selection. In
Proc. of InterSpeech?09, Brighton (UK).
O. Pietquin and H. Hastie. 2011. A survey on metrics for
the evaluation of user simulations. Knowledge Engi-
neering Review.
O. Pietquin, M. Geist, and S. Chandramohan. 2011a.
Sample Efficient On-line Learning of Optimal Dia-
logue Policies with Kalman Temporal Differences. In
Proc. of IJCAI 2011, Barcelona, Spain.
O. Pietquin, M. Geist, S. Chandramohan, and H. Frezza-
Buet. 2011b. Sample-Efficient Batch Reinforce-
ment Learning for Dialogue Management Optimiza-
tion. ACM Transactions on Speech and Language Pro-
cessing, 7(3):7:1?7:21, May.
O. Pietquin. 2006. Consistent goal-directed user model
for realistic man-machine task-oriented spoken dia-
logue simulation. In ICME?06, Toronto (Canada).
J. Schatzmann, M.Stuttle, K. Weilhammer, and S. Young.
2005. Effects of the user model on simulation-based
learning of dialogue strategies. In Proc. of ASRU?05.
J. Schatzmann, K. Weilhammer, M. Stuttle, and
S. Young. 2006. A survey of statistical user simula-
tion techniques for reinforcement-learning of dialogue
management strategies. Knowledge Engineering Re-
view, vol. 21(2), pp. 97?126.
J. Schatzmann, B. Thomson, K. Weilhammer, H. Ye, and
S. Young. 2007. Agenda-based User Simulation for
Bootstrapping a POMDP Dialogue System. In Proc.
of HLT NAACL.
I. Zukerman and D. Albrecht. 2001. Predictive statistical
models for user modeling. User Modeling and User-
Adapted Interaction, 11(1-2):5?18. invited paper.
10
Proceedings of the SIGDIAL 2013 Conference, pages 102?106,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Model-free POMDP optimisation of tutoring systems with echo-state
networks
Lucie Daubigney1,3 Matthieu Geist1
1IMS-MaLIS ? Supe?lec (Metz, France), 2UMI2958 ? GeorgiaTech/CNRS (Metz, France)
3Team project MaIA ? Loria (Nancy, France)
Olivier Pietquin1,2
Abstract
Intelligent Tutoring Systems (ITSs) are
now recognised as an interesting alter-
native for providing learning opportuni-
ties in various domains. The Reinforce-
ment Learning (RL) approach has been
shown reliable for finding efficient teach-
ing strategies. However, similarly to other
human-machine interaction systems such
as spoken dialogue systems, ITSs suffer
from a partial knowledge of the interlocu-
tor?s intentions. In the dialogue case, en-
gineering work can infer a precise state of
the user by taking into account the uncer-
tainty provided by the spoken understand-
ing language module. A model-free ap-
proach based on RL and Echo State New-
torks (ESNs), which retrieves similar in-
formation, is proposed here for tutoring.
1 Introduction
For the last decades, Intelligent Tutoring Sys-
tems (ITSs) have become powerful tools in various
domains such as mathematics (Koedinger et al,
1997), physics (Vanlehn et al, 2005; Litman and
Silliman, 2004; Graesser et al, 2005), computer
sciences (Corbett et al, 1995), reading (Mostow
and Aist, 2001), or foreign languages (Heift and
Schulze, 2007; Amaral and Meurers, 2011). Their
appeal relies on the fact that each student does not
have to follow an average teaching strategy, espe-
cially as the one-to-one tutoring has been proven
the most efficent (Bloom, 1968). The expertise of
a teacher relies on his capacity to advice at the
right time the student to acquire new skills. To
do so, the teacher is able to choose iteratively ped-
agogical activities. From this perspective, teach-
ing is a sequential decision-making problem. To
solve it, the reinforcement learning (Sutton and
Barto, 1998) approach and the Markov Decision
Process (MDP) paradigm have been successfully
used (Iglesias et al, 2009). Given a situation, each
teacher?s decision is locally quantified by a re-
ward. However, the consequences of the teacher?s
actions on the student?s cognition cannot be ex-
actly determined, which introduce uncertainty.
To find a solution, one can notice that spoken
dialogue management and tutoring are closely re-
lated. Both are humain-computer interactions in
which the human user?s intentions are not per-
fectly known. In the spoken dialogue case, the
partial observability is due to the recognition er-
rors introduced by the speech understanding mod-
ule. They are taken into account by using some
hypotheses about how the language is constructed.
Thus, accurate models to link observations from
the user?s recognised utterances to the underlying
intentions can be set up. For example, the Hidden
Information State paradigm (Young et al, 2006;
Young et al, 2010) builds a state which is a sum-
mary of the dialogue history (Gas?ic? et al, 2010;
Daubigney et al, 2011; Daubigney et al, 2012).
However, in the ITS case, such a state is harder to
develop since the cognition cannot be determined
by analysing a physical signal. Thus, a model-free
approach is preferred here.
To do so, a memory of the past observations
and actions is built by means of a Recurrent Neu-
ral Network (RNN) and more precisely an Echo
State Network (ESN) (Jaeger, 2001). The inter-
nal state of the network can be shown (under some
resonable conditions) to meet the Markov prop-
erty (Szita et al, 2006). This internal state is then
used with a standard RL algorithm to estimate the
optimal solution. It has already been applied to RL
in (Szita et al, 2006) in limited toy applications
and it is, to our knowledge, the first attempt to use
it in an interaction framework. The proof of con-
cept presented in Szita?s article uses the common
SARSA algorithm which is an on-line and on-
policy algorithm. Each improvement of the strat-
102
egy is directly tested. In the case of teaching, test-
ing poor decisions can be problematic. Here, we
thus propose the combination of an ESN with an
off-line and off-policy algorithm, namely the Least
Square Policy Algorithm (LSPI) (Lagoudakis and
Parr, 2003), which is another original contribu-
tion of this paper. Indeed, learning the solution
with Partially Observable MDPs in a batch and
off-policy manner is not common in the literature.
2 Markov Decision Process and
Reinforcement Learning
Formally, an MDP is a tuple {S,A, T,R, ?} set
up to describe the tutor environment. The set
S is the state space which represents the infor-
mation about the student, A is the action space
which contains the tutor?s actions, T is a set of
transition probabilities defined such that T =
{p(s?|s, a),?(s?, s, a) ? S ? S ? A}, R is the
reward function, given according to the student
progression for example, and ? ? [0, 1] is the
discount factor which weights the future rewards.
The set of transitions probabilities in the ITS case
is unknown: the evolution of the student intentions
cannot be determined. Solving the MPD consists
in finding the optimal strategy, called the optimal
policy which brings the highest expected cumula-
tive reward.
However, in the ITS case, information about the
student?s knowledge, represented by s, can only
be known through observations. Let O = {oi} be
the set of possible observations. Yet, if only ob-
servations are available, a memory of what hap-
pened during previous interactions (the history)
is necessary, because the process of observations
does not meet the Markov property. The his-
tory is the sequence of observation-action pairs
encountered during a whole teaching phase. Let
H = {hi} be the set of all possible histories with
hi = {o0, a0, o1, a1, ..., oi?1, ai?1, oi}.
When the POMDP framework is used, the un-
derlying state si is inferred from the history by
means of a model of probabilities linking si to
hi. In the case of human-machine interactions, this
model is not available. It can be approximated but
the considered solutions are ad-hoc to a particular
problem, thus difficult to reuse. Here, we propose
an approach with as few assumptions as possible
about the student cognitive model by using Echo
States Networks (ESNs). This approach builds a
compact representation of the history space H .
u0
u1
u2
Input
x0
x1
x2
x3
x4 x5
x6
x7 x8
x9
Reservoir
y0
y1
y2
y3
Output
1
Figure 1: RNN structure (for sake of readability,
all the connections do not appear).
3 Echo State Networks
An Echo State Network is represented by three
layers of neurons (Fig. 1): an input, a hidden and
an output. The number of neurons in the hidden
layer is supposed to be large and each of them
can be connected to itself. These recurrent con-
nections are responsible for reusing the value of
the neurons at a previous time step. Consequently,
a memory is built in the reservoir and trajectories
can be encoded. Only the connections from the
hidden layer to the output one are learnt since all
the other connections are randomly and sparsely
set. The recurrent connections are defined so that
the echo state property is met (Jaeger, 2001): if
after a given number of updates of the input neu-
rons, two internal states are exactly the same, then
the input sequences which led to these two internal
states are identical.
The connections of the ESN are presented
in Fig. 2, with uk ? RNi , xk ? RNh and
yk ? RNo , respectively representing the values
of the input, hidden and output layers, Ni, Nh
and No being the respective number of neurons
and W in ? MNh?Ni , W hid ? MNh?Nh and
W out ? MNo?Nh , matrix containing the synap-
tic weights. After a training, the output yk returns
a linear approximation of the internal state of the
reservoir. This output depends on the sequence of
inputs u0, ? ? ? , uk and not only uk, through xk.
Combining ESNs and RL is of interest. By
means of the echo state property, a summary of
the observations and decisions encountered during
the tutoring phase is provided through the internal
state x. In (Szita et al, 2006), it has been proven
to meet the Markov property with high probabil-
ity. It thus can be used as a state for standard
RL algorithms. Here, more precisely, it represents
the basis function of an approximation of the Q-
103
Input
u(k)
[?]
MNi?1
Hidden
x(k)?
???
?
?
?
...
?
?
???
MNh?1
Output
y(k)
[?]
MNo?1
Whid
MNh?Nh
W in
MNh?Ni
W out
MNo?Nh
Input
u(k)
[?]
MNi?1
Hidden
x(k)?
???
?
?
?
...
?
?
???
MNh?1
Projection
x?(k)??
?
?
?
Mm?1
Output
y(k)
[?]
MNo?1
Whid
MNh?Nh
W in
MNh?Ni
?
Mm?Nh
W out
MNo?m
Figure 2: Structure of an ESN. For the example,
Ni = 1 and No = 1.
function. This function is associated with a policy
pi, defined for each couple (s, a) ? S ? A such
that Qpi(s, a) = E [?i ?iri|s0 = s, a0 = a
] and
quantifies the policy. ESNs are used in the fol-
lowing way to solve RL problems. The network
is responsible for giving, from an observations ok
and an action ak at time step k, a linear estimation
of the value of the Q-function Q??(hk, ak) (with
hk = {o0, a0, ..., ok?1, ak?1, ok}). The state s is
not used in the estimation of the Q-function since
it is unknown. Instead, it is replaced by the history
hk. The input of the ESN, uk, is thus the con-
catenation of the observation ok and the action ak:
uk = (ok, ak). The internal state xk which com-
ponent are in [?1, 1], is a summary of the history
hk and the action ak. Thus, the estimation of the
Q-function is Q??(hk, ak) = ?>xk. The values of
the output connections are learnt by means of the
LSPI algorithm. With this algorithm, the optimal
policy is learnt from a fixed set of data.
4 Experimental settings
For the experiments, we assume that the teaching
can be done by means of three actions. First, a les-
son can be presented to make the knowledge of the
student increase. The second and third actions are
evaluations. They can either be a simple question
or a final exam. The final exam consists in ask-
ing a hundred yes/no questions of equal complex-
ity and on the same topic. The student does not
have a feedback. Once it is proposed, a new teach-
ing episode starts. Three observations are returned
to the ITS. If a lesson is proposed to the user, the
observation is neutral: no feedback comes from
the student since the direct influence of the lesson
remains unknown. The two other obervations ap-
pear when a question is asked (yes or no). Conse-
quently, one observation is not enough to choose
the next action since no clue is given about how
many lessons have led to this result. A non-null re-
ward is only given when a final exam is proposed.
In this case, it is proportional to the rate of cor-
rect answers among all the answers given during
the exam. Thus, each improvement is taken into
account. The ? factor is set to 0.97.
In this proof of concept, the results have been
obtained with simulated students from (Chang et
al., 2006) to ensure the reproducibility of the ex-
periments. The simulation implements two abili-
ties: answering a question and learning with a les-
son. Three groups of students have been set up.
The first one, T1, is supposed to be able to learn
very efficiently, the second, T2, needs a few more
lessons to provide good answers, and the third, T3,
needs a lot of lessons to answer correctly.
5 Results
Several teaching strategies have been compared.
As a lower bound baseline, a random strategy has
been tested. With a probability (w.p.) of 0.6, a les-
son is proposed, w.p. of 0.2 a question is chosen,
and w.p. of 0.2 a final exam is proposed. The data
generated with this random strategy have been
used by the LSPI algorithm and an informed state
space. The second baseline proposed is the reac-
tive policy learnt by LSPI (called reactive-LSPI),
only from obervations. Neither the information
about the number of lessons proposed nor the in-
ternal state of the ESN is used. The third strategy
is learnt by using the observations and a counter
of lessons already given (called informed-LSPI).
Thus, this state supposedly contains sufficient in-
formation to take the decision. For this case, since
the numbers of observations and lessons are dis-
crete thus countable, a tabular representation is
chosen for the Q-function. The fourth strategy
uses the internal state of the ESN as basis function
for the Q-function (called ESN-LSPI). There are
50 hidden neurons. Different sizes of training data
sets are tested. Among the data, the three types of
students are represented in equal proportions. One
hundred policies are learnt for each of the methods
presented, except for the ESN-LSPI. For this one,
10 ESNs are generated and 10 training sessions are
performed with each one of them. The mean over
the average results of each of the 10 learnings is
presented in the results. Each of the policies have
been tested 1000 times.
Fig. 3 shows a comparison of the learnt strate-
gies. The three types of students are used for
the training and test phases. One can notice that
104
 0.3
 0.4
 0.5
 0.6
 0.7
 0  2000  4000  6000  8000  10000
A
v
e
r
a
g
e
 
s
u
m
 
o
f
 
d
i
s
c
o
u
n
t
e
d
 
r
e
w
a
r
d
s
Number of transitions
RandomReactive-LSPIInformed-LSPIESN-LSPI
Figure 3: Comparison of the different strategies.
the standard deviation is larger when the ESN are
used because uncertainty is added when generat-
ing the ESN since the connections are randomly
set. The random and the reactive policies give
the poorest results. Yet, the average reward in-
creases because of the data in the training set. For
small sets, long sequences of lessons only have not
been encountered. Thus, larger rewards have not
been encountered either. For the two other curves,
with a reasonable number of interactions (around
8000), a good strategy is learnt by using informed-
LSPI. The strategies learnt with the ESN require
fewer transitions and allow a faster learning. In
this case, the optimum is reached with 2000 transi-
tions while 8000 ones are needed to reach the same
quality with the informed-LSPI strategy. Around
10000 samples, both policies give the same re-
sults. However, less information is given in the
ESN approach (only observations). Thus, this ap-
proach is more generic. The counter information
may not be sufficient for more complex problems.
To compare the efficiency of the learnt policies,
the informed-LSPI and ESN-LSPI are plotted for
each group of students in Fig. 4. All the strate-
gies are learnt with the same data sets than pre-
viously, but only one type of students is tested at
a time. For the T2 and T3 types, the average re-
sults are better with ESN-LSPI (especially for the
T3 type). For the T1 group, informed-LSPI re-
turns slighlty better results. A better insight of
the behaviour of each policy is given in Fig. 5 by
plotting the distribution of the actions used dur-
ing the test phase. A comparison reveals that the
number of lessons is higher in the ESN-LSPI case
(around 3) whereas only one lesson is given in av-
erage with informed-LSPI. This is of benefit to
students of the third group and thus implicitly to
those of the first and second groups. The number
of lessons is even larger for the third group than for
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0  2000  4000  6000  8000  10000
A
v
e
r
a
g
e
 
s
u
m
 
o
f
 
d
i
s
c
o
u
n
t
e
d
 
r
e
w
a
r
d
s
Number of transitions
Informed-LSPI (StudentT1)Informed-LSPI (StudentT2)Informed-LSPI (StudentT3)ESN-LSPI (StudentT1)ESN-LSPI (StudentT2)ESN-LSPI (StudentT3)
Figure 4: Results of the learnt policies for each
group of students.
0
1
2
3
4
Lesson Question FinalExam
A
v
g
.
 
n
u
m
b
e
r
 
o
f
 
a
c
t
i
o
n
s
Actions proposed with Informed LSPI
StudentT1StudentT2StudentT3
0
1
2
3
4
Lesson Question FinalExam
A
v
g
.
 
n
u
m
b
e
r
 
o
f
 
a
c
t
i
o
n
s
Actions proposed with ESN LSPI
StudentT1StudentT2StudentT3
Figure 5: Distribution of the actions (the size of
the training dataset is 10000).
the two others (0.5 more in average). However, in
the informed-LSPI case, the learnt policy is only
profitable for those of the first group, who are al-
ready skilled (this conclusion is consistent with the
Fig. 4). Questions are very rarely asked because
once the number of lessons has been learnt, they
bring no more information.
6 Conclusion
We proposed a model-free approach which uses
only observations to find optimal teaching state-
gies. A summary of the history encountered is
implemented by means of an ESN. This summary
has been proven to be Markovian by (Szita et al,
2006). A standard RL algorithm which can learn
from already collected data, is then used to per-
form the learning. Preliminary experiments have
been presented on simulated data. In future works,
we plan to apply this method to SDSs.
Acknowledgments
Results have been computed with the InterCell
cluster funded by the Re?gion Lorraine.
105
References
L. Amaral and D. Meurers. 2011. On using intelli-
gent computer-assisted language learning in real-life
foreign language teaching and learning. ReCALL,
23(1):4?24.
B. Bloom. 1968. Learning for mastery. Evaluation
comment, 1(2):1?5.
K. Chang, J. Beck, J. Mostow, and A. Corbett. 2006. A
bayes net toolkit for student modeling in intelligent
tutoring systems. In Intelligent Tutoring Systems,
pages 104?113. Springer.
A. Corbett, J. Anderson, and A. OBrien. 1995. Student
modeling in the act programming tutor. Cognitively
diagnostic assessment, pages 19?41.
L. Daubigney, M. Gas?ic?, S. Chandramohan, M. Geist,
O. Pietquin, and S. Young. 2011. Uncertainty
management for on-line optimisation of a POMDP-
based large-scale spoken dialogue system. In Pro-
ceedings of Interspeech?11.
L. Daubigney, M. Geist, S. Chandramohan, and
O. Pietquin. 2012. A Comprehensive Reinforce-
ment Learning Framework for Dialogue Manage-
ment Optimisation. IEEE Journal of Selected Topics
in Signal Processing, 6(8):891?902.
M. Gas?ic?, F. Jurc???c?ek, S. Keizer, F. Mairesse, B. Thom-
son, K. Yu, and S. Young. 2010. Gaussian pro-
cesses for fast policy optimisation of POMDP-based
dialogue managers. In Proceedings of SIGdial?10.
A. Graesser, P. Chipman, B. Haynes, and A. Olney.
2005. Autotutor: An intelligent tutoring system
with mixed-initiative dialogue. Education, IEEE
Transactions on, 48(4):612?618.
T. Heift and M. Schulze. 2007. Errors and intelligence
in computer-assisted language learning: Parsers
and pedagogues, volume 2. Psychology Press.
Ana Iglesias, Paloma Mart??nez, Ricardo Aler, and Fer-
nando Ferna?ndez. 2009. Learning teaching strate-
gies in an adaptive and intelligent educational sys-
tem through reinforcement learning. Applied Intel-
ligence, 31(1):89?106.
H. Jaeger. 2001. The ?echo state? approach to
analysing and training recurrent neural networks.
Technical report, Technical Report GMD Report
148, German National Research Center for Informa-
tion Technology.
K. Koedinger, J. Anderson, W. Hadley, M. Mark, et al
1997. Intelligent tutoring goes to school in the big
city. International Journal of Artificial Intelligence
in Education (IJAIED), 8:30?43.
M. Lagoudakis and R. Parr. 2003. Least-squares pol-
icy iteration. The Journal of Machine Learning Re-
search, 4:1107?1149.
D. Litman and S. Silliman. 2004. Itspoke: An intel-
ligent tutoring spoken dialogue system. In Demon-
stration Papers at HLT-NAACL 2004, pages 5?8. As-
sociation for Computational Linguistics.
J. Mostow and G. Aist. 2001. Evaluating tutors that
listen: an overview of project listen. In Smart ma-
chines in education, pages 169?234. MIT Press.
R. Sutton and A. Barto. 1998. Reinforcement learning:
An introduction. The MIT press.
I. Szita, V. Gyenes, and A. Lo?rincz. 2006. Reinforce-
ment learning with echo state networks. Artificial
Neural Networks?ICANN 2006, pages 830?839.
K. Vanlehn, C. Lynch, K. Schulze, J. Shapiro,
R. Shelby, L. Taylor, D. Treacy, A. Weinstein, and
M. Wintersgill. 2005. The andes physics tutoring
system: Lessons learned. International Journal of
Artificial Intelligence in Education, 15(3):147?204.
S. Young, J. Schatzmann, B. Thomson, H. Ye, and
K. Weilhammer. 2006. The HIS dialogue manager.
In Proceedings of IEEE/ACL Workshop on Spoken
Language Technology (SLT?06).
S. Young, M. Gasic, S. Keizer, F. Mairesse, J. Schatz-
mann, B. Thomson, and K. Yu. 2010. The hid-
den information state model: A practical frame-
work for POMDP-based spoken dialogue manage-
ment. Computer Speech & Language, 24(2):150?
174.
106
