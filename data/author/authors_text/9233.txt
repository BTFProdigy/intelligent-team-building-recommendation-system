Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 1073?1080
Manchester, August 2008
Automatic Seed Word Selection for Unsupervised Sentiment 
Classification of Chinese Text
Taras Zagibalov    John Carroll
University of Sussex
Department of Informatics
Brighton  BN1 9QH, UK
{T.Zagibalov,J.A.Carroll}@sussex.ac.uk
Abstract 
We describe and evaluate a new method 
of automatic seed word selection for un-
supervised  sentiment  classification  of 
product  reviews  in  Chinese.  The  whole 
method is unsupervised and does not re-
quire any annotated training data; it only 
requires information about commonly oc-
curring negations  and adverbials.  Unsu-
pervised  techniques  are  promising  for 
this task since they avoid problems of do-
main-dependency  typically  associated 
with supervised methods. The results ob-
tained  are  close  to  those  of  supervised 
classifiers and sometimes better, up to an 
F1 of 92%.
1 Introduction
Automatic classification of document  sentiment 
(and more generally extraction of opinion from 
text) has recently attracted a lot of interest. One 
of the main reasons for this is the importance of 
such  information  to  companies,  other 
organizations,  and  individuals.  Applications 
include  marketing  research  tools  that  help  a 
company see market or media reaction towards 
their  brands,  products  or  services,  or  search 
engines  that  help potential  purchasers  make  an 
informed choice of a product they want to buy. 
Sentiment  classification  research  has  drawn on 
and contributed to research in text classification, 
unsupervised  machine  learning,  and  cross-
domain adaptation.
This paper presents a new, automatic approach 
to automatic seed word selection as part of senti-
ment classification of product reviews written in 
Chinese,  which  addresses  the  problem  of  do-
 ? 2008. Licensed under the Creative Commons Attribu-
tion-Noncommercial-Share Alike 3.0 Unported license 
(http://creativecommons.org/licenses/by-nc-sa/3.0/). Some 
rights reserved.
main-dependency of sentiment classification that 
has been observed in previous work. It may also 
facilitate  building  sentiment  classification  sys-
tems in other languages since the approach as-
sumes a very small amount of linguistic knowl-
edge: the only language-specific information re-
quired is a basic description of the most frequent 
negated adverbial constructions in the language.
The paper is structured as follows. Section 2 
surveys related work in sentiment classification, 
unsupervised  machine  learning  and  Chinese 
language  processing.  Section  3  motivates  our 
approach,  which  is  described  in  detail  in 
Section 4.  The  data  used  for  experiments  and 
baselines,  as well  as the results of  experiments 
are covered in Section 5. Section 6 discusses the 
lessons learned and proposes directions for future 
work.
2 Related Work
2.1 Sentiment Classification
Most work on sentiment classification has used 
approaches based on supervised machine learn-
ing.  For  example,  Pang  et  al. (2002)  collected 
movie reviews that had been annotated with re-
spect to sentiment by the authors of the reviews, 
and used this data to train supervised classifiers. 
A number of studies have investigated the impact 
on classification accuracy of different factors, in-
cluding choice of  feature set,  machine learning 
algorithm,  and pre-selection of the segments of 
text  to  be  classified.  For  example,  Dave  et  al. 
(2003) experiment with the use of linguistic, sta-
tistical and n-gram features and measures for fea-
ture  selection  and  weighting.  Pang  and  Lee 
(2004)  use  a  graph-based technique to  identify 
and  analyze  only subjective  parts  of  texts.  Yu 
and  Hatzivassiloglou  (2003)  use  semantically-
oriented  words  for  identification  of  polarity  at 
the sentence level. Most of this work assumes bi-
nary classification (positive and negative), some-
1073
times  with  the  addition  of  a  neutral  class  (in 
terms of polarity, representing lack of sentiment).
While  supervised  systems  generally  achieve 
reasonably high accuracy, they do so only on test 
data that is similar to the training data. To move 
to another domain one would have to collect an-
notated data in the new domain and retrain the 
classifier. Engstr?m (2004) reports decreased ac-
curacy in cross-domain classification since senti-
ment in different domains is often expressed in 
different ways. However, it is impossible in prac-
tice to have annotated data for all  possible do-
mains  of  interest.  Aue  and  Gamon  (2005)  at-
tempt  to  solve  the  problem of  the  absence  of 
large  amounts  of  labeled  data  by  customizing 
sentiment classifiers to new domains using train-
ing data from other domains. Blitzer et al (2007) 
investigate domain adaptation for sentiment clas-
sifiers using structural correspondence learning. 
Read  (2005)  also  observed  significant  differ-
ences between the accuracy of classification of 
reviews in the same domain but published in dif-
ferent time periods.
Recently, there has been a shift of interest to-
wards more fine-grained approaches to process-
ing of sentiment, in which opinion is extracted at 
the sentence level, sometimes including informa-
tion about different features of a product that are 
commented on and/or the opinion holder (Hu and 
Liu, 2004; Ku et al, 2006). But even in such ap-
proaches, McDonald et al (2007) note that infor-
mation about the overall sentiment orientation of 
a document  facilitates more  accurate extraction 
of more specific information from the text.
2.2 Unsupervised Approach
One way of tackling the problem of domain de-
pendency could be to use an approach that does 
not  rely  on  annotated  data.  Turney  (2002)  de-
scribes a method of sentiment classification  us-
ing two human-selected seed words (the words 
poor and  excellent)  in conjunction with a very 
large  text  corpus;  the  semantic  orientation  of 
phrases is computed as their association with the 
seed words (as measured by pointwise mutual in-
formation). The sentiment of a document is cal-
culated as the average semantic orientation of all 
such phrases.
Yarowsky  (1995)  describes  a  'semi-unsuper-
vised' approach to the problem of sense disam-
biguation  of  words,  also  using  a  set  of  initial 
seeds, in this case a few high quality sense anno-
tations. These annotations are used to start an it-
erative process of learning information about the 
contexts  in  which  senses  of  words  appear,  in 
each iteration labeling senses of previously unla-
beled  word  tokens  using  information  from the 
previous iteration.
2.3 Chinese Language Processing
A major issue in processing Chinese text is the 
fact that words are not delimited in the written 
language. In many cases, NLP researchers work-
ing  with  Chinese  use  an  initial  segmentation 
module  that  is  intended  to  break  a  text  into 
words.  Although  this  can  facilitate  the  use  of 
subsequent computational techniques, there is no 
a clear definition of what a 'word' is in the mod-
ern Chinese  language,  so the  use  of  such  seg-
menters is of dubious theoretical status; indeed, 
good  results  have  been  reported  from systems 
which do not assume such pre-processing (Foo 
and Li, 2004; Xu et al, 2004). 
2.4 Seed Word Selection
We are not aware of any sentiment analysis sys-
tem that uses unsupervised seed word selection. 
However, Pang et al (2002) showed that it is dif-
ficult  to  get  good coverage of  a  target  domain 
from manually selected words, and even simple 
corpus  frequency counts  may  produce  a  better 
list of features for supervised classification: hu-
man-created lists resulted in 64% accuracy on a 
movie  review  corpus,  while  a  list  of  frequent 
words scored 69%. Pang et al also observed that 
some  words  without  any  significant  emotional 
orientation were quite good features: for exam-
ple, the word ?still? turned out to be a good indi-
cator of positive reviews as it was often used in 
sentences  such  as  ?Still,  though,  it  was  worth 
seeing''.
3 Our Approach
Our main goal is to overcome the problem of do-
main-dependency  in  sentiment  classification. 
Unsupervised approaches seem promising in this 
regard, since they do not require annotated train-
ing data, just access to sufficient raw text in each 
domain. We base our approach on a previously 
described,  'almost-unsupervised'  system  that 
starts with only a single, human-selected seed ? 
(good) and uses an iterative method to extract a 
training sub-corpus (Zagibalov & Carroll, 2008). 
The approach does not use a word segmentation 
module;  in  this  paper  we use  the  term 'lexical 
item' to denote any sequence of Chinese charac-
ters that is treated by the system as a unit, what-
ever it is linguistically ? a morpheme, a word or 
a phrase.
1074
Our initial aim was to investigate ways of im-
proving the classifier by automatically finding a 
better seed, because Zagibalov & Carroll indicate 
that in different domains they could, by manual 
trial and error, find a seed other than  ? (good) 
which produced better results.
To find such a seed automatically,  we make 
two assumptions:
1. Attitude  is  often  expressed  through  the 
negation of vocabulary items with the op-
posite meaning; for example in Chinese it 
is  more  common  to  say  not  good than 
bad  (Tan,  2002). Zagibalov  &  Carroll's 
system uses this observation to find nega-
tive lexical items while nevertheless start-
ing only from a positive seed. This leads 
us  to  believe  that  it  is  possible  to  find 
candidate  seeds  themselves  by  looking 
for  sequences  of  characters  which  are 
used with negation. 
2. The polarity of a candidate seed needs to 
be determined. To do this we assume we 
can use the lexical item   ? (good) as a 
gold  standard  for  positive  lexical  items 
and  compare  the  pattern  of  contexts  a 
candidate seed occurs in to the pattern ex-
hibited by the gold standard.
Looking at product review corpora, we observed 
that  good is  always  more  often  used  without 
negation in positive texts, while in negative texts 
it  is  more  often  used  with  negation  (e.g.  not  
good). Also,  good occurs more often in positive 
texts than negative, and more frequently without 
negation than with it. We use the latter observa-
tion  as  the  basis  for  identifying  seed  lexical 
items,  finding those which occur with negation 
but more frequently occur without it.
As well as detecting negation1 we also use ad-
verbials2 to  avoid  hypothesizing  non-contentful 
seeds: the characters following the sequence of a 
negation and an adverbial are in general content-
ful units, as opposed to parts of words, function 
words, etc. In what follows we refer to such con-
structions as negated adverbial constructions.
1We use only six frequently occurring negations: ? (bu), ?
? (buhui), ?? (meiyou), ?? (baituo), ?? (mianqu), 
and ?? (bimian). We are trying to be as language-inde-
pendent as possible so we take a simplistic approach to de-
tecting negation.
2We use five frequently occurring adverbials: ? (hen), ?? 
(feichang), ? (tai), ? (zui), and ?? (bijiao). Similarly to 
negation, we deliberately take a simplistic approach.
4 Method 
We use a similar  sentiment  classifier and itera-
tive retraining technique to the almost-unsuper-
vised  system  of  Zagibalov  &  Carroll  (2008), 
summarized below in Sections 4.2 and 4.3. The 
main  new contributions  of  this  paper  are  tech-
niques for automatically finding the seeds from 
raw text in a particular domain (Section 4.1), and 
for detecting when the process should stop (Sec-
tion 4.4). This new system therefore differs from 
that of Zagibalov & Carroll (2008) in being com-
pletely unsupervised and not depending on arbi-
trary iteration limits. (The evaluation also differs 
since we focus in this paper on the effects of do-
main on sentiment classification accuracy).
4.1 Seed Lexical Item Identification
The first step is to identify suitable positive seeds 
for  the  given  corpus.  The  intuition  behind  the 
way this is done is outlined above in Section 3. 
The algorithm is as follows:
1. find all sequences of characters between 
non-character  symbols  (i.e.  punctuation 
marks,  digits  and  so  on)  that  contain 
negation  and  an  adverbial,  split  the  se-
quence at the negation, and store the char-
acter  sequence  that  follows  the  negated 
adverbial construction;
2. count the number of occurrences of each 
distinct  sequence that  follows a negated 
adverbial construction (X);
3. count the number of occurrences of each 
distinct sequence without the construction 
(Y);
4. find all sequences with Y ? X > 0.
4.2 Sentiment Classification
This  approach  to  Chinese  language  processing 
does not use pre-segmentation (in the sense dis-
cussed in Section 2.3) or grammatical analysis: 
the basic unit of processing is the 'lexical item', 
each of which is a sequence of one or more Chi-
nese characters excluding punctuation marks (so 
a lexical item may actually form part of a word, a 
whole word or a sequence of words), and 'zones', 
each of which is a sequence of characters delim-
ited by punctuation marks.
Each  zone  is  classified  as  either  positive  or 
negative based whether positive or negative vo-
cabulary  items  predominate.  As  there  are  two 
parts of the vocabulary (positive and negative), 
we  correspondingly  calculate  two  scores  (Si , 
1075
where  i is  either  positive or  negative)  using 
Equation (1), where Ld is the length in characters 
of a matching lexical item (raised to the power of 
two to increase the significance of longer items 
which capture more context),  Lphrase is the length 
of the current zone in characters, Sd is the current 
sentiment score of the matching lexical item (ini-
tially 1.0), and Nd is a negation check coefficient.
 
S i= Ld
2
L phrase S d N d
                   (1)
The negation check is a regular expression which 
determines  if  the lexical  item is  preceded by a 
negation within its enclosing zone. If a negation 
is found then Nd is set to ?1.
The sentiment  score of a zone is the sum of 
sentiment of all the items found in it.
To determine the sentiment orientation of the 
whole document, the classifier computes the dif-
ference between the number of positive and neg-
ative zones.  If the result is greater than zero the 
document is classified as positive, and vice ver-
sa.
4.3 Iterative Retraining
Iterative retraining is used to enlarge the initial 
seed  vocabulary into  a  comprehensive  vocabu-
lary  list  of  sentiment-bearing  lexical  items.  In 
each iteration, the current version of the classifier 
is run on the input corpus to classify each docu-
ment,  resulting in a training subcorpus of posi-
tive and a negative documents. The subcorpus is 
used to adjust the scores of existing positive and 
negative vocabulary items and to find new items 
to be included in the vocabulary. 
Each lexical item that occurs at least twice in 
the corpus is a candidate for inclusion in the vo-
cabulary list. After candidate items are found, the 
system  calculates  their  relative  frequencies  in 
both the positive and negative parts of the current 
training subcorpus.  The system also checks for 
negation while counting occurrences: if a lexical 
item is preceded by a negation, its count is re-
duced by one. 
For all candidate items we compare their rela-
tive frequencies in the positive and negative doc-
uments in the subcorpus using Equation (2).
difference= ?F p? F n??F p?Fn?/2
        (2)
If difference < 1, then the frequencies are similar 
and the item does not have enough distinguishing 
power,  so it  is  not  included in  the vocabulary. 
Otherwise  the  sentiment  score  of  the  item  is 
(re-)calculated  ?  according  to  Equation  (3)  for 
positive  items,  and  analogously  for  negative 
items.
F p?Fn         (3)
Finally, the adjusted vocabulary list with the new 
scores is ready for the next iteration3.
4.4 Iteration Control
To maximize the number of productive iterations 
while avoiding unnecessary processing and arbi-
trary  iteration  limits,  iterative  retraining  is 
stopped when there is no change to the classifica-
tion of any document over the previous two itera-
tions.
5 Experiments
5.1 Data
As our approach is unsupervised, we do not use 
an annotated training corpus, but run our iterative 
procedure on the raw data extracted from an an-
notated test corpus, and evaluate the final accura-
cy of the system with respect to the annotations 
in that corpus.
Our  test  corpus  is  derived  from product  re-
views harvested from the website IT1684. All the 
reviews  were  tagged by their  authors  as  either 
positive or negative overall.  Most reviews con-
sist of two or three distinct parts: positive opin-
ions, negative opinions, and comments ('other') ? 
although some reviews have only one part.  We 
removed  duplicate  reviews  automatically  using 
approximate matching, giving a corpus of 29531 
reviews of which 23122 are positive (78%) and 
6409 are  negative  (22%).  The  total  number  of 
different  products  in  the  corpus  is  10631,  the 
number of product categories is 255, and most of 
the reviewed products are either software prod-
ucts  or  consumer  electronics.  Unfortunately,  it 
appears  that  some  users  misuse  the  sentiment 
3An alternative approach might be to use point-wise mutual 
information instead of relative frequencies of newly found 
features in a subcorpus produced in the previous iteration. 
However, in preliminary experiments, SO-PMI did not pro-
duce good corpora from the first iteration. Also, it is not 
clear how to manage subsequent iterations since PMI would 
have to be calculated between thousands of new vocabulary 
items and every newly found sequence of characters, which 
would be computationally intractable.
4http://product.it168.com
1076
tagging facility on the website so quite a lot of 
reviews have incorrect tags. However, the parts 
of the reviews are much more reliably identified 
as being positive or negative so we used these as 
the items of the test corpus. In the experiments 
described below we use 10 subcorpora contain-
ing a total of 7982 reviews, distributed between 
product types as shown in Table 1. 
Corpus/product type Reviews
Monitors 683
Mobile phones 2317
Digital cameras 1705
MP3 players 779
Computer  parts  (CD-drives,  mother-
boards)
308
Video cameras and lenses 361
Networking (routers, network cards) 350
Office equipment (copiers,
multifunction devices, scanners)
611
Printers (laser, inkjet) 569
Computer peripherals (mice, keyboards, 
speakers)
457
Table 1. Product types and sizes of the test 
corpora.
We constructed five of the corpora by combin-
ing smaller ones of 100?250 reviews each (as in-
dicated  in  parentheses  in  Table  1)  in  order  to 
have reasonable amounts of data.
Each corpus has equal numbers of positive and 
negative reviews so we can derive upper bounds 
from the corpora (Section 5.2)  by applying su-
pervised  classifiers.  We  balance  the  corpora 
since (at least on this data) these classifiers per-
form less well with skewed class distributions5.
5.2 Baseline and Upper Bound
Since the  corpora  are  balanced with respect  to 
sentiment  orientation  the  na?ve  (unsupervised) 
baseline  is  50%.  We  also  produced  an  upper 
bound  using  Naive  Bayes  multinomial  (NBm) 
and Support Vector Machine (SVM)6 classifiers 
with the NTU Sentiment  Dictionary (Ku  et al, 
2006)  vocabulary items  as  the  feature  set.  The 
dictionary contains  2809 items  in  the  'positive' 
part  and  8273  items  in  the  'negative'.  We  ran 
5We have made this corpus publicly available at http://
www.informatics.sussex.ac.uk/users/tz21/coling08.zip
6We used WEKA 3.4.11 (http://www.cs.waikato.ac.nz/?ml/
weka )
both classifiers in 10-fold stratified cross-valida-
tion mode, resulting in the accuracies shown in 
Table 2. The macroaveraged accuracies across all 
10  corpora  are  82.78%  (NBm)  and  80.89% 
(SVM).
Corpus Nbm 
(%)
SVM 
(%)
Monitors 86.21 83.87
Mobile phones 86.52 84.49
Digital cameras 82.27 82.04
MP3 players 82.64 79.43
Computer parts 81.10 79.47
Video cameras and lenses 83.05 84.16
Networking 77.65 75.35
Office equipment 82.13 80.00
Printers 81.33 79.57
Computer peripherals 84.86 80.48
Table 2. Upper bound accuracies.
 We also tried adding the negations and adver-
bials specified in Section 3 to the feature set, and 
this resulted in slightly improved accuracies, of 
83.90% (Nbm) and 82.49% (SVM).
An alternative  approach would have been to 
automatically segment the reviews and then de-
rive a feature set of a manageable size by setting 
a threshold on word frequencies; however the ex-
tra processing means that this is a less valid up-
per bound.
Another possible comparison could be with a 
version of Turney's (2002) sentiment  classifica-
tion method applied to Chinese. However, the re-
sults  would  not  be  comparable  since  Turney's 
method would require the additional use of very 
large  text  corpus  and  the  manual  selection  of 
positive and negative seed words. 
5.3 Experiment 1
To be able to compare to the accuracy of the al-
most-unsupervised  approach  of  Zagibalov  & 
Carroll (2008), we ran our system using the seed 
 ? (good) for each corpus. The results are shown 
in Table 3. We compute precision, recall and F1 
measure rather than just accuracy, since our clas-
sifier can omit some reviews whereas the super-
vised classifiers attempt to classify all  reviews. 
The macroaveraged F1 measure is 80.55, which 
beats the na?ve baseline by over 30 percentage 
points, and approaches the two upper bounds.
1077
Corpus Iter P R F1
Monitors 12 86.62 86.24 86.43
Mobile phones 11 90.15 89.68 89.91
Digital cameras 13 81.33 80.23 80.78
MP3 players 13 86.10 85.10 85.60
Computer parts 10 69.10 67.53 68.31
Video cameras and 
lenses
10 82.81 81.44 82.12
Networking 11 69.28 68.29 68.78
Office equipment 12 81.83 80.36 81.09
Printers 12 81.04 79.61 80.32
Computer peripherals 10 82.20 81.84 82.02
Macroaverage 81.05 80.03 80.54
Table 3. Results with the single, manually
chosen seed ? (good) for each corpus.
5.4 Experiment 2
We then ran our full system, including the seed 
identifier. Appendix A shows that for most of the 
corpora the algorithm found different (highly do-
main-salient)  seeds.  Table  4  shows  the  results 
achieved.
Corpus Iter P R F1
Monitors 11 85.57 85.07 85.32
Mobile phones 10 92.63 92.19 92.41
Digital cameras 13 84.92 83.58 84.24
MP3 players 13 88.69 87.55 88.11
Computer parts 12 77.78 77.27 77.52
Video cameras and 
lenses
11 83.62 81.99 82.8
Networking 13 72.83 72.00 72.41
Office equipment 10 82.42 81.34 81.88
Printers 12 81.04 79.61 80.32
Computer peripherals 10 82.24 82.06 82.15
Macroaverage 83.17 82.27 82.72
Table 4. Results with the seeds automatically 
identified for each corpus. 
Across all 10 subcorpora, the improvement us-
ing  automatically  identified  seed  words  com-
pared with just using the seed good is significant 
(paired t-test, P<0.0001), and the F1 measure lies 
between the two upper bounds.
6 Conclusions and Future Work
The unsupervised approach to seed words selec-
tion for sentiment classification presented in this 
paper produces results which in most  cases are 
close to the results of supervised classifiers and 
to  the  previous  almost-unsupervised  approach: 
eight  out  of  ten  results  showed  improvement 
over the human selected seed word and three re-
sults  outperformed  the  supervised  approach, 
while three other results were less than 1% infe-
rior to the supervised ones.
How does  it  happen that  the  chosen seed is 
usually (in our  dataset  ? always)  positive?  We 
think that this happens due to the socially accept-
ed norm of behaviour: as a rule one needs to be 
friendly to communicate with others. This in turn 
defines  linguistic  means  of  expressing  ideas  ? 
they will be at least slightly positive overall. The 
higher  prevalence of positive  reviews has  been 
observed previously: for example, in our corpus 
before  we  balanced  it  almost  80% of  reviews 
were  positive;  Pang  et  al.  (2002)  constructed 
their move  review  corpus  from  an  original 
dataset  of  1301  positive  and  752  negative  re-
views (63% positive). Ghose et al (2007) quote 
typical  examples  of  highly  positive  language 
used in the online marketplace.  We can make a 
preliminary conclusion that a relatively high fre-
quency of positive  words  is  determined  by the 
usage of language that reflects the social  beha-
viour of people.
In future work we intend to explore these is-
sues of positivity of language use. We will also 
apply  our  approach  to  other  genres  containing 
some quantity of evaluative language (for exam-
ple newspaper articles), and see if it works equal-
ly well  for  languages  other  than Chinese.  It  is 
also likely we can use a smaller set of negation 
words and adverbials to produce the seed lists.
Acknowledgements
The first author is supported by the Ford Founda-
tion International Fellowships Program.
References
Aue, Anthony, and Michael Gamon. 2005. Customiz-
ing Sentiment Classifiers to New Domains: a Case 
Study. In Proceedings of the International Confer-
ence  RANLP-2005  Recent  Advances  in  Natural  
Language Processing.
1078
Blitzer,  John,  Mark  Dredze,  and  Fernando  Pereira. 
2007.  Biographies,  Bollywood,  Boom-boxes  and 
Blenders: Domain Adaptation for Sentiment Clas-
sification. In Proceedings of the 45th Annual Meet-
ing of  the Association of  Computational Linguis-
tics. 440?447.
Dave,  Kushal,  Steve Lawrence,  and David M. Pen-
nock.  2003.  Mining  the  Peanut  Gallery:  Opinion 
Extraction and Semantic Classification of Product 
Reviews.  In  Proceedings  of  the  Twelfth  Interna-
tional World Wide Web Conference. 519?528.
Engstr?m, Charlotte. 2004. Topic Dependence in Sen-
timent Classification. Unpublished MPhil Disserta-
tion. University of Cambridge.
Foo, Schubert, and Hui Li. 2004. Chinese Word Seg-
mentation and Its Effects on Information Retrieval. 
Information  Processing  and  Management,  40(1). 
161?190.
Ghose, Anindya, Panagiotis Ipeirotis, and Arun Sun-
dararajan. 2007. Opinion Mining using Economet-
rics: A Case Study on Reputation Systems. In Pro-
ceedings of the 45th Annual Meeting of the Associ-
ation of Computational Linguistics. 416?423.
Hu, Minqing, and Bing Liu. 2004. Mining and Sum-
marizing Customer Reviews. In Proceedings of the 
10th ACM SIGKDD International Conference on  
Knowledge Discovery and Data Mining. 168?177.
Ku,  Lun-Wei,  Yu-Ting  Liang,  and  Hsin-Hsi  Chen. 
2006.  Opinion  Extraction,  Summarization  and 
Tracking in News and Blog Corpora. In  Proceed-
ings of the AAAI-2006 Spring Symposium on Com-
putational  Approaches  to  Analyzing  Weblogs. 
AAAI Technical Report.
McDonald, Ryan, Kerry Hannan, Tyler Neylon, Mike 
Wells,  and Jeff Reynar.  2007. Structured  Models 
for  Fine-to-Coarse  Sentiment  Analysis.  In  Pro-
ceedings of the 45th Annual Meeting of the Associ-
ation of Computational Linguistics. 432?439.
Pang, Bo, and Lillian Lee. 2004. A Sentimental Edu-
cation:  Sentiment  Analysis  Using  Subjectivity 
Summarization Based on Minimum Cuts. In  Pro-
ceedings of the 42nd Annual Meeting of the Associ-
ation for Computational Linguistics. 271?278.
Pang,  Bo,  Lillian  Lee,  and  Shivakumar 
Vaithyanathan. 2002. Thumbs up? Sentiment Clas-
sification using Machine Learning Techniques.  In 
Proceedings of the 2002 Conference on Empirical  
Methods in Natural Language Processing. 79?86.
Read,  Jonathon.  2005.  Using  Emoticons  to  Reduce 
Dependency in Machine Learning Techniques for 
Sentiment  Classification.  In  Proceedings  of  the 
ACL Student Research Workshop at ACL-05. 43?
48.
Tan, Aoshuang.  2002.  Problemy skrytoj grammatiki. 
Sintaksis,  semantika  i  pragmatika  jazyka  izoliru-
ju??ego stroja na primere kitajskogo jazyka  [Prob-
lems of a hidden grammar. Syntax, semantics and 
pragmatics of a language of the isolating type, tak-
ing the Chinese language  as an example]. Jazyki  
Slavjanskoj Kultury.
Turney,  Peter  D.  2002.  Thumbs  Up  or  Thumbs 
Down? Semantic Orientation Applied to Unsuper-
vised Classification of Reviews. In Proceedings of  
the  40th  Annual  Meeting  of  the  Association  for  
Computational Linguistics. 417?424.
Xu, Jia, Richard Zens, and Hermann Ney. 2004. Do 
We Need Chinese Word Segmentation for Statisti-
cal  Machine  Translation?  In  Proceedings  of  the 
Third  SIGHAN  Workshop  on  Chinese  Language 
Learning. 122?128.
Yarowsky,  David.  1995.  Unsupervised  Word  Sense 
Disambiguation  Rivaling Supervised  Methods.  In 
Proceedings of the 33rd Annual Meeting of the As-
sociation for Computational Linguistics. 189?196.
Yu, Hong, and Vasileios Hatzivassiloglou. 2003. To-
wards  Answering  Opinion  Questions:  Separating 
Facts from Opinions and Identifying the Polarity of 
Opinion  Sentences.  In  Proceedings  of  the  2003 
Conference on Empirical Methods in Natural Lan-
guage Processing. 129?136.
Zagibalov,  Taras,  and John Carroll.  2008. Unsuper-
vised  Classification of  Sentiment  and  Objectivity 
in Chinese Text. In Proceedings of the Third Inter-
national  Joint  Conference  on  Natural  Language 
Processing. 304?311.
1079
Appendix A. Seeds Automatically Identified for each Corpus.
Corpus Seed Corpus Seed
Monitors ? (good)
? (convenient; cheap)
?? (clear)
? (straight)
?? (comfortable)
? (fill, fulfill)
?? (sharp)
?? (comfortable)
? (cool)
Video 
cameras 
and lenses
?? (clear ?  of  sound  or  image)
?? (comfortable)
?? (practical)
?? (perfect)
? (cool)
Mobile 
phones
? (good)
?? (support)
? (convenient; cheap)
?? (comfortable)
?? (clear ?of sound or image)
? (sufficient)
?? (easy to use)
?? (comfortable)
??? (user friendly)
?? (smooth and easy)
?? (distinct) 
? (cool)
?? (has become better)
?? (durable)
??? (comfortable)
??? (satisfied)
?? (fit, suit)
??? (has become comfortable)
?? (applicable)
?? (handy)
?? (science, scientific)
Digital 
cameras 
? (good)
? (convenient; cheap)
?? (comfortable)
?? (clear?of sound or image)
?? (special)
? (cool)
?? (satisfied)
?? (durable)
?? (comfortable)
?? (perfect)
?? (straight)
?? (stable)
??? (has become comfortable)
?? (polite)
?? (detailed)
Networking ?? (stable) Printers ? (good)
MP3 players ? (good)
? (convenient; cheap)
?? (comfortable)
?? (practical)
?? (sensitive)
?? (comfortable)
? (cool)
??? (has become comfortable)
Computer 
peripherals 
? (good)
? (convenient;cheap)
?? (comfortable)
? (precise)
?? (comfortable)
?? (habitual)
?? (smooth and easy)
?? (stable)
Computer 
parts
? (good)
?? (stable)
Office 
equipment
? (good)
?? (comfortable)
?? (stable)
?? (practical)
1080
Unsupervised Classification of Sentiment and Objectivity
in Chinese Text
TarasZagibalov John Carroll
University of Sussex
Department of Informatics
Brighton BN1 9QH, UK
{T.Zagibalov,J.A.Carroll}@sussex.ac.uk
Abstract
We address the problem of sentiment and
objectivity classification of product re-
views in Chinese. Our approach is distinct-
ive in that it treats both positive / negative
sentiment and subjectivity / objectivity not
as distinct classes but rather as a con-
tinuum; we argue that this is desirable from
the perspective of would-be customers who
read the reviews. We use novel unsuper-
vised techniques, including a one-word
'seed' vocabulary and iterative retraining
for sentiment processing, and a criterion of
'sentiment density' for determining the ex-
tent to which a document is opinionated.
The classifier achieves up to 87% F-meas-
ure for sentiment polarity detection.
1 Introduction
Automatic classification of sentiment has been a
focus of a number of recent research efforts (e.g.
(Turney, 2002; Pang et al, 2002; Dave at al.,
2003). An important potential application of such
work is in business intelligence: brands and com-
pany image are valuable property, so organizations
want to know how they are viewed by the media
(what the 'spin' is on news stories, and editorials),
business analysts (as expressed in stock market re-
ports), customers (for example on product review
sites) and their own employees. Another important
application is to help people find out others' views
about products they have purchased (e.g. consumer
electronics), services and entertainment (e.g.
movies), stocks and shares (from investor bulletin
boards), and so on. In the work reported in this pa-
per we focus on product reviews, with the intended
users of the processing being would-be customers.
Our approach is based on the insight that posi-
tive and negative sentiments are extreme points in
a continuum of sentiment, and that intermediate
points in this continuum are of potential interest.
For instance, in one scenario, someone might want
to get an idea of the types of things people are say-
ing about a particular product through reading a
sample of reviews covering the spectrum from
highly positive, through balanced, to highly nega-
tive. (We call a review balanced if it is an opinion-
ated text with an undecided or weak sentiment di-
rection). In another scenario, a would-be customer
might only be interested in reading balanced re-
views, since they often present more reasoned ar-
guments with fewer unsupported claims. Such a
person might therefore want to avoid reviews such
as Example (1) ? written by a Chinese purchaser of
a mobile phone (our English gloss).
(1)
?????????????????
?????????????????
??????????????????
??????????????????
?????????????????
????
The software is bad, some sent SMS are nev-
er received by the addressee; compatibility
is also bad, on some mobile phones the re-
ceived messages are in a scrambled encod-
ing! And sometimes the phone 'dies'! Photos
are horrible! It doesn't have a cyclic or pro-
304
grammable alarm-clock, you have to set it
every time, how cumbersome! The back cov-
er does not fit! The original software has
many holes!
In a third scenario, someone might decide they
would like only to read opinionated, weakly nega-
tive reviews such as Example (2), since these often
contain good argumentation while still identifying
the most salient bad aspects of a product.
(2)
?????????????????
?30KB??????????MP3???
??????????????????
?????????????????
?????????????????
????????????????
The response time of this mobile is very
long, MMS should be less than 30kb only to
be downloaded, also it doesn't support MP3
ring tones, (while) the built-in tunes are not
good, and from time to time it 'dies', but
when I was buying it I really liked it: very
original, very nicely matching red and white
colours, it has its individuality, also it's not
expensive, but when used it always causes
trouble, makes one's head ache
The review contains both positive and negative
sentiment covering different aspects of the product,
and the fact that it contains a balance of views
means that it is likely to be useful for a would-be
customer. Moving beyond review classification,
more advanced tasks such as automatic summa-
rization of reviews (e.g. Feiguina & LaPalme,
2007) might also benefit from techniques which
could distinguish more shades of sentiment than
just a binary positive / negative distinction.
A second dimension, orthogonal to positive /
negative, is opinionated / unopinionated (or equiv-
alently subjective / objective). When shopping for
a product, one might be interested in the physical
characteristics of the product or what features the
product has, rather than opinions about how well
these features work or about how well the product
as a whole functions. Thus, if one is looking for a
review that contains more factual information than
opinion, one might be interested in reviews like
Example (3).
(3)
?????????????????
???????5??800??500???
?????????????????
???WAP????????????
(My) overall feeling about this mobile is not
bad, it features: 5 alarm-clocks that switch
the phone on (off), phone book for 800 items
(500 people), lunar and solar calendars,
fast switching between time and date modes,
WAPnetworking, organizer,notebook and
so on.
This review is mostly neutral (unopinionated), but
contains information that could be useful to a
would-be customer which might not be in a prod-
uct specification document, e.g. fast switching be-
tween different operating modes. Similarly, would-
be customers might be interested in retrieving
completely unopinionated documents such as tech-
nical descriptions and user manuals. Again, as with
sentiment classification, we argue that opinionated
and unopinionated texts are not easily distinguish-
able separate sets, but form a continuum. In this
continuum, intermediate points are of interest as
well as the extremes.
A major obstacle for automatic classification of
sentiment and objectivity is lack of training data,
which limits the applicability of approaches based
on supervised machine learning. With the rapid
growth in textual data and the emergence of new
domains of knowledge it is virtually impossible to
maintain corpora of tagged data that cover all ? or
even most ? areas of interest. The cost of manual
tagging also adds to the problem. Reusing the same
corpus for training classifiers for new domains is
also not effective: several studies report decreased
accuracy in cross-domain classification (Engstr?m,
2004; Aue & Gamon, 2005) a similar problem has
also been observed in classification of documents
created over different time periods (Read, 2005).
In this paper we describe an unsupervised classi-
fication technique which is able to build its own
sentiment vocabulary starting from a very small
seed vocabulary, using iterative retraining to en-
large the vocabulary. In order to avoid problems of
domain dependence, the vocabulary is built using
text from the same source as the text which is to be
classified. In this paper we work with Chinese, but
using a very small seed vocabulary may mean that
this approach would in principle need very little
linguistic adjustment to be applied to a different
305
language. Written Chinese has some specific fea-
tures, one of which is the absence of explicitly
marked word boundaries, which makes word-based
processing problematic. In keeping with our unsu-
pervised, knowledge-poor approach, we do not use
any preliminary word segmentation tools or higher
level grammatical analysis.
The paper is structured as follows. Section 2 re-
views related work in sentiment classification and
more generally in unsupervised training of classi-
fiers. Section 3 describes our datasets, and Section
4 the techniques we use for unsupervised classifi-
cation and iterative retraining. Sections 5 and 6 de-
scribe a number of experiments into how well the
approaches work, and Section 7 concludes.
2 Related Work
2.1 Sentiment Classification
Most previous work on the problem of categoriz-
ing opinionated texts has focused on the binary
classification of positive and negative sentiment
(Turney, 2002; Pang et al, 2002; Dave at al.,
2003). However, Pang & Lee (2005) describe an
approach closer to ours in which they determine an
author's evaluation with respect to a multi-point
scale, similar to the 'five-star' sentiment scale
widely used on review sites. However, authors of
reviews are inconsistent in assigning fine-grained
ratings and quite often star systems are not consis-
tent between critics. This makes their approach
very author-dependent. The main differences are
that Pang and Lee use discrete classes (although
more than two), not a continuum as in our ap-
proach, and use supervised machine learning rather
than unsupervised techniques. A similar approach
was adopted by Hagedorn et al (2007), applied to
news stories: they defined five classes encoding
sentiment intensity and trained their classifier on a
manually tagged training corpus. They note that
world knowledge is necessary for accurate classifi-
cation in such open-ended domains.
There has also been previous work on determin-
ing whether a given text is factual or expresses
opinion (Yu& Hatzivassiloglu, 2003; Pang & Lee,
2004); again this work uses a binary distinction,
and supervised rather than unsupervised approach-
es.
Recent work on classification of terms with re-
spect to opinion (Esuli & Sebastiani, 2006) uses a
three-category system to characterize the opinion-
related properties of word meanings, assigning nu-
merical scores to Positive, Negative and Objective
categories. The visualization of these scores some-
what resembles our graphs in Section 5, although
we use two orthogonal scales rather than three cat-
egories; we are also concerned with classification
of documents rather than terms.
2.2 Unsupervised Classification
Abney (2002) compares two major kinds of unsu-
pervised approach to classification (co-training and
the Yarowsky algorithm). As we do not use multi-
ple classifiers our approach is quite far from co-
training. But it is close to the paradigm described
by Yarowsky (1995) and Turney (2002) as it also
employs self-training based on a relatively small
seed data set which is incrementally enlarged with
unlabelled samples. But our approach does not use
point-wise mutual information. Instead we use rel-
ative frequencies of newly found features in a
training subcorpus produced by the previous itera-
tion of the classifier. We also use the smallest pos-
sible seed vocabulary, containing just a single
word; however there are no restrictions regarding
the maximum number of items in the seed vocabu-
lary.
3 Data
3.1 Seed Vocabulary
Our approach starts out with a seed vocabulary
consisting of a single word, ? (good). This word
is tagged as a positive vocabulary item; initially
there are no negative items. The choice of word
was arbitrary, and other words with strongly posi-
tive or negative meaning would also be plausible
seeds. Indeed, ? might not be the best possible
seed, as it is relatively ambiguous: in some con-
texts it means to like or acts as the adverbial very,
and is often used as part of other words (although
usually contributing a positive meaning). But since
it is one of the most frequent units in the Chinese
language, it is likely to occur in a relatively large
number of reviews, which is important for the
rapid growth of the vocabulary list.
3.2 TestCorpus
Our test corpus is derived from product reviews
harvested from the website IT1681. All the reviews
were tagged by their authors as either positive or
negative overall. Most reviews consist of two or
three distinct parts: positive opinions, negative
opinions, and comments ('other') ? although some
1http://product.it168.com
306
reviews have only one part. We removed duplicate
reviews automatically using approximate match-
ing, giving a corpus of 29531 reviews of which
23122 are positive (78%) and 6409 are negative
(22%). The total number of different products in
the corpus is 10631, the number of product cate-
gories is 255, and most of the reviewed products
are either software products or consumer electron-
ics. Unfortunately, it appears that some users mis-
used the sentiment tagging facility on the website
so quite a lot of reviews have incorrect tags. How-
ever, the parts of the reviews are much more reli-
ably identified as being positive or negative so we
used these as the items of the test corpus. In the ex-
periments described in this paper we used 2317 re-
views of mobile phones of which 1158 are nega-
tive and 1159 are positive. Thus random choice
would have approximately 50% accuracy if all
items were tagged either as negative or positive2.
4 Method
4.1 Sentiment Classification
As discussed in Section 1, we do not carry out any
word segmentation or grammatical processing of
input documents. We use a very broad notion of
words (or phrases) in the Chinese language. The
basic units of processing are 'lexical items', each of
which is a sequence of one or more Chinese char-
acters excluding punctuation marks (which may
actually form part of a word, a whole word or a se-
quence of words), and `zones', each of which is a
sequence of characters delimited by punctuation
marks.
Each zone is classified as either positive or neg-
ative based whether positive or negative vocabu-
lary items predominate. In more detail, a simple
maximum match algorithm is used to find all lexi-
cal items (character sequences) in the zone that are
in the vocabulary list. As there are two parts of the
vocabulary (positive and negative), we correspond-
ingly calculate two scores using Equation (1)3,
S i=
Ld
L phrase
S d N d (1)
where Ld is the length in characters of a matching
lexical item, Lphrase is the length of the current zone
2This corpus is publicly available at http://www.informatics.
sussex.ac.uk/users/tz21/it168test.zip
3In the first iteration, when we have only one item in the vo-
cabulary, negative zones are found by means of the negation
check (so not + good = negative item).
in characters, Sd is the current sentiment score of
the matching lexical item (initially 1.0), and Nd is a
negation check coefficient. The negation check is a
regular expression which determines if the lexical
item is preceded by a negation within its enclosing
zone. If a negation is found then Nd is set to ?1.
The check looks for six frequently occurring nega-
tions:? (bu),?? (buhui),?? (meiyou),??
(baituo),?? (mianqu), and?? (bimian).
The sentiment score of a zone is the sum of sen-
timent scores of all the items found in it. In fact
there are two competing sentiment scores for every
zone: one positive (the sum of all scores of items
found in the positive part of the vocabulary list)
and one negative (the sum of the scores for the
items in the negative part). The sentiment direction
of a zone is determined from the maximum of the
absolute values of the two competing scores for the
zone.
This procedure is applied to all zones in a docu-
ment, classifying each zone as positive, negative,
or neither (in cases where there are no positive or
negative vocabulary items in the zone). To deter-
mine the sentiment direction of the whole docu-
ment, the classifier computes the difference be-
tween the number of positive and negative zones.
If the result is greater than zero the document is
classified as positive, and vice versa. If the result is
zero the document is balanced or neutral for senti-
ment.
4.2 Iterative Retraining
The task of iterative retraining is to enlarge the ini-
tial seed vocabulary (consisting of a single word as
discussed in Section 3.1) into a comprehensive vo-
cabulary list of sentiment-bearing lexical items. In
each iteration, the current version of the classifier
is run on the product review corpus to classify each
document, resulting in a training subcorpus of pos-
itive and a negative documents. The subcorpus is
used to adjust the scores of existing positive and
negative vocabulary items and to find new items to
be included in the vocabulary.
Each lexical item that occurs at least twice in the
corpus is a candidate for inclusion in the vocabu-
lary list. After candidate items are found, the sys-
tem calculates their relative frequencies in both the
positive and negative parts of the current training
subcorpus. The system also checks for negation
while counting occurrences: if a lexical item is pre-
ceded by a negation, its count is reduced by one.
This results in negative counts (and thus negative
relative frequencies and scores) for those items that
307
are usually used with negation; for example, ??
???(the quality is far too bad) is in the positive
part of the vocabulary with a score of ?1.70. This
means that the item was found in reviews classified
by the system as positive but it was preceded by a
negation. If during classification this item is found
in a document it will reduce the positive score for
that document (as it is in the positive part of the
vocabulary), unless the item is preceded by a nega-
tion. In this situation the score will be reversed
(multiplied by ?1), and the positive score will be
increased ? see Equation (1) above.
For all candidate items we compare their relative
frequencies in the positive and negative documents
in the subcorpus using Equation (2).
difference= ?F p? F n??F p?Fn?/2
(2)
If difference < 1, then the frequencies are similar
and the item does not have enough distinguishing
power, so it is not included in the vocabulary. Oth-
erwise the the sentiment score of the item is (re-)
calculated ? according to Equation (3) for positive
items, and analogously for negative items.
F p
F p?F n
(3)
Finally, the adjusted vocabulary list with the new
scores is ready for the next iteration.
4.3 Objectivity Classification
Given a sentiment classification for each zone in a
document, we compute sentiment density as the
proportion of opinionated zones with respect to the
total number of zones in the document. Sentiment
density measures the proportion of opinionated text
in a document, and thus the degree to which the
document as a whole is opinionated.
It should be noted that neither sentiment score
nor sentiment density are absolute values, but are
relative and only valid for comparing one docu-
ment with other. Thus, a sentiment density of 0.5
does not mean that the review is half opinionated,
half not. It means that the review is less opinionat-
ed than a review with density 0.9.
5 Experiments
We ran the system on the product review corpus
(Section 3.2) for 20 iterations. The results for bina-
ry sentiment classification are shown in Table 1.
We see increasing F-measure up to iteration 18, af-
ter which both precision and recall start to de-
screase; we therefore use the version of the classi-
fier as it stood after iteration 184. These figures are
only indicative of the classification accuracy of the
system. Accuracy might be lower for unseen text,
although since our approach is unsupervised we
could in principle perform further retraining itera-
tions on any sample of new text to tune the vocab-
ulary list to it.
We also computed a (strong) baseline, using as
the vocabulary list the NTU Sentiment Dictionary
(Ku et al, 2006)5 which is intended to contain only
sentiment-related words and phrases. We assigned
each positive and negative vocabulary item a score
of 1 or ?1 respectively. This setup achieved 87.77
precision and 77.09 recall on the product review
corpus.
In Section 1 we argued that sentiment and objec-
tivity should both be considered as continuums, not
Table 1. Results for binary sentiment classifica-
tion during iterative retraining.
4The size of the sentiment vocabulary after iteration 18 was
22530 (13462 positive and 9068 negative).
5Ku et al automatically generated the dictionary by enlarging
an initial manually created seed vocabulary by consulting two
thesauri, including tong2yi4ci2ci2lin2 and the Academia Sini-
ca Bilingual Ontological WordNet 3.
Iteration Precision Recall F-measure
1 77.62 28.43 41.62
2 76.15 73.81 74.96
3 81.15 80.07 80.61
4 83.54 82.79 83.16
5 84.66 83.78 84.22
6 85.51 84.77 85.14
7 86.59 85.76 86.17
8 86.78 86.11 86.44
9 87.15 86.32 86.74
10 87.01 86.37 86.69
11 86.9 86.15 86.53
12 87.05 86.41 86.73
13 86.87 86.19 86.53
14 87.35 86.67 87.01
15 87.13 86.45 86.79
16 87.14 86.5 86.82
17 86.8 86.24 86.52
18 87.57 86.89 87.22
19 87.23 86.67 86.95
20 87.18 86.54 86.86
308
binary distinctions. Section 4.1 describes how our
approach compares the number of positive and
negative zones for a document and treats the differ-
ence as a measure of the 'positivity' or 'negativity'
of a review. The document in Example (2), with 12
zones, is assigned a score of ?1 (the least negative
score possible): the review contains some positive
sentiment but the overall sentiment direction of the
review is negative. In contrast, Example (1) is
identified as a highly negative review, as would be
expected, with a score of ?8, from 11 zones.
Similarly, with regard to objectivity, the senti-
ment density of the text in Example (3) is 0.53,
which reflects its more factual character compared
to Example (1), which has a score of 0.91. We can
represent sentiment and objectivity on the follow-
ing scales:
Negative Balanced Positive
Unopinionated Neutral Opinionated
The scales are orthogonal, so we can combine
them into a single coordinate system:
Opinionated
Negative Positive
We would expect most product reviews to be
placed towards the top of the the coordinate system
(i.e. opinionated), and stretch from left to right.
Figure 1 plots the results of sentiment and objec-
tivity classification of the test corpus in this two di-
mensional coordinate system, where X represents
sentiment (with scores scaled with respect to the
number of zones so that ?100 is the most negative
possible and +100 the most positive), and Y repre-
sents sentiment density (0 being unopinionated and
1 being highly opinionated).
Most of the reviews are located in the upper part
of the coordinate system, indicating that they have
been classified as opinionated, with either positive
or negative sentiment direction. Looking at the
overall shape of the plot, more opinionated docu-
ments tend to have more explicit sentiment direc-
tion, while less opinionated texts stay closer to the
balanced / neutral region (around X = 0).
Figure 1. Reviews classified according to
sentiment (X axis) and degree of
opinionation (Y axis).
6 Discussion
As can be seen in Figure 1, the classifier managed
to map the reviews onto the coordinate system.
However, there are very few points in the neutral
region, that is, on the same X = 0 line as balanced
but with low sentiment density. By inspection, we
know that there are neutral reviews in our data set.
We therefore conducted a further experiment to in-
vestigate what the problem might be. We took
Wikipedia6 articles written in Chinese on mobile
telephony and related issues, as well as several ar-
ticles about the technology, the market and the his-
tory of mobile telecommunications, and split them
into small parts (about a paragraph long, to make
their size close to the size of the reviews) resulting
in a corpus of 115 documents, which we assume to
be mostly unopinionated. We processed these doc-
uments with the trained classifier and found that
they were mapped almost exactly where balanced
documents should be (see Figure 2).
Most of these documents have weak sentiment
direction (X = ?5 to +10), but are classified as rel-
atively opinionated (Y > 0.5). The former is to be
expected, whereas the latter is not. When investi-
gating the possible reasons for this behavior we no-
ticed that the classifier found not only feature de-
scriptions (like ???? nice touch) or expres-
sions which describe attitude (?? (one) like(s)),
but also product features (for example,?? MMS
or ?? TV) to be opinionated. This is because the
presence of some advanced features such as MMS
in mobile phones is often regarded as a positive by
6www.wikipedia.org
-40 -30 -20 -10 0 10 20 30 40 50 60
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
309
Figure 2. Classification of a sample of articles
from Wikipedia.
Figure 3. Classification of a sample of articles
from Wikipedia, using the NTU Sentiment
Dictionary as the vocabulary list.
authors of reviews. In addition, the classifier found
words that were used in reviews to describe situa-
tions connected with a product and its features: for
example,?? (service) was often used in descrip-
tions of quite unpleasant situations when a user had
to turn to a manufacturer's post-sales service for re-
pair or replacement of a malfunctioning phone, and
?? (user) was often used to describe what one
can do with some advanced features. Thus the clas-
sifier was able to capture some product-specific as
well as market-specific sentiment markers, howev-
er, it was not able to distinguish the context these
generally objective words were used in. This re-
sulted in relatively high sentiment density of neu-
tral texts which contained these words but used in
other types of context.
To verify this hypothesis we applied the same
processing to our corpus derived from Wikipedia
articles, but using as the vocabulary list the NTU
Sentiment Dictionary. The results (Figure 3) show
that most of the neutral texts are now mapped to
the lower part of the opinionation scale (Y < 0.5),
as expected. Therefore, to successfully distinguish
between balanced reviews and neutral documents a
classifier should be able to detect when product
features are used as sentiment markers and when
they are not.
7 Conclusions and Future Work
We have described an approach to classification of
documents with respect to sentiment polarity and
objectivity, representing both as a continuum, and
mapping classified documents onto a coordinate
system that also represents the difference between
balanced and neutral text. We have presented a
novel, unsupervised, iterative retraining procedure
for deriving the classifier, starting from the most
minimal size seed vocabulary, in conjunction with
a simple negation check. We have verified that the
approach produces reasonable results. The ap-
proach is extremely minimal in terms of language
processing technology, giving it good possibilities
for porting to different genres, domains and lan-
guages.
We also found that the accuracy of the method
depends a lot on the seed word chosen. If the word
has a relatively low frequency or does not have a
definite sentiment-related meaning, the results may
be very poor. For example, an antonymous word to
? (good) in Chinese is ? (bad), but the latter is
not a frequent word: the Chinese prefer to say??
(not good). When this word was used as the seed
word, accuracy was little more than 15%. Al-
though the first iteration produced high precision
(82%), the size of the extracted subcorpus was
only 24 items, resulting in the system being unable
to produce a good classifier for the following itera-
tions. Every new iteration produced an even poorer
result as each new extracted corpus was of lower
accuracy.
On the other hand, it seems that a seed list con-
sisting of several low-frequency one-character
words can compensate each other and produce bet-
ter results by capturing a larger part of the corpus
(thus increasing recall). Nevertheless a single word
may also produce results even better than those for
multiword seed lists. For example, the two-charac-
ter word ?? (comfortable) as seed reached 91%
-40 -30 -20 -10 0 10 20 30 40 50 60
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
-40 -30 -20 -10 0 10 20 30 40 50 60
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
310
accuracy with 90% recall. We can conclude that
our method relies on the quality of the seed word.
We therefore need to investigate ways of choosing
'lucky' seeds and avoiding 'unlucky' ones.
Future work should also focus on improving
classification accuracy: adding a little language-
specific knowledge to be able to detect some word
boundaries should help; we also plan to experiment
with more sophisticated methods of sentiment
score calculation. In addition, the notion of 'zone'
needs refining and language-specific adjustments
(for example, a 'reversed comma' should not be
considered to be a zone boundary marker, since
this punctuation mark is generally used for the enu-
meration of related objects).
More experiments are also necessary to deter-
mine how the approach works across domains, and
further investigation into methods for distinguish-
ing between balanced and neutral text.
Finally, we need to produce a new corpus that
would enable us to evaluate the performance of a
pre-trained version of the classifier that did not
have any prior access to the documents it was clas-
sifying: we need the reviews to be tagged not in a
binary way as they are now, but in a way that re-
flects the two continuums we use (sentiment and
objectivity).
Acknowledgements
The first author is supported by the Ford Founda-
tion International Fellowships Program.
References
Abney, Steven (2002) Bootstrapping. In Proceedings of
the 40th Annual Meeting of the Association for Com-
putational Linguistics, Philadelphia, PA. 360?367.
Aue, Anthony & Michael Gamon (2005) Customizing
sentiment classifiers to new domains: a case study. In
Proceedings of RANLP-2005.
Dave, Kushal, Steve Lawrence & David M. Pennock
(2003) Mining the peanut gallery: opinion extraction
and semantic classification of product reviews. In
Proceedings of the Twelfth International World Wide
WebConference. 519?528.
Engstr?m, Charlotte (2004) Topic dependence in senti-
ment classification. Unpublished MPhil dissertation,
University of Cambridge.
Esuli, Andrea & Fabrizio Sebastiani (2006) SENTI-
WORDNET: a publicly available lexical resource for
opinion mining. In Proceedings of LREC-06, the 5th
Conference on Language Resources and Evaluation,
Genoa, Italy.
Hagedorn, Bennett, Massimiliano Ciaramita & Jordi At-
serias (2007) World knowledge in broad-coverage in-
formation filtering. In Proceedings of the 30th ACM
SIGIR Conference on Research and Development in
Information Retrieval. 801?802.
Ku, Lun-Wei, Yu-Ting Liang & Hsin-Hsi Chen (2006)
Opinion extraction, summarization and tracking in
news and blog corpora. In Proceedings of the AAAI-
2006 Spring Symposium on Computational Ap-
proaches to Analyzing Weblogs, AAAI Technical Re-
port.
Feiguina, Olga & Guy Lapalme (2007) Query-based
summarization of customer reviews. In Proceedings
of the 20th Canadian Conference on Artificial Intelli-
gence, Montreal, Canada. 452?463.
Pang, Bo, Lillian Lee & Shivakumar Vaithyanathan
(2002) Thumbs up? Sentiment classification using
machine learning techniques. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, Philadelphia, PA. 79?86.
Pang, Bo & Lillian Lee (2004) A sentimental education:
sentiment analysis using subjectivity summarization
based on minimum cuts. In Proceedings of the 42nd
Annual Meeting of the Association for Computation-
al Linguistics, Barcelona, Spain. 271?278.
Pang, Bo & Lillian Lee (2005) Seeing stars: exploiting
class relationships for sentiment categorization with
respect to rating scales. In Proceedings of the 43rd
Annual Meeting of the Association for Computation-
al Linguistics, Ann Arbor, MI. 115?124.
Read, Jonathon (2005) Using emoticons to reduce de-
pendency in machine learning techniques for senti-
ment classification. In Proceedings of the Student
Research Workshop at ACL-05, Ann Arbor, MI.
Turney, Peter D. (2002) Thumbs up or thumbs down?
Semantic orientation applied to unsupervised classifi-
cation of reviews. In Proceedings of the 40th Annual
Meeting of the Association for Computational Lin-
guistics, Philadelphia, PA. 417?424.
Yarowsky, David (1995) Unsupervised word sense dis-
ambiguation rivaling supervised methods. In Pro-
ceedings of the 33rd Annual Meeting of the Associa-
tion for Computational Linguistics, Cambridge, MA.
189?196.
Yu, Hong & Vasileios Hatzivassiloglou (2003) Towards
answering opinion questions: separating facts from
opinions and identifying the polarity of opinion sen-
tences. In Proceedings of the 2003 Conference on
Empirical Methods in Natural Language Processing,
Sapporo, Japan. 129?136.
311
Proceedings of the ACL 2007 Student Research Workshop, pages 37?42,
Prague, June 2007. c?2007 Association for Computational Linguistics
Kinds of Features for Chinese Opinionated Information Retrieval
Taras Zagibalov
Department of Informatics
University of Sussex
United Kingdom
T.Zagibalov@sussex.ac.uk
Abstract
This paper presents the results of experi-
ments in which we tested different kinds of
features for retrieval of Chinese opinionated
texts. We assume that the task of retrieval of
opinionated texts (OIR) can be regarded as
a subtask of general IR, but with some dis-
tinct features. The experiments showed that
the best results were obtained from the com-
bination of character-based processing, dic-
tionary look up (maximum matching) and a
negation check.
1 Introduction
The extraction of opinionated information has re-
cently become an important research topic. Business
and governmental institutions often need to have in-
formation about how their products or actions are
perceived by people. Individuals may be interested
in other people?s opinions on various topics ranging
from political events to consumer products.
At the same time globalization has made the
whole world smaller, and a notion of the world as
a ?global village? does not surprise people nowa-
days. In this context we assume information in Chi-
nese to be of particular interest as the Chinese world
(the mainland China, Taiwan, Hong Kong, Singa-
pore and numerous Chinese communities all over
the world) is getting more and more influential over
the world economy and politics.
We therefore believe that a system capable of pro-
viding access to opinionated information in other
languages (especially in Chinese) might be of great
use for individuals as well as for institutions in-
volved in international trade or international rela-
tions.
The sentiment classification experiments pre-
sented in this paper were done in the context of
Opinionated Information Retrieval which is planned
to be a module in a Cross-Language Opinion Extrac-
tion system (CLOE). The main goal of this system is
to provide access to opinionated information on any
topic ad-hoc in a language different to the language
of a query.
To implement the idea the CLOE system which
is the context for the experiments described in the
paper will consist of four main modules:
1. Query translation
2. Opinionated Information Retrieval
3. Opinionated Information Extraction
4. Results presentation
The OIR module will process complex queries
consisting of a word sequence indicating a topic and
sentiment information. An example of such a query
is: ?Asus laptop + OPINIONS?, another, more de-
tailed query, might be ?Asus laptop + POSITIVE
OPINIONS?.
Another possible approach to the architecture of
the CLOE system would be to implement the pro-
cessing as a pipeline consisting, first, of using IR to
retrieve certain articles relevant to the topic followed
by second stage of classifying them according to
sentiment polarity. But such an approach probably
would be too inefficient, as the search will produce
a lot of irrelevant results (containing no opinionated
information).
37
2 Chinese NLP and Feature Selection
Problem
One of the central problems in Chinese NLP is what
the basic unit1 of processing should be. The problem
is caused by a distinctive feature of the Chinese lan-
guage - absence of explicit word boundaries, while it
is widely assumed that a word is of extreme impor-
tance for any NLP task. This problem is also crucial
for the present study as the basic unit definition af-
fects the kinds of features to be used.
In this study we use a mixed approached, based
both on words (tokens consisting of more than one
character) and characters as basic units. It is also
important to note, that we use notion of words in
the sense of Vocabulary Word as it was stated by Li
(2000). This means that we use only tokens that are
listed in a dictionary, and do not look for all words
(including grammar words).
3 Related Work
Processing of subjective texts and opinions has re-
ceived a lot of interest recently. Most of the authors
traditionally use a classification-based approach for
sentiment extraction and sentiment polarity detec-
tion (for example, Pang et al (2002), Turney (2002),
Kim and Hovy (2004) and others), however, the re-
search described in this paper uses the information
retrieval (IR) paradigm which has also been used by
some researchers.
Several sentiment information retrieval models
were proposed in the framework of probabilistic lan-
guage models by Eguchi and Lavrenko (2006). The
setting for the study was a situation when a user?s
query specifies not only terms expressing a certain
topic and also specifies a sentiment polarity of in-
terest in some manner, which makes this research
very similar to the present one. However, we use
sentiment scores (not probabilistic language mod-
els) for sentiment retrieval (see Section 4.1). Dave
et al (Dave et al, 2003) described a tool for sift-
ing through and synthesizing product reviews, au-
tomating the sort of work done by aggregation sites
or clipping services. The authors of this paper used
probability scores of arbitrary-length substrings that
provide optimal classification. Unlike this approach
1In the context of this study terms ?feature? and ?basic unit?
are used interchangeably.
we use a combination of sentiment weights of char-
acters and words (see Section 4).
Recently several works on sentiment extraction
from Chinese texts were published. In a paper by
Ku et al (2006a) a dictionary-based approach was
used in the context of sentiment extraction and sum-
marization. The same authors describe a corpus of
opinionated texts in another paper (2006b). This pa-
per also defines the annotations for opinionated ma-
terials. Although we use the same dictionary in our
research, we do not use only word-based approach
to sentiment detection, but we also use scores for
characters obtained by processing the dictionary as
a training corpus (see Section 4).
4 Experiments
In this paper we present the results of sentiment clas-
sification experiments in which we tested different
kinds of features for retrieval of Chinese opinionated
information.
As stated earlier (see Section 1), we assume that
the task of retrieval of opinionated texts (OIR) can
be regarded as a subtask of general IR with a query
consisting of two parts: (1) words indicating topic
and (2) a semantic class indicating sentiment (OPIN-
IONS). The latter part of the query cannot be speci-
fied in terms that can be instantly used in the process
of retrieval.
The sentiment part of the query can be further de-
tailed into subcategories such as POSITIVE OPIN-
IONS, NEGATIVE OPINIONS, NEUTRAL OPIN-
IONS each of which can be split according to sen-
timent intensity (HIGHLY POSITIVE OPINIONS,
SLIGHTLY NEGATIVE OPINIONS etc.). But
whatever level of categorisation we use, the query
is still too abstract and cannot be used in practice. It
therefore needs to be put into words and most prob-
ably expanded. The texts should also be indexed
with appropriate sentiment tags which in the context
of sentiment processing implies classification of the
texts according to presence / absence of a sentiment
and, if the texts are opinionated, according to their
sentiment polarity.
To test the proposed approach we designed two
experiments.
The purpose of the first experiment was to find the
most effective kind of features for sentiment polar-
38
ity discrimination (detection) which can be used for
OIR 2. Nie et al (2000) found that for Chinese IR
the most effective kinds of features were a combina-
tion of dictionary look up (longest-match algorithm)
together with unigrams (single characters). The ap-
proach was tested in the first experiment.
The second experiment was designed to test the
found set of features for text classification (index-
ing) for an OIR query of the first level (finds opin-
ionated information) and for an OIR query of the
second level (finds opinionated information with
sentiment direction detection), thus the classifier
should 1) detect opinionated texts and 2) classify the
found items either as positive or as negative.
As training corpus for the second experiment we
use the NTU sentiment dictionary (NTUSD) (by Ku
et al (2006a))3 as well as a list of sentiment scores
of Chinese characters obtained from processing of
the same dictionary. Dictionary look up used the
longest-match algorithm. The dictionary has 2809
items in the ?positive? part and 8273 items in the
?negative?. The same dictionary was also used as a
corpus for calculating the sentiment scores of Chi-
nese characters. The use of the dictionary as a
training corpus for obtaining the sentiment scores
of characters is justified by two reasons: 1) it is
domain-independent and 2) it contains only relevant
(sentiment-related) information. The above men-
tioned parts of the dictionary used as the corpus
comprised 24308 characters in the ?negative? part
and 7898 characters in the ?positive? part.
4.1 Experiment 1
A corpus of E-Bay4 customers? reviews of products
and services was used as a test corpus. The total
number of reviews is 128, of which 37 are nega-
tive (average length 64 characters) and 91 are pos-
itive (average length 18 characters), all of the re-
views were tagged as ?positive? or ?negative? by the
2For simplicity we used only binary polarity in both exper-
iments: positive or negative. Thus terms ?sentiment polarity?
and ?sentiment direction? are used interchangeably in this pa-
per.
3Ku et al (2006a) automatically generated the dictionary
by enlarging an initial manually created seed vocabulary by
consulting two thesauri, including tong2yi4ci2ci2lin2 and the
Academia Sinica Bilingual Ontological Wordnet 3.
4http://www.ebay.com.cn/
reviewers5.
We computed two scores for each item (a review):
one for positive sentiment, another for negative sen-
timent. The decision about an item?s sentiment po-
larity was made every time by finding the biggest
score of the two.
For every phrase (a chunk of characters between
punctuation marks) a score was calculated as:
Scphrase =
?
(Scdictionary) +
?
(Sccharacter)
where Scdictionary is a dictionary based score calcu-
lated using following formula:
Scdictionary =
Ld
Ls
? 100
where Ld - length of a dictionary item, Ls - length of
a phrase. The constant value 100 is used to weight
the score, obtained by a series of preliminary tests
as a value that most significantly improved the accu-
racy.
The sentiment scores for characters were obtained
by the formula:
Sci = Fi/F(i+j)
where Sci is the sentiment score for a character for a
given class i, Fi - the character?s relative frequency
in a class i, F(i+j) - the character?s relative frequency
in both classes i and j taken as one unit. The relative
frequency of character c is calculated as
Fc =
? Nc
? N(1...n)
where
?Nc is a number of the character?s occur-
rences in the corpus, and
? N(1...n) is the number of
all characters in the same corpus.
Preliminary tests showed that inverting all the
characters for which Sci ? 1 improves accuracy.
The inverting is calculated as follows:
Scinverted = Sci ? 1
We compute scores rather than probabilities since
we are combining information from two distinct
sources (characters and words).
5The corpus is available at
http://www.informatics.sussex.ac.uk/users/tz21/corpSmall.zip.
39
In addition to the features specified (characters
and dictionary items) we also used a simple negation
check. The system checked two most widely used
negations in Chinese: bu and mei. Every phrase was
compared with the following pattern: negation+ 0-2
characters+ phrase. The scores of all the unigrams
in the phrase that matched the pattern were multi-
plied by -1.
Finally, the score was calculated for an item as the
sum of the phrases? scores modified by the negation
check:
Scitem =
?
(Scphrase ? NegCheck)
For sentiment polarity detection the item scores
for each of the two polarities were compared to each
other: the polarity with bigger score was assigned to
the item.
SentimentPolarity = argmax(Sci|Scj)
where Sci is an item score for one polarity and Scj
is an item score for the other.
The main evaluation measure was accuracy of
sentiment identification, expressed in percent.
4.1.1 Results of Experiment 1
To find out which kinds of features perform best
for sentiment polarity detection the system was run
several times with different settings.
Running without character scores (with dictionary
longest-match only) gave the following results: al-
most 64% of positive and near 65% for negative re-
views were detected correctly, which is 64% accu-
racy for the whole corpus (note that a baseline clas-
sifier tagging all items as positive achieves an accu-
racy of 71.1%). Characters with sentiment scores
alone performed much better on negative reviews
(84% accuracy) rather than on positive (65%), but
overall performance was still better: 70%. Both
methods combined gave a significant increase on
positive reviews (73%) and no improvement on neg-
ative (84%), giving 77% overall. The last run was
with the dictionary look up, the characters and the
negation check. The results were: 77% for positive
and 89% for negative, 80% corpus-wide (see Table
1).
Judging from the results it is possible to suggest
that both the word-based dictionary look up method
Method Positive Negative All
Dictionary 63.7 64.8 64.0
Characters 64.8 83.7 70.3
Characters+Dictionary 73.6 83.7 76.5
Char?s+Dictionary+negation 76.9 89.1 80.4
Table 1: Results of Experiment 1 (accuracy in per-
cent).
and character-based method contributed to the final
result. It also corresponds to the results obtained by
Nie et al (2000) for Chinese information retrieval,
where the same combination of features (characters
and words) also performed best.
The negation check increased the performance by
3% overall, up to 80%. Although the performance
gain is not very high, the computational cost of this
feature is very low.
As we used a non-balanced corpus (71% of the
reviews are positive), it is quite difficult to compare
the results with the results obtained by other authors.
But the proposed classifier outperformed some stan-
dart classifiers on the same data set: a Naive Bayes
(multinomial) classifier gained only 49.6 % of ac-
curacy (63 items tagged correctly) while a Support
vector machine classifier got 64.5 % of accuracy (82
items).6
4.2 Experiment 2
The second experiment included two parts: deter-
mining whether texts are opinionated which is a pre-
condition for the processing of the OPINION part of
the query; and tagging found texts with relevant sen-
timent for processing a more detailed form of this
query POSITIVE/NEGATIVE OPINION.
For this experiment we used the features that
showed the best performance as described in section
4.1: the dictionary items and the characters with the
sentiment scores.
The test corpus for this experiment consisted of
282 items, where every item is a paragraph. We used
paragraphs as basic items in this experiment because
of two reasons: 1. opinionated texts (reviews) are
usually quite short (in our corpus all of them are one
paragraph), while texts of other genres are usually
much longer; and 2. for IR tasks it is more usual to
retrieve units longer then a sentence.
6We used WEKA 3.4.10
(http://www.cs.waikato.ac.nz/ ml/weka )
40
The test corpus has following structure: 128 items
are opinionated, of which 91 are positive and 37 are
negative (all the items are the reviews used in the
first experiment, see 4.1). 154 items are not opin-
ionated, of which 97 are paragraphs taken from a
scientific book on Chinese linguistics and 57 items
are from articles taken form a Chinese on-line ency-
clopedia Baidu Baike7.
For the first task we used the following tech-
nique: every item was assigned a score (a sum of the
characters? scores and dictionary scores described in
4.1). The score was divided by the number of char-
acters in the item to obtain the average score:
averScitem =
Scitem
Litem
where Scitem is the item score, and Litem is the
length of an item (number of characters in it).
A positive and a negative average score is com-
puted for each item.
4.2.1 Results of Experiment 2
To determine whether an item is opinionated (for
OPINION query), the maximum of the two scores
was compared to a threshold value. The best perfor-
mance was achieved with the threshold value of 1.6
- more than 85% of accuracy8 (see Table 2).
Next task (NEGATIVE/POSITIVE OPINIONS)
was processed by comparing the negative and pos-
itive scores for each found item (see Table 2).
Query Recall Precision F-measure
OPINION 71.8 85.1 77.9
POS/NEG OPINION 64.0 75.9 69.4
Table 2: Results of Experiment 2 (in percent).
Although the unopinionated texts are very dif-
ferent from the opinionated ones in terms of genre
and topic, the standard classifiers (Naive Bayes
(multinomial) and SVM) failed to identify any non-
opinionated texts. The most probable explanation
for this is that there were no items tagged ?unopin-
ionated? in the training corpus (the sentiment dictio-
nary) and there were only words and phrases with
predominant sentiment meaning rather then topic-
related.
7http://baike.baidu.com/
8A random choice could have approximately 55% of accu-
racy if tagged all items as negative.
It is worth noting that we observed the same rela-
tion between subjectivity detection and polarity clas-
sification accuracy as described by Pang and Lee
(2004) and Eriksson (2006). The accuracy of the
sentiment detection of opinionated texts (excluding
erroneously detected unopinionated texts) in Exper-
iment 2 has increased by 13% for positive reviews
and by 6% for negative reviews (see Table 3).
Query Positive Negative
Experiment 1 76.9 89.1
Experiment 2 89.9 95.6
Table 3: Accuracy of sentiment polarity detection of
opinionated texts (in percent).
5 Conclusion and Future Work
These preliminary experiments showed that using
single characters and dictionary items modified by
the negation check can produce reasonable results:
about 78% F-measure for sentiment detection (see
4.1.1) and almost 70% F-measure for sentiment
polarity identification (see 4.2.1) in the context
of domain-independent opinionated information re-
trieval. However, since the test corpus is very small
the results obtained need further validation on bigger
corpora.
The use of the dictionary as a training corpus
helped to avoid domain-dependency, however, using
a dictionary as a training corpus makes it impossible
to obtain grammar information by means of analysis
of punctuation marks and grammar word frequen-
cies.
More intensive use of context information could
improve the accuracy. The dictionary-based pro-
cessing may benefit from the use of word relations
information: some words have sentiment informa-
tion only when used with others. For example,
a noun dongxi (?a thing?) does not seem to have
any sentiment information on its own, although it
is tagged as ?negative? in the dictionary.
Some manual filtering of the dictionary may im-
prove the output. It might also be promising to test
the influence on performance of the different classes
of words in the dictionary, for example, to use only
adjectives or adjectives and nouns together (exclud-
ing adverbials).
Another technique to be tested is computing the
41
positive and negative scores for the characters used
only in one class, but absent in another. In the cur-
rent system, characters are assigned only one score
(for the class they are present in). It might improve
accuracy if such characters have an appropriate neg-
ative score for the other class.
Finally, the average sentiment score may be used
for sentiment scaling. For example, if in our exper-
iments items with a score less than 1.6 were con-
sidered not to be opinionated, then ones with score
more than 1.6 can be put on a scale where higher
scores are interpreted as evidence for higher senti-
ment intensity (the highest score was 52). The ?scal-
ing? approach could help to avoid the problem of as-
signing documents to more than one sentiment cate-
gory as the approach uses a continuous scale rather
than a predefined number of rigid classes. The scale
(or the scores directly) may be used as a means of
indexing for a search engine comprising OIR func-
tionality.
References
Kushal Dave, Steve Lawrence, and David M. Pennock.
2003. Mining the peanut gallery: Opinion extraction
and semantic classification of product reviews. In Pro-
ceedings of the International World Wide Web Con-
ference, pages 519 ? 528, Budapest, Hungary. ACM
Press.
Koji Eguchi and Victor Lavrenko. 2006. Sentiment re-
trieval using generative models. In Proceedings of
the 2006 Conference on Empirical Methods in Natural
Language Processing (EMNLP 2006), pages 345?354,
Sydney, July.
Brian Eriksson. 2006. Sentiment classifica-
tion of movie reviews using linguistic parsing.
http://www.cs.wisc.edu/?apirak/cs/cs838/
eriksson final.pdf.
Soo-Min Kim and Eduard H. Hovy. 2004. Determin-
ing the sentiment of opinions. In Proceedings of
COLING-04, pages 1367?1373, Geneva, Switzerland,
August 23-27.
Lun-Wei Ku, Yu-Ting Liang, and Hsin-Hsi Chen. 2006a.
Opinion extraction, summarization and tracking in
news and blog corpora. In Proceedings of AAAI-2006
Spring Symposium on Computational Approaches to
Analyzing Weblogs, volume AAAI Technical Report,
pages 100?107, March.
Lun-Wei Ku, Yu-Ting Liang, and Hsin-Hsi Chen. 2006b.
Tagging heterogeneous evaluation corpora for opin-
ionated tasks. In Proceedings of the Fifth International
Conference on Language Resources and Evaluation,
pages 667?670, Genoa, Italy, May.
Wei Li. 2000. On Chinese parsing without using a sep-
arate word segmenter. Communication of COLIPS,
10:17?67.
Jian-Yun Nie, Jiangfeng Gao, Jian Zhang, and Ming
Zhou. 2000. On the use of words and n-grams
for Chinese information retrieval. In Proceedings of
the 5th International Workshop Information Retrieval
with Asian Languages, pages 141?148. ACM Press,
November.
Bo Pang and Lillian Lee. 2004. A sentimental education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. In Proceedings of the 42nd
Annual Meeting of the Association for Computational
Linguistics, pages 271?278, Barcelona, Spain.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment classification using ma-
chine learning techniques. In Proceedings of the 2002
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 79?86, University of Penn-
sylvania.
Peter D. Turney. 2002. Thumbs up or thumbs down?
Semantic orientation applied to unsupervised classifi-
cation of reviews. In Proceedings of the 40th Annual
Meeting of the Association for Computational Linguis-
tics (ACL?02), pages 417?424, Philadelphia, Pennsyl-
vania.
42
