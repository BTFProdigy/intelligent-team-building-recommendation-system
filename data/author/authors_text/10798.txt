Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 812?819,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Attacking Decipherment Problems Optimally with Low-Order N-gram
Models
Sujith Ravi and Kevin Knight
University of Southern California
Information Sciences Institute
Marina del Rey, California 90292
{sravi,knight}@isi.edu
Abstract
We introduce a method for solving substi-
tution ciphers using low-order letter n-gram
models. This method enforces global con-
straints using integer programming, and it
guarantees that no decipherment key is over-
looked. We carry out extensive empirical ex-
periments showing how decipherment accu-
racy varies as a function of cipher length and
n-gram order. We also make an empirical in-
vestigation of Shannon?s (1949) theory of un-
certainty in decipherment.
1 Introduction
A number of papers have explored algorithms
for automatically solving letter-substitution ciphers.
Some use heuristic methods to search for the best de-
terministic key (Peleg and Rosenfeld, 1979; Gane-
san and Sherman, 1993; Jakobsen, 1995; Olson,
2007), often using word dictionaries to guide that
search. Others use expectation-maximization (EM)
to search for the best probabilistic key using letter
n-gram models (Knight et al, 2006). In this paper,
we introduce an exact decipherment method based
on integer programming. We carry out extensive de-
cipherment experiments using letter n-gram models,
and we find that our accuracy rates far exceed those
of EM-based methods.
We also empirically explore the concepts in Shan-
non?s (1949) paper on information theory as applied
to cipher systems. We provide quantitative plots for
uncertainty in decipherment, including the famous
unicity distance, which estimates how long a cipher
must be to virtually eliminate such uncertainty.
We find the ideas in Shannon?s (1949) paper rel-
evant to problems of statistical machine translation
and transliteration. When first exposed to the idea
of statistical machine translation, many people natu-
rally ask: (1) how much data is needed to get a good
result, and (2) can translation systems be trained
without parallel data? These are tough questions by
any stretch, and it is remarkable that Shannon was
already in the 1940s tackling such questions in the
realm of code-breaking, creating analytic formulas
to estimate answers.
Our novel contributions are as follows:
? We outline an exact letter-substitution deci-
pherment method which:
- guarantees that no key is overlooked, and
- can be executed with standard integer pro-
gramming solvers
? We present empirical results for decipherment
which:
- plot search-error-free decipherment results at
various cipher lengths, and
- demonstrate accuracy rates superior to EM-
based methods
? We carry out empirical testing of Shannon?s
formulas for decipherment uncertainty
2 Language Models
We work on letter substitution ciphers with spaces.
We look for the key (among 26! possible ones)
that, when applied to the ciphertext, yields the most
English-like result. We take ?English-like? to mean
812
most probable according to some statistical lan-
guage model, whose job is to assign some proba-
bility to any sequence of letters. According to a 1-
gram model of English, the probability of a plaintext
p1...pn is given by:
P (p1...pn) = P (p1) ? P (p2) ? ... ? P (pn)
That is, we obtain the probability of a sequence
by multiplying together the probabilities of the in-
dividual letters that make it up. This model assigns
a probability to any letter sequence, and the proba-
bilities of all letter sequences sum to one. We col-
lect letter probabilities (including space) from 50
million words of text available from the Linguistic
Data Consortium (Graff and Finch, 1994). We also
estimate 2- and 3-gram models using the same re-
sources:
P (p1...pn) = P (p1 | START ) ? P (p2 | p1) ? P (p3 | p2) ?
... ? P (pn | pn?1) ? P (END | pn)
P (p1...pn) = P (p1 | START ) ? P (p2 | START p1) ?
P (p3 | p1 p2) ? ... ? P (pn | pn?2 pn?1) ?
P (END | pn?1 pn)
Unlike the 1-gram model, the 2-gram model will
assign a low probability to the sequence ?ae? be-
cause the probability P (e | a) is low. Of course, all
these models are fairly weak, as already known by
(Shannon, 1949). When we stochastically generate
text according to these models, we get, for example:
1-gram: ... thdo detusar ii c ibt deg irn toihytrsen ...
2-gram: ... itariaris s oriorcupunond rke uth ...
3-gram: ... ind thnowelf jusision thad inat of ...
4-gram: ... rece bence on but ther servier ...
5-gram: ... mrs earned age im on d the perious ...
6-gram: ... a party to possible upon rest of ...
7-gram: ... t our general through approve the ...
We can further estimate the probability of a whole
English sentence or phrase. For example, the prob-
abilities of two plaintext phrases ?het oxf? and
?the fox? (which have the same letter frequency
distribution) is shown below. The 1-gram model
which counts only the frequency of occurrence of
each letter in the phrase, estimates the same proba-
bility for both the phrases ?het oxf? and ?the fox?,
since the same letters occur in both phrases. On the
other hand, the 2-gram and 3-gram models, which
take context into account, are able to distinguish be-
tween the English and non-English phrases better,
and hence assign a higher probability to the English
phrase ?the fox?.
Model P(het oxf) P(the fox)
1-gram 1.83? 10?9 1.83? 10?9
2-gram 3.26? 10?11 1.18? 10?7
3-gram 1.89? 10?13 1.04? 10?6
Over a longer sequence X of length N , we can
also calculate ?log2(P (X))/N , which (per Shan-
non) gives the compression rate permitted by the
model, in bits per character. In our case, we get:1
1-gram: 4.19
2-gram: 3.51
3-gram: 2.93
3 Decipherment
Given a ciphertext c1...cn, we search for the key that
yields the most probable plaintext p1...pn. There are
26! possible keys, too many to enumerate. How-
ever, we can still find the best one in a guaranteed
fashion. We do this by taking our most-probable-
plaintext problem and casting it as an integer pro-
gramming problem.2
Here is a sample integer programming problem:
variables: x, y
minimize:
2x+ y
subject to:
x+ y < 6.9
y ? x < 2.5
y > 1.1
We require that x and y take on integer values. A
solution can be obtained by typing this integer pro-
gram into the publicly available lp solve program,
1Because spacing is fixed in our letter substitution ciphers,
we normalize P (X) by the sum of probabilities of all English
strings that match the spacing pattern of X .
2For an overview of integer and linear programming, see for
example (Schrijver, 1998).
813
1    2    3    4    5    6    7    8  ?
_    Q    W    B    S    Q    W    _  ?
a    a    a    a    a    a    a    a  ?
b    b    b    b    b    b    b    b  ?
c    c    c    c    c    c    c    c  ?
d    d    d    d    d    d    d    d  ?
e    e    e    e    e    e    e    e  ?
?    ?    ?    ?    ?    ?    ?    ?  ?
z    z    z    z    z    z    z    z  ?
_    _    _    _    _    _    _    _  ?
ciphertext
network of
possible
plaintexts
link-2de link-5ad link-7e_
Figure 1: A decipherment network. The beginning of the ciphertext is shown at the top of the figure (underscores
represent spaces). Any left-to-right path through the network constitutes a potential decipherment. The bold path
corresponds to the decipherment ?decade?. The dotted path corresponds to the decipherment ?ababab?. Given a
cipher length of n, the network has 27 ? 27 ? (n ? 1) links and 27n paths. Each link corresponds to a named variable
in our integer program. Three links are shown with their names in the figure.
or the commercially available CPLEX program,
which yields the result: x = 4, y = 2.
Suppose we want to decipher with a 2-gram lan-
guage model, i.e., we want to find the key that yields
the plaintext of highest 2-gram probability. Given
the ciphertext c1...cn, we create an integer program-
ming problem as follows. First, we set up a net-
work of possible decipherments (Figure 1). Each
of the 27 ? 27 ? (n ? 1) links in the network is a
binary variable in the integer program?it must be
assigned a value of either 0 or 1. We name these
variables linkXY Z , where X indicates the column
of the link?s source, and Y and Z represent the rows
of the link?s source and destination (e.g. variables
link1aa, link1ab, link5qu, ...).
Each distinct left-to-right path through the net-
work corresponds to a different decipherment. For
example, the bold path in Figure 1 corresponds to
the decipherment ?decade?. Decipherment amounts
to turning some links ?on? (assigning value 1 to the
link variable) and others ?off? (assigning value 0).
Not all assignments of 0?s and 1?s to link variables
result in a coherent left-to-right path, so we must
place some ?subject to? constraints in our integer
program.
We observe that a set of variables forms a path if,
for every node in columns 2 through n?1 of the net-
work, the following property holds: the sum of the
values of the link variables entering the node equals
the sum of the link variables leaving the node. For
nodes along a chosen decipherment path, this sum
will be 1, and for others, it will be 0.3 Therefore,
we create one ?subject to? constraint for each node
(? ? stands for space). For example, for the node in
column 2, row e we have:
subject to:
link1ae + link1be + link1ce + ...+ link1 e
= link2ea + link2eb + link2ec + ...+ link2e
Now we set up an expression for the ?minimize?
part of the integer program. Recall that we want
to select the plaintext p1...pn of highest probability.
For the 2-gram language model, the following are
equivalent:
(a) Maximize P (p1...pn)
(b) Maximize log2 P (p1...pn)
(c) Minimize ?log2 P (p1...pn)
(d) Minimize ?log2 [ P (p1 |START )
3Strictly speaking, this constraint over nodes still allows
multiple decipherment paths to be active, but we can rely on
the rest of our integer program to select only one.
814
?P (p2 | p1)
? ...
?P (pn | pn?1)
?P (END | pn) ]
(e) Minimize ?log2 P (p1 |START )
?log2 P (p2 | p1)
? ...
?log2 P (pn | pn?1)
?log2 P (END | pn)
We can guarantee this last outcome if we con-
struct our minimization function as a sum of 27 ?27 ?
(n? 1) terms, each of which is a linkXY Z variable
multiplied by ?log2P (Z|Y ):
Minimize link1aa ? ?log2 P (a | a)
+ link1ab ? ?log2 P (b | a)
+ link1ac ? ?log2 P (c | a)
+ ...
+ link5qu ? ?log2 P (u | q)
+ ...
When we assign value 1 to link variables along
some decipherment path, and 0 to all others, this
function computes the negative log probability of
that path.
We must still add a few more ?subject to? con-
straints. We need to ensure that the chosen path im-
itates the repetition pattern of the ciphertext. While
the bold path in Figure 1 represents the fine plain-
text choice ?decade?, the dotted path represents the
choice ?ababab?, which is not consistent with the
repetition pattern of the cipher ?QWBSQW?. To
make sure our substitutions obey a consistent key,
we set up 27 ? 27 = 729 new keyxy variables to
represent the choice of key. These new variables
are also binary, taking on values 0 or 1. If variable
keyaQ = 1, that means the key maps plaintext a to
ciphertext Q. Clearly, not all assignments to these
729 variables represent valid keys, so we augment
the ?subject to? part of our integer program by re-
quiring that for any letter x,
subject to:
keyxA + keyxB + ...+ keyxZ + keyx = 1
keyAx + keyBx + ...+ keyZx + key x = 1
That is, every plaintext letter must map to exactly
one ciphertext letter, and every ciphertext letter must
map to exactly one plaintext letter. We also add a
constraint to ensure that the ciphertext space charac-
ter maps to the plaintext space character:
subject to:
key = 1
Finally, we ensure that any chosen decipherment
path of linkXY Z variables is consistent with the
chosen key. We know that for every node A along
the decipherment path, exactly one active link has
A as its destination. For all other nodes, zero active
links lead in. Suppose node A represents the de-
cipherment of ciphertext letter ci as plaintext letter
pj?for all such nodes, we stipulate that the sum of
values for link(i?1)xpj (for all x) equals the value of
keypjci . In other words, whether a node lies along
the chosen decipherment path or not, the chosen key
must support that decision.
Figure 2 summarizes the integer program that we
construct from a given ciphertext c1...cn. The com-
puter code that transforms any given cipher into a
corresponding integer program runs to about one
page. Variations on the decipherment network yield
1-gram and 3-gram decipherment capabilities. Once
an integer program is generated by machine, we
ask the commercially-available CPLEX software
to solve it, and then we note which keyXY variables
are assigned value 1. Because CPLEX computes
the optimal key, the method is not fast?for ciphers
of length 32, the number of variables and constraints
encoded in the integer program (IP) along with aver-
age running times are shown below. It is possible to
obtain less-than-optimal keys faster by interrupting
the solver.
Model # of IP # of IP Average
variables constraints running time
1-gram 1, 755 1, 083 0.01 seconds
2-gram 27, 700 2, 054 50 seconds
3-gram 211, 600 27, 326 450 seconds
4 Decipherment Experiments
We create 50 ciphers each of lengths 2, 4, 8, ..., 256.
We solve these with 1-gram, 2-gram, and 3-gram
language models. We record the average percentage
of ciphertext tokens decoded incorrectly. 50% error
means half of the ciphertext tokens are deciphered
wrong, while 0% means perfect decipherment. Here
815
variables:
linkipr 1 if the ith cipher letter is deciphered as plaintext letter p AND the (i+1)th cipher letter is
deciphered as plaintext letter r
0 otherwise
keypq 1 if decipherment key maps plaintext letter p to ciphertext letter q
0 otherwise
minimize:?n?1
i=1
?
p,r
linkipr ? ?log P (r|p) (2-gram probability of chosen plaintext)
subject to:
for all p:
?
r
keypr = 1 (each plaintext letter maps to exactly one ciphertext letter)
for all p:
?
r
keyrp = 1 (each ciphertext letter maps to exactly one plaintext letter)
key = 1 (cipher space character maps to plain space character)
for (i=1...n-2), for all r: [
?
p
linkipr =
?
p
link(i+1)rp ] (chosen links form a left-to-right path)
for (i=1...n-1), for all p:
?
r
linkirp = keypci+1 (chosen links are consistent with chosen key)
Figure 2: Summary of how to build an integer program for any given ciphertext c1...cn. Solving the integer program
will yield the decipherment of highest probability.
we illustrate some automatic decipherments with er-
ror rates:
42% error: the avelage ongrichman hal cy wiof a
sevesonme qus antizexty that he buprk lathes we blung
than soment - fotes mmasthes
11% error: the average englishman has so week a
reference for antiality that he would rather be prong than
recent - deter quarteur
2% error: the average englishman has so keep a
reference for antiquity that he would rather be wrong than
recent - peter mcarthur
0% error: the average englishman has so deep a
reverence for antiquity that he would rather be wrong
than recent - peter mcarthur
Figure 3 shows our automatic decipherment re-
sults. We note that the solution method is exact, not
heuristic, so that decipherment error is not due to
search error. Our use of global key constraints also
leads to accuracy that is superior to (Knight et al,
2006). With a 2-gram model, their EM algorithm
gives 10% error for a 414-letter cipher, while our
method provides a solution with only 0.5% error.
At shorter cipher lengths, we observe much higher
improvements when using our method. For exam-
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 100
De
cip
he
rm
en
t E
rro
r (
%)
Cipher Length (letters)2 4 8 16 32 64 128 256 512 1024
1-gram
2-gram
3-gram
Figure 3: Average decipherment error using integer pro-
gramming vs. cipher length, for 1-gram, 2-gram and 3-
gram models of English. Error bars indicate 95% confi-
dence intervals.
ple, on a 52-letter textbook cipher, using a 2-gram
model, the solution from our method resulted in 21%
error as compared to 85% error given by the EM so-
lution.
We see that deciphering with 3-grams works well
on ciphers of length 64 or more. This confirms
816
that such ciphers can be attacked with very limited
knowledge of English (no words or grammar) and
little custom programming.
The 1-gram model works badly in this scenario,
which is consistent with Bauer?s (2006) observation
that for short texts, mechanical decryption on the ba-
sis of individual letter frequencies does not work. If
we had infinite amounts of ciphertext and plaintext
drawn from the same stochastic source, we would
expect the plain and cipher frequencies to eventually
line up, allowing us to read off a correct key from the
frequency tables. The upper curve in Figure 3 shows
that convergence to this end is slow.
5 Shannon Equivocation and Unicity
Distance
Very short ciphers are hard to solve accurately.
Shannon (1949) pinpointed an inherent difficulty
with short ciphers, one that is independent of the so-
lution method or language model used; the cipher
itself may not contain enough information for its
proper solution. For example, given a short cipher
like XY Y X , we can never be sure if the answer is
peep, noon, anna, etc. Shannon defined a mathemat-
ical measure of our decipherment uncertainty, which
he called equivocation (now called entropy).
Let C be a cipher, M be the plaintext message it
encodes, and K be the key by which the encoding
takes place. Before even seeing C, we can compute
our uncertainty about the key K by noting that there
are 26! equiprobable keys:4
H(K) = ?(26!) ? (1/26!) ? log2 (1/26!)
= 88.4 bits
That is, any secret key can be revealed in 89 bits.
When we actually receive a cipher C, our uncer-
tainty about the key and the plaintext message is re-
duced. Shannon described our uncertainty about the
plaintext message, letting m range over all decipher-
ments:
H(M |C) = equivocation of plaintext message
= ?
?
m
P (m|C) ? log2 P (m|C)
4(Shannon, 1948) The entropy associated with a set of pos-
sible events whose probabilities of occurrence are p1, p2, ..., pn
is given by H = ?
?n
i=1 pi ? log2(pi).
P (m|C) is probability of plaintext m (according
to the language model) divided by the sum of proba-
bilities of all plaintext messages that obey the repeti-
tion pattern of C. While integer programming gives
us a method to find the most probable decipherment
without enumerating all keys, we do not know of a
similar method to compute a full equivocation with-
out enumerating all keys. Therefore, we sample up
to 100,000 plaintext messages in the neighborhood
of the most probably decipherment5 and compute
H(M |C) over that subset.6
Shannon also described H(K|C), the equivoca-
tion of key. This uncertainty is typically larger than
H(M |C), because a given message M may be de-
rived from C via more than one key, in case C does
not contain all 26 letters of the alphabet.
We compute H(K|C) by letting r(C) be the
number of distinct letters in C, and letting q(C) be
(26 ? r(C))!. Letting i range over our sample of
plaintext messages, we get:
H(K|C) = equivocation of key
= ?
?
i
q(C) ? (P (i)/q(C)) ? log2 (P (i)/q(C))
= ?
?
i
P (i) ? log2 (P (i)/q(C))
= ?
?
i
P (i) ? (log2 P (i)? log2 q(C))
= ?
?
i
P (i) ? log2 P (i) +
?
i
P (i) ? log2 q(C)
= H(M |C) + log2 q(C)
Shannon (1949) used analytic means to roughly
sketch the curves for H(K|C) and H(M |C), which
we reproduce in Figure 4. Shannon?s curve is drawn
for a human-level language model, and the y-axis is
given in ?decimal digits? instead of bits.
5The sampling used to compute H(M |C) starts with the
optimal key and expands out a frontier, by swapping letters in
the key, and recursing to generate new keys (and corresponding
plaintext message decipherments). The plaintext messages are
remembered so that the frontier expands efficiently. The sam-
pling stops if 100,000 different messages are found.
6Interestingly, as we grow our sample out from the most
probable plaintext, we do not guarantee that any intermediate
result is a lower bound on the equivocation. An example is pro-
vided by the growing sample (0.12, 0.01, 0.01, 0.01, 0.01, 0.01,
0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01), whose entropy steadily
increases. However, if we add a 14th item whose P (m) is 0.12,
the entropy suddenly decreases from 2.79 to 2.78.
817
Unicity Distance
Key Equivocation
Message Equivocation
Figure 4: Equivocation for simple substitution on English (Shannon, 1949).
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 0  10  20  30  40  50  60  70  80  90  100
 110
 120
 130
 140
 150
 160
 170
 180
 190
 200
 210
 220
 230
 240
 250
 260
Eq
uiv
oc
ati
on
 of
 ke
y (
bit
s)
Cipher Length
1-gram
2-gram
3-gram
Figure 5: Average key equivocation observed (bits) vs.
cipher length (letters), for 1-gram, 2-gram and 3-gram
models of English.
For comparison, we plot in Figures 5 and 6 the av-
erage equivocations as we empirically observe them
using our 1-, 2-, and 3-gram language models.
The shape of the key equivocation curve follows
Shannon, except that it is curved from the start,
rather than straight.
The message equivocation curve follows Shan-
non?s prediction, rising then falling. Because very
short ciphers have relatively few solutions (for ex-
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 0  10  20  30  40  50  60  70  80  90  100
 110
 120
 130
 140
 150
 160
 170
 180
 190
 200
 210
 220
 230
 240
 250
 260
Eq
uiv
oc
ati
on
 of
 m
es
sa
ge
 (b
its
)
Cipher Length
1-gram
2-gram
3-gram
Figure 6: Average message equivocation observed (bits)
vs. cipher length (letters), for 1-gram, 2-gram and 3-gram
models of English.
ample, a one-letter cipher has only 26), the overall
uncertainty is not that great.7 As the cipher gets
longer, message equivocation rises. At some point,
it then decreases, as the cipher begins to reveal its
secret through patterns of repetition.
Shannon?s analytic model also predicts a sharp
decline of message equivocation towards zero. He
7Uncertainty is only loosely related to accuracy?even if we
are quite certain about a solution, it may still be wrong.
818
defines the unicity distance (U ) as the cipher length
at which we have virtually no more uncertainty
about the plaintext. Using analytic means (and vari-
ous approximations), he gives:
U = H(K)/(A?B)
where:
A = bits per character of a 0-gram model (4.7)
B = bits per character of the model used to decipher
For a human-level language model (B ? 1.2), he
concludes U ? 25, which is confirmed by practice.
For our language models, the formula gives:
U = 173 (1-gram)
U = 74 (2-gram)
U = 50 (3-gram)
These numbers are in the same ballpark as
Bauer (2006), who gives 167, 74, and 59. We note
that these predicted unicity distances are a bit too
rosy, according to our empirical message equivoca-
tion curves. Our experience confirms this as well, as
1-gram frequency counts over a 173-letter cipher are
generally insufficient to pin down a solution.
6 Conclusion
We provide a method for deciphering letter substi-
tution ciphers with low-order models of English.
This method, based on integer programming, re-
quires very little coding and can perform an opti-
mal search over the key space. We conclude by not-
ing that English language models currently used in
speech recognition (Chelba and Jelinek, 1999) and
automated language translation (Brants et al, 2007)
are much more powerful, employing, for example,
7-gram word models (not letter models) trained on
trillions of words. Obtaining optimal keys accord-
ing to such models will permit the automatic deci-
pherment of shorter ciphers, but this requires more
specialized search than what is provided by gen-
eral integer programming solvers. Methods such
as these should also be useful for natural language
decipherment problems such as character code con-
version, phonetic decipherment, and word substitu-
tion ciphers with applications in machine translation
(Knight et al, 2006).
7 Acknowledgements
The authors wish to gratefully acknowledge
Jonathan Graehl, for providing a proof to support
the argument that taking a larger number of samples
does not necessarily increase the equivocation. This
research was supported by the Defense Advanced
Research Projects Agency under SRI International?s
prime Contract Number NBCHD040058.
References
Friedrich L. Bauer. 2006. Decrypted Secrets: Methods
and Maxims of Cryptology. Springer-Verlag.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large language mod-
els in machine translation. In Proceedings of EMNLP-
CoNLL.
Ciprian Chelba and Frederick Jelinek. 1999. Structured
language modeling for speech recognition. In Pro-
ceedings of NLDB: 4th International Conference on
Applications of Natural Language to Information Sys-
tems.
Ravi Ganesan and Alan T. Sherman. 1993. Statistical
techniques for language recognition: An introduction
and guide for cryptanalysts. Cryptologia, 17(4):321?
366.
David Graff and Rebecca Finch. 1994. Multilingual text
resources at the linguistic data consortium. In Pro-
ceedings of the HLT Workshop on Human Language
Technology.
Thomas Jakobsen. 1995. A fast method for cryptanalysis
of substitution ciphers. Cryptologia, 19(3):265?274.
Kevin Knight, Anish Nair, Nishit Rathod, and Kenji Ya-
mada. 2006. Unsupervised analysis for decipherment
problems. In Proceedings of the COLING/ACL.
Edwin Olson. 2007. Robust dictionary attack of short
simple substitution ciphers. Cryptologia, 31(4):332?
342.
Shmuel Peleg and Azriel Rosenfeld. 1979. Break-
ing substitution ciphers using a relaxation algorithm.
Comm. ACM, 22(11):598?605.
Alexander Schrijver. 1998. Theory of Linear and Integer
Programming. John Wiley & Sons.
Claude E. Shannon. 1948. A mathematical theory
of communication. Bell System Technical Journal,
27:379?423 and 623?656.
Claude E. Shannon. 1949. Communication theory
of secrecy systems. Bell System Technical Journal,
28:656?715.
819
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 887?896,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Automatic Prediction of Parser Accuracy
Sujith Ravi and Kevin Knight
University of Southern California
Information Sciences Institute
Marina del Rey, California 90292
{sravi,knight}@isi.edu
Radu Soricut
Language Weaver, Inc.
4640 Admiralty Way, Suite 1210
Marina del Rey, California 90292
rsoricut@languageweaver.com
Abstract
Statistical parsers have become increasingly
accurate, to the point where they are useful in
many natural language applications. However,
estimating parsing accuracy on a wide variety
of domains and genres is still a challenge in
the absence of gold-standard parse trees.
In this paper, we propose a technique that au-
tomatically takes into account certain charac-
teristics of the domains of interest, and ac-
curately predicts parser performance on data
from these new domains. As a result, we have
a cheap (no annotation involved) and effective
recipe for measuring the performance of a sta-
tistical parser on any given domain.
1 Introduction
Statistical natural language parsers have recently
become more accurate and more widely available.
As a result, they are being used in a variety of
applications, such as question answering (Herm-
jakob, 2001), speech recognition (Chelba and Je-
linek, 1998), language modeling (Roark, 2001), lan-
guage generation (Soricut, 2006) and, most notably,
machine translation (Charniak et al, 2003; Galley et
al., 2004; Collins et al, 2005; Marcu et al, 2006;
Huang et al, 2006; Avramidis and Koehn, 2008).
These applications are employed on a wide range of
domains and genres, and therefore the question of
how accurate a parser is on the domain and genre of
interest becomes acute. Ideally, one would want to
have available a recipe for precisely answering this
question: ?given a parser and a particular domain of
interest, how accurate are the parse trees produced??
The only recipe that is implicitly given in the large
literature on parsing to date is to have human anno-
tators build parse trees for a sample set from the do-
main of interest, and consequently use them to com-
pute a PARSEVAL (Black et al, 1991) score that is
indicative of the intrinsic performance of the parser.
Given the wide range of domains and genres for
which NLP applications are of interest, combined
with the high expertise required from human anno-
tators to produce parse tree annotations, this recipe
is, albeit precise, too expensive. The other recipe
that is currently used on a large scale is to measure
the performance of a parser on existing treebanks,
such as WSJ (Marcus et al, 1993), and assume that
the accuracy measure will carry over to the domains
of interest. This recipe, albeit cheap, cannot provide
any guarantee regarding the performance of a parser
on a new domain, and, as experiments in this paper
show, can give wrong indications regarding impor-
tant decisions for the design of NLP systems that
use a syntactic parser as an important component.
This paper proposes another method for measur-
ing the performance of a parser on a given domain
that is both cheap and effective. It is a fully auto-
mated procedure (no expensive annotation involved)
that uses properties of both the domain of interest
and the domain on which the parser was trained in
order to measure the performance of the parser on
the domain of interest. It is, in essence, a solution to
the following prediction problem:
Input: (1) a statistical parser and its training data,
(2) some chunk of text from a new domain or genre
Output: an estimate of the accuracy of the parse
trees produced for that chunk of text
887
Accurate estimations for this prediction problem
will allow a system designer to make the right de-
cisions for the given domain of interest. Such deci-
sions include, but are not restricted to, the choice of
the parser, the choice of the training data, the choice
of how to implement various components such as the
treatment of unknown words, etc. Altogether, a cor-
rect estimation of the impact of such decisions on the
resulting parse trees can guide a system designer in a
hill-climbing scenario for which an extrinsic metric
(such as the impact on the overall quality of the sys-
tem) is usually too expensive to be employed often
enough. To provide an example, a machine transla-
tion engine that requires parse trees as training data
in order to learn syntax-based translation rules (Gal-
ley et al, 2006) needs to employ a syntactic parser
as soon as the training process starts, but it can take
up to hundreds and even thousands of CPU hours
(for large training data sets) to train the engine be-
fore translations can be produced and measured. Al-
though a real estimate of the impact of a parser de-
sign decision in this scenario can only be gauged
from the quality of the translations produced, it is
impractical to create such estimates for each design
decision. On the other hand, estimates using the so-
lution proposed in this paper can be obtained fast,
before submitting the parser output to a costly train-
ing procedure.
2 Related Work and Experimental
Framework
There have been previous studies which explored the
problem of automatically predicting the task diffi-
culty for various NLP applications. (Albrecht and
Hwa, 2007) presented a regression based method
for developing automatic evaluation metrics for ma-
chine translation systems without directly relying on
human reference translations. (Hoshino and Nak-
agawa, 2007) built a computer-adaptive system for
generating questions to teach English grammar and
vocabulary to students, by predicting the difficulty
level of a question using various features. There
have been a few studies of English parser accuracy
in domains/genres other than WSJ (Gildea, 2001;
Bacchiani et al, 2006; McClosky et al, 2006), but
in order to make measurements for such studies, it
is necessary to have gold-standard parses in the non-
WSJ domain of interest.
Gildea (2001) studied how well WSJ-trained
parsers do on the Brown corpus, for which a gold
standard exists. He looked at sentences with 40
words or less. (Bacchiani et al, 2006) carried out
a similar experiment on sentences of all lengths,
and (McClosky et al, 2006) report additional re-
sults. The table below shows results from our own
measurements of Charniak parser1 (Charniak and
Johnson, 2005) accuracy (F-measure on sentences of
all lengths), which are consistent with these studies.
For the Brown corpus, the test set was formed from
every tenth sentence in the corpus (Gildea, 2001).
Training Set Test Set Sent.
count
Charniak
accuracy
WSJ sec. 02-21 WSJ sec. 24 1308 90.48
(39,832 sent.) WSJ sec. 23 2343 91.13
Brown-test 2186 86.34
Here we investigate algorithms for predicting the
accuracy of a parser P on sentences, chunks of sen-
tences, and whole corpora. We also investigate and
contrast several scenarios for prediction: (1) the pre-
dictor looks at the input text only, (2) the predictor
looks at the input text and the output parse trees of
P , and (3) the predictor looks at the input text, the
output parse trees of P , and the outputs of other pro-
grams, such as the output parse trees of a different
parser Pref used as a reference. Under none of these
scenarios is the predictor allowed to look at gold-
standard parses in the new domain/genre.
The intuition behind what we are trying to achieve
here can be compared to an analogous task?trying
to assess the performance of a median student from
a math class on a given test, without having access to
the answer sheet. Looking at the test only, we could
probably tell whether the test looks hard or not, and
therefore whether the student will do well or not.
Looking at the student?s answers will likely give us
an even better idea of the performance. Finally, the
answers of a second student with similar proficiency
will provide even better clues: if the students agree
on every answer, then they probably both did well,
but if they disagree frequently, then they (and hence
our student) probably did not do as well.
Our first experiments are concerned with validat-
ing the idea itself: can a predictor be trained such
1Downloaded from ftp.cs.brown.edu/pub/nlparser/reranking-
parserAug06.tar.gz in February, 2007.
888
that it predicts the same F-scores as the ones ob-
tained using gold-trees? We first validate this using
the WSJ corpus itself, by dividing the WSJ treebank
into several sections:
1. Training (WSJ section 02-21). The parser P is
trained on this data.
2. Development (WSJ section 24). We use this
data for training our predictor.
3. Test (WSJ section 23). We use this data for
measuring our predictions. For each test sentence,
we compute (1) the PARSEVAL F-measure score
using the test gold standard, and (2) our predicted
F-measure. We report the correlation coefficient (r)
between the actual F-scores and our predicted F-
scores. We will also use a root-mean-square error
(rms error) metric to compare actual and predicted
F-scores.
Section 3 describes the features used by our pre-
dictor. Given these features, as well as actual
F-scores computed for the development data, we
use supervised learning to set the feature weights.
To this end, we use SVM-Regression2 (Smola and
Schoelkopf, 1998) with an RBF kernel, to learn the
feature weights and build our predictor system.3 We
validate the accuracy of the predictor trained in this
fashion on both WSJ (Section 4) and the Brown cor-
pus (Section 5).
3 Features Used for Predicting Parser
Accuracy
3.1 Text-based Features
One hypothesis we explore is that (all other things
being equal) longer sentences are harder to parse
correctly than shorter sentences. When exposed
to the development set, SVM-Regression learns
weights to best predict F-scores using the values for
this feature corresponding to each sentence in the
corpus.
Does the predicted F-score correlate with actual
F-score on a sentence by sentence basis? There was
a positive but weak correlation:
2Weka software (http://www.cs.waikato.ac.nz/ml/weka/)
3We compared a few regression algorithms like SVM-
Regression (using different kernels and parameter settings) and
Multi-Layer Perceptron (neural networks) ? we trained the al-
gorithms separately on dev data and picked the one that gave
the best cross-validation accuracy (F-measure).
Feature set dev (r) test (r)
Length 0.13 0.19
Another hypothesis is that the parser performance
is influenced by the number of UNKNOWN words
in the sentence to be parsed, i.e., the number of
words in the test sentence that were never seen be-
fore in the training set. Training the predictor with
this feature produces a positive correlation, slightly
weaker compared to the Length feature.
Feature set dev (r) test (r)
UNK 0.11 0.11
Unknown words are not the only ones that can in-
fluence the performance of a parser. Rare words,
for which statistical models do not have reliable es-
timates, are also likely to impact parsing accuracy.
To test this hypothesis, we add a language model
perplexity?based (LM-PPL) feature. We extract the
yield of the training trees, on which we train a tri-
gram language model.4 We compute the perplexity
of each test sentence with respect to this language
model, and use it as feature in our predictor system.
Note that this feature is meant as a refinement of the
previous UNK feature, in the sense that perplexity
numbers are meant to signal the occurrence of un-
known words, as well as rare (from the training data
perspective) words. However, the correlation we ob-
serve for this feature is similar to the correlation ob-
served for the UNK feature, which seems to suggest
that the smoothing techniques used by the parsers
employed in these experiments lead to correct treat-
ment of the rare words.
Feature set dev (r) test (r)
LM-PPL 0.11 0.10
We also look at the possibility of automatically
detecting certain ?cue? words that are appropriate
for our prediction problem. That is, we want to see
if we can detect certain words that have a discrimi-
nating power in deciding whether parsing a sentence
that contains them is difficult or easy. To this end,
we use a subset of the development data, which con-
tains the 200 best-parsed and 200 worst-parsed sen-
tences (based on F-measure scores). For each word
in the development dataset, we compute the infor-
mation gain (IG) (Yang and Pedersen, 1997) score
for that word with respect to the best/worst parsed
4We trained using the SRILM language modeling toolkit,
with default settings.
889
dataset. These words are then ranked by their IG
scores, and the top 100 words are included as lex-
ical features in our predictor system. As expected,
the correlation on the development set is quite high
(given that these lexical cues are extracted from this
particular set), but a positive correlation holds for
the test set as well.
Feature set dev (r) test (r)
lexCount100 0.43 0.18
3.2 Parser P?based Features
Besides exploiting the information present in the in-
put text, we can also inspect the output tree of the
parser P for which we are interested in predicting
accuracy. We create a rootSYN feature based on
the syntactic category found at the root of the out-
put tree (?is it S??, ?is it FRAG??). We also create
a puncSYN feature based on the number of words
labeled as punctuation tags (based on the intuition
that heavy use of punctuation can be indicative of
the difficulty of the input sentences), and a label-
SYN feature in which we bundled together informa-
tion regarding the number of internal nodes in the
parse tree output that have particular labels (?how
many nodes are labeled with PP??). In our predictor,
we use 72 such labelSYN features corresponding to
all the syntactic labels found in the parse tree out-
put for the development set. The test set correlation
given by the rootSYN and the labelSYN features are
higher than some of the text-based features, whereas
the puncSYN feature seems to have little discrimi-
native power.
Feature set dev (r) test (r)
rootSYN 0.21 0.17
puncSYN 0.09 0.01
labelSYN 0.33 0.28
3.3 Reference Parser Pref ?based Features
In addition to the text-based features and parser P?
based features, we can bring in an additional parser
Pref whose output is used as a reference against
which the output of parser P is measured. For the
reference parser feature, our goal is to measure how
similar/different are the results from the two parsers.
We find that if the parses are similar, they are more
likely to be right. In order to compute similarity, we
can compare the constituents in the two parse trees
from P and Pref , and see how many constituents
match. This is most easily accomplished by consid-
ering Pref to be a ?gold standard? (even though it is
not necessarily a correct parse) and computing the
F-measure score of parser P against Pref . We use
this F-measure score as a feature for prediction.
For the experiments presented in this section we
use as Pref , the parser from (Bikel, 2002). Intu-
itively, the requirement for choosing parser Pref in
conjunction with parser P seems to be that they
are different enough to produce non-identical trees
when presented with the same input, and at the
same time to be accurate enough to produce reli-
able parse trees. The choice of P as (Charniak and
Johnson, 2005) and Pref as (Bikel, 2002) fits this
bill, but many other choices can be made regarding
Pref , such as (Klein and Manning, 2003; Petrov and
Klein, 2007; McClosky et al, 2006; Huang, 2008).
We leave the task of creating features based on the
consensus of multiple parsers as future work.
The correlation given by the reference parser?
based feature Pref on the test set is the highest
among all the features we explored.
Feature set dev (r) test (r)
Pref 0.40 0.36
3.4 The Aggregated Power of Features
The table below lists all the individual features we
have described in this section, sorted according to
the correlation value obtained on the test set.
Feature set dev (r) test (r)
Pref 0.40 0.36
labelSYN 0.33 0.28
lexCount500 0.56 0.23
lexBool500 0.58 0.20
lexCount1000 0.67 0.20
lexBool1000 0.58 0.20
Length 0.13 0.19
lexCount100 0.43 0.18
lexBool100 0.43 0.18
rootSYN 0.21 0.17
UNK 0.11 0.11
LM-PPL 0.11 0.10
puncSYN 0.09 0.01
Note how the lexical features tend to over-fit the
development data?the words were specifically cho-
sen for their discriminating power on that particular
set. Hence, adding more lexical features to the pre-
dictor system improves the correlation on develop-
ment (due to over-fitting), but it does not produce
consistent improvement on the test set. However,
890
Method (using 3 features:
Length, UNK, Pref )
# of random
restarts
dev (r)
SVM Regression 0.42
1 0.138
5 0.136
Maximum Correlation 10 0.166
Training (MCT) 25 0.178
100 0.232
1000 0.27
10,000 0.401
Table 1: Comparison of correlation (r) obtained using MCT versus
SVM-Regression on development corpus.
there is some indication that the counts of the lex-
ical features are important, and count-based lexical
features tend to have similar or better performance
compared to their boolean-based counterparts.
Since these features measure different but over-
lapping pieces of the information available, it is to
be expected that some of the feature combinations
would provide better correlation that the individual
features, but the gains are not strictly additive. By
taking the individual features that provide the best
discriminative power, we are able to get a correla-
tion score of 0.42 on the test set.
Feature set dev (r) test (r)
Pref + labelSYN + Length + lexCount100 +
rootSYN + UNK + LM-PPL
0.55 0.42
3.5 Optimizing for Maximum Correlation
If our goal is to obtain the highest correlations
with the F-score measure, is SVM regression the
best method? Liu and Gildea (2007) recently in-
troduced Maximum Correlation Training (MCT), a
search procedure that follows the gradient of the for-
mula for correlation coefficient (r). We implemented
MCT, but obtained no better results. Moreover, it
required many random re-starts just to obtain results
comparable to SVM regression (Table 1).
4 Predicting Accuracy on Multiple
Sentences
The results for the scenario presented in Section 3
are encouraging, but other scenarios are also im-
portant from a practical perspective. For instance,
we are interested in predicting the performance of a
particular parser not on a sentence-by-sentence ba-
sis, but for a representative chunk of sentences from
the new domain. In order to predict the F-measure
on multiple sentences, we modify our feature set to
generate information on a whole chunk of sentences
Sentences in
chunk (n)
WSJ-test (r) WSJ-test
(rms error)
1 0.42 0.098
20 0.61 0.026
50 0.62 0.019
100 0.69 0.015
500 0.79 0.011
Table 2: Performance of predictor on n-sentence chunks from WSJ-test
(Correlation and rms error between actual/predicted accuracies).
rather than a single sentence. Predicting the corre-
lation at chunk level is, not unexpectedly, an eas-
ier problem than predicting correlation at sentence
level, as the results in the first two columns of Ta-
ble 2 show.
For 100-sentence chunks, we also plot the pre-
dicted accuracies versus actual accuracies for the
WSJ-test set in Figure 1. This scatterplot brings to
light an artifact of using correlation metric (r) for
evaluating our predictor?s performance. Although
our objective is to improve correlation between ac-
tual and predicted F-scores, the correlation metric (r)
does not tell us directly how well the predictor is
doing. In Figure 1, the system predicts that on
an average, most sentence chunks can be parsed
with an accuracy of 0.9085 (which is the mean pre-
dicted F-score on WSJ-test). But the range of pre-
dictions from our system [0.89,0.92] is smaller than
the actual F-score range [0.86,0.95]. Hence, even
though the correlation scores are high, this does not
necessarily mean that our predictions are on target.
An additional metric, root-mean-square (rms) error,
which measures the distance between actual and pre-
dicted F-measures, can be used to gauge the qual-
ity of our predictions. For a particular chunk-size,
lowering the rms error translates into aligning the
points of a scatterplot as the one in Figure 1, closer
to the x=y line, implying that the predictor is getting
better at exactly predicting the F-score values. The
third column in Table 2 shows the rms error for our
predictor at different chunk sizes. The results using
this metric also show that the prediction problem be-
comes easier as the chunk size increases.
Assuming that we have the test set of WSJ sec-
tion 23, but without the gold-standard trees, how
can we get an approximation for the overall accu-
racy of a parser P on this test set? One possibility,
which we use here as a baseline, is to compute the
F-score on a set for which we do have gold-standard
trees. If we use our development set (WSJ section
891
 0.85
 0.86
 0.87
 0.88
 0.89
 0.9
 0.91
 0.92
 0.93
 0.94
 0.95
 0.85  0.86  0.87  0.88  0.89  0.9  0.91  0.92  0.93  0.94  0.95
Act
ual
 Ac
cur
acy
Predicted Accuracy
per-chunk-accuracy
x=y lineFitted-line
Figure 1: Plot showing Actual vs. Predicted accuracies for
WSJ-test (100-sentence chunks). Each plot point represents a
100-sentence chunk. (rms error = 0.015)
 0.85
 0.86
 0.87
 0.88
 0.89
 0.9
 0.91
 0.92
 0.93
 0.94
 0.95
 0.85  0.86  0.87  0.88  0.89  0.9  0.91  0.92  0.93  0.94  0.95
Act
ual
 Ac
cur
acy
Predicted Accuracy
per-chunk-accuracy
x=y line
Figure 2: Plot showing Actual vs. Adjusted Predicted accu-
racies (shifting with ? = 0.757, skewing with ? = 1.0) for
WSJ-test (100-sentence chunks). (rms error = 0.014)
System F-measure
Charniak F-measure on WSJ-dev (baseline) 90.48 (fd)
Predictor (feature weights set with WSJ-dev) 90.85 (fp)
Actual Charniak accuracy 91.13 (ft)
Table 3: Comparing Charniak parser accuracy (from different systems)
on entire WSJ-test corpus
24) for this purpose, and (Charniak and Johnson,
2005) as the parser P , the baseline is an F-score of
90.48 (fd), which is the actual Charniak parser accu-
racy on WSJ section 24. Instead, if we run our pre-
dictor on the test set (a single chunk containing all
the sentences in the test set), it predicts an F-score
of 90.85 (fp). These two predictions are listed as
the first two rows in Table 3. Of course, having the
actual gold-standard trees for WSJ section 23 helps
us decide which prediction is better: the actual ac-
curacy of the Charniak parser on WSJ section 23 is
an F-score of 91.13 (ft), which makes our prediction
better than the baseline.
4.1 Shifting Predictions to Match Actual
Accuracy
We correctly predict (in Table 3) that the
WSJ-test is easier to parse than the WSJ-
dev (90.85 > 90.48). However, our predictor is too
conservative?the WSJ-test is actually even easier
to parse (91.13 > 90.85). We can fix this by shift-
ing the mean predicted F-score (which is equal to
fp) further away from the dev F-measure (fd), and
closer to the actual F-measure (ft). This is achieved
by shifting all the individual predictions by a certain
amount as shown below.
Let p be an individual prediction from our system.
The shifted prediction p? is given by:
p? = p+ ?(fp ? fd) (1)
We can tune ? to make the new mean predic-
tion (f ?p) to be equal to the actual F-measure (ft).
f ?p = fp + ?(fp ? fd) (2)
? =
ft ? fp
fp ? fd
(3)
Using the F-score values from Table 3, we get an
? = 0.757 and an exact prediction of 91.13. Of
course, this is because we tune on test, so we need
to validate this idea on a new test set to see if it leads
to improved predictions (Section 5).
4.2 Skewing to Widen Prediction Range
Our predictor is also too conservative about its dis-
tribution (see Figure 1). It knows (roughly) which
chunks are easier to parse and which are harder, but
its range of predictions is lower than the range of
actual F-measure scores.
We can skew individual predictions so that sen-
tences predicted to be easy are re-predicted to be
even easier (and those that are hard to be even
harder). For each prediction p? (from Equation 1),
we compute
p?? = p? + ?(p? ? f ?p) (4)
We simply set ? to 1.0, doubling the distance
of each prediction p? (in Equation 1) from the (ad-
justed) mean prediction f ?p, to obtain the skewed pre-
diction p??.
Figure 2 shows how the points representing 100-
sentence chunks in Figure 1 look after the predic-
tions have been shifted (? = 0.757) and skewed
(? = 1.0). These two operations have the desired
effect of changing the range of predictions from
[0.89,0.92] to [0.87,0.94], much closer to the actual
892
Sentences
in chunk
(n)
WSJ-test
(rms error)
Brown-test
Prediction
(rms error)
Brown-test
Adjusted
Prediction
(rms error)
1 0.098 0.129 0.139
20 0.026 0.039 0.036
50 0.019 0.032 0.029
100 0.015 0.025 0.020
500 0.011 0.038 0.024
Table 4: Performance of predictor on n-sentence chunks from WSJ-test
and Brown-test (rms error between actual/predicted accuracies).
range of [0.86,0.95]. The points in the new plot (Fig-
ure 2) also align closer to the ?x=y? line than in the
original graph (Figure 1). The rms error also drops
from 0.015 to 0.014 (7% relative reduction), show-
ing that the predictions have improved.
Since we use the WSJ-test corpus to tune the pa-
rameter values for shifting and skewing, we need to
apply our predictor on a different test set to see if we
get similar improvements by using these techniques,
which we do in the next section.
5 Predicting Accuracy on the Brown
Corpus
The Brown corpus represents a genuine challenge
for our predictor, as it presents us with the oppor-
tunity to test the performance of our predictor in
an out-of-domain scenario. Our predictor, trained
on WSJ data, is now employed to predict the per-
formance of a WSJ-trained parser P on the Brown-
test corpus. As in the previous experiments, we use
(Charniak and Johnson, 2005) trained on WSJ sec-
tions 02-21 as parser P . The feature weights for our
predictor are again trained on section 24 of WSJ, and
the shifting and skewing parameters (? = 0.757,
? = 1.0) are determined using section 23 of WSJ.
The results on the Brown-test, both the origi-
nal predictions and after they have been adjusted
(shifted/skewed), are shown in Table 4, at different
level of chunking. For chunks of size n > 1, the
shifting and skewing techniques help in lowering the
rms error. On 100-sentence chunks from the Brown
test, shifting and skewing (? = 0.757, ? = 1.0)
leads to a 20% relative reduction in the rms error.
In a similar vein with the evaluation done in Sec-
tion 4, we are interested in estimating the overall ac-
curacy of a WSJ-trained parser P given an out-of-
domain set such as the Brown test set (for which, at
least for now, we do not have access to gold-standard
System F-measure
Baseline1 (F-measure on WSJ sec. 23) 91.13
Baseline2 (F-measure on WSJ sec. 24) 90.48
Predictor (base) 88.48
Adjusted Predictor (shifting using ? = 0.757) 86.96
Actual accuracy 86.34
Table 5: Charniak parser accuracy on entire Brown-test corpus
trees). If we use (Charniak and Johnson, 2005) as
parser P , a cheap and readily-available answer is
to approximate the performance using the Charniak
parser performance on WSJ section 23, which has
an F-score of 91.13. Another cheap and readily-
available answer is to take the Charniak parser per-
formance on WSJ section 24 with an F-score of
90.48. Table 5 lists these baselines, along with the
prediction made by our system when using a single
chunk containing all the sentences in the Brown test
set (both base predictions and adjusted predictions,
i.e. shifting using ? = 0.757). Again, having gold-
standard trees for the Brown test set helps us decide
which prediction is better. Our predictions are much
closer to the actual Charniak parser performance on
the Brown-test set, with the adjusted prediction at
86.96 compared to the actual F-score of 86.34.
6 Ranking Parser Performance
One of the main goals for computing F-score figures
(either by traditional PARSEVAL evaluation against
gold standards or by methods such as the one pro-
posed in this paper) is to compare parsing accu-
racy when confronted with a choice between vari-
ous parser deployments. Not only are there many
parsing techniques available (Collins, 2003; Char-
niak and Johnson, 2005; Petrov and Klein, 2007;
McClosky et al, 2006; Huang, 2008), but recent
annotation efforts in providing training material for
statistical parsing (LDC, 2005; LDC, 2006a; LDC,
2006b; LDC, 2006c; LDC, 2007) have compounded
the difficulty of the choices (?Do I parse using parser
X??, ?Do I train parser X using the treebank Y or
Z??). In this section, we show how our predictor can
provide guidance when dealing with some of these
choices, namely the choice of the training material
to use with a statistical parser, prior to its applica-
tion in an NLP task.
For the experiments reported in this paper, we
use as parser P , our in-house implementation of
the Collins parser (Collins, 2003), to which various
893
speed-related enhancements (Goodman, 1997) have
been applied. This choice has been made to better
reflect a scenario in which parser P would be used
in a data-intensive application such as syntax-driven
machine translation, in which the parser must be
able to run through hundreds of millions of training
words in a timely manner. We use the more accurate,
but slower Charniak parser (Charniak and Johnson,
2005) as the reference parser Pref in our predictor
(see Section 3.3). In order to predict the Collins-
style parser behavior on the ranking task, we use the
same predictor model (including feature weights and
adjustment parameters) that was used for predicting
Charniak parser behavior on the Brown corpus (Sec-
tion 5).
We compare three training scenarios that make for
three different parsers:
(1) PWSJ - trained on sections 02-21 of WSJ.
(2) PNews - trained on the union of the English
Chinese Translation Treebank (LDC, 2007) (news
stories from Xinhua News Agency translated from
Chinese into English) and the English Newswire
Translation Treebank (LDC, 2005; LDC, 2006a;
LDC, 2006b; LDC, 2006c) (An-Nahar new stories
translated from Arabic into English).
(3) PWSJ?News - trained on the union of all the
above training material.
When comparing the performance of these three
parsers on a development set from WSJ (section 0),
we get the following F-scores.5
Parser WSJ (sec. 0) Accuracy
(F-scores)
PWSJ 88.25
PNews 83.00
PWSJ?News 88.00
Consider now that we are interested in compar-
ing the parsing accuracy of these parsers on a do-
main completely different from WSJ. The ranking
PWSJ>PWSJ?News>PNews, given by the evalua-
tion above, provides some guidance, but is this guid-
ance accurate? The intuition here is that the in-
formation that we already have about the new do-
main of interest (which implicitly appears in texts
5Because of tokenization differences between the different
treebanks involved in these experiments, we have to adopt a to-
kenization scheme different from the one used in the Penn Tree-
bank, and therefore the F-scores, albeit in the same range, are
not directly comparable with the ones in the parsing literature.
Parser Xinhua News
Prediction
(F-scores)
Xinhua News
Accuracy
(F-scores)
PWSJ 85.1 79.14
PNews 87.0 84.84
PWSJ?News 89.4 85.14
Table 6: Performance of predictor on the Xinhua News domain, com-
pared with actual F-scores.
extracted from this domain), can be used to bet-
ter guide this decision. Our predictor is able to
capitalize on this information, and provide domain-
informed guidance for choosing the most accurate
parser to use with the new data, which in this case
relates to choosing the best training strategy for the
parser P . If we consider as our domain of interest,
news stories from Xinhua News Agency, then using
our predictor on a chunk of 1866 sentences from this
domain gives the F-scores shown in the second col-
umn of Table 6.
As with the previous experiments, we can com-
pute the actual PARSEVAL F-scores (using gold-
standard) for this particular 1866-sentence test set,
as it happens to be part of the English Chinese Trans-
lation Treebank (LDC, 2007). These F-score fig-
ures are shown in the third column of Table 6. As
these results show, for this particular domain the cor-
rect ranking is PWSJ?News>PNews>PWSJ , which
is exactly the ranking predicted by our method, with-
out the aid of gold-standard trees.
We observe that even though the system predicts
the ranking correctly, the predictions in the Xinhua
News domain might not be as accurate in compar-
ison to the predictions on Brown corpus (predicted
F-score = 86.96, actual F-score = 86.34). One pos-
sible reason for this lower accuracy is that we use
the same prediction model without optimizing for
the particular parser on which we wish to make pre-
dictions. Still, the model was able to make distinc-
tions between multiple parsers for the ranking task
correctly, and decide the best parser to use with the
given data. We believe this to be useful in typical
NLP applications which use parsing as a component,
and where making the right choice between differ-
ent parsers can affect the end-to-end accuracy of the
system.
7 Conclusion
The steady advances in statistical parsing over the
last years have taken this technology to the point
894
where it is accurate enough to be useful in a va-
riety of natural language applications. However,
due to large variations in the characteristics of the
domains for which these applications are devel-
oped, estimating parsing accuracy becomes more
involved than simply taking for granted accuracy
estimates done on a certain well-studied domain,
such as WSJ. As the results in this paper show, it
is possible to take into account these variations in
the domain characteristics (encoded in our predictor
as text-based, syntax-based, and agreement-based
features)?to make better predictions about the ac-
curacy of certain statistical parsers (and under dif-
ferent training scenarios), instead of relying on accu-
racy estimates done on a standard domain. We have
provided a mechanism to incorporate these domain
variations for making predictions about parsing ac-
curacy, without the costly requirement of creating
human annotations for each of the domains of inter-
est. The experiments shown in the paper were lim-
ited to readily available statistical parsers (which are
widely deployed in a number of applications), and
certain domains/genres (because of ready access to
gold-standard data on which we could verify predic-
tions). However, the features we use in our predic-
tor are independent of the particular type of parser
or domain, and the same technique could be applied
for making predictions on other parsers as well.
There are many avenues for future work opened
up by the work presented here. The accuracy of the
predictor can be further improved by incorporating
more complex syntax-based features and multiple-
agreement features. Moreover, rather than predict-
ing an intrinsic metric such as the PARSEVAL F-
score, the metric that the predictor learns to pre-
dict can be chosen to better fit the final metric on
which an end-to-end system is measured, in the style
of (Och, 2003). The end-result is a finely-tuned tool
for predicting the impact of various parser design de-
cisions on the overall quality of a system.
8 Acknowledgements
We wish to acknowledge our colleagues at ISI, who
provided useful suggestions and constructive criti-
cism on this work. We are also grateful to all the
reviewers for their detailed comments. This work
was supported in part by NSF grant IIS-0428020.
References
Joshua Albrecht and Rebecca Hwa. 2007. Regression for
sentence-level mt evaluation with pseudo references.
In Proc. of ACL.
Eleftherios Avramidis and Philipp Koehn. 2008. Enrich-
ing morphologically poor languages for statistical ma-
chine translation. In Proc. of ACL.
Michiel Bacchiani, Michael Riley, Brian Roark, and
Richard Sproat. 2006. MAP adaptation of stochastic
grammars. Computer Speech & Language, 20(1).
Daniel M. Bikel. 2002. Design of a multi-lingual,
parallel-processing statistical parsing engine. In Proc.
of HLT.
E. Black, S. Abney, D. Flickinger, C. Gdaniec, R. Gr-
ishman, P. Harrison, D. Hindle, R. Ingria, F. Jelinek,
J. Klavans, M. Liberman, M. Marcus, S. Roukos,
B. Santorini, and T. Strzalkowski. 1991. A proce-
dure for quantitatively comparing the syntactic cover-
age of english grammars. In Proc. of Speech and Nat-
ural Language Workshop.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and MaxEnt discriminative rerank-
ing. In Proc. of ACL.
Eugene Charniak, Kevin Knight, and Kenji Yamada.
2003. Syntax-based language models for statistical
machine translation. In Proc. of MT Summit IX. IAMT.
Ciprian Chelba and Frederick Jelinek. 1998. Exploiting
syntactic structure for language modeling. In Proc. of
ACL.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause restructuring for statistical machine
translation. In Proc. of ACL.
Michael Collins. 2003. Head-driven statistical models
for natural language parsing. Computational Linguis-
tics, 29(4).
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Proc.
of HLT/NAACL.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inferences and training of
context-rich syntax translation models. In Proc. of
ACL.
Daniel Gildea. 2001. Corpus variation and parser perfor-
mance. In Proc. of EMNLP.
Joshua Goodman. 1997. Global thresholding and
multiple-pass parsing. In Proc. of EMNLP.
Ulf Hermjakob. 2001. Parsing and question classifica-
tion for question answering. In Proc. of ACL Work-
shop on Open-Domain Question Answering.
Ayako Hoshino and Hiroshi Nakagawa. 2007. A cloze
test authoring system and its automation. In Proc. of
ICWL.
895
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proc. of AMTA.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proc. of ACL.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proc. of ACL.
LDC. 2005. English newswire translation tree-
bank. Linguistic Data Consortium, Catalog number
LDC2005E85.
LDC. 2006a. English newswire translation tree-
bank. Linguistic Data Consortium, Catalog number
LDC2006E36.
LDC. 2006b. GALE Y1 Q3 release - English translation
treebank. Linguistic Data Consortium, Catalog num-
ber LDC2006E82.
LDC. 2006c. GALE Y1 Q4 release - English translation
treebank. Linguistic Data Consortium, Catalog num-
ber LDC2006E95.
LDC. 2007. English chinese translation tree-
bank. Linguistic Data Consortium, Catalog number
LDC2007T02.
Ding Liu and Daniel Gildea. 2007. Source-language fea-
tures and maximum correlation training for machine
translation evaluation. In Proc. of NAACL-HLT.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and
Kevin Knight. 2006. Spmt: Statistical machine trans-
lation with syntactified target language phraases. In
Proc. of EMNLP.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2).
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Reranking and self-training for parser adapta-
tion. In Proc. of COLING-ACL.
Franz Joseph Och. 2003. Minimum error rate training in
machine translation. In Proc. of ACL.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proc. of HLT/NAACL.
Brian Roark. 2001. Probabilistic top-down parsing
and language modelling. Computational Linguistics,
27(2).
A.J. Smola and B. Schoelkopf. 1998. A tutorial on sup-
port vector regression. NeuroCOLT2 Technical Report
NC2-TR-1998-030.
Radu Soricut. 2006. Natural Language Generation us-
ing an Information-Slim Representation. Ph.D. thesis,
University of Southern California,.
Y. Yang and J. Pedersen. 1997. A comparative study
on feature selection in text categorization. In Proc. of
ICML.
896
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 37?45,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Learning Phoneme Mappings for Transliteration without Parallel Data
Sujith Ravi and Kevin Knight
University of Southern California
Information Sciences Institute
Marina del Rey, California 90292
{sravi,knight}@isi.edu
Abstract
We present a method for performing machine
transliteration without any parallel resources.
We frame the transliteration task as a deci-
pherment problem and show that it is possi-
ble to learn cross-language phoneme mapping
tables using only monolingual resources. We
compare various methods and evaluate their
accuracies on a standard name transliteration
task.
1 Introduction
Transliteration refers to the transport of names and
terms between languages with different writing sys-
tems and phoneme inventories. Recently there has
been a large amount of interesting work in this
area, and the literature has outgrown being citable
in its entirety. Much of this work focuses on back-
transliteration, which tries to restore a name or
term that has been transported into a foreign lan-
guage. Here, there is often only one correct target
spelling?for example, given jyon.kairu (the
name of a U.S. Senator transported to Japanese), we
must output ?Jon Kyl?, not ?John Kyre? or any other
variation.
There are many techniques for transliteration and
back-transliteration, and they vary along a number
of dimensions:
? phoneme substitution vs. character substitution
? heuristic vs. generative vs. discriminative mod-
els
? manual vs. automatic knowledge acquisition
We explore the third dimension, where we see
several techniques in use:
? Manually-constructed transliteration models,
e.g., (Hermjakob et al, 2008).
? Models constructed from bilingual dictionaries
of terms and names, e.g., (Knight and Graehl,
1998; Huang et al, 2004; Haizhou et al, 2004;
Zelenko and Aone, 2006; Yoon et al, 2007;
Li et al, 2007; Karimi et al, 2007; Sherif
and Kondrak, 2007b; Goldwasser and Roth,
2008b).
? Extraction of parallel examples from bilin-
gual corpora, using bootstrap dictionaries e.g.,
(Sherif and Kondrak, 2007a; Goldwasser and
Roth, 2008a).
? Extraction of parallel examples from compara-
ble corpora, using bootstrap dictionaries, and
temporal and word co-occurrence, e.g., (Sproat
et al, 2006; Klementiev and Roth, 2008).
? Extraction of parallel examples from web
queries, using bootstrap dictionaries, e.g., (Na-
gata et al, 2001; Oh and Isahara, 2006; Kuo et
al., 2006; Wu and Chang, 2007).
? Comparing terms from different languages in
phonetic space, e.g., (Tao et al, 2006; Goldberg
and Elhadad, 2008).
In this paper, we investigate methods to acquire
transliteration mappings from non-parallel sources.
We are inspired by previous work in unsupervised
learning for natural language, e.g. (Yarowsky, 1995;
37
WFSA - A WFST - B
English word 
sequence
English sound 
sequence
( SPENCER ABRAHAM ) ( S P EH N S ER  EY B R AH HH AE M )
WFST - C WFST - D
Japanese sound 
sequence
( ? ? ? ? ? ? ? ? ? ? ? ? )
Japanese katakana 
sequence
( S U P E N S A A  
E E B U R A H A M U )
Figure 1: Model used for back-transliteration of Japanese katakana names and terms into English. The model employs
a four-stage cascade of weighted finite-state transducers (Knight and Graehl, 1998).
Goldwater and Griffiths, 2007), and we are also in-
spired by cryptanalysis?we view a corpus of for-
eign terms as a code for English, and we attempt to
break the code.
2 Background
We follow (Knight and Graehl, 1998) in tackling
back-transliteration of Japanese katakana expres-
sions into English. Knight and Graehl (1998) devel-
oped a four-stage cascade of finite-state transducers,
shown in Figure 1.
? WFSA A - produces an English word sequence
w with probability P(w) (based on a unigram
word model).
? WFST B - generates an English phoneme se-
quence e corresponding to w with probability
P(e|w).
? WFST C - transforms the English phoneme se-
quence into a Japanese phoneme sequence j ac-
cording to a model P(j|e).
? WFST D - writes out the Japanese phoneme
sequence into Japanese katakana characters ac-
cording to a model P(k|j).
Using the cascade in the reverse (noisy-channel)
direction, they are able to translate new katakana
names and terms into English. They report 36% er-
ror in translating 100 U.S. Senators? names, and they
report exceeding human transliteration performance
in the presence of optical scanning noise.
The only transducer that requires parallel training
data is WFST C. Knight and Graehl (1998) take sev-
eral thousand phoneme string pairs, automatically
align them with the EM algorithm (Dempster et
al., 1977), and construct WFST C from the aligned
phoneme pieces.
We re-implement their basic method by instanti-
ating a densely-connected version of WFST C with
all 1-to-1 and 1-to-2 phoneme connections between
English and Japanese. Phoneme bigrams that occur
fewer than 10 times in a Japanese corpus are omit-
ted, and we omit 1-to-3 connections. This initial
WFST C model has 15320 uniformly weighted pa-
rameters. We then train the model on 3343 phoneme
string pairs from a bilingual dictionary, using the
EM algorithm. EM immediately reduces the con-
nections in the model to those actually observed in
the parallel data, and after 14 iterations, there are
only 188 connections left with P(j|e) ? 0.01. Fig-
ure 2 shows the phonemic substitution table learnt
from parallel training.
We use this trained WFST C model and apply it
to the U.S. Senator name transliteration task (which
we update to the 2008 roster). We obtain 40% er-
ror, roughly matching the performance observed in
(Knight and Graehl, 1998).
3 Task and Data
The task of this paper is to learn the mappings in
Figure 2, but without parallel data, and to test those
mappings in end-to-end transliteration. We imagine
our problem as one faced by monolingual English
speaker wandering around Japan, reading a multi-
tude of katakana signs, listening to people speak
Japanese, and eventually deciphering those signs
into English. To mis-quote Warren Weaver:
?When I look at a corpus of Japanese
katakana, I say to myself, this is really
written in English, but it has been coded
in some strange symbols. I will now pro-
ceed to decode.?
Our larger motivation is to move toward
easily-built transliteration systems for all language
pairs, regardless of parallel resources. While
Japanese/English transliteration has its own partic-
ular features, we believe it is a reasonable starting
point.
38
e j P(j|e) e j P(j|e) e j P(j|e) e j P(j|e) e j P(j|e) e j P(j|e) e j P(j|e) e j P(j|e)
AA o 0.49 AY a i 0.84 EH e 0.94 HH h 0.95 L r 0.62 OY o i 0.89 SH sh y 0.33 V b 0.75a 0.46 i 0.09 a 0.03 w 0.02 r u 0.37 o e 0.04 sh 0.31 b u 0.17o o 0.02 a 0.03 h a 0.02 o 0.04 y u 0.17 w 0.03a a 0.02 i y 0.01 i 0.04 ssh y 0.12 a 0.02a y 0.01 sh i 0.04ssh 0.02e 0.01AE a 0.93 B b 0.82 ER a a 0.8 IH i 0.89 M m 0.68 P p 0.63 T t 0.43 W w 0.73a ssh 0.02 b u 0.15 a 0.08 e 0.05 m u 0.22 p u 0.16 t o 0.25 u 0.17a n 0.02 a r 0.03 i n 0.01 n 0.08 pp u 0.13 tt o 0.17 o 0.04r u 0.02 a 0.01 pp 0.06 ts 0.04 i 0.02o r 0.02 tt 0.03e r 0.02 u 0.02ts u 0.02ch 0.02AH a 0.6 CH tch i 0.27 EY e e 0.58 IY i i 0.58 N n 0.96 PAUSE pause 1.0 TH s u 0.48 Y y 0.7o 0.13 ch 0.24 e 0.15 i 0.3 nn 0.02 s 0.22 i 0.26e 0.11 ch i 0.23 e i 0.12 e 0.07 sh 0.16 e 0.02i 0.07 ch y 0.2 a 0.1 e e 0.03 t o 0.04 a 0.02u 0.06 tch y 0.02 a i 0.03 ch 0.04tch 0.02 t e 0.02ssh y 0.01 t 0.02k 0.01 a 0.02AO o 0.6 D d 0.54 F h 0.58 JH j y 0.35 NG n 0.62 R r 0.61 UH u 0.79 Z z 0.27o o 0.27 d o 0.27 h u 0.35 j 0.24 g u 0.22 a 0.27 u u 0.09 z u 0.25a 0.05 dd o 0.06 hh 0.04 j i 0.21 n g 0.09 o 0.07 u a 0.04 u 0.16o n 0.03 z 0.02 hh u 0.02 jj i 0.14 i 0.04 r u 0.03 dd 0.03 s u 0.07a u 0.03 j 0.02 z 0.04 u 0.01 a a 0.01 u ssh 0.02 j 0.06u 0.01 u 0.01 o 0.01 o 0.02 a 0.06a 0.01 n 0.03i 0.03s 0.02o 0.02AW a u 0.69 DH z 0.87 G g 0.66 K k 0.53 OW o 0.57 S s u 0.43 UW u u 0.67 ZH j y 0.43a w 0.15 z u 0.08 g u 0.19 k u 0.2 o o 0.39 s 0.37 u 0.29 j i 0.29a o 0.06 a z 0.04 gg u 0.1 kk u 0.16 o u 0.02 sh 0.08 y u 0.02 j 0.29a 0.04 g y 0.03 kk 0.05 u 0.05u u 0.02 gg 0.01 k y 0.02 ss 0.02o o 0.02 g a 0.01 k i 0.01 ssh 0.01o 0.02
Figure 2: Phonemic substitution table learnt from 3343 parallel English/Japanese phoneme string pairs. English
phonemes are in uppercase, Japanese in lowercase. Mappings with P(j|e) > 0.01 are shown.
A A CH I D O CH E N J I N E B A D A W A K O B I A
A A K U P U R A Z A CH E S : W A N K A PP U
A A N D O : O P U T I K U S U W A N T E N P O
A A T I S U T O D E K O R A T I B U : W A S E R I N
A A T O S E R I N A P I S U T O N D E T O M O R U T O P I I T A A Y U N I O N
A I A N B I R U E P I G U R A M U P I KK A A Y U N I TT O SH I S U T E M U
A I D I I D O E R A N D O P I N G U U Y U U
A I K E N B E R I I : P I P E R A J I N A M I D O :
A J I A K A PP U J Y A I A N TS U P I S A :
A J I T O J Y A Z U P I U R A Z E N E R A R U E A K O N
A K A SH I A K O O S U : P O I N T O Z E R O
A K U A M Y U U Z E U M U : Z O N B I I Z U
: : : :
: : : :
Figure 3: Some Japanese phoneme sequences generated from the monolingual katakana corpus using WFST D.
Our monolingual resources are:
? 43717 unique Japanese katakana sequences
collected from web newspaper data. We split
multi-word katakana phrases on the center-dot
(???) character, and select a final corpus of
9350 unique sequences. We add monolingual
Japanese versions of the 2008 U.S. Senate ros-
ter.1
? The CMU pronunciation dictionary of English,
1We use ?open? EM testing, in which unlabeled test data
is allowed to be part of unsupervised training. However, no
parallel data is allowed.
with 112,151 entries.
? The English gigaword corpus. Knight and
Graehl (1998) already use frequently-occurring
capitalized words to build the WFSA A compo-
nent of their four-stage cascade.
We seek to use our English knowledge (derived
from 2 and 3) to decipher the Japanese katakana cor-
pus (1) into English. Figure 3 shows a portion of the
Japanese corpus, which we transform into Japanese
phoneme sequences using the monolingual resource
of WFST D. We note that the Japanese phoneme in-
ventory contains 39 unique (?ciphertext?) symbols,
39
compared to the 40 English (?plaintext?) phonemes.
Our goal is to compare and evaluate the WFST C
model learnt under two different scenarios?(a) us-
ing parallel data, and (b) using monolingual data.
For each experiment, we train only the WFST C
model and then apply it to the name translitera-
tion task?decoding 100 U.S. Senator names from
Japanese to English using the automata shown in
Figure 1. For all experiments, we keep the rest of
the models in the cascade (WFSA A, WFST B, and
WFST D) unchanged. We evaluate on whole-name
error-rate (maximum of 100/100) as well as normal-
ized word edit distance, which gives partial credit
for getting the first or last name correct.
4 Acquiring Phoneme Mappings from
Non-Parallel Data
Our main data consists of 9350 unique Japanese
phoneme sequences, which we can consider as a sin-
gle long sequence j. As suggested by Knight et
al (2006), we explain the existence of j as the re-
sult of someone initially producing a long English
phoneme sequence e, according to P(e), then trans-
forming it into j, according to P(j|e). The probabil-
ity of our observed data P(j) can be written as:
P (j) =?
e
P (e) ? P (j|e)
We take P(e) to be some fixed model of mono-
lingual English phoneme production, represented
as a weighted finite-state acceptor (WFSA). P(j|e)
is implemented as the initial, uniformly-weighted
WFST C described in Section 2, with 15320 phone-
mic connections.
We next maximize P(j) by manipulating the sub-
stitution table P(j|e), aiming to produce a result
such as shown in Figure 2. We accomplish this by
composing the English phoneme model P(e) WFSA
with the P(j|e) transducer. We then use the EM al-
gorithm to train just the P(j|e) parameters (inside
the composition that predicts j), and guess the val-
ues for the individual phonemic substitutions that
maximize the likelihood of the observed data P(j).2
2In our experiments, we use the Carmel finite-state trans-
ducer package (Graehl, 1997), a toolkit with an algorithm for
EM training of weighted finite-state transducers.
We allow EM to run until the P(j) likelihood ra-
tio between subsequent training iterations reaches
0.9999, and we terminate early if 200 iterations are
reached.
Finally, we decode our test set of U.S. Senator
names. Following Knight et al(2006), we stretch
out the P(j|e) model probabilities after decipher-
ment training and prior to decoding our test set, by
cubing their values.
Decipherment under the conditions of translit-
eration is substantially more difficult than solv-
ing letter-substitution ciphers (Knight et al, 2006;
Ravi and Knight, 2008; Ravi and Knight, 2009) or
phoneme-substitution ciphers (Knight and Yamada,
1999). This is because the target table contains sig-
nificant non-determinism, and because each symbol
has multiple possible fertilities, which introduces
uncertainty about the length of the target string.
4.1 Baseline P(e) Model
Clearly, we can design P(e) in a number of ways. We
might expect that the more the system knows about
English, the better it will be able to decipher the
Japanese. Our baseline P(e) is a 2-gram phoneme
model trained on phoneme sequences from the CMU
dictionary. The second row (2a) in Figure 4 shows
results when we decipher with this fixed P(e). This
approach performs poorly and gets all the Senator
names wrong.
4.2 Consonant Parity
When training under non-parallel conditions, we
find that we would like to keep our WFST C model
small, rather than instantiating a fully-connected
model. In the supervised case, parallel training al-
lows the trained model to retain only those con-
nections which were observed from the data, and
this helps eliminate many bad connections from the
model. In the unsupervised case, there is no parallel
data available to help us make the right choices.
We therefore use prior knowledge and place a
consonant-parity constraint on the WFST C model.
Prior to EM training, we throw out any mapping
from the P(j|e) substitution model that does not
have the same number of English and Japanese con-
sonant phonemes. This is a pattern that we observe
across a range of transliteration tasks. Here are ex-
40
Phonemic Substitution Model Name Transliteration Error
whole-name error norm. edit distance
1 e ? j = { 1-to-1, 1-to-2 } 40 25.9
+ EM aligned with parallel data
2a e ? j = { 1-to-1, 1-to-2 } 100 100.0
+ decipherment training with 2-gram English P(e)
2b e ? j = { 1-to-1, 1-to-2 } 98 89.8
+ decipherment training with 2-gram English P(e)
+ consonant-parity
2c e ? j = { 1-to-1, 1-to-2 } 94 73.6
+ decipherment training with 3-gram English P(e)
+ consonant-parity
2d e ? j = { 1-to-1, 1-to-2 } 77 57.2
+ decipherment training with a word-based English model
+ consonant-parity
2e e ? j = { 1-to-1, 1-to-2 } 73 54.2
+ decipherment training with a word-based English model
+ consonant-parity
+ initialize mappings having consonant matches with higher proba-
bility weights
Figure 4: Results on name transliteration obtained when using the phonemic substitution model trained under different
scenarios?(1) parallel training data, (2a-e) using only monolingual resources.
amples of mappings where consonant parity is vio-
lated:
K => a N => e e
EH => s a EY => n
Modifying the WFST C in this way leads to bet-
ter decipherment tables and slightly better results
for the U.S. Senator task. Normalized edit distance
drops from 100 to just under 90 (row 2b in Figure 4).
4.3 Better English Models
Row 2c in Figure 4 shows decipherment results
when we move to a 3-gram English phoneme model
for P(e). We notice considerable improvements in
accuracy. On the U.S. Senator task, normalized edit
distance drops from 89.8 to 73.6, and whole-name
error decreases from 98 to 94.
When we analyze the results from deciphering
with a 3-gram P(e) model, we find that many of the
Japanese phoneme test sequences are decoded into
English phoneme sequences (such as ?IH K R IH
N? and ?AE G M AH N?) that are not valid words.
This happens because the models we used for de-
cipherment so far have no knowledge of what con-
stitutes a globally valid English sequence. To help
the phonemic substitution model learn this infor-
mation automatically, we build a word-based P(e)
from English phoneme sequences in the CMU dic-
tionary and use this model for decipherment train-
ing. The word-based model produces complete En-
glish phoneme sequences corresponding to 76,152
actual English words from the CMU dictionary.
The English phoneme sequences are represented as
paths through a WFSA, and all paths are weighted
equally. We represent the word-based model in com-
pact form, using determinization and minimization
techniques applicable to weighted finite-state au-
tomata. This allows us to perform efficient EM train-
ing on the cascade of P(e) and P(j|e) models. Under
this scheme, English phoneme sequences resulting
from decipherment are always analyzable into actual
words.
Row 2d in Figure 4 shows the results we ob-
tain when training our WFST C with a word-based
English phoneme model. Using the word-based
model produces the best result so far on the phone-
mic substitution task with non-parallel data. On the
U.S. Senator task, word-based decipherment outper-
forms the other methods by a large margin. It gets
23 out of 100 Senator names exactly right, with a
much lower normalized edit distance (57.2). We
have managed to achieve this performance using
only monolingual data. This also puts us within
reach of the parallel-trained system?s performance
(40% whole-name errors, and 25.9 word edit dis-
tance error) without using a single English/Japanese
pair for training.
To summarize, the quality of the English phoneme
41
e j P(j|e) e j P(j|e) e j P(j|e) e j P(j|e) e j P(j|e) e j P(j|e) e j P(j|e) e j P(j|e)
AA a 0.37 AY a i 0.36 EH e 0.37 HH h 0.45 L r 0.3 OY a 0.27 SH sh y 0.22 V b 0.34o 0.25 o o 0.13 a 0.24 s 0.12 n 0.19 i 0.16 m 0.11 k 0.14i 0.15 e 0.12 o 0.12 k 0.09 r u 0.15 y u 0.1 r 0.1 m 0.13u 0.08 i 0.11 i 0.12 b 0.08 r i 0.04 o i 0.1 s 0.06 s 0.07e 0.07 a 0.11 u 0.06 m 0.07 t 0.03 y a 0.09 p 0.06 d 0.07o o 0.03 u u 0.05 o o 0.04 w 0.03 m u 0.02 y o 0.08 s a 0.05 r 0.04y a 0.01 y u 0.02 y u 0.01 p 0.03 m 0.02 e 0.08 h 0.05 t 0.03a a 0.01 u 0.02 a i 0.01 g 0.03 w a 0.01 o 0.06 b 0.05 h 0.02o 0.02 k y 0.02 t a 0.01 o o 0.02 t 0.04 sh 0.01e e 0.02 d 0.02 r a 0.01 e i 0.02 k 0.04 n 0.01AE a 0.52 B b 0.41 ER a a 0.47 IH i 0.36 M m 0.3 P p 0.18 T t 0.2 W w 0.23i 0.19 p 0.12 a 0.17 e 0.25 n 0.08 p u 0.08 t o 0.16 r 0.2e 0.11 k 0.09 u 0.08 a 0.15 k 0.08 n 0.05 t a 0.05 m 0.13o 0.08 m 0.07 o 0.07 u 0.09 r 0.07 k 0.05 n 0.04 s 0.08u 0.03 s 0.04 e 0.04 o 0.09 s 0.06 sh i 0.04 k u 0.03 k 0.07u u 0.02 g 0.04 o o 0.03 o o 0.01 h 0.05 k u 0.04 k 0.03 h 0.06o o 0.02 t 0.03 i i 0.03 t 0.04 s u 0.03 t e 0.02 b 0.06z 0.02 y u 0.02 g 0.04 p a 0.03 s 0.02 t 0.04d 0.02 u u 0.02 b 0.04 t 0.02 r 0.02 p 0.04ch y 0.02 i 0.02 m u 0.03 m a 0.02 g u 0.02 d 0.02AH a 0.31 CH g 0.12 EY e e 0.3 IY i 0.25 N n 0.56 PAUSE pause 1.0 TH k 0.21 Y s 0.25o 0.23 k 0.11 a 0.22 i i 0.21 r u 0.09 p u 0.11 k 0.18i 0.17 b 0.09 i 0.11 a 0.15 s u 0.04 k u 0.1 m 0.07e 0.12 sh 0.07 u 0.09 a a 0.12 m u 0.02 d 0.08 g 0.06u 0.1 s 0.07 o 0.06 u 0.07 kk u 0.02 h u 0.07 p 0.05e e 0.02 r 0.07 e 0.06 o 0.05 k u 0.02 s u 0.05 b 0.05o o 0.01 ch y 0.07 o o 0.05 o o 0.02 h u 0.02 b u 0.04 r 0.04a a 0.01 p 0.06 e i 0.04 i a 0.02 t o 0.01 k o 0.03 d 0.04m 0.06 i i 0.02 e e 0.02 pp u 0.01 g a 0.03 u r 0.03ch 0.06 u u 0.01 e 0.02 b i 0.01 s a 0.02 n y 0.03AO o 0.29 D d 0.16 F h 0.18 JH b 0.13 NG tt o 0.21 R r 0.53 UH a 0.24 Z t o 0.14a 0.26 d o 0.15 h u 0.14 k 0.1 r u 0.17 n 0.07 o 0.14 z u 0.11e 0.14 n 0.05 b 0.09 j y 0.1 n 0.14 u r 0.05 e 0.11 r u 0.11o o 0.12 t o 0.03 sh i 0.07 s 0.08 kk u 0.1 r i 0.03 y u 0.1 s u 0.1i 0.08 sh i 0.03 p 0.07 m 0.08 s u 0.07 r u 0.02 a i 0.09 g u 0.09u 0.05 k u 0.03 m 0.06 t 0.07 m u 0.06 d 0.02 i 0.08 m u 0.07y u 0.03 k 0.03 r 0.04 j 0.07 dd o 0.04 t 0.01 u u 0.07 n 0.06e e 0.01 g u 0.03 s 0.03 h 0.07 tch i 0.03 s 0.01 o o 0.07 d o 0.06b 0.03 h a 0.03 sh 0.06 pp u 0.03 m 0.01 a a 0.03 j i 0.02s 0.02 b u 0.02 d 0.05 jj i 0.03 k 0.01 u 0.02 ch i 0.02AW o o 0.2 DH h 0.13 G g u 0.13 K k 0.17 OW a 0.3 S s u 0.4 UW u 0.39 ZH m 0.17a u 0.19 r 0.12 g 0.11 n 0.1 o 0.25 n 0.11 a 0.15 p 0.16a 0.18 b 0.09 k u 0.08 k u 0.1 o o 0.12 r u 0.05 o 0.13 t 0.15a i 0.11 w 0.08 b u 0.06 kk u 0.05 u 0.09 t o 0.03 u u 0.12 h 0.13a a 0.11 t 0.07 k 0.04 t o 0.03 i 0.07 k u 0.03 i 0.04 d 0.1e 0.05 p 0.07 b 0.04 s u 0.03 y a 0.04 sh i 0.02 y u 0.03 s 0.08o 0.04 g 0.06 t o 0.03 sh i 0.02 e 0.04 r i 0.02 i i 0.03 b 0.07i 0.04 j y 0.05 t 0.03 r 0.02 u u 0.02 m u 0.02 e 0.03 r 0.05i y 0.02 d 0.05 h a 0.03 k o 0.02 a i 0.02 h u 0.02 o o 0.02 j y 0.03e a 0.01 k 0.03 d 0.03 k a 0.02 i i 0.01 ch i 0.02 e e 0.02 k 0.02
Figure 5: Phonemic substitution table learnt from non-parallel corpora. For each English phoneme, only the top ten
mappings with P(j|e) > 0.01 are shown.
model used in decipherment training has a large ef-
fect on the learnt P(j|e) phonemic substitution ta-
ble (i.e., probabilities for the various phoneme map-
pings within the WFST C model), which in turn af-
fects the quality of the back-transliterated English
output produced when decoding Japanese.
Figure 5 shows the phonemic substitution table
learnt using word-based decipherment. The map-
pings are reasonable, given the lack of parallel data.
They are not entirely correct?for example, the map-
ping ?S? s u? is there, but ?S? s? is missing.
Sample end-to-end transliterations are illustrated
in Figure 6. The figure shows how the transliteration
results from non-parallel training improve steadily
as we use stronger decipherment techniques. We
note that in one case (LAUTENBERG), the deci-
pherment mapping table leads to a correct answer
where the mapping table derived from parallel data
does not. Because parallel data is limited, it may not
contain all of the necessary mappings.
4.4 Size of Japanese Training Data
Monolingual corpora are more easily available than
parallel corpora, so we can use increasing amounts
of monolingual Japanese training data during de-
cipherment training. The table below shows that
using more Japanese training data produces bet-
ter transliteration results when deciphering with the
word-based English model.
Japanese training data Error on name transliteration task
(# of phoneme sequences) whole-name error normalized word
edit distance
4,674 87 69.7
9,350 77 57.2
42
? ? ?????????????????????????
?????? ?????? ???? ???????? ?????
????????
??????
????????????
??????
????????????
??????
???????????
???? ?
????
????
?????
???????
?????
???????
????
?????
??????? ?????
???????
????
???
???????? ???? ???? ??????? ????????? ??????????
????
????
??????????? ?????????? ? ?????? ???? ??? ???????????
????
????
? ??? ?? ????? ?? ?????? ?? ???? ?? ????
????? ????????? ???????? ???????? ?????????? ??????????
????
????
???????? ??????? ???????? ?????????? ????????
????
????
?????????? ?????????? ????????? ??? ??????? ??? ???????
????
?????
?????
??????
????
??? ??
??????? ??????
?? ?????
??????
?? ?????
?????????????????????????????????????????????????????????
?????? ?????? ???? ???????? ?????
????????
??????
????????????
??????
????????????
??????
???????????
???? ?
??????? ??????? ? ???????????? ????????? ??????? ???????????
??????? ???????? ???? ???? ??????? ????????? ??????????
???? ??????????? ?????????? ? ?????? ???? ??? ???????????
???????? ? ??? ?? ????? ?? ?????? ?? ???? ?? ????
????? ????????? ???????? ???????? ?????????? ??????????
?????? ??? ???? ??????? ???????? ?????????? ??? ????
???????? ?????????? ?????????? ????????? ??? ??????? ??? ???????
??????? ?? ???????? ??????? ?? ??????? ? ?????? ????? ? ?????? ?????
??????????????????????????????
?????? ????????????????????
?????? ?????? ???? ???????? ?????
????????
??????
????????????
??????
????????????
??????
???????????
???? ?
????
?????
?? ????
?????
???????
????
?????
??????? ?????
???????
??
???
???????? ? ?? ???? ??????? ????????? ??????????
?
????
? ????????? ?? ?????? ? ?????? ???? ??? ???????????
????
?
? ??? ?? ????? ?? ?????? ?? ???? ?? ????
?? ?? ??? ? ??? ?? ? ?????? ? ?????????? ??????????
???
????
??? ??? ??? ?? ? ?? ????? ?????????? ????????
???
?? ? ? ?? ?? ????? ?? ?????? ??? ??????? ??? ???????
??
?????
? ??
??????
? ?
??? ??
??? ? ? ??????
?? ?????
??????
?? ?????
?????????????????????????????????????????????????????????
?????? ?????? ???? ???????? ?????
????????
??????
????????????
??????
????????????
??????
???????????
???? ?
?????? ??????? ? ??????????? ????????? ??????? ???????????
????? ???????? ???? ? ?? ??????? ????????? ??????????
?? ??????????? ?????????? ? ???? ? ???? ??? ???????????
?????? ? ??? ?? ????? ?? ?????? ?? ???? ?? ????
????? ????????? ??????? ? ???? ? ?????????? ??????????
??????? ??? ???? ?????? ???????? ?????????? ??? ????
??????? ?????????? ????????? ? ??? ?? ??? ??????? ??? ???????
??????? ?? ???????? ??????? ?? ??????? ? ?????? ????? ? ?????? ?????
??????????????????????????????
Figure 6: Results for end-to-end name transliteration. This figure shows the correct answer, the answer obtained
by training mappings on parallel data (Knight and Graehl, 1998), and various answers obtained by deciphering non-
parallel data. Method 1 uses a 2-gram P(e), Method 2 uses a 3-gram P(e), and Method 3 uses a word-based P(e).
4.5 P(j|e) Initialization
So far, the P(j|e) connections within the WFST C
model were initialized with uniform weights prior
to EM training. It is a known fact that the EM al-
gorithm does not necessarily find a global minimum
for the given objective function. If the search space
is bumpy and non-convex as is the case in our prob-
lem, EM can get stuck in any of the local minima
depending on what weights were used to initialize
the search. Different sets of initialization weights
can lead to different convergence points during EM
training, or in other words, depending on how the
P(j|e) probabilities are initialized, the final P(j|e)
substitution table learnt by EM can vary.
We can use some prior knowledge to initialize the
probability weights in our WFST C model, so as to
give EM a good starting point to work with. In-
stead of using uniform weights, in the P(j|e) model
we set higher weights for the mappings where En-
glish and Japanese sounds share common consonant
phonemes.
For example, mappings such as:
N => n N => a n
D => d D => d o
are weighted X (a constant) times higher than
other mappings such as:
N => b N => r
D => B EY => a a
in the P(j|e) model. In our experiments, we set
the value X to 100.
Initializing the WFST C in this way results in EM
learning better substitution tables and yields slightly
better results for the Senator task. Normalized edit
distance drops from 57.2 to 54.2, and the whole-
name error is also reduced from 77% to 73% (row
2e in Figure 4).
4.6 Size of English Training Data
We saw earlier (in Section 4.4) that using more
monolingual Japanese training data yields improve-
ments in decipherment results. Similarly, we hy-
pothesize that using more monolingual English data
can drive the decipherment towards better translit-
eration results. On the English side, we build dif-
ferent word-based P(e) models, each trained on dif-
ferent amounts of data (English phoneme sequences
from the CMU dictionary). The table below shows
that deciphering with a word-based English model
43
built from more data produces better transliteration
results.
English training data Error on name transliteration task
(# of phoneme sequences) whole-name error normalized word
edit distance
76,152 73 54.2
97,912 66 49.3
This yields the best transliteration results on the
Senator task with non-parallel data, getting 34 out
of 100 Senator names exactly right.
4.7 Re-ranking Results Using the Web
It is possible to improve our results on the U.S. Sen-
ator task further using external monolingual re-
sources. Web counts are frequently used to auto-
matically re-rank candidate lists for various NLP
tasks (Al-Onaizan and Knight, 2002). We extract
the top 10 English candidates produced by our word-
based decipherment method for each Japanese test
name. Using a search engine, we query the entire
English name (first and last name) corresponding to
each candidate, and collect search result counts. We
then re-rank the candidates using the collected Web
counts and pick the most frequent candidate as our
choice.
For example, France Murkowski gets only 1 hit
on Google, whereas Frank Murkowski gets 135,000
hits. Re-ranking the results in this manner lowers
the whole-name error on the Senator task from 66%
to 61%, and also lowers the normalized edit dis-
tance from 49.3 to 48.8. However, we do note that
re-ranking using Web counts produces similar im-
provements in the case of parallel training as well
and lowers the whole-name error from 40% to 24%.
So, the re-ranking idea, which is simple and re-
quires only monolingual resources, seems like a nice
strategy to apply at the end of transliteration exper-
iments (during decoding), and can result in further
gains on the final transliteration performance.
5 Comparable versus Non-Parallel
Corpora
We also present decipherment results when using
comparable corpora for training the WFST C model.
We use English and Japanese phoneme sequences
derived from a parallel corpus containing 2,683
phoneme sequence pairs to construct comparable
corpora (such that for each Japanese phoneme se-
quence, the correct back-transliterated phoneme se-
quence is present somewhere in the English data)
and apply the same decipherment strategy using a
word-based English model. The table below com-
pares the transliteration results for the U.S. Sena-
tor task, when using comparable versus non-parallel
data for decipherment training. While training on
comparable corpora does have benefits and reduces
the whole-name error to 59% on the Senator task, it
is encouraging to see that our best decipherment re-
sults using only non-parallel data comes close (66%
error).
English/Japanese Corpora Error on name transliteration task
(# of phoneme sequences) whole-name error normalized word
edit distance
Comparable Corpora 59 41.8
(English = 2,608
Japanese = 2,455)
Non-Parallel Corpora 66 49.3
(English = 98,000
Japanese = 9,350)
6 Conclusion
We have presented a method for attacking machine
transliteration problems without parallel data. We
developed phonemic substitution tables trained us-
ing only monolingual resources and demonstrated
their performance in an end-to-end name translitera-
tion task. We showed that consistent improvements
in transliteration performance are possible with the
use of strong decipherment techniques, and our best
system achieves significant improvements over the
baseline system. In future work, we would like to
develop more powerful decipherment models and
techniques, and we would like to harness the infor-
mation available from a wide variety of monolingual
resources, and use it to further narrow the gap be-
tween parallel-trained and non-parallel-trained ap-
proaches.
7 Acknowledgements
This research was supported by the Defense Ad-
vanced Research Projects Agency under SRI Inter-
national?s prime Contract Number NBCHD040058.
44
References
Y. Al-Onaizan and K. Knight. 2002. Translating named
entities using monolingual and bilingual resources. In
Proc. of ACL.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society Se-
ries, 39(4):1?38.
Y. Goldberg and M. Elhadad. 2008. Identification of
transliterated foreign words in Hebrew script. In Proc.
of CICLing.
D. Goldwasser and D. Roth. 2008a. Active sample se-
lection for named entity transliteration. In Proc. of
ACL/HLT Short Papers.
D. Goldwasser and D. Roth. 2008b. Transliteration as
constrained optimization. In Proc. of EMNLP.
S. Goldwater and L. Griffiths, T. 2007. A fully Bayesian
approach to unsupervised part-of-speech tagging. In
Proc. of ACL.
J. Graehl. 1997. Carmel finite-state toolkit.
http://www.isi.edu/licensed-sw/carmel.
L. Haizhou, Z. Min, and S. Jian. 2004. A joint source-
channel model for machine transliteration. In Proc. of
ACL.
U. Hermjakob, K. Knight, and H. Daume. 2008. Name
translation in statistical machine translation?learning
when to transliterate. In Proc. of ACL/HLT.
F. Huang, S. Vogel, and A. Waibel. 2004. Improving
named entity translation combining phonetic and se-
mantic similarities. In Proc. of HLT/NAACL.
S. Karimi, F. Scholer, and A. Turpin. 2007. Col-
lapsed consonant and vowel models: New ap-
proaches for English-Persian transliteration and back-
transliteration. In Proc. of ACL.
A. Klementiev and D. Roth. 2008. Named entity translit-
eration and discovery in multilingual corpora. In
Learning Machine Translation. MIT press.
K. Knight and J. Graehl. 1998. Machine transliteration.
Computational Linguistics, 24(4):599?612.
K. Knight and K. Yamada. 1999. A computational ap-
proach to deciphering unknown scripts. In Proc. of the
ACL Workshop on Unsupervised Learning in Natural
Language Processing.
K. Knight, A. Nair, N. Rathod, and K. Yamada. 2006.
Unsupervised analysis for decipherment problems. In
Proc. of COLING/ACL.
J. Kuo, H. Li, and Y. Yang. 2006. Learning translitera-
tion lexicons from the web. In Proc. of ACL/COLING.
H. Li, C. Sim, K., J. Kuo, and M. Dong. 2007. Semantic
transliteration of personal names. In Proc. of ACL.
M. Nagata, T. Saito, and K. Suzuki. 2001. Using the web
as a bilingual dictionary. In Proc. of the ACL Work-
shop on Data-driven Methods in Machine Translation.
J. Oh and H. Isahara. 2006. Mining the web for translit-
eration lexicons: Joint-validation approach. In Proc.
of the IEEE/WIC/ACM International Conference on
Web Intelligence.
S. Ravi and K. Knight. 2008. Attacking decipherment
problems optimally with low-order n-gram models. In
Proc. of EMNLP.
S. Ravi and K. Knight. 2009. Probabilistic methods for a
Japanese syllable cipher. In Proc. of the International
Conference on the Computer Processing of Oriental
Languages (ICCPOL).
T. Sherif and G. Kondrak. 2007a. Bootstrapping a
stochastic transducer for arabic-english transliteration
extraction. In Proc. of ACL.
T. Sherif and G. Kondrak. 2007b. Substring-based
transliteration. In Proc. of ACL.
R. Sproat, T. Tao, and C. Zhai. 2006. Named entity
transliteration with comparable corpora. In Proc. of
ACL.
T. Tao, S. Yoon, A. Fister, R. Sproat, and C. Zhai. 2006.
Unsupervised named entity transliteration using tem-
poral and phonetic correlation. In Proc. of EMNLP.
J. Wu and S. Chang, J. 2007. Learning to find English
to Chinese transliterations on the web. In Proc. of
EMNLP/CoNLL.
D. Yarowsky. 1995. Unsupervised word sense disam-
biguation rivaling supervised methods. In Proc. of
ACL.
S. Yoon, K. Kim, and R. Sproat. 2007. Multilingual
transliteration using feature based phonetic method. In
Proc. of ACL.
D. Zelenko and C. Aone. 2006. Discriminative methods
for transliteration. In Proc. of EMNLP.
45
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 504?512,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Minimized Models for Unsupervised Part-of-Speech Tagging
Sujith Ravi and Kevin Knight
University of Southern California
Information Sciences Institute
Marina del Rey, California 90292
{sravi,knight}@isi.edu
Abstract
We describe a novel method for the task
of unsupervised POS tagging with a dic-
tionary, one that uses integer programming
to explicitly search for the smallest model
that explains the data, and then uses EM
to set parameter values. We evaluate our
method on a standard test corpus using
different standard tagsets (a 45-tagset as
well as a smaller 17-tagset), and show that
our approach performs better than existing
state-of-the-art systems in both settings.
1 Introduction
In recent years, we have seen increased interest in
using unsupervised methods for attacking differ-
ent NLP tasks like part-of-speech (POS) tagging.
The classic Expectation Maximization (EM) algo-
rithm has been shown to perform poorly on POS
tagging, when compared to other techniques, such
as Bayesian methods.
In this paper, we develop new methods for un-
supervised part-of-speech tagging. We adopt the
problem formulation of Merialdo (1994), in which
we are given a raw word sequence and a dictio-
nary of legal tags for each word type. The goal is
to tag each word token so as to maximize accuracy
against a gold tag sequence. Whether this is a real-
istic problem set-up is arguable, but an interesting
collection of methods and results has accumulated
around it, and these can be clearly compared with
one another.
We use the standard test set for this task, a
24,115-word subset of the Penn Treebank, for
which a gold tag sequence is available. There
are 5,878 word types in this test set. We use
the standard tag dictionary, consisting of 57,388
word/tag pairs derived from the entire Penn Tree-
bank.1 8,910 dictionary entries are relevant to the
5,878 word types in the test set. Per-token ambigu-
ity is about 1.5 tags/token, yielding approximately
106425 possible ways to tag the data. There are 45
distinct grammatical tags. In this set-up, there are
no unknown words.
Figure 1 shows prior results for this prob-
lem. While the methods are quite different,
they all make use of two common model ele-
ments. One is a probabilistic n-gram tag model
P(ti|ti?n+1...ti?1), which we call the grammar.
The other is a probabilistic word-given-tag model
P(wi|ti), which we call the dictionary.
The classic approach (Merialdo, 1994) is
expectation-maximization (EM), where we esti-
mate grammar and dictionary probabilities in or-
der to maximize the probability of the observed
word sequence:
P (w1...wn) =
?
t1...tn
P (t1...tn) ? P (w1...wn|t1...tn)
?
?
t1...tn
n?
i=1
P (ti|ti?2 ti?1) ? P (wi|ti)
Goldwater and Griffiths (2007) report 74.5%
accuracy for EM with a 3-gram tag model, which
we confirm by replication. They improve this to
83.9% by employing a fully Bayesian approach
which integrates over all possible parameter val-
ues, rather than estimating a single distribution.
They further improve this to 86.8% by using pri-
ors that favor sparse distributions. Smith and Eis-
ner (2005) employ a contrastive estimation tech-
1As (Banko and Moore, 2004) point out, unsupervised
tagging accuracy varies wildly depending on the dictionary
employed. We follow others in using a fat dictionary (with
49,206 distinct word types), rather than a thin one derived
only from the test set.
504
System Tagging accuracy (%)
on 24,115-word corpus
1. Random baseline (for each word, pick a random tag from the alternatives given by
the word/tag dictionary)
64.6
2. EM with 2-gram tag model 81.7
3. EM with 3-gram tag model 74.5
4a. Bayesian method (Goldwater and Griffiths, 2007) 83.9
4b. Bayesian method with sparse priors (Goldwater and Griffiths, 2007) 86.8
5. CRF model trained using contrastive estimation (Smith and Eisner, 2005) 88.6
6. EM-HMM tagger provided with good initial conditions (Goldberg et al, 2008) 91.4*
(*uses linguistic constraints and manual adjustments to the dictionary)
Figure 1: Previous results on unsupervised POS tagging using a dictionary (Merialdo, 1994) on the full
45-tag set. All other results reported in this paper (unless specified otherwise) are on the 45-tag set as
well.
nique, in which they automatically generate nega-
tive examples and use CRF training.
In more recent work, Toutanova and John-
son (2008) propose a Bayesian LDA-based gener-
ative model that in addition to using sparse priors,
explicitly groups words into ambiguity classes.
They show considerable improvements in tagging
accuracy when using a coarser-grained version
(with 17-tags) of the tag set from the Penn Tree-
bank.
Goldberg et al (2008) depart from the Bayesian
framework and show how EM can be used to learn
good POS taggers for Hebrew and English, when
provided with good initial conditions. They use
language specific information (like word contexts,
syntax and morphology) for learning initial P(t|w)
distributions and also use linguistic knowledge to
apply constraints on the tag sequences allowed by
their models (e.g., the tag sequence ?V V? is dis-
allowed). Also, they make other manual adjust-
ments to reduce noise from the word/tag dictio-
nary (e.g., reducing the number of tags for ?the?
from six to just one). In contrast, we keep all the
original dictionary entries derived from the Penn
Treebank data for our experiments.
The literature omits one other baseline, which
is EM with a 2-gram tag model. Here we obtain
81.7% accuracy, which is better than the 3-gram
model. It seems that EM with a 3-gram tag model
runs amok with its freedom. For the rest of this pa-
per, we will limit ourselves to a 2-gram tag model.
2 What goes wrong with EM?
We analyze the tag sequence output produced by
EM and try to see where EM goes wrong. The
overall POS tag distribution learnt by EM is rel-
atively uniform, as noted by Johnson (2007), and
it tends to assign equal number of tokens to each
tag label whereas the real tag distribution is highly
skewed. The Bayesian methods overcome this ef-
fect by using priors which favor sparser distribu-
tions. But it is not easy to model such priors into
EM learning. As a result, EM exploits a lot of rare
tags (like FW = foreign word, or SYM = symbol)
and assigns them to common word types (in, of,
etc.).
We can compare the tag assignments from the
gold tagging and the EM tagging (Viterbi tag se-
quence). The table below shows tag assignments
(and their counts in parentheses) for a few word
types which occur frequently in the test corpus.
word/tag dictionary Gold tagging EM tagging
in? {IN, RP, RB, NN, FW, RBR} IN (355) IN (0)
RP (3) RP (0)
FW (0) FW (358)
of? {IN, RP, RB} IN (567) IN (0)
RP (0) RP (567)
on? {IN,RP, RB} RP (5) RP (127)
IN (129) IN (0)
RB (0) RB (7)
a? {DT, JJ, IN, LS, FW, SYM, NNP} DT (517) DT (0)
SYM (0) SYM (517)
We see how the rare tag labels (like FW, SYM,
etc.) are abused by EM. As a result, many word to-
kens which occur very frequently in the corpus are
incorrectly tagged with rare tags in the EM tagging
output.
We also look at things more globally. We inves-
tigate the Viterbi tag sequence generated by EM
training and count how many distinct tag bigrams
there are in that sequence. We call this the ob-
served grammar size, and it is 915. That is, in
tagging the 24,115 test tokens, EM uses 915 of the
available 45 ? 45 = 2025 tag bigrams.2 The ad-
vantage of the observed grammar size is that we
2We contrast observed size with the model size for the
grammar, which we define as the number of P(t2|t1) entries
in EM?s trained tag model that exceed 0.0001 probability.
505
L8
L0
they    can          fish       .       I        fish
L1
L2 L3
L4
L6
L5 L7
L9
L10
L11
START
PRO
AUX
V
N
PUNC
d1 PRO-they
d2 AUX-can
d3 V-can
d4 N-fish
d5 V-fish
d6 PUNC-.
d7 PRO-I
g1 PRO-AUX
g2 PRO-V
g3 AUX-N
g4  AUX-V
g5 V-N
g6 V-V
g7 N-PUNC
g8 V-PUNC
g9 PUNC-PRO
g10 PRO-N
dictionary 
variables
grammar 
variables
Integer Program
Minimize: ?i=1?10 gi
Constraints:
1. Single left-to-right path (at each node, flow in = flow out)
e.g.,  L0 = 1
L1 = L3 + L4
2. Path consistency constraints (chosen path respects chosen 
dictionary & grammar)
e.g., L0 ? d1
L1 ? g1
IP formulation
training text
link 
variables
Figure 2: Integer Programming formulation for finding the smallest grammar that explains a given word
sequence. Here, we show a sample word sequence and the corresponding IP network generated for that
sequence.
can compare it with the gold tagging?s observed
grammar size, which is 760. So we can safely say
that EM is learning a grammar that is too big, still
abusing its freedom.
3 Small Models
Bayesian sparse priors aim to create small mod-
els. We take a different tack in the paper and
directly ask: What is the smallest model that ex-
plains the text? Our approach is related to mini-
mum description length (MDL). We formulate our
question precisely by asking which tag sequence
(of the 106425 available) has the smallest observed
grammar size. The answer is 459. That is, there
exists a tag sequence that contains 459 distinct tag
bigrams, and no other tag sequence contains fewer.
We obtain this answer by formulating the prob-
lem in an integer programming (IP) framework.
Figure 2 illustrates this with a small sample word
sequence. We create a network of possible tag-
gings, and we assign a binary variable to each link
in the network. We create constraints to ensure
that those link variables receiving a value of 1
form a left-to-right path through the tagging net-
work, and that all other link variables receive a
value of 0. We accomplish this by requiring the
sum of the links entering each node to equal to
the sum of the links leaving each node. We also
create variables for every possible tag bigram and
word/tag dictionary entry. We constrain link vari-
able assignments to respect those grammar and
dictionary variables. For example, we do not allow
a link variable to ?activate? unless the correspond-
ing grammar variable is also ?activated?. Finally,
we add an objective function that minimizes the
number of grammar variables that are assigned a
value of 1.
Figure 3 shows the IP solution for the example
word sequence from Figure 2. Of course, a small
grammar size does not necessarily correlate with
higher tagging accuracy. For the small toy exam-
ple shown in Figure 3, the correct tagging is ?PRO
AUX V . PRO V? (with 5 tag pairs), whereas the
IP tries to minimize the grammar size and picks
another solution instead.
For solving the integer program, we use CPLEX
software (a commercial IP solver package). Alter-
natively, there are other programs such as lp solve,
which are free and publicly available for use. Once
we create an integer program for the full test cor-
pus, and pass it to CPLEX, the solver returns an
506
word sequence: they can fish . I fish
Tagging Grammar Size
PRO AUX N . PRO N 5
PRO AUX V . PRO N 5
PRO AUX N . PRO V 5
PRO AUX V . PRO V 5
PRO V N . PRO N 5
PRO V V . PRO N 5
PRO V N . PRO V 4
PRO V V . PRO V 4
Figure 3: Possible tagging solutions and corre-
sponding grammar sizes for the sample word se-
quence from Figure 2 using the given dictionary
and grammar. The IP solver finds the smallest
grammar set that can explain the given word se-
quence. In this example, there exist two solutions
that each contain only 4 tag pair entries, and IP
returns one of them.
objective function value of 459.3
CPLEX also returns a tag sequence via assign-
ments to the link variables. However, there are
actually 104378 tag sequences compatible with the
459-sized grammar, and our IP solver just selects
one at random. We find that of all those tag se-
quences, the worst gives an accuracy of 50.8%,
and the best gives an accuracy of 90.3%. We
also note that CPLEX takes 320 seconds to return
the optimal solution for the integer program corre-
sponding to this particular test data (24,115 tokens
with the 45-tag set). It might be interesting to see
how the performance of the IP method (in terms
of time complexity) is affected when scaling up to
larger data and bigger tagsets. We leave this as
part of future work. But we do note that it is pos-
sible to obtain less than optimal solutions faster by
interrupting the CPLEX solver.
4 Fitting the Model
Our IP formulation can find us a small model, but
it does not attempt to fit the model to the data. For-
tunately, we can use EM for that. We still give
EM the full word/tag dictionary, but now we con-
strain its initial grammar model to the 459 tag bi-
grams identified by IP. Starting with uniform prob-
abilities, EM finds a tagging that is 84.5% accu-
rate, substantially better than the 81.7% originally
obtained with the fully-connected grammar. So
we see a benefit to our explicit small-model ap-
proach. While EM does not find the most accurate
3Note that the grammar identified by IP is not uniquely
minimal. For the same word sequence, there exist other min-
imal grammars having the same size (459 entries). In our ex-
periments, we choose the first solution returned by CPLEX.
in on
IN IN
RP RP
word/tag dictionary RB RB
NN
FW
RBR
observed EM dictionary FW (358) RP (127)
RB (7)
observed IP+EM dictionary IN (349) IN (126)
RB (9) RB (8)
observed gold dictionary IN (355) IN (129)
RB (3) RP (5)
Figure 4: Examples of tagging obtained from dif-
ferent systems for prepositions in and on.
sequence consistent with the IP grammar (90.3%),
it finds a relatively good one.
The IP+EM tagging (with 84.5% accuracy) has
some interesting properties. First, the dictionary
we observe from the tagging is of higher qual-
ity (with fewer spurious tagging assignments) than
the one we observe from the original EM tagging.
Figure 4 shows some examples.
We also measure the quality of the two observed
grammars/dictionaries by computing their preci-
sion and recall against the grammar/dictionary we
observe in the gold tagging.4 We find that preci-
sion of the observed grammar increases from 0.73
(EM) to 0.94 (IP+EM). In addition to removing
many bad tag bigrams from the grammar, IP min-
imization also removes some of the good ones,
leading to lower recall (EM = 0.87, IP+EM =
0.57). In the case of the observed dictionary, using
a smaller grammar model does not affect the pre-
cision (EM = 0.91, IP+EM = 0.89) or recall (EM
= 0.89, IP+EM = 0.89).
During EM training, the smaller grammar with
fewer bad tag bigrams helps to restrict the dictio-
nary model from making too many bad choices
that EM made earlier. Here are a few examples
of bad dictionary entries that get removed when
we use the minimized grammar for EM training:
in ? FW
a ? SYM
of ? RP
In ? RBR
During EM training, the minimized grammar
4For any observed grammar or dictionary X,
Precision (X) =
|{X}?{observedgold}|
|{X}|
Recall (X) =
|{X}?{observedgold}|
|{observedgold}|
507
Model Tagging accuracy Observed size Model size
on 24,115-word
corpus
grammar(G), dictionary(D) grammar(G), dictionary(D)
1. EM baseline with full grammar + full dictio-
nary
81.7 G=915, D=6295 G=935, D=6430
2. EM constrained with minimized IP-grammar
+ full dictionary
84.5 G=459, D=6318 G=459, D=6414
3. EM constrained with full grammar + dictio-
nary from (2)
91.3 G=606, D=6245 G=612, D=6298
4. EM constrained with grammar from (3) + full
dictionary
91.5 G=593, D=6285 G=600, D=6373
5. EM constrained with full grammar + dictio-
nary from (4)
91.6 G=603, D=6280 G=618, D=6337
Figure 5: Percentage of word tokens tagged correctly by different models. The observed sizes and model
sizes of grammar (G) and dictionary (D) produced by these models are shown in the last two columns.
helps to eliminate many incorrect entries (i.e.,
zero out model parameters) from the dictionary,
thereby yielding an improved dictionary model.
So using the minimized grammar (which has
higher precision) helps to improve the quality of
the chosen dictionary (examples shown in Fig-
ure 4). This in turn helps improve the tagging ac-
curacy from 81.7% to 84.5%. It is clear that the
IP-constrained grammar is a better choice to run
EM on than the full grammar.
Note that we used a very small IP-grammar
(containing only 459 tag bigrams) during EM
training. In the process of minimizing the gram-
mar size, IP ends up removing many good tag bi-
grams from our grammar set (as seen from the low
measured recall of 0.57 for the observed gram-
mar). Next, we proceed to recover some good tag
bigrams and expand the grammar in a restricted
fashion by making use of the higher-quality dic-
tionary produced by the IP+EM method. We now
run EM again on the full grammar (all possible
tag bigrams) in combination with this good dictio-
nary (containing fewer entries than the full dictio-
nary). Unlike the original training with full gram-
mar, where EM could choose any tag bigram, now
the choice of grammar entries is constrained by
the good dictionary model that we provide EM
with. This allows EM to recover some of the
good tag pairs, and results in a good grammar-
dictionary combination that yields better tagging
performance.
With these improvements in mind, we embark
on an alternating scheme to find better models and
taggings. We run EM for multiple passes, and in
each pass we alternately constrain either the gram-
mar model or the dictionary model. The procedure
is simple and proceeds as follows:
1. Run EM constrained to the last trained dictio-
nary, but provided with a full grammar.5
2. Run EM constrained to the last trained gram-
mar, but provided with a full dictionary.
3. Repeat steps 1 and 2.
We notice significant gains in tagging perfor-
mance when applying this technique. The tagging
accuracy increases at each step and finally settles
at a high of 91.6%, which outperforms the exist-
ing state-of-the-art systems for the 45-tag set. The
system achieves a better accuracy than the 88.6%
from Smith and Eisner (2005), and even surpasses
the 91.4% achieved by Goldberg et al (2008)
without using any additional linguistic constraints
or manual cleaning of the dictionary. Figure 5
shows the tagging performance achieved at each
step. We found that it is the elimination of incor-
rect entries from the dictionary (and grammar) and
not necessarily the initialization weights from pre-
vious EM training, that results in the tagging im-
provements. Initializing the last trained dictionary
or grammar at each step with uniform weights also
yields the same tagging improvements as shown in
Figure 5.
We find that the observed grammar also im-
proves, growing from 459 entries to 603 entries,
with precision increasing from 0.94 to 0.96, and
recall increasing from 0.57 to 0.76. The figure
also shows the model?s internal grammar and dic-
tionary sizes.
Figure 6 and 7 show how the precision/recall
of the observed grammar and dictionary varies for
different models from Figure 5. In the case of the
observed grammar (Figure 6), precision increases
5For all experiments, EM training is allowed to run for
40 iterations or until the likelihood ratios between two subse-
quent iterations reaches a value of 0.99999, whichever occurs
earlier.
508
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
Pre
cis
ion
 / R
eca
ll o
f o
bse
rve
d g
ram
ma
r
Tagging Model
Model 1 Model 2 Model 3 Model 4 Model 5
PrecisionRecall
Figure 6: Comparison of observed grammars from
the model tagging vs. gold tagging in terms of pre-
cision and recall measures.
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
Pre
cis
ion
 / R
eca
ll o
f o
bse
rve
d d
icti
on
ary
Tagging Model
Model 1 Model 2 Model 3 Model 4 Model 5
PrecisionRecall
Figure 7: Comparison of observed dictionaries from
the model tagging vs. gold tagging in terms of pre-
cision and recall measures.
Model Tagging accuracy on
24,115-word corpus
no-restarts with 100 restarts
1. Model 1 (EM baseline) 81.7 83.8
2. Model 2 84.5 84.5
3. Model 3 91.3 91.8
4. Model 4 91.5 91.8
5. Model 5 91.6 91.8
Figure 8: Effect of random restarts (during EM
training) on tagging accuracy.
at each step, whereas recall drops initially (owing
to the grammar minimization) but then picks up
again. The precision/recall of the observed dictio-
nary on the other hand, is not affected by much.
5 Restarts and More Data
Multiple random restarts for EM, while not often
emphasized in the literature, are key in this do-
main. Recall that our original EM tagging with a
fully-connected 2-gram tag model was 81.7% ac-
curate. When we execute 100 random restarts and
select the model with the highest data likelihood,
we get 83.8% accuracy. Likewise, when we ex-
tend our alternating EM scheme to 100 random
restarts at each step, we improve our tagging ac-
curacy from 91.6% to 91.8% (Figure 8).
As noted by Toutanova and Johnson (2008),
there is no reason to limit the amount of unlabeled
data used for training the models. Their models
are trained on the entire Penn Treebank data (in-
stead of using only the 24,115-token test data),
and so are the tagging models used by Goldberg
et al (2008). But previous results from Smith and
Eisner (2005) and Goldwater and Griffiths (2007)
show that their models do not benefit from using
more unlabeled training data. Because EM is ef-
ficient, we can extend our word-sequence train-
ing data from the 24,115-token set to the entire
Penn Treebank (973k tokens). We run EM training
again for Model 5 (the best model from Figure 5)
but this time using 973k word tokens, and further
increase our accuracy to 92.3%. This is our final
result on the 45-tagset, and we note that it is higher
than previously reported results.
6 Smaller Tagset and Incomplete
Dictionaries
Previously, researchers working on this task have
also reported results for unsupervised tagging with
a smaller tagset (Smith and Eisner, 2005; Gold-
water and Griffiths, 2007; Toutanova and John-
son, 2008; Goldberg et al, 2008). Their systems
were shown to obtain considerable improvements
in accuracy when using a 17-tagset (a coarser-
grained version of the tag labels from the Penn
Treebank) instead of the 45-tagset. When tag-
ging the same standard test corpus with the smaller
17-tagset, our method is able to achieve a sub-
stantially high accuracy of 96.8%, which is the
best result reported so far on this task. The ta-
ble in Figure 9 shows a comparison of different
systems for which tagging accuracies have been
reported previously for the 17-tagset case (Gold-
berg et al, 2008). The first row in the table
compares tagging results when using a full dictio-
nary (i.e., a lexicon containing entries for 49,206
word types). The InitEM-HMM system from
Goldberg et al (2008) reports an accuracy of
93.8%, followed by the LDA+AC model (Latent
Dirichlet Allocation model with a strong Ambigu-
ity Class component) from Toutanova and John-
son (2008). In comparison, the Bayesian HMM
(BHMM) model from Goldwater et al (2007) and
509
Dict IP+EM (24k) InitEM-HMM LDA+AC CE+spl BHMM
Full (49206 words) 96.8 (96.8) 93.8 93.4 88.7 87.3
? 2 (2141 words) 90.6 (90.0) 89.4 91.2 79.5 79.6
? 3 (1249 words) 88.0 (86.1) 87.4 89.7 78.4 71
Figure 9: Comparison of different systems for English unsupervised POS tagging with 17 tags.
the CE+spl model (Contrastive Estimation with a
spelling model) from Smith and Eisner (2005) re-
port lower accuracies (87.3% and 88.7%, respec-
tively). Our system (IP+EM) which uses inte-
ger programming and EM, gets the highest accu-
racy (96.8%). The accuracy numbers reported for
Init-HMM and LDA+AC are for models that are
trained on all the available unlabeled data from
the Penn Treebank. The IP+EM models used in
the 17-tagset experiments reported here were not
trained on the entire Penn Treebank, but instead
used a smaller section containing 77,963 tokens
for estimating model parameters. We also include
the accuracies for our IP+EM model when using
only the 24,115 token test corpus for EM estima-
tion (shown within parenthesis in second column
of the table in Figure 9). We find that our perfor-
mance does not degrade when the parameter esti-
mation is done using less data, and our model still
achieves a high accuracy of 96.8%.
6.1 Incomplete Dictionaries and Unknown
Words
The literature also includes results reported in a
different setting for the tagging problem. In some
scenarios, a complete dictionary with entries for
all word types may not be readily available to us
and instead, we might be provided with an incom-
plete dictionary that contains entries for only fre-
quent word types. In such cases, any word not
appearing in the dictionary will be treated as an
unknown word, and can be labeled with any of
the tags from given tagset (i.e., for every unknown
word, there are 17 tag possibilities). Some pre-
vious approaches (Toutanova and Johnson, 2008;
Goldberg et al, 2008) handle unknown words ex-
plicitly using ambiguity class components condi-
tioned on various morphological features, and this
has shown to produce good tagging results, espe-
cially when dealing with incomplete dictionaries.
We follow a simple approach using just one
of the features used in (Toutanova and Johnson,
2008) for assigning tag possibilities to every un-
known word. We first identify the top-100 suffixes
(up to 3 characters) for words in the dictionary.
Using the word/tag pairs from the dictionary, we
train a simple probabilistic model that predicts the
tag given a particular suffix (e.g., P(VBG | ing) =
0.97, P(N | ing) = 0.0001, ...). Next, for every un-
known word ?w?, the trained P(tag | suffix) model
is used to predict the top 3 tag possibilities for
?w? (using only its suffix information), and subse-
quently this word along with its 3 tags are added as
a new entry to the lexicon. We do this for every un-
known word, and eventually we have a dictionary
containing entries for all the words. Once the com-
pleted lexicon (containing both correct entries for
words in the lexicon and the predicted entries for
unknown words) is available, we follow the same
methodology from Sections 3 and 4 using integer
programming to minimize the size of the grammar
and then applying EM to estimate parameter val-
ues.
Figure 9 shows comparative results for the 17-
tagset case when the dictionary is incomplete. The
second and third rows in the table shows tagging
accuracies for different systems when a cutoff of
2 (i.e., all word types that occur with frequency
counts < 2 in the test corpus are removed) and
a cutoff of 3 (i.e., all word types occurring with
frequency counts < 3 in the test corpus are re-
moved) is applied to the dictionary. This yields
lexicons containing 2,141 and 1,249 words respec-
tively, which are much smaller compared to the
original 49,206 word dictionary. As the results
in Figure 9 illustrate, the IP+EM method clearly
does better than all the other systems except for
the LDA+AC model. The LDA+AC model from
Toutanova and Johnson (2008) has a strong ambi-
guity class component and uses more features to
handle the unknown words better, and this con-
tributes to the slightly higher performance in the
incomplete dictionary cases, when compared to
the IP+EM model.
7 Discussion
The method proposed in this paper is simple?
once an integer program is produced, there are
solvers available which directly give us the so-
lution. In addition, we do not require any com-
plex parameter estimation techniques; we train our
models using simple EM, which proves to be effi-
cient for this task. While some previous methods
510
word type Gold tag Automatic tag # of tokens tagged incorrectly
?s POS VBZ 173
be VB VBP 67
that IN WDT 54
New NNP NNPS 33
U.S. NNP JJ 31
up RP RB 28
more RBR JJR 27
and CC IN 23
have VB VBP 20
first JJ JJS 20
to TO IN 19
out RP RB 17
there EX RB 15
stock NN JJ 15
what WP WDT 14
one CD NN 14
? POS : 14
as RB IN 14
all DT RB 14
that IN RB 13
Figure 10: Most frequent mistakes observed in the model tagging (using the best model, which gives
92.3% accuracy) when compared to the gold tagging.
introduced for the same task have achieved big
tagging improvements using additional linguistic
knowledge or manual supervision, our models are
not provided with any additional information.
Figure 10 illustrates for the 45-tag set some of
the common mistakes that our best tagging model
(92.3%) makes. In some cases, the model actually
gets a reasonable tagging but is penalized perhaps
unfairly. For example, ?to? is tagged as IN by our
model sometimes when it occurs in the context of
a preposition, whereas in the gold tagging it is al-
ways tagged as TO. The model also gets penalized
for tagging the word ?U.S.? as an adjective (JJ),
which might be considered valid in some cases
such as ?the U.S. State Department?. In other
cases, the model clearly produces incorrect tags
(e.g., ?New? gets tagged incorrectly as NNPS).
Our method resembles the classic Minimum
Description Length (MDL) approach for model
selection (Barron et al, 1998). In MDL, there
is a single objective function to (1) maximize the
likelihood of observing the data, and at the same
time (2) minimize the length of the model descrip-
tion (which depends on the model size). How-
ever, the search procedure for MDL is usually
non-trivial, and for our task of unsupervised tag-
ging, we have not found a direct objective function
which we can optimize and produce good tagging
results. In the past, only a few approaches uti-
lizing MDL have been shown to work for natural
language applications. These approaches employ
heuristic search methods with MDL for the task
of unsupervised learning of morphology of natu-
ral languages (Goldsmith, 2001; Creutz and La-
gus, 2002; Creutz and Lagus, 2005). The method
proposed in this paper is the first application of
the MDL idea to POS tagging, and the first to
use an integer programming formulation rather
than heuristic search techniques. We also note
that it might be possible to replicate our models
in a Bayesian framework similar to that proposed
in (Goldwater and Griffiths, 2007).
8 Conclusion
We presented a novel method for attacking
dictionary-based unsupervised part-of-speech tag-
ging. Our method achieves a very high accuracy
(92.3%) on the 45-tagset and a higher (96.8%) ac-
curacy on a smaller 17-tagset. The method works
by explicitly minimizing the grammar size using
integer programming, and then using EM to esti-
mate parameter values. The entire process is fully
automated and yields better performance than any
existing state-of-the-art system, even though our
models were not provided with any additional lin-
guistic knowledge (for example, explicit syntactic
constraints to avoid certain tag combinations such
as ?V V?, etc.). However, it is easy to model some
of these linguistic constraints (both at the local and
global levels) directly using integer programming,
and this may result in further improvements and
lead to new possibilities for future research. For
direct comparison to previous works, we also pre-
sented results for the case when the dictionaries
are incomplete and find the performance of our
system to be comparable with current best results
reported for the same task.
9 Acknowledgements
This research was supported by the Defense
Advanced Research Projects Agency under
SRI International?s prime Contract Number
NBCHD040058.
511
References
M. Banko and R. C. Moore. 2004. Part of speech
tagging in context. In Proceedings of the Inter-
national Conference on Computational Linguistics
(COLING).
A. Barron, J. Rissanen, and B. Yu. 1998. The min-
imum description length principle in coding and
modeling. IEEE Transactions on Information The-
ory, 44(6):2743?2760.
M. Creutz and K. Lagus. 2002. Unsupervised discov-
ery of morphemes. In Proceedings of the ACL Work-
shop on Morphological and Phonological Learning
of.
M. Creutz and K. Lagus. 2005. Unsupervised
morpheme segmentation and morphology induction
from text corpora using Morfessor 1.0. Publications
in Computer and Information Science, Report A81,
Helsinki University of Technology, March.
Y. Goldberg, M. Adler, and M. Elhadad. 2008. EM can
find pretty good HMM POS-taggers (when given a
good start). In Proceedings of the ACL.
J. Goldsmith. 2001. Unsupervised learning of the mor-
phology of a natural language. Computational Lin-
guistics, 27(2):153?198.
S. Goldwater and T. L. Griffiths. 2007. A fully
Bayesian approach to unsupervised part-of-speech
tagging. In Proceedings of the ACL.
M. Johnson. 2007. Why doesnt EM find good HMM
POS-taggers? In Proceedings of the Joint Confer-
ence on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL).
B. Merialdo. 1994. Tagging English text with a
probabilistic model. Computational Linguistics,
20(2):155?171.
N. Smith and J. Eisner. 2005. Contrastive estimation:
Training log-linear models on unlabeled data. In
Proceedings of the ACL.
K. Toutanova and M. Johnson. 2008. A Bayesian
LDA-based model for semi-supervised part-of-
speech tagging. In Proceedings of the Advances in
Neural Information Processing Systems (NIPS).
512
Proceedings of the NAACL HLT Workshop on Integer Linear Programming for Natural Language Processing, pages 28?35,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
A New Objective Function for Word Alignment
Tugba Bodrumlu Kevin Knight Sujith Ravi
Information Sciences Institute & Computer Science Department
University of Southern California
bodrumlu@usc.edu, knight@isi.edu, sravi@isi.edu
Abstract
We develop a new objective function for word
alignment that measures the size of the bilin-
gual dictionary induced by an alignment. A
word alignment that results in a small dictio-
nary is preferred over one that results in a large
dictionary. In order to search for the align-
ment that minimizes this objective, we cast the
problem as an integer linear program. We then
extend our objective function to align corpora
at the sub-word level, which we demonstrate
on a small Turkish-English corpus.
1 Introduction
Word alignment is the problem of annotating a bilin-
gual text with links connecting words that have the
same meanings. Figure 1 shows sample input for
a word aligner (Knight, 1997). After analyzing the
text, we may conclude, for example, that sprok cor-
responds to dat in the first sentence pair.
Word alignment has several downstream con-
sumers. One is machine translation, where pro-
grams extract translation rules from word-aligned
corpora (Och and Ney, 2004; Galley et al, 2004;
Chiang, 2007; Quirk et al, 2005). Other down-
stream processes exploit dictionaries derived by
alignment, in order to translate queries in cross-
lingual IR (Scho?nhofen et al, 2008) or re-score can-
didate translation outputs (Och et al, 2004).
Many methods of automatic alignment have been
proposed. Probabilistic generative models like IBM
1-5 (Brown et al, 1993), HMM (Vogel et al, 1996),
ITG (Wu, 1997), and LEAF (Fraser and Marcu,
2007) define formulas for P(f | e) or P(e, f), with
ok-voon ororok sprok
at-voon bichat dat
erok sprok izok hihok ghirok
totat dat arrat vat hilat
ok-drubel ok-voon anok plok sprok
at-drubel at-voon pippat rrat dat
ok-voon anok drok brok jok 
at-voon krat pippat sat lat 
wiwok farok izok stok
totat jjat quat cat 
lalok sprok izok jok stok
wat dat krat quat cat 
lalok farok ororok lalok sprok izok enemok
wat jjat bichat wat dat vat eneat 
lalok brok anok plok nok 
iat lat pippat rrat nnat
wiwok nok izok kantok ok-yurp
totat nnat quat oloat at-yurp
lalok mok nok yorok ghirok clok
wat nnat gat mat bat hilat
lalok nok crrrok hihok yorok zanzanok
wat nnat arrat mat zanzanat 
lalok rarok nok izok hihok mok
wat nnat forat arrat vat gat 
Figure 1: Word alignment exercise (Knight, 1997).
28
hidden alignment variables. EM algorithms estimate
dictionary and other probabilities in order to maxi-
mize those quantities. One can then ask for Viterbi
alignments that maximize P(alignment | e, f). Dis-
criminative models, e.g. (Taskar et al, 2005), in-
stead set parameters to maximize alignment accu-
racy against a hand-aligned development set. EMD
training (Fraser and Marcu, 2006) combines genera-
tive and discriminative elements.
Low accuracy is a weakness for all systems. Most
practitioners still use 1990s algorithms to align their
data. It stands to reason that we have not yet seen
the last word in alignment models.
In this paper, we develop a new objective function
for alignment, inspired by watching people manu-
ally solve the alignment exercise of Figure 1. When
people attack this problem, we find that once they
create a bilingual dictionary entry, they like to re-
use that entry as much as possible. Previous ma-
chine aligners emulate this to some degree, but they
are not explicitly programmed to do so.
We also address another weakness of current
aligners: they only align full words. With few ex-
ceptions, e.g. (Zhang et al, 2003; Snyder and Barzi-
lay, 2008), aligners do not operate at the sub-word
level, making them much less useful for agglutina-
tive languages such as Turkish.
Our present contributions are as follows:
? We offer a simple new objective function that
scores a corpus alignment based on how many
distinct bilingual word pairs it contains.
? We use an integer programming solver to
carry out optimization and corpus alignment.
? We extend the system to perform sub-
word alignment, which we demonstrate on a
Turkish-English corpus.
The results in this paper constitute a proof of con-
cept of these ideas, executed on small corpora. We
conclude by listing future directions.
2 New Objective Function for Alignment
We search for the legal alignment that minimizes the
size of the induced bilingual dictionary. By dictio-
nary size, we mean the number of distinct word-
pairs linked in the corpus alignment. We can im-
mediately investigate how different alignments stack
up, according to this objective function. Figure 2
garcia and associates 
garcia y asociados
his associates are not strong
sus asociados no son fuertes
carlos garcia has three associates 
carlos garcia tiene tres asociados
garcia has a company also
garcia tambien tiene una empresa
its clients are angry
sus clientes estan enfadados
the associates are also angry
los asociados tambien estan enfadados
the clients and the associates are enemies
los clientes y los asociados son enemigos
the company has three groups
la empresa tiene tres grupos
its groups are in europe
sus grupos estan en europa
the modern groups sell strong pharmaceuticals
los grupos modernos venden medicinas fuertes
the groups do not sell zenzanine
los grupos no venden zanzanina
the small groups are not modern
los grupos pequenos no son modernos
Figure 2: Gold alignment. The induced bilingual dic-
tionary has 28 distinct entries, including garcia/garcia,
are/son, are/estan, not/no, has/tiene, etc.
29
garcia and associates 
garcia y asociados
his associates are not strong
sus asociados no son fuertes
carlos garcia has three associates 
carlos garcia tiene tres asociados
garcia has a company also
garcia tambien tiene una empresa
its clients are angry
sus clientes estan enfadados
the associates are also angry
los asociados tambien estan enfadados
the clients and the associates are enemies
los clientes y los asociados son enemigos
the company has three groups
la empresa tiene tres grupos
its groups are in europe
sus grupos estan en europa
the modern groups sell strong pharmaceuticals
los grupos modernos venden medicinas fuertes
the groups do not sell zenzanine
los grupos no venden zanzanina
the small groups are not modern
los grupos pequenos no son modernos
Figure 3: IP alignment. The induced bilingual dictionary
has 28 distinct entries.
shows the gold alignment for the corpus in Figure 1
(displayed here as English-Spanish), which results
in 28 distinct bilingual dictionary entries. By con-
trast, a monotone alignment induces 39 distinct en-
tries, due to less re-use.
Next we look at how to automatically rifle through
all legal alignments to find the one with the best
score. What is a legal alignment? For now, we con-
sider it to be one where:
? Every foreign word is aligned exactly once
(Brown et al, 1993).
? Every English word has either 0 or 1 align-
ments (Melamed, 1997).
We formulate our integer program (IP) as follows.
We set up two types of binary variables:
? Alignment link variables. If link-i-j-k = 1, that
means in sentence pair i, the foreign word at
position j aligns to the English words at posi-
tion k.
? Bilingual dictionary variables. If dict-f-e = 1,
that means word pair (f, e) is ?in? the dictio-
nary.
We constrain the values of link variables to sat-
isfy the two alignment conditions listed earlier. We
also require that if link-i-j-k = 1 (i.e., we?ve decided
on an alignment link), then dict-fij -eik should also
equal 1 (the linked words are recorded as a dictio-
nary entry).1 We do not require the converse?just
because a word pair is available in the dictionary, the
aligner does not have to link every instance of that
word pair. For example, if an English sentence has
two the tokens, and its Spanish translation has two
la tokens, we should not require that all four links
be active?in fact, this would conflict with the 1-1
link constraints and render the integer program un-
solvable. The IP reads as follows:
minimize:?
f,e dict-f-e
subject to:
?i,j
?
k link-i-j-k = 1
?i,k
?
j link-i-j-k ? 1
?i,j,k link-i-j-k ? dict-fij -eik
On our Spanish-English corpus, the cplex2 solver
obtains a minimal objective function value of 28. To
1fij is the jth foreign word in the ith sentence pair.
2www.ilog.com/products/cplex
30
get the second-best alignment, we add a constraint
to our IP requiring the sum of the n variables active
in the previous solution to be less than n, and we
re-run cplex. This forces cplex to choose different
variable settings on the second go-round. We repeat
this procedure to get an ordered list of alignments.3
We find that there are 8 distinct solutions that
yield the same objective function value of 28. Fig-
ure 3 shows one of these. This alignment is not bad,
considering that word-order information is not en-
coded in the IP. We can now compare several align-
ments in terms of both dictionary size and alignment
accuracy. For accuracy, we represent each alignment
as a set of tuples < i, j, k >, where i is the sentence
pair, j is a foreign index, and k is an English index.
We use these tuples to calculate a balanced f-score
against the gold alignment tuples.4
Method Dict size f-score
Gold 28 100.0
Monotone 39 68.9
IBM-1 (Brown et al, 1993) 30 80.3
IBM-4 (Brown et al, 1993) 29 86.9
IP 28 95.9
The last line shows an average f-score over the 8 tied
IP solutions.
Figure 4 further investigates the connection be-
tween our objective function and alignment accu-
racy. We sample up to 10 alignments at each of
several objective function values v, by first adding
a constraint that dict variables add to exactly v, then
iterating the n-best list procedure above. We stop
when we have 10 solutions, or when cplex fails to
find another solution with value v. In this figure, we
see a clear relationship between the objective func-
tion and alignment accuracy?minimizing the for-
mer is a good way to maximize the latter.
3This method not only suppresses the IP solutions generated
so far, but it suppresses additional solutions as well. In partic-
ular, it suppresses solutions in which all link and dict variables
have the same values as in some previous solution, but some
additional dict variables are flipped to 1. We consider this a fea-
ture rather than a bug, as it ensures that all alignments in the
n-best list are unique. For what we report in this paper, we only
create n-best lists whose elements possess the same objective
function value, so the issue does not arise.
4P = proportion of proposed links that are in gold,
R = proportion of gold links that are proposed, and f-
score = 2PR/(P+R).
 0.5
 0.55
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 0.9
 0.95
 1
A
ve
ra
ge
 a
lig
nm
en
t a
cc
ur
ac
y 
 (f
-s
co
re
)
Number of bilingual dictionary entries in solution 
 (objective function value)
28 30 32 34 36 38 40 42 44 46
Figure 4: Relationship between IP objective (x-axis =
size of induced bilingual dictionary) and alignment ac-
curacy (y-axis = f-score).
Turkish English
yururum i walk
yururler they walk
Figure 5: Two Turkish-English sentence pairs.
3 Sub-Word Alignment
We now turn to alignment at the sub-word level.
Agglutinative languages like Turkish present chal-
lenges for many standard NLP techniques. An ag-
glutinative language can express in a single word
(e.g., yurumuyoruz) what might require many words
in another language (e.g., we are not walking).
Naively breaking on whitespace results in a very
large vocabulary for Turkish, and it ignores the
multi-morpheme structure inside Turkish words.
Consider the tiny Turkish-English corpus in Fig-
ure 5. Even a non-Turkish speaker might plausi-
bly align yurur to walk, um to I, and ler to they.
However, none of the popular machine aligners
is able to do this, since they align at the whole-
word level. Designers of translation systems some-
times employ language-specific word breakers be-
fore alignment, though these are hard to build and
maintain, and they are usually not only language-
specific, but also language-pair-specific. Good un-
31
supervised monolingual morpheme segmenters are
also available (Goldsmith, 2001; Creutz and Lagus,
2005), though again, these do not do joint inference
of alignment and word segmentation.
We extend our objective function straightfor-
wardly to sub-word alignment. To test our exten-
sion, we construct a Turkish-English corpus of 1616
sentence pairs. We first manually construct a regu-
lar tree grammar (RTG) (Gecseg and Steinby, 1984)
for a fragment of English. This grammar produces
English trees; it has 86 rules, 26 states, and 53 ter-
minals (English words). We then construct a tree-to-
string transducer (Rounds, 1970) that converts En-
glish trees into Turkish character strings, including
space. Because it does not explicitly enumerate the
Turkish vocabulary, this transducer can output a very
large number of distinct Turkish words (i.e., charac-
ter sequences preceded and followed by space). This
transducer has 177 rules, 18 states, and 23 termi-
nals (Turkish characters). RTG generation produces
English trees that the transducer converts to Turk-
ish, both via the tree automata toolkit Tiburon (May
and Knight, 2006). From this, we obtain a parallel
Turkish-English corpus. A fragment of the corpus is
shown in Figure 6. Because we will concentrate on
finding Turkish sub-words, we manually break off
the English sub-word -ing, by rule, as seen in the
last line of the figure.
This is a small corpus, but good for demonstrat-
ing our concept. By automatically tracing the inter-
nal operation of the tree transducer, we also produce
a gold alignment for the corpus. We use the gold
alignment to tabulate the number of morphemes per
Turkish word:
n % Turkish types % Turkish tokens
with n morphemes with n morphemes
1 23.1% 35.5%
2 63.5% 61.6%
3 13.4% 2.9%
Naturally, these statistics imply that standard whole-
word aligners will fail. By inspecting the corpus, we
find that 26.8 is the maximium f-score available to
whole-word alignment methods.
Now we adjust our IP formulation. We broaden
the definition of legal alignment to include breaking
any foreign word (token) into one or more sub-word
(tokens). Each resulting sub-word token is aligned
to exactly one English word token, and every En-
glish word aligns to 0 or 1 foreign sub-words. Our
dict-f-e variables now relate Turkish sub-words to
English words. The first sentence pair in Figure 5
would have previously contributed two dict vari-
ables; now it contributes 44, including things like
dict-uru-walk. We consider an alignment to be a set
of tuples < i, j1, j2, k >, where j1 and j2 are start
and end indices into the foreign character string. We
create align-i-j1-j2-k variables that connect Turkish
character spans with English word indices. Align-
ment variables constrain dictionary variables as be-
fore, i.e., an alignment link can only ?turn on? when
licensed by the dictionary.
We previously constrained every Turkish word to
align to something. However, we do not want ev-
ery Turkish character span to align?only the spans
explicitly chosen in our word segmentation. So we
introduce span-i-j1-j2 variables to indicate segmen-
tation decisions. Only when span-i-j1-j2 = 1 do we
require
?
k align-i-j1-j2-k = 1.
For a coherent segmentation, the set of active span
variables must cover all Turkish letter tokens in the
corpus, and no pair of spans may overlap each other.
To implement these constraints, we create a lattice
where each node represents a Turkish index, and
each transition corresponds to a span variable. In a
coherent segmentation, the sum of all span variables
entering an lattice-internal node equals the sum of
all span variables leaving that node. If the sum of
all variables leaving the start node equals 1, then we
are guaranteed a left-to-right path through the lat-
tice, i.e., a coherent choice of 0 and 1 values for span
variables.
The IP reads as follows:
minimize:?
f,e dict-f-e
subject to:
?i,j1,j2
?
k align-i-j1-j2-k = span-i-j1-j2
?i,k
?
j1,j2 align-i-j1-j2-k ? 1
?i,j1,j2,k align-i-j1-j2-k ? dict-fi,j1,j2-ei,k
?i,j
?
j3 span-i-j3-j =
?
j3 span-i-j-j3
?i,w
?
j>w span-i-w-j = 1
(w ranges over Turkish word start indices)
With our simple objective function, we obtain an
f-score of 61.4 against the gold standard. Sample
gold and IP alignments are shown in Figure 7.
32
Turkish English
onlari gordum i saw them
gidecekler they will go
onu stadyumda gordum i saw him in the stadium
ogretmenlerim tiyatroya yurudu my teachers walked to the theatre
cocuklar yurudu the girls walked
babam restorana gidiyor my father is walk ing to the restaurant
. . . . . .
Figure 6: A Turkish-English corpus produced by an English grammar pipelined with an English-to-Turkish tree-to-
string transducer.
you   go   to   his   office
onun ofisi- -ne   gider- -sin
Gold alignment IP sub-word alignment
you   go   to   his   office
onun ofisi- -ne   gider- -sin
my  teachers  ran  to  their  house
ogretmenler- -im onlarin evi- -ne  kostu
my  teachers  ran  to  their  house
ogretmenler- -im onlarin evi- -ne  kostu
i  saw  him
onu gordu- -m
i  saw  him
onu gordu- -m
we  go  to  the  theatre
tiyatro- -ya gider- -iz
we  go  to  the  theatre
tiyatro- -ya gider- -iz
they  walked  to  the  store
magaza- -ya yurudu- -ler
they  walked  to  the  store
magaza- -ya yurudu- -ler
my aunt goes to their house
hala- -m  onlarin evi- -ne  gider
my aunt goes to their house
hal- -am  onlarin evi- -ne  gider
1.
2.
3.
5.
6.
4.
Figure 7: Sample gold and (initial) IP sub-word alignments on our Turkish-English corpus. Dashes indicate where the
IP search has decided to break Turkish words in the process of aligning. For examples, the word magazaya has been
broken into magaza- and -ya.
33
The last two incorrect alignments in the figure
are instructive. The system has decided to align
English the to the Turkish noun morphemes tiyatro
and magaza, and to leave English nouns theatre and
store unaligned. This is a tie-break decision. It is
equally good for the objective function to leave the
unaligned instead?either way, there are two rele-
vant dictionary entries.
We fix this problem by introducing a special
NULL Turkish token, and by modifying the IP to re-
quire every English token to align (either to NULL
or something else). This introduces a cost for fail-
ing to align an English token x to Turkish, because
a new x/NULL dictionary entry will have to be cre-
ated. (The NULL token itself is unconstrained in
how many tokens it may align to.)
Under this scheme, the last two incorrect align-
ments in Figure 7 induce four relevant dictio-
nary entries (the/tiyatro, the/magaza, theatre/NULL,
store/NULL) while the gold alignment induces only
three (the/NULL, theatre/tiyatro, store/magaza), be-
cause the/NULL is re-used. The gold alignment is
therefore now preferred by the IP optimizer. There
is a rippling effect, causing the system to correct
many other decisions as well. This revision raises
the alignment f-score from 61.4 to 83.4.
The following table summarizes our alignment re-
sults. In the table, ?Dict? refers to the size of the
induced dictionary, and ?Sub-words? refers to the
number of induced Turkish sub-word tokens.
Method Dict Sub-words f-score
Gold (sub-word) 67 8102 100.0
Monotone (word) 512 4851 5.5
IBM-1 (word) 220 4851 21.6
IBM-4 (word) 230 4851 20.3
IP (word) 107 4851 20.1
IP (sub-word, 60 7418 61.4
initial)
IP (sub-word, 65 8105 83.4
revised)
Our search for an optimal IP solution is not fast.
It takes 1-5 hours to perform sub-word alignment on
the Turkish-English corpus. Of course, if we wanted
to obtain optimal alignments under IBM Model 4,
that would also be expensive, in fact NP-complete
(Raghavendra and Maji, 2006). Practical Model 4
systems therefore make substantial search approxi-
mations (Brown et al, 1993).
4 Related Work
(Zhang et al, 2003) and (Wu, 1997) tackle the prob-
lem of segmenting Chinese while aligning it to En-
glish. (Snyder and Barzilay, 2008) use multilingual
data to compute segmentations of Arabic, Hebrew,
Aramaic, and English. Their method uses IBM mod-
els to bootstrap alignments, and they measure the re-
sulting segmentation accuracy.
(Taskar et al, 2005) cast their alignment model as
a minimum cost quadratic flow problem, for which
optimal alignments can be computed with off-the-
shelf optimizers. Alignment in the modified model
of (Lacoste-Julien et al, 2006) can be mapped to a
quadratic assignment problem and solved with linear
programming tools. In that work, linear program-
ming is not only used for alignment, but also for
training weights for the discriminative model. These
weights are trained on a manually-aligned subset of
the parallel data. One important ?mega? feature for
the discriminative model is the score assigned by an
IBM model, which must be separately trained on the
full parallel data. Our work differs in two ways: (1)
our training is unsupervised, requiring no manually
aligned data, and (2) we do not bootstrap off IBM
models. (DeNero and Klein, 2008) gives an integer
linear programming formulation of another align-
ment model based on phrases. There, integer pro-
gramming is used only for alignment, not for learn-
ing parameter values.
5 Conclusions and Future Work
We have presented a novel objective function for
alignment, and we have applied it to whole-word and
sub-word alignment problems. Preliminary results
look good, especially given that new objective func-
tion is simpler than those previously proposed. The
integer programming framework makes the model
easy to implement, and its optimal behavior frees us
from worrying about search errors.
We believe there are good future possibilities for
this work:
? Extend legal alignments to cover n-to-m
and discontinuous cases. While morpheme-
to-morpheme alignment is more frequently a
34
1-to-1 affair than word-to-word alignment is,
the 1-to-1 assumption is not justified in either
case.
? Develop new components for the IP objec-
tive. Our current objective function makes no
reference to word order, so if the same word
appears twice in a sentence, a tie-break en-
sues.
? Establish complexity bounds for optimiz-
ing dictionary size. We conjecture that opti-
mal alignment according to our model is NP-
complete in the size of the corpus.
? Develop a fast, approximate alignment al-
gorithm for our model.
? Test on large-scale bilingual corpora.
Acknowledgments
This work was partially supported under DARPA
GALE, Contract No. HR0011-06-C-0022.
References
P. Brown, V. Della Pietra, S. Della Pietra, and R. Mercer.
1993. The mathematics of statistical machine trans-
lation: Parameter estimation. Computational linguis-
tics, 19(2).
D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2).
M. Creutz and K. Lagus. 2005. Inducing the morpho-
logical lexicon of a natural language from unannotated
text. In Proc. AKRR.
J. DeNero and D. Klein. 2008. The complexity of phrase
alignment problems. In Proc. ACL.
A. Fraser and D. Marcu. 2006. Semi-supervised training
for statistical word alignment. In Proc. ACL-COLING.
A. Fraser and D. Marcu. 2007. Getting the structure right
for word alignment: LEAF. In Proc. EMNLP-CoNLL.
M. Galley, M. Hopkins, K. Knight, and D Marcu. 2004.
What?s in a translation rule. In Proc. NAACL-HLT.
F. Gecseg and M. Steinby. 1984. Tree automata.
Akademiai Kiado.
J. Goldsmith. 2001. Unsupervised learning of the mor-
phology of a natural language. Computational Lin-
guistics, 27(2).
K. Knight. 1997. Automating knowledge acquisition for
machine translation. AI Magazine, 18(4).
S. Lacoste-Julien, B. Taskar, D. Klein, and M. Jordan.
2006. Word alignment via quadratic assignment. In
Proc. HLT-NAACL.
J. May and K. Knight. 2006. Tiburon: A weighted tree
automata toolkit. In Proc. CIAA.
I. D. Melamed. 1997. A word-to-word model of transla-
tional equivalence. In Proc. ACL.
F. J. Och and H. Ney. 2004. The alignment template
approach to statistical machine translation. Computa-
tional Linguistics, 30(4).
F. J. Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Ya-
mada, A. Fraser, S. Kumar, L. Shen, D. Smith, K. Eng,
V. Jain, Z. Jin, and D. Radev. 2004. A smorgasbord
of features for statistical machine translation. In Proc.
HLT-NAACL.
C. Quirk, A. Menezes, and C. Cherry. 2005. De-
pendency treelet translation: Syntactically informed
phrasal SMT. In Proc. ACL.
U. Raghavendra and H. K. Maji. 2006. Computational
complexity of statistical machine translation. In Proc.
EACL.
W. Rounds. 1970. Mappings and grammars on trees.
Theory of Computing Systems, 4(3).
P. Scho?nhofen, A. Benczu?r, I. B??ro?, and K. Csaloga?ny,
2008. Cross-language retrieval with wikipedia.
Springer.
B. Snyder and R. Barzilay. 2008. Unsupervised multi-
lingual learning for morphological segmentation. In
Proc. ACL.
B. Taskar, S. Lacoste-Julien, and D. Klein. 2005. A dis-
criminative matching approach to word alignment. In
Proc. EMNLP.
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-based
word alignment in statistical translation. In Proc. ACL.
D. Wu. 1997. Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora. Com-
putational Linguistics, 23(3).
Y. Zhang, S. Vogel, and A. Waibel. 2003. Integrated
phrase segmentation and alignment algorithm for sta-
tistical machine translation. In Proc. Intl. Conf. on
NLP and KE.
35
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 940?948,
Beijing, August 2010
Fast, Greedy Model Minimization for Unsupervised Tagging
Sujith Ravi and Ashish Vaswani and Kevin Knight and David Chiang
University of Southern California
Information Sciences Institute
{sravi,avaswani,knight,chiang}@isi.edu
Abstract
Model minimization has been shown to
work well for the task of unsupervised
part-of-speech tagging with a dictionary.
In (Ravi and Knight, 2009), the authors in-
voke an integer programming (IP) solver
to do model minimization. However,
solving this problem exactly using an
integer programming formulation is in-
tractable for practical purposes. We pro-
pose a novel two-stage greedy approxima-
tion scheme to replace the IP. Our method
runs fast, while yielding highly accurate
tagging results. We also compare our
method against standard EM training, and
show that we consistently obtain better
tagging accuracies on test data of varying
sizes for English and Italian.
1 Introduction
The task of unsupervised part-of-speech (POS)
tagging with a dictionary as formulated by Meri-
aldo (1994) is: given a raw word sequence and a
dictionary of legal POS tags for each word type,
tag each word token in the text. A common ap-
proach to modeling such sequence labeling prob-
lems is to build a bigram Hidden Markov Model
(HMM) parameterized by tag-bigram transition
probabilities P (ti|ti?1) and word-tag emission
probabilities P (wi|ti). Given a word sequence w
and a tag sequence t, of length N , the joint prob-
ability P (w, t) is given by:
P (w, t) =
N?
i=1
P (wi|ti) ? P (ti|ti?1) (1)
We can train this model using the Expectation
Maximization (EM) algorithm (Dempster and Ru-
bin, 1977) which learns P (wi|ti) and P (ti|ti?1)
that maximize the likelihood of the observed data.
Once the parameters are learnt, we can find the
best tagging using the Viterbi algorithm.
t? = arg max
t
P (w, t) (2)
Ravi and Knight (2009) attack the Merialdo
task in two stages. In the first stage, they search
for a minimized transition model (i.e., the small-
est set of tag bigrams) that can explain the data
using an integer programming (IP) formulation.
In the second stage, they build a smaller HMM
by restricting the transition parameters to only
those tag bigrams selected in the minimization
step. They employ the EM algorithm to train this
model, which prunes away some of the emission
parameters. Next, they use the pruned emission
model along with the original transition model
(which uses the full set of tag bigrams) and re-
train using EM. This alternating EM training pro-
cedure is repeated until the number of tag bigrams
in the Viterbi tagging output does not change be-
tween subsequent iterations. The final Viterbi tag-
ging output from their method achieves state-of-
the-art accuracy for this task. However, their mini-
mization step involves solving an integer program,
which can be very slow, especially when scal-
ing to large-scale data and more complex tagging
problems which use bigger tagsets. In this pa-
per, we present a novel method that optimizes the
same objective function using a fast greedy model
selection strategy. Our contributions are summa-
rized below:
940
? We present an efficient two-phase greedy-
selection method for solving the minimiza-
tion objective from Ravi and Knight (2009),
which runs much faster than their IP.
? Our method easily scales to large data
sizes (and big tagsets), unlike the previ-
ous minimization-based approaches and we
show runtime comparisons for different data
sizes.
? We achieve very high tagging accuracies
comparable to state-of-the-art results for un-
supervised POS tagging for English.
? Unlike previous approaches, we also show
results obtained when testing on the entire
Penn Treebank data (973k word tokens) in
addition to the standard 24k test data used for
this task. We also show the effectiveness of
this approach for Italian POS tagging.
2 Previous work
There has been much work on the unsupervised
part-of-speech tagging problem. Goldwater and
Griffiths (2007) also learn small models employ-
ing a fully Bayesian approach with sparse pri-
ors. They report 86.8% tagging accuracy with
manual hyperparameter selection. Smith and Eis-
ner (2005) design a contrastive estimation tech-
nique which yields a higher accuracy of 88.6%.
Goldberg et al (2008) use linguistic knowledge to
initialize the the parameters of the HMM model
prior to EM training. They achieve 91.4% ac-
curacy. Ravi and Knight (2009) use a Minimum
Description Length (MDL) method and achieve
the best results on this task thus far (91.6% word
token accuracy, 91.8% with random restarts for
EM). Our work follows a similar approach using a
model minimization component and alternate EM
training.
Recently, the integer programming framework
has been widely adopted by researchers to solve
other NLP tasks besides POS tagging such as se-
mantic role labeling (Punyakanok et al, 2004),
sentence compression (Clarke and Lapata, 2008),
decipherment (Ravi and Knight, 2008) and depen-
dency parsing (Martins et al, 2009).
3 Model minimization formulated as a
Path Problem
The complexity of the model minimization step
in (Ravi and Knight, 2009) and its proposed ap-
proximate solution can be best understood if we
formulate it as a path problem in a graph.
Let w = w0, w1, . . . , wN , wN+1 be a word se-
quence where w1, . . . , wN are the input word to-
kens and {w0, wN+1} are the start/end tokens.
Let T = {T1, . . . , TK}?{T0, TK+1} be the fixed
set of all possible tags. T0 and TK+1 are special
tags that we add for convenience. These would be
the start and end tags that one typically adds to
the HMM lattice. The tag dictionary D contains
entries of the form (wi, Tj) for all the possible
tags Tj that word token wi can have. We add en-
tries (w0, T0) and (wK+1, TK+1) to D. Given this
input, we now create a directed graph G(V,E).
Let C0, C1 . . . , CK+1 be columns of nodes in G,
where column Ci corresponds to word token wi.
For all i = 0, . . . , N+1 and j = 0, . . . ,K+1, we
add node Ci,j in column Ci if (wi, Tj) ? D. Now,
?i = 0, . . . , N , we create directed edges from ev-
ery node in Ci to every node in Ci+1. Each of
these edges e = (Ci,j , Ci+1,k) is given the label
(Tj , Tk) which corresponds to a tag bigram. This
creates our directed graph. Let l(e) be the tag bi-
gram label of edges e ? E. For every path P from
C0,0 to CN+1,K+1, we say that P uses an edge la-
bel or tag bigram (Tj , Tk) if there exists an edge
e in P such that l(e) = (Tj , Tk). We can now
formulate the the optimization problem as: Find
the smallest set S of tag bigrams such that there
exists at least one path from C0,0 to CN+1,K+1 us-
ing only the tag bigrams in S. Let us call this the
Minimal Tag Bigram Path (MinTagPath) problem.
Figure 1 shows an example graph where the
input word sequence is w1, . . . , w4 and T =
{T1, . . . , T3} is the input tagset. We add the
start/end word tokens {w0, w5} and correspond-
ing tags {T0, T4}. The edges in the graph are in-
stantiated according to the word/tag dictionary D
provided as input. The node and edge labels are
also illustrated in the graph. Our goal is to find a
path from C0,0 to C5,4 using the smallest set of tag
bigrams.
941
T0
T
1
T
2
T
3
T
4
w
0
w
1
w
2
w
3
w
4
w
5
T
0
,T
1
T
0
,T
3
T
1
,T
2
T
1
,T
2
T
2
,T
1
T
2
,T
2
T
3
,T
2
T
3
,T
4
T
2
,T
4
T
2
,T
3
T
2
,T
2
T
1
,T
3
C
0,0
C
1,1
C
1,3
C
2,2
C
3,1
C
3,2 C
4,2
C
4,3
C
5,4
word sequence: 
POS tags
Initial graph: G (V, E)
Figure 1: Graph instantiation for the MinTagPath problem.
4 Problem complexity
Having defined the problem, we now show that
it can be solved in polynomial time even though
the number of paths from C0,0 to CN+1,K+1 is
exponential in N , the input size. This relies on the
assumption that the tagset T is fixed in advance,
which is the case for most tagging tasks.1 Let B
be the set of all the tag bigram labels in the graph,
B = {l(e), ?e ? E}. Now, the size of B would
be at most K2 + 2K where every word could be
tagged with every possible tag. For m = 1 . . . |B|,
let Bm be the set of subsets of B each of which
have size m. Algorithm 1 optimally solves the
MinTagPath problem.
Algorithm 1 basically enumerates all the possi-
ble subsets of B, from the smallest to the largest,
and checks if there is a path. It exits the first time a
path is found and therefore finds the smallest pos-
sible set si of size m such that a path exists that
uses only the tag bigrams in si. This implies the
correctness of the algorithm. To check for path ex-
istence, we could either throw away all the edges
from E not having a label in si, and then execute
a Breadth-First-Search (BFS) or we could traverse
1If K, the size of the tagset, is a variable as well, then we
suspect the problem is NP-hard.
Algorithm 1 Brute Force solution to MinTagPath
for m = 1 to |B| do
for si ? Bm do
Use Breadth First Search (BFS) to check
if ? path P from C0,0 to CN+1,K+1 using
only the tag bigrams in si.
if P exists then
return si,m
end if
end for
end for
only the edges with labels in si during BFS. The
running time of Algorithm 1 is easy to calculate.
Since, in the worst case we go over all the sub-
sets of size m = 1, . . . , |B| of B, the number of
iterations we can perform is at most 2|B|, the size
of the powerset P of B. In each iteration, we do
a BFS through the lattice, which has O(N) time
complexity2 since the lattice size is linear in N
and BFS is linear in the lattice size. Hence the run-
ning time is? 2|B| ?O(N) = O(N). Even though
this shows that MinTagPath can be solved in poly-
nomial time, the time complexity is prohibitively
large. For the Penn Treebank, K = 45 and the
2Including throwing away edges or not.
942
worst case running time would be ? 1013.55 ? N .
Clearly, for all practical purposes, this approach is
intractable.
5 Greedy Model Minimization
We do not know of an efficient, exact algorithm
to solve the MinTagPath problem. Therefore, we
present a simple and fast two-stage greedy ap-
proximation scheme. Notice that an optimal path
P (or any path) covers all the input words i.e., ev-
ery word token wi has one of its possible taggings
in P . Exploiting this property, in the first phase,
we set our goal to cover all the word tokens using
the least possible number of tag bigrams. This can
be cast as a set cover problem (Garey and John-
son, 1979) and we use the set cover greedy ap-
proximation algorithm in this stage. The output
tag bigrams from this phase might still not allow
any path from C0,0 to CN+1,K+1. So we carry out
a second phase, where we greedily add a few tag
bigrams until a path is created.
5.1 Phase 1: Greedy Set Cover
In this phase, our goal is to cover all the word to-
kens using the least number of tag bigrams. The
covering problem is exactly that of set cover. Let
U = {w0, . . . , wN +1} be the set of elements that
needs to be covered (in this case, the word tokens).
For each tag bigram (Ti, Tj) ? B, we define its
corresponding covering set STi,Tj as follows:
STi,Tj = {wn : ((wn, Ti) ? D
? (Cn,i, Cn+1,j) ? E
? l(Cn,i, Cn+1,j) = (Ti, Tj))?
((wn, Tj) ? D
? (Cn?1,i, Cn,j) ? E
? l(Cn?1,i, Cn,j) = (Ti, Tj))}
Let the set of covering sets be X . We assign
a cost of 1 to each covering set in X . The goal
is to select a set CHOSEN ? X such that?
STi,Tj?CHOSEN
= U , minimizing the total cost
of CHOSEN . This corresponds to covering all
the words with the least possible number of tag
bigrams. We now use the greedy approximation
algorithm for set cover to solve this problem. The
pseudo code is shown in Algorithm 2.
Algorithm 2 Set Cover : Phase 1
Definitions
Define CAND : Set of candidate covering sets
in the current iteration
Define Urem : Number of elements in U re-
maining to be covered
Define ESTi,Tj : Current effective cost of a setDefine Itr : Iteration number
Initializations
LET CAND = X
LET CHOSEN = ?
LET Urem = U
LET Itr = 0
LET ESTi,Tj = 1|STi,Tj | , ? STi,Tj ? CAND
while Urem 6= ? do
Itr ? Itr + 1
Define S?Itr = argmin
STi,Tj?CAND
ESTi,Tj
CHOSEN = CHOSEN
?
S?Itr
Remove S?Itr from CAND
Remove all the current elements in S?Itr from
Urem
Remove all the current elements in S?Itr from
every STi,Tj ? CAND
Update effective costs, ? STi,Tj ? CAND,
ESTi,Tj =
1
|STi,Tj |end while
return CHOSEN
For the graph shown in Figure 1, here are a few
possible covering sets STi,Tj and their initial ef-
fective costs ESTi,Tj .
? ST0,T1 = {w0, w1}, EST0,T1 = 1/2
? ST1,T2 = {w1, w2, w3, w4}, EST1,T2 = 1/4
? ST2,T2 = {w2, w3, w4}, EST2,T2 = 1/3
In every iteration Itr of Algorithm 2, we pick a
set S?Itr that is most cost effective. The elements
that S?Itr covers are then removed from all the re-
maining candidate sets and Urem and the effec-
tiveness of the candidate sets is recalculated for
the next iteration. The algorithm stops when all
elements of U i.e., all the word tokens are cov-
ered. Let, BCHOSEN = {(Ti, Tj) : STi,Tj ?
943
CHOSEN}, be the set of tag bigrams that have
been chosen by set cover. Now, we check, using
BFS, if there exists a path from C0,0 to CN+1,K+1
using only the tag bigrams in BCHOSEN . If not,
then we have to add tag bigrams to BCHOSEN to
enable a path. To accomplish this, we carry out the
second phase of this scheme with another greedy
strategy (described in the next section).
For the example graph in Figure 1,
one possible solution BCHOSEN =
{(T0, T1), (T1, T2), (T2, T4)}.
5.2 Phase 2: Greedy Path Completion
We define a graph GCHOSEN (V ?, E?) ?
G(V,E) that contains the edges e ? E such
l(e) ? BCHOSEN .
Let BCAND = B \ BCHOSEN , be the current
set of candidate tag bigrams that can be added to
the final solution which would create a path. We
would like to know how many holes a particular
tag bigram (Ti, Tj) can fill. We define a hole as an
edge e such that e ? G \ GCHOSEN and there
exists e?, e?? ? GCHOSEN such that tail(e?) =
head(e) ? tail(e) = head(e??).
Figure 2 illustrates the graph GCHOSEN using
tag bigrams from the example solution to Phase 1
(Section 5.1). The dotted edge (C2,2, C3,1) rep-
resents a hole, which has to be filled in the cur-
rent phase in order to complete a path from C0,0
to C5,4.
In Algorithm 3, we define the effectiveness of a
candidate tag bigram H(Ti, Tj) to be the number
of holes it covers. In every iteration, we pick the
most effective tag bigram, fill the holes and recal-
culate the effectiveness of the remaining candidate
tag bigrams.
Algorithm 3 returns BFINAL, the final set of
chosen tag bigrams. It terminates when a path has
been found.
5.3 Fitting the Model
Once the greedy algorithm terminates and returns
a minimized grammar of tag bigrams, we follow
the approach of Ravi and Knight (2009) and fit
the minimized model to the data using the alter-
nating EM strategy. The alternating EM iterations
are terminated when the change in the size of the
observed grammar (i.e., the number of unique tag
Algorithm 3 Greedy Path Complete : Phase 2
Define BFINAL : Final set of tag bigrams se-
lected by the two-phase greedy approach
LET BFINAL = BCHOSEN
LET H(Ti, Tj) = |{e}| such that l(e) =
(Ti, Tj) and e is a hole, ? (Ti, Tj) ? BCAND
while @ path P from C0,0 to CN+1,K+1 using
only (Ti, Tj) ? BCHOSEN do
Define (T?i, T?j) = argmax
(Ti,Tj)?BCAND
H(Ti, Tj)
BFINAL = BFINAL
?
(T?i, T?j)
Remove (T?i, T?j) from BCAND
GCHOSEN = GCHOSEN
?{e} such that
l(e) = (Ti, Tj)
? (Ti, Tj) ? BCAND, Recalculate H(Ti, Tj)
end while
return BFINAL
bigrams in the tagging output) is ? 5%. We refer
to our entire approach using greedy minimization
followed by EM training as MIN-GREEDY.
6 Experiments and Results
6.1 English POS Tagging
Data: We use a standard test set (consisting of
24,115 word tokens from the Penn Treebank) for
the POS tagging task (described in Section 1). The
tagset consists of 45 distinct tag labels and the
dictionary contains 57,388 word/tag pairs derived
from the entire Penn Treebank. Per-token ambi-
guity for the test data is about 1.5 tags/token. In
addition to the standard 24k dataset, we also train
and test on larger data sets of 48k, 96k, 193k, and
the entire Penn Treebank (973k).
Methods: We perform comparative evaluations
for POS tagging using three different methods:
1. EM: Training a bigram HMM model using
EM algorithm.
2. IP: Minimizing grammar size using inte-
ger programming, followed by EM training
(Ravi and Knight, 2009).
3. MIN-GREEDY: Minimizing grammar size
using the Greedy method described in Sec-
944
T0
T
1
T
2
T
3
T
4
w
0
w
1
w
2
w
3
w
4
w
5
T
0
,T
1
T
1
,T
2
T
1
,T
2
T
2
,T
1
T
2
,T
4
C
0,0
C
1,1
C
1,3
C
2,2
C
3,1
C
3,2 C
4,2
C
4,3
C
5,4
word sequence: 
POS tags
 T
0
,T
1
 T
1
,T
2
 T
2
,T
4
Tag bigrams chosen after Phase 1 
(B
CHOSEN
)
Hole in graph: Edge e = (C
2,2
, C
3,1
)
Graph after Phase 1: G
CHOSEN 
(V?, E?)
Figure 2: Graph constructed with tag bigrams chosen in Phase 1 of the MIN-GREEDY method.
tion 5, followed by EM training.
Results: Figure 3 shows the tagging perfor-
mance (word token accuracy %) achieved by the
three methods on the standard test (24k tokens) as
well as Penn Treebank test (PTB = 973k tokens).
On the 24k test data, the MIN-GREEDY method
achieves a high tagging accuracy comparable to
the previous best from the IP method. However,
the IP method does not scale well which makes
it infeasible to run this method in a much larger
data setting (the entire Penn Treebank). MIN-
GREEDY on the other hand, faces no such prob-
lem and in fact it achieves high tagging accuracies
on all four datasets, consistently beating EM by
significant margins. When tagging all the 973k
word tokens in the Penn Treebank data, it pro-
duces an accuracy of 87.1% which is much better
than EM (82.3%) run on the same data.
Ravi and Knight (2009) mention that it is pos-
sible to interrupt the IP solver and obtain a sub-
optimal solution faster. However, the IP solver did
not return any solution when provided the same
amount of time as taken by MIN-GREEDY for
any of the data settings. Also, our algorithms
were implemented in Python while the IP method
employs the best available commercial software
package (CPLEX) for solving integer programs.
Figure 4 compares the running time efficiency
for the IP method versus MIN-GREEDY method
Test set Efficiency
(average running time in secs.)
IP MIN-GREEDY
24k test 93.0 34.3
48k test 111.7 64.3
96k test 397.8 93.3
193k test 2347.0 331.0
PTB (973k) test ? 1485.0
Figure 4: Comparison of MIN-GREEDY versus
MIN-GREEDY approach in terms of efficiency
(average running time in seconds) for different
data sizes. All the experiments were run on a sin-
gle machine with a 64-bit, 2.4 GHz AMD Opteron
850 processor.
as we scale to larger datasets. Since the IP solver
shows variations in running times for different
datasets of the same size, we show the average
running times for both methods (for each row in
the figure, we run a particular method on three
different datasets with similar sizes and average
the running times). The figure shows that the
greedy approach can scale comfortably to large
data sizes, and a complete run on the entire Penn
Treebank data finishes in just 1485 seconds. In
contrast, the IP method does not scale well?on
average, it takes 93 seconds to finish on the 24k
test (versus 34 seconds for MIN-GREEDY) and
on the larger PTB test data, the IP solver runs for
945
Method Tagging accuracy (%)
when training & testing on:
24k 48k 96k 193k PTB (973k)
EM 81.7 81.4 82.8 82.0 82.3
IP 91.6 89.3 89.5 91.6 ?
MIN-GREEDY 91.6 88.9 89.4 89.1 87.1
Figure 3: Comparison of tagging accuracies on test data of varying sizes for the task of unsupervised
English POS tagging with a dictionary using a 45-tagset. (? IP method does not scale to large data).
 400
 600
 800
 1000
 1200
 1400
 1600
Ob
se
rv
ed
 g
ra
mm
ar
 s
iz
e 
(#
 o
f 
ta
g 
bi
gr
am
s)
 
 i
n 
fi
na
l 
ta
gg
in
g 
ou
tp
ut
Size of test data (# of word tokens)
24k 48k 96k 193k PTB (973k)
EM
IP
Greedy
Figure 5: Comparison of observed grammar size
(# of tag bigram types) in the final tagging output
from EM, IP and MIN-GREEDY.
more than 3 hours without returning a solution.
It is interesting to see that for the 24k dataset,
the greedy strategy finds a grammar set (contain-
ing only 478 tag bigrams). We observe that MIN-
GREEDY produces 452 tag bigrams in the first
minimization step (phase 1), and phase 2 adds an-
other 26 entries, yielding a total of 478 tag bi-
grams in the final minimized grammar set. That
is almost as good as the optimal solution (459
tag bigrams from IP) for the same problem. But
MIN-GREEDY clearly has an advantage since it
runs much faster than IP (as shown in Figure 4).
Figure 5 shows a plot with the size of the ob-
served grammar (i.e., number of tag bigram types
in the final tagging output) versus the size of the
test data for EM, IP and MIN-GREEDY methods.
The figure shows that unlike EM, the other two
approaches reduce the grammar size considerably
and we observe the same trend even when scaling
Test set Average Speedup Optimality Ratio
24k test 2.7 0.96
48k test 1.7 0.98
96k test 4.3 0.98
193k test 7.1 0.93
Figure 6: Average speedup versus Optimality ra-
tio computed for the model minimization step
(when using MIN-GREEDY over IP) on different
datasets.
to larger data. Minimizing the grammar size helps
remove many spurious tag combinations from the
grammar set, thereby yielding huge improvements
in tagging accuracy over the EM method (Fig-
ure 3). We observe that for the 193k dataset, the
final observed grammar size is greater for IP than
MIN-GREEDY. This is because the alternating
EM steps following the model minimization step
add more tag bigrams to the grammar.
We compute the optimality ratio of the MIN-
GREEDY approach with respect to the grammar
size as follows:
Optimality ratio = Size of IP grammarSize of MIN-GREEDY grammar
A value of 1 for this ratio implies that the solu-
tion found by MIN-GREEDY algorithm is exact.
Figure 6 compares the optimality ratio versus av-
erage speedup (in terms of running time) achieved
in the minimization step for the two approaches.
The figure illustrates that our solution is nearly op-
timal for all data settings with significant speedup.
The MIN-GREEDY algorithm presented here
can also be applied to scenarios where the dictio-
nary is incomplete (i.e., entries for all word types
are not present in the dictionary) and rare words
946
Method Tagging accuracy (%) Number of unique tag bigrams in final tagging output
EM 83.4 1195
IP 88.0 875
MIN-GREEDY 88.0 880
Figure 7: Results for unsupervised Italian POS tagging with a dictionary using a set of 90 tags.
take on all tag labels. In such cases, we can fol-
low a similar approach as Ravi and Knight (2009)
to assign tag possibilities to every unknown word
using information from the known word/tag pairs
present in the dictionary. Once the completed dic-
tionary is available, we can use the procedure de-
scribed in Section 5 to minimize the size of the
grammar, followed by EM training.
6.2 Italian POS Tagging
We also compare the three approaches for Italian
POS tagging and show results.
Data: We use the Italian CCG-TUT corpus (Bos
et al, 2009), which contains 1837 sentences. It
has three sections: newspaper texts, civil code
texts and European law texts from the JRC-Acquis
Multilingual Parallel Corpus. For our experi-
ments, we use the POS-tagged data from the
CCG-TUT corpus, which uses a set of 90 tags.
We created a tag dictionary consisting of 8,733
word/tag pairs derived from the entire corpus
(42,100 word tokens). We then created a test set
consisting of 926 sentences (21,878 word tokens)
from the original corpus. The per-token ambiguity
for the test data is about 1.6 tags/token.
Results: Figure 7 shows the results on Italian
POS tagging. We observe that MIN-GREEDY
achieves significant improvements in tagging ac-
curacy over the EM method and comparable to IP
method. This also shows that the idea of model
minimization is a general-purpose technique for
such applications and provides good tagging ac-
curacies on other languages as well.
7 Conclusion
We present a fast and efficient two-stage greedy
minimization approach that can replace the inte-
ger programming step in (Ravi and Knight, 2009).
The greedy approach finds close-to-optimal solu-
tions for the minimization problem. Our algo-
rithm runs much faster and achieves accuracies
close to state-of-the-art. We also evaluate our
method on test sets of varying sizes and show that
our approach outperforms standard EM by a sig-
nificant margin. For future work, we would like
to incorporate some linguistic constraints within
the greedy method. For example, we can assign
higher costs to unlikely tag combinations (such as
?SYM SYM?, etc.).
Our greedy method can also be used for solving
other unsupervised tasks where model minimiza-
tion using integer programming has proven suc-
cessful, such as word alignment (Bodrumlu et al,
2009).
Acknowledgments
The authors would like to thank Shang-Hua Teng
and Anup Rao for their helpful comments and
also the anonymous reviewers. This work was
jointly supported by NSF grant IIS-0904684,
DARPA contract HR0011-06-C-0022 under sub-
contract to BBN Technologies and DARPA con-
tract HR0011-09-1-0028.
References
Bodrumlu, T., K. Knight, and S. Ravi. 2009. A new
objective function for word alignment. In Proceed-
ings of the NAACL/HLT Workshop on Integer Pro-
gramming for Natural Language Processing.
Bos, J., C. Bosco, and A. Mazzei. 2009. Converting a
dependency treebank to a categorial grammar tree-
bank for Italian. In Proceedings of the Eighth In-
ternational Workshop on Treebanks and Linguistic
Theories (TLT8).
Clarke, J. and M. Lapata. 2008. Global inference for
sentence compression: An integer linear program-
ming approach. Journal of Artificial Intelligence
Research (JAIR), 31(4):399?429.
Dempster, A.P., N.M. Laird and D.B. Rubin. 1977.
Maximum likelihood from incomplete data via the
947
EM algorithm. Journal of the Royal Statistical So-
ciety, 39(1):1?38.
Garey, M. R. and D. S. Johnson. 1979. Computers
and Intractability: A Guide to the Theory of NP-
Completeness. John Wiley & Sons.
Goldberg, Y., M. Adler, and M. Elhadad. 2008.
EM can find pretty good HMM POS-taggers (when
given a good start). In Proceedings of the 46th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies
(ACL/HLT).
Goldwater, Sharon and Thomas L. Griffiths. 2007.
A fully Bayesian approach to unsupervised part-of-
speech tagging. In Proceedings of the 45th Annual
Meeting of the Association for Computational Lin-
guistics (ACL).
Martins, A., N. A. Smith, and E. P. Xing. 2009. Con-
cise integer linear programming formulations for
dependency parsing. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the As-
sociation for Computational Linguistics (ACL) and
the 4th International Joint Conference on Natural
Language Processing of the AFNLP.
Merialdo, B. 1994. Tagging English text with a
probabilistic model. Computational Linguistics,
20(2):155?171.
Punyakanok, V., D. Roth, W. Yih, and D. Zimak.
2004. Semantic role labeling via integer linear pro-
gramming inference. In Proceedings of the Inter-
national Conference on Computational Linguistics
(COLING).
Ravi, S. and K. Knight. 2008. Attacking decipher-
ment problems optimally with low-order n-gram
models. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP).
Ravi, S. and K. Knight. 2009. Minimized models
for unsupervised part-of-speech tagging. In Pro-
ceedings of the Joint Conference of the 47th An-
nual Meeting of the Association for Computational
Linguistics (ACL) and the 4th International Joint
Conference on Natural Language Processing of the
AFNLP.
Smith, N. and J. Eisner. 2005. Contrastive estima-
tion: Training log-linear models on unlabeled data.
In Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics (ACL).
948
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1489?1499, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Revisiting the Predictability of Language:
Response Completion in Social Media
Bo Pang Sujith Ravi
Yahoo! Research
4401 Great America Parkway
Santa Clara, CA 95054, USA
bopang42@gmail.com sujith ravi@yahoo.com
Abstract
The question ?how predictable is English??
has long fascinated researchers. While prior
work has focused on formal English typically
used in news articles, we turn to texts gener-
ated by users in online settings that are more
informal in nature. We are motivated by a
novel application scenario: given the difficulty
of typing on mobile devices, can we help re-
duce typing effort with message completion,
especially in conversational settings? We pro-
pose a method for automatic response comple-
tion. Our approach models both the language
used in responses and the specific context pro-
vided by the original message. Our experi-
mental results on a large-scale dataset show
that both components help reduce typing ef-
fort. We also perform an information-theoretic
study in this setting and examine the entropy
of user-generated content, especially in con-
versational scenarios, to better understand pre-
dictability of user generated English.
1 Introduction
How predictable is language? As early as 1951, long
before large quantities of texts (or the means to pro-
cess them) were easily available, Shannon had raised
this question and proceeded to answer it with a set
of clever analytical estimations. He studied the pre-
dictability of printed English, or ?how well can the
next letter of a text be predicted when the preced-
ing N letters are known? (Shannon, 1951). This
was quantified as the conditional entropy, which
measures the amount of information conveyed from
statistics over the preceding context. In this paper,
we discuss a novel application setting which mirrors
the predictability study as defined by Shannon.
(a) Google (b) Amazon (c) Netflix
Figure 1: Query completion as users type into the ?Search
using Google? box on a browser, as well as the search box
in Amazon and Netflix.
Text completion for user-generated texts: Con-
sider a user who is chatting with her contact or post-
ing to a social media site using a mobile device. If
we can predict the next word given the preceding
words that were already typed in, we can help reduce
the typing cost by offering users suggestions of pos-
sible completions of their partially typed messages
(e.g., in a drop-down list). If the intended word is
ranked reasonably high, the user can select the word
instead of typing it. Assuming a lower cost associ-
ated with selections, this could lead to less typing
effort for the user.
An interface like this would be quite familiar to
Web users today. Providing suggestions of possi-
ble completions to partially typed queries, which we
will refer to as query completion,1 is a common fea-
ture of search boxes (Figure 1). In spite of the
similarity in the interface, the underlying technical
challenge can be quite different. Query completion
does not necessarily rely on language models: can-
1Note that this feature is often tagged as ?query suggestion?
in the user interface; we avoid that terminology since it is often
used to refer to query re-formulation (of a completely entered
query) in the literature, which is a very different task.
1489
didate completions can be limited to popular queries
that were previously submitted to the site or entries
in a closed database of available objects, and rank-
ing can be done by overall popularity. In contrast,
our scenario requires generation of unseen texts.
Given the difficulty of generating full-length text,
we consider a more realistic setting, where we per-
form completion on a word-by-word basis. Each
time, we propose candidate completions at the word-
level when the user is about to start a new word,
or has partially entered the first few letters; once
this word is successfully completed, we move on
to the next one. This predict-verify-predict process
exactly mirrors the human experiment described by
Shannon (1951), except we do this at the word-level
rather than the letter-level: having the user examine
and verify predictions at the letter level would not be
a practical solution for the intended application.
The response completion task: In addition, our
task has another interesting difference from Shan-
non?s human experiment. Consider the mobile-
device user mentioned previously. If the user is re-
plying to a piece of text (e.g., an instant message
sent by a contact), we have an additional source of
contextual information in the stimulus, or the text
which triggered the response that the user is try-
ing to type. Can we learn from previously observed
stimulus-response pairs (which we will refer to as
exchanges)? That is, can we take advantage of this
conversational setting and effectively use the infor-
mation provided by stimulus to better predict the
next word in the response? We refer to this task as
the response completion task.
Our task is different from ?chatter-bots? (Weizen-
baum, 1966), where the goal is to generate a re-
sponse to an input that would resemble a human con-
versation partner. Instead, we want to complete a
response as the replier intends to. Recently, Ritter
et. al (2011) experimented with automatic response
generation in social media. They had a similar con-
versational setting, but instead of completion based
on partial input, they attempted to generate a re-
sponse in its entirety given only the stimulus. While
many of the generated responses are deemed possi-
ble replies to the stimulus, they have a low chance
of actually matching the real response given by the
user: they reported BLEU scores between 0 and 2
for various systems. This clearly shows the diffi-
culty of the task. While we are addressing a more
modest setting, would the problem prove to be too
difficult even in this case?
In this paper, we propose a method for auto-
matic response completion. Our approach models
the generic language used in responses, as well as
the contextual information provided by the stim-
ulus. We construct a large-scale dataset of user-
generated textual exchanges, and our experimental
results show that both components help reduce typ-
ing effort. In addition, to better understand pre-
dictability of user generated English, we perform
an information-theoretic study in this conversational
setting to investigate the entropy of user-generated
content.
2 Related work
There has been previous work in the area of human-
computer interaction that examined text entry for
mobile devices (MacKenzie and Soukoreff, 2002).
In particular, one line of work looked into predic-
tive text input, which examined input effort reduc-
tion by language prediction. Previous work in pre-
dictive text input had very different focus from our
study. Oftentimes, the focus was to model actual
typing efforts using mobile device keypads, examine
the speed and cognitive load of different input meth-
ods, and evaluate with emprical user studies in lab
settings (James and Reischel, 2001; How and Kan,
2005), where the underlying technique for language
prediction can be as simple as unigram frequency
(James and Reischel, 2001), or restricted to narrow
domains such as grocery shopping lists (Nurmi et
al., 2009). In addition, to the best of our knowledge,
no previous work in predictive text input addressed
the conversationl setting.
As discussed in Section 1, the response genera-
tion task (Ritter et al 2011) also considered the con-
verstational setting, but the MT-based technique was
not well-suited to produce responses as intended by
the user. There has been extensive previous research
in language modeling (Rosenfeld, 2000). While pre-
vious work has explored Web text sources that are
?better matched to a conversational speaking style?
(Bulyko et al 2003), we are not aware of much pre-
vious work that has taken advantage of information
in the stimulus for word predictions in responses.
1490
Previous work on entropy of language stems from
the field of information theory (Shannon, 1948),
starting with Shannon (1951). An extensive bibliog-
raphy covering early related work (e.g., insights into
the structure of language via information theory, en-
tropy estimates via other techniques and/or for dif-
ferent languages, as well as a broad range of applica-
tions of such estimates) can be found in (Cover and
King, 1978). More recently, Brown et. al (1992)
computed an upper bound for the entropy of En-
glish with a trigram model, using the Brown corpus.
Some other related works on this topic include (Tea-
han and Cleary, 1996; Moradi et al 1998). There
was also a recent study using entropy in the context
of Web search (Mei and Church, 2008). In other set-
tings, entropy has also been employed as a tool for
studying the linguistic properties of ancient scripts
(e.g., Indus Script) (Rao et al 2009). While this
seems like an interesting application of information
theory for linguistic studies, it has also generated
some controversies (Farmer et al 2004).
In contrast, our work departs from traditional sce-
narios significantly. We perform entropy studies
over texts generated in online settings which are
more informal in nature. Additionally, we utilize the
properties of language predictability within a novel
application for automatically completing responses
in conversational settings. Also, in our case we do
not have to worry about issues like ?is this a lan-
guage or not?? because we work with real English
news data which include articles written by pro-
fessional editors and comments generated by users
reading those articles.
3 Model
In this section, we first state our problem more for-
mally, followed by descriptions of the basicN -gram
language model we use, as well as two approaches
that model both stimulus and preceding words in
response as the context for the next-word genera-
tion. Given the intended application, we hope to
achieve better prediction without incurring signifi-
cant increase in model size.
3.1 Problem definition
Consider a stimulus-response pair, where the stim-
ulus is a sequence of tokens s = (s1, s2, ..., sm),
and the response is a sequence of tokens r =
(r1, r2, ..., rn). Let r1..i = (r1, r2, ..., ri), our task
is to generate and rank candidates for ri+1 given s
and r1..i.
Note the models described in this section do not
assume any knowledge of partial input for ri+1. For
the setting where the first c characters of ri+1 were
also entered, we can restrict the candidate list to the
subset with the matching prefix, and use the same
ranking function.
3.2 Generic Response Language Model
First, we consider an N -gram language model
trained on all responses in the training data as our
generic response language model. Here we consider
N = 3. Normally, trigram models use back-off to
both bigrams and unigrams; in order to compare the
effectiveness of trigram models vs. bigram models
under comparable model size, we use back-off only
to unigrams in both cases:
trigram: P (ri+1 | r1..i) = ?1 ? P3(ri+1 | ri, ri?1)
+(1? ?1) ? P1(ri+1)
bigram: P (ri+1 | r1..i) = ?1 ? P2(ri+1 | ri)
+(1? ?1) ? P1(ri+1)
If we ignore the context provided by texts in the
stimulus, we can simply generate and rank candidate
words from the dictionary according to the generic
response LM: P (ri+1 | r1..i).
As we will discuss in more detail in Section 6.2,
modeling s and r1..i jointly in the prediction of ri+1
would be rather expensive. In the following sec-
tions, we follow two main approaches to break this
down into separate components: P (ri+1 | r1..i) and
P (ri+1 | s), and model each one separately.
3.3 Translating Stimuli to Responses
As mentioned in Section 1, Ritter et. al (2011) have
considered a related task of generating a response in
its entirety given only the text in the stimulus. They
cast the problem as a translation task, where the
stimulus is considered as the source language and
the response is considered as the target language.
We can adapt this approach for our response com-
pletion task.
Consider the noisy channel model used in statisti-
cal machine translation: P (r|s) ? P (r)?P (s|r). In
1491
order to predict ri+1 given r1..i and s, for each can-
didate ri+1, in principle one can marginalize over
all possible completions of r1..i+1, and rank candi-
date ri+1 by that. That is, let P(n) be the distribution
of response length, let r? be a possible completion
of r1..i+1 (i.e., a response whose first i + 1 tokens
match r1..i+1). For each possible n > i, we need to
marginalize over all possible r? of length n, and rank
ri+1 according to
P (r1..i+1 | s) =
?
n>i
P (n) ?
?
|r?|=n
P (r? | s)
Clearly this will be computationally expensive. In-
stead, we take a greedy approach, and choose ri+1
which yields the optimal partial response (without
looking ahead):
P (r1..i+1 | s) ? P (r1..i+1) ? P (s | r1..i+1)
which is equivalent to ranking candidate ri+1 by
P (ri+1 | r1..i) ? P (s | r1..i+1) (1)
Since the first component is our LM model, and the
second component is a translation model, we denote
this as the LM+TM model. We use IBM Model-1 to
learn the translation table on the training data. At
test time, equal number of candidates are generated
by each component, and combined to be ranked by
Eq. 1.
3.4 Mixture Model
One potential concern over applying the translation
model is that the response can often contain novel in-
formation not implied by the stimulus. While tech-
nically this could be generated from the so-called
null token used in machine translation (added to the
source text to account for target text with no clear
alignment in the source text), significant amount of
text corresponding to new information not in the
source text is not what null tokens are meant to be
capturing. In general, our problem here is a lack of
clear word-to-word or phrase-to-phrase mapping in
a stimulus-response pair, at least not what one would
expect in clean parallel data.
Alternatively, one can model the response genera-
tion process with a mixture model: with probability
?s, we generate a word according to a distribution
over s (P (w | s)), and with probability 1 ? ?s, we
generate a word using the response language model:
P (ri+1 | s, r1..i) = ?s ? P (ri+1 | s)
+ (1? ?s) ? P (ri+1 | r1..i)
(2)
We examine two concrete ways of exploiting the
context provided by s.
Model 1 ? LM + Selection model First, we ex-
amine a very simple instantiation of P (w | s) where
we select a token in s uniformly at random. This
is based on the intuition that to be semantically co-
herent, a reply often needs to repeat certain content
words in the stimulus. (Similar intuition has been
explored in the context of text coherency (Barzilay
and Lapata, 2005).) This is particularly useful for
words that are less frequently used: they may not
be able to receive enough statistics to be promoted
otherwise. More specifically,
P (ri+1 | s) =
1ri+1?s
|s|
We can take ?s to be a constant ?select, which can be
estimated in the training data as the probability of a
response token being a repetition of a token in the
corresponding stimulus.
Model 2 ? LM + Topic model Another way to
incorporate information provided in s is to use it to
constrict the topic in r. We can learn a topic model
over conversations in the training data using Latent
Dirchlet Allocation (LDA) (Blei et al 2003). At test
time, we identify the most likely topic of the conver-
sation based on s, and expect ri+1 to be generated
from this topic. That is,
P (ri+1 | s) = P (ri+1 | t?)
where t? = argmaxtP (topic = t | s)
More specifically, we first train a topic model on (s,
r) pairs from the training data. Given a new stim-
ulus s, we then select the highest ranked topic as
being representative of s. Note that alternatively we
could consider all possible topic assignments; in that
case we would have had to sum probabilities over
all topics, and that could also introduce noise. A
similar strategy has been previously employed for
1492
other topic modeling applications in information re-
trieval, where documents are smoothed with their
highest ranked topic (Yi and Allan, 2009). We lower
the weight ?s if P (t? | s) is low. That is, we use
?s = ?topic ? P (t? | s) in Eq. 2.
4 Data
In order to investigate text completion in a conver-
sational setting, we need to construct a large-scale
dataset with textual exchanges among users. An
ideal dataset would have been a collection of in-
stant messages, but these type of datasets are diffi-
cult to obtain given privacy concerns. To the best
of our knowledge, existing SMS (short message ser-
vice) datasets only contain isolated text spans and do
not provide enough information to reconstruct the
conversations. There are, however, a high volume
of textual exchanges taking places in public forums.
Many sites with a user comment environment allow
other users to reply to existing comments, where the
original comment and its reply can form a (stimulus,
response) pair for our purposes.
To this end, we extracted (comment, reply) pairs
from Yahoo! News2, where under each news article,
a user can post a new comment or reply to an exist-
ing comment. In fact, a popular comment can have a
long thread of replies where multi-party discussions
take place. To ensure the reply is a direct response to
the original comment, we took only the first reply to
a comment, and consider the resulting pair as a tex-
tual exchange in the form of a (stimulus, response)
pair. We gathered data from a period of 14 weeks
between March and May, 2011. A random sample
yielded a total of 1,487,995 exchanges, representing
237,040 unique users posting responses to stimuli
comments authored by 357,811 users. In the raw
dataset (i.e., before tokenization), stimuli average at
59 tokens (332 characters), and responses average at
26 tokens (144 characters).
We took the first 12 weeks of data as training data
(1,269,732 exchanges) and the rest 2 weeks of data
as test data (218,263 exchanges).
2Note that previous work has used a dataset with 1 million
Twitter conversations extracted from a scraping of the site (Rit-
ter et al 2011), where a status update and its replies in Twitter
form ?conversations?. This dataset is no longer publicly avail-
able. At the time of this writing, we were not able to identify a
data source to re-construct a dataset like that.
5 Experiments
5.1 Evaluation measures
Recall@k : Here, we follow a standard evaluation
strategy used to assess ranking quality in informa-
tion retrieval applications. For each word, we check
if the correct answer is one of the top-k tokens being
suggested. We then compute the recall at different
values of k. While this is a straight-forward mea-
sure to assess the overall quality of different top-k
lists, it is not tailored to suit our specific task of re-
sponse completion. In particular, this measure (a)
does not distinguish between (typing) savings for a
short word versus a long one, and (b) does not dis-
tinguish between the correct answer being higher up
in the list versus lower as long as the word is present
in the top-k list.
TypRed : Our main evaluation measure is based on
?reduction in typing effort for a user of the system?,
which is a more informative measure for our task.
We estimate the typing reduction via a hypothetical
typing model3 in the following manner:
Suppose we show top k predictions for a given
setting. Now, there are two possible scenarios:
1. if the user does not find the correct answer in
the top-k list, he/she gives up on this word and
will have to type the entire word. The typing
cost is then estimated to be the number of char-
acters in the word lw;
2. if the user spots the correct answer in the list,
the cost for choosing the word is proportional
to the rank of the word rankw, with a fixed cost
ratio c0. Suppose the user scrolls down the list
using the down-arrow (?) to reach the intended
word (instead of typing), then rankw ? c0 re-
flects the scrolling effort required, where c0 is
the relative cost of scrolling down versus typing
a character.
In general, pressing a fixed key can have a lower
cost than typing a new one, in addition, we can
imagine a virtual keyboard where navigational keys
occupy bigger real-estate, and thus incur less cost
to press. As a result, it?s reasonable to assume c0
value that is smaller than 1. In all our experiments,
c0 = 0.5 unless otherwise noted. Note that if the
3More accurate measures can be be developed by observing
user behavior in a lab setting. We leave this as future work.
1493
System TypRed (c = 0) TypRed (c = 1) TypRed (c = 2)
1. Generic Response LM (trigram) 15.10 22.57 14.29
2. Generic Response LM (trigram) + TM 9.03 17.53 11.56
3. Mixture Model 1: 15.18? 23.43? 15.13?
Generic Response LM (trigram) + Selection
4. Mixture Model 2: 15.10 22.57 14.33
Generic Response LM (trigram) + Topic Model
Table 1: Comparison of various prediction models (in terms of TypRed score @ rank 5) on all (stimulus,response)
pairs from a large test collection (218,263 exchanges) when the first c characters of each word are typed in by the user.
A higher score indicates better performance. ? indicates statistical significance (p < 0.05) over the baseline score.
typing model assumes a user selects the intended
word using an interface that is similar to a mousing
device, the cost may increase with rankw at a sub-
linear rate; in that case, our measure will be over-
estimating the cost.
In order to have a consistent measure that al-
ways improves as the ranking improves, we assume
a clever user who will choose to finish the word by
typing or by selecting, depending on which cost is
lower. Combining these two cases under the clever-
user model, we estimate the reduction in typing cost
for every word as follows:
TypRed(w, rankw) = 100?[1?
min(lw, rankw ? c0)
lw
]
where w is the correct word, lw is the length of
w, and rankw is the rank of w in the top-k list.
A higher value of TypRed implies higher savings
achieved in typing cost and thereby better prediction
performance.
5.2 Experimental setup
We run experiments using the models described in
Section 3 under two different settings: (1) previous
words from the response are provided, and (2) pre-
vious words from response + first c characters of the
current word are provided.
During the candidate generation phase, for every
position in the response message we present the top
1,000 candidates (as scored by the generic response
language model or mixture models). We reserve a
small subset of (?1,000) exchanges as development
data for tuning parameters from our models.
For the generic response language models, we
set the interpolation weight ?1 = 0.9. For the
selection-based mixture model, we estimate the mix-
ture weights on the training data and set ?select
(0.09). For the topic-based mixture model, we ran
a grid search with different parameter settings for
?topic on the held-out development set and chose the
value (0.01) that gave the best performance (in terms
of TypRed).
5.3 Results
Previous words from response observed: We first
present results for the setting where only previous
words from the response are provided as context.
We use TypRed scores as our evaluation measure
here (higher TypRed implies more savings in typ-
ing effort). Even with a unigram LM we achieve
a small but non-negligible reduction (TypRed=2.15)
in the typing cost. But a bigram LM significantly
improves performance (TypRed=11.91), and with
trigram LM we observe even better performance
(TypRed=15.10). Since the trigram LM yields a
high performance, we set this as our default LM for
all other models.
Recall that in all experiments, we set c0, the cost
ratio of selecting a candidate from the ranked top-
k list (via scrolling) versus typing a character to a
value of 0.5. But we also experimented with a hy-
pothetical setting where c0 = 1 and noticed that the
trigram LM achieves a slightly lower but still signif-
icant typing reduction (TypRed score of 9.58 versus
15.10 for the earlier case).
The first column of Table 1 (c = 0) compares the
performance of other models for this setting. We
find that adding a translation model (LM+TM) does
not help for this task; in fact, it results in lower
scores than using the LM alone. This suggests that
a translation-based generative approach may not be
suitable, if the goal is to predict text as intended by
the user. This is consistent with previous observa-
tions on a related task (Ritter et al 2011), as we
1494
discussed in Section 1.
In contrast, the mixture models do much better.
In fact, LM+Selection model produces better results
than trigram LM alone. We also note that estimat-
ing the mixture parameter on the training data rather
than using a fixed value increases TypRed scores:
14.02 with a fixed ?select = 0.5 versus 15.18 with
??select = 0.09. This comparison also holds for
c > 0 ? that is, a naive version of LM+Selection
that selects a word from the stimulus whenever the
prefix allows would not have worked well.
In principle the LM+Topic model is potentially
more powerful in that P (w | s) is not limited to
the words in s. However, in our experiments it
does not yield any considerable improvement over
the original LM. We postulate that this could be due
to the following reason: once the context provided
by s is reduced to the topic level, it is not specific
enough to provide additional information over pro-
ceding words in the response.
Previous words from response + first c characters
of current word observed: Table 1 also compares
the TypRed performance of all the models under set-
tings where c > 0. We notice striking improve-
ments in performance for c = 1 which is consistent
across all models. Our best model is able to save
the user approximately 23% in terms of typing ef-
fort (according to TypRed scores). Interestingly a lot
less reduction was observed for c = 2: the second
character, on average, does not improve the ranking
enough to justify the cost of typing this extra char-
acter.
Next, we pick our best model (Mixture Model 1)
and perform some further analysis. We examine the
effect of providing longer list (shown in Table 2) and
notice little further improvement beyond k = 10.
We also note that the TypRed improvement achieved
over the baseline (LM) model at rank 10 is more than
twice the gain achieved at rank 5.
We also evaluated its performance using a stan-
dard measure (Recall@k). Figure 2 plots the recall
achieved by the system at different ranks k. An in-
creasing recall at even high ranks (k = 100) sug-
gests that the quality of the candidate list retrieved
by this model is good. This also suggests that there
is still room for improvements, and we leave that as
interesting future work.
Rank (k) TypRed score
1 9.02
5 15.18
10 16.14
15 16.28
20 16.29
25 16.29
Table 2: Comparison of typing reductions achieved over
the entire test data when top k list is provided to the user.
 0 10 20 30 40 50 60 70  0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 100
Recall @ k
Rank 
(k)
Figure 2: Recall @ rank k for Mixture Model 1 on the
entire test data.
 
0
 
5
 
10
 
15
 
20
 
25
 
30
 
35  0
 
2
 
4
 
6
 
8
 
10
 
12
Average TypRed
Word 
length
 (# of ch
aracters
)
no lett
er prov
ided (c=
0)
first le
tter pro
vided 
(c=1)
first 2 
letters
 provid
ed (c=2
)
Figure 3: Average TypRed score versus Word length (#
of characters) for Mixture Model 1 when the first c char-
acters of the word is typed in by the user.
Finally, in Figure 3, we plot the average TypRed
scores against individual token (word) length. Fig-
ure 3 indicates that the model is able to achieve a
higher reduction for shorter words compared to very
long ones. This demonstrates the utility of such a re-
sponse completion system, especially since shorter
words are predominant in conversational settings.
We also compared the average reduction achieved
on messages of different lengths (number of words).
Overall we observe consistent reduction for differ-
ent message lengths. This suggests our system can
1495
Source Top translations
:) :) ! you ? :D
lmao lol lmao u ... i
feeling feeling feel better ! you
question . question the , to
Are I . , are yes
Table 3: Examples of a few stimulus/response transla-
tions learned using IBM Model-1.
OBAMA, USA, Fact, Meghan, GIVE, PRESIDENT, Canadian,
Mitch, Jon, Kerry, TODAY, Justice, Liberalism, ...
President, Notice, Tax, LMAO, Hmmm, Trump, people,
OBAMA, common, Aren, WAIT, Bachman, mon, McCain, ...
Great, Cut, Release, Ummm, Rest, Mark, isnt, YAHOO, Sad,
END, RON, jesus, Ugh, TRUMP, ...
Nice, Navy, Make, Interesting, Remember, Excuse, WAKE,
Hooray, Birth, mon, Yeah, Dumb, Michael, geronimo, ...
Table 4: Examples of top representative words for a few
topics generated by the LDA topic model trained on news
comment data.
be useful for both Tweet-like short messages as well
as more lengthy exchanges in detailed discussions.
5.4 Discussion
Table 3 displays some sample translations learned
using the TM model described in Section 3.3. In-
terestingly, emoticons and informal expressions like
:) or lmao in the stimulus tend to evoke similar
type of expressions in the response (as seen in Ta-
ble 3). Some translations (e.g., feeling? better) are
indicative of question/answer type of exchanges in
our data. But most of the other translations are noisy
or uninformative (e.g., Are? .). This provides fur-
ther evidence as to why a translation-based approach
is not well suited for this particular task and hence
does not perform as well as other methods.
Finally, in Table 4, we provide a few sample top-
ics generated by the LDA topic model (which is used
by Mixture Model 2 described in Section 3.4). We
find that while a few topics display some seman-
tic coherence (e.g., political figures), many of them
are noisy (or too generic) which further supports our
earlier observation that they are not useful enough to
help in the prediction task.
6 Entropy of user comments
We adapt the notion of predictability of English as
examined by Shannon (1951) from letter-prediction
to token-prediction, and define the predictability of
English as how well can the next token be predicted
when the precedingN tokens are known. How much
does the immediate context in the response help re-
duce the uncertainty? How does user-generated con-
tent compare with more formal English in this re-
spect? And how about the corresponding stimuli ?
given the preceding N tokens, does the knowledge
of stimulus further reduce the uncertainty? These
questions motivated a series of studies over entropy
in different datasets.4
6.1 Comparison of N -gram entropy
Following Shannon (1951), we consider the follow-
ing function FN , which can be called the N -gram
entropy, as the measure of predictability:
FN = ?
?
i,j
p(bi, j) log2 p(j | bi)
where bi is a block of N ? 1 tokens, j is an arbitrary
token following bi, and p(j | bi) is the conditional
probability of j given bi. This conditional entropy
reflects how much is the uncertainty of the next to-
ken reduced by knowing the precedingN?1 tokens.
Under this measure, is user-generated content
more predictable or less predictable than the more
formal ?printed? English examined by Shannon?
Maybe it is more predictable, since most users in
informal settings use simpler English, which may
contain fewer variations than the complex structures
observed in more formal English. Or perhaps it is
less predictable ? variations among different users
(who may not follow proper grammar) may lead
to more uncertainty in the prediction of ?the next
word?. Which would be the case?
To answer this question empirically, we construct
a reference dataset written in more formal English
(Df ) to be compared against the user comments
dataset described in Section 4 (Du). If Df covers
very different topics fromDu, then even if we do ob-
serve differences in entropy, it could be due to topi-
cal differences. A standard mixed-topic dataset like
4Note that our findings are not to be interpreted as prediction
performance over unseen texts. For that, one needs to compute
cross-entropy between training and test corpora. Since Section
5 is already addressing this question with proper training / test
split, in this section, we focus on the variability of language
usage in a corpus. This also avoids having to control for ?com-
parable? training/test splits in different types of datasets.
1496
the Brown Corpus (Kucera and Francis, 1967) may
not be ideal in this sense (e.g., it contains fiction cat-
egories such as ?Romance and Love Story?, which
may not be represented in our Du). Instead, we ob-
tained a sample of news articles on Yahoo! News
during March - May, 2011, and extracted unique
sentences from these articles. This yields a Df with
more comparable subject matters to Du.5
Next, we compare both the entropy over unigrams
and N -gram entropy in three datasets: the news ar-
ticle dataset described above, and comments data
(Section 4) separated into stimuli and responses. We
also report corresponding numbers computed on the
Brown Corpus as references. Note that datasets with
different vocabulary size can lead to different en-
tropy: the entropy of picking a word from the vo-
cabulary uniformly at random would have been dif-
ferent. Thus, we sample each dataset at different
rates, and plot the (conditional) entropy in the sam-
ple against the corresponding vocabulary size.
As shown in Figure 4(a), the entropy of unigrams
in Du (both stimuli and responses) is consistently
lower than in Df .6 On the other hand, both stimuli
and responses exhibit higher uncertainty in bigram
entropy (Figure 4(b)) and trigram entropy (Figure
4(c)). That is, when no contexts are provided, word
choices (from similarly-sized vocabularies) in Df is
more evenly distributed than in Du; but once the
proceeding words are given, the next word is more
predictable in Df than in Du. We postulate that
the difference in unigram entropy could be due to
(a) more balanced topic coverage in Df vs. more
skewed topic coverage in Du, or (b) professional re-
porters mastering a more balanced use of the vocab-
ulary. If (b) is the main reason, however, the lower
trigram entropy in Df would seem unexpected ?
shouldn?t professional journalists also have a more
balanced use of different phrases? Upon further
contemplation, what we hypothesized earlier could
be true: professional writers use the ?proper? En-
glish expected in news coverage, which could limit
5We note that this does not guarantee the exact same topic
distribution as in the comment data.
6For reference, Shannon (1951) estimated the entropy of En-
glish to be 11.82 bits per word, due to an incorrect calculation
of a 8727-word vocabulary given Zipf distribution. The correct
number should be 9.27 bits per word for a vocabulary size of
12,366 (Yavuz, 1978).
 9
 9.5
 10
 10.5
 11
 10000  100000  1e+06
en
tro
py
vocabulary size
brown corpusnews articlesstimulusresponse
(a) Entropy of unigrams
 4
 4.5
 5
 5.5
 6
 6.5
 7
 7.5
 10000  100000  1e+06
con
ditio
na
l en
tro
py
vocabulary size
brown corpusnews articlestimulusresponse
(b) Bigram entropy (F2)
 1
 1.5
 2
 2.5
 3
 3.5
 4
 4.5
 5
 10000  100000  1e+06
con
ditio
na
l en
tro
py
vocabulary size
brown corpusnews articlestimulusresponse
(c) Trigram entropy (F3)
Figure 4: Entropy of unigrams and N -gram entropy.
their trigram uncertainties; on the other hand, users
are not bound by conventions (or even grammars),
which could lead to higher variations.
Interestingly, distributions in the stimulus dataset
are closer to news articles: they have a higher uni-
gram entropy than responses, but a lower trigram en-
tropy at comparable vocabulary sizes. In particular,
recall from Section 4 that our comments dataset con-
tains roughly 237K repliers and 357K original com-
1497
 0
 1
 2
 3
 4
 5
 6
 7
 8
 10000  100000  1e+06
con
ditio
nal
 en
trop
y
vocabulary size
bigram (F 2 )bigram+stimulus (G 2 )trigram (F 3 )
Figure 5: Predicting the next word in responses: bigram
entropy vs. bigram+stimulus entropy vs. trigram entropy.
menters. If higher trigram entropy is due to variance
among different users, the stimulus dataset should
have had a higher trigram entropy. We leave an ex-
planation of this interesting behavior as future work.
6.2 Information in stimuli
We now examine the next question: does knowing
words in the stimulus further reduce the uncertainty
of the next word in the response? For simplicity,
we model the stimulus as a collection of unigrams.
Consider the following conditional entropy:
GN = ?
?
i,k,j
p(bi, j, sk) log2 p(j | bi, sk)
where bi is a block of N ? 1 tokens in a response
r, j is an arbitrary token following bi, and sk is an
arbitrary token in the corresponding stimulus s for r.
Note that for each bi, we consider every token in the
corresponding s. That is, a (stimulus, response) pair
withm and n tokens respectively generatesm?(n?
N +1) observations of (bi, j, sk) tuples. We refer to
this as the N -gram+stimulus entropy. If knowing sk
in addition to bi does not provide extra information,
then p(j | bi, sk) = p(j | bi), and GN = FN .
Figure 5 plots GN for N = 2. Interestingly, we
observe F2 > G2 > F3 (this trend holds for larger
values ofN , omitted here for clarity). That is, know-
ing both the preceding N ? 1 tokens and tokens in
the stimulus results lowers the uncertainty over the
next token in response (bigram+stimulus entropy <
bigram entropy); on the other hand, this is not as ef-
fective as knowing one more token in the preceding
block (trigram entropy< bigram+stimulus entropy).
Note that from the model size perspective, mod-
eling p(j | bi, sk) as in GN would have been much
more expensive than p(j | bi) in FN+1. Take the
case of G2 vs. F3. Let V be the vocabulary of
user comments (ignore for now differences in re-
sponses and stimuli). While both seem to require
computations over V ? V ? V , the number of
unique observed (bi, j, sk) tuples for G2 (i.e., num-
ber of unique bigrams in responses paired up with
unigrams in corresponding stimuli) is 725,458,892,
whereas the number of unique observed (bi, j) pairs
for F3 (i.e., number of unique trigrams) is only
14,692,952. This means modeling trigrams would
result in a model 2% the size of bigram+stimulus,
yet it could achieve better reduction in uncertainty.
Note that in order to reduce model complex-
ity, the models proposed in Section 3 all broke
down P (ri+1 | s, r1..i) into independent components
P (ri+1 | s) and P (ri+1 | r1..i), rather than model-
ing the effect of s and r1..i jointly as the underlying
model corresponding to GN . Indeed, it would have
been impractical to model p(j | bi, sk) directly. Our
studies confirmed the validity of this choice: even if
we look at the performance on the training data itself
(i.e., igoring data sparseness issues), the smaller tri-
gram model would have yielded better results than
the significantly more expensive bigram+stimulus
model. Still, since GN shows a consistent improve-
ment over FN , there could be more information in
the stimulus that we are not yet fully utilizing, which
can be interesting future work.
7 Conclusions
In this paper, we examined a novel application: au-
tomatic response completion in conversational set-
tings. We investigated the effectiveness of several
models that incorporate contextual information pro-
vided by the partially typed response as well as the
stimulus. We found that the partially typed response
provides strong signals. In addition, using a mix-
ture model which also incorporates stimulus content
yielded the best overall result. We also performed
empirical studies to examine the predictability of
user-generated content. Our analysis (entropy es-
timates along with upper-bound numbers observed
from experiments) suggest that there can be interest-
ing future work to explore the contextual informa-
tion provided by the stimulus more effectively and
further improve the response completion task.
1498
References
Regina Barzilay and Mirella Lapata. 2005. Modeling
local coherence: An entity-based approach. In Pro-
ceedings of the 43rd Annual Meeting on Association
for Computational Linguistics, ACL?05.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alcation. Journal of Machine
Learning Research, 3:993?1022.
Peter F. Brown, Vincent J. Della Pietra, Robert L. Mercer,
Stephen A. Della Pietra, and Jennifer C. Lai. 1992. An
estimate of an upper bound for the entropy of English.
Comput. Linguist., 18:31?40.
Ivan Bulyko, Mari Ostendorf, and Andreas Stolcke.
2003. Getting more mileage from web text sources for
conversational speech language modeling using class-
dependent mixtures. In Proceedings of the 2003 Con-
ference of the North American Chapter of the Associ-
ation for Computational Linguistics on Human Lan-
guage Technology: companion volume of the Proceed-
ings of HLT-NAACL 2003?short papers - Volume 2,
NAACL-Short ?03, pages 7?9.
Thomas M. Cover and Roger C. King. 1978. A con-
vergent gambling estimate of the entropy of English.
IEEE Transactions on Information Theory, 24:413?
421.
Steve Farmer, Richard Sproat, and Michael Witzel. 2004.
The collapse of the Indus-script thesis: The myth of a
literate Harappan civilization. Electronic Journal of
Vedic Studies, 11:379?423 and 623?656.
Yijue How and Min-Yen Kan. 2005. Optimizing pre-
dictive text entry for short message service on mobile
phones. In Proceedings of the Human Computer In-
terfaces International (HCII).
Christina L. James and Kelly M. Reischel. 2001. Text
input for mobile devices: comparing model prediction
to actual performance. In Proceedings of the SIGCHI
conference on Human factors in computing systems,
CHI ?01, pages 365?371, New York, NY, USA. ACM.
Henry Kucera and W. Nelson Francis. 1967. Com-
putational analysis of present-day American English.
Brown University Press.
I. Scott MacKenzie and R. William Soukoreff. 2002.
Text entry for mobile computing: Models and meth-
ods,theory and practice. Human-Computer Interac-
tion, 17(2-3):147?198.
Qiaozhu Mei and Kenneth Church. 2008. Entropy of
search logs: how hard is search? with personalization?
with backoff? In Proceedings of the international con-
ference on Web search and web data mining, WSDM
?08, pages 45?54.
Hamid Moradi, Jerzy W. Grzymala-busse, and James A.
Roberts. 1998. Entropy of english text: experiments
with humans and a machine learning system based on
rough sets. Information Sciences, 104:31?47.
Petteri Nurmi, Andreas Forsblom, Patrik Flore?en, Peter
Peltonen, and Petri Saarikko. 2009. Predictive text
input in a mobile shopping assistant: methods and in-
terface design. In Proceedings of the 14th interna-
tional conference on Intelligent user interfaces, IUI
?09, pages 435?438, New York, NY, USA. ACM.
Rajesh P. N. Rao, Nisha Yadav, Mayank N. Vahia,
Hrishikesh Joglekar, R. Adhikari, and Iravatham Ma-
hadevan. 2009. Entropic evidence for linguistic struc-
ture in the Indus script. Science.
Alan Ritter, Colin Cherry, and William B. Dolan. 2011.
Data-driven response generation in social media. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing, pages 583?593.
Ronald Rosenfeld. 2000. Two decades of statistical lan-
guage modeling: Where do we go from here? Pro-
ceedings of the IEEE, 88.
Claude E. Shannon. 1948. A mathematical theory
of communication. Bell System Technical Journal,
27:379?423 and 623?656.
Claude E. Shannon. 1951. Prediction and entropy
of printed English. Bell System Technical Journal,
30:50?64.
W. J. Teahan and John G. Cleary. 1996. The entropy of
English using PPM-based models. In In Data Com-
pression Conference, pages 53?62. IEEE Computer
Society Press.
Joseph Weizenbaum. 1966. Eliza: a computer program
for the study of natural language communication be-
tween man and machine. Commun. ACM, 9:36?45.
D. Yavuz. 1978. Zipf?s law and entropy (Corresp.).
IEEE Transactions on Information Theory, 20:650.
Xing Yi and James Allan. 2009. A comparative study
of utilizing topic models for information retrieval. In
Proceedings of the European Conference on IR Re-
search on Advances in Information Retrieval, pages
29?41.
1499
Squibs
Does GIZA++ Make Search Errors?
Sujith Ravi?
University of Southern California
Kevin Knight??
University of Southern California
Word alignment is a critical procedure within statistical machine translation (SMT). Brown
et al (1993) have provided the most popular word alignment algorithm to date, one that has
been implemented in the GIZA (Al-Onaizan et al 1999) and GIZA++ (Och and Ney 2003)
software and adopted by nearly every SMT project. In this article, we investigate whether this
algorithm makes search errors when it computes Viterbi alignments, that is, whether it returns
alignments that are sub-optimal according to a trained model.
1. Background
Word alignment is the problem of annotating a bilingual text with links connecting
words that have the same meanings. Brown et al (1993) align an English/French sen-
tence pair by positing a probabilistic model by which an English sentence is translated
into French.1 The model provides a set of non-deterministic choices. When a particular
sequence of choices is applied to an English input sentence e1...el, the result is a partic-
ular French output sentence f1...fm. In the Brown et al models, a decision sequence also
implies a specific word alignment vector a1...am. We say aj = i when French word fj was
produced by English word ei during the translation. Here is a sample sentence pair (e, f)
and word alignment a:
e: NULL0 Mary1 did2 not3 slap4 the5 green6 witch7
f : Mary1 no2 dio?3 una4 bofetada5 a6 la7 bruja8 verde9
a: [ 1 3 4 5 5 0 5 7 6 ]
Notice that the English sentence contains a special NULL word (e0) that generates
?spurious? target words (in this case, a6). The Brown et al (1993) models are many-
to-one, meaning that each English word can produce several French children, but each
? Information Sciences Institute, University of Southern California, 4676 Admiralty Way, Marina del Rey,
CA 90292. E-mail: sravi@isi.edu.
?? Information Sciences Institute, University of Southern California, 4676 Admiralty Way, Marina del Rey,
CA 90292. E-mail: knight@isi.edu.
1 We follow standard convention in using ?English? and ?French? simply as shorthand for ?source? and
?target?. In fact, we use English/Spanish examples in this article.
Submission received: 16 December 2009; accepted for publication: 7 April 2010.
? 2010 Association for Computational Linguistics
Computational Linguistics Volume 36, Number 3
French word has only one English parent. This is why we can represent an alignment as
a vector a1...am. There are (l + 1)m ways to align (e, f). For Brown et al the goal of word
alignment is to find the alignment a that is most likely, given a sentence pair (e, f):
argmaxa P(a|e, f) = argmaxaP(a, f|e) (1)
2. IBM Model 3
Supplied with a formula for P(a|e, f), we can search through the (l + 1)m alignments
for the highest-scoring one. Brown et al (1993) come up with such a formula by first
positing this generative story (IBM Model 3):
Given an English sentence e1...el:
1. Choose a fertility ?i for each English word ei, according to the distribution
n(?i|ei).
2. Let m? =
?
i=1...l ?i.
3. Choose a number ?0 of ?spurious? French words, by doing the following
m? times: with probability p1, increment ?0 by one.
4. Let m = m? + ?0.
5. Choose a French translation ?ik for each English word (including e0) and
fertility value, according to the distribution t(?ik|ei).
6. Choose a French position j for each ?ik (i > 0), according to d(j|i, l,m). If the
same j is chosen twice, fail the procedure.
7. Place each of the ?0 spuriously generated words, one by one, into vacant
positions in the English string, according to uniform probability.
8. Output the French string f1...fm, where fj is the French word ?ik that was
placed into position j in Step 6 or 7.
9. Output alignment a1...am, where aj is the English position i from that
same ?ik.
Different decision sequences can result in the same outputs f and a. With this in
mind, Brown et al provide the following formula:
P(a, f|e) =
m
?
j=1
t( fj|eaj ) ?
l
?
i=1
n(?i|ei) ?
m
?
aj =0,j=1
d( j|aj, l,m)
?
l
?
i=0
?i! ? 1?0!
?
(
m ? ?0
?0
)
? p?01 ? p
m?2?0
0 (2)
Note that terms ?0...?l are only shorthand, as their values are completely deter-
mined by the alignment a1...am.
296
Ravi and Knight Does GIZA++ Make Search Errors?
3. Finding the Viterbi Alignment
We assume that probability tables n, t, d, and p have already been learned from data,
using the EM method described by Brown et al (1993). We are concerned solely with the
problem of then finding the best alignment for a given sentence pair (e, f) as described
in Equation 1. Brown et al were unable to discover a polynomial time algorithm for
this problem, which was in fact subsequently shown to be NP-complete (Udupa and
Maji 2006). Brown et al therefore devise a hill-climbing algorithm. This algorithm starts
with a reasonably good alignment (Viterbi IBM Model 2, computable in quadratic time),
after which it greedily executes small changes to the alignment structure, gradually
increasing P(a, f|e). The small changes consist of moves, in which the value of some
aj is changed, and swaps, in which a pair aj and ak exchange values. At each step in
the greedy search, all possible moves and swaps are considered, and the one which
increases P(a, f|e) the most is executed. When P(a, f|e) can no longer be improved, the
search halts.2
Our question is whether this hill-climbing algorithm, as implemented in GIZA++,
makes search errors when it tries to locate IBM Model 3 Viterbi alignments. To answer
this, we built a slow but optimal IBM Model 3 aligner, by casting the problem in the
integer linear programming (ILP) framework. Given a sentence pair (e1...el, f1...fm), we
set up the following variables:
 Binary link variables linkij. We have one such variable for each pair of
English and French tokens. If linkij = 1, then there is an alignment link
between ei and fj. The English NULL word is represented by i = 0.
 Binary fertility variables fertik. We have one variable for each English token
paired with an integer fertility value 0..9. If fertik = 1, then token ei has
fertility k.
To ensure that values we assign to variables are legal and self-consistent, we in-
troduce several constraints. First, we require that for each j, all linkxj values sum to
one. This means each French token has exactly one alignment link, as required by IBM
Model 3. Second, we require that for each i, all fertix values sum to one, so that each
English token has a unique fertility. Third, we require that link and fertility variable
assignments be consistent with one another: for each i, the sum of linkix variables equals
1 ? ferti1 + 2 ? ferti2 + . . . + 9 ? ferti9.3
We then define an objective function whose minimization corresponds to finding
the Viterbi IBM Model 3 alignment. Figure 1 presents the components of the objective
function, alongside counterparts from the P(a, f|e) formula previously given. Note that
the coefficients for the objective function are derived from the already-learned probabil-
ity tables n, d, t, and p.
For each sentence pair in our test set, we create and solve an ILP problem. Figure 2
demonstrates this for a simple example. The reader can extract the optimal alignment
from the alignment variables chosen in the ILP solution.
2 Brown et al (1993) describe a variation called pegging, which carries out multiple additional greedy
hill-climbs, each with a different IBM Model 2 Viterbi link fixed (pegged) for the duration of the
hill-climb. In practice, pegging is slow, and the vast majority of GIZA++ users do not employ it.
3 The default configuration of GIZA++ includes this same fertility cap of 9, though the Brown et al (1993)
description does not.
297
Computational Linguistics Volume 36, Number 3
Figure 1
Objective function and constraints (left) for the ILP formulation that encodes the problem of
selecting the Viterbi alignment for a sentence pair under IBM Model 3. Components of the ILP
objective function are paired with counterparts (right) from the Model 3?s P(a, f|e) formula.
4. Experiments
For Chinese/English experiments, we run GIZA++ training on 101,880 sentence pairs.
We evaluate Viterbi alignments on a smaller test set of 1,880 sentence pairs with manual
alignments. For Arabic/English, we train on 300,000 sentence pairs and test on 2,000.
In tests, we compare GIZA++ Viterbi alignments (based on greedy hill-climbing) with
optimal ILP alignments. We use CPLEX to solve our ILP problems.
298
Ravi and Knight Does GIZA++ Make Search Errors?
Figure 2
Illustration of ILP-based optimal alignment of a single sentence pair. Already-trained log
probabilities are shown at the top of the figure. In the middle is a schematic of variables
introduced for the English/Spanish sentence pair seatbelts / los cinturones de seguridad.
At the bottom is the ILP formulation and its solution.
Figure 3 compares the results from GIZA++ alignment with optimal ILP alignment
for different language pairs and alignment directions, and for unions of uni-directional
alignments. We measure the rate at which GIZA++ makes search errors, and we com-
pute alignment F-scores for the various testing conditions. We conclude that although
299
Computational Linguistics Volume 36, Number 3
Figure 3
Comparison of GIZA++ Viterbi alignments (based on greedy hill-climbing) with optimal ILP
alignments for different language pairs and alignment directions in terms of alignment quality
(F-score). The figure also shows the alignment F-scores for UNION alignments that combine
alignment links from both directions. The fourth column shows the percentage of sentences on
which GIZA++ makes search errors in comparison to optimal ILP alignments.
GIZA++ makes search errors on 5?15% of sentence pairs, these errors do not contribute
to an overall loss in alignment task accuracy, as measured by F-score.
Focusing on sentence pairs where GIZA++ makes a search error, we plot the av-
erage difference in log model scores between GIZA++ and ILP Viterbi alignments in
Figure 4. We notice a positive correlation between sentence length and the search error
Figure 4
Average difference in log model scores between GIZA++ and ILP alignments at different English
sentence lengths for English/Chinese alignment. Points in the plot that appear to be on the
x-axis actually lie just above it.
300
Ravi and Knight Does GIZA++ Make Search Errors?
Figure 5
Average time (msec) taken by the ILP aligner at different English sentence lengths for
English/Chinese alignment. The experiments were run on a single machine with a 64-bit,
2.4 GHz AMD Opteron 850 processor.
gap between GIZA++ and ILP scores. As we move to longer sentences, the alignment
procedure becomes harder and GIZA++ makes more errors. Finally, Figure 5 plots the
time taken for ILP alignment at different sentence lengths showing a positive correlation
as well.
5. Discussion
We have determined that GIZA++ makes few search errors, despite the heuristic nature
of the algorithm. These search errors do not materially affect overall alignment accuracy.
In practice, this means that researchers should not spend time optimizing this particular
aspect of SMT systems.
Search errors can occur in many areas of SMT. The area that has received the most
attention is runtime decoding/translation. For example, Germann et al (2001) devise an
optimal ILP decoder to identify types of search errors made by other decoders. A second
area (this article) is finding Viterbi alignments, given a set of alignment parameter
values. A third area is actually learning those parameter values. Brown et al?s (1993)
EM learning algorithm aims to optimize the probability of the French side of the parallel
corpus given the English side. For Model 3 and above, Brown et al collect parameter
counts over subsets of alignments, instead of over all alignments. These subsets, like
Viterbi alignments, are generated heuristically, and it may be that true n-best lists of
301
Computational Linguistics Volume 36, Number 3
alignments would yield better counts and better overall parameter values. Of course,
even if we were able to collect accurate counts, EM is not guaranteed to find a global
optimum, which provides further opportunity for search errors. We leave the problem
of search errors in alignment training to future study.
Acknowledgments
This work was supported by NSF grant
0904684 and DARPA GALE Contract
Number HR0011-06-C-0022.
References
Al-Onaizan, Yaser, Jan Curin, Michael Jahr,
Kevin Knight, John Lafferty, Franz
Josef Och Dan Melamed, David Purdy,
Noah Smith, and David Yarowsky. 1999.
Statistical machine translation. Technical
report, Johns Hopkins University.
Brown, Peter, Vincent Della Pietra,
Stephen Della Pietra, and Robert Mercer.
1993. The mathematics of statistical
machine translation: Parameter estimation.
Computational Linguistics, 19(2):263?311.
Germann, Ulrich, Michael Jahr, Kevin
Knight, Daniel Marcu, and Kenji Yamada.
2001. Fast decoding and optimal decoding
for machine translation. In Proceedings of
the 39th Annual Meeting of the Association
for Computational Linguistics (ACL),
pages 228?235, Toulouse.
Och, Franz Josef and Hermann Ney. 2003.
A systematic comparison of various
statistical alignment models. Computational
Linguistics, 29(1):19?51.
Udupa, Raghavendra and Hemanta K. Maji.
2006. Computational complexity of
statistical machine translation. In
Proceedings of the Conference of the
European Chapter of the Association of
Computational Linguistics (EACL), pages
25?32, Trento.
302
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 447?455,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Bayesian Inference for Finite-State Transducers?
David Chiang1 Jonathan Graehl1 Kevin Knight1 Adam Pauls2 Sujith Ravi1
1Information Sciences Institute
University of Southern California
4676 Admiralty Way, Suite 1001
Marina del Rey, CA 90292
2Computer Science Division
University of California at Berkeley
Soda Hall
Berkeley, CA 94720
Abstract
We describe a Bayesian inference algorithm
that can be used to train any cascade of
weighted finite-state transducers on end-to-
end data. We also investigate the problem
of automatically selecting from among mul-
tiple training runs. Our experiments on four
different tasks demonstrate the genericity of
this framework, and, where applicable, large
improvements in performance over EM. We
also show, for unsupervised part-of-speech
tagging, that automatic run selection gives a
large improvement over previous Bayesian ap-
proaches.
1 Introduction
In this paper, we investigate Bayesian infer-
ence for weighted finite-state transducers (WFSTs).
Many natural language models can be captured
by weighted finite-state transducers (Pereira et al,
1994; Sproat et al, 1996; Knight and Al-Onaizan,
1998; Clark, 2002; Kolak et al, 2003; Mathias and
Byrne, 2006), which offer several benefits:
? WFSTs provide a uniform knowledge represen-
tation.
? Complex problems can be broken down into a
cascade of simple WFSTs.
? Input- and output-epsilon transitions allow
compact designs.
? Generic algorithms exist for doing inferences
with WFSTs. These include best-path de-
coding, k-best path extraction, composition,
?The authors are listed in alphabetical order. Please direct
correspondence to Sujith Ravi (sravi@isi.edu). This work
was supported by NSF grant IIS-0904684 and DARPA contract
HR0011-06-C0022.
intersection, minimization, determinization,
forward-backward training, forward-backward
pruning, stochastic generation, and projection.
? Software toolkits implement these generic al-
gorithms, allowing designers to concentrate on
novel models rather than problem-specific in-
ference code. This leads to faster scientific ex-
perimentation with fewer bugs.
Weighted tree transducers play the same role for
problems that involve the creation and transforma-
tion of tree structures (Knight and Graehl, 2005). Of
course, many problems do not fit either the finite-
state string or tree transducer framework, but in this
paper, we concentrate on those that do.
Bayesian inference schemes have become popu-
lar recently in natural language processing for their
ability to manage uncertainty about model param-
eters and to allow designers to incorporate prior
knowledge flexibly. Task-accuracy results have gen-
erally been favorable. However, it can be time-
consuming to apply Bayesian inference methods to
each new problem. Designers typically build cus-
tom, problem-specific sampling operators for ex-
ploring the derivation space. They may factor their
programs to get some code re-use from one problem
to the next, but highly generic tools for string and
tree processing are not available.
In this paper, we marry the world of finite-state
machines with the world of Bayesian inference, and
we test our methods across a range of natural lan-
guage problems. Our contributions are:
? We describe a Bayesian inference algorithm
that can be used to train any cascade of WFSTs
on end-to-end data.
? We propose a method for automatic run selec-
447
tion, i.e., how to automatically select among
multiple training runs in order to achieve the
best possible task accuracy.
The natural language applications we consider
in this paper are: (1) unsupervised part-of-speech
(POS) tagging (Merialdo, 1994; Goldwater and
Griffiths, 2007), (2) letter substitution decipher-
ment (Peleg and Rosenfeld, 1979; Knight et al,
2006; Ravi and Knight, 2008), (3) segmentation of
space-free English (Goldwater et al, 2009), and (4)
Japanese/English phoneme alignment (Knight and
Graehl, 1998; Ravi and Knight, 2009a). Figure 1
shows how each of these problems can be repre-
sented as a cascade of finite-state acceptors (FSAs)
and finite-state transducers (FSTs).
2 Generic EM Training
We first describe forward-backward EM training for
a single FST M. Given a string pair (v,w) from our
training data, we transform v into an FST Mv that
just maps v to itself, and likewise transform w into
an FST Mw. Then we compose Mv with M, and com-
pose the result with Mw. This composition follows
Pereira and Riley (1996), treating epsilon input and
output transitions correctly, especially with regards
to their weighted interleaving. This yields a deriva-
tion lattice D, each of whose paths transforms v into
w.1 Each transition in D corresponds to some tran-
sition in the FST M. We run the forward-backward
algorithm over D to collect fractional counts for the
transitions in M. After we sum fractional counts for
all examples, we normalize with respect to com-
peting transitions in M, assign new probabilities to
M, and iterate. Transitions in M compete with each
other if they leave the same state with the same input
symbol, which may be empty ().
In order to train an FSA on observed string data,
we convert the FSA into an FST by adding an input-
epsilon to every transition. We then convert each
training string v into the string pair (, v). After run-
ning the above FST training algorithm, we can re-
move all input- from the trained machine.
It is straightforward to modify generic training to
support the following controls:
1Throughout this paper, we do not assume that lattices are
acyclic; the algorithms described work on general graphs.
B:E
a:A b:B A:D
A:C
=
a: 
 :D
 :E b: 
a:  :C
Figure 2: Composition of two FSTs maintaining separate
transitions.
Maximum iterations and early stopping. We spec-
ify a maximum number of iterations, and we halt
early if the ratio of log P(data) from one iteration
to the next exceeds a threshold (such as 0.99999).
Initial point. Any probabilities supplied on the pre-
trained FST are interpreted as a starting point for
EM?s search. If no probabilities are supplied, EM
begins with uniform probabilities.
Random restarts. We can request n random restarts,
each from a different, randomly-selected initial
point.
Locking and tying. Transitions on the pre-trained
FST can be marked as locked, in which case EM
will not modify their supplied probabilities. Groups
of transitions can be tied together so that their frac-
tional counts are pooled, and when normalization
occurs, they all receive the same probability.
Derivation lattice caching. If memory is available,
training can cache the derivation lattices computed
in the first EM iteration for all training pairs. Subse-
quent iterations then run much faster. In our experi-
ments, we observe an average 10-fold speedup with
caching.
Next we turn to training a cascade of FSTs on
end-to-end data. The algorithm takes as input: (1) a
sequence of FSTs, and (2) pairs of training strings
(v,w), such that v is accepted by the first FST in
the cascade, and w is produced by the last FST. The
algorithm outputs the same sequence of FSTs, but
with trained probabilities.
To accomplish this, we first compose the supplied
FSTs, taking care to keep the transitions from differ-
ent machines separate. Figure 2 illustrates this with a
small example. It may thus happen that a single tran-
sition in an input FST is represented multiple times
in the composed device, in which case their prob-
448
ABCD:a 
REY:r 
?:c 
1.  Unsupervised part-of-speech tagging with constrained dictionary 
POS Tag 
sequence 
Observed 
word 
sequence 
2.  Decipherment of letter-substitution cipher 
English 
letter 
sequence 
Observed 
enciphered 
text 
3.  Re-Spacing of English text written without spaces 
Word 
sequence 
Observed 
letter 
sequence 
w/o spaces 
4.  Alignment of Japanese/English phoneme sequences 
English 
phoneme 
sequence 
Japanese 
katakana 
phoneme 
sequence 
26 x 26 table 
letter bigram model, 
learned separately 
constrained tag?word 
substitution model tag bigram model 
unigram model over 
words and non-words deterministic spell-out 
mapping from each English  
phoneme to each Japanese  
phoneme sequence of length 1 to 3 
NN 
JJ 
JJ 
JJ 
NN 
VB ? 
? 
? 
NN:fish 
IN:at 
VB:fish 
SYM:a DT:a 
a 
b 
b 
b 
a 
c ? 
? 
? 
a:A 
a:B 
a:C 
b:A b:B b:C 
A AR 
ARE AREY 
AREYO 
?:? 
AREY:a 
?:b 
?:d 
?:r ?:e 
?:y 
AE:? 
?:S 
?:S 
?:U 
Figure 1: Finite-state cascades for five natural language problems.
449
abilities are tied together. Next, we run FST train-
ing on the end-to-end data. This involves creating
derivation lattices and running forward-backward on
them. After FST training, we de-compose the trained
device back into a cascade of trained machines.
When the cascade?s first machine is an FSA,
rather than an FST, then the entire cascade is viewed
as a generator of strings rather than a transformer of
strings. Such a cascade is trained on observed strings
rather than string pairs. By again treating the first
FSA as an FST with empty input, we can train using
the FST-cascade training algorithm described in the
previous paragraph.
Once we have our trained cascade, we can apply it
to new data, obtaining (for example) the k-best out-
put strings for an input string.
3 Generic Bayesian Training
Bayesian learning is a wide-ranging field. We focus
on training using Gibbs sampling (Geman and Ge-
man, 1984), because it has been popularly applied
in the natural language literature, e.g., (Finkel et al,
2005; DeNero et al, 2008; Blunsom et al, 2009).
Our overall plan is to give a generic algorithm
for Bayesian training that is a ?drop-in replacement?
for EM training. That is, we input an FST cas-
cade and data and output the same FST cascade
with trained weights. This is an approximation to a
purely Bayesian setup (where one would always in-
tegrate over all possible weightings), but one which
makes it easy to deploy FSTs to efficiently decode
new data. Likewise, we do not yet support non-
parametric approaches?to create a drop-in replace-
ment for EM, we require that all parameters be spec-
ified in the initial FST cascade. We return to this is-
sue in Section 5.
3.1 Particular Case
We start with a well-known application of Bayesian
inference, unsupervised POS tagging (Goldwater
and Griffiths, 2007). Raw training text is provided,
and each potential corpus tagging corresponds to a
hidden derivation of that data. Derivations are cre-
ated and probabilistically scored as follows:
1. i? 1
2. Choose tag t1 according to P0(t1)
3. Choose word w1 according to P0(w1 | t1)
4. i? i + 1
5. Choose tag ti according to
?P0(ti | ti?1) + ci?11 (ti?1, ti)
? + ci?11 (ti?1)
(1)
6. Choose word wi according to
?P0(wi | ti) + ci?11 (ti,wi)
? + ci?11 (ti)
(2)
7. With probability Pquit, quit; else go to 4.
This defines the probability of any given derivation.
The base distribution P0 represents prior knowl-
edge about the distribution of tags and words, given
the relevant conditioning context. The ci?11 are the
counts of events occurring before word i in the
derivation (the ?cache?).
When ? and ? are large, tags and words are essen-
tially generated according to P0. When ? and ? are
small, tags and words are generated with reference
to previous decisions inside the cache.
We use Gibbs sampling to estimate the distribu-
tion of tags given words. The key to efficient sam-
pling is to define a sampling operator that makes
some small change to the overall corpus derivation.
With such an operator, we derive an incremental
formula for re-scoring the probability of an entire
new derivation based on the probability of the old
derivation. Exchangeability makes this efficient?
we pretend like the area around the small change oc-
curs at the end of the corpus, so that both old and
new derivations share the same cache. Goldwater
and Griffiths (2007) choose the re-sampling operator
?change the tag of a single word,? and they derive
the corresponding incremental scoring formula for
unsupervised tagging. For other problems, design-
ers develop different sampling operators and derive
different incremental scoring formulas.
3.2 Generic Case
In order to develop a generic algorithm, we need
to abstract away from these problem-specific de-
sign choices. In general, hidden derivations corre-
spond to paths through derivation lattices, so we first
450
Figure 3: Changing a decision in the derivation lattice.
All paths generate the observed data. The bold path rep-
resents the current sample, and the dotted path represents
a sidetrack in which one decision is changed.
compute derivation lattices for our observed training
data through our cascade of FSTs. A random path
through these lattices constitutes the initial sample,
and we calculate its derivation probability directly.
One way to think about a generic small change
operator is to consider a single transition in the cur-
rent sample. This transition will generally compete
with other transitions. One possible small change is
to ?sidetrack? the derivation to a competing deriva-
tion. Figure 3 shows how this works. If the sidetrack
path quickly re-joins the old derivation path, then an
incremental score can be computed. However, side-
tracking raises knotty questions. First, what is the
proper path continuation after the sidetracking tran-
sition is selected? Should the path attempt to re-join
the old derivation as soon as possible, and if so, how
is this efficiently done? Then, how can we compute
new derivation scores for all possible sidetracks, so
that we can choose a new sample by an appropriate
weighted coin flip? Finally, would such a sampler be
reversible? In order to satisfy theoretical conditions
for Gibbs sampling, if we move from sample A to
sample B, we must be able to immediately get back
to sample A.
We take a different tack here, moving from point-
wise sampling to blocked sampling. Gao and John-
son (2008) employed blocked sampling for POS tag-
ging, and the approach works nicely for arbitrary
derivation lattices. We again start with a random
derivation for each example in the corpus. We then
choose a training example and exchange its entire
derivation lattice to the end of the corpus. We cre-
ate a weighted version of this lattice, called the pro-
posal lattice, such that we can approximately sample
whole paths by stochastic generation. The probabil-
ities are based on the event counts from the rest of
the sample (the cache), and on the base distribution,
and are computed in this way:
P(r | q) =
?P0(r | q) + c(q, r)
? + c(q)
(3)
where q and r are states of the derivation lattice, and
the c(?) are counts collected from the corpus minus
the entire training example being resampled. This is
an approximation because we are ignoring the fact
that P(r | q) in general depends on choices made
earlier in the lattice. The approximation can be cor-
rected using the Metropolis-Hastings algorithm, in
which the sample drawn from the proposal lattice is
accepted only with a certain probability ?; but Gao
and Johnson (2008) report that ? > 0.99, so we skip
this step.
3.3 Choosing the best derivations
After the sampling run has finished, we can choose
the best derivations using two different methods.
First, if we want to find the MAP derivations of the
training strings, then following Goldwater and Grif-
fiths (2007), we can use annealing: raise the proba-
bilities in the sampling distribution to the 1T power,
where T is a temperature parameter, decrease T to-
wards zero, and take a single sample.
But in practice one often wants to predict the
MAP derivation for a new string w? not contained
in the training data. To approximate the distribution
of derivations of w? given the training data, we aver-
age the transition counts from all the samples (after
burn-in) and plug the averaged counts into (3) to ob-
tain a single proposal lattice.2 The predicted deriva-
tion is the Viterbi path through this lattice. Call this
method averaging. An advantage of this approach is
that the trainer, taking a cascade of FSAs as input,
outputs a weighted version of the same cascade, and
this trained cascade can be used on unseen examples
without having to rerun training.
3.4 Implementation
That concludes the generic Bayesian training algo-
rithm, to which we add the following controls:
2A better approximation might have been to build a proposal
lattice for each sample (after burn-in), and then construct a sin-
gle FSA that computes the average of the probability distribu-
tions computed by all the proposal lattices. But this FSA would
be rather large.
451
Number of Gibbs sampling iterations. We execute
the full number specified.
Base distribution. Any probabilities supplied on the
pre-trained FST are interpreted as base distribution
probabilities. If no probabilities are supplied, then
the base distribution is taken to be uniform.
Hyperparameters. We supply a distinct ? for each
machine in the FST cascade. We do not yet support
different ? values for different states within a single
FST.
Random restarts. We can request multiple runs
from different, randomly-selected initial samples.
EM-based initial point. If random initial samples
are undesirable, we can request that the Gibbs sam-
pler be initialized with the Viterbi path using param-
eter values obtained by n iterations of EM.
Annealing schedule. If annealing is used, it follows
a linear annealing schedule with starting and stop-
ping temperature specified by the user.
EM and Bayesian training for arbitrary FST
cascades are both implemented in the finite-state
toolkit Carmel, which is distributed with source
code.3 All controls are implemented as command-
line switches. We use Carmel to carry out the exper-
iments in the next section.
4 Run Selection
For both EM and Bayesian methods, different train-
ing runs yield different results. EM?s objective func-
tion (probability of observed data) is very bumpy for
the unsupervised problems we work on?different
initial points yield different trained WFST cascades,
with different task accuracies. Averaging task accu-
racies across runs is undesirable, because we want to
deploy a particular trained cascade in the real world,
and we want an estimate of its performance. Select-
ing the run with the best task accuracy is illegal in an
unsupervised setting. With EM, we have a good al-
ternative: select the run that maximizes the objective
function, i.e., the likelihood of the observed training
data. We find a decent correlation between this value
and task accuracy, and we are generally able to im-
prove accuracy using this run selection method. Fig-
ure 4 shows a scatterplot of 1000 runs for POS tag-
ging. A single run with a uniform start yields 81.8%
3http://www.isi.edu/licensed-sw/carmel
 0.75
 0.8
 0.85
 0.9
 211200
 211300
 211400
 211500
 211600
 211700
 211800
 211900
 212000
 212100
 212200
T
a
g
g
i
n
g
 
a
c
c
u
r
a
c
y
 
(
%
 
o
f
 
w
o
r
d
 
t
o
k
e
n
s
)
-log P(data)
EM (random start)EM (uniform start)
Figure 4: Multiple EM restarts for POS tagging. Each
point represents one random restart; the y-axis is tag-
ging accuracy and the x-axis is EM?s objective function,
? log P(data).
accuracy, while automatic selection from 1000 runs
yields 82.4% accuracy.
Gibbs sampling runs also yield WFST cascades
with varying task accuracies, due to random initial
samples and sampling decisions. In fact, the varia-
tion is even larger than what we find with EM. It is
natural to ask whether we can do automatic run se-
lection for Gibbs sampling. If we are using anneal-
ing, it makes sense to use the probability of the fi-
nal sample, which is supposed to approximate the
MAP derivation. When using averaging, however,
choosing the final sample would be quite arbitrary.
Instead, we propose choosing the run that has the
highest average log-probability (that is, the lowest
entropy) after burn-in. The rationale is that the runs
that have found their way to high-probability peaks
are probably more representative of the true distri-
bution, or at least capture a part of the distribution
that is of greater interest to us.
We find that this method works quite well in prac-
tice. Figure 5 illustrates 1000 POS tagging runs
for annealing with automatic run selection, yield-
ing 84.7% accuracy. When using averaging, how-
ever, automatic selection from 1000 runs (Figure 6)
produces a much higher accuracy of 90.7%. This
is better than accuracies reported previously using
452
 0.75
 0.8
 0.85
 0.9
 235100
 235150
 235200
 235250
 235300
 235350
 235400
T
a
g
g
i
n
g
 
a
c
c
u
r
a
c
y
 
(
%
 
o
f
 
w
o
r
d
 
t
o
k
e
n
s
)
-log P(derivation) for final sample
Bayesian run (with annealing)
Figure 5: Multiple Bayesian learning runs (using anneal-
ing with temperature decreasing from 2 to 0.08) for POS
tagging. Each point represents one run; the y-axis is tag-
ging accuracy and the x-axis is the ? log P(derivation) of
the final sample.
 0.75
 0.8
 0.85
 0.9
 236800
 236900
 237000
 237100
 237200
 237300
 237400
 237500
 237600
 237700
 237800
 237900
T
a
g
g
i
n
g
 
a
c
c
u
r
a
c
y
 
(
%
 
o
f
 
w
o
r
d
 
t
o
k
e
n
s
)
-log P(derivation) averaged over all post-burnin samples
Bayesian run (using averaging)
Figure 6: Multiple Bayesian learning runs (using averag-
ing) for POS tagging. Each point represents one run; the
y-axis is tagging accuracy and the x-axis is the average
? log P(derivation) over all samples after burn-in.
Bayesian methods (85.2% from Goldwater and Grif-
fiths (2007), who use a trigram model) and close to
the best accuracy reported on this task (91.8% from
Ravi and Knight (2009b), who use an integer linear
program to minimize the model directly).
5 Experiments and Results
We run experiments for various natural language ap-
plications and compare the task accuracies achieved
by the EM and Bayesian learning methods. The
tasks we consider are:
Unsupervised POS tagging. We adopt the com-
mon problem formulation for this task described
by Merialdo (1994), in which we are given a raw
24,115-word sequence and a dictionary of legal tags
for each word type. The tagset consists of 45 dis-
tinct grammatical tags. We use the same modeling
approach as as Goldwater and Griffiths (2007), us-
ing a probabilistic tag bigram model in conjunction
with a tag-to-word model.
Letter substitution decipherment. Here, the task
is to decipher a 414-letter substitution cipher and un-
cover the original English letter sequence. The task
accuracy is defined as the percent of ciphertext to-
kens that are deciphered correctly. We work on the
same standard cipher described in previous litera-
ture (Ravi and Knight, 2008). The model consists
of an English letter bigram model, whose probabil-
ities are fixed and an English-to-ciphertext channel
model, which is learnt during training.
Segmentation of space-free English. Given
a space-free English text corpus (e.g.,
iwalkedtothe...), the task is to segment the
text into words (e.g., i walked to the ...).
Our input text corpus consists of 11,378 words,
with spaces removed. As illustrated in Figure 1,
our method uses a unigram FSA that models every
letter sequence seen in the data, which includes
both words and non-words (at most 10 letters long)
composed with a deterministic spell-out model.
In order to evaluate the quality of our segmented
output, we compare it against the gold segmentation
and compute the word token f-measure.
Japanese/English phoneme alignment. We
use the problem formulation of Knight and
Graehl (1998). Given an input English/Japanese
katakana phoneme sequence pair, the task is to
produce an alignment that connects each English
453
MLE Bayesian
EM prior VB-EM Gibbs
POS tagging 82.4 ? = 10?2, ? = 10?1 84.1 90.7
Letter decipherment 83.6 ? = 106, ? = 10?2 83.6 88.9
Re-spacing English 0.9 ? = 10?8, ? = 104 0.8 42.8
Aligning phoneme strings? 100 ? = 10?2 99.9 99.1
Table 1: Gibbs sampling for Bayesian inference outperforms both EM and Variational Bayesian EM. ?The output of
EM alignment was used as the gold standard.
phoneme to its corresponding Japanese sounds (a
sequence of one or more Japanese phonemes). For
example, given a phoneme sequence pair ((AH B
AW T) ? (a b a u t o)), we have to produce
the alignments ((AH ? a), (B ? b), (AW ?
a u), (T ? t o)). The input data consists of
2,684 English/Japanese phoneme sequence pairs.
We use a model that consists of mappings from each
English phoneme to Japanese phoneme sequences
(of length up to 3), and the mapping probabilities
are learnt during training. We manually analyzed
the alignments produced by the EM method for
this task and found them to be nearly perfect.
Hence, for the purpose of this task we treat the EM
alignments as our gold standard, since there are no
gold alignments available for this data.
In all the experiments reported here, we run EM
for 200 iterations and Bayesian for 5000 iterations
(the first 2000 for burn-in). We apply automatic run
selection using the objective function value for EM
and the averaging method for Bayesian.
Table 1 shows accuracy results for our four tasks,
using run selection for both EM and Bayesian learn-
ing. For the Bayesian runs, we compared two infer-
ence methods: Gibbs sampling, as described above,
and Variational Bayesian EM (Beal and Ghahra-
mani, 2003), both of which are implemented in
Carmel. We used the hyperparameters (?, ?) as
shown in the table. Setting a high value yields a fi-
nal distribution that is close to the original one (P0).
For example, in letter decipherment we want to keep
the language model probabilities fixed during train-
ing, and hence we set the prior on that model to
be very strong (? = 106). Table 1 shows that the
Bayesian methods consistently outperform EM for
all the tasks (except phoneme alignment, where EM
was taken as the gold standard). Each iteration of
Gibbs sampling was 2.3 times slower than EM for
POS tagging, and in general about twice as slow.
6 Discussion
We have described general training algorithms for
FST cascades and their implementation, and exam-
ined the problem of run selection for both EM and
Bayesian training. This work raises several interest-
ing points for future study.
First, is there an efficient method for perform-
ing pointwise sampling on general FSTs, and would
pointwise sampling deliver better empirical results
than blocked sampling across a range of tasks?
Second, can generic methods similar to the ones
described here be developed for cascades of tree
transducers? It is straightforward to adapt our meth-
ods to train a single tree transducer (Graehl et al,
2008), but as most types of tree transducers are
not closed under composition (Ge?cseg and Steinby,
1984), the compose/de-compose method cannot be
directly applied to train cascades.
Third, what is the best way to extend the FST for-
malism to represent non-parametric Bayesian mod-
els? Consider the English re-spacing application. We
currently take observed (un-spaced) data and build
a giant unigram FSA that models every letter se-
quence seen in the data of up to 10 letters, both
words and non-words. This FSA has 207,253 tran-
sitions. We also define P0 for each individual transi-
tion, which allows a preference for short words. This
set-up works fine, but in a nonparametric approach,
P0 is defined more compactly and without a word-
length limit. An extension of FSTs along the lines
of recursive transition networks may be appropriate,
but we leave details for future work.
454
References
Matthew J. Beal and Zoubin Ghahramani. 2003. The
Variational Bayesian EM algorithm for incomplete
data: with application to scoring graphical model
structures. Bayesian Statistics, 7:453?464.
Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Os-
borne. 2009. A Gibbs sampler for phrasal syn-
chronous grammar induction. In Proceedings of ACL-
IJCNLP 2009.
Alexander Clark. 2002. Memory-based learning of mor-
phology with stochastic transducers. In Proceedings
of ACL 2002.
John DeNero, Alexandre Bouchard-Co?te?, and Dan Klein.
2008. Sampling alignment structure under a Bayesian
translation model. In Proceedings of EMNLP 2008.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by Gibbs sam-
pling. In Proceedings of ACL 2005.
Jianfeng Gao and Mark Johnson. 2008. A comparison of
Bayesian estimators for unsupervised Hidden Markov
Model POS taggers. In Proceedings of EMNLP 2008.
Ferenc Ge?cseg and Magnus Steinby. 1984. Tree Au-
tomata. Akade?miai Kiado?, Budapest.
Stuart Geman and Donald Geman. 1984. Stochastic re-
laxation, Gibbs distributions and the Bayesian restora-
tion of images. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 6(6):721?741.
Sharon Goldwater and Thomas L. Griffiths. 2007.
A fully Bayesian approach to unsupervised part-of-
speech tagging. In Proceedings of ACL 2007.
Sharon Goldwater, Thomas L. Griffiths, and Mark John-
son. 2009. A Bayesian framework for word segmen-
tation: Exploring the effects of context. Cognition,
112(1):21 ? 54.
Jonathan Graehl, Kevin Knight, and Jonathan May. 2008.
Training tree transducers. Computational Linguistics,
34(3):391?427.
Kevin Knight and Yaser Al-Onaizan. 1998. Transla-
tion with finite-state devices. In Proceedings of AMTA
1998.
Kevin Knight and Jonathan Graehl. 1998. Machine
transliteration. Computational Linguistics, 24(4):599?
612.
Knight Knight and Jonathan Graehl. 2005. An overview
of probabilistic tree transducers for natural language
processing. In Proceedings of CICLing-2005.
Kevin Knight, Anish Nair, Nishit Rathod, and Kenji Ya-
mada. 2006. Unsupervised analysis for decipherment
problems. In Proceedings of COLING-ACL 2006.
Okan Kolak, Willian Byrne, and Philip Resnik. 2003. A
generative probabilistic OCR model for NLP applica-
tions. In Proceedings of HLT-NAACL 2003.
Lambert Mathias and William Byrne. 2006. Statisti-
cal phrase-based speech translation. In Proceedings
of ICASSP 2006.
Bernard Merialdo. 1994. Tagging English text with
a probabilistic model. Computational Linguistics,
20(2):155?171.
Shmuel Peleg and Azriel Rosenfeld. 1979. Break-
ing substitution ciphers using a relaxation algorithm.
Communications of the ACM, 22(11):598?605.
Fernando C. N. Pereira and Michael D. Riley. 1996.
Speech recognition by composition of weighted finite
automata. Finite-State Language Processing, pages
431?453.
Fernando Pereira, Michael Riley, and Richard Sproat.
1994. Weighted rational transductions and their appli-
cations to human language processing. In ARPA Hu-
man Language Technology Workshop.
Sujith Ravi and Kevin Knight. 2008. Attacking deci-
pherment problems optimally with low-order n-gram
models. In Proceedings of EMNLP 2008.
Sujith Ravi and Kevin Knight. 2009a. Learning
phoneme mappings for transliteration without parallel
data. In Proceedings of NAACL HLT 2009.
Sujith Ravi and Kevin Knight. 2009b. Minimized mod-
els for unsupervised part-of-speech tagging. In Pro-
ceedings of ACL-IJCNLP 2009.
Richard Sproat, Chilin Shih, William Gale, and Nancy
Chang. 1996. A stochastic finite-state word-
segmentation algorithm for Chinese. Computational
Linguistics, 22(3):377?404.
455
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 495?503,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Minimized models and grammar-informed initialization
for supertagging with highly ambiguous lexicons
Sujith Ravi1 Jason Baldridge2 Kevin Knight1
1University of Southern California
Information Sciences Institute
Marina del Rey, California 90292
{sravi,knight}@isi.edu
2Department of Linguistics
The University of Texas at Austin
Austin, Texas 78712
jbaldrid@mail.utexas.edu
Abstract
We combine two complementary ideas
for learning supertaggers from highly am-
biguous lexicons: grammar-informed tag
transitions and models minimized via in-
teger programming. Each strategy on its
own greatly improves performance over
basic expectation-maximization training
with a bitag Hidden Markov Model, which
we show on the CCGbank and CCG-TUT
corpora. The strategies provide further er-
ror reductions when combined. We de-
scribe a new two-stage integer program-
ming strategy that efficiently deals with
the high degree of ambiguity on these
datasets while obtaining the full effect of
model minimization.
1 Introduction
Creating accurate part-of-speech (POS) taggers
using a tag dictionary and unlabeled data is an
interesting task with practical applications. It
has been explored at length in the literature since
Merialdo (1994), though the task setting as usu-
ally defined in such experiments is somewhat arti-
ficial since the tag dictionaries are derived from
tagged corpora. Nonetheless, the methods pro-
posed apply to realistic scenarios in which one
has an electronic part-of-speech tag dictionary or
a hand-crafted grammar with limited coverage.
Most work has focused on POS-tagging for
English using the Penn Treebank (Marcus et al,
1993), such as (Banko and Moore, 2004; Gold-
water and Griffiths, 2007; Toutanova and John-
son, 2008; Goldberg et al, 2008; Ravi and Knight,
2009). This generally involves working with the
standard set of 45 POS-tags employed in the Penn
Treebank. The most ambiguous word has 7 dif-
ferent POS tags associated with it. Most methods
have employed some variant of Expectation Max-
imization (EM) to learn parameters for a bigram
or trigram Hidden Markov Model (HMM). Ravi
and Knight (2009) achieved the best results thus
far (92.3% word token accuracy) via a Minimum
Description Length approach using an integer pro-
gram (IP) that finds a minimal bigram grammar
that obeys the tag dictionary constraints and cov-
ers the observed data.
A more challenging task is learning supertag-
gers for lexicalized grammar formalisms such as
Combinatory Categorial Grammar (CCG) (Steed-
man, 2000). For example, CCGbank (Hocken-
maier and Steedman, 2007) contains 1241 dis-
tinct supertags (lexical categories) and the most
ambiguous word has 126 supertags. This pro-
vides a much more challenging starting point
for the semi-supervised methods typically ap-
plied to the task. Yet, this is an important task
since creating grammars and resources for CCG
parsers for new domains and languages is highly
labor- and knowledge-intensive. Baldridge (2008)
uses grammar-informed initialization for HMM
tag transitions based on the universal combinatory
rules of the CCG formalism to obtain 56.1% accu-
racy on ambiguous word tokens, a large improve-
ment over the 33.0% accuracy obtained with uni-
form initialization for tag transitions.
The strategies employed in Ravi and Knight
(2009) and Baldridge (2008) are complementary.
The former reduces the model size globally given
a data set, while the latter biases bitag transitions
toward those which are more likely based on a uni-
versal grammar without reference to any data. In
this paper, we show how these strategies may be
combined straightforwardly to produce improve-
ments on the task of learning supertaggers from
lexicons that have not been filtered in any way.1
We demonstrate their cross-lingual effectiveness
on CCGbank (English) and the Italian CCG-TUT
1See Banko and Moore (2004) for a description of how
many early POS-tagging papers in fact used a number of
heuristic cutoffs that greatly simplify the problem.
495
corpus (Bos et al, 2009). We find a consistent im-
proved performance by using each of the methods
compared to basic EM, and further improvements
by using them in combination.
Applying the approach of Ravi and Knight
(2009) naively to CCG supertagging is intractable
due to the high level of ambiguity. We deal with
this by defining a new two-stage integer program-
ming formulation that identifies minimal gram-
mars efficiently and effectively.
2 Data
CCGbank. CCGbank was created by semi-
automatically converting the Penn Treebank to
CCG derivations (Hockenmaier and Steedman,
2007). We use the standard splits of the data
used in semi-supervised tagging experiments (e.g.
Banko and Moore (2004)): sections 0-18 for train-
ing, 19-21 for development, and 22-24 for test.
CCG-TUT. CCG-TUT was created by semi-
automatically converting dependencies in the Ital-
ian Turin University Treebank to CCG deriva-
tions (Bos et al, 2009). It is much smaller than
CCGbank, with only 1837 sentences. It is split
into three sections: newspaper texts (NPAPER),
civil code texts (CIVIL), and European law texts
from the JRC-Acquis Multilingual Parallel Corpus
(JRC). For test sets, we use the first 400 sentences
of NPAPER, the first 400 of CIVIL, and all of JRC.
This leaves 409 and 498 sentences from NPAPER
and CIVIL, respectively, for training (to acquire a
lexicon and run EM). For evaluation, we use two
different settings of train/test splits:
TEST 1 Evaluate on the NPAPER section of test
using a lexicon extracted only from NPAPER
section of train.
TEST 2 Evaluate on the entire test using lexi-
cons extracted from (a) NPAPER + CIVIL,
(b) NPAPER, and (c) CIVIL.
Table 1 shows statistics for supertag ambiguity
in CCGbank and CCG-TUT. As a comparison, the
POS word token ambiguity in CCGbank is 2.2: the
corresponding value of 18.71 for supertags is in-
dicative of the (challenging) fact that supertag am-
biguity is greatest for the most frequent words.
3 Grammar informed initialization for
supertagging
Part-of-speech tags are atomic labels that in and of
themselves encode no internal structure. In con-
Data Distinct Max Type ambig Tok ambig
CCGbank 1241 126 1.69 18.71
CCG-TUT
NPAPER+CIVIL 849 64 1.48 11.76
NPAPER 644 48 1.42 12.17
CIVIL 486 39 1.52 11.33
Table 1: Statistics for the training data used to ex-
tract lexicons for CCGbank and CCG-TUT. Dis-
tinct: # of distinct lexical categories; Max: # of
categories for the most ambiguous word; Type
ambig: per word type category ambiguity; Tok
ambig: per word token category ambiguity.
trast, supertags are detailed, structured labels; a
universal set of grammatical rules defines how cat-
egories may combine with one another to project
syntactic structure.2 Because of this, properties of
the CCG formalism itself can be used to constrain
learning?prior to considering any particular lan-
guage, grammar or data set. Baldridge (2008) uses
this observation to create grammar-informed tag
transitions for a bitag HMM supertagger based on
two main properties. First, categories differ in
their complexity and less complex categories tend
to be used more frequently. For example, two cat-
egories for buy in CCGbank are (S[dcl]\NP)/NP
and ((((S[b]\NP)/PP)/PP)/(S[adj]\NP))/NP; the
former occurs 33 times, the latter once. Second,
categories indicate the form of categories found
adjacent to them; for example, the category for
sentential complement verbs ((S\NP)/S) expects
an NP to its left and an S to its right.
Categories combine via rules such as applica-
tion and composition (see Steedman (2000) for de-
tails). Given a lexicon containing the categories
for each word, these allow derivations like:
Ed might see a cat
NP (S\NP)/(S\NP) (S\NP)/NP NP/N N
>B >
(S\NP)/NP NP
>
S\NP
>
S
Other derivations are possible. In fact, every pair
of adjacent words above may be combined di-
rectly. For example, see and a may combine
through forward composition to produce the cate-
gory (S\NP)/N, and Ed?s category may type-raise
to S/(S\NP) and compose with might?s category.
Baldridge uses these properties to define tag
2Note that supertags can be lexical categories of CCG
(Steedman, 2000), elementary trees of Tree-adjoining Gram-
mar (Joshi, 1988), or types in a feature hierarchy as in Head-
driven Phrase Structure Grammar (Pollard and Sag, 1994).
496
transition distributions that have higher likeli-
hood for simpler categories that are able to
combine. For example, for the distribution
p(ti|ti?1=NP ), (S\NP)\NP is more likely than
((S\NP)/(N/N))\NP because both categories may
combine with a preceding NP but the former is
simpler. In turn, the latter is more likely than NP: it
is more complex but can combine with the preced-
ing NP. Finally, NP is more likely than (S/NP)/NP
since neither can combine, but NP is simpler.
By starting EM with these tag transition dis-
tributions and an unfiltered lexicon (word-to-
supertag dictionary), Baldridge obtains a tagging
accuracy of 56.1% on ambiguous words?a large
improvement over the accuracy of 33.0% obtained
by starting with uniform transition distributions.
We refer to a model learned from basic EM (uni-
formly initialized) as EM, and to a model with
grammar-informed initialization as EMGI .
4 Minimized models for supertagging
The idea of searching for minimized models is
related to classic Minimum Description Length
(MDL) (Barron et al, 1998), which seeks to se-
lect a small model that captures the most regularity
in the observed data. This modeling strategy has
been shown to produce good results for many nat-
ural language tasks (Goldsmith, 2001; Creutz and
Lagus, 2002; Ravi and Knight, 2009). For tagging,
the idea has been implemented using Bayesian
models with priors that indirectly induce sparsity
in the learned models (Goldwater and Griffiths,
2007); however, Ravi and Knight (2009) show a
better approach is to directly minimize the model
using an integer programming (IP) formulation.
Here, we build on this idea for supertagging.
There are many challenges involved in using IP
minimization for supertagging. The 1241 distinct
supertags in the tagset result in 1.5 million tag bi-
gram entries in the model and the dictionary con-
tains almost 3.5 million word/tag pairs that are rel-
evant to the test data. The set of 45 POS tags for
the same data yields 2025 tag bigrams and 8910
dictionary entries. We also wish to scale our meth-
ods to larger data settings than the 24k word tokens
in the test data used in the POS tagging task.
Our objective is to find the smallest supertag
grammar (of tag bigram types) that explains the
entire text while obeying the lexicon?s constraints.
However, the original IP method of Ravi and
Knight (2009) is intractable for supertagging, so
we propose a new two-stage method that scales to
the larger tagsets and data involved.
4.1 IP method for supertagging
Our goal for supertagging is to build a minimized
model with the following objective:
IPoriginal: Find the smallest supertag gram-
mar (i.e., tag bigrams) that can explain the en-
tire text (the test word token sequence).
Using the full grammar and lexicon to perform
model minimization results in a very large, diffi-
cult to solve integer program involving billions of
variables and constraints. This renders the mini-
mization objective IPoriginal intractable. One way
of combating this is to use a reduced grammar
and lexicon as input to the integer program. We
do this without further supervision by using the
HMM model trained using basic EM: entries are
pruned based on the tag sequence it predicts on
the test data. This produces an observed grammar
of distinct tag bigrams (Gobs) and lexicon of ob-
served lexical assignments (Lobs). For CCGbank,
Gobs and Lobs have 12,363 and 18,869 entries,
respectively?far less than the millions of entries
in the full grammar and lexicon.
Even though EM minimizes the model some-
what, many bad entries remain in the grammar.
We prune further by supplying Gobs and Lobs as
input (G,L) to the IP-minimization procedure.
However, even with the EM-reduced grammar and
lexicon, the IP-minimization is still very hard to
solve. We thus split it into two stages. The first
stage (Minimization 1) finds the smallest grammar
Gmin1 ? G that explains the set of word bigram
types observed in the data rather than the word
sequence itself, and the second (Minimization 2)
finds the smallest augmentation of Gmin1 that ex-
plains the full word sequence.
Minimization 1 (MIN1). We begin with a sim-
pler minimization problem than the original one
(IPoriginal), with the following objective:
IPmin 1: Find the smallest set of tag bigrams
Gmin1 ? G, such that there is at least one
tagging assignment possible for every word bi-
gram type observed in the data.
We formulate this as an integer program, creat-
ing binary variables gvari for every tag bigram
gi = tjtk in G. Binary link variables connect tag
bigrams with word bigrams; these are restricted
497
          :
          :
        t
i
 t
j
          :
          :
Input Grammar (G) word bigrams: 
w
1
 w
2
w
2
 w
3
:
:
w
i
 w
j
:
:
MIN 1
          :
          :
        t
i
 t
j
          :
          :
Input Grammar (G) word bigrams: 
w
1
 w
2
w
2
 w
3
:
:
w
i
 w
j
:
:
word sequence: 
w
1
       w
2          
w
3          
w
4        
w
5
t
1
t
2
t
3   
:
:
     
t
k
supertags
tag bigrams chosen in first minimization step (G
min1
)
(does not explain the word sequence)
word sequence: 
w
1
       w
2          
w
3          
w
4        
w
5
t
1
t
2
t
3   
:
:
     
t
k
supertags
tag bigrams chosen in second minimization step (G
min2
)
MIN 2
IP Minimization 1
IP Minimization 2
Figure 1: Two-stage IP method for selecting minimized models for supertagging.
to the set of links that respect the lexicon L pro-
vided as input, i.e., there exists a link variable
linkjklm connecting tag bigram tjtk with word bi-
gram wlwm only if the word/tag pairs (wl, tj) and
(wm, tk) are present in L. The entire integer pro-
gramming formulation is shown Figure 2.
The IP solver3 solves the above integer program
and we extract the set of tag bigrams Gmin1 based
on the activated grammar variables. For the CCG-
bank test data, MIN1 yields 2530 tag bigrams.
However, a second stage is needed since there is
no guarantee that Gmin1 can explain the test data:
it contains tags for all word bigram types, but it
cannot necessarily tag the full word sequence. Fig-
ure 1 illustrates this. Using only tag bigrams from
MIN1 (shown in blue), there is no fully-linked tag
path through the network. There are missing links
between words w2 and w3 and between words w3
and w4 in the word sequence. The next stage fills
in these missing links.
Minimization 2 (MIN2). This stage uses the
original minimization formulation for the su-
pertagging problem IPoriginal, again using an in-
teger programming method similar to that pro-
posed by Ravi and Knight (2009). If applied to
the observed grammar Gobs, the resulting integer
program is hard to solve.4 However, by using the
partial solution Gmin1 obtained in MIN1 the IP
optimization speeds up considerably. We imple-
ment this by fixing the values of all binary gram-
mar variables present in Gmin1 to 1 before opti-
mization. This reduces the search space signifi-
3We use the commercial CPLEX solver.
4The solver runs for days without returning a solution.
Minimize:
?
?gi?G
gvari
Subject to constraints:
1. For every word bigram wlwm, there exists at least
one tagging that respects the lexicon L.
?
? tj?L(wl), tk?L(wm)
linkjklm ? 1
where L(wl) and L(wm) represent the set of tags seen
in the lexicon for words wl and wm respectively.
2. The link variable assignments are constrained to re-
spect the grammar variables chosen by the integer pro-
gram.
linkjklm ? gvari
where gvari is the binary variable corresponding to tag
bigram tjtk in the grammar G.
Figure 2: IP formulation for Minimization 1.
cantly, and CPLEX finishes in just a few hours.
The details of this method are described below.
We instantiate binary variables gvari and lvari
for every tag bigram (in G) and lexicon entry (in
L). We then create a network of possible taggings
for the word token sequence w1w2....wn in the
corpus and assign a binary variable to each link
in the network. We name these variables linkcjk,
where c indicates the column of the link?s source
in the network, and j and k represent the link?s
source and destination (i.e., linkcjk corresponds to
tag bigram tjtk in column c). Next, we formulate
the integer program given in Figure 3.
Figure 1 illustrates how MIN2 augments the
grammar Gmin1 (links shown in blue) with addi-
498
Minimize:
?
?gi?G
gvari
Subject to constraints:
1. Chosen link variables form a left-to-right path
through the tagging network.
?c=1..n?2?k
?
j linkcjk =
?
j link(c+1)kj
2. Link variable assignments should respect the chosen
grammar variables.
for every link: linkcjk ? gvari
where gvari corresponds to tag bigram tjtk
3. Link variable assignments should respect the chosen
lexicon variables.
for every link: linkcjk ? lvarwctj
for every link: linkcjk ? lvarwc+1tk
where wc is the cth word in the word sequence w1...wn,
and lvarwctj is the binary variable corresponding to the
word/tag pair wc/tj in the lexicon L.
4. The final solution should produce at least one com-
plete tagging path through the network.
?
?j,k
link1jk ? 1
5. Provide minimized grammar from MIN1as partial
solution to the integer program.
?gi?Gmin1 gvari = 1
Figure 3: IP formulation for Minimization 2.
tional tag bigrams (shown in red) to form a com-
plete tag path through the network. The minimized
grammar set in the final solution Gmin2 contains
only 2810 entries, significantly fewer than the
original grammar Gobs?s 12,363 tag bigrams.
We note that the two-stage minimization pro-
cedure proposed here is not guaranteed to yield
the optimal solution to our original objective
IPoriginal. On the simpler task of unsupervised
POS tagging with a dictionary, we compared
our method versus directly solving IPoriginal and
found that the minimization (in terms of grammar
size) achieved by our method is close to the opti-
mal solution for the original objective and yields
the same tagging accuracy far more efficiently.
Fitting the minimized model. The IP-
minimization procedure gives us a minimal
grammar, but does not fit the model to the data.
In order to estimate probabilities for the HMM
model for supertagging, we use the EM algorithm
but with certain restrictions. We build the transi-
tion model using only entries from the minimized
grammar set Gmin2, and instantiate an emission
model using the word/tag pairs seen in L (pro-
vided as input to the minimization procedure). All
the parameters in the HMM model are initialized
with uniform probabilities, and we run EM for 40
iterations. The trained model is used to find the
Viterbi tag sequence for the corpus. We refer to
this model (where the EM output (Gobs, Lobs) was
provided to the IP-minimization as initial input)
as EM+IP.
Bootstrapped minimization. The quality of the
observed grammar and lexicon improves consid-
erably at the end of a single EM+IP run. Ravi
and Knight (2009) exploited this to iteratively im-
prove their POS tag model: since the first mini-
mization procedure is seeded with a noisy gram-
mar and tag dictionary, iterating the IP procedure
with progressively better grammars further im-
proves the model. We do likewise, bootstrapping a
new EM+IP run using as input, the observed gram-
mar Gobs and lexicon Lobs from the last tagging
output of the previous iteration. We run this until
the chosen grammar set Gmin2 does not change.5
4.2 Minimization with grammar-informed
initialization
There are two complementary ways to use
grammar-informed initialization with the IP-
minimization approach: (1) using EMGI output
as the starting grammar/lexicon and (2) using the
tag transitions directly in the IP objective function.
The first takes advantage of the earlier observation
that the quality of the grammar and lexicon pro-
vided as initial input to the minimization proce-
dure can affect the quality of the final supertagging
output. For the second, we modify the objective
function used in the two IP-minimization steps to
be:
Minimize:
?
?gi?G
wi ? gvari (1)
where, G is the set of tag bigrams provided as in-
put to IP, gvari is a binary variable in the integer
program corresponding to tag bigram (ti?1, ti) ?
G, and wi is negative logarithm of pgii(ti|ti?1)
as given by Baldridge (2008).6 All other parts of
5In our experiments, we run three bootstrap iterations.
6Other numeric weights associated with the tag bi-
grams could be considered, such as 0/1 for uncombin-
499
the integer program including the constraints re-
main unchanged, and, we acquire a final tagger in
the same manner as described in the previous sec-
tion. In this way, we combine the minimization
and GI strategies into a single objective function
that finds a minimal grammar set while keeping
the more likely tag bigrams in the chosen solution.
EMGI+IPGI is used to refer to the method that
uses GI information in both ways: EMGI output
as the starting grammar/lexicon and GI weights in
the IP-minimization objective.
5 Experiments
We compare the four strategies described in Sec-
tions 3 and 4, summarized below:
EM HMM uniformly initialized, EM training.
EM+IP IP minimization using initial grammar
provided by EM.
EMGI HMM with grammar-informed initializa-
tion, EM training.
EMGI+IPGI IP minimization using initial gram-
mar/lexicon provided by EMGI and addi-
tional grammar-informed IP objective.
For EM+IP and EMGI+IPGI , the minimization
and EM training processes are iterated until the
resulting grammar and lexicon remain unchanged.
Forty EM iterations are used for all cases.
We also include a baseline which randomly
chooses a tag from those associated with each
word in the lexicon, averaged over three runs.
Accuracy on ambiguous word tokens. We
evaluate the performance in terms of tagging accu-
racy with respect to gold tags for ambiguous words
in held-out test sets for English and Italian. We
consider results with and without punctuation.7
Recall that unlike much previous work, we do
not collect the lexicon (tag dictionary) from the
test set: this means the model must handle un-
known words and the possibility of having missing
lexical entries for covering the test set.
Precision and recall of grammar and lexicon.
In addition to accuracy, we measure precision and
able/combinable bigrams.
7The reason for this is that the ?categories? for punctua-
tion in CCGbank are for the most part not actual categories;
for example, the period ?.? has the categories ?.? and ?S?.
As such, these supertags are outside of the categorial system:
their use in derivations requires phrase structure rules that are
not derivable from the CCG combinatory rules.
Model ambig ambig all all
-punc -punc
Random 17.9 16.2 27.4 21.9
EM 38.7 35.6 45.6 39.8
EM+IP 52.1 51.0 57.3 53.9
EMGI 56.3 59.4 61.0 61.7
EMGI+IPGI 59.6 62.3 63.8 64.3
Table 2: Supertagging accuracy for CCGbank sec-
tions 22-24. Accuracies are reported for four
settings?(1) ambiguous word tokens in the test
corpus, (2) ambiguous word tokens, ignoring
punctuation, (3) all word tokens, and (4) all word
tokens except punctuation.
recall for each model on the observed bitag gram-
mar and observed lexicon on the test set. We cal-
culate them as follows, for an observed grammar
or lexicon X:
Precision =
|{X} ? {Observedgold}|
|{X}|
Recall =
|{X} ? {Observedgold}|
|{Observedgold}|
This provides a measure of model performance on
bitag types for the grammar and lexical entry types
for the lexicon, rather than tokens.
5.1 English CCGbank results
Accuracy on ambiguous tokens. Table 2 gives
performance on the CCGbank test sections. All
models are well above the random baseline, and
both of the strategies individually boost perfor-
mance over basic EM by a large margin. For the
models using GI, accuracy ignoring punctuation is
higher than for all almost entirely due to the fact
that ?.? has the supertags ?.? and S, and the GI
gives a preference to S since it can in fact combine
with other categories, unlike ?.??the effect is that
nearly every sentence-final period (?5.5k tokens) is
tagged S rather than ?.?.
EMGI is more effective than EM+IP; however,
it should be kept in mind that IP-minimization
is a general technique that can be applied to
any sequence prediction task, whereas grammar-
informed initialization may be used only with
tasks in which the interactions of adjacent labels
may be derived from the labels themselves. In-
terestingly, the gap between the two approaches
is greater when punctuation is ignored (51.0 vs.
59.4)?this is unsurprising because, as noted al-
ready, punctuation supertags are not actual cate-
500
EM EM+IP EMGI EMGI+IPGI
Grammar
Precision 7.5 32.9 52.6 68.1
Recall 26.9 13.2 34.0 19.8
Lexicon
Precision 58.4 63.0 78.0 80.6
Recall 50.9 56.0 71.5 67.6
Table 3: Comparison of grammar/lexicon ob-
served in the model tagging vs. gold tagging
in terms of precision and recall measures for su-
pertagging on CCGbank data.
gories, so EMGI is unable to model their distribu-
tion. Most importantly, the complementary effects
of the two approaches can be seen in the improved
results for EMGI+IPGI , which obtains about 3%
better accuracy than EMGI .
Accuracy on all tokens. Table 2 also gives per-
formance when taking all tokens into account. The
HMM when using full supervision obtains 87.6%
accuracy (Baldridge, 2008),8 so the accuracy of
63.8% achieved by EMGI+IPGI nearly halves the
gap between the supervised model and the 45.6%
obtained by basic EM semi-supervised model.
Effect of GI information in EM and/or IP-
minimization stages. We can also consider the
effect of GI information in either EM training or
IP-minimization to see whether it can be effec-
tively exploited in both. The latter, EM+IPGI ,
obtains 53.2/51.1 for all/no-punc?a small gain
compared to EM+IP?s 52.1/51.0. The former,
EMGI+IP, obtains 58.9/61.6?a much larger gain.
Thus, the better starting point provided by EMGI
has more impact than the integer program that in-
cludes GI in its objective function. However, we
note that it should be possible to exploit the GI
information more effectively in the integer pro-
gram than we have here. Also, our best model,
EMGI+IPGI , uses GI information in both stages
to obtain our best accuracy of 59.6/62.3.
P/R for grammars and lexicons. We can ob-
tain a more-fine grained understanding of how the
models differ by considering the precision and re-
call values for the grammars and lexicons of the
different models, given in Table 3. The basic EM
model has very low precision for the grammar, in-
dicating it proposes many unnecessary bitags; it
8A state-of-the-art, fully-supervised maximum entropy
tagger (Clark and Curran, 2007) (which also uses part-of-
speech labels) obtains 91.4% on the same train/test split.
achieves better recall because of the sheer num-
ber of bitags it proposes (12,363). EM+IP prunes
that set of bitags considerably, leading to better
precision at the cost of recall. EMGI ?s higher re-
call and precision indicate the tag transition dis-
tributions do capture general patterns of linkage
between adjacent CCG categories, while EM en-
sures that the data filters out combinable, but un-
necessary, bitags. With EMGI+IPGI , we again
see that IP-minimization prunes even more entries,
improving precision at the loss of some recall.
Similar trends are seen for precision and recall
on the lexicon. IP-minimization?s pruning of inap-
propriate taggings means more common words are
not assigned highly infrequent supertags (boosting
precision) while unknown words are generally as-
signed more sensible supertags (boosting recall).
EMGI again focuses taggings on combinable con-
texts, boosting precision and recall similarly to
EM+IP, but in greater measure. EMGI+IPGI then
prunes some of the spurious entries, boosting pre-
cision at some loss of recall.
Tag frequencies predicted on the test set. Ta-
ble 4 compares gold tags to tags generated by
all four methods for the frequent and highly am-
biguous words the and in. Basic EM wanders
far away from the gold assignments; it has little
guidance in the very large search space available
to it. IP-minimization identifies a smaller set of
tags that better matches the gold tags; this emerges
because other determiners and prepositions evoke
similar, but not identical, supertags, and the gram-
mar minimization pushes (but does not force)
them to rely on the same supertags wherever pos-
sible. However, the proportions are incorrect;
for example, the tag assigned most frequently to
in is ((S\NP)\(S\NP))/NP though (NP\NP)/NP
is more frequent in the test set. EMGI ?s tags
correct that balance and find better proportions,
but also some less common categories, such as
(((N/N)\(N/N))\((N/N)\(N/N)))/N, sneak in be-
cause they combine with frequent categories like
N/N and N. Bringing the two strategies together
with EMGI+IPGI filters out the unwanted cate-
gories while getting better overall proportions.
5.2 Italian CCG-TUT results
To demonstrate that both methods and their com-
bination are language independent, we apply them
to the Italian CCG-TUT corpus. We wanted
to evaluate performance out-of-the-box because
501
Lexicon Gold EM EM+IP EMGI EMGI+IPGI
the? (41 distinct tags in Ltrain) (14 tags) (18 tags) (9 tags) (25 tags) (12 tags)
NP[nb]/N 5742 0 4544 4176 4666
((S\NP)\(S\NP))/N 14 5 642 122 107
(((N/N)\(N/N))\((N/N)\(N/N)))/N 0 0 0 698 0
((S/S)/S[dcl])/(S[adj]\NP) 0 733 0 0 0
PP/N 0 1755 0 3 1
: : : : : :
in? (76 distinct tags in Ltrain) (35 tags) (20 tags) (17 tags) (37 tags) (14 tags)
(NP\NP)/NP 883 0 649 708 904
((S\NP)\(S\NP))/NP 793 0 911 320 424
PP/NP 177 1 33 12 82
((S[adj]\NP)/(S[adj]\NP))/NP 0 215 0 0 0
: : : : : :
Table 4: Comparison of tag assignments from the gold tags versus model tags obtained on the test set.
The table shows tag assignments (and their counts for each method) for the and in in the CCGbank test
sections. The number of distinct tags assigned by each method is given in parentheses. Ltrain is the
lexicon obtained from sections 0-18 of CCGbank that is used as the basis for EM training.
Model TEST 1 TEST 2 (using lexicon from:)
NPAPER+CIVIL NPAPER CIVIL
Random 9.6 9.7 8.4 9.6
EM 26.4 26.8 27.2 29.3
EM+IP 34.8 32.4 34.8 34.6
EMGI 43.1 43.9 44.0 40.3
EMGI+IPGI 45.8 43.6 47.5 40.9
Table 5: Comparison of supertagging results for
CCG-TUT. Accuracies are for ambiguous word
tokens in the test corpus, ignoring punctuation.
bootstrapping a supertagger for a new language is
one of the main use scenarios we envision: in such
a scenario, there is no development data for chang-
ing settings and parameters. Thus, we determined
a train/test split beforehand and ran the methods
exactly as we had for CCGbank.
The results, given in Table 5, demonstrate the
same trends as for English: basic EM is far more
accurate than random, EM+IP adds another 8-10%
absolute accuracy, and EMGI adds an additional 8-
10% again. The combination of the methods gen-
erally improves over EMGI , except when the lex-
icon is extracted from NPAPER+CIVIL. Table 6
gives precision and recall for the grammars and
lexicons for CCG-TUT?the values are lower than
for CCGbank (in line with the lower baseline), but
exhibit the same trends.
6 Conclusion
We have shown how two complementary
strategies?grammar-informed tag transitions and
IP-minimization?for learning of supertaggers
from highly ambiguous lexicons can be straight-
EM EM+IP EMGI EMGI+IPGI
Grammar
Precision 23.1 26.4 44.9 46.7
Recall 18.4 15.9 24.9 22.7
Lexicon
Precision 51.2 52.0 54.8 55.1
Recall 43.6 42.8 46.0 44.9
Table 6: Comparison of grammar/lexicon ob-
served in the model tagging vs. gold tagging
in terms of precision and recall measures for su-
pertagging on CCG-TUT.
forwardly integrated. We verify the benefits of
both cross-lingually, on English and Italian data.
We also provide a new two-stage integer program-
ming setup that allows model minimization to be
tractable for supertagging without sacrificing the
quality of the search for minimal bitag grammars.
The experiments in this paper use large lexi-
cons, but the methodology will be particularly use-
ful in the context of bootstrapping from smaller
ones. This brings further challenges; in particular,
it will be necessary to identify novel entries con-
sisting of seen word and seen category and to pre-
dict unseen, but valid, categories which are needed
to explain the data. For this, it will be necessary
to forgo the assumption that the provided lexicon
is always obeyed. The methods we introduce here
should help maintain good accuracy while open-
ing up these degrees of freedom. Because the lexi-
con is the grammar in CCG, learning new word-
category associations is grammar generalization
and is of interest for grammar acquisition.
502
Finally, such lexicon refinement and generaliza-
tion is directly relevant for using CCG in syntax-
based machine translation models (Hassan et al,
2009). Such models are currently limited to lan-
guages for which corpora annotated with CCG
derivations are available. Clark and Curran (2006)
show that CCG parsers can be learned from sen-
tences labeled with just supertags?without full
derivations?with little loss in accuracy. The im-
provements we show here for learning supertag-
gers from lexicons without labeled data may be
able to help create annotated resources more ef-
ficiently, or enable CCG parsers to be learned with
less human-coded knowledge.
Acknowledgements
The authors would like to thank Johan Bos, Joey
Frazee, Taesun Moon, the members of the UT-
NLL reading group, and the anonymous review-
ers. Ravi and Knight acknowledge the support
of the NSF (grant IIS-0904684) for this work.
Baldridge acknowledges the support of a grant
from the Morris Memorial Trust Fund of the New
York Community Trust.
References
J. Baldridge. 2008. Weakly supervised supertagging
with grammar-informed initialization. In Proceed-
ings of the 22nd International Conference on Com-
putational Linguistics (Coling 2008), pages 57?64,
Manchester, UK, August.
M. Banko and R. C. Moore. 2004. Part of speech
tagging in context. In Proceedings of the Inter-
national Conference on Computational Linguistics
(COLING), page 556, Morristown, NJ, USA.
A. R. Barron, J. Rissanen, and B. Yu. 1998. The
minimum description length principle in coding and
modeling. IEEE Transactions on Information The-
ory, 44(6):2743?2760.
J. Bos, C. Bosco, and A. Mazzei. 2009. Converting a
dependency treebank to a categorial grammar tree-
bank for Italian. In Proceedings of the Eighth In-
ternational Workshop on Treebanks and Linguistic
Theories (TLT8), pages 27?38, Milan, Italy.
S. Clark and J. Curran. 2006. Partial training for
a lexicalized-grammar parser. In Proceedings of
the Human Language Technology Conference of the
NAACL, Main Conference, pages 144?151, New
York City, USA, June.
S. Clark and J. Curran. 2007. Wide-coverage efficient
statistical parsing with CCG and log-linear models.
Computational Linguistics, 33(4).
M. Creutz and K. Lagus. 2002. Unsupervised discov-
ery of morphemes. In Proceedings of the ACLWork-
shop on Morphological and Phonological Learning,
pages 21?30, Morristown, NJ, USA.
Y. Goldberg, M. Adler, and M. Elhadad. 2008. EM can
find pretty good HMM POS-taggers (when given a
good start). In Proceedings of the ACL, pages 746?
754, Columbus, Ohio, June.
J. Goldsmith. 2001. Unsupervised learning of the mor-
phology of a natural language. Computational Lin-
guistics, 27(2):153?198.
S. Goldwater and T. L. Griffiths. 2007. A fully
Bayesian approach to unsupervised part-of-speech
tagging. In Proceedings of the ACL, pages 744?751,
Prague, Czech Republic, June.
H. Hassan, K. Sima?an, and A. Way. 2009. A syntac-
tified direct translation model with linear-time de-
coding. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1182?1191, Singapore, August.
J. Hockenmaier and M. Steedman. 2007. CCGbank:
A corpus of CCG derivations and dependency struc-
tures extracted from the Penn Treebank. Computa-
tional Linguistics, 33(3):355?396.
A. Joshi. 1988. Tree Adjoining Grammars. In David
Dowty, Lauri Karttunen, and Arnold Zwicky, ed-
itors, Natural Language Parsing, pages 206?250.
Cambridge University Press, Cambridge.
M. P. Marcus, M. A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguis-
tics, 19(2).
B. Merialdo. 1994. Tagging English text with a
probabilistic model. Computational Linguistics,
20(2):155?171.
C. Pollard and I. Sag. 1994. Head Driven Phrase
Structure Grammar. CSLI/Chicago University
Press, Chicago.
S. Ravi and K. Knight. 2009. Minimized models
for unsupervised part-of-speech tagging. In Pro-
ceedings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the
AFNLP, pages 504?512, Suntec, Singapore, August.
M. Steedman. 2000. The Syntactic Process. MIT
Press, Cambridge, MA.
Kristina Toutanova and Mark Johnson. 2008. A
Bayesian LDA-based model for semi-supervised
part-of-speech tagging. In Proceedings of the Ad-
vances in Neural Information Processing Systems
(NIPS), pages 1521?1528, Cambridge, MA. MIT
Press.
503
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 12?21,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Deciphering Foreign Language
Sujith Ravi and Kevin Knight
University of Southern California
Information Sciences Institute
Marina del Rey, California 90292
{sravi,knight}@isi.edu
Abstract
In this work, we tackle the task of ma-
chine translation (MT) without parallel train-
ing data. We frame the MT problem as a de-
cipherment task, treating the foreign text as
a cipher for English and present novel meth-
ods for training translation models from non-
parallel text.
1 Introduction
Bilingual corpora are a staple of statistical machine
translation (SMT) research. From these corpora,
we estimate translation model parameters: word-
to-word translation tables, fertilities, distortion pa-
rameters, phrase tables, syntactic transformations,
etc. Starting with the classic IBM work (Brown et
al., 1993), training has been viewed as a maximiza-
tion problem involving hidden word alignments (a)
that are assumed to underlie observed sentence pairs
(e, f ):
argmax
?
?
e,f
P?(f |e) (1)
= argmax
?
?
e,f
?
a
P?(f, a|e) (2)
Brown et al (1993) give various formulas that boil
P?(f, a|e) down to the specific parameters to be es-
timated.
Of course, for many language pairs and domains,
parallel data is not available. In this paper, we
address the problem of learning a full transla-
tion model from non-parallel data, and we use the
learned model to translate new foreign strings. As
successful work develops along this line, we expect
more domains and language pairs to be conquered
by SMT.
How can we learn a translation model from non-
parallel data? Intuitively, we try to construct trans-
lation model tables which, when applied to ob-
served foreign text, consistently yield sensible En-
glish. This is essentially the same approach taken by
cryptanalysts and epigraphers when they deal with
source texts.
In our case, we observe a large number of foreign
strings f , and we apply maximum likelihood train-
ing:
argmax
?
?
f
P?(f) (3)
Following Weaver (1955), we imagine that this cor-
pus of foreign strings ?is really written in English,
but has been coded in some strange symbols,? thus:
argmax
?
?
f
?
e
P (e) ? P?(f |e) (4)
The variable e ranges over all possible English
strings, and P (e) is a language model built from
large amounts of English text that is unrelated to the
foreign strings. Re-writing for hidden alignments,
we get:
argmax
?
?
f
?
e
P (e) ?
?
a
P?(f, a|e) (5)
Note that this formula has the same free
P?(f, a|e) parameters as expression (2). We seek
to manipulate these parameters in order to learn the
12
same full translation model. We note that for each
f , not only is the alignment a still hidden, but now
the English translation e is hidden as well.
A language model P (e) is typically used in SMT
decoding (Koehn, 2009), but here P (e) actually
plays a central role in training translation model pa-
rameters. To distinguish the two, we refer to (5) as
decipherment, rather than decoding.
We can now draw on previous decipherment
work for solving simpler substitution/transposition
ciphers (Bauer, 2006; Knight et al, 2006). We must
keep in mind, however, that foreign language is a
much more demanding code, involving highly non-
deterministic mappings and very large substitution
tables.
The contributions of this paper are therefore:
? We give first results for training a full transla-
tion model from non-parallel text, and we apply
the model to translate previously-unseen text.
This work is thus distinguished from prior work
on extracting or augmenting partial lexicons
using non-parallel corpora (Rapp, 1995; Fung
and McKeown, 1997; Koehn and Knight, 2000;
Haghighi et al, 2008). It also contrasts with
self-training (McClosky et al, 2006), which re-
quires a parallel seed and often does not engage
in iterative maximization.
? We develop novel methods to deal with large-
scale vocabularies inherent in MT problems.
2 Word Substitution Decipherment
Before we tackle machine translation without par-
allel data, we first solve a simpler problem?word
substitution decipherment. Here, we do not have to
worry about hidden alignments since there is only
one alignment. In a word substitution cipher, every
word in the natural language (plaintext) sequence is
substituted by a cipher token, according to a substi-
tution key. The key is deterministic?there exists a
1-to-1 mapping between cipher units and the plain-
text words they encode.
For example, the following English plaintext se-
quences:
I SAW THE BOY .
THE BOY RAN .
may be enciphered as:
xyzz fxyy crqq tmnz lxwz
crqq tmnz gdxx lxwz
according to the key:
THE ? crqq, SAW ? fxyy, RAN ? gdxx,
. ? lxwz, BOY ? tmnz, I ? xyzz
The goal of word substitution decipherment is to
guess the original plaintext from given cipher data
without any knowledge of the substitution key.
Word substitution decipherment is a good test-bed
for unsupervised statistical NLP techniques for two
reasons?(1) we face large vocabularies and corpora
sizes typically seen in large-scale MT problems, so
our methods need to scale well, (2) similar deci-
pherment techniques can be applied for solving NLP
problems such as unsupervised part-of-speech tag-
ging.
Probabilistic decipherment: Our decipherment
method follows a noisy-channel approach. We first
model the process by which the ciphertext sequence
c = c1...cn is generated. The generative story for
decipherment is described here:
1. Generate an English plaintext sequence e =
e1...en, with probability P (e).
2. Substitute each plaintext word ei with a cipher-
text token ci, with probability P?(ci|ei) in order
to generate the ciphertext sequence c = c1...cn.
We model P (e) using a statistical word n-gram
English language model (LM). During decipher-
ment, our goal is to estimate the channel model pa-
rameters ?. Re-writing Equations 3 and 4 for word
substitution decipherment, we get:
argmax
?
?
c
P?(c) (6)
= argmax
?
?
c
?
e
P (e) ?
n?
i=1
P?(ci|ei) (7)
Challenges: Unlike letter substitution ciphers
(having only 26 plaintext letters), here we have to
deal with large-scale vocabularies (10k-1M word
types) and corpora sizes (100k cipher tokens). This
poses some serious scalability challenges for word
substitution decipherment.
13
We propose novel methods that can deal with
these challenges effectively and solve word substi-
tution ciphers:
1. EM solution: We would like to use the Expecta-
tion Maximization (EM) algorithm (Dempster
et al, 1977) to estimate ? from Equation 7, but
EM training is not feasible in our case. First,
EM cannot scale to such large vocabulary sizes
(running the forward-backward algorithm for
each iteration requires O(V 2) time). Secondly,
we need to instantiate the entire channel and re-
sulting derivation lattice before we can run EM,
and this is too big to be stored in memory. So,
we introduce a new training method (Iterative
EM) that fixes these problems.
2. Bayesian decipherment: We also propose a
novel decipherment approach using Bayesian
inference. Typically, Bayesian inference is very
slow when applied to such large-scale prob-
lems. Our method overcomes these challenges
and does fast, efficient inference using (a) a
novel strategy for selecting sampling choices,
and (b) a parallelized sampling scheme.
In the next two sections, we describe these meth-
ods in detail.
2.1 Iterative EM
We devise a method which overcomes memory and
running time efficiency issues faced by EM. Instead
of instantiating the entire channel model (with all its
parameters), we iteratively train the model in small
steps. The training procedure is described here:
1. Identify the top K frequent word types in both
the plaintext and ciphertext data. Replace all
other word tokens with Unknown. Now, instan-
tiate a small channel with just (K + 1)2 pa-
rameters and use the EM algorithm to train this
model to maximize likelihood of cipher data.
2. Extend the plaintext and ciphertext vocabular-
ies from the previous step by adding the next
K most frequent word types (so the new vo-
cabulary size becomes 2K + 1). Regenerate
the plaintext and ciphertext data.
3. Instantiate a new (2K+1)? (2K+1) channel
model. From the previous EM-trained channel,
identify all the e ? c mappings that were as-
signed a probability P (c|e) > 0.5. Fix these
mappings in the new channel, i.e. set P (c|e) =
1.0. From the new channel, eliminate all other
parameters e ? cj associated with the plain-
text word type e (where cj 6= c). This yields a
much smaller channel with size < (2K + 1)2.
Retrain the new channel using EM algorithm.
4. Goto Step 2 and repeat the procedure, extend-
ing the channel size iteratively in each stage.
Finally, we decode the given ciphertext c by using
the Viterbi algorithm to choose the plaintext decod-
ing e that maximizes P (e) ? P?trained(c|e)
3, stretch-
ing the channel probabilities (Knight et al, 2006).
2.2 Bayesian Decipherment
Bayesian inference methods have become popular
in natural language processing (Goldwater and Grif-
fiths, 2007; Finkel et al, 2005; Blunsom et al, 2009;
Chiang et al, 2010; Snyder et al, 2010). These
methods are attractive for their ability to manage un-
certainty about model parameters and allow one to
incorporate prior knowledge during inference.
Here, we propose a novel decipherment approach
using Bayesian learning. Our method holds sev-
eral other advantages over the EM approach?(1)
inference using smart sampling strategies permits
efficient training, allowing us to scale to large
data/vocabulary sizes, (2) incremental scoring of
derivations during sampling allows efficient infer-
ence even when we use higher-order n-gram LMs,
(3) there are no memory bottlenecks since the full
channel model and derivation lattice are never in-
stantiated during training, and (4) prior specification
allows us to learn skewed distributions that are useful
here?word substitution ciphers exhibit 1-to-1 cor-
respondence between plaintext and cipher types.
We use the same generative story as before for
decipherment, except that we use Chinese Restau-
rant Process (CRP) formulations for the source and
channel probabilities. We use an English word bi-
gram LM as the base distribution (P0) for the source
model and specify a uniform P0 distribution for the
14
channel.1 We perform inference using point-wise
Gibbs sampling (Geman and Geman, 1984). We de-
fine a sampling operator that samples plaintext word
choices for every cipher token, one at a time. Using
the exchangeability property, we efficiently score
the probability of each derivation in an incremental
fashion. In addition, we make further improvements
to the sampling procedure which makes it faster.
Smart sample-choice selection: In the original
sampling step, for each cipher token we have to sam-
ple from a list of all possible plaintext choices (10k-
1M English words). There are 100k cipher tokens
in our data which means we have to perform ? 109
sampling operations to make one entire pass through
the data. We have to then repeat this process for
2000 iterations. Instead, we now reduce our choices
in each sampling step.
Say that our current plaintext hypothesis contains
English words X, Y and Z at positions i ? 1, i and
i+1 respectively. In order to sample at position i, we
choose the topK English words Y ranked by P (X Y
Z), which can be computed offline from a statistical
word bigram LM. If this probability is 0 (i.e., X and
Z never co-occurred), we randomly pick K words
from the plaintext vocabulary. We set K = 100 in
our experiments. This significantly reduces the sam-
pling possibilities (10k-1M reduces to 100) at each
step and allows us to scale to large plaintext vocab-
ulary sizes without enumerating all possible choices
at every cipher position.2
Parallelized Gibbs sampling: Secondly, we paral-
lelize our sampling step using a Map-Reduce frame-
work. In the past, others have proposed parallelized
sampling schemes for topic modeling applications
(Newman et al, 2009). In our method, we split the
entire corpus into separate chunks and we run the
sampling procedure on each chunk in parallel. At
1For word substitution decipherment, we want to keep the
language model probabilities fixed during training, and hence
we set the prior on that model to be high (? = 104). We use a
sparse Dirichlet prior for the channel (? = 0.01). We use the
output from Iterative EM decoding (using 101 x 101 channel)
as initial sample and run the sampler for 2000 iterations. Dur-
ing sampling, we use a linear annealing schedule decreasing the
temperature from 1? 0.08.
2Since we now sample from an approximate distribution, we
have to correct this with the Metropolis-Hastings algorithm. But
in practice we observe that samples from our proposal distribu-
tion are accepted with probability > 0.99, so we skip this step.
the end of each sampling iteration, we combine the
samples corresponding to each chunk and collect the
counts of all events?this forms our cache for the
next sampling iteration. In practice, we observe that
the parallelized sampling run converges quickly and
runs much faster than the conventional point-wise
sampling?for example, 3.1 hours (using 10 nodes)
versus 11 hours for one of the word substitution ex-
periments. We also notice a higher speedup when
scaling to larger vocabularies.3
Decoding the ciphertext: After the sampling run
has finished, we choose the final sample and ex-
tract a trained version of the channel model P?(c|e)
from this sample following the technique of Chi-
ang et al (2010). We then use the Viterbi algo-
rithm to choose the English plaintext e that maxi-
mizes P (e) ? P?trained(c|e)
3.
2.3 Experiments and Results
Data: For the word substitution experiments, we use
two corpora:
? Temporal expression corpus containing short
English temporal expressions such as ?THE
NEXT MONTH?, ?THE LAST THREE
YEARS?, etc. The cipher data contains 5000
expressions (9619 tokens, 153 word types).
We also have access to a separate English
corpus (which is not parallel to the ciphertext)
containing 125k temporal expressions (242k
word tokens, 201 word types) for LM training.
? Transtac corpus containing full English sen-
tences. The data consists of 10k cipher sen-
tences (102k tokens, 3397 word types); and
a plaintext corpus of 402k English sentences
(2.7M word tokens, 25761 word types) for LM
training. We use all the cipher data for deci-
pherment training but evaluate on the first 1000
cipher sentences.
The cipher data was originally generated from En-
glish text by substituting each English word with a
unique cipher word. We use the plaintext corpus to
3Type sampling could be applied on top of our methods to
further optimize performance. But more complex problems like
MT do not follow the same principles (1-to-1 key mappings)
as seen in word substitution ciphers, which makes it difficult to
identify type dependencies.
15
Method Decipherment Accuracy (%)
Temporal expr. Transtac
9k 100k
0. EM with 2-gram LM 87.8 Intractable
1. Iterative EM
with 2-gram LM 87.8 70.5 71.8
2. Bayesian
with 2-gram LM 88.6 60.1 80.0
with 3-gram LM 82.5
Figure 1: Comparison of word substitution decipherment
results using (1) Iterative EM, and (2) Bayesian method.
For the Transtac corpus, decipherment performance is
also shown for different training data sizes (9k versus
100k cipher tokens).
build an English word n-gram LM, which is used in
the decipherment process.
Evaluation: We compute the accuracy of a particu-
lar decipherment as the percentage of cipher tokens
that were correctly deciphered from the whole cor-
pus. We run the two methods (Iterative EM4 and
Bayesian) and then compare them in terms of word
substitution decipherment accuracies.
Results: Figure 1 compares the word substitution
results from Iterative EM and Bayesian decipher-
ment. Both methods achieve high accuracies, de-
coding 70-90% of the two word substitution ciphers.
Overall, Bayesian decipherment (with sparse priors)
performs better than Iterative EM and achieves the
best results on this task. We also observe that both
methods benefit from better LMs and more (cipher)
training data. Figure 2 shows sample outputs from
Bayesian decipherment.
3 Machine Translation as a Decipherment
Task
We now turn to the problem of MT without par-
allel data. From a decipherment perspective, ma-
chine translation is a much more complex task than
word substitution decipherment and poses several
technical challenges: (1) scalability due to large
corpora sizes and huge translation tables, (2) non-
determinism in translation mappings (a word can
have multiple translations), (3) re-ordering of words
4For Iterative EM, we start with a channel of size 101x101
(K=100) and in every pass we iteratively increase the vocabu-
lary sizes by 50, repeating the training procedure until the chan-
nel size becomes 351x351.
C: 3894 9411 4357 8446 5433
O: a diploma that?s good .
D: a fence that?s good .
C: 8593 7932 3627 9166 3671
O: three families living here ?
D: three brothers living here ?
C: 6283 8827 7592 6959 5120 6137 9723 3671
O: okay and what did they tell you ?
D: okay and what did they tell you ?
C: 9723 3601 5834 5838 3805 4887 7961 9723 3174 4518
9067 4488 9551 7538 7239 9166 3671
O: you mean if we come to see you in the afternoon after
five you?ll be here ?
D: i mean if we come to see you in the afternoon after thirty
you?ll be here ?
...
Figure 2: Comparison of the original (O) English plain-
text with output from Bayesian word substitution deci-
pherment (D) for a few samples cipher (C) sentences
from the Transtac corpus.
or phrases, (4) a single word can translate into a
phrase, and (5) insertion/deletion of words.
Problem Formulation: We formulate the MT de-
cipherment problem as?given a foreign text f (i.e.,
foreign word sequences f1...fm) and a monolingual
English corpus, our goal is to decipher the foreign
text and produce an English translation.
Probabilistic decipherment: Unlike parallel train-
ing, here we have to estimate the translation model
P?(f |e) parameters using only monolingual data.
During decipherment training, our objective is to es-
timate the model parameters ? in order to maximize
the probability of the foreign corpus f . From Equa-
tion 4 we have:
argmax
?
?
f
?
e
P (e) ? P?(f |e)
For P (e), we use a word n-gram LM trained on
monolingual English data. We then estimate param-
eters of the translation model P?(f |e) during train-
ing. Next, we present two novel decipherment ap-
proaches for MT training without parallel data.
1. EM Decipherment: We propose a new transla-
tion model for MT decipherment which can be
efficiently trained using the EM algorithm.
2. Bayesian Decipherment: We introduce a novel
method for estimating IBM Model 3 parame-
ters without parallel data, using Bayesian learn-
ing. Unlike EM, this method does not face any
16
memory issues and we use sampling to perform
efficient inference during training.
3.1 EM Decipherment
For the translation model P?(f |e), we would like
to use a well-known statistical model such as IBM
Model 3 and subsequently train it using the EM
algorithm. But without parallel training data, EM
training for IBM Model 3 becomes intractable due
to (1) scalability and efficiency issues because of
large-sized fertility and distortion parameter tables,
and (2) the resulting derivation lattices become too
big to be stored in memory.
Instead, we propose a simpler generative story for
MT without parallel data. Our model accounts for
(word) substitutions, insertions, deletions and local
re-ordering during the translation process but does
not incorporate fertilities or global re-ordering. We
describe the generative process here:
1. Generate an English string e = e1...el, with
probability P (e).
2. Insert a NULL word at any position in the En-
glish string, with uniform probability.
3. For each English word token ei (including
NULLs), choose a foreign word translation fi,
with probability P?(fi|ei). The foreign word
may be NULL.
4. Swap any pair of adjacent foreign words
fi?1, fi, with probability P?(swap). We set
this value to 0.1.
5. Output the foreign string f = f1...fm, skipping
over NULLs.
We use the EM algorithm to estimate all the pa-
rameters ? in order to maximize likelihood of the
foreign corpus. Finally, we use the Viterbi algo-
rithm to decode the foreign sentence f and pro-
duce an English translation e that maximizes P (e) ?
P?trained(f |e).
Linguistic knowledge for decipherment: To help
limit translation model size and deal with data spar-
sity problem, we use prior linguistic knowledge. We
use identity mappings for numeric values (for ex-
ample, ?8? maps to ?8?), and we split nouns into
morpheme units prior to decipherment training (for
example, ?YEARS?? ?YEAR? ?+S?).
Whole-segment Language Models: When using
word n-gram models of English for decipherment,
we find that some of the foreign sentences are
decoded into sequences (such as ?THANK YOU
TALKING ABOUT ??) that are not good English.
This stems from the fact that n-gram LMs have no
global information about what constitutes a valid
English segment. To learn this information auto-
matically, we build a P (e) model that only recog-
nizes English whole-segments (entire sentences or
expressions) observed in the monolingual training
data. We then use this model (in place of word n-
gram LMs) for decipherment training and decoding.
3.2 Bayesian Method
Brown et al (1993) provide an efficient algorithm
for training IBM Model 3 translation model when
parallel sentence pairs are available. But we wish
to perform IBM Model 3 training under non-parallel
conditions, which is intractable using EM training.
Instead, we take a Bayesian approach.
Following Equation 5, we represent the transla-
tion model as P?(f, a|e) in terms of hidden align-
ments a. Recall the generative story for IBM Model
3 translation which has the following formula:
P?(f, a|e) =
l?
i=0
t?(faj |ei) ?
l?
i=1
n?(?i|ei)
?
m?
aj 6=0,j=1
d?(aj |i, l,m) ?
l?
i=0
?i!
?
1
?0!
?
(
m? ?0
?0
)
?p?01? ? p
m?2?0
0?
(8)
The alignment a is represented as a vector; aj = i
implies that the foreign word fj is produced by the
English word ei during translation.
Bayesian Formulation: Our goal is to learn the
probability tables t (translation parameters) n (fer-
tility parameters), d (distortion parameters), and p
(English NULL word probabilities) without parallel
data. In order to apply Bayesian inference for de-
cipherment, we model each of these tables using a
17
Chinese Restaurant Process (CRP) formulation. For
example, to model the translation probabilities, we
use the formula:
t?(fj |ei) =
? ? P0(fj |ei) + Chistory(ei, fj)
?+ Chistory(ei)
(9)
where, P0 represents the base distribution (which
is set to uniform) and Chistory represents the count
of events occurring in the history (cache). Similarly,
we use CRP formulations for the other probabilities
(n, d and p). We use sparse Dirichlet priors for all
these models (i.e., low values for ?) and plug these
probabilities into Equation 8 to get P?(f, a|e).
Sampling IBM Model 3: We use point-wise Gibbs
sampling to estimate the IBM Model 3 parameters.
The sampler is seeded with an initial English sample
translation and a corresponding alignment for every
foreign sentence. We define several sampling oper-
ators, which are applied in sequence one after the
other to generate English samples for the entire for-
eign corpus. Some of the sampling operators are de-
scribed below:
? TranslateWord(j): Sample a new English word
translation for foreign word fj , from all possi-
bilities (including NULL).
? SwapSegment(i1, i2): Swap the alignment
links for English words ei1 and ei2 .
? JoinWords(i1, i2): Eliminate the English word
ei1 and transfer its links to the word ei2 .
During sampling, we apply each of these opera-
tors to generate a new derivation e, a for the foreign
text f and compute its score as P (e) ? P?(f, a|e).
These small-change operators are similar to the
heuristic techniques used for greedy decoding by
German et al (2001). But unlike the greedy method,
which can easily get stuck, our Bayesian approach
guarantees that once the sampler converges we will
be sampling from the true posterior distribution.
As with Bayesian decipherment for word sub-
stitution, we compute the probability of each new
derivation incrementally, which makes sampling ef-
ficient. We also apply blocked sampling on top
of point-wise sampling?we treat all occurrences
of a particular foreign sentence as a single block
and sample a single derivation for the entire block.
We also parallelize the sampling procedure (as de-
scribed in Section 2.2).5
Choosing the best translation: Once the sampling
run finishes, we select the final sample and extract
the corresponding English translations for every for-
eign sentence. This yields the final decipherment
output.
3.3 MT Experiments and Results
Data: We work with the Spanish/English language
pair and use the following corpora in our MT exper-
iments:
? Time corpus: We mined English newswire
text on the Web and collected 295k tempo-
ral expressions such as ?LAST YEAR?, ?THE
FOURTH QUARTER?, ?IN JAN 1968?, etc.
We first process the data and normalize num-
bers and names of months/weekdays?for ex-
ample, ?1968? is replaced with ?NNNN?,
?JANUARY? with ?[MONTH]?, and so on. We
then translate the English temporal phrases into
Spanish using an automatic translation soft-
ware (Google Translate) followed by manual
annotation to correct mistakes made by the
software. We create the following splits out of
the resulting parallel corpus:
TRAIN (English): 195k temporal expressions
(7588 unique), 382k word tokens, 163 types.
TEST (Spanish): 100k temporal expressions
(2343 unique), 204k word tokens, 269 types.
? OPUS movie subtitle corpus: This is a large
open source collection of parallel corpora avail-
able for multiple language pairs (Tiedemann,
2009). We downloaded the parallel Span-
ish/English subtitle corpus which consists of
aligned Spanish/English sentences from a col-
lection of movie subtitles. For our MT ex-
periments, we select only Spanish/English sen-
tences with frequency > 10 and create the fol-
lowing train/test splits:
5For Bayesian MT decipherment, we set a high prior value
on the language model (104) and use sparse priors for the IBM 3
model parameters t, n, d, p (0.01, 0.01, 0.01, 0.01). We use the
output from EM decipherment as the initial sample and run the
sampler for 2000 iterations, during which we apply annealing
with a linear schedule (2? 0.08).
18
Method Decipherment Accuracy
Time expressions OPUS subtitles
1a. Parallel training (MOSES)
with 2-gram LM 5.6 (85.6) 26.8 (63.6)
with 5-gram LM 4.7 (88.0)
1b. Parallel training (IBM 3 without distortion)
with 2-gram LM 10.1 (78.9) 29.9 (59.6)
with whole-segment LM 9.0 (79.2)
2a. Decipherment (EM)
with 2-gram LM 37.6 (44.6) 67.2 (15.3)
with whole-segment LM 28.7 (48.7) 65.1 (19.3)
2b. Decipherment (Bayesian IBM 3)
with 2-gram LM 34.0 (30.2) 66.6 (15.1)
Figure 3: Comparison of Spanish/English MT performance on the Time and OPUS test corpora achieved by various
MT systems trained under (1) parallel?(a) MOSES, (b) IBM 3 without distortion, and (2) decipherment settings?
(a) EM, (b) Bayesian. The scores reported here are normalized edit distance values with BLEU scores shown in
parentheses.
TRAIN (English): 19770 sentences (1128
unique), 62k word tokens, 411 word types.
TEST (Spanish): 13181 sentences (1127
unique), 39k word tokens, 562 word types.
Both Spanish/English sides of TRAIN are used for
parallel MT training, whereas decipherment uses
only monolingual English data for training LMs.
MT Systems: We build and compare different MT
systems under two training scenarios:
1. Parallel training using: (a) MOSES, a phrase
translation system (Koehn et al, 2007) widely
used in MT literature, and (b) a simpler version
of IBM Model 3 (without distortion param-
eters) which can be trained tractably using the
strategy of Knight and Al-Onaizan (1998).
2. Decipherment without parallel data using:
(a) EM method (from Section 3.1), and (b)
Bayesian method (from Section 3.2).
Evaluation: All the MT systems are run on the
Spanish test data and the quality of the result-
ing English translations are evaluated using two
different measures?(1) Normalized edit distance
score (Navarro, 2001),6 and (2) BLEU (Papineni et
6When computing edit distance, we account for substitu-
tions, insertions, deletions as well as local-swap edit operations
required to convert a given English string into the (gold) refer-
ence translation.
al., 2002), a standard MT evaluation measure.
Results: Figure 3 compares the results of vari-
ous MT systems (using parallel versus decipherment
training) on the two test corpora in terms of edit dis-
tance scores (a lower score indicates closer match to
the gold translation). The figure also shows the cor-
responding BLEU scores in parentheses for compar-
ison (higher scores indicate better MT output).
We observe that even without parallel training
data, our decipherment strategies achieve MT accu-
racies comparable to parallel-trained systems. On
the Time corpus, the best decipherment (Method
2a in the figure) achieves an edit distance score of
28.7 (versus 4.7 for MOSES). Better LMs yield bet-
ter MT results for both parallel and decipherment
training?for example, using a segment-based En-
glish LM instead of a 2-gram LM yields a 24% re-
duction in edit distance and a 9% improvement in
BLEU score for EM decipherment.
We also investigate how the performance of dif-
ferent MT systems vary with the size of the training
data. Figure 4 plots the BLEU scores versus training
sizes for different MT systems on the Time corpus.
Clearly, using more training data yields better per-
formance for all systems. However, higher improve-
ments are observed when using parallel data in com-
parison to decipherment training which only uses
monolingual data. We also notice that the scores do
not improve much when going beyond 10,000 train-
19
Figure 4: Comparison of training data size versus MT ac-
curacy in terms of BLEU score under different training
conditions: (1) Parallel training?(a) MOSES, (b) IBM
Model 3 without distortion, and (2) Decipherment with-
out parallel data using EM method (from Section 3.1).
ing instances for this domain.
It is interesting to quantify the value of parallel
versus non-parallel data for any given MT task. In
other words, ?how much non-parallel data is worth
how much parallel data in order to achieve the same
MT accuracy?? Figure 4 provides a reasonable an-
swer to this question for the Spanish/English MT
task described here. We see that deciphering with
10k monolingual Spanish sentences yields the same
performance as training with around 200-500 paral-
lel English/Spanish sentence pairs. This is the first
attempt at such a quantitative comparison for MT
and our results are encouraging. We envision that
further developments in unsupervised methods will
help reduce this gap further.
4 Conclusion
Our work is the first attempt at doing MT with-
out parallel data. We discussed several novel deci-
pherment approaches for achieving this goal. Along
the way, we developed efficient training methods
that can deal with large-scale vocabularies and data
sizes. For future work, it will be interesting to see if
we can exploit both parallel and non-parallel data to
improve on both.
Acknowledgments
This material is based in part upon work supported
by the National Science Foundation (NSF) under
Grant No. IIS-0904684 and the Defense Advanced
Research Projects Agency (DARPA) through the
Department of Interior/National Business Center un-
der Contract No. NBCHD040058. Any opinion,
findings and conclusions or recommendations ex-
pressed in this material are those of the author(s) and
do not necessarily reflect the views of the Defense
Advanced Research Projects Agency (DARPA), or
the Department of the Interior/National Business
Center.
References
Friedrich L. Bauer. 2006. Decrypted Secrets: Methods
and Maxims of Cryptology. Springer-Verlag.
Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Os-
borne. 2009. A Gibbs sampler for phrasal syn-
chronous grammar induction. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the Asian Federa-
tion of Natural Language Processing (ACL-IJCNLP),
pages 782?790.
Peter Brown, Vincent Della Pietra, Stephen Della Pietra,
and Robert Mercer. 1993. The mathematics of statis-
tical machine translation: Parameter estimation. Com-
putational linguistics, 19(2):263?311.
David Chiang, Jonathan Graehl, Kevin Knight, Adam
Pauls, and Sujith Ravi. 2010. Bayesian inference for
finite-state transducers. In Proceedings of the Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics - Human Language
Technologies (NAACL/HLT), pages 447?455.
Arthur P. Dempster, Nan M. Laird, and Donald B. Ru-
bin. 1977. Maximum likelihood from incomplete data
via the EM algorithm. Journal of the Royal Statistical
Society, Series B, 39(1):1?38.
Jenny Finkel, Trond Grenager, and Christopher Manning.
2005. Incorporating non-local information into infor-
mation extraction systems by Gibbs sampling. In Pro-
ceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL), pages 363?
370.
Pascal Fung and Kathleen McKeown. 1997. Finding ter-
minology translations from non-parallel corpora. In
Proceedings of the Fifth Annual Workshop on Very
Large Corpora, pages 192?202.
20
Stuart Geman and Donald Geman. 1984. Stochastic re-
laxation, Gibbs distributions and the Bayesian restora-
tion of images. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 6(6):721?741.
Ulrich Germann, Michael Jahr, Kevin Knight, Daniel
Marcu, and Kenji Yamada. 2001. Fast decoding and
optimal decoding for machine translation. In Proceed-
ings of the 39th Annual Meeting on Association for
Computational Linguistics, pages 228?235.
Sharon Goldwater and Thomas Griffiths. 2007. A fully
Bayesian approach to unsupervised part-of-speech tag-
ging. In Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics, pages 744?
751.
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexi-
cons from monolingual corpora. In Proceedings of
the Annual Meeting of the Association for Compu-
tational Linguistics - Human Language Technologies
(ACL/HLT), pages 771?779.
Kevin Knight and Yaser Al-Onaizan. 1998. Transla-
tion with finite-state devices. In David Farwell, Laurie
Gerber, and Eduard Hovy, editors, Machine Transla-
tion and the Information Soup, volume 1529 of Lecture
Notes in Computer Science, pages 421?437. Springer
Berlin / Heidelberg.
Kevin Knight, Anish Nair, Nishit Rathod, and Kenji Ya-
mada. 2006. Unsupervised analysis for decipherment
problems. In Proceedings of the Joint Conference of
the International Committee on Computational Lin-
guistics and the Association for Computational Lin-
guistics, pages 499?506.
Philipp Koehn and Kevin Knight. 2000. Estimating word
translation probabilities from unrelated monolingual
corpora using the EM algorithm. In Proceedings of
the Seventeenth National Conference on Artificial In-
telligence and Twelfth Conference on Innovative Ap-
plications of Artificial Intelligence, pages 711?715.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondr?ej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the ACL on Inter-
active Poster and Demonstration Sessions.
Philip Koehn. 2009. Statistical Machine Translation.
Cambridge University Press.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Effective self-training for parsing. In Proceed-
ings of the main conference on Human Language Tech-
nology Conference of the North American Chapter of
the Association of Computational Linguistics, pages
152?159.
Gonzalo Navarro. 2001. A guided tour to approximate
string matching. ACM Computing Surveys, 33:31?88,
March.
David Newman, Arthur Asuncion, Padhraic Smyth, and
Max Welling. 2009. Distributed algorithms for
topic models. Journal of Machine Learning Research,
10:1801?1828.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics, pages 311?318.
Reinhard Rapp. 1995. Identifying word translations in
non-parallel texts. In Proceedings of the Conference of
the Association for Computational Linguistics, pages
320?322.
Benjamin Snyder, Regina Barzilay, and Kevin Knight.
2010. A statistical model for lost language decipher-
ment. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics, pages
1048?1057.
Jo?rg Tiedemann. 2009. News from OPUS - A collec-
tion of multilingual parallel corpora with tools and in-
terfaces. In N. Nicolov, K. Bontcheva, G. Angelova,
and R. Mitkov, editors, Recent Advances in Natural
Language Processing, volume V, pages 237?248. John
Benjamins, Amsterdam/Philadelphia.
Warren Weaver. 1955. Translation (1949). Reproduced
in W.N. Locke, A.D. Booth (eds.). In Machine Trans-
lation of Languages, pages 15?23. MIT Press.
21
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 239?247,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Bayesian Inference for Zodiac and Other Homophonic Ciphers
Sujith Ravi and Kevin Knight
University of Southern California
Information Sciences Institute
Marina del Rey, California 90292
{sravi,knight}@isi.edu
Abstract
We introduce a novel Bayesian approach for
deciphering complex substitution ciphers. Our
method uses a decipherment model which
combines information from letter n-gram lan-
guage models as well as word dictionaries.
Bayesian inference is performed on our model
using an efficient sampling technique. We
evaluate the quality of the Bayesian deci-
pherment output on simple and homophonic
letter substitution ciphers and show that un-
like a previous approach, our method consis-
tently produces almost 100% accurate deci-
pherments. The new method can be applied
on more complex substitution ciphers and we
demonstrate its utility by cracking the famous
Zodiac-408 cipher in a fully automated fash-
ion, which has never been done before.
1 Introduction
Substitution ciphers have been used widely in the
past to encrypt secrets behind messages. These
ciphers replace (English) plaintext letters with ci-
pher symbols in order to generate the ciphertext se-
quence.
There exist many published works on automatic
decipherment methods for solving simple letter-
substitution ciphers. Many existing methods use
dictionary-based attacks employing huge word dic-
tionaries to find plaintext patterns within the ci-
phertext (Peleg and Rosenfeld, 1979; Ganesan and
Sherman, 1993; Jakobsen, 1995; Olson, 2007).
Most of these methods are heuristic in nature and
search for the best deterministic key during deci-
pherment. Others follow a probabilistic decipher-
ment approach. Knight et al (2006) use the Expec-
tation Maximization (EM) algorithm (Dempster et
al., 1977) to search for the best probabilistic key us-
ing letter n-gram models. Ravi and Knight (2008)
formulate decipherment as an integer programming
problem and provide an exact method to solve sim-
ple substitution ciphers by using letter n-gram mod-
els along with deterministic key constraints. Corlett
and Penn (2010) work with large ciphertexts con-
taining thousands of characters and provide another
exact decipherment method using an A* search al-
gorithm. Diaconis (2008) presents an analysis of
Markov Chain Monte Carlo (MCMC) sampling al-
gorithms and shows an example application for solv-
ing simple substitution ciphers.
Most work in this area has focused on solving
simple substitution ciphers. But there are variants
of substitution ciphers, such as homophonic ciphers,
which display increasing levels of difficulty and
present significant challenges for decipherment. The
famous Zodiac serial killer used one such cipher sys-
tem for communication. In 1969, the killer sent a
three-part cipher message to newspapers claiming
credit for recent shootings and crimes committed
near the San Francisco area. The 408-character mes-
sage (Zodiac-408) was manually decoded by hand in
the 1960?s. Oranchak (2008) presents a method for
solving the Zodiac-408 cipher automatically with a
dictionary-based attack using a genetic algorithm.
However, his method relies on using plaintext words
from the known solution to solve the cipher, which
departs from a strict decipherment scenario.
In this paper, we introduce a novel method for
239
solving substitution ciphers using Bayesian learn-
ing. Our novel contributions are as follows:
? We present a new probabilistic decipherment
approach using Bayesian inference with sparse
priors, which can be used to solve different
types of substitution ciphers.
? Our new method combines information from
word dictionaries along with letter n-gram
models, providing a robust decipherment
model which offsets the disadvantages faced by
previous approaches.
? We evaluate the Bayesian decipherment output
on three different types of substitution ciphers
and show that unlike a previous approach, our
new method solves all the ciphers completely.
? Using the Bayesian decipherment, we show for
the first time a truly automated system that suc-
cessfully solves the Zodiac-408 cipher.
2 Letter Substitution Ciphers
We use natural language processing techniques to
attack letter substitution ciphers. In a letter substi-
tution cipher, every letter p in the natural language
(plaintext) sequence is replaced by a cipher token c,
according to some substitution key.
For example, an English plaintext
?H E L L O W O R L D ...?
may be enciphered as:
?N O E E I T I M E L ...?
according to the key:
p: ABCDEFGHIJKLMNOPQRSTUVWXYZ
c: XYZLOHANBCDEFGIJKMPQRSTUVW
where, ? ? represents the space character (word
boundary) in the English and ciphertext messages.
If the recipients of the ciphertext message have
the substitution key, they can use it (in reverse) to
recover the original plaintext. The sender can en-
crypt the message using one of many different ci-
pher systems. The particular type of cipher system
chosen determines the properties of the key. For ex-
ample, the substitution key can be deterministic in
both the encipherment and decipherment directions
as shown in the above example?i.e., there is a 1-to-
1 correspondence between the plaintext letters and
ciphertext symbols. Other types of keys exhibit non-
determinism either in the encipherment (or decipher-
ment) or both directions.
2.1 Simple Substitution Ciphers
The key used in a simple substitution cipher is deter-
ministic in both the encipherment and decipherment
directions, i.e., there is a 1-to-1 mapping between
plaintext letters and ciphertext symbols. The exam-
ple shown earlier depicts how a simple substitution
cipher works.
Data: In our experiments, we work with a 414-
letter simple substitution cipher. We encrypt an
original English plaintext message using a randomly
generated simple substitution key to create the ci-
phertext. During the encipherment process, we pre-
serve spaces between words and use this information
for decipherment?i.e., plaintext character ? ? maps
to ciphertext character ? ?. Figure 1 (top) shows
a portion of the ciphertext along with the original
plaintext used to create the cipher.
2.2 Homophonic Ciphers
A homophonic cipher uses a substitution key that
maps a plaintext letter to more than one cipher sym-
bol.
For example, the English plaintext:
?H E L L O W O R L D ...?
may be enciphered as:
?65 82 51 84 05 60 54 42 51 45 ...?
according to the key:
A: 09 12 33 47 53 67 78 92
B: 48 81
...
E: 14 16 24 44 46 55 57 64 74 82 87
...
L: 51 84
...
Z: 02
Here, ? ? represents the space character in both
English and ciphertext. Notice the non-determinism
involved in the enciphering direction?the English
240
letter ?L? is substituted using different symbols (51,
84) at different positions in the ciphertext.
These ciphers are more complex than simple sub-
stitution ciphers. Homophonic ciphers are generated
via a non-deterministic encipherment process?the
key is 1-to-many in the enciphering direction. The
number of potential cipher symbol substitutes for a
particular plaintext letter is often proportional to the
frequency of that letter in the plaintext language?
for example, the English letter ?E? is assigned more
cipher symbols than ?Z?. The objective of this is
to flatten out the frequency distribution of cipher-
text symbols, making a frequency-based cryptanaly-
sis attack difficult.
The substitution key is, however, deterministic in
the decipherment direction?each ciphertext symbol
maps to a single plaintext letter. Since the ciphertext
can contain more than 26 types, we need a larger
alphabet system?we use a numeric substitution al-
phabet in our experiments.
Data: For our decipherment experiments
on homophonic ciphers, we use the same
414-letter English plaintext used in Sec-
tion 2.1. We encrypt this message using a
homophonic substitution key (available from
http://www.simonsingh.net/The Black Chamber/ho
mophoniccipher.htm). As before, we preserve
spaces between words in the ciphertext. Figure 1
(middle) displays a section of the homophonic
cipher (with spaces) and the original plaintext
message used in our experiments.
2.3 Homophonic Ciphers without spaces
(Zodiac-408 cipher)
In the previous two cipher systems, the word-
boundary information was preserved in the cipher.
We now consider a more difficult homophonic ci-
pher by removing space characters from the original
plaintext.
The English plaintext from the previous example
now looks like this:
?HELLOWORLD ...?
and the corresponding ciphertext is:
?65 82 51 84 05 60 54 42 51 45 ...?
Without the word boundary information, typical
dictionary-based decipherment attacks fail on such
ciphers.
Zodiac-408 cipher: Homophonic ciphers with-
out spaces have been used extensively in the past to
encrypt secret messages. One of the most famous
homophonic ciphers in history was used by the in-
famous Zodiac serial killer in the 1960?s. The killer
sent a series of encrypted messages to newspapers
and claimed that solving the ciphers would reveal
clues to his identity. The identity of the Zodiac killer
remains unknown to date. However, the mystery
surrounding this has sparked much interest among
cryptanalysis experts and amateur enthusiasts.
The Zodiac messages include two interesting ci-
phers: (1) a 408-symbol homophonic cipher without
spaces (which was solved manually by hand), and
(2) a similar looking 340-symbol cipher that has yet
to be solved.
Here is a sample of the Zodiac-408 cipher mes-
sage:
...
and the corresponding section from the original
English plaintext message:
I L I K E K I L L I N G P E O P L
E B E C A U S E I T I S S O M U C
H F U N I T I S M O R E F U N T H
A N K I L L I N G W I L D G A M E
I N T H E F O R R E S T B E C A U
S E M A N I S T H E M O S T D A N
G E R O U E A N A M A L O F A L L
T O K I L L S O M E T H I N G G I
...
Besides the difficulty with missing word bound-
aries and non-determinism associated with the key,
the Zodiac-408 cipher poses several additional chal-
lenges which makes it harder to solve than any
standard homophonic cipher. There are spelling
mistakes in the original message (for example,
the English word ?PARADISE? is misspelt as
241
?PARADICE?) which can divert a dictionary-based
attack. Also, the last 18 characters of the plaintext
message does not seem to make any sense (?EBE-
ORIETEMETHHPITI?).
Data: Figure 1 (bottom) displays the Zodiac-408
cipher (consisting of 408 tokens, 54 symbol types)
along with the original plaintext message. We run
the new decipherment method (described in Sec-
tion 3.1) and show that our approach can success-
fully solve the Zodiac-408 cipher.
3 Decipherment
Given a ciphertext message c1...cn, the goal of de-
cipherment is to uncover the hidden plaintext mes-
sage p1...pn. The size of the keyspace (i.e., num-
ber of possible key mappings) that we have to navi-
gate during decipherment is huge?a simple substi-
tution cipher has a keyspace size of 26!, whereas a
homophonic cipher such as the Zodiac-408 cipher
has 2654 possible key mappings.
Next, we describe a new Bayesian decipherment
approach for tackling substitution ciphers.
3.1 Bayesian Decipherment
Bayesian inference methods have become popular
in natural language processing (Goldwater and Grif-
fiths, 2007; Finkel et al, 2005; Blunsom et al, 2009;
Chiang et al, 2010). Snyder et al (2010) proposed
a Bayesian approach in an archaeological decipher-
ment scenario. These methods are attractive for their
ability to manage uncertainty about model parame-
ters and allow one to incorporate prior knowledge
during inference. A common phenomenon observed
while modeling natural language problems is spar-
sity. For simple letter substitution ciphers, the origi-
nal substitution key exhibits a 1-to-1 correspondence
between the plaintext letters and cipher types. It is
not easy to model such information using conven-
tional methods like EM. But we can easily spec-
ify priors that favor sparse distributions within the
Bayesian framework.
Here, we propose a novel approach for decipher-
ing substitution ciphers using Bayesian inference.
Rather than enumerating all possible keys (26! for
a simple substitution cipher), our Bayesian frame-
work requires us to sample only a small number of
keys during the decipherment process.
Probabilistic Decipherment: Our decipherment
method follows a noisy-channel approach. We are
faced with a ciphertext sequence c = c1...cn and
we want to find the (English) letter sequence p =
p1...pn that maximizes the probability P (p|c).
We first formulate a generative story to model the
process by which the ciphertext sequence is gener-
ated.
1. Generate an English plaintext sequence p =
p1...pn, with probability P (p).
2. Substitute each plaintext letter pi with a cipher-
text token ci, with probability P (ci|pi) in order
to generate the ciphertext sequence c = c1...cn.
We build a statistical English language model
(LM) for the plaintext source model P (p), which
assigns a probability to any English letter sequence.
Our goal is to estimate the channel model param-
eters ? in order to maximize the probability of the
observed ciphertext c:
argmax
?
P (c) = argmax
?
?
p
P?(p, c) (1)
= argmax
?
?
p
P (p) ? P?(c|p) (2)
= argmax
?
?
p
P (p) ?
n?
i=1
P?(ci|pi) (3)
We estimate the parameters ? using Bayesian
learning. In our decipherment framework, a Chinese
Restaurant Process formulation is used to model
both the source and channel. The detailed genera-
tive story using CRPs is shown below:
1. i? 1
2. Generate the English plaintext letter p1, with
probability P0(p1)
3. Substitute p1 with cipher token c1, with proba-
bility P0(c1|p1)
4. i? i+ 1
5. Generate English plaintext letter pi, with prob-
ability
? ? P0(pi|pi?1) + C
i?1
1 (pi?1, pi)
?+ Ci?11 (pi?1)
242
Plaintext: D E C I P H E R M E N T I S T H E A N A L Y S I S O F D O C U M E N T S
W R I T T E N I N A N C I E N T L A N G U A G E S W H E R E T H E ...
Ciphertext: i n g c m p n q s n w f c v f p n o w o k t v c v h u i h g z s n w f v
r q c f f n w c w o w g c n w f k o w a z o a n v r p n q n f p n ...
Bayesian solution: D E C I P H E R M E N T I S T H E A N A L Y S I S O F D O C U M E N T S
W R I T T E N I N A N C I E N T L A N G U A G E S W H E R E T H E ...
Plaintext: D E C I P H E R M E N T I S T H E A N A L Y S I S
O F D O C U M E N T S W R I T T E N I N ...
Ciphertext: 79 57 62 93 95 68 44 77 22 74 59 97 32 86 85 56 82 67 59 67 84 52 86 73 11
99 10 45 90 13 61 27 98 71 49 19 60 80 88 85 20 55 59 32 91 ...
Bayesian solution: D E C I P H E R M E N T I S T H E A N A L Y S I S
O F D O C U M E N T S W R I T T E N I N ...
Ciphertext:
Plaintext:
Bayesian solution (final decoding): I L I K E K I L L I N G P E O P L E B E C A U S E
I T I S S O M U C H F U N I T I A M O R E F U N T
H A N K I L L I N G W I L D G A M E I N T H E F O
R R E S T B E C A U S E M A N I S T H E M O A T D
A N G E R T U E A N A M A L O F A L L ...
(with spaces shown): I L I K E K I L L I N G P E O P L E B E C A U S E
I T I S S O M U C H F U N I T I A M O R E
F U N T H A N K I L L I N G W I L D G A M E I N
T H E F O R R E S T B E C A U S E M A N I S T H E
M O A T D A N G E R T U E A N A M A L O F A L L ...
Figure 1: Samples from the ciphertext sequence, corresponding English plaintext message and output from Bayesian
decipherment (using word+3-gram LM) for three different ciphers: (a) Simple Substitution Cipher (top), (b) Homo-
phonic Substitution Cipher with spaces (middle), and (c) Zodiac-408 Cipher (bottom).
243
6. Substitute pi with cipher token ci, with proba-
bility
? ? P0(ci|pi) + C
i?1
1 (pi, ci)
? + Ci?11 (pi)
7. With probability Pquit, quit; else go to Step 4.
This defines the probability of any given deriva-
tion, i.e., any plaintext hypothesis corresponding to
the given ciphertext sequence. The base distribu-
tion P0 represents prior knowledge about the model
parameter distributions. For the plaintext source
model, we use probabilities from an English lan-
guage model and for the channel model, we spec-
ify a uniform distribution (i.e., a plaintext letter can
be substituted with any given cipher type with equal
probability). Ci?11 represents the count of events
occurring before plaintext letter pi in the derivation
(we call this the ?cache?). ? and ? represent Dirich-
let prior hyperparameters over the source and chan-
nel models respectively. A large prior value implies
that characters are generated from the base distribu-
tion P0, whereas a smaller value biases characters
to be generated with reference to previous decisions
inside the cache (favoring sparser distributions).
Efficient inference via type sampling: We use a
Gibbs sampling (Geman and Geman, 1984) method
for performing inference on our model. We could
follow a point-wise sampling strategy, where we
sample plaintext letter choices for every cipher to-
ken, one at a time. But we already know that the
substitution ciphers described here exhibit determin-
ism in the deciphering direction,1 i.e., although we
have no idea about the key mappings themselves,
we do know that there exists only a single plaintext
letter mapping for every cipher symbol type in the
true key. So sampling plaintext choices for every
cipher token separately is not an efficient strategy?
our sampler may spend too much time exploring in-
valid keys (which map the same cipher symbol to
different plaintext letters).
Instead, we use a type sampling technique similar
to the one proposed by Liang et al (2010). Under
1This assumption does not strictly apply to the Zodiac-408
cipher where a few cipher symbols exhibit non-determinism in
the decipherment direction as well.
this scheme, we sample plaintext letter choices for
each cipher symbol type. In every step, we sample
a new plaintext letter for a cipher type and update
the entire plaintext hypothesis (i.e., plaintext letters
at all corresponding positions) to reflect this change.
For example, if we sample a new choice pnew for
a cipher symbol which occurs at positions 4, 10, 18,
then we update plaintext letters p4, p10 and p18 with
the new choice pnew.
Using the property of exchangeability, we derive
an incremental formula for re-scoring the probabil-
ity of a new derivation based on the probability of
the old derivation?when sampling at position i, we
pretend that the area affected (within a context win-
dow around i) in the current plaintext hypothesis oc-
curs at the end of the corpus, so that both the old
and new derivations share the same cache.2 While
we may make corpus-wide changes to a derivation
in every sampling step, exchangeability allows us to
perform scoring in an efficient manner.
Combining letter n-gram language models with
word dictionaries: Many existing probabilistic ap-
proaches use statistical letter n-gram language mod-
els of English to assign P (p) probabilities to plain-
text hypotheses during decipherment. Other de-
cryption techniques rely on word dictionaries (using
words from an English dictionary) for attacking sub-
stitution ciphers.
Unlike previous approaches, our decipherment
method combines information from both sources?
letter n-grams and word dictionaries. We build an
interpolated word+n-gram LM and use it to assign
P (p) probabilities to any plaintext letter sequence
p1...pn.3 The advantage is that it helps direct the
sampler towards plaintext hypotheses that resemble
natural language?high probability letter sequences
which form valid words such as ?H E L L O? in-
stead of sequences like ??T X H R T?. But in ad-
dition to this, using letter n-gram information makes
2The relevant context window that is affected when sam-
pling at position i is determined by the word boundaries to the
left and right of i.
3We set the interpolation weights for the word and n-gram
LM as (0.9, 0.1). The word-based LM is constructed from a
dictionary consisting of 9,881 frequently occurring words col-
lected from Wikipedia articles. We train the letter n-gram LM
on 50 million words of English text available from the Linguis-
tic Data Consortium.
244
our model robust against variations in the origi-
nal plaintext (for example, unseen words or mis-
spellings as in the case of Zodiac-408 cipher) which
can easily throw off dictionary-based attacks. Also,
it is hard for a point-wise (or type) sampler to ?find
words? starting from a random initial sample, but
easier to ?find n-grams?.
Sampling for ciphers without spaces: For ciphers
without spaces, dictionaries are hard to use because
we do not know where words start and end. We in-
troduce a new sampling operator which counters this
problem and allows us to perform inference using
the same decipherment model described earlier. In
a first sampling pass, we sample from 26 plaintext
letter choices (e.g., ?A?, ?B?, ?C?, ...) for every ci-
pher symbol type as before. We then run a second
pass using a new sampling operator that iterates over
adjacent plaintext letter pairs pi?1, pi in the current
hypothesis and samples from two choices?(1) add
a word boundary (space character ? ?) between pi?1
and pi, or (2) remove an existing space character be-
tween pi?1 and pi.
For example, given the English plaintext hypoth-
esis ?... A B O Y ...?, there are two sam-
pling choices for the letter pair A,B in the second
step. If we decide to add a word boundary, our new
plaintext hypothesis becomes ?... A B O Y
...?.
We compute the derivation probability of the new
sample using the same efficient scoring procedure
described earlier. The new strategy allows us to ap-
ply Bayesian decipherment even to ciphers without
spaces. As a result, we now have a new decipher-
ment method that consistently works for a range of
different types of substitution ciphers.
Decoding the ciphertext: After the sampling run
has finished,4 we choose the final sample as our En-
glish plaintext decipherment output.
4For letter substitution decipherment we want to keep the
language model probabilities fixed during training, and hence
we set the prior on that model to be high (? = 104). We use
a sparse prior for the channel (? = 0.01). We instantiate a key
which matches frequently occurring plaintext letters to frequent
cipher symbols and use this to generate an initial sample for the
given ciphertext and run the sampler for 5000 iterations. We
use a linear annealing schedule during sampling decreasing the
temperature from 10? 1.
4 Experiments and Results
We run decipherment experiments on different types
of letter substitution ciphers (described in Sec-
tion 2). In particular, we work with the following
three ciphers:
(a) 414-letter Simple Substitution Cipher
(b) 414-letter Homophonic Cipher (with spaces)
(c) Zodiac-408 Cipher
Methods: For each cipher, we run and compare the
output from two different decipherment approaches:
1. EM Method using letter n-gram LMs follow-
ing the approach of Knight et al (2006). They
use the EM algorithm to estimate the chan-
nel parameters ? during decipherment training.
The given ciphertext c is then decoded by us-
ing the Viterbi algorithm to choose the plain-
text decoding p that maximizes P (p)?P?(c|p)3,
stretching the channel probabilities.
2. Bayesian Decipherment method using
word+n-gram LMs (novel approach described
in Section 3.1).
Evaluation: We evaluate the quality of a particular
decipherment as the percentage of cipher tokens that
are decoded correctly.
Results: Figure 2 compares the decipherment per-
formance for the EM method with Bayesian deci-
pherment (using type sampling and sparse priors)
on three different types of substitution ciphers. Re-
sults show that our new approach (Bayesian) out-
performs the EM method on all three ciphers, solv-
ing them completely. Even with a 3-gram letter LM,
our method yields a +63% improvement in decipher-
ment accuracy over EM on the homophonic cipher
with spaces. We observe that the word+3-gram LM
proves highly effective when tackling more complex
ciphers and cracks the Zodiac-408 cipher. Figure 1
shows samples from the Bayesian decipherment out-
put for all three ciphers. For ciphers without spaces,
our method automatically guesses the word bound-
aries for the plaintext hypothesis.
245
Method LM Accuracy (%) on 414-letter
Simple Substitution Cipher
Accuracy (%) on 414-letter
Homophonic Substitution
Cipher (with spaces)
Accuracy (%) on Zodiac-
408 Cipher
1. EM 2-gram 83.6 30.9
3-gram 99.3 32.6 0.3?
(?28.8 with 100 restarts)
2. Bayesian 3-gram 100.0 95.2 23.0
word+2-gram 100.0 100.0
word+3-gram 100.0 100.0 97.8
Figure 2: Comparison of decipherment accuracies for EM versus Bayesian method when using different language
models of English on the three substitution ciphers: (a) 414-letter Simple Substitution Cipher, (b) 414-letter Homo-
phonic Substitution Cipher (with spaces), and (c) the famous Zodiac-408 Cipher.
For the Zodiac-408 cipher, we compare the per-
formance achieved by Bayesian decipherment under
different settings:
? Letter n-gram versus Word+n-gram LMs?
Figure 2 shows that using a word+3-gram LM
instead of a 3-gram LM results in +75% im-
provement in decipherment accuracy.
? Sparse versus Non-sparse priors?We find that
using a sparse prior for the channel model (? =
0.01 versus 1.0) helps for such problems and
produces better decipherment results (97.8%
versus 24.0% accuracy).
? Type versus Point-wise sampling?Unlike
point-wise sampling, type sampling quickly
converges to better decipherment solutions.
After 5000 sampling passes over the entire
data, decipherment output from type sampling
scores 97.8% accuracy compared to 14.5% for
the point-wise sampling run.5
We also perform experiments on shorter substitu-
tion ciphers. On a 98-letter simple substitution ci-
pher, EM using 3-gram LM achieves 41% accuracy,
whereas the method from Ravi and Knight (2009)
scores 84% accuracy. Our Bayesian method per-
forms the best in this case, achieving 100% with
word+3-gram LM.
5 Conclusion
In this work, we presented a novel Bayesian deci-
pherment approach that can effectively solve a va-
5Both sampling runs were seeded with the same random ini-
tial sample.
riety of substitution ciphers. Unlike previous ap-
proaches, our method combines information from
letter n-gram language models and word dictionar-
ies and provides a robust decipherment model. We
empirically evaluated the method on different substi-
tution ciphers and achieve perfect decipherments on
all of them. Using Bayesian decipherment, we can
successfully solve the Zodiac-408 cipher?the first
time this is achieved by a fully automatic method in
a strict decipherment scenario.
For future work, there are other interesting deci-
pherment tasks where our method can be applied.
One challenge is to crack the unsolved Zodiac-340
cipher, which presents a much harder problem than
the solved version.
Acknowledgements
The authors would like to thank the reviewers for
their comments. This research was supported by
NSF grant IIS-0904684.
References
Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Os-
borne. 2009. A Gibbs sampler for phrasal syn-
chronous grammar induction. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the Asian Federa-
tion of Natural Language Processing (ACL-IJCNLP),
pages 782?790.
David Chiang, Jonathan Graehl, Kevin Knight, Adam
Pauls, and Sujith Ravi. 2010. Bayesian inference for
finite-state transducers. In Proceedings of the Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics - Human Language
Technologies (NAACL/HLT), pages 447?455.
246
Eric Corlett and Gerald Penn. 2010. An exact A* method
for deciphering letter-substitution ciphers. In Proceed-
ings of the 48th Annual Meeting of the Association for
Computational Linguistics, pages 1040?1047.
Arthur P. Dempster, Nan M. Laird, and Donald B. Ru-
bin. 1977. Maximum likelihood from incomplete data
via the EM algorithm. Journal of the Royal Statistical
Society, Series B, 39(1):1?38.
Persi Diaconis. 2008. The Markov Chain Monte Carlo
revolution. Bulletin of the American Mathematical So-
ciety, 46(2):179?205.
Jenny Finkel, Trond Grenager, and Christopher Manning.
2005. Incorporating non-local information into infor-
mation extraction systems by Gibbs sampling. In Pro-
ceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL), pages 363?
370.
Ravi Ganesan and Alan T. Sherman. 1993. Statistical
techniques for language recognition: An introduction
and guide for cryptanalysts. Cryptologia, 17(4):321?
366.
Stuart Geman and Donald Geman. 1984. Stochastic re-
laxation, Gibbs distributions and the Bayesian restora-
tion of images. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 6(6):721?741.
Sharon Goldwater and Thomas Griffiths. 2007. A fully
Bayesian approach to unsupervised part-of-speech tag-
ging. In Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics, pages 744?
751.
Thomas Jakobsen. 1995. A fast method for cryptanalysis
of substitution ciphers. Cryptologia, 19(3):265?274.
Kevin Knight, Anish Nair, Nishit Rathod, and Kenji Ya-
mada. 2006. Unsupervised analysis for decipherment
problems. In Proceedings of the Joint Conference of
the International Committee on Computational Lin-
guistics and the Association for Computational Lin-
guistics, pages 499?506.
Percy Liang, Michael I. Jordan, and Dan Klein. 2010.
Type-based MCMC. In Proceedings of the Conference
on Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 573?
581.
Edwin Olson. 2007. Robust dictionary attack of short
simple substitution ciphers. Cryptologia, 31(4):332?
342.
David Oranchak. 2008. Evolutionary algorithm for de-
cryption of monoalphabetic homophonic substitution
ciphers encoded as constraint satisfaction problems. In
Proceedings of the 10th Annual Conference on Genetic
and Evolutionary Computation, pages 1717?1718.
Shmuel Peleg and Azriel Rosenfeld. 1979. Break-
ing substitution ciphers using a relaxation algorithm.
Comm. ACM, 22(11):598?605.
Sujith Ravi and Kevin Knight. 2008. Attacking deci-
pherment problems optimally with low-order n-gram
models. In Proceedings of the Empirical Methods in
Natural Language Processing (EMNLP), pages 812?
819.
Sujith Ravi and Kevin Knight. 2009. Probabilistic meth-
ods for a Japanese syllable cipher. In Proceedings
of the International Conference on the Computer Pro-
cessing of Oriental Languages (ICCPOL), pages 270?
281.
Benjamin Snyder, Regina Barzilay, and Kevin Knight.
2010. A statistical model for lost language decipher-
ment. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics, pages
1048?1057.
247
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 362?371,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Scalable Decipherment for Machine Translation via Hash Sampling
Sujith Ravi
Google
Mountain View, CA 94043
sravi@gooogle.com
Abstract
In this paper, we propose a new Bayesian
inference method to train statistical ma-
chine translation systems using only non-
parallel corpora. Following a probabilis-
tic decipherment approach, we first intro-
duce a new framework for decipherment
training that is flexible enough to incorpo-
rate any number/type of features (besides
simple bag-of-words) as side-information
used for estimating translation models. In
order to perform fast, efficient Bayesian
inference in this framework, we then de-
rive a hash sampling strategy that is in-
spired by the work of Ahmed et al (2012).
The new translation hash sampler enables
us to scale elegantly to complex mod-
els (for the first time) and large vocab-
ulary/corpora sizes. We show empirical
results on the OPUS data?our method
yields the best BLEU scores compared to
existing approaches, while achieving sig-
nificant computational speedups (several
orders faster). We also report for the
first time?BLEU score results for a large-
scale MT task using only non-parallel data
(EMEA corpus).
1 Introduction
Statistical machine translation (SMT) systems
these days are built using large amounts of bilin-
gual parallel corpora. The parallel corpora are
used to estimate translation model parameters in-
volving word-to-word translation tables, fertilities,
distortion, phrase translations, syntactic transfor-
mations, etc. But obtaining parallel data is an ex-
pensive process and not available for all language
pairs or domains. On the other hand, monolin-
gual data (in written form) exists and is easier to
obtain for many languages. Learning translation
models from monolingual corpora could help ad-
dress the challenges faced by modern-day MT sys-
tems, especially for low resource language pairs.
Recently, this topic has been receiving increasing
attention from researchers and new methods have
been proposed to train statistical machine trans-
lation models using only monolingual data in the
source and target language. The underlying moti-
vation behind most of these methods is that statis-
tical properties for linguistic elements are shared
across different languages and some of these sim-
ilarities (mappings) could be automatically identi-
fied from large amounts of monolingual data.
The MT literature does cover some prior work
on extracting or augmenting partial lexicons using
non-parallel corpora (Rapp, 1995; Fung and McK-
eown, 1997; Koehn and Knight, 2000; Haghighi
et al, 2008). However, none of these meth-
ods attempt to train end-to-end MT models, in-
stead they focus on mining bilingual lexicons from
monolingual corpora and often they require par-
allel seed lexicons as a starting point. Some of
them (Haghighi et al, 2008) also rely on addi-
tional linguistic knowledge such as orthography,
etc. to mine word translation pairs across related
languages (e.g., Spanish/English). Unsupervised
training methods have also been proposed in the
past for related problems in decipherment (Knight
and Yamada, 1999; Snyder et al, 2010; Ravi and
Knight, 2011a) where the goal is to decode un-
known scripts or ciphers.
The body of work that is more closely related to
ours include that of Ravi and Knight (2011b) who
introduced a decipherment approach for training
translation models using only monolingual cor-
362
pora. Their best performing method uses an EM
algorithm to train a word translation model and
they show results on a Spanish/English task. Nuhn
et al (2012) extend the former approach and im-
prove training efficiency by pruning translation
candidates prior to EM training with the help of
context similarities computed from monolingual
corpora.
In this work we propose a new Bayesian in-
ference method for estimating translation mod-
els from scratch using only monolingual corpora.
Secondly, we introduce a new feature-based repre-
sentation for sampling translation candidates that
allows one to incorporate any amount of additional
features (beyond simple bag-of-words) as side-
information during decipherment training. Fi-
nally, we also derive a new accelerated sampling
mechanism using locality sensitive hashing in-
spired by recent work on fast, probabilistic infer-
ence for unsupervised clustering (Ahmed et al,
2012). The new sampler allows us to perform fast,
efficient inference with more complex translation
models (than previously used) and scale better to
large vocabulary and corpora sizes compared to
existing methods as evidenced by our experimen-
tal results on two different corpora.
2 Decipherment Model for Machine
Translation
We now describe the decipherment problem for-
mulation for machine translation.
Problem Formulation: Given a source text f
(i.e., source word sequences f1...fm) and a mono-
lingual target language corpus, our goal is to deci-
pher the source text and produce a target transla-
tion.
Contrary to standard machine translation train-
ing scenarios, here we have to estimate the transla-
tion model P?(f |e) parameters using only mono-
lingual data. During decipherment training, our
objective is to estimate the model parameters in or-
der to maximize the probability of the source text
f as suggested by Ravi and Knight (2011b).
argmax
?
?
f
?
e
P (e) ? P?(f |e) (1)
For P (e), we use a word n-gram language
model (LM) trained on monolingual target text.
We then estimate the parameters of the translation
model P?(f |e) during training.
Translation Model: Machine translation is a
much more complex task than solving other de-
cipherment tasks such as word substitution ci-
phers (Ravi and Knight, 2011b; Dou and Knight,
2012). The mappings between languages involve
non-determinism (i.e., words can have multiple
translations), re-ordering of words can occur as
grammar and syntax varies with language, and
in addition word insertion and deletion operations
are also involved.
Ideally, for the translation model P (f |e) we
would like to use well-known statistical models
such as IBM Model 3 and estimate its parame-
ters ? using the EM algorithm (Dempster et al,
1977). But training becomes intractable with com-
plex translation models and scalability is also an
issue when large corpora sizes are involved and the
translation tables become huge to fit in memory.
So, instead we use a simplified generative process
for the translation model as proposed by Ravi and
Knight (2011b) and used by others (Nuhn et al,
2012) for this task:
1. Generate a target (e.g., English) string e =
e1...el, with probability P (e) according to an
n-gram language model.
2. Insert a NULL word at any position in the
English string, with uniform probability.
3. For each target word token ei (including
NULLs), choose a source word translation fi,
with probability P?(fi|ei). The source word
may be NULL.
4. Swap any pair of adjacent source words
fi?1, fi, with probability P (swap); set to
0.1.
5. Output the foreign string f = f1...fm, skip-
ping over NULLs.
Previous approaches (Ravi and Knight, 2011b;
Nuhn et al, 2012) use the EM algorithm to es-
timate all the parameters ? in order to maximize
likelihood of the foreign corpus. Instead, we pro-
pose a new Bayesian inference framework to esti-
mate the translation model parameters. In spite of
using Bayesian inference which is typically slow
in practice (with standard Gibbs sampling), we
show later that our method is scalable and permits
decipherment training using more complex trans-
lation models (with several additional parameters).
363
2.1 Adding Phrases, Flexible Reordering and
Fertility to Translation Model
We now extend the generative process (described
earlier) to more complex translation models.
Non-local Re-ordering: The generative process
described earlier limits re-ordering to local or ad-
jacent word pairs in a source sentence. We ex-
tend this to allow re-ordering between any pair of
words in the sentence.
Fertility: We also add a fertility model P?fert to
the translation model using the formula:
P?fert =
?
i
n?(?i|ei) ? p?01 (2)
n?(?i|ei) =
?fert ? P0(?i|ei) + C?i(ei, ?i)
?fert + C?i(ei)
(3)
where, P0 represents the base distribution
(which is set to uniform) in a Chinese Restau-
rant Process (CRP)1 for the fertility model and
C?i represents the count of events occurring in
the history excluding the observation at position i.
?i is the number of source words aligned to (i.e.,
generated by) the target word ei. We use sparse
Dirichlet priors for all the translation model com-
ponents.2 ?0 represents the target NULL word fer-
tility and p1 is the insertion probability which is
fixed to 0.1. In addition, we set a maximum thresh-
old for fertility values ?i ? ? ?m, where m is the
length of the source sentence. This discourages
a particular target word (e.g., NULL word) from
generating too many source words in the same sen-
tence. In our experiments, we set ? = 0.3. We en-
force this constraint in the training process during
sampling.3
Modeling Phrases: Finally, we extend the trans-
lation candidate set in P?(fi|ei) to model phrases
in addition to words for the target side (i.e., ei can
now be a word or a phrase4 previously seen in the
monolingual target corpus). This greatly increases
the training time since in each sampling step, we
now have many more ei candidates to choose
from. In Section 4, we describe how we deal
1Each component in the translation model (word/phrase
translations P?(fi|ei), fertility P?fert , etc.) is modeled usinga CRP formulation.
2i.e., All the concentration parameters are set to low val-
ues; ?f |e = ?fert = 0.01.
3We only apply this constraint when training on source
text/corpora made of long sentences (>10 words) where the
sampler might converge very slowly. For short sentences, a
sparse prior on fertility ?fert typically discourages a target
word from being aligned to too many different source words.
4Phrase size is limited to two words in our experiments.
with this problem by using a fast, efficient sam-
pler based on hashing that allows us to speed up
the Bayesian inference significantly whereas stan-
dard Gibbs sampling would be extremely slow.
3 Feature-based representation for
Source and Target
The model described in the previous section while
being flexible in describing the translation pro-
cess, poses several challenges for training. As
the source and target vocabulary sizes increase the
size of the translation table (|Vf | ? |Ve|) increases
significantly and often becomes too huge to fit in
memory. Additionally, performing Bayesian in-
ference with such a complex model using stan-
dard Gibbs sampling can be very slow in prac-
tice. Here, we describe a new method for doing
Bayesian inference by first introducing a feature-
based representation for the source and target
words (or phrases) from which we then derive a
novel proposal distribution for sampling transla-
tion candidates.
We represent both source and target words in
a vector space similar to how documents are rep-
resented in typical information retrieval settings.
But unlike documents, here each word w is as-
sociated with a feature vector w1...wd (where wi
represents the weight for the feature indexed by i)
which is constructed from monolingual corpora.
For instance, context features for word w may in-
clude other words (or phrases) that appear in the
immediate context (n-gram window) surrounding
w in the monolingual corpus. Similarly, we can
add other features based on topic models, orthog-
raphy (Haghighi et al, 2008), temporal (Klemen-
tiev et al, 2012), etc. to our representation all of
which can be extracted from monolingual corpora.
Next, given two high dimensional vectors u and
v it is possible to calculate the similarity between
the two words denoted by s(u,v). The feature
construction process is described in more detail
below:
Target Language: We represent each word (or
phrase) ei with the following contextual features
along with their counts: (a) f?context: every (word
n-gram, position) pair immediately preceding ei
in the monolingual corpus (n=1, position=?1), (b)
similar features f+context to model the context fol-
lowing ei, and (c) we also throw in generic context
features fscontext without position information?
every word that co-occurs with ei in the same sen-
364
tence. While the two position-features provide
specific context information (may be sparse for
large monolingual corpora), this feature is more
generic and captures long-distance co-occurrence
statistics.
Source Language: Words appearing in a source
sentence f are represented using the correspond-
ing target translation e = e1...em generated for
f in the current sample during training. For each
source word fj ? f , we look at the corresponding
word ej in the target translation. We then extract
all the context features of ej in the target trans-
lation sample sentence e and add these features
(f?context, f+context, fscontext) with weights to the
feature representation for fj .
Unlike the target word feature vectors (which
can be pre-computed from the monolingual tar-
get corpus), the feature vector for every source
word fj is dynamically constructed from the tar-
get translation sampled in each training iteration.
This is a key distinction of our framework com-
pared to previous approaches that use contextual
similarity (or any other) features constructed from
static monolingual corpora (Rapp, 1995; Koehn
and Knight, 2000; Nuhn et al, 2012).
Note that as we add more and more features for
a particular word (by training on larger monolin-
gual corpora or adding new types of features, etc.),
it results in the feature representation becoming
more sparse (especially for source feature vectors)
which can cause problems in efficiency as well
as robustness when computing similarity against
other vectors. In the next section, we will describe
how we mitigate this problem by projecting into a
low-dimensional space by computing hash signa-
tures.
In all our experiments, we only use the features
described above for representing source and tar-
get words. We note that the new sampling frame-
work is easily extensible to many additional fea-
ture types (for example, monolingual topic model
features, etc.) which can be efficiently handled by
our inference algorithm and could further improve
translation performance but we leave this for fu-
ture work.
4 Bayesian MT Decipherment via Hash
Sampling
The next step is to use the feature representations
described earlier and iteratively sample a target
word (or phrase) translation candidate ei for every
word fi in the source text f . This involves choos-
ing from |Ve| possible target candidates in every
step which can be highly inefficient (and infeasi-
ble for large vocabulary sizes). One possible strat-
egy is to compute similarity scores s(wfi ,we?) be-
tween the current source word feature vector wfi
and feature vectors we??Ve for all possible candi-
dates in the target vocabulary. Following this, we
can prune the translation candidate set by keeping
only the top candidates e? according to the sim-
ilarity scores. Nuhn et al (2012) use a similar
strategy to obtain a more compact translation table
that improves runtime efficiency for EM training.
Their approach requires calculating and sorting all
|Ve| ? |Vf | distances in timeO(V 2 ? log(V )), where
V = max(|Ve|, |Vf |).
Challenges: Unfortunately, there are several ad-
ditional challenges which makes inference very
hard in our case. Firstly, we would like to in-
clude as many features as possible to represent
the source/target words in our framework besides
simple bag-of-words context similarity (for exam-
ple, left-context, right-context, and other general-
purpose features based on topic models, etc.). This
makes the complexity far worse (in practice) since
the dimensionality of the feature vectors d is a
much higher value than |Ve|. Computing similar-
ity scores alone (na??vely) would incur O(|Ve| ? d)
time which is prohibitively huge since we have to
do this for every token in the source language cor-
pus. Secondly, for Bayesian inference we need to
sample from a distribution that involves comput-
ing probabilities for all the components (language
model, translation model, fertility, etc.) described
in Equation 1. This distribution needs to be com-
puted for every source word token fi in the corpus,
for all possible candidates ei ? Ve and the process
has to be repeated for multiple sampling iterations
(typically more than 1000). Doing standard col-
lapsed Gibbs sampling in this scenario would be
very slow and intractable.
We now present an alternative fast, efficient
inference strategy that overcomes many of the
challenges described above and helps acceler-
ate the sampling process significantly. First,
we set our translation models within the con-
text of a more generic and widely known fam-
ily of distributions?mixtures of exponential fam-
ilies. Then we derive a novel proposal distribu-
tion for sampling translation candidates and intro-
duce a new sampler for decipherment training that
365
is based on locality sensitive hashing (LSH).
Hashing methods such as LSH have been
widely used in the past in several scenarios in-
cluding NLP applications (Ravichandran et al,
2005). Most of these approaches employ LSH
within heuristic methods for speeding up nearest-
neighbor look up and similarity computation tech-
niques. However, we use LSH hashing within
a probabilistic framework which is very different
from the typical use of LSH.
Our work is inspired by some recent work by
Ahmed et al (2012) on speeding up Bayesian in-
ference for unsupervised clustering. We use a sim-
ilar technique as theirs but a different approximate
distribution for the proposal, one that is better-
suited for machine translation models and without
some of the additional overhead required for com-
puting certain terms in the original formulation.
Mixtures of Exponential Families: The transla-
tion models described earlier (Section 2) can be
represented as mixtures of exponential families,
specifically mixtures of multinomials. In exponen-
tial families, distributions over random variables
are given by:
p(x; ?) = exp(??(x), ??)? g(?) (4)
where, ? : X ? F is a map from x to the space
of sufficient statistics and ? ? F . The term g(?)
ensures that p(x; ?) is properly normalized. X is
the domain of observations X = x1, ..., xm drawn
from some distribution p. Our goal is to estimate
p. In our case, this refers to the translation model
from Equation 1.
We also choose corresponding conjugate
Dirichlet distributions for priors which have the
property that the posterior distribution p(?|X)
over ? remains in the same family as p(?).
Note that the (translation) model in our
case consists of multiple exponential families
components?a multinomial pertaining to the lan-
guage model (which remains fixed5), and other
components pertaining to translation probabilities
P?(fi|ei), fertility P?fert , etc. To do collapsed
Gibbs sampling under this model, we would per-
form the following steps during sampling:
1. For a given source word token fi draw target
5A high value for the LM concentration parameter ? en-
sures that the LM probabilities do not deviate too far from the
original fixed base distribution during sampling.
translation
ei ? p(ei|F,E?i)
? p(e) ? p(fi|ei, F?i, E?i)
? pfert(?|ei, F?i, E?i) ? ... (5)
where, F is the full source text and E the full
target translation generated during sampling.
2. Update the sufficient statistics for the changed
target translation assignments.
For large target vocabularies, computing
p(fi|ei, F?i, E?i) dominates the inference pro-
cedure. We can accelerate this step significantly
using a good proposal distribution via hashing.
Locality Sensitive Hash Sampling: For general
exponential families, here is a Taylor approxima-
tion for the data likelihood term (Ahmed et al,
2012):
p(x|?) ? exp(??(x), ???)? g(??) (6)
where, ?? is the expected parameter (sufficient
statistics).
For sampling the translation model, this involves
computing an expensive inner product ??(fi), ??e??
for each source word fi which has to be repeated
for every translation candidate e?, including candi-
dates that have very low probabilities and are un-
likely to be chosen as the translation for fj .
So, during decipherment training a standard
collapsed Gibbs sampler will waste most of its
time on expensive computations that will be dis-
carded in the end anyways. Also, unlike some
standard generative models used in other unsu-
pervised learning scenarios (e.g., clustering) that
model only observed features (namely words ap-
pearing in the document), here we would like to
enrich the translation model with a lot more fea-
tures (side-information).
Instead, we can accelerate the computation of
the inner product ??(fi), ??e?? using a hash sam-
pling strategy similar to (Ahmed et al, 2012).
The underlying idea here is to use binary hash-
ing (Charikar, 2002) to explore only those can-
didates e? that are sufficiently close to the best
matching translation via a proposal distribution.
Next, we briefly introduce some notations and ex-
isting theoretical results related to binary hashing
before describing the hash sampling procedure.
For any two vectors u, v ? Rn,
?u, v? = ?u? ? ?v? ? cos](u, v) (7)
366
](u, v) = piPr{sgn[?u,w?] 6= sgn[?v, w?]}
(8)
where, w is a random vector drawn from a sym-
metric spherical distribution and the term inside
Pr{?} represents the relation between the signs of
the two inner products.
Let hl(v) ? {0, 1}l be an l-bit binary hash of v
where: [hl(v)]i := sgn[?v, wi?]; wi ? Um. Then
the probability of matching signs is given by:
zl(u, v) := 1l ?h(u)? h(v)?1 (9)
So, zl(u, v) measures how many bits differ be-
tween the hash vectors h(u) and h(v) associated
with u, v. Combining this with Equations 6 and 7
we can estimate the unnormalized log-likelihood
of a source word fi being translated as target e?
via:
sl(fi, e?) ? ??e?? ? ??(fi)? ? cospizl(?(fi), ?e?)
(10)
For each source word fi, we now sample from
this new distribution (after normalization) instead
of the original one. The binary hash representa-
tion for the two vectors yield significant speedups
during sampling since Hamming distance compu-
tation between h(u) and h(v) is highly optimized
on modern CPUs. Hence, we can compute an es-
timate for the inner product quite efficiently.6
Updating the hash signatures: During training,
we compute the target candidate projection h(?e?)
and corresponding norm only once7 which is dif-
ferent from the setup of Ahmed et al (2012). The
source word projection ?(fi) is dynamically up-
dated in every sampling step. Note that doing this
na??vely would scale slowly as O(Dl) where D is
the total number of features but instead we can up-
date the hash signatures in a more efficient manner
that scales as O(Di>0 l) where Di>0 is the number
of non-zero entries in the feature representation for
the source word ?(fi). Also, we do not need to
store the random vectors w in practice since these
can be computed on the fly using hash functions.
The inner product approximation also yields some
theoretical guarantees for the hash sampler.8
6We set l = 32 bits in our experiments.
7In practice, we can ignore the norm terms to further
speed up sampling since this is only an estimate for the pro-
posal distribution and we follow this with the Metropolis
Hastings step.
8For further details, please refer to (Ahmed et al, 2012).
4.1 Metropolis Hastings
In each sampling step, we use the distribution
from Equation 10 as a proposal distribution in
a Metropolis Hastings scheme to sample target
translations for each source word.
Once a new target translation e? is sampled
for source word fi from the proposal distribution
q(?) ? expsl(fi,e?), we accept the proposal (and
update the corresponding hash signatures) accord-
ing to the probability r
r = q(e
old
i ) ? pnew(?)
q(enewi ) ? pold(?)
(11)
where, pold(?), pnew(?) are the true conditional
likelihood probabilities according to our model
(including the language model component) for the
old, new sample respectively.
5 Training Algorithm
Putting together all the pieces described in the pre-
vious section, we perform the following steps:
1. Initialization: We initialize the starting sample
as follows: for each source word token, randomly
sample a target word. If the source word also ex-
ists in the target vocabulary, then choose identity
translation instead of the random one.9
2. Hash Sampling Steps: For each source word
token fi, run the hash sampler:
(a) Generate a proposal distribution by comput-
ing the hamming distance between the feature vec-
tors for the source word and each target translation
candidate. Sample a new target translation ei for
fi from this distribution.
(b) Compute the acceptance probability for the
chosen translation using a Metropolis Hastings
scheme and accept (or reject) the sample. In prac-
tice, computation of the acceptance probability
only needs to be done every r iterations (where
r can be anywhere from 5 or 100).
Iterate through steps (2a) and (2b) for every word
in the source text and then repeat this process for
multiple iterations (usually 1000).
3. Other Sampling Operators: After every k it-
erations,10 perform the following sampling opera-
tions:
(a) Re-ordering: For each source word token fi
at position i, randomly choose another position j
9Initializing with identity translation rather than random
choice helps in some cases, especially for unknown words
that involve named entities, etc.
10We set k = 3 in our experiments.
367
Corpus Language Sent. Words Vocab.
OPUS Spanish 13,181 39,185 562
English 19,770 61,835 411
EMEA French 550,000 8,566,321 41,733
Spanish 550,000 7,245,672 67,446
Table 1: Statistics of non-parallel corpora used
here.
in the source sentence and swap the translations ei
with ej . During the sampling process, we compute
the probabilities for the two samples?the origi-
nal and the swapped versions, and then sample an
alignment from this distribution.
(b) Deletion: For each source word token,
delete the current target translation (i.e., align it
with the target NULL token). As with the re-
ordering operation, we sample from a distribution
consisting of the original and the deleted versions.
4. Decoding the foreign sentence: Finally, once
the training is done (i.e., after all sampling iter-
ations) we choose the final sample as our target
translation output for the source text.
6 Experiments and Results
We test our method on two different corpora.
To evaluate translation quality, we use BLEU
score (Papineni et al, 2002), a standard evaluation
measure used in machine translation.
First, we present MT results on non-parallel
Spanish/English data from the OPUS cor-
pus (Tiedemann, 2009) which was used by Ravi
and Knight (2011b) and Nuhn et al (2012).
We show that our method achieves the best
performance (BLEU scores) on this task while
being significantly faster than both the previous
approaches. We then apply our method to a
much larger non-parallel French/Spanish corpus
constructed from the EMEA corpus (Tiedemann,
2009). Here the vocabulary sizes are much larger
and we show how our new Bayesian decipherment
method scales well to this task inspite of using
complex translation models. We also report the
first BLEU results on such a large-scale MT task
under truly non-parallel settings (without using
any parallel data or seed lexicon).
For both the MT tasks, we also report BLEU
scores for a baseline system using identity trans-
lations for common words (words appearing in
both source/target vocabularies) and random trans-
lations for other words.
6.1 MT Task and Data
OPUS movie subtitle corpus (Tiedemann, 2009):
This is a large open source collection of parallel
corpora available for multiple language pairs. We
use the same non-parallel Spanish/English corpus
used in previous works (Ravi and Knight, 2011b;
Nuhn et al, 2012). The details of the corpus are
listed in Table 1. We use the entire Spanish source
text for decipherment training and evaluate the fi-
nal English output to report BLEU scores.
EMEA corpus (Tiedemann, 2009): This is a par-
allel corpus made out of PDF documents (arti-
cles from the medical domain) from the Euro-
pean Medicines Agency. We reserve the first 1k
sentences in French as our source text (also used
in decipherment training). To construct a non-
parallel corpus, we split the remaining 1.1M lines
as follows: first 550k sentences in French, last
550k sentences in Spanish. The latter is used to
construct a target language model used for deci-
pherment training. The corpus statistics are shown
in Table 1.
6.2 Results
OPUS: We compare the MT results (BLEU
scores) from different systems on the OPUS cor-
pus in Table 2. The first row displays baseline
performance. The next three rows 1a?1c display
performance achieved by two methods from Ravi
and Knight (2011b). Rows 2a, 2b show results
from the of Nuhn et al (2012). The last two rows
display results for the new method using Bayesian
hash sampling. Overall, using a 3-gram language
model (instead of 2-gram) for decipherment train-
ing improves the performance for all methods. We
observe that our method produces much better re-
sults than the others even with a 2-gram LM. With
a 3-gram LM, the new method achieves the best
performance; the highest BLEU score reported on
this task. It is also interesting to note that the hash
sampling method yields much better results than
the Bayesian inference method presented in (Ravi
and Knight, 2011b). This is due to the accelerated
sampling scheme introduced earlier which helps it
converge to better solutions faster.
Table 2 (last column) also compares the effi-
ciency of different methods in terms of CPU time
required for training. Both our 2-gram and 3-gram
based methods are significantly faster than those
previously reported for EM based training meth-
ods presented in (Ravi and Knight, 2011b; Nuhn
368
Method BLEU Time (hours)
Baseline system (identity translations) 6.9
1a. EM with 2-gram LM (Ravi and Knight, 2011b) 15.3 ?850h
1b. EM with whole-segment LM (Ravi and Knight, 2011b) 19.3
1c. Bayesian IBM Model 3 with 2-gram LM (Ravi and Knight, 2011b) 15.1
2a. EM+Context with 2-gram LM (Nuhn et al, 2012) 15.2 50h
2b. EM+Context with 3-gram LM (Nuhn et al, 2012) 20.9 200h
3. Bayesian (standard) Gibbs sampling with 2-gram LM 222h
4a. Bayesian Hash Sampling? with 2-gram LM (this work) 20.3 2.6h
4b. Bayesian Hash Sampling? with 3-gram LM (this work) 21.2 2.7h
(?sampler was run for 1000 iterations)
Table 2: Comparison of MT performance (BLEU scores) and efficiency (running time in CPU hours)
on the Spanish/English OPUS corpus using only non-parallel corpora for training. For the Bayesian
methods 4a and 4b, the samplers were run for 1000 iterations each on a single machine (1.8GHz Intel
processor). For 1a, 2a, 2b, we list the training times as reported by Nuhn et al (2012) based on their EM
implementation for different settings.
Method BLEU
Baseline system (identity translations) 3.0
Bayesian Hash Sampling with 2-gram LM
vocab=full (Ve), add fertility=no 4.2
vocab=pruned?, add fertility=yes 5.3
Table 3: MT results on the French/Spanish EMEA
corpus using the new hash sampling method. ?The
last row displays results when we sample target
translations from a pruned candidate set (most fre-
quent 1k Spanish words + identity translation can-
didates) which enables the sampler to run much
faster when using more complex models.
et al, 2012). This is very encouraging since Nuhn
et al (2012) reported obtaining a speedup by prun-
ing translation candidates (to ?1/8th the original
size) prior to EM training. On the other hand, we
sample from the full set of translation candidates
including additional target phrase (of size 2) can-
didates which results in a much larger vocabulary
consisting of 1600 candidates (?4 times the orig-
inal size), yet our method runs much faster and
yields better results. The table also demonstrates
the siginificant speedup achieved by the hash sam-
pler over a standard Gibbs sampler for the same
model (?85 times faster when using a 2-gram
LM).
We also compare the results against MT per-
formance from parallel training?MOSES sys-
tem (Koehn et al, 2007) trained on 20k sentence
pairs. The comparable number for Table 2 is 63.6
BLEU.
Spanish (e) French (f)
el ? les
la ? la
por ? des
seccio?n ? rubrique
administracio?n ? administration
Table 4: Sample (1-best) Spanish/French transla-
tions produced by the new method on the EMEA
corpus using word translation models trained with
non-parallel corpora.
EMEAResults Table 3 shows the results achieved
by our method on the larger task involving EMEA
corpus. Here, the target vocabulary Ve is much
higher (67k). In spite of this challenge and the
model complexity, we can still perform decipher-
ment training using Bayesian inference. We report
the first BLEU score results on such a large-scale
task using a 2-gram LM. This is achieved without
using any seed lexicon or parallel corpora. The re-
sults are encouraging and demonstrates the ability
of the method to scale to large-scale settings while
performing efficient inference with complex mod-
els, which we believe will be especially useful for
future MT application in scenarios where parallel
data is hard to obtain. Table 4 displays some sam-
ple 1-best translations learned using this method.
For comparison purposes, we also evaluate MT
performance on this task using parallel training
(MOSES trained with hundred sentence pairs) and
observe a BLEU score of 11.7.
369
7 Discussion and Future Work
There exists some work (Dou and Knight, 2012;
Klementiev et al, 2012) that uses monolingual
corpora to induce phrase tables, etc. These when
combined with standard MT systems such as
Moses (Koehn et al, 2007) trained on parallel cor-
pora, have been shown to yield some BLEU score
improvements. Nuhn et al (2012) show some
sample English/French lexicon entries learnt us-
ing EM algorithm with a pruned translation can-
didate set on a portion of the Gigaword corpus11
but do not report any actual MT results. In ad-
dition, as we showed earlier our method can use
Bayesian inference (which has a lot of nice proper-
ties compared to EM for unsupervised natural lan-
guage tasks (Johnson, 2007; Goldwater and Grif-
fiths, 2007)) and still scale easily to large vocabu-
lary, data sizes while allowing the models to grow
in complexity. Most importantly, our method pro-
duces better translation results (as demonstrated
on the OPUS MT task). And to our knowledge,
this is the first time that anyone has reported MT
results under truly non-parallel settings on such a
large-scale task (EMEA).
Our method is also easily extensible to out-
of-domain translation scenarios similar to (Dou
and Knight, 2012). While their work also uses
Bayesian inference with a slice sampling scheme,
our new approach uses a novel hash sampling
scheme for decipherment that can easily scale
to more complex models. The new decipher-
ment framework also allows one to easily incorpo-
rate additional information (besides standard word
translations) as features (e.g., context features,
topic features, etc.) for unsupervised machine
translation which can help further improve the per-
formance in addition to accelerating the sampling
process. We already demonstrated the utility of
this system by going beyond words and incorpo-
rating phrase translations in a decipherment model
for the first time.
In the future, we can obtain further speedups
(especially for large-scale tasks) by parallelizing
the sampling scheme seamlessly across multiple
machines and CPU cores. The new framework can
also be stacked with complementary techniques
such as slice sampling, blocked (and type) sam-
pling to further improve inference efficiency.
11http://www.ldc.upenn.edu/Catalog/catalogEntry.jsp?
catalogId=LDC2003T05
8 Conclusion
To summarize, our method is significantly faster
than previous methods based on EM or Bayesian
with standard Gibbs sampling and obtains better
results than any previously published methods for
the same task. The new framework also allows
performing Bayesian inference for decipherment
applications with more complex models than pre-
viously shown. We believe this framework will
be useful for further extending MT models in the
future to improve translation performance and for
many other unsupervised decipherment applica-
tion scenarios.
References
Amr Ahmed, Sujith Ravi, Shravan Narayanamurthy,
and Alex Smola. 2012. Fastex: Hash clustering
with exponential families. In Proceedings of the
26th Conference on Neural Information Processing
Systems (NIPS).
Moses S. Charikar. 2002. Similarity estimation tech-
niques from rounding algorithms. In Proceedings of
the thiry-fourth annual ACM Symposium on Theory
of Computing, pages 380?388.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the
em algorithm. Journal of the Royal Statistical Soci-
ety, Series B, 39(1):1?38.
Qing Dou and Kevin Knight. 2012. Large scale deci-
pherment for out-of-domain machine translation. In
Proceedings of the 2012 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning, pages
266?275.
Pascale Fung and Kathleen McKeown. 1997. Finding
terminology translations from non-parallel corpora.
In Proceedings of the 5th Annual Workshop on Very
Large Corpora, pages 192?202.
Sharon Goldwater and Tom Griffiths. 2007. A fully
bayesian approach to unsupervised part-of-speech
tagging. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 744?751.
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexicons
from monolingual corpora. In Proceedings of ACL:
HLT, pages 771?779.
Mark Johnson. 2007. Why doesn?t EM find good
HMM POS-taggers? In Proceedings of the Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 296?305.
370
Alex Klementiev, Ann Irvine, Chris Callison-Burch,
and David Yarowsky. 2012. Toward statistical ma-
chine translation without parallel corpora. In Pro-
ceedings of the 13th Conference of the European
Chapter of the Association for Computational Lin-
guistics.
Kevin Knight and Kenji Yamada. 1999. A computa-
tional approach to deciphering unknown scripts. In
Proceedings of the ACL Workshop on Unsupervised
Learning in Natural Language Processing, pages
37?44.
Philipp Koehn and Kevin Knight. 2000. Estimating
word translation probabilities from unrelated mono-
lingual corpora using the em algorithm. In Proceed-
ings of the Seventeenth National Conference on Ar-
tificial Intelligence and Twelfth Conference on Inno-
vative Applications of Artificial Intelligence, pages
711?715.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondr?ej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
pages 177?180.
Malte Nuhn, Arne Mauser, and Hermann Ney. 2012.
Deciphering foreign language by combining lan-
guage models and context vectors. In Proceedings
of the 50th Annual Meeting of the Association for
Computational Linguistics, pages 156?164.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics, pages 311?318.
Reinhard Rapp. 1995. Identifying word translations
in non-parallel texts. In Proceedings of the 33rd An-
nual Meeting on Association for Computational Lin-
guistics, pages 320?322.
Sujith Ravi and Kevin Knight. 2011a. Bayesian in-
ference for zodiac and other homophonic ciphers.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies - Volume 1, pages 239?247.
Sujith Ravi and Kevin Knight. 2011b. Deciphering
foreign language. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
12?21.
Deepak Ravichandran, Patrick Pantel, and Eduard
Hovy. 2005. Randomized algorithms and nlp: us-
ing locality sensitive hash function for high speed
noun clustering. In Proceedings of the 43rd Annual
Meeting on Association for Computational Linguis-
tics, pages 622?629.
Benjamin Snyder, Regina Barzilay, and Kevin Knight.
2010. A statistical model for lost language deci-
pherment. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 1048?1057.
Jo?rg Tiedemann. 2009. News from opus - a collection
of multilingual parallel corpora with tools and inter-
faces. In N. Nicolov, K. Bontcheva, G. Angelova,
and R. Mitkov, editors, Recent Advances in Natural
Language Processing, volume V, pages 237?248.
371
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1014?1022,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Summarization Through Submodularity and Dispersion
Anirban Dasgupta
Yahoo! Labs
Sunnyvale, CA 95054
anirban@yahoo-inc.com
Ravi Kumar
Google
Mountain View, CA 94043
tintin@google.com
Sujith Ravi
Google
Mountain View, CA 94043
sravi@gooogle.com
Abstract
We propose a new optimization frame-
work for summarization by generalizing
the submodular framework of (Lin and
Bilmes, 2011). In our framework the sum-
marization desideratum is expressed as a
sum of a submodular function and a non-
submodular function, which we call dis-
persion; the latter uses inter-sentence dis-
similarities in different ways in order to
ensure non-redundancy of the summary.
We consider three natural dispersion func-
tions and show that a greedy algorithm
can obtain an approximately optimal sum-
mary in all three cases. We conduct ex-
periments on two corpora?DUC 2004
and user comments on news articles?and
show that the performance of our algo-
rithm outperforms those that rely only on
submodularity.
1 Introduction
Summarization is a classic text processing prob-
lem. Broadly speaking, given one or more doc-
uments, the goal is to obtain a concise piece
of text that contains the most salient points in
the given document(s). Thanks to the om-
nipresent information overload facing all of us,
the importance of summarization is gaining; semi-
automatically summarized content is increasingly
becoming user-facing: many newspapers equip
editors with automated tools to aid them in choos-
ing a subset of user comments to show. Summa-
rization has been studied for the past in various
settings?a large single document, multiple docu-
ments on the same topic, and user-generated con-
tent.
Each domain throws up its own set of idiosyn-
crasies and challenges for the summarization task.
On one hand, in the multi-document case (say, dif-
ferent news reports on the same event), the text is
often very long and detailed. The precision/recall
requirements are higher in this domain and a se-
mantic representation of the text might be needed
to avoid redundancy. On the other hand, in the
case of user-generated content (say, comments on
a news article), even though the text is short, one
is faced with a different set of problems: volume
(popular articles generate more than 10,000 com-
ments), noise (most comments are vacuous, lin-
guistically deficient, and tangential to the article),
and redundancy (similar views are expressed by
multiple commenters). In both cases, there is a
delicate balance between choosing the salient, rel-
evant, popular, and diverse points (e.g., sentences)
versus minimizing syntactic and semantic redun-
dancy.
While there have been many approaches to au-
tomatic summarization (see Section 2), our work
is directly inspired by the recent elegant frame-
work of (Lin and Bilmes, 2011). They employed
the powerful theory of submodular functions for
summarization: submodularity embodies the ?di-
minishing returns? property and hence is a natural
vocabulary to express the summarization desider-
ata. In this framework, each of the constraints (rel-
evance, redundancy, etc.) is captured as a submod-
ular function and the objective is to maximize their
sum. A simple greedy algorithm is guaranteed to
produce an approximately optimal summary. They
used this framework to obtain the best results on
the DUC 2004 corpus.
Even though the submodularity framework is
quite general, it has limitations in its expressiv-
ity. In particular, it cannot capture redundancy
constraints that depend on pairwise dissimilarities
between sentences. For example, a natural con-
straint on the summary is that the sum or the mini-
mum of pairwise dissimilarities between sentences
chosen in the summary should be maximized; this,
unfortunately, is not a submodular function. We
call functions that depend on inter-sentence pair-
1014
wise dissimilarities in the summary as dispersion
functions. Our focus in this work is on signif-
icantly furthering the submodularity-based sum-
marization framework to incorporate such disper-
sion functions.
We propose a very general graph-based sum-
marization framework that combines a submod-
ular function with a non-submodular dispersion
function. We consider three natural dispersion
functions on the sentences in a summary: sum
of all-pair sentence dissimilarities, the weight of
the minimum spanning tree on the sentences, and
the minimum of all-pair sentence dissimilarities.
These three functions represent three different
ways of using the sentence dissimilarities. We
then show that a greedy algorithm can obtain ap-
proximately optimal summary in each of the three
cases; the proof exploits some nice combinatorial
properties satisfied by the three dispersion func-
tions. We then conduct experiments on two cor-
pora: the DUC 2004 corpus and a corpus of user
comments on news articles. On DUC 2004, we
obtain performance that matches (Lin and Bilmes,
2011), without any serious parameter tuning; note
that their framework does not have the dispersion
function. On the comment corpus, we outperform
their method, demonstrating that value of disper-
sion functions. As part of our methodology, we
also use a new structured representation for sum-
maries.
2 Related Work
Automatic summarization is a well-studied prob-
lem in the literature. Several methods have been
proposed for single- and multi-document summa-
rization (Carbonell and Goldstein, 1998; Con-
roy and O?Leary, 2001; Takamura and Okumura,
2009; Shen and Li, 2010).
Related concepts have also been used in several
other scenarios such as query-focused summariza-
tion in information retrieval (Daume? and Marcu,
2006), microblog summarization (Sharifi et al,
2010), event summarization (Filatova, 2004), and
others (Riedhammer et al, 2010; Qazvinian et al,
2010; Yatani et al, 2011).
Graph-based methods have been used for sum-
marization (Ganesan et al, 2010), but in a dif-
ferent context?using paths in graphs to produce
very short abstractive summaries. For a detailed
survey on existing automatic summarization tech-
niques and other related topics, see (Kim et al,
2011; Nenkova and McKeown, 2012).
3 Framework
In this section we present the summarization
framework. We start by describing a generic ob-
jective function that can be widely applied to sev-
eral summarization scenarios. This objective func-
tion is the sum of a monotone submodular cov-
erage function and a non-submodular dispersion
function. We then describe a simple greedy algo-
rithm for optimizing this objective function with
provable approximation guarantees for three natu-
ral dispersion functions.
3.1 Preliminaries
Let C be a collection of texts. Depending on the
summarization application, C can refer to the set
of documents (e.g., newswire) related to a partic-
ular topic as in standard summarization; in other
scenarios (e.g., user-generated content), it is a col-
lection of comments associated with a news article
or a blog post, etc. For each document c ? C,
let S(c) denote the set of sentences in c. Let
U = ?c?CS(c) be the universe of all sentences;
without loss of generality, we assume each sen-
tence is unique to a document. For a sentence
u ? U , let C(u) be the document corresponding
to u.
Each u ? U is associated with a weight w(u),
which might indicate, for instance, how similar u
is to the main article (and/or the query, in query-
dependent settings). Each pair u, v ? U is as-
sociated with a similarity s(u, v) ? [0, 1]. This
similarity can then be used to define an inter-
sentence distance d(?, ?) as follows: let d?(u, v) =
1 ? s(u, v) and define d(u, v) to be the shortest
path distance from u to v in the graph where the
weight of each edge (u, v) is d?(u, v). Note that
d(?, ?) is a metric unlike d?(?, ?), which may not be
a metric. (In addition to being intuitive, d(?, ?) be-
ing a metric helps us obtain guarantees on the al-
gorithm?s output.) For a set S, and a point u 6? S,
define d(u, S) = minv?S d(u, v).
Let k > 0 be fixed. A summary of U is a subset
S ? U, |S| = k. Our aim is to find a summary that
maximizes
f(S) = g(S) + ?h(S), (1)
where g(S) is the coverage function that is non-
negative, monotone, and submodular1, h(S) is a
1A function f : U ? < is submodular if for every
1015
dispersion function, and ? ? 0 is a parameter that
can be used to scale the range of h(?) to be com-
parable to that of g(?).
For two sets S and T , let P be the set of un-
ordered pairs {u, v} where u ? S and v ? T . Our
focus is on the following dispersion functions: the
sum measure hs(S, T ) = ?{u,v}?P d(u, v), the
spanning tree measure ht(S, T ) given by the cost
of the minimum spanning tree of the set S?T , and
the min measure hm(S, T ) = min{u,v}?P d(u, v).
Note that these functions span from consider-
ing the entire set of distances in S to consider-
ing only the minimum distance in S; also it is
easy to construct examples to show that none of
these functions is submodular. Define h?(u, S) =
h?({u}, S) and h?(S) = h?(S, S).
Let O be the optimal solution of the function
f . A summary S? is a ?-approximation if f(S?) ?
?f(O).
3.2 Algorithm
Maximizing (1) is NP-hard even if ? = 0 or if
g(?) = 0 (Chandra and Halldo?rsson, 2001). For
the special case ? = 0, since g(?) is submodular,
a classical greedy algorithm obtains a (1 ? 1/e)-
approximation (Nemhauser et al, 1978). But if
? > 0, since the dispersion function h(?) is not
submodular, the combined objective f(?) is not
submodular as well. Despite this, we show that
a simple greedy algorithm achieves a provable ap-
proximation factor for (1). This is possible due to
some nice structural properties of the dispersion
functions we consider.
Algorithm 2 Greedy algorithm, parametrized by
the dispersion function h; here, U, k, g, ? are fixed.
S0 ? ?; i? 0
for i = 0, . . . , k ? 1 do
v ? argmaxu?U\Si g(Si+u)+?h(Si+u)
Si+1 ? Si ? {v}
end for
3.3 Analysis
In this section we obtain a provable approximation
for the greedy algorithm. First, we show that a
greedy choice is well-behaved with respect to the
dispersion function h?(?).
Lemma 1. Let O be any set with |O| = k. If S is
such that |S| = ` < k, then
(i)?u?O\S hs(u, S) ? |O \ S| `hs(O)k(k?1) ;
A,B ? U , we have f(A)+f(B) ? f(A?B)+f(A?B).
(ii)?u?O\S d(u, S) ? 12ht(O)? ht(S); and(iii) there exists u ? O \ S such that hm(u, S) ?
hm(O)/2.
Proof. The proof for (i) follows directly from
Lemma 1 in (Borodin et al, 2012).
To prove (ii) let T be the tree obtained by adding
all points of O \S directly to their respective clos-
est points on the minimum spanning tree of S. T
is a spanning tree, and hence a Steiner tree, for the
points in set S ? O. Hence, cost(T ) = ht(S) +?
u?O\S d(u, S). Let smt(S) denote the cost of
a minimum Steiner tree of S. Thus, cost(T ) ?
smt(O ? S). Since a Steiner tree of O ? S is also
a Steiner tree of O, smt(O ? S) ? smt(O). Since
this is a metric space, smt(O) ? 12ht(O) (see, forexample, (Cieslik, 2001)). Thus,
ht(S) +
?
u?O\S
d(u, S) ? 12ht(O)
?
?
u?O\S
d(u, S) ? 12ht(O)? ht(S).
To prove (iii), let O = {u1, . . . , uk}. By def-
inition, for every i 6= j, d(ui, uj) ? hm(O).
Consider the (open) ball Bi of radius hm(O)/2
around each element ui. By construction for each
i, Bi ? O = {ui} and for each pair i 6= j,
Bi ?Bj = ?. Since |S| < k, and there are k balls
Bi, there exists k?` ballsBi such that S?Bi = ?,
proving (iii).
We next show that the tree created by the greedy
algorithm for h = ht is not far from the optimum.
Lemma 2. Let u1, . . . , uk be a sequence of points
and let Si = {uj , j ? i}. Then, ht(Sk) ?
1/log k
?
2?j?k d(uj , Sj?1).
Proof. The proof follows by noting that we get a
spanning tree by connecting each ui to its closest
point in Si?1. The cost of this spanning tree is?
2?j?k d(uj , Sj?1) and this tree is also the re-
sult of the greedy algorithm run in an online fash-
ion on the input sequence {u1, . . . , uk}. Using the
result of (Imase and Waxman, 1991), the compet-
itive ratio of this algorithm is log k, and hence the
proof.
We now state and prove the main result about
the quality of approximation of the greedy algo-
rithm.
1016
Theorem 3. For k > 1, there is a polynomial-time
algorithm that obtains a ?-approximation to f(S),
where ? = 1/2 for h = hs, ? = 1/4 for h = hm,
and ? = 1/3 log k for h = ht.
Proof. For hs and ht, we run Algorithm 1 using
a new dispersion function h?, which is a slightly
modified version of h. In particular, for h = hs,
we use h?(S) = 2hs(S). For h = ht, we
abuse notation and define h? to be a function over
an ordered set S = {u1, . . . , uk} as follows:
h?(S) =
?
j?|S| d(uj , Sj?1), where Sj?1 =
{u1, . . . , uj?1}. Let f ?(S) = g(S) + ?h?(S).
Consider the ith iteration of the algorithm. By
the submodularity of g(?),
?
u?O\Si
g(Si ? {u})? g(Si) (2)
? g(O ? Si)? g(Si) ? g(O)? g(Sk),
where we use monotonicity of g(?) to infer g(O ?
Si) ? g(O) and g(Si) ? g(Sk).
For h = hs, the proof follows by Lemma 1(i)
and by Theorem 1 in (Borodin et al, 2012).
For ht, using the above argument of submodu-
larity and monotonicity of g, and the result from
Lemma 1(ii), we have
?
u?O\Si
g(Si ? u)? g(Si) + ?d(u, Si)
? g(O)? g(Si) + ?(ht(O)/2? ht(Si))
? (g(O) + ?ht(O)/2)? (g(Si) + ?ht(Si))
? f(O)/2? (g(Si) + ?ht(Si)).
Also, ht(Si) ? 2 smt(Si) since this is a met-
ric space. Using the monotonicity of the Steiner
tree cost, smt(Si) ? smt(Sk) ? ht(Sk). Hence,
ht(Si) ? 2ht(Sk). Thus,
?
u?O\Si
g(Si ? u)? g(Si) + ?d(u, Si)
? f(O)/2? (g(Si) + ?ht(Si))
? f(O)/2? (g(Sk) + 2?ht(Sk))
? f(O)/2? 2f(Sk). (3)
By the greedy choice of ui+1,
f ?(Si ? ui+1)? f ?(Si)
= g(Si ? ui+1)? g(Si) + ?d(ui+1, Si)
? (f(O)/2? 2f(Sk))/|O \ Si|
? 1k (
f(O)/2? 2f(Sk)).
Summing over all i ? [1, k ? 1],
f ?(Sk) ? (k?1)/k(f(O)/2? 2f(Sk)). (4)
Using Lemma 2 we obtain
f(Sk) = g(Sk) + ?ht(Sk) ?
f ?(Sk)
log k
? 1?
1/k
log k (f(O)/2? 2f(Sk)).
By simplifying, we obtain f(Sk) ? f(O)/3 log k.
Finally for hm, we run Algorithm 1 twice: once
with g as given and h ? 0, and the second
time with g ? 0 and h ? hm. Let Sg and
Sh be the solutions in the two cases. Let Og
and Oh be the corresponding optimal solutions.
By the submodularity and monotonicity of g(?),
g(Sg) ? (1 ? 1/e)g(Og) ? g(Og)/2. Similarly,
using Lemma 1(iii), hm(Sh) ? hm(Oh)/2 since
in any iteration i < k we can choose an ele-
ment ui+1 such that hm(ui+1, Si) ? hm(Oh)/2.
Let S = argmaxX?{Sg ,Sh} f(X). Using an av-eraging argument, since g and hm are both non-
negative,
f(X) ? (f(Sg)+f(Sh))/2 ? (g(Og)+?hm(Oh))/4.
Since by definition g(Og) ? g(O) and hm(Oh) ?
hm(O), we have a 1/4-approximation.
3.4 A universal constant-factor
approximation
Using the above algorithm that we used for hm,
it is possible to give a universal algorithm that
gives a constant-factor approximation to each of
the above objectives. By running the Algorithm 1
once for g ? 0 and next for h ? 0 and taking
the best of the two solutions, we can argue that the
resulting set gives a constant factor approximation
to f . We do not use this algorithm in our exper-
iments, as it is oblivious of the actual dispersion
functions used.
4 Using the Framework
Next, we describe how the framework described
in Section 3 can be applied to our tasks of interest,
i.e., summarizing documents or user-generated
content (in our case, comments). First, we repre-
sent the elements of interest (i.e., sentences within
comments) in a structured manner by using depen-
dency trees. We then use this representation to
1017
generate a graph and instantiate our summariza-
tion objective function with specific components
that capture the desiderata of a given summariza-
tion task.
4.1 Structured representation for sentences
In order to instantiate the summarization graph
(nodes and edges), we first need to model each
sentence (in multi-document summarization) or
comment (i.e., set of sentences) as nodes in the
graph. Sentences have been typically modeled
using standard ngrams (unigrams or bigrams) in
previous summarization work. Instead, we model
sentences using a structured representation, i.e., its
syntax structure using dependency parse trees. We
first use a dependency parser (de Marneffe et al,
2006) to parse each sentence and extract the set
of dependency relations associated with the sen-
tence. For example, the sentence ?I adore tennis?
is represented by the dependency relations (nsubj:
adore, I) and (dobj: adore, tennis).
Each sentence represents a single node u in
the graph (unless otherwise specified) and is com-
prised of a set of dependency relations (or ngrams)
present in the sentence. Furthermore, the edge
weights s(u, v) represent pairwise similarity be-
tween sentences or comments (e.g., similarity be-
tween views expressed in different comments).
The edge weights are then used to define the
inter-sentence distance metric d(u, v) for the dif-
ferent dispersion functions. We identify simi-
lar views/opinions by computing semantic simi-
larity rather than using standard similarity mea-
sures (such as cosine similarity based on ex-
act lexical matches between different nodes in
the graph). For each pair of nodes (u, v) in
the graph, we compute the semantic similarity
score (using WordNet) between every pair of
dependency relation (rel: a, b) in u and v as:
s(u, v) =
?
reli?u,relj?v
reli=relj
WN(ai, aj)?WN(bi, bj),
where rel is a relation type (e.g., nsubj) and a, b
are the two arguments present in the dependency
relation (b does not exist for some relations).
WN(wi, wj) is defined as the WordNet similar-
ity score between words wi and wj .2 The edge
weights are then normalized across all edges in the
2There exists various semantic relatedness measures
based on WordNet (Patwardhan and Pedersen, 2006). In our
experiments, for WN we pick one that is based on the path
length between the two words in the WordNet graph.
graph.
This allows us to perform approximate match-
ing of syntactic treelets obtained from the depen-
dency parses using semantic (WordNet) similar-
ity. For example, the sentences ?I adore tennis?
and ?Everyone likes tennis? convey the same view
and should be assigned a higher similarity score
as opposed to ?I hate tennis?. Using the syntac-
tic structure along with semantic similarity helps
us identify useful (valid) nuggets of information
within comments (or documents), avoid redun-
dancies, and identify similar views in a semantic
space.
4.2 Components of the coverage function
Our coverage function is a linear combination of
the following.
(i) Popularity. One of the requirements for a good
summary (especially, for user-generated content)
is that it should include (or rather not miss) the
popular views or opinions expressed by several
users across multiple documents or comments. We
model this property in our objective function as
follows.
For each node u, we define w(u) as the num-
ber of documents |Curel ? C| from the collection
such that at least one of the dependency relations
rel ? u appeared in a sentence within some doc-
ument c ? Curel . The popularity scores are then
normalized across all nodes in the graph. We then
add this component to our objective function as
w(S) =
?
u?S w(u).
(ii) Cluster contribution. This term captures the
fact that we do not intend to include multiple sen-
tences from the same comment (or document).
Define B to be the clustering induced by the sen-
tence to comment relation, i.e., two sentences in
the same comment belong to the same cluster. The
corresponding contribution to the objective func-
tion is ?B?B |S ?B|1/2.
(iii) Content contribution. This term promotes the
diversification of content. We look at the graph of
sentences where the weight of each edge is s(u, v).
This graph is then partitioned based on a local
random walk based method to give us clusters
D = {D1, . . . , Dn}. The corresponding contribu-
tion to the objective function is?D?D |S?D|1/2.
(iv) Cover contribution. We also measure the
cover of the set S as follows: for each element
s in U first define cover of an element u by a
set S? as cov(u, S?) = ?v?S? s(u, v). Then, the
1018
cover value of the set S is defined as cov(S) =?
u?S min(cov(u, S), 0.25cov(u, U)).3
Thus, the final coverage function is: g(S) =
w(S) + ?
?
B?B |S ? B|1/2 + ?
?
D?D |S ?
D|1/2 + ?cov(S), where ?, ?, ? are non-negative
constants. By using the monotone submodularity
of each of the component functions, and the fact
that addition preserves submodularity, the follow-
ing is immediate.
Fact 4. g(S) is a monotone, non-negative, sub-
modular function.
We then apply Algorithm 1 to optimize (1).
5 Experiments
5.1 Data
Multi-document summarization. We use the
DUC 2004 corpus4 that comprises 50 clusters (i.e.,
50 different summarization tasks) with 10 docu-
ments per cluster on average. Each document con-
tains multiple sentences and the goal is to produce
a summary of all the documents for a given cluster.
Comments summarization. We extracted a set
of news articles and corresponding user comments
from Yahoo! News website. Our corpus contains a
set of 34 articles and each article is associated with
anywhere from 100?500 comments. Each com-
ment contains more than three sentences and 36
words per sentence on average.
5.2 Evaluation
For each summarization task, we compare the
system output (i.e., summaries automatically pro-
duced by the algorithm) against the human-
generated summaries and evaluate the perfor-
mance in terms of ROUGE score (Lin, 2004), a
standard recall-based evaluation measure used in
summarization. A system that produces higher
ROUGE scores generates better quality summary
and vice versa.
We use the following evaluation settings in our
experiments for each summarization task:
(1) For multi-document summarization, we
compute the ROUGE-15 scores that was the main
evaluation criterion for DUC 2004 evaluations.
3The choice of the value 0.25 in the cover component is
inspired by the observations made by (Lin and Bilmes, 2011)
for the ? value used in their cover function.
4http://duc.nist.gov/duc2004/tasks.html
5ROUGE v1.5.5 with options: -a -c 95 -b 665 -m -n 4 -w
1.2
(2) For comment summarization, the collection
of user comments associated with a given arti-
cle is typically much larger. Additionally, indi-
vidual comments are noisy, wordy, diverse, and
informally written. Hence for this task, we use
a slightly different evaluation criterion that is in-
spired from the DUC 2005-2007 summarization
evaluation tasks.
We represent the content within each comment
c (i.e., all sentences S(c) comprising the com-
ment) as a single node in the graph. We then run
our summarization algorithm on the instantiated
graph to produce a summary for each news article.
In addition, each news article and corresponding
set of comments were presented to three human
annotators. They were asked to select a subset of
comments (at most 20 comments) that best rep-
resented a summary capturing the most popular
as well as diverse set of views and opinions ex-
pressed by different users that are relevant to the
given news article. We then compare the auto-
matically generated comment summaries against
the human-generated summaries and compute the
ROUGE-1 and ROUGE-2 scores.6
This summarization task is particularly hard for
even human annotators since user-generated com-
ments are typically noisy and there are several
hundreds of comments per article. Similar to ex-
isting work in the literature (Sekine and Nobata,
2003), we computed inter-annotator agreement for
the humans by comparing their summaries against
each other on a small held-out set of articles. The
average ROUGE-1 F-scores observed for humans
was much higher (59.7) than that of automatic sys-
tems measured against the human-generated sum-
maries (our best system achieved a score of 28.9
ROUGE-1 on the same dataset). This shows that
even though this is a new type of summariza-
tion task, humans tend to generate more consistent
summaries and hence their annotations are reliable
for evaluation purposes as in multi-document sum-
marization.
5.3 Results
Multi-document summarization. (1) Table 1
compares the performance of our system with
the previous best reported system that partici-
pated in the DUC 2004 competition. We also in-
clude for comparison another baseline?a version
6ROUGE v1.5.5 with options: -a -n 2 -x -m -2 4 -u -c 95
-r 1000 -f A -p 0.5 -t 0 -d -l 150
1019
of our system that approximates the submodular
objective function proposed by (Lin and Bilmes,
2011).7 As shown in the results, our best system8
which uses the hs dispersion function achieves a
better ROUGE-1 F-score than all other systems.
(2) We observe that the hm and ht dispersion func-
tions produce slightly lower scores than hs, which
may be a characteristic of this particular summa-
rization task. We believe that the empirical results
achieved by different dispersion functions depend
on the nature of the summarization tasks and there
are task settings under which hm or ht perform
better than hs. For example, we show later how us-
ing the ht dispersion function yields the best per-
formance on the comments summarization task.
Regardless, the theoretical guarantees presented in
this paper cover all these cases.
(3) We also analyze the contributions of individ-
ual components of the new objective function to-
wards summarization performance by selectively
setting certain parameters to 0. Table 2 illustrates
these results. We clearly see that each component
(popularity, cluster contribution, dispersion) indi-
vidually yields a reasonable summarization per-
formance but the best result is achieved by the
combined system (row 5 in the table). We also
contrast the performance of the full system with
and without the dispersion component (row 4 ver-
sus row 5). The results show that optimizing for
dispersion yields an improvement in summariza-
tion performance.
(4) To understand the effect of utilizing syntactic
structure and semantic similarity for constructing
the summarization graph, we ran the experiments
using just the unigrams and bigrams; we obtained
a ROUGE-1 F-score of 37.1. Thus, modeling
the syntactic structure (using relations extracted
7Note that Lin & Bilmes (2011) report a slightly higher
ROUGE-1 score (F-score 38.90) on DUC 2004. This is be-
cause their system was tuned for the particular summarization
task using the DUC 2003 corpus. On the other hand, even
without any parameter tuning our method yields good perfor-
mance, as evidenced by results on the two different summa-
rization tasks. However, since individual components within
our objective function are parametrized it is easy to tune them
for a specific task or genre.
8For the full system, we weight certain parameters per-
taining to cluster contributions and dispersion higher (? =
? = ? = 5) compared to the rest of the objective function
(? = 1). Lin & Bilmes (2011) also observed a similar find-
ing (albeit via parameter tuning) where weighting the cluster
contribution component higher yielded better performance.
If the maximum number of sentences/comments chosen were
k, we brought both hs and ht to the same approximate scale
as hm by dividing hs by k(k ? 1)/2 and ht by k ? 1.
from dependency parse tree) along with comput-
ing similarity in semantic spaces (using WordNet)
clearly produces an improvement in the summa-
rization quality (+1.4 improvement in ROUGE-1
F-score). However, while the structured represen-
tation is beneficial, we observed that dispersion
(and other individual components) contribute sim-
ilar performance gains even when using ngrams
alone. So the improvements obtained from the
structured representation and dispersion are com-
plementary.
System ROUGE-1 F
Best system in DUC 2004 37.9
(Lin and Bilmes, 2011), no tuning 37.47
Our algorithm with h = hm 37.5
h = hs 38.5
h = ht 36.8
Table 1: Performance on DUC 2004.
Comments summarization. (1) Table 3 com-
pares the performance of our system against a
baseline system that is constructed by picking
comments in order of decreasing length, i.e., we
first pick the longest comment (comprising the
most number of characters), then the next longest
comment and so on, to create an ordered set of
comments. The intuition behind this baseline is
that longer comments contain more content and
possibly cover more topics than short ones.
From the table, we observe that the new sys-
tem (using either dispersion function) outperforms
the baseline by a huge margin (+44% relative
improvement in ROUGE-1 and much bigger im-
provements in ROUGE-2 scores). One reason be-
hind the lower ROUGE-2 scores for the baseline
might be that while long comments provide more
content (in terms of size), they also add noise and
irrelevant information to the generated summaries.
Our system models sentences using the syntactic
structure and semantics and jointly optimizes for
multiple summarization criteria (including disper-
sion) which helps weed out the noise and identify
relevant, useful information within the comments
thereby producing better quality summaries. The
95% confidence interval scores for the best system
on this task is [36.5?46.9].
(2) Unlike the multi-document summarization,
here we observe that the ht dispersion function
yields the best empirical performance for this
task. This observation supports our claim that the
choice of the specific dispersion function depends
1020
Objective function components ROUGE-1 F
? = ? = ? = ? = 0 35.7
w(S) = ? = ? = ? = 0 35.1
h = hs, w(S) = ? = ? = ? = 0 37.1
? = 0 37.4
w(S), ?, ?, ?, ? > 0 38.5
Table 2: Performance with different parameters
(DUC).
on the summarization task and that the dispersion
functions proposed in this paper have a wider va-
riety of use cases.
(3) Results showing contributions from individual
components of the new summarization objective
function are listed in Table 4. We observe a sim-
ilar pattern as with multi-document summariza-
tion. The full system using all components out-
perform all other parameter settings, achieving the
best ROUGE-1 and ROUGE-2 scores. The table
also shows that incorporating dispersion into the
objective function yields an improvement in sum-
marization quality (row 4 versus row 5).
System ROUGE-1 ROUGE-2
Baseline (decreasing length) 28.9 2.9
Our algorithm with h = hm 39.2 13.2
h = hs 40.9 15.0
h = ht 41.6 16.2
Table 3: Performance on comments summariza-
tion.
Objective function ROUGE-1 ROUGE-2
components
? = ? = ? = ? = 0 36.1 9.4
w(S) = ? = ? = ? = 0 32.1 4.9
h = ht, w(S) = ? = ? = ? = 0 37.8 11.2
? = 0 38.0 11.6
w(S), ?, ?, ?, ? > 0 41.6 16.2
Table 4: Performance with different parameters
(comments).
6 Conclusions
We introduced a new general-purpose graph-based
summarization framework that combines a sub-
modular coverage function with a non-submodular
dispersion function. We presented three natural
dispersion functions that represent three different
ways of ensuring non-redundancy (using sentence
dissimilarities) for summarization and proved that
a simple greedy algorithm can obtain an approxi-
mately optimal summary in all these cases. Exper-
iments on two different summarization tasks show
that our algorithm outperforms algorithms that
rely only on submodularity. Finally, we demon-
strated that using a structured representation to
model sentences in the graph improves summa-
rization quality.
For future work, it would be interesting to in-
vestigate other related developments in this area
and perhaps combine them with our approach to
see if further improvements are possible. Firstly,
it would interesting to see if dispersion offers sim-
ilar improvements over a tuned version of the sub-
modular framework of Lin and Bilmes (2011). In a
very recent work, Lin and Bilmes (2012) demon-
strate a further improvement in performance for
document summarization by using mixtures of
submodular shells. This is an interesting exten-
sion of their previous submodular framework and
while the new formulation permits more complex
functions, the resulting function is still submodu-
lar and hence can be combined with the dispersion
measures proposed in this paper. A different body
of work uses determinantal point processes (DPP)
to model subset selection problems and adapt it
for document summarization (Kulesza and Taskar,
2011). Note that DPPs use similarity kernels for
performing inference whereas our measures are
combinatorial and not kernel-representable. While
approximation guarantees for DPPs are open, it
would be interesting to investigate the empiri-
cal gains by combining DPPs with dispersion-like
functions.
Acknowledgments
We thank the anonymous reviewers for their many
useful comments.
References
Allan Borodin, Hyun Chul Lee, and Yuli Ye. 2012.
Max-sum diversification, monotone submodular
functions and dynamic updates. In Proc. PODS,
pages 155?166.
Jaime Carbonell and Jade Goldstein. 1998. The use of
MMR, diversity-based reranking for reordering doc-
uments and producing summaries. In Proc. SIGIR,
pages 335?336.
Barun Chandra and Magnu?s Halldo?rsson. 2001. Facil-
ity dispersion and remote subgraphs. J. Algorithms,
38(2):438?465.
Dietmar Cieslik. 2001. The Steiner Ratio. Springer.
1021
John M. Conroy and Dianne P. O?Leary. 2001. Text
summarization via hidden Markov models. In Proc.
SIGIR, pages 406?407.
Hal Daume?, III and Daniel Marcu. 2006. Bayesian
query-focused summarization. In Proc. COL-
ING/ACL, pages 305?312.
Marie-Catherine de Marneffe, Bill Maccartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proc. LREC, pages 449?454.
Elena Filatova. 2004. Event-based extractive summa-
rization. In Proc. ACL Workshop on Summarization,
pages 104?111.
Kavita Ganesan, ChengXiang Zhai, and Jiawei Han.
2010. Opinosis: A graph based approach to abstrac-
tive summarization of highly redundant opinions. In
Proc. COLING.
Makoto Imase and Bernard M. Waxman. 1991. Dy-
namic Steiner tree problem. SIAM J. Discrete Math-
ematics, 4(3):369?384.
Hyun Duk Kim, Kavita Ganesan, Parikshit Sondhi, and
ChengXiang Zhai. 2011. Comprehensive review of
opinion summarization. Technical report, Univer-
sity of Illinois at Urbana-Champaign.
Alex Kulesza and Ben Taskar. 2011. Learning deter-
minantal point processes. In Proc. UAI, pages 419?
427.
Hui Lin and Jeff Bilmes. 2011. A class of submodu-
lar functions for document summarization. In Proc.
ACL, pages 510?520.
Hui Lin and Jeff Bilmes. 2012. Learning mixtures
of submodular shells with application to document
summarization. In Proc. UAI, pages 479?490.
Chin-Yew Lin. 2004. ROUGE: A package for auto-
matic evaluation of summaries. In Workshop on Text
Summarization Branches Out: Proc. ACL Work-
shop, pages 74?81.
G. L. Nemhauser, L. A. Wolsey, and M. L. Fisher.
1978. An analysis of approximations for maximiz-
ing submodular set functions I. Mathematical Pro-
gramming, 14(1):265?294.
Ani Nenkova and Kathleen McKeown. 2012. A survey
of text summarization techniques. In Charu C. Ag-
garwal and ChengXiang Zhai, editors, Mining Text
Data, pages 43?76. Springer.
Siddharth Patwardhan and Ted Pedersen. 2006. Us-
ing WordNet-based context vectors to estimate the
semantic relatedness of concepts. In Proc. EACL
Workshop on Making Sense of Sense: Bringing
Computational Linguistics and Psycholinguistics
Together, pages 1?8.
Vahed Qazvinian, Dragomir R. Radev, and Arzucan
O?zgu?r. 2010. Citation summarization through
keyphrase extraction. In Proc. COLING, pages 895?
903.
Korbinian Riedhammer, Benoit Favre, and Dilek
Hakkani-Tu?r. 2010. Long story short?Global
unsupervised models for keyphrase based meeting
summarization. Speech Commun., 52(10):801?815.
Satoshi Sekine and Chikashi Nobata. 2003. A survey
for multi-document summarization. In Proc. HLT-
NAACL Workshop on Text Summarization, pages
65?72.
Beaux Sharifi, Mark-Anthony Hutton, and Jugal Kalita.
2010. Summarizing microblogs automatically. In
Proc. HLT/NAACL, pages 685?688.
Chao Shen and Tao Li. 2010. Multi-document summa-
rization via the minimum dominating set. In Proc.
COLING, pages 984?992.
Hiroya Takamura and Manabu Okumura. 2009. Text
summarization model based on maximum coverage
problem and its variant. In Proc. EACL, pages 781?
789.
Koji Yatani, Michael Novati, Andrew Trusty, and
Khai N. Truong. 2011. Review spotlight: A user in-
terface for summarizing user-generated reviews us-
ing adjective-noun word pairs. In Proc. CHI, pages
1541?1550.
1022
Parallel Algorithms for Unsupervised Tagging
Sujith Ravi
Google
Mountain View, CA 94043
sravi@google.com
Sergei Vassilivitskii
Google
Mountain View, CA 94043
sergeiv@google.com
Vibhor Rastogi?
Twitter
San Francisco, CA
vibhor.rastogi@gmail.com
Abstract
We propose a new method for unsupervised
tagging that finds minimal models which are
then further improved by Expectation Max-
imization training. In contrast to previous
approaches that rely on manually specified
and multi-step heuristics for model minimiza-
tion, our approach is a simple greedy approx-
imation algorithm DMLC (DISTRIBUTED-
MINIMUM-LABEL-COVER) that solves this
objective in a single step.
We extend the method and show how to ef-
ficiently parallelize the algorithm on modern
parallel computing platforms while preserving
approximation guarantees. The new method
easily scales to large data and grammar sizes,
overcoming the memory bottleneck in previ-
ous approaches. We demonstrate the power
of the new algorithm by evaluating on various
sequence labeling tasks: Part-of-Speech tag-
ging for multiple languages (including low-
resource languages), with complete and in-
complete dictionaries, and supertagging, a
complex sequence labeling task, where the
grammar size alone can grow to millions of
entries. Our results show that for all of these
settings, our method achieves state-of-the-art
scalable performance that yields high quality
tagging outputs.
1 Introduction
Supervised sequence labeling with large labeled
training datasets is considered a solved problem. For
?The research described herein was conducted while the
author was working at Google.
instance, state of the art systems obtain tagging ac-
curacies over 97% for part-of-speech (POS) tagging
on the English Penn Treebank. However, learning
accurate taggers without labeled data remains a chal-
lenge. The accuracies quickly drop when faced with
data from a different domain, language, or when
there is very little labeled information available for
training (Banko and Moore, 2004).
Recently, there has been an increasing amount
of research tackling this problem using unsuper-
vised methods. A popular approach is to learn from
POS-tag dictionaries (Merialdo, 1994), where we
are given a raw word sequence and a dictionary of
legal tags for each word type. Learning from POS-
tag dictionaries is still challenging. Complete word-
tag dictionaries may not always be available for use
and in every setting. When they are available, the
dictionaries are often noisy, resulting in high tag-
ging ambiguity. Furthermore, when applying tag-
gers in new domains or different datasets, we may
encounter new words that are missing from the dic-
tionary. There have been some efforts to learn POS
taggers from incomplete dictionaries by extending
the dictionary to include these words using some
heuristics (Toutanova and Johnson, 2008) or using
other methods such as type-supervision (Garrette
and Baldridge, 2012).
In this work, we tackle the problem of unsuper-
vised sequence labeling using tag dictionaries. The
first reported work on this problem was on POS tag-
ging from Merialdo (1994). The approach involved
training a standard Hidden Markov Model (HMM)
using the Expectation Maximization (EM) algo-
rithm (Dempster et al., 1977), though EM does not
105
Transactions of the Association for Computational Linguistics, 2 (2014) 105?118. Action Editor: Sharon Goldwater.
Submitted 11/2013; Revised 2/2014; Published 4/2014. c?2014 Association for Computational Linguistics.
perform well on this task (Johnson, 2007). More re-
cent methods have yielded better performance than
EM (see (Ravi and Knight, 2009) for an overview).
One interesting line of research introduced by
Ravi and Knight (2009) explores the idea of per-
forming model minimization followed by EM train-
ing to learn taggers. Their idea is closely related
to the classic Minimum Description Length princi-
ple for model selection (Barron et al., 1998). They
(1) formulate an objective function to find the small-
est model that explains the text (model minimization
step), and then, (2) fit the minimized model to the
data (EM step). For POS tagging, this method (Ravi
and Knight, 2009) yields the best performance to
date; 91.6% tagging accuracy on a standard test
dataset from the English Penn Treebank. The orig-
inal work from (Ravi and Knight, 2009) uses an in-
teger linear programming (ILP) formulation to find
minimal models, an approach which does not scale
to large datasets. Ravi et al. (2010b) introduced a
two-step greedy approximation to the original ob-
jective function (called the MIN-GREEDY algo-
rithm) that runs much faster while maintaining the
high tagging performance. Garrette and Baldridge
(2012) showed how to use several heuristics to fur-
ther improve this algorithm (for instance, better
choice of tag bigrams when breaking ties) and stack
other techniques on top, such as careful initialization
of HMM emission models which results in further
performance gains. Their method also works un-
der incomplete dictionary scenarios and can be ap-
plied to certain low-resource scenarios (Garrette and
Baldridge, 2013) by combining model minimization
with supervised training.
In this work, we propose a new scalable algorithm
for performing model minimization for this task. By
making an assumption on the structure of the solu-
tion, we prove that a variant of the greedy set cover
algorithm always finds an approximately optimal la-
bel set. This is in contrast to previous methods that
employ heuristic approaches with no guarantee on
the quality of the solution. In addition, we do not
have to rely on ad hoc tie-breaking procedures or
careful initializations for unknown words. Finally,
not only is the proposed method approximately op-
timal, it is also easy to distribute, allowing it to eas-
ily scale to very large datasets. We show empirically
that our method, combined with an EM training step
outperforms existing state of the art systems.
1.1 Our Contributions
? We present a new method, DISTRIBUTED
MINIMUM LABEL COVER, DMLC, for model
minimization that uses a fast, greedy algorithm
with formal approximation guarantees to the
quality of the solution.
? We show how to efficiently parallelize the al-
gorithm while preserving approximation guar-
antees. In contrast, existing minimization ap-
proaches cannot match the new distributed al-
gorithm when scaling from thousands to mil-
lions or even billions of tokens.
? We show that our method easily scales to both
large data and grammar sizes, and does not re-
quire the corpus or label set to fit into memory.
This allows us to tackle complex tagging tasks,
where the tagset consists of several thousand
labels, which results in more than one million
entries in the grammar.
? We demonstrate the power of the new
method by evaluating under several differ-
ent scenarios?POS tagging for multiple lan-
guages (including low-resource languages),
with complete and incomplete dictionaries, as
well as a complex sequence labeling task of su-
pertagging. Our results show that for all these
settings, our method achieves state-of-the-art
performance yielding high quality taggings.
2 Related Work
Recently, there has been an increasing amount of
research tackling this problem from multiple di-
rections. Some efforts have focused on inducing
POS tag clusters without any tags (Christodoulopou-
los et al., 2010; Reichart et al., 2010; Moon et
al., 2010), but evaluating such systems proves dif-
ficult since it is not straightforward to map the clus-
ter labels onto gold standard tags. A more pop-
ular approach is to learn from POS-tag dictionar-
ies (Merialdo, 1994; Ravi and Knight, 2009), incom-
plete dictionaries (Hasan and Ng, 2009; Garrette and
Baldridge, 2012) and human-constructed dictionar-
ies (Goldberg et al., 2008).
106
Another direction that has been explored in the
past includes bootstrapping taggers for a new lan-
guage based on information acquired from other lan-
guages (Das and Petrov, 2011) or limited annota-
tion resources (Garrette and Baldridge, 2013). Ad-
ditional work focused on building supervised tag-
gers for noisy domains such as Twitter (Gimpel et
al., 2011). While most of the relevant work in this
area centers on POS tagging, there has been some
work done for building taggers for more complex
sequence labeling tasks such as supertagging (Ravi
et al., 2010a).
Other related work include alternative methods
for learning sparse models via priors in Bayesian in-
ference (Goldwater and Griffiths, 2007) and poste-
rior regularization (Ganchev et al., 2010). But these
methods only encourage sparsity and do not explic-
itly seek to minimize the model size, which is the ob-
jective function used in this work. Moreover, taggers
learned using model minimization have been shown
to produce state-of-the-art results for the problems
discussed here.
3 Model
Following Ravi and Knight (2009), we formulate the
problem as that of label selection on the sentence
graph. Formally, we are given a set of sequences,
S = {S1, S2, . . . , Sn} where each Si is a sequence
of words, Si = wi1, wi2, . . . , wi,|Si|. With each
word wij we associate a set of possible tags Tij . We
will denote by m the total number of (possibly du-
plicate) words (tokens) in the corpus.
Additionally, we define two special words w0 and
w? with special tags start and end, and consider
the modified sequences S?i = w0, Si, w?. To sim-
plify notation, we will refer to w? = w|Si|+1. The
sequence label problem asks us to select a valid tag
tij ? Tij for each word wij in the input to minimize
a specific objective function.
We will refer to a tag pair (ti,j?1, tij) as a label.
Our aim is to minimize the number of distinct labels
used to cover the full input. Formally, given a se-
quence S?i and a tag tij for each word wij in S?i, let
the induced set of labels for sequence S?i be
Li =
|S?i|?
j=1
{(ti,j?1, tij)}.
The total number of distinct labels used over all se-
quences is then
? =
?? ?i Li| =
???
i
|Si|+1?
j=1
{(ti,j?1, tij)}|.
Note that the order of the tokens in the label makes
a difference as {(NN, VP)} and {(VP, NN)} are two
distinct labels.
Now we can define the problem formally, follow-
ing (Ravi and Knight, 2009).
Problem 1 (Minimum Label Cover). Given a set S
of sequences of words, where each word wij has a
set of valid tags Tij , the problem is to find a valid tag
assignment tij ? Tij for each word that minimizes
the number of distinct labels or tag pairs over all
sequences, ? = ???i
?|Si|+1
j=1 {(ti,j?1, tij)}| .
The problem is closely related to the classical Set
Cover problem and is also NP-complete. To reduce
Set Cover to the label selection problem, map each
element i of the Set Cover instance to a single word
sentence Si = wi1, and let the valid tags Ti1 con-
tain the names of the sets that contain element i.
Consider a solution to the label selection problem;
every sentence Si is covered by two labels (w0, ki)
and (ki, w?), for some ki ? Ti1, which corresponds
to an element i being covered by set ki in the Set
Cover instance. Thus any valid solution to the label
selection problem leads to a feasible solution to the
Set Cover problem ({k1, k2, . . .}) of exactly half the
size.
Finally, we will use {{. . .}} notation to denote a
multiset of elements, i.e. a set where an element may
appear multiple times.
4 Algorithm
In this Section, we describe the DISTRIBUTED-
MINIMUM-LABEL-COVER, DMLC, algorithm for
approximately solving the minimum label cover
problem. We describe the algorithm in a central-
ized setting, and defer the distributed implementa-
tion to Section 5. Before describing the algorithm,
we briefly explain the relationship of the minimum
label cover problem to set cover.
4.1 Modification of Set Cover
As we pointed out earlier, the minimum label cover
problem is at least as hard as the Set Cover prob-
107
1: Input: A set of sequences S with each
words wij having possible tags Tij .
2: Output: A tag assignment tij ? Tij for
each word wij approximately minimizing
labels.
3: LetM be the multi set of all possible labels
generated by choosing each possible tag t ?
Tij .
M =
?
i
?
????
|Si|+1?
j=1
?
t??Ti,j?1
t?Tij
{{(t?, t)}}
?
????
(1)
4: Let L = ? be the set of selected labels.
5: repeat
6: Select the most frequent label not yet se-
lected: (t?, t) = arg max(s?,s)/?L |M ?
(s?, s)|.
7: For each bigram (wi,j?1, wij) where t? ?
Ti,j?1 and t ? Tij tentatively assign t? to
wi,j?1 and t to wij . Add (t?, t) to L.
8: If a word gets two assignments, select
one at random with equal probability.
9: If a bigram (wi,j?1, wij) is consistent
with assignments in (t?, t), fix the tenta-
tive assignments, and set Ti,j?1 = {t?}
and Tij = {t}. RecomputeM, the multi-
set of possible labels, with the updated
Ti,j?1 and Tij .
10: until there are no unassigned words
Algorithm 1: MLC Algorithm
1: Input: A set of sequences S with each words wij
having possible tags Tij .
2: Output: A tag assignment tij ? Tij for each word
wij approximately minimizing labels.
3: (Graph Creation) Initialize each vertex vij with the
set of possible tags Tij and its neighbors vi,j+1 and
vi,j?1.
4: repeat
5: (Message Passing) Each vertex vij sends its pos-
sibly tags Tij to its forward neighbor vij+1.
6: (Counter Update) Each vertex receives the
the tags Ti,j?1 and adds all possible labels
{(s, s?)|s ? Ti,j?1, s? ? Tij} to a global counter
(M).
7: (MaxLabel Selection) Each vertex queries the
global counter M to find the maximum label
(t, t?).
8: (Tentative Assignment) Each vertex vij selects a
tag tentatively as follows: If one of the tags t, t?
is in the feasible set Tij , it tentatively selects the
tag.
9: (Random Assignment) If both are feasible it se-
lects one at random. The vertex communicates
its assignment to its neighbors.
10: (Confirmed Assignment) Each vertex receives
the tentative assignment from its neighbors. If
together with its neighbors it can match the se-
lected label, the assignment is finalized. If the
assigned tag is T , then the vertex vij sets the
valid tag set Tij to {t}.
11: until no unassigned vertices exist.
Algorithm 2: DMLC Implementation
lem. An additional challenge comes from the fact
that labels are tags for a pair of words, and hence
are related. For example, if we label a word pair
(wi,j?1, wij) as (NN, VP), then the label for the next
word pair (wij , wi,j+1) has to be of the form (VP, *),
i.e., it has to start with VP.
Previous work (Ravi et al., 2010a; Ravi et al.,
2010b) recognized this challenge and employed two
phase heuristic approaches. Eschewing heuristics,
we will show that with one natural assumption, even
with this extra set of constraints, the standard greedy
algorithm for this problem results in a solution with
a provable approximation ratio of O(logm). In
practice, however, the algorithm performs far better
than the worst case ratio, and similar to the work
of (Gomes et al., 2006), we find that the greedy
approach selects a cover approximately 11% worse
than the optimum solution.
4.2 MLC Algorithm
We present in Algorithm 1 our MINIMUM LABEL
COVER algorithm to approximately solve the mini-
mum label cover problem. The algorithm is simple,
efficient, and easy to distribute.
The algorithm chooses labels one at a time, select-
ing a label that covers as many words as possible in
108
every iteration. For this, it generates and maintains
a multi-set of all possible labels M (Step 3). The
multi-set contains an occurrence of each valid label,
for example, if wi,j?1 has two possible valid tags
NN and VP, and wij has one possible valid tag VP,
then M will contain two labels, namely (NN, VP)
and (VP, VP). Since M is a multi-set it will contain
duplicates, e.g. the label (NN, VP) will appear for
each adjacent pair of words that have NN and VP as
valid tags, respectively.
In each iteration, the algorithm picks a label with
the most number of occurrences inM and adds it to
the set of chosen labels (Step 6). Intuitively, this is
a greedy step to select a label that covers the most
number of word pairs.
Once the algorithm picks a label (t?, t), it tries to
assign as many words to tags t or t? as possible (Step
7). A word can be assigned t? if t? is a valid tag for
it, and t a valid tag for the next word in sequence.
Similarly, a word can be assigned t, if t is a valid
tag for it, and t? a valid tag for the previous word.
Some words can get both assignments, in which case
we choose one tentatively at random (Step 8). If
a word?s tentative random tag, say t, is consistent
with the choices of its adjacent words (say t? from
the previous word), then the tentative choice is fixed
as a permanent one. Whenever a tag is selected, the
set of valid tags Tij for the word is reduced to a sin-
gleton {t}. Once the set of valid tags Tij changes,
the multi-setM of all possible labels also changes,
as seen from Eq 1. The multi-set is then recom-
puted (Step 9) and the iterations repeated until all
of words have been tagged.
We can show that under a natural assumption this
simple algorithm is approximately optimal.
Assumption 1 (c-feasibility). Let c ? 1 be any num-
ber, and k be the size of the optimal solution to the
original problem. In each iteration, the MLC algo-
rithm fixes the tags for some words. We say that the
algorithm is c-feasible, if after each iteration there
exists some solution to the remaining problem, con-
sistent with the chosen tags, with size at most ck .
The assumption encodes the fact that a single bad
greedy choice is not going to destroy the overall
structure of the solution, and a nearly optimal so-
lution remains. We note that this assumption of c-
feasibility is not only sufficient, as we will formally
show, but is also necessary. Indeed, without any as-
sumptions, once the algorithm fixes the tag for some
words, an optimal label may no longer be consis-
tent with the chosen tags, and it is not hard to find
contrived examples where the size of the optimal so-
lution doubles after each iteration of MLC.
Since the underlying problem is NP-complete, it
is computationally hard to give direct evidence ver-
ifying the assumption on natural language inputs.
However, on small examples we are able to show
that the greedy algorithm is within a small constant
factor of the optimum, specifically it is within 11%
of the optimum model size for the POS tagging
problem using the standard 24k dataset (Ravi and
Knight, 2009). Combined with the fact that the final
method outperforms state of the art approaches, this
leads us to conclude that the structural assumption is
well justified.
Lemma 1. Under the assumption of c-feasibility,
the MLC algorithm achieves a O(c logm) approx-
imation to the minimum label cover problem, where
m =
?
i |Si| is the total number of tokens.
Proof. To prove the Lemma we will define an objec-
tive function ??, counting the number of unlabeled
word pairs, as a function of possible labels, and
show that ?? decreases by a factor of (1?O(1/ck)) at
every iteration.
To define ??, we first define ?, the number of la-
beled word pairs. Consider a particular set of la-
bels, L = {L1, L2, . . . , Lk} where each label is a
pair (ti, tj). Call {tij} a valid assignment of to-
kens if for each wij , we have tij ? Tij . Then the
score of L under an assignment t, which we denote
by ?t, is the number of bigram labels that appear in
L. Formally, ?t(L) = | ?i,j {{(ti,j?1, tij) ? L}}|.
Finally, we define ?(L) to be the best such assign-
ment, ?(L) = maxt ?t(L), and ??(L) = m ? ?(L)
the number of uncovered labels.
Consider the label selected by the algorithm in ev-
ery step. By the c-feasibility assumption, there ex-
ists some solution having ck labels. Thus, some la-
bel from that solution covers at least a 1/ck fraction
of the remaining words. The selected label (t, t?)
maximizes the intersection with the remaining fea-
sible labels. The conflict resolution step ensures that
in expectation the realized benefit is at least a half
of the maximum, thereby reducing ?? by at least a
109
(1 ? 1/2ck) fraction. Therefore, after O(kc logm)
operations all of the labels are covered.
4.3 Fitting the Model Using EM
Once the greedy algorithm terminates and returns a
minimized grammar of tag bigrams, we follow the
approach of Ravi and Knight (2009) and fit the min-
imized model to the data using the alternating EM
strategy.
In this step, we run an alternating optimization
procedure iteratively in phases. In each phase,
we initialize (and prune away) parameters within
the two HMM components (transition or emission
model) using the output from the previous phase.
We initialize this procedure by restricting the tran-
sition parameters to only those tag bigrams selected
in the model minimization step. We train in con-
junction with the original emission model using EM
algorithm which prunes away some of the emission
parameters. In the next phase, we alternate the ini-
tialization by choosing the pruned emission model
along with the original transition model (with full
set of tag bigrams) and retrain using EM. The alter-
nating EM iterations are terminated when the change
in the size of the observed grammar (i.e., the number
of unique bigrams in the tagging output) is ? 5%.1
We refer to our entire approach using greedy mini-
mization followed by EM training as DMLC + EM.
5 Distributed Implementation
The DMLC algorithm is directly suited towards
parallelization across many machines. We turn to
Pregel (Malewicz et al., 2010), and its open source
version Giraph (Apa, 2013). In these systems the
computation proceeds in rounds. In every round, ev-
ery machine does some local processing and then
sends arbitrary messages to other machines. Se-
mantically, we think of the communication graph as
fixed, and in each round each vertex performs some
local computation and then sends messages to its
neighbors. This mode of parallel programming di-
rects the programmers to ?Think like a vertex.?
The specific systems like Pregel and Giraph build
infrastructure that ensures that the overall system
1For more details on the alternating EM strategy and how
initialization with minimized models improve EM performance
in alternating iterations, refer to (Ravi and Knight, 2009).
is fault tolerant, efficient, and fast. In addition,
they provide implementation of commonly used dis-
tributed data structures, such as, for example global
counters. The programmer?s job is simply to specify
the code that each vertex will run at every round.
We implemented the DMLC algorithm in Pregel.
The implementation is straightforward and given in
Algorithm 2. The multi-set M of Algorithm 1 is
represented as a global counter in Algorithm 2. The
message passing (Step 3) and counter update (Step
4) steps update this global counter and hence per-
form the role of Step 3 of Algorithm 1. Step 5 se-
lects the label with largest count, which is equivalent
to the greedy label picking step 6 of Algorithm 1. Fi-
nally steps 6, 7, and 8 update the tag assignment of
each vertex performing the roles of steps 7, 8, and 9,
respectively, of Algorithm 1.
5.1 Speeding up the Algorithm
The implementation described above directly copies
the sequential algorithm. Here we describe addi-
tional steps we took to further improve the parallel
running times.
Singleton Sets: As the parallel algorithm pro-
ceeds, the set of feasible sets associated with a node
slowly decreases. At some point there is only one
tag that a node can take on, however this tag is rare,
and so it takes a while for it to be selected using the
greedy strategy. Nevertheless, if a node and one of
its neighbors have only a single tag left, then it is
safe to assign the unique label 2.
Modifying the Graph: As is often the case, the
bottleneck in parallel computations is the commu-
nication. To reduce the amount of communication
we reduce the graph on the fly, removing nodes and
edges once they no longer play a role in the compu-
tation. This simple modification decreases the com-
munication time in later rounds as the total size of
the problem shrinks.
6 Experiments and Results
In this Section, we describe the experimental setup
for various tasks, settings and compare empirical
performance of our method against several existing
2We must judiciously initialize the global counter to take
care of this assignment, but this is easily accomplished.
110
baselines. The performance results for all systems
(on all tasks) are measured in terms of tagging accu-
racy, i.e. % of tokens from the test corpus that were
labeled correctly by the system.
6.1 Part-of-Speech Tagging Task
6.1.1 Tagging Using a Complete Dictionary
Data: We use a standard test set (consisting of
24,115 word tokens from the Penn Treebank) for
the POS tagging task. The tagset consists of 45 dis-
tinct tag labels and the dictionary contains 57,388
word/tag pairs derived from the entire Penn Tree-
bank. Per-token ambiguity for the test data is about
1.5 tags/token. In addition to the standard 24k
dataset, we also train and test on larger data sets?
973k tokens from the Penn Treebank, 3M tokens
from PTB+Europarl (Koehn, 2005) data.
Methods: We evaluate and compare performance
for POS tagging using four different methods that
employ the model minimization idea combined with
EM training:
? EM: Training a bigram HMM model using EM
algorithm (Merialdo, 1994).
? ILP + EM: Minimizing grammar size using
integer linear programming, followed by EM
training (Ravi and Knight, 2009).
? MIN-GREEDY + EM: Minimizing grammar
size using the two-step greedy method (Ravi et
al., 2010b).
? DMLC + EM: This work.
Results: Table 1 shows the results for POS tag-
ging on English Penn Treebank data. On the smaller
test datasets, all of the model minimization strate-
gies (methods 2, 3, 4) tend to perform equally well,
yielding state-of-the-art results and large improve-
ment over standard EM. When training (and testing)
on larger corpora sizes, DMLC yields the best re-
ported performance on this task to date. A major
advantage of the new method is that it can easily
scale to large corpora sizes and the distributed na-
ture of the algorithm still permits fast, efficient op-
timization of the global objective function. So, un-
like the earlier methods (such as MIN-GREEDY) it
is fast enough to run on several millions of tokens
to yield additional performance gains (shown in last
column).
Speedups: We also observe a significant speedup
when using the parallelized version of the DMLC
algorithm. Performing model minimization on the
24k tokens dataset takes 55 seconds on a single ma-
chine, whereas parallelization permits model mini-
mization to be feasible even on large datasets. Fig 1
shows the running time for DMLC when run on a
cluster of 100 machines. We vary the input data
size from 1M word tokens to about 8M word tokens,
while holding the resources constant. Both the algo-
rithm and its distributed implementation in DMLC
are linear time operations as evident by the plot.
In fact, for comparison, we also plot a straight line
passing through the first two runtimes. The straight
line essentially plots runtimes corresponding to a
linear speedup. DMLC clearly achieves better run-
times showing even better than linear speedup. The
reason for this is that distributed version has a con-
stant overhead for initialization, independent of the
data size. While the running time for rest of the im-
plementation is linear in data size. Thus, as the data
size becomes larger, the constant overhead becomes
less significant, and the distributed implementation
appears to complete slightly faster as data size in-
creases.
Figure 1: Runtime vs. data size (measured in # of word
tokens) on 100 machines. For comparison, we also plot a
straight line passing through the first two runtimes. The
straight line essentially plots runtimes corresponding to a
linear speedup. DMLC clearly achieves better runtimes
showing a better than linear speedup.
6.1.2 Tagging Using Incomplete Dictionaries
We also evaluate our approach for POS tagging
under other resource-constrained scenarios. Obtain-
111
Method Tagging accuracy (%)
te=24k te=973k
tr=24k tr=973k tr=3.7M
1. EM 81.7 82.3
2. ILP + EM (Ravi and Knight, 2009) 91.6
3. MIN-GREEDY + EM (Ravi et al., 2010b) 91.6 87.1
4. DMLC + EM (this work) 91.4 87.5 87.8
Table 1: Results for unsupervised part-of-speech tagging on English Penn Treebank dataset. Tagging accuracies for
different methods are shown on multiple datasets. te shows the size (number of tokens) in the test data, tr represents
the size of the raw text used to perform model minimization.
ing a complete dictionary is often difficult, espe-
cially for new domains. To verify the utility of our
method when the input dictionary is incomplete, we
evaluate against standard datasets used in previous
work (Garrette and Baldridge, 2012) and compare
against the previous best reported performance for
the same task. In all the experiments (described
here and in subsequent sections), we use the fol-
lowing terminology?raw data refers to unlabeled
text used by different methods (for model minimiza-
tion or other unsupervised training procedures such
as EM), dictionary consists of word/tag entries that
are legal, and test refers to data over which tagging
evaluation is performed.
English Data: For English POS tagging with in-
complete dictionary, we evaluate on the Penn Tree-
bank (Marcus et al., 1993) data. Following (Garrette
and Baldridge, 2012), we extracted a word-tag dic-
tionary from sections 00-15 (751,059 tokens) con-
sisting of 39,087 word types, 45,331 word/tag en-
tries, a per-type ambiguity of 1.16 yielding a per-
token ambiguity of 2.21 on the raw corpus (treating
unknown words as having all 45 possible tags). As
in their setup, we then use the first 47,996 tokens
of section 16 as raw data and perform final evalua-
tion on the sections 22-24. We use the raw corpus
along with the unlabeled test data to perform model
minimization and EM training. Unknown words are
allowed to have all possible tags in both these pro-
cedures.
Italian Data: The minimization strategy pre-
sented here is a general-purpose method that does
not require any specific tuning and works for other
languages as well. To demonstrate this, we also per-
form evaluation on a different language (Italian) us-
ing the TUT corpus (Bosco et al., 2000). Follow-
ing (Garrette and Baldridge, 2012), we use the same
data splits as their setting. We take the first half of
each of the five sections to build the word-tag dic-
tionary, the next quarter as raw data and the last
quarter as test data. The dictionary was constructed
from 41,000 tokens comprised of 7,814 word types,
8,370 word/tag pairs, per-type ambiguity of 1.07 and
a per-token ambiguity of 1.41 on the raw data. The
raw data consisted of 18,574 tokens and the test con-
tained 18,763 tokens. We use the unlabeled corpus
from the raw and test data to perform model mini-
mization followed by unsupervised EM training.
Other Languages: In order to test the effective-
ness of our method in other non-English settings, we
also report the performance of our method on sev-
eral other Indo-European languages using treebank
data from CoNLL-X and CoNLL-2007 shared tasks
on dependency parsing (Buchholz and Marsi, 2006;
Nivre et al., 2007). The corpus statistics for the five
languages (Danish, Greek, Italian, Portuguese and
Spanish) are listed below. For each language, we
construct a dictionary from the raw training data.
The unlabeled corpus from the raw training and test
data is used to perform model minimization fol-
lowed by unsupervised EM training. As before, un-
known words are allowed to have all possible tags.
We report the final tagging performance on the test
data and compare it to baseline EM.
Garrette and Baldridge (2012) treat unknown
words (words that appear in the raw text but are
missing from the dictionary) in a special manner and
use several heuristics to perform better initialization
for such words (for example, the probability that an
unknown word is associated with a particular tag is
112
conditioned on the openness of the tag). They also
use an auto-supervision technique to smooth counts
learnt from EM onto new words encountered dur-
ing testing. In contrast, we do not apply any such
technique for unknown words and allow them to be
mapped uniformly to all possible tags in the dictio-
nary. For this particular set of experiments, the only
difference from the Garrette and Baldridge (2012)
setup is that we include unlabeled text from the test
data (but without any dictionary tag labels or special
heuristics) to our existing word tokens from raw text
for performing model minimization. This is a stan-
dard practice used in unsupervised training scenar-
ios (for example, Bayesian inference methods) and
in general for scalable techniques where the goal is
to perform inference on the same data for which one
wishes to produce some structured prediction.
Language Train Dict Test
(tokens) (entries) (tokens)
DANISH 94386 18797 5852
GREEK 65419 12894 4804
ITALIAN 71199 14934 5096
PORTUGUESE 206678 30053 5867
SPANISH 89334 17176 5694
Results: Table 2 (column 2) compares previously
reported results against our approach for English.
We observe that our method obtains a huge improve-
ment over standard EM and gets comparable results
to the previous best reported scores for the same task
from (Garrette and Baldridge, 2012). It is encourag-
ing to note that the new system achieves this per-
formance without using any of the carefully-chosen
heuristics employed by the previous method. How-
ever, we do note that some of these techniques can
be easily combined with our method to produce fur-
ther improvements.
Table 2 (column 3) also shows results on Ital-
ian POS tagging. We observe that our method
achieves significant improvements in tagging accu-
racy over all the baseline systems including the pre-
vious best system (+2.9%). This demonstrates that
the method generalizes well to other languages and
produces consistent tagging improvements over ex-
isting methods for the same task.
Results for POS tagging on CoNLL data in five
different languages are displayed in Figure 2. Note
that the proportion of raw data in test versus train
50
60
70
80
90
DANISH GREEK ITALIAN PORTUGUESE SPANISH
79.4
66.3
84.6
80.1
83.1
77.8
65.6
82
78.5
81.3
EM DMLC+EM


	
	



	













Figure 2: Part-of-Speech tagging accuracy for different
languages on CoNLL data using incomplete dictionaries.
(from the standard CoNLL shared tasks) is much
smaller compared to the earlier experimental set-
tings. In general, we observe that adding more raw
data for EM training improves the tagging quality
(same trend observed earlier in Table 1: column 2
versus column 3). Despite this, DMLC + EM still
achieves significant improvements over the baseline
EM system on multiple languages (as shown in Fig-
ure 2). An additional advantage of the new method
is that it can easily scale to larger corpora and it pro-
duces a much more compact grammar that can be
efficiently incorporated for EM training.
6.1.3 Tagging for Low-Resource Languages
Learning part-of-speech taggers for severely low-
resource languages (e.g., Malagasy) is very chal-
lenging. In addition to scarce (token-supervised)
labeled resources, the tag dictionaries avail-
able for training taggers are tiny compared to
other languages such as English. Garrette and
Baldridge (2013) combine various supervised and
semi-supervised learning algorithms into a common
POS tagger training pipeline to address some of
these challenges. They also report tagging accuracy
improvements on low-resource languages when us-
ing the combined system over any single algorithm.
Their system has four main parts, in order: (1) Tag
dictionary expansion using label propagation algo-
rithm, (2) Weighted model minimization, (3) Ex-
pectation maximization (EM) training of HMMs us-
ing auto-supervision, (4) MaxEnt Markov Model
(MEMM) training. The entire procedure results in
a trained tagger model that can then be applied to
tag any raw data.3 Step 2 in this procedure involves
3For more details, refer (Garrette and Baldridge, 2013).
113
Method Tagging accuracy (%)
English (PTB 00-15) Italian (TUT)
1. Random 63.53 62.81
2. EM 69.20 60.70
3. Type-supervision + HMM initialization (Garrette and Baldridge, 2012) 88.52 72.86
4. DMLC + EM (this work) 88.11 75.79
Table 2: Part-of-Speech tagging accuracy using PTB sections 00-15 and TUT to build the tag dictionary. For compar-
ison, we also include the results for the previously reported state-of-the-art system (method 3) for the same task.
Method Tagging accuracy (%)
Total Known Unknown
Low-resource tagging using (Garrette and Baldridge, 2013) 80.7 (70.2) 87.6 (90.3) 66.1 (45.1)
Low-resource tagging using DMLC + EM (this work) 81.1 (70.8) 87.9 (90.3) 66.7 (46.5)
Table 3: Part-of-Speech tagging accuracy for a low-resource language (Malagasy) on All/Known/Unknown tokens in
the test data. Tagging performance is shown for multiple experiments using different (incomplete) dictionary sizes:
(a) small, (b) tiny (shown in parentheses). The new method (row 2) significantly outperforms the existing method with
p < 0.01 for small dictionary and p < 0.05 for tiny dictionary.
a weighted version of model minimization which
uses the multi-step greedy approach from Ravi et
al. (2010b) enhanced with additional heuristics that
uses tag weights learnt via label propagation (in Step
1) within the minimization process.
We replace the model minimization procedure in
their Step 2 with our method (DMLC + EM) and di-
rectly compare this new system with their approach
in terms of tagging accuracy. Note for all other steps
in the pipeline we follow the same procedure (and
run the same code) as Garrette and Baldridge (2013),
including the same smoothing procedure for EM ini-
tialization in Step 3.
Data: We use the exact same setup as Garrette
and Baldridge (2013) and run experiments on Mala-
gasy, an Austronesian language spoken in Madagas-
car. We use the publicly available data4: 100k raw
tokens for training, a word-tag dictionary acquired
with 4 hours of human annotation effort (used for
type-supervision), and a held-out test dataset (5341
tokens). We provide the unlabeled corpus from the
raw training data along with the word-tag dictionary
as input to model minimization and evaluate on the
test corpus. We run multiple experiments for dif-
ferent (incomplete) dictionary scenarios: (a) small =
2773 word/tag pairs, (b) tiny = 329 word/tag pairs.
Results: Table 3 shows results on Malagasy
data comparing a system that employs (unweighted)
4github.com/ dhgarrette/low-resource-pos-tagging-2013
DMLC against the existing state-of-the-art system
that incorporates a multi-step weighted model min-
imization combined with additional heuristics. We
observe that switching to the new model minimiza-
tion procedure alone yields significant improvement
in tagging accuracy under both dictionary scenarios.
It is encouraging that a better minimization proce-
dure also leads to higher tagging quality on the un-
known word tokens (column 4 in the table), even
when the input dictionary is tiny.
6.2 Supertagging
Compared to POS tagging, a more challenging task
is learning supertaggers for lexicalized grammar
formalisms such as Combinatory Categorial Gram-
mar (CCG) (Steedman, 2000). For example, CCG-
bank (Hockenmaier and Steedman, 2007) contains
1241 distinct supertags (lexical categories) and the
most ambiguous word has 126 supertags. This pro-
vides a much more challenging starting point for
the semi-supervised methods typically applied to
the task. Yet, this is an important task since cre-
ating grammars and resources for CCG parsers for
new domains and languages is highly labor- and
knowledge-intensive.
As described earlier, our approach scales easily to
large datasets as well as label sizes. To evaluate it on
the supertagging task, we use the same dataset from
(Ravi et al., 2010a) and compare against their base-
line method that uses an modified (two-step) version
114
Method Supertagging accuracy (%)
Ambiguous Total
1. EM 38.7 45.6
2. ILP? + EM (Ravi et al., 2010a) 52.1 57.3
3. DMLC + EM (this work) 55.9 59.3
Table 4: Results for unsupervised supertagging with a dictionary. Here, we report the total accuracy as well as
accuracy on just the ambiguous tokens (i.e., tokens which have more than one tagging possibility). ?The baseline
method 2 requires several pre-processing steps in order to run feasibly for this task (described in Section 6.2). In
contrast, the new approach (DMLC) runs fast and also permits efficient parallelization.
of the ILP formulation for model minimization.
Data: We use the CCGbank data for this ex-
periment. This data was created by semi- auto-
matically converting the Penn Treebank to CCG
derivations (Hockenmaier and Steedman, 2007). We
use the standard splits of the data used in semi-
supervised tagging experiments (Banko and Moore,
2004)?sections 0-18 for training (i.e., to construct
the word-tag dictionary), and sections 22-24 for test.
Results: Table 4 compares the results for two
baseline systems?standard EM (method 1), and a
previously reported system using model minimiza-
tion (method 2) for the same task. We observe
that DMLC produces better taggings than either of
these and yields significant improvement in accu-
racy (+2% overall, +3.8% on ambiguous tokens).
Note that it is not feasible to run the ILP-based
baseline (method 2 in the table) directly since it is
very slow in practice, so Ravi et al. (2010a) use
a set of pre-processing steps to prune the original
grammar size (unique tag pairs) from >1M to sev-
eral thousand entries followed by a modified two-
step ILP minimization strategy. This is required to
permit their model minimization step to be run in
a feasible manner. On the other hand, the new ap-
proach DMLC (method 3) scales better even when
the data/label sizes are large, hence it can be run with
the full data using the original model minimization
formulation (rather than a two-step heuristic).
Ravi et al. (2010a) also report further improve-
ments using an alternative approach involving an
ILP-based weighted minimization procedure. In
Section 7 we briefly discuss how the DMLC method
can be extended to this setting and combined with
other similar methods.
7 Discussion and Conclusion
We present a fast, efficient model minimization
algorithm for unsupervised tagging that improves
upon previous two-step heuristics. We show that un-
der a fairly natural assumption of c-feasibility the
solution obtained by our minimization algorithm is
O(c logm)-approximate to the optimal. Although
in the case of two-step heuristics, the first step guar-
antees an O(logm)-approximation, the second step,
which is required to get a consistent solution, can
introduce many additional labels resulting in a so-
lution arbitrarily away from the optimal. Our one
step approach ensures consistency at each step of the
algorithm, while the c-feasibility assumption means
that the solution does not diverge too much from the
optimal in each iteration.
In addition to proving approximation guarantees
for the new algorithm, we show that it is paralleliz-
able, allowing us to easily scale to larger datasets
than previously explored. Our results show that
the algorithm achieves state-of-the-art performance,
outperforming existing methods on several differ-
ent tasks (both POS tagging and supertagging) and
works well even with incomplete dictionaries and
extremely low-resource languages like Malagasy.
For future work, it would be interesting to apply a
weighted version of the DMLC algorithm where la-
bels (i.e., tag pairs) can have different weight distri-
butions instead of uniform weights. Our algorithm
can be extended to allow an input weight distribu-
tion to be specified for minimization. In order to
initialize the weights we could use existing strate-
gies such as grammar-informed initialization (Ravi
et al., 2010a) or output distributions learnt via other
methods such as label propagation (Garrette and
Baldridge, 2013).
115
References
2013. Apache giraph. http://giraph.apache.
org/.
Michele Banko and Robert C. Moore. 2004. Part-of-
speech tagging in context. In Proceedings of COLING,
pages 556?561.
Andrew R Barron, Jorma Rissanen, and Bin Yu. 1998.
The Minimum Description Length Principle in Cod-
ing and Modeling. IEEE Transactions of Information
Theory, 44(6):2743?2760.
Cristina Bosco, Vincenzo Lombardo, Daniela Vassallo,
and Leonardo Lesmo. 2000. Building a Treebank for
Italian: a data-driven annotation schema. In Proceed-
ings of the Second International Conference on Lan-
guage Resources and Evaluation LREC-2000, pages
99?105.
Sabine Buchholz and Erwin Marsi. 2006. Conll-x shared
task on multilingual dependency parsing. In Proceed-
ings of CoNLL, pages 149?164.
Christos Christodoulopoulos, Sharon Goldwater, and
Mark Steedman. 2010. Two decades of unsupervised
POS induction: How far have we come? In Proceed-
ings of the Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP), pages 575?584.
Dipanjan Das and Slav Petrov. 2011. Unsupervised part-
of-speech tagging with bilingual graph-based projec-
tions. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies - Volume 1, pages 600?
609.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society, Se-
ries B, 39(1):1?38.
Kuzman Ganchev, Joa?o Grac?a, Jennifer Gillenwater, and
Ben Taskar. 2010. Posterior regularization for struc-
tured latent variable models. Journal of Machine
Learning Research, 11:2001?2049.
Dan Garrette and Jason Baldridge. 2012. Type-
supervised Hidden Markov Models for part-of-speech
tagging with incomplete tag dictionaries. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 821?
831.
Dan Garrette and Jason Baldridge. 2013. Learning a
part-of-speech tagger from two hours of annotation. In
Proceedings of the Conference of the North American
Chapter of the Association for Computational Linguis-
tics: Human Language Technologies, pages 138?147.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael
Heilman, Dani Yogatama, Jeffrey Flanigan, and
Noah A. Smith. 2011. Part-of-speech tagging for
Twitter: annotation, features, and experiments. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies: short papers - Volume 2, pages 42?47.
Yoav Goldberg, Meni Adler, and Michael Elhadad. 2008.
EM can find pretty good HMM POS-taggers (when
given a good start). In Proceedings of ACL, pages
746?754.
Sharon Goldwater and Thomas L. Griffiths. 2007.
A fully Bayesian approach to unsupervised part-of-
speech tagging. In ACL.
Fernando C. Gomes, Cludio N. Meneses, Panos M.
Pardalos, and Gerardo Valdisio R. Viana. 2006. Ex-
perimental analysis of approximation algorithms for
the vertex cover and set covering problems.
Kazi Saidul Hasan and Vincent Ng. 2009. Weakly super-
vised part-of-speech tagging for morphologically-rich,
resource-scarce languages. In Proceedings of the 12th
Conference on the European Chapter of the Associa-
tion for Computational Linguistics, pages 363?371.
Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: A corpus of CCG derivations and dependency
structures extracted from the Penn Treebank. Compu-
tational Linguistics, 33(3):355?396.
Mark Johnson. 2007. Why doesn?t EM find good HMM
POS-taggers? In Proceedings of the Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL), pages 296?305.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In Machine Transla-
tion Summit X, pages 79?86.
Grzegorz Malewicz, Matthew H. Austern, Aart J.C Bik,
James C. Dehnert, Ilan Horn, Naty Leiser, and Grze-
gorz Czajkowski. 2010. Pregel: a system for large-
scale graph processing. In Proceedings of the 2010
ACM SIGMOD International Conference on Manage-
ment of data, pages 135?146.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313?330.
Bernard Merialdo. 1994. Tagging English text with
a probabilistic model. Computational Linguistics,
20(2):155?171.
Taesun Moon, Katrin Erk, and Jason Baldridge. 2010.
Crouching Dirichlet, Hidden Markov Model: Unsu-
pervised POS tagging with context local tag genera-
tion. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, pages 196?
206.
116
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan McDon-
ald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.
2007. The CoNLL 2007 shared task on dependency
parsing. In Proceedings of the CoNLL Shared Task
Session of EMNLP-CoNLL, pages 915?932.
Sujith Ravi and Kevin Knight. 2009. Minimized models
for unsupervised part-of-speech tagging. In Proceed-
ings of the Joint Conferenceof the 47th Annual Meet-
ing of the Association for Computational Linguistics
and the 4th International Joint Conference on Natural
Language Processing of the Asian Federation of Natu-
ral Language Processing (ACL-IJCNLP), pages 504?
512.
Sujith Ravi, Jason Baldridge, and Kevin Knight. 2010a.
Minimized models and grammar-informed initializa-
tion for supertagging with highly ambiguous lexicons.
In Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics (ACL), pages
495?503.
Sujith Ravi, Ashish Vaswani, Kevin Knight, and David
Chiang. 2010b. Fast, greedy model minimization for
unsupervised tagging. In Proceedings of the 23rd In-
ternational Conference on Computational Linguistics
(COLING), pages 940?948.
Roi Reichart, Raanan Fattal, and Ari Rappoport. 2010.
Improved unsupervised POS induction using intrinsic
clustering quality and a Zipfian constraint. In Proceed-
ings of the Fourteenth Conference on Computational
Natural Language Learning, pages 57?66.
Mark Steedman. 2000. The Syntactic Process. MIT
Press, Cambridge, MA, USA.
Kristina Toutanova and Mark Johnson. 2008. A
Bayesian LDA-based model for semi-supervised part-
of-speech tagging. In Advances in Neural Information
Processing Systems (NIPS), pages 1521?1528.
117
118
Proceedings of EMNLP 2011, Conference on Empirical Methods in Natural Language Processing, pages 105?112,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Unsupervised Name Ambiguity Resolution Using A Generative Model
Zornitsa Kozareva and Sujith Ravi
USC Information Sciences Institute
4676 Admiralty Way
Marina del Rey, CA 90292-6695
{kozareva,sravi}@isi.edu
Abstract
Resolving ambiguity associated with names
found on the Web, Wikipedia or medical texts
is a very challenging task, which has been
of great interest to the research community.
We propose a novel approach to disambiguat-
ing names using Latent Dirichlet Allocation,
where the learned topics represent the under-
lying senses of the ambiguous name. We con-
duct a detailed evaluation on multiple data sets
containing ambiguous person, location and or-
ganization names and for multiple languages
such as English, Spanish, Romanian and Bul-
garian. We conduct comparative studies with
existing approaches and show a substantial
improvement of 15 to 35% in task accuracy.
1 Introduction
Recently, ambiguity resolution for names found on
the Web (Artiles et al, 2007), Wikipedia articles
(Bunescu and Pasca, 2006), news texts (Pedersen et
al., 2005) and medical literature (Ginter et al, 2004)
has become an active area of research. Like words,
names are ambiguous and can refer to multiple enti-
ties. For example, a Web search for Jerry Hobbs on
Google returns a mixture of documents associated
with two different entities in the top 10 search re-
sults. One refers to a computational linguist at Uni-
versity of Southern California and the other refers to
a fugitive and murderer. Disambiguating the names
and identifying the correct entity is very important
especially for Web search applications since 11-17%
of the Web search queries are composed of person
name and a term (Artiles et al, 2009a).
In the past, there has been a substantial body of
work in the area of name disambiguation under a va-
riety of different names and using diverse set of ap-
proaches. Some refer to the task as cross-document
coreference resolution (Bagga and Baldwin, 1998),
name discrimination (Pedersen et al, 2005) or Web
People Search (WebPS) (Artiles et al, 2007). The
majority of the approaches focus on person name
ambiguity (Chen and Martin, 2007; Artiles et al,
2010), some have also explored organization and lo-
cation name disambiguation (Pedersen et al, 2006).
The intuition behind most approaches follows the
distributional hypothesis (Harris, 1954) according
to which ambiguous names sharing the same con-
texts tend to refer to the same individual. To model
these characteristics, Bunescu and Pasca (2006)
and Cucerzan (2007) incorporate information from
Wikipedia articles, Artiles et al (2007) use Web
page content, Mann and Yarowsky (2003) extract bi-
ographic facts. The approaches used in the WebPS
tasks mainly rely on bag-of-words representations
(Artiles et al, 2007; Chen and Martin, 2007; Artiles
et al, 2009b). Most methods suffer from a com-
mon drawback?they rely on surface features such
as word co-occurrences, which are insufficient to
capture hidden information pertaining to the entities
(senses) associated with the documents.
We take a novel approach for tackling the prob-
lem of name ambiguity using an unsupervised topic
modeling framework. To our knowledge, no one
has yet explored the disambiguation of names using
Latent Dirichlet Allocation (LDA) nor has shown
LDA?s behavior on multiple data sources and set-
tings. Our motivation for using an unsupervised
105
topic modeling framework for name disambiguation
is based on the advantages generative models offer
in contrast to the existing ones. For instance, topic
models such as Latent Dirichlet alocation (LDA)
method (Blei et al, 2003) have been widely used in
the literature for other applications to uncover hid-
den (or latent) groupings underlying a set of obser-
vations. Topic models are capable of handling ambi-
guity and distinguishing between uses of words with
multiple meanings depending on context. Thereby,
they provide a natural fit for our name disambigua-
tion task, where latent topics correspond to the en-
tities (name senses) representing the documents for
an ambiguous name. Identifying these latent topics
helps us identify the particular sense of a given am-
biguous name that is used in the context of a particu-
lar document and hence resolve name ambiguity. In
addition, this approach offers several advantages?
(1) entities (senses) can be learnt automatically from
a collection of documents in an unsupervised man-
ner, (2) efficient methods already exist for perform-
ing inference in this model so we can easily scale
to Web data, and (3) unlike typical approaches, we
can easily apply our learnt model to resolve name
ambiguity for unseen documents.
The main contributions of this paper are:
? We propose a novel model for name disam-
biguation using Latent Dirichlet Allocation.
? Unlike previous approaches, which are de-
signed for specific tasks, corpora and lan-
guages, we conduct a detailed evaluation taking
into consideration the multiple properties of the
data and names.
? Our experimental study shows that LDA can be
used as a general name disambiguation frame-
work, which can be successfully applied on
any corpora (i.e. Web, news, Wikipedia), lan-
guages (i.e. English, Spanish, Romanian and
Bulgarian) and types of ambiguous names (i.e.
people, organizations, locations).
? We conduct a comparative study with existing
state-of-the-art clustering approaches and show
substantial improvements of 15 to 35% in task
accuracy.
The rest of the paper is organized as follows. In Sec-
tion 2 we describe related work. Section 3 describes
the Latent Dirichlet Allocation model used to dis-
ambiguate the names. Section 4 describes the exper-
iments we have conducted on multiple data sets and
languages. Finally, we conclude in Section 5.
2 Related Work
Ambiguous names have been disambiguated with
varying success from structured texts (Pedersen et
al., 2006), semi-structured texts such as Wikipedia
articles (Bunescu and Pasca, 2006; Cucerzan, 2007)
or unstructured texts such as those found on the Web
(Pedersen and Kulkarni, 2007; Artiles et al, 2009b).
Most approaches (Artiles et al, 2009b; Chen et al,
2009; Lan et al, 2009) focus on person name dis-
ambiguation, while others (Pedersen et al, 2006)
also explore ambiguity in organization and location
names. In the medical domain, Hatzivassiloglou et
al. (2001) and Ginter et al (2004) tackle the problem
of gene and protein name disambiguation.
Due to the high interest in this task, researchers
have explored a wide range of approaches and fea-
tures. Among the most common and efficient ones
are those based on clustering and bag-of-words rep-
resentation (Pedersen et al, 2005; Artiles et al,
2009b). Mann and Yarowsky (2003) extract bio-
graphic facts such as date or place of birth, occu-
pation, relatives among others to help resolve am-
biguous names of people. Others (Bunescu and
Pasca, 2006; Cucerzan, 2007; Nguyen and Cao,
2008) work on Wikipedia articles, using infobox
and link information. Pedersen et al (2006) rely
on second order co-occurrence vectors. A few oth-
ers (Matthias, 2005; Wan et al, 2005; Popescu and
Magnini, 2007) identify names of people, locations
and organizations and use them as a source of evi-
dence to measure the similarity between documents
containing the ambiguous names. The most simi-
lar work to ours is that of Song et al (2007) who
use a topic-based modeling approach for name dis-
ambiguation. However, their method explicitly tries
to model the distribution of latent topics with regard
to person names and words appearing within docu-
ments whereas in our method, the latent topics rep-
resent the underlying entities (name senses) for an
ambiguous name.
Unlike the previous approaches which were
specifically designed and evaluated on the WebPS
106
task or a corpus such as Wikipedia or the Web, in
this paper we show a novel unsupervised topic mod-
eling approach for name disambiguation for any cor-
pora (i.e. Web, news, Wikipedia), languages (i.e.
English, Spanish, Romanian and Bulgarian) and se-
mantic categories (i.e. people, location and organi-
zation). The obtained results show substantial im-
provements over the existing approaches.
3 Name Disambiguation with LDA
Recently, topic modeling methods have found
widespread applications in NLP for various
tasks such as summarization (Daume? III and
Marcu, 2006), inferring concept-attribute attach-
ments (Reisinger and Pasca, 2009), selectional
preferences (Ritter et al, 2010) and cross-document
co-reference resolution (Haghighi and Klein, 2010).
Topic models such as LDA are generative models
for documents and represent hidden or latent top-
ics (where a topic is a probability distribution over
words) underlying the semantic structure of docu-
ments. An important use for methods such as LDA
is to infer the set of topics associated with a given
document (or a collection of documents). Next, we
present a novel approach for the task of name dis-
ambiguation using unsupervised topic models.
3.1 Method Description
Given a document corpus D associated with a cer-
tain ambiguous name, our task is to group the docu-
ments into K sets such that each document set cor-
responds to one particular entity (sense) for the am-
biguous name. We first formulate the name disam-
biguation problem as a topic modeling task and then
apply the standard LDA method to infer hidden top-
ics (senses). Our generative story is as follows:
for each name sense sk where k ? {1, ...,K} do
Generate ?sk according to Dir(?)
end for
for each document i in the corpus D do
Choose ?i ? Dir(?)
for each word wi,j where j ? {1, ..., Ni} do
Choose a sense zi,j ?Multinomial(?i)
Choose a word wi,j ?Multinomial(?zi,j )
end for
end for
3.2 Inference
We perform inference on this model using collapsed
Gibbs sampling, where each of the hidden sense
variables zi,j are sampled conditioned on an as-
signment for all other variables, while integrating
over all possible parameter settings (Griffiths and
Steyvers, 2002). We use the MALLET (McCallum,
2002) implementation of LDA for our experiments.
We ran LDA with different parameter settings on a
held out data set and found that the following con-
figuration resulted in the best performance. We set
the hyperparameter ? to the default value of 0.01.
For the name discrimination task, we have to choose
from a smaller set of name senses and each docu-
ment is representative of a single sense, so we use
a sparse prior (?=0.1). On the other hand, the Web
People Search data is more noisy and also involves
a large number of senses, so we use a higher prior
(?=50).
For the name discrimination task (Section 4.1),
we are given a set of senses to choose from and
hence we can use this value to fix the number of top-
ics (senses) K in LDA. However, it is possible that
the number of senses may be unknown to us apriori.
For example, it is difficult to identify all the senses
associated with names of people on the Web. In such
scenarios, we set the value ofK to a fixed value. For
experiments on Web People Search, we set K = 40,
which is roughly the average number of senses as-
sociated with people names on the Web. An alter-
native strategy is to automatically choose the num-
ber of senses based on the model that leads to the
highest posterior probability (Griffiths and Steyvers,
2004). It is easy to incorporate this technique into
our model, but we leave this for future work.
3.3 Interpreting Name Senses From Topics
As a result of training, our model outputs the topic
(sense) distributions for each document in the cor-
pus. Although the LDA model can assign multi-
ple senses to a document, the name disambiguation
task specifies that each document should be assigned
only to a single name sense. Hence, for each docu-
ment i we assign it the most probable sense from its
sense distribution. This allows us to cluster all the
documents in D into K sets.
To evaluate our results against the gold standard
107
data, we further need to find a mapping between our
document clusters and the true name sense labels.
For each cluster k, we identify the true sense labels
(using the gold data) for every document which was
assigned to sense k in our output, and pick the ma-
jority sense label labelkmaj. as being representative
of the entire cluster (i.e., all documents in cluster k
will be labeled as belonging to sense labelkmaj.). Fi-
nally, we evaluate our labeling against the gold data.
4 Experimental Evaluation
Our objective is to study LDA?s performance on
multiple datasets, name categories and languages.
For this purpose, we evaluate our approach on two
tasks: name discrimination and Web People Search,
which are described in the next subsections. We use
freely available data from (Pedersen et al, 2006) and
(Artiles et al, 2009b), which enable us to compare
performance against existing methods.
4.1 Name Discrimination
Pedersen et al (2006) create ambiguous data by
conflating together tuples of non-ambiguous well
known names. The goal is to cluster the contexts
containing the conflated names such that the origi-
nal and correct names are re-discovered. This task is
known as name discrimination.
An advantage of the name conflation process is
that data can be easily created for any type of names
and languages. In our study, we use the whole data
set developed by Pedersen et al (2006) for the En-
glish, Spanish, Romanian and Bulgarian languages.
Table 1 shows the conflated names and the seman-
tic category they belong to (i.e. person, organization
or location) together with the distribution of the in-
stances for each underlying entity in the name. In
total there are eight person, eight location and three
organization conflated name pairs which represent a
diverse set of names of politicians, countries, cities,
political parties and software companies. For four
conflated name pairs the data is balanced. For ex-
ample, there are 3800 examples in total for the con-
flated name Bill Clinton ? Tony Blair of which 1900
are for the underlying entity Bill Clinton and 1900
for Tony Blair. For the rest of the cases the data is
imbalanced. For example, there are 3344 examples
for the conflated name Yaser Arafat ? Bill Clinton of
which 1004 belong to Yaser Arafat and 2340 to Bill
Clinton. The balanced and imbalanced data also lets
us study whether LDA?s performance if affected by
the different sense distributions.
Next, we show in Table 2 the overall results from
the disambiguation process. For each name, we first
show the baseline score which is calculated as the
percentage of instances belonging to the most fre-
quent underlying entity over all instances of that
conflated name pair. For example, for the Bill Clin-
ton ? Tony Blair conflated name pair, the baseline
is 50% since both underlying entities have the same
number of examples. This baseline is equivalent to
a clustering method that would assign all of the con-
texts to exactly one cluster.
The second column corresponds to the results
achieved by the second order co-occurrence cluster-
ing approach of (Pedersen et al, 2006). This ap-
proach is considered as state-of-the-art in name dis-
crimination after numerous features like unigram,
bigram, co-occurrence and multiple clustering algo-
rithms were tested. We denote this approach in Table
2 as Pedersen and use it as a comparison. Note that
in this experiment (Pedersen et al, 2006) predefine
the exact number of clusters, therefore we also use
the exact number of senses for the LDA topics. The
third column shows the results obtained by our LDA
approach. The final two columns represent the dif-
ference between our LDA approach and the baseline
denoted as ?B , as well as the difference between
our LDA approach and those of Pedersen denoted as
?P . We have highlighted in bold the improvements
of LDA over these methods.
The obtained results show that for all experiments
independent of whether the name sense data was bal-
anced or imbalanced, LDA has a positive increase
over the baseline. For some conflated tuples like the
Spanish NATO?ETZIN, the improvement over the
baseline is 47%. For seventeen out of the twenty
name conflated pairs LDA has also improved upon
Pedersen. The improvements range from +1.29 to
+19.18.
Unfortunately, we are not deeply familiar with
Romanian to provide a detailed analysis of the con-
texts and the errors that occurred. However, we no-
ticed that for English, Spanish and Bulgarian often
the same context containing two or three of the con-
flated names is used multiple times. Imagine that
108
Category Name Distribution
ENGLISH
person/politician Bill Cinton ? Tony Blair 1900+1900=3800
person/politician Bill Clinton ? Tony Blair ? Ehud Barak 1900+1900+1900=5700
organization IBM ? Microsoft 2406+3401=5807
location/country Mexico ? Uganda 1256+1256=2512
location/country&state Mexico ? India ? California ? Peru 1500+1500+1500+1500=6000
SPANISH
person/politician Yaser Arafat ? Bill Clinton 1004+2340=3344
person/politician Juan Pablo II ? Boris Yeltsin 1447+1450=2897
organization OTAN (NATO) ? EZLN 1093+1093=2186
location/city New York ? Washington 1517+2418=3935
location/city&country New York ? Brasil ? Washington 1517+1748+2418=5863
ROMANIAN
person/politician Traian Basescu ? Adrian Nastase 1804+1932=3736
person/politician Traian Basescu ? Ion Illiescu ? Adrian Nastase 1948+1966+2301=6215
organization Romanian Democratic Party ? Socialist Party 2037+3264=5301
location/city Brasov ? Bucarest 2310+2559=4869
location/country France ? USA ? Romania 1370+2396+3890=7656
BULGARIAN
person/politician Petar Stoyanov ? Ivan Kostov ? Georgi Parvanov 318+524+811=1653
person/politician Nadejda Mihaylova ? Nikolay Vasilev ? Stoyan Stoyanov 645+849+976=2470
organization Bulgarian Socialist Party ? Union Democratic Forces 2921+4680=7601
location/country France ? Germany ?Russia 1726+2095+2645=6466
location/city Varna ? Bulgaria 1240+1261=2501
Table 1: Data Set Characteristics of the Name Discrimination Task.
Name Baseline Pedersen LDA ?B ?P
ENGLISH
Bill Cinton ? Tony Blair 50.00% 80.95% 81.13% +31.13 +0.18
Bill Clinton ? Tony Blair ? Ehud Barak 33.33% 47.93% 67.19% +33.86 +19.26
IBM ? Microsoft 58.57% 63.70% 65.44% +6.87 +1.74
Mexico ? Uganda 50.00% 59.16% 78.34% +28.35 +19.18
Mexico ? India ? California ? Peru 25.00% 28.78% 46.43% +21.43 +17.65
SPANISH
Yaser Arafat ? Bill Clinton 69.98% 77.72% 83.67% +13.69 +5.95
Juan Pablo II ? Boris Yeltsin 50.05% 87.75% 52.36% +2.31 -35.39
OTAN (NATO) ? EZLN 50.00% 69.81% 96.89% +46.89 +27.08
New York ? Washington 61.45% 54.66% 66.73% +5.28 +12.07
New York ? Brasil ? Washington 42.55% 42.88% 59.28% +16.73 +16.40
ROMANIAN
Traian Basescu ? Adrian Nastase 51.34% 51.34% 58.51% +7.17 +7.17
Traian Basescu ? Ion Illiescu ? Adrian Nastase 37.02% 39.31% 47.69% +10.67 +8.38
Romanian Democratic Party ? Socialist Party 61.57% 77.70% 61.57% 0.00 -16.13
Brasov ? Bucarest 52.56% 63.67% 64.96% +12.40 +1.29
France ? USA ? Romania 50.81% 52.66% 55.39% +4.58 +2.73
BULGARIAN
Petar Stoyanov ? Ivan Kostov ? Georgi Parvanov 49.06% 58.68% 57.96% +8.90 -0.72
Nadejda Mihaylova ? Nikolay Vasilev ? Stoyan Stoyanov 39.51% 59.39% 53.97% +14.46 -5.42
Bulgarian Socialist Party ? Union Democratic Forces 61.57% 57.31% 61.76% +0.19 +4.45
France ? Germany ?Russia 40.91% 41.60% 46.74% +5.83 +5.14
Varna ? Bulgaria 50.42% 50.38% 51.78% +1.36 +1.40
Table 2: Results on the Multilingual and Multi-category Name Discrimination Task.
109
there is a single context in which both names Nade-
jda Mihaylova and Stoyan Stoyanov are mentioned.
This context is used to create two name conflated
examples. In the first case only the name Nadejda
Mihaylova was hidden with the Nadejda Mihaylova
? Nikolay Vasilev ? Stoyan Stoyanov label while the
name Stoyan Stoyanov was preserved as it is. In
the second case, the name Stoyan Stoyanov was hid-
den with the label Nadejda Mihaylova ? Nikolay
Vasilev ? Stoyan Stoyanov while the name Nadejda
Mihaylova was preserved. Since the example con-
tains two name conflations of the same context, it
becomes very difficult for any algorithm to identify
this phenomenon and discriminate the names cor-
rectly.
According to a study conducted by (Pedersen et
al., 2006), the conflated entities in the automatically
collected data sets can be ambiguous and can be-
long to multiple semantic categories. For example,
they mention that the city Varna occurred in the col-
lection as part of other named entities such as the
University of Varna, the Townhall of Varna. There-
fore, by conflating the name Varna in the organiza-
tion named entity University of Varna, the context
starts to deviate the meaning of Varna as a city into
the meaning of university. Such cases transmit ad-
ditional ambiguity to the conflated name pair and
make the task even harder.
Finally, our current approach does not use stop-
words except for English. According to Pedersen et
al. (2006) the usage of stop-words is crucial for this
task and leads to a substantial improvement.
4.2 Web People Search
Recently, Artiles et al (2009b) introduced the Web
People Search task (WebPS), where given the top
100 web search results produced for an ambiguous
person name, the goal is to produce clusters that con-
tain documents referring to the same individual.
We have randomly selected from the WebPS-2
test data three names from the Wikipedia, ACL?08
and Census categories. Unlike the previous data,
WebPS has (1) names with higher ambiguity from
3 to 56 entities per name, (2) only person names and
(3) unstructured and semi-structured texts from the
Web and Wikipedia1. Table 3 shows the number of
1We clean all html tags and remove stopwords.
entities (senses) (#E) and the number of documents
for each ambiguous name (#Doc).
In contrast to the previous task where the number
of topics is equal to the exact number of senses, in
this task the number of topics is approximate to the
number of senses2. In our experiments we set the
number of topics to 40. We embarked on this exper-
imental set up in order to make our results compara-
ble with the rest of the systems in WebPS. However,
if we use the exact number of name senses then LDA
achieves higher results.
To evaluate the performance of our approach, we
use the official WebPS evaluation script. We re-
port BCubed Precision, Recall and F-scores for our
LDA approach, two baseline systems and the ECNU
(Lan et al, 2009) system from the WebPS-2 chal-
lenge. We compare our results against ECNL, be-
cause they use similar word representation but in-
stead of relying on LDA they use a clustering algo-
rithm. We denote in Table 3 the difference between
the F-score performances of LDA and the ECNU
system as ?F1 . We highlight the differences in bold.
Since a name disambiguation system must have
good precision and recall results, we decided to
compare our results against two baselines which rep-
resent the extreme case of a system that reaches
100% precision (called ONE-IN-ONE) or a sys-
tem that reaches 100% recall (called ALL-IN-ONE).
Practically ONE-IN-ONE corresponds to assign-
ing each document to a different cluster (individ-
ual sense), while the ALL-IN-ONE baseline groups
together all web pages into a single cluster corre-
sponding to one name sense (the majority sense). A
more detailed explanation about the evaluation mea-
sures and the intuition behind them can be found in
(Artiles et al, 2007) and (Artiles et al, 2009b).
For six out of the nine names, LDA outperformed
the two baselines and the ECNU system with 5 to
41% on F-score. Precision and recall scores for LDA
are comparable except for Tom Linton and Helen
Thomas where precision is much higher. The de-
crease in performance is due to the low number of
senses (entities associated with a name) and the fact
that LDA was tuned to produce 40 topics. To over-
come this limitation, in the future we plan to work
on estimating the number of topics automatically.
2Researchers use from 15 to 50 number of clusters/senses.
110
ONE-IN-ONE ALL-IN-ONE ECNU LDA
Name #E #Doc BEP BER F1 BEP BER F1 BEP BER F1 BEP BER F1 ?F1
Wikipedia Names
Louis Lowe 24 100 1.00 .32 .48 .23 1.00 .37 .39 .78 .52 .63 .52 .57 +5
Mike Robertson 39 123 1.00 .44 .61 .11 1.00 .19 .14 .96 .25 .59 .62 .61 +36
Tom Linton 10 135 1.00 .11 .19 .54 1.00 .70 .68 .48 .56 .89 .22 .35 -21
ACL ?08 Names
Benjamin Snyder 28 95 1.00 .51 .67 .08 1.00 .15 .16 .79 .27 .59 .81 .68 +41
Emily Bender 19 120 1.00 .21 .35 .24 1.00 .39 .45 .60 .51 .78 .42 .55 +4
Hao Zhang 24 100 1.00 .26 .41 .21 1.00 .35 .45 .78 .57 .72 .36 .48 -9
Census Names
Helen Thomas 3 127 1.00 .03 .06 .96 1.00 .98 .96 .24 .39 .97 .08 .15 -24
Jonathan Shaw 26 126 1.00 .32 .49 .10 1.00 .18 .18 .60 .34 .66 .51 .58 +24
Susan Jones 56 110 1.00 .70 .82 .03 1.00 .06 .13 .81 .22 .51 .79 .62 +40
Table 3: Results for Web People Search-2.
5 Conclusion
We have shown how ambiguity in names can be
modeled and resolved using a generative probabilis-
tic model. Our LDA approach learns a distribution
over topics which correspond to entities (senses) as-
sociated with an ambiguous name. We evaluate our
novel approach on two tasks: name discrimination
and Web People Search. We conduct a detailed eval-
uation on (1) Web, Wikipedia and news documents;
(2) English, Spanish, Romanian and Bulgarian lan-
guages; (3) people, location and organization names.
Our method achieves consistent performance and
substantial improvements over baseline and existing
state-of-the-art clustering methods.
In the future, we would like to model the bi-
ographical fact extraction approach of (Mann and
Yarowsky, 2003) in our LDA model. We plan to es-
timate the number of topics automatically from the
distributions. We want to explore variants of our
current model. For example, currently all words are
generated by multiple topics (senses), but ideally we
want them to be generated by a single topic. Finally,
we want to impose additional constraints within the
topic models using hierarchical topic models.
Acknowledgments
We acknowledge the support of DARPA contract
FA8750-09-C-3705 and NSF grant IIS-0429360.
References
Javier Artiles, Julio Gonzalo, and Satoshi Sekine. 2007.
The semeval-2007 weps evaluation: Establishing a
benchmark for the web people search task. In Pro-
ceedings of the Fourth International Workshop on Se-
mantic Evaluations (SemEval-2007, pages 64?69.
Javier Artiles, Enrique Amigo?, and Julio Gonzalo.
2009a. The role of named entities in Web People
Search. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing,
pages 534?542.
Javier Artiles, Julio Gonzalo, and Satoshi Sekine. 2009b.
WePS 2 evaluation campaign: overview of the web
people search clustering task. In 2nd Web Peo-
ple Search Evaluation Workshop (WePS 2009), 18th
WWW Conference.
Javier Artiles, Andrew Borthwick, Julio Gonzalo, Satoshi
Sekine, and Enrique Amigo?. 2010. WePS-3 evalu-
ation campaign: Overview of the web people search
clustering and attribute extraction ta. In Conference
on Multilingual and Multimodal Information Access
Evaluation (CLEF).
Amit Bagga and Breck Baldwin. 1998. Entity-based
cross-document coreferencing using the vector space
model. In Proceedings of the 36th Annual Meeting
of the Association for Computational Linguistics and
17th International Conference on Computational Lin-
guistics - Volume 1, ACL ?98, pages 79?85.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. J. Mach. Learn.
Res., 3:993?1022, March.
Razvan C. Bunescu and Marius Pasca. 2006. Using en-
cyclopedic knowledge for named entity disambigua-
tion. In EACL 2006, 11st Conference of the European
Chapter of the Association for Computational Linguis-
tics, Proceedings of the Conference.
Ying Chen and James H. Martin. 2007. Cu-comsem:
Exploring rich features for unsupervised web per-
sonal name disambiguation. In Proceedings of the
Fourth International Workshop on Semantic Evalua-
tions (SemEval-2007), pages 125?128, June.
111
Ying Chen, Sophia Yat Mei Lee, and Chu-Ren Huang.
2009. Polyuhk: A robust information extraction
system for web personal names. In 2nd Web Peo-
ple Search Evaluation Workshop (WePS 2009), 18th
WWW Conference.
Silviu Cucerzan. 2007. Large-scale named entity dis-
ambiguation based on wikipedia data. In EMNLP-
CoNLL 2007, Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing, pages 708?716.
Hal Daume? III and Daniel Marcu. 2006. Bayesian query-
focused summarization. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and 44th Annual Meeting of the Association for Com-
putational Linguistics, pages 305?312, Sydney, Aus-
tralia, July. Association for Computational Linguistics.
Filip Ginter, Jorma Boberg, Jouni Ja?rvinen, and Tapio
Salakoski. 2004. New techniques for disambiguation
in natural language and their application to biological
text. J. Mach. Learn. Res., 5:605?621, December.
Thomas L Griffiths and Mark Steyvers. 2002. A prob-
abilistic approach to semantic representation. In Pro-
ceedings of the Twenty-Fourth Annual Conference of
Cognitive Science Society.
Thomas L Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences of the United States of America,
101 Suppl 1(Suppl 1):5228?5235.
Aria Haghighi and Dan Klein. 2010. Coreference reso-
lution in a modular, entity-centered model. In Human
Language Technologies: The 2010 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics, pages 385?393.
Zellig Harris. 1954. Distributional structure.
10(23):146?162.
Vasileios Hatzivassiloglou, Pablo A. Duboue, and An-
drey Rzhetsky. 2001. Disambiguating proteins, genes,
and rna in text: A machine learning approach. In Pro-
ceedings of the 9th International Conference on Intel-
ligent Systems for Molecular Biology.
Man Lan, Yu Zhe Zhang, Yue Lu, Jian Su, and Chew Lim
Tan. 2009. Which who are they? people attribute ex-
traction and disambiguation in web search results. In
2nd Web People Search Evaluation Workshop (WePS
2009), 18th WWW Conference.
Gideon S. Mann and David Yarowsky. 2003. Unsuper-
vised personal name disambiguation. In Proceedings
of the seventh conference on Natural language learn-
ing at HLT-NAACL 2003 - Volume 4, CONLL ?03,
pages 33?40.
Matthias Blume Matthias. 2005. Automatic entity dis-
ambiguation: Benefits to ner, relation extraction, link
analysis, and inference. In International Conference
on Intelligence Analysis.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://www.cs.umass.edu/ mccallum/mallet.
Hien T. Nguyen and Tru H. Cao. 2008. Named entity dis-
ambiguation: A hybrid statistical and rule-based incre-
mental approach. In Proceedings of the 3rd Asian Se-
mantic Web Conference on The Semantic Web, ASWC
?08, pages 420?433.
Ted Pedersen and Anagha Kulkarni. 2007. Unsuper-
vised discrimination of person names in web contexts.
In Computational Linguistics and Intelligent Text Pro-
cessing, 8th International Conference, CICLing 2007,
pages 299?310.
Ted Pedersen, Amruta Purandare, and Anagha Kulka-
rni. 2005. Name discrimination by clustering simi-
lar contexts. In Computational Linguistics and Intel-
ligent Text Processing, 6th International Conference,
CICLing 2005, pages 226?237.
Ted Pedersen, Anagha Kulkarni, Roxana Angheluta, Zor-
nitsa Kozareva, and Thamar Solorio. 2006. An
unsupervised language independent method of name
discrimination using second order co-occurrence fea-
tures. In Computational Linguistics and Intelligent
Text Processing, 7th International Conference, CI-
CLing 2006, pages 208?222.
Octavian Popescu and Bernardo Magnini. 2007. Irst-bp:
Web people search using name entities. In Proceed-
ings of the Fourth International Workshop on Semantic
Evaluations (SemEval-2007, pages 195?198. Associa-
tion for Computational Linguistics.
Joseph Reisinger and Marius Pasca. 2009. Latent vari-
able models of concept-attribute attachment. In Pro-
ceedings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the
AFNLP, pages 620?628. Association for Computa-
tional Linguistics, August.
Alan Ritter, Mausam, and Oren Etzioni. 2010. A latent
dirichlet alocation method for selectional preferences.
In Proceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 424?434.
Association for Computational Linguistics, July.
Yang Song, Jian Huang, Isaac G. Councill, Jia Li, and
C. Lee Giles. 2007. Efficient topic-based unsuper-
vised name disambiguation. In Proceedings of the 7th
ACM/IEEE-CS Joint Conference on Digital libraries,
pages 342?351.
Xiaojun Wan, Jianfeng Gao, Mu Li, and Binggong Ding.
2005. Person resolution in person search results: Web-
hawk. In Proceedings of the 14th ACM international
conference on Information and knowledge manage-
ment, CIKM ?05, pages 163?170.
112
