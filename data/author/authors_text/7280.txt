Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 79?88,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Scaling Textual Inference to the Web
Stefan Schoenmackers, Oren Etzioni, and Daniel S. Weld
Turing Center
University of Washington
Computer Science and Engineering
Box 352350
Seattle, WA 98195, USA
stef,etzioni,weld@cs.washington.edu
Abstract
Most Web-based Q/A systems work by find-
ing pages that contain an explicit answer to
a question. These systems are helpless if the
answer has to be inferred from multiple sen-
tences, possibly on different pages. To solve
this problem, we introduce the HOLMES sys-
tem, which utilizes textual inference (TI) over
tuples extracted from text.
Whereas previous work on TI (e.g., the lit-
erature on textual entailment) has been ap-
plied to paragraph-sized texts, HOLMES uti-
lizes knowledge-based model construction to
scale TI to a corpus of 117 million Web pages.
Given only a few minutes, HOLMES doubles
recall for example queries in three disparate
domains (geography, business, and nutrition).
Importantly, HOLMES?s runtime is linear in
the size of its input corpus due to a surprising
property of many textual relations in the Web
corpus?they are ?approximately? functional
in a well-defined sense.
1 Introduction and Motivation
Numerous researchers have identified the Web as
a rich source of answers to factual questions, e.g.,
(Kwok et al, 2001; Brill et al, 2002), but often the
desired information is not stated explicitly even in a
textual corpus as massive as the Web. Consider the
question ?What vegetables help prevent osteoporo-
sis?? Since there is likely no sentence on the Web
directly stating ?Kale prevents osteoporosis?, a sys-
tem must infer that kale is an answer by combining
facts from multiple sentences, possibly from differ-
ent pages, which justify that conclusion: i.e., that
kale is a vegetable, kale contains calcium, and cal-
cium helps prevent osteoporosis.
Figure 1: The architecture of HOLMES.
Textual Inference (TI) methods have advanced in
recent years. For example, textual entailment tech-
niques aim to determine whether one textual frag-
ment (the hypothesis) follows from another (the text)
(Dagan et al, 2005). While most TI researchers have
focused on high-quality inferences from a small
source text, we seek to utilize sizable chunks of the
Web corpus as our source text. In order to do this,
we must confront two major challenges. The first is
uncertainty: TI is an imperfect process, particularly
when applied to the Web corpus, hence probabilistic
methods help to assess the confidence in inferences.
The second challenge is scalability: how does infer-
ence time scale given increasingly large corpora as
input?
1.1 HOLMES: A Scalable TI System
This paper describes HOLMES, an implemented sys-
tem, which addresses both challenges by carrying
out scalable, probabilistic inference over ground
assertions extracted from the Web. The input to
HOLMES is a conjunctive query, a set of inference
rules expressed as Horn clauses, and large sets of
ground assertions extracted from theWeb, WordNet,
and other knowledge bases. As shown in Figure 1,
HOLMES chains backward from the query, using the
inference rules to construct a forest of proof trees
from the ground assertions. This forest is converted
79
into a Markov network (a form of Knowledge-
Based Model Construction (KBMC) (Wellman et
al., 1992)) and evaluated using approximate prob-
abilistic inference. HOLMES operates in an anytime
fashion ? if desired it can keep iterating: search-
ing for more proofs, and elaborating the Markov net-
work.
HOLMES makes some important simplifying as-
sumptions. Specifically, we use simple ground
tuples to represent extracted assertions (e.g.,
contains(kale, calcium)). Syntactic prob-
lems (e.g., anaphora, relative clauses) and seman-
tic challenges (e.g., quantification, counterfactuals,
temporal qualification) are delegated to the extrac-
tion system or simply ignored. This paper focuses
on scalability for this subset of the TI task.
1.2 Summary of Experimental Results
We tested HOLMES on 183 million distinct ground
assertions extracted from the Web by the TEX-
TRUNNER system (Banko et al, 2007), coupled
with 159 thousand ground assertions from Word-
Net (Miller et al, 1990), and a compact set of hand-
coded inference rules. Given a total of 55 to 145
seconds, HOLMES was able to produce high-quality
inferences that doubled the number of answers to
example queries in three disparate domains: geog-
raphy, business, and nutrition.
We also evaluated how the speed of HOLMES
scaled with the size of its input corpus. In the
general case, logical inference over a Horn theory
(needed in order to produce the probabilistic net-
work) is polynomial in the number of ground asser-
tions, and hence in the size of the textual corpus.1
Unfortunately, this is prohibitive, since even low-
order polynomial growth is fatal on a 117 million-
page corpus, let alne the full Web.
1.3 Why HOLMES Scales Linearly
Fortunately, the Web?s long tail works in our favor.
The relations we extract from text are approximately
pseudo-functional (APF), as we formalize in Sec-
tion 3, and this property leads to runtime that scales
linearly with the corpus. To see the underlying in-
tuition, consider the APF relation denoted by the
phrase ?is married to;? most of the time it maps a
person?s name to a small number of spousal names
1In fact, it is P-complete ? as hard as any polynomial-time
problem.
so this relation is APF. Section 3 shows why this
APF property ensures linear scaling, and Section 4
demonstrates linear scaling in practice.
2 An Overview of HOLMES
HOLMES is a system designed to answer complex
queries over large, noisy knowledge bases. As a mo-
tivating example, we consider the question ?What
vegetables help prevent osteoporosis?? As of this
writing, Google has no pages explicitly stating ?kale
helps prevent osteoporosis?, making it challenging
to return ?kale? as an answer. However, there are
numerous web pages stating that ?kale is high in cal-
cium? and others declaring that ?calcium helps pre-
vent osteoporosis?. If we could combine those facts
we could easily infer that ?kale? is an answer to the
question ?What vegetables help prevent osteoporo-
sis?? HOLMES was designed to make such infer-
ences while accounting for uncertainty in the pro-
cess.
Given a query, expressed as a conjunctive
Datalog rule, HOLMES generates a probabilistic
model using knowledge-based model construction
(KBMC) (Wellman et al, 1992). Specifically,
HOLMES utilizes fast, logical inference to find the
subset of ground assertions and inference rules that
may influence the answers to the query ? enabling
the construction of a small and focused Markov net-
work. Since this graphical model is much smaller
than one incorporating all ground assertions, prob-
abilistic inference will be much faster than if naive
compilation were used.
Figure 1 summarizes the operation of HOLMES.
As with many theorem provers or KBMC systems,
HOLMES takes three inputs:
1. A set of knowledge bases ? databases of
ground relational assertions, each with an
estimate of its probability, which can be
generated by TextRunner (Banko et al,
2007) or Kylin (Wu and Weld, 2007). In
our example, we would extract the as-
sertions IsHighIn(kale, calcium) and
Prevents(calcium, osteoporosis) from
those sentences.
2. A domain theory ? A set of probabilis-
tic inference rules written as Markov logic
Horn clauses, which can be used to de-
rive new assertions. The weight associ-
ated with each clause specifies its reliability.
80
kaleis high incalcium(TextRunner : 0.39)kaleis high inmagnesium(TextRunner : 0.39) magnesiumhelps preventosteoporosis(TextRunner : 0.39) calciumhelps preventosteoporosis(TextRunner : 0.68) broccoliis high incalcium(TextRunner : 0.39)
kalehelps preventosteoporosis(Inferred : 0.88) broccolihelps preventosteoporosis(Inferred : 0.49)kaleIS-Avegetable(WordNet : 0.9) broccoliIS-Avegetable(WordNet : 0.9)Inf. Rule:Transitive-Throughhigh in Inf. Rule:Transitive-Throughhigh in Inf. Rule:Transitive-Throughhigh in
kale matches the query(Inferred : 0.91) broccoli matches the query(Inferred : 0.58)Query Result Query Result
Figure 2: Partial proof ?tree? (DAG) for the query ?What
vegetables help prevent osteoporosis?? Rectangles de-
pict ground assertions from a knowledge base, rounded
boxes are inferred assertions, and shaded squared repre-
sent the application of inference rules. HOLMES converts
this DAG into a Markov network in order to estimate the
probability of each node.
In Section 2.3 we identify several domain-
independent rules, but a user may (optionally)
specify additional, domain-specific rules if de-
sired. In our example, we assume we are given
the domain-specific rule: Prevents(X,Z) :-
IsHighIn(X,Y) ? Prevents(Y,Z)
3. A conjunctive query is specified as a Datalog
rule. For example, the question ?What vegeta-
bles help prevent osteoporosis?? could be writ-
ten as: query(X) :- IS-A(X,Vegetable)
? Prevents(X,osteoporosis)
and returns a set of answers to the query, each with
an associated probability.
2.1 Basic Operation
To find these answers and their associated proba-
bilities, HOLMES first finds all ground assertions in
the knowledge bases that are potentially relevant to
the query. This is efficiently done using the infer-
ence rules to chain backwards from the query. Note
that the generated candidate answers, themselves,
are less important than the associated proof trees.
Furthermore, since HOLMES uses these ?trees? (ac-
tually, DAGs) to generate a probabilistic graphical
model, HOLMES seeks to find as many proof trees
as possible for each query result ? each may influ-
ence the final belief in that result. Figure 2 shows a
partial proof tree for our example query.
To handle uncertainty, HOLMES now constructs a
ground Markov network from the proof trees and the
Markov-logic-encoded inference rules. Markov net-
works (Pearl, 1988) model the joint distribution of a
set of variables by creating an undirected graph with
one node for each random variable, and represent-
ing dependencies between variables with cliques in
the graph. Each clique has a corresponding poten-
tial function ?k, which returns a non-negative value
based on the state of variables in the clique. The
probability of a state, x, is given by
P (x) =
1
Z
?
?k(x{k})
where the partition functionZ is a normalizing term,
and x{k} denotes the state of all the variables in
clique k.
HOLMES converts the proof trees into a Markov
network in a manner pioneered by the Markov Logic
framework of Richardson and Domingos (2006). A
Boolean variable is created to represent the truth of
each assertion in the proof forest. Next, HOLMES
adds edges to the Markov network to create a clique
corresponding to each application of an inference
rule in the proof forest.
Following the Markov Logic framework, the po-
tential function of a clique has form ?(x) = ew if all
member nodes are true (w denotes the weight of the
inference rule), and ?(x) = 1 otherwise. The proba-
bilities of leaf nodes are derived from the underlying
knowledge base,2 and inferred nodes are biased with
an exponential prior.
Finally, HOLMES computes the approximate
probability of each answer by running a variant
of loopy belief propagation (Pearl, 1988) over the
Markov network. In our experience this method
performs well on networks derived from our Horn
clause proof forest, but one could use Monte Carlo
techniques or even exact methods if desired.
Note that this architecture allows HOLMES to
combine information from multiple web pages to in-
fer assertions not explicitly seen in the textual cor-
pus. Because this inference is done using a Markov
network, it correctly handles uncertain extractions
and probabilistic dependencies. By using KBMC to
create a custom, focused network for each query, the
2In our experiments, ground assertions from WordNet get
a uniformly high probability of correctness (0.9), but those ex-
tracted from the Web are assigned probabilities derived from
redundancy statistics, following the intuition that frequently ex-
tracted facts are more likely to be true (Etzioni et al, 2005).
81
amount of probabilistic inference is reduced to man-
ageable proportions.
2.2 Anytime, Incremental Expansion
Because exact probabilistic inference is #P-
complete, HOLMES uses approximate methods, but
even these techniques have problems if the Markov
network gets too large. As a result, HOLMES creates
the network incrementally. After the first proof trees
are generated, HOLMES creates the model and per-
forms approximate probabilistic inference. If more
time is available then HOLMES searches for addi-
tional proof trees and updates the network (Fig-
ure 1). This incremental process allows HOLMES
to return initial results (with preliminary probability
estimates) as soon as they are discovered.
For efficiency, HOLMES exploits standard Data-
log optimizations (e.g., it only expands proofs of re-
cently added nodes and it uses an approximation to
magic sets (Ullman, 1989), rather than simple back-
wards chaining). For tractability, we also allow the
user to limit the number of transitive inference steps
for any inference rule.
HOLMES also includes a few enhancements for
dealing with information extracted from natural lan-
guage. For example, HOLMES?s inference rules sup-
port substring/regex matching of ground assertions,
to accommodate simple variations in text. HOLMES
also can be restricted to only operate over proper
nouns, which is useful for queries involving named
entities.
2.3 Markov Logic Inference Rules
HOLMES is given the following set of six domain-
independent rules, which are similar to the up-
ward monotone rules introduced by (MacCartney
and Manning, 2007).
1. Observed relations are likely to be true:
R(X,Y) :- ObservedInCorpus(X, R, Y)
2. Synonym substitution preserves meaning:
RTR(X?,Y) :- RTR(X,Y) ? Synonym(X, X?)
3. RTR(X,Y?) :- RTR(X,Y) ? Synonym(Y, Y?)
4. Generalizations preserve meaning:
RTR(X?,Y) :- RTR(X,Y) ? IS-A(X, X?)
5. RTR(X,Y?) :- RTR(X,Y) ? IS-A(Y, Y?)
6. Transitivity of Part Meronyms:
RTR(X,Y?) :- RTR(X,Y) ? Part-Of(Y, Y?)
where RTR matches ?* in? (e.g., ?born in?).
For example, if Q(X):-BornIn(X,?France?),
and we know from WordNet that Paris is in
France, then by inference rule 6, we know that
BornIn(X,?Paris?) will yield valid results for
Q(X). Although all of these rules contain at most
two relations in the body, HOLMES allows an
arbitrary number of relations in the query and rule
bodies. However, we have found that even simple
rules can dramatically improve some queries.
We set the rule weights to capture the intuition
that deeper inferences decrease the likelihood (as
there are more chances to make mistakes), whereas
additional, independent proof trees increase the
likelihood (as there is more supporting evidence).
Specifically, in our experiments we set the prior on
inferred facts to -0.75, the weight on rule 1 to 1.5,
and the weights on all other rules to 0.6.
At present, we define these weights manually, but
we expect to learn the parameter values in the future.
3 Scaling Inference to the Web
If TI is applied to a corpus containing hundreds of
millions or even billions of pages, its run time has to
be at most linear in the size of the corpus. This sec-
tion shows that under some reasonable assumptions
inference does scale linearly.
We start our analysis with two simplifications.
First, we assume that the number of distinct, ground
assertions in the KBs, |A|, grows at most linearly
with the size of the textual corpus. This is cer-
tainly true for assertions extracted by TextRunner
and Kylin, and follows from our exclusion of texts
with complex quantified sentences. Our analysis
now proceeds to consider scaling with respect to |A|
for a fixed query and set of inference rules.
Our second assumption is that the size of every
proof tree is bounded by some constant, m. This
is a strong assumption and one that depends on the
precise set of inference rules and pattern of ground
assertions. However, it holds in our experience, and
if necessary could be enforced by terminating the
search for proof trees at a certain depth, e.g., log(m).
HOLMES?s knowledge-based model construction
has two parts: construction of the proof forest and
conversion of the forest into a Markov network.
Since the Markov network is essentially isomorphic
to the proof forest, the conversion will be O(|A|) if
the forest is linear in size, which is ensured if the
time to construct the proof trees isO(|A|). We show
82
this in the remainder of this section.
Recall that HOLMES requires inference rules to
be function-free Horn clauses. While this limits ex-
pressivity to some degree, it provides a huge speed
benefit ? logical inference over Horn clauses can
be done in polynomial time, whereas general propo-
sitional inference (i.e., from grounded first-order
rules) is NP-complete.
Alas, even low-order polynomial blowup is un-
acceptable when the textual corpus reaches Web
scale; we seek linear growth. Intuitively, there are
two places where polynomial expansion could cause
trouble. First, the number of different types of proofs
(i.e., first order proofs) could grow too quickly, and
secondly, a given type of proof tree might apply
to too many ground assertions (?tuples? in database
lingo). We treat these issues in turn.
Under our assumptions, each proof tree can be
represented as an expression in relational algebra
with at most m equijoins (Ullman, 1989),3 each
stemming from the application of an inference rule.
Since the number of rules is fixed, as is m, there are
a constant number of possible first-order proof trees.
The bigger concern is that any one of these first-
order trees might result in a polynomial number of
ground trees; if so, the size of the ground forest
(and corresponding Markov network) could grow
too quickly. In fact, polynomial growth is a common
phenomena in database query evaluation. Luckily,
most relations in the Web corpus behave more fa-
vorably. We introduce a property of relations that
ensures m-way joins, and therefore all proof trees
up to size m, can be computed in O(|A|) time.
The intuition is that most relations derived from
large corpora have a ?heavy-tailed? distribution,
wherein a few objects appear many times in a rela-
tion, but most appear only once or twice, thus joins
involving rare objects lead to a small number of re-
sults, and so the main limitation on scalability is
common objects. We now prove that if these com-
mon objects account for a small enough fraction of
the relation, then joins will still scale linearly. We
focus on binary relations, but these results can eas-
ily be extended to relations of larger arity.
3Note that an inference rule of the form H(X) :-
R1(X,Y),R2(Y,Z) is equivalent to the algebraic expression
piX(R1 ./ R2). First a join is performed between R1 and R2
testing for equality between values of Y ; then a projection elim-
inates all columns besides X .
Definition 1 A relation, R = {(xi, yi)} ? X ?
Y , is pseudo-functional (PF) in x with degree k, if
?x ? X : |{y|(x, y) ? R}| ? k. When the precise
variable and degree is irrelevant to discussion, we
simply say ?R is PF.?
An m-way equijoin over relations that are PF in
the join variables will have at most km ? |R| results.
Since km is constant for a given join and |R| scales
linearly in the size of the textual corpus, proof tree
construction over PF relations also scales linearly.
However, due to their heavy-tailed distributions,
most relations extracted from theWeb fit the pseudo-
functional definition in most, but not all values of
X . Fortunately, it turns out that in most cases these
?bad? values ofX are rare and hence don?t influence
the join size significantly. We formalize this intu-
ition by defining a class of approximately pseudo-
functional (APF) relations and proving that joining
two APF relations produces at most a linear number
of results.
Definition 2 A relation, R, is approximately
pseudo-functional (APF) in x with degree k, if X
can be partitioned into two sets XG and XB such
that for all x ? XG R is PF with degree k and?
x?XB
|{y|(x, y) ? R}| ? k ? log(|R|)
Theorem 1. If relation R1 is APF in y with de-
gree k1 and R2 is APF in y with degree k2 then
the relation Q = R1 ./ R2 has size at most
O(max(|R1|, |R2|)).
Proof. Since R1 and R2 are APF, we know that
Y can be partitioned into four groups: YBB =
YB1
?
YB2, YBG = YB1
?
YG2, YGB = YG1
?
YB2,
YGG = YG1
?
YG2.4 We can show that each group
leads to at most O(|A|) entries in Q. For y ? YBB
there are at most k1 ? k2 ? log(|R1|) ? log(|R2|) en-
tries in Q. The y ? YGB and y ? YBG lead to at
most k1 ? k2 ? log(|R2|) and k1 ? k2 ? log(|R1|)
entries, respectively. For y ? YGG there are at
most k1 ? k2 ? max(|R1|, |R2|). Summing the re-
sults from the four partitions, we see that |Q| is
O(max(|R1|, |R2|)), thus it is O(|A|).
This theorem and proof can easily be extended to
4YBB are the ?doubly bad? values of y that violate the PF
definition for both relations, YGG are the values that do not vio-
late the PF definition for either relation, and YBG and YGB are
the values that violate it in only R1 or R2, resp.
83
an m-way equijoin, as long as each relation is APF
in all arguments that are being joined.
Theorem 2. IfQ is the relation obtained by an equi-
join over m relationsR1..m, each having size at most
O(|A|), and if all R1..m are APF in all arguments
that they are joined in with degree at most kmax, and
if
?
1?i?m
log(|Ri|) ? |A|, then |Q| is O(|A|).
The inequality in Theorem 2 relates the sizes of
the relations (|R|), the join (m) and the number of
ground assertions (|A|). However, in many cases we
are interested in much smaller values of m than the
inequality enables. We can relax the APF definition
to allow a broader, but still scalable, class ofm-way-
APF relations.
Corollary 3. If Q is the relation obtained by an m-
way join, and if each participating relation is APF
in their joined variables with a bound of ki ? m
?
|Ri|
instead of ki ? log(|Ri|), then the join is O(|A|).
The final step in our scaling argument concerns
probabilistic inference, which is #P-Complete if per-
formed exactly. This is addressed in two ways. First,
HOLMES uses approximate methods, e.g., loopy be-
lief propagation, which avoids the cost of exact in-
ference ? at the cost of reduced precision. Sec-
ondly, at a practical level, HOLMES?s incremental
construction of the graphical model (Figure 1) al-
lows it to bound the size of the network by terminat-
ing the search for additional proofs.
4 Experimental Results
This section reports on measurements that confirm
that linear scaling with |A| occurs in practice, and
that HOLMES?s inference is not only scalable but
also improves precision/recall on sample queries in
a diverse set of domains. After describing the exper-
imental domains and queries, Section 4.2 reports on
the boost to the area under the precision/recall curve
for a set of example queries in three domains: ge-
ography, business, and nutrition. Section 4.3 then
shows that APF relations are very common in the
Web corpus, and finally Section 4.4 demonstrates
empirically that HOLMES?s inference time scales
linearly with the number of pages in the corpus.
4.1 Experimental Setup
HOLMES utilized two knowledge bases in these ex-
periments: TEXTRUNNER and WordNet. TEX-
TRUNNER contains approximately 183 million dis-
tinct ground assertions extracted from over 117 mil-
lion web pages, and WordNet contains 159 thousand
manually created IS-A, Part-Of, and Synonym asser-
tions.
In all queries, HOLMES utilizes the domain-
independent inference rules described in Sec-
tion 2.3. HOLMES additionally makes use of two
domain-specific inference rules in the Nutrition
domain, to demonstrate the benefits of including
domain-specific information. Estimating the preci-
sion and relative recall of HOLMES requires exten-
sive and careful manual tagging of HOLMES output.
To make this feasible, we restricted ourselves to a
set of twenty queries in three domains, but made the
domains diverse to illustrate the broad scope of the
system.
We now describe each domain briefly.
Geography: the query issued is: ?Who was born in
one of the following countries?? More formally,
Q(X) :- BornIn(X,{country}) where {country}
is bound to each of the following nine countries
in turn {France, Germany, China, Thailand, Kenya,
Morocco, Peru, Columbia, Guatemala}, yielding a
total of nine queries.
Because Web text often refers to a person?s
birth city rather than birth country, this query il-
lustrates how combining an ground assertion (e.g.,
BornIn(Alberto Fujimori, Lima)) with back-
ground knowledge (e.g., LocatedIn(Lima, Peru))
enables the system to draw new conclusions (e.g.,
BornIn(Alberto Fujimori, Peru)).
Business: we issued the following two queries.
1) Which companies are acquiring software com-
panies? Formally, Q(X) :- Acquired(X, Y)
? Develops(Y, ?software?) This query tests
HOLMES?s ability to scalably join a large number of
assertions from multiple pages.
2) Which companies are headquartered in the
USA? Q(X) :- HeadquarteredIn(X, ?USA?)
? IS-A(X, ?company?)
Answering this query comprehensively requires
HOLMES to combine a join (over the relations Head-
quarteredIn and IS-A) with transitive inference on
PartOf (e.g., Seattle is PartOf Washington which is
PartOf the USA) and on IS-A (e.g., Microsoft IS-A
software company which IS-A company). The IS-
A assertions came from both TEXTRUNNER (using
patterns from (Hearst, 1992)) and WordNet.
84
0
0.2
0.4
0.6
0.8
1
0 1000 2000 3000 4000 5000Estimated Recall
Precis
ion
BaselineHolmes Increase in AuC
Figure 3: PR Curve for BornIn(X, {country}). Inference
boosts the Area under the PR Curve (AuC) by 102 %.
Domain Increase Total Inference
in AuC Time
Geography +102% 55 s
Business +2,643% 145 s
Nutrition +5,595% 64 s
Table 1: Improvement in the AuC of HOLMES over the
BASELINE and total inference time taken by HOLMES.
Results are summed over all queries in the geography,
business, and nutrition domains. Inference time mea-
sured on unoptimized prototype.
Nutrition: the nine queries issued are instances
of ?What foods prevent disease?? Where a food is
a member of one of the classes: fruit, vegetable, or
grain, and a disease is one of: anemia, scurvy, or
osteoporosis. More formally, Q(X, {disease}) :-
Prevents(X, {disease}) ? IS-A(X, {food})
Our experiments in the nutrition domain utilized
two domain-specific inference rules in addition to
the ones presented in Section 2.3:
Prevents(X,Y):-HighIn(X,Z) ? Prevents(Z,Y)
Prevents(X,Y):-Contains(X,Z) ? Prevents(Z,Y)
4.2 Effect of Inference on Recall
To measure the cost and benefit of HOLMES?s in-
ference we need to define a baseline for compar-
ison. Answering the conjunctive queries in the
business and nutrition domains requires computing
joins, which TEXTRUNNER does not do. Thus, we
defined a baseline system, BASELINE, which has
access to the underlying Knowledge Bases (KBs)
(TEXTRUNNER and WordNet), and the ability to
compute joins using information explicitly stated in
either KB, but does not have the ability to infer new
assertions.
We compared HOLMES with BASELINE in all
three domains. Figure 3 depicts the combined pre-
cision/relative recall curves for the nine Geography
queries. HOLMES yields substantially higher re-
call (the shaded region) at modestly lower preci-
sion, doubling the area under the precision/recall
curve (AuC). The other precision/recall curves also
showed a slight drop in precision for substantial
gains in recall. Table 1 summarizes the results, along
with the total runtime needed for inference. Because
relations in the business domain are much larger
than in the other domains (i.e., 100x ground asser-
tions), inference is slower in this domain.
We note that inference is particularly helpful with
rarely mentioned instances. However, inference can
lead to errors when the proof tree contains joins on
generic terms (e.g., ?company?) or common extrac-
tion errors (e.g., ?LLC? as a company name). This
is a key area for future work.
4.3 Prevalence of APF Relations
To determine the prevalence of APF relations inWeb
text, we examined a sample of 500 binary relations
selected randomly from TEXTRUNNER?s ground as-
sertions. The surface forms of the relations and ar-
guments may misrepresent the true properties of the
underlying concepts, so to better estimate the true
properties we merged synonymous values as given
by Resolver (Yates and Etzioni, 2007) or the most
frequent sense of the word in WordNet. For exam-
ple, we would consider BornIn(baby, hospital)
and BornAt(infant, infirmary) to represent the
same concept, and so would merge them into one
instance of the ?Born In? relation. The largest two re-
lations had over 1.25 million unique instances each,
and 52% of the relations had more than 10,000 in-
stances.
For each relation R, we first found all instances
of R extracted by TEXTRUNNER and merged all
synonymous instances as described above. Then,
for each argument of R we computed the smallest
value, Kmin, such that R is APF with degree Kmin.
Since many interesting assertions can be inferred by
simply joining two relations, we also considered the
special case of 2-way joins using Corollary 3. We
computed the smallest value, K2./, such that the re-
lation is two-way-APF with degree K2./.
Figure 4 shows the fraction of relations with
Kmin andK2./ of at mostK as a function of varying
85
0%
20%
40%
60%
80%
100%
0 1000 2000 3000 4000 5000 6000Degree of Approximate Pseudo-Functionality
APFAPF for two-way join
Figure 4: Prevalence of APF relations in Web text. The
x-axis depicts the degree of pseudo-functionality, e.g.,
Kmin and K2./, (see definition 2); the y-axis lists the
percent of relations that are APF with that degree. Re-
sults are averaged over both arguments.
values of K. The results are averaged over both ar-
guments of each binary relation. For arbitrary joins
in this KB, 80% of the relations are APF with de-
gree less than 496; for 2-way joins (like the ones in
our inference rules and test queries), 80% of the rela-
tions are APF with degree less than 65. These results
indicate that the majority of relations TEXTRUNNER
extracted from text are APF, and so we can expect
HOLMES?s techniques will allow efficient inference
over most relations.
While Theorem 2 guarantees that joins over those
relations will beO(|R|), that notation hides a poten-
tially large constant factor of Kminm. Fortunately
the constant factor is significantly smaller in prac-
tice. To see why, we re-examine the proof: the large
factor comes from assuming that all of R?s first ar-
guments which meet the PF definition are associated
with exactly Kmin distinct second arguments. How-
ever, in our corpus 83% of first arguments are as-
sociated with only one second argument. Clearly,
our worst-case analysis substantially over-estimates
inference time for most queries. Moreover, in ad-
ditional experiments (omitted due to space limita-
tions), measured join sizes grew linearly in the size
of the corpus, but were on average two to three or-
ders of magnitude smaller than the bounds given in
the theory. This observation held across relations
with different sizes and values of Kmin.
While the results in Figure 4 may vary for other
sets of relations, we believe the general trends
hold. This is promising for Question Answering and
Textual Inference systems, since if true it implies
R2 = 0.9881
R2 = 0.9808
R2 = 0.9931
020
4060
80100
120140
160
0% 20% 40% 60% 80% 100%Fraction of Corpus
GeographyBusinessNutrition
Figure 5: The effects of corpus size on total inference
time. We see approximately linear growth in all domains,
and display the best fit lines and coefficient of determina-
tion (R2) of each.
that combining information frommultiple difference
source is feasible, and can allow such systems to in-
fer answers not explicitly seen in any source.
4.4 Scalability of Inference Speed
Since the previous subsection showed that most re-
lations are APF in their arguments, our theory pre-
dicts HOLMES?s inference will scale linearly. We
tested this hypothesis empirically by running infer-
ence over the test queries in our three domains, while
varying the number of pages in the textual corpus.
Figure 5 shows how the inference time HOLMES
used to answer all queries in each domain scales
with KB size. For these queries, and several oth-
ers we tested (not shown here), inference time grows
linearly with the size of the KB. Based on these re-
sults we believe that HOLMES can provide scalable
inference over a wide variety of domains.
5 Related Work
Textual Entailment systems are given two textual
fragments, text T and hypothesis H , and attempt to
decide if the meaning of H can be inferred from
the meaning of T (Dagan et al, 2005). While
many approaches have addressed this problem, our
work is most closely related to that of (Raina et al,
2005; MacCartney and Manning, 2007; Tatu and
Moldovan, 2006; Braz et al, 2005), which convert
the inputs into logical forms and then attempt to
?prove? H from T plus a set of axioms. For in-
stance, (Braz et al, 2005) represents T , H , and a
set of rewrite rules in a description logic framework,
and determines entailment by solving an integer lin-
86
ear program derived from that representation.
These approaches and related ones (e.g.,
(Van Durme and Schubert, 2008)) use highly
expressive representations, enabling them to ex-
press negation, temporal information, and more.
HOLMES?s representation is much simpler?
Markov Logic Horn Clauses for inference rules
coupled with a massive database of ground asser-
tions. However, this simplification allows HOLMES
to tackle a ?text? of enormously larger size: 117
million Web pages versus a single paragraph. A sec-
ond, if smaller, difference stems from the fact that
instead of determining whether a single hypothesis
sentence, H , follows from the text, HOLMES tries to
find all consequents that match a conjunctive query.
HOLMES is also related to open-domain question-
answering systems such as Mulder (Kwok et al,
2001), AskMSR (Brill et al, 2002), and others
(Harabagiu et al, 2000; Brill et al, 2001). How-
ever, these Q/A systems attempt to find individual
documents or sentences containing the answer. They
often perform deep analysis on promising texts, and
back off to shallower, less reliable methods if those
fail. In contrast, HOLMES utilizes TI and attempts
to combine information from multiple different sen-
tences in a scalable way.
While its ability to combine information from
multiple sources is promising, HOLMES has several
limitations these Q/A systems do not have. Since
HOLMES relies on an information extraction sys-
tem to convert sentences into ground predicates,
any limitations of the IE system will be propagated
to HOLMES. Additionally, the logical representa-
tion HOLMES uses limits the reasoning and types
of questions it can answer. HOLMES is geared to-
wards answering questions which are naturally ex-
pressed as properties and relations of entities, and is
not well suited to answering more abstract or open
ended questions. Although we have demonstrated
that HOLMES is scalable, further work is needed to
make it to run at interactive speeds.
Finally, research in statistical relational learning
such as MLNs (Richardson and Domingos, 2006),
RMNs (Taskar et al, 2002), and others (Getoor
and Taskar, 2007) have studied techniques for com-
bining logical and probabilistic inference. Our in-
ference rules are more restrictive than those al-
lowed in MLNs, but this trade-off allows us to ef-
ficiently scale inference to large, open domain cor-
pora. By constructing only cliques for satisfied in-
ference rules, HOLMES explicitly models the intu-
ition behind LazySAT inference (Singla and Domin-
gos, 2006) as used in MLNs. I.e., most Horn clause
inference rules will be trivially satisfied since their
antecedents will be false, so we only need to worry
about ones where the antecedent is true.
6 Conclusions
This paper makes three main contributions:
1. We introduce and evaluate the HOLMES sys-
tem, which leverages KBMC methods in order
to scale a class of TI methods to the Web.
2. We define the notion of Approximately Pseudo-
Functional (APF) relations and prove that, for
a APF relations, HOLMES?s inference time in-
creases linearly with the size of the input cor-
pus. We show empirically that APF relations
appear to be prevalent in our Web corpus (Fig-
ure 4), and that HOLMES?s runtime does scale
linearly with the size of its input (Figure 5), tak-
ing only a few CPU minutes when run over 183
million distinct ground assertions.
3. We present experiments demonstrating that, for
a set of queries in the domains of geography,
business, and nutrition, HOLMES substantially
improves the quality of answers (measured by
AuC) relative to a ?no inference? baseline.
In the future, we plan more extensive tests to char-
acterize when HOLMES?s inference is helpful. We
also hope to examine in what cases jointly perform-
ing extraction and inference (as opposed to perform-
ing them separately) is feasible at scale. Finally, we
plan to examine methods for HOLMES to learn both
rule weights and new inference rules.
Acknowledgements
We thank the following for helpful comments on
previous drafts: Fei Wu, Michele Banko, Mausam,
Doug Downey, and Alan Ritter. This research was
supported in part by NSF grants IIS-0535284, IIS-
0312988, and IIS-0307906, ONR grants N00014-
08-1-0431 and N00014-06-1-0147, CALO grant 03-
000225, the WRF / TJ Cable Professorship as well
as gifts from Google. The work was performed at
the University of Washington?s Turing Center.
87
References
M. Banko, M. Cafarella, S. Soderland, M. Broadhead,
and O. Etzioni. 2007. Open information extraction
from the Web. In Procs. of IJCAI.
R. Braz, R. Girju, V. Punyakanok, D. Roth, and M. Sam-
mons. 2005. An inference model for semantic en-
tailment in natural language. In Proceedings of the
National Conference on Artificial Intelligence (AAAI),
pages 1678?1679.
E. Brill, J. Lin, M. Banko, S. T. Dumais, and A. Y. Ng.
2001. Data-intensive question answering. In Procs.
of Text REtrieval Conference (TREC-10), pages 393?
400.
Eric Brill, Susan Dumais, and Michele Banko. 2002. An
analysis of the AskMSR question-answering system.
In EMNLP ?02: Proceedings of the ACL-02 conference
on Empirical methods in natural language processing,
pages 257?264, Morristown, NJ, USA. Association for
Computational Linguistics.
I. Dagan, O. Glickman, and B. Magnini. 2005. The
PASCAL Recognising Textual Entailment Challenge.
Proceedings of the PASCAL Challenges Workshop on
Recognising Textual Entailment, pages 1?8.
O. Etzioni, M. Cafarella, D. Downey, S. Kok, A. Popescu,
T. Shaked, S. Soderland, D. Weld, and A. Yates.
2005. Unsupervised named-entity extraction from the
web: An experimental study. Artificial Intelligence,
165(1):91?134.
L. Getoor and B. Taskar. 2007. Introduction to Statistical
Relational Learning. MIT Press.
S. Harabagiu, M. Pasca, and S. Maiorano. 2000. Exper-
iments with open-domain textual question answering.
In Procs. of the COLING-2000.
M. Hearst. 1992. Automatic Acquisition of Hyponyms
from Large Text Corpora. In Procs. of the 14th In-
ternational Conference on Computational Linguistics,
pages 539?545, Nantes, France.
C.C.T. Kwok, O. Etzioni, and D.S. Weld. 2001. Scal-
ing question answering to the Web. Proceedings of
the 10th international conference on World Wide Web,
pages 150?161.
B. MacCartney and C.D. Manning. 2007. Natural Logic
for Textual Inference. In Workshop on Textual Entail-
ment and Paraphrasing.
G. Miller, R. Beckwith, C. Fellbaum, D. Gross, and
K. Miller. 1990. Introduction to wordnet: An on-line
lexical database. International Journal of Lexicogra-
phy, 3(4):235?312.
Judea Pearl. 1988. Probabilistic reasoning in intelli-
gent systems: networks of plausible inference. Morgan
Kaufmann Publishers Inc. San Francisco, CA, USA.
Rajat Raina, Andrew Y. Ng, and Christopher D. Man-
ning. 2005. Robust textual inference via learning and
abductive reasoning. In Proceedings of AAAI 2005.
AAAI Press.
M. Richardson and P. Domingos. 2006. Markov Logic
Networks. Machine Learning, 62(1-2):107?136.
Parag Singla and Pedro Domingos. 2006. Memory-
efficient inference in relational domains. In AAAI.
B. Taskar, P. Abbeel, and D. Koller. 2002. Discrimi-
native probabilistic models for relational data. Eigh-
teenth Conference on Uncertainty in Artificial Intelli-
gence (UAI02).
Marta Tatu and Dan Moldovan. 2006. A logic-based
semantic approach to recognizing textual entailment.
In Proceedings of the COLING/ACL on Main confer-
ence poster sessions, pages 819?826, Morristown, NJ,
USA. Association for Computational Linguistics.
J. Ullman. 1989. Database and knowledge-base systems.
Computer Science Press.
B. Van Durme and L.K. Schubert. 2008. Open knowl-
edge extraction through compositional language pro-
cessing. In Symposium on Semantics in Systems for
Text Processing.
M. Wellman, J. Breese, and R. Goldman. 1992. From
knowledge bases to decision models. The Knowledge
Engineering Review, 7(1):35?53.
F. Wu and D. Weld. 2007. Autonomously semantifying
Wikipedia. In Proceedings of the ACM Sixteenth Con-
ference on Information and Knowledge Management
(CIKM-07), Lisbon, Porgugal.
A. Yates and O. Etzioni. 2007. Unsupervised resolution
of objects and relations on the Web. In Procs. of HLT.
88
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 696?703,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Sparse Information Extraction:
Unsupervised Language Models to the Rescue
Doug Downey, Stefan Schoenmackers, and Oren Etzioni
Turing Center, Department of Computer Science and Engineering
University of Washington, Box 352350
Seattle, WA 98195, USA
{ddowney,stef,etzioni}@cs.washington.edu
Abstract
Even in a massive corpus such as the Web, a
substantial fraction of extractions appear in-
frequently. This paper shows how to assess
the correctness of sparse extractions by uti-
lizing unsupervised language models. The
REALM system, which combines HMM-
based and n-gram-based language models,
ranks candidate extractions by the likeli-
hood that they are correct. Our experiments
show that REALM reduces extraction error
by 39%, on average, when compared with
previous work.
Because REALM pre-computes language
models based on its corpus and does not re-
quire any hand-tagged seeds, it is far more
scalable than approaches that learn mod-
els for each individual relation from hand-
tagged data. Thus, REALM is ideally suited
for open information extraction where the
relations of interest are not specified in ad-
vance and their number is potentially vast.
1 Introduction
Information Extraction (IE) from text is far from in-
fallible. In response, researchers have begun to ex-
ploit the redundancy in massive corpora such as the
Web in order to assess the veracity of extractions
(e.g., (Downey et al, 2005; Etzioni et al, 2005;
Feldman et al, 2006)). In essence, such methods uti-
lize extraction patterns to generate candidate extrac-
tions (e.g., ?Istanbul?) and then assess each candi-
date by computing co-occurrence statistics between
the extraction and words or phrases indicative of
class membership (e.g., ?cities such as?).
However, Zipf?s Law governs the distribution of
extractions. Thus, even the Web has limited redun-
dancy for less prominent instances of relations. In-
deed, 50% of the extractions in the data sets em-
ployed by (Downey et al, 2005) appeared only
once. As a result, Downey et al?s model, and re-
lated methods, had no way of assessing which ex-
traction is more likely to be correct for fully half of
the extractions. This problem is particularly acute
when moving beyond unary relations. We refer to
this challenge as the task of assessing sparse extrac-
tions.
This paper introduces the idea that language mod-
eling techniques such as n-gram statistics (Manning
and Schu?tze, 1999) and HMMs (Rabiner, 1989) can
be used to effectively assess sparse extractions. The
paper introduces the REALM system, and highlights
its unique properties. Notably, REALM does not
require any hand-tagged seeds, which enables it to
scale to Open IE?extraction where the relations of
interest are not specified in advance, and their num-
ber is potentially vast (Banko et al, 2007).
REALM is based on two key hypotheses. The
KnowItAll hypothesis is that extractions that oc-
cur more frequently in distinct sentences in the
corpus are more likely to be correct. For exam-
ple, the hypothesis suggests that the argument pair
(Giuliani, New York) is relatively likely to be
appropriate for the Mayor relation, simply because
this pair is extracted for the Mayor relation rela-
tively frequently. Second, we employ an instance of
the distributional hypothesis (Harris, 1985), which
696
can be phrased as follows: different instances of
the same semantic relation tend to appear in sim-
ilar textual contexts. We assess sparse extractions
by comparing the contexts in which they appear to
those of more common extractions. Sparse extrac-
tions whose contexts are more similar to those of
common extractions are judged more likely to be
correct based on the conjunction of the KnowItAll
and the distributional hypotheses.
The contributions of the paper are as follows:
? The paper introduces the insight that the sub-
field of language modeling provides unsuper-
vised methods that can be leveraged to assess
sparse extractions. These methods are more
scalable than previous assessment techniques,
and require no hand tagging whatsoever.
? The paper introduces an HMM-based tech-
nique for checking whether two arguments are
of the proper type for a relation.
? The paper introduces a relational n-gram
model for the purpose of determining whether
a sentence that mentions multiple arguments
actually expresses a particular relationship be-
tween them.
? The paper introduces a novel language-
modeling system called REALM that combines
both HMM-based models and relational n-
gram models, and shows that REALM reduces
error by an average of 39% over previous meth-
ods, when applied to sparse extraction data.
The remainder of the paper is organized as fol-
lows. Section 2 introduces the IE assessment task,
and describes the REALM system in detail. Section
3 reports on our experimental results followed by a
discussion of related work in Section 4. Finally, we
conclude with a discussion of scalability and with
directions for future work.
2 IE Assessment
This section formalizes the IE assessment task and
describes the REALM system for solving it. An IE
assessor takes as input a list of candidate extractions
meant to denote instances of a relation, and outputs
a ranking of the extractions with the goal that cor-
rect extractions rank higher than incorrect ones. A
correct extraction is defined to be a true instance of
the relation mentioned in the input text.
More formally, the list of candidate extrac-
tions for a relation R is denoted as ER =
{(a1, b1), . . . , (am, bm)}. An extraction (ai, bi) is
an ordered pair of strings. The extraction is correct
if and only if the relation R holds between the argu-
ments named by ai and bi. For example, for R =
Headquartered, a pair (ai, bi) is correct iff there
exists an organization ai that is in fact headquartered
in the location bi.1
ER is generated by applying an extraction mech-
anism, typically a set of extraction ?patterns?, to
each sentence in a corpus, and recording the results.
Thus, many elements of ER are identical extractions
derived from different sentences in the corpus.
This task definition is notable for the minimal
inputs required?IE assessment does not require
knowing the relation name nor does it require hand-
tagged seed examples of the relation. Thus, an IE
Assessor is applicable to Open IE.
2.1 System Overview
In this section, we describe the REALM system,
which utilizes language modeling techniques to per-
form IE Assessment.
REALM takes as input a set of extractions ER,
and outputs a ranking of those extractions. The
algorithm REALM follows is outlined in Figure 1.
REALM begins by automatically selecting from ER
a set of bootstrapped seeds SR intended to serve as
correct examples of the relation R. REALM utilizes
the KnowItAll hypothesis, setting SR equal to the
h elements in ER extracted most frequently from
the underlying corpus. This results in a noisy set of
seeds, but the methods that use these seeds are noise
tolerant.
REALM then proceeds to rank the remaining
(non-seed) extractions by utilizing two language-
modeling components. An n-gram language model
is a probability distribution P (w1, ..., wn) over con-
secutive word sequences of length n in a corpus.
Formally, if we assume a seed (s1, s2) is a correct
extraction of a relation R, the distributional hypoth-
esis states that the context distribution around the
seed extraction, P (w1, ..., wn|wi = s1, wj = s2)
for 1 ? i, j ? n tends to be ?more similar? to
1For clarity, our discussion focuses on relations between
pairs of arguments. However, the methods we propose can be
extended to relations of any arity.
697
P (w1, ..., wn|wi = e1, wj = e2) when the extrac-
tion (e1, e2) is correct. Naively comparing context
distributions is problematic, however, because the
arguments to a relation often appear separated by
several intervening words. In our experiments, we
found that when relation arguments appear together
in a sentence, 75% of the time the arguments are
separated by at least three words. This implies that
n must be large, and for sparse argument pairs it is
not possible to estimate such a large language model
accurately, because the number of modeling param-
eters is proportional to the vocabulary size raised to
the nth power. To mitigate sparsity, REALM utilizes
smaller language models in its two components as a
means of ?backing-off? from estimating context dis-
tributions explicitly, as described below.
First, REALM utilizes an HMM to estimate
whether each extraction has arguments of the proper
type for the relation. Each relation R has a set
of types for its arguments. For example, the rela-
tion AuthorOf(a, b) requires that its first ar-
gument be an author, and that its second be some
kind of written work. Knowing whether extracted
arguments are of the proper type for a relation can
be quite informative for assessing extractions. The
challenge is, however, that this type information is
not given to the system since the relations (and the
types of the arguments) are not known in advance.
REALM solves this problem by comparing the dis-
tributions of the seed arguments and extraction ar-
guments. Type checking mitigates data sparsity by
leveraging every occurrence of the individual extrac-
tion arguments in the corpus, rather than only those
cases in which argument pairs occur near each other.
Although argument type checking is invalu-
able for extraction assessment, it is not suf-
ficient for extracting relationships between ar-
guments. For example, an IE system us-
ing only type information might determine that
Intel is a corporation and that Seattle is
a city, and therefore erroneously conclude that
Headquartered(Intel, Seattle) is cor-
rect. Thus, REALM?s second step is to employ an
n-gram-based language model to assess whether the
extracted arguments share the appropriate relation.
Again, this information is not given to the system,
so REALM compares the context distributions of the
extractions to those of the seeds. As described in
REALM(Extractions ER = {e1, ..., em})
SR = the h most frequent extractions in ER
UR = ER - SR
TypeRankings(UR)? HMM-T(SR, UR)
RelationRankings(UR)? REL-GRAMS(SR, UR)
return a ranking of ER with the elements of SR at the
top (ranked by frequency) followed by the elements of
UR = {u1, ..., um?h} ranked in ascending order of
TypeRanking(ui) ?RelationRanking(ui).
Figure 1: Pseudocode for REALM at run-time.
The language models used by the HMM-T and
REL-GRAMS components are constructed in a pre-
processing step.
Section 2.3, REALM employs a relational n-gram
language model in order to accurately compare con-
text distributions when extractions are sparse.
REALM executes the type checking and relation
assessment components separately; each component
takes the seed and non-seed extractions as arguments
and returns a ranking of the non-seeds. REALM then
combines the two components? assessments into a
single ranking. Although several such combinations
are possible, REALM simply ranks the extractions in
ascending order of the product of the ranks assigned
by the two components. The following subsections
describe REALM?s two components in detail.
We identify the proper nouns in our corpus us-
ing the LEX method (Downey et al, 2007). In ad-
dition to locating the proper nouns in the corpus,
LEX also concatenates each multi-token proper noun
(e.g.,Los Angeles) together into a single token.
Both of REALM?s components construct language
models from this tokenized corpus.
2.2 Type Checking with HMM-T
In this section, we describe our type-checking com-
ponent, which takes the form of a Hidden Markov
Model and is referred to as HMM-T. HMM-T ranks
the set UR of non-seed extractions, with a goal of
ranking those extractions with arguments of proper
type for R above extractions containing type errors.
Formally, let URi denote the set of the ith arguments
of the extractions in UR. Let SRi be defined simi-
larly for the seed set SR.
Our type checking technique exploits the distri-
butional hypothesis?in this case, the intuition that
698
Intel , headquartered in Santa+Clara
Figure 2: Graphical model employed by HMM-
T. Shown is the case in which k = 2. Corpus
pre-processing results in the proper noun Santa
Clara being concatenated into a single token.
extraction arguments in URi of the proper type will
likely appear in contexts similar to those in which
the seed arguments SRi appear. In order to iden-
tify terms that are distributionally similar, we train
a probabilistic generative Hidden Markov Model
(HMM), which treats each token in the corpus as
generated by a single hidden state variable. Here, the
hidden states take integral values from {1, . . . , T},
and each hidden state variable is itself generated by
some number k of previous hidden states.2 For-
mally, the joint distribution of the corpus, repre-
sented as a vector of tokens w, given a correspond-
ing vector of states t is:
P (w|t) =
?
i
P (wi|ti)P (ti|ti?1, . . . , ti?k) (1)
The distributions on the right side of Equation 1
can be learned from a corpus in an unsupervised
manner, such that words which are distributed sim-
ilarly in the corpus tend to be generated by simi-
lar hidden states (Rabiner, 1989). The generative
model is depicted as a Bayesian network in Figure 2.
The figure also illustrates the one way in which our
implementation is distinct from a standard HMM,
namely that proper nouns are detected a priori and
modeled as single tokens (e.g., Santa Clara is
generated by a single hidden state). This allows
the type checker to compare the state distributions
of different proper nouns directly, even when the
proper nouns contain differing numbers of words.
To generate a ranking of UR using the learned
HMM parameters, we rank the arguments ei accord-
ing to how similar their state distributions P (t|ei)
2Our implementation makes the simplifying assumption that
each sentence in the corpus is generated independently.
are to those of the seed arguments.3 Specifically, we
define a function:
f(e) =
?
ei?e
KL(
?
w??SRi
P (t|w?)
|SRi|
, P (t|ei)) (2)
where KL represents KL divergence, and the outer
sum is taken over the arguments ei of the extraction
e. We rank the elements of UR in ascending order of
f(e).
HMM-T has two advantages over a more tradi-
tional type checking approach of simply counting
the number of times in the corpus that each extrac-
tion appears in a context in which a seed also ap-
pears (cf. (Ravichandran et al, 2005)). The first
advantage of HMM-T is efficiency, as the traditional
approach involves a computationally expensive step
of retrieving the potentially large set of contexts in
which the extractions and seeds appear. In our ex-
periments, using HMM-T instead of a context-based
approach results in a 10-50x reduction in the amount
of data that is retrieved to perform type checking.
Secondly, on sparse data HMM-T has the poten-
tial to improve type checking accuracy. For exam-
ple, consider comparing Pickerington, a sparse
candidate argument of the type City, to the seed
argument Chicago, for which the following two
phrases appear in the corpus:
(i) ?Pickerington, Ohio?
(ii) ?Chicago, Illinois?
In these phrases, the textual contexts surrounding
Chicago and Pickerington are not identical,
so to the traditional approach these contexts offer
no evidence that Pickerington and Chicago
are of the same type. For a sparse token like
Pickerington, this is problematic because the
token may never occur in a context that precisely
matches that of a seed. In contrast, in the HMM, the
non-sparse tokens Ohio and Illinois are likely
to have similar state distributions, as they are both
the names of U.S. States. Thus, in the state space
employed by the HMM, the contexts in phrases (i)
and (ii) are in fact quite similar, allowing HMM-
T to detect that Pickerington and Chicago
are likely of the same type. Our experiments quan-
tify the performance improvements that HMM-T of-
3The distribution P (t|ei) for any ei can be obtained from
the HMM parameters using Bayes Rule.
699
fers over the traditional approach for type checking
sparse data.
The time required to learn HMM-T?s parameters
scales proportional to T k+1 times the corpus size.
Thus, for tractability, HMM-T uses a relatively small
state space of T = 20 states and a limited k value
of 3. While these settings are sufficient for type
checking (e.g., determining that Santa Clara is
a city) they are too coarse-grained to assess relations
between arguments (e.g., determining that Santa
Clara is the particular city in which Intel is
headquartered). We now turn to the REL-GRAMS
component, which performs the latter task.
2.3 Relation Assessment with REL-GRAMS
REALM?s relation assessment component, called
REL-GRAMS, tests whether the extracted arguments
have a desired relationship, but given REALM?s min-
imal input it has no a priori information about the
relationship. REL-GRAMS relies instead on the dis-
tributional hypothesis to test each extraction.
As argued in Section 2.1, it is intractable to build
an accurate language model for context distributions
surrounding sparse argument pairs. To overcome
this problem, we introduce relational n-gram mod-
els. Rather than simply modeling the context distri-
bution around a given argument, a relational n-gram
model specifies separate context distributions for an
arguments conditioned on each of the other argu-
ments with which it appears. The relational n-gram
model allows us to estimate context distributions for
pairs of arguments, even when the arguments do not
appear together within a fixed window of n words.
Further, by considering only consecutive argument
pairs, the number of distinct argument pairs in the
model grows at most linearly with the number of
sentences in the corpus. Thus, the relational n-gram
model can scale.
Formally, for a pair of arguments (e1, e2), a re-
lational n-gram model estimates the distributions
P (w1, ..., wn|wi = e1, e1 ? e2) for each 1 ? i ?
n, where the notation e1 ? e2 indicates the event
that e2 is the next argument to either the right or the
left of e1 in the corpus.
REL-GRAMS begins by building a relational n-
gram model of the arguments in the corpus. For
notational convenience, we represent the model?s
distributions in terms of ?context vectors? for each
pair of arguments. Formally, for a given sentence
containing arguments e1 and e2 consecutively, we
define a context of the ordered pair (e1, e2) to be
any window of n tokens around e1. Let C =
{c1, c2, ..., c|C|} be the set of all contexts of all ar-
gument pairs found in the corpus.4 For a pair of ar-
guments (ej , ek), we model their relationship using
a |C| dimensional context vector v(ej ,ek), whose i-th
dimension corresponds to the number of times con-
text ci occurred with the pair (ej , ek) in the corpus.
These context vectors are similar to document vec-
tors from Information Retrieval (IR), and we lever-
age IR research to compare them, as described be-
low.
To assess each extraction, we determine how sim-
ilar its context vector is to a canonical seed vec-
tor (created by summing the context vectors of the
seeds). While there are many potential methods
for determining similarity, in this work we rank ex-
tractions by decreasing values of the BM25 dis-
tance metric. BM25 is a TF-IDF variant intro-
duced in TREC-3(Robertson et al, 1992), which
outperformed both the standard cosine distance and
a smoothed KL divergence on our data.
3 Experimental Results
This section describes our experiments on IE assess-
ment for sparse data. We start by describing our
experimental methodology, and then present our re-
sults. The first experiment tests the hypothesis that
HMM-T outperforms an n-gram-based method on
the task of type checking. The second experiment
tests the hypothesis that REALM outperforms multi-
ple approaches from previous work, and also outper-
forms each of its HMM-T and REL-GRAMS compo-
nents taken in isolation.
3.1 Experimental Methodology
The corpus used for our experiments consisted of a
sample of sentences taken from Web pages. From
an initial crawl of nine million Web pages, we se-
lected sentences containing relations between proper
nouns. The resulting text corpus consisted of about
4Pre-computing the set C requires identifying in advance
the potential relation arguments in the corpus. We consider the
proper nouns identified by the LEX method (see Section 2.1) to
be the potential arguments.
700
three million sentences, and was tokenized as de-
scribed in Section 2. For tractability, before and after
performing tokenization, we replaced each token oc-
curring fewer than five times in the corpus with one
of two ?unknown word? markers (one for capital-
ized words, and one for uncapitalized words). This
preprocessing resulted in a corpus containing about
sixty-five million total tokens, and 214,787 unique
tokens.
We evaluated performance on four relations:
Conquered, Founded, Headquartered, and
Merged. These four relations were chosen because
they typically take proper nouns as arguments, and
included a large number of sparse extractions. For
each relationR, the candidate extraction listER was
obtained using TEXTRUNNER (Banko et al, 2007).
TEXTRUNNER is an IE system that computes an in-
dex of all extracted relationships it recognizes, in the
form of (object, predicate, object) triples. For each
of our target relations, we executed a single query
to the TEXTRUNNER index for extractions whose
predicate contained a phrase indicative of the rela-
tion (e.g., ?founded by?, ?headquartered in?), and
the results formed our extraction list. For each rela-
tion, the 10 most frequent extractions served as boot-
strapped seeds. All of the non-seed extractions were
sparse (no argument pairs were extracted more than
twice for a given relation). These test sets contained
a total of 361 extractions.
3.2 Type Checking Experiments
As discussed in Section 2.2, on sparse data HMM-T
has the potential to outperform type checking meth-
ods that rely on textual similarities of context vec-
tors. To evaluate this claim, we tested the HMM-T
system against an N-GRAMS type checking method
on the task of type-checking the arguments to a re-
lation. The N-GRAMS method compares the context
vectors of extractions in the same way as the REL-
GRAMS method described in Section 2.3, but is not
relational (N-GRAMS considers the distribution of
each extraction argument independently, similar to
HMM-T). We tagged an extraction as type correct iff
both arguments were valid for the relation, ignoring
whether the relation held between the arguments.
The results of our type checking experiments are
shown in Table 1. For all types, HMM-T outper-
forms N-GRAMS, and HMM-T reduces error (mea-
Type HMM-T N-GRAMS
Conquered 0.917 0.767
Founded 0.827 0.636
Headquartered 0.734 0.589
Merged 0.920 0.854
Average 0.849 0.712
Table 1: Type Checking Performance. Listed is area
under the precision/recall curve. HMM-T outper-
forms N-GRAMS for all relations, and reduces the
error in terms of missing area under the curve by
46% on average.
sured in missing area under the precision/recall
curve) by 46%. The performance difference on each
relation is statistically significant (p < 0.01, two-
sampled t-test), using the methodology for measur-
ing the standard deviation of area under the preci-
sion/recall curve given in (Richardson and Domin-
gos, 2006). N-GRAMS, like REL-GRAMS, employs
the BM-25 metric to measure distributional similar-
ity between extractions and seeds. Replacing BM-
25 with cosine distance cuts HMM-T?s advantage
over N-GRAMS, but HMM-T?s error rate is still 23%
lower on average.
3.3 Experiments with REALM
The REALM system combines the type checking
and relation assessment components to assess ex-
tractions. Here, we test the ability of REALM to
improve the ranking of a state of the art IE system,
TEXTRUNNER. For these experiments, we evalu-
ate REALM against the TEXTRUNNER frequency-
based ordering, a pattern-learning approach, and the
HMM-T and REL-GRAMS components taken in iso-
lation. The TEXTRUNNER frequency-based order-
ing ranks extractions in decreasing order of their ex-
traction frequency, and importantly, for our task this
ordering is essentially equivalent to that produced by
the ?Urns? (Downey et al, 2005) and Pointwise Mu-
tual Information (Etzioni et al, 2005) approaches
employed in previous work.
The pattern-learning approach, denoted as PL, is
modeled after Snowball (Agichtein, 2006). The al-
gorithm and parameter settings for PL were those
manually tuned for the Headquartered relation
in previous work (Agichtein, 2005). A sensitivity
analysis of these parameters indicated that the re-
701
Conquered Founded Headquartered Merged Average
Avg. Prec. 0.698 0.578 0.400 0.742 0.605
TEXTRUNNER 0.738 0.699 0.710 0.784 0.733
PL 0.885 0.633 0.651 0.852 0.785
PL+ HMM-T 0.883 0.722 0.727 0.900 0.808
HMM-T 0.830 0.776 0.678 0.864 0.787
REL-GRAMS 0.929 (39%) 0.713 0.758 0.886 0.822
REALM 0.907 (19%) 0.781 (27%) 0.810 (35%) 0.908 (38%) 0.851 (39%)
Table 2: Performance of REALM for assessment of sparse extractions. Listed is area under the preci-
sion/recall curve for each method. In parentheses is the percentage reduction in error over the strongest
baseline method (TEXTRUNNER or PL) for each relation. ?Avg. Prec.? denotes the fraction of correct
examples in the test set for each relation. REALM outperforms its REL-GRAMS and HMM-T components
taken in isolation, as well as the TEXTRUNNER and PL systems from previous work.
sults are sensitive to the parameter settings. How-
ever, we found no parameter settings that performed
significantly better, and many settings performed
significantly worse. As such, we believe our re-
sults reasonably reflect the performance of a pattern
learning system on this task. Because PL performs
relation assessment, we also attempted combining
PL with HMM-T in a hybrid method (PL+ HMM-T)
analogous to REALM.
The results of these experiments are shown in Ta-
ble 2. REALM outperforms the TEXTRUNNER and
PL baselines for all relations, and reduces the miss-
ing area under the curve by an average of 39% rel-
ative to the strongest baseline. The performance
differences between REALM and TEXTRUNNER are
statistically significant for all relations, as are differ-
ences between REALM and PL for all relations ex-
cept Conquered (p < 0.01, two-sampled t-test).
The hybrid REALM system also outperforms each
of its components in isolation.
4 Related Work
To our knowledge, REALM is the first system to use
language modeling techniques for IE Assessment.
Redundancy-based approaches to pattern-based
IE assessment (Downey et al, 2005; Etzioni et al,
2005) require that extractions appear relatively fre-
quently with a limited set of patterns. In contrast,
REALM utilizes all contexts to build a model of ex-
tractions, rather than a limited set of patterns. Our
experiments demonstrate that REALM outperforms
these approaches on sparse data.
Type checking using named-entity taggers has
been previously shown to improve the precision of
pattern-based IE systems (Agichtein, 2005; Feld-
man et al, 2006), but the HMM-T type-checking
component we develop differs from this work in im-
portant ways. Named-entity taggers are limited in
that they typically recognize only small set of types
(e.g., ORGANIZATION, LOCATION, PERSON),
and they require hand-tagged training data for each
type. HMM-T, by contrast, performs type check-
ing for any type. Finally, HMM-T does not require
hand-tagged training data.
Pattern learning is a common technique for ex-
tracting and assessing sparse data (e.g. (Agichtein,
2005; Riloff and Jones, 1999; Pas?ca et al, 2006)).
Our experiments demonstrate that REALM outper-
forms a pattern learning system closely modeled af-
ter (Agichtein, 2005). REALM is inspired by pat-
tern learning techniques (in particular, both use the
distributional hypothesis to assess sparse data) but
is distinct in important ways. Pattern learning tech-
niques require substantial processing of the corpus
after the relations they assess have been specified.
Because of this, pattern learning systems are un-
suited to Open IE. Unlike these techniques, REALM
pre-computes language models which allow it to as-
sess extractions for arbitrary relations at run-time.
In essence, pattern-learning methods run in time lin-
ear in the number of relations whereas REALM?s run
time is constant in the number of relations. Thus,
REALM scales readily to large numbers of relations
whereas pattern-learning methods do not.
702
A second distinction of REALM is that its type
checker, unlike the named entity taggers employed
in pattern learning systems (e.g., Snowball), can be
used to identify arbitrary types. A final distinction is
that the language models REALM employs require
fewer parameters and heuristics than pattern learn-
ing techniques.
Similar distinctions exist between REALM and a
recent system designed to assess sparse extractions
by bootstrapping a classifier for each target relation
(Feldman et al, 2006). As in pattern learning, con-
structing the classifiers requires substantial process-
ing after the target relations have been specified, and
a set of hand-tagged examples per relation, making
it unsuitable for Open IE.
5 Conclusions
This paper demonstrated that unsupervised language
models, as embodied in the REALM system, are an
effective means of assessing sparse extractions.
Another attractive feature of REALM is its scal-
ability. Scalability is a particularly important con-
cern forOpen Information Extraction, the task of ex-
tracting large numbers of relations that are not spec-
ified in advance. Because HMM-T and REL-GRAMS
both pre-compute language models, REALM can be
queried efficiently to perform IE Assessment. Fur-
ther, the language models are constructed indepen-
dently of the target relations, allowing REALM to
perform IE Assessment even when relations are not
specified in advance.
In future work, we plan to develop a probabilistic
model of the information computed by REALM. We
also plan to evaluate the use of non-local context for
IE Assessment by integrating document-level mod-
eling techniques (e.g., Latent Dirichlet Allocation).
Acknowledgements
This research was supported in part by NSF grants
IIS-0535284 and IIS-0312988, DARPA contract
NBCHD030010, ONR grant N00014-05-1-0185 as
well as a gift from Google. The first author is sup-
ported by an MSR graduate fellowship sponsored by
Microsoft Live Labs. We thank Michele Banko, Jeff
Bilmes, Katrin Kirchhoff, and Alex Yates for helpful
comments.
References
E. Agichtein. 2005. Extracting Relations From Large
Text Collections. Ph.D. thesis, Department of Com-
puter Science, Columbia University.
E. Agichtein. 2006. Confidence estimation methods for
partially supervised relation extraction. In SDM 2006.
M. Banko, M. Cararella, S. Soderland, M. Broadhead,
and O. Etzioni. 2007. Open information extraction
from the web. In Procs. of IJCAI 2007.
D. Downey, O. Etzioni, and S. Soderland. 2005. A Prob-
abilistic Model of Redundancy in Information Extrac-
tion. In Procs. of IJCAI 2005.
D. Downey, M. Broadhead, and O. Etzioni. 2007. Locat-
ing complex named entities in web text. In Procs. of
IJCAI 2007.
O. Etzioni, M. Cafarella, D. Downey, S. Kok, A. Popescu,
T. Shaked, S. Soderland, D. Weld, and A. Yates.
2005. Unsupervised named-entity extraction from the
web: An experimental study. Artificial Intelligence,
165(1):91?134.
R. Feldman, B. Rosenfeld, S. Soderland, and O. Etzioni.
2006. Self-supervised relation extraction from the
web. In ISMIS, pages 755?764.
Z. Harris. 1985. Distributional structure. In J. J. Katz,
editor, The Philosophy of Linguistics, pages 26?47.
New York: Oxford University Press.
C. D. Manning and H. Schu?tze. 1999. Foundations of
Statistical Natural Language Processing.
M. Pas?ca, D. Lin, J. Bigham, A. Lifchits, and A. Jain.
2006. Names and similarities on the web: Fact extrac-
tion in the fast lane. In Procs. of ACL/COLING 2006.
L. R. Rabiner. 1989. A tutorial on hidden markov models
and selected applications in speech recognition. Pro-
ceedings of the IEEE, 77(2):257?286.
D. Ravichandran, P. Pantel, and E. H. Hovy. 2005. Ran-
domized Algorithms and NLP: Using Locality Sensi-
tive Hash Functions for High Speed Noun Clustering.
In Procs. of ACL 2005.
M. Richardson and P. Domingos. 2006. Markov Logic
Networks. Machine Learning, 62(1-2):107?136.
E. Riloff and R. Jones. 1999. Learning Dictionaries for
Information Extraction by Multi-level Boot-strapping.
In Procs. of AAAI-99, pages 1044?1049.
S. E. Robertson, S. Walker, M. Hancock-Beaulieu,
A. Gull, and M. Lau. 1992. Okapi at TREC-3. In
Text REtrieval Conference, pages 21?30.
703
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 27?34,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Detecting Parser Errors Using Web-based Semantic Filters
Alexander Yates Stefan Schoenmackers
University of Washington
Computer Science and Engineering
Box 352350
Seattle, WA 98195-2350
{ayates, stef, etzioni} @cs.washington.edu
Oren Etzioni
Abstract
NLP systems for tasks such as question
answering and information extraction typ-
ically rely on statistical parsers. But the ef-
ficacy of such parsers can be surprisingly
low, particularly for sentences drawn from
heterogeneous corpora such as the Web.
We have observed that incorrect parses of-
ten result in wildly implausible semantic
interpretations of sentences, which can be
detected automatically using semantic in-
formation obtained from the Web.
Based on this observation, we introduce
Web-based semantic filtering?a novel,
domain-independent method for automat-
ically detecting and discarding incorrect
parses. We measure the effectiveness of
our filtering system, called WOODWARD,
on two test collections. On a set of TREC
questions, it reduces error by 67%. On
a set of more complex Penn Treebank
sentences, the reduction in error rate was
20%.
1 Introduction
Semantic processing of text in applications such
as question answering or information extraction
frequently relies on statistical parsers. Unfortu-
nately, the efficacy of state-of-the-art parsers can
be disappointingly low. For example, we found
that the Collins parser correctly parsed just 42%
of the list and factoid questions from TREC 2004
(that is, 42% of the parses had 100% precision and
100% recall on labeled constituents). Similarly,
this parser produced 45% correct parses on a sub-
set of 100 sentences from section 23 of the Penn
Treebank.
Although statistical parsers continue to improve
their efficacy over time, progress is slow, par-
ticularly for Web applications where training the
parsers on a ?representative? corpus of hand-
tagged sentences is not an option. Because of the
heterogeneous nature of text on the Web, such a
corpus would be exceedingly difficult to generate.
In response, this paper investigates the possibil-
ity of detecting parser errors by using semantic in-
formation obtained from the Web. Our fundamen-
tal hypothesis is that incorrect parses often result
in wildly implausible semantic interpretations of
sentences, which can be detected automatically in
certain circumstances. Consider, for example, the
following sentence from the Wall Street Journal:
?That compares with per-share earnings from con-
tinuing operations of 69 cents.? The Collins parser
yields a parse that attaches ?of 69 cents? to ?op-
erations,? rather than ?earnings.? By computing
the mutual information between ?operations? and
?cents? on the Web, we can detect that this attach-
ment is unlikely to be correct.
Our WOODWARD system detects parser errors
as follows. First, it maps the tree produced by a
parser to a relational conjunction (RC), a logic-
based representation language that we describe in
Section 2.1. Second, WOODWARD employs four
distinct methods for analyzing whether a conjunct
in the RC is likely to be ?reasonable? as described
in Section 2.
Our approach makes several assumptions. First,
if the sentence is absurd to begin with, then a cor-
rect parse could be deemed incorrect. Second, we
require a corpus whose content overlaps at least in
part with the content of the sentences to be parsed.
Otherwise, much of our semantic analysis is im-
possible.
In applications such as Web-based question an-
swering, these assumptions are quite natural. The
27
questions are about topics that are covered exten-
sively on the Web, and we can assume that most
questions link verbs to nouns in reasonable com-
binations. Likewise, when using parsing for infor-
mation extraction, we would expect our assump-
tions to hold as well.
Our contributions are as follows:
1. We introduce Web-based semantic filtering?
a novel, domain-independent method for de-
tecting and discarding incorrect parses.
2. We describe four techniques for analyzing
relational conjuncts using semantic informa-
tion obtained from the Web, and assess their
efficacy both separately and in combination.
3. We find that WOODWARD can filter good
parses from bad on TREC 2004 questions for
a reduction of 67% in error rate. On a harder
set of sentences from the Penn Treebank, the
reduction in error rate is 20%.
The remainder of this paper is organized as fol-
lows. We give an overview of related work in Sec-
tion 1.1. Section 2 describes semantic filtering, in-
cluding our RC representation and the four Web-
based filters that constitute the WOODWARD sys-
tem. Section 3 presents our experiments and re-
sults, and section 4 concludes and gives ideas for
future work.
1.1 Related Work
The problem of detecting parse errors is most sim-
ilar to the idea of parse reranking. Collins (2000)
describes statistical techniques for reranking alter-
native parses for a sentence. Implicitly, a rerank-
ing method detects parser errors, in that if the
reranking method picks a new parse over the orig-
inal one, it is classifying the original one as less
likely to be correct. Collins uses syntactic and lex-
ical features and trains on the Penn Treebank; in
contrast, WOODWARD uses semantic features de-
rived from the web. See section 3 for a comparison
of our results with Collins?.
Several systems produce a semantic interpreta-
tion of a sentence on top of a parser. For example,
Bos et al (2004) build semantic representations
from the parse derivations of a CCG parser, and
the English Resource Grammar (ERG) (Toutanova
et al, 2005) provides a semantic representation us-
ing minimal recursion semantics. Toutanova et al
also include semantic features in their parse se-
lection mechanism, although it is mostly syntax-
driven. The ERG is a hand-built grammar and thus
does not have the same coverage as the grammar
we use. We also use the semantic interpretations
in a novel way, checking them against semantic
information on the Web to decide if they are plau-
sible.
NLP literature is replete with examples of sys-
tems that produce semantic interpretations and
use semantics to improve understanding. Sev-
eral systems in the 1970s and 1980s used hand-
built augmented transition networks or semantic
networks to prune bad semantic interpretations.
More recently, people have tried incorporating
large lexical and semantic resources like WordNet,
FrameNet, and PropBank into the disambiguation
process. Allen (1995) provides an overview of
some of this work and contains many references.
Our work focuses on using statistical techniques
over large corpora, reducing the need for hand-
built resources and making the system more robust
to changes in domain.
Numerous systems, including Question-
Answering systems like MULDER (Kwok et
al., 2001), PiQASso (Attardi et al, 2001), and
Moldovan et al?s QA system (2003), use parsing
technology as a key component in their analysis
of sentences. In part to overcome incorrect parses,
Moldovan et al?s QA system requires a complex
set of relaxation techniques. These systems
would greatly benefit from knowing when parses
are correct or incorrect. Our system is the first
to suggest using the output of a QA system to
classify the input parse as good or bad.
Several researchers have used pointwise mu-
tual information (PMI) over the Web to help make
syntactic and semantic judgments in NLP tasks.
Volk (2001) uses PMI to resolve preposition at-
tachments in German. Lapata and Keller (2005)
use web counts to resolve preposition attachments,
compound noun interpretation, and noun count-
ability detection, among other things. And Mark-
ert et al (2003) use PMI to resolve certain types of
anaphora. We use PMI as just one of several tech-
niques for acquiring information from the Web.
2 Semantic Filtering
This section describes semantic filtering as imple-
mented in the WOODWARD system. WOODWARD
consists of two components: a semantic interpreter
that takes a parse tree and converts it to a conjunc-
tion of first-order predicates, and a sequence of
four increasingly sophisticated methods that check
semantic plausibility of conjuncts on the Web. Be-
low, we describe each component in turn.
28
1. What(NP1) ? are(VP1, NP1, NP2) ? states(NP2) ? producing(VP2, NP2, NP3) ? oil(NP3) ? in(PP1, NP2, U.S.)
2. What(NP1) ? states(NP2) ? producing(VP1, NP3, NP2, NP1) ? oil(NP3) ? in(PP1, NP2, U.S.)
Figure 2: Example relational conjunctions. The first RC is the correct one for the sentence ?What are oil producing
states in the U.S.?? The second is the RC derived from the Collins parse in Figure 1. Differences between the two RCs
appear in bold.

	 



		 
	 

Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1088?1098,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Learning First-Order Horn Clauses from Web Text
Stefan Schoenmackers, Oren Etzioni, Daniel S. Weld
Turing Center
University of Washington
Computer Science and Engineering
Box 352350
Seattle, WA 98125, USA
stef,etzioni,weld@cs.washington.edu
Jesse Davis
Katholieke Universiteit Leuven
Department of Computer Science
POBox 02402 Celestijnenlaan 200a
B-3001 Heverlee, Belgium
jesse.davis@cs.kuleuven.be
Abstract
Even the entire Web corpus does not explic-
itly answer all questions, yet inference can un-
cover many implicit answers. But where do
inference rules come from?
This paper investigates the problem of learn-
ing inference rules from Web text in an un-
supervised, domain-independent manner. The
SHERLOCK system, described herein, is a
first-order learner that acquires over 30,000
Horn clauses from Web text. SHERLOCK em-
bodies several innovations, including a novel
rule scoring function based on Statistical Rel-
evance (Salmon et al, 1971) which is effec-
tive on ambiguous, noisy and incomplete Web
extractions. Our experiments show that in-
ference over the learned rules discovers three
times as many facts (at precision 0.8) as the
TEXTRUNNER system which merely extracts
facts explicitly stated in Web text.
1 Introduction
Today?s Web search engines locate pages that match
keyword queries. Even sophisticated Web-based
Q/A systems merely locate pages that contain an ex-
plicit answer to a question. These systems are help-
less if the answer has to be inferred from multiple
sentences, possibly on different pages. To solve this
problem, Schoenmackers et al(2008) introduced the
HOLMES system, which infers answers from tuples
extracted from text.
HOLMES?s distinction is that it is domain inde-
pendent and that its inference time is linear in the
size of its input corpus, which enables it to scale to
the Web. However, HOLMES?s Achilles heel is that
it requires hand-coded, first-order, Horn clauses as
input. Thus, while HOLMES?s inference run time
is highly scalable, it requires substantial labor and
expertise to hand-craft the appropriate set of Horn
clauses for each new domain.
Is it possible to learn effective first-order Horn
clauses automatically from Web text in a domain-
independent and scalable manner? We refer to the
set of ground facts derived from Web text as open-
domain theories. Learning Horn clauses has been
studied extensively in the Inductive Logic Program-
ming (ILP) literature (Quinlan, 1990; Muggleton,
1995). However, learning Horn clauses from open-
domain theories is particularly challenging for sev-
eral reasons. First, the theories denote instances of
an unbounded and unknown set of relations. Sec-
ond, the ground facts in the theories are noisy, and
incomplete. Negative examples are mostly absent,
and certainly we cannot make the closed-world as-
sumption typically made by ILP systems. Finally,
the names used to denote both entities and relations
are rife with both synonyms and polysymes making
their referents ambiguous and resulting in a particu-
larly noisy and ambiguous set of ground facts.
This paper presents a new ILP method, which is
optimized to operate on open-domain theories de-
rived from massive and diverse corpora such as the
Web, and experimentally confirms both its effective-
ness and superiority over traditional ILP algorithms
in this context. Table 1 shows some example rules
that were learned by SHERLOCK.
This work makes the following contributions:
1. We describe the design and implementation of
the SHERLOCK system, which utilizes a novel,
unsupervised ILP method to learn first-order
Horn clauses from open-domain Web text.
1088
IsHeadquarteredIn(Company, State) :-
IsBasedIn(Company, City) ? IsLocatedIn(City, State);
Contains(Food, Chemical) :-
IsMadeFrom(Food, Ingredient) ? Contains(Ingredient, Chemical);
Reduce(Medication, Factor) :-
KnownGenericallyAs(Medication, Drug) ? Reduce(Drug, Factor);
ReturnTo(Writer, Place) :- BornIn(Writer, City) ? CapitalOf(City, Place);
Make(Company1, Device) :- Buy(Company1, Company2) ? Make(Company2, Device);
Table 1: Example rules learned by SHERLOCK from Web extractions. Note that the italicized rules are unsound.
2. We derive an innovative scoring function that is
particularly well-suited to unsupervised learn-
ing from noisy text. For Web text, the scoring
function yields more accurate rules than several
functions from the ILP literature.
3. We demonstrate the utility of SHERLOCK?s
automatically learned inference rules. Infer-
ence using SHERLOCK?s learned rules identi-
fies three times as many high quality facts (e.g.,
precision ? 0.8) as were originally extracted
from the Web text corpus.
The remainder of this paper is organized as fol-
lows. We start by describing previous work. Sec-
tion 3 introduces the SHERLOCK rule learning sys-
tem, with Section 3.4 describing how it estimates
rule quality. We empirically evaluate SHERLOCK in
Section 4, and conclude.
2 Previous Work
SHERLOCK is one of the first systems to learn first-
order Horn clauses from open-domain Web extrac-
tions. The learning method in SHERLOCK belongs
to the Inductive logic programming (ILP) subfield
of machine learning (Lavrac and Dzeroski, 2001).
However, classical ILP systems (e.g., FOIL (Quin-
lan, 1990) and Progol (Muggleton, 1995)) make
strong assumptions that are inappropriate for open
domains. First, ILP systems assume high-quality,
hand-labeled training examples for each relation of
interest. Second, ILP systems assume that constants
uniquely denote individuals; however, in Web text
strings such as ?dad? or ?John Smith? are highly
ambiguous. Third, ILP system typically assume
complete, largely noise-free data whereas tuples ex-
tracted from Web text are both noisy and radically
incomplete. Finally, ILP systems typically utilize
negative examples, which are not available when
learning from open-domain facts. One system that
does not require negative examples is LIME (Mc-
Creath and Sharma, 1997); We compare SHERLOCK
with LIME?s methods in Section 4.3. Most prior ILP
and Markov logic structure learning systems (e.g.,
(Kok and Domingos, 2005)) are not designed to han-
dle the noise and incompleteness of open-domain,
extracted facts.
NELL (Carlson et al, 2010) performs coupled
semi-supervised learning to extract a large knowl-
edge base of instances, relations, and inference
rules, bootstrapping from a few seed examples of
each class and relation of interest and a few con-
straints among them. In contrast, SHERLOCK fo-
cuses mainly on learning inference rules, but does so
without any manually specified seeds or constraints.
Craven et al(1998) also used ILP to help infor-
mation extraction on the Web, but required training
examples and focused on a single domain.
Two other notable systems that learn inference
rules from text are DIRT (Lin and Pantel, 2001)
and RESOLVER (Yates and Etzioni, 2007). How-
ever, both DIRT and RESOLVER learn only a lim-
ited set of rules capturing synonyms, paraphrases,
and simple entailments, not more expressive multi-
part Horn clauses. For example, these systems may
learn the rule X acquired Y =? X bought Y ,
which captures different ways of describing a pur-
chase. Applications of these rules often depend on
context (e.g., if a person acquires a skill, that does
not mean they bought the skill). To add the neces-
sary context, ISP (Pantel et al, 2007) learned selec-
tional preferences (Resnik, 1997) for DIRT?s rules.
The selectional preferences act as type restrictions
1089
Figure 1: Architecture of SHERLOCK. SHERLOCK learns
inference rules offline and provides them to the HOLMES
inference engine, which uses the rules to answer queries.
on the arguments, and attempt to filter out incorrect
inferences. While these approaches are useful, they
are strictly more limited than the rules learned by
SHERLOCK.
The Recognizing Textual Entailment (RTE)
task (Dagan et al, 2005) is to determine whether
one sentence entails another. Approaches to RTE
include those of Tatu and Moldovan (2007), which
generates inference rules from WordNet lexical
chains and a set of axiom templates, and Pennac-
chiotti and Zanzotto (2007), which learns inference
rules based on similarity across entailment pairs. In
contrast with this work, RTE systems reason over
full sentences, but benefit by being given the sen-
tences and training data. SHERLOCK operates over
simpler Web extractions, but is not given guidance
about which facts may interact.
3 System Description
SHERLOCK takes as input a large set of open domain
facts, and returns a set of weighted Horn-clause in-
ference rules. Other systems (e.g., HOLMES) use the
rules to answer questions, infer additional facts, etc.
SHERLOCK?s basic architecture is depicted in
Figure 1. To learn inference rules, SHERLOCK per-
forms the following steps:
1. Identify a ?productive? set of classes and in-
stances of those classes
2. Discover relations between classes
3. Learn inference rules using the discovered rela-
tions and determine the confidence in each rule
The first two steps help deal with the synonyms,
homonyms, and noise present in open-domain the-
ories by identifying a smaller, cleaner, and more co-
hesive set of facts to learn rules over.
SHERLOCK learns inference rules from a collec-
tion of open-domain extractions produced by TEX-
TRUNNER (Banko et al, 2007). The rules learned
by SHERLOCK are input to an inference engine and
used to find answers to a user?s query. In this paper,
SHERLOCK utilizes HOLMES as its inference engine
when answering queries, and uses extracted facts
of the form R(arg1, arg2) provided by the authors
of TEXTRUNNER, but the techniques presented are
more broadly applicable.
3.1 Finding Classes and Instances
SHERLOCK first searches for a set of well-defined
classes and class instances. Instances of the same
class tend to behave similarly, so identifying a good
set of instances will make it easier to discover the
general properties of the entire class.
Options for identifying interesting classes include
manually created methods (WordNet (Miller et al,
1990)), textual patterns (Hearst, 1992), automated
clustering (Lin and Pantel, 2002), and combina-
tions (Snow et al, 2006). We use Hearst patterns
because they are simple, capture how classes and in-
stances are mentioned in Web text, and yield intu-
itive, explainable groups.
Hearst (1992) identified a set of textual patterns
which indicate hyponymy (e.g., ?Class such as In-
stance?). Using these patterns, we extracted 29 mil-
lion (instance, class) pairs from a large Web crawl.
We then cleaned them using word stemming, nor-
malization, and by dropping modifiers.
Unfortunately, the patterns make systematic er-
rors (e.g., extracting Canada as the name of a city
from the phrase ?Toronto, Canada and other cities.?)
To address this issue, we discard the low frequency
classes of each instance. This heuristic reduces the
noise due to systematic error while still capturing the
important senses of each word. Additionally, we use
the extraction frequency to estimate the probability
that a particular mention of an instance refers to each
of its potential classes (e.g., New York appears as a
city 40% of the time, a state 35% of the time, and a
place, area, or center the rest of the time).
1090
Ambiguity presents a significant obstacle when
learning inference rules. For example, the corpus
contains the sentences ?broccoli contains this vita-
min? and ?this vitamin prevents scurvy,? but it is un-
clear if the sentences refer to the same vitamin. The
two main sources of ambiguity we observed are ref-
erences to a more general class instead of a specific
instance (e.g., ?vitamin?), and references to a person
by only their first or last name. We eliminate the
first by removing terms that frequently appear as the
class name with other instances, and the second by
removing common first and last names.
The 250 most frequently mentioned class names
include a large number of interesting classes (e.g.,
companies, cities, foods, nutrients, locations) as
well as ambiguous concepts (e.g., ideas, things). We
focus on the less ambiguous classes by eliminating
any class not appearing as a descendant of physical
entity, social group, physical condition, or event in
WordNet. Beyond this filtering we make no use of a
type hierarchy and treat classes independently.
In our corpus, we identify 1.1 million distinct,
cleaned (instance, class) pairs for 156 classes.
3.2 Discovering Relations between Classes
Next, SHERLOCK discovers how classes relate to
and interact with each other. Prior work in relation
discovery (Shinyama and Sekine, 2006) has investi-
gated the problem of finding relationships between
different classes. However, the goal of this work is
to learn rules on top of the discovered typed rela-
tions. We use a few simple heuristics to automati-
cally identify interesting relations.
For every pair of classes (C1, C2), we find a set
of typed, candidate relations from the 100 most fre-
quent relations in the corpus where the first argu-
ment is an instance of C1 and the second argument
is an instance of C2. For extraction terms with mul-
tiple senses (e.g., New York), we split their weight
based on how frequently they appear with each class
in the Hearst patterns.
However, many discovered relations are rare and
meaningless, arising from either an extraction error
or word-sense ambiguity. For example, the extrac-
tion ?Apple is based in Cupertino? gives some evi-
dence that a fruit may possibly be based in a city.
We attempt to filter out incorrectly-typed relations
using two heuristics. We first discard any relation
whose weighted frequency falls below a threshold,
since rare relations are more likely to arise due to
extraction errors or word-sense ambiguity. We also
remove relations whose pointwise mutual informa-
tion (PMI) is below a threshold T=exp(2) ? 7.4:
PMI(R(C1, C2)) =
p(R,C1, C2)
p(R, ?, ?) ? p(?, C1, ?) ? p(?, ?, C2)
where p(R, ?, ?) is the probability a random extrac-
tion has relation R, p(?, C1, ?) is the probability a
random extraction has an instance of C1 as its first
argument, p(?, ?, C2) is similar for the second argu-
ment, and p(R,C1, C2) is the probability that a ran-
dom extraction has relation R and instances of C1
and C2 as its first and second arguments, respec-
tively. A low PMI indicates the relation occurred by
random chance, which is typically due to ambiguous
terms or extraction errors.
Finally, we use two TEXTRUNNER specific clean-
ing heuristics: we ignore a small set of stop-relations
(?be?, ?have?, and ?be preposition?) and extractions
whose arguments are more than four tokens apart.
This process identifies 10,000 typed relations.
3.3 Learning Inference Rules
SHERLOCK attempts to learn inference rules for
each typed relation in turn. SHERLOCK receives a
target relation, R, a set of observed examples of the
relation, E+, a maximum clause length k, a mini-
mum support, s, and an acceptance threshold, t, as
input. SHERLOCK generates all first-order, definite
clauses up to length k, where R appears as the head
of the clause. It retains each clause that:
1. Contains no unbound variables
2. Infers at least s examples from E+
3. Scores at least t according to the score function
We now propose a novel score function, and empir-
ically validate our choice in Sections 4.3 and 4.4.
3.4 Evaluating Rules by Statistical Relevance
The problem of evaluating candidate rules has been
studied by many researchers, but typically in either a
supervised or propositional context whereas we are
learning first-order Horn-clauses from a noisy set of
positive examples. Moreover, due to the incomplete
nature of the input corpus and the imperfect yield of
1091
extraction?many true facts are not stated explicitly
in the set of ground assertions used by the learner to
evaluate rules.
The absence of negative examples, coupled with
noise, means that standard ILP evaluation functions
(e.g., (Quinlan, 1990) and (Dzeroski and Bratko,
1992)) are not appropriate. Furthermore, when eval-
uating a particular rule with consequent C and an-
tecedent A, it is natural to consider p(C|A) but, due
to missing data, this absolute probability estimate is
often misleading: in many cases C will hold given
A but the fact C is not mentioned in the corpus.
Thus to evaluate rules over extractions, we need
to consider relative probability estimates. I.e., is
p(C|A)  p(C)? If so, then A is said to be sta-
tistically relevant to C (Salmon et al, 1971).
Statistical relevance tries to infer the simplest set
of factors which explain an observation. It can be
viewed as searching for the simplest propositional
Horn-clause which increases the likelihood of a goal
proposition g. The two key ideas in determining sta-
tistical relevance are discovering factors which sub-
stantially increase the likelihood of g (even if the
probabilities are small in an absolute sense), and dis-
missing irrelevant factors.
To illustrate these concepts, consider the follow-
ing example. Suppose our goal is to predict if New
York City will have a storm (S). On an arbitrary
day, the probability of having a storm is fairly low
(p(S)  1). However, if we know that the atmo-
spheric pressure on that day is low, this substantially
increases the probability of having a storm (although
that absolute probability may still be small). Ac-
cording to the principle of statistical relevance, low
atmospheric pressure (LP ) is a factor which predicts
storms (S :- LP ), since p(S|LP ) p(S) .
The principle of statistical relevance also identi-
fies and removes irrelevant factors. For example, let
M denote the gender of New York?s mayor. Since
p(S|LP,M) p(S), it na??vely appears that storms
in New York depend on the gender of the mayor in
addition to the air pressure. The statistical relevance
principle sidesteps this trap by removing any fac-
tors which are conditionally independent of the goal,
given the remaining factors. For example, we ob-
serve p(S|LP )=p(S|LP,M), and so we say that M
is not statistically relevant to S. This test applies Oc-
cam?s razor by searching for the simplest rule which
explains the goal.
Statistical relevance appears useful in the open-
domain context, since all the necessary probabilities
can be estimated from only positive examples. Fur-
thermore, approximating relative probabilities in the
presence of missing data is much more reliable than
determining absolute probabilities.
Unfortunately, Salmon defined statistical rele-
vance in a propositional context. One technical
contribution of our work is to lift statistical rele-
vance to first order Horn-clauses as follows. For
the Horn-clause Head(v1, ..., vn):-Body(v1, ..., vm)
(where Body(v1, ..., vm) is a conjunction of function-
free, non-negated, first-order relations, and vi ? V
is the set of typed variables used in the rule), we say
the body helps explain the head if:
1. Observing an instance of the body substantially
increases the probability of observing the head.
2. The body contains no irrelevant (conditionally
independent) terms.
We evaluate conditional independence of terms
using ILP?s technique of ?-subsumption, ensuring
there is no more general clause that is similarly
predictive of the head. Formally, clause C1 ?-
subsumes clause C2 if and only if there exists a sub-
stitution ? such thatC1? ? C2 where each clause is
treated as the set of its literals. For example, R(x, y)
?-subsumes R(x, x), since {R(x, y)}? ? {R(x, x)}
when ?={y/x}. Intuitively, if C1 ?-subsumes C2,
it means that C1 is more general than C2.
Definition 1 A first-order Horn-clause
Head(...):-Body(...) is statistically relevant if
p(Head(...)|Body(...))  p(Head(...)) and if there
is no clause body B?(...)? ? Body(...) such that
p(Head(...)|Body(...)) ? p(Head(...)|B?(...))
In practice it is difficult to determine the proba-
bilities exactly, so when checking for statistical rele-
vance we ensure that the probability of the rule is at
least a factor t greater than the probability of any
subsuming rule, that is, p(Head(...)|Body(...)) ?
t ? p(Head(...)|B?(...))
We estimate p(Head(...)|B(...)) from the observed
facts by assuming values of Head(...) are generated
by sampling values of B(...) as follows: for variables
vs shared between Head(...) and B(...), we sample
1092
values of vs uniformly from all observed ground-
ings of B(...). For variables vi, if any, that appear
in Head(...) but not in B(...), we sample their values
according to a distribution p(vi|classi). We estimate
p(vi|classi) based on the relative frequency that vi
was extracted using a Hearst pattern with classi.
Finally, we ensure the differences are statistically
significant using the likelihood ratio statistic:
2Nr
X
H(...)?
{Head(...),?Head(...)}
p(H(...)|Body(...)) ? log
p(H(...)|Body(...))
p(H(...)|B?(...))
where p(?Head(...)|B(...)) = 1?p(Head(...)|B(...))
and Nr is the number of results inferred by the
rule Head(...):-Body(...). This test is distributed ap-
proximately as ?2 with one degree of freedom. It
is similar to the statistical significance test used in
mFOIL (Dzeroski and Bratko, 1992), but has two
modifications since SHERLOCK doesn?t have train-
ing data. In lieu of positive and negative examples,
we use whether or not the inferred head value was
observed, and compare against the distribution of a
subsuming clause B?(...) rather than a known prior.
This method of evaluating rules has two impor-
tant differences from ILP under a closed world as-
sumption. First, our probability estimates consider
the fact that examples provide varying amounts of
information. Second, statistical relevance finds rules
with large increases in relative probability, not nec-
essarily a large absolute probability. This is crucial
in an open domain setting where most facts are false,
which means the trivial rule that everything is false
will have high accuracy. Even for true rules, the ob-
served estimates p(Head(...)|Body(...))  1 due to
missing data and noise.
3.5 Making Inferences
In order to benefit from learned rules, we need
an inference engine; with its linear-time scalabil-
ity, HOLMES is a natural choice (Schoenmackers
et al, 2008). As input HOLMES requires a target
atom H(...), an evidence set E and weighted rule
set R as input. It performs a form of knowledge
based model construction (Wellman et al, 1992),
first finding facts using logical inference, then esti-
mating the confidence of each using a Markov Logic
Network (Richardson and Domingos, 2006).
Prior to running inference, it is necessary to assign
a weight to each rule learned by SHERLOCK. Since
the rules and inferences are based on a set of noisy
and incomplete extractions, the algorithms used for
both weight learning and inference should capture
the following characteristics of our problem:
C1. Any arbitrary unknown fact is highly unlikely
to be true.
C2. The more frequently a fact is extracted from the
Web, the more likely it is to be true. However,
facts in E should have a confidence bounded
by a threshold pmax < 1. E contains system-
atic extraction errors, so we want uncertainty in
even the most frequently extracted facts.
C3. An inference that combines uncertain facts
should be less likely than each fact it uses.
Next, we describe the needed modifications to the
weight learning and inference algorithm to achieve
the desired behavior.
3.5.1 Weight Learning
We use the discriminative weight learning proce-
dure described by Huynh and Mooney (2008). Set-
ting the weights involves counting the number of
true groundings for each rule in the data (Richard-
son and Domingos, 2006). However, the noisy na-
ture of Web extractions will make this an overesti-
mate. Consequently, we compute ni(E), the number
of true groundings of rule i, as follows:
ni(E) =
?
j
max
k
?
B(...)?Bodyijk
p(B(...)) (1)
where E is the evidence, j ranges over heads of the
rule, Bodyijk is the body of the kth grounding for
jth head of rule i, and p(B(...)) is approximated us-
ing a logistic function of the number of times B(...)
was extracted,1 scaled to be in the range [0,0.75].
This models C2 by giving increasing but bounded
confidence for more frequently extracted facts. In
practice, this also helps address C3 by giving longer
rules smaller values of ni, which reflects that infer-
ences arrived at through a combination of multiple,
noisy facts should have lower confidence. Longer
rules are also more likely to have multiple ground-
ings that infer a particular head, so keeping only the
most likely grounding prevents a head from receiv-
ing undue weight from any single rule.
1We note that this approximation is equivalent to an MLN
which uses only the two rules defined in 3.5.2
1093
Finally, we place a very strong Gaussian prior
(i.e., L2 penalty) on the weights. Longer rules have a
higher prior to capture the notion that they are more
likely to make incorrect inferences. Without a high
prior, each rule would receive an unduly high weight
as we have no negative examples.
3.5.2 Probabilistic Inference
After learning the weights, we add the following
two rules to our rule set:
1. H(...) with negative weight wprior
2. H(...):-ExtractedFrom(H(...),sentencei)
with weight 1
The first rule models C1, by saying that most facts
are false. The second rule models C2, by stating the
probability of fact depends on the number of times it
was extracted. The weights of these rules are fixed.
We do not include these rules during weight learning
as doing so swamps the effects of the other inference
rules (i.e., forces them to zero).
HOLMES attempts to infer the truth value of each
ground atom H(...) in turn by treating all other ex-
tractionsE in our corpus as evidence. Inference also
requires computing ni(E) which we do according to
Equation 1 as in weight learning.
4 Experiments
One can attempt to evaluate a rule learner by esti-
mating the quality of learned rules, or by measuring
their impact on a system that uses the learned rules.
Since the notion of ?rule quality? is vague except
in the context of an application, we evaluate SHER-
LOCK in the context of the HOLMES inference-based
question answering system.
Our evaluation focuses on three main questions:
1. Does inference utilizing learned Horn rules im-
prove the precision/recall of question answer-
ing and by how much?
2. How do different rule-scoring functions affect
the performance of learning?
3. What role does each of SHERLOCK?s compo-
nents have in the resulting performance?
4.1 Methodology
Our objective with rule learning was to improve the
system?s ability to answer questions such as ?What
foods prevent disease?? So we focus our evaluation
on the task of computing as many instances as pos-
sible of an atomic pattern Rel(x, y). In this exam-
ple, Rel would be bound to ?Prevents?, xwould have
type ?Food? and y would have type ?Disease.?
But which relations should be used in the test?
There is a large variance in behavior across relations,
so examining any particular relation may give mis-
leading results. Instead, we examine the global per-
formance of the system by querying HOLMES for
all open-domain relations identified in Section 3.2
as follows:
1. Score all candidate rules according to the rule
scoring metric M , accept all rules with a score
at least tM (tuned on a small development set of
rules), and learn weights for all accepted rules.
2. Find all facts inferred by the rules and use the
rule weights to estimate the fact probabilities.
3. Reduce type information. For each fact, (e.g.,
BasedIn(Diebold, Ohio)) which has been de-
duced with multiple type signatures (e.g., Ohio
is both a state and a geographic location), keep
only the one with maximum probability (i.e.,
conservatively assuming dependence).
4. Place all results into bins based on their proba-
bilities, and estimate the precision and the num-
ber of correct facts in the bin using a random
sample.
In these experiments we consider rules with up to
k = 2 relations in the body. We use a corpus of
1 million raw extractions, corresponding to 250,000
distinct facts. SHERLOCK found 5 million candidate
rules that infer at least two of the observed facts. Un-
less otherwise noted, we use SHERLOCK?s rule scor-
ing function to evaluate the rules (Section 3.4).
The results represent a wide variety of domains,
covering a total of 10,672 typed relations. We ob-
serve between a dozen and 2,375 distinct, ground
facts for each relation. SHERLOCK learned a total
of 31,000 inference rules.2 Learning all rules, rule
2The learned rules are available at:
http://www.cs.washington.edu/research/sherlock-hornclauses/
1094
 0
 0.2
 0.4
 0.6
 0.8
 1
0 350000 700000 1050000 1400000
P
re
c
is
io
n
 o
f 
In
fe
rr
e
d
 F
a
c
ts
Estimated Number of Correct Facts
Benefits of Inference using Learned Rules
Sherlock With Complex Rules
Sherlock With Only Simple Entailments
No Inference
Inferred by Simple
Entailment Rules
Inferred by 
Multi-Part
Horn Rules
Extracted
Facts
Figure 2: Inference discovers many facts which are not
explicitly extracted, identifying 3x as many high quality
facts (precision 0.8) and more than 5x as many facts over-
all. Horn-clauses with multiple relations in the body in-
fer 30% more correct facts than are identified by simpler
entailment rules, inferring many facts not present in the
corpus in any form.
weights, and performing the inference took 50 min-
utes on a 72 core cluster. However, we note that for
half of the relations SHERLOCK accepts no inference
rules, and remind the reader that the performance on
any particular relation may be substantially differ-
ent, and depends on the facts observed in the corpus
and on the rules learned.
4.2 Benefits of Inference
We first evaluate the utility of the learned Horn rules
by contrasting the precision and number of correct
and incorrect facts identified with and without infer-
ence over learned rules. We compare against two
simpler variants of SHERLOCK. The first is a no-
inference baseline that uses no rules, returning only
facts that are explicitly extracted. The second base-
line only accepts rules of length k = 1, allowing it to
make simple entailments but not more complicated
inferences using multiple facts.
Figure 2 compares the precision and estimated
number of correct facts with and without inference.
As is apparent, the learned inference rules substan-
tially increase the number of known facts, quadru-
pling the number of correct facts beyond what are
explicitly extracted.
The Horn rules having a body-length of two iden-
tify 30% more facts than the simpler length-one
rules. Furthermore, we find the Horn rules yield
slightly increased precision at comparable levels of
recall, although the increase is not statistically sig-
nificant. This behavior can be attributed to learn-
ing smaller weights for the length-two rules than
the length-one rules, allowing the length-two rules
provide a small amount of additional evidence as
to which facts are true, but typically not enough to
overcome the confidence of a more reliable length-
one rule.
Analyzing the errors, we found that about
one third of SHERLOCK?s mistakes are due
to metonymy and word sense ambiguity (e.g.,
confusing Vancouver, British Columbia with
Vancouver, Washington), one third are due to
inferences based on incorrectly-extracted facts
(e.g., inferences based on the incorrect fact
IsLocatedIn(New York, Suffolk County),
which was extracted from sentences like ?Deer
Park, New York is located in Suffolk County?),
and the rest are due to unsound or incorrect
inference rules (e.g., BasedIn(Company, City):-
BasedIn(Company, Country)? CapitalOf(City,
Country)). Without negative examples it is difficult
to distinguish correct rules from these unsound
rules, since the unsound rules are correct more often
than expected by chance.
Finally, we note that although simple, length-one
rules capture many of the results, in some respects
they are just rephrasing facts that are extracted in
another form. However, the more complex, length-
two rules synthesize facts extracted from multiple
pages, and infer results that are not stated anywhere
in the corpus.
4.3 Effect of Scoring Function
We now examine how SHERLOCK?s rule scoring
function affects its results, by comparing it with
three rule scoring functions used in prior work:
LIME. The LIME ILP system (McCreath and
Sharma, 1997) proposed a metric that generalized
Muggleton?s (1997) positive-only score function
by modeling noise and limited sample sizes.
M-Estimate of rule precision. This is a common
approach for handling noise in ILP (Dzeroski and
Bratko, 1992). It requires negative examples,
which we generated by randomly swapping argu-
ments between positive examples.
1095
 0
 0.2
 0.4
 0.6
 0.8
 1
0 500000 1000000 1500000 2000000 2500000
P
re
c
is
io
n
 o
f 
In
fe
rr
e
d
 F
a
c
ts
Estimated Number of Correct Facts
Comparison of Rule Scoring Functions
Sherlock
LIME
M-Estimate
L1 Reg.
Figure 3: SHERLOCK identifies rules that lead to more
accurate inferences over a large set of open-domain ex-
tracted facts, deducing 2x as many facts at precision 0.8.
L1 Regularization. As proposed in (Huynh and
Mooney, 2008), this learns weights for all can-
didate rules using L1-regularization (encouraging
sparsity) instead of L2-regularization, and retains
only those with non-zero weight.
Figure 3 compares the precision and estimated
number of correct facts inferred by the rules of
each scoring function. SHERLOCK has consistently
higher precision, and finds twice as many correct
facts at precision 0.8.
M-Estimate accepted eight times as many rules as
SHERLOCK, increasing the number of inferred facts
at the cost of precision and longer inference times.
Most of the errors in M-Estimate and L1 Regulariza-
tion come from incorrect or unsound rules, whereas
most of the errors for LIME stem from systematic
extraction errors.
4.4 Scoring Function Design Decisions
SHERLOCK requires a rule to have statistical rele-
vance and statistical significance. We perform an
ablation study to understand how each of these con-
tribute to SHERLOCK?s results.
Figure 4 compares the precision and estimated
number of correct facts obtained when requiring
rules to be only statistically relevant, only statisti-
cally significant, or both. As is expected, there is
a precision/recall tradeoff. SHERLOCK has higher
precision, finding more than twice as many results at
precision 0.8 and reducing the error by 39% at a re-
call of 1 million correct facts. Statistical significance
finds twice as many correct facts as SHERLOCK, but
the extra facts it discovers have precision < 0.4.
 0
 0.2
 0.4
 0.6
 0.8
 1
0 500000 1000000 1500000 2000000 2500000 3000000
P
re
c
is
io
n
 o
f 
In
fe
rr
e
d
 F
a
c
ts
Estimated Number of Correct Facts
Design Decisions of Sherlock?s Scoring Function
Sherlock
Statistical Relevance
Statistical Significance
Figure 4: By requiring rules to have both statistical rel-
evance and statistical significance, SHERLOCK rejects
many error-prone rules that are accepted by the metrics
individually. The better rule set yields more accurate in-
ferences, but identifies fewer correct facts.
Comparing the rules accepted in each case, we
found that statistical relevance and statistical signifi-
cance each accepted about 180,000 rules, compared
to about 31,000 for SHERLOCK. The smaller set
of rules accepted by SHERLOCK not only leads to
higher precision inferences, but also speeds up in-
ference time by a factor of seven.
In a qualitative analysis, we found the statisti-
cal relevance metric overestimates probabilities for
sparse rules, which leads to a number of very high
scoring but meaningless rules. The statistical signif-
icance metric handles sparse rules better, but is still
overconfident in the case of many unsound rules.
4.5 Analysis of Weight Learning
Finally, we empirically validate the modifications of
the weight learning algorithm from Section 3.5.1.
The learned-rule weights only affect the probabil-
ities of the inferred facts, not the inferred facts them-
selves, so to measure the influence of the weight
learning algorithm we examine the recall at preci-
sion 0.8 and the area under the precision-recall curve
(AuC). We build a test set by holding SHERLOCK?s
inference rules constant and randomly sampling 700
inferred facts. We test the effects of:
? Fixed vs. Variable Penalty - Do we use the
same L2 penalty on the weights for all rules or
a stronger L2 penalty for longer rules?
? Full vs. Weighted Grounding Counts - Do we
count all unweighted rule groundings (as in
(Huynh and Mooney, 2008)), or only the best
weighted one (as in Equation 1)?
1096
Recall
(p=0.8) AuC
Variable Penalty, Weighted 0.35 0.735
Counts (used by SHERLOCK)
Variable Penalty, Full Counts 0.28 0.726
Fixed Penalty, Weighted Counts 0.27 0.675
Fixed Penalty, Full Counts 0.17 0.488
Table 2: SHERLOCK?s modified weight learning algo-
rithm gives better probability estimates over noisy and in-
complete Web extractions. Most of the gains come from
penalizing longer rules more, but using weighted ground-
ing counts further improves recall by 0.07, which corre-
sponds to almost 100,000 additional facts at precision 0.8.
We vary each of these independently, and give the
performance of all 4 combinations in Table 2.
The modifications from Section 3.5.1 improve
both the AuC and the recall at precision 0.8. Most
of the improvement is due to using stronger penal-
ties on longer rules, but using the weighted counts
in Equation 1 improves recall by a factor of 1.25 at
precision 0.8. While this may not seem like much,
the scale is such that it leads to almost 100,000 ad-
ditional correct facts at precision 0.8.
5 Conclusion
This paper addressed the problem of learning first-
order Horn clauses from the noisy and heteroge-
neous corpus of open-domain facts extracted from
Web text. We showed that SHERLOCK is able
to learn Horn clauses in a large-scale, domain-
independent manner. Furthermore, the learned rules
are valuable, because they infer a substantial number
of facts which were not extracted from the corpus.
While SHERLOCK belongs to the broad category
of ILP learners, it has a number of novel features that
enable it to succeed in the challenging, open-domain
context. First, SHERLOCK automatically identifies
a set of high-quality extracted facts, using several
simple but effective heuristics to defeat noise and
ambiguity. Second, SHERLOCK is unsupervised and
does not require negative examples; this enables it to
scale to an unbounded number of relations. Third, it
utilizes a novel rule-scoring function, which is toler-
ant of the noise, ambiguity, and missing data issues
prevalent in facts extracted from Web text. The ex-
periments in Figure 3 show that, for open-domain
facts, SHERLOCK?s method represents a substantial
improvement over traditional ILP scoring functions.
Directions for future work include inducing
longer inference rules, investigating better methods
for combining the rules, allowing deeper inferences
across multiple rules, evaluating our system on other
corpora and devising better techniques for handling
word sense ambiguity.
Acknowledgements
We thank Sonal Gupta and the anonymous review-
ers for their helpful comments. This research was
supported in part by NSF grant IIS-0803481, ONR
grant N00014-08-1-0431, the WRF / TJ Cable Pro-
fessorship and carried out at the University of Wash-
ington?s Turing Center. The University of Washing-
ton gratefully acknowledges the support of Defense
Advanced Research Projects Agency (DARPA) Ma-
chine Reading Program under Air Force Research
Laboratory (AFRL) prime contract nos. FA8750-
09-C-0179 and FA8750-09-C-0181. Any opinions,
findings, and conclusion or recommendations ex-
pressed in this material are those of the author(s) and
do not necessarily reflect the view of the DARPA,
AFRL, or the US government.
References
M. Banko, M. Cafarella, S. Soderland, M. Broadhead,
and O. Etzioni. 2007. Open information extraction
from the Web. In Procs. of IJCAI.
Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam R. Hruschka Jr., and Tom M.
Mitchell. 2010. Toward an architecture for never-
ending language learning. In Proceedings of the
Twenty-Fourth Conference on Artificial Intelligence
(AAAI 2010).
M. Craven, D. DiPasquo, D. Freitag, A.K. McCallum,
T. Mitchell, K. Nigam, and S. Slattery. 1998. Learn-
ing to Extract Symbolic Knowledge from the World
Wide Web. In Procs. of the 15th Conference of the
American Association for Artificial Intelligence, pages
509?516, Madison, US. AAAI Press, Menlo Park, US.
I. Dagan, O. Glickman, and B. Magnini. 2005. The
PASCAL Recognising Textual Entailment Challenge.
Proceedings of the PASCAL Challenges Workshop on
Recognising Textual Entailment, pages 1?8.
S. Dzeroski and I. Bratko. 1992. Handling noise in in-
ductive logic programming. In Proceedings of the 2nd
1097
International Workshop on Inductive Logic Program-
ming.
M. Hearst. 1992. Automatic Acquisition of Hyponyms
from Large Text Corpora. In Procs. of the 14th In-
ternational Conference on Computational Linguistics,
pages 539?545, Nantes, France.
T.N. Huynh and R.J. Mooney. 2008. Discriminative
structure and parameter learning for Markov logic net-
works. In Proceedings of the 25th international con-
ference on Machine learning, pages 416?423. ACM.
Stanley Kok and Pedro Domingos. 2005. Learning the
structure of markov logic networks. In ICML ?05:
Proceedings of the 22nd international conference on
Machine learning, pages 441?448, New York, NY,
USA. ACM.
N. Lavrac and S. Dzeroski, editors. 2001. Relational
Data Mining. Springer-Verlag, Berlin, September.
D. Lin and P. Pantel. 2001. DIRT ? Discovery of Infer-
ence Rules from Text. In KDD.
D. Lin and P. Pantel. 2002. Concept discovery from text.
In Proceedings of the 19th International Conference
on Computational linguistics (COLING-02), pages 1?
7.
E. McCreath and A. Sharma. 1997. ILP with noise
and fixed example size: a Bayesian approach. In Pro-
ceedings of the Fifteenth international joint conference
on Artifical intelligence-Volume 2, pages 1310?1315.
Morgan Kaufmann Publishers Inc.
G. Miller, R. Beckwith, C. Fellbaum, D. Gross, and
K. Miller. 1990. Introduction to WordNet: An on-line
lexical database. International Journal of Lexicogra-
phy, 3(4):235?312.
S. Muggleton. 1995. Inverse entailment and Progol.
New Generation Computing, 13:245?286.
S. Muggleton. 1997. Learning from positive data. Lec-
ture Notes in Computer Science, 1314:358?376.
P. Pantel, R. Bhagat, B. Coppola, T. Chklovski, and
E. Hovy. 2007. ISP: Learning inferential selectional
preferences. In Proceedings of NAACL HLT, vol-
ume 7, pages 564?571.
M. Pennacchiotti and F.M. Zanzotto. 2007. Learning
Shallow Semantic Rules for Textual Entailment. Pro-
ceedings of RANLP 2007.
J. R. Quinlan. 1990. Learning logical definitions from
relations. Machine Learning, 5:239?2666.
Philip Resnik. 1997. Selectional preference and sense
disambiguation. In Proc. of the ACL SIGLEX Work-
shop on Tagging Text with Lexical Semantics: Why,
What, and How?
M. Richardson and P. Domingos. 2006. Markov Logic
Networks. Machine Learning, 62(1-2):107?136.
W.C. Salmon, R.C. Jeffrey, and J.G. Greeno. 1971. Sta-
tistical explanation & statistical relevance. Univ of
Pittsburgh Pr.
S. Schoenmackers, O. Etzioni, and D. Weld. 2008. Scal-
ing Textual Inference to the Web. In Procs. of EMNLP.
Y. Shinyama and S. Sekine. 2006. Preemptive informa-
tion extraction using unrestricted relation discovery.
In Procs. of HLT/NAACL.
R. Snow, D. Jurafsky, and A. Y. Ng. 2006. Semantic
taxonomy induction from heterogenous evidence. In
COLING/ACL 2006.
M. Tatu and D. Moldovan. 2007. COGEX at RTE3. In
Proceedings of the ACL-PASCAL Workshop on Textual
Entailment and Paraphrasing, pages 22?27.
M.P. Wellman, J.S. Breese, and R.P. Goldman. 1992.
From knowledge bases to decision models. The
Knowledge Engineering Review, 7(1):35?53.
A. Yates and O. Etzioni. 2007. Unsupervised resolution
of objects and relations on the Web. In Procs. of HLT.
1098
Proceedings of the NAACL HLT 2010 First International Workshop on Formalisms and Methodology for Learning by Reading, pages 87?95,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Machine Reading at the University of Washington
Hoifung Poon, Janara Christensen, Pedro Domingos, Oren Etzioni, Raphael Hoffmann,
Chloe Kiddon, Thomas Lin, Xiao Ling, Mausam, Alan Ritter, Stefan Schoenmackers,
Stephen Soderland, Dan Weld, Fei Wu, Congle Zhang
Department of Computer Science & Engineering
University of Washington
Seattle, WA 98195
{hoifung,janara,pedrod,etzioni,raphaelh,chloe,tlin,xiaoling,mausam,
aritter,stef,soderland,weld,wufei,clzhang}@cs.washington.edu
Abstract
Machine reading is a long-standing goal of AI
and NLP. In recent years, tremendous progress
has been made in developing machine learning
approaches for many of its subtasks such as
parsing, information extraction, and question
answering. However, existing end-to-end so-
lutions typically require substantial amount of
human efforts (e.g., labeled data and/or man-
ual engineering), and are not well poised for
Web-scale knowledge acquisition. In this pa-
per, we propose a unifying approach for ma-
chine reading by bootstrapping from the easi-
est extractable knowledge and conquering the
long tail via a self-supervised learning pro-
cess. This self-supervision is powered by joint
inference based on Markov logic, and is made
scalable by leveraging hierarchical structures
and coarse-to-fine inference. Researchers at
the University of Washington have taken the
first steps in this direction. Our existing work
explores the wide spectrum of this vision and
shows its promise.
1 Introduction
Machine reading, or learning by reading, aims to
extract knowledge automatically from unstructured
text and apply the extracted knowledge to end tasks
such as decision making and question answering. It
has been a major goal of AI and NLP since their
early days. With the advent of the Web, the billions
of online text documents contain virtually unlimited
amount of knowledge to extract, further increasing
the importance and urgency of machine reading.
In the past, there has been a lot of progress in
automating many subtasks of machine reading by
machine learning approaches (e.g., components in
the traditional NLP pipeline such as POS tagging
and syntactic parsing). However, end-to-end solu-
tions are still rare, and existing systems typically re-
quire substantial amount of human effort in manual
engineering and/or labeling examples. As a result,
they often target restricted domains and only extract
limited types of knowledge (e.g., a pre-specified re-
lation). Moreover, many machine reading systems
train their knowledge extractors once and do not
leverage further learning opportunities such as ad-
ditional text and interaction with end users.
Ideally, a machine reading system should strive to
satisfy the following desiderata:
End-to-end: the system should input raw text, ex-
tract knowledge, and be able to answer ques-
tions and support other end tasks;
High quality: the system should extract knowledge
with high accuracy;
Large-scale: the system should acquire knowledge
at Web-scale and be open to arbitrary domains,
genres, and languages;
Maximally autonomous: the system should incur
minimal human effort;
Continuous learning from experience: the
system should constantly integrate new infor-
mation sources (e.g., new text documents) and
learn from user questions and feedback (e.g.,
via performing end tasks) to continuously
improve its performance.
These desiderata raise many intriguing and chal-
lenging research questions. Machine reading re-
search at the University of Washington has explored
87
a wide spectrum of solutions to these challenges and
has produced a large number of initial systems that
demonstrated promising performance. During this
expedition, an underlying unifying vision starts to
emerge. It becomes apparent that the key to solving
machine reading is to:
1. Conquer the long tail of textual knowledge via
a self-supervised learning process that lever-
ages data redundancy to bootstrap from the
head and propagates information down the long
tail by joint inference;
2. Scale this process to billions of Web documents
by identifying and leveraging ubiquitous struc-
tures that lead to sparsity.
In Section 2, we present this vision in detail, iden-
tify the major dimensions these initial systems have
explored, and propose a unifying approach that sat-
isfies all five desiderata. In Section 3, we reivew
machine reading research at the University of Wash-
ington and show how they form synergistic effort
towards solving the machine reading problem. We
conclude in Section 4.
2 A Unifying Approach for Machine
Reading
The core challenges to machine reading stem from
the massive scale of the Web and the long-tailed dis-
tribution of textual knowledge. The heterogeneous
Web contains texts that vary substantially in subject
matters (e.g., finance vs. biology) and writing styles
(e.g., blog posts vs. scientific papers). In addition,
natural languages are famous for their myraid vari-
ations in expressing the same meaning. A fact may
be stated in a straightforward way such as ?kale con-
tains calcium?. More often though, it may be stated
in a syntactically and/or lexically different way than
as phrased in an end task (e.g., ?calcium is found in
kale?). Finally, many facts are not even stated ex-
plicitly, and must be inferred from other facts (e.g.,
?kale prevents osteoporosis? may not be stated ex-
plicitly but can be inferred by combining facts such
as ?kale contains calcium? and ?calcium helps pre-
vent osteoporosis?). As a result, machine reading
must not rely on explicit supervision such as manual
rules and labeled examples, which will incur pro-
hibitive cost in the Web scale. Instead, it must be
able to learn from indirect supervision.





	






Figure 1: A unifying vision for machine reading: boot-
strap from the head regime of the power-law distribu-
tion of textual knowledge, and conquer the long tail in
a self-supervised learning process that raises certainty on
sparse extractions by propagating information via joint
inference from frequent extractions.
A key source of indirect supervision is meta
knowledge about the domains. For example, the
TextRunner system (Banko et al, 2007) hinges on
the observation that there exist general, relation-
independent patterns for information extraction. An-
other key source of indirect supervision is data re-
dundancy. While a rare extracted fact or inference
pattern may arise by chance of error, it is much less
likely so for the ones with many repetitions (Downey
et al, 2010). Such highly-redundant knowledge can
be extracted easily and with high confidence, and
can be leveraged for bootstrapping. For knowledge
that resides in the long tail, explicit forms of redun-
dancy (e.g., identical expressions) are rare, but this
can be circumvented by joint inference. For exam-
ple, expressions that are composed with or by sim-
ilar expressions probably have the same meaning;
the fact that kale prevents osteoporosis can be de-
rived by combining the facts that kale contains cal-
cium and that calcium helps prevent osteoporosis via
a transitivity-through inference pattern. In general,
joint inference can take various forms, ranging from
simple voting to shrinkage in a probabilistic ontol-
ogy to sophisticated probabilistic reasoning based
on a joint model. Simple ones tend to scale bet-
ter, but their capability in propagating information
is limited. More sophisticated methods can uncover
implicit redundancy and propagate much more in-
88
formation with higher quality, yet the challenge is
how to make them scale as well as simple ones.
To do machine reading, a self-supervised learning
process, informed by meta knowledege, stipulates
what form of joint inference to use and how. Effec-
tively, it increases certainty on sparse extractions by
propagating information from more frequent ones.
Figure 1 illustrates this unifying vision.
In the past, machine reading research at the Uni-
versity of Washington has explored a variety of so-
lutions that span the key dimensions of this uni-
fying vision: knowledge representation, bootstrap-
ping, self-supervised learning, large-scale joint in-
ference, ontology induction, continuous learning.
See Section 3 for more details. Based on this ex-
perience, one direction seems particularly promising
that we would propose here as our unifying approach
for end-to-end machine reading:
Markov logic is used as the unifying framework for
knowledge representation and joint inference;
Self-supervised learning is governed by a joint
probabilistic model that incorporates a small
amount of heuristic knowledge and large-scale
relational structures to maximize the amount
and quality of information to propagate;
Joint inference is made scalable to the Web by
coarse-to-fine inference.
Probabilistic ontologies are induced from text to
guarantee tractability in coarse-to-fine infer-
ence. This ontology induction and popula-
tion are incorporated into the joint probabilistic
model for self-supervision;
Continuous learning is accomplished by combin-
ing bootstrapping and crowdsourced content
creation to synergistically improve the reading
system from user interaction and feedback.
A distinctive feature of this approach is its empha-
sis on using sophisticated joint inference. Recently,
joint inference has received increasing interest in
AI, machine learning, and NLP, with Markov logic
(Domingos and Lowd, 2009) being one of the lead-
ing unifying frameworks. Past work has shown that
it can substantially improve predictive accuracy in
supervised learning (e.g., (Getoor and Taskar, 2007;
Bakir et al, 2007)). We propose to build on these ad-
vances, but apply joint inference beyond supervised
learning, with labeled examples supplanted by indi-
rect supervision.
Another distinctive feature is that we propose
to use coarse-to-fine inference (Felzenszwalb and
McAllester, 2007; Petrov, 2009) as a unifying
framework to scale inference to the Web. Es-
sentially, coarse-to-fine inference leverages the
sparsity imposed by hierarchical structures that
are ubiquitous in human knowledge (e.g., tax-
onomies/ontologies). At coarse levels (top levels in
a hierarchy), ambiguities are rare (there are few ob-
jects and relations), and inference can be conducted
very efficiently. The result is then used to prune un-
promising refinements at the next level. This process
continues down the hierarchy until decision can be
made. In this way, inference can potentially be sped
up exponentially, analogous to binary tree search.
Finally, we propose a novel form of continuous
learning by leveraging the interaction between the
system and end users to constantly improve the per-
formance. This is straightforward to do in our ap-
proach given the self-supervision process and the
availability of powerful joint inference. Essentially,
when the system output is applied to an end task
(e.g., answering questions), the feedback from user
is collected and incorporated back into the system
as a bootstrap source. The feedback can take the
form of explicit supervision (e.g., via community
content creation or active learning) or indirect sig-
nals (e.g., click data and query logs). In this way,
we can bootstrap an online community by an initial
machine reading system that provides imperfect but
valuable service in end tasks, and continuously im-
prove the quality of system output, which attracts
more users with higher degree of participation, thus
creating a positive feedback loop and raising the ma-
chine reading performance to a high level that is dif-
ficult to attain otherwise.
3 Summary of Progress to Date
The University of Washington has been one of the
leading places for machine reading research and has
produced many cutting-edge systems, e.g., WIEN
(first wrapper induction system for information ex-
traction), Mulder (first fully automatic Web-scale
question answering system), KnowItAll/TextRunner
(first systems to do open-domain information extrac-
89
tion from the Web corpus at large scale), Kylin (first
self-supervised system for Wikipedia-based infor-
mation extraction), UCR (first unsupervised corefer-
ence resolution system that rivals the performance of
supervised systems), Holmes (first Web-scale joint
inference system), USP (first unsupervised system
for semantic parsing).
Figure 2 shows the evolution of the major sys-
tems; dashed lines signify influence in key ideas
(e.g., Mulder inspires KnowItAll), and solid lines
signify dataflow (e.g., Holmes inputs TextRunner tu-
ples). These systems span a wide spectrum in scal-
ability (assessed by speed and quantity in extrac-
tion) and comprehension (assessed by unit yield of
knowledge at a fixed precision level). At one ex-
treme, the TextRunner system is highly scalable, ca-
pable of extracting billions of facts, but it focuses on
shallow extractions from simple sentences. At the
other extreme, the USP and LOFT systems achieve
much higher level of comprehension (e.g., in a task
of extracting knowledge from biomedical papers and
answering questions, USP obtains more than three
times as many correct answers as TextRunner, and
LOFT obtains more than six times as many correct
answers as TextRunner), but are much less scalable
than TextRunner.
In the remainder of the section, we review the
progress made to date and identify key directions for
future work.
3.1 Knowledge Representation and Joint
Inference
Knowledge representations used in these systems
vary widely in expressiveness, ranging from sim-
ple ones like relation triples (<subject, relation,
object>; e.g., in KnowItAll and TextRunner), to
clusters of relation triples or triple components (e.g.,
in SNE, RESOLVER), to arbitrary logical formulas
and their clusters (e.g., in USP, LOFT). Similarly,
a variety forms of joint inference have been used,
ranging from simple voting to heuristic rules to so-
phisticated probabilistic models. All these can be
compactly encoded in Markov logic (Domingos and
Lowd, 2009), which provides a unifying framework
for knowledge representation and joint inference.
Past work at Washington has shown that in su-
pervised learning, joint inference can substantially
improve predictive performance on tasks related to
machine reading (e.g., citation information extrac-
tion (Poon and Domingos, 2007), ontology induc-
tion (Wu and Weld, 2008), temporal information
extraction (Ling and Weld, 2010)). In addition, it
has demonstrated that sophisticated joint inference
can enable effective learning without any labeled
information (UCR, USP, LOFT), and that joint in-
ference can scale to millions of Web documents by
leveraging sparsity in naturally occurring relations
(Holmes, Sherlock), showing the promise of our uni-
fying approach.
Simpler representations limit the expressiveness
in representing knowledge and the degree of sophis-
tication in joint inference, but they currently scale
much better than more expressive ones. A key direc-
tion for future work is to evaluate this tradeoff more
thoroughly, e.g., for each class of end tasks, to what
degree do simple representations limit the effective-
ness in performing the end tasks? Can we automate
the choice of representations to strike the best trade-
off for a specific end task? Can we advance joint
inference algorithms to such a degree that sophisti-
cated inference scales as well as simple ones?
3.2 Bootstrapping
Past work at Washington has identified and lever-
aged a wide range of sources for bootstrapping. Ex-
amples include Wikipedia (Kylin, KOG, IIA, WOE,
WPE), Web lists (KnowItAll, WPE), Web tables
(WebTables), Hearst patterns (KnowItAll), heuristic
rules (TextRunner), semantic role labels (SRL-IE),
etc.
In general, potential bootstrap sources can be
broadly divided into domain knowledge (e.g., pat-
terns and rules) and crowdsourced contents (e.g., lin-
guistic resources, Wikipedia, Amazon Mechanical
Turk, the ESP game).
A key direction for future work is to combine
bootstrapping with crowdsourced content creation
for continuous learning. (Also see Subsection 3.6.)
3.3 Self-Supervised Learning
Although the ways past systems conduct self-
supervision vary widely in detail, they can be di-
vided into two broad categories. One uses heuristic
rules that exploit existing semi-structured resources
to generate noisy training examples for use by su-
pervised learning methods and with cotraining (e.g.,
90

	


		

	
 	