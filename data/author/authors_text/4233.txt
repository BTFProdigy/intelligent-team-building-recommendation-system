Proceedings of the ACL-HLT 2011 Student Session, pages 30?35,
Portland, OR, USA 19-24 June 2011. c?2011 Association for Computational Linguistics
A Latent Topic Extracting Method based on Events in a Document
and its Application
Risa Kitajima
Ochanomizu University
kitajima.risa@is.ocha.ac.jp
Ichiro Kobayashi
Ochanomizu University
koba@is.ocha.ac.jp
Abstract
Recently, several latent topic analysis methods
such as LSI, pLSI, and LDA have been widely
used for text analysis. However, those meth-
ods basically assign topics to words, but do not
account for the events in a document. With
this background, in this paper, we propose a
latent topic extracting method which assigns
topics to events. We also show that our pro-
posed method is useful to generate a document
summary based on a latent topic.
1 Introduction
Recently, several latent topic analysis methods such
as Latent Semantic Indexing (LSI) (Deerwester
et al, 1990), Probabilistic LSI (pLSI) (Hofmann,
1999), and Latent Dirichlet Allocation (LDA) (Blei
et al, 2003) have been widely used for text analy-
sis. However, those methods basically assign top-
ics to words, but do not account for the events in a
document. Here, we define a unit of informing the
content of document at the level of sentence as an
?Event? 1, and propose a model that treats a docu-
ment as a set of Events. We use LDA as a latent
topic analysis method, and assign topics to Events
in a document. To examine our proposed method?s
performance on extracting latent topics from a doc-
ument, we compare the accuracy of our method to
that of the conventional methods through a common
document retrieval task. Furthermore, as an appli-
cation of our method, we apply it to a query-biased
document summarization (Tombros and Sanderson,
1For the definition of an Event, see Section 3.
1998; Okumura and Mochizuki, 2000; Berger and
Mittal, 2000) to verify that the method is useful for
various applications.
2 Related Studies
Suzuki et al (2010) proposed a flexible latent top-
ics inference in which topics are assigned to phrases
in a document. Matsumoto et al (2005) showed
that the accuracy of document classification will be
improved by introducing a feature dealing with the
dependency relationships among words.
In case of assigning topics to words, it is likely
that two documents, which have the same word fre-
quency in themselves, tend to be estimated as they
have the same topic probablistic distribution without
considering the dependency relation among words.
However, there are many cases where the relation-
ship among words is regarded as more important
rather than the frequency of words as the feature
identifying the topics of a document. For example,
in case of classifying opinions to objects in a doc-
ument, we have to identify what sort of opinion is
assigned to the target objects, therefore, we have to
focus on the relationship among words in a sentence,
not only on the frequent words appeared in a docu-
ment. For this reason, we propose a method to as-
sign topics to Events instead of words.
As for studies on document summarization, there
are various methods, such as the method based on
word frequency (Luhn, 1958; Nenkova and Van-
derwende, 2005), and the method based on a graph
(Radev, 2004; Wan and Yang, 2006). Moreover,
several methods using a latent topic model have
been proposed (Bing et al, 2005; Arora and Ravin-
30
dran, 2008; Bhandari et al, 2008; Henning, 2009;
Haghighi and Vanderwende, 2009). In those stud-
ies, the methods estimate a topic distribution on each
sentence in the same way as the latent semantic anal-
ysis methods normally do that on each document,
and generate a summary based on the distribution.
We also show that our proposed method is useful for
the document summarization based on extracting la-
tent topics from sentences.
3 Topic Extraction based on Events
In this study, since we deal with a document as a
set of Events, we extract Events from each docu-
ment; define some of the extracted Events as the in-
dex terms for the whole objective documents; and
then make an Event-by-document matrix consisting
of the frequency of Events to the documents. A la-
tent topic distribution is estimated based on this ma-
trix.
3.1 Definition of an Event
In this study, we define a pair of words in depen-
dent relation which meets the following conditions:
(Subject, Predicate) or (Predicate1, Predicate2) , as
an Event. A noun and unknown words correspond
to Subject, while a verb, adjective and adjective
verb correspond to Predicate. To extract these pairs,
we analyze the dependency structure of sentences
in a document by a Japanese dependency structure
analyzer, CaboCha 2. The reason why we define
(Predicete1, Predicate2) as an Event is because we
recognized the necessity of such type of an Event by
investigating the extracted pairs of words and com-
paring them with the content of the target document
in preliminary experiments, and could not extract
any Event in case of extracting an Event from the
sentences without subject.
3.2 Making an Event-by-Document Matrix
In making a word-by-document matrix, high-
frequent words appeared in any documents, and ex-
tremely infrequent words are usually not included in
the matrix. In our method, high-frequent Events like
the former case were not observed in preliminary ex-
periments. We think the reason for this is because an
Event, a pair of words, can be more meaningful than
2http://chasen.org/ taku/software/cabocha/
a single word, therefore, an Event is particularly a
good feature to express the meaning of a document.
Meanwhile, the average number of Events per sen-
tence is 4.90, while the average number of words per
sentence is 8.93. A lot of infrequent Events were ob-
served in the experiments because of the nature of an
Event, i.e., a pair of words. This means that the same
process of making a word-by-document matrix can-
not be applied to making an Event-by-document ma-
trix because the nature of an Event as a feature ex-
pressing a document is different from that of a word.
In concrete, if the events, which once appear in doc-
uments, would be removed from the candidates to
be a part of a document vector, there might be a case
where the constructed document vector does not re-
flect the content of the original documents. Consid-
ering this, in order to make the constructed docu-
ment vector reflect the content of the original doc-
uments, we do not remove the Event only itself ex-
tracted from a sentence, even though it appears only
once in a document.
3.3 Estimating a Topic Distribution
After making an Event-by-document matrix, a la-
tent topic distribution of each Event is estimated by
means of Latent Dirichlet Allocation. Latent Dirich-
let Allocation is a generative probabilistic model that
allows multiple topics to occur in a document, and
gets the topic distribution based on the idea that
each topic emerges in a document based on a certain
probability. Each topic is expressed as a multinomial
distribution of words.
In this study, since a topic is assigned to an Event,
each topic is expressed as a multinomial distribution
of Events. As a method to estimate a topic distri-
bution, while a variational Bayes method (Blei et
al., 2003) and its application (Teh et al, 2006) have
been proposed, in this study we use Gibbs sampling
method (Grififths and Steyvers, 2004). Furthermore,
we define a sum of topic distributions of the events
in a query as the topic distribution of the query.
4 Performance Evaluation Experiment
Through a common document retrieval task, we
compare our method with the conventional method
and evaluate both of them. In concrete, we regard
the documents which have a similar topic distribu-
31
tion to a query?s topic distribution as the result of
retrieval, and then examine whether or not the esti-
mated topic distribution can represent the latent se-
mantics of each document based on the accuracy of
retrieval results. Henceforth, we call the conven-
tional word-based LDA ?wordLDA? and our pro-
posed event-based LDA ?eventLDA?.
4.1 Measures for Topic Distribution
As measures for identifying the similarity of
topic distribution, we adopt Kullback-Leibler Di-
vergence (Kullback and Leibler, 1951), Symmetric
Kullback-Leibler Divergence (Kullback and Leibler,
1951), Jensen-Shannon Divergence (Lin, 2002), and
cosine similarity. As for wordLDA, Henning (2009)
has reported that Jensen-Shannon Divergence shows
the best performance among the above measures in
terms of estimating the similarity between two sen-
tences. We also compare the performance of the
above measures when using eventLDA.
4.2 Experimental Settings
As for the documents used in the experiment, we use
a set of data including users? reviews and their eval-
uations for hotels and their facilities, provided by
Rakuten Travel3. Each review has five-grade eval-
uations of a hotel?s facilities such as room, location,
and so on. Since the data hold the relationships be-
tween objects and their evaluations, therefore, it is
said that they are appropriate for the performance
evaluation of our method because the relationship is
usually expressed in a pair of words, i.e., an Event.
The query we used in the experiment was ?a room is
good?. The total number of documents is 2000, con-
sisting of 1000 documents randomly selected from
the users? reviews whose evaluation for ?a room? is
1 (bad) and 1000 documents randomly selected from
the reviews whose evaluation is 5 (good). The latter
1000 documents are regarded as the objective doc-
uments in retrieval. Because of this experiment de-
sign, it is clear that the random choice for retrieving
?good? vs. ?bad? is 50%. As for the evaluation mea-
sure, we adopt 11-point interpolated average preci-
sion.
In this experiment, a comparison between the
both methods, i.e., wordLDA and eventLDA, is con-
3http://travel.rakuten.co.jp/
ducted from the viewpoints of the proper number
of topics and the most useful measure to estimate
similarity. At first, we use Jensen-Shannon Diver-
gence as the measure to estimate the similarity of
topic distribution, changing the number of topics k
in the following, k = 5, k = 10, k = 20, k = 50,
k = 100, and k = 200. Next, the number of topics
is fixed based on the result of the first process, and
then it is decided which measure is the most useful
by applying each measure to estimate the similarity
of topic distributions. Here, the iteration count of
Gibbs Sampling is 200. The number of trials is 20,
and all trials are averaged. The same experiment is
conducted for wordLDA to compare both results.
4.3 Result
Table 1 shows the retrieval result examined by 11-
point interpolated average precision, changing the
number of topics k. High accuracy is shown at k = 5
in eventLDA, and k = 50 in wordLDA, respectively.
Overall, we see that eventLDA keeps higher accu-
racy than wordLDA.
number of topics wordLDA eventLDA
5 0.5152 0.6256
10 0.5473 0.5744
20 0.5649 0.5874
50 0.5767 0.5740
100 0.5474 0.5783
200 0.5392 0.5870
Table 1: Result based on the number of topics.
Table 2 shows the retrieval result examined by
11-point interpolated average precision under vari-
ous measures. The number of topics k is k = 50
in wordLDA and k = 5 in eventLDA respectively,
based on the above result. Under any measures,
we see that eventLDA keeps higher accuracy than
wordLDA.
similarity measure wordLDA eventLDA
Kullback-Leibler 0.5009 0.5056
Symmetric Kullback-Leibler 0.5695 0.6762
Jensen-Shannon 0.5753 0.6754
cosine 0.5684 0.6859
Table 2: Performance under various measures.
4.4 Discussions
The result of the experiment shows that eventLDA
provides a better performance than wordLDA, there-
32
fore, we see our method can properly treat the latent
topics of a document. In addition, as for a prop-
erty of eventLDA, we see that it can provide detail
classification with a small number of topics. As the
reason for this, we think that a topic distribution on
a feature is narrowed down to some extent by using
an Event as the feature instead of a word, and then
as a result, the possibility of generating error topics
decreased.
On the other hand, a proper measure for our
method is identified as cosine similarity, although
cosine similarity is not a measure to estimate prob-
abilistic distribution. It is unexpected that the mea-
sures proper to estimate probabilistic distribution got
the result of lower performance than cosine similar-
ity. From this, there are some space where we need
to examine the characteristics of topic distribution as
a probabilistic distribution.
5 Application to Summarization
Here, we show multi-document summarization as
an application of our proposed method. We make
a query-biased summary, and show the effectiveness
of our method by comparing the accuracy of a gener-
ated summary by our method with that of summaries
by the representative summarization methods often
used as benchmark methods to compare.
5.1 Extracting Sentences by MMR-MD
In extracting important sentences, considering only
similarity to a given query, we may generate a redun-
dant summary. To avoid this problem, a measure,
MMR-MD (Maximal Marginal Relevance Multi-
Document), was proposed (Goldstein et al, 2000).
This measure is the one which prevents extracting
similar sentences by providing penalty score that
corresponds to similarity between a newly extracted
sentence and the previously extracted sentences. It
is defined by Eq. 1 (Okumura and Nanba, 2005).
MMR-MD ? argmaxCi?R\S [?Sim1(Ci,Q)
?(1??)maxCj?SSim2(Ci,Cj)] (1)
We aim to choose sentences whose content is sim-
ilar to query?s content based on a latent topic, while
reducing the redundancy of choosing similar sen-
tences to the previously chosen sentences. There-
fore, we adopt the similarity of topic distributions
Ci ? sentence in the document sets
Q ? query
R ? a set of sentences retrieved by Q from the document sets
S ? a set of sentences in R already extracted
? ? weighting parameter
for Sim1 which estimates similarity between a sen-
tence and a query, and adopt cosine similarity based
on Events as a feature unit for Sim2 which estimates
the similarity with the sentences previously chosen.
As the measures to estimate topic distribution simi-
larity, we use the four measures explained in Section
4.1. Here, as for the weighting parameter ?, we set
? = 0.5.
5.2 Experimental Settings
In the experiment, we use a data set provided at NT-
CIR4 (NII Test Collection for IR Systems 4) TSC3
(Text Summarization Challenge 3) 4 .
The data consists of 30 topic sets of documents
in which each set has about 10 Japanese newspaper
articles, and the total number of the sentences in the
data is 3587. In order to make evaluation for the re-
sult provided by our method easier, we compile a set
of questions, provided by the data sets for evaluating
the result of summarization, as a query, and then use
it as a query for query-biased summarization. As an
evaluation method, we adopt precision and coverage
used at TSC3 (Hirao et al, 2004), and the number
of extracted sentences is the same as used in TSC3.
Precision is an evaluation measure which indicates
the ratio of the number of correct sentences to that
of the sentences generated by the system. Coverage
is an evaluation measure which indicates the degree
of how the system output is close to the summary
generated by a human, taking account of the redun-
dancy.
Moreover, to examine the characteristics of the
proposed method, we compare both methods in
terms of the number of topics and the proper mea-
sure to estimate similarity. The number of trials is
20 at each condition. 5 sets of documents selected
at random from 30 sets of documents are used in the
trials, and all the trials are totally averaged. As a
target for comparison with the proposed method, we
also conduct an experiment using wordLDA.
4http://research.nii.ac.jp/ntcir/index-en.html
33
5.3 Result
As a result, there is no difference among the four
measures ? the same result is obtained by the
four measures. Table 3 shows comparison between
eventLDA and wordLDA in terms of precision and
coverage. The number of topics providing the high-
est accuracy is k = 5 for wordLDA, and k = 10 for
eventLDA, respectively.
number of topics wordLDA eventLDA
Precision Coverage Precision Coverage
5 0.314 0.249 0.404 0.323
10 0.264 0.211 0.418 0.340
20 0.261 0.183 0.413 0.325
50 0.253 0.171 0.392 0.319
Table 3: Comparison of the number of topics.
Furthermore, Table 4 shows comparison between
the proposed method and representative summa-
rization methods which do not deal with latent
topics. As representative summarization methods
to compare our method, we took up the Lead
method (Brandow et al, 1995) which is effective
for document sumarization of newspapers, and the
important sentence extraction-based summarization
method using TF-IDF.
method Precision Coverage
Lead 0.426 0.212
TF-IDF 0.454 0.305
wordLDA (k=5) 0.314 0.249
eventLDA (k=10) 0.418 0.340
Table 4: Comparison of each method.
5.4 Discussions
Under any condition, eventLDA provides a higher
accuracy than wordLDA. We see that the proposed
method is useful for estimating a topic on a sentence.
As the reason for that the accuracy does not depend
on any kinds of similarity measures, we think that
an estimated topic distribution is biased to a particu-
lar topic, therefore, there was not any influence due
to the kinds of similarity measures. Moreover, the
proper number of topics of eventLDA is bigger than
that of wordLDA. We consider the reason for this
is because we used newspaper articles as the objec-
tive documents, so it can be thought that the top-
ics onto the words in the articles were specific to
some extent; in other words, the words often used
in a particular field are often used in newspaper ar-
ticles, therefore, we think that wordLDA can clas-
sify the documents with the small number of top-
ics. In comparison with the representative methods,
the proposed method takes close accuracy to their
accuracy, therefore, we see that the performance of
our method is at the same level as those representa-
tive methods which directly deal with words in doc-
uments. In particular, as for coverage, our method
shows high accuracy. We think the reason for this
is because a comprehensive summary was made by
latent topics.
6 Conclusion
In this paper, we have defined a pair of words with
dependency relationship as ?Event? and proposed a
latent topic extracting method in which the content
of a document is comprehended by assigning latent
topics onto Events. We have examined the ability
of our proposed method in Section 4, and as its ap-
plication, we have shown a document summariza-
tion using the proposed method in Section 5. We
have shown that eventLDA has higher ability than
wordLDA in terms of estimating a topic distribu-
tion on even a sentence or a document; furthermore,
even in case of assigning a topic on an Event, we see
that latent topics can be properly estimated. Since
an Event can hold a relationship between a pair of
words, it can be said that our proposed method, i.e.,
eventLDA, can comprehend the content of a docu-
ment more deeper and proper than the conventional
method, i.e., wordLDA. Therefore, eventLDA can
be effectively applied to various document data sets
rather than wordLDA can be. We have also shown
that another feature other than a word, i.e., an Event
is also useful to estimate latent topics in a document.
As future works, we will conduct experiments with
various types of data and query, and further investi-
gate the characteristic of our proposed method.
Acknowledgments
We would like to thank Rakuten, Inc. for permission
to use the resources of Rakuten Travel, and thank
the National Institute of Informatics for providing
NTCIR data sets.
34
References
Adam Berger and Vibhu O. Mittal. 2000. Query-relevant
summarization using FAQs. In ACL ?00 Proceedings
of the 38th Annual Meeting on Association for Com-
putational Linguistics:294?301.
Anastasios Tombros and Mark Sanderson. 1998. Ad-
vantages of query biased summaries in information re-
trieval. In Proceedings of the 21st Annual Interna-
tional ACM-SIGIR Conference on Research and De-
velopment in Information Retrieval:2?10.
Ani Nenkova and Lucy Vanderwende. 2005. The Im-
pact of Frequency on Summarization. Technical re-
port, Microsoft Research.
Aria Haghighi and Lucy Vanderwende. 2009. Explor-
ing Content Models for Multi-Document Summariza-
tion. In Human Language Technologies: The 2009 An-
nual Conference of the North American Chapter of the
ACL:362?370.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. Journal of Ma-
chine Learning Research,3:993?1022.
Dragomir R. Radev. 2004. Lexrank: graph-based cen-
trality as salience in text summarization. Journal of
Artificial Intelligence Research (JAIR.
Harendra Bhandari, Masashi Shimbo, Takahiko Ito, and
Yuji Matsumoto. 2008. Generic Text Summarization
Using Probabilistic Latent Semantic Indexing. In Pro-
ceedings of the 3rd International Joint Conference on
Natural Langugage Proceeding:133-140.
H. P. Luhn. 1958. The automatic creation of literature
abstracts. IBM Journal of Research and Development.
Jade Goldstein, Vibhu Mittal, Jaime Carbonell, and
Mark Kantrowitz. 2000. Multi-document sum-
marization by sentence extraction. In Proceedings
of the 2000 NAALP-ANLP Workshop on Automatic
Summarization:40?48.
Jianhua Lin. 2002. Divergence Measures based on the
Shannon Entropy. IEEE Transactions on Information
Theory, 37(1):145?151.
Leonhard Henning. 2009. Topic-based Multi-Document
Summarization with Probabilistic Latent Semantic
Analysis. Recent Advances in Natural Language
Processing:144?149.
Manabu Okumura and Eiji Nanba. 2005. Sci-
ence of knowledge: Automatic Text Summarization.(in
Japanese) ohmsha.
Manabu Okumura and Hajime Mochizuki. 2000. Query-
Biased Summarization Based on Lexical Chaining.
Computational Intelligence,16(4):578?585.
Qin Bing, Liu Ting, Zhang Yu, and Li Sheng. 2005. Re-
search on Multi-Document Summarization Based on
Latent Semantic Indexing. Journal of Harbin Institute
of Technology,12(1):91?94.
Rachit Arora and Balaraman Ravindran. 2008. Latent
dirichlet alocation based multi-document summariza-
tion. In Proceedings of the 2nd Workshop on Analytics
for Noisy Unstructured Text Data.
Ronald Brandow, Karl Mitze, and Lisa F. Rau. 1995.
Automatic condensation of electronic publications by
sentence selection. Information Processing and Man-
agement: an International Journal - Special issue:
summarizing text,31(5):675?685.
Scott Deerwester, Susan T. Dumais, George W. Furnas,
Thomas K. Landauer, and Richard Harshman. 1990.
Indexing by Latent Semantic Analysis. Journal of the
American Society of Information Science, 41(6):391?
407.
Shotaro Matsumoto, Hiroya Takamura, and Manabu
Okumura. 2005. Sentiment Classification Us-
ing Word Sub-sequences and Dependency Sub-trees.
In Proceedings of the 9th Pacific-Asia Interna-
tional Conference on Knowledge Discovery and Data
Mining:301?310.
Solomon Kullback and Richard A. Leibler. 1951. On
Information and Sufficiency. Annuals of Mathematical
Statistics, 22:49?86.
Thomas L. Grififths and Mark Steyvers. 2004. Find-
ing scientific topics. In Proceedings of the Na-
tional Academy of Sciences of the United States of
America,101:5228?5235.
Thomas Hofmann. 1999. Probabilistic Latent Seman-
tic Indexing. In Proceedings of the 22nd Annual In-
ternational ACM-SIGIR Conference on Research and
Development in Information Retrieval:50?57.
Tsutomu Hirao, Takahiro Fukusima, Manabu Okumura,
Chikashi Nobata, and Hidetsugu Nanba. 2004. Cor-
pus and evaluation measures for multiple document
summarization with multiple sources. In Proceed-
ings of the 20th International Conference on Compu-
tational Linguistics:535?541.
Xiaojun Wan and Jianwu Yang. 2006. Improved affinity
graph based multi-document summarization. In Pro-
ceedings of the Human Language Technology Confer-
ence of the NAACL, Companion Volume: Short Papers
Yasuhiro Suzuki, Takashi Uemura, Takuya Kida, and Hi-
roki Arimura. 2010. Extension to word phrase on la-
tent dirichlet alocation. Forum on Data Engineering
and Information Management,i-6.
Yee W. Teh, David Newman, and Max Welling. 2006.
A Collapsed Variational Bayesian Inference Algorithm
for Latent Dirichlet Allocation. Advances in Neural
Information Processing Systems Conference,19:1353?
1360.
35
Proceedings of the ACL Student Research Workshop, pages 46?51,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Text Classification based on the Latent Topics of Important Sentences
extracted by the PageRank Algorithm
Yukari Ogura and Ichiro Kobayashi
Advanced Sciences, Graduate School of Humanities and Sciences,
Ochanomizu University
2-1-1 Ohtsuka Bunkyo-ku Tokyo, 112-8610 JAPAN
{ogura.yukari, koba}@is.ocha.ac.jp
Abstract
In this paper, we propose a method to raise the
accuracy of text classification based on latent
topics, reconsidering the techniques necessary
for good classification ? for example, to de-
cide important sentences in a document, the
sentences with important words are usually re-
garded as important sentences. In this case,
tf.idf is often used to decide important words.
On the other hand, we apply the PageRank al-
gorithm to rank important words in each doc-
ument. Furthermore, before clustering docu-
ments, we refine the target documents by rep-
resenting them as a collection of important
sentences in each document. We then clas-
sify the documents based on latent informa-
tion in the documents. As a clustering method,
we employ the k-means algorithm and inves-
tigate how our proposed method works for
good clustering. We conduct experiments with
Reuters-21578 corpus under various condi-
tions of important sentence extraction, using
latent and surface information for clustering,
and have confirmed that our proposed method
provides better result among various condi-
tions for clustering.
1 Introduction
Text classification is an essential issue in the field
of natural language processing and many techniques
using latent topics have so far been proposed and
used under many purposes. In this paper, we aim
to raise the accuracy of text classification using la-
tent information by reconsidering elemental tech-
niques necessary for good classification in the fol-
lowing three points: 1) important words extraction
? to decide important words in documents is a cru-
cial issue for text classification, tf.idf is often used to
decide them. Whereas, we apply the PageRank al-
gorithm (Brin et al, 1998) for the issue, because the
algorithm scores the centrality of a node in a graph,
and important words should be regarded as having
the centrality (Hassan et al, 2007). Besides, the al-
gorithm can detect centrality in any kind of graph,
so we can find important words for any purposes.
In our study, we express the relation of word co-
occurrence in the form of a graph. This is because
we use latent information to classify documents, and
documents with high topic coherence tend to have
high PMI of words in the documents (Newman et
al., 2010). So, we construct a graph from a view-
point of text classification based on latent topics. 2)
Refinement of the original documents ? we recom-
pile the original documents with a collection of the
extracted important sentences in order to refine the
original documents for more sensitive to be classi-
fied. 3) Information used for classification ? we
use latent information estimated by latent Dirichlet
allocation (LDA) (Blei et al, 2003) to classify doc-
uments, and compare the results of the cases using
both surface and latent information. We experiment
text classification with Reuters-21578 corpus; evalu-
ate the result of our method with the results of those
which have various other settings for classification;
and show the usefulness of our proposed method.
2 Related studies
Many studies have proposed to improve the accu-
racy of text classification. In particular, in terms
of improving a way of weighting terms in a docu-
46
ment for text classification, there are many studies
which use the PageRank algorithm. In (Hassan et
al., 2007), they have applied a random-walk model
on a graph constructed based on the words which
co-occur within a given window size, e.g., 2,4,6,8
words in their experiments, and confirmed that the
windows of size 2 and 4 supplied the most signif-
icant results across the multiple data set they used.
Zaiane et al (2002) and Wang et al (2005) have
introduced association rule mining to decide impor-
tant words for text classification. In particular, Wang
et al have used a PageRank-style algorithm to rank
words and shown their method is useful for text clas-
sification. Scheible et al (2012) have proposed a
method for bootstrapping a sentiment classifier from
a seed lexicon. They apply topic-specific PageRank
to a graph of both words and documents, and in-
troduce Polarity PageRank, a new semi-supervised
sentiment classifier that integrates lexicon induction
with document classification. As a study related to
topic detection by important words obtained by the
PageRank algorithm, Kubek et al (2011) has de-
tected topics in a document by constructing a graph
of word co-occurrence and applied the PageRank al-
gorithm on it.
To weight words is not the issue for only text clas-
sification, but also an important issue for text sum-
marization, Erkan et al (2004) and Mihlcea et al
(2004b; 2004a) have proposed multi-document sum-
marization methods using the PageRank algorithm,
called LexRank and TextRank, respectively. They
use PageRank scores to extract sentences which
have centrality among other sentences for generat-
ing a summary from multi-documents.
On the other hand, since our method is to clas-
sify texts based on latent information. The graph
used in our method is constructed based on word co-
occurrence so that important words which are sen-
sitive to latent information can be extracted by the
PageRank algorithm. At this point, our attempt dif-
fers from the other approaches.
3 Techniques for text classification
3.1 Extraction of important words
To decide important words, tf.idf is often adopted,
whereas, another methods expressing various rela-
tion among words in a form of a graph have been
proposed (2005; Hassan et al, 2007). In particular,
(Hassan et al, 2007) shows that the PageRank score
is more clear to rank important words rather than
tf.idf. In this study, we refer to their method and use
PageRank algorithm to decide important words.
The PageRank algorithm was developed by (Brin
et al, 1998). The algorithm has been used as the
basic algorithm of Google search engine, and also
used for many application to rank target information
based on the centrality of information represented in
the form of a graph.
In this study, the important words are selected
based on PageRank score of a graph which repre-
sents the relation among words. In other words, in
order to obtain good important sentences for classi-
fication, it is of crucial to have a good graph (Zhu
et al, 2005) because the result will be considerably
changed depending on what kind of a graph we will
have for important words. In this study, since we
use latent information for text classification, there-
fore, we construct a graph representing the relation
of words from a viewpoint topic coherence. Ac-
cording to (Newman et al, 2010), topic coherence
is related to word co-occurrence. Referring to their
idea, we construct a graph over words in the follow-
ing manner: each word is a node in the graph, and
there is an undirected edge between every pair of
words that appear within a three-sentence window ?
to take account of contextual information for words,
we set a three-sentence window. We then apply the
PageRank algorithm to this graph to obtain a score
for every word which is a measurement of its cen-
trality ? the centrality of a word corresponds to the
importance of a word. A small portion of a graph
might look like the graph in Figure 1.
3.2 Refinement of target documents
After selecting important words, the important sen-
tences are extracted until a predefined ratio of whole
sentences in each document based on the selected
important words, and then we reproduce refined
documents with a collection of extracted important
sentences. An important sentence is decided by how
many important words are included in the sentence.
The refined documents are composed of the impor-
tant sentences extracted from a viewpoint of latent
information, i.e., word co-occurrence, so they are
proper to be classified based on latent information.
47
Figure 1: A graph of word cooccurrence
3.3 Clustering based on latent topics
After obtaining a collection of refined documents for
classification, we adopt LDA to estimate the latent
topic probabilistic distributions over the target doc-
uments and use them for clustering. In this study,
we use the topic probability distribution over docu-
ments to make a topic vector for each document, and
then calculate the similarity among documents.
3.4 Clustering algorithm
step.1 Important words determination
The important words are decided based on tf.idf
or PageRank scores. As for the words decided
based on PageRank scores, we firstly have to
make a graph on which the PargeRank algo-
rithm is applied. In our study, we construct a
graph based on word co-occurrence. So, im-
portant words are selected based on the words
which have centrality in terms of word co-
occurrence. In particular, in our study we se-
lect co-occurred words in each three sentences
in a document, taking account of the influence
of contextual information.
step.2 Refinement of the target documents
After selecting the important words, we select
the sentences with at least one of the words
within the top 3 PageRank score as important
sentences in each document, and then we re-
produce refined documents with a collection of
the extracted important sentences.
step.3 Clustering based on latent topics
As for the refined document obtained in step
2, the latent topics are estimated by means of
LDA. Here, we decide the number of latent top-
ics k in the target documents by measuring the
value of perplexity P (w) shown in equation
(1). The similarity of documents are measured
by the Jenshen-Shannon divergence shown in
equation (2).
P (w) = exp(? 1N
?
mn
log(
?
z
?mz?zwmn))
(1)
Here, N is the number of all words in the target
documents, wmn is the n-th word in the m-th
document; ? is the topic probabilistic distribu-
tion for the documents, and ? is the word prob-
abilistic distribution for every topic.
DJS(P ||Q)
= 12(
?
x
P (x)logP (x)R(x) +
?
x
logQ(x)R(x) )
where,R(x) = P (x) +Q(x)2 (2)
4 Experiment
We evaluate our proposed method by comparing
the accuracy of document clustering between our
method and the method using tf.idf for extracting im-
portant words.
4.1 Experimental settings
As the documents for experiments, we use Reuters-
21578 dataset 1 collected from the Reuters newswire
in 1987.In our proposed method, the refined doc-
uments consisting of important sentences extracted
from the original documents are classified, there-
fore, if there are not many sentences in a document,
we will not be able to verify the usefulness of our
proposed method. So, we use the documents which
have more than 5 sentences in themselves. Of the
135 potential topic categories in Reuters-21578, re-
ferring to other clustering study (Erkan, 2006; 2005;
Subramanya et al, 2008), we also use the most fre-
quent 10 categories: i.e., earn, acq, grain, wheat,
money, crude, trade, interest, ship, corn. In the
1http://www.daviddlewis.com/resources/testcollections/reuters21578/
48
sequel, we use 792 documents whose number of
words is 15,835 for experiments ? the 792 docu-
ments are the all documents which have more than 5
sentences in themselves in the corpus. For each doc-
ument, stemming and stop-word removal processes
are adopted. Furthermore, the hyper-parameters for
topic probability distribution and word probability
distribution in LDA are ?=0.5 and ?=0.5, respec-
tively. We use Gibbs sampling and the number of
iteration is 200. The number of latent topics is de-
cided by perplexity, and we decide the optimal num-
ber of topics by the minimum value of the average of
10 times trial, changing the number of topics rang-
ing from 1 to 30.
As the first step for clustering with our method,
in this study we employ the k-means clustering al-
gorithm because it is a representative and a simple
clustering algorithm.
4.2 Evaluation method
For evaluation, we use both accuracy and F-value,
referring to the methods used in (Erkan, 2006). As
for a document di, li is the label provided to di by
the clustering algorithm, and ?i is the correct label
for di. The accuracy is expressed in equation (3).
Accuracy =
?n
i=1 ? (map (li) , ?i)
n (3)
? (x, y) is 1 if x = y, otherwise 0. map (li) is the
label provided to di by the k-means clustering algo-
rithm. For evaluation, the F-value of each category
is computed and then the average of the F-values of
the whole categories, used as an index for evalua-
tion, is computed (see, equation (4)).
F = 1|C|
?
ci?C
F (ci) (4)
As the initial data for the k-means clustering al-
gorithm, a correct document of each category is ran-
domly selected and provided. By this, the cate-
gory of classified data can be identified as in (Erkan,
2006).
4.3 Experiment results
To obtain the final result of the experiment, we ap-
plied the k-means clustering algorithm for 10 times
for the data set and averaged the results. Here, in the
case of clustering the documents based on the topic
probabilistic distribution by LDA, the topic distribu-
tion over documents ? is changed in every estima-
tion. Therefore, we estimated ? for 8 times and then
applied the k-means clustering algorithm with each
? for 10 times. We averaged the results of the 10
trials and finally evaluated it. The number of latent
topics was estimated as 11 by perplexity. We used it
in the experiments. To measure the latent similarity
among documents, we construct topic vectors with
the topic probabilistic distribution, and then adopt
the Jensen-Shannon divergence to measures it, on
the other hand, in the case of using document vec-
tors we adopt cosine similarity.
Table 1 and Table 2 show the cases of with and
without refining the original documents by recom-
piling the original documents with the important
sentences.
Table 1: Extracting important sentences
Methods Measure Accuracy F-value
PageRank Jenshen-Shannon 0.567 0.485
Cosine similarity 0.287 0.291
tf.idf Jenshen-Shannon 0.550 0.435
Cosine similarity 0.275 0.270
Table 2: Without extracting important sentences
Similarity measure Accuracy F-value
Jenshen-Shannon 0.518 0.426
Cosine similarity 0.288 0.305
Table 3, 4 show the number of words and sen-
tences after applying each method to decide impor-
tant words.
Table 3: Change of number of words
Methods 1 word 2 words 3 words 4 words 5 words
PageRank 12,268 13,141 13,589 13,738 13,895
tf ? idf 13,999 14,573 14,446 14,675 14,688
Furthermore, Table 5 and 6 show the accuracy and
F-value of both methods, i.e., PageRank scores and
tf.idf, in the case that we use the same number of
sentences in the experiment to experiment under the
same conditions.
49
Table 4: Change of number of sentences
Methods 1 word 2 words 3 words 4 words 5 words
PageRank 1,244 1,392 1,470 1,512 1,535
tf ? idf 1,462 1,586 1,621 1,643 1,647
Table 5: Accuracy to the number of topics
Num. of topics 8 9 10 11 12
PageRank 0.525 0.535 0.566 0.553 0.524
tf.idf 0.556 0.525 0.557 0.550 0.541
4.4 Discussion
We see from the experiment results that as for the
measures based on the Jenshen-Shannon divergence,
both accuracy and F-value of the case where refined
documents are clustered is better than the case where
the original documents are clustered. We have con-
ducted t-test to confirm whether or not there is sig-
nificant difference between the cases: with and with-
out extracting important sentences. As a result, there
is significant difference with 5 % and 1 % level for
the accuracy and F-value, respectively.
When extracting important sentences, although
the size of the document set to be clustered is smaller
than the original set, the accuracy increases. So, it
can be said that necessary information for clustering
is adequately extracted from the original document
set.
From this, we have confirmed that the documents
are well refined for better clustering by recompil-
ing the documents with important sentences. We
think the reason for this is because only important
sentences representing the contents of a document
are remained by refining the original documents and
then it would become easier to measure the differ-
ence between probabilistic distributions of topics in
a document. Moreover, as for extracting important
sentences, we confirmed that the accuracy of the
case of using PageRank scores is better than the case
of using tf.idf. By this, constructing a graph based
on word co-occurrence of each 3 sentences in a doc-
ument works well to rank important words, taking
account of the context of the word.
We see from Table 3 , 4 that the number of words
and sentences decreases when applying PageRank
scores. In the case of applying tf.idf, the tf.idf value
Table 6: F-value to the number of topics
Num. of topics 8 9 10 11 12
PageRank 0.431 0.431 0.467 0.460 0.434
tf.idf 0.466 0.430 0.461 0.435 0.445
tends to be higher for the words which often ap-
pear in a particular document. Therefore, the ex-
traction of sentences including the words with high
tf.idf value may naturally lead to the extraction of
many sentences.
The reason for low accuracy in the case of us-
ing cosine similarity for clustering is that it was ob-
served that the range of similarity between docu-
ments is small, therefore, the identification of differ-
ent categorized documents was not well achieved.
Table 5 and Table 6 show the accuracy and F-
value to the number of latent topics, respectively.
We see that both accuracy and F-value of the case
of using PageRank scores are better than those of
the case of using tf.idf in the case of the number
of topics is 9,10,and 11. In particular, the highest
score is made when the number of topics is 10 for
both evaluation measures ? we think the reason for
this is because we used document sets of 10 cate-
gories, therefore, it is natural to make the highest
score when the number of topics is 10. So, we had
better look at the score of the case where the number
of topics is 10 to compare the ability of clustering.
By the result, we can say that PageRank is better in
refining the documents so as they suit to be classified
based on latent information.
5 Conclusions
In this study, we have proposed a method of text
clustering based on latent topics of important sen-
tences in a document. The important sentences are
extracted through important words decided by the
PageRank algorithm. In order to verify the useful-
ness of our proposed method, we have conducted
text clustering experiments with Reuters-21578 cor-
pus under various conditions ? we have adopted ei-
ther PageRank scores or tf.idf to decide important
words for important sentence extraction, and then
adopted the k-means clustering algorithm for the
documents recompiled with the extracted important
sentences based on either latent or surface informa-
50
tion. We see from the results of the experiments that
the clustering based on latent information is gener-
ally better than that based on surface information in
terms of clustering accuracy. Furthermore, deciding
important words with PageRank scores is better than
that with tf.idf in terms of clustering accuracy. Com-
pared to the number of the extracted words in impor-
tant sentences between PageRank scores and tf.idf,
we see that the number of sentences extracted based
on PageRank scores is smaller than that based on
tf.idf, therefore, it can be thought that more context-
sensitive sentences are extracted by adopting PageR-
ank scores to decide important words.
As future work, since clustering accuracy will be
changed by how many sentences are compiled in a
refined document set, therefore, we will consider a
more sophisticated way of selecting proper impor-
tant sentences. Or, to avoid the problem of selecting
sentences, we will also directly use the words ex-
tracted as important words for clustering. Moreover,
at this moment, we use only k-means clustering al-
gorithm, so we will adopt our proposed method to
other various clustering methods to confirm the use-
fulness of our method.
References
David M. Blei and Andrew Y. Ng and Michael I. Jordan
and John Lafferty. 2003. Latent dirichlet alocation,
Journal of Machine Learning Research,
Sergey Brin and Lawrence Page. 1998. The Anatony of
a Large-scale Hypertextual Web Search Engine, Com-
puter Networks and ISDN Systems, pages. 107?117.
Gunes Erkan, 2004. LexRank: Graph-based Lexical
Centrality as Salience in Text Summarization Journal
of Artificial Intelligence Research 22, pages.457-479
Gunes Erkan. 2006. Language Model-Based Docu-
ment Clustering Using Random Walks, Association for
Computational Linguistics, pages.479?486.
Samer Hassan, Rada Mihalcea and Carmen Banea. 2007.
Random-Walk Term Weighting for Improved Text Clas-
sification, SIGIR ?07 Proceedings of the 30th annual
international ACM SIGIR conference on Research and
development in information retrieval, pages.829-830.
Mario Kubek and Herwig Unger, 2011 Topic Detec-
tion Based on the PageRank?s Clustering Property,
IICS?11, pages.139-148,
Rada Mihalcea. 2004. Graph-based Ranking Algorithms
for Sentence Extraction, Applied to Text Summariza-
tion, Proceeding ACLdemo ?04 Proceedings of the
ACL 2004 on Interactive poster and demonstration
sessions Article No. 20.
Rada Mihalcea and Paul Tarau 2004. TextRank: Bring-
ing Order into Texts, Conference on Empirical Meth-
ods in Natural Language Processing.
David Newman, Jey Han Lau, Karl Grieser, and Timo-
thy Baldwin, 2010. Automatic evaluation of topic co-
herence, Human Language Technologies: The 2010
Annual Conference of the North Ametican Chapter of
the Association for Computational Linguistics, pages.
100?108, Los Angeles.
Christian Scheible, Hinrich Shutze. 2012. Bootstrapping
Sentiment Labels For Unannotated Documents With
Polarity PageRank, Proceedings of the Eight Interna-
tional Conference on Language Resources and Evalu-
ation.
Amarnag Subramanya, Jeff Bilmes. 2008. Soft-
Supervised Learning for Text Classification Proceed-
ings of the 2008 Conference on Empirical Methods in
Natural Language Processing, pages.1090?1099, Hon-
olulu.
Wei Wang, Diep Bich Do, and Xuemin Lin. 2005. Term
Graph Model for Text Classification, Springer-Verlag
Berlin Heidelberg 2005, pages.19?30.
Osmar R. Zaiane and Maria-luiza Antonie. 2002. Clas-
sifying Text Documents by Associating Terms with Text
Categories, In Proc. of the Thirteenth Australasian
Database Conference (ADC?02), pages.215?222,
X Zhu. 2005. Semi-supervised learning with Graphs,
Ph.D thesis, Carnegie Mellon University.
51
Proceedings of the ACL Student Research Workshop, pages 136?141,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
High-quality Training Data Selection using Latent Topics
for Graph-based Semi-supervised Learning
Akiko Eriguchi
Ochanomizu University
2-1-1 Otsuka Bunkyo-ku Tokyo, Japan
g0920506@is.ocha.ac.jp
Ichiro Kobayashi
Ochanomizu University
2-1-1 Otsuka Bunkyo-ku Tokyo, Japan
koba@is.ocha.ac.jp
Abstract
In a multi-class document categorization
using graph-based semi-supervised learn-
ing (GBSSL), it is essential to construct
a proper graph expressing the relation
among nodes and to use a reasonable cat-
egorization algorithm. Furthermore, it is
also important to provide high-quality cor-
rect data as training data. In this con-
text, we propose a method to construct a
similarity graph by employing both sur-
face information and latent information
to express similarity between nodes and
a method to select high-quality training
data for GBSSL by means of the PageR-
ank algorithm. Experimenting on Reuters-
21578 corpus, we have confirmed that our
proposed methods work well for raising
the accuracy of a multi-class document
categorization.
1 Introduction
Graph-based semi-supervised learning (GBSSL)
algorithm is known as a useful and promising tech-
nique in natural language processings. It has been
widely used for solving many document catego-
rization problems (Zhu and Ghahramani, 2002;
Zhu et al, 2003; Subramanya and Bilmes, 2008).
A good accuracy of GBSSL depends on success
in dealing with three crucial issues: graph con-
struction, selection of high-quality training data,
and categorization algorithm. We particularly fo-
cus on the former two issues in our study.
In a graph-based categorization of documents,
a graph is constructed based on a certain relation
between nodes (i.e. documents). It is similar-
ity that is often used to express the relation be-
tween nodes in a graph. We think of two types of
similarity: the one is between surface information
obtained by document vector (Salton and McGill,
1983) and the other is between latent information
obtained by word probabilistic distribution (Latent
Dirichlet Allocation (Blei et al, 2003)). Here,
we propose a method. We use both surface in-
formation and latent information at the ratio of
(1 ? ?) : ?(0 ? ? ? 1) to construct a similarity
graph for GBSSL, and we investigate the optimal
? for raising the accuracy in GBSSL.
In selecting high-quality training data, it is im-
portant to take two aspects of data into consider-
ation: quantity and quality. The more the train-
ing data are, the better the accuracy becomes. We
do not always, however, have a large quantity of
training data. In such a case, the quality of train-
ing data is generally a key for better accuracy. It is
required to assess the quality of training data ex-
actly. Now, we propose another method. We use
the PageRank algorithm (Brin and Page, 1998) to
select high-quality data, which have a high cen-
trality in a similarity graph of training data (i.e.
labeled data) in each category.
We apply our methods to solving the problem
of a multi-class document categorization. We in-
troduce PRBEP (precision recall break even point)
as a measure which is popular in the area of infor-
mation retrieval. We evaluate the results of exper-
iments for each category and for the whole cat-
egory. We confirm that the way of selecting the
high-quality training data from data on a similar-
ity graph based on both surface information and
latent information is superior to that of selecting
from a graph based on just surface information or
latent information.
2 Related studies
Graph-based semi-supervised learning has re-
cently been studied so much and applied to many
applications (Subramanya and Bilmes, 2008; Sub-
ramanya and Bilmes, 2009; Subramanya et al,
2010; Dipanjan and Petrov, 2011; Dipanjan and
Smith, 2012; Whitney and Sarkar, 2012).
136
Subramanya and Bilmes (2008; 2009) have pro-
posed a soft-clustering method using GBSSL and
have shown that their own method is better than
the other main clustering methods of those days.
Subramanya et al (2010) have also applied their
method to solve the problem of tagging and have
shown that it is useful. Dipanjan and Petrov
(2011) have applied a graph-based label propa-
gation method to solve the problem of part-of-
speech tagging. They have shown that their pro-
posed method exceeds a state-of-the-art baseline
of those days. Dipanjan and Smith (2012) have
also applied GBSSL to construct compact natu-
ral language lexicons. To achieve compactness,
they used the characteristics of a graph. Whitney
and Sarkar (2012) have proposed the bootstrap-
ping learning method in which a graph propaga-
tion algorithm is adopted.
There are two main issues in GBSSL: the one
is the way of constructing a graph to propagate la-
bels, and the other is the way of propagating la-
bels. It is essential to construct a good graph in
GBSSL (Zhu, 2005). On the one hand, graph con-
struction is a key to success of any GBSSL. On
the other hand, as for semi-supervised learning, it
is quite important to select better training data (i.e.
labeled data), because the effect of learning will
be changed by the data we select as training data.
Considering the above mentioned, in our study,
we focus on the way of selecting training data so
as to be well propagated in a graph. We use the
PageRank algorithm to select high-quality train-
ing data and evaluate how our proposed method
influences the way of document categorization.
3 Text classification based on a graph
The details of our proposed GBSSL method in
a multi-class document categorization are as fol-
lows.
3.1 Graph construction
In our study, we use a weighted undirected graph
G = (V,E) whose node and edge represent a doc-
ument and the similarity between nodes, respec-
tively. Similarity is regarded as weight. V and E
represent nodes and edges in a graph, respectively.
A graph G can be represented as an adjacency ma-
trix, and wij ? W represents the similarity be-
tween nodes i and j. In particular, in the case of
GBSSL method, the similarity between nodes are
formed as wij = sim(xi,xj)?(j ? K(i)). K(i)
is a set of i?s k-nearest neighbors, and ?(z) is 1 if
z is true, otherwise 0.
3.2 Similarity in a graph
Generally speaking, when we construct a graph
to represent some relation among documents, co-
sine similarity (simcos) of document vectors is
adopted as a similarity measure based on surface
information. In our study, we add the similarity
(simJS) based on latent information and the simi-
larity (simcos) based on surface information in the
proportion of ? : (1 ? ?)(0 ? ? ? 1). We define
the sum of simJS and simcos as simnodes (see,
Eq. (1)).
In Eq. (1), P and Q represent the latent topic
distributions of documents S and T , respectively.
We use Latent Dirichlet Allocation (LDA) (Blei
et al, 2003) to estimate the latent topic distribu-
tion of a document, and we use a measure Jensen-
Shannon divergence (DJS) for the similarity be-
tween topic distributions. Incidentally, simJS in
Eq (1) is expressed by Eq. (2).
simnodes(S, T ) ? ? ? simJS(P,Q)
+(1? ?) ? simcos(tfidf(S), tfidf(T )) (1)
simJS(P,Q) ? 1?DJS(P,Q) (2)
3.3 Selection of training data
We use the graph-based document summarization
methods (Erkan and Radev, 2004; Kitajima and
Kobayashi, 2012) in order to select high-quality
training data. Erkan and Radev (2004) proposed a
multi-document summarization method using the
PageRank algorithm (Brin and Page, 1998) to ex-
tract important sentences. They showed that it
is useful to extract the important sentences which
have higher PageRank scores in a similarity graph
of sentences. Then, Kitajima and Kobayashi
(2012) have expanded the idea of Erkan and
Radev?s. They introduced latent information to
extract important sentences. They call their own
method TopicRank.
We adopt TopicRank method in our study. In or-
der to get high-quality training data, we first con-
struct a similarity graph of training data in each
category, and then compute a TopicRank score for
each training datum in every category graph. We
employ the data with a high TopicRank score as
training data in GBSSL.
In TopicRank method, Kitajima and Kobayashi
(2012) regard a sentence as a node in a graph on
137
surface information and latent information. The
TopicRank score of each sentence is computed by
Eq. (3). Each sentence is ranked by its TopicRank
score. In Eq. (3), d indicates a damping factor.
We, however, deal with documents, so we replace
a sentence with a document (i.e. sentences) as a
node in a graph. In Eq. (3), N indicates total num-
ber of documents, adj[u] indicates the adjoining
nodes of document u.
r(u) = d
?
v?adj[u]
simnodes(u, v)?
z?adj[v]
simnodes(z, v)
r(u)
+1? dN (3)
3.4 Label propagation
We use the label propagation method (Zhu et al,
2003; Zhou et al, 2004) in order to categorize doc-
uments. It is one of graph-based semi-supervised
learnings. It estimates the value of label based
on the assumption that the nodes linked to each
other in a graph should belong to the same cate-
gory. Here, W indicates an adjacency matrix. l
indicates the number of training data among all n
nodes in a graph. The estimation values f for n
nodes are obtained as the solution (Eq. (6)) of the
following objective function of an optimal prob-
lem (Eq. (4)). The first term in Eq. (4) expresses
the deviation between an estimation value and a
correct value of training data. The second term in
Eq. (4) expresses the difference between the esti-
mation values of the nodes which are next to an-
other in the adjacency graph. ?(> 0) is a param-
eter balancing both of the terms. Eq. (4) is trans-
formed into Eq. (5) by means ofL. L(? D?W )
is called the Laplacian matrix. D is a diagonal ma-
trix, each diagonal element of which is equal to the
sum of elements inW ?s each row (or column).
J(f) =
l?
i=1
(y(i) ? f (i))2
+?
?
i<j
w(i,j)(f (i) ? f (j))2 (4)
= ||y ? f ||22 + ?fTLf (5)
f = (I + ?L)?1y (6)
4 Experiment
4.1 Experimental settings
We use Reuters-21578 corpus data set1 collected
from the Reuters newswire in 1987 as target doc-
uments for a multi-class document categorization.
It consists of English news articles (classified into
135 categories). We use the ?ModApte? split to
get training documents (i.e. labeled data) and
test documents (i.e. unlabeled data), extract doc-
uments which have only its title and text body,
and apply the stemming and the stop-word re-
moval processes to the documents. Then, follow-
ing the experimental settings of Subramanya and
Bilmes (2008)2 , we use 10 most frequent cate-
gories out of the 135 potential topic categories:
earn, acq, grain, wheat, money-fx, crude, trade,
interest, ship, and corn. We apply the one-versus-
the-rest method to give a category label to each
test document. Labels are given when the estima-
tion values of each document label exceed each of
the predefined thresholds.
We prepare 11 data sets. Each data set consists
of 3299 common test data and 20 training data.
We use 11 kinds of categories of training data:
the above mentioned 10 categories and a category
(other) which indicates 125 categories except 10
categories. The categories of 20 training data are
randomly chosen only if one of the 11 categories
is chosen at least once.
Selecting high-quality training data, we use the
Gibbs sampling for latent topic estimation in LDA.
The number of iteration is 200. The number of la-
tent topics in the target documents is decided by
averaging 10 trials of estimation with perplexity
(see, Eq. (7)). Here, N is the number of all words
in the target documents. wmn is the n-th word in
the m-th document. ? is an occurrence probability
of the latent topics for the documents. ? is an oc-
currence probability of the words for every latent
topic.
P (w) = exp(? 1N
?
mn
log(
?
z
?mz?zwmn)) (7)
In each category, a similarity graph is con-
structed for the TopicRank method. The number
of nodes (i.e. |Vcategory|) in a graph corresponds to
1http://www.daviddlewis.com/resources/testcollections/
reuters21578/
2Our data sets lack any tags and information excluding a
title and a text body. Therefore, we cannot directly
compare with Subramanya and Bilmes? results.
138
the total number of training data in each category,
and the number of edges is E = (|Vcategory| ?
|Vcategory|). So, the graph is a complete graph.
The parameter ? in Eq (1) is varied from 0.0 to
1.0 every 0.1. We regard the average of TopicRank
scores after 5 trials as the TopicRank score of each
document. The number of training data in each
category is decided in each target data set. We
adopt training data with a higher TopicRank score
from the top up to the predefined number.
In label propagation, we construct another kind
of similarity graph. The number of nodes in a
graph is |Vl+u| = n(= 3319), and the similar-
ity between nodes is based on only surface infor-
mation (in the case of ? = 0 in Eq. (1)). The
parameter k in the k-nearest neighbors method is
k ? {2, 10, 50, 100, 250, 500, 1000, 2000, n}, the
parameter ? in the label propagation method, is
? ? {1, 0.1, 0.01, 1e ? 4, 1e ? 8}. Using one of
the 11 data sets, we decide a pair of optimal pa-
rameters (k, ?) for each category. We categorize
the remaining 10 data sets by means of the decided
parameters. Then, we obtain the value of precision
recall break even point (PRBEP) and the average
of PRBEP in each category. The value of PRBEP
is that of precision or recall at the time when the
former is equal to the latter. It is often used as
an index to measure the ability of information re-
trieval.
4.2 Result
Table 1 shows a pair of the optimal parameters
(k, ?) in each category corresponding to the value
of ? ranging from 0.0 to 1.0 every 0.1. Figures
from 1 to 10 show the experimental results in us-
ing these parameters in each category. The hori-
zontal axis indicates the value of ? and the verti-
cal axis indicates the value of PRBEP. Each figure
shows the average of PRBEP in each category af-
ter 10 trials for each ?. Fig. 11 shows how the
relative ratio of PRBEP changes corresponding to
each ? in each category, when we let the PRBEP
at ? = 0 an index 100. Fig. 12 shows the macro
average of PRBEP after 10 trials in the whole cat-
egory corresponding to each ?. Error bars indicate
the standard deviations.
In all figures, the case at ? = 0 means that only
surface information is used for selecting the train-
ing data. The case at ? = 1 means that only latent
information is used. The other cases at ? 6= 0 or 1
mean that both latent information and surface in-
formation are mixed at the ratio of ? : (1 ? ?)
(0 < ? < 1).
First, we tell about Fig. 1-10. On the one hand,
in Fig. 4, 5, 6, 8, 10, the PRBEPs at ? 6= 0 are
greater than that at ? = 0, although the PRBEP at
? = 1 is less than that at ? = 0 in Fig. 4. On
the other hand, in Fig. 2, 7, the PRBEPs at ? 6= 0
are less than that at ? = 0. In Fig. 1, 3, 9, the
PRBEPs at ? 6= 0 fluctuate widely or narrowly
around that at ? = 0. In addition, the PRBEPs at
? = 0 range from 7.7 to 74.3 and those at ? = 1
range from 8.0 to 72.6 in all 10 figures. It is hard
to find significant correlation between PRBEP and
?.
Secondly, in Fig. 11, some curves show an in-
creasing trend and others show a decreasing trend.
At best, the maximum value is three times as large
as that at ? = 0. At worst, the minimum is one-
fifth. Indexes at ? 6= 0 are greater than or equal to
an index 100 at ? = 0 in most categories.
Finally, in Fig. 12, the local maximums are
46.2, 46.9, 45.0 respectively at ? = 0.2, 0.6, 0.9.
The maximum is 46.9 at ? = 0.6. The mini-
mum value of the macro average is 35.8 at ? = 0,
though the macro average at ? = 1 is 43.4. Hence,
the maximum macro average is greater than that at
? = 1 by 3.5% and still greater than that at ? = 0
by 11.1%. The macro average at ? = 1 is greater
than that at ? = 0 by 7.6%. Furthermore, the
macro average increases monotonically from 35.8
to 46.2 as ? increases from 0.0 to 0.2. When ? is
more than 0.2, the macro averages fluctuate within
the range from 40.3 to 46.9. It follows that the
macro average values at 0.1 ? ? ? 1 are greater
than that at ? = 0. What is more important, the
macro averages at ? = 0.2, 0.4, 0.6, 0.7, 0.9 are
greater than that at ? = 1 and of course greater
than that at ? = 0.
5 Discussion
Looking at each Fig. 1-10, each optimal ? at
which PRBEP is the maximum is different and not
uniform in respective categories. So, we cannot
simply tell a specific ratio of balancing both infor-
mation (i.e. surface information and latent infor-
mation) which gives the best accuracy.
From a total point of view, however, we can see
a definite trend or relationship. In Fig. 11, we
can see the upward tendency of PREBP in half of
categories. Indexes of the PRBEP at ? ? 0.1 are
greater than or equal to 100 in most categories.
139
Table 1: the optimal parameters (k, ?) for each category
Category\? 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
earn (500, 1) (50, 1) (1000, 1) (1000, 1) (50, 1) (50, 1) (50, 1) (50, 1) (50, 1) (50, 1) (50, 1)
acq (100, 0.01) (100, 0.01) (100, 0.01) (2, 1) (100, 0.01) (100, 0.01) (100, 1e-8) (100, 1e-8) (100, 1e-8) (100, 1e-8) (100, 1e-8)
money-fx (250, 0.01) (100, 1e-8) (10, 1e-4) (100, 1e-8) (2, 0.1) (2, 0.1) (2, 1e-8) (250, 1e-8) (2, 0.1) (2, 1e-8) (250, 1e-8)
grain (250, 0.1) (2000, 1e-4) (100, 1) (250, 0.1) (100, 1) (50, 1) (250, 1) (50, 1) (50, 1) (50, 1) (100, 1)
crude (50, 0.1) (2, 1) (250, 0.01) (50, 1e-8) (10, 0.01) (250, 0.01) (250, 0.01) (250, 1e-8) (10, 0.01) (250, 0.01) (250, 0.01)
trade (2, 1) (10, 0.1) (50, 0.01) (10, 1e-8) (10, 1e-8) (10, 1e-8) (50, 1e-8) (10, 1e-8) (10, 1e-4) (10, 0.1) (10, 0.1)
interest (10, 1) (50, 1e-8) (50, 1e-8) (10, 1) (2, 0.1) (250, 1e-8) (250, 0.01) (250, 0.01) (2, 1) (2, 0.1) (500, 1e-8)
ship (3318, 1) (50, 1) (50, 1) (250, 0.1) (50, 0.1) (50, 0.1) (50, 1e-8) (50, 1e-8) (100, 0.1) (100, 0.1) (50, 0.01)
wheat (500, 1e-8) (500, 1e-8) (250, 1e-8) (500, 1e-8) (500, 0.01) (1000, 0.01) (500, 1e-8) (250, 1e-8) (250, 1e-8) (250, 1e-8) (250, 1e-8)
corn (10, 1e-8) (100, 1e-8) (250, 1e-8) (10, 1e-8) (250, 1e-8) (250, 1e-4) (500, 1e-8) (100, 1e-8) (250, 1e-8) (50, 0.01) (250, 1e-4)
Figure 1: earn Figure 2: acq Figure 3: money-fx
Figure 4: grain Figure 5: crude Figure 6: trade
Figure 7: interest Figure 8: ship Figure 9: wheat
Figure 10: corn Figure 11: Relative value Figure 12: Macro average
140
The macro average of the whole category is shown
in Fig. 12. Regarding the macro average at ? = 0
as a baseline, the macro average at ? = 1 is greater
than that at ? = 0 by 7.6% and still more, the max-
imum at ? = 0.6 is greater by 11.1%. Besides,
five macro averages at 0.1 ? ? ? 1 are greater
than that at ? = 1. Therefore, we can say that
using latent information gives a higher accuracy
than using only surface information and that using
both information gives a higher accuracy than us-
ing only latent information. So, if a proper ? is
decided, we will get a better accuracy.
6 Conclusion
We have proposed methods to construct a sim-
ilarity graph based on both surface information
and latent information and to select high-quality
training data for GBSSL. Through experiments,
we have found that using both information gives
a better accuracy than using either only surface
information or only latent information. We used
the PageRank algorithm in the selection of high-
quality training data. In this condition, we have
confirmed that our proposed methods are useful
for raising the accuracy of a multi-class document
categorization using GBSSL in the whole cate-
gory.
Our future work is as follows. We will verify
in other data corpus sets that the selection of high-
quality training data with both information gives
a better accuracy and that the optimal ? is around
0.6. We will revise the way of setting a pair of the
optimal parameters (k, ?) and use latent informa-
tion in the process of label propagation.
References
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. Journal of Ma-
chine Learning Research.
Sergey Brin and Lawrence Page. 1998. The Anatomy
of a Large-scale Hypertextual Web Search Engine.
Computer Networks and ISDN Systems, pages 107?
117.
Das Dipanjan and Noah A. Smith. 2012. Graph-based
lexicon expansion with sparsity-inducing penalties.
In Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
NAACL HLT ?12, pages 677?687.
Das Dipanjan and Slav Petrov. 2011. Unsupervised
Part-of-Speech Tagging with Bilingual Graph-Based
Projections. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies - Vol. 1,
pages 600?609.
Gu?nes? Erkan and Dragomir R. Radev. 2004. LexRank:
Graph-based Lexical Centrality as Salience in Text
Summarization. Journal of Artificial Intelligence
Research 22, pages 457-479.
Gu?nes? Erkan. 2006. Language Model-Based Docu-
ment Clustering Using Random Walks. Association
for Computational Linguistics, pages 479?486.
Risa Kitajima and Ichiro Kobayashi. 2012. Multiple-
document Summarization baed on a Graph con-
structed based on Latent Information. In Proceed-
ings of ARG Web intelligence and interaction, 2012-
WI2-1-21.
Gerard Salton and Michael J. McGill. 1983. Intro-
duction to Modern Information Retrieval. McGraw-
Hill.
Amarnag Subramanya and Jeff Bilmes. 2008. Soft-
Supervised Learning for Text Classification. In Pro-
ceedings of the 2008 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1090?
1099.
Amarnag Subramanya and Jeff Bilmes. 2009. En-
tropic graph regularization in non-parametric semi-
supervised classification. In Proceedings of NIPS.
Amarnag Subramanya, Slav Petrov and Fernando
Pereira. 2010. Efficient graph-based semi-
supervised learning of structured tagging models.
In Proceedings of the 2010 Conference on Empiri-
cal Methods in Natural Language Processing, pages
167?176.
Dengyong Zhou, Oliver Bousquet, Thomas Navin Lal,
Jason Weston, and Bernhard Scho?lkopf. 2004.
Learning with Local and Global Consistency. In
NIPS 16.
Xiaojin Zhu and Zoubin Ghahramani. 2002. Learning
from Labeled and Unlabeled Data with Label Prop-
agation. Technical report, Carnegie Mellon Univer-
sity.
Xiaojin Zhu, Zoubin Ghahramani, and John Lafferty.
2003. Semi-Supervised Learning Using Gaussian
Fields and Harmonic Functions. In Proceedigns of
the International Conference on Machine Learning
(ICML).
Xiaojin Zhu. 2005. Semi-Supervised Learning with
Graphs. PhD thesis, Carnegie Mellon University.
Max Whitney and Anoop Sarkar. 2012. Bootstrapping
via Graph Propagation. The 50th Annual Meeting of
the Association for Computational Linguistics.
141
