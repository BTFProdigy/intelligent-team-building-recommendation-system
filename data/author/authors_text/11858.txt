Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 65?68,
Suntec, Singapore, 4 August 2009.
c?2009 ACL and AFNLP
Detecting Compositionality in Multi-Word Expressions
Ioannis Korkontzelos
Department of Computer Science
The University of York
Heslington, York, YO10 5NG, UK
johnkork@cs.york.ac.uk
Suresh Manandhar
Department of Computer Science
The University of York
Heslington, York, YO10 5NG, UK
suresh@cs.york.ac.uk
Abstract
Identifying whether a multi-word expres-
sion (MWE) is compositional or not is im-
portant for numerous NLP applications.
Sense induction can partition the context
of MWEs into semantic uses and there-
fore aid in deciding compositionality. We
propose an unsupervised system to ex-
plore this hypothesis on compound nom-
inals, proper names and adjective-noun
constructions, and evaluate the contribu-
tion of sense induction. The evaluation
set is derived from WordNet in a semi-
supervised way. Graph connectivity mea-
sures are employed for unsupervised pa-
rameter tuning.
1 Introduction and related work
Multi-word expressions (MWEs) are sequences of
words that tend to cooccur more frequently than
chance and are either idiosyncratic or decompos-
able into multiple simple words (Baldwin, 2006).
Deciding idiomaticity of MWEs is highly impor-
tant for machine translation, information retrieval,
question answering, lexical acquisition, parsing
and language generation.
Compositionality refers to the degree to which
the meaning of a MWE can be predicted by com-
bining the meanings of its components. Unlike
syntactic compositionality (e.g. by and large), se-
mantic compositionality is continuous (Baldwin,
2006).
In this paper, we propose a novel unsupervised
approach that compares the major senses of a
MWE and its semantic head using distributional
similarity measures to test the compositionality of
the MWE. These senses are induced by a graph
based sense induction system, whose parameters
are estimated in an unsupervised manner exploit-
ing a number of graph connectivity measures (Ko-
rkontzelos et al, 2009). Our method partitions the
context space and only uses the major senses, fil-
tering out minor senses. In our approach the only
language dependent components are a PoS tagger
and a parser.
There are several studies relevant to detecting
compositionality of noun-noun MWEs (Baldwin et
al., 2003) verb-particle constructions (Bannard et
al., 2003; McCarthy et al, 2003) and verb-noun
pairs (Katz and Giesbrecht, 2006). Datasets with
human compositionality judgements are available
for these MWE categories (Cook et al, 2008).
Here, we focus on compound nominals, proper
names and adjective-noun constructions.
Our contributions are three-fold: firstly, we ex-
perimentally show that sense induction can as-
sist in identifying compositional MWEs. Sec-
ondly, we show that unsupervised parameter tun-
ing (Korkontzelos et al, 2009) results in accuracy
that is comparable to the best manually selected
combination of parameters. Thirdly, we propose
a semi-supervised approach for extracting non-
compositional MWEs from WordNet, to decrease
annotation cost.
2 Proposed approach
Let us consider the non-compositional MWE ?red
carpet?. It mainly refers to a strip of red carpeting
laid down for dignitaries to walk on. However, it
is possible to encounter instances of ?red carpet?
referring to any carpet of red colour. Our method
first applies sense induction to identify the major
semantic uses (senses) of a MWE (?red carpet?)
and its semantic head (?carpet?). Then, it com-
pares these uses to decide MWE compositionality.
The more diverse these uses are, the more possi-
bly the MWE is non-compositional. Our algorithm
consists of 4 steps:
A. Corpora collection and preprocessing. Our
approach receives as input a MWE (e.g. ?red car-
pet?). The dependency output of Stanford Parser
(Klein and Manning, 2003) is used to locate the
65
Figure 1: ?red carpet?, sense induction example
MWE semantic head. Two different corpora are
collected (for the MWE and its semantic head).
Each consists of webtext snippets of length 15 to
200 tokens in which the MWE/semantic head ap-
pears. Given a MWE, a set of queries is created:
All synonyms of the MWE extracted from Word-
Net are collected
1
. The MWE is paired with each
synonym to create a set of queries. For each query,
snippets are collected by parsing the web-pages re-
turned by Yahoo!. The union of all snippets pro-
duces the MWE corpus. The corpus for a semantic
head is created equivalently.
To keep the computational time reasonable,
only the longest 3, 000 snippets are kept from each
corpus. Both corpora are PoS tagged (GENIA tag-
ger). In common with Agirre et al (2006), only
nouns are kept and lemmatized, since they are
more discriminative than other PoS.
B. Sense Induction methods can be broadly di-
vided into vector-space models and graph based
models. Sense induction methods are evaluated
under the SemEval-2007 framework (Agirre and
Soroa, 2007). We employ the collocational graph-
based sense induction of Klapaftis and Manand-
har (2008) in this work (henceforth referred to as
KM). The method consists of 3 stages:
Corpus preprocessing aims to capture nouns
that are contextually related to the target
MWE/head. Log-likelihood ratio (G
2
) (Dunning,
1993) with respect to a large reference corpus, Web
1T 5-gram Corpus (Brants and Franz, 2006), is
used to capture the contextually relevant nouns.
P
1
is the G
2
threshold below which nouns are re-
moved from corpora.
Graph creation. A collocation is defined as a
pair of nouns cooccuring within a snippet. Each
1
Thus, for ?red carpet?, corpora will be collected for ?red
carpet? and ?carpet?. The synonyms of ?red carpet? are
?rug?, ?carpet? and ?carpeting?
noun within a snippet is combined with every
other, generating
(
n
2
)
collocations. Each collo-
cation is represented as a weighted vertex. P
2
thresholds collocation frequencies and P
3
colloca-
tion weights. Weighted edges are drawn based on
cooccurrence of the corresponding vertices in one
or more snippets (e.g. w
8
and w
7,9
, fig. 1). In con-
trast to KM, frequencies for weighting vertices and
edges are obtained from Yahoo! web-page counts
to deal with data sparsity.
Graph clustering uses Chinese Whispers
2
(Bie-
mann, 2006) to cluster the graph. Each cluster now
represents a sense of the target word.
KM produces larger number of clusters (uses)
than expected. To reduce it we exploit the one
sense per collocation property (Yarowsky, 1995).
Given a cluster l
i
, we compute the set S
i
of snip-
pets that contain at least one collocation of l
i
. Any
clusters l
a
and l
b
are merged if S
a
? S
b
.
C. Comparing the induced senses. We used
two techniques to measure the distributional simi-
larity of major uses of the MWE and its semantic
head, both based on Jaccard coefficient (J). ?Ma-
jor use? denotes the cluster of collocations which
tags the most snippets. Lee (1999) shows that
J performs better than other symmetric similarity
measures such as cosine, Jensen-Shannon diver-
gence, etc. The first is J
c
= J(A,B) =
|A?B|
|A?B|
,
where A, B are sets of collocations. The second,
J
sn
, is based on the snippets that are tagged by
the induced uses. Let K
i
be the set of snippets in
which at least one collocation of the use i occurs.
J
sn
= J(K
j
,K
k
), where j, k are the major uses
of the MWE and its semantic head, respectively.
D. Determining compositionality. Given the
major uses of a MWE and its semantic head,
the MWE is considered as compositional, when
the corresponding distributional similarity mea-
sure (J
c
or J
sn
) value is above a parameter thresh-
old, sim. Otherwise, it is considered as non-
compositional.
3 Test set of MWEs
To the best of our knowledge there are no noun
compound datasets accompanied with composi-
tionality judgements available. Thus, we devel-
oped an algorithm to aid human annotation. For
each of the 52, 217 MWEs of WordNet 3.0 (Miller,
1995) we collected:
2
Chinese Whispers is not guaranteed to converge, thus
200 was adopted as the maximum number of iterations.
66
Non-compositional MWEs
agony aunt, black maria, dead end, dutch oven,
fish finger, fool?s paradise, goat?s rue, green light,
high jump, joint chiefs, lip service, living rock,
monkey puzzle, motor pool, prince Albert,
stocking stuffer, sweet bay, teddy boy, think tank
Compositional MWEs
box white oak, cartridge brass, common iguana,
closed chain, eastern pipistrel, field mushroom,
hard candy, king snake, labor camp, lemon tree,
life form, parenthesis-free notation, parking brake,
petit juror, relational adjective, taxonomic category,
telephone service, tea table, upland cotton
Table 1: Test set with compositionality annotation.
MWEs whose compositionality was successfully
detected by: (a) 1c1word baseline are in bold font,
(b) manual parameter selection are underlined and
(c) average cluster coefficient are in italics.
1. all synonyms of the MWE
2. all hypernyms of the MWE
3. sister-synsets of the MWE, within distance
3
3
4. synsets that are in holonymy or meronymy re-
lation to the MWE, within distance 3
If the semantic head of the MWE is also in the
above collection then the MWE is likely to be com-
positional, otherwise it is likely that the MWE is
non-compositional.
6, 287 MWEs were judged as potentially non-
compositional. We randomly chose 19 and
checked them manually. Those that were compo-
sitional were replaced by other randomly chosen
ones. The process was repeated until we ended up
with 19 non-compositional examples. Similarly,
19 negative examples that were judged as compo-
sitional were collected (Table 1).
4 Evaluation setting and results
The sense induction component of our algorithm
depends upon 3 parameters: P
1
is the G
2
threshold
below which noun are removed from corpora. P
2
thresholds collocation frequencies and P
3
colloca-
tion weights. We chose P
1
? {5, 10, 15}, P
2
?
{10
2
, 10
3
, 10
4
, 10
5
} and P
3
? {0.2, 0.3, 0.4}. For
reference, P
1
values of 3.84, 6.63, 10.83 and
15.13 correspond to G
2
values for confidence lev-
els of 95%, 99%, 99.9% and 99.99%, respectively.
To assess the performance of the proposed al-
gorithm we compute accuracy, the percentage of
MWEs whose compositionality was correctly de-
termined against the gold standard.
3
Locating sister synsets at distance D implies ascending
D steps and then descending D steps.
Figure 2: Proposed system and 1c1word accuracy.
Figure 3: Unweighted graph con/vity measures.
We compared the system?s performance against
a baseline, 1c1word, that assigns the whole graph
to a single cluster and no graph clustering is
performed. 1c1word corresponds to a relevant
SemEval-2007 baseline (Agirre and Soroa, 2007)
and helps in showing whether sense induction can
assist determining compositionality.
Our method was evaluated for each ?P
1
, P
2
, P
3
?
combination and similarity measures J
c
and J
sn
,
separately. We used our development set to deter-
mine if there are parameter values that verify our
hypothesis. Given a sim value (see section 2, last
paragraph), we chose the best performing parame-
ter combination manually.
The best results for manual parameter selection
were obtained for sim = 95% giving an accu-
racy of 68.42% for detecting non-compositional
MWEs. In all experiments, J
sn
outperforms J
c
.
With manually selected parameters, our system?s
accuracy is higher than 1c1word for all sim values
(5% points) (fig. 2, table 1). The initial hypothesis
holds; sense induction improves MWE composi-
tionality detection.
5 Unsupervised parameter tuning
We followed Korkontzelos et al (2009) to select
the ?best? parameters ?P
1
, P
2
, P
3
? for the collo-
cational graph of each MWE or head word. We
applied 8 graph connectivity measures (weighted
and unweighted versions of average degree, clus-
ter coefficient, graph entropy and edge density)
separately on each of the clusters (resulting from
the application of the chinese whispers algorithm).
Each graph connectivity measure assigns a
score to each cluster. We averaged the scores over
67
Figure 4: Weighted graph connectivity measures.
the clusters from the same graph. For each con-
nectivity measure, we chose the parameter combi-
nation ?P
1
, P
2
, P
3
? that gave the highest score.
While manual parameter tuning chooses a sin-
gle globally best set of parameters (see section 4),
the graph connectivity measures generate different
values of ?P
1
, P
2
, P
3
? for each graph.
5.1 Evaluation results
The best performing distributional similarity mea-
sure is J
sn
. Unweighted versions of graph con-
nectivity measures perform better than weighted
ones. Figures 3 and 4 present a comparison be-
tween the unweighted and weighted versions of
all graph connectivity measures, respectively, for
all sim values. Average cluster coefficient per-
forms better or equally well to the other graph
connectivity measures for all sim values (except
for sim ? [90%, 100%]). The accuracy of aver-
age cluster coefficient is equal (68.42%) to that
of manual parameter selection (section 4, table
1). The second best performing unweighted graph
connectivity measures is average graph entropy.
For weighted graph connectivity measures, aver-
age graph entropy performs best, followed by av-
erage weighted clustering coefficient.
6 Conclusion and Future Work
We hypothesized that sense induction can assist in
identifying compositional MWEs. We introduced
an unsupervised system to experimentally explore
the hypothesis, and showed that it holds. We
proposed a semi-supervised way to extract non-
compositional MWEs from WordNet. We showed
that graph connectivity measures can be success-
fully employed to perform unsupervised parame-
ter tuning of our system. It would be interesting
to explore ways to substitute querying Yahoo! so
as to make the system quicker. Experimentation
with more sophisticated graph connectivity mea-
sures could possibly improve accuracy.
References
E. Agirre and A. Soroa. 2007. Semeval-2007, task
02: Evaluating WSI and discrimination systems. In
proceedings of SemEval-2007. ACL.
E. Agirre, D. Mart??nez, O. de Lacalle, and A. Soroa.
2006. Two graph-based algorithms for state-of-the-
art WSD. In proceedings of EMNLP-2006. ACL.
T. Baldwin, C. Bannard, T. Tanaka, and D. Widdows.
2003. An empirical model of MWE decomposabil-
ity. In proceedings of the MWE workshop. ACL.
T. Baldwin. 2006. Compositionality and MWEs: Six
of one, half a dozen of the other? In proceedings of
the MWE workshop. ACL.
C. Bannard, T. Baldwin, and A. Lascarides. 2003.
A statistical approach to the semantics of verb-
particles. In proceedings of the MWE workshop.
ACL.
C. Biemann. 2006. Chinese whispers - an efficient
graph clustering algorithm and its application to
NLP problems. In proceedings of TextGraphs. ACL.
T. Brants and A. Franz. 2006. Web 1t 5-gram corpus,
version 1. Technical report, Google Research.
P. Cook, A. Fazly, and S. Stevenson. 2008. The VNC-
Tokens Dataset. In proceedings of the MWE work-
shop. ACL.
T. Dunning. 1993. Accurate methods for the statistics
of surprise and coincidence. Computational Lin-
guistics, 19(1):61?74.
G. Katz and E. Giesbrecht. 2006. Automatic identifi-
cation of non-compositional MWEs using latent se-
mantic analysis. In proceedings of the MWE work-
shop. ACL.
I. P. Klapaftis and S. Manandhar. 2008. WSI using
graphs of collocations. In proceedings of ECAI-
2008.
D. Klein and C. Manning. 2003. Fast exact inference
with a factored model for natural language parsing.
In proceedings of NIPS 15. MIT Press.
I. Korkontzelos, I. Klapaftis, and S. Manandhar. 2009.
Graph connectivity measures for unsupervised pa-
rameter tuning of graph-based sense induction sys-
tems. In proceedings of the UMSLLS Workshop,
NAACL HLT 2009.
L. Lee. 1999. Measures of distributional similarity. In
proceedings of ACL.
D. McCarthy, B. Keller, and J. Carroll. 2003. De-
tecting a continuum of compositionality in phrasal
verbs. In proceedings of the MWE workshop. ACL.
G. A. Miller. 1995. WordNet: a lexical database for
English. ACM, 38(11):39?41.
D. Yarowsky. 1995. Unsupervised WSD rivaling su-
pervised methods. In proceedings of ACL.
68
Proceedings of the NAACL HLT Workshop on Unsupervised and Minimally Supervised Learning of Lexical Semantics, pages 36?44,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Graph Connectivity Measures for Unsupervised Parameter Tuning
of Graph-Based Sense Induction Systems
Ioannis Korkontzelos, Ioannis Klapaftis and Suresh Manandhar
Department of Computer Science
The University of York
Heslington, York, YO10 5NG, UK
{johnkork, giannis, suresh}@cs.york.ac.uk
Abstract
Word Sense Induction (WSI) is the task of
identifying the different senses (uses) of a tar-
get word in a given text. This paper focuses
on the unsupervised estimation of the free pa-
rameters of a graph-based WSI method, and
explores the use of eight Graph Connectiv-
ity Measures (GCM) that assess the degree of
connectivity in a graph. Given a target word
and a set of parameters, GCM evaluate the
connectivity of the produced clusters, which
correspond to subgraphs of the initial (unclus-
tered) graph. Each parameter setting is as-
signed a score according to one of the GCM
and the highest scoring setting is then selected.
Our evaluation on the nouns of SemEval-2007
WSI task (SWSI) shows that: (1) all GCM es-
timate a set of parameters which significantly
outperform the worst performing parameter
setting in both SWSI evaluation schemes, (2)
all GCM estimate a set of parameters which
outperform the Most Frequent Sense (MFS)
baseline by a statistically significant amount
in the supervised evaluation scheme, and (3)
two of the measures estimate a set of parame-
ters that performs closely to a set of parame-
ters estimated in supervised manner.
1 Introduction
Using word senses instead of word forms is essential
in many applications such as information retrieval
(IR) and machine translation (MT) (Pantel and Lin,
2002). Word senses are a prerequisite for word sense
disambiguation (WSD) algorithms. However, they
are usually represented as a fixed-list of definitions
of a manually constructed lexical database. The
fixed-list of senses paradigm has several disadvan-
tages. Firstly, lexical databases often contain general
definitions and miss many domain specific senses
(Agirre et al, 2001). Secondly, they suffer from the
lack of explicit semantic and topical relations be-
tween concepts (Agirre et al, 2001). Thirdly, they
often do not reflect the exact content of the context
in which the target word appears (Veronis, 2004).
WSI aims to overcome these limitations of hand-
constructed lexicons.
Most WSI systems are based on the vector-space
model that represents each context of a target word
as a vector of features (e.g. frequency of cooccur-
ring words). Vectors are clustered and the resulting
clusters are taken to represent the induced senses.
Recently, graph-based methods have been employed
to WSI (Dorow and Widdows, 2003; Veronis, 2004;
Agirre and Soroa, 2007b).
Typically, graph-based approaches represent each
word co-occurring with the target word, within a
pre-specified window, as a vertex. Two vertices
are connected via an edge if they co-occur in one
or more contexts of the target word. This co-
occurrence graph is then clustered employing differ-
ent graph clustering algorithms to induce the senses.
Each cluster (induced sense) consists of words ex-
pected to be topically related to the particular sense.
As a result, graph-based approaches assume that
each context word is related to one and only one
sense of the target one.
Recently, Klapaftis and Manandhar (2008) argued
that this assumption might not be always valid, since
a context word may be related to more than one
senses of the target one. As a result, they pro-
36
posed the use of a graph-based model for WSI, in
which each vertex of the graph corresponds to a
collocation (word-pair) that co-occurs with the tar-
get word, while edges are drawn based on the co-
occurrence frequency of their associated colloca-
tions. Clustering of this collocational graph would
produce clusters, which consist of a set of collo-
cations. The intuition is that the produced clusters
will be less sense-conflating than those produced
by other graph-based approaches, since collocations
provide strong and consistent clues to the senses of
a target word (Yarowsky, 1995).
The collocational graph-based approach as well
as the majority of state-of-the-art WSI systems es-
timate their parameters either empirically or by em-
ploying supervised techniques. The SemEval-2007
WSI task (SWSI) participating systems UOY and
UBC-AS used labeled data for parameter estimation
(Agirre and Soroa, 2007a), while the authors of I2R,
UPV SI and UMND2 have empirically chosen val-
ues for their parameters. This issue imposes limits
on the unsupervised nature of these algorithms, as
well as on their performance on different datasets.
More specifically, when applying an unsupervised
WSI system on different datasets, one cannot be sure
that the same set of parameters is appropriate for all
datasets (Karakos et al, 2007). In most cases, a new
parameter tuning might be necessary. Unsupervised
estimation of free parameters may enhance the unsu-
pervised nature of systems, making them applicable
to any dataset, even if there are no tagged data avail-
able.
In this paper, we focus on estimating the free
parameters of the collocational graph-based WSI
method (Klapaftis and Manandhar, 2008) using
eight graph connectivity measures (GCM). Given a
parameter setting and the associated induced cluster-
ing solution, each induced cluster corresponds to a
subgraph of the original unclustered graph. A graph
connectivity measure GCMi scores each cluster by
evaluating the degree of connectivity of its corre-
sponding subgraph. Each clustering solution is then
assigned the average of the scores of its clusters. Fi-
nally, the highest scoring solution is selected.
Our evaluation on the nouns of SWSI shows
that GCM improve the worst performing parame-
ter setting by large margins in both SWSI evaluation
schemes, although they are below the best perform-
ing parameter setting. Moreover, the evaluation in
a WSD setting shows that all GCM estimate a set
of parameters which are above the Most Frequent
Sense (MFS) baseline by a statistically significant
amount. Finally our results show that two of the
measures, i.e. average degree and weighted average
degree, estimate a set of parameters that performs
closely to a set of parameters estimated in a super-
vised manner. All of these findings, suggest that
GCM are able to identify useful differences regard-
ing the quality of the induced clusters for different
parameter combinations, in effect being useful for
unsupervised parameter estimation.
2 Collocational graphs for WSI
Let bc, be the base corpus, which consists of para-
graphs containing the target word tw. The aim is
to induce the senses of tw given bc as the only in-
put. Let rc be a large reference corpus. In Klapaftis
and Manandhar (2008) the British National Corpus1
is used as a reference corpus. The WSI algorithm
consists of the following stages.
Corpus pre-processing The target of this stage is
to filter the paragraphs of the base corpus, in order to
keep the words which are topically (and possibly se-
mantically) related to the target one. Initially, tw is
removed from bc and both bc and rc are PoS-tagged.
In the next step, only nouns are kept in the para-
graphs of bc, since they are characterised by higher
discriminative ability than verbs, adverbs or adjec-
tives which may appear in a variety of different con-
texts. At the end of this pre-processing step, each
paragraph of bc and rc is a list of lemmatized nouns
(Klapaftis and Manandhar, 2008).
In the next step, the paragraphs of bc are fil-
tered by removing common nouns which are noisy;
contextually not related to tw. Given a contex-
tual word cw that occurs in the paragraphs of bc, a
log-likelihood ratio (G2) test is employed (Dunning,
1993), which checks if the distribution of cw in bc
is similar to the distribution of cw in rc; p(cw|bc) =
p(cw|rc) (null hypothesis). If this is true, G2 has a
small value. If this value is less than a pre-specified
threshold (parameter p1) the noun is removed from
bc.
1The British National Corpus (BNC) (2001, version 2). Dis-
tributed by Oxford University Computing Services.
37
Target: cnn nbc Target: nbc news
nbc tv nbc tv
cnn tv soap opera
cnn radio nbc show
news newscast news newscast
radio television nbc newshour
cnn headline cnn headline
nbc politics radio tv
breaking news breaking news
Table 1: Collocations connected to cnn nbc and nbc news
This process identifies nouns that are more indica-
tive in bc than in rc and vice versa. However, in this
setting we are not interested in nouns which have
a distinctive frequency in rc. As a result, each cw
which has a relative frequency in bc less than in rc
is filtered out. At the end of this stage, each para-
graph of bc is a list of nouns which are assumed to
be contextually related to the target word tw.
Creating the initial collocational graph The tar-
get of this stage is to determine the related nouns,
which will form the collocations, and the weight of
each collocation. Klapaftis and Manandhar (2008)
consider collocations of size 2, i.e. pairs of nouns.
For each paragraph of bc of size n, collocations
are identified by generating all the possible (cn2
)
combinations. The frequency of a collocation c is
the number of paragraphs in the whole SWSI corpus
(27132 paragraphs), in which c occurs.
Each collocation is assigned a weight, measuring
the relative frequency of two nouns co-occurring.
Let freqij denote the number of paragraphs in
which nouns i and j cooccur, and freqj denote the
number of paragraphs, where noun j occurs. The
conditional probability p(i|j) is defined in equation
1, and p(j|i) is computed in a similar way. The
weight of collocation cij is the average of these con-
ditional probabilities wcij = p(i|j) + p(j|i).
p(i|j) = freqijfreqj (1)
Finally, Klapaftis and Manandhar (2008) only ex-
tract collocations which have frequency (parame-
ter p2) and weight (parameter p3) higher than pre-
specified thresholds. This filtering appears to com-
pensate for inaccuracies in G2, as well as for low-
frequency distant collocations that are ambiguous.
Each weighted collocation is represented as a ver-
tex. Two vertices share an edge, if they co-occur in
one or more paragraphs of bc.
Populating and weighing the collocational graph
The constructed graph, G, is sparse, since the pre-
vious stage attempted to identify rare events, i.e.
co-occurring collocations. To address this problem,
Klapaftis and Manandhar (2008) apply a smooth-
ing technique, similar to the one in Cimiano et
al. (2005), extending the principle that a word is
characterised by the company it keeps (Firth, 1957)
to collocations. The target is to discover new edges
between vertices and to assign weights to all edges.
Each vertex i (collocation ci) is associated to
a vector V Ci containing its neighbouring vertices
(collocations). Table 1 shows an example of two
vertices, cnn nbc and nbc news, which are discon-
nected in G of the target word network. The example
was taken from Klapaftis and Manandhar (2008).
In the next step, the similarity between all vertex
vectors V Ci and V Cj is calculated using the Jaccard
coefficient, i.e. JC(V Ci, V Cj) = |V Ci?V Cj ||V Ci?V Cj | . Twocollocations ci and cj are mutually similar if ci is the
most similar collocation to cj and vice versa.
Given that collocations ci and cj are mutually
similar, an occurrence of a collocation ck with one
of ci, cj is also counted as an occurrence with the
other collocation. For example in Table 1, if cnn nbc
and nbc news are mutually similar, then the zero-
frequency event between nbc news and cnn tv is
set equal to the joint frequency between cnn nbc
and cnn tv. Marginal frequencies of collocations
are updated and the overall result is consequently a
smoothing of relative frequencies.
The weight applied to each edge connecting ver-
tices i and j (collocations ci and cj ) is the maximum
of their conditional probabilities: p(i|j) = freqijfreqj ,where freqi is the number of paragraphs collocation
ci occurs. p(j|i) is defined similarly.
Inducing senses and tagging In this final stage,
the collocational graph is clustered to produced the
senses (clusters) of the target word. The clustering
method employed is Chinese Whispers (CW) (Bie-
mann, 2006). CW is linear to the number of graph
edges, while it offers the advantage that it does not
require any input parameters, producing the clusters
of a graph automatically.
38
Figure 1: An example undirected weighted graph.
Initially, CW assigns all vertices to different
classes. Each vertex i is processed for a number of
iterations and inherits the strongest class in its lo-
cal neighbourhood (LN) in an update step. LN is
defined as the set of vertices which share an edge
with i. In each iteration for vertex i: each class, cl,
receives a score equal to the sum of the weights of
edges (i, j), where j has been assigned to class cl.
The maximum score determines the strongest class.
In case of multiple strongest classes, one is chosen
randomly. Classes are updated immediately, mean-
ing that a vertex can inherit from its LN classes that
were introduced in the same iteration.
Once CW has produced the clusters of a target
word, each of the instances of tw is tagged with
one of the induced clusters. This process is simi-
lar to Word Sense Disambiguation (WSD) with the
difference that the sense repository has been auto-
matically produced. Particularly, given an instance
of tw in paragraph pi: each induced cluster cl is as-
signed a score equal to the number of its collocations
(i.e. pairs of words) occurring in pi. We observe that
the tagging method exploits the one sense per collo-
cation property (Yarowsky, 1995), which means that
WSD based on collocations is probably finer than
WSD based on simple words, since ambiguity is re-
duced (Klapaftis and Manandhar, 2008).
3 Unsupervised parameter tuning
In this section we investigate unsupervised ways to
address the issue of choosing parameter values. To
this end, we employ a variety of GCM, which mea-
sure the relative importance of each vertex and as-
sess the overall connectivity of the corresponding
graph. These measures are average degree, cluster
coefficient, graph entropy and edge density (Navigli
and Lapata, 2007; Zesch and Gurevych, 2007).
GCM quantify the degree of connectivity of the
produced clusters (subgraphs), which represent the
senses (uses) of the target word for a given cluster-
ing solution (parameter setting). Higher values of
GCM indicate subgraphs (clusters) of higher con-
nectivity. Given a parameter setting, the induced
clustering solution and a graph connectivity measure
GCMi, each induced cluster is assigned the result-
ing score of applying GCMi on the corresponding
subgraph of the initial unclustered graph. Each clus-
tering solution is assigned the average of the scores
of its clusters (table 6), and the highest scoring one
is selected.
For each measure, we have developed two ver-
sions, i.e. one which considers the edge weights in
the subgraph, and a second which does not. In the
following description the terms graph and subgraph
are interchangeable.
Let G = (V,E) be an undirected graph (in-
duced sense), where V is a set of vertices and E =
{(u, v) : u, v ? V } a set of edges connecting vertex
pairs. Each edge is weighted by a positive weight,
W : wuv ? [0,?). Figure 1 shows a small example
to explain the computation of GCM. The graph con-
sists of 8 vertices, |V | = 8, and 10 edges, |E| = 10.
Edge weights appear on edges, e.g. wab = 14 .
Average Degree The degree (deg) of a vertex u is
the number of edges connected to u:
deg(u) = |{(u, v) ? E : v ? V }| (2)
The average degree (AvgDeg) of a graph can be
computed as:
AvgDeg(G(V,E)) = 1|V |
?
u?V
deg(u) (3)
The first row of table 2 shows the vertex degrees
of the example graph (figure 1) and AvgDeg(G) =
20
8 = 2.5.Edge weights can be integrated into the degree
computation. Let mew be the maximum edge
weight in the graph:
mew = max
(u,v)?E
wuv (4)
Average Weighted Degree The weighted de-
gree(w deg) of a vertex is defined as:
w deg(u) = 1|V |
?
(u,v)?E
wuv
mew (5)
39
a b c d e f g h
deg(u) 2 2 3 4 3 3 2 1
wdeg(u) 54 1 52 94 74 32 32 14
Tu 1 1 1 1 1 2 1 0
cc(u) 1 1 13 16 13 23 1 0
WTu 34 1 14 14 12 32 14 0
wcc(u) 34 1 112 124 16 12 14 0
p(u) 110 110 320 15 320 320 110 120
en(u) ? 100 33 33 41 46 41 41 33 22
wp(u) 116 120 18 980 780 340 340 180
we(u) ? 100 25 22 38 35 31 28 28 8
Table 2: Computations of graph connectivity measures
and relevant quantities on the example graph (figure 1).
Average weighted degree (AvgWDeg), similarly to
AvgDeg, is averaged over all vertices of the graph.
In the graph of figure 1, mew = 1. The second row
of table 2 shows the weighted degrees of all vertices.
AvgWDeg(G) = 4836 ' 1.33.
Average Cluster Coefficient The cluster coeffi-
cient (cc) of a vertex, u, is defined as:
cc(u) = Tu2?1ku(ku ? 1) (6)
Tu =
?
(u,v)?E
?
(v,x)?E
x 6=u
1 (7)
Tu is the number of edges between the ku neigh-
bours of u. Obviously ku = deg(u). 2?1ku(ku? 1)
would be the number of edges between the neigh-
bours of u if the graph they define was fully con-
nected. Average cluster coefficient (AvgCC) is aver-
aged over all vertices of the graph.
The computations of Tu and cc(u) on the example
graph are shown in the third and fourth rows of table
2. Consequently, AvgCC(G) = 916 = 0.5625.
Average Weighted Cluster Coefficient Let WTu
be the sum of edge weights between the neighbours
of u over mew. Weighted cluster coefficient (wcc)
can be computed as:
wcc(u) = WTu2?1ku(ku ? 1) (8)
WTu = 1mew
?
(u,v)?E
?
(v,x)?E
x 6=u
wvx (9)
Average weighted cluster coefficient (AvgWCC) is
averaged over all vertices of the graph. The com-
putations of WTu and wcc(u) on the example graph
(figure 1) are shown in the fifth and sixth rows of
table 2 and AvgWCC(G) = 678?24 ' 0.349.
Graph Entropy Entropy measures the amount of
information (alternatively the uncertainty) in a ran-
dom variable. For a graph, high entropy indicates
that many vertices are equally important and low en-
tropy that only few vertices are relevant (Navigli and
Lapata, 2007). The entropy (en) of a vertex u can be
defined as:
en(u) = ?p(u) log2 p(u) (10)
The probability of a vertex, p(u), is determined by
the degree distribution:
p(u) =
{deg(u)
2|E|
}
u?V
(11)
Graph entropy (GE) is computed by summing all
vertex entropies and normalising by log2 |V |. The
seventh and eighth row of table 2 show the compu-
tations of p(u) and en(u) on the example graph, re-
spectively. Thus, GE ' 0.97.
Weighted Graph Entropy Similarly to previous
graph connectivity measures, the weighted entropy
(wen) of a vertex u is defined as:
we(u) = ?wp(u) log2 wp(u) (12)
where: wp(u) =
{ w deg(u)
2 ?mew ? |E|
}
u?V
Weighted graph entropy (GE) is computed by sum-
ming all vertex weighted entropies and normalising
by log2 |V |. The last two rows of table 2 show the
computations of wp(u) and we(u) on the example
graph. Consequently, WGE ' 0.73.
Edge Density and Weighted Edge Density Edge
density (ed) quantifies how many edges the graph
has, as a ratio over the number of edges of a fully
connected graph of the same size:
A(V ) = 2
(|V |
2
)
(13)
40
Edge density (ed) is a global graph connectivity
measure; it refers to the whole graph and not a spe-
cific vertex. Edge density (ed) and weighted edge
density (wed) can be defined as follows:
ed(G(V,E)) = |E|A(V ) (14)
wed(G(V,E)) = 1A(V )
?
(u,v)?E
wu,v
mew (15)
In the graph of figure 1: A(V ) = 2(82
) = 28,
ed(G) = 1028 ' 0.357,
? wu,v
mew = 6 and wed(G) =6
28 ' 0.214.The use of the aforementioned GCM allows the
estimation of a different parameter setting for each
target word. Table 3 shows the parameters of the col-
locational graph-based WSI system (Klapaftis and
Manandhar, 2008). These parameters affect how the
collocational graph is constructed, and in effect the
quality of the induced clusters.
4 Evaluation
4.1 Experimental setting
The collocational WSI approach was evaluated un-
der the framework and corpus of SemEval-2007
WSI task (Agirre and Soroa, 2007a). The corpus
consists of text of the Wall Street Journal corpus,
and is hand-tagged with OntoNotes senses (Hovy et
al., 2006). The evaluation focuses on all 35 nouns of
SWSI. SWSI task employs two evaluation schemes.
In unsupervised evaluation, the results are treated as
clusters of contexts and gold standard (GS) senses
as classes. In a perfect clustering solution, each in-
duced cluster contains the same contexts as one of
the classes (Homogeneity), and each class contains
the same contexts as one of the clusters (Complete-
ness). F-Score is used to assess the overall quality of
clustering. Entropy and purity are also used, com-
plementarily. F-Score is a better measure than en-
tropy or purity, since F-Score measures both homo-
geneity and completeness, while entropy and purity
measure only the former. In the second scheme, su-
pervised evaluation, the training corpus is used to
map the induced clusters to GS senses. The testing
corpus is then used to measure WSD performance
(Table 4, Sup. Recall).
The graph-based collocational WSI method is re-
ferred as Col-Sm (where ?Col? stands for the ?col-
Parameter Range Value
G2 threshold 5, 10, 15 p1 = 5
Collocation frequency 4, 6, 8, 10 p2 = 8
Collocation weight 0.2, 0.3, 0.4 p3 = 0.2
Table 3: Parameters ranges and values in Klapaftis and
Manandhar (2008)
locational WSI? approach and ?Sm? for its ver-
sion using ?smoothing?). Col-Bl (where ?Bl? stands
for ?baseline?) refers to the same system without
smoothing. The parameters of Col-Sm were origi-
nally estimated by cross-validation on the training
set of SWSI. Out of 72 parameter combinations, the
setting with the highest F-Score was chosen and ap-
plied to all 35 nouns of the test set. This is referred
as Col-Sm-org (where ?org? stands for ?original?) in
Table 4. Table 3 shows all values for each parameter,
and the chosen values, under supervised parameter
estimation2. Col-Bl-org (Table 4) induces senses as
Col-Sm-org does, but without smoothing.
In table 4, Col-Sm-w (respectively Col-Bl-w)
refers to the evaluation of Col-Sm (Col-Bl), follow-
ing the same technique for parameter estimation as
in Klapaftis and Manandhar (2008) for each target
word separately (?w? stands for ?word?). Given that
GCM are applied for each target word separately,
these baselines will allow to see the performance of
GCM compared to a supervised setting.
The 1c1inst baseline assigns each instance to a
distinct cluster, while the 1c1w baseline groups all
instances of a target word into a single cluster. 1c1w
is equivalent to MFS in this setting. The fifth column
of table 4 shows the average number of clusters.
The SWSI participant systems UOY and UBC-AS
used labeled data for parameter estimation. The au-
thors of I2R, UPV SI and UMND2 have empirically
chosen values for their parameters.
The next subsection presents the evaluation of
GCM as well as the results of SWSI systems. Ini-
tially, we provide a brief discussion on the differ-
ences between the two evaluation schemes of SWSI
that will allow for a better understanding of GCM
performance.
4.2 Analysis of results and discussion
Evaluation of WSI methods is a difficult task. For
instance, 1c1inst (Table 4) achieves perfect purity
2CW performed 200 iterations for all experiments, because
it is not guaranteed to converge.
41
System Unsupervised Evaluation Sup.
FSc. Pur. Ent. # Cl. Recall
Col-Sm-org 78.0 88.6 31.0 5.9 86.4
Col-Bl-org 73.1 89.6 29.0 8.0 85.6
Col-Sm-w 80.9 88.0 32.5 4.3 85.5
Col-Bl-w 78.1 88.3 31.7 5.4 84.3
UBC-AS 80.8 83.6 43.5 1.6 80.7
UPV SI 69.9 87.4 30.9 7.2 82.5
I2R 68.0 88.4 29.7 3.1 86.8
UMND2 67.1 85.8 37.6 1.7 84.5
UOY 65.8 89.8 25.5 11.3 81.6
1c1w-MFS 80.7 82.4 46.3 1 80.9
1c1inst 6.6 100 0 73.1 N/A
Table 4: Evaluation of WSI systems and baselines.
and entropy. However, F-Score of 1c1inst is low,
because the GS senses are spread among clusters,
decreasing unsupervised recall. Supervised recall of
1c1inst is undefined, because each cluster tags only
one instance. Hence, clusters tagging instances in
the test corpus do not tag any instances in the train
corpus and the mapping cannot be performed. 1c1w
achieves high F-Score due to the dominance of MFS
in the testing corpus. However, its purity, entropy
and supervised recall are much lower than other sys-
tems, because it only induces the dominant sense.
Clustering solutions that achieve high supervised
recall do not necessarily achieve high F-Score,
mainly because F-Score penalises systems for in-
ducing more clusters than the corresponding GS
classes, as 1cl1inst does. Supervised evaluation
seems to be more neutral regarding the number of
clusters, since clusters are mapped into a weighted
vector of senses. Thus, inducing a number of clus-
ters similar to the number of senses is not a require-
ment for good results (Agirre and Soroa, 2007a).
High supervised recall means high purity and en-
tropy, as in I2R, but not vice versa, as in UOY. UOY
produces many clean clusters, however these are un-
reliably mapped to senses due to insufficient train-
ing data. On the contrary, I2R produces a few clean
clusters, which are mapped more reliably.
Comparing the performance of SWSI systems
shows that none performs well in both evaluation
settings, in effect being biased against one of the
schemes. However, this is not the case for the collo-
cational WSI method, which achieves a high perfor-
mance in both evaluation settings.
Table 6 presents the results of applying the graph
System Bound Unsupervised Evaluation Sup.
type FSc. Pur. Ent. # Cl. Recall
Col-Sm MaxR 79.3 90.5 26.6 7.0 88.6
Col-Sm MinR 62.9 89.0 26.7 12.7 78.8
Col-Bl MaxR 72.9 91.8 23.2 9.6 88.7
Col-Bl MinR 57.5 89.0 26.4 14.4 76.2
Col-Sm MaxF 83.2 90.0 28.7 4.9 86.6
Col-Sm MinF 43.6 90.2 22.1 17.6 83.7
Col-Bl MaxF 81.1 90.0 28.7 5.3 81.8
Col-Bl MinF 34.1 90.5 20.5 20.4 81.5
Table 5: Upper and lower performance bounds for sys-
tems Col-Sm and Col-Bl.
connectivity measures of section 3 in order to choose
the parameter values for the collocational WSI sys-
tem, for each word separately. The evaluation is
done both for Col-Sm and Col-Bl that use and ignore
smoothing, respectively.
To evaluate the supervised recall performance
using the graph connectivity measures, we com-
puted both the upper and lower bounds of Col-Sm,
i.e. the best and worst supervised recall, respectively
(MaxR and MinR in table 5). In the former case,
we selected the parameter combination per target
word that performs best (Col-Sm, MaxR in table 5),
which resulted in 88.6% supervised recall (F-Score:
79.3%), while in the latter we selected the worst per-
forming one, which resulted in 78.8% supervised re-
call (F-Score: 62.9%). In table 6 we observe that
the supervised recall of all measures is significantly
lower than the upper bound. However, all measures
perform significantly better than the lower bound
(McNemar?s test, confidence level: 95%); the small-
est difference is 4.9%, in the case of weighted edge
density. The picture is the same for Col-Bl.
In the same vein, we computed both the upper and
lower bounds of Col-Sm in terms of F-Score, 83.2%
and 43.6%, respectively (Col-Sm, MinF and MaxF
in table 5). The performance of the system is lower
than the upper bound, for all GCM. Despite that, we
observe that all measures except edge density and
weighted edge density outperform the lower bound
by large margins.
The comparison of GCM performance against
the lower and upper bounds of Col-Sm and Col-Bl
shows that GCM are able to identify useful differ-
ences regarding the degree of connectivity of in-
duced clusters, and in effect suggest parameter val-
ues that perform significantly better than the worst
42
Col-Sm Col-Bl
Unsupervised Evaluation Sup. Unsupervised Evaluation Sup.
Graph Connectivity Measure FSc Pur. Ent. # Cl. Recall FSc Pur. Ent. # Cl. Recall
Average Degree 79.2 87.2 34.2 3.9 84.8 77.5 31.3 88.4 5.7 83.8
Average Weighted Degree 77.1 87.8 32.0 5.5 84.2 75.1 28.3 89.6 8.5 83.3
Average Cluster Coefficient 72.5 88.8 28.5 9.1 83.9 68.7 24.0 90.9 12.9 83.9
Average Weighted Cluster Coefficient 65.8 88.4 28.0 9.6 84.1 68.9 22.4 91.3 13.9 83.7
Graph Entropy 67.0 89.6 25.9 12.3 83.8 68.5 22.1 91.8 14.4 84.4
Weighted Graph Entropy 72.7 89.4 28.1 9.6 84.1 72.2 23.5 91.2 12.5 84.0
Edge Density 47.8 91.8 19.4 18.4 84.8 42.0 16.9 92.8 21.9 84.1
Weighted Edge Density 53.4 90.2 23.1 15.5 83.7 42.2 17.1 92.7 21.9 83.9
Table 6: Unsupervised & supervised evaluation of the collocational WSI approach using graph connectivity measures.
case. However, they are all unable to approximate
the upper bound for both evaluation schemes, which
is also the case for the supervised estimation of pa-
rameters per target word (Col-Sm-w and Col-Bl-w).
In Table 6, we also observe that all measures
achieve higher supervised recall scores than the
MFS baseline. The increase is statistically signif-
icant (McNemar?s test, confidence level: 95%) in
all cases. This result shows that irrespective of the
number of clusters produced (low F-Score), GCM
are able to estimate a set of parameters that provides
clean clusters (low entropy), which when mapped to
GS senses improve upon the most frequent heuristic,
unlike the majority of unsupervised WSD systems.
Regarding the comparison between different
GCM, we observe that average degree and weighted
average degree for Col-Sm (Col-Bl) perform
closely to Col-Sm-w (Col-Bl-w) for both evaluation
schemes. This is due to the fact that they produce a
number of clusters similar to Col-Sm-w (Col-Bl-w),
while at the same time their distributions of clusters
over the target words? instances are also similar.
On the contrary, the remaining GCM tend to pro-
duce larger numbers of clusters compared to both
Col-Sm-w (Col-Bl-w) and the GS, in effect being
penalised by F-Score. As it has already been men-
tioned, supervised recall is less affected by a large
number of clusters, which causes small differences
among GCM.
Determining whether the weighted or unweighted
version of GCM performs better depends on the
GCM itself. Weighted graph entropy performs in all
cases better than the unweighted version. For aver-
age cluster coefficient and edge density, we cannot
extract a safe conclusion. Unweighted average de-
gree performs better than the weighted version.
5 Conclusion and future work
In this paper, we explored the use of eight graph con-
nectivity measures for unsupervised estimation of
free parameters of a collocational graph-based WSI
system. Given a parameter setting and the associ-
ated induced clustering solution, each cluster was
scored according to the connectivity degree of its
corresponding subgraph, as assessed by a particular
graph connectivity measure. Each clustering solu-
tion was then assigned the average of its clusters?
scores, and the highest scoring one was selected.
Evaluation on the nouns of SemEval-2007 WSI
task (SWSI) showed that all eight graph connectiv-
ity measures choose parameters for which the corre-
sponding performance of the system is significantly
higher than the lower performance bound, for both
the supervised and unsupervised evaluation scheme.
Moreover, the selected parameters produce results
which outperform the MFS baseline by a statisti-
cally significant amount in the supervised evalua-
tion scheme. The best performing measures, average
degree and weighted average degree, perform com-
parably well to the set of parameters chosen by a
supervised parameter estimation. In general, graph
connectivity measures can quantify significant dif-
ferences regarding the degree of connectivity of in-
duced clusters.
Future work focuses on further exploiting graph
connectivity measures. Graph theoretic literature
proposes a variety of measures capturing graph
properties. Some of these measures might help in
improving WSI performance, while at the same time
keeping graph-based WSI systems totally unsuper-
vised.
43
References
Eneko Agirre and Aitor Soroa. 2007a. Semeval-2007
task 02: Evaluating word sense induction and discrim-
ination systems. In Proceedings of the Fourth Interna-
tional Workshop on Semantic Evaluations (SemEval-
2007), pages 7?12, Prague, Czech Republic. Associa-
tion for Computational Linguistics.
Eneko Agirre and Aitor Soroa. 2007b. Ubc-as: A graph
based unsupervised system for induction and classi-
fication. In Proceedings of the Fourth International
Workshop on Semantic Evaluations (SemEval-2007),
pages 346?349, Prague, Czech Republic. Association
for Computational Linguistics.
Eneko Agirre, Olatz Ansa, Eduard Hovy, and David Mar-
tinez. 2001. Enriching wordnet concepts with topic
signatures, Sep.
Chris Biemann. 2006. Chinese whispers - an efficient
graph clustering algorithm and its application to nat-
ural language processing problems. In Proceedings
of TextGraphs: the Second Workshop on Graph Based
Methods for Natural Language Processing, pages 73?
80, New York City, June. Association for Computa-
tional Linguistics.
Philipp Cimiano, Andreas Hotho, and Steffen Staab.
2005. Learning concept hierarchies from text corpora
using formal concept analysis. Journal of Artificial In-
telligence research, 24:305?339.
Beate Dorow and Dominic Widdows. 2003. Discover-
ing corpusspecific word senses. In Proceedings 10th
conference of the European chapter of the ACL, pages
79?82, Budapest, Hungary.
Ted E. Dunning. 1993. Accurate methods for the statis-
tics of surprise and coincidence. Computational Lin-
guistics, 19(1):61?74.
John R. Firth. 1957. A synopsis of linguistic theory,
1930-1955. Studies in Linguistic Analysis, pages 1?
32.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes:
The 90% solution. In Proceedings of the Human Lan-
guage Technology Conference of the NAACL, Com-
panion Volume: Short Papers, pages 57?60, New York
City, USA. Association for Computational Linguistics.
Damianos Karakos, Jason Eisner, Sanjeev Khudanpur,
and Carey Priebe. 2007. Cross-instance tuning of un-
supervised document clustering algorithms. In Human
Language Technologies 2007: The Conference of the
North American Chapter of the Association for Com-
putational Linguistics; Proceedings of the Main Con-
ference, pages 252?259, Rochester, New York, April.
Association for Computational Linguistics.
Ioannis P. Klapaftis and Suresh Manandhar. 2008. Word
sense induction using graphs of collocations. In In
Proceedings of the 18th European Conference on Ar-
tificial Intelligence, (ECAI-2008), Patras, Greece.
R. Navigli and M. Lapata. 2007. Graph connectiv-
ity measures for unsupervised word sense disambigua-
tion. In 20th International Joint Conference on Artifi-
cial Intelligence (IJCAI 2007), pages 1683?1688, Hy-
derabad, India, January.
Patrick Pantel and Dekang Lin. 2002. Discovering
word senses from text. In KDD ?02: Proceedings
of the eighth ACM SIGKDD international conference
on Knowledge discovery and data mining, pages 613?
619, New York, NY, USA. ACM Press.
Jean Veronis. 2004. Hyperlex: lexical cartography for
information retrieval. Computer Speech & Language,
18(3):223?252, July.
David Yarowsky. 1995. Unsupervised word sense disam-
biguation rivaling supervised methods. In Meeting of
the Association for Computational Linguistics, pages
189?196.
Torsten Zesch and Iryna Gurevych. 2007. Analysis of
the wikipedia category graph for NLP applications. In
Proceedings of the Second Workshop on TextGraphs:
Graph-Based Algorithms for Natural Language Pro-
cessing, pages 1?8, Rochester, NY, USA. Association
for Computational Linguistics.
44
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1263?1271,
Beijing, August 2010
Estimating Linear Models for Compositional Distributional Semantics
Fabio Massimo Zanzotto1
(1) Department of Computer Science
University of Rome ?Tor Vergata?
zanzotto@info.uniroma2.it
Ioannis Korkontzelos
Department of Computer Science
University of York
johnkork@cs.york.ac.uk
Francesca Fallucchi1,2
(2) Universita` Telematica
?G. Marconi?
f.fallucchi@unimarconi.it
Suresh Manandhar
Department of Computer Science
University of York
suresh@cs.york.ac.uk
Abstract
In distributional semantics studies, there
is a growing attention in compositionally
determining the distributional meaning of
word sequences. Yet, compositional dis-
tributional models depend on a large set
of parameters that have not been explored.
In this paper we propose a novel approach
to estimate parameters for a class of com-
positional distributional models: the addi-
tive models. Our approach leverages on
two main ideas. Firstly, a novel idea for
extracting compositional distributional se-
mantics examples. Secondly, an estima-
tion method based on regression models
for multiple dependent variables. Experi-
ments demonstrate that our approach out-
performs existing methods for determin-
ing a good model for compositional dis-
tributional semantics.
1 Introduction
Lexical distributional semantics has been largely
used to model word meaning in many fields as
computational linguistics (McCarthy and Carroll,
2003; Manning et al, 2008), linguistics (Harris,
1964), corpus linguistics (Firth, 1957), and cogni-
tive research (Miller and Charles, 1991). The fun-
damental hypothesis is the distributional hypoth-
esis (DH): ?similar words share similar contexts?
(Harris, 1964). Recently, this hypothesis has been
operationally defined in many ways in the fields of
physicology, computational linguistics, and infor-
mation retrieval (Li et al, 2000; Pado and Lapata,
2007; Deerwester et al, 1990).
Given the successful application to words, dis-
tributional semantics has been extended to word
sequences. This has happened in two ways: (1)
via the reformulation of DH for specific word se-
quences (Lin and Pantel, 2001); and (2) via the
definition of compositional distributional seman-
tics (CDS) models (Mitchell and Lapata, 2008;
Jones and Mewhort, 2007). These are two differ-
ent ways of addressing the problem.
Lin and Pantel (2001) propose the pattern dis-
tributional hypothesis that extends the distribu-
tional hypothesis for specific patterns, i.e. word
sequences representing partial verb phrases. Dis-
tributional meaning for these patterns is derived
directly by looking to their occurrences in a cor-
pus. Due to data sparsity, patterns of different
length appear with very different frequencies in
the corpus, affecting their statistics detrimentally.
On the other hand, compositional distributional
semantics (CDS) propose to obtain distributional
meaning for sequences by composing the vectors
of the words in the sequences (Mitchell and Lap-
ata, 2008; Jones and Mewhort, 2007). This ap-
proach is fairly interesting as the distributional
meaning of sequences of different length is ob-
tained by composing distributional vectors of sin-
gle words. Yet, many of these approaches have a
large number of parameters that cannot be easily
estimated.
In this paper we propose a novel approach to es-
1263
timate parameters for additive compositional dis-
tributional semantics models. Our approach lever-
ages on two main ideas. Firstly, a novel way for
extracting compositional distributional semantics
examples and counter-examples. Secondly, an es-
timation model that exploits these examples and
determines an equation system that represents a
regression problem with multiple dependent vari-
ables. We propose a method to estimate a solu-
tion of this equation system based on the Moore-
Penrose pseudo-inverse matrices (Penrose, 1955).
The rest of the paper is organised as follows:
Firstly, we shortly review existing compositional
distributional semantics (CDS) models (Sec. 2).
Then we describe our model for estimating CDS
models parameters (Sec. 3). In succession, we
introduce a way to extract compositional dis-
tributional semantics examples from dictionaries
(Sec. 4). Then, we discuss the experimental set up
and the results of our linear CDS model with es-
timated parameters with respect to existing CDS
models (Sec. 5).
2 Models for compositional
distributional semantics (CDS)
A CDS model is a function  that computes the
distributional vector of a sequence of words s by
combining the distributional vectors of its com-
ponent words w1 . . .wn. Let(s) be the distribu-
tional vector describing s and ~wi the distributional
vectors describing its component word wi. Then,
the CDS model can be written as:
(s) = (w1 . . .wn) = ~w1  . . . ~wn (1)
This generic model has been fairly studied and
many different functions have been proposed and
tested.
Mitchell and Lapata (2008) propose the fol-
lowing general CDS model for 2-word sequences
s = xy:
(s) = (xy) = f(~x, ~y,R,K) (2)
where ~x and ~y are respectively the distributional
vectors of x and y, R is the particular syntactic
and/or semantic relation connecting x and y, and,
K represents the amount of background knowl-
edge that the vector composition process takes
vector dimensions
betwe
en
gap proce
ss
socialtwo
contact < 11, 0, 3, 0, 11 >
x: close < 27, 3, 2, 5, 24 >
y: interaction < 23, 0, 3, 8, 4 >
Table 1: Example of distributional
frequency vectors for the triple t =
( ~contact, ~close, ~interaction)
into account. Two specialisations of the gen-
eral CDS model are proposed: the basic additive
model and the basic multiplicative model.
The basic additive model (BAM) is written as:
(s) = ?~x+ ?~y (3)
where ? and ? are two scalar parameters. The
simplistic parametrisation is ? = ? = 1. For
example, given the vectors ~x and ~y of Table 1,
BAM (s) =< 50, 3, 5, 13, 28 >.
The basic multiplicative model (BMM) is writ-
ten as:
si = xiyi (4)
where si, xi, and yi are the i-th dimensions of
the vectors (s), ~x, and ~y, respectively. For
the example of Table 1, BMM (s) =< 621, 0,
6, 40, 96 >.
Erk and Pado? (2008) look at the problem in a
different way. Let the general distributional mean-
ing of the word w be ~w. Their model computes a
different vector ~ws that represents the specific dis-
tributional meaning of w with respect to s, i.e.:
~ws = (w, s) (5)
In general, this operator gives different vectors for
each word wi in the sequence s, i.e. (wi, s) 6=
(wj , s) if i 6= j. It also gives different vectors
for a word wi appearing in different sequences sk
and sl, i.e. (wi, sk) 6= (wi, sl) if k 6= l.
The model of Erk and Pado? (2008) was de-
signed to disambiguate the distributional mean-
ing of a word w in the context of the sequence
s. However, substituting the word w with the se-
mantic head h of s, allows to compute the distri-
butional meaning of sequence s as shaped by the
1264
word that is governing the sequence (c.f. Pollard
and Sag (1994)). For example, the distributional
meaning of the word sequence eats mice is gov-
erned by the verb eats. Following this model, the
distributional vector (s) can be written as:
(s) ? (h, s) (6)
The function (h, s) explicitly uses the re-
lation R and the knowledge K of the general
equation 2, being based on the notion of selec-
tional preferences. We exploit the model for se-
quences of two words s=xy where the two words
are related with an oriented syntactic relation r
(e.g. r=adj modifier). For making the syntac-
tic relation explicit, we indicate the sequence as:
s = x r?? y.
Given a word w, the model has to keep track
of its selectional preferences. Consequently, each
word w is represented with a triple:
(~w,Rw, R?1w ) (7)
where ~w is the distributional vector of the word w,
Rw is the set of the vectors representing the direct
selectional preferences of the word w, and R?1w is
the set of the vectors representing the indirect se-
lectional preferences of the word w. Given a set of
syntactic relationsR, the set Rw and R?1w contain
respectively a selectional preference vectorRw(r)
and Rw(r)?1 for each r ? R. Selectional prefer-
ences are computed as in Erk (2007). If x is the
semantic head of sequence s, then the model can
be written as:
(s) = (x, x r?? y) = ~xRy(r) (8)
Otherwise, if y is the semantic head:
(s) = (y, x r?? y) = ~y R?1x (r) (9)
 is in both cases realised using BAM or BMM.
We will call these models: basic additive model
with selectional preferences (BAM-SP) and basic
multiplicative model with selectional preferences
(BMM-SP).
Both Mitchell and Lapata (2008) and Erk and
Pado? (2008) experimented with few empirically
estimated parameters. Thus, the general additive
CDS model has not been adequately explored.
3 Estimating Additive Compositional
Semantics Models from Data
The generic additive model sums the vectors ~x
and ~y in a new vector ~z:
(s) = ~z = A~x+B~y (10)
where A and B are two square matrices captur-
ing the relation R and the background knowledge
K of equation 2. Writing matrices A and B by
hand is impossible because of their large size. Es-
timating these matrices is neither a simple classi-
fication learning problem nor a simple regression
problem. It is a regression problem with multiple
dependent variables. In this section, we propose
our model to solve this regression problem using
a set of training examples E.
The set of training examples E contains triples
of vectors (~z, ~x, ~y). ~x and ~y are the two distribu-
tional vectors of the words x and y. ~z is the ex-
pected distributional vector of the composition of
~x and ~y. Note that for an ideal perfectly perform-
ing CDS model we can write ~z = (xy). How-
ever, in general the expected vector ~z is not guar-
anteed to be equal to the composed one (xy).
Figure 1 reports an example of these triples, i.e.,
t = ( ~contact, ~close, ~interaction), with the re-
lated distributional vectors. The construction of
E is discussed in section 4.
In the rest of the section, we describe how the
regression problem with multiple dependent vari-
ables can be solved with a linear equation system
and we give a possible solution of this equation
system. In the experimental section, we refer to
our model as the estimated additive model (EAM).
3.1 Setting the linear equation system
The matrices A and B of equation 10 can be
joined in a single matrix:
~z =
(
A B
)(~x
~y
)
(11)
For the triple t of table 1, equation 11 is:
~contact =
(
A B
)
(
~close
~interaction
)
(12)
1265
and it can be rewritten as:
?
?????
11
0
3
0
11
?
?????
=
(
A5?5 B5?5
)
?
??????????
27
3
2
5
24
23
0
3
8
4
?
??????????
(13)
Focusing on matrix (AB), we can transpose the
matrices as follows:
~zT =
((
A B
)(~x
~y
))T
=
(
~xT ~yT
)(AT
BT
)
(14)
Matrix (~xT ~yT ) is known and matrix
(
AT
BT
)
is
to be estimated.
Equation 14 is the prototype of our final equa-
tion system. The larger the matrix (AB) to be
estimated, the more equations like 14 are needed.
Given set E that contains n triples (~z, ~x, ~y), we
can write the following system of equations:
?
????
~zT1
~zT2...
~zTn
?
???? =
?
????
(
~xT1 ~yT1
)
(
~xT2 ~yT2
)
...(
~xTn ~yTn
)
?
????
(
AT
BT
)
(15)
The vectors derived from the triples can be seen as
two matrices of n rows, Z and (XY ) related to ~zTi
and (~xTi ~yTi
), respectively. The overall equation
system is then the following:
Z =
(
X Y
)(AT
BT
)
(16)
This equation system represents the constraints
that matrices A and B have to satisfy in order to
be a possible linear CDS model that can at least
describe seen examples. We will hereafter call
? =
(
A B
) and Q = (X Y ). The system
in equation 16 can be simplified as:
Z = Q?T (17)
As Q is a rectangular and singular matrix, it is
not invertible and the system in equation 16 has
no solutions. It is possible to use the principle
of Least Square Estimation for computing an ap-
proximation solution. The idea is to compute the
solution ?? that minimises the residual norm, i.e.:
??T = arg min
?T
?Q?T ? Z?2 (18)
One solution for this problem is the Moore-
Penrose pseudoinverse Q+ (Penrose, 1955) that
gives the following final equation:
??T = Q+Z (19)
In the next section, we discuss how the Moore-
Penrose pseudoinverse is obtained using singular
value decomposition (SVD).
3.2 Computing the pseudo-inverse matrix
The pseudo-inverse matrix can provide an approx-
imated solution even if the equation system has no
solutions. We here compute the Moore-Penrose
pseudoinverse using singular value decomposi-
tion (SVD) that is widely used in computational
linguistics and information retrieval for reducing
spaces (Deerwester et al, 1990).
Moore-Penrose pseudoinverse (Penrose, 1955)
is computed in the following way. Let the original
matrix Q have n rows and m columns and be of
rank r. The SVD decomposition of the original
matrix Q is Q = U?V T where ? is a square di-
agonal matrix of dimension r. Then, the pseudo-
inverse matrix that minimises the equation 18 is:
Q+ = V ?+UT (20)
where the diagonal matrix ?+ is the r ? r trans-
posed matrix of ? having as diagonal elements the
reciprocals of the singular values 1?1 , 1?2 , ..., 1?r of
?.
Using SVD to compute the pseudo-inverse ma-
trix allows for different approximations (Fallucchi
and Zanzotto, 2009). The algorithm for comput-
ing the singular value decomposition is iterative
(Golub and Kahan, 1965). Firstly derived dimen-
sions have higher singular value. Then, dimension
k is more informative than dimension k? > k. We
can consider different values for k to obtain differ-
ent SVD for the approximations Q+k of the origi-nal matrix Q+ in equation 20), i.e.:
Q+k = Vn?k?+k?kUTk?m (21)
1266
where Q+k is a matrix n by m obtained consider-ing the first k singular values.
4 Building positive and negative
examples
As explained in the previous section, estimating
CDS models, needs a set of triples E, similar to
triple t of table 1. This set E should contain pos-
itive examples in the form of triples (~zi, ~xi, ~yi).
Examples are positive in the sense that ~zi =
(xy) for an ideal CDS. There are no available
sets to contain such triples, with the exception of
the set used in Mitchell and Lapata (2008) which
is designed only for testing purposes. It contains
similar and dissimilar pairs of sequences (s1,s2)
where each sequence is a verb-noun pair (vi,ni).
From the positive part of this set, we can only de-
rive quadruples where (v1n1) ? (v2n2) but
we cannot derive the ideal resulting vector of the
composition (vini). Sets used to test multi-
word expression (MWE) detection models (e.g.,
(Schone and Jurafsky, 2001; Nicholson and Bald-
win, 2008; Kim and Baldwin, 2008; Cook et
al., 2008; Villavicencio, 2003; Korkontzelos and
Manandhar, 2009)) are again not useful as con-
taining only valid MWE that cannot be used to
determine the set of training triples needed here.
As a result, we need a novel idea to build sets
of triples to train CDS models. We can leverage
on knowledge stored in dictionaries. In the rest of
the section, we describe how we build the positive
example set E and a control negative example set
NE. Elements of the two sets are pairs (t,s) where
t is a target word s is a sequence of words. t is the
word that represent the distributional meaning of
s in the case ofE. Contrarily, t is totally unrelated
to the distributional meaning of s inNE. The sets
E and NE can be used both for training and for
testing. In the testing phase, we can use these sets
to determine whether a CDS model is good or not
and to compare different CDS models.
4.1 Building Positive Examples using
Dictionaries
Dictionaries as natural repositories of equivalent
expressions can be used to extract positive exam-
ples for training and testing CDS models. The
basic idea is the following: dictionary entries are
declarations of equivalence. Words or, occasion-
ally, multi-word expressions t are declared to be
semantically similar to their definition sequences
s. This happens at least for some sense of the
defined words. We can then observe that t ? s.
For example, we report some sample definitions
of contact and high life:
target word (t) definition sequence (s)
contact close interaction
high life excessive spending
In the first case, a word, i.e. contact, is semanti-
cally similar to a two-word expression, i.e. close
interaction. In the second case, two two-word ex-
pressions are semantically similar.
Then, the pairs (t, s) can be used to model
positive cases of compositional distributional se-
mantics as we know that the word sequence s
is compositional and it describes the meaning of
the word t. The distributional meaning ~t of t is
the expected distributional meaning of s. Conse-
quently, the vector ~t is what the CDS model (s)
should compositionally obtain from the vectors of
the components ~s1 . . . ~sm of s. This way of ex-
tracting similar expressions has some interesting
properties:
First property Defined words t are generally
single words. Thus, we can extract stable and
meaningful distributional vectors for these words
and then compare them to the distributional vec-
tors composed by CDS model. This is an impor-
tant property as we cannot compare directly the
distributional vector ~s of a word sequence s and
the vector (s) obtained by composing its com-
ponents. As the word sequence s grows in length,
the reliability of the vector ~s decreases since the
sequence s becomes rarer.
Second property Definitions s have a large va-
riety of different syntactic structures ranging from
simple structures as Adjective-Noun to more com-
plex ones. This gives the possibility to train and
test CDS models that take into account syntax.
Table 2 represents the distribution of the more
frequent syntactic structures in the definitions of
WordNet1 (Miller, 1995).
1Definitions were extracted from WordNet 3.0 and were
parsed with the Charniak parser (Charniak, 2000)
1267
Freq. Structure
2635 (FRAG (PP (IN) (NP (DT) (JJ) (NN))))
833 (NP (DT) (JJ) (NN))
811 (NP (NNS))
645 (NP (NNP))
623 (S (VP (VB) (ADVP (RB))))
610 (NP (JJ) (NN))
595 (NP (NP (DT) (NN)) (PP (IN) (NP (NN))))
478 (NP (NP (DT) (NN)) (PP (IN) (NP (NNP))))
451 (FRAG (PP (IN) (NP (NN))))
419 (FRAG (RB) (ADJP (JJ)))
375 (S (VP (VB) (PP (IN) (NP (DT) (NN)))))
363 (S (VP (VB) (PP (IN) (NP (NN)))))
342 (NP (NP (DT) (NN)) (PP (IN) (NP (DT) (NN))))
341 (NP (DT) (JJ) (JJ) (NN))
330 (ADJP (RB) (JJ))
307 (NP (JJ) (NNS))
244 (NP (DT) (NN) (NN))
241 (S (NP (NN)) (NP (NP (NNS)) (PP (IN) (NP (DT) (NNP)))))
239 (NP (NP (DT) (JJ) (NN)) (PP (IN) (NP (DT) (NN))))
Table 2: Top 20 syntactic structures of WordNet
definitions
4.2 Extracting Negative Examples from
Word Etymology
In order to devise complete training and testing
sets for CDS models, we need to find a sensible
way to extract negative examples. An option is to
randomly generate totally unrelated triples for the
negative examples set, NE. In this case, due to
data sparseness NE would mostly contain triples
(~z, ~x, ~y) where it is expected that ~z 6= (xy). Yet,
these can be too generic and too loosely related to
be interesting cases.
Instead we attempt to extract sets of negative
pairs (t,s) comparable with the one used for build-
ing the training set E. The target word t should
be a single word and s should be a sequence of
words. The latter should be a sequence of words
related by construction to t but the meaning of t
and s should be unrelated.
The idea is the following: many words are et-
ymologically derived from very old or ancient
words. These words represent a collocation which
is in general not related to the meaning of the
target word. For example, the word philosophy
derives from two Greek words philos (beloved)
and sophia (wisdom). However, the use of the
word philosophy in not related to the collocation
beloved wisdom. This word has lost its origi-
nal compositional meaning. The following table
shows some more etymologically complex words
along with the compositionally unrelated colloca-
tions:
target word compositionally unrelated seq.
municipal receive duty
octopus eight foot
As the examples suggest, we are able to build a
set NE with features similar to the features of
N . In particular, each target word is paired with
a related word sequence derived from its etymol-
ogy. These etymologically complex words are un-
related to the corresponding compositional collo-
cations. To derive a set NE with the above char-
acteristics we can use dictionaries containing ety-
mological information as Wiktionary2.
5 Experimental evaluation
In the previous sections, we presented the esti-
mated additive model (EAM): our approach to es-
timate the parameters of a generic additive model
for CDS. In this section, we experiment with this
model to determine whether it performs better
than existing models: the basic additive model
(BAM), the basic multiplicative model (BMM),
the basic additive model with selectional pref-
erences (BAM-SP), and the basic multiplicative
model with selectional preferences (BMM-SP)
(c.f. Sec. 2). In succession, we explore whether
our estimated additive model (EAM) is better than
any possible BAM obtained with parameter ad-
justment. In the rest of the section, we firstly give
the experimental setup and then we discuss the ex-
periments and the results.
5.1 Experimental setup
Our experiments aim to compare compositional
distributional semantic (CDS) models  with re-
spect to their ability of detecting statistically sig-
nificant difference between sets E and NE. In
particular, the average similarity sim(~z,(xy))
for (~z, ~x, ~y) ? E should be significantly different
from sim(~z,(xy)) for (~z, ~x, ~y) ? NE. In this
section, we describe the chosen similarity mea-
sure sim, statistical significance testing and con-
struction details for the training and testing set.
Cosine similarity was used to compare the con-
text vector ~z representing the target word z with
the composed vector (xy) representing the con-
text vector of sequence x y. Cosine similarity be-
2http://www.wiktionary.org
1268
tween two vectors ~x and ~y of the same dimension
is defined as:
sim(~x, ~y) = ~x ? ~y?~x? ?~y? (22)
where ? is the dot product and ?~a? is the magni-
tude of vector ~a computed the Euclidean norm.
To evaluate whether a CDS model distinguishes
positive examples E from negative examples
NE, we test if the distribution of similarities
sim(~z,(xy)) for (~z, ~x, ~y) ? E is statistically
different from the distribution of the same simi-
larities for (~z, ~x, ~y) ? NE. For this purpose, we
used Student?s t-test for two independent samples
of different sizes. t-test assumes that the two dis-
tributions are Gaussian and determines the prob-
ability that they are similar, i.e., derive from the
same underlying distribution. Low probabilities
indicate that the distributions are highly dissimilar
and that the corresponding CDS model performs
well, as it detects statistically different similarities
for the positive set E and the negative set NE.
Based on the null hypothesis that the means of
the two samples are equal, ?1 = ?2, Student?s t-
test takes into account the sizes N , means M and
variances s2 of the two samples to compute the
following value:
t = (M1 ?M2) ?1
?
2(s21 + s22)
df ?Nh
(23)
where df = N1 + N2 ? 2 stands for the degrees
of freedom and Nh = 2(N?11 + N?12 )?1 is the
harmonic mean of the sample sizes. Given the
statistic t and the degrees of freedom df , we can
compute the corresponding p-value, i.e., the prob-
ability that the two samples derive from the same
distribution. The null hypothesis can be rejected if
the p-value is below the chosen threshold of statis-
tical significance (usually 0.1, 0.05 or 0.01), oth-
erwise it is accepted. In our case, rejecting the
null hypothesis means that the similarity values of
instances of E are significantly different from in-
stances of NE, and that the corresponding CDS
model perform well. p-value can be used as a per-
formance ranking function for CDS models.
We constructed two sets of instances: (a) a
set containing Adjective-Noun or Noun-Noun se-
NN set VN set
BAM 0.05690 0.50753
BMM 0.20262 0.37523
BAM-SP 0.42574 0.01710
BMM-SP <1.00E-10 0.23552
EAM (k=20) 0.00431 0.00453
Table 3: Probability of confusing E and NE with
different CDS models
quences (NN set); and (b) a set containing Verb-
Noun sequences (VN set). Capturing different
syntactic relations, these two sets can support that
our results are independent from the syntactic re-
lation between the words of each sequence. For
each set, we used WordNet for extracting positive
examples E and Wiktionary for extracting nega-
tive examples NE as described in Section 4. We
obtained the following sets: (a) NN consists of
1065 word-sequence pairs from WordNet defini-
tions and 377 pairs extracted from Wiktionary;
and (b) VN consists of 161 word-sequence pairs
from WordNet definitions and 111 pairs extracted
from Wiktionary. We have then divided these two
sets in two parts of 50% each, for training and
testing. Instances of the training part of E have
been used to estimate matricesA andB for model
EAM , while the testing parts have been used for
testing all models. Frequency vectors for all sin-
gle words occurring in the above pairs were con-
structed from the British National Corpus using
sentences as contextual windows and words as
features. The resulting space has 689191 features.
5.2 Results and Analysis
The first set of experiments compares EAM with
other existing CDS models: BAM, BMM, BAM-
SP, and BMM-SP. Results are shown in Table 3.
The table reports the p-value, i.e., the probability
of confusing the positive set E and the negative
set NE for all models. Lower probabilities char-
acterise better models. Probabilities below 0.05
indicate that the model detects a statistically sig-
nificant difference between setsE andNE. EAM
has been computed with k = 20 different dimen-
sions for the pseudo-inverse matrix. The two basic
additive models (BAM and BAM-SP) have been
computed for ? = ? = 1.
1269
NN set V N set
Figure 1: p-values of BAM with different values for parameter ? (where ? = 1 ? ?) and of EAM for
different approximations of the SVD pseudo-inverse matrix (k)
The first observation is that EAM models sig-
nificantly separate positive from negative exam-
ples for both sets. This is not the case for any
of the other models. Only, the selectional prefer-
ences based models in two cases have this prop-
erty, but this cannot be generalised: BAM-SP on
the VN set and BMM-SP on the NN set. In gen-
eral, these models do not offer the possibility of
separating positive from negative examples.
In the second set of experiments, we attempt to
investigate whether simple parameter adjustment
of BAM can perform better than EAM. Results are
shown in figure 1. Plots show the basic additive
model (BAM) with different values for parameter
? (where ? = 1??) and EAM computed for dif-
ferent approximations of the SVD pseudo-inverse
matrix (i.e., with different k). The x-axis of the
plots represents parameter ? and the y-axis repre-
sents the probability of confusing the positive set
E and the negative setNE. The representation fo-
cuses on the performance ofBAM with respect to
different ? values. The performance of EAM for
different k values is represented with horizontal
lines. Probabilities of different models are directly
comparable. Line SS represents the threshold of
statistical significance; the value below which the
detected difference between the E and NE sets
becomes statistically significant.
Experimental results show some interesting
facts: While BAM for ? > 0 perform better than
EAM computed with k = 1 in the NN set, they
do not perform better in the VN set. EAM with
k = 1 has 1 degree of freedom corresponding to
1 parameter, the same as BAM. The parameter of
EAM is tuned on the training set, in contrast to
?, the parameter of BAM. Increasing the number
of considered dimensions, k of EAM, estimated
models outperform BAM for all values of param-
eter ?. Moreover, EAM detect a statistically sig-
nificant difference between theE and theNE sets
for k ? 10 and k = 20 for the NN set and the
VN set set, respectively. Simple parametrisation
of a BAM does not outperform the proposed esti-
mated additive model.
6 Conclusions
In this paper, we presented an innovative method
to estimate linear compositional distributional se-
mantics models. The core of our approach con-
sists on two parts: (1) providing a method to es-
timate the regression problem with multiple de-
pendent variables and (2) providing a training set
derived from dictionary definitions. Experiments
showed that our model is highly competitive with
respect to state-of-the-art models for composi-
tional distributional semantics.
References
Charniak, Eugene. 2000. A maximum-entropy-
inspired parser. In proceedings of the 1st NAACL,
pages 132?139, Seattle, Washington.
Cook, Paul, Afsaneh Fazly, and Suzanne Stevenson.
2008. The VNC-Tokens Dataset. In proceedings
of the LREC Workshop: Towards a Shared Task for
Multiword Expressions (MWE 2008), Marrakech,
Morocco.
1270
Deerwester, Scott C., Susan T. Dumais, Thomas K.
Landauer, George W. Furnas, and Richard A. Harsh-
man. 1990. Indexing by latent semantic analysis.
Journal of the American Society of Information Sci-
ence, 41(6):391?407.
Erk, Katrin and Sebastian Pado?. 2008. A structured
vector space model for word meaning in context. In
proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, pages 897?
906. Association for Computational Linguistics.
Erk, Katrin. 2007. A simple, similarity-based model
for selectional preferences. In proceedings of ACL.
Association for Computer Linguistics.
Fallucchi, Francesca and Fabio Massimo Zanzotto.
2009. SVD feature selection for probabilistic tax-
onomy learning. In proceedings of the Workshop on
Geometrical Models of Natural Language Seman-
tics, pages 66?73. Association for Computational
Linguistics, Athens, Greece.
Firth, John R. 1957. Papers in Linguistics. Oxford
University Press, London.
Golub, Gene and William Kahan. 1965. Calculat-
ing the singular values and pseudo-inverse of a ma-
trix. Journal of the Society for Industrial and Ap-
plied Mathematics, Series B: Numerical Analysis,
2(2):205?224.
Harris, Zellig. 1964. Distributional structure. In Katz,
Jerrold J. and Jerry A. Fodor, editors, The Philos-
ophy of Linguistics, New York. Oxford University
Press.
Jones, Michael N. and Douglas J. K. Mewhort. 2007.
Representing word meaning and order information
in a composite holographic lexicon. Psychological
Review, 114:1?37.
Kim, Su N. and Timothy Baldwin. 2008. Standard-
ised evaluation of english noun compound inter-
pretation. In proceedings of the LREC Workshop:
Towards a Shared Task for Multiword Expressions
(MWE 2008), pages 39?42, Marrakech, Morocco.
Korkontzelos, Ioannis and Suresh Manandhar. 2009.
Detecting compositionality in multi-word expres-
sions. In proceedings of ACL-IJCNLP 2009, Sin-
gapore.
Li, Ping, Curt Burgess, and Kevin Lund. 2000. The
acquisition of word meaning through global lexical
co-occurrences. In proceedings of the 31st Child
Language Research Forum.
Lin, Dekang and Patrick Pantel. 2001. DIRT-
discovery of inference rules from text. In Proceed-
ings of the ACM Conference on Knowledge Discov-
ery and Data Mining (KDD-01). San Francisco, CA.
Manning, Christopher D., Prabhakar Raghavan, and
Hinrich Schu?tze. 2008. Introduction to Information
Retrieval. Cambridge University Press, Cambridge,
UK.
McCarthy, Diana and John Carroll. 2003. Disam-
biguating nouns, verbs, and adjectives using auto-
matically acquired selectional preferences. Compu-
tational Linguistics, 29(4):639?654.
Miller, George A. and Walter G. Charles. 1991. Con-
textual correlates of semantic similarity. Language
and Cognitive Processes, VI:1?28.
Miller, George A. 1995. WordNet: A lexical
database for English. Communications of the ACM,
38(11):39?41.
Mitchell, Jeff and Mirella Lapata. 2008. Vector-based
models of semantic composition. In proceedings
of ACL-08: HLT, pages 236?244, Columbus, Ohio.
Association for Computational Linguistics.
Nicholson, Jeremy and Timothy Baldwin. 2008. Inter-
preting compound nominalisations. In proceedings
of the LREC Workshop: Towards a Shared Task for
Multiword Expressions (MWE 2008), pages 43?45,
Marrakech, Morocco.
Pado, Sebastian and Mirella Lapata. 2007.
Dependency-based construction of semantic space
models. Computational Linguistics, 33(2):161?
199.
Penrose, Roger. 1955. A generalized inverse for ma-
trices. In Proceedings of Cambridge Philosophical
Society.
Pollard, Carl J. and Ivan A. Sag. 1994. Head-driven
Phrase Structured Grammar. Chicago CSLI, Stan-
ford.
Schone, Patrick and Daniel Jurafsky. 2001. Is
knowledge-free induction of multiword unit dictio-
nary headwords a solved problem? In Lee, Lil-
lian and Donna Harman, editors, proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 100?108.
Villavicencio, Aline. 2003. Verb-particle construc-
tions and lexical resources. In proceedings of
the ACL 2003 workshop on Multiword expressions,
pages 57?64, Morristown, NJ, USA. Association for
Computational Linguistics.
1271
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 2270?2279, Dublin, Ireland, August 23-29 2014.
Comparable Study of Event Extraction in Newswire and Biomedical
Domains
Makoto Miwa
?,?
Paul Thompson
?
Ioannis Korkontzelos
?
Sophia Ananiadou
?
?
National Centre for Text Mining and School of Computer Science,
University of Manchester, United Kingdom
?
Graduate School of Engineering, Toyota Technological Institute, Japan
{makoto.miwa, paul.thompson, ioannis.korkontzelos, sophia.ananiadou}@manchester.ac.uk
Abstract
Event extraction is a popular research topic in natural language processing. Several event extrac-
tion tasks have been defined for both the newswire and biomedical domains. In general, different
systems have been developed for the two domains, despite the fact that the tasks in both domains
share a number of characteristics. In this paper, we analyse the commonalities and differences
between the tasks in the two domains. Based on this analysis, we demonstrate how an event
extraction method originally designed for the biomedical domain can be adapted for application
to the newswire domain. The performance is state-of-the-art for both domains, with F-scores of
52.7% for the biomedical domain and 52.1% for the newswire domain in terms of their primary
evaluation metrics.
1 Introduction
Research into event extraction was initially focussed on the general language domain, largely driven by
the Message Understanding Conferences (MUC) series (e.g., Chinchor (1998)) and the Automated Con-
tent Extraction (ACE) evaluations
1
. More recently, the focus of research has been widened to the biomed-
ical domain, motivated by the ongoing series of biomedical natural language processing (BioNLP) shared
tasks (STs) (e.g., Kim et al. (2013)).
Although the textual characteristics and the types of relevant events to be extracted can vary consid-
erably between domains, the same general features of events normally hold across domains. An event
usually consists of a trigger and arguments (see Figures 1 and 2.) A trigger is typically a verb or a nom-
inalised verb that denotes the presence of the event in the text, while the arguments are usually entities.
In general, arguments are assigned semantic roles that characterise their contribution towards the event
description.
Until now, however, there has been little, if any, effort by researchers working on event extraction in
different domains to share ideas and techniques, unlike syntactic tasks (e.g., (Miyao and Tsujii, 2008))
and other information extraction tasks, such as named entity recognition (e.g., (Giuliano et al., 2006))
and relation extraction (e.g., (Qian and Zhou, 2012)). This means that the potential to exploit cross-
domain features of events to develop more adaptable event extraction systems is an under-studied area.
Consequently, although there is a large number of published studies on event extraction, proposing many
different methods, no work has previously been reported that aims to adapt an event extraction method
developed for one domain to a new domain.
In response to the above, we have investigated the feasibility of adapting an event extraction method
developed for the biomedical domain to the newswire domain. To facilitate this, we firstly carry out a
detailed static analysis of the differences that hold between event extraction tasks in the newswire and
biomedical domains. Specifically, we consider the ACE 2005 event extraction task (Walker et al., 2006)
for the newswire domain and the Genia Event Extraction task (GENIA) in BioNLP ST 2013 (Kim et al.,
2013) for the biomedical domain. Based on the results of this analysis, we adapt the biomedical event
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
1
itl.nist.gov/iad/mig/tests/ace
2270
Jim McMahon was body slammed to the ground in the mid 80's about five seconds after he had released a pass.
PER_Individual Conflict_Attack ?? timex2 PER_Individual
timex2
Target Time-Within
Time-At-End
Figure 1: ACE 2005 event example (ID: MARKBACKER 20041220.0919)
p300 immunoprecipitated Foxp3 when both proteins were overexpressed in HEK 293T cells
Pro Binding Pro +Reg
+Reg
Gene expression
Gene expressionTheme Theme2 Cause
CauseTheme
Theme
Theme
Theme
Figure 2: GENIA event example (ID: PMC-1447668-08-Results)
extraction method to the task of extracting events in the newswire domain, according to the specification
of the ACE 2005 event extraction task. The original method consists of a classification pipeline that has
previously been applied to extract events according to task descriptions that are similar to GENIA. In
order to address the differences between this task and the ACE task, we have made a number of changes
to the original method, including modifications to the classification labels assigned, the pipeline itself
and the features used. We retrained the model of the adapted system on the ACE task, compared the
performance, and empirically analysed the differences between the two tasks in terms of entity-related
information. We demonstrate that the resulting system achieves state-of-the-art performance for tasks in
both domains.
2 Related Work
In this section, we introduce the two domain specific event extraction tasks on which we will focus, i.e.,
the ACE 2005 event extraction task, which concerns events in the newswire domain, and the GENIA
event task from the BioNLP ST 2013, which deals with biomedical event extraction. We also examine
state-of-the-art systems that have been developed to address each task.
2.1 Newswire Event Extraction
The extraction of events from news-related texts has been widely researched, largely due to motivation
from the various MUC and ACE shared tasks. Whilst MUC focussed on filling a single event template
on a single topic by gathering information from different parts of a document, ACE defined a more
comprehensive task, involving the recognition of multiple fine-grained and diverse types of entities and
associated intra-sentential events within each document.
A common approach to tackling the MUC template filling task has involved the employment of
pattern-based methods, e.g., Riloff (1996). In contrast, supervised learning approaches have constituted
a more popular means of approaching the ACE tasks
2
. In this paper, we choose to focus on adapting
our biomedical-focussed event extraction method to the ACE 2005 task. Our choice is based on the task
definition for ACE 2005 having more in common with the BioNLP 2013 GENIA ST definition than the
MUC event template task definition.
In terms of the characteristics of state-of-the-art event extraction systems designed according to the
ACE 2005 model, pipeline-based approaches have been popular (Grishman et al., 2005; Ahn, 2006).
Grishman et al. (2005) proposed a method that sequentially identifies textual spans of arguments, role
types, and event triggers. This pipeline approach has been further extended in several subsequent studies.
For example, Liao et al. (2010) investigated document-level cross-event consistency using co-occurrence
of events and event arguments, while Hong et al. (2011) exploited information gathered from the web to
ensure cross-entity consistency.
2
Note that there are also approaches using few or no training data (e.g., (Ji and Grishman, 2008; Lu and Roth, 2012)) for
the ACE 2005 task, but they are not so many and we will focus on the supervised learning approaches in this paper.
2271
Li et al. (2013) recently proposed a joint detection method to detect both triggers and arguments
(together with their role types) using a structured perceptron model. The system outperformed the best
results reported for the ACE 2005 task in the literature, without the use of any external resources.
2.2 Biomedical Event Extraction
The task of event extraction has received a large amount of attention from BioNLP researchers in recent
years. Interest in this task was largely initiated by the BioNLP 2009 ST, and has been sustained through
the organisation of further STs in 2011 and 2013. The STs consist of a number of different sub-tasks, the
majority of which concern the extraction of events from biomedical papers from the PubMed database.
Events generally concern interactions between biomedical entities, such as proteins, cells and chemicals.
Similarly to newswire event extraction systems, pipeline-based methods have constituted a popular
approach to extracting events in the biomedical domain (Bj?orne and Salakoski, 2013; Miwa et al., 2012).
The pipeline developed by Miwa et al. (2012) consists of a number of modules, which sequentially
detect event triggers, event arguments, event structures and hedges (i.e., speculations and negations).
The system has been applied to several event extraction tasks, and has achieved the best performance on
most of these, in comparison to other systems. It should be noted that the ordering of the components
in biomedical event extraction pipelines often differs from pipelines designed for news event extraction,
e.g., Grishman et al. (2005), which was described above.
As in newswire event detection, some joint (non pipeline-based) approaches have also been proposed
for biomedical event extraction. For example, McClosky et al. (2012) used a stacking model to combine
the results of applying two different methods to event extraction. The first method is a joint method,
similar to Li et al. (2013), that detects triggers, arguments and their roles. However, in contrast to
the structured perceptron employed in Li et al. (2013), McClosky et al. (2012) use a dual-decomposition
approach for the detection. The second method is based on dependency parsing and treats event structures
as dependency trees.
3 Adaptation of Biomedical Event Extraction to Newswire Event Extraction
In this section, we firstly analyse the differences between the domain-specific ACE 2005 and GENIA
event extraction tasks. Based on our findings, we propose an approach to adapting an existing event ex-
traction method, originally developed for biomedical event extraction, to the ACE 2005 task, by resolving
the observed differences between the two task definitions.
3.1 Differences in event extraction tasks
Both the ACE 2005 and GENIA tasks concern the task of event extraction, i.e., the identification of
relationships between entities. For both tasks, the requirement is to extract events from text that conform
to the general event description introduced earlier, i.e., a trigger and its arguments, each of which is
assigned a semantic role. Despite this high-level similarity between the tasks, their finer-grained details
diverge in a number of ways. Apart from the different textual domain, the tasks adopt varying annotation
schemes. The exact kinds of annotations provided at training time are also different, as are the evaluation
settings.
Several variants of the official task setting for the ACE 2005 corpus have been defined. This is partly
due to the demanding nature of the official task definition, which requires the detection of events from
scratch, including the recognition of named entities participating in events, together with the resolution
of coreferences. Alternative task settings (such as Ji and Grishman (2008); Liao and Grishman (2010)))
generally simplify the official task definition, e.g., by omitting the requirement to perform coreference
resolution. A further issue is that the test data sets for the official task setting have not been made publicly
available. As a result of the multiple existing variations of the ACE 2005 task definition that have been
employed by different research efforts, direct comparison of our results with those obtained by other
state-of-the art systems is problematic. The solution we have chosen is to adopt the same ACE 2005
event extraction task specification that has been adopted in recent research, by Hong et al. (2011) and Li
et al. (2013). For GENIA, we follow the specification of the original GENIA event extraction task.
2272
ACE 2005 GENIA
# of entity types 13 (type) / 53 (subtype) 2
Argument Entity/Nominal/Value/Time Entity
# of event types 8 (type) / 33 (subtype) 13
# of argument role types 35 7
Max # of arguments for an event 11 4
Nested events None Possible
Overlaps of events None Possible
Correspondences of arguments None Possible
Entity Available (Given) Available (Partially given)
Entity attributes Available (Given) Not available
Event attributes Available (Not given) Available (Not given)
Entity coreference Available (Given) Available (Not given)
Event coreference Available (Not given) Not available
Evaluation Trigger/Role Event
Table 1: Comparison of event definitions and event extraction tasks. ?Available annotations? are annota-
tions available in the corresponding corpus, while ?Given annotations? are annotations provided during
(training and) prediction. ?Given annotations? do not need to be predicted during event extraction.
Event annotation examples for ACE 2005 and GENIA are shown in Figures 1 and 2, respectively.
Table 1 summarises the following comparison between the two event extraction tasks.
Semantic types There are more event, role and entity types and a greater potential number of arguments
in ACE 2005 events than in GENIA events. There is also a hierarchy of event types and entity types
in ACE 2005. For example, the Life event type has Be-Born, Marry, Divorce, Injure, Die event
subtypes. Some GENIA event types can also be arranged to have a hierarchy but they are limited.
Events in ACE 2005 can take non-entity arguments, e.g., Time.
Nested events/Overlapping events Event structures are flat in ACE 2005, but they can be nested in
GENIA, i.e., an event can take other events as its arguments. Events in GENIA can also be over-
lapping, in the sense that a particular word or phrase can be a trigger for multiple events. Figure 2
illustrates both nesting and overlapping in GENIA events. These properties of GENIA events are
not addressed by methods developed for event extraction according to the ACE 2005 specification,
making direct application of these methods to the GENIA task impossible.
Links amongst arguments A specific feature of the GENIA event extraction task, which is completely
absent from the ACE 2005 task, is that links amongst arguments sometimes have to be identified.
For example, the Binding event type in the GENIA task can take the following argument role types:
Theme, Theme2, Site and Site2. The number 2 is attached to differentiate specific linkages between
arguments: Site is the location of Theme, while Site2 is the location of Theme2.
Entities, events and their attributes Entities in ACE 2005 have rich attributes associated with them.
For example, the Time entity type has an attribute to store a normalised temporal format (e.g., 2003-
03-04 for entities ?20030304?, ?March 4? and ?Tuesday?) while the GPE (Geo-Political Entity)
type has attributes such as subtypes (e.g., Nation), mention type (proper name, common noun or
pronoun), roles (location of a group or person) and style (literal or metonymic). In contrast, GENIA
entities have no attributes
3
. In ACE 2005, all entities are provided (gold) in the training and test
data and they do not need to be predicted. In GENIA, some named entities (i.e., Proteins) are also
provided, but other types of named or non-named entities that can constitute event arguments, such
as locations and sites of proteins, are not provided in the test data and thus need to be predicted
as part of the extraction process. Events in both corpora also have associated attributes: modality,
3
Types are not counted as attributes in this paper.
2273
polarity, genericity and tense in ACE 2005 and negation and speculation in GENIA. The GENIA
task definition requires event attributes to be predicted, but the ACE 2005 task definition does not.
Coreference Both entity and event coreference are annotated in ACE 2005, but only entity coreference is
annotated in GENIA. Events in ACE 2005 can take non-entity mentions, such as pronouns, as their
arguments. However, events in GENIA can take only entity mentions as arguments. Thus, instead
of non-entity mentions, coreferent entity mentions that are the closest to triggers are annotated as
arguments in GENIA. For example, in Figure 2, ?p300? and ?Foxp3? are annotated as Themes of
Gene expression events instead of ?both proteins?.
Evaluation In ACE 2005, the accuracy of extracted events is evaluated at the level of individual ar-
guments and their roles. Completeness of events is not taken into consideration (Li et al., 2013),
presumably because each event can take many arguments. Evaluation is performed by taking into
account the 33 event subtypes, rather than the 8 coarser-grained event types. In contrast, evaluation
of events according to the GENIA specification considers only the correctness of complete events,
after nested events have been broken down.
In summary, the ACE 2005 task is in some respects more complex than the GENIA task, because it
concerns a greater number event types, whose arguments may constitute a greater range of entity types,
and whose semantic roles are drawn from a larger set, some of which are specific to particular event
types and entities. In other respects, the task is more straightforward than the GENIA task, because of
the simpler nature of the event structures in ACE 2005, i.e., there are no nested or overlapping event
structures.
3.2 Adaptation of event extraction method
Since event structures are simpler in ACE 2005 than GENIA, we choose to adapt a biomedical event
extraction method to the ACE 2005 task rather than the other way around. The inverse adaptation,
starting from a newswire event extraction method, is considered more complex, since we would need to
extend the method to capture the more complex event structures required in the GENIA task. It would
additionally be inappropriate to employ domain adaptation methods (Daum?e III and Marcu, 2006; Pan
and Yang, 2010) to allow GENIA-trained models to be applied to the ACE 2005 tasks. This is because
such methods require that there is at least a certain degree of overlap between the target information
types, which is not the case in this scenario.
We employ the biomedical event extraction pipeline method described in Miwa et al. (2012) as our
starting point. Our motivation is that, due to their modular nature, pipeline approaches are often easier
to adapt to other task settings than joint approaches, e.g., (McClosky et al., 2012; Li et al., 2013).
In addition, the method has previously been shown to achieve state-of-the-art performance in several
biomedical event extraction tasks (Miwa et al., 2012).
The pipeline consists of four detectors, i.e., trigger/entity, event role, event structure, and hedge de-
tectors. The trigger/entity detector finds triggers and entities in text. The event role detector determines
which triggers/entities constitute arguments of events, links them to the appropriate event trigger and as-
signs semantic roles to the arguments. The event structure detector merges trigger-argument pairs into all
possible complete event structures, and determines which of these structures constitute actual events. The
same detector determines links between arguments, such as Theme2 and Site2. The hedge detector finds
negation and speculation information associated with events. Each detector solves multi-label multi-
class classification problems using lexical and syntactic features obtained from multiple parsers. These
features include character n-grams, word n-grams, and shortest paths between triggers and participants
within parse structures. More detailed information can be found in Miwa et al. (2012).
We have updated the original method by simplifying the format of the classification labels used by
both the event role detector and event structure detector modules. We refer to this method as BioEE,
which we have applied to the GENIA task. We use only the role types (e.g., Theme) as classification
labels for instances in the event role detector, instead of the more complex labels used in the original
version of the module, which combined event types, roles and semantic entity types of arguments (e.g.,
2274
Binding:Theme-Protein). Similarly, in the event structure detector, we use only two labels (?EVENT?
or ?NOT-EVENT?), instead of the previously used composite labels, which consisted of the event type,
together with the roles and semantic entity types of all arguments of the event (e.g., Regulation:Cause-
Protein:Theme-Protein.) We employed the simplified labels, since they increase the number of training
instances for each label. The use of such labels, compared to the more complex ones, could reduce the
potential of carrying out detailed modelling of specific aspects of the task. However, this was found not
to be an issue, since the use of the simplified labels improved the performance of the pipeline in detecting
events within the GENIA development data set (about 1% improvement in F-score). The simplification of
the set of classification labels was also vital to ensure the tractability of the classification problems within
the context of the ACE 2005 task. For example, using the same conventions to formulate classification
labels as in the original system would result in 345 possible labels (compared to 91 in GENIA) to be
predicted by the event role detector (and an even greater number of labels for the event structure detector),
based on event-role-semantic type combinations found in the ACE training/development sets.
In order to adapt the system to extract events according to the ACE 2005 specification, we modified
BioEE in several ways, making changes to both the pipeline itself and the features employed by the
different modules. We refer to this method as Adapted BioEE, and we applied this method to the ACE
2005 task. These changes were made in an attempt to address the two major differences between the
GENIA and ACE 2005 tasks, i.e., the simpler event structures and the availability of entity attribute and
coreference information in ACE.
The pipeline-based modifications consisted of removing certain modules from the original pipeline,
such that only two modules remained, i.e., the trigger/entity and event role detectors. The other two
modules of the original pipeline, i.e., the event structure and hedge detectors, were designed to deal with
problems that do not exist in the ACE 2005 extraction task, and thus their usage would be redundant.
Instead of using the event structure detector to piece the different elements of an event, we simply aggre-
gate all the arguments of the same trigger into a single event structure, after the event role detector has
been applied.
As mentioned above, the ACE 2005 task definition includes rich information about entities, including
attributes and coreference information. Existing systems developed to address this task have exploited
this information to generate rich feature sets for classification (Liao and Grishman, 2010; Li et al.,
2013). Based on the demonstrated utility of this information within the context of event extraction, we
also choose to use it, by adding binary feature that indicate the presence of base forms, entity subtypes,
and attributes of the entities and their coreferent entities to features in both detectors above. We choose
to use base forms, since surface forms of entities are not used by most biomedical event extraction
systems, including BioEE. We also add the features for Brown clusters (Brown et al., 1992) following Li
et al. (2013). Further details can be found in Li et al. (2013).
4 Evaluation
4.1 Evaluation settings
To assess the performance of Adapted BioEE on the ACE 2005 task, we followed the evaluation process
and settings used in previously reported studies (Hong et al., 2011; Li et al., 2013). ACE 2005 consists
of 599 documents. In order to facilitate direct comparison with other systems trained on the same data,
we conducted a blind test on the same 40 newswire documents that were used for evaluation in (Ji and
Grishman, 2008; Li et al., 2013), and used the remaining documents as training/development sets. We
use precision (P), recall (R) and F-score (F) to report the performance of the adapted system in classifying
triggers and argument roles. We use the latter F-score as our primary metric for comparing our system
with other systems, since this score better reflects the performance of the extraction of event structures.
GENIA consists of 34 full paper articles (Kim et al., 2013). To evaluate the performance of BioEE
on the GENIA task, we followed the task setting in BioNLP ST 2013 and used the official evaluation
systems provided by the organisers. We also used the same partitioning of data that was employed in
the official BioNLP ST 2013 evaluation, with 20 articles being used as the training/development set, and
the remaining 14 articles being held back as the test set. For brevity, we show the only the primary P,
2275
Arg. Role Decomposition Event Detection
P R F P R F (%)
BioEE 71.76 47.44 57.12 64.36 44.62 52.71
BioEE (+Entity) 69.47 46.94 56.02 61.81 44.11 51.48
EVEX 64.30 48.51 55.30 58.03 45.44 50.97
TEES-2.1 62.69 49.40 55.26 56.32 46.17 50.74
Table 2: Overall performance of BioEE on the GENIA data set
Trigger Classification Arg. Role Classification Event Detection
P R F P R F P R F (%)
Adapted BioEE 59.9 72.6 65.7 54.2 50.2 52.1 20.7 21.7 21.2
Adapted BioEE (-Entity) 57.9 71.5 64.0 51.0 48.1 49.5 19.7 19.3 19.5
Li et al. (2013) 73.7 62.3 67.5 64.7 44.4 52.7 - - -
Hong et al. (2011) 72.9 64.3 68.3 51.6 45.5 48.4 - - -
Table 3: Overall performance of Adapted BioEE on the ACE 2005 data set
R and F scores in the shared task, i.e., the EVENT TOTAL results obtained using the approximate span
& recursive evaluation method, as recommended by the organisers. The method individually evaluates
each complete core event, i.e., event triggers with their Theme and/or Cause role arguments, with relaxed
span matching, after nested events have been broken down as explained in Section 3.1. Note that the
scores do not count the non-named entities, hedges, and links between arguments, since only core events
are considered in the official evaluation.
We applied both a deep parser, Enju (Miyao and Tsujii, 2008) and a dependency parser, ksdep (Sagae
and Tsujii, 2007) to generate features for the ACE 2005 task, and their bio-adapted versions for the
GENIA task. We also employed the GENIA sentence splitter (S?tre et al., 2007) for sentence splitting,
and the snowball (Porter2) stemmer
4
for stemming. We did not make use of any other external resources,
such as dictionaries, since this would hinder direct comparison of the two versions of the system.
4.2 Evaluation on GENIA
The ?Event Detection? column in Table 2 shows evaluation results of BioEE on GENIA. The effects
on performance by including entity-related features, i.e., entity base forms and Brown clustering, as
introduced in Section 3.2, are shown as ?BioEE (+Entity)?. The inclusion of these features slightly
degrades the performance.
For completeness, we also show in Table 2 the best and second best performing systems that took
part in the official BioNLP 2013 ST evaluation: EVEX (Hakala et al., 2013) and TEES-2.1 (Bj?orne and
Salakoski, 2013). TEES-2.1 consists of a modular pipeline similar to BioEE, but it uses a different set
of features. EVEX enhances the output of TEES-2.1, by using information obtained from the results of
large-scale event extraction. The comparison shows that BioEE achieves state-of-the-art event extraction
performance on the GENIA task.
4.3 Evaluation on ACE 2005
The ?Trigger Classification? and ?Arg. Role Classification? columns of Table 3 summarise the evaluation
results of the Adapted BioEE system (as described in Section 3.2) on the ACE 2005 task.
We analysed the effects of incorporating features based on entity-related information into the extrac-
tion process, by repeating the experiments with such features omitted (-Entity). As can be observed in
Table 3, the removal of entity-related features led to 3% performance decrease in F-score.
For completeness, Table 3 also illustrates the results of state-of-the-art systems that were specifi-
cally developed for ACE 2005: the system based on a joint approach (Li et al., 2013) and the pipeline-
based system enhanced with web-gathered information (Hong et al., 2011). The difference between the
4
snowball.tartarus.org
2276
Adapted BioEE and the best system is small and insignificant and the Adapted BioEE achieved perfor-
mance that is comparable to or better than these other systems, in terms of the F-scores in argument role
classification.
5 Discussion
To further investigate the differences in performance of the BioEE and Adapted BioEE systems on the
two tasks, we evaluate the scores achieved for each task using the evaluation criteria originally designed
for the other task. Specifically, we apply the ACE 2005 argument role classification criteria to the out-
put of GENIA task, and we apply the complete event-based evaluation, originally used to evaluate the
GENIA task, to the events extracted for the ACE 2005 task. The ?Arg. Role Decomposition? column of
Table 2 depicts the former evaluation, while the ?Event Detection? column of Table 3 shows the latter.
Table 2 also shows the performance of the other biomedical event extraction systems introduced above
in carrying out argument role classification, since such information was provided as ?Decomposition?
within the results of the original task evaluation
5
. Although the results shown for ?Arg. Role Decompo-
sition? in Table 2 are not directly comparable to those shown for ?Arg. Role Classification? in Table 3
(given the different characteristics of GENIA and ACE 2005 tasks), the scores are broadly comparable.
This demonstrates that the task of argument role classifications is equally challenging for both tasks.
The ?Event Detection? column of Table 3 illustrates event-based evaluation scores on ACE 2005.
The event structure detector was added to the pipeline to facilitate comparison of the results of the two
different tasks in a similar setting, and performance was evaluated according to the GENIA evaluation
criteria. Evaluation scores on ACE 2005 are unexpectedly low compared to those in Table 2. Considering
that the performance of argument role classification is similar in both tasks, this low performance is likely
to be due to the large number of potential event arguments in ACE 2005. This means that, in comparison
to GENIA events, which have a small number of possible argument types, there is a greater chance that
some arguments of more complex ACE 2005 events will fail to be detected. According to the GENIA
evaluation criteria, even if the majority of arguments has been correctly identified, the complete event
structure will still be evaluated as incorrect. This helps to explain why such evaluation criteria may have
been deemed inappropriate in the original ACE 2005 evaluations.
Subsequently, we analysed the effects of utilising entity-related features. We show the results obtained
by adding entity information (+Entity) in Table 2 and the results obtained by removing entity information
(-Entity) in Table 3. The positive or negative effect on performance of adding or removing these features
is consistent across all subtask evaluations shown in the two tables, although the exact level of perfor-
mance improvement or degradation depends on the subtask under evaluation. Overall, the inclusion of
the features degraded the performance of BioEE on the GENIA task, but improved the performance of
Adapted BioEE on the ACE 2005 task. These differences may be due to the increased richness of en-
tity information in the ACE 2005 corpus, suggesting that enriching entities in the GENIA corpus with
attribute information could be a possible way to further improve the performance of the system on this
task.
6 Conclusions and Future Work
In this paper, we have described our adaptation of a biomedical event extraction method to the newswire
domain. We firstly evaluated the method on a biomedical event extraction task (GENIA), and showed
that its performance was superior to other state-of-the-art systems designed for the task. We then adapted
the method to a newswire event extraction task (ACE 2005), by addressing the major differences between
the tasks. With only a small number of adaptations, the resulting system was also able to achieve state-of-
the-art performance on the newswire extraction task. These results show that there is no need to develop
separate systems for event extraction tasks in different domains, as long as the types of tasks being
addressed exhibit domain-independent features. However, further discussion and evaluation is needed to
better understand how different potential methods for adapting such tools from one domain to another
can be used and/or combined effectively.
5
bionlp-st.dbcls.jp/GE/2013/results
2277
As future work, we intend to further investigate the adaptation of alternative methods proposed for
use in one domain to another domain. Several interesting approaches have been described, such as the
utilisation of contextual information beyond the boundaries of individual sentences in the newswire do-
main (Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011) and joint approaches in the
biomedical domain (McClosky et al., 2012), but their adaptability to other domains has not yet been
investigated. We also intend to investigate the possibility of discovering and utilising shared information
between the two domains (Goldwasser and Roth, 2013). Encouraging greater levels of communication
between researchers working on NLP tasks in different domains will help to stimulate such new direc-
tions of research, both for event extraction and for other related information extraction tasks, such as
relation extraction and coreference resolution.
Acknowledgements
This work was supported by the Arts and Humanities Research Council (AHRC) [grant number
AH/L00982X/1], the Medical Research Council [grant number MR/L01078X/1], the European Commu-
nity?s Seventh Program (FP7/2007-2013) [grant number 318736 (OSSMETER)], and the JSPS Grant-in-
Aid for Young Scientists (B) [grant number 25730129].
References
David Ahn. 2006. The stages of event extraction. In Proceedings of the Workshop on Annotating and Reasoning
about Time and Events, pages 1?8, Sydney, Australia, July. ACL.
Jari Bj?orne and Tapio Salakoski. 2013. Tees 2.1: Automated annotation scheme learning in the bionlp 2013 shared
task. In Proceedings of the BioNLP Shared Task 2013 Workshop, pages 16?25, Sofia, Bulgaria, August. ACL.
Peter F Brown, Peter V Desouza, Robert L Mercer, Vincent J Della Pietra, and Jenifer C Lai. 1992. Class-based
n-gram models of natural language. Computational linguistics, 18(4):467?479.
Nancy A. Chinchor. 1998. Overview of MUC-7/MET-2. In Proceedings of the 7th Message Understanding
Conference (MUC-7/MET-2).
Hal Daum?e III and Daniel Marcu. 2006. Domain adaptation for statistical classifiers. Journal of Artificial Intelli-
gence Research, 26:101?126.
Claudio Giuliano, Alberto Lavelli, and Lorenza Romano. 2006. Simple information extraction (sie): A portable
and effective ie system. In Proceedings of the Workshop on Adaptive Text Extraction and Mining (ATEM 2006),
pages 9?16, Trento, Italy, April. Association for Computational Linguistics.
Dan Goldwasser and Dan Roth. 2013. Leveraging domain-independent information in semantic parsing. In
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short
Papers), pages 462?466, Sofia, Bulgaria, August. Association for Computational Linguistics.
Ralph Grishman, David Westbrook, and Adam Meyers. 2005. NYU?s english ACE 2005 system description. In
Proceedings of ACE 2005 Evaluation Workshop, Washington, US.
Kai Hakala, Sofie Van Landeghem, Tapio Salakoski, Yves Van de Peer, and Filip Ginter. 2013. Evex in st?13:
Application of a large-scale text mining resource to event extraction and network construction. In Proceedings
of the BioNLP Shared Task 2013 Workshop, pages 26?34, Sofia, Bulgaria, August. ACL.
Yu Hong, Jianfeng Zhang, Bin Ma, Jianmin Yao, Guodong Zhou, and Qiaoming Zhu. 2011. Using cross-entity in-
ference to improve event extraction. In Proceedings of the 49th ACL-HLT, pages 1127?1136, Portland, Oregon,
USA, June. ACL.
Heng Ji and Ralph Grishman. 2008. Refining event extraction through cross-document inference. In Proceedings
of ACL-08: HLT, pages 254?262, Columbus, Ohio, June. ACL.
Jin-Dong Kim, Yue Wang, and Yamamoto Yasunori. 2013. The genia event extraction shared task, 2013 edition
- overview. In Proceedings of the BioNLP Shared Task 2013 Workshop, pages 8?15, Sofia, Bulgaria, August.
ACL.
Qi Li, Heng Ji, and Liang Huang. 2013. Joint event extraction via structured prediction with global features. In
Proceedings of the 51st ACL, pages 73?82, Sofia, Bulgaria, August. ACL.
2278
Shasha Liao and Ralph Grishman. 2010. Using document level cross-event inference to improve event extraction.
In Proceedings of the 48th ACL, pages 789?797, Uppsala, Sweden, July. ACL.
Wei Lu and Dan Roth. 2012. Automatic event extraction with structured preference modeling. In Proceedings
of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages
835?844, Jeju Island, Korea, July. Association for Computational Linguistics.
David McClosky, Sebastian Riedel, Mihai Surdeanu, Andrew McCallum, and Christopher Manning. 2012. Com-
bining joint models for biomedical event extraction. BMC Bioinformatics, 13(Suppl 11):S9.
Makoto Miwa, Paul Thompson, and Sophia Ananiadou. 2012. Boosting automatic event extraction from the
literature using domain adaptation and coreference resolution. Bioinformatics, 28(13):1759?1765.
Yusuke Miyao and Jun?ichi Tsujii. 2008. Feature forest models for probabilistic HPSG parsing. Computational
Linguistics, 34(1):35?80, March.
Sinno Jialin Pan and Qiang Yang. 2010. A survey on transfer learning. IEEE Transactions on Knowledge and
Data Engineering, 22(10):1345?1359.
Longhua Qian and Guodong Zhou. 2012. Tree kernel-based protein?protein interaction extraction from biomedi-
cal literature. Journal of biomedical informatics, 45(3):535?543.
Ellen Riloff. 1996. Automatically generating extraction patterns from untagged text. In Proceedings of the
national conference on artificial intelligence, pages 1044?1049.
Rune S?tre, Kazuhiro Yoshida, Akane Yakushiji, YusukeMiyao, Yuichiro Matsubayashi, and Tomoko Ohta. 2007.
AKANE System: Protein-protein interaction pairs in BioCreAtIvE2 Challenge, PPI-IPS subtask. In Proceed-
ings of the Second BioCreative Challenge Evaluation Workshop, pages 209?212, CNIO, Madrid, Spain, April.
Kenji Sagae and Jun?ichi Tsujii. 2007. Dependency parsing and domain adaptation with LR models and parser
ensembles. In Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pages 1044?1050,
Prague, Czech Republic, June. ACL.
Christopher Walker, Stephanie Strassel, Julie Medero, and Kazuaki Maeda. 2006. Ace 2005 multilingual training
corpus. Linguistic Data Consortium.
2279
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1701?1712,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Combining String and Context Similarity
for Bilingual Term Alignment from Comparable Corpora
Georgios Kontonatsios
1,2
Ioannis Korkontzelos
1,2
Jun?ichi Tsujii
3
Sophia Ananiadou
1,2
National Centre for Text Mining, University of Manchester, Manchester, UK
1
School of Computer Science, University of Manchester, Manchester, UK
2
Microsoft Research Asia, Beijing, China
3
{gkontonatsios,ikorkontzelos,sananiadou}@cs.man.ac.uk
jtsujii@microsoft.com
Abstract
Automatically compiling bilingual dictio-
naries of technical terms from comparable
corpora is a challenging problem, yet with
many potential applications. In this paper,
we exploit two independent observations
about term translations: (a) terms are of-
ten formed by corresponding sub-lexical
units across languages and (b) a term and
its translation tend to appear in similar lex-
ical context. Based on the first observa-
tion, we develop a new character n-gram
compositional method, a logistic regres-
sion classifier, for learning a string similar-
ity measure of term translations. Accord-
ing to the second observation, we use an
existing context-based approach. For eval-
uation, we investigate the performance of
compositional and context-based methods
on: (a) similar and unrelated languages,
(b) corpora of different degree of compa-
rability and (c) the translation of frequent
and rare terms. Finally, we combine the
two translation clues, namely string and
contextual similarity, in a linear model and
we show substantial improvements over
the two translation signals.
1 Introduction
Bilingual dictionaries of technical terms are re-
sources useful for various tasks, such as computer-
aided human translation (Dagan and Church,
1994; Fung and McKeown, 1997), Statistical Ma-
chine Translation (Och and Ney, 2003) and Cross-
Language Information Retrieval (Ballesteros and
Croft, 1997). In the last two decades, researchers
have focused on automatically compiling bilingual
term dictionaries either from parallel (Smadja et
al., 1996; Van der Eijk, 1993) or comparable cor-
pora (Rapp, 1999; Fung and Yee, 1998). While
parallel corpora contain the same sentences in two
languages, comparable corpora consist of bilin-
gual pieces of text that share some features, only,
such as topic, domain, or time period. Comparable
corpora can be constructed more easily than paral-
lel corpora. Freely available, up-to-date, on-line
resources (e.g., Wikipedia) can be employed.
In this paper, we exploit two different sources
of information to extract bilingual terminology
from comparable corpora: the compositional and
the contextual clue. The compositional clue is
the hypothesis that the representations of a term
in any pair of languages tend to consist of cor-
responding lexical or sub-lexical units, e.g., pre-
fixes, suffices and morphemes. In order to cap-
ture associations of textual units across languages,
we investigate three different character n-gram ap-
proaches, namely a Random Forest (RF) classifier
(Kontonatsios et al., 2014), Support Vector Ma-
chines with an RBF kernel (SVM-RBF) and a Lo-
gistic Regression (LogReg) classifier. Whilst the
previous approaches take as an input monolingual
features and then try to find cross-lingual map-
pings, our proposed method (LogReg classifier)
considers multilingual features, i.e., tuples of co-
occurring n-grams.
The contextual clue is the hypothesis that mu-
tual translations of a term tend to occur in similar
lexical context. Context-based approaches are un-
supervised methods that compare the context dis-
tributions of a source and a target term. A bilin-
gual seed dictionary is used to map context vec-
tor dimensions of two languages. Li and Gaussier
(2010) suggested that the seed dictionary can be
used to estimate the degree of comparability of a
bilingual corpus. Given a seed dictionary, the cor-
pus comparability is the expectation of finding for
each word of the source corpus, its translation in
the target part of the corpus. The performance of
context-based methods has been shown to depend
on the frequency of terms to be translated and the
1701
corpus comparability. In this work, we use an ex-
isting distributional semantics approach to locate
term translations.
Furthermore, we hypothesise that the compo-
sitional and contextual clue are orthogonal, since
the former considers the internal structure of terms
while the latter exploits the surrounding lexical
context. Based on the above hypothesis, we com-
bine the two translation clues in a linear model.
For experimentation, we construct compara-
ble corpora for four language pairs (English-
Spanish, English-French, English-Greek and
English-Japanese) of the biomedical domain.
We choose this domain because a large propor-
tion of the medical terms tends to composition-
ally translate across languages (Lovis et al., 1997;
Namer and Baud, 2007). Additionally, given the
vast amount of newly introduced terms (neolo-
gisms) in the medical domain (Pustejovsky et al.,
2001), term alignment methods are needed in or-
der to automatically update existing resources.
We investigate the following aspects of term
alignment: (a) the performance of compositional
methods on closely related and on distant lan-
guages, (b) the performance of context vectors and
compositional methods when translating frequent
or rare terms, (c) the degree to which the corpus
comparability affects the performance of context-
based and compositional methods (d) the improve-
ments that we can achieve when we combine the
compositional and context clue.
Our experiments show that the performance of
compositional methods largely depends on the dis-
tance between the two languages. The perfor-
mance of the context-based approach is greatly
affected by corpus-specific parameters (the fre-
quency of occurrence of the terms to be translated
and the degree of corpora comparability). It is also
shown that the combination of compositional and
contextual methods performs better than each of
the clues, separately. Combined systems can be
deployed in application environments with differ-
ent language pairs, comparable corpora and seeds
dictionaries.
The LogReg, dictionary extraction method de-
scribed in this paper is freely available
1
.
1
http://personalpages.manchester.
ac.uk/postgrad/georgios.kontonatsios/
Software/LogReg-TermAlign.tar.gz
2 Related Work
Context-based methods (Fung and Yee, 1998;
Rapp, 1999) adapt the Distributional Hypothesis
(Harris, 1954), i.e., words that occur in similar
lexical context tend to have the same meaning, in
a multilingual environment. They represent the
context of each term t as a context vector, usu-
ally following the bag-of-words model. Each di-
mension of the vector corresponds to a context
word occurring within a predefined window, while
the corresponding value is computed by a corre-
lation metric, e.g., Log-Likelihood Ratio (Morin
et al., 2007; Chiao and Zweigenbaum, 2002) or
Point-wise Mutual Information (Andrade et al.,
2010). A general bilingual dictionary is then used
to translate/project the target context vectors into
the source language. As a result, the source and
target context vectors become directly compara-
ble. In a final step, candidate translations are being
ranked according to a distance metric, e.g., cosine
similarity (Tamura et al., 2012) or Jaccard index
(Zanzotto et al., 2010; Apidianaki et al., 2012).
Whilst context-based methods have become a
common practise for bilingual dictionary extrac-
tion from comparable corpora, nonetheless, their
performance is subject to various factors, one of
which is the quality of the comparable corpus. Li
and Gaussier (2010) introduced the corpus com-
parability metric and showed that it is related to
the performance of context vectors. The higher
the corpus comparability is, the higher the perfor-
mance of context vectors is. Furthermore, context
vector approaches are sensitive to the frequency of
terms. For frequent terms, distributional seman-
tics methods exhibit robust performance since the
corresponding context is more informative. Chiao
and Zweigenbaum (2002) reported an accuracy of
91% for the top 20 candidates when translating
terms that occur 100 times or more. However,
the performance of context vectors drastically de-
creases for lower frequency terms (Kontonatsios et
al., 2014; Morin and Daille, 2010).
Our work is more closely related to a second
class of term alignment methods that exploits the
internal structure of terms between a source and
a target language. Compositional translation al-
gorithms are based on the principal of composi-
tionality (Keenan and Faltz, 1985), which claims
that the translation of the whole is a function of
the translation of its parts. Lexical (Morin and
Daille, 2010; Daille, 2012; Robitaille et al., 2006;
1702
Tanaka, 2002) and sub-lexical (Delpech et al.,
2012) compositional algorithms are knowledge-
rich approaches that proceed in two steps, namely
generation and selection. In the generation step,
an input source term is segmented into basic trans-
lation units: words (lexical compositional meth-
ods) or morphemes (sub-lexical methods). Then
a pre-compiled, seed dictionary of words or mor-
phemes is used to translate the components of the
source term. Finally, a permutation function gen-
erates candidate translations using the list of the
translated segments. In the selection step, candi-
date translations are ranked according to their fre-
quency (Morin and Daille, 2010; Robitaille et al.,
2006) or their context similarity with the source
term (Tanaka, 2002). The performance of the
compositional translation algorithms is bound to
the coverage of the seed dictionary (Daille, 2012).
Delpech et al. (2012) noted that 30% of untrans-
lated terms were due to the low coverage of the
seed dictionary.
Kontonatsios et al. (2014) introduced a Random
Forest (RF) classifier that learns correspondences
of character n-grams between a source and target
language. Unlike lexical and sub-lexical compo-
sitional methods, a RF classifier does not require
a bilingual dictionary of translation units. The
model is able to automatically build correlation
paths between source and target sub-lexical seg-
ments that best discriminate translation from non-
translation pairs. However, being a supervised
method, it still requires a seed bilingual dictio-
nary of technical terms for training. The RF classi-
fier was previously applied on an English-Spanish
comparable corpus and it was shown to signifi-
cantly outperform context-based approaches.
3 Methods
In this section we describe the character n-gram
models, the context vector method and the hybrid
system. The lexicon induction task is formalised
as a two-class classification problem. Given a pair
of terms in a source and a target language, the out-
put is a prediction of whether the terms are mutual
translations are not. Furthermore, each term align-
ment method implements a ranking function that
calculates a similarity score between a source and
a target term. The methods rank target terms ac-
cording to the similarity score and select the top N
ranked terms as candidate translations. The rank-
ing functions will be discussed in the following
subsections.
3.1 Character n-gram models
Let s be a source term containing p character n-
grams (s={s
1
, s
2
, ..., s
p
} s
i
? S, ?i ? [1, p])
and t a target term of q n-grams (t={t
1
, t
2
, ..., t
q
}
t
i
? T , ?i ? [1, q]). We extract charac-
ter n-grams by considering any contiguous, non-
linguistically motivated sequence of characters
that occurs within a window size of [2 ? 5]
2
) for
English, French and Greek. For Japanese, uni-
grams are included (window size of [1 ? 5] be-
cause Japanese terms often contain Kanji (Chi-
nese) characters.
Given the two lists of source and target n-grams,
our objective is to find an underlying relationship
between S and T that best discriminates trans-
lation from non-translation pairs. The RF clas-
sifier was previously shown to exhibit such be-
haviour (Kontonatsios et al., 2014). An RF clas-
sifier (Breiman, 2001) is a collection of decision
trees voting for the most popular class. For a pair
of source and target terms ?s, t?, the RF method
creates feature vectors of a fixed size 2r, i.e., first
order feature space. The first r features are ex-
tracted from the source term, while the last r fea-
tures from the target term. Each feature has a
boolean value (0 or 1) that designates the pres-
ence/absence of the corresponding n-gram in the
input instance.
The ability of the RF to detect latent associa-
tions between S and T relies on the decision trees.
The internal nodes of a decision tree represent the
n-gram features that are linked together in the tree-
hierarchy. Each leaf node of the trees is labelled as
translation or non-translation indicating whether
the parent path of n-gram features is positively or
negatively associated. The classification margin
that we use to rank the candidate translations is
given by a margin function (Breiman, 2001):
mg(X,Y ) = av(I(x) = 1)?av(I(x)) = 0) (1)
where x is an instance ?s, t?, y ? Y = {0, 1} the
class label, I(?) : (s, t) ?? {0, 1} is the indicator
function of a decision tree and av(I(?)) the aver-
age number of trees voting for the same class la-
bel. In our experiments, we used the same settings
as the ones reported in Kontonatsios et al. (2014).
2
we have experiments with larger and narrower window
sizes but this setting resulted in better translation accuracy
1703
We used 140 decision trees and log
2
|2q| + 1 ran-
dom features. For training an RF model, we used
the WEKA platform (Hall et al., 2009).
The second class of machine learning algo-
rithms that we investigate is Support Vector Ma-
chines (SVMs). The simplest version of SVMs
is a linear classifier (linear-SVM) that tries to
place a hyperplane, a decision boundary, that sepa-
rates translation from non-translation instances. A
linear-SVM is a feature agnostic method since the
model only exploits the position of the vectors in
the hyperspace to achieve class separation (Hastie
et al., 2009).
The first order feature representation used with
the RF classifier does not model associations be-
tween S and T . Hence, intuitively, a first or-
der feature space is not linearly separable, i.e.,
there exists no decision boundary that divides the
data points into translations and non-translations.
3
. To solve non-linear classification problems,
SVMs employ non-linear kernels. A kernel func-
tion projects input instances into a higher dimen-
sional space to discover non-linear associations
between the initial features. In this new, projected
feature space, the SVM attempts to define a sep-
arating plane. For training an non-linear SVM on
the first order feature space, we used the LIBSVM
package (Chang and Lin, 2011) with a radial ba-
sis function (RBF) kernel. For ranking candidate
translations, we used the decision value given by
LIBSVM which represents the distance between
an instance and the hyperplane. To translate a
source term, the method ranks candidate transla-
tions by decision value and suggests as best trans-
lation the candidate with the maximum distance
(maximum margin).
While the first order models try to find cross-
lingual mappings between monolingual features,
our proposed method follows a different approach.
It models cross-lingual links between the source
and target character n-grams and uses them as
second order features to train a linear classifier.
A second order feature is a tuple of n-grams in
S and T , respectively, that co-occur in a train-
ing, translation instance. Second order feature
3
We applied a linear-SVM with the first order feature
representation on the four comparable corpora for English-
French, English-Spanish, English-Greek and English-
Japanese. In all cases, the best accuracies achieved were close
to zero. Additionally, the ranked list of candidate translations
was the same for all source terms. Hence, we can empiri-
cally suggest that the linear-SVM cannot exploit a first order
feature space.
values are boolean. Given a translation instance
?s, t? of p source and q target n-grams, there are
p?q second order features. For dimensionality re-
duction, we consider as second order features the
most frequent out of all possible first order feature
combinations, only. Experiments indicate that a
large number of features needs to be considered
to achieve robust performance. To cope with the
high dimensional second order space, we use LI-
BLINEAR (Fan et al., 2008), which is designed
to solve large-scale, linear classifications prob-
lems. LIBLINEAR implements two linear clas-
sification algorithms: LogReg and linear-SVM.
Both models solve the same optimisation problem,
i.e., determine the optimal separating plane, but
they adopt different loss functions. Since LIBLIN-
EAR does not support decision value estimations
for the linear-SVM, we only experimented with
LogReg. Similarly to SVM-RBF, LogReg ranks
candidate translations by classification margin.
3.2 Context vectors
We follow a standard approach to calculate context
similarity of source and target terms (Rapp, 1999;
Morin and Daille, 2010; Morin and Prochasson,
2011a; Delpech et al., 2012). Context vectors
of candidate terms in the source and target lan-
guage are populated after normalising each bilin-
gual corpus, separately. Normalisation consists
of stop-word filtering, tokenisation, lemmatisa-
tion and Part-of-Speech (PoS) tagging. For En-
glish, Spanish and French we used the TreeTagger
(Schmid, 1994) while for Greek we used the ILSP
toolkit (Papageorgiou et al., 2000). The Japanese
corpus was segmented and PoS-tagged using Ju-
man (Kurohashi and Kawahara, 2005).
In succession, monolingual context vectors are
compiled by considering all lexical units that oc-
cur within a window of 3 words before or af-
ter a term (a seven-word window). Only lexical
units (seeds) that occur in a bilingual dictionary
are retained The values in context vectors are Log-
Likelihood Ratio associations (Dunning, 1993) of
the term and a seed lexical unit occurring in it. In
a second step, we use the translations in the seed
dictionary to map target context vectors into the
source vector space. If there are several transla-
tions for a term, they are all considered with equal
weights. Finally, candidate translations are ranked
in descending order of the cosine of the angle be-
tween the mapped target vectors and the source
1704
Trainingcorpus
Testcorpus
character n-grammodel context vectors
hybrid model
Annotate Annotate
Train Project
seed termdictionary seed worddictionary
Figure 1: Architecture of the hybrid term align-
ment system.
vector.
3.3 Hybrid term alignment system
Figure 1 illustrates a block diagram of our term
alignment system. We use two bilingual seed dic-
tionaries: (a) a dictionary of term translation pairs
to train the n-gram models and (b) a dictionary of
word-to-word correspondences to translate target
context vectors. The n-gram and context vector
methods are used separately to score term pairs.
The n-gram model computes the value of the com-
positional clue while the context vector estimates
the score of the contextual clue. The hybrid model
combines both methods by using the correspond-
ing scores as features to train a linear classifier.
For this, we used a linear-SVM of the LIBSVM
package with default values for all parameters.
4 Data
Following previous research (Prochasson and
Fung, 2011; Irvine and Callison-Burch, 2013;
Klementiev et al., 2012), we construct compara-
ble biomedical corpora using Wikipedia as a freely
available resource.
Starting with a list of 4K biomedical English
terms (query-terms), we collected 4K English
Wikipedia articles, by matching query-terms to the
topic signatures of articles. Then, we followed
the Wikipedia interlingual links to retrieve the-
matically related articles in each target language.
Since not all English articles contain links for all
four target languages (Spanish, French, Greek and
Japanese), we used a different list of query-terms
for each language pair. Corpora were randomly
divided into training and testing parts. For train-
ing we used 3K documents and for testing the re-
maining 1K. Table 1 shows the size of corpora in
terms of numbers of source (SW) and target words
(TW).
4.1 Seed dictionaries
As shown in Figure 1, the term alignment methods
require two seed bilingual dictionaries: a term and
a word dictionary. The character n-gram models
rely on a bilingual term dictionary to learn asso-
ciations of n-grams that appear often in technical
terms. The dictionary may contain both single-
word and multi-word terms. For English-Spanish
and English-French we used UMLS (Bodenreider,
2004) while for English-Japanese we used an elec-
tronic dictionary of medical terms (Denshika and
Kenkyukai, 1991).
An English-Greek biomedical dictionary was
not available at the time of conducting these ex-
periments, thus we automatically compiled a dic-
tionary from a parallel corpus. For this, we trained
a standard Statistical Machine Translation system
(Koehn et al., 2007) on EMEA (Tiedemann, 2009),
a biomedical parallel corpus containing sentence-
aligned documents from the European Medicines
Agency. Then, we extracted all English-Greek
pairs for which: (a) the English sequence was
listed in UMLS and (b) the translation probability
was equal or higher to 0.7.
The sizes of the seed term dictionaries vary sig-
nificantly, e.g., 500K entries for English-French
but only 20K entries for English-Greek. How-
ever, the character n-gram models require a rela-
tively small portion of the corresponding dictio-
nary to converge. In the reported experiments,
we used 10K translation pairs as positive, train-
ing instances. In addition, we generated an equal
number of pseudo-negative instances by randomly
matching non-translation terms.
Morin and Prochasson (2011b) showed that the
translation accuracy of context vectors is higher
when using bilingual dictionaries that contain both
general language entries and technical terms rather
than general or domain-specific dictionaries, sep-
1705
Training corpus Test Corpus
# SW # TW # SW # TW
en-fr 4.8M 2.2M 1.9M 1.1M
en-es 4.9M 2.5M 1.8M 0.9M
en-el 10.2M 2.4M 3.3M 1.3M
en-jpn 5.3M 2.4M 2.3M 1.2M
Table 1: Statistics of the English-French (en-
fr), Engish-Spanish (en-es), English-Greek (en-
el) and English-Japanese (en-jpn) Wikipedia com-
parable corpora. SW: source words, TW: target
words
Corpus Seed words
Comparability in dictionary
en-fr 0.71 66K
en-es 0.75 40K
en-el 0.68 22K
en-jpn 0.49 57K
Table 2: Corpus comparability and number of fea-
tures of the seed word dictionaries
arately. In a mixed dictionary, lexical units are
either single-word technical terms, such as ?dis-
ease? and ?patient?, or general language words,
such as ?occur? and ?high?. Note that we have
already compiled a seed term dictionary for each
pair of languages. Following the suggestion of
Morin and Prochasson (2011b), we attempt to en-
rich the seed term dictionaries with general lan-
guage entries. For this, we extracted bilingual
word dictionaries for English-Spanish, English-
French and English-Greek by applying GIZA++
(Och and Ney, 2003) on the EMEA corpus. We
then concatenated the word with the term dictio-
naries to obtain enhanced seeds for the three lan-
guage pairs. For English-Japanese, we only used
the term dictionary to translate the target context
vectors.
Once the word dictionaries have been compiled,
we compute the corpus comparability measure. Li
and Gaussier (2010) define corpus comparability
as the percentage of words that can be translated
bi-directionally, given a seed dictionary.
Table 2 shows corpus comparability scores of
the four corpora accompanied with the number
of English, single words in the seed dictionar-
ies. It can be observed that seed dictionary sizes
are not necessarily proportional to the correspond-
ing corpus comparability scores. As expected, for
English-Japanese, corpus comparability is low be-
cause the dictionary contains single-word terms,
only. The English-Spanish dictionary is smaller
than the English-French but achieved higher cor-
pus comparability, i.e., a higher percentage of
words can be bi-directionally translated using the
corresponding seed dictionary. A possible ex-
planation is that the comparable corpora were
constructed using different lists of query-terms.
Hence, the query-terms used for English-Spanish
retrieved a more coherent corpus. The resulting
values of corpus comparability indicate that the
context vectors will perform the best for English-
Spanish while for English-Japanese the perfor-
mance is expected to be substantially lower.
4.2 Training and evaluation datasets
For evaluation, we construct a test dataset of
single-word terms, in particular nouns or adjec-
tives. The dataset contains 1K terms that occur
more frequently than 20 but not more than 200
times and are listed in the English part of the
UMLS. In order to extract candidate translations,
we considered all nouns or adjectives that occur
at least 5 times in the target part of the corpus.
Furthermore, we do not constraint the evaluation
datasets only to those terms whose corresponding
translation occurs in the corpus.
The hybrid model that combines the composi-
tional and context clue, is based on a two-feature
model. Therefore, the model converges using only
a few hundred instances. For training a hybrid
model, we used 1K translation instances that oc-
curred in the training comparable corpora. Sim-
ilarly, to the character n-gram models, pseudo-
negative instances were generated by randomly
coupling non-translation terms. The ratio of posi-
tive to negative instances is 1 : 1.
5 Experiments
In this section, we present three experiments con-
ducted to evaluate the character n-gram, con-
text vector and hybrid methods. Firstly, we
examine the performance of the n-gram mod-
els on closely related language pairs (English-
French, English-Spanish), on a distant language
pair (English-Greek) and on an unrelated language
pair (English-Japanese). English and Greek are
not unrelated because they are members of the
same language family, but also not closely re-
lated because they use different scripts. Secondly,
1706
we compare the character n-gram methods against
context vectors when translating frequent or rare
terms and on comparable corpora of similar lan-
guage pairs (English-French, English-Spanish) but
of different corpus comparability scores. Thirdly,
we evaluate the hybrid method on all four com-
parable corpora and investigate the improvement
margin of combining the contextual with the com-
positional clue.
As evaluation metrics, we adopt the top-N
translation accuracy, following most previous ap-
proaches (Rapp, 1999; Chiao and Zweigenbaum,
2002; Morin et al., 2007; Tamura et al., 2012). The
top-N translation accuracy is defined as the per-
centage of source terms for which a given method
has output the correct translation among the top N
candidate translations.
5.1 Character n-gram models
In the first experiment, we investigate the perfor-
mance of the character n-gram models consider-
ing an increasing number of features. The features
were sorted in order of decreasing frequency of oc-
currence. Starting from the top of the list, more
features were incrementally added and translation
accuracy was recorded.
Figure 2 shows the top-20 translation accu-
racy of single-word terms on an increasing num-
ber of first and second order features. With re-
gards to the first order models (Subfigure 2a),
the Random Forest (RF) classifier outperforms
our baseline method (SVM-RBF) for all four lan-
guage pairs. The largest margin between RF and
SVM-RBF can be observed for the English-Greek
dataset while for closely related language pairs,
i.e., English-French and English-Spanish, the mar-
gin is smaller. Furthermore, it can be noted that
using only a small number of first order features,
1K features (500 for the source and 500 for the
target language, both n-gram models reach a sta-
ble performance.
In contrast to the first order models, the Lo-
gReg classifier requires a large number of sec-
ond order features to achieve a robust performance
(Subfigure 2b). Starting from 100K features, the
translation accuracy continuously increases. The
best performance is observed for a total number
of 4M second order features when considering
the English-French, English-Spanish and English-
Greek datasets. For English-Japanese, the best
performance is achieved for 2M features. Beyond
this point, translation accuracy decreases slightly.
After feature selection is performed, we directly
compare all the character n-gram models. Table 3
summarises performance achieved by the LogReg,
RF and SVM-RBF models. It can be noted that
LogReg and RF performed similarly for closely
related languages (no statistically significant dif-
ferences were observed) while both methods out-
performed the SVM-RBF. However, for English-
Greek and English-Japanese, LogReg achieved
a statistically significant improvement over the
translation accuracy of RF and SVM-RBF. Lo-
gReg outperformed RF by 7% for English-Greek,
while for English-Japanese the improvement was
10% and 17% percent for top-1 and top-20 accu-
racy, respectively. Finally, it can be observed that
the more distant the language pair is, the lower the
performance.
5.2 N-gram methods and context vectors
In this experiment, we compare the n-gram meth-
ods against context vectors with regards to two pa-
rameters: (a) the frequency of source terms to be
translated and (b) corpus comparability. English-
French and English-Spanish are similar language
pairs but the corresponding corpora are of dif-
ferent corpus comparability scores. To investi-
gate how performance is affected by term occur-
rence frequency, we compiled an additional test
dataset of 1K rare English terms in the frequency
range [10, 20]. Our intuition is, that character n-
gram methods will perform similarly for all set-
tings since character n-grams are corpus indepen-
dent features.
We compare (a) the character n-gram models
(LogReg, RF and SVM-RBF) with (b) the con-
text vector method (context) and (c) an upper
bound. The latter represents the percentage of
source terms for which a reference translation ac-
tually occurs in the target corpus. Hence, the up-
per bound is the maximum performance achiev-
able according to the reference evaluation.
Figure 3a shows the top-20 translation accu-
racy for high and medium frequency terms, within
the frequency range [20, 200]. Context vectors
achieved a robust performance of 52% and 45%
for English-Spanish and English-French, respec-
tively. The difference in corpus comparability
can explain this 7% margin between these perfor-
mances. As shown in Table 2, the corpus com-
parability scores for English-Spanish and English-
1707
0
0.1
0.2
0.3
0.4
0.5
0.6
100 200 400 600 800 1000
trans
lation
 accu
racy 
@ 20
# first order features
RF (en-jpn)SVM-RBF (en-jpn)RF (en-el)SVM-RBF (en-el)
RF (en-fr)SVM-RBF (en-fr)RF (en-es)SVM-RBF (en-es)
(a) First order n-gram models
0.3
0.35
0.4
0.45
0.5
0.55
0.6
0.65
100 200 300 400 500 1000 2000 3000 4000
trans
lation
 accu
racy 
@ 20
# second order features (x10^3)
en-jpn en-el en-fr en-es
(b) Second order n-gram model
Figure 2: Top-20 translation accuracy of models trained on (a) first and (b) second order features
English-French English-Spanish English-Greek English-Japanese
acc@1 acc@20 acc@1 acc@20 acc@1 acc@20 acc@1 acc@20
LogReg 0.45 0.61 0.42 0.62 0.3 0.48 0.25 0.41
RF 0.47 0.58 0.43 0.59 0.23 0.41 0.15 0.24
SVM-RBF 0.38 0.51 0.33 0.53 0.1 0.25 0.06 0.16
Table 3: Top-1 (acc@1) and top-20 (acc@20) translation accuracy of LogReg, RF and SVM-RBF
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
en-fr en-es%
 top
-20 
tran
slati
on a
ccur
acy
LogRegRFSVM-RBF
Contextupper bound
(a) Test terms with frequency [20, 200]
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
en-fr en-es%
 top
-20 
tran
slati
on a
ccur
acy
LogRegRFSVM-RBF
Contextupper bound
(b) Test terms with frequency [10, 20]
Figure 3: Top-20 translation accuracy of terms in the frequency range of [10, 200] and [10, 20]
French are 0.75 and 0.71, respectively. In contrast
to context vectors, the character n-gram methods
performed comparably.
A second factor that affects the performance of
context vectors, is the frequency of the terms to
be translated. The translation of rare terms has
been shown to be a challenging case for context
vectors. For example, Morin and Daille (2010)
reported low accuracy (21% for the top-20 can-
didates) of context vectors for terms occurring 20
times or less. In our experiments, Figure 3b illus-
trates accuracies achieved for less frequent terms
([10, 20]). The performance of context vectors is
significantly lower, 26% for English-Spanish and
21% for English-French. Furthermore, the trans-
lation accuracy of the n-gram methods decreases
slightly (? 5% to 8%). This can be explained
by the decrease of the upper bound for lower fre-
quency terms (? 3% to 6%).
5.3 Combining internal and contextual
similarity
We have hypothesised that the compositional and
contextual clue are orthogonal, i.e., they convey
1708
 0.1 0.2
 0.3 0.4
 0.5 0.6
 0.7 0.8
 0.9
en-fr en-es en-el en-jpn%
 top
-1 tr
ansl
atio
n ac
cura
cy
LogRegLogReg+ContextRFRF+Context
SVM-RBFSVM-RBF+ContextContextupper bound
(a) Top-20 accuracy (acc@20)
 0.1 0.2
 0.3 0.4
 0.5 0.6
 0.7 0.8
 0.9
en-fr en-es en-el en-jpn%
 top
-1 tr
ansl
atio
n ac
cura
cy
LogRegLogReg+ContextRFRF+Context
SVM-RBFSVM-RBF+ContextContextupper bound
(b) Top-1 accuracy (acc@1)
Figure 4: Overall performance. Top-20 and top-1 translation accuracy
different and possibly complimentary information.
To investigate this intuition, we evaluate the hybrid
model on all four comparable corpora, for term oc-
currence frequencies in [20, 200].
Figure 4a illustrates top-20 translation accu-
racy scores for (a) the character n-gram models,
(b) the context vector method and (c) the hy-
brid models, i.e., LogReg+Context, RF+Context,
SVM-RBF+Context. We observe that the com-
bination of the compositional and contextual clue
improved the performance of all methods. The hy-
brid model largely improved the performance of
the SVM-RBF (? 14% to 20%). With regards
to the combined signals the translation accuracy
of LogReg and RF increased by ? 4% for the
English-Japanese corpus and ? 8% for all other
corpora.
For the top 1 candidate translation, we observe
in Figure 4 smaller improvements achieved by the
hybrid model in comparison to the top-20 accu-
racy. Interestingly, the RF classifier performed
slightly better on its own for English-French,
English-Spanish and English-Japanese. This in-
dicates that the hybrid method ranks more correct
translations in the top 20 candidates but it does not
always assign the best score to the correct answer.
6 Discusion and Future work
In this paper, we investigated a compositional
and a context-based approach useful for compil-
ing bilingual dictionaries of terms automatically
from comparable corpora. Compositional transla-
tion methods exploit the internal structure of terms
across languages while context-based approaches
investigate the surrounding lexical context.
We proposed a character n-gram composi-
tional method, i.e., a Logistic Regression clas-
sifier, which uses a multilingual representation,
i.e., source and target terms. Experimental evi-
dence showed that the LogReg classifier signifi-
cantly outperformed the baseline methods on dis-
tant languages. For closely related languages, Lo-
gReg performed comparably to an existing n-gram
method based on a Random Forest classifier.
Furthermore, we compared the n-gram models
against a context-based approach under different
corpus-specific parameters: (a) corpus compara-
bility, which is relevant to the seed dictionary, and
(b) the occurrence frequency of the terms to be
translated. It was shown that the performance of
n-gram methods was not affected by different pa-
rameter settings. Only small fluctuations were ob-
served, since the n-gram methods are based on
corpus-independent features, only. In contrast,
the context-based method was affected by corpus
comparability scores. The corresponding transla-
tion accuracy declined significantly for rare terms.
Finally, we hypothesised that the n-gram and
context-based methods provide complimentary in-
formation. To test this hypothesis, we developed a
hybrid method that combines compositional and
contextual similarity scores as features in a lin-
ear classifier. The hybrid model achieved signif-
icantly better top-20 translation accuracy than the
two methods separately but minor improvements
were observed in terms of top-1 accuracy.
As future work, we plan to improve the qual-
ity of the extracted dictionary further by exploiting
additional translation signals. For example, previ-
ous works (Schafer and Yarowsky, 2002; Klemen-
tiev et al., 2012) have reported that the temporal
and topic similarity are clues that indicate transla-
tion equivalence. It would be interesting to investi-
gate the contribution of different clues for various
1709
experimental parameters, e.g., domain, distance of
languages, types of comparable corpora.
Acknowledgements
The authors would like to thank Dr. Danushka
Bollegala for providing feedback on this paper
and the three anonymous reviewers for their useful
comments and suggestions. This work was funded
by the European Community?s Seventh Frame-
work Program (FP7/2007-2013) [grant number
318736 (OSSMETER)].
References
Daniel Andrade, Tetsuya Nasukawa, and Jun?ichi Tsu-
jii. 2010. Robust measurement and comparison of
context similarity for finding translation pairs. In
Proceedings of the 23rd International Conference
on Computational Linguistics, pages 19?27. Asso-
ciation for Computational Linguistics.
Marianna Apidianaki, Nikola Ljube?sic, and Darja
Fi?ser. 2012. Disambiguating vectors for bilin-
gual lexicon extraction from comparable corpora.
In Eighth Language Technologies Conference, pages
10?15.
Lisa Ballesteros and W.Bruce Croft. 1997. Phrasal
translation and query expansion techniques for
cross-language information retrieval. In ACM SIGIR
Forum, volume 31, pages 84?91. ACM.
Olivier Bodenreider. 2004. The unified medical lan-
guage system (umls): integrating biomedical termi-
nology. Nucleic acids research, 32(suppl 1):D267?
D270.
Leo Breiman. 2001. Random Forests. Machine Learn-
ing, 45:5?32.
Chih-Chung Chang and Chih-Jen Lin. 2011. Lib-
svm: a library for support vector machines. ACM
Transactions on Intelligent Systems and Technology
(TIST), 2(3):27.
Yun-Chuang Chiao and Pierre Zweigenbaum. 2002.
Looking for Candidate Translational Equivalents in
Specialized, Comparable Corpora. In International
Conference on Computational Linguistics.
Ido Dagan and Ken Church. 1994. Termight: Identi-
fying and translating technical terminology. In Pro-
ceedings of the fourth conference on Applied natural
language processing, pages 34?40. Association for
Computational Linguistics.
Emmanuel Morin B??eatrice Daille. 2012. Revising the
compositional method for terminology acquisition
from comparable corpora. COLING 2012, 1810.
Estelle Delpech, B?eatrice Daille, Emmanuel Morin,
and Claire Lemaire. 2012. Extraction of domain-
specific bilingual lexicon from comparable corpora:
Compositional translation and ranking. In COLING,
pages 745?762.
Igakuyo Denshika and Jisho Kenkyukai. 1991.
250,000 medical term dictionary (in japanese).
Nichigai Associates, Inc.
Ted Dunning. 1993. Accurate methods for the statis-
tics of surprise and coincidence. Computational lin-
guistics, 19(1):61?74.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. The Journal of
Machine Learning Research, 9:1871?1874.
Pascale Fung and Kathleen McKeown. 1997. A
technical word-and term-translation aid using noisy
parallel corpora across language groups. Machine
Translation, 12(1-2):53?87.
Pascale Fung and Lo Yuen Yee. 1998. An ir approach
for translating new words from nonparallel, compa-
rable texts. In Proceedings of the 17th international
conference on Computational linguistics-Volume 1,
pages 414?420. Association for Computational Lin-
guistics.
M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute-
mann, and I.H. Witten. 2009. The weka data mining
software: an update. ACM SIGKDD Explorations
Newsletter, 11(1):10?18.
Z.S. Harris. 1954. Distributional structure. Word.
Trevor Hastie, Robert Tibshirani, Jerome Friedman,
T Hastie, J Friedman, and R Tibshirani. 2009. The
elements of statistical learning, volume 2. Springer.
Ann Irvine and Chris Callison-Burch. 2013. Su-
pervised bilingual lexicon induction with multiple
monolingual signals. In Proceedings of NAACL-
HLT, pages 518?523.
Edward L Keenan and Leonard M Faltz. 1985.
Boolean semantics for natural language, volume 23.
Springer.
Alexandre Klementiev, Ann Irvine, Chris Callison-
Burch, and David Yarowsky. 2012. Toward statisti-
cal machine translation without parallel corpora. In
Proceedings of the 13th Conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 130?140. Association for Computa-
tional Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, et al. 2007. Moses: Open source
toolkit for statistical machine translation. In Pro-
ceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
1710
pages 177?180. Association for Computational Lin-
guistics.
G. Kontonatsios, I. Korkontzelos, J. Tsujii, and S. Ana-
niadou. 2014. Using a random forest classifier
to compile bilingual dictionaries of technical terms
from comparable corpora. In Proceedings of the
14th Conference of the European Chapter of the As-
sociation for Computational Linguistics, volume 2:
Short Papers, pages 111?116. Association for Com-
putational Linguistics.
Sadao Kurohashi and Daisuke Kawahara. 2005.
Japanese morphological analysis system juman ver-
sion 5.1 manual.
Bo Li and Eric Gaussier. 2010. Improving corpus
comparability for bilingual lexicon extraction from
comparable corpora. In Proceedings of the 23rd In-
ternational Conference on Computational Linguis-
tics, pages 644?652. Association for Computational
Linguistics.
Christian Lovis, R Baud, PA Michel, JR Scherrer, and
AM Rassinoux. 1997. Building medical dictionar-
ies for patient encoding systems: A methodology. In
Artificial Intelligence in Medicine, pages 373?380.
Springer.
Emmanuel Morin and B?eatrice Daille. 2010. Com-
positionality and lexical alignment of multi-word
terms. Language Resources and Evaluation, 44(1-
2):79?95.
Emmanuel Morin and Emmanuel Prochasson. 2011a.
Bilingual lexicon extraction from comparable cor-
pora enhanced with parallel corpora. In Proceedings
of the 4th Workshop on Building and Using Compa-
rable Corpora: Comparable Corpora and the Web,
pages 27?34. Association for Computational Lin-
guistics.
Emmanuel Morin and Emmanuel Prochasson. 2011b.
Bilingual lexicon extraction from comparable cor-
pora enhanced with parallel corpora. In Proceedings
of the 4th Workshop on Building and Using Compa-
rable Corpora: Comparable Corpora and the Web,
pages 27?34, Portland, Oregon, June. Association
for Computational Linguistics.
Emmanuel Morin, B?eatrice Daille, Koichi Takeuchi,
and Kyo Kageura. 2007. Bilingual terminology
mining - using brain, not brawn comparable corpora.
In Proceedings of the 45th Annual Meeting of the As-
sociation of Computational Linguistics, pages 664?
671, Prague, Czech Republic, June. Association for
Computational Linguistics.
Fiammetta Namer and Robert Baud. 2007. Defin-
ing and relating biomedical terms: towards a cross-
language morphosemantics-based system. Interna-
tional Journal of Medical Informatics, 76(2):226?
233.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational linguistics, 29(1):19?51.
Harris Papageorgiou, Prokopis Prokopidis, Voula
Giouli, and Stelios Piperidis. 2000. A unified pos
tagging architecture and its application to greek. In
Proceedings of the 2nd Language Resources and
Evaluation Conference, pages 1455?1462, Athens,
June. European Language Resources Association.
Emmanuel Prochasson and Pascale Fung. 2011. Rare
word translation extraction from aligned compara-
ble documents. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies-Volume
1, pages 1327?1335. Association for Computational
Linguistics.
James Pustejovsky, Jose Castano, Brent Cochran, Ma-
ciej Kotecki, and Michael Morrell. 2001. Au-
tomatic extraction of acronym-meaning pairs from
medline databases. Studies in health technology and
informatics, (1):371?375.
Reinhard Rapp. 1999. Automatic identification of
word translations from unrelated english and german
corpora. In Proceedings of the 37th annual meeting
of the Association for Computational Linguistics on
Computational Linguistics, pages 519?526. Associ-
ation for Computational Linguistics.
Xavier Robitaille, Yasuhiro Sasaki, Masatsugu
Tonoike, Satoshi Sato, and Takehito Utsuro. 2006.
Compiling french-japanese terminologies from the
web. In EACL.
Charles Schafer and David Yarowsky. 2002. Inducing
translation lexicons via diverse similarity measures
and bridge languages. In proceedings of the 6th con-
ference on Natural language learning-Volume 20,
pages 1?7. Association for Computational Linguis-
tics.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of the
International Conference on New Methods in Lan-
guage Processing, volume 12, pages 44?49. Manch-
ester, UK.
Frank Smadja, Kathleen R McKeown, and Vasileios
Hatzivassiloglou. 1996. Translating collocations for
bilingual lexicons: A statistical approach. Compu-
tational linguistics, 22(1):1?38.
Akihiro Tamura, Taro Watanabe, and Eiichiro Sumita.
2012. Bilingual lexicon extraction from compara-
ble corpora using label propagation. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 24?36. Associa-
tion for Computational Linguistics.
Takaaki Tanaka. 2002. Measuring the similarity be-
tween compound nouns in different languages us-
ing non-parallel corpora. In Proceedings of the
19th international conference on Computational
linguistics-Volume 1, pages 1?7. Association for
Computational Linguistics.
1711
J?org Tiedemann. 2009. News from opus-a collection
of multilingual parallel corpora with tools and in-
terfaces. In Recent Advances in Natural Language
Processing, volume 5, pages 237?248.
Pim Van der Eijk. 1993. Automating the acquisition of
bilingual terminology. In Proceedings of the sixth
conference on European chapter of the Association
for Computational Linguistics, pages 113?119. As-
sociation for Computational Linguistics.
Fabio Massimo Zanzotto, Ioannis Korkontzelos,
Francesca Fallucchi, and Suresh Manandhar. 2010.
Estimating linear models for compositional distribu-
tional semantics. In Proceedings of the 23rd Inter-
national Conference on Computational Linguistics,
COLING ?10, pages 1263?1271, Stroudsburg, PA,
USA. Association for Computational Linguistics.
1712
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 111?116,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
Using a Random Forest Classifier to Compile Bilingual Dictionaries of
Technical Terms from Comparable Corpora
Georgios Kontonatsios
1,2
Ioannis Korkontzelos
1,2
Jun?ichi Tsujii
3
Sophia Ananiadou
1,2
National Centre for Text Mining, University of Manchester, Manchester, UK
1
School of Computer Science, University of Manchester, Manchester, UK
2
Microsoft Research Asia, Beijing, China
3
{gkontonatsios,ikorkontzelos,sananiadou}@cs.man.ac.uk
jtsujii@microsoft.com
Abstract
We describe a machine learning approach,
a Random Forest (RF) classifier, that is
used to automatically compile bilingual
dictionaries of technical terms from com-
parable corpora. We evaluate the RF clas-
sifier against a popular term alignment
method, namely context vectors, and we
report an improvement of the translation
accuracy. As an application, we use the
automatically extracted dictionary in com-
bination with a trained Statistical Machine
Translation (SMT) system to more accu-
rately translate unknown terms. The dic-
tionary extraction method described in this
paper is freely available
1
.
1 Background
Bilingual dictionaries of technical terms are im-
portant resources for many Natural Language
Processing (NLP) tasks including Statistical Ma-
chine Translation (SMT) (Och and Ney, 2003) and
Cross-Language Information Retrieval (Balles-
teros and Croft, 1997). However, manually cre-
ating and updating such resources is an expensive
process. In addition to this, new terms are con-
stantly emerging. Especially in the biomedical
domain, which is the focus of this work, there is
a vast number of neologisms, i.e., newly coined
terms, (Pustejovsky et al., 2001).
Early work on bilingual lexicon extraction
focused on clean, parallel corpora providing
satisfactory results (Melamed, 1997; Kay and
R?oscheisen, 1993). However, parallel corpora are
expensive to construct and for some domains and
language pairs are scarce resources. For these rea-
sons, the focus has shifted to comparable corpora
1
http://personalpages.manchester.
ac.uk/postgrad/georgios.kontonatsios/
Software/RF-TermAlign.tar.gz
that are more readily available, more up-to-date,
larger and cheaper to construct than parallel data.
Comparable corpora are collections of monolin-
gual documents in a source and target language
that share the same topic, domain and/or docu-
ments are from the same period, genre and so
forth.
Existing methods for bilingual lexicon extrac-
tion from comparable corpora are mainly based
on the same principle. They hypothesise that a
word and its translation tend to appear in simi-
lar lexical context (Fung and Yee, 1998; Rapp,
1999; Morin et al., 2007; Chiao and Zweigen-
baum, 2002). Context vector methods are reported
to achieve robust performance on terms that occur
frequently in the corpus. Chiao and Zweigenbaum
(2002) achieved a performance of 94% accuracy
on the top 20 candidates when translating high fre-
quency, medical terms (frequency of 100 or more).
In contrast, Morin and Daille (2010) reported an
accuracy of 21% for multi-word terms occurring
20 times or less, noting that translating rare terms
is a challenging problem for context vectors.
Kontonatsios et al. (2013) introduced an RF
classifier that is able to automatically learn as-
sociation rules of textual units between a source
and target language. However, they applied their
method only on artificially constructed datasets
containing an equal number of positive and neg-
ative instances. In the case of comparable cor-
pora, the datasets are highly unbalanced (given n,
m source and target terms respectively, we need to
classify n?m instances). In this work, we incor-
porate the classification margin into the RF model,
to allow the method to cope with the skewed dis-
tribution of positive and negative instances that oc-
curs in comparable corpora.
Our proposed method ranks candidate transla-
tions using the classification margin and suggests
as the best translation the candidate with the max-
imum margin. We evaluate our method on an
111
English-Spanish comparable corpus of Wikipedia
articles that are related to the medical sub-domain
of ?breast cancer?. Furthermore, we show that dic-
tionaries extracted from comparable corpora can
be used to dynamically augment an SMT sys-
tem in order to better translate Out-of-Vocabulary
(OOV) terms.
2 Methodology
A pair of terms in a source and target language is
represented as a feature vector where each dimen-
sion corresponds to a unique character n-gram.
The value of each dimension is 0 or 1 and desig-
nates the occurrence of the corresponding n-gram
in the input terms. The feature vectors that we
use contain 2q dimensions where the first q dimen-
sions correspond to the n-gram features extracted
from the source terms and the last q dimensions to
those from the target terms. In the reported experi-
ments, we use the 600 (300 source and 300 target)
most frequently occurring n-grams.
The underlying mechanism that allows the RF
method to learn character gram mappings between
terms of a source and target language is the de-
cision trees. A node in the decision tree is a
unique character n-gram. The nodes are linked
through the branches of the trees and therefore the
two sub-spaces of q source and q target charac-
ter grams are combined. Each decision tree in the
forest is constructed as follows: every node is split
by considering |?| random n-gram features of the
initial feature set ?, and a decision tree is fully
grown. This process is repeated |? | times and con-
structs |? | decision trees. We tuned the RF clas-
sifier using 140 random trees where we observed
a plateau in the classification performance. Fur-
thermore, we set the number of random features
using |?| = log
2
|?|+ 1 as suggested by Breiman
(2001).
The classification margin that we use to rank
the candidate translations is calculated by simply
subtracting the average number of trees predicting
that the input terms are not translations from the
average number of decision trees predicting that
the terms are mutual translations. A larger classi-
fication margin means that more decision trees in
the forest classify an instance as a translation pair.
For training an RF model, we use a bilingual
dictionary of technical terms. When the dictionary
lists more than one translation for an English term,
we randomly select only one. Negative instances
are created by randomly matching non-translation
pairs of terms. We used an equal number of posi-
tive and negative instances for training the model.
Starting from 20, 000 translation pairs we gener-
ated a training dataset of 40, 000 positive and neg-
ative instances.
2.1 Baseline method
The context projection method was first pro-
posed by (Fung and Yee, 1998; Rapp, 1999) and
since then different variations have been suggested
(Chiao and Zweigenbaum, 2002; Morin et al.,
2007; Andrade et al., 2010; Morin and Prochas-
son, 2011). Our implementation more closely
follows the context vector method introduced by
(Morin and Prochasson, 2011).
As a preprocessing step, stop words are re-
moved using an online list
2
and lemmatisation
is performed using TreeTagger (Schmid, 1994) on
both the English and Spanish part of the compa-
rable corpus. Afterwards, the method proceeds
in three steps. Firstly, for each source and target
term of the comparable corpus, i.e., i, we collect
all lexical units that: (a) occur within a window
of 3 words around i (a seven-word window) and
(b) are listed in the seed bilingual dictionary. The
lexical units that satisfy the above two conditions
are the dimensions of the context vectors. Each
dimension has a value that indicates the correla-
tion between the context lexical unit and the term
i. In our approach, we use the log-likelihood ra-
tio. In the second step, the seed dictionary is used
to translate the lexical units of the Spanish context
vectors. In this way the Spanish and English vec-
tors become comparable. When several transla-
tions are listed in the seed dictionary, we consider
all of them. In the third step, we compute the con-
text similarity, i.e., distance metric, between the
vector of an English term to be translated with ev-
ery projected, Spanish context vector. For this we
use the cosine similarity.
3 Experiments
In this section, we evaluate the two dictionary ex-
traction methods, namely context vectors and RF,
on a comparable corpus of Wikipedia articles.
For the evaluation metric, we use the top-k
translation accuracy
3
and the mean reciprocal
2
http://members.unine.ch/jacques.savoy/clef/index.html
3
the percentage of English terms whose top k candidates
contain a correct translation
112
rank (MRR)
4
as in previous approaches (Chiao
and Zweigenbaum, 2002; Chiao and Zweigen-
baum, 2002; Morin and Prochasson, 2011; Morin
et al., 2007; Tamura et al., 2012). As a refer-
ence list, we use the UMLS metathesaurus
5
. In
addition to this, considering that in several cases
the dictionary extraction methods retrieved syn-
onymous translations that do not appear in the ref-
erence list, we manually inspected the answers.
Finally, unlike previous approaches (Chiao and
Zweigenbaum, 2002), we do not restrict the test
list only to those English terms whose Spanish
translations are known to occur in the target cor-
pus. In such cases, the performance of dictionary
extraction methods have been shown to achieve a
lower performance (Tamura et al., 2012).
3.1 Data
We constructed a comparable corpus of Wikipedia
articles. For this, we used Wikipedia?s search en-
gine
6
and submitted the queries ?breast cancer?
and ?c?ancer de mama? for English and Spanish
respectively. From the returned list of Wikipedia
pages, we used the 1, 000 top articles for both lan-
guages.
The test list contains 1, 200 English single-word
terms that were extracted by considering all nouns
that occur more than 10 but not more than 200
times and are listed in UMLS. For the Spanish part
of the corpus, we considered all nouns as candi-
date translations (32, 347 in total).
3.2 Results
Table 1 shows the top-k translation accuracy and
the MRR of RF and context vectors.
Acc
1
Acc
10
Acc
20
MRR
RF 0.41 0.57 0.59 0.47
Cont.
Vectors 0.1 0.21 0.26 0.11
Table 1: top-k translation accuracy and MRR of
RF and context vectors on 1, 200 English terms
We observe that the proposed RF method
achieves a considerably better top-k translation ac-
4
MRR =
1
|Q|
?
Q
i=1
1
rank
i
where |Q| is the number of
English terms for which we are extracting translations and
rank
i
is the position of the first correct translation from re-
turned list of candidates
5
nlm.nih.gov/research/umls
6
http://en.wikipedia.org/wiki/Help:Searching
curacy and MRR than the baseline method. More-
over, we segmented the 1, 200 test terms into 7
frequency ranges
7
, from high-frequency to rare
terms. Figure 1 shows the translation accuracy at
top 20 candidates for the two methods. We note
Figure 1: Translation accuracy of top 20 candi-
dates on different frequency ranges
that for high frequency terms, i.e. [100,200] range,
the performance achieved by the two methods is
similar (53% and 52% for the RF and context vec-
tors respectively). However, for lower frequency
terms, the translation accuracy of the context vec-
tors continuously declines. This confirms that con-
text vectors do not behave robustly for rare terms
(Morin and Daille, 2010). In contrast, the RF
slightly fluctuates over different frequency ranges
and presents approximately the same translation
accuracy for both frequent and rare terms.
4 Application
As an application of our method, we use the pre-
viously extracted dictionaries to on-line augment
the phrase table of an SMT system and observe
the translation performance on test sentences that
contain OOV terms. For the translation probabil-
ities in the phrase table, we use the distance met-
ric given by the dictionary extraction methods i.e.,
classification margin and cosine similarity of RF
and context vectors respectively, normalised by
the uniform probability (if a source term has m
candidate translations, we normalise the distance
metric by dividing by m as in (Wu et al., 2008) .
4.1 Data and tools
We construct a parallel, sentence-aligned corpus
from the biomedical domain, following the pro-
cess described in (Wu et al., 2011; Yepes et al.,
2013). The parallel corpus comprises of article ti-
tles indexed by PubMed in both English and Span-
ish. We collect 120K parallel sentences for train-
7
each frequency range contains 100 randomly sampled
terms
113
ing the SMT and 1K sentences for evaluation. The
test sentences contain 1, 200 terms that do not ap-
pear in the training parallel corpus. These terms
occur in the Wikipedia comparable corpus. Hence,
the previously extracted dictionaries list a possible
translation. Using the PubMed parallel corpus, we
train Moses (Koehn et al., 2007), a phrase-based
SMT system.
4.2 Results
We evaluated the translation performance of the
SMT that uses the dictionary extracted by the RF
against the following baselines: (i) Moses using
only the training parallel data (Moses), (ii) Moses
using the dictionary extracted by context vectors
(Moses+context vector). The evaluation metric is
BLEU (Papineni et al., 2002).
Table 2 shows the BLEU score achieved by the
SMT systems when we append the top-k transla-
tions to the phrase table.
BLEU
on top-k translations
1 10 20
Moses 24.22 24.22 24.22
Moses+
RF 25.32 24.626 24.42
Moses+
Context Vectors 23.88 23.69 23.74
Table 2: Translation performance when adding
top-k translations to the phrase table
We observe that the best performance is
achieved by the RF when we add the top 1 trans-
lation with a total gain of 1.1 BLEU points over
the baseline system. In contrast, context vec-
tors decreased the translation performance of the
SMT system. This indicates that the dictionary ex-
tracted by the context vectors is too noisy and as
a result the translation performance dropped. Fur-
thermore, it is noted that the augmented SMT sys-
tems achieve the highest performance for the top 1
translation while for k greater than 1, the transla-
tion performance decreases. This behaviour is ex-
pected since the target language model was trained
only on the training Spanish sentences of the par-
allel corpus. Hence, the target language model
does not have a prior knowledge of the OOV trans-
lations and as a result it cannot choose the correct
translation among k candidates.
To further investigate the effect of the language
model on the translation performance of the aug-
mented SMT systems, we conducted an oracle ex-
periment. In this ideal setting, we assume a strong
language model, that is trained on both training
and test Spanish sentences of the parallel corpus,
in order to assign a higher probability to a correct
translation if it exists in the deployed dictionary.
As we observe in Table 3, a strong language model
can more accurately select the correct translation
among top-k candidates. The dictionary extracted
by the RF improved the translation performance
by 2.5 BLEU points for the top-10 candidates and
context vectors by 0.45 for the top-20 candidates.
BLEU
on top-k translations
1 10 20
Moses 28.85 28.85 28.85
Moses+
RF 30.98 31.35 31.2
Moses+
Context Vectors 28.18 29.17 29.3
Table 3: Translation performance when adding
top-k translations to the phrase table. SMT sys-
tems use a language model trained on training and
test Spanish sentences of the parallel corpus.
5 Discussion
In this paper, we presented an RF classifier that
is used to extract bilingual dictionaries of techni-
cal terms from comparable corpora. We evaluated
our method on a comparable corpus of Wikipedia
articles. The experimental results showed that our
proposed method performs robustly when translat-
ing both frequent and rare terms.
As an application, we used the automatically
extracted dictionary to augment the phrase table of
an SMT system. The results demonstrated an im-
provement of the overall translation performance.
As future work, we plan to integrate the RF clas-
sifier with context vectors. Intuitively, the two
methods are complementary considering that the
RF exploits the internal structure of terms while
context vectors use the surrounding lexical con-
text. Therefore, it will be interesting to investigate
how we can incorporate the two feature spaces in
a machine learner.
114
6 Acknowledgements
This work was funded by the European Commu-
nity?s Seventh Framework Program (FP7/2007-
2013) [grant number 318736 (OSSMETER)].
References
Daniel Andrade, Tetsuya Nasukawa, and Jun?ichi Tsu-
jii. 2010. Robust measurement and comparison of
context similarity for finding translation pairs. In
Proceedings of the 23rd International Conference on
Computational Linguistics, COLING ?10, pages 19?
27, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Lisa Ballesteros and W.Bruce Croft. 1997. Phrasal
translation and query expansion techniques for
cross-language information retrieval. In ACM SIGIR
Forum, volume 31, pages 84?91. ACM.
Leo Breiman. 2001. Random Forests. Machine Learn-
ing, 45:5?32.
Yun-Chuang Chiao and Pierre Zweigenbaum. 2002.
Looking for Candidate Translational Equivalents in
Specialized, Comparable Corpora. In International
Conference on Computational Linguistics.
Pascale Fung and Lo Yuen Yee. 1998. An ir approach
for translating new words from nonparallel, compa-
rable texts. In Proceedings of the 17th international
conference on Computational linguistics-Volume 1,
pages 414?420. Association for Computational Lin-
guistics.
Martin Kay and Martin R?oscheisen. 1993. Text-
translation alignment. computational Linguistics,
19(1):121?142.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, et al. 2007. Moses: Open source
toolkit for statistical machine translation. In Pro-
ceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
pages 177?180. Association for Computational Lin-
guistics.
Georgios Kontonatsios, Ioannis Korkontzelos, Jun?ichi
Tsujii, and Sophia Ananiadou. 2013. Using ran-
dom forest to recognise translation equivalents of
biomedical terms across languages. In Proceed-
ings of the Sixth Workshop on Building and Using
Comparable Corpora, pages 95?104. Association
for Computational Linguistics, August.
I. Dan Melamed. 1997. A portable algorithm for map-
ping bitext correspondence. In Proceedings of the
35th Annual Meeting of the Association for Com-
putational Linguistics and Eighth Conference of the
European Chapter of the Association for Computa-
tional Linguistics, pages 305?312. Association for
Computational Linguistics.
Emmanuel Morin and B?eatrice Daille. 2010. Com-
positionality and lexical alignment of multi-word
terms. Language Resources and Evaluation, 44(1-
2):79?95.
Emmanuel Morin and Emmanuel Prochasson. 2011.
Bilingual lexicon extraction from comparable cor-
pora enhanced with parallel corpora. In Proceedings
of the 4th Workshop on Building and Using Compa-
rable Corpora: Comparable Corpora and the Web,
pages 27?34, Portland, Oregon, June. Association
for Computational Linguistics.
Emmanuel Morin, B?eatrice Daille, Koichi Takeuchi,
and Kyo Kageura. 2007. Bilingual terminology
mining - using brain, not brawn comparable corpora.
In Proceedings of the 45th Annual Meeting of the As-
sociation of Computational Linguistics, pages 664?
671, Prague, Czech Republic, June. Association for
Computational Linguistics.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational linguistics, 29(1):19?51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings of
the 40th annual meeting on association for compu-
tational linguistics, pages 311?318. Association for
Computational Linguistics.
James Pustejovsky, Jose Castano, Brent Cochran, Ma-
ciej Kotecki, and Michael Morrell. 2001. Au-
tomatic extraction of acronym-meaning pairs from
medline databases. Studies in health technology and
informatics, (1):371?375.
Reinhard Rapp. 1999. Automatic identification of
word translations from unrelated english and german
corpora. In Proceedings of the 37th annual meeting
of the Association for Computational Linguistics on
Computational Linguistics, pages 519?526. Associ-
ation for Computational Linguistics.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of the
International Conference on New Methods in Lan-
guage Processing, volume 12, pages 44?49. Manch-
ester, UK.
Akihiro Tamura, Taro Watanabe, and Eiichiro Sumita.
2012. Bilingual lexicon extraction from compara-
ble corpora using label propagation. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 24?36. Associa-
tion for Computational Linguistics.
Hua Wu, Haifeng Wang, and Chengqing Zong. 2008.
Domain adaptation for statistical machine transla-
tion with domain dictionary and monolingual cor-
pora. In Proceedings of the 22nd International
Conference on Computational Linguistics-Volume
1, pages 993?1000. Association for Computational
Linguistics.
115
Cuijun Wu, Fei Xia, Louise Deleger, and Imre Solti.
2011. Statistical machine translation for biomedical
text: are we there yet? In AMIA Annual Sympo-
sium Proceedings, volume 2011, page 1290. Ameri-
can Medical Informatics Association.
Antonio Jimeno Yepes,
?
Elise Prieur-Gaston, and
Aur?elie N?ev?eol. 2013. Combining medline and
publisher data to create parallel corpora for the auto-
matic translation of biomedical text. BMC bioinfor-
matics, 14(1):146.
116
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 636?644,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Can Recognising Multiword Expressions Improve Shallow Parsing?
Ioannis Korkontzelos, Suresh Manandhar
Department of Computer Science
The University of York
Heslington, York, YO10 5NG, UK
{johnkork, suresh}@cs.york.ac.uk
Abstract
There is significant evidence in the literature
that integrating knowledge about multiword
expressions can improve shallow parsing ac-
curacy. We present an experimental study to
quantify this improvement, focusing on com-
pound nominals, proper names and adjective-
noun constructions. The evaluation set of
multiword expressions is derived from Word-
Net and the textual data are downloaded from
the web. We use a classification method to
aid human annotation of output parses. This
method allows us to conduct experiments on
a large dataset of unannotated data. Experi-
ments show that knowledge about multiword
expressions leads to an increase of between
7.5% and 9.5% in accuracy of shallow pars-
ing in sentences containing these multiword
expressions.
1 Introduction
Multiword expressions are sequences of words that
tend to co-occur more frequently than chance and
are characterised by various levels of idiosyncracy
(Baldwin et al, 2003; Baldwin, 2006). There is ex-
tended literature on various issues relevant to mul-
tiword expression; recognition, classification, lexi-
cography, etc. (see Section 6). The vast majority of
these publications identifies as motivation for mul-
tiword expression research its potential contribution
to deep or shallow parsing. On the other side of this
issue, the state-of-the-art parsing systems seem to
ignore the fact that treating multiword expressions
as syntactic units would potentially increase parser?s
accuracy.
In this paper, we present an experimental study
attempting to estimate the contribution of integrat-
ing multiword expressions into shallow parsing. We
focus on multiword expressions that consist of two
successive tokens; in particular, compound nominals
proper names and adjective-noun constructions. We
also present a detailed classification method to aid
human annotation during the procedure of deciding
if a parse is correct or wrong. We present experi-
mental results about the different classes of changes
that occur in the parser output while unifying multi-
word expression components.
We conclude that treating known multiwords ex-
pressions as singletons leads to an increase of be-
tween 7.5% and 9.5% in accuracy of shallow pars-
ing of sentences containing these multiword expres-
sions. Increase percentages are higher for multiword
expressions that consist of an adjective followed by
a noun (12% to 15%); and even higher for non-
compositional multiword expressions1 that consist
of an adjective and a noun (15.5% to 19.5%).
The rest of the paper is structured as follows: In
Section 2 we present how multiword expressions can
be annotated in text and used by a shallow parser. In
Section 3 we present an overview of our experimen-
tal process. Section 4 explains how the set of target
multiword expressions and textual corpora were cre-
ated. In Section 5 we present and discuss the results
of the experimental process. In Section 6 we present
parts of the related literature. Section 7 concludes
the paper and proposes some future work.
1Compositionality is defined as the degree to which the
meaning of a multiword expression can be predicted by com-
bining the meanings of its components (Nunberg et al, 1994).
636
2 Annotating Multiword expressions
In this paper, we present a study to inspect the ex-
tent to which knowledge of multiword expressions
improves shallow parsing. Our approach focuses
on English multiword expressions that appear as se-
quences in text. In particular, we focus on com-
pound nominals (e.g. lemon tree), proper names
(e.g. prince Albert) and adjective-noun construc-
tions (e.g. red carpet).
Shallow or deep parsing should treat multiword
expression as units that cannot be divided in any
way. We replace the multiword expression tokens
with a special made up token, i.e. the multiword ex-
pression constituents joined with an underscore. For
example, we replace all occurrences of ?lemon tree?
with ?lemon tree?.
We choose to replace the multiword expression
words with a token that does not exist in the dictio-
nary of the part of speech tagger. This is quite an
important decision. Usually, a part of sheech tagger
assigns to an unknown words the part of speech that
best fits to it with respect to the parts of speech of
the words around it and the training data. This is a
desirable behaviour for our purposes.
The experimental results of our study quantify the
difference between the shallow parser output of a big
number of sentences after the replacement and the
shallow parser output of the same sentences before
the replacement. The comparison is done ignoring
changes of parts of speech, assigned by the part of
speech tagger.
3 Evaluation
The target of our experiment is to evaluate whether
replacing the multiword expression tokens with a
single token, unknown to the part of speech tagger,
improves shallow parsing accuracy. The ideal way
to perform this evaluation would be to use a cor-
pus with manual annotation about parsing and mul-
tiword expressions. Given this corpus we would be
able to measure the accuracy of a shallow (or deep)
parser before and after replacing multiword expres-
sions. However, to the best of our knowledge there
is no corpus available to include this type of annota-
tions in English.
Instead, there are two options: Firstly, we can
use treebank data, where manual parsing annotation
Figure 1: Evaluation process
is readily available, and manually annotate multi-
word expressions. The advantage of this approach
is that results are directly comparable with other re-
sults of the literature, due to the use of benchmark
data. Manual annotation of multiword expressions
is a very time- and effort-consuming process due to
the large size of most treebanks. Alternatively, mul-
tiword expression annotation could be done using a
method of recognition. Annotating the multiword
expressions that appear in WordNet could be a safe
decision, in terms of correctness, however, WordNet
is reported to have limited coverage of multiword
expressions (Baldwin, 2006; Laporte and Voyatzi,
2008). WordNet covers only 9.1 % and 16.1 % of the
datasets of Nicholson and Baldwin (2008) (484 noun
compounds) and Kim and Baldwin (2008) (2169
noun compounds), respectively.
Secondly, we can use a set of multiword expres-
sions as a starting point and then create corpora that
contain instances of these multiword expressions. In
succession, these sentences need to be manually an-
notated in terms of parsing, and this requires huge
human effort. Alternatively, we can parse the cor-
pora before and after replacing the multiword ex-
pression and then compare the parser output. This
is the evaluation procedure that we chose to follow,
and is shown in Figure 1.
The above procedure is only able to retrieve in-
stances where the replacement of the multiword ex-
pression leads to a different parsing, a different allo-
cation of tokens to phrases. It is not able to spot in-
stances where the parser output remains unchanged
after the replacement, no matter if they are correct.
Since we are interested in measuring if replacing
637
Example A - Replacement causes no change
Before: [NP they] [VP jumped] [PP over] [NP a
bonfire] and [VP rolled] [NP a fire wheel] .
After: [NP they] [VP jumped] [PP over] [NP a
bonfire] and [VP rolled] [NP a fire wheel] .
Example B - Replacement corrects an error
Before: [NP the blades] [VP ignited] and [NP he]
[VP threw] [NP the fire] wheel up
[PP into] [NP the air] .
After: [NP the blades] [VP ignited] and [NP he]
[VP threw] [NP the fire wheel] [PRT up]
[PP into] [NP the air] .
Table 1: 2 shallow parsing examples. Multiword expres-
sion: ?fire wheel?
multiword expressions with a single token improves
parsing accuracy, we are not interested in instances
that remain unchanged. We focus on instances that
changed; either they were corrected or they were
made wrong or they remain erroneous. For example,
the shallow parser output for example A in Table 1
did not change after the replacement. Example B in
Table 1 shows a sentence which was corrected after
the replacement.
Instead of manually annotating the sentences
whose parser output changed after the replacement
as corrected or not, we identify a number of change
classes under which we classify all these sentences.
In the following section, we present the change
classes. For each we thoroughly discuss whether
its form guarantees that its sentences are wrongly
parsed before the change and correctly parsed after
the change. In this case, the sentences of the corre-
sponding class should be counted as false positives.
We also discuss the opposite; if the form of each
change class guarantees that its sentences are cor-
rectly parsed before the change and wrongly parsed
after the change. In this case, the sentences of the
corresponding class should be counted as true nega-
tives. For this discussion we hypothesize that among
the possible output shallow parses for a given sen-
tence the correct one has (a) the smallest number
phrases, and (b) the smallest number of tokens not
assigned to any phrase.
3.1 Shallow parsing change classes
In this section, we present a classification of cases
where the shallow parser output of the sentence is
Figure 2: Change classes (following the notation of Bille
(2005)). Triangles denote phrases and uppercase bold let-
ters V...Z denote phrase labels. Lowercase letters k...n
denote parsing leaves. For change classes P2LMw and
L2PMw, X includes the multiword expression tokens.
For change classes P2L and L2P it does not. For change
class MwA, the multiword expression tokens are not as-
signed to the same phrase Y or Z.
different from the parser output of the same sen-
tence after replacing the multiword expression with
a single token. The secondary focus of this discus-
sion is to estimate whether the specific form of each
change class can lead to a safe conclusion about if
the parser output of the sentence under discussion:
(a) was wrong before the replacement and was then
corrected, (b) was correct before the replacement
and was then made wrong, or (c) was wrong before
the replacement and remained wrong. For this dis-
cussion, we refer to words that are not assigned to
any phase in the shallow parser output as ?leaves?.
Hypothesis: We base our analysis on the hypoth-
esis that among the possible output shallow parses
for a given sentence the correct one has (a) the small-
est number phrases, and (b) the smallest number of
leaves. The theoretical intuitions behind the hypoth-
esis are: (a) parse trees with just leaves are par-
tial parse trees and hence should not be preferred
over complete parse trees. (b) when mistaken parse
638
trees are generally larger (with more phrases). We
checked the hypothesis by manually annotating 80
randomly chosen instances; 10 for each change class
that is counted as correct or wrong (see Table 2). 74
instances validated the hypothesis (92.5%).
Table 2 shows one example for each change class.
Figure 2 presents the classes as transformations be-
tween trees, following the notation of Bille (2005).
Change class P2LMw (Phrase to Leaves includ-
ing the Multiword expression) Before replacing the
multiword expression sequence with a single to-
ken, the multiword expression is assigned to some
phrase, possibly together with other words. After
the replacement, the components of that phrase are
not assigned to any phrase, but instead as leaves.
Change class P2L (Phrase to Leaves excluding
the multiword expression) Similarly to change class
P2LMw, before the replacement, some successive
tokens excluding the multiword expression itself are
assigned to some phrase. After the replacement, the
components of that phrase appear as leaves.
Change class L2PMw (Leaves to Phrase includ-
ing the Multiword expression) The changes covered
by this class are the opposite changes of change class
P2LMw. Before the replacing the multiword expres-
sion sequence with a single token, the multiword ex-
pression sequence is not assign to any phrase possi-
bly among other words. After the replacement, the
multiword expression is assigned to a phrase.
Change class L2P (Leaves to Phrase excluding
the multiword expression) Similarly to change class
L2PMw, before the replacement, one or more suc-
cessive tokens excluding the multiword expression
itself appear as leaves. After the replacement, these
tokens are assigned to a phrase.
Change class PL2P (Phrases or Leaves to Phrase)
After the replacement, the tokens of more than one
phrases or leaves are assigned to a single phrase.
Change class P2PL (Phrase to Phrases or Leaves)
In contrast to change class PL2P, after the replace-
ment, the tokens of one phrase either are assigned to
more than one phrases or appear as leaves.
Change class PN (Phrase label Name) After re-
placing the multiword expression sequence with a
single token, one phrase appears with a different
phrase label, although it retains exactly the same
component tokens.
Change class PoS (Part of Speech) After replac-
ing the multiword expression sequence with a single
token, one or more tokens appears with a different
part of speech. This class of changes comes from the
part of speech tagger, and are out of the scope of this
study. Thus, in the results section we show a size es-
timate of this class, and then we present results about
change classes, ignoring change class PoS.
Change class P2P (Phrases to less Phrases) After
replacing the multiword expression sequence with a
single token, the component tokens of more than one
successive phrases? are assigned to a different set of
successive phrases ?. However, it is always the case
that phrases ? are less than phrases ? (|?| < |?|).
Change class MwA (Multiword expression
Allocation) Before replacing the multiword ex-
pression sequence, the multiword expression
constituents are assigned to different phrases.
The instances of change classes where the parser
output after the replacement has more parsing leaves
or phrases than before are counted towards sen-
tences that were parsed wrongly after the replace-
ment. For these classes, change classes P2LMw,
P2L and P2PL, most probably the parser output after
the replacement is wrong.
In contrast, the instances of change classes where
a sequence of tokens is assigned to a phrase, or many
phrases are merged are counted towards sentences
that were parsed wrongly before the replacement
and correctly after the replacement. These changes,
that are described by classes L2PMw, L2P, PL2P
and P2P, most probably describe improvements in
shallow parsing. The instances of change class MwA
are counted as correct after the replacement because
by definition all tokens of a multiword expression
are expected to be assigned to the same phrase.
The instances of change class PN can be either
correct or wrong after the replacement. For this rea-
son, we present our results as ranges (see Table 4).
The minimum value is computed when the instances
of class PN are counted as wrong after the replace-
ment. In contrast, the maximum value is computed
when the instances of this class are counted as cor-
rect after the replacement.
3.2 Shallow parsing complex change classes
During the inspection of instances where the shal-
low parser output before the replacement is dif-
639
P
2L
M
w B [NP the(DT) action(NN) officer(NN)] [NP logistic(JJ) course(NN)] [VP is(VBZ) designed(VBN) ] 7
[VP to(TO) educate(VB) ] and(CC) [VP train(VB)] [NP military(JJ) personnel(NNS)] ...
A the(DT) action officer(NN) [NP logistic(JJ) course(NN)] [VP is(VBZ) designed(VBN)]
[VP to(TO) educate(VB) ] and(CC) [VP train(VB)] [NP military(JJ) personnel(NNS)] ...
P
2L B ... [NP the(DT) action(NN) officer(NN)] [PP in(IN)] [NP armenia(NN)] [VP signed(VBN)] ... 7
A ... [NP the(DT) action officer(NN)] in(IN) [NP armenia(NN) ] [VP signed(VBN) ] ...
L
2P
M
w B ?(?) affirmative(JJ) action(NN) officer(NN) ?(?) [NP aao(NN)] [VP refers(VBZ)] [PP to(TO)]
X
[NP the(DT) regional(JJ) affirmative(JJ) action(NN) officer(NN)] or(CC) [NP director(NN)] ...
A ?(?) [NP affirmative(JJ) action officer(NN)] ?(?) [NP aao(NN)] [VP refers(VBZ)] [PP to(TO)]
[NP the(DT) regional(JJ) affirmative(JJ) action officer(NN)] or(CC) [NP director(NN)] ...
L
2P B [NP the(DT) action(NN) officer(NN) ] usually(RB) [VP delivers(VBZ)] ... X
A [NP the(DT) action officer(NN) ] [ADVP usually(RB)] [VP delivers(VBZ)] ...
P
L
2P
B ... [VP to(TO) immediately(RB) report(VB)] [NP the(DT) incident(NN)] [PP to(TO)] [NP the(DT)
X
equal(JJ) opportunity(NN)] and(CC) [NP affirmative(JJ) action(NN) officer(NN)] .(.)
A ... [VP to(TO) immediately(RB) report(VB)] [NP the(DT) incident(NN)] [PP to(TO)] [NP the(DT)
equal(JJ) opportunity(NN) and(CC) affirmative(JJ) action officer(NN)] .(.)
P
2P
L B ... [NP action(NN) officer(NN)] [VP shall(MD) prepare(VB) and(CC) transmit(VB)] ...
7
A ... [NP action officer(NN)] [VP shall(MD) prepare(VB)] and(CC) [VP transmit(VB)] ...
P
N B ... [NP an(DT) action(NN) officer(NN)] [SBAR for(IN)] [NP communications(NNS)] ... ?
A ... [NP an(DT) action officer(NN)] [PP for(IN)] [NP communications(NNS)] ...
Po
S B ... [NP security(NN) officer(NN)] or(CC) ?(?) [NP youth(JJ) action(NN) officer(NN) ] .(.) ?(?)
?
A ... [NP security(NN) officer(NN)] or(CC) ?(?) [NP youth(NN) action officer(NN)] .(.) ?(?)
P
2P
B ... ,(,) [PP as(IN)] [NP a(DT) past(JJ) action(NN) officer(NN)] and(CC) command(NN) and(CC)
X
control(NN) and(CC) [NP intelligence(NN) communications(NNS) inspector(NN)] ...
A ... ,(,) [PP as(IN)] [NP a(DT) past(JJ) action officer(NN) and(CC) command(NN) and(CC)
(control(NN) ] and(CC) [NP intelligence(NN) communications(NNS) inspector(NN)] ...
M
w
A B the(DT) campus(NN) affirmative(JJ) action(NN) [NP officer(NN)] [VP serves(VBZ)] ...
X
A [NP the(DT) campus(NN) affirmative(JJ) action officer(NN)] [VP serves(VBZ)]...
Table 2: Examples for change classes. Multiword expression: ?action officer?. Parts of speech appear within paren-
theses. ?B? stands for ?before? and ?A? for ?after? (multiword expression replacement). Xor 7 denote change classes
that count positively or negatively towards improving shallow parsing. ? denotes classes that are treated specially.
ferent from the shallow parser output after the re-
placement, we came across a number of instances
that were classified in more than one class of the
previous subsection. In other words, two or more
classes of change happened. For example, in a num-
ber of instances, before the replacement, the multi-
word expression constituents are assigned to differ-
ent phrases (change class MwA). After the replace-
ment, the tokens of more than one phrases are as-
signed to a single phrase (change class PL2P). These
instances consist new complex change classes and
are named as the sum of names of the participating
classes. The instances of the example above consist
the complex change class PL2P+MwA.
4 Target multiword expressions and
corpora collection
We created our set of target multiword expres-
sions using WordNet 3.0 (Miller, 1995). Out of its
52, 217 multiword expressions we randomly chose
120. Keeping the ones that consist of two tokens
resulted in the 118 expressions of Table 3. Manu-
ally inspecting these multiword expressions proved
that they are all compound nominals, proper names
or adjective-noun constructions. Each multiword
expression was manually tagged as compositional
or non-compositional, following the procedure de-
scribed in Korkontzelos and Manandhar (2009). Ta-
ble 3 shows the chosen multiword expressions to-
gether with information about their compositionality
and the parts of speech of their components.
640
Compositional Multiword expressions (Noun - Noun sequences)
action officer (3119) bile duct (21649) cartridge brass (479) field mushroom (789) fire wheel (480)
key word (3131) king snake (2002) labor camp (3275) life form (5301) oyster bed (1728)
pack rat (3443) palm reading (4428) paper chase (1115) paper gold (1297) paper tiger (1694)
picture palace (2231) pill pusher (924) pine knot (1026) potato bean (265) powder monkey (1438)
prison guard (4801) rat race (2556) road agent (1281) sea lion (9113) spin doctor (1267)
tea table (62) telephone service (9771) upland cotton (3235) vegetable sponge (806) winter sweet (460)
Non-Compositional Multiword expressions (Noun - Noun sequences)
agony aunt (751) air conditioner (24202) band aid (773) beach towel (1937) car battery (3726)
checker board (1280) corn whiskey (1862) corner kick (2882) cream sauce (1569) fire brigade (5005)
fish finger (1423) flight simulator (5955) honey cake (843) jazz band (6845) jet plane (1466)
laser beam (16716) lemon tree (3805) lip service (3388) love letter (3265) luggage van (964)
memory device (4230) monkey puzzle (1780) motor pool (3184) power cord (5553) prince Albert (2019)
sausage pizza (598) savoy cabbage (1320) surface fire (2607) torrey tree (10) touch screen (9654)
water snake (2649) water tank (5158) wood aster (456)
Compositional Multiword expressions (Adjective - Noun sequences)
basic color (2453) cardiac muscle (6472) closed chain (1422) common iguana (668) cubic meter (4746)
eastern pipistrel (128) graphic designer (8228) hard candy (2357) ill health (2055) kinetic theory (2934)
male parent (1729) medical report (3178) musical harmony (1109) mythical monster (770) red fox (10587)
relational adjective (279) parking brake (7199) petit juror (991) taxonomic category (1277) thick skin (1338)
toxic waste (7220) universal donor (1454) parenthesis-free notation (113)
Non-Compositional Multiword expressions (Adjective - Noun sequences)
black maria (930) dead end (5256) dutch oven (4582) golden trumpet (607) green light (5960)
high jump (4455) holding pattern (3622) joint chiefs (2865) living rock (985) magnetic head (2457)
missing link (5314) personal equation (873) personal magnetism (2869) petit four (1506) pink lady (1707)
pink shower (351) poor devil (1594) public eye (3231) quick time (2323) red devil (2043)
red dwarf (6526) red tape (2024) round window (1380) silent butler (332) small beer (2302)
small voice (4313) stocking stuffer (7486) sweet bay (1367) teddy boy (2413) think tank (4586)
Table 3: 118 multiword expressions randomly chosen from WordNet. The size of the respective corpus in sentences
appears within parentheses.
For each multiword expression we created a dif-
ferent corpus. Each consists of webtext snippets of
length 15 to 200 tokens in which the multiword ex-
pression appears. Snippets were collected follow-
ing Korkontzelos and Manandhar (2009). Given a
multiword expression, a set of queries is created:
All synonyms of the multiword expression extracted
from WordNet are collected2. The multiword ex-
pression is paired with each synonym to create a set
of queries. For each query, snippets are collected
by parsing the web-pages returned by Yahoo!. The
union of all snippets produces the multiword expres-
sion corpus.
In Table 3, the number of collected corpus sen-
tences for each multiword expression are shown
within parentheses. GENIA tagger (Tsuruoka et al,
2005) was used as part of speech tagger. SNoW-
based Shallow Parser (Munoz et al, 1999) was used
for shallow parsing.
2e.g. for ?red carpet?, corpora are collected for ?red carpet?
and ?carpet?. The synonyms of ?red carpet? are ?rug?, ?carpet?
and ?carpeting?.
5 Experimental results and discussion
The corpora collecting procedure of Section 4 re-
sulted in a corpus of 376, 007 sentences, each one
containing one or more multiword expressions. In
85, 527 sentences (22.75%), the shallow parser out-
put before the replacement is different than the shal-
low parser output after the replacement. 7.20% of
these change instances are due to one or more parts
of speech changes, and are classified to change class
PoS. In other words, in 7.20% of cases where there
is a difference between the shallow parses before
and after replacing the multiword expression tokens
there is one or more tokens that were assigned a dif-
ferent part of speech. However, excluding parts of
speech from the comparison, there is no other dif-
ference between the two parses.
The focus of this study is to quantify the effect
of unifying multiword expressions in shallow pars-
ing. Part of speech tagging is a component of our ap-
proach and parts of speech are not necessarily parts
of the parser output. For this reason, we chose to
ignore part of speech changes, the changes of class
PoS. Below, we discuss results for all other classes.
641
Multiword Shallow Parsing
expressions improvement
class PS sentences min. max.
On average - 376,007 7.47% 9.49%
Comp. N N 93,166 5.54% 7.19%
Non-Comp. N N 127,875 3.66% 4.44%
Comp. J N 68,707 7.34% 9.21%
Non-Comp. J N 86,259 15.32% 19.67%
- N N 221,041 4.45% 5.60%
J N 154,966 11.78% 15.03%
Comp. - 161,873 6.30% 8.05%
Non-Comp. - 214,134 8.36% 10.57%
Table 4: Summary of results. PS: parts of speech, Comp:
compositional, N: noun, J: adjective, min.: minimum,
max.: maximum.
Table 4 shows a summary of our results. The first
two columns describe classes of multiword expres-
sion with respect to compositionality and the parts
of speech of the component words. The first line ac-
counts for the average of all multiword expressions,
the second one for compositional multiword expres-
sions made of nouns, etc. The third column shows
the number of corpus sentences of each class.
For each one of the classes of Table 4, the fourth
and fifth columns show the minimum and maxi-
mum improvement in shallow parsing, respectively,
caused by unifying multiword expression tokens.
Let ?X? be the function that returns the number of
instances assigned to change class X . With respect
to the discussion of Subsection 3.1 about how the in-
stances of each class should be counted towards the
final results, the minimum and maximum improve-
ments in shallow parsing are:
min = ??P2LMw???P2L?+?L2PMw?+?L2P?+
+?PL2P???P2PL?+?PL2P+MwA?+
+?P2P?+?P2P+MwA???PN? (1)
max = ??P2LMw???P2L?+?L2PMw?+?L2P?+
+?PL2P???P2PL?+?PL2P+MwA?+
+?P2P?+?P2P+MwA?+?PN? (2)
On average of all multiword expressions, unify-
ing multiword expression tokens contributes from
7.47% to 9.49% in shallow parsing accuracy. It
should be noted that this improvement is reported
on sentences which contain at least one known mul-
tiword expression. To project this improvement on
any general text, one needs to know the percentage
of sentences that contain known multiword expres-
Figure 3: Average change percentages per change class.
sions. Then the projected improvement can be com-
puted by multiplying these two percentages.
Table 4 shows that the increase in shallow pars-
ing accuracy is lower for expressions that consist of
nouns than for those that consist of an adjective and
a noun. Moreover, the improvement is higher for
non-compositional expressions than compositional
ones. This is expected, due to the idiosyncratic na-
ture of non-compositional multiword expressions.
The highest improvement, 15.32% to 19.67%, oc-
curs for non-compositional multiword expressions
that consist of an adjective followed by a noun.
Figure 3 shows the percentage of each class over
the sum of sentences whose parse before unify-
ing multiword expression tokens is different for the
parse after the replacement. The most common
change class is PL2P. It contains sentences in the
shallow parser output of which many phrases or
leaves were all assigned to a single phrase. 34.03%
of the changes are classified in this class. The least
common classes are change classes P2L, L2PMw
and L2P. Each of these accounts for less than 3%
of the overall changes.
6 Related Work
There have been proposed several ways to clas-
sify multiword expressions according to various
properties such as compositionality and institution-
alisation3 (Moon, 1998; Sag et al, 2002; Bald-
win, 2006). There is a large variety of meth-
ods in the literature that address recognising mul-
tiword expressions or some subcategory. Mc-
Carthy (2006) divides multiword expression detect-
3Institutionalisation is the degree that a multiword expres-
sion is accepted as lexical item through consistent use over time.
642
ing methods into statistical (e.g. pointwise mutual
information (PMI)), translation-based, dictionary-
based, substitution-based, and distributional. Sta-
tistical methods score multiword expression candi-
dates based on co-occurrence counts (Manning and
Schutze, 1999; Dunning, 1993; Lin, 1999; Frantzi et
al., 2000). Translation-based methods usually take
advantage of alignment to discover potential multi-
word expressions (Venkatapathy and Joshi, 2005).
Other methods use dictionaries to reveal semantic
relationships between the components of potential
multiword expressions and their context (Baldwin
et al, 2003; Hashimoto et al, 2006). Substitution-
based methods decide for multiword expressions
by substituting their components with other similar
words and measuring their frequency of occurrence
(Lin, 1999; Fazly and Stevenson, 2006). These tech-
niques can be enriched with selectional preference
information (Van de Cruys and Moiro?n, 2007; Katz
and Giesbrecht, 2006). Fazly and Stevenson (2007)
propose measures for institutionalisation, syntactic
fixedness and compositionality based on the selec-
tional preferences of verbs. There are several studies
relevant to detecting compositionality of noun-noun,
verb-particle and light verb constructions and verb-
noun pairs (e.g. Katz and Giesbrecht (2006)).
To the best of our knowledge there are no ap-
proaches integrating multiword expression knowl-
edge in deep or shallow parsing. However, there
are several attempts to integrate other forms of lex-
ical semantics into parsing. Bikel (2000) merged
the Brown portion of the Penn Treebank with Sem-
Cor, and used it to evaluate a generative bilexical
model for joint word sense disambiguation and pars-
ing. Similarly, Agirre et al (Agirre et al, 2008)
integrated semantic information in the form of se-
mantic classes and observed significant improve-
ment in parsing and PP attachment tasks. Xiong et
al. (2005) integrated first-sense and hypernym fea-
tures in a generative parse model applied to the Chi-
nese Penn Treebank and achieved significant im-
provement over their baseline model. Fujita et
al. (2007) extended this work by implementing a
discriminative parse selection model, incorporating
word sense information and achieved great improve-
ments as well. Examples of integrating selectional
preference information into parsing are Dowding et
al. (1994) and Hektoen (1997).
7 Conlusion and future work
In this paper, we presented an experimental study
attempting to estimate the contribution of unify-
ing multiword expression components into shallow
parsing. The evaluation is done based on 118 multi-
word expressions extracted from WordNet 3.0. They
consist of two successive components and are in
particular, compound nominals, proper names or
adjective-noun constructions.
Instead of using pre-annotated text, we collected
sentences that contain the above multiword expres-
sions from the web. We applied shallow parsing be-
fore and after unifying multiword expression tokens
and compared the outputs. We presented a detailed
classification of changes in the shallow parser out-
put to aid human annotation during the procedure of
deciding if a parser output is correct or wrong.
We presented experimental results about change
classes and about the overall improvement of uni-
fying multiword expression tokens with respect to
compositionality and the parts of speech of their
components. We conclude that unifying the tokens
of known multiwords expressions leads to an in-
crease of between 7.5% and 9.5% in accuracy of
shallow parsing of sentences that contain these mul-
tiword expressions. Increase percentages are higher
on adjective-noun constructions (12% to 15%); and
even higher on non-compositional adjective-noun
constructions (15.5% to 19.5%).
Future work will focus in conducting similar ex-
periments for multiword expressions longer than
two words. One would expect that due to their
size, a wrong interpretation of their structure would
affect the shallow parser output more than it does
for multiword expressions consisting of two words.
Thus, unifying multiword expressions longer than
two words would potentially contribute more to
shallow parsing accuracy.
Furthermore, the evaluation results presented in
this paper could be strengthened by adding man-
ual multiword expression annotation to some tree-
bank. This would provide a way to avoid the change
class analysis presented in Subsection 3.1 and com-
pute statistics more accurately. Finally, the results of
this paper suggest that implementing a parser able
to recognise multiword expressions would be very
helpful towards high accuracy parsing.
643
References
E. Agirre, T. Baldwin, and D. Martinez. 2008. Improv-
ing parsing and PP attachment performance with sense
information. In Proceedings of ACL, pages 317?325,
USA. ACL.
T. Baldwin, C. Bannard, T. Tanaka, and D. Widdows.
2003. An empirical model of multiword expression
decomposability. In proceedings of the ACL workshop
on MWEs, pages 89?96, USA. ACL.
T. Baldwin. 2006. Compositionality and multiword ex-
pressions: Six of one, half a dozen of the other? In
proceedings of the ACL workshop on MWEs, Aus-
tralia. ACL.
D. Bikel. 2000. A statistical model for parsing and word-
sense disambiguation. In proceedings of the 2000
Joint SIGDAT conference: EMNLP/VLC, pages 155?
163, USA. ACL.
P. Bille. 2005. A survey on tree edit distance and re-
lated problems. Theoretical Computer Science, 337(1-
3):217?239.
J. Dowding, R. Moore, F. Andryt, and D. Moran.
1994. Interleaving syntax and semantics in an efficient
bottom-up parser. In proceedings of ACL, pages 110?
116, USA. ACL.
T. Dunning. 1993. Accurate methods for the statistics of
surprise and coincidence. Computational Linguistics,
19(1):61?74.
A. Fazly and S. Stevenson. 2006. Automatically con-
structing a lexicon of verb phrase idiomatic combina-
tions. In Proceedings of EACL, pages 337?344, Italy.
A. Fazly and S. Stevenson. 2007. Distinguishing sub-
types of multiword expressions using linguistically-
motivated statistical measures. In proceedings of the
ACL workshop on MWEs, pages 9?16, Czech Repub-
lic. ACL.
K. Frantzi, S. Ananiadou, and H. Mima. 2000. Auto-
matic recognition of multi-word terms: the c-value/nc-
value method. International Journal on Digital Li-
braries, 3(2):115?130.
S. Fujita, F. Bond, S. Oepen, and T. Tanaka. 2007. Ex-
ploiting semantic information for hpsg parse selection.
In proceedings of DeepLP, pages 25?32, USA. ACL.
C. Hashimoto, S. Sato, and T. Utsuro. 2006. Detecting
japanese idioms with a linguistically rich dictionary.
Language Resources and Evaluation, 40(3):243?252.
E. Hektoen. 1997. Probabilistic parse selection based
on semantic cooccurrences. In proceedings of IWPT,
pages 113?122, USA.
G. Katz and E. Giesbrecht. 2006. Automatic identi-
fication of non-compositional multi-word expressions
using latent semantic analysis. In proceedings of the
ACL workshop on MWEs, pages 12?19, Australia.
ACL.
S. Kim and T. Baldwin. 2008. Standardised evaluation
of english noun compound interpretation. In proceed-
ings of the LREC workshop on MWEs, pages 39?42,
Morocco.
I. Korkontzelos and S. Manandhar. 2009. Detecting
compositionality in multi-word expressions. In pro-
ceedings of ACL-IJCNLP, Singapore.
E. Laporte and S. Voyatzi. 2008. An Electronic Dictio-
nary of French Multiword Adverbs. In proceedings of
the LREC workshop on MWEs, pages 31?34, Marocco.
D. Lin. 1999. Automatic identification of non-
compositional phrases. In proceedings of ACL, pages
317?324, USA. ACL.
C. Manning and H. Schutze, 1999. Foundations of Sta-
tistical NLP, Collocations, chapter 5. MIT Press.
D. McCarthy. 2006. Automatic methods to detect
the compositionality of MWEs. presentation slides.
url: www.sunum.org/myfiles/B2/McCarthyCollocId-
ioms06.ppt last accessed: 28/11/2009.
G. Miller. 1995. Wordnet: a lexical database for english.
Communications of the ACM, 38(11):39?41.
R. Moon. 1998. Fixed Expressions and Idioms in En-
glish. A Corpus-based Approach. Oxford: Clarendon
Press.
M. Munoz, V. Punyakanok, D. Roth, and D. Zimak.
1999. A learning approach to shallow parsing. In pro-
ceedings of EMNLP/VLC, pages 168?178, USA.
J. Nicholson and T. Baldwin. 2008. Interpreting com-
pound nominalisations. In proceedings of the LREC
workshop on MWEs, pages 43?45, Morocco.
G. Nunberg, T. Wasow, and I. Sag. 1994. Idioms. Lan-
guage, 70(3):491?539.
I. Sag, T. Baldwin, F. Bond, A. Copestake, and
D. Flickinger. 2002. Multiword expressions: A pain
in the neck for nlp. In proceedings of CICLing, pages
1?15, Mexico.
Y. Tsuruoka, Y. Tateishi, J. Kim, T. Ohta, J. McNaught,
S. Ananiadou, and J. Tsujii. 2005. Developing a ro-
bust part-of-speech tagger for biomedical text. Ad-
vances in Informatics, pages 382?392.
T. Van de Cruys and B. Moiro?n. 2007. Semantics-based
multiword expression extraction. In proceedings of the
ACL workshop on MWEs, pages 25?32, Czech Repub-
lic. ACL.
S. Venkatapathy and A. Joshi. 2005. Measuring the rela-
tive compositionality of verb-noun (V-N) collocations
by integrating features. In proceedings of HLT, pages
899?906, USA. ACL.
D. Xiong, S. Li, Q. Liu, S. Lin, and Y. Qian. 2005. Pars-
ing the penn chinese treebank with semantic knowl-
edge. In proceedings of IJCNLP, pages 70?81, Korea.
644
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 43?48,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Extending an interoperable platform to facilitate the creation
of multilingual and multimodal NLP applications
Georgios Kontonatsios?, Paul Thompson?, Riza Theresa Batista-Navarro?,
Claudiu Miha?ila??, Ioannis Korkontzelos and Sophia Ananiadou
The National Centre for Text Mining,
School of Computer Science, The University of Manchester
131 Princess Street, Manchester M1 7DN, UK
{kontonag,batistar,thompsop,mihailac,
korkonti,ananiads}@cs.man.ac.uk
Abstract
U-Compare is a UIMA-based workflow
construction platform for building natu-
ral language processing (NLP) applica-
tions from heterogeneous language re-
sources (LRs), without the need for pro-
gramming skills. U-Compare has been
adopted within the context of the META-
NET Network of Excellence, and over
40 LRs that process 15 European lan-
guages have been added to the U-Compare
component library. In line with META-
NET?s aims of increasing communication
between citizens of different European
countries, U-Compare has been extended
to facilitate the development of a wider
range of applications, including both mul-
tilingual and multimodal workflows. The
enhancements exploit the UIMA Subject
of Analysis (Sofa) mechanism, that allows
different facets of the input data to be rep-
resented. We demonstrate how our cus-
tomised extensions to U-Compare allow
the construction and testing of NLP appli-
cations that transform the input data in dif-
ferent ways, e.g., machine translation, au-
tomatic summarisation and text-to-speech.
1 Introduction
Currently, there are many repositories that con-
tain a range of NLP components, e.g., OpenNLP1,
Stanford CoreNLP2, JULIE NLP Toolsuite3 and
NaCTeM software tools4. The ability to chain
components from these repositories into pipelines
is a prerequisite to facilitate the development of
?The authors have contributed equally to the development
of this work and production of the manuscript.
1http://opennlp.sourceforge.net/projects.html
2http://nlp.stanford.edu/software/corenlp.shtml
3http://www.julielab.de/Resources/Software/NLP Tools.html
4http://nactem.ac.uk/software.php
complex NLP applications. Combining together
heterogeneous components is not, however, al-
ways straightforward. The various components
used in a pipeline may be implemented using dif-
ferent programming languages, may have incom-
patible input/output formats, e.g., stand-off or in-
line annotations, or may require or produce incom-
patible data types, e.g., a particular named entity
recogniser (NER) may require specific types of
syntactic constituents as input, making it impor-
tant to choose the right type of syntactic parser to
run prior to the NER. Thus, the tools required to
build a new application may not be interoperable
with each other, and considerable extra work may
be required to make the tools talk to each other.
The Unstructured Information Management Ar-
chitecture (UIMA) (Ferrucci and Lally, 2004) was
created as a means to alleviate such problems. It
is a framework that facilitates the straightforward
combination of LRs, i.e., tools and corpora, into
workflow applications. UIMA is an OASIS stan-
dard that enables interoperability of LRs by defin-
ing a standard workflow metadata format and stan-
dard input/output representations.
U-Compare (Kano et al, 2011) is a graphical
NLP workflow construction platform built on top
of UIMA. It facilitates the rapid construction, test-
ing and evaluation of NLP workflows using drag-
and-drop actions within its graphical user inter-
face (GUI). U-Compare enhances interoperabil-
ity among UIMA-compliant LRs, by defining a
common and sharable Type System, i.e., a hier-
archy of annotation types, which models a wide
range of NLP data types, e.g., sentence, token,
part-of-speech tag, named entity and discourse
annotations. The aim is for all components in
U-Compare?s library to be compliant with this
type system. In the context of META-NET, U-
Compare?s library has been extended with 46 new
LRs supporting 15 European languages, all of
which are compliant with the same type system.
43
This makes U-Compare the world?s largest repos-
itory of type system-compatible LRs, allowing
users to seamlessly combine together resources to
create a range of NLP applications.
Previously, U-Compare was able to support the
development of a wide range of monolingual lex-
ical, syntactic and semantic processing tasks ap-
plications that enriched textual input documents
by adding annotations of various types. However,
not all NLP applications operate in this way; some
workflows transform the input data to create new
?views? of the input data. The META-NET project
aims to ensure equal access to information by all
European citizens. This aim implies the devel-
opment of both multilingual applications, which
transform input data from one language into an-
other, or multimodal applications, in which text
may be transformed into speech, or vice versa.
U-Compare has been extended in several ways
to support the construction of these more complex
workflow types. Specifically, information about
both the original and transformed data, together
with annotations associated with each view, can
now be visualised in a straightforward manner.
The changes support two new categories of work-
flow. Firstly, workflows that produce two or more
textual views of an input text are useful not only
for multilingual applications, such as those that
carry out machine translation, but also applica-
tions that transform the input text in other ways,
such as those that produce a summary of an in-
put text. Secondly, workflows that output audio as
well as textual views, e.g., text-to-speech applica-
tions, are also supported.
2 Related work
Over the past few years, an increasing num-
bers of researchers have begun to create and dis-
tribute their own workflow construction architec-
tures (Ferrucci and Lally, 2004; Cunningham et
al., 2002; Grishman et al, 1997; Scha?fer, 2006)
or platforms (Kano et al, 2011; Rak et al, 2012;
Ogrodniczuk and Karagiozov, 2011; Savova et al,
2010) that allow the rapid development of NLP ap-
plications.
GATE (Cunningham et al, 2002) is a workflow
construction framework that has been used to de-
velop several types of NLP applications, including
summarisation systems. It facilitates the develop-
ment of a wide range of NLP applications by pro-
viding a collection of components that can process
various languages, together with Java libraries that
handle character encoding for approximately 100
languages. However, GATE does not formally de-
fine any standards to model multilingual or mul-
timodal applications, but rather aims to boost the
development process of NLP applications.
TIPSTER (Grishman et al, 1997) is a generic
framework for the development of NLP applica-
tions. TIPSTER provides multilingual function-
alities by associating text segments of a paral-
lel document with one or more languages. This
allows language-dependent NLP components to
process only the appropriate mono-lingual sub-
documents. However, TIPSTER does not provide
explicit guidelines regarding the annotation types
and attributes that are produced by components.
This lack of a common and sharable system of
annotation types discourages interoperability be-
tween LRs. However, TIPSTER does not provide
a mechanism that facilitates the development of
multilingual or multimodal NLP applications.
Heart of Gold (Scha?fer, 2006) is an XML-
based workflow construction architecture that en-
ables interoperability of tools developed in dif-
ferent programming languages to be combined
into pipelines. Heart of Gold contains a rich li-
brary of shallow and deep parsing components
supporting several languages, e.g., English, Ger-
man, Japanese and Greek. Nonetheless, Heart of
Gold does not specifically support the construction
of multilingual or multimodal workflows.
In contrast to the other frameworks introduced
above, UIMA (Ferrucci and Lally, 2004) provides
an abstract-level mechanism that can be used to
support the development of workflows that carry
out transformations of the input data. This mech-
anism is called the Subject of Analysis or Sofa.
Multiple Sofas can be linked with an input file,
each of which stores different data and associ-
ated annotations. This mechanism can thus be ex-
ploited to represent alternative ?views? of the in-
put data, such as a source text and its translation.
The data stored in different Sofas is not restricted
to textual information; it can also correspond to
other modalities, such as audio data. This makes
the Sofa mechanism equally suitable for storing
the output of text-to-speech workflows. Our ex-
tensions to U-Compare are thus implemented by
reading and displaying the contents of different
types of Sofas.
The Sofa mechanism has previously been
44
under-exploited by UIMA developers, despite its
power in allowing more complex NLP workflows
to be constructed. Indeed, no other existing
UIMA-based platform (Kano et al, 2011; Rak et
al., 2012; Savova et al, 2010; Hahn et al, 2008)
has demonstrated the use of Sofas to construct
multilingual or multimodal applications. Thus, to
our knowledge, our enhancements to U-Compare
constitute the first attempt to make the construc-
tion of workflows that carry out transformations of
input data more readily available to UIMA users,
without the need for programming skills.
3 METANET4U Components in
U-Compare
The two dozen national and many regional lan-
guages of Europe present linguistic barriers that
can severely limit the free flow of goods, infor-
mation and services. The META-NET Network
of Excellence was created to respond to this is-
sue. Consisting of 60 research centres from 34
countries, META-NET has aimed to stimulate a
concerted, substantial and continent-wide effort to
push forward language technology research and
engineering, in order to ensure equal access to
information and knowledge for all European cit-
izens.
META-NET?s aims are dependent on the ready
availability of LRs that can carry out NLP and
text mining (TM) on a range of European lan-
guages. Such resources constitute the building
blocks for constructing language technology ap-
plications that can help European citizens to gain
easy access to the information they require. One
of the major outcomes of META-NET has been
the development of META-SHARE, an open, dis-
tributed facility for sharing and exchange of LRs
in a large number of European languages.
Within the context of META-NET, interoper-
ability of LRs is clearly of utmost importance, to
expedite the process of developing new NLP ap-
plications. In order to provide a concrete demon-
stration of the utility and power of promoting in-
teroperability within META-SHARE, one of the
sub-projects of META-NET, i.e., METANET4U,
has carried out a pilot study on interoperability,
making use of the UIMA framework and the U-
Compare platform. It is in this context that a set
of 46 new LRs, available in META-SHARE, were
wrapped as UIMA components and made avail-
able in U-Compare. Of these components, 37 op-
erate on one or more specific languages other than
English and 4 are language-independent. Table 1
shows the full set of categories of UIMA com-
ponents created during the METANET4U project,
together with the languages supported.
Several of these new components output mul-
tiple Sofas, i.e., two machine translation compo-
nents, two automatic summarisation components
and a text-to-speech component. It is hoped that
our U-Compare extensions will help to stimulate
the development of a greater number of related
UIMA components, and thus promote a new level
of complexity for future UIMA workflows.
Component Function Supported Languages
Language Identifier 54 modern languages
Paragraph breaker pt, mt
Sentence splitter en, pt ,mt, es, ca, ast,cy, gl, it
Tokeniser en, pt, mt, es, ca, ast,cy, gl, it, fr
Morph. Analyser en, pt, es, ca, ast,cy, gl, it, ro, eu, fr
POS Tagger en, es, ca, cy, gl, it,pt, ro, eu, fr, mt
Syntactic chunker en, es, ca, gl,ast, ro, fr
NP chunker ro
Segmenter ro, en
FDG Parser ro
Dependency Parser en, es, ca, gl, ast
Discourse Parser ro
NER Languageindependent
Summariser ro, en
Machine translation es?{gl,pt,ca}en?es, eu?es
Table 1: METANET4U UIMA components
4 Enhancements to U-Compare
In UIMA, an artefact, i.e., raw text, audio, im-
age, video, and its annotations, e.g., part-of-
speech tags, are represented in a standard format,
namely the Common Analysis Structure (CAS).
A CAS can contain any number of smaller sub-
CASes, i.e., Sofas, that carry different artefacts
with their linked annotations. Figure 1 illustrates
the different types of Sofas that are created by the
three types of workflows that we will demonstrate.
Firstly, for a machine translation workflow, at least
45
Multi-lingualMulti-modalWorkflowsDocuments aZ ??
CAS
?CASCAS
SOFA SOFA
SOFA SOFA
SOFA SOFA
Figure 1: UIMA based multilingual and multi-
modal workflow architecture
two CAS views, i.e., Sofas, are created, the first
corresponding to the text in the source language,
and the other Sofas corresponding to the transla-
tion(s) of the source text into target language(s).
The second type of workflow, i.e., automatic sum-
marisation, is related to the former workflow, in
that the two Sofas produced by the workflow are
both textual, one containing the input text and one
containing a summary of the original text. The
third type of workflow is different, in that a Sofa
containing audio data is used to represent the out-
put of a multimodal workflow.
Two specific extensions have been made to U-
Compare to handle both textual and audio So-
fas. When the output of a workflow consists of
multiple textual views (Sofas), the default anno-
tation viewer is automatically split to allow mul-
tiple views of the text to be displayed and side-
by-side. This can be useful, e.g., to allow careful
comparison of a source text and target translation
in a machine translation workflow. To handle au-
dio Sofas, we have developed a new, customised
viewer that can visualise and play audio data. The
visualisation consists of a graphical display of the
waveform, power information and spectrogram, as
well as segmentation of the audio data into re-
gions (such as individual tokens) and transcrip-
tions, if such information is present in the audio
Sofa. The viewer makes use the open-source li-
brary Java Speech Toolkit (JSTK)5.
5 Workflow applications
In order to provide a practical demonstration of
the enhanced capabilities of U-Compare, we show
5http://code.google.com/p/jstk
three different workflows that transform the input
data in different ways, namely translation, auto-
matic summarisation and speech synthesis. In this
section, we provide brief details of these work-
flows.
5.1 Machine translation
The University of Manchester has created UIMA
wrapper components corresponding to different
modules of Apertium (Corb??-Bellot et al, 2005), a
free rule-based machine translation engine. These
components consist of a morphological analyser,
POS tagger and translator. The three components
must be run in sequence to carry out translation,
although the first two components can be used
in other workflows to carry out monolingual
analyses. The UIMA components currently
handle a subset of the 27 languages dealt with
by the complete Apertium system, corresponding
to the languages of the METANET4U partners,
i.e., English?Spanish, Galician?Spanish,
Portuguese?Spanish, Catalan?Spanish and
Basque?Spanish. However, additional language
pairs can be added straightforwardly. Our sample
workflow includes as its initial component the
Language Identifier from the Romanian Academy
Research Institute for Artificial Intelligence
(RACAI), to automatically detect the language of
the text in the input Sofa. The subsequent compo-
nents in the workflow are the Apertium modules.
The workflow demonstrates how heterogeneous
components from different research groups can
be combined into workflows to create new NLP
applications. A sample output from running the
workflow is shown in Figure 2. The input text
was detected as English by the RACAI Language
Identifier. The English text was subsequently
analysed by the morphological analyser and POS
Tagger, and translated to Spanish by the translator.
Figure 2 illustrates the side-by-side display of the
contents of the two Sofas.
5.2 Automatic summarisation
Automatic summarisation for Romanian text can
be carried out by creating a workflow consisting
of two components developed by the Universitatea
?Alexandru Ioan Cuza? din Ias?i (UAIC). Firstly,
a segmenter (UAICSeg) splits the input text into
fragments, which are in turn used as input to the
summariser component (UAICSum). The length
of the output summary (percentage of the whole
document) is parameterised. As can be seen in
46
Figure 2: Translation of English text to Spanish
Figure 3: Summarisation of Romanian text
Figure 3, the output of this workflow is displayed
using the same parallel Sofa viewer. In this case,
the full text is displayed in the left-hand pane and
the summary is shown in the right-hand pane.
5.3 Speech synthesis
The Universitat Polite`cnica de Catalunya (UPC)
developed a speech synthesiser component that
is based around their Ogmios text-to-speech sys-
tem (Bonafonte et al, 2006). The UIMA com-
ponent version of this tool generates separate text
and audio Sofas; the former stores the textual to-
kens and textual representations of their pronun-
ciations, whilst the latter stores the start and end
time offsets of each of the tokens in the audio file,
together with their transcriptions. Fig. 4 shows
how the textual Sofa information is displayed in
U-Compare?s default annotation viewer, whilst the
audio Sofa information is shown in the new au-
dio visualiser mentioned above. The three differ-
ent types of visual information are displayed be-
low each other, and the segments (tokens) of the
audio file, together with their transcriptions, are
displayed at the bottom of the window. A ?Play?
button allows either the complete file or a selected
segment to be played.
6 Conclusions
The requirements of META-NET have motivated
several new enhancements to the U-Compare plat-
form, which, to our knowledge, make it the first
UIMA-based workflow construction platform that
is fully geared towards the development of NLP
applications that support a wide range of European
languages. The 46 new UIMA-wrapped LRs that
have been made available through U-Compare,
supporting 15 different European languages and
all compliant with the same type system, mean
that the improved U-Compare is essentially a hub
of multilingual resources, which can be freely and
flexibly combined to create new workflows. In
47
Figure 4: Speech Synthesis
addition, our enhancements to U-Compare mean
that various types of multilingual and multimodal
workflows can now be created with the minimum
effort. These enhancements are intended to make
U-Compare more attractive to users, and to help
stimulate the development of a new generation of
more complex UIMA-based NLP applications. As
future work, we intend to extend the library of
components that output multiple Sofas, and further
extend the functionalities of U-Compare to handle
other data modalities, e.g., video.
Acknowledgements
This work was partially funded by the Euro-
pean Community?s Seventh Framework Program
(FP7/2007-2013) [grant number 318736 (OSS-
METER)]; MetaNet4U project (ICT PSP Pro-
gramme) [grant number 270893]; and Engineer-
ing and Physical Sciences Research Council [grant
numbers EP/P505631/1, EP/J50032X/1].
References
A. Bonafonte, P. Agu?ero, J. Adell, J. Pe?rez, and
A. Moreno. 2006. Ogmios: The upc text-to-speech
synthesis system for spoken translation. In TC-
STAR Workshop on Speech-to-Speech Translation,
pages 199?204.
A. Corb??-Bellot, M. Forcada, S. Ortiz-Rojas, J. Pe?rez-
Ortiz, G. Ram??rez-Sa?nchez, F. Sa?nchez-Mart??nez,
I. Alegria, A. Mayor, and K. Sarasola. 2005.
An open-source shallow-transfer machine transla-
tion engine for the romance languages of Spain. In
Proceedings of the 10th Conference of the EAMT,
pages 79?86.
H. Cunningham, D. Maynard, K. Bontcheva, and
V. Tablan. 2002. GATE: an architecture for devel-
opment of robust HLT applications.
D. Ferrucci and A. Lally. 2004. Building an ex-
ample application with the unstructured information
management architecture. IBM Systems Journal,
43(3):455?475.
R. Grishman, B. Caid, J. Callan, J. Conley, H. Corbin,
J. Cowie, K. DiBella, P. Jacobs, M. Mettler, B. Og-
den, et al 1997. TIPSTER text phase ii architecture
design version 2.1 p 19 june 1996.
U. Hahn, E. Buyko, R. Landefeld, M. Mu?hlhausen,
M. Poprat, K. Tomanek, and J. Wermter. 2008. An
overview of JCoRe, the JULIE lab UIMA compo-
nent repository. In LREC?08 Workshop ?Towards
Enhanced Interoperability for Large HLT Systems:
UIMA for NLP?, pages 1?7, Marrakech, Morocco,
May.
Y. Kano, M. Miwa, K. Cohen, L. Hunter, S. Ananiadou,
and J. Tsujii. 2011. U-compare: A modular nlp
workflow construction and evaluation system. IBM
Journal of Research and Development, 55(3):11.
M. Ogrodniczuk and D. Karagiozov. 2011. Atlas - the
multilingual language processing platform. Proce-
samiento de Lenguaje Natural, 47(0):241?248.
R. Rak, A. Rowley, W. Black, and S. Ananiadou.
2012. Argo: an integrative, interactive, text mining-
based workbench supporting curation. Database:
The Journal of Biological Databases and Curation,
2012.
G. Savova, J. Masanz, P. Ogren, J. Zheng, S. Sohn,
K. Kipper-Schuler, and C. Chute. 2010. Mayo clin-
ical text analysis and knowledge extraction system
(ctakes): architecture, component evaluation and ap-
plications. Journal of the American Medical Infor-
matics Association, 17(5):507?513.
U. Scha?fer. 2006. Middleware for creating and com-
bining multi-dimensional nlp markup. In Proceed-
ings of the 5th Workshop on NLP and XML: Multi-
Dimensional Markup in Natural Language Process-
ing, pages 81?84. ACL.
48
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 355?358,
Uppsala, Sweden, 15-16 July 2010.
c
?2010 Association for Computational Linguistics
UoY: Graphs of Unambiguous Vertices
for Word Sense Induction and Disambiguation
Ioannis Korkontzelos, Suresh Manandhar
Department of Computer Science
The University of York
Heslington, York, YO10 5NG, UK
{johnkork, suresh}@cs.york.ac.uk
Abstract
This paper presents an unsupervised
graph-based method for automatic word
sense induction and disambiguation. The
innovative part of our method is the as-
signment of either a word or a word pair
to each vertex of the constructed graph.
Word senses are induced by clustering the
constructed graph. In the disambiguation
stage, each induced cluster is scored ac-
cording to the number of its vertices found
in the context of the target word. Our sys-
tem participated in SemEval-2010 word
sense induction and disambiguation task.
1 Introduction
There exists significant evidence that word sense
disambiguation is important for a variety of nat-
ural language processing tasks: machine transla-
tion, information retrieval, grammatical analysis,
speech and text processing (Veronis, 2004). How-
ever, the ?fixed-list? of senses paradigm, where the
senses of a target word is a closed list of defini-
tions coming from a standard dictionary (Agirre
et al, 2006), was long ago abandoned. The rea-
son is that sense lists, such as WordNet (Miller,
1995), miss many senses, especially domain-
specific ones (Pantel and Lin, 2002). The miss-
ing concepts are not recognised. Moreover, senses
cannot be easily related to their use in context.
Word sense induction methods can be divided
into vector-space models and graph based ones.
In a vector-space model, each context of a target
word is represented as a feature vector, e.g. fre-
quency of cooccurring words (Katz and Gies-
brecht, 2006). Context vectors are clustered and
the resulting clusters represent the induced senses.
Recently, graph-based methods have been em-
ployed for word sense induction (Agirre and
Soroa, 2007). Typically, graph-based methods
represent each context word of the target word as
a vertex. Two vertices are connected via an edge
if they cooccur in one or more instances. Once
the cooccurrence graph has been constructed, dif-
ferent graph clustering algorithms are applied to
partition the graph. Each cluster (partition) con-
sists of a set of words that are semantically related
to the particular sense (Veronis, 2004). The poten-
tial advantage of graph-based methods is that they
can combine both local and global cooccurrence
information (Agirre et al, 2006).
Klapaftis and Manandhar (2008) presented a
graph-based approach that represents pairs of
words as vertices instead of single words. They
claimed that single words might appear with more
than one senses of the target word, while they hy-
pothesize that a pair of words is unambiguous.
Hard-clustering the graph will potentially identify
less conflating senses of the target word.
In this paper, we relax the above hypothesis be-
cause in some cases a single word is unambiguous.
We present a method that generates two-word ver-
tices only when a single word vertex is unambigu-
ous. If the word is judged as unambiguous, then it
is represented as a single-word vertex. Otherwise,
it is represented as a pair-of-words vertex.
The approach of Klapaftis and Manandhar
(2008) achieved good results in both evaluation
settings of the SemEval-2007 task. A test in-
stance is disambiguated towards one of the in-
duced senses if one or more pairs of words rep-
resenting that sense cooccur in the test instance.
This creates a sparsity problem, because a cooc-
currence of two words is generally less likely than
the occurrence of a single word. We expect our ap-
proach to address the data sparsity problem with-
out conflating the induced senses.
2 Word Sense Induction
In this section we present our word sense in-
duction and disambiguation algorithms. Figure
355
1 shows an example showing how the sense in-
duction algorithm works: The left side of part
I shows the context nouns of four snippets con-
taining the target noun ?chip?. The most rele-
vant of these nouns are represented as single word
vertices (part II). Note that ?customer? was not
judged to be significantly relevant. In addition,
the system introduced several vertices represent-
ing pairs of nouns. For example, note the vertex
?company potato?. The set of sentences contain-
ing the context word ?company? was judged as
very different from the set of sentences contain-
ing ?company? and ?potato?. Thus, our system
hypothesizes that probably ?company? and ?com-
pany potato? are relevant to different senses of
?chip?, and allows them to be clustered accord-
ingly. Vertices whose content nouns or pairs of
nouns cooccur in some snippet are connected with
an edge (part III and right side of part I). Edge
weights depend upon the conditional probabilities
of the occurrence frequencies of the vertex con-
tents in a large corpus, e.g. w
2,6
in part III. Hard-
clustering the graph produces the induced senses
of ?chip?: (a) potato crisp, and (b) microchip.
In the following subsections, the system is de-
scribed in detail. Figure 2 shows a block diagram
overview of the sense induction system. It consists
of three main components: (a) corpus preprocess-
ing, (b) graph construction, and (c) clustering.
In a number of different stages, the system uses
a reference corpus to count occurrences of word
or word pairs. It is chosen to be large because fre-
quencies of words in a large corpus are more sig-
nificant statistically. Ideally we would use the web
or another large repository, but for the purposes of
the SemEval-2010 task we used the union of all
snippets of all target words.
2.1 Corpus Preprocessing
Corpus preprocessing aims to capture words that
are contextually related to the target word. Ini-
tially, all snippets
1
that contain the target word are
lemmatised and PoS tagged using the GENIA tag-
ger
2
. Words that occur in a stoplist are filtered out.
Instead of using all words as context, only nouns
are kept, since they are more discriminative than
verbs, adverbs and adjectives, that appear in a va-
riety of different contexts.
1
We refer to instances of the target word as snippets, since
they can be either sentences or paragraphs.
2
www-tsujii.is.s.u-tokyo.ac.jp/GENIA/tagger
Figure 1: An example showing how the proposed
word sense induction system works.
Nouns that occur infrequently in the reference
corpus are removed (parameter P
1
). Then, log-
likelihood ratio (LL) (Dunning, 1993) is em-
ployed to compare the distribution of each noun
to its distribution in reference corpus. The null
hypothesis is that the two distributions are simi-
lar. If this is true, LL is small value and the cor-
responding noun is removed (parameter P
2
). We
also filter out nouns that are more indicative in the
reference corpus than in the target word corpus;
i.e. the nouns whose relative frequency in the for-
mer is larger than in the latter. At the end of this
stage, each snippet is a list of lemmatised nouns
contextually related to the target word.
2.2 Constructing the Graph
All nouns appearing in the list of the previous
stage output are represented as graph vertices.
Moreover, some vertices representing pairs of
nouns are added. Each noun within a snippet is
combined with every other, generating
(
n
2
)
pairs.
Log-likelihood filtering with respect to the refer-
ence corpus is used to filter out unimportant pairs.
Thereafter, we aim to keep only pairs that might
refer to a different sense of the target word than
their component nouns. For each pair we construct
a vector containing the snippet IDs in which they
occur. Similarly we construct a vector for each
component noun. We discard a pair if its vector is
very similar to both the vectors of its component
nouns, otherwise we represent it as a vertex pair.
Dice coefficient was used as a similarity measure
and parameter P
4
as threshold value.
Edges are drawn based on cooccurrence of the
corresponding vertices contents in one or more
snippets. Edges whose respective vertices con-
tents are infrequent are rejected. The weight ap-
356
Figure 2: A: Block diagram presenting the system overview. B, C, D: Block diagrams further analysing
the structure of complex components of A. Parameter names appear within square brackets.
plied to each edge is the maximum of the condi-
tional probabilities of the corresponding vertices
contents (e.g. w
2,6
, part III, figure 1). Low weight
edges are filtered out (parameter P
3
).
2.3 Clustering the Graph
Chinese Whispers (CW) (Biemann, 2006) was
used to cluster the graph. CW is a randomised
graph-clustering algorithm, time-linear to the
number of edges. The number of clusters it pro-
duces is automatically inferred. Evaluation has
shown that CW suits well in sense induction appli-
cations, where class distributions are often highly
skewed. In our experiments, CW produced less
clusters using a constant mutation rate (5%).
To further reduce the number of induced clus-
ters, we applied a post-processing stage, which
exploits the one sense per collocation property
(Yarowsky, 1995). For each cluster l
i
, we gener-
ated the set S
i
of all snippets that contain at least
one vertex content of l
i
. Then, any clusters l
a
and
l
b
were merged if S
a
? S
b
or S
a
? S
b
.
3 Word Sense Disambiguation
The induced senses are used to sense-tag each test
instance of the target word (snippet). Given a snip-
pet, each induced cluster is assigned a score equal
to the number of its vertex contents (single or pairs
of words) occurring in the snippet. The instance is
assigned to the sense with the highest score or with
equal weights to all highest scoring senses.
4 Tuning parameter and inducing senses
The algorithm depends upon 4 parameters: P
1
thresholds frequencies and P
3
collocation weights.
P
2
is the LL threshold and P
4
the similarity thresh-
old for discarding pair-of-nouns vertices.
We chose P
1
? {5, 10, 15}, P
2
?
{2, 3, 4, 5, 10, 15, 25, 35}, P
3
? {0.2, 0.3, 0.4}
and P
4
? {0.2, 0.4, 0.6, 0.8}. The parameter tun-
ing was done using the trial data of the SemEval-
2010 task and on the noun data of correspond-
ing SemEval-2007 task. Parameters were tuned
by choosing the maximum supervised recall. For
both data sets, the chosen parameter values were
P
1
? 10, P
3
? 0.4 and P
4
? 0.8. Due to the
size difference of the datasets, for the Semeval-
2010 trial data P
2
? 3, while for the SemEval-
2007 noun data P
2
? 10. The latter was adopted
because the size of training data was announced to
be large. We induced senses on the training data
and then disambiguated the test data instances.
5 Evaluation results
Three different measures, V-Measure, F-Score,
and supervised recall on word sense disambigua-
tion task, were used for evaluation. V-Measure
and F-Score are unsupervised. Supervised recall
was measured on two different data splits. Table 1
shows the performance of our system, UoY, for all
measures and in comparison with the best, worst
and average performing system and the random
and most frequent sense (MFS) baselines. Results
are shown for all words, and nouns and verbs only.
357
System V-Msr F-Sc S-R
80
S-R
60
A
l
l
UoY 15.70 49.76 62.44 61.96
Best 16.20 63.31 62.44 61.96
Worst 0.00 16.10 18.72 18.91
Average 6.36 48.72 54.95 54.27
MFS 0.00 63.40 58.67 58.25
Random 4.40 31.92 57.25 56.52
N
o
u
n
s
UoY 20.60 38.23 59.43 58.62
Best 20.60 57.10 59.43 58.62
Average 7.08 44.42 47.85 46.90
Worst 0.00 15.80 1.55 1.52
MFS 0.00 57.00 53.22 52.45
Random 4.20 30.40 51.45 50.21
V
e
r
b
s
UoY 8.50 66.55 66.82 66.82
Best 15.60 72.40 69.06 68.59
Average 5.95 54.23 65.25 65.00
Worst 0.10 16.40 43.76 44.23
MFS 0.00 72.70 66.63 66.70
Random 4.64 34.10 65.69 65.73
Table 1: Summary of results (%). V-Msr: V-
Measure, F-Sc: F-Score, S-R
X
: Supervised recall
under data split: X% training, (100-X)% test
Table 2 shows the ranks of UoY for all evalu-
ation categories. Our system was generally very
highly ranked. It outperformed the random base-
line in all cases and the MFS baseline in measures
but F-Score. No participant system managed to
achive higher F-Score than the MFS baseline.
The main disadvantage of the system seems to
be the large number of induced senses. The rea-
sons are data sparcity and tuning on nouns, that
might have led to parameters that induce more
senses. However, the system performs best among
systems that produce comparable numbers of clus-
ters. Table 3 shows the number of senses of UoY
and the gold-standard. UoY produces significantly
more senses than the gold-standard, especially for
nouns, while for verbs figures are similar.
The system achieves low F-Scores, because this
measure favours fewer induced senses. Moreover,
we observe that most scores are lower for verbs
than nouns. This is probably because parameters
are tuned on nouns and because in general nouns
appear with more senses than verbs, allowing our
system to adapt better. As an overall conclusion,
each evaluation measure is more or less biased to-
wards small or large numbers of induced senses.
6 Conclusion
We presented a graph-based approach for word
sense induction and disambiguation. Our ap-
proach represents as a graph vertex an unambigu-
ous unit: (a) a single word, if it is judged as unam-
biguous, or (b) a pair of words, otherwise. Graph
edges model the cooccurrences of the content of
V-Msr F-Sc S-R
80
S-R
60
All 2 15 1 1
Nouns|Verbs 1|3 18|6 1|16 1|15
Table 2: Ranks of UoY (out of 26 systems)
All Nouns Verbs
Gold-standard 3.79 4.46 3.12
UoY 11.54 17.32 5.76
Table 3: Number of senses
the vertices that they join. Hard-clustering the
graph induces a set of senses. To disambiguate
a test instance, we assign it to the induced sense
whose vertices contents occur mostly in the in-
stance. Results show that our system achieves very
high recall and V-measure performance, higher
than both baselines. It achieves low F-Scores due
to the large number of induced senses.
References
E. Agirre and A. Soroa. 2007. Semeval-2007 task 02:
Evaluating word sense induction and discrimination
systems. In proceedings of SemEval-2007, Czech
Republic. ACL.
E. Agirre, D. Martinez, O. Lopez de Lacalle, and
A. Soroa. 2006. Two graph-based algorithms for
state-of-the-art wsd. In proceedings of EMNLP,
Sydney, Australia. ACL.
C. Biemann. 2006. Chinese whispers - an efficient
graph clustering algorithm and its application to nat-
ural language processing problems. In proceedings
of TextGraphs, New York City. ACL.
T. Dunning. 1993. Accurate methods for the statistics
of surprise and coincidence. Computational Lin-
guistics, 19(1):61?74.
G. Katz and E. Giesbrecht. 2006. Automatic identifi-
cation of non-compositional multi-word expressions
using latent semantic analysis. In proceedings of the
ACL workshop on Multi-Word Expressions, Sydney,
Australia. ACL.
I. Klapaftis and S. Manandhar. 2008. Word sense in-
duction using graphs of collocations. In proceedings
of ECAI-2008, Patras, Greece.
G. Miller. 1995. Wordnet: a lexical database for en-
glish. Communications of the ACM, 38(11):39?41.
P. Pantel and D. Lin. 2002. Discovering word senses
from text. In proceedings of KDD-2002, New York,
NY, USA. ACM Press.
J. Veronis. 2004. Hyperlex: lexical cartography for in-
formation retrieval. Computer Speech & Language,
18(3):223?252, July.
D. Yarowsky. 1995. Unsupervised word sense disam-
biguation rivaling supervised methods. In proceed-
ings of ACL, Cambridge, MA, USA. ACL.
358
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 39?47, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
SemEval-2013 Task 5: Evaluating Phrasal Semantics
Ioannis Korkontzelos
National Centre for Text Mining
School of Computer Science
University of Manchester, UK
ioannis.korkontzelos@man.ac.uk
Torsten Zesch
UKP Lab, CompSci Dept.
Technische Universita?t Darmstadt
Germany
zesch@ukp.informatik.tu-darmstadt.de
Fabio Massimo Zanzotto
Department of Enterprise Engineering
University of Rome ?Tor Vergata?
Italy
zanzotto@info.uniroma2.it
Chris Biemann
FG Language Technology, CompSci Dept.
Technische Universita?t Darmstadt
Germany
biem@cs.tu-darmstadt.de
Abstract
This paper describes the SemEval-2013 Task
5: ?Evaluating Phrasal Semantics?. Its first
subtask is about computing the semantic simi-
larity of words and compositional phrases of
minimal length. The second one addresses
deciding the compositionality of phrases in a
given context. The paper discusses the impor-
tance and background of these subtasks and
their structure. In succession, it introduces the
systems that participated and discusses evalu-
ation results.
1 Introduction
Numerous past tasks have focused on leveraging the
meaning of word types or words in context. Exam-
ples of the former are noun categorization and the
TOEFL test, examples of the latter are word sense
disambiguation, metonymy resolution, and lexical
substitution. As these tasks have enjoyed a lot suc-
cess, a natural progression is the pursuit of models
that can perform similar tasks taking into account
multiword expressions and complex compositional
structure. In this paper, we present two subtasks de-
signed to evaluate such phrasal models:
a. Semantic similarity of words and compositional
phrases
b. Evaluating the compositionality of phrases in
context
For example, the first subtask addresses computing
how similar the word ?valuation? is to the compo-
sitional sequence ?price assessment?, while the sec-
ond subtask addresses deciding whether the phrase
?piece of cake? is used literally or figuratively in the
sentence ?Labour was a piece of cake!?.
The aim of these subtasks is two-fold. Firstly,
considering that there is a spread interest lately in
phrasal semantics in its various guises, they provide
an opportunity to draw together approaches to nu-
merous related problems under a common evalua-
tion set. It is intended that after the competition,
the evaluation setting and the datasets will comprise
an on-going benchmark for the evaluation of these
phrasal models.
Secondly, the subtasks attempt to bridge the
gap between established lexical semantics and full-
blown linguistic inference. Thus, we anticipate that
they will stimulate an increased interest around the
general issue of phrasal semantics. We use the no-
tion of phrasal semantics here as opposed to lexi-
cal compounds or compositional semantics. Bridg-
ing the gap between lexical semantics and linguis-
tic inference could provoke novel approaches to cer-
tain established tasks, such as lexical entailment and
paraphrase identification. In addition, it could ul-
39
timately lead to improvements in a wide range of
applications in natural language processing, such
as document retrieval, clustering and classification,
question answering, query expansion, synonym ex-
traction, relation extraction, automatic translation,
or textual advertisement matching in search engines,
all of which depend on phrasal semantics.
The remainder of this paper is structured as fol-
lows: Section 2 presents details about the data
sources and the variety of sources applicable to the
task. Section 3 discusses the first subtask, which
is about semantic similarity of words and compo-
sitional phrases. In subsection 3.1 the subtask is
described in detail together with some information
about its background. Subsection 3.2 discusses the
data creation process and subsection 3.3 discusses
the participating systems and their results. Section 4
introduces the second subtask, which is about eval-
uating the compositionality of phrases in context.
Subsection 4.1 explains the data creation process for
this subtask. In subsection 4.2 the evaluation statis-
tics of participating systems are presented. Section
5 is a discussion about the conclusions of the entire
task. Finally, in section 6 we summarize this presen-
tation and discuss briefly our vision about challenges
in distributional semantics.
2 Data Sources & Methodology
Data instances of both subtasks are drawn from the
large-scale, freely available WaCky corpora (Baroni
et al, 2009). The resource contains corpora in 4 lan-
guages: English, French, German and Italian. The
English corpus, ukWaC, consists of 2 billion words
and was constructed by crawling to the .uk domain
of the web and using medium-frequency words from
the BNC as seeds. The corpus is part-of-speech
(PoS) tagged and lemmatized using the TreeTagger
(Schmid, 1994). The French corpus, frWaC, con-
tains 1.6 billion word corpus and was constructed
by web-crawling the .fr domain and using medium-
frequency words from the Le Monde Diplomatique
corpus and basic French vocabulary lists as seeds.
The corpus was PoS tagged and lemmatized with
the TreeTagger. The French corpus, deWaC, con-
sists of 1.7 billion word corpus and was constructed
by crawling the .de domain and using medium-
frequency words from the SudDeutsche Zeitung cor-
pus and basic German vocabulary lists as seeds. The
corpus was PoS tagged and lemmatized with the
TreeTagger. The Italian corpus, itWaC, is a 2 billion
word corpus constructed from the .it domain of the
web using medium-frequency words from the Re-
pubblica corpus and basic Italian vocabulary lists as
seeds. The corpus was PoS tagged with the Tree-
Tagger, and lemmatized using the Morph-it! lexicon
(Zanchetta and Baroni, 2005). Several versions of
the WaCky corpora, with various extra annotations
or modifications are also available1.
We ensured that data instances occur frequently
enough in the WaCky corpora, so that participat-
ing systems could gather statistics for building dis-
tributional vectors or other uses. As the evalua-
tion data only contains very small annotated sam-
ples from freely available web documents, and the
original source is provided, we could provide them
without violating copyrights.
The size of the WaCky corpora is suitable for
training reliable distributional models. Sentences
are already lemmatized and part-of-speech tagged.
Participating approaches making use of distribu-
tional methods, part-of-speech tags or lemmas, were
strongly encouraged to use these corpora and their
shared preprocessing, to ensure the highest possi-
ble comparability of results. Additionally, this had
the potential to considerably reduce the workload of
participants. For the first subtask, data were pro-
vided in English, German and Italian and for the sec-
ond subtask in English and German.
The range of methods applicable to both subtasks
was deliberately not limited to any specific branch of
methods, such as distributional or vector models of
semantic compositionality. We believe that the sub-
tasks can be tackled from different directions and we
expect a great deal of the scientific benefit to lie in
the comparison of very different approaches, as well
as how these approaches can be combined. An ex-
ception to this rule is the fact that participants in the
first subtask were not allowed to use directly defini-
tions extracted from dictionaries or lexicons. Since
the subtask is considered fundamental and its data
were created from online knowledge resources, sys-
tems using the same tools to address it would be of
limited use. However, participants were allowed to
1WaCky website: wacky.sslmit.unibo.it
40
use other information residing in dictionaries, such
as Wordnet synsets or synset relations.
Participating systems were allowed to attempt one
or both subtasks, in one or all of the languages sup-
ported. However, it was expected that systems per-
forming well at the first basic subtask would pro-
vide a good starting point for dealing with the sec-
ond subtask, which is considered harder. Moreover,
language-independent models were of special inter-
est.
3 Subtask 5a: Semantic Similarity of
Words and Compositional Phrases
The aim of this subtask is to evaluate the compo-
nent of a semantic model that computes the simi-
larity between word sequences of different length.
Participating systems are asked to estimate the se-
mantic similarity of a word and a short sequence of
two words. For example, they should be able to fig-
ure out that contact and close interaction are similar
whereas megalomania and great madness are not.
This subtask addresses a core problem, since sat-
isfactory performance in computing the similarity of
full sentences depends on similarity computations
on shorter sequences.
3.1 Background and Description
This subtask is based on the assumption that we
first need a basic set of functions to compose the
meaning of two words, in order to construct more
complex models that compositionally determine the
meaning of sentences, as a second step. For compo-
sitional distributional semantics, the need for these
basic functions is discussed in Mitchell and Lapata
(2008). Since then, many models have been pro-
posed for addressing the task (Mitchell and Lapata,
2010; Baroni and Zamparelli, 2010; Guevara, 2010),
but still comparative analysis is in general based on
comparing sequences that consist of two words.
As in Zanzotto et al (2010), this subtask proposes
to compare the similarity of a 2-word sequence and
a single word. This is important as it is the basic
step to analyse models that can compare any word
sequences of different length.
The development and testing set for this subtask
were built based on the idea described in Zanzotto
et al (2010). Dictionaries were used as sources of
contact/[kon-takt]
1. the act or state of touching;
a touching or meeting, as of
two things or people.
2. close interaction
3. an acquaintance, colleague,
or relative through whom a
person can gain access to
information, favors, influ-
ential people, and the like.
Figure 1: The definition of contact in a sample dictionary
positive training examples. Dictionaries are natural
repositories of equivalences between words under
definition and sequences of words used for defining
them. Figure 1 presents the definition of the word
contact, from which the pair (contact, close interac-
tion) can be extracted. Such equivalences extracted
from dictionaries can be seen as natural and unbi-
ased data instances. This idea opens numerous op-
portunities:
? Since definitions in dictionaries are syntacti-
cally rich, we are able to create examples for
different syntactic relations.
? We have the opportunity to extract positive ex-
amples for languages for which dictionaries
with sufficient entries are available.
Negative examples were generated by matching
words under definition with randomly chosen defin-
ing sequences. In the following subsection, we pro-
vide details about the application of this idea to build
the development and testing set for subtask 5a.
3.2 Data Creation
Data for this subtask were provided in English, Ger-
man and Italian. Pairs of words under definitions and
defining sequences were extracted from the English,
German and Italian part of Wiktionary, respectively.
In particular, for each language, all Wiktionary en-
tries were downloaded and part-of-speech tagged us-
ing the Genia tagger (Tsuruoka et al, 2005). In
succession, definitions that start with noun phrases
41
Language Train set Test set Total
English 5,861 3,907 9,768
German 1,516 1,010 2,526
Italian 1,275 850 2,125
German - no names 1,101 733 1,834
Table 1: Quantitative characteristics of the datasets
were kept, only. For the purpose of extracting word
and sequence pairs for this subtask, we consider as
noun phrases, sequences that consist of adjectives
or noun and end with a noun. In cases where the
extracted noun phrase was longer than two words,
the right-most two sequences were kept, since in
most cases noun phrases are governed by their right-
most component. Subsequently, we discarded in-
stances whose words occur too infrequently in the
WaCky corpora (Baroni et al, 2009) of each lan-
guage. WaCky corpora are available freely and are
large enough for participating systems to extract dis-
tributional statistics. Taking the numbers of ex-
tracted instances into account, we set the frequency
thresholds at 10 occurrences for English and 5 for
German and Italian.
Data instances extracted following this process
were then checked by a computational linguist. Can-
didate pairs in which the definition sequence was not
judged to be a precise and adequate definition of the
word under definition were discarded. These cases
were very limited and mostly account for shortcom-
ings of the very simple pattern used for extraction.
For example, the pair (standard, transmission vehi-
cle) coming from the definition of ?standard? as ?A
manual transmission vehicle? was discarded. Simi-
larly in German, the pair (Fremde (Eng. stranger),
weibliche Person (Eng. female person)) was dis-
carded. ?Fremde?, which is of female grammat-
ical genre, was defined as ?weibliche Person, die
man nicht kennt (Eng. female person, one does not
know)?. In Italian, the pair (paese (Eng. land, coun-
try, region), grande estensione (Eng. large tract))
was discarded, since the original definition was
?grande estensione di terreno abitato e generalmente
coltivato (Eng. large tract of land inhabited and cul-
tivated in general)?.
The final data sets were divided into training and
held-out testing sets, according to a 60% and 40%
ratio, respectively. The first three rows of table 1
present the numbers of the train and test sets for the
three languages chosen. It was identified that a fair
percentage of the German instances (approximately
27%) refer to the definitions of first names or family
names. This is probably a flaw of the German part of
Wiktionary. In addition, the pattern used for extrac-
tion happens to apply to the definitions of names.
Name instances were discarded from the German
data set to produce the data set described in the last
row of table 1.
The training set was released approximately 3
months earlier than the test data. Instances in both
set ware annotated as positive or negative. Test set
annotations were not released to the participants, but
were used for evaluation, only.
3.3 Results
Participating systems were evaluated on their ability
to predict correctly whether the components of each
test instance, i.e. word-sequence pair, are semanti-
cally similar or distinct. Participants were allowed
to use or ignore the training data, i.e. the systems
could be supervised or unsupervised. Unsupervised
systems were allowed to use the training data for de-
velopment and parameter tuning. Since this is a core
task, participating systems were not be able to use
dictionaries or other prefabricated lists. Instead, they
were allowed to use distributional similarity models,
selectional preferences, measures of semantic simi-
larity etc.
Participating system responses were scored in
terms of standard information retrieval measures:
accuracy (A), precision (P), recall (R) and F1 score
(Radev et al, 2003). Systems were encouraged to
submit at most 3 solutions for each language, but
submissions for fewer languages were accepted.
Five research teams participated. Ten system runs
were submitted for English, one for German (on data
set: German - no names) and one for Italian. Table 2
illustrates the results of the evaluation process. The
teams of (HsH) (Wartena, 2013), CLaC (Siblini and
Kosseim, 2013), UMCC DLSI-(EPS) (Da?vila et al,
2013), and ITNLP, the Harbin Institute of Technol-
ogy, approached the task in a supervised way, while
MELODI (Van de Cruys et al, 2013) participated
with two unsupervised approaches. Interestingly,
42
Language Rank Participant Id run Id A R P rej. R rej. P F1
1 HsH 1 .803 .752 .837 .854 .775 .792
3 CLaC 3 .794 .707 .856 .881 .750 .774
2 CLaC 2 .794 .695 .867 .893 .745 .771
4 CLaC 1 .788 .638 .910 .937 .721 .750
English 5 MELODI lvw .748 .614 .838 .882 .695 .709
6 UMCC DLSI-(EPS) 1 .724 .613 .787 .834 .683 .689
7 ITNLP 3 .703 .501 .840 .904 .645 .628
8 MELODI dm .689 .481 .825 .898 .634 .608
9 ITNLP 1 .663 .392 .857 .934 .606 .538
10 ITNLP 2 .659 .427 .797 .891 .609 .556
German 1 HsH 1 .825 .765 .870 .885 .790 .814
Italian 1 UMCC DLSI-(EPS) 1 .675 .576 .718 .774 .646 .640
Table 2: Task 5a: Evaluation results. A, P, R, rej. and F1 stand for accuracy, precision, recall, rejection and F1 score,
respectively.
these approaches performed better than some super-
vised ones for this experiment. Below, we sum-
marise the properties of participating systems.
(HsH) (Wartena, 2013) used distributed similarity
and especially random indexing to compute similar-
ities between words and possible definitions, under
the hypothesis that a word and its definition are dis-
tributionally more similar than a word and an arbi-
trary definition. Considering all open-class words,
context vectors over the entire WaCky corpus were
computed for the word under definition, the defining
sequence, its component words separately, the ad-
dition and multiplication of the vectors of the com-
ponent words and a general context vector. Then,
various similarity measures were computed on the
vectors, including an innovative length-normalised
version of Jensen-Shannon divergence. The similar-
ity values are used to train a Support Vector Machine
(SVM) classifier (Cortes and Vapnik, 1995).
The first approach (run 1) of CLaC (Siblini and
Kosseim, 2013) is based on a weighted semantic
network to measure semantic relatedness between
the word and the components of the phrase. A
PART classifier is used to generate a partial decision
trained on the semantic relatedness information of
the labelled training set. The second approach uses
a supervised distributional method based on words
frequently occurring in the Web1TB corpus to cal-
culate relatedness. A JRip classifier is used to gen-
erate rules trained on the semantic relatedness infor-
mation of the training set. This approach was used
in conjunction with the first one as a backup method
(run 2). In addition, features generated by both ap-
proaches were used to train the JRIP classifier col-
lectively (run 3).
The first approach of MELODI (Van de Cruys
et al, 2013), called lvw, uses a dependency-based
vector space model computed over the ukWaC cor-
pus, in combination with Latent Vector Weighting
(Van de Cruys et al, 2011). The system computes
the similarity between the first noun and the head
noun of the second phrase, which was weighted ac-
cording to the semantics of the modifier. The second
approach, called dm, used a dependency-based vec-
tor space model, but, unlike the first approach, disre-
garded the modifier in the defining sequence. Since
both systems are unsupervised, the training data was
used to train a similarity threshold parameter, only.
UMCC DLSI-(EPS) (Da?vila et al, 2013) locates
the synsets of words in data instances and computes
the semantic distances between each synset of the
word under definition and each synsets of the defin-
ing sequence words. In succession, a classifier is
trained using features based on distance and Word-
Net relations.
The first attempt of ITNLP (run 1) consisted of an
SVM classifier trained on semantic similarity com-
putations between the word under definition and
43
the defining sequence in each instance. Their sec-
ond attempt also uses an SVM, however trained on
WordNet-based similarities. The third attempt of
ITNLP is a combination of the previous two; it com-
bines their features to train an SVM classifier.
4 Subtask 5b: Semantic Compositionality
in Context
An interesting sub-problem of semantic composi-
tionality is to decide whether a target phrase is used
in its literal or figurative meaning in a given con-
text. For example ?big picture? might be used lit-
erally as in Click here for a bigger picture or figura-
tively as in To solve this problem, you have to look at
the bigger picture. Another example is ?old school?
which can also be used literally or figuratively: He
will go down in history as one of the old school, a
true gentlemen. vs. During the 1970?s the hall of the
old school was converted into the library.
Being able to detect whether a phrase is used lit-
erally or figuratively is e.g. especially important for
information retrieval, where figuratively used words
should be treated separately to avoid false positives.
For example, the example sentence He will go down
in history as one of the old school, a true gentle-
men. should probably not be retrieved for the query
?school?. Rather, the insights generated from sub-
task 5a could be utilized to retrieve sentences using
a similar phrase such as ?gentleman-like behavior?.
The task may also be of interest to the related re-
search fields of metaphor detection and idiom iden-
tification.
There were no restrictions regarding the array of
methods, and the kind of resources that could be
employed for this task. In particular, participants
were allowed to make use of pre-fabricated lists of
phrases annotated with their probability of being
used figuratively from publicly available sources, or
to produce these lists from corpora. Assessing how
well the phrase suits its context might be tackled
using e.g. measures of semantic relatedness as well
as distributional models learned from the underlying
corpus.
Participants of this subtask were provided with
real usage examples of target phrases. For each us-
age example, the task is to make a binary decision
whether the target phrase is used literally or figu-
ratively in this context. Systems were tested in two
different disciplines: a known phrases task where all
target phrases in the test set were contained in the
training, and an unknown phrases setting, where all
target phrases in the test set were unseen.
4.1 Data Creation
The first step in creating the corpus was to compile
a list of phrases that can be used either literally or
metaphorically. Thus, we created an initial list of
several thousand English idioms from Wiktionary by
listing all entries under the category ENGLISH ID-
IOMS using the JWKTL Wiktionary API (Zesch et
al., 2008). We manually filtered the list removing
most idioms that are very unlikely to be ever used
literally (anymore), e.g. to knock on heaven?s door.
For each of the resulting list of phrases, we extracted
usage contexts from the ukWaC corpus (Baroni et
al., 2009). Each usage context contains 5 sentences,
where the sentence with the target phrase appears in
a randomized position. Due to segmentation errors,
some usage contexts actually might contain less than
5 sentences, but we manually filtered all usage con-
texts where the remaining context was insufficient.
This was done in the final cleaning step where we
also manually removed (near) duplicates, obvious
spam, encoding problems etc.
The target phrases in context were annotated for
figurative, literal, both or impossible to tell usage,
using the CrowdFlower2 crowdsourcing annotation
platform. We used about 8% of items as ?gold?
items for quality assurance, and had each example
annotated by three crowdworkers. The task was
comparably easy for crowdworkers, who reached
90%-94% pairwise agreement, and 95% success on
the gold items. About 5% of items with low agree-
ment and marked as impossible were removed. Ta-
ble 3 summarizes the quantitative characteristics of
all datasets resulting from this process. We took care
in sampling the data as to keep similar distributions
across the training, development and testing parts.
4.2 Results
Training and development datasets were made avail-
able in advance, test data was provided during the
evaluation period without labels. System perfor-
2www.crowdflower.com
44
Task Dataset # Phrases # Items Items per phrase # Liter. # Figur. # Both
known
train 10 1,424 68?188 702 719 3
dev 10 358 17?47 176 181 1
test 10 594 28?78 294 299 1
unseen
train 31 1,114 4?75 458 653 3
dev 9 342 4?74 141 200 1
test 15 518 8?73 198 319 1
Table 3: Quantitative characteristics of the datasets
Rank System Run Accuracy
1 IIRG 3 .779
2 UNAL 2 .754
3 UNAL 1 .722
5 IIRG 1 .530
4 Baseline MFC - .503
6 IIRG 2 .502
Table 4: Task 5b: Evaluation results for the known
phrases setting
Rank System Run Accuracy
1 UNAL 1 .668
2 UNAL 2 .645
3 Baseline MFC - .616
4 CLaC 1 .550
Table 5: Task 5b: Evaluation results for the unseen
phrases setting
mance was measured in accuracy. Since all partic-
ipants provided classifications for all test items, the
accuracy score is equivalent to precision/recall/F1.
Participants were allowed to enter up to three dif-
ferent runs for evaluation. We also provide baseline
accuracy scores, which are obtained by always as-
signing the most frequent class (figurative).
Table 4 provides the evaluation results for the
known phrases task, while Table 5 ranks participants
for the unseen phrases task. As expected, the un-
seen phrases setting is much harder than the known
phrases setting, as for unseen phrases it is not possi-
ble to learn lexicalised contextual clues. In both set-
tings, the winning entries were able to beat the MFC
baseline. While performance in the known phrases
setting is close to 80% and thus acceptable, the gen-
eral task of recognizing the literal or figurative use of
unseen phrases remains very challenging, with only
a small improvement over the baseline. We refer to
the system descriptions for more details on the tech-
niques used for this subtask: UNAL (Jimenez et al,
2013), IIRG (Byrne et al, 2013) and CLaC (Siblini
and Kosseim, 2013).
5 Task Conclusions
In this section, we further discuss the findings and
conclusion of the evaluation challenge in the task of
?Phrasal Semantics?.
Looking at the results of both subtasks, one ob-
serves that the maximum performance achieved is
higher for the first than the second subtask. For
this comparison to be fair, trivial baselines should be
taken into account. A system randomly assigning an
output value would be on average 50% correct in the
first subtask, since the numbers of positive and neg-
ative instances in the testing set are equal. Similarly,
a system assigning the most frequent class, i.e. the
figurative use of any phrase, would be 50.3% and
61.6% accurate in the second subtask for seen and
unseen test instances, respectively. It should also be
noted that the testing instances in the first subtask
are unseen in the respective training set. As a result,
in terms of baselines, the second subtask on unseen
data (Table 5) should be considered easier than the
first subtask (Table 2). However, the best perform-
ing systems achieved much higher accuracy in the
first than in the second subtask. This contradiction
confirms our conception that the first subtask is less
complex than the second.
In the first subtask, it is evident that no method
performs much better or much worse than the others.
45
Although the participating systems have employed a
wide variety of approaches and tools, the difference
between the best and worst accuracy achieved is
relatively limited, in particular approximately 14%.
Even more interestingly, unsupervised approaches
performed better than some supervised ones. This
observation suggests that no ?golden recipe? has
been identified so far for this task. Thus, probably
different processing tools take advantage of different
sources of information. It is a matter of future re-
search to identify these sources and the correspond-
ing tools, and then develop hybrid methods of im-
proved performance.
In the second subtask, the results of evaluation
on known phrases are much higher than on unseen
phrases. This was expected, as for unseen phrases it
is not possible to learn lexicalised contextual clues.
Thus, the second subtask has succeeded in identify-
ing the complexity threshold up to which the cur-
rent state-of-the-art can address the computational
problem. Further than this threshold, i.e. for unseen
phrases, current systems have not yet succeeded in
addressing it. In conclusion, the difficulty in eval-
uating the compositionality of previously unseen
phrases in context highlights the overall complexity
of the second subtask.
6 Summary and Future Work
In this paper we have presented the 5th task of Se-
mEval 2013, ?Evaluating Phrasal Semantics?, which
consists of two subtasks: (1) semantic similarity of
words and compositional phrases, and (2) compo-
sitionality of phrases in context. The former sub-
task, which focussed on the first step of composing
the meaning of phrases of any length, is less com-
plex than the latter subtask, which considers the ef-
fect of context to the semantics of a phrase. The
paper presents details about the background and im-
portance of these subtasks, the data creation process,
the systems that took part in the evaluation and their
results.
In the future, we expect evaluation challenges on
phrasal semantics to progress towards two direc-
tions: (a) the synthesis of semantics of sequences
longer than two words, and (b) aiming to improve
the performance of systems that determine the com-
positionality of previously unseen phrases in con-
text. The evaluation results of the first task sug-
gest that state-of-the-art systems can compose the
semantics of two word sequences with a promising
level of success. However, this task should be seen
as the first step towards composing the semantics
of sentence-long sequences. As far as subtask 5b
is concerned, the accuracy achieved by the partici-
pating systems on unseen testing data was low, only
slightly better than the most frequent class baseline,
which assigns the figurative use to all test phrases.
Thus, the subtask cannot be considered well ad-
dressed by the state-of-the-art and further progress
should be sought.
Acknowledgements
The work relevant to subtask 5a described in this pa-
per is funded by the European Community?s Seventh
Framework Program (FP7/2007-2013) under grant
agreement no. 318736 (OSSMETER).
We would like to thank Tristan Miller for help-
ing with the subtleties of English idiomatic ex-
pressions, and Eugenie Giesbrecht for support
in the organization of subtask 5b. This work
has been supported by the Volkswagen Founda-
tion as part of the Lichtenberg-Professorship Pro-
gram under grant No. I/82806, and by the Hes-
sian research excellence program Landes-Offensive
zur Entwicklung Wissenschaftlich-o?konomischer
Exzellenz (LOEWE) as part of the research center
Digital Humanities.
46
References
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of the 2010 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1183?1193, Cambridge, MA. Association for Compu-
tational Linguistics.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and
Eros Zanchetta. 2009. The WaCky wide web: a
collection of very large linguistically processed web-
crawled corpora. Language Resources and Evalua-
tion, 43(3):209?226.
Lorna Byrne, Caroline Fenlon, and John Dunnion. 2013.
IIRG: A naive approach to evaluating phrasal seman-
tics. In Proceedings of the 6th International Work-
shop on Semantic Evaluation (SemEval 2012), At-
lanta, Georgia, USA.
Corinna Cortes and Vladimir Vapnik. 1995. Support-
vector networks. Machine Learning, 20(3):273?297.
He?ctor Da?vila, Antonio Ferna?ndez Orqu??n, Alexander
Cha?vez, Yoan Gutie?rrez, Armando Collazo, Jose? I.
Abreu, Andre?s Montoyo, and Rafael Mun?oz. 2013.
UMCC DLSI-(EPS): Paraphrases detection based on
semantic distance. In Proceedings of the 6th Inter-
national Workshop on Semantic Evaluation (SemEval
2012), Atlanta, Georgia, USA.
Emiliano Guevara. 2010. A regression model of
adjective-noun compositionality in distributional se-
mantics. In Proceedings of the 2010 Workshop on
GEometrical Models of Natural Language Semantics,
pages 33?37, Uppsala, Sweden. Association for Com-
putational Linguistics.
Sergio Jimenez, Claudia Becerra, and Alexander Gel-
bukh. 2013. UNAL: Discriminating between literal
and figurative phrasal usage using distributional statis-
tics and POS tags. In Proceedings of the 6th Inter-
national Workshop on Semantic Evaluation (SemEval
2012), Atlanta, Georgia, USA.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL-08: HLT, pages 236?244, Columbus, Ohio. As-
sociation for Computational Linguistics.
Jeff Mitchell and Mirella Lapata. 2010. Composition in
distributional models of semantics. Cognitive Science,
34(8):1388?1429.
Dragomir R. Radev, Simone Teufel, Horacio Saggion,
Wai Lam, John Blitzer, Hong Qi, Arda C?elebi, Danyu
Liu, and Elliott Drabek. 2003. Evaluation challenges
in large-scale document summarization. In Proceed-
ings of the 41st Annual Meeting on Association for
Computational Linguistics - Volume 1, ACL ?03, pages
375?382, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Helmut Schmid. 1994. Probabilistic Part-of-Speech tag-
ging using decision trees. In Proceedings of the In-
ternational Conference on New Methods in Language
Processing, Manchester, UK.
Reda Siblini and Leila Kosseim. 2013. CLaC: Semantic
relatedness of words and phrases. In Proceedings of
the 6th International Workshop on Semantic Evalua-
tion (SemEval 2012), Atlanta, Georgia, USA.
Yoshimasa Tsuruoka, Yuka Tateishi, Jin-Dong Kim,
Tomoko Ohta, John McNaught, Sophia Ananiadou,
and Jun?ichi Tsujii. 2005. Developing a robust Part-
of-Speech tagger for biomedical text. In Panayiotis
Bozanis and Elias N. Houstis, editors, Advances in In-
formatics, volume 3746, chapter 36, pages 382?392.
Springer Berlin Heidelberg, Berlin, Heidelberg.
Tim Van de Cruys, Thierry Poibeau, and Anna Korho-
nen. 2011. Latent vector weighting for word mean-
ing in context. In Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing, EMNLP ?11, pages 1012?1022, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Tim Van de Cruys, Stergos Afantenos, and Philippe
Muller. 2013. MELODI: Semantic similarity of words
and compositional phrases using latent vector weight-
ing. In Proceedings of the 6th International Work-
shop on Semantic Evaluation (SemEval 2012), At-
lanta, Georgia, USA.
Christian Wartena. 2013. HsH: Estimating semantic sim-
ilarity of words and short phrases with frequency nor-
malized distance measures. In Proceedings of the 6th
International Workshop on Semantic Evaluation (Se-
mEval 2012), Atlanta, Georgia, USA.
Eros Zanchetta and Marco Baroni. 2005. Morph-it!: A
free corpus-based morphological resource for the ital-
ian language. Corpus Linguistics 2005, 1(1).
Fabio Massimo Zanzotto, Ioannis Korkontzelos,
Francesca Fallucchi, and Suresh Manandhar. 2010.
Estimating linear models for compositional dis-
tributional semantics. In Proceedings of the 23rd
International Conference on Computational Linguis-
tics (COLING).
Torsten Zesch, Christof Mu?ller, and Iryna Gurevych.
2008. Extracting lexical semantic knowledge from
Wikipedia and Wiktionary. Proceedings of the Confer-
ence on Language Resources and Evaluation (LREC),
15:60.
47
Proceedings of the 5th ACL-HLT Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 44?53,
Portland, OR, USA, 24 June 2011. c?2011 Association for Computational Linguistics
Enrichment and Structuring of Archival Description Metadata
Kalliopi Zervanou?, Ioannis Korkontzelos?, Antal van den Bosch? and Sophia Ananiadou?
? Tilburg centre for Cognition and Communication (TiCC), University of Tilburg
Warandelaan 2 - PO Box 90153, 5000 LE Tilburg, The Netherlands
{K.Zervanou, Antal.vdnBosch}@uvt.nl
? National Centre for Text Mining, University of Manchester
131 Princess Street, Manchester M1 7DN, UK
{Ioannis.Korkontzelos, Sophia.Ananiadou}@manchester.ac.uk
Abstract
Cultural heritage institutions are making their
digital content available and searchable on-
line. Digital metadata descriptions play an im-
portant role in this endeavour. This metadata
is mostly manually created and often lacks de-
tailed annotation, consistency and, most im-
portantly, explicit semantic content descrip-
tors which would facilitate online browsing
and exploration of available information. This
paper proposes the enrichment of existing
cultural heritage metadata with automatically
generated semantic content descriptors. In
particular, it is concerned with metadata en-
coding archival descriptions (EAD) and pro-
poses to use automatic term recognition and
term clustering techniques for knowledge ac-
quisition and content-based document classi-
fication purposes.
1 Introduction
The advent of the digital age has long changed the
processes and the media which cultural heritage in-
stitutions (such as libraries, archives and museums)
apply for describing and cataloguing their objects:
electronic cataloguing systems support classification
and search, while cultural heritage objects are asso-
ciated to digital metadata content descriptions. The
expansion of the web and the increasing engagement
of web users throughout the world has brought about
the need for cultural heritage institutions to make
their content available and accessible to a wider au-
dience online.
In this endeavour, cultural heritage institutions
face numerous challenges. In terms of metadata,
different metadata standards currently exist for de-
scribing various types of objects, both within the
same institution and across different institutions.
Moreover, metadata object descriptions have been
typically both created by and addressed to librar-
ian and archivist experts who have been expected
to assist visitors in their search. For this reason,
they primarily refer to bibliographic descriptions
(e.g. author/creator, title, etc.), or physical descrip-
tions (e.g. size, shape, material, etc.), and location.
The lack of semantic descriptors in this type of meta-
data makes it difficult for potential online visitors to
browse and explore available information based on
more intuitive content criteria.
Work on metadata in cultural heritage institu-
tions has been largely focused on the issue of meta-
data heterogeneity. There have been efforts towards
the development and adoption of collection-specific
metadata standards, such as MARC 21 (Library of
Congress, 2010) and EAD (Library of Congress,
2002), for library and archival material respectively,
which are intended to standardise metadata descrip-
tions across different institutions. To address the is-
sue of heterogeneity across different types of object
collections, generic metadata schemas have been
proposed, such as the Dublin Core Metadata Initia-
tive (DCMI, 2011). Moreover, current research has
attempted to integrate diverse metadata schemas by
mappings across existing schemas (Bountouri and
Gergatsoulis, 2009), or mappings of existing meta-
data to ontologies, either based on ad-hoc manually
developed ontologies (Liao et al, 2010), or on ex-
isting standard ontologies for cultural heritage pur-
poses (Lourdi et al, 2009), such as the CIDOC Con-
44
ceptual Reference Model (CIDOC, 2006). Other
approaches attempt to address the issue of meta-
data heterogeneity from a pure information retrieval
perspective and discard the diverse metadata struc-
tures in favour of the respective text content descrip-
tions for full text indexing (Koolen et al, 2007).
Zhang and Kamps (2009) attempt to exploit the ex-
isting metadata XML structure for XML-based re-
trieval, thus targeting individual document compo-
nents. Similarly to our approach, they investigate
metadata describing archive collections.
The work presented in this paper focuses on meta-
data for textual objects, such as archive documents,
and on the issue of explicit, semantic, content de-
scriptors in this metadata, rather than heterogene-
ity. In particular, we are concerned with the lack
of explicit content descriptors which would support
exploratory information search. For this purpose,
we attempt to automatically enrich manually cre-
ated metadata with content information. We view
the problem from an unsupervised, text mining per-
spective, whereby multi-word terms recognised in
free text are assumed to indicate content. In turn,
the respective inter-relationships among the recog-
nised terms in the hierarchy are assumed to reveal
the knowledge structure of the document collection.
In this paper, we start with a description of our
EAD dataset and the challenges which our dataset
poses in text processing. Subsequently, we discuss
our approach to the enrichment and structuring of
these archival descriptions and present our experi-
ments. We conclude with a discussion on our results
and our considerations for future work.
2 EAD and Challenges in Text Processing
The Encoded Archival Description (EAD) was con-
ceived as ?a nonproprietary encoding standard for
machine-readable finding aids such as inventories,
registers, indexes, and other documents created by
archives, libraries, museums, and manuscript repos-
itories to support the use of their holdings? (Li-
brary of Congress, 2002). It is intended to be a data
communication format based on SGML/XML syn-
tax, aiming at supporting the accessibility to archival
resources across different institutions and focusing
on the structural content of the archival descrip-
tion, rather than its presentation. For this reason,
the EAD schema is characterised by a hierarchi-
cal informational structure, where the deepest lev-
els in the schema may inherit descriptive informa-
tion defined in the upper levels. The schema de-
fines a total of 146 elements. The three highest level
elements are <eadheader>, <frontmatter>,
and <archdesc>. <eadheader> is an ele-
ment containing bibliographic and descriptive in-
formation about the metadata document, while
<frontmatter> is an optional element describ-
ing the creation, publication, or use of the metadata
document (Library of Congress, 2002). Both these
two upper level elements do not contain information
about the archival material itself. The designated el-
ement for this purpose is <archdesc> which de-
scribes ?the content, context, and extent of a body
of archival materials, including administrative and
supplemental information that facilitates use of the
materials? (Library of Congress, 2002).
EAD metadata files can be lengthy and com-
plex in structure, with deep nesting of the XML
hierarchy elements. As Zhang and Kamps (2009)
also observe, the EAD elements may be of three
types:
i. atomic units (or text content elements) which
contain only text and no XML elements;
ii. composite units (or nested elements) which
contain as nested other XML elements;
iii. mixed elements which contain both atomic and
composite units.
The EAD documents used in this study describe
archival collections of the International Institute of
Social History (IISH). They are of varying length
and are often characterised by long spans of non-
annotated, free text. The degree of annotation, es-
pecially within mixed element types is inconsistent.
For example, some names may be annotated in one
element and others not, while quite often repeated
mentions of the same name may not be annotated.
Moreover, the text within an annotated element may
include annotator comments (e.g., translations, alter-
nate names, questions, notes, etc.), either in square
brackets or parentheses, again in an inconsistent
manner. The multilingual text content poses another
challenge. In particular, the languages used in the
description text vary, not only within a single EAD
document, but often also within an element (mixed
or atomic). In our approach, the former is addressed
45
by identifying the language at element level (cf. Sec-
tion 3.2). However, the issue of mixed languages
within an element is not addressed. This introduces
errors, especially for multilingual elements of short
text length.
3 Enrichment and Structuring Method
The overall rationale behind our method for the en-
richment of EAD metadata with semantic content in-
formation is based on two hypotheses:
i. multi-word terms recognised in free text are
valid indicators of content, and
ii. the respective term inter-relationships reflect
the knowledge structure of the collection.
Thus, automatic term recognition and subsequent
term clustering constitute the two core components
of our EAD processing. In particular, as illustrated
in Figure 1, we start with a pre-processing phase,
where the EAD input SGML/XML files are first
parsed, in order to retrieve the respective text con-
tent snippets, and then classified, based on language.
Subsequently, terms are recognised automatically.
The resulting terms are clustered as a hierarchy and,
finally, the documents are classified according to the
term hierarchy, based on the terms that they contain.
To evaluate our term recognition process, we exploit
knowledge from two sources: existing annotations
in the EAD files, such as entity annotation residing
in mixed elements (cf. Section 2) and entity and sub-
ject term information originating from the respective
cultural heritage institution Authority files, namely
the library files providing standard references for en-
tities and terms that curators should use in their ob-
ject descriptions. In this section, we discuss in more
detail the methodology for each of the components
of our approach.
3.1 EAD Text Element Extraction
In our processing of the EAD metadata XML, we
focused on the free text content structured below
the <archdesc> root element. As discussed in
Section 2, it is the only top element which con-
tains information about the archival material itself.
In the text element extraction process, we parse
the EAD XML and, from the hierarchically struc-
tured elements below <archdesc>, we select the
text contained in <abstract>, <bioghist>,
<scopecontent>, <odd>, <note> , <dsc>
and <descgrp> and their nested elements.
Among these elements, the <dsc> (Description
of Subordinate Components) provides information
about the hierarchical groupings of the materials be-
ing described, whereas <descgrp> (DSC Group)
defines nested encoded finding aids. They were se-
lected because they may contain nested information
of interest. The rest of the elements were selected
because they contain important free text information
related to the archive content:
- <bioghist>: describing the archive creator
e.g. the life of the individual or family, or
the administrative history of the organisation
which created the archive;
- <scopecontent>: referring to the range
and topical coverage of the described materials,
often naming significant organisations, individ-
uals, events, places, and subjects represented;
- <odd>: other descriptive data;
- <note>: referring to archivist comments and
explanations;
- <abstract>: brief summaries of all the
above information.
All other elements not referring to the archive se-
mantic content, such as administrative information,
storage arrangement, physical location, etc. were ig-
nored. Moreover, atomic or composite elements
without free text descriptions were not selected, be-
cause the descriptive information therein is assumed
to be already fully structured.
3.2 Language Identification
As mentioned in Section 2, the languages used in
the description text of the EAD documents vary, not
only within a single EAD document, but often also
within an EAD element. In our approach, the objec-
tive of the language identification process is to de-
tect the language of the text content snippets, i.e. the
output of the text element extraction process, and
classify these snippets accordingly (cf. Figure 1).
Language identification is a text categorisation
task, whereby identifiers attempt to learn the mor-
phology of a language based on training text and,
subsequently, use this information to classify un-
known text accordingly. For this reason, training a
language identification component requires a train-
ing corpus for each language of interest.
46
Figure 1: Block diagram of EAD metadata enrichment and structuring process
Computational approaches to language identifi-
cation can be coarsely classified into information-
theoretic, word-based, and N-gram-based.
Information-theoretic approaches compare the
compressibility of the input text to the compress-
ibility of text in the known languages. Measuring
compressibility employs mutual information mea-
sures (Poutsma, 2002). Word-based approaches
consider the amount of common words or special
characters between the input text and a known
language. Finally, N-gram-based approaches con-
struct language models beyond word boundaries,
based on the occurrence statistics of N-grams up
to some predefined length N (Dunning, 1994).
The subsequent language identification in unknown
text is based on the similarity of the unknown text
N-gram model to each training language model.
As evidenced by these approaches, language iden-
tification relies on some form of comparison of the
unknown text to known languages. For this reason,
the respective text categorisation into a given lan-
guage suffers when the input text is not long enough:
the shorter the input text is, the fewer the available
features for comparison against known language
models. Moreover, errors in the categorisation pro-
cess are also introduced, when the language models
under comparison share the same word forms.
In our approach, we have opted for the most pop-
ular language identification method, the one based
on N-grams. Nevertheless, any other language iden-
tification method could have been applied.
3.3 Term Recognition
The objective of term recognition is the identifica-
tion of linguistic expressions denoting specialised
concepts, namely domain or scientific terms. For in-
formation management and retrieval purposes, the
automatic identification of terms is of particular im-
portance because these specialised concept expres-
sions reflect the respective document content.
Term recognition approaches largely rely on the
identification of term formation patterns. Linguistic
approaches use either syntactic (Justeson and Katz,
1995; Hearst, 1998), or morphological (Heid, 1998)
rule patterns, often in combination with termino-
logical or other lexical resources (Gaizauskas et al,
2000) and are typically language and domain spe-
cific.
Statistical approaches typically combine linguis-
tic information with statistical measures. These
measures can be coarsely classified into two
categories: unithood-based and termhood-based.
Unithood-based approaches measure the attachment
strength among the constituents of a candidate
term. For example, some unithood-based mea-
sures are frequency of co-occurrence, hypothesis
testing statistics, log-likelihood ratios test (Dunning,
1993) and pointwise mutual information (Church
and Hanks, 1990). Termhood-based approaches at-
tempt to measure the degree up to which a candidate
expression is a valid term, i.e. refers to a specialised
concept. They attempt to measure this degree by
considering nestedness information, namely the fre-
47
quencies of candidate terms and their subsequences.
Examples of such approaches are C-Value and NC-
Value (Frantzi et al, 2000) and the statistical barrier
method (Nakagawa, 2000).
It has been experimentally shown that termhood-
based approaches to automatic term extraction out-
perform unithood-based ones and that C-Value
(Frantzi et al, 2000) is among the best perform-
ing termhood-based approaches (Korkontzelos et
al., 2008). For this reason, we choose to employ
the C-Value measure in our pipeline. C-Value ex-
ploits nestedness and comes together with a com-
putationally efficient algorithm, which scores can-
didate multi-word terms according to the measure,
considering:
- the total frequency of occurrence of the candi-
date term;
- the frequency of the candidate term as part of
longer candidate terms;
- the number of these distinct longer candidates;
- the length of the candidate term (in tokens).
These arguments are expressed in the following
nestedness formula:
N(?) =
?
?
?
f(?), if ? is not nested
f(?)?
1
|T?|
?
b?T?
f(b), otherwise (1)
where ? is the candidate term, f(?) is its frequency,
T? is the set of candidate terms that contain ? and
|T?| is the cardinality of T?. In simple terms, the
more frequently a candidate term appears as a sub-
string of other candidates, the less likely it is to be a
valid term. However, the greater the number of dis-
tinct term candidates in which the target term can-
didate occurs as nested, the more likely it is to be
a valid term. The final C-Value score considers the
length (|?|) of each candidate term (?) as well:
C-value(?) = log2 |?| ?N(?) (2)
The C-Value method requires linguistic pre-
processing in order to detect syntactic term for-
mation patterns. In our approach, we used Lex-
Tagger (Vasilakopoulos, 2003), which combines
transformation-based learning with decision trees
and we adapted its respective lexicon to our domain.
We also included WordNet lemma information in
our processing, for text normalisation purposes. Lin-
guistic pre-processing is followed by the computa-
tion of C-Value on the candidate terms, in length or-
der, longest first. Candidates that satisfy a C-Value
threshold are sorted in decreasing C-Value order.
3.4 Hierarchical Agglomerative Clustering
In our approach, term recognition provides content
indicators. In order to make explicit the knowl-
edge structure of the EAD, our method requires
some form of concept classification and structuring.
The process of hierarchical agglomerative cluster-
ing serves this objective.
Agglomerative algorithms are very popular in
the field of unsupervised concept hierarchy induc-
tion and are typically employed to produce unla-
belled taxonomies (King, 1967; Sneath and Sokal,
1973). Hierarchical clustering algorithms are based
on measuring the distance (dissimilarity) between
pairs of objects. Given an object distance metric D,
the similarity of two clusters, A and B, can be de-
fined as a function of the distance D between the
objects that the clusters contain. According to this
similarity, also called linkage criterion, the choice
of which clusters to merge or split is made. In our
approach, we have experimented with the three most
popular criteria, namely:
Complete linkage (CL): The similarity of two clus-
ters is the maximum distance between their elements
simCL(A,B) = max
x?A,y?B
D(x, y) (3)
Single linkage (SL): The similarity of two clusters
is the minimum distance between their elements
simSL(A,B) = min
x?A,y?B
D(x, y) (4)
Average linkage (AL): The similarity of two clusters
is the average distance between their elements
simAL(A,B) =
1
|A| ? |B|
?
x?A
?
y?B
D(x, y) (5)
To estimate the distance metric D we use either
the document co-occurrence or the lexical similar-
ity metric. The chosen distance metric D and link-
age criterion are employed to derive a hierarchy of
terms by agglomerative clustering.
Our document co-occurrence (DC) metric is de-
fined as the number of documents (d) in the collec-
tion (R) in which both terms (t1 and t2) co-occur:
DC =
1
|R|
|{d : (d ? R) ? (t1 ? d) ? (t2 ? d)}| (6)
48
The above metric accepts that the distance between
two terms is inversely proportional to the number of
documents in which they co-occur.
Lexical Similarity (LS), as defined in Nenadic?
and Ananiadou (2006), is based on shared term con-
stituents:
LS =
|P (h1) ? P (h2)|
|P (h1)|+ |P (h2)|
+
|P (t1) ? P (t2)|
|P (t1)|+ |P (t2)|
(7)
where t1 and t2 are two terms, h1 and h2 their heads,
P (h1) and P (h2) their set of head words, and P (t1)
and P (t2) their set of constituent words, respec-
tively.
3.5 Document Classification
The term hierarchy is used in our approach for se-
mantic classification of documents. In this process,
we start by assigning to each leaf node of the term
hierarchy the set of EAD documents in which the
corresponding term occurs. Higher level nodes are
assigned the union of the document sets of their
daughters. The process is bottom-up and applied it-
eratively, until all hierarchy nodes are assigned a set
of documents.
Document classification, i.e. the assignment of
document sets to term hierarchy nodes, is use-
ful, among others, for structured search and index-
ing purposes. Moreover, it provides a direct soft-
clustering of documents based on semantics, given
the number of desired clusters, C. C corresponds
to a certain horizontal cut of the term hierarchy, so
that C top nodes appear, instead of one. The doc-
ument sets assigned to these C top nodes represent
the C desired clusters. This document clustering ap-
proach is soft, since each document can occur in one
or more clusters.
3.6 Evaluation Process
The automatic evaluation process, illustrated in Fig-
ure 1, serves the purpose of evaluating the term
recognition accuracy. Since the objective of term
recognition tools is the detection of linguistic ex-
pressions denoting specialised concepts, i.e. terms,
the results evaluation would ideally require input
from the respective domain experts. This is a la-
borious and time consuming process which also en-
tails finding the experts willing to dedicate effort
and time for this task. In response to this issue,
we decided to exploit the available domain-specific
knowledge resources and automate part of the eval-
uation process by comparing our results to this ex-
isting information. Thus, the automatic evaluation
process is intended to give us an initial estimate
of our performance and reduce the amount of re-
sults requiring manual evaluation. The available re-
sources used are of two types:
i. entity annotations in the EAD documents (i.e.
names of persons, organisations and geograph-
ical locations);
ii. entity and subject terms originating from the
cultural heritage institution Authority files.
The entity annotations in the EAD documents
were not considered during our term recognition.
The entity and subject terms of the respective Au-
thority file records are encoded in MARC21/XML
format (Library of Congress, 2010). MARC
(MAchine-Readable Cataloging) is a standard initi-
ated by the US Library of Congress and concerns
the representation of bibliographic information and
related data elements used in library catalogues. The
MARC21 Authority files resource used in our eval-
uation provides, among other information, the stan-
dard references for entities and the respective pos-
sible entity reference variations, such as alternate
names or acronyms, etc., that curators should use
in their object descriptions. The subject term Au-
thority records provide mappings between a legacy
subject term thesaurus which is no longer used for
classification, and current library records.
In the evaluation process the EAD SGML/XML
and the MARC21/XML Authority files are first
parsed by the respective parsers in order to extract
the XML elements of interest. Subsequently, the
text-content of the elements is processed for nor-
malisation and variant generation purposes. In this
process, normalisation involves cleaning up the text
from intercepted comments and various types of
inconsistent notes, such as dates, aliases and al-
ternate names, translations, clarifications, assump-
tions, questions, lists, etc. Variant generation in-
volves detecting the acronyms, abbreviated names
and aliases mentioned in the element text and cre-
ating the reversed variants for, e.g., [Last Name,
First Name] sequences. The results of this pro-
cess, from both EAD and Authority files, are merged
into a single list for every respective category (or-
49
language snippets language snippets
Dutch 50,363 Spanish 3,430
German 41,334 Danish 2,478
English 19,767 Italian 1,100
French 6,182 Swedish 699
Table 1: Number of snippets per identified language.
ganisations, persons, geographic locations and sub-
ject terms) and are compared to our term results list.
4 Experimental Setting
For training the language identification component,
we used the European Parliament Proceedings Par-
allel Corpus (Europarl) which covers the proceed-
ings of the European Parliament from 1996 to 2006
(Koehn, 2005). The corpus size is 40 million words
per language and is translated in Danish, German,
Greek, English, Spanish, Finnish, French, Italian,
Dutch, Portuguese and Swedish. In our experiments,
we take as input for subsequent term recognition
only the snippets identified as English text.
In the experiments reported in this work, we ac-
cept as term candidates morpho-syntactic pattern se-
quences which consist of adjectives and nouns, and
end with a noun. The C-Value algorithm (cf. Sec-
tion 3.3) was implemented under two different set-
tings:
i. one only considering as term candidates adjec-
tive and noun sequences that appear at least
once as non-nested in other candidate terms;
and
ii. one that considers all adjective and noun se-
quences, even if they never occur as non-
nested.
Considering that part-of-speech taggers usually suf-
fer high error rates when applied on specialty do-
mains, the former setting is expected to increase pre-
cision, whereas the latter to increase recall (cf. Sec-
tion 5).
We accepted as valid terms all term candidates
whose C-Value score exceeds a threshold, which
was set to 3.0 after experimentation. In the subse-
quent hierarchical agglomerative clustering process,
we experimented with all six combinations of the
three linkage criteria (i.e. complete, single and aver-
age) with the two distance metrics (i.e. document
co-occurrence and lexical similarity) described in
Figure 2: Length of snippets per identified language.
Section 3.4.
5 Results
The EAD document collection used for this study
consisted of 3, 093 SGML/XML files. As shown on
Table 1, according to our language identifier, the ma-
jority of the text snippets of the selected EAD XML
elements were in Dutch, followed by German and
English. We selected for later processing 19, 767
snippets classified as English text, corresponding to
419, 857 tokens. A quantitative evaluation of the
language identifier results has not been performed.
However, our observation of the term recognition re-
sults showed that there were some phrases, mostly
Dutch and German entity names (organisations and
persons mostly) classified as English. This might be
due to these entities appearing in their original lan-
guage within English text, as it is often the case in
our EAD files. Moreover, manual inspection of our
results showed that other languages classified as En-
glish, e.g. Turkish and Czech, were not covered by
Europarl.
As mentioned in Section 3.2, short text snip-
pets may affect language identification performance.
Figure 2 illustrates the snippet length per identified
language. We observe that the majority of text snip-
pets is below 10 tokens, few fall within an average
length of 20 to 50 tokens approximately, and very
few are above 100 tokens.
Figure 3 shows the results of our automatic evalu-
ation for the term recognition process. In this graph,
the upper, red curve shows the percentage of cor-
rect terms for the C-Value setting considering as
term candidates adjective and noun sequences that
appear at least once as non-nested in other candi-
date terms. The lower, blue curve shows the per-
50
Figure 3: Term coverage for each C-Value setting based
on EAD & Authority entity and subject term evaluation.
centage of correct terms for the C-Value setting con-
sidering all adjective and noun sequences, even if
they never occur as non-nested. In this automatic
evaluation, correct terms are, as presented in Sec-
tion 3.6, those candidate terms matching the com-
bined lists of entity and subject terms acquired by
the respective EAD and MARC21 Authority files.
We observe that the C-Value setting which considers
only noun phrase patterns occurring at least once as
non-nested, displays precision up to approximately
70% for the top terms in the ranked list, whereas the
other setting considering all noun phrase sequences,
reaches a maximum of 49%. The entire result set
above the 3.0 C-Value threshold amounts to 1, 345
and 2, 297 terms for each setting, and reaches pre-
cision of 42.01% and 28.91% respectively. Thus,
regarding precision, the selective setting clearly out-
performs the one considering all noun phrases, but it
also reaches a lower recall, as indicated by the ac-
tual terms within the threshold. We also observe
that precision drops gradually below the threshold,
an indication that the ranking of the C-Value mea-
sure is effective in promoting valid terms towards
the top. This automatic evaluation considers as erro-
neous unknown terms which may be valid. Further
manual evaluation by domain experts is required for
a more complete picture of the results.
Figure 4 shows six dendrograms, each represent-
ing the term hierarchy produced by the respective
combination of linkage criterion to distance metric.
The input for these experiments consists of all terms
exceeding the C-Value threshold, and by considering
only noun phrase sequences appearing at least once
as non-nested. Since the hierarchies contain 1, 345
terms, the dendrograms are very dense and difficult
to inspect thoroughly. However, we include them
based on the fact that the overall shape of the den-
drogram can indicate how much narrow or broad the
corresponding hierarchy is and indirectly its quality.
Narrow here characterises hierarchies whose most
non-terminal nodes are parents of one terminal and
one non-terminal node. Narrow hierarchies are deep
while broader hierarchies are shallower.
Broad and shallow hierarchies are, in our case, of
higher quality, since terms are expected to be related
to each other and form distinct groups. In this view,
average linkage leads to richer hierarchies (Figures
4(c), 4(f)), followed by single linkage (Figures 4(b),
4(e)) and, finally, complete linkage (Figures 4(a),
4(d)). The hierarchy of higher quality seems to
be the result of average linkage and document co-
occurrence combination (Figure 4(c)), followed by
the combination of average linkage and lexical sim-
ilarity (Figure 4(f)). Clearly, these two hierarchies
need to be investigated manually and closely to ex-
tract further conclusions. Moreover, an application-
based evaluation could investigate whether different
clustering settings suit different tasks.
6 Conclusion and Future Work
In this paper, we have presented a methodology for
semantically enriching archival description meta-
data and structuring the metadata collection. We
consider that terms are indicators of content seman-
tics. In our approach, we perform term recogni-
tion and then hierarchically structure the recognised
terms. Finally, we use the term hierarchy to classify
the metadata documents. We also propose an auto-
matic evaluation of the recognised terms, by com-
paring them to domain knowledge resources.
For term recognition, we used the C-Value al-
gorithm and found that considering noun phrases
which appear at least once independently, outper-
forms considering all noun phrases. Regarding hier-
archical clustering, we observe that the average link-
age criterion combined with a distance metric based
on document co-occurrence produces a rich broad
hierarchy. A more thorough evaluation of these re-
sults is required. This should include a manual eval-
uation of recognised terms by domain experts and
an application-based evaluation of the resulting doc-
ument classification.
51
(a) Complete linkage - DC (b) Single linkage - DC (c) Average linkage - DC
(d) Complete linkage - LS (e) Single linkage - LS (f) Average linkage - LS
Figure 4: Dendrograms showing the results of agglomerative clustering for all linkage criteria and distance metrics,
document co-occurrence (DC) and Lexical Similarity (LS).
References
Lina Bountouri and Manolis Gergatsoulis. 2009. Inter-
operability between archival and bibliographic meta-
data: An EAD to MODS crosswalk. Journal of Li-
brary Metadata, 9(1-2):98?133.
Kenneth Church and Patrick Hanks. 1990. Word asso-
ciation norms, mutual information, and lexicography.
Computational Linguistics, 16(1):22?29.
CIDOC. 2006. The CIDOC Conceptual Reference
Model. CIDOC Documentation Standards Working
Group, International Documentation Committee, In-
ternational Council of Museums. http://www.
cidoc-crm.org/.
DCMI. 2011. The Dublin Core Metadata Initiative.
http://dublincore.org/.
Ted Dunning. 1993. Accurate methods for the statistics
of surprise and coincidence. Computational Linguis-
tics, 19(1):61?74.
Ted Dunning. 1994. Statistical identification of lan-
guage. MCCS 94-273. Technical report, Computing
Research Laboratory, New Mexico State University.
Katerina Frantzi, Sophia Ananiadou, and Hideki Mima.
2000. Automatic recognition of multi-word terms: the
C-value/NC-value method. International Journal on
Digital Libraries, 3(2):115?130.
Robert Gaizauskas, George Demetriou, and Kevin
Humphreys. 2000. Term recognition in biological sci-
ence journal articles. In Proc. of the NLP 2000 Work-
shop on Computational Terminology for Medical and
Biological Applications, pages 37?44, Patras, Greece.
Marti Hearst. 1998. Automated discovery of WordNet
relations. In Christiane Fellbaum, editor, WordNet:
An Electronic Lexical Database, pages 131?153. MIT
Press.
Ulrich Heid. 1998. A linguistic bootstrapping approach
to the extraction of term candidates from german text.
Terminology, 5(2):161?181.
John Justeson and Slava Katz. 1995. Technical terminol-
ogy: some linguistic properties and an algorithm for
identification in text. Natural Language Engineering,
1(1):9?27.
Benjamin King. 1967. Step-Wise clustering proce-
dures. Journal of the American Statistical Association,
62(317):86?101.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Machine Transla-
tion Summit X, pages 79?86, Phuket, Thailand.
Marijn Koolen, Avi Arampatzis, Jaap Kamps, Vincent
de Keijzer, and Nir Nussbaum. 2007. Unified access
to heterogeneous data in cultural heritage. In Proc. of
RIAO ?07, pages 108?122, Pittsburgh, PA, USA.
Ioannis Korkontzelos, Ioannis Klapaftis, and Suresh
Manandhar. 2008. Reviewing and evaluating auto-
matic term recognition techniques. In Bengt Nord-
stro?m and Aarne Ranta, editors, Proc. of GoTAL ?08,
volume 5221 of LNCS, pages 248?259, Gothenburg,
Sweden. Springer.
Shu-Hsien Liao, Hong-Chu Huang, and Ya-Ning Chen.
2010. A semantic web approach to heterogeneous
metadata integration. In Jeng-Shyang Pan, Shyi-Ming
Chen, and Ngoc Thanh Nguyen, editors, Proc. of
ICCCI ?10, volume 6421 of LNCS, pages 205?214,
Kaohsiung, Taiwan. Springer.
Library of Congress. 2002. Encoded archival descrip-
tion (EAD), version 2002. Encoded Archival Descrip-
tion Working Group: Society of American Archivists,
52
Network Development and MARC Standards Office,
Library of Congress. http://www.loc.gov/
ead/.
Library of Congress. 2010. MARC standards. Network
Development and MARC Standards Office, Library of
Congress, USA. http://www.loc.gov/marc/
index.html.
Irene Lourdi, Christos Papatheodorou, and Martin Doerr.
2009. Semantic integration of collection description:
Combining CIDOC/CRM and Dublin Core collections
application profile. D-Lib Magazine, 15(7/8).
Hiroshi Nakagawa. 2000. Automatic term recognition
based on statistics of compound nouns. Terminology,
6(2):195?210.
Goran Nenadic? and Sophia Ananiadou. 2006. Min-
ing semantically related terms from biomedical liter-
ature. ACM Transactions on Asian Language Infor-
mation Processing (TALIP), 5(1):22?43.
Arjen Poutsma. 2002. Applying monte carlo techniques
to language identification. Language and Computers,
45:179?189.
Peter Sneath and Robert Sokal. 1973. Numerical taxon-
omy: the principles and practice of numerical classifi-
cation. Freeman, San Francisco, USA.
Argyris Vasilakopoulos. 2003. Improved unknown word
guessing by decision tree induction for POS tagging
with tbl. In Proc. of CLUK ?03, Edinburgh, UK.
Junte Zhang and Jaap Kamps. 2009. Focused search
in digital archives. In Gottfried Vossen, Darrell D. E.
Long, and Jeffrey Xu Yu, editors, Proc. of WISE
?09, volume 5802 of LNCS, pages 463?471, Poznan,
Poland. Springer.
53
Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 79?88,
Sofia, Bulgaria, August 8-9, 2013. c?2013 Association for Computational Linguistics
Towards a Better Understanding of Discourse:
Integrating Multiple Discourse Annotation Perspectives Using UIMA
Claudiu Miha?ila??, Georgios Kontonatsios?, Riza Theresa Batista-Navarro?,
Paul Thompson?, Ioannis Korkontzelos and Sophia Ananiadou
The National Centre for Text Mining,
School of Computer Science, The University of Manchester
{mihailac,kontonag,batistar,thompsop,
korkonti,ananiads}@cs.man.ac.uk
Abstract
There exist various different discourse an-
notation schemes that vary both in the
perspectives of discourse structure consid-
ered and the granularity of textual units
that are annotated. Comparison and inte-
gration of multiple schemes have the po-
tential to provide enhanced information.
However, the differing formats of cor-
pora and tools that contain or produce
such schemes can be a barrier to their
integration. U-Compare is a graphical,
UIMA-based workflow construction plat-
form for combining interoperable natu-
ral language processing (NLP) resources,
without the need for programming skills.
In this paper, we present an extension
of U-Compare that allows the easy com-
parison, integration and visualisation of
resources that contain or output annota-
tions based on multiple discourse anno-
tation schemes. The extension works by
allowing the construction of parallel sub-
workflows for each scheme within a single
U-Compare workflow. The different types
of discourse annotations produced by each
sub-workflow can be either merged or vi-
sualised side-by-side for comparison. We
demonstrate this new functionality by us-
ing it to compare annotations belonging
to two different approaches to discourse
analysis, namely discourse relations and
functional discourse annotations. Integrat-
ing these different annotation types within
an interoperable environment allows us to
study the correlations between different
types of discourse and report on the new
insights that this allows us to discover.
?The authors have contributed equally to the development
of this work and production of the manuscript.
1 Introduction
Over the past few years, there has been an increas-
ing sophistication in the types of available natural
language processing (NLP) tools, with named en-
tity recognisers being complemented by relation
and event extraction systems. Such relations and
events are not intended to be understood in isola-
tion, but rather they are arranged to form a coher-
ent discourse. In order to carry out complex tasks
such as automatic summarisation to a high degree
of accuracy, it is important for systems to be able
to analyse the discourse structure of texts automat-
ically. To facilitate the development of such sys-
tems, various textual corpora containing discourse
annotations have been made available to the NLP
community. However, there is a large amount of
variability in the types of annotations contained
within these corpora, since different perspectives
on discourse have led to the development of a
number of different annotation schemes.
Corpora containing discourse-level annotations
usually treat the text as a sequence of coherent tex-
tual zones (e.g., clauses and sentences). One line
of research has been to identify which zones are
logically connected to each other, and to charac-
terise these links through the assignment of dis-
course relations. There are variations in the com-
plexity of the schemes used to annotate these dis-
course relations. For example, Rhetorical Struc-
ture Theory (RST) (Mann and Thompson, 1988)
defines 23 types of discourse relations that are
used to structure the text into complex discourse
trees. Whilst this scheme was used to enrich the
Penn TreeBank (Carlson et al, 2001), the Penn
Discourse TreeBank (PDTB) (Prasad et al, 2008)
used another scheme to identify discourse rela-
tions that hold between pairs of text spans. It cate-
gorises the relations into types such as ?causal?,
?temporal? and ?conditional?, which can be ei-
ther explicit or implicit, depending on whether or
79
not they are represented in text using overt dis-
course connectives. In the biomedical domain, the
Biomedical Discourse Relation Bank (BioDRB)
(Prasad et al, 2011) annotates a similar set of re-
lation types, whilst BioCause focusses exclusively
on causality (Miha?ila? et al, 2013).
A second line of research does not aim to link
textual zones, but rather to classify them accord-
ing to their specific function in the discourse. Ex-
amples of functional discourse annotations include
whether a particular zone asserts new information
into the discourse or represents a speculation or
hypothesis. In scientific texts, knowing the type
of information that a zone represents (e.g., back-
ground knowledge, hypothesis, experimental ob-
servation, conclusion, etc.) allows for automatic
isolation of new knowledge claims (Sa?ndor and de
Waard, 2012). Several annotation schemes have
been developed to classify textual zones accord-
ing to their rhetorical status or general informa-
tion content (Teufel et al, 1999; Mizuta et al,
2006; Wilbur et al, 2006; de Waard and Pan-
der Maat, 2009; Liakata et al, 2012a). Related
to these studies are efforts to capture information
relating to discourse function at the level of events,
i.e., structured representations of pieces of knowl-
edge which, when identified, facilitate sophisti-
cated semantic searching (Ananiadou et al, 2010).
Since there can be multiple events in a sentence
or clause, the identification of discourse informa-
tion at the event level can allow for a more de-
tailed analysis of discourse elements than is possi-
ble when considering larger units of text. Certain
event corpora such as ACE 2005 (Walker, 2006)
and GENIA-MK (Thompson et al, 2011) have
been annotated with various types of functional
discourse information.
It has previously been shown that considering
several functional discourse annotation schemes in
parallel can be beneficial (Liakata et al, 2012b),
since each scheme offers a different perspective.
For a common set of documents, the cited study
analysed and compared functional discourse an-
notations at different levels of textual granular-
ity (i.e., sentences, clauses and events), showing
how the different schemes could complement each
other in order to lay the foundations for a possible
future harmonisation of the schemes. The results
of this analysis provide evidence that it would be
useful to carry out further such analyses involv-
ing other such schemes, including an investiga-
tion of how discourse relations and functional dis-
course annotations could complement each other,
e.g., which types of functional annotations occur
within the arguments of discourse relations. There
are, however, certain barriers to carrying out such
an analysis. For example, a comparison of an-
notation schemes would ideally allow the differ-
ent types of annotations to be visualised simul-
taneously or seamlessly merged together. How-
ever, the fact that annotations in different corpora
are encoded using different formats (e.g., stand-off
or in-line) and different encoding schemes means
that this can be problematic.
A solution to the challenges introduced above is
offered by the Unstructured Information Manage-
ment Architecture (UIMA) (Ferrucci and Lally,
2004), which defines a common workflow meta-
data format facilitating the straightforward combi-
nation of NLP resources into a workflow. Based
on the interoperability of the UIMA framework,
numerous researchers distribute their own tools as
UIMA-compliant components (Kano et al, 2011;
Baumgartner et al, 2008; Hahn et al, 2008;
Savova et al, 2010; Gurevych et al, 2007; Rak
et al, 2012b). However, UIMA is only intended
to provide an abstract framework for the interop-
erability of language resources, leaving the actual
implementation to third-party developers. Hence,
UIMA does not explicitly address interoperability
issues of tools and corpora.
U-Compare (Kano et al, 2011) is a UIMA-
based workflow construction platform that pro-
vides a graphical user interface (GUI) via which
users can rapidly create NLP pipelines using a
drag-and-drop mechanism. Conforming to UIMA
standards, U-Compare components and pipelines
are compatible with any UIMA application via a
common and sharable type system (i.e., a hier-
archy of annotation types). In defining this type
system, U-Compare promotes interoperability of
tools and corpora, by exhaustively modelling a
wide range of NLP data types (e.g., sentences, to-
kens, part-of-speech tags, named entities). This
type system was recently extended to include dis-
course annotations to model three discourse phe-
nomena, namely causality, coreference and meta-
knowledge (Batista-Navarro et al, 2013).
In this paper, we describe our extensions to U-
Compare, supporting the integration and visuali-
sation of resources annotated according to mul-
tiple discourse annotation schemes. Our method
80
decomposes pipelines into parallel sub-workflows,
each linked to a different annotation scheme.
The resulting annotations produced by each sub-
workflow can be either merged within a single
document or visualised in parallel views.
2 Related work
Previous studies have shown the advantages of
comparing and integrating different annotation
schemes on a corpus of documents (Guo et al,
2010; Liakata et al, 2010; Liakata et al, 2012b).
Guo et al (2010) compared three different dis-
course annotation schemes applied to a corpus
of biomedical abstracts on cancer risk assess-
ment and concluded that two of the schemes pro-
vide more fine-grained information than the other
scheme. They also revealed a subsumption rela-
tion between two schemes. Such outcomes from
comparing schemes are meaningful for users who
wish to select the most appropriate scheme for an-
notating their data. Liakata et al (2012) under-
line that different discourse annotation schemes
capture different dimensions of discourse. Hence,
there might be complementary information across
different schemes. Based on this hypothesis, they
provide a comparison of three annotation schemes,
namely CoreSC (Liakata et al, 2012a), GENIA-
MK (Thompson et al, 2011) and DiscSeg (de
Waard, 2007), on a corpus of three full-text pa-
pers. Their results showed that the categories in
the three schemes can complement each other. For
example, the values of the Certainty Level dimen-
sion of the GENIA-MK scheme can be used to as-
sign confidence values to the Conclusion, Result,
Implication and Hypothesis categories of CoreSC
and DiscSeg. In contrast to previous studies, our
proposed approach automatically integrates mul-
tiple annotation schemes. The proposed mecha-
nism allows users to easily compare, integrate and
visualise multiple discourse annotation schemes
in an interoperable NLP infrastructure, i.e., U-
Compare.
There are currently a number of freely-available
NLP workflow infrastructures (Ferrucci and Lally,
2004; Cunningham et al, 2002; Scha?fer, 2006;
Kano et al, 2011; Grishman, 1996; Baumgartner
et al, 2008; Hahn et al, 2008; Savova et al, 2010;
Gurevych et al, 2007; Rak et al, 2012b). Most
of the available infrastructures support the devel-
opment of standard NLP applications, e.g., part-
of-speech tagging, deep parsing, chunking, named
entity recognition and several of them allow the
representation and analysis of discourse phenom-
ena (Kano et al, 2011; Cunningham et al, 2002;
Savova et al, 2010; Gurevych et al, 2007). How-
ever, none of them has demonstrated the integra-
tion of resources annotated according to multiple
annotation schemes within a single NLP pipeline.
GATE (Cunningham et al, 2002) is an open
source NLP infrastructure that has been used for
the development of various language processing
tasks. It is packaged with an exhaustive number
of NLP components, including discourse analy-
sis modules, e.g., coreference resolution. Further-
more, GATE offers a GUI environment and wrap-
pers for UIMA-compliant components. However,
GATE implements a limited workflow manage-
ment mechanism that does not support the execu-
tion of parallel or nested workflows. In addition to
this, GATE does not promote interoperability of
language resources since it does not define any hi-
erarchy of NLP data types and components do not
formally declare their input/output capabilities.
In contrast to GATE, UIMA implements a more
sophisticated workflow management mechanism
that supports the construction of both parallel
and nested pipelines. In this paper, we exploit
this mechanism to integrate multiple annotation
schemes in NLP workflows. cTAKES (Savova
et al, 2010) and DKPro (Gurevych et al, 2007)
are two repositories containing UIMA-compliant
components that are tuned for the medical and
general domain, respectively. However, both of
these repositories support the representation of
only one discourse phenomenon, i.e., coreference.
Argo (Rak et al, 2012a; Rak et al, 2012b) is a
web-based platform that allows multiple branch-
ing and merging of UIMA pipelines. It incorpo-
rates several U-Compare components and conse-
quently, supports the U-Compare type system.
3 A UIMA architecture for processing
multiple annotation schemes
In UIMA, a document, together with its associated
annotations, is represented as a standardised data
structure, namely the Common Analysis Struc-
ture (CAS). Each CAS can contain any number
of nested sub-CASes, i.e., Subjects of Analysis
(Sofas), each of which can associate a different
type of annotation with the input document. In
this paper, we employ this UIMA mechanism to
allow the integration and comparison of multiple
81
Collection of Documents
Multi-SofaReader
Parallel Annotation Viewer Annotation Merger
ComparingSchemes Integrating Schemes
ComponentC_1
Sofa S_1
ComponentC_2
Sofa S_2
ComponentC_N-1
Sofa S_N-1
ComponentC_N
Sofa S_N
sub-workflows
Figure 1: Integrating annotations from multiple
annotation schemes in UIMA workflows
annotation schemes in a single U-Compare work-
flow. Assume that we have a corpus of documents
which has been annotated according to n different
schemes, S1, S2, ..., Sn?1, Sn. Also, assume that
we will use a library of m text analysis compo-
nents, C1, C2, ..., Cm?1, Cm, to enrich the corpus
with further annotations.
Our implemented architecture is illustrated in
Figure 1. Using multiple Sofas, we are able to split
a UIMA workflow into parallel sub-workflows.
Starting from a Multi-Sofa reader, we create n
sub-workflows, i.e., Sofas, each of which is linked
to a particular scheme for a different annotation
type. Each sub-workflow can then apply the anal-
ysis components that are most suitable for pro-
cessing the annotations from the corresponding
scheme.
U-Compare offers two different modes for visu-
alising corpora that have been annotated accord-
ing to multiple schemes. In the comparison mode,
the default annotation viewer is automatically split
to allow annotations from different schemes to be
displayed side-by-side. The second type of visu-
alisation merges the annotations produced by the
parallel sub-workflows into a single view. The
most appropriate view may depend on the prefer-
ences of the user and the task at hand, e.g., iden-
tifying similarities, differences or complementary
information between different schemes.
4 Application Workflows
In this section, we demonstrate two workflow ap-
plications that integrate multiple discourse anno-
tation schemes. The first workflow exploits U-
Compare?s comparison mode to visualise in par-
allel functional discourse annotations from two
schemes, namely, CoreSC (Liakata et al, 2012a)
and GENIA-MK (Thompson et al, 2011). The
second application integrates functional discourse
annotations in the ACE 2005 corpus with dis-
course relations obtained by an automated tool.
4.1 Visualising functional discourse
annotations from different schemes
The purpose of this workflow application is to re-
veal the different interpretations given by two dis-
course annotation schemes applied to a biomed-
ical corpus of three full-text papers (Liakata et
al., 2012b). The pipeline contains two read-
ers that take as input the annotations (in the
BioNLP Shared Task stand-off format) from the
two schemes and map them to U-Compare?s
type system. In this way, the annotations be-
come interoperable with existing components in
U-Compare?s library. U-Compare detects that the
workflow contains two annotation schemes and
automatically creates two parallel sub-workflows
as explained earlier. Furthermore, we configure
the workflow to use the comparison mode. There-
fore, the annotation viewer will display the two
different types of annotations based on the input
schemes side-by-side. Figure 2 illustrates the par-
allel viewing of a document annotated according
to both the CoreSC (left-hand side) and GENIA-
MK (right-hand side) annotation schemes. The
CoreSC scheme assigns a single category per sen-
tence. The main clause in the highlighted sen-
tence on the left-hand side constitutes the hypoth-
esis that transcription factors bind to exon-1. Ac-
cordingly, as can be confirmed from the annota-
tion table on the far right-hand side of the figure,
the (Hyp)othesis category has been assigned to the
sentence.
In the GENIA-MK corpus, the different pieces
of information contained within the sentence have
been separately annotated as structured events.
One of these events corresponds to the hypothe-
sis, but this is not the only information expressed:
information about a previous experimental out-
come from the authors, i.e., that exon1 is impli-
cated in CCR3 transcription, is annotated as a sep-
82
Figure 2: Comparing discourse annotations schemes in U-Compare. The pipeline uses two Sofas corre-
sponding to the CoreSC (left panel) and GENIA-MK (right panel) schemes.
arate event. Since functional discourse informa-
tion is annotated directly at the event level in the
GENIA-MK corpus, the bind event is considered
independently from the other event as represent-
ing an Analysis. Furthermore, the word hypoth-
esized is annotated as a cue for this categorisa-
tion. There are several ways in which the an-
notations of the two schemes can be seen to be
complementary to each other. For example, the
finer-grained categorisation of analytical informa-
tion in the CoreSC scheme could help to determine
that the analytical bind event in the GENIA-MK
corpus specifically represents a hypothesis, rather
than, e.g., a conclusion. Conversely, the event-
based annotation in the GENIA-MK corpus can
help to determine exactly which part of the sen-
tence represents the hypothesis. Furthermore, the
cue phrases annotated in the GENIA-MK corpus
could be used as additional features in a system
trained to assign CoreSC categories. Although in
this paper we illustrate only the visualisation of
different types of functional discourse annotations,
it is worth noting that U-Compare provides sup-
port for further processing. Firstly, unlike annota-
tion platforms such as brat (Stenetorp et al, 2012),
U-Compare allows for analysis components to be
integrated into workflows in a straightforward and
user-interactive manner. If, for example, it is of in-
terest to determine the tokens (and the correspond-
ing parts-of-speech) which frequently act as cues
in Analysis events, syntactic analysis components
(e.g., tokenisers and POS taggers) can be incorpo-
rated via a drag-and-drop mechanism. Also, U-
Compare allows the annotations to be saved in a
computable format using the provided Xmi Writer
CAS Consumer component. This facilitates fur-
ther automatic comparison of annotations.
4.2 Integrating discourse relations with
functional discourse annotations
To demonstrate the integration of annotations orig-
inating from two completely different perspectives
on discourse, we have created a workflow that
merges traditional discourse relations with func-
tional discourse annotations in a general domain
corpus. For this application, we used the ACE
2005 corpus, which consists of 599 documents
coming from broadcast conversation, broadcast
news, conversational telephone speech, newswire,
weblog and usenet newsgroups. This corpus
contains event annotations which have been en-
riched by attributes such as polarity (positive or
negative), modality (asserted or other), generic-
ity (generic or specific) and tense (past, present,
future or unspecified). We treat the values of
these attributes as functional discourse annota-
tions, since they provide further insight into the
interpretation of the events. We created a compo-
nent that reads the event annotations in the corpus
and maps them to U-Compare?s type system.
To obtain discourse relation annotations (which
are not available in the ACE corpus) we em-
ployed an end-to-end discourse parser trained
on the Penn Discourse TreeBank (Lin et al,
2012). It outputs three general types of anno-
tations, namely, explicit relations, non-explicit
relations and attribution spans. Explicit rela-
tions (i.e., those having overt discourse connec-
tives) are further categorised into the following 16
PDTB level-2 types: Asynchronous, Synchrony,
Cause, Pragmatic cause, Contrast, Concession,
Conjunction, Instantiation, Restatement, Alterna-
tive, List, Condition, Pragmatic condition, Prag-
matic contrast, Pragmatic concession and Excep-
83
Figure 3: Integrating different discourse annotation schemes in U-Compare.
tion. Non-explicit relations, on the other hand,
consist of EntRel and NoRel types, in addition to
the same first 11 explicit types mentioned above.
We created a workflow consisting of the ACE
corpus reader and the discourse parser (available
in U-Compare as a UIMA web service). This al-
lowed us to merge traditional discourse relations
with event-based functional discourse annotations,
and to visualise them in the same document (Fig-
ure 3). Furthermore, with the addition of the
Xmi Writer CAS Consumer in the workflow, the
merged annotations can be saved in a computable
format for further processing, allowing users to
perform deeper analyses on the discourse annota-
tions. This workflow has enabled us to gain some
insights into the correlations between functional
discourse annotations and discourse relations.
5 Correlations between discourse
relations and functional discourse
annotations
Based on the merged annotation format described
in the previous section, we computed cases in
which at least one of the arguments of a discourse
relation also contains an event. Figure 4 is a
heatmap depicting the correlations between differ-
ent types of discourse relations and the attribute
values of ACE events that co-occur with these re-
lations. The darker the colour, the smaller the ratio
of the given discourse relation co-occurring with
the specified ACE event attribute value. For in-
stance, the Cause relation co-occurs mostly with
positive events (over 95%) and the correspond-
ing cell is a very light shade of green. These are
discussed and exemplified below. In the exam-
ples, the following marking convention is used:
discourse connectives are capitalised, whilst argu-
ments are underlined. Event triggers are shown in
bold, and cues relating to functional discourse cat-
egories are italicised.
For all discourse relation types, at least 50% of
co-occurring events are assigned the specific value
of the Genericity attribute. Specific events are
those that describe a specific occurrence or situ-
ation, rather than a more generic situation. In gen-
eral, this high proportion of specific events is to be
expected. The types of text contained within the
corpus, consisting largely of news and transcrip-
tions of conversions, would be expected to intro-
duce a large amount of information about specific
events.
For two types of discourse relations, i.e. Condi-
tion and Concession, there are more or less equal
numbers of specific and generic events. The na-
ture of these relation types helps to explain these
proportions. Conditional relations often describe
how a particular, i.e., specific, situation will hold
if some hypothetical situation is true. Since hypo-
thetical situations do not denote specific instances,
they will usually be labelled as generic. Con-
cessions, meanwhile, usually describe how a spe-
cific situation holds, even though another (more
generic) situation would normally hold, that would
be inconsistent with this. For the Instantiation re-
lation category, it may once again be expected that
similar proportions of generic and specific events
would co-occur within their arguments, since an
instantiation describes a specific instance of a
more generic situation. However, contrary to these
84
AlternativeAsynchronousAttributionCauseConcessionConditionConjunctionContrastEntRelInstantiationListNoRelRestatementSynchronous
GenericSpecific AssertedOther NegativePositive FuturePast PresentUnspecif
ied
0
1
0.5
0.25
0.75
Genericity Modality Polarity Tense0
1
0.5
0.25
0.75
0
1
0.5
0.25
0.75
0
1
0.5
0.25
0.75
Figure 4: Heatmap showing the distribution of correlations between discourse relations and event-based
functional discourse categories. A darker shade indicates a smaller percentage of instances of a discourse
relation co-occurring with an event attribute.
expectations, the ratio of specific to generic events
is 3:1. A reason for this is that discourse argu-
ments corresponding to the description of a spe-
cific instance may contain several different events,
as illustrated in Example (1).
(1) Toefting has been convicted before. In
1999 he was given a 20-day suspended sentence
for assaulting a fan who berated him for
playing with German club Duisburg.
In terms of the Modality attribute, most dis-
course relations correlate with definite, asserted
events. Simillarly to the Genericity attribute, this
can be largely explained by the nature of the texts.
However, there are two relation types, i.e., Condi-
tion and Consession, which have particularly high
proportions of co-occurring events whose modal-
ity is other. Events that are assigned this attribute
value correspond to those that are not described as
though they are real occurrences. This includes,
e.g., speculated or hypothetical events. The fact
that Condition relations are usually hypothetical
in nature explain why 76% of events that co-occur
with such relations are assigned the other value
for the Modality attribute. Example (2) illustrates
a sentence containing this relation type.
(2) And I?ve said many times, IF we all
agreed on everything, everybody would want to
marry Betty and we would really be in a mess,
wouldn?t we, Bob.
An even higher proportion of Concession re-
lations co-occurs with events whose modality is
other. Example (3) helps to explain this. In the
first clause (the generic situation), the mention of
minimising civilian casualties is only described as
an effort, rather than a definite situation. The hedg-
ing of this generic situation is necessary in order to
concede that the more specific situation described
in the second clause could actually be true, i.e.,
that a large number of civilians have already been
killed. Due to the nature of news reporting, which
may come from potentially unreliable sources, the
killed event in this second clause is also hedged,
through the use of the word reportedly.
(3) ALTHOUGH the coalition leaders have
repeatedly assured that every effort would be
made to minimize civilian casualties in the
current Iraq war, at least 130 Iraqi civilians have
been reportedly killed since the war started five
days ago.
Almost 96% of events that co-occur with argu-
ments of discourse relations have positive polarity.
Indeed, for eight relation types, 100% of the cor-
responding events are positive. This can partly be
explained by the fact that, in texts reporting news,
85
there is an emphasis on reporting events that have
happened, rather than events that did not happen.
It can, however, be noted that events that co-occur
with certain discourse relation types have a greater
likelihood of having negative polarity. These rela-
tions include Contrast (9% of events having neg-
ative polarity) and Cause (5% negative events).
Contrasts can include comparisons of positive and
negative situations, as in Example (4), whilst for
Causes, it can sometimes be relevant to state that
a particular situation caused a specific event not to
take place, as shown in Example (5).
(4) The message from the Israeli government
is that its soldiers are not targeting journalists,
BUT that journalists who travel to places where
there could be live fire exchange between
Israeli forces and Palestinian gunmen have a
responsibility to take greater precautions.
(5) His father didn?t want to invade Iraq, BE-
CAUSE of all these problems they?re having
now.
For most relation types, around 60% of their co-
occurring events are annotated as describing past
tense situations. This nature of newswire and con-
versations mean that this is largely to be expected,
since they normally report mainly on events that
have already happened. The proportion of events
assigned the future tense value is highest when
they co-occur with discourse relations of type Al-
ternative. In this relation type, it is often the case
that one of the arguments describes a possible fu-
ture alternative to a current situation, as the case in
Example (6). This possible information pattern for
Alternative relations, where one of the arguments
represents a currently occurring situation, would
also help to explain why, even though very few
events in general are annotated as present tense,
almost 10% of events that co-occur with Alter-
native relations describe events that are currently
ongoing. As for events whose Tense value is un-
specified, two of the most common discourse re-
lation types with which they occur are Condition
and Concession. As exemplified above, Condition
relations are often hypothetical in nature, meaning
that no specific tense can be assigned. The generic
argument of a Concession relation can also remain
unmarked for tense. As in Example (3), it is not
clear whether the effort to minimise civilian casu-
alties has already been initiated, or will be initiated
in the future.
(6) Saddam wouldn?t be destroying missiles
UNLESS he thought he was going to be
destroyed if he didn?t.
6 Conclusions
Given the level of variability in existing discourse-
annotated corpora, it is meaningful for users to
identify the relative merits of different schemes.
In this paper, we have presented an extension of
the U-Compare infrastructure that facilitates the
comparison, integration and visualisation of doc-
uments annotated according to different annota-
tion schemes. U-Compare constructs multiple and
parallel annotation sub-workflows nested within a
single workflow, with each sub-workflow corre-
sponding to a distinct scheme. We have applied
the implemented method to visualise the similar-
ities and differences of two functional discourse
annotation schemes, namely CoreSC and GENIA-
MK. To demonstrate the integration of multiple
schemes in U-Compare, we developed a work-
flow that merged event annotations from the ACE
2005 corpus (which include certain types of func-
tional discourse information) with discourse rela-
tions obtained by an end-to-end parser. Moreover,
we have analysed the merged annotations obtained
by this workflow and this has allowed us to iden-
tify various correlations between the two different
types of discourse annotations.
Based on the intuition that there is comple-
mentary information across different types of dis-
course annotations, we intend to examine how the
integration of multiple discourse schemes, e.g.,
features obtained by merging annotations, affects
the performance of machine learners for discourse
analysis.
7 Acknowledgements
We are grateful to Dr. Ziheng Lin (Na-
tional University of Singapore) for providing us
with the discourse parser used for this work.
This work was partially funded by the Euro-
pean Community?s Seventh Framework Program
(FP7/2007-2013) [grant number 318736 (OSS-
METER)]; Engineering and Physical Sciences Re-
search Council [grant numbers EP/P505631/1,
EP/J50032X/1]; and MRC Text Mining and
Screening (MR/J005037/1).
86
References
Sophia Ananiadou, Sampo Pyysalo, Junichi Tsujii, and
Douglas B. Kell. 2010. Event extraction for sys-
tems biology by text mining the literature. Trends in
Biotechnology, 28(7):381 ? 390.
Riza Theresa B. Batista-Navarro, Georgios Kontonat-
sios, Claudiu Miha?ila?, Paul Thompson, Rafal Rak,
Raheel Nawaz, Ioannis Korkontzelos, and Sophia
Ananiadou. 2013. Facilitating the analysis of dis-
course phenomena in an interoperable NLP plat-
form. In Computational Linguistics and Intelligent
Text Processing, volume 7816 of Lecture Notes in
Computer Science, pages 559?571. Springer Berlin
Heidelberg, March.
William A. Baumgartner, Kevin Bretonnel Cohen, and
Lawrence Hunter. 2008. An open-source frame-
work for large-scale, flexible evaluation of biomedi-
cal text mining systems. Journal of biomedical dis-
covery and collaboration, 3:1+, January.
Lynn Carlson, Daniel Marcu, and Mary Ellen
Okurowski. 2001. Building a discourse-tagged cor-
pus in the framework of Rhetorical Structure Theory.
In Proceedings of the Second SIGdial Workshop on
Discourse and Dialogue - Volume 16, SIGDIAL ?01,
pages 1?10, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Hamish Cunningham, Diana Maynard, Kalina
Bontcheva, and Valentin Tablan. 2002. GATE:
an architecture for development of robust HLT
applications. In In Recent Advanced in Language
Processing, pages 168?175.
Anita de Waard and Henk Pander Maat. 2009. Epis-
temic segment types in biology research articles. In
Proceedings of the Workshop on Linguistic and Psy-
cholinguistic Approaches to Text Structuring (LPTS
2009).
Anita de Waard. 2007. A pragmatic structure for re-
search articles. In Proceedings of the 2nd inter-
national conference on Pragmatic web, ICPW ?07,
pages 83?89, New York, NY, USA. ACM.
David Ferrucci and Adam Lally. 2004. Building an
example application with the unstructured informa-
tion management architecture. IBM Systems Jour-
nal, 43(3):455?475.
Ralph Grishman. 1996. TIPSTER Text Phase II archi-
tecture design version 2.1p 19 june 1996. In Pro-
ceedings of the TIPSTER Text Program: Phase II,
pages 249?305, Vienna, Virginia, USA, May. Asso-
ciation for Computational Linguistics.
Yufan Guo, Anna Korhonen, Maria Liakata,
Ilona Silins Karolinska, Lin Sun, and Ulla Ste-
nius. 2010. Identifying the information structure of
scientific abstracts: An investigation of three differ-
ent schemes. In Proceedings of the 2010 Workshop
on Biomedical Natural Language Processing, pages
99?107. Association for Computational Linguistics.
Iryna Gurevych, Max Mu?hlha?user, Christof Mu?ller,
Ju?rgen Steimle, Markus Weimer, and Torsten Zesch.
2007. Darmstadt knowledge processing repository
based on UIMA. In Proceedings of the First Work-
shop on Unstructured Information Management Ar-
chitecture at Biannual Conference of the GSCL.
Udo Hahn, Ekaterina Buyko, Rico Landefeld, Matthias
Mu?hlhausen, Michael Poprat, Katrin Tomanek, and
Joachim Wermter. 2008. An overview of JCoRe,
the JULIE lab UIMA component repository. In
LREC?08 Workshop ?Towards Enhanced Interoper-
ability for Large HLT Systems: UIMA for NLP?,
pages 1?7, Marrakech, Morocco, May.
Yoshinobu Kano, Makoto Miwa, Kevin Cohen,
Lawrence Hunter, Sophia Ananiadou, and Jun?ichi
Tsujii. 2011. U-Compare: A modular NLP work-
flow construction and evaluation system. IBM Jour-
nal of Research and Development, 55(3):11.
Maria Liakata, Simone Teufel, Advaith Siddharthan,
and Colin Batchelor. 2010. Corpora for the concep-
tualisation and zoning of scientific papers. In Pro-
ceedings of LREC, volume 10.
Maria Liakata, Shyamasree Saha, Simon Dobnik,
Colin Batchelor, and Dietrich Rebholz-Schuhmann.
2012a. Automatic recognition of conceptualization
zones in scientific articles and two life science appli-
cations. Bioinformatics, 28(7):991?1000.
Maria Liakata, Paul Thompson, Anita de Waard, Ra-
heel Nawaz, Henk Pander Maat, and Sophia Ana-
niadou. 2012b. A three-way perspective on scien-
tic discourse annotation for knowledge extraction.
In Proceedings of the ACL Workshop on Detecting
Structure in Scholarly Discourse (DSSD), pages 37?
46, July.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2012. A
PDTB-styled end-to-end discourse parser. Natural
Language Engineering, FirstView:1?34, 10.
William C. Mann and Sandra A. Thompson. 1988.
Rhetorical Structure Theory: Toward a functional
theory of text organization. Text, 8(3):243?281.
Claudiu Miha?ila?, Tomoko Ohta, Sampo Pyysalo, and
Sophia Ananiadou. 2013. BioCause: Annotating
and analysing causality in the biomedical domain.
BMC Bioinformatics, 14(1):2, January.
Yoko Mizuta, Anna Korhonen, Tony Mullen, and Nigel
Collier. 2006. Zone analysis in biology articles as a
basis for information extraction. International Jour-
nal of Medical Informatics, 75(6):468 ? 487. Re-
cent Advances in Natural Language Processing for
Biomedical Applications Special Issue.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bon-
nie Webber. 2008. The Penn Discourse Tree-
Bank 2.0. In Nicoletta Calzolari, Khalid Choukri,
Bente Maegaard, Joseph Mariani, Jan Odjik, Stelios
87
Piperidis, and Daniel Tapias, editors, In Proceedings
of the 6th International Conference on language Re-
sources and Evaluation (LREC), pages 2961?2968.
Rashmi Prasad, Susan McRoy, Nadya Frid, Aravind
Joshi, and Hong Yu. 2011. The biomedical
discourse relation bank. BMC Bioinformatics,
12(1):188.
Rafal Rak, Andrew Rowley, and Sophia Ananiadou.
2012a. Collaborative development and evaluation
of text-processing workflows in a UIMA-supported
web-based workbench.
Rafal Rak, Andrew Rowley, William Black, and Sophia
Ananiadou. 2012b. Argo: an integrative, in-
teractive, text mining-based workbench supporting
curation. Database: The Journal of Biological
Databases and Curation, 2012.
A?gnes Sa?ndor and Anita de Waard. 2012. Identifying
claimed knowledge updates in biomedical research
articles. In Proceedings of the Workshop on De-
tecting Structure in Scholarly Discourse, ACL ?12,
pages 10?17, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Guergana Savova, James Masanz, Philip Ogren, Jiap-
ing Zheng, Sunghwan Sohn, Karin Kipper-Schuler,
and Christopher Chute. 2010. Mayo clini-
cal text analysis and knowledge extraction system
(cTAKES): architecture, component evaluation and
applications. Journal of the American Medical In-
formatics Association, 17(5):507?513.
Ulrich Scha?fer. 2006. Middleware for creating and
combining multi-dimensional nlp markup. In Pro-
ceedings of the 5th Workshop on NLP and XML:
Multi-Dimensional Markup in Natural Language
Processing, pages 81?84. ACL.
Pontus Stenetorp, Sampo Pyysalo, Goran Topic?,
Tomoko Ohta, Sophia Ananiadou, and Jun?ichi Tsu-
jii. 2012. brat: a web-based tool for NLP-assisted
text annotation. In Proceedings of the Demonstra-
tions Session at EACL 2012, Avignon, France, April.
Association for Computational Linguistics.
Simone Teufel, Jean Carletta, and Marc Moens. 1999.
An annotation scheme for discourse-level argumen-
tation in research articles. In Proceedings of the
ninth conference on European chapter of the Asso-
ciation for Computational Linguistics, EACL ?99,
pages 110?117, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Paul Thompson, Raheel Nawaz, John McNaught, and
Sophia Ananiadou. 2011. Enriching a biomedi-
cal event corpus with meta-knowledge annotation.
BMC Bioinformatics, 12(1):393.
Christopher Walker. 2006. ACE 2005 Multilingual
Training Corpus.
W John Wilbur, Andrey Rzhetsky, and Hagit Shatkay.
2006. New directions in biomedical text annota-
tion: definitions, guidelines and corpus construction.
BMC Bioinformatics, 7(1):356.
88
Proceedings of the 6th Workshop on Building and Using Comparable Corpora, pages 95?104,
Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational Linguistics
Using a Random Forest Classifier to recognise translations of biomedical
terms across languages
Georgios Kontonatsios1,2 Ioannis Korkontzelos1,2 Jun?ichi Tsujii3 Sophia Ananiadou1,2
National Centre for Text Mining, University of Manchester, Manchester, UK1
School of Computer Science, University of Manchester, Manchester, UK2
Microsoft Research Asia, Beijing, China3
{gkontonatsios,ikorkontzelos,sananiadou}@cs.man.ac.uk
jtsujii@microsoft.com
Abstract
We present a novel method to recognise
semantic equivalents of biomedical terms
in language pairs. We hypothesise that
biomedical term are formed by seman-
tically similar textual units across lan-
guages. Based on this hypothesis, we
employ a Random Forest (RF) classifier
that is able to automatically mine higher
order associations between textual units
of the source and target language when
trained on a corpus of both positive and
negative examples. We apply our method
on two language pairs: one that uses the
same character set and another with a dif-
ferent script, English-French and English-
Chinese, respectively. We show that
English-French pairs of terms are highly
transliterated in contrast to the English-
Chinese pairs. Nonetheless, our method
performs robustly on both cases. We eval-
uate RF against a state-of-the-art align-
ment method, GIZA++, and we report a
statistically significant improvement. Fi-
nally, we compare RF against Support
Vector Machines and analyse our results.
1 Introduction
Given a term in a source language and term in a
target language the task of this paper is to classify
this pair as a translation or not. We investigate the
performance of the proposed classifier by apply-
ing it on a balanced classification problem, i.e. our
experimental datasets contain an equal number of
positive and negative examples. The proposed
classification model can be used as a component of
a larger system that automatically compiles bilin-
gual dictionaries of technical terms across lan-
guages. Bilingual dictionaries of terms are impor-
tant resources for many Natural Language Pro-
cessing (NLP) applications including Statistical
Machine Translation (SMT) (Feng et al, 2004;
Huang and Vogel, 2002; Wu et al, 2008), Cross-
Language Information Retrieval (Ballesteros and
Croft, 1997) and Question Answering systems
(Al-Onaizan and Knight, 2002). Especially in the
biomedical domain, manually creating and more
importantly updating such resources is an expen-
sive process, due to the vast amount of neologisms,
i.e. newly introduced terms (Pustejovsky et al,
2001). The UMLS metathesaurus which is one the
most popular hub of multilingual resources in the
biomedical domain, contains technical terms in 21
languages that are linked together using a con-
cept identifier. In Spanish, the second most popu-
lar language in UMLS, only 16.44% of the 7.6M
English terms are covered while other languages
fluctuate between 0.0052% (for Hebrew terms) to
3.26% (for Japanese terms). Hence, these lex-
ica are far for complete and methods that semi-
automatically (i.e., in a post-processing step, cu-
rators can manually remove erroneous dictionary
entries) discover pairs of terms across languages
are needed to enrich such multilingual resources.
Our method can be applied to parallel, aligned cor-
pora, where we expect approximately the same,
balanced classification problem. However, in
comparable corpora the search space of candidate
alignments is of vast size, i.e., quadratic the the
size of the input data. To cope with this heavily
unbalanced classification problem, we would need
to narrow down the number of negative instances
before classification.
We hypothesise that there are language in-
dependent rules that apply to biomedical terms
across many languages. Often the same or simi-
lar textual units (e.g., morphemes and suffixes) are
concatenated to realise the same terms in different
languages. For example, Table 1 illustrates how
a morpheme expressing pain (ache in English) is
used to realise the same terms in English, Chinese
and French. The realisations of the term ?head-
95
English Morpheme: -ache Chinese Morpheme: ? French Morpheme: -mal
head-ache ?-? mal de te?te
back-ache ?-? mal au dos
ear-ache ??-? mal d?oreille
Table 1: An example of English, Chinese and French terms consisting of the same morphemes
ache? is expected to consist of the units for ?head?
and ?ache? regardless of the language of realisa-
tion. Hence, knowing the translations of ?head?
and ?ache? allows the reconstruction ?headache?
in a target language.
In our method, we use a Random Forest (RF) clas-
sifier (Breiman, 2001) to learn the underlying rules
according to which terms are being constructed
across languages. An RF is an ensemble of De-
cision Trees voting for the most popular class. RF
classifiers are popular in the biomedical domain
for various tasks: classification of microarray data
(D??az-Uriarte and De Andres, 2006), compound
classification in cheminformatics (Svetnik et al,
2003), classification of microRNA data (Jiang et
al., 2007) and protein-protein interactions in Sys-
tems Biology (Chen and Liu, 2005). In NLP, RF
classifiers have been used for: Language Mod-
elling (Xu and Jelinek, 2004) and semantic pars-
ing (Nielsen and Pradhan, 2004). To the best of
the authors? knowledge, this is the first attempt to
employ RF for identifying translation equivalents
of biomedical terms.
We prefer RF over other traditional machine learn-
ing approaches such as Support Vector Machines
(SVMs) for a number of reasons. Firstly, RF is
able to automatically construct correlation paths
from the feature space, i.e. decision rules that cor-
respond to the translation rules that we intend
to capture. Secondly, RF is considered one of
the most accurate classifier available (D??az-Uriarte
and De Andres, 2006; Jiang et al, 2007). Finally,
RF is reported to cope well with datasets where the
number of features is larger than the number of ob-
servations (D??az-Uriarte and De Andres, 2006). In
our dataset, the number of features is almost four
times more than that of the observations.
We represent pairs of terms using character gram
features (i.e., first order features). Such shal-
low features have been proven effective in a num-
ber of NLP applications including: Named En-
tity Recognition (Klein et al, 2003), Multilin-
gual Named Entity Transliteration (Klementiev
and Roth, 2006; Freitag and Khadivi, 2007) and
predicting authorship (Stamatatos, 2006). In ad-
dition, by selecting character n-grams instead of
word n-grams, one avoids to segment words in
Chinese which has been proven to be a challenging
topic (Sproat and Emerson, 2003). We evaluate
our proposed method on two datasets of biomed-
ical terms (English-French and English-Chinese)
that contain equal numbers of positive and neg-
ative instances. RF achieves higher classifica-
tion performance than baseline methods. To boost
SVM?s performance further, we used a second or-
der feature space to represent the data. It consists
of pairs of character grams that co-occur in trans-
lation pairs. In the second order feature space, the
performance of SVMs improved significantly.
The rest of the paper is structured as follows. In
Section 2, we present previous approaches in iden-
tifying translation equivalents of terms or named
entities. In Section 3, we define the classifica-
tion problem, we formulate the RF classifier and
we discuss the first and second order feature space
that we use to represent pairs of terms. In Sec-
tion 4, we show that RF achieves superior classi-
fication performance. In Section 5, we overview
our method and we discuss how it can be used to
compile large-scale bilingual dictionaries of terms
from comparable corpora.
2 Related Work
In this section, we review previous approaches
that exploit the internal structure of sequences to
align terms or named entities across languages.
(Klementiev and Roth, 2006; Freitag and Khadivi,
2007) use character gram features, similar to the
feature space that we propose in this paper, to train
discriminative, supervised models. Klementiev
and Roth (2006) introduce a supervised Percep-
tron model for English and Russian named enti-
ties. They construct a character gram feature space
as follows: firstly, they extract all distinct charac-
ter grams from both source and target named en-
tity. Then, they pair character grams of the source
named entity with character grams of the corre-
sponding target named entity into features. In or-
96
der to reduce the number of features, they link
only those character grams whose position offsets
in the source and target sequence differs by -1, 0
or 1. Freitag and Khadivi (2007) employ the same
character gram feature space but they do not con-
straint the included character-grams to their rela-
tive position offsets in the source and target se-
quence. The boolean features are defined for ev-
ery distinct character-grams observed in the data
of length k or shorter. Using this feature space
they train an Averaged Perceptron model, able to
incorporate an arbitrary number of features in the
input vectors, for English and Arabic named en-
tities. The above character gram based methods
mainly focused on aligning named entities of the
general domain, i.e. person names, locations, or-
ganizations, etc., that are transliterated, i.e. present
phonetic similarities, across languages.
SMT-based approaches built on top of existing
SMT frameworks to identify translation pairs of
terms (Tsunakawa et al, 2008; Wu et al, 2008).
Tsunakawa et al (2008), align terms between
a source language Ls and a target language Lt
using a pivot language Lp. They assume that
two bilingual dictionaries exist: from Ls to Lp
and from Lp to Lt. Then, they train GIZA++
(Och and Ney, 2003) on both directions and they
merge the resulting phrase tables into one table
between Ls and Lt, using grow-diag-final heuris-
tics (Koehn et al, 2007). Wu et al (2008), use
morphemes instead of words as translation units
to train a phrase based SMT system for technical
terms in English and Chinese. The use of shorter
lexical fragments, e.g. lemmas, stems and suf-
fixes, as translation units has reportedly reduced
the Out-Of-Vocabulary problem (Virpioja et al,
2007; Popovic and Ney, 2004; Oflazer and El-
Kahlout, 2007).
Hybrid methods exploit that a term or a named en-
tity can be translated in various ways across lan-
guages (Shao and Ng, 2004; Feng et al, 2004; Lu
and Zhao, 2006). For instance, person names are
usually translated by transliteration (i.e., words
exhibiting pronunciation similarities across lan-
guages, are likely to be mutual translations) while
technical terms are likely to be translated by
meaning (i.e., the same semantic units are used to
generate the translation of the term in the target
language). The resulting hybrid systems were re-
ported to perform at least as well as existing SMT
systems (Feng et al, 2004).
Lepage and Denoual (2005) presented an analog-
ical learning machine translation system as part
of the IWSLT task (Eck and Hori, 2005) that re-
quires no training process and it is able to achieve
state-of-the art performance. The core method
of their system models relationships between se-
quences of characters, e.g., sentences, phrases or
words, across languages using proportional analo-
gies, i.e., [a : b = c : d], ?a is to b as c is to d?, and
is able to solve unknown analogical equations,
i.e., [x : y = z :?] (Lepage, 1998). Analogical
learning has been proven effective in translating
unseen words (Langlais and Patry, 2007). Further-
more, analogical learning is reported to achieve a
better precision but a lower recall than a phrase-
based machine translation system when translating
medical terms (Langlais et al, 2009).
3 Methodology
Let em = (e1, ? ? ? , em) be an English term
consisting of m translation units and fn =
(f1, ? ? ? , fn) a French or Chinese term consist-
ing of n units. As translation units, we con-
sider character grams. We define a function f :
(em, fn) ?? {0, 1}:
f(em, fn) =
{
1, if em translates into fn
0, otherwise
The function can be learned by training a Random
Forest (RF) classifier1. Let N be the number of
training instances, |?| the total number of features,
i.e. the number of dimensions of the feature space,
|? | a predefined number of random decision trees
and |?| a predefined number of random features.
An RF classifier is defined as a collection of fully
grown decision tree classifiers, ?i(X) (Breiman,
2001):
RF = {?1(X), ? ? ? , ?? (X)}, X = (e
m, chn)
(1)
A pair of terms is classified as a translation pair
if the majority of the trees is voting for this class
label. Let I(?i(X)) be the vote of the ith tree
in the forest and avj?{0,1} the average number of
votes for class labels 0 (translation) and 1 (non-
translation). The function f of ? decision trees
can be written as the majority function:
f(em, chn) = Maj (I(?1(X)), ? ? ? , I(?? (X)))
=
?
1
2
??
1 I(?i(X)) + 1/2(?1)
r
?
?
(2)
1The WEKA implementation (Hall et al, 2009) of RF was
used for all experiments of this paper.
97
The majority function returns 1 if the majority
of I(?i(X)) is 1, or returns 0 if the majority of
I(?i(X)) is 0. Adding or subtracting 1/2 controls
whether a tie is resolved towards 1 or 0, respec-
tively. In RF ties are resolved randomly. To rep-
resent this, the negative unit (?1) is raised to a
randomly chosen positive integer r ? N+.
We tuned the RF classifier using 140 random
trees and |?| = log2 |?|+ 1 features as suggested
in Breiman (Breiman, 2001).
The RF mechanism that triggers term construction
rules across languages lies in the decision trees.
A RF grows a decision tree by selecting the most
informative feature, i.e. corresponding to the
lowest entropy, out of ? random features. For
each selected feature, a node is created and this
process is repeated for all ? random features of
the unprunned decision trees. In other words, the
process starts with the most informative feature
and builds association rules between all random
features. These are the construction rules that
we are interested in. Figure 1 illustrates a path
in one of the decision trees of an RF classifier
taken from the experiments we conducted on
the English-Chinese dataset. In only one of
thousands of branches of the forest, the classifier
is able to partially trigger the construction rule of
kinase, a type of enzyme, between English and
Chinese. The translation rule correctly associates
the English n-grams kin and as with their Chinese
translation ??. In addition, the translation rule
contains both positive and negative associations
between features. The English n-grams ing and
or are negatively correlated with the term kinase.
3.1 Feature Engineering
Each pair of terms is represented as a feature vec-
tor of character n-grams. We further define two
types of character n-gram features, namely first
order and second order. First order character n-
grams are boolean features that designate the oc-
currence of a corresponding character gram of pre-
defined length in the input term. These features are
monolingual, extracted separately from the source
and target term. The RF classifier is shown to ben-
efit from only monolingual features and achieves
the best observed performance. In contrast, SVMs
were shown not to perform well using the first or-
der feature space because they cannot directly as-
sociate the source with the target character grams.
To enhance the performance of SVMs, we con-
structed a second order feature space that contains
associations between first order features. A sec-
ond order feature is a tuple of a source and a tar-
get character gram that co-occur in one or more
translation pairs. Table 2 illustrates an example.
Second order character n-grams are multilingual
features and are defined over true translation pairs.
For this reason, we extract second order features
from the training data only.
In all experiments, the features were sorted in de-
creasing order of frequency of occurrence. We
trained a RF and two SVM classifiers, namely
linear-SVM and RBF-SVM, using a gradually in-
creasing number of features, always starting from
the top of the list. SMT frameworks cannot be
trained on an increasing number of features be-
cause each training instance needs to correspond
to at least one known translation unit (i.e., first or-
der features). Therefore, GIZA++ is trained on the
complete set of translation units.
4 Experiments
In this section, we discuss the employed datasets
of biomedical terms in English-French and
English-Chinese and three baseline methods. We
compare and discuss RF and SVMs trained on the
first order and second order features. Finally, we
report results of all classification methods evalu-
ated on the same datasets.
4.1 Datasets
For our experiments, we used an online bilin-
gual dictionary2 for English-Chinese terms and the
UMLS metathesaurus3 for English-French terms.
The former contains 31, 700 entries while the lat-
ter is a much larger dictionary containing 84, 000
entries. For training, we used the same number of
instances for both language pairs (i.e., 21, 000 en-
tries) in order not to bias the performance towards
the larger English-French dataset. The remain-
ing instances were used for testing (i.e., 10, 7000
and 63, 000 English-Chinese and English-French
respectively). In the case where a source term cor-
responded to more that one target terms according
to the seed dictionary, we randomly selected only
one translation. Negative instances were created
by randomly matching non-translation pairs of
terms. Since we are dealing with a balanced clas-
2www2.chkd.cnki.net/kns50/
3nlm.nih.gov/research/umls
98
Figure 1: Example of a term construction rule as a branch in a decision tree.
Input pair of English-French terms : (e1, e2, e3, f1, f2, f3)
English first order French first order Second order
?1(e1, e2) ?1(f1, f2) ?1(e1e2, f1f2), ?1(e1e2, f2f3)
?1(e2, e3) ?1(f2, f3) ?1(e2e3, f1f2), ?1(e2e3, f2f3)
Table 2: Example of first and second order features using a predefined n-gram size of 2.
sification problem, we created as many negative
instances as the positive ones in all our datasets.
In all experiments we performed a 3-fold cross-
validation.
4.2 Baselines
We evaluated RF against three classification meth-
ods, namely SVMs, GIZA++ and a Levenshtein
distance-based classifier.
SVMs coordinate a hyperplane in the hyperspace
defined by the features to best separate the posi-
tive and negative instances, i.e. aligned from non-
aligned pairs. In contrast to RF, SVMs do not sup-
port building association rules between features,
i.e., translation units, which in our task seems to be
a deficiency. SVMs produce one final association
rule, i.e. the classification boundary which sepa-
rates positive from negative examples. Its abil-
ity to distinguish aligned from non-aligned pair
of terms depends on how separable the two clus-
ters are. We evaluated several settings for the
SVM classifier. Apart from the default linear ker-
nel function, we applied a radial basis function,
i.e. RBF-SVM. RBF-SVM uses the kernel trick to
project the instances in a higher dimensional space
to better separate the two clusters. While tuning
the SVM?s classification cost C, we observed op-
timal performance for a value of 100. Secondly,
we seeded the association rules of translation units
to the SVM classifier by creating a second or-
der feature space, discussed in detail in section
3.1. We employed the LIBSVM implementation
(Chang and Lin, 2011) of SVMs using both the
linear and RBF kernels.
The second baseline method is GIZA++, an
open source implementation of the 5 IBM-models
(Brown et al, 1993). GIZA++ is traditionally
trained on a bilingual, parallel corpus of aligned
sentences and estimates the probability P (s|t) of a
source translation unit (typically a word), s, given
a target unit t. To apply GIZA++ on our dataset,
we consider the list of terms as parallel sentences.
GIZA++, trained on a list of terms, estimates
the alignment probability of English-Chinese and
English-French textual units, i.e. character n-
grams. Each entry i, j in the translation table
is the probability P (si|tj), where si and tj are
the source and target character n-grams in row i
and column j, respectively. Further details about
training a SMT toolkit for aligning technical terms
can be found in (Tsunakawa et al, 2008; Freitag
and Khadivi, 2007; Wu et al, 2008). After train-
ing GIZA++ we estimate the posterior probabil-
ity P (cfn|em) that a test, Chinese or French term
cfn = {cf1, ? ? ? , cfn} is aligned with a given En-
glish term em = {e1, ? ? ? , em} as follows:
p(cfn|em) = n?m
n?
i=1
m?
j=1
P (cfi|ej) (3)
A threshold ? was defined to classify a pair of
terms into translations or non-translations:
f(em, cfn) =
{
1, if p(cfn|em) ? ?
0, otherwise
(4)
We experimented with different values of ?
(greedy search) and we selected a value that max-
imizes classification performance.
In order to estimate how phonetically similar the
two language pairs are, we employed a third base-
99
(a) English-French dataset (b) English-Chinese dataset
Figure 2: F-Score of the RF and SVM, GIZA++ and Levenshtein distance-based classifier on the first
order dataset
line method that uses the Edit/Levenshtein dis-
tance of pairs of terms to classify instances as
translations or not. The Levenshtein distance is
defined as the minimum edit operations, i.e., inser-
tion, deletions and substitution, required to trans-
form one sequence of characters to another. We
cannot directly calculate the Levenshtein distance
between English-Chinese pairs of terms since the
two languages are using different scripts. There-
fore, before we applied the Levenshtein distance-
based classifier, we converted the Chinese terms
to their pinyin form, i.e., Romanization system of
Chinese characters. As with GIZA++, we selected
a threshold ? that maximizes the performance of
the classifier.
4.3 Results
We hypothesise that a RF classifier is able to form
association paths between first order features. We
also have the theoretical intuition that SVM clas-
sifiers are not able to form such association paths.
As a result, we expect limited performance on the
first order feature set, because it does not contain
any associations among character grams.
Figure 2 shows the F-Score achieved by RF, linear-
SVM, RBF-SVM, GIZA++ and Levenshtein/Edit
distance-based classifier on the English-French
and English-Chinese datasets. RF and SVMs are
trained on an increasing number of features. The
behaviour of the classifiers is approximately the
same in both datasets. Performance is greater on
the English-French dataset since English is more
similar to French than to Chinese.
We also observe that linear-SVM and RBF-SVM
do not behave consistently. RBF-SVM?s perfor-
mance quickly climbs to a maximum and after-
wards it declines while linear-SVM?s performance
is constantly increasing until it balances to a very
high error rate, almost corresponding to random
classification. The linear-SVM classifier performs
poorly using first order features only, indicating
that this feature space is non-linearly separable,
i.e. there exists no hyperplane that separates trans-
lation from non-translation instances. Contrary,
RBF-SVM is able to construct a higher dimen-
sional space by applying the kernel trick so as
to take full advantage of a small number of fre-
quent and informative first order features. In this
higher dimensional space of few but informative
first order features, the RBF-SVM classifier coor-
dinates a hyperplane that effectively separates pos-
itive from negative instances. However, increas-
ing the number of features introduces noise that
affects the performance.
The RF is able to profit from larger sets of first
order features; thus, its performance is continu-
ously increasing until it stabilises at 6, 000 fea-
tures. The branches of the decision trees are shown
to manage features correctly to construct most of
the translation rules. Increasing the size of the fea-
ture space minimises the classification error, be-
cause more translation rules that generalize well
on unseen data are constructed.
The bilingual dictionary that we use for our
experiments contains heterogeneous biomedical
terms of diverse semantic categories. For ex-
ample, our data-set contains common medical
terms such as Intellectual Products (e.g. Pain
Management, prise en charge de la douleur, ?
???) or complex biological concepts such
as Enzymes (e.g. homogentisate 1,2-dioxygenase,
100
(a) English-French dataset (b) English-Chinese dataset
Figure 3: F-Score of the RF and SVM, GIZA++ and Levenshtein distance-based classifier on the second
order dataset
English-French pairs English-Chinese pairs
P R F1 P R F1
GIZA++ 0.901 0.826 0.862 0.907 0.742 0.816
Levenshtein Distance 0.762 0.821 0.791 0.501 0.990 0.668
SVM -RBFsecond-order 0.946 0.884 0.914 0.750 0.899 0.818
Linear-SVMsecond-order 0.866 0.887 0.8763 0.765 0.893 0.824
RFfirst-order 0.962 0.874 0.916 0.779 0.940 0.851
Table 3: Best observed performance of RF, SVM and GIZA++ and Levenshtein Distance
acide homogentisique-oxydase, ???1,2-??
?). Therefore, we would expect poor perfor-
mance of the supervised methods using only a
small portion of the total set of first order features
due to the high diversity of the terms. For exam-
ple the morpheme ache/ mal/ ? is more frequent
in Disease or Syndrome named entities rather than
Enzyme named entities. However, the results indi-
cate that RF can generalize well on heterogeneous
terms. Figure 2 shows that the RF classifier out-
performs SMT based methods, using only 1000
features.
The Levenshtein distance-based classifier per-
forms considerably better in the English-French
dataset than in English-Chinese. In fact, its best
performance for the English-Chinese dataset is
achieved when classifying every pair of terms as
a translation, i.e. 100% recall but 50% precision.
In a second experiment, we attempted to explore
whether the performance of SVMs can be im-
proved by providing cross-language association
features. We employed the second order feature
set discussed in subsection 3.1. We used a constant
number of 6, 000 first order features, the num-
ber of features that achieved maximum F-Score
for RF in the previous experiment. Besides these
first order features, we added an increasing num-
ber of second order ones. Figure 3 shows the F-
Score curves of the RF, linear-SVM, RBF-SVM,
GIZA++ and Levenshtein distance using this fea-
ture space.
We observe that second order features improved
the performance of both SVMs considerably. In
contrast to the previous experiment, the two SVMs
present consistent bevaviour. Interestingly, the
performance of the RF slightly decreased when
using a small number of second order features.
A possible explanation of this behaviour is that
the second order associative features added noise,
since the RF had already formed the association
rules from first order features. In addition, for m
English and n Chinese or French first order fea-
tures there were m ? n possible combinations of
second order features as explained in Subsection
3.1. Hence, there was a large number of second
order features that we excluded from the train-
ing process. Consequently, decision tree branches
were populated with incomplete association rules
while the RF was able to form these associa-
tions automatically. Nevertheless, as more sec-
ond order features were added, more association
rules were explored and the RF performance in-
101
creased. Table 3 summarises the highest perfor-
mance achieved by the RF, SVMs, GIZA++ and
Levenshtein distance all trained and tested on the
same dataset. The resulting performance of the RF
compared with GIZA++ is statistically significant
(p < 0.0001) in all experiments. Comparing the
RF with the SVMs, we note that in the English-
French dataset, the performance of the SVM-RBF
is approximately the same with the performance
of our proposed method. However, this comes
with a cost. Firstly, SVMs can possibly achieve
a comparable performance to the RF when us-
ing multilingual, second order features. In con-
trast, our experiments show that RF benefit from
monolingual, first order features only. Secondly,
SVMs need a large number of additional multi-
lingual features, (6.000 second order features or
more) to perform similarly to RF. As a conse-
quence, the resulting models of the SVM classi-
fiers are more complex. We measured the aver-
age time needed by the two classifiers to decide
for a single pair of terms. The RF is approx-
imately 30 times faster than SVMs (on average
0.010 and 0.292 seconds, respectively). Finally,
in the English-Chinese dataset the RF performed
significantly better than both SVMs.
5 Discussion And Future Work
In this paper, we presented a novel classification
method that uses Random Forest (RF) to recognise
translations of biomedical terms across languages.
Our approach is based on the hypothesis that in
many languages, there exist some rules for com-
bining textual units, e.g. n-grams, to form biomed-
ical terms. Based on this assumption, we de-
fined a first order feature space of character grams
and demonstrated that an RF classifier is able to
discover such cross language translation rules for
terms. We experimented with two diverse lan-
guage pairs: English-French and English-Chinese.
In the former case, pairs of terms exhibit high pho-
netic similarity while in the latter case they do not.
Our results showed that the proposed method per-
forms robustly in both cases and achieves a signif-
icantly better performance than GIZA++. We also
evaluated Support Vector Machines (SVM) clas-
sifiers on the same first order feature space and
showed that they fail to form translation rules in
both language pairs, possibly because it cannot
associate first order features with each other suc-
cessfully. We attempted to boost the performance
of the SVM classifier by adding association evi-
dence of textual units to the features. We extracted
second order features from the training data and
we defined a new feature set consisting of both first
order and second order features. In this feature
space, the performance of the SVMs improved sig-
nificantly.
In addition to this, we observe from the reported
experiments that RF achieves a better F-Score per-
formance than GIZA++ in all datasets. Nonethe-
less, GIZA++ presents a better precision (but
lower recall) in one dataset, i.e., English/Chinese.
Based on this observation we plan to investigate
the performance of a hybrid system combining RF
with MT approaches.
One trivial approach to apply the proposed method
for compiling large-scale bilingual dictionaries of
terms from comparable corpora would be to di-
rectly classify all possible pairs of terms into
translations or non-translations. However, in
comparable corpora, the size of the search space
is quadratic to the input data. Therefore, the clas-
sification task is much more challenging since the
distribution of positive and negative instances is
highly skewed. To cope with the vast search space
of comparable corpora, we plan to incorporate
context-based approaches with the RF classifica-
tion method. Context-based approaches, such as
distributional vector similarity (Fung and McKe-
own, 1997; Rapp, 1995; Koehn and Knight, 2002;
Haghighi et al, 2008), can be used to limit the
number of candidate translations by filtering out
pairs of terms with low contextual similarity.
Finally, the proposed method can be also used to
online augment the phrase table of Statistical Ma-
chine Translation (SMT) in order to better han-
dle the Out-of-Vocabulary problem i.e. inability
to translate textual units that consist of one or
more words and do not occur in the training data
(Habash, 2008).
Acknowledgements
The work described in this paper is partially
funded by the European Community?s Seventh
Framework Program (FP7/2007-2013) under grant
agreement no. 318736 (OSSMETER).
102
References
Y. Al-Onaizan and K. Knight. 2002. Translating
named entities using monolingual and bilingual re-
sources. In Proceedings of the 40th Annual Meet-
ing on Association for Computational Linguistics,
pages 400?408. Association for Computational Lin-
guistics.
L. Ballesteros and W.B. Croft. 1997. Phrasal trans-
lation and query expansion techniques for cross-
language information retrieval. In ACM SIGIR Fo-
rum, volume 31, pages 84?91. ACM.
L. Breiman. 2001. Random forests. Machine learn-
ing, 45(1):5?32.
P.F. Brown, V.J.D. Pietra, S.A.D. Pietra, and R.L. Mer-
cer. 1993. The mathematics of statistical machine
translation: Parameter estimation. Computational
linguistics, 19(2):263?311.
C.C. Chang and C.J. Lin. 2011. Libsvm: a library
for support vector machines. ACM Transactions on
Intelligent Systems and Technology (TIST), 2(3):27.
X.W. Chen and M. Liu. 2005. Prediction of protein?
protein interactions using random decision forest
framework. Bioinformatics, 21(24):4394?4400.
R. D??az-Uriarte and S.A. De Andres. 2006. Gene se-
lection and classification of microarray data using
random forest. BMC bioinformatics, 7(1):3.
Matthias Eck and Chiori Hori. 2005. Overview of the
iwslt 2005 evaluation campaign. In Proc. of the In-
ternational Workshop on Spoken Language Transla-
tion, pages 1?22.
D. Feng, Y. Lv, and M. Zhou. 2004. A new approach
for english-chinese named entity alignment. In Em-
pirical Methods in Natural Language Processing,
pages 372?379.
D. Freitag and S. Khadivi. 2007. A sequence align-
ment model based on the averaged perceptron. In
Conference on Empirical methods in Natural Lan-
guage Processing, pages 238?247.
P. Fung and K. McKeown. 1997. A technical word-
and term-translation aid using noisy parallel cor-
pora across language groups. Machine Translation,
12(1):53?87.
N. Habash. 2008. Four techniques for online han-
dling of out-of-vocabulary words in arabic-english
statistical machine translation. In Proceedings of the
46th Annual Meeting of the Association for Compu-
tational Linguistics on Human Language Technolo-
gies: Short Papers, pages 57?60. Association for
Computational Linguistics.
A. Haghighi, P. Liang, T. Berg-Kirkpatrick, and
D. Klein. 2008. Learning bilingual lexicons
from monolingual corpora. Proceedings of ACL-08:
HLT, pages 771?779.
M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute-
mann, and I.H. Witten. 2009. The weka data mining
software: an update. ACM SIGKDD Explorations
Newsletter, 11(1):10?18.
F. Huang and S. Vogel. 2002. Improved named en-
tity translation and bilingual named entity extrac-
tion. In International Conference on Multimodal In-
teraction, pages 253?258. IEEE.
P. Jiang, H. Wu, W. Wang, W. Ma, X. Sun, and Z. Lu.
2007. Mipred: classification of real and pseudo
microrna precursors using random forest prediction
model with combined features. Nucleic acids re-
search, 35(suppl 2):W339?W344.
D. Klein, J. Smarr, H. Nguyen, and C.D. Manning.
2003. Named entity recognition with character-level
models. In Proceedings of the seventh conference
on Natural language learning at HLT-NAACL, pages
180?183. Association for Computational Linguis-
tics.
A. Klementiev and D. Roth. 2006. Weakly supervised
named entity transliteration and discovery from mul-
tilingual comparable corpora. In Proceedings of
the 21st International Conference on Computational
Linguistics and the 44th annual meeting of the Asso-
ciation for Computational Linguistics, pages 817?
824. Association for Computational Linguistics.
P. Koehn and K. Knight. 2002. Learning a transla-
tion lexicon from monolingual corpora. In Proceed-
ings of the ACL-02 workshop on Unsupervised lex-
ical acquisition-Volume 9, pages 9?16. Association
for Computational Linguistics.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, et al 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
pages 177?180. Association for Computational Lin-
guistics.
Philippe Langlais and Alexandre Patry. 2007. Trans-
lating unknown words by analogical learning. In
Proceedings of EMNLP-CoNLL, pages 877?886.
Philippe Langlais, Franc?ois Yvon, and Pierre Zweigen-
baum. 2009. Improvements in analogical learning:
application to translating multi-terms of the medical
domain. In Proceedings of the 12th Conference of
the European Chapter of the Association for Com-
putational Linguistics, pages 487?495. Association
for Computational Linguistics.
Yves Lepage. 1998. Solving analogies on words: an
algorithm. In Proceedings of the 17th international
conference on Computational linguistics-Volume 1,
pages 728?734. Association for Computational Lin-
guistics.
M. Lu and J. Zhao. 2006. Multi-feature based chinese-
english named entity extraction from comparable
corpora. pages 131?141.
103
R.D. Nielsen and S. Pradhan. 2004. Mixing weak
learners in semantic parsing. In Empirical Methods
in Natural Language Processing.
F.J. Och and H. Ney. 2003. A systematic comparison
of various statistical alignment models. Computa-
tional linguistics, 29(1):19?51.
K. Oflazer and I.D. El-Kahlout. 2007. Exploring
different representational units in english-to-turkish
statistical machine translation. In Proceedings of
the Second Workshop on Statistical Machine Trans-
lation, pages 25?32. Association for Computational
Linguistics.
Maja Popovic and Hermann Ney. 2004. Towards the
Use of Word Stems and Suffixes for Statistical Ma-
chine Translation. In 4th International Conference
on Language Resources and Evaluation (LREC),
pages 1585?1588, Lisbon,Portugal.
J. Pustejovsky, J. Castano, B. Cochran, M. Kotecki,
and M. Morrell. 2001. Automatic extraction
of acronym-meaning pairs from medline databases.
Studies in health technology and informatics,
(1):371?375.
R. Rapp. 1995. Identifying word translations in non-
parallel texts. In Proceedings of the 33rd annual
meeting on Association for Computational Linguis-
tics, pages 320?322. Association for Computational
Linguistics.
L. Shao and H.T. Ng. 2004. Mining new word trans-
lations from comparable corpora. In Proceedings
of the 20th international conference on Computa-
tional Linguistics, page 618. Association for Com-
putational Linguistics.
R. Sproat and T. Emerson. 2003. The first international
chinese word segmentation bakeoff. In Proceedings
of the second SIGHAN workshop on Chinese lan-
guage processing-Volume 17, pages 133?143. Asso-
ciation for Computational Linguistics.
Efstathios Stamatatos. 2006. Ensemble-based author
identification using character n-grams. In In Proc.
of the 3rd Int. Workshop on Textbased Information
Retrieval, pages 41?46.
V. Svetnik, A. Liaw, C. Tong, J.C. Culberson, R.P.
Sheridan, and B.P. Feuston. 2003. Random forest:
a classification and regression tool for compound
classification and qsar modeling. Journal of chemi-
cal information and computer sciences, 43(6):1947?
1958.
T. Tsunakawa, N. Okazaki, and J. Tsujii. 2008.
Building bilingual lexicons using lexical translation
probabilities via pivot languages. In Proceedings
of the Sixth International Conference on Language
Resources and Evaluation (LREC?08), Marrakech,
Morocco, may.
S. Virpioja, J.J. Va?yrynen, M. Creutz, and M. Sade-
niemi. 2007. Morphology-aware statistical machine
translation based on morphs induced in an unsu-
pervised manner. Machine Translation Summit XI,
2007:491?498.
X. Wu, N. Okazaki, T. Tsunakawa, and J. Tsujii. 2008.
Improving English-to-Chinese Translation for Tech-
nical Terms Using Morphological Information. In
AMTA-2008. MT at work: Proceedings of the Eighth
Conference of the Association for Machine Trans-
lation in the Americas, pages 202?211, Waikiki,
Hawai?i, October.
P. Xu and F. Jelinek. 2004. Random forests in lan-
guage modeling. In Empirical Methods in Natural
Language Processing, pages 325?332. Association
for Computational Linguistics.
104
