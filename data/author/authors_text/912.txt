Using Encyclopedic Knowledge for Named Entity Disambiguation
Razvan Bunescu
Department of Computer Sciences
University of Texas at Austin
Austin, TX 78712-0233
razvan@cs.utexas.edu
Marius Pas?ca
Google Inc.
1600 Amphitheatre Parkway
Mountain View, CA 94043
mars@google.com
Abstract
We present a new method for detecting and
disambiguating named entities in open do-
main text. A disambiguation SVM kernel
is trained to exploit the high coverage and
rich structure of the knowledge encoded
in an online encyclopedia. The resulting
model significantly outperforms a less in-
formed baseline.
1 Introduction
1.1 Motivation
The de-facto web search paradigm defines the re-
sult to a user?s query as roughly a set of links to the
best-matching documents selected out of billions
of items available. Whenever the queries search
for pinpointed, factual information, the burden
of filling the gap between the output granularity
(whole documents) and the targeted information (a
set of sentences or relevant phrases) stays with the
users, by browsing the returned documents in or-
der to find the actually relevant bits of information.
A frequent case are queries about named entities,
which constitute a significant fraction of popu-
lar web queries according to search engine logs.
When submitting queries such as John Williams
or Python, search engine users could also be pre-
sented with a compilation of facts and specific at-
tributes about those named entities, rather than a
set of best-matching web pages. One of the chal-
lenges in creating such an alternative search result
page is the inherent ambiguity of the queries, as
several instances of the same class (e.g., different
people) or different classes (e.g., a type of snake,
a programming language, or a movie) may share
the same name in the query. As an example, the
Work done during a summer internship at Google.
contexts below are part of web documents refer-
ring to different people who share the same name
John Williams:
1. ?John Williams and the Boston Pops con-
ducted a summer Star Wars concert at Tan-
glewood.?
2. ?John Williams lost a Taipei death match
against his brother, Axl Rotten.?
3. ?John Williams won a Victoria Cross for his
actions at the battle of Rorke?s Drift.?
The effectiveness of the search could be greatly
improved if the search results were grouped
together according to the corresponding sense,
rather than presented as a flat, sense-mixed list
of items (whether links to full-length documents,
or extracted facts). As an added benefit, users
would have easier access to a wider variety of re-
sults, whenever the top 10 or so results returned by
the largest search engines happen to refer to only
one particular (arguably the most popular) sense
of the query (e.g., the programming language in
the case of Python), thus submerging or ?hiding?
documents that refer to other senses of the query.
In various natural language applications, signif-
icant performance gains are achieved as a func-
tion of data size rather than algorithm complex-
ity, as illustrated by the increasingly popular use
of the web as a (very large) corpus (Dale, 2003).
It seems therefore natural to try to exploit the web
in order to also improve the performance of re-
lation extraction, i.e. the discovery of useful re-
lationships between named entities mentioned in
text documents. However, if one wants to combine
evidence from multiple web pages, then one needs
again to solve the name disambiguation problem.
9
Without solving it, a relation extraction system an-
alyzing the sentences in the above example could
mistakenly consider the third as evidence that John
Williams the composer fought at Rorke?s Drift.
1.2 Approach
The main goal of the research reported in this pa-
per is to develop a named entity disambiguation
method that is intrinsically linked to a dictionary
mapping proper names to their possible named en-
titiy denotations. More exactly, the method:
1. Detects whether a proper name refers to a
named entity included in the dictionary (de-
tection).
2. Disambiguates between multiple named enti-
ties that can be denoted by the same proper
name (disambiguation).
As a departure from the methodology of previous
approaches, the paper exploits a non-traditional
web-based resource. Concretely, it takes advan-
tage of some of the human knowledge available
in Wikipedia, a free online encyclopedia created
through decentralized, collective efforts of thou-
sands of users (Remy, 2002). We show that the
structure of Wikipedia lends itself to a set of
useful features for the detection and disambigua-
tion of named entities. The remainder of the pa-
per is organized as follows. Section 2 describes
Wikipedia, with an emphasis on the features that
are most important to the entity disambiguation
task. Section 3 describes the extraction of named
entity entries (versus other types of entries) from
Wikipedia. Section 4 introduces two disambigua-
tion methods, which are evaluated experimentally
in Section 5. We conclude with future work and
conclusions.
2 Wikipedia ? A Wiki Encyclopedia
Wikipedia is a free online encyclopedia written
collaboratively by volunteers, using a wiki soft-
ware that allows almost anyone to add and change
articles. It is a multilingual resource - there are
about 200 language editions with varying levels
of coverage. Wikipedia is a very dynamic and
quickly growing resource ? articles about news-
worthy events are often added within days of their
occurrence. As an example, the September 2005
version contains 751,666 articles, around 180,000
more articles than four months earlier. The work
in this paper is based on the English version from
May 2005, which contains 577,860 articles.
Each article in Wikipedia is uniquely identified
by its title ? a sequence of words separated by
underscores, with the first word always capital-
ized. Typically, the title is the most common name
for the entity described in the article. When the
name is ambiguous, it is further qualified with a
parenthetical expression. For instance, the arti-
cle on John Williams the composer has the title
John Williams (composer).
Because each article describes a specific en-
tity or concept, the remainder of the paper some-
times uses the term ?entity? interchangeably to re-
fer to both the article and the corresponding entity.
Also, let E denote the entire set of entities from
Wikipedia. For any entity e2E, e:title is the title
name of the corresponding article, and e:T is the
text of the article.
In general, there is a many-to-many correspon-
dence between names and entities. This relation
is captured in Wikipedia through redirect and dis-
ambiguation pages, as described in the next two
sections.
2.1 Redirect Pages
A redirect page exists for each alternative name
that can be used to refer to an entity in Wikipedia.
The name is transformed (using underscores for
spaces) into a title whose article contains a
redirect link to the actual article for that en-
tity. For example, John Towner Williams is the
full name of the composer John Williams. It
is therefore an alternative name for the com-
poser, and consequently the article with the ti-
tle John Towner Williams is just a pointer to the
article for John Williams (composer). An exam-
ple entry with a considerably higher number of
redirect pages is United States. Its redirect pages
correspond to acronyms (U.S.A., U.S., USA, US),
Spanish translations (Los Estados Unidos, Esta-
dos Unidos), misspellings (Untied States) or syn-
onyms (Yankee land).
For any given Wikipedia entity e2E, let e:R be
the set of all names that redirect to e.
2.2 Disambiguation Pages
Another useful structure is that of disambiguation
pages, which are created for ambiguous names,
i.e. names that denote two or more entities in
Wikipedia. For example, the disambiguation page
for the name John Williams lists 22 associated
10
TITLE REDIRECT DISAMBIG CATEGORIES
Star Wars music, ...
John Williams (composer) John Towner Williams John Williams Film score composers,
20th century classical composers
John Williams (wrestler) Ian Rotten John Williams Professional wrestlers,
People living in Baltimore
John Williams (VC) none John Williams British Army soldiers,
British Victoria Cross recipients
Boston Pops Orchestra Boston Pops, Pops American orchestras,
The Boston Pops Orchestra Massachusetts musicians
United States US, USA, ... US, USA, North American countries,
United States of America United States Republics, United States
Venus, Venus
Venus (planet) Planet Venus Morning Star, Planets of the Solar System,
Evening Star Planets, Solar System, ...
Table 1: Examples of Wikipedia titles, aliases and categories
entities. Therefore, besides the non-ambiguous
names that come from redirect pages, additional
aliases can be found by looking for all disam-
biguation pages that list a particular Wikipedia en-
tity. In his philosophical article ?On Sense and
Reference? (Frege, 1999), Gottlob Frege gave a
famous argument to show that sense and reference
are distinct. In his example, the planet Venus may
be referred to using the phrases ?morning star? and
?evening star?. This theoretical example is nicely
captured in practice in Wikipedia by two disam-
biguation pages, Morning Star and Evening Star,
both listing Venus as a potential referent.
For any given Wikipedia entity e 2 E, let e:D
be the set of names whose disambiguation pages
contain a link to e.
2.3 Categories
Every article in Wikipedia is required to have at
least one category. As shown in Table 1, John
Williams (composer) is associated with a set of
categories, among them Star Wars music, Film
score composers, and 20th century classical com-
posers. Categories allow articles to be placed into
one or more topics. These topics can be further
categorized by associating them with one or more
parent categories. In Table 1 Venus is shown as
both an article title and a category. As a cate-
gory, it has one direct parent Planets of the Solar
System, which in turn belongs to two more gen-
eral categories, Planets and Solar System. Thus,
categories form a directed acyclic graph, allowing
multiple categorization schemes to co-exist simul-
taneously. There are in total 59,759 categories in
Wikipedia.
For a given Wikipedia entity e 2E, let e:C be
the set of categories to which e belongs (i.e. e?s
immediate categories and all their ancestors in the
Wikipedia taxonomy).
2.4 Hyperlinks
Articles in Wikipedia often contain mentions of
entities that already have a corresponding arti-
cle. When contributing authors mention an ex-
isting Wikipedia entity inside an article, they are
required to link at least its first mention to the cor-
responding article, by using links or piped links.
Both types of links are exemplified in the follow-
ing wiki source code of a sentence from the article
on Italy: ?The [[Vatican City|Vatican]] is now an
independent enclave surrounded by [[Rome]]?.
The string from the second link (?Rome?) denotes
the title of the referenced article. The same string
is also used in the display version. If the author
wants another string displayed (e.g., ?Vatican? in-
stead of ?Vatican City?), then the alternative string
is included in a piped link, after the title string.
Consequently, the display string for the aforemen-
tioned example is: ?The Vatican is now an inde-
pendent enclave surrounded by Rome?. As de-
scribed later in Section 4, the hyperlinks can pro-
vide useful training examples for a named entity
disambiguator.
3 A Dictionary of Named Entities
We organize all named entities from Wikipedia
into a dictionary structure D, where each string
entry d 2 D is mapped to the set of entities
d:E that can be denoted by d in Wikipedia. The
first step is to identify named entities, i.e. entities
with a proper name title. Because every title in
Wikipedia must begin with a capital letter, the de-
cision whether a title is a proper name relies on the
following sequence of heuristic steps:
11
1. If e:title is a multiword title, check the cap-
italization of all content words, i.e. words
other than prepositions, determiners, con-
junctions, relative pronouns or negations.
Consider e a named entity if and only if all
content words are capitalized.
2. If e:title is a one word title that contains at
least two capital letters, then e is a named en-
tity. Otherwise, go to step 3.
3. Count how many times e:title occurs in the
text of the article, in positions other than at
the beginning of sentences. If at least 75% of
these occurrences are capitalized, then e is a
named entity.
The combined heuristics extract close to half a
million named entities from Wikipedia. The sec-
ond step constructs the actual dictionary D as fol-
lows:
 The set of entries in D consists of all strings
that may denote a named entity, i.e. if e2E
is a named entity, then its title name e:title,
its redirect names e:R, and its disambigua-
tion names e:D are all added as entries in D.
 Each entry string d2D is mapped to d:E, the
set of entities that d may denote in Wikipedia.
Consequently, a named entity e is included in
d:E if and only if d = e:title, d 2 e:R, or
d2e:D.
4 Named Entity Disambiguation
As illustrated in Section 1, the same proper name
may refer to more than one named entity. The
named entity dictionary from Section 3 and the hy-
perlinks from Wikipedia articles provide a dataset
of disambiguated occurrences of proper names,
as described in the following. As shown in Sec-
tion 2.4, each link contains the title name of an en-
tity, and the proper name (the display string) used
to refer to it. We use the term query to denote the
occurrence of a proper name inside a Wikipedia
article. If there is a dictionary entry matching the
proper name in the query q such that the set of
denoted entities q:E contains at least two entities,
one of them the true answer entity q:e, then the
query q is included in the dataset. More exactly, if
q:E contains n named entities e
1
, e
2
, ..., e
n
, then
the dataset will be augmented with n pairs hq; e
k
i
represented as follows:
hq; e
k
i = [?(e
k
; q:e) j q:T j e
k
:title]
The field q:T contains all words occurring in a
limit length window centered on the proper name.
The window size is set to 55, which is the value
that was observed to give optimum performance
in the related task of cross-document coreference
(Gooi and Allan, 2004). The Kronecker delta
function ?(e
k
; q:e) is 1 when e
k
is the same as
the entity q:e referred in the link. Table 2 lists
the query pairs created for the three John Williams
queries from Section 1.1, assuming only three en-
tities in Wikipedia correspond to this name.
? Query Text Entity Title
1 Boston Pops conduct ... John Williams (composer)
0 Boston Pops conduct ... John Williams (wrestler)
0 Boston Pops conduct ... John Williams (VC)
1 lost Taipei match ... John Williams (wrestler)
0 lost Taipei match ... John Williams (composer)
0 lost Taipei match ... John Williams (VC)
1 won Victoria Cross ... John Williams (VC)
0 won Victoria Cross ... John Williams (composer)
0 won Victoria Cross ... John Williams (wrestler)
Table 2: Disambiguation dataset.
The application of this procedure on Wikipedia
results into a dataset of 1,783,868 disambiguated
queries.
4.1 Context-Article Similarity
Using the representation from the previous sec-
tion, the name entity disambiguation problem can
be cast as a ranking problem. Assuming that an
appropriate scoring function score(q; e
k
) is avail-
able, the named entity corresponding to query q is
defined to be the one with the highest score:
e^ = argmax
e
k
score(q; e
k
) (1)
If e^ = q:e then e^ represents a hit, otherwise e^ is
a miss. Disambiguation methods will then differ
based on the way they define the scoring function.
One ranking function that is evaluated experimen-
tally in this paper is based on the cosine similarity
between the context of the query and the text of
the article:
score(q; e
k
) = cos(q:T; e
k
:T ) =
q:T
kq:Tk
e
k
:T
ke
k
:Tk
The factors q:T and e
k
:T are represented in the
standard vector space model, where each compo-
nent corresponds to a term in the vocabulary, and
the term weight is the standard tf  idf score
(Baeza-Yates and Ribeiro-Neto, 1999). The vo-
cabulary V is created by reading all Wikipedia
12
articles and recording, for each word stem w, its
document frequency df(w) in Wikipedia. Stop-
words and words that are too frequent or too rare
are discarded. A generic document d is then repre-
sented as a vector of length jV j, with a position for
each vocabulary word. If f(w) is the frequency of
word w in document d, and N is the total num-
ber of Wikipedia articles, then the weight of word
w2V in the tf  idf representation of d is:
d
w
= f(w) ln
N
df(w)
(2)
4.2 Taxonomy Kernel
An error analysis of the cosine-based ranking
method reveals that, in many cases, the pair hq; ei
fails to rank first, even though words from the
query context unambiguously indicate e as the ac-
tual denoted entity. In these cases, cue words from
the context do not appear in e?s article due to two
main reasons:
1. The article may be too short or incomplete.
2. Even though the article captures most of the
relevant concepts expressed in the query con-
text, it does this by employing synonymous
words or phrases.
The cosine similarity between q and e
k
can be seen
as an expression of the total degree of correlation
between words from the context of query q and a
given named entity e
k
. When the correlation is too
low because the Wikipedia article for named entity
e
k
does not contain all words that are relevant to
e
k
, it is worth considering the correlation between
context words and the categories to which e
k
be-
longs. For illustration, consider the two queries
for the name John Williams from Figure 1.
To avoid clutter, Figure 1 depicts only two enti-
ties with the name John Williams in Wikipedia: the
composer and the wrestler. On top of each entity,
the figure shows one of their Wikipedia categories
(Film score composers and Professional wrestlers
respectively), together with some of their ances-
tor categories in the Wikipedia taxonomy. The
two query contexts are shown at the bottom of
the figure. In the context on the left, words such
as conducted and concert denote concepts that are
highly correlated with the Musicians, Composers
and Film score composers categories. On the other
hand, their correlation with other categories in
Figure 1 is considerably lower. Consequently, a
Musicians
Composers
Film score composers
People by occupation
People
People known in connection
with sports and hobbies
Wrestlers
Professional wrestlers
high correlationshigh correlations
? ?
conducted
a summer Star Wars
John Williams John Williams
a Taipei death
lost
concert match[...] [...]
John Williams (composer) John Williams (wrestler)
Figure 1: Word-Category correlations.
goal of this paper is to design a disambiguation
method that 1) learns the magnitude of these cor-
relations, and 2) uses these correlations in a scor-
ing function, together with the cosine similarity.
Our intuition is that, given the query context on the
left, such a ranking function has a better chance
of ranking the ?composer? entity higher than the
?wrestler? entity, when compared with the simple
cosine similarity baseline.
We consider using a linear ranking function as
follows:
e^ = argmax
e
k
w (q; e
k
) (3)
The feature vector (q; e
k
) contains a dedicated
feature 
cos
for cosine similarity, and jV j  jCj
features 
w;c
corresponding to combinations of
words w from the Wikipedia vocabulary V and
categories c from the Wikipedia taxonomy C:

cos
(q; e
k
) = cos(q:T; e
k
:T ) (4)

w;c
(q; e
k
) =
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 724?731, Vancouver, October 2005. c?2005 Association for Computational Linguistics
A Shortest Path Dependency Kernel for Relation Extraction
Razvan C. Bunescu and Raymond J. Mooney
Department of Computer Sciences
University of Texas at Austin
1 University Station C0500
Austin, TX 78712
razvan,mooney@cs.utexas.edu
Abstract
We present a novel approach to relation
extraction, based on the observation that
the information required to assert a rela-
tionship between two named entities in
the same sentence is typically captured
by the shortest path between the two en-
tities in the dependency graph. Exper-
iments on extracting top-level relations
from the ACE (Automated Content Ex-
traction) newspaper corpus show that the
new shortest path dependency kernel out-
performs a recent approach based on de-
pendency tree kernels.
1 Introduction
One of the key tasks in natural language process-
ing is that of Information Extraction (IE), which is
traditionally divided into three subproblems: coref-
erence resolution, named entity recognition, and
relation extraction. Consequently, IE corpora are
typically annotated with information corresponding
to these subtasks (MUC (Grishman, 1995), ACE
(NIST, 2000)), facilitating the development of sys-
tems that target only one or a subset of the three
problems. In this paper we focus exclusively on ex-
tracting relations between predefined types of en-
tities in the ACE corpus. Reliably extracting re-
lations between entities in natural-language docu-
ments is still a difficult, unsolved problem, whose
inherent difficulty is compounded by the emergence
of new application domains, with new types of nar-
rative that challenge systems developed for previous
well-studied domains. The accuracy level of cur-
rent syntactic and semantic parsers on natural lan-
guage text from different domains limit the extent
to which syntactic and semantic information can be
used in real IE systems. Nevertheless, various lines
of work on relation extraction have shown experi-
mentally that the use of automatically derived syn-
tactic information can lead to significant improve-
ments in extraction accuracy. The amount of syntac-
tic knowledge used in IE systems varies from part-
of-speech only (Ray and Craven, 2001) to chunking
(Ray and Craven, 2001) to shallow parse trees (Ze-
lenko et al, 2003) to dependency trees derived from
full parse trees (Culotta and Sorensen, 2004). Even
though exhaustive experiments comparing the per-
formance of a relation extraction system based on
these four levels of syntactic information are yet to
be conducted, a reasonable assumption is that the ex-
traction accuracy increases with the amount of syn-
tactic information used. The performance however
depends not only on the amount of syntactic infor-
mation, but also on the details of the exact models
using this information. Training a machine learn-
ing system in a setting where the information used
for representing the examples is only partially rele-
vant to the actual task often leads to overfitting. It is
therefore important to design the IE system so that
the input data is stripped of unnecessary features as
much as possible. In the case of the tree kernels
from (Zelenko et al, 2003; Culotta and Sorensen,
2004), the authors reduce each relation example to
the smallest subtree in the parse or dependency tree
that includes both entities. We will show in this
paper that increased extraction performance can be
724
obtained by designing a kernel method that uses an
even smaller part of the dependency structure ? the
shortest path between the two entities in the undi-
rected version of the dependency graph.
2 Dependency Graphs
Let e1 and e2 be two entities mentioned in the same
sentence such that they are observed to be in a re-
lationship R, i.e R(e1, e2) = 1. For example, R
can specify that entity e1 is LOCATED (AT) entity
e2. Figure 1 shows two sample sentences from ACE,
with entity mentions in bold. Correspondingly, the
first column in Table 1 lists the four relations of type
LOCATED that need to be extracted by the IE sys-
tem. We assume that a relation is to be extracted
only between entities mentioned in the same sen-
tence, and that the presence or absence of a relation
is independent of the text preceding or following the
sentence. This means that only information derived
from the sentence including the two entities will be
relevant for relation extraction. Furthermore, with
each sentence we associate its dependency graph,
with words figured as nodes and word-word depen-
dencies figured as directed edges, as shown in Fig-
ure 1. A subset of these word-word dependencies
capture the predicate-argument relations present in
the sentence. Arguments are connected to their tar-
get predicates either directly through an arc point-
ing to the predicate (?troops ? raided?), or indirectly
through a preposition or infinitive particle (?warning
? to ? stop?). Other types of word-word dependen-
cies account for modifier-head relationships present
in adjective-noun compounds (?several ? stations?),
noun-noun compounds (?pumping ? stations?), or
adverb-verb constructions (?recently ? raided?). In
Figure 1 we show the full dependency graphs for two
sentences from the ACE newspaper corpus.
Word-word dependencies are typically catego-
rized in two classes as follows:
? [Local Dependencies] These correspond to lo-
cal predicate-argument (or head-modifier) con-
structions such as ?troops ? raided?, or ?pump-
ing ? stations? in Figure 1.
? [Non-local Dependencies] Long-distance de-
pendencies arise due to various linguistic con-
structions such as coordination, extraction,
raising and control. In Figure 1, among non-
local dependencies are ?troops ? warning?, or
?ministers ? preaching?.
A Context Free Grammar (CFG) parser can be
used to extract local dependencies, which for each
sentence form a dependency tree. Mildly context
sensitive formalisms such as Combinatory Catego-
rial Grammar (CCG) (Steedman, 2000) model word-
word dependencies more directly and can be used to
extract both local and long-distance dependencies,
giving rise to a directed acyclic graph, as illustrated
in Figure 1.
3 The Shortest Path Hypothesis
If e1 and e2 are two entities mentioned in the same
sentence such that they are observed to be in a rela-
tionship R, our hypothesis stipulates that the con-
tribution of the sentence dependency graph to es-
tablishing the relationship R(e1, e2) is almost exclu-
sively concentrated in the shortest path between e1
and e2 in the undirected version of the dependency
graph.
If entities e1 and e2 are arguments of the same
predicate, then the shortest path between them will
pass through the predicate, which may be con-
nected directly to the two entities, or indirectly
through prepositions. If e1 and e2 belong to different
predicate-argument structures that share a common
argument, then the shortest path will pass through
this argument. This is the case with the shortest path
between ?stations? and ?workers? in Figure 1, pass-
ing through ?protesters?, which is an argument com-
mon to both predicates ?holding? and ?seized?. In
Table 1 we show the paths corresponding to the four
relation instances encoded in the ACE corpus for the
two sentences from Figure 1. All these paths sup-
port the LOCATED relationship. For the first path, it
is reasonable to infer that if a PERSON entity (e.g.
?protesters?) is doing some action (e.g. ?seized?) to
a FACILITY entity (e.g. ?station?), then the PERSON
entity is LOCATED at that FACILITY entity. The sec-
ond path captures the fact that the same PERSON
entity (e.g. ?protesters?) is doing two actions (e.g.
?holding? and ?seized?) , one action to a PERSON en-
tity (e.g. ?workers?), and the other action to a FACIL-
ITY entity (e.g. ?station?). A reasonable inference in
this case is that the ?workers? are LOCATED at the
725
S1 =
=S2
Protesters stations workers
Troops churches ministers
seized   several   pumping , holding   127   Shell hostage .
recently   have   raided , warning to   stop   preaching .
Figure 1: Sentences as dependency graphs.
Relation Instance Shortest Path in Undirected Dependency Graph
S1: protesters AT stations protesters ?? seized ?? stations
S1: workers AT stations workers ?? holding ?? protesters ?? seized ?? stations
S2: troops AT churches troops ?? raided ?? churches
S2: ministers AT churches ministers ?? warning ?? troops ?? raided ?? churches
Table 1: Shortest Path representation of relations.
?station?.
In Figure 2 we show three more examples of the
LOCATED (AT) relationship as dependency paths
created from one or two predicate-argument struc-
tures. The second example is an interesting case,
as it illustrates how annotation decisions are accom-
modated in our approach. Using a reasoning similar
with that from the previous paragraph, it is reason-
able to infer that ?troops? are LOCATED in ?vans?,
and that ?vans? are LOCATED in ?city?. However,
because ?vans? is not an ACE markable, it cannot
participate in an annotated relationship. Therefore,
?troops? is annotated as being LOCATED in ?city?,
which makes sense due to the transitivity of the rela-
tion LOCATED. In our approach, this leads to short-
est paths that pass through two or more predicate-
argument structures.
The last relation example is a case where there ex-
ist multiple shortest paths in the dependency graph
between the same two entities ? there are actually
two different paths, with each path replicated into
three similar paths due to coordination. Our current
approach considers only one of the shortest paths,
nevertheless it seems reasonable to investigate using
all of them as multiple sources of evidence for rela-
tion extraction.
There may be cases where e1 and e2 belong
to predicate-argument structures that have no argu-
ment in common. However, because the depen-
dency graph is always connected, we are guaran-
teed to find a shortest path between the two enti-
ties. In general, we shall find a shortest sequence of
predicate-argument structures with target predicates
P1, P2, ..., Pn such that e1 is an argument of P1, e2 is
an argument of Pn, and any two consecutive predi-
cates Pi and Pi+1 share a common argument (where
by ?argument? we mean both arguments and com-
plements).
4 Learning with Dependency Paths
The shortest path between two entities in a depen-
dency graph offers a very condensed representation
of the information needed to assess their relation-
ship. A dependency path is represented as a se-
quence of words interspersed with arrows that in-
726
(1) He had no regrets for his actions in Brcko.
his? actions? in? Brcko
(2) U.S. troops today acted for the first time to capture an
alleged Bosnian war criminal, rushing from unmarked vans
parked in the northern Serb-dominated city of Bijeljina.
troops? rushing? from? vans? parked? in? city
(3) Jelisic created an atmosphere of terror at the camp by
killing, abusing and threatening the detainees.
detainees? killing? Jelisic? created? at? camp
detainees? abusing? Jelisic? created? at? camp
detainees? threatning? Jelisic? created? at? camp
detainees? killing? by? created? at? camp
detainees? abusing? by? created? at? camp
detainees? threatening? by? created? at? camp
Figure 2: Relation examples.
dicate the orientation of each dependency, as illus-
trated in Table 1. These paths however are com-
pletely lexicalized and consequently their perfor-
mance will be limited by data sparsity. We can al-
leviate this by categorizing words into classes with
varying degrees of generality, and then allowing
paths to use both words and their classes. Examples
of word classes are part-of-speech (POS) tags and
generalizations over POS tags such as Noun, Active
Verb or Passive Verb. The entity type is also used for
the two ends of the dependency path. Other poten-
tially useful classes might be created by associating
with each noun or verb a set of hypernyms corre-
sponding to their synsets in WordNet.
The set of features can then be defined as a
Cartesian product over these word classes, as illus-
trated in Figure 3 for the dependency path between
?protesters? and ?station? in sentence S1. In this rep-
resentation, sparse or contiguous subsequences of
nodes along the lexicalized dependency path (i.e.
path fragments) are included as features simply by
replacing the rest of the nodes with their correspond-
ing generalizations.
The total number of features generated by this de-
pendency path is 4?1?3?1?4, and some of them
are listed in Table 2.
?
?
?
protesters
NNS
Noun
PERSON
?
?
?
? [?]?
[
seized
VBD
Verb
]
? [?]?
?
?
?
stations
NNS
Noun
FACILITY
?
?
?
Figure 3: Feature generation from dependency path.
protesters ? seized ? stations
Noun ? Verb ? Noun
PERSON ? seized ? FACILITY
PERSON ? Verb ? FACILITY
... (48 features)
Table 2: Sample Features.
For verbs and nouns (and their respective word
classes) occurring along a dependency path we also
use an additional suffix ?(-)? to indicate a negative
polarity item. In the case of verbs, this suffix is used
when the verb (or an attached auxiliary) is modi-
fied by a negative polarity adverb such as ?not? or
?never?. Nouns get the negative suffix whenever
they are modified by negative determiners such as
?no?, ?neither? or ?nor?. For example, the phrase ?He
never went to Paris? is associated with the depen-
dency path ?He ? went(-) ? to ? Paris?.
Explicitly creating for each relation example a
vector with a position for each dependency path fea-
ture is infeasible, due to the high dimensionality of
the feature space. Here we can exploit dual learn-
ing algorithms that process examples only via com-
puting their dot-products, such as the Support Vec-
tor Machines (SVMs) (Vapnik, 1998; Cristianini
and Shawe-Taylor, 2000). These dot-products be-
tween feature vectors can be efficiently computed
through a kernel function, without iterating over all
the corresponding features. Given the kernel func-
tion, the SVM learner tries to find a hyperplane that
separates positive from negative examples and at the
same time maximizes the separation (margin) be-
tween them. This type of max-margin separator has
been shown both theoretically and empirically to re-
sist overfitting and to provide good generalization
performance on unseen examples.
Computing the dot-product (i.e. kernel) between
two relation examples amounts to calculating the
727
number of common features of the type illustrated
in Table 2. If x = x1x2...xm and y = y1y2...yn are
two relation examples, where xi denotes the set of
word classes corresponding to position i (as in Fig-
ure 3), then the number of common features between
x and y is computed as in Equation 1.
K(x, y) =
{
0, m 6= n
?n
i=1 c(xi, yi), m = n
(1)
where c(xi, yi) = |xi?yi| is the number of common
word classes between xi and yi.
This is a simple kernel, whose computation takes
O(n) time. If the two paths have different lengths,
they correspond to different ways of expressing a re-
lationship ? for instance, they may pass through a
different number of predicate argument structures.
Consequently, the kernel is defined to be 0 in this
case. Otherwise, it is the product of the number of
common word classes at each position in the two
paths. As an example, let us consider two instances
of the LOCATED relationship:
1. ?his actions in Brcko?, and
2. ?his arrival in Beijing?.
Their corresponding dependency paths are:
1. ?his ? actions ? in ? Brcko?, and
2. ?his ? arrival ? in ? Beijing?.
Their representation as a sequence of sets of word
classes is given by:
1. x = [x1 x2 x3 x4 x5 x6 x7], where x1 =
{his, PRP, PERSON}, x2 = {?}, x3 = {actions,
NNS, Noun}, x4 = {?}, x5 = {in, IN}, x6 =
{?}, x7 = {Brcko, NNP, Noun, LOCATION}
2. y = [y1 y2 y3 y4 y5 y6 y7], where y1 = {his,
PRP, PERSON}, y2 = {?}, y3 = {arrival, NN,
Noun}, y4 = {?}, y5 = {in, IN}, y6 = {?}, y7
= {Beijing, NNP, Noun, LOCATION}
Based on the formula from Equation 1, the kernel is
computed as K(x, y) = 3?1?1?1?2?1?3 = 18.
We use this relation kernel in conjunction with
SVMs in order to find decision hyperplanes that best
separate positive examples from negative examples.
We modified the LibSVM1 package for SVM learn-
ing by plugging in the kernel described above, and
used its default one-against-one implementation for
multiclass classification.
5 Experimental Evaluation
We applied the shortest path dependency kernel to
the problem of extracting top-level relations from
the ACE corpus (NIST, 2000), the version used
for the September 2002 evaluation. The training
part of this dataset consists of 422 documents, with
a separate set of 97 documents allocated for test-
ing. This version of the ACE corpus contains three
types of annotations: coreference, named entities
and relations. Entities can be of the type PERSON,
ORGANIZATION, FACILITY, LOCATION, and GEO-
POLITICAL ENTITY. There are 5 general, top-level
relations: ROLE, PART, LOCATED, NEAR, and SO-
CIAL. The ROLE relation links people to an organi-
zation to which they belong, own, founded, or pro-
vide some service. The PART relation indicates sub-
set relationships, such as a state to a nation, or a sub-
sidiary to its parent company. The AT relation indi-
cates the location of a person or organization at some
location. The NEAR relation indicates the proxim-
ity of one location to another. The SOCIAL rela-
tion links two people in personal, familial or profes-
sional relationships. Each top-level relation type is
further subdivided into more fine-grained subtypes,
resulting in a total of 24 relation types. For exam-
ple, the LOCATED relation includes subtypes such
as LOCATED-AT, BASED-IN, and RESIDENCE. In
total, there are 7,646 intra-sentential relations, of
which 6,156 are in the training data and 1,490 in the
test data.
We assume that the entities and their labels are
known. All preprocessing steps ? sentence segmen-
tation, tokenization, and POS tagging ? were per-
formed using the OpenNLP2 package.
5.1 Extracting dependencies using a CCG
parser
CCG (Steedman, 2000) is a type-driven theory of
grammar where most language-specific aspects of
the grammar are specified into lexicon. To each lex-
1URL:http://www.csie.ntu.edu.tw/?cjlin/libsvm/
2URL: http://opennlp.sourceforge.net
728
ical item corresponds a set of syntactic categories
specifying its valency and the directionality of its
arguments. For example, the words from the sen-
tence ?protesters seized several stations? are mapped
in the lexicon to the following categories:
protesters : NP
seized : (S\NP )/NP
several : NP/NP
stations : NP
The transitive verb ?seized? expects two arguments:
a noun phrase to the right (the object) and another
noun phrase to the left (the subject). Similarly, the
adjective ?several? expects a noun phrase to its right.
Depending on whether its valency is greater than
zero or not, a syntactic category is called a functor
or an argument. In the example above, ?seized? and
?several? are functors, while ?protesters? and ?sta-
tions? are arguments.
Syntactic categories are combined using a small
set of typed combinatory rules such as functional ap-
plication, composition and type raising. In Table 3
we show a sample derivation based on three func-
tional applications.
protesters seized several stations
NP (S\NP )/NP NP/NP NP
NP (S\NP )/NP NP
NP S\NP
S
Table 3: Sample derivation.
In order to obtain CCG derivations for all sen-
tences in the ACE corpus, we used the CCG
parser introduced in (Hockenmaier and Steedman,
2002)3. This parser also outputs a list of dependen-
cies, with each dependency represented as a 4-tuple
?f, a, wf , wa?, where f is the syntactic category of
the functor, a is the argument number, wf is the head
word of the functor, and wa is the head word of the
argument. For example, the three functional appli-
cations from Table 3 result in the functor-argument
dependencies enumerated below in Table 4.
3URL:http://www.ircs.upenn.edu/?juliahr/Parser/
f a wf wa
NP/NP 1 ?several? ?stations?
(S\NP )/NP 2 ?seized? ?stations?
(S\NP )/NP 1 ?seized? ?protesters?
Table 4: Sample dependencies.
Because predicates (e.g. ?seized?) and adjuncts
(e.g. ?several?) are always represented as func-
tors, while complements (e.g. ?protesters? and ?sta-
tions?) are always represented as arguments, it is
straightforward to transform a functor-argument de-
pendency into a head-modifier dependency. The
head-modifier dependencies corresponding to the
three functor-argument dependencies in Table 4 are:
?protesters ? seized?, ?stations ? seized?, and ?sev-
eral ? stations?.
Special syntactic categories are assigned in CCG
to lexical items that project unbounded dependen-
cies, such as the relative pronouns ?who?, ?which?
and ?that?. Coupled with a head-passing mechanism,
these categories allow the extraction of long-range
dependencies. Together with the local word-word
dependencies, they create a directed acyclic depen-
dency graph for each parsed sentence, as shown in
Figure 1.
5.2 Extracting dependencies using a CFG
parser
Local dependencies can be extracted from a CFG
parse tree using simple heuristic rules for finding
the head child for each type of constituent. Alter-
natively, head-modifier dependencies can be directly
output by a parser whose model is based on lexical
dependencies. In our experiments, we used the full
parse output from Collins? parser (Collins, 1997), in
which every non-terminal node is already annotated
with head information. Because local dependencies
assemble into a tree for each sentence, there is only
one (shortest) path between any two entities in a de-
pendency tree.
5.3 Experimental Results
A recent approach to extracting relations is de-
scribed in (Culotta and Sorensen, 2004). The au-
thors use a generalized version of the tree kernel
from (Zelenko et al, 2003) to compute a kernel over
729
relation examples, where a relation example consists
of the smallest dependency tree containing the two
entities of the relation. Precision and recall values
are reported for the task of extracting the 5 top-level
relations in the ACE corpus under two different sce-
narios:
? [S1] This is the classic setting: one multi-class
SVM is learned to discriminate among the 5 top-
level classes, plus one more class for the no-relation
cases.
? [S2] Because of the highly skewed data distribu-
tion, the recall of the SVM approach in the first sce-
nario is very low. In (Culotta and Sorensen, 2004)
the authors propose doing relation extraction in two
steps: first, one binary SVM is trained for rela-
tion detection, which means that all positive rela-
tion instances are combined into one class. Then the
thresholded output of this binary classifier is used as
training data for a second multi-class SVM, which is
trained for relation classification. The same kernel
is used in both stages.
We present in Table 5 the performance of our
shortest path (SP) dependency kernel on the task of
relation extraction from ACE, where the dependen-
cies are extracted using either a CCG parser (SP-
CCG), or a CFG parser (SP-CFG). We also show
the results presented in (Culotta and Sorensen, 2004)
for their best performing kernel K4 (a sum between
a bag-of-words kernel and their dependency kernel)
under both scenarios.
Method Precision Recall F-measure
(S1) SP-CCG 67.5 37.2 48.0
(S1) SP-CFG 71.1 39.2 50.5
(S1) K4 70.3 26.3 38.0
(S2) SP-CCG 63.7 41.4 50.2
(S2) SP-CFG 65.5 43.8 52.5
(S2) K4 67.1 35.0 45.8
Table 5: Extraction Performance on ACE.
The shortest-path dependency kernels outperform
the dependency kernel from (Culotta and Sorensen,
2004) in both scenarios, with a more significant dif-
ference for SP-CFG. An error analysis revealed that
Collins? parser was better at capturing local depen-
dencies, hence the increased accuracy of SP-CFG.
Another advantage of our shortest-path dependency
kernels is that their training and testing are very fast
? this is due to representing the sentence as a chain
of dependencies on which a fast kernel can be com-
puted. All the four SP kernels from Table 5 take
between 2 and 3 hours to train and test on a 2.6GHz
Pentium IV machine.
To avoid numerical problems, we constrained the
dependency paths to pass through at most 10 words
(as observed in the training data) by setting the ker-
nel to 0 for longer paths. We also tried the alterna-
tive solution of normalizing the kernel, however this
led to a slight decrease in accuracy. Having longer
paths give larger kernel scores in the unnormalized
version does not pose a problem because, by defi-
nition, paths of different lengths correspond to dis-
joint sets of features. Consequently, the SVM algo-
rithm will induce lower weights for features occur-
ring in longer paths, resulting in a linear separator
that works irrespective of the size of the dependency
paths.
6 Related Work
In (Zelenko et al, 2003), the authors do relation
extraction using a tree kernel defined over shallow
parse tree representations of sentences. The same
tree kernel is slightly generalized in (Culotta and
Sorensen, 2004) and used in conjunction with de-
pendency trees. In both approaches, a relation in-
stance is defined to be the smallest subtree in the
parse or dependency tree that includes both entities.
In this paper we argued that the information relevant
to relation extraction is almost entirely concentrated
in the shortest path in the dependency tree, leading to
an even smaller representation. Another difference
between the tree kernels above and our new kernel
is that the tree kernels used for relation extraction
are opaque i.e. the semantics of the dimensions in
the corresponding Hilbert space is not obvious. For
the shortest-path kernels, the semantics is known by
definition: each path feature corresponds to a dimen-
sion in the Hilbert space. This transparency allows
us to easily restrict the types of patterns counted by
the kernel to types that we deem relevant for relation
extraction. The tree kernels are also more time con-
suming, especially in the sparse setting, where they
count sparse subsequences of children common to
nodes in the two trees. In (Zelenko et al, 2003), the
730
tree kernel is computed in O(mn) time, where m
and n are the number of nodes in the two trees. This
changes to O(mn3) in the sparse setting.
Our shortest-path intuition bears some similar-
ity with the underlying assumption of the relational
pathfinding algorithm from (Richards and Mooney,
1992) : ?in most relational domains, important con-
cepts will be represented by a small number of fixed
paths among the constants defining a positive in-
stance ? for example, the grandparent relation is de-
fined by a single fixed path consisting of two parent
relations.? We can see this happening also in the task
of relation extraction from ACE, where ?important
concepts? are the 5 types of relations, and the ?con-
stants? defining a positive instance are the 5 types of
entities.
7 Future Work
Local and non-local (deep) dependencies are equally
important for finding relations. In this paper we tried
extracting both types of dependencies using a CCG
parser, however another approach is to recover deep
dependencies from syntactic parses, as in (Camp-
bell, 2004; Levy and Manning, 2004). This may
have the advantage of preserving the quality of lo-
cal dependencies while completing the representa-
tion with non-local dependencies.
Currently, the method assumes that the named en-
tities are known. A natural extension is to automati-
cally extract both the entities and their relationships.
Recent research (Roth and Yih, 2004) indicates that
integrating entity recognition with relation extrac-
tion in a global model that captures the mutual influ-
ences between the two tasks can lead to significant
improvements in accuracy.
8 Conclusion
We have presented a new kernel for relation extrac-
tion based on the shortest-path between the two rela-
tion entities in the dependency graph. Comparative
experiments on extracting top-level relations from
the ACE corpus show significant improvements over
a recent dependency tree kernel.
9 Acknowledgements
This work was supported by grants IIS-0117308 and
IIS-0325116 from the NSF.
References
Richard Campbell. 2004. Using linguistic principles to recover
empty categories. In Proceedings of the 42nd Annual Meet-
ing of the Association for Computational Linguistics (ACL-
04), pages 645?652, Barcelona, Spain, July.
Michael J. Collins. 1997. Three generative, lexicalised mod-
els for statistical parsing. In Proceedings of the 35th An-
nual Meeting of the Association for Computational Linguis-
tics (ACL-97), pages 16?23.
Nello Cristianini and John Shawe-Taylor. 2000. An Introduc-
tion to Support Vector Machines and Other Kernel-based
Learning Methods. Cambridge University Press.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency tree
kernels for relation extraction. In Proceedings of the 42nd
Annual Meeting of the Association for Computational Lin-
guistics (ACL-04), Barcelona, Spain, July.
Ralph Grishman. 1995. Message Understanding Conference 6.
http://cs.nyu.edu/cs/faculty/grishman/muc6.html.
Julia Hockenmaier and Mark Steedman. 2002. Generative
models for statistical parsing with combinatory categorial
grammar. In Proceedings of the 40th Annual Meeting of
the Association for Computational Linguistics (ACL-2002),
pages 335?342, Philadelphia, PA.
Roger Levy and Christopher Manning. 2004. Deep dependen-
cies from context-free statistical parsers: Correcting the sur-
face dependency approximation. In Proceedings of the 42nd
Annual Meeting of the Association for Computational Lin-
guistics (ACL-04), pages 327?334, Barcelona, Spain, July.
NIST. 2000. ACE ? Automatic Content Extraction.
http://www.nist.gov/speech/tests/ace.
Soumya Ray and Mark Craven. 2001. Representing sentence
structure in hidden Markov models for information extrac-
tion. In Proceedings of the Seventeenth International Joint
Conference on Artificial Intelligence (IJCAI-2001), pages
1273?1279, Seattle, WA.
Bradley L. Richards and Raymond J. Mooney. 1992. Learning
relations by pathfinding. In Proceedings of the Tenth Na-
tional Conference on Artificial Intelligence (AAAI-92), pages
50?55, San Jose, CA, July.
D. Roth and W. Yih. 2004. A linear programming formula-
tion for global inference in natural language tasks. In Pro-
ceedings of the Annual Conference on Computational Natu-
ral Language Learning (CoNLL), pages 1?8, Boston, MA.
Mark Steedman. 2000. The Syntactic Process. The MIT Press,
Cambridge, MA.
Vladimir N. Vapnik. 1998. Statistical Learning Theory. John
Wiley & Sons.
D. Zelenko, C. Aone, and A. Richardella. 2003. Kernel meth-
ods for relation extraction. Journal of Machine Learning
Research, 3:1083?1106.
731
 	
ffProceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 576?583,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Learning to Extract Relations from the Web
using Minimal Supervision
Razvan C. Bunescu
Department of Computer Sciences
University of Texas at Austin
1 University Station C0500
Austin, TX 78712
razvan@cs.utexas.edu
Raymond J. Mooney
Department of Computer Sciences
University of Texas at Austin
1 University Station C0500
Austin, TX 78712
mooney@cs.utexas.edu
Abstract
We present a new approach to relation ex-
traction that requires only a handful of train-
ing examples. Given a few pairs of named
entities known to exhibit or not exhibit a
particular relation, bags of sentences con-
taining the pairs are extracted from the web.
We extend an existing relation extraction
method to handle this weaker form of su-
pervision, and present experimental results
demonstrating that our approach can reliably
extract relations from web documents.
1 Introduction
A growing body of recent work in information
extraction has addressed the problem of relation
extraction (RE), identifying relationships between
entities stated in text, such as LivesIn(Person,
Location) or EmployedBy(Person, Company).
Supervised learning has been shown to be effective
for RE (Zelenko et al, 2003; Culotta and Sorensen,
2004; Bunescu and Mooney, 2006); however, anno-
tating large corpora with examples of the relations
to be extracted is expensive and tedious.
In this paper, we introduce a supervised learning
approach to RE that requires only a handful of
training examples and uses the web as a corpus.
Given a few pairs of well-known entities that
clearly exhibit or do not exhibit a particular re-
lation, such as CorpAcquired(Google, YouTube)
and not(CorpAcquired(Yahoo, Microsoft)), a
search engine is used to find sentences on the web
that mention both of the entities in each of the pairs.
Although not all of the sentences for positive pairs
will state the desired relationship, many of them
will. Presumably, none of the sentences for negative
pairs state the targeted relation. Multiple instance
learning (MIL) is a machine learning framework
that exploits this sort of weak supervision, in
which a positive bag is a set of instances which is
guaranteed to contain at least one positive example,
and a negative bag is a set of instances all of which
are negative. MIL was originally introduced to
solve a problem in biochemistry (Dietterich et
al., 1997); however, it has since been applied to
problems in other areas such as classifying image
regions in computer vision (Zhang et al, 2002), and
text categorization (Andrews et al, 2003; Ray and
Craven, 2005).
We have extended an existing approach to rela-
tion extraction using support vector machines and
string kernels (Bunescu and Mooney, 2006) to han-
dle this weaker form of MIL supervision. This ap-
proach can sometimes be misled by textual features
correlated with the specific entities in the few train-
ing pairs provided. Therefore, we also describe a
method for weighting features in order to focus on
those correlated with the target relation rather than
with the individual entities. We present experimen-
tal results demonstrating that our approach is able to
accurately extract relations from the web by learning
from such weak supervision.
2 Problem Description
We address the task of learning a relation extrac-
tion system targeted to a fixed binary relationship
R. The only supervision given to the learning algo-
576
rithm is a small set of pairs of named entities that are
known to belong (positive) or not belong (negative)
to the given relationship. Table 1 shows four posi-
tive and two negative example pairs for the corpo-
rate acquisition relationship. For each pair, a bag of
sentences containing the two arguments can be ex-
tracted from a corpus of text documents. The corpus
is assumed to be sufficiently large and diverse such
that, if the pair is positive, it is highly likely that the
corresponding bag contains at least one sentence that
explicitly asserts the relationship R between the two
arguments. In Section 6 we describe a method for
extracting bags of relevant sentences from the web.
+/? Arg a1 Arg a2
+ Google YouTube
+ Adobe Systems Macromedia
+ Viacom DreamWorks
+ Novartis Eon Labs
? Yahoo Microsoft
? Pfizer Teva
Table 1: Corporate Acquisition Pairs.
Using a limited set of entity pairs (e.g. those in
Table 1) and their associated bags as training data,
the aim is to induce a relation extraction system that
can reliably decide whether two entities mentioned
in the same sentence exhibit the target relationship
or not. In particular, when tested on the example
sentences from Figure 1, the system should classify
S1, S3,and S4 as positive, and S2 and S5 as negative.
+/S1: Search engine giant Google has bought video-
sharing website YouTube in a controversial $1.6 billion
deal.
?/S2: The companies will merge Google?s search ex-
pertise with YouTube?s video expertise, pushing what
executives believe is a hot emerging market of video
offered over the Internet.
+/S3: Google has acquired social media company,
YouTube for $1.65 billion in a stock-for-stock transaction
as announced by Google Inc. on October 9, 2006.
+/S4: Drug giant Pfizer Inc. has reached an agreement
to buy the private biotechnology firm Rinat Neuroscience
Corp., the companies announced Thursday.
?/S5: He has also received consulting fees from Al-
pharma, Eli Lilly and Company, Pfizer, Wyeth Pharmaceu-
ticals, Rinat Neuroscience, Elan Pharmaceuticals, and For-
est Laboratories.
Figure 1: Sentence examples.
As formulated above, the learning task can be
seen as an instance of multiple instance learning.
However, there are important properties that set it
apart from problems previously considered in MIL.
The most distinguishing characteristic is that the
number of bags is very small, while the average size
of the bags is very large.
3 Multiple Instance Learning
Since its introduction by Dietterich (1997), an ex-
tensive and quite diverse set of methods have been
proposed for solving the MIL problem. For the task
of relation extraction, we consider only MIL meth-
ods where the decision function can be expressed in
terms of kernels computed between bag instances.
This choice was motivated by the comparatively
high accuracy obtained by kernel-based SVMs when
applied to various natural language tasks, and in par-
ticular to relation extraction. Through the use of ker-
nels, SVMs (Vapnik, 1998; Scho?lkopf and Smola,
2002) can work efficiently with instances that im-
plicitly belong to a high dimensional feature space.
When used for classification, the decision function
computed by the learning algorithm is equivalent to
a hyperplane in this feature space. Overfitting is
avoided in the SVM formulation by requiring that
positive and negative training instances be maxi-
mally separated by the decision hyperplane.
Gartner et al (2002) adapted SVMs to the MIL
setting using various multi-instance kernels. Two
of these ? the normalized set kernel, and the statis-
tic kernel ? have been experimentally compared to
other methods by Ray and Craven (2005), with com-
petitive results. Alternatively, a simple approach to
MIL is to transform it into a standard supervised
learning problem by labeling all instances from pos-
itive bags as positive. An interesting outcome of the
study conducted by Ray and Craven (2005) was that,
despite the class noise in the resulting positive ex-
amples, such a simple approach often obtains com-
petitive results when compared against other more
sophisticated MIL methods.
We believe that an MIL method based on multi-
instance kernels is not appropriate for training
datasets that contain just a few, very large bags. In
a multi-instance kernel approach, only bags (and
not instances) are considered as training examples,
577
which means that the number of support vectors is
going to be upper bounded by the number of train-
ing bags. Taking the bags from Table 1 as a sam-
ple training set, the decision function is going to be
specified by at most seven parameters: the coeffi-
cients for at most six support vectors, plus an op-
tional bias parameter. A hypothesis space character-
ized by such a small number of parameters is likely
to have insufficient capacity.
Based on these observations, we decided to trans-
form the MIL problem into a standard supervised
problem as described above. The use of this ap-
proach is further motivated by its simplicity and its
observed competitive performance on very diverse
datasets (Ray and Craven, 2005). Let X be the set
of bags used for training, Xp ? X the set of posi-
tive bags, and Xn ? X the set of negative bags. For
any instance x ? X from a bag X ? X , let ?(x)
be the (implicit) feature vector representation of x.
Then the corresponding SVM optimization problem
can be formulated as in Figure 2:
minimize:
J(w, b, ?) = 12?w?2 + CL
(
cp LnL ?p + cn
Lp
L ?n
)
?p =
?
X?Xp
?
x?X
?x
?n =
?
X?Xn
?
x?X
?x
subject to:
w?(x) + b ? +1? ?x, ?x ? X ? Xp
w?(x) + b ? ?1 + ?x, ?x ? X ? Xn
?x ? 0
Figure 2: SVM Optimization Problem.
The capacity control parameter C is normalized
by the total number of instances L = Lp + Ln =
?
X?Xp |X| +
?
X?Xn |X|, so that it remains in-
dependent of the size of the dataset. The additional
non-negative parameter cp (cn = 1?cp) controls the
relative influence that false negative vs. false posi-
tive errors have on the value of the objective func-
tion. Because not all instances from positive bags
are real positive instances, it makes sense to have
false negative errors be penalized less than false pos-
itive errors (i.e. cp < 0.5).
In the dual formulation of the optimization prob-
lem from Figure 2, bag instances appear only inside
dot products of the form K(x1, x2) = ?(x1)?(x2).
The kernel K is instantiated to a subsequence ker-
nel, as described in the next section.
4 Relation Extraction Kernel
The training bags consist of sentences extracted
from online documents, using the methodology de-
scribed in Section 6. Parsing web documents in
order to obtain a syntactic analysis often gives un-
reliable results ? the type of narrative can vary
greatly from one web document to another, and sen-
tences with grammatical errors are frequent. There-
fore, for the initial experiments, we used a modi-
fied version of the subsequence kernel of Bunescu
and Mooney (2006), which does not require syn-
tactic information. This kernel computes the num-
ber of common subsequences of tokens between two
sentences. The subsequences are constrained to be
?anchored? at the two entity names, and there is
a maximum number of tokens that can appear in
a sequence. For example, a subsequence feature
for the sentence S1 in Figure 1 is s? = ??e1? . . .
bought . . . ?e2? . . . in . . . billion . . . deal?, where
?e1? and ?e2? are generic placeholders for the two
entity names. The subsequence kernel induces a
feature space where each dimension corresponds
to a sequence of words. Any such sequence that
matches a subsequence of words in a sentence exam-
ple is down-weighted as a function of the total length
of the gaps between every two consecutive words.
More exactly, let s = w1w2 . . . wk be a sequence of
k words, and s? = w1 g1 w2 g2 . . . wk?1 gk?1 wk a
matching subsequence in a relation example, where
gi stands for any sequence of words between wi and
wi+1. Then the sequence s will be represented in the
relation example as a feature with weight computed
as ?(s) = ?g(s?). The parameter ? controls the mag-
nitude of the gap penalty, where g(s?) = ?i |gi| is
the total gap.
Many relations, like the ones that we explore in
the experimental evaluation, cannot be expressed
without using at least one content word. We there-
fore modified the kernel computation to optionally
ignore subsequence patterns formed exclusively of
578
stop words and punctuation signs. In Section 5.1,
we introduce a new weighting scheme, wherein a
weight is assigned to every token. Correspondingly,
every sequence feature will have an additional mul-
tiplicative weight, computed as the product of the
weights of all the tokens in the sequence. The aim
of this new weighting scheme, as detailed in the next
section, is to eliminate the bias caused by the special
structure of the relation extraction MIL problem.
5 Two Types of Bias
As already hinted at the end of Section 2, there is
one important property that distinguishes the cur-
rent MIL setting for relation extraction from other
MIL problems: the training dataset contains very
few bags, and each bag can be very large. Con-
sequently, an application of the learning model de-
scribed in Sections 3 & 4 is bound to be affected by
the following two types of bias:
 [Type I Bias] By definition, all sentences inside
a bag are constrained to contain the same two ar-
guments. Words that are semantically correlated
with either of the two arguments are likely to oc-
cur in many sentences. For example, consider the
sentences S1 and S2 from the bag associated with
?Google? and ?YouTube? (as shown in Figure 1).
They both contain the words ?search? ? highly cor-
related with ?Google?, and ?video? ? highly corre-
lated with ?YouTube?, and it is likely that a signifi-
cant percentage of sentences in this bag contain one
of the two words (or both). The two entities can be
mentioned in the same sentence for reasons other
than the target relation R, and these noisy training
sentences are likely to contain words that are corre-
lated with the two entities, without any relationship
to R. A learning model where the features are based
on words, or word sequences, is going to give too
much weight to words or combinations of words that
are correlated with either of individual arguments.
This overweighting will adversely affect extraction
performance through an increased number of errors.
A method for eliminating this type of bias is intro-
duced in Section 5.1.
 [Type II Bias] While Type I bias is due to words
that are correlated with the arguments of a relation
instance, the Type II bias is caused by words that
are specific to the relation instance itself. Using
FrameNet terminology (Baker et al, 1998), these
correspond to instantiated frame elements. For ex-
ample, the corporate acquisition frame can be seen
as a subtype of the ?Getting? frame in FrameNet.
The core elements in this frame are the Recipi-
ent (e.g. Google) and the Theme (e.g. YouTube),
which for the acquisition relationship coincide with
the two arguments. They do not contribute any
bias, since they are replaced with the generic tags
?e1? and ?e2? in all sentences from the bag. There
are however other frame elements ? peripheral, or
extra-thematic ? that can be instantiated with the
same value in many sentences. In Figure 1, for in-
stance, sentence S3 contains two non-core frame ele-
ments: the Means element (e.g ?in a stock-for-stock
transaction?) and the Time element (e.g. ?on Oc-
tober 9, 2006?). Words from these elements, like
?stock?, or ?October?, are likely to occur very often
in the Google-YouTube bag, and because the train-
ing dataset contains only a few other bags, subse-
quence patterns containing these words will be given
too much weight in the learned model. This is prob-
lematic, since these words can appear in many other
frames, and thus the learned model is likely to make
errors. Instead, we would like the model to fo-
cus on words that trigger the target relationship (in
FrameNet, these are the lexical units associated with
the target frame).
5.1 A Solution for Type I Bias
In order to account for how strongly the words in a
sequence are correlated with either of the individual
arguments of the relation, we modify the formula for
the sequence weight ?(s) by factoring in a weight
?(w) for each word in the sequence, as illustrated in
Equation 1.
?(s) = ?g(s?) ?
?
w?s
?(w) (1)
Given a predefined set of weights ?(w), it is straight-
forward to update the recursive computation of
the subsequence kernel so that it reflects the new
weighting scheme.
If all the word weights are set to 1, then the new
kernel is equivalent to the old one. What we want,
however, is a set of weights where words that are
correlated with either of the two arguments are given
lower weights. For any word, the decrease in weight
579
should reflect the degree of correlation between that
word and the two arguments. Before showing the
formula used for computing the word weights, we
first introduce some notation:
? Let X ? X be an arbitrary bag, and let X.a1
and X.a2 be the two arguments associated with
the bag.
? Let C(X) be the size of the bag (i.e. the num-
ber of sentences in the bag), and C(X,w) the
number of sentences in the bag X that contain
the word w. Let P (w|X) = C(X,w)/C(X).
? Let P (w|X.a1 ? X.a2) be the probability that
the word w appears in a sentence due only to
the presence of X.a1 or X.a2, assuming X.a1
and X.a2 are independent causes for w.
The word weights are computed as follows:
?(w) = C(X,w)? P (w|X.a1 ?X.a2) ? C(X)C(X,w)
= 1? P (w|X.a1 ?X.a2)P (w|X) (2)
The quantity P (w|X.a1 ? X.a2) ? C(X) represents
the expected number of sentences in which w would
occur, if the only causes were X.a1 or X.a2, inde-
pendent of each other. We want to discard this quan-
tity from the total number of occurrences C(X,w),
so that the effect of correlations with X.a1 or X.a2
is eliminated.
We still need to compute P (w|X.a1 ?X.a2). Be-
cause in the definition of P (w|X.a1 ?X.a2), the ar-
guments X.a1 and X.a2 were considered indepen-
dent causes, P (w|X.a1 ? X.a2) can be computed
with the noisy-or operator (Pearl, 1986):
P (?) = 1?(1?P (w|a1)) ? (1?P (w|a2)) (3)
= P (w|a1)+P (w|a2)?P (w|a1) ? P (w|a2)
The quantity P (w|a) represents the probability that
the word w appears in a sentence due only to the
presence of a, and it could be estimated using counts
on a sufficiently large corpus. For our experimen-
tal evaluation, we used the following approxima-
tion: given an argument a, a set of sentences con-
taining a are extracted from web documents (de-
tails in Section 6). Then P (w|a) is simply approxi-
mated with the ratio of the number of sentences con-
taining w over the total number of sentences, i.e.
P (w|a) = C(w, a)/C(a). Because this may be an
overestimate (w may appear in a sentence contain-
ing a due to causes other than a), and also because
of data sparsity, the quantity ?(w) may sometimes
result in a negative value ? in these cases it is set to
0, which is equivalent to ignoring the word w in all
subsequence patterns.
6 MIL Relation Extraction Datasets
For the purpose of evaluation, we created two
datasets: one for corporate acquisitions, as shown
in Table 2, and one for the person-birthplace rela-
tion, with the example pairs from Table 3. In both
tables, the top part shows the training pairs, while
the bottom part shows the test pairs.
+/? Arg a1 Arg a2 Size
+ Google YouTube 1375
+ Adobe Systems Macromedia 622
+ Viacom DreamWorks 323
+ Novartis Eon Labs 311
? Yahoo Microsoft 163
? Pfizer Teva 247
+ Pfizer Rinat Neuroscience 50 (41)
+ Yahoo Inktomi 433 (115)
? Google Apple 281
? Viacom NBC 231
Table 2: Corporate Acquisition Pairs.
+/? Arg a1 Arg a2 Size
+ Franz Kafka Prague 552
+ Andre Agassi Las Vegas 386
+ Charlie Chaplin London 292
+ George Gershwin New York 260
? Luc Besson New York 74
? Wolfgang A. Mozart Vienna 288
+ Luc Besson Paris 126 (6)
+ Marie Antoinette Vienna 105 (39)
? Charlie Chaplin Hollywood 266
? George Gershwin London 104
Table 3: Person-Birthplace Pairs.
Given a pair of arguments (a1, a2), the corre-
sponding bag of sentences is created as follows:
 A query string ?a1 ? ? ? ? ? ? ? a2? containing
seven wildcard symbols between the two arguments
is submitted to Google. The preferences are set to
search only for pages written in English, with Safe-
search turned on. This type of query will match doc-
uments where an occurrence of a1 is separated from
an occurrence of a2 by at most seven content words.
This is an approximation of our actual information
580
need: ?return all documents containing a1 and a2 in
the same sentence?.
 The returned documents (limited by Google to
the first 1000) are downloaded, and then the text
is extracted using the HTML parser from the Java
Swing package. Whenever possible, the appropriate
HTML tags (e.g. BR, DD, P, etc.) are used as hard
end-of-sentence indicators. The text is further seg-
mented into sentences with the OpenNLP1 package.
 Sentences that do not contain both arguments a1
and a2 are discarded. For every remaining sentence,
we find the occurrences of a1 and a2 that are clos-
est to each other, and create a relation example by
replacing a1 with ?e1? and a2 with ?e2?. All other
occurrences of a1 and a2 are replaced with a null
token ignored by the subsequence kernel.
The number of sentences in every bag is shown in
the last column of Tables 2 & 3. Because Google
also counts pages that are deemed too similar in the
first 1000, some of the bags can be relatively small.
As described in Section 5.1, the word-argument
correlations are modeled through the quantity
P (w|a) = C(w, a)/C(a), estimated as the ratio be-
tween the number of sentences containing w and a,
and the number of sentences containing a. These
counts are computed over a bag of sentences con-
taining a, which is created by querying Google for
the argument a, and then by processing the results
as described above.
7 Experimental Evaluation
Each dataset is split into two sets of bags: one
for training and one for testing. The test dataset
was purposefully made difficult by including neg-
ative bags with arguments that during training were
used in positive bags, and vice-versa. In order to
evaluate the relation extraction performance at the
sentence level, we manually annotated all instances
from the positive test bags. The last column in Ta-
bles 2 & 3 shows, between parentheses, how many
instances from the positive test bags are real pos-
itive instances. The corporate acquisition test set
has a total of 995 instances, out of which 156 are
positive. The person-birthplace test set has a total
of 601 instances, and only 45 of them are positive.
Extrapolating from the test set distribution, the pos-
1http://opennlp.sourceforge.net
itive bags in the person-birthplace dataset are sig-
nificantly sparser in real positive instances than the
positive bags in the corporate acquisition dataset.
The subsequence kernel described in Section 4
was used as a custom kernel for the LibSVM2 Java
package. When run with the default parameters,
the results were extremely poor ? too much weight
was given to the slack term in the objective func-
tion. Minimizing the regularization term is essen-
tial in order to capture subsequence patterns shared
among positive bags. Therefore LibSVM was mod-
ified to solve the optimization problem from Fig-
ure 2, where the capacity parameter C is normal-
ized by the size of the transformed dataset. In this
new formulation, C is set to its default value of 1.0
? changing it to other values did not result in signifi-
cant improvement. The trade-off between false pos-
itive and false negative errors is controlled by the
parameter cp. When set to its default value of 0.5,
false-negative errors and false positive errors have
the same impact on the objective function. As ex-
pected, setting cp to a smaller value (0.1) resulted
in better performance. Tests with even lower values
did not improve the results.
We compare the following four systems:
 SSK?MIL: This corresponds to the MIL formu-
lation from Section 3, with the original subsequence
kernel described in Section 4.
 SSK?T1: This is the SSK?MIL system aug-
mented with word weights, so that the Type I bias
is reduced, as described in Section 5.1.
 BW-MIL: This is a bag-of-words kernel, in
which the relation examples are classified based on
the unordered words contained in the sentence. This
baseline shows the performance of a standard text-
classification approach to the problem using a state-
of-the art algorithm (SVM).
 SSK?SIL: This corresponds to the original sub-
sequence kernel trained with traditional, single in-
stance learning (SIL) supervision. For evaluation,
we train on the manually labeled instances from the
test bags. We use a combination of one positive bag
and one negative bag for training, while the other
two bags are used for testing. The results are aver-
aged over all four possible combinations. Note that
the supervision provided to SSK?SIL requires sig-
2http://www.csie.ntu.edu.tw/?cjlin/libsvm
581
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 100
 0  10  20  30  40  50  60  70  80  90  100
Pr
ec
isi
on
 (%
)
Recall (%)
SSK-T1
SSK-MIL
BW-MIL
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 100
 0  10  20  30  40  50  60  70  80  90  100
Pr
ec
isi
on
 (%
)
Recall (%)
SSK-T1
SSK-MIL
BW-MIL
(a) Corporate Acquisitions (b) Person-Birthplace
Figure 3: Precision-Recall graphs on the two datasets.
nificantly more annotation effort, therefore, given a
sufficient amount of training examples, we expect
this system to perform at least as well as its MIL
counterpart.
In Figure 3, precision is plotted against recall by
varying a threshold on the value of the SVM deci-
sion function. To avoid clutter, we show only the
graphs for the first three systems. In Table 4 we
show the area under the precision recall curves of
all four systems. Overall, the learned relation extrac-
tors are able to identify the relationship in novel sen-
tences quite accurately and significantly out-perform
a bag-of-words baseline. The new version of the
subsequence kernel SSK?T1 is significantly more
accurate in the MIL setting than the original sub-
sequence kernel SSK?MIL, and is also competitive
with SSK?SIL, which was trained using a reason-
able amount of manually labeled sentence examples.
Dataset SSK?MIL SSK?T1 BW?MIL SSK?SIL
(a) CA 76.9% 81.1% 45.9% 80.4%
(b) PB 72.5% 78.2% 69.2% 73.4%
Table 4: Area Under Precision-Recall Curve.
8 Future Work
An interesting potential application of our approach
is a web relation-extraction system similar to Google
Sets, in which the user provides only a handful of
pairs of entities known to exhibit or not to exhibit
a particular relation, and the system is used to find
other pairs of entities exhibiting the same relation.
Ideally, the user would only need to provide pos-
itive pairs. Sentences containing one of the rela-
tion arguments could be extracted from the web, and
likely negative sentence examples automatically cre-
ated by pairing this entity with other named enti-
ties mentioned in the sentence. In this scenario, the
training set can contain both false positive and false
negative noise. One useful side effect is that Type
I bias is partially removed ? some bias still remains
due to combinations of at least two words, each cor-
related with a different argument of the relation.
We are also investigating methods for reducing Type
II bias, either by modifying the word weights, or by
integrating an appropriate measure of word distri-
bution across positive bags directly in the objective
function for the MIL problem. Alternatively, im-
plicit negative evidence can be extracted from sen-
tences in positive bags by exploiting the fact that, be-
sides the two relation arguments, a sentence from a
positive bag may contain other entity mentions. Any
pair of entities different from the relation pair is very
likely to be a negative example for that relation. This
is similar to the concept of negative neighborhoods
introduced by Smith and Eisner (2005), and has the
potential of eliminating both Type I and Type II bias.
9 Related Work
One of the earliest IE methods designed to work
with a reduced amount of supervision is that of
Hearst (1992), where a small set of seed patterns
is used in a bootstrapping fashion to mine pairs of
582
hypernym-hyponym nouns. Bootstrapping is actu-
ally orthogonal to our method, which could be used
as the pattern learner in every bootstrapping itera-
tion. A more recent IE system that works by boot-
strapping relation extraction patterns from the web is
KNOWITALL (Etzioni et al, 2005). For a given tar-
get relation, supervision in KNOWITALL is provided
as a rule template containing words that describe the
class of the arguments (e.g. ?company?), and a small
set of seed extraction patterns (e.g. ?has acquired?).
In our approach, the type of supervision is different ?
we ask only for pairs of entities known to exhibit the
target relation or not. Also, KNOWITALL requires
large numbers of search engine queries in order to
collect and validate extraction patterns, therefore ex-
periments can take weeks to complete. Compara-
tively, the approach presented in this paper requires
only a small number of queries: one query per rela-
tion pair, and one query for each relation argument.
Craven and Kumlien (1999) create a noisy train-
ing set for the subcellular-localization relation by
mining Medline for sentences that contain tuples
extracted from relevant medical databases. To our
knowledge, this is the first approach that is using a
?weakly? labeled dataset for relation extraction. The
resulting bags however are very dense in positive ex-
amples, and they are also many and small ? conse-
quently, the two types of bias are not likely to have
significant impact on their system?s performance.
10 Conclusion
We have presented a new approach to relation ex-
traction that leverages the vast amount of informa-
tion available on the web. The new RE system is
trained using only a handful of entity pairs known to
exhibit and not exhibit the target relationship. We
have extended an existing relation extraction ker-
nel to learn in this setting and to resolve problems
caused by the minimal supervision provided. Exper-
imental results demonstrate that the new approach
can reliably extract relations from web documents.
Acknowledgments
We would like to thank the anonymous reviewers for
their helpful suggestions. This work was supported
by grant IIS-0325116 from the NSF, and a gift from
Google Inc.
References
Stuart Andrews, Ioannis Tsochantaridis, and Thomas Hofmann.
2003. Support vector machines for multiple-instance learn-
ing. In NIPS 15, pages 561?568, Vancouver, BC. MIT Press.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe. 1998.
The Berkeley FrameNet project. In Proc. of COLING?ACL
?98, pages 86?90, San Francisco, CA. Morgan Kaufmann
Publishers.
Razvan C. Bunescu and Raymond J. Mooney. 2006. Sub-
sequence kernels for relation extraction. In Y. Weiss,
B. Scho?lkopf, and J. Platt, editors, NIPS 18.
M. Craven and J. Kumlien. 1999. Constructing biologi-
cal knowledge bases by extracting information from text
sources. In Proc. of ISMB?99, pages 77?86, Heidelberg,
Germany.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency tree
kernels for relation extraction. In Proc. of ACL?04, pages
423?429, Barcelona, Spain, July.
Thomas G. Dietterich, Richard H. Lathrop, and Tomas Lozano-
Perez. 1997. Solving the multiple instance problem with
axis-parallel rectangles. Artificial Intelligence, 89(1-2):31?
71.
Oren Etzioni, Michael Cafarella, Doug Downey, Ana-Maria
Popescu, Tal Shaked, Stephen Soderland, Daniel S. Weld,
and Alexander Yates. 2005. Unsupervised named-entity ex-
traction from the web: an experimental study. Artificial In-
telligence, 165(1):91?134.
T. Gartner, P.A. Flach, A. Kowalczyk, and A.J. Smola. 2002.
Multi-instance kernels. In In Proc. of ICML?02, pages 179?
186, Sydney, Australia, July. Morgan Kaufmann.
M. A. Hearst. 1992. Automatic acquisition of hyponyms from
large text corpora. In Proc. of ACL?92, Nantes, France.
Judea Pearl. 1986. Fusion, propagation, and structuring in be-
lief networks. Artificial Intelligence, 29(3):241?288.
Soumya Ray and Mark Craven. 2005. Supervised versus mul-
tiple instance learning: An empirical comparison. In Proc.
of ICML?05, pages 697?704, Bonn, Germany.
Bernhard Scho?lkopf and Alexander J. Smola. 2002. Learning
with kernels - support vector machines, regularization, opti-
mization and beyond. MIT Press, Cambridge, MA.
N. A. Smith and J. Eisner. 2005. Contrastive estimation: Train-
ing log-linear models on unlabeled data. In Proc. of ACL?05,
pages 354?362, Ann Arbor, Michigan.
Vladimir N. Vapnik. 1998. Statistical Learning Theory. John
Wiley & Sons.
D. Zelenko, C. Aone, and A. Richardella. 2003. Kernel meth-
ods for relation extraction. Journal of Machine Learning
Research, 3:1083?1106.
Q. Zhang, S. A. Goldman, W. Yu, and J. Fritts. 2002. Content-
based image retrieval using multiple-instance learning. In
Proc. of ICML?02, pages 682?689.
583
Proceedings of the ACL-ISMB Workshop on Linking Biological Literature, Ontologies and Databases: Mining
Biological Semantics, pages 46?53, Detroit, June 2005. c?2005 Association for Computational Linguistics
Using Biomedical Literature Mining to Consolidate the Set of Known
Human Protein?Protein Interactions
Arun Ramani, Edward Marcotte
Institute for Cellular and Molecular Biology
University of Texas at Austin
1 University Station A4800
Austin, TX 78712
arun@icmb.utexas.edu
marcotte@icmb.utexas.edu
Razvan Bunescu, Raymond Mooney
Department of Computer Sciences
University of Texas at Austin
1 University Station C0500
Austin, TX 78712
razvan@cs.utexas.edu
mooney@cs.utexas.edu
Abstract
This paper presents the results of a large-
scale effort to construct a comprehensive
database of known human protein inter-
actions by combining and linking known
interactions from existing databases and
then adding to them by automatically min-
ing additional interactions from 750,000
Medline abstracts. The end result is a
network of 31,609 interactions amongst
7,748 proteins. The text mining sys-
tem first identifies protein names in the
text using a trained Conditional Random
Field (CRF) and then identifies interac-
tions through a filtered co-citation anal-
ysis. We also report two new strategies
for mining interactions, either by finding
explicit statements of interactions in the
text using learned pattern-based rules or
a Support-Vector Machine using a string
kernel. Using information in existing on-
tologies, the automatically extracted data
is shown to be of equivalent accuracy to
manually curated data sets.
1 Introduction
Proteins are often considered in terms of their net-
works of interactions, a view that has spurred con-
siderable effort in mapping large-scale protein in-
teraction networks. Thus far, the most complete
protein networks are measured for yeast and de-
rive from the synthesis of varied large scale experi-
mental interaction data and in-silico interaction pre-
dictions (summarized in (von Mering et al, 2002;
Lee et al, 2004; Jansen et al, 2003)). Unlike the
case of yeast, only minimal progress has been made
with respect to the human proteome. While some
moderate-scale interaction maps have been created,
such as for the purified TNFa/NFKB protein com-
plex (Bouwmeester et al, 2004) and the proteins in-
volved in the human Smad signaling pathway (Col-
land et al, 2004), the bulk of known human pro-
tein interaction data derives from individual, small-
scale experiments reported in Medline. Many of
these interactions have been collected in the Reac-
tome (Joshi-Tope et al, 2005), BIND (Bader et al,
2003), DIP (Xenarios et al, 2002), and HPRD (Peri
et al, 2004) databases, with Reactome contributing
11,000 interactions that have been manually entered
from articles focusing on interactions in core cellular
pathways, and HPRD contributing a set of 12,000
interactions recovered by manual curation of Med-
line articles using teams of readers. Additional inter-
actions have been transferred from other organisms
based on orthology (Lehner and Fraser, 2004).
A comparison of these existing interaction data
sets is enlightening. Although the interactions from
these data sets are in principle derived from the same
source (Medline), the sets are quite disjoint (Fig-
ure 1) implying either that the sets are biased for
different classes of interactions, or that the actual
number of interactions in Medline is quite large.
We suspect both reasons. It is clear that each data
set has a different explicit focus (Reactome towards
core cellular machinery, HPRD towards disease-
linked genes, and DIP and BIND more randomly
46
distributed). Due to these biases, it is likely that
many interactions from Medline are still excluded
from these data sets. The maximal overlap between
interaction data sets is seen for BIND: 25% of these
interactions are also in HPRD or Reactome; only 1%
of Reactome interactions are in HPRD or BIND.
Figure 1: Overlap diagram for known datasets.
Medline now has records from more than 4,800
journals accounting for around 15 million articles.
These citations contain thousands of experimentally
recorded protein interactions, and even a cursory in-
vestigation of Medline reveals human protein inter-
actions not present in the current databases. How-
ever, retrieving these data manually is made diffi-
cult by the large number of articles, all lacking for-
mal structure. Automated extraction of informa-
tion would be preferable, and therefore, mining data
from Medline abstracts is a growing field (Jenssen
et al, 2001; Rzhetsky et al, 2004; Liu and Wong,
2003; Hirschman et al, 2002).
In this paper, we describe a framework for
automatic extraction of protein interactions from
biomedical literature. We focus in particular on the
difficult and important problem of identifying inter-
actions concerning human proteins. We describe a
system for first accurately identifying the names of
human proteins in the documents, then on identify-
ing pairs of interacting human proteins, and demon-
strate that the extracted protein interactions are com-
parable to those extracted manually. In the pro-
cess, we consolidate the existing set of publically-
available human protein interactions into a network
of 31,609 interactions between 7,748 proteins.
2 Assembling existing protein interaction
data
We previously gathered the existing human protein
interaction data sets ((Ramani et al, 2005); sum-
marized in Table 1), representing the current sta-
tus of the publically-available human interactome.
This required unification of the interactions under
a shared naming and annotation convention. For
this purpose, we mapped each interacting protein
to LocusLink (now EntrezGene) identification num-
bers and retained only unique interactions (i.e., for
two proteins A and B, we retain only A?B or B?A,
not both). We have chosen to omit self-interactions,
A?A or B?B, for technical reasons, as their qual-
ity cannot be assessed on the functional benchmark
that we describe in Section 3. In most cases, a small
loss of proteins occurred in the conversion between
the different gene identifiers (e.g., converting from
the NCBI ?gi? codes in BIND to LocusLink iden-
tifiers). In the case of Human Protein Reference
Database (HPRD), this processing resulted in a sig-
nificant reduction in the number of interactions from
12,013 total interactions to 6,054 unique, non-self
interactions, largely due to the fact that HPRD often
records both A-B and B-A interactions, as well as a
large number of self interactions, and indexes genes
by their common names rather than conventional
database entries, often resulting in multiple entries
for different synonyms. An additional 9,283 (or
60,000 at lower confidence) interactions are avail-
able from orthologous transfer of interactions from
large-scale screens in other organisms (orthology-
core and orthology-all) (Lehner and Fraser, 2004).
3 Two benchmark tests of accuracy for
interaction data
To measure the relative accuracy of each protein in-
teraction data set, we established two benchmarks
of interaction accuracy, one based on shared protein
function and the other based on previously known
interactions. First, we constructed a benchmark in
which we tested the extent to which interaction part-
ners in a data set shared annotation, a measure previ-
ously shown to correlate with the accuracy of func-
tional genomics data sets (von Mering et al, 2002;
Lee et al, 2004; Lehner and Fraser, 2004). We
used the functional annotations listed in the KEGG
47
Dataset Version Total Is (Ps) Self (A-A) Is (Ps) Unique (A-B) Is (Ps)
Reactome 08/03/04 12,497 (6,257) 160 (160) 12,336 (807)
BIND 08/03/04 6,212 (5,412) 549 (549) 5,663 (4,762)
HPRD* 04/12/04 12,013 (4,122) 3,028 (3,028) 6,054 (2,747)
Orthology (all) 03/31/04 71,497 (6,257) 373 (373) 71,124 (6,228)
Orthology (core) 03/31/04 11,488 (3,918) 206 (206) 11,282 (3,863)
Table 1: Is = Interactions, Ps = Proteins.
(Kanehisa et al, 2004) and Gene Ontology (Ash-
burner et al, 2000) annotation databases. These
databases provide specific pathway and biological
process annotations for approximately 7,500 human
genes, assigning human genes into 155 KEGG path-
ways (at the lowest level of KEGG) and 1,356 GO
pathways (at level 8 of the GO biological process
annotation). KEGG and GO annotations were com-
bined into a single composite functional annotation
set, which was then split into independent testing
and training sets by randomly assigning annotated
genes into the two categories (3,800 and 3,815 anno-
tated genes respectively). For the second benchmark
based on known physical interactions, we assembled
the human protein interactions from Reactome and
BIND, a set of 11,425 interactions between 1,710
proteins. Each benchmark therefore consists of a
set of binary relations between proteins, either based
on proteins sharing annotation or physically inter-
acting. Generally speaking, we expect more accu-
rate protein interaction data sets to be more enriched
in these protein pairs. More specifically, we expect
true physical interactions to score highly on both
tests, while non-physical or indirect associations,
such as genetic associations, should score highly on
the functional, but not physical interaction, test.
For both benchmarks, the scoring scheme for
measuring interaction set accuracy is in the form of
a log odds ratio of gene pairs either sharing anno-
tations or physically interacting. To evaluate a data
set, we calculate a log likelihood ratio (LLR) as:
LLR = ln
P (DjI)
P (Dj:I)
= ln
P (IjD)P (:I)
P (:IjD)P (I)
(1)
where P (DjI) and P (Dj:I) are the probability
of observing the data D conditioned on the genes
sharing benchmark associations (I) and not sharing
benchmark associations (:I). In its expanded form
(obtained by applying Bayes theorem), P (IjD) and
P (:IjD) are estimated using the frequencies of in-
teractions observed in the given data set D between
annotated genes sharing benchmark associations and
not sharing associations, respectively, while the pri-
ors P (I) and P (:I) are estimated based on the to-
tal frequencies of all benchmark genes sharing the
same associations and not sharing associations, re-
spectively. A score of zero indicates interaction part-
ners in the data set being tested are no more likely
than random to belong to the same pathway or to in-
teract; higher scores indicate a more accurate data
set.
Among the literature-derived interactions (Reac-
tome, BIND, HPRD), a total of 17,098 unique in-
teractions occur in the public data sets. Testing the
existing protein interaction data on the functional
benchmark reveals that Reactome has the highest
accuracy (LLR = 3.8), followed by BIND (LLR =
2.9), HPRD (LLR = 2.1), core orthology-inferred in-
teractions (LLR = 2.1) and the non-core orthology-
inferred interaction (LLR = 1.1). The two most
accurate data sets, Reactome and BIND, form the
basis of the protein interaction?based benchmark.
Testing the remaining data sets on this benchmark
(i.e., for their consistency with these accurate pro-
tein interaction data sets) reveals a similar ranking in
the remaining data. Core orthology-inferred interac-
tions are the most accurate (LLR = 5.0), followed by
HPRD (LLR = 3.7) and non-core orthology inferred
interactions (LLR = 3.7).
4 Framework for Mining Protein?Protein
Interactions
The extraction of interacting proteins from Medline
abstracts proceeds in two separate steps:
1. First, we automatically identify protein names
48
using a CRF system trained on a set of 750
abstracts manually annotated for proteins (see
Section 5 for details).
2. Based on the output of the CRF tagger, we fil-
ter out less confident extractions and then try to
detect which pairs of the remaining extracted
protein names are interaction pairs.
For the second step, we investigate two general
methods:
 Use co-citation analysis to score each pair of
proteins based on the assumption that proteins
co-occurring in a large number of abstracts tend
to be interacting proteins. Out of the resulting
protein pairs we keep only those that co-occur
in abstracts likely to discuss interactions, based
on a Naive Bayes classifier (see Section 6 for
details).
 Given that we already have a set of 230 Med-
line abstracts manually tagged for both proteins
and interactions, we can use it to train an inter-
action extractor. In Section 7 we discuss two
different methods for learning this interaction
extractor.
5 A CRF Tagger for Protein Names
The task of identifying protein names is made diffi-
cult by the fact that unlike other organisms, such as
yeast or E. coli, the human genes have no standard-
ized naming convention, and thus present one of the
hardest sets of gene/protein names to extract. For
example, human proteins may be named with typ-
ical English words, such as ?light?, ?map?, ?com-
plement?, and ?Sonic Hedgehog?. It is therefore
necessary that an information extraction algorithm
be specifically trained to extract gene and protein
names accurately.
We have previously described (Bunescu et al,
2005) effective protein and gene name tagging us-
ing a Maximum Entropy based algorithm. Condi-
tional Random Fields (CRF) (Lafferty et al, 2001)
are new types of probabilistic models that preserve
all the advantages of Maximum Entropy models and
at the same time avoid the label bias problem by al-
lowing a sequence of tagging decisions to compete
against each other in a global probabilistic model.
In both training and testing the CRF protein-name
tagger, the corresponding Medline abstracts were
processed as follows. Text was tokenized using
white-space as delimiters and treating all punctua-
tion marks as separate tokens. The text was seg-
mented into sentences, and part-of-speech tags were
assigned to each token using Brill?s tagger (Brill,
1995). For each token in each sentence, a vector of
binary features was generated using the feature tem-
plates employed by the Maximum Entropy approach
described in (Bunescu et al, 2005). Generally, these
features make use of the words occurring before and
after the current position in the text, their POS tags
and capitalization patterns. Each feature occurring
in the training data is associated with a parameter in
the CRF model. We used the CRF implementation
from (McCallum, 2002). To train the CRF?s parame-
ters, we used 750 Medline abstracts manually anno-
tated for protein names (Bunescu et al, 2005). We
then used the trained system to tag protein and gene
names in the entire set of 753,459 Medline abstracts
citing the word ?human?.
In Figure 2 we compare the performance of the
CRF tagger with that of the Maximum Entropy tag-
ger from (Bunescu et al, 2005), using the same
set of features, by doing 10-fold cross-validation on
Yapex ? a smaller dataset of 200 manually annotated
abstracts (Franzen et al, 2002). Each model assigns
to each extracted protein name a normalized confi-
dence value. The precision?recall curves from Fig-
ure 2 are obtained by varying a threshold on the min-
imum accepted confidence. We also plot the preci-
sion and recall obtained by simply matching textual
phrases against entries from a protein dictionary.
 50
 60
 70
 80
 90
 100
 0  20  40  60  80  100
Pr
ec
is
io
n 
(%
)
Recall (%)
CRF
MaxEnt
Dict
Figure 2: Protein Tagging Performance.
49
The dictionary of human protein names was
assembled from the LocusLink and Swissprot
databases by manually curating the gene names
and synonyms (87,723 synonyms between 18,879
unique gene names) to remove genes that were re-
ferred to as ?hypothetical? or ?probable? and also to
omit entries that referred to more than one protein
identifier.
6 Co-citation Analysis and Bayesian
Classification
In order to establish which interactions occurred
between the proteins identified in the Medline ab-
stracts, we used a 2-step strategy: measure co-
citation of protein names, then enrich these pairs for
physical interactions using a Bayesian filter. First,
we counted the number of abstracts citing a pair of
proteins, and then calculated the probability of co-
citation under a random model based on the hyper-
geometric distribution (Lee et al, 2004; Jenssen et
al., 2001) as:
P (kjN;m; n) =

n
k

N   n
m  k


N
m
 (2)
where N equals the total number of abstracts, n of
which cite the first protein, m cite the second pro-
tein, and k cite both.
Empirically, we find the co-citation probability
has a hyperbolic relationship with the accuracy on
the functional annotation benchmark from Section 3,
with protein pairs co?cited with low random proba-
bility scoring high on the benchmark.
With a threshold on the estimated extraction con-
fidence of 80% (as computed by the CRF model)
in the protein name identification, close to 15,000
interactions are extracted with the co-citation ap-
proach that score comparable or better on the func-
tional benchmark than the manually extracted inter-
actions from HPRD, which serves to establish a min-
imal threshold for our mined interactions.
However, it is clear that proteins are co-cited for
many reasons other than physical interactions. We
therefore tried to enrich specifically for physical in-
teractions by applying a secondary filter. We applied
a Bayesian classifier (Marcotte et al, 2001) to mea-
sure the likelihood of the abstracts citing the pro-
tein pairs to discuss physical protein?protein inter-
actions. The classifier scores each of the co-citing
abstracts according to the usage frequency of dis-
criminating words relevant to physical protein inter-
actions. For a co-cited protein pair, we calculated
the average score of co-citing Medline abstracts and
used this to re-rank the top-scoring 15,000 co-cited
protein pairs.
Interactions extracted by co-citation and filtered
using the Bayesian estimator compare favorably
with the other interaction data sets on the functional
annotation benchmark (Figure 3). Testing the accu-
racy of these extracted protein pairs on the physi-
cal interaction benchmark (Figure 4) reveals that the
co-cited proteins scored high by this classifier are
indeed strongly enriched for physical interactions.
 0.5
 1
 1.5
 2
 2.5
 3
 3.5
 4
 0  10000  20000  30000  40000  50000  60000  70000
LL
R
 s
co
re
, f
un
ct
io
na
l b
en
ch
m
ar
k
# of interactions recovered
Co-citation, Bayes filter
BIND
Reactome
HPRD
Orthology (core)
Orthology (core)
Figure 3: Accuracy, functional benchmark
 2
 2.5
 3
 3.5
 4
 4.5
 5
 5.5
 6
 0  10000  20000  30000  40000  50000  60000  70000
LL
R
 s
co
re
, p
hy
sic
al
 b
en
ch
m
ar
k
# of interactions recovered
Co-citation, Bayes filter
HPRD
Orthology (core)
Orthology (core)
Figure 4: Accuracy, physical benchmark
Keeping all the interactions that score better than
HPRD, our co-citation / Bayesian classifier analy-
sis yields 6,580 interactions between 3,737 proteins.
By combining these interactions with the 26,280 in-
teractions from the other sources, we obtained a fi-
50
nal set of 31,609 interactions between 7,748 human
proteins.
7 Learning Interaction Extractors
In (Bunescu et al, 2005) we described a dataset of
230 Medline abstracts manually annotated for pro-
teins and their interactions. This can be used as a
training dataset for a method that learns interaction
extractors. Such a method simply classifies a sen-
tence containing two protein names as positive or
negative, where positive means that the sentence as-
serts an interaction between the two proteins. How-
ever a sentence in the training data may contain more
than two proteins and more than one pair of inter-
acting proteins. In order to extract the interacting
pairs, we replicate the sentences having n proteins
(n  2) into Cn
2
sentences such that each one has
exactly two of the proteins tagged, with the rest of
the protein tags omitted. If the tagged proteins in-
teract, then the replicated sentence is added to the
set of positive sentences, otherwise it is added to the
set of negative sentences. During testing, a sentence
having n proteins (n  2) is again replicated into
C
n
2
sentences in a similar way.
7.1 Extraction using Longest Common
Subsequences (ELCS)
Blaschke et al (Blaschke and Valencia, 2001;
Blaschke and Valencia, 2002) manually developed
rules for extracting interacting proteins. Each of
their rules (or frames) is a sequence of words (or
POS tags) and two protein-name tokens. Between
every two adjacent words is a number indicating
the maximum number of intervening words allowed
when matching the rule to a sentence. In (Bunescu
et al, 2005) we described a new method ELCS (Ex-
traction using Longest Common Subsequences) that
automatically learns such rules. ELCS? rule repre-
sentation is similar to that in (Blaschke and Valen-
cia, 2001; Blaschke and Valencia, 2002), except that
it currently does not use POS tags, but allows dis-
junctions of words. Figure 5 shows an example of a
rule learned by ELCS. Words in square brackets sep-
arated by ?j? indicate disjunctive lexical constraints,
i.e. one of the given words must match the sen-
tence at that position. The numbers in parentheses
between adjacent constraints indicate the maximum
number of unconstrained words allowed between the
two (called a word gap). The protein names are de-
noted here with PROT. A sentence matches the rule
if and only if it satisfies the word constraints in the
given order and respects the respective word gaps.
- (7) interaction (0) [between j of] (5) PROT (9) PROT (17) .
Figure 5: Sample extraction rule learned by ELCS.
7.2 Extraction using a Relation Kernel (ERK)
Both Blaschke and ELCS do interaction extraction
based on a limited set of matching rules, where a rule
is simply a sparse (gappy) subsequence of words (or
POS tags) anchored on the two protein-name tokens.
Therefore, the two methods share a common limita-
tion: either through manual selection (Blaschke), or
as a result of the greedy learning procedure (ELCS),
they end up using only a subset of all possible an-
chored sparse subsequences. Ideally, we would want
to use all such anchored sparse subsequences as fea-
tures, with weights reflecting their relative accuracy.
However explicitly creating for each sentence a vec-
tor with a position for each such feature is infeasi-
ble, due to the high dimensionality of the feature
space. Here we can exploit an idea used before in
string kernels (Lodhi et al, 2002): computing the
dot-product between two such vectors amounts to
calculating the number of common anchored sub-
sequences between the two sentences. This can be
done very efficiently by modifying the dynamic pro-
gramming algorithm from (Lodhi et al, 2002) to ac-
count only for anchored subsequences i.e. sparse
subsequences which contain the two protein-name
tokens. Besides restricting the word subsequences
to be anchored on the two protein tokens, we can
further prune down the feature space by utilizing the
following property of natural language statements:
whenever a sentence asserts a relationship between
two entity mentions, it generally does this using one
of the following three patterns:
 [FI] Fore?Inter: words before and between the
two entity mentions are simultaneously used to
express the relationship. Examples: ?interac-
tion of hP
1
i with hP
2
i?, ?activation of hP
1
i by
hP
2
i?.
51
 [I] Inter: only words between the two entity
mentions are essential for asserting the rela-
tionship. Examples: ?hP
1
i interacts with hP
2
i?,
?hP
1
i is activated by hP
2
i?.
 [IA] Inter?After: words between and after the
two entity mentions are simultaneously used
to express the relationship. Examples: hP
1
i ?
hP
2
i complex?, ?hP
1
i and hP
2
i interact?.
Another useful observation is that all these pat-
terns use at most 4 words to express the relationship
(not counting the two entities). Consequently, when
computing the relation kernel, we restrict the count-
ing of common anchored subsequences only to those
having one of the three types described above, with a
maximum word-length of 4. This type of feature se-
lection leads not only to a faster kernel computation,
but also to less overfitting, which results in increased
accuracy (we omit showing here comparative results
supporting this claim, due to lack of space).
We used this kernel in conjunction with Support
Vector Machines (Vapnik, 1998) learning in or-
der to find a decision hyperplane that best separates
the positive examples from negative examples. We
modified the libsvm package for SVM learning by
plugging in the kernel described above.
7.3 Preliminary experimental results
We compare the following three systems on the task
of retrieving protein interactions from the dataset of
230 Medline abstracts (assuming gold standard pro-
teins):
 [Manual]: We report the performance of the
rule-based system of (Blaschke and Valencia,
2001; Blaschke and Valencia, 2002).
 [ELCS]: We report the 10-fold cross-validated
results from (Bunescu et al, 2005) as a
precision-recall graph.
 [ERK]: Based on the same splits as those
used by ELCS, we compute the corresponding
precision-recall graph.
The results, summarized in Figure 6, show that
the relation kernel outperforms both ELCS and the
manually written rules. In future work, we intend
to analyze the complete Medline with ERK and in-
tegrate the extracted interactions into a larger com-
posite set.
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 100
 0  10  20  30  40  50  60  70  80  90  100
Pr
ec
is
io
n 
(%
)
Recall (%)
ERK
Manual
ELCS
Figure 6: PR curves for interaction extractors.
8 Conclusion
Through a combination of automatic text mining and
consolidation of existing databases, we have con-
structed a large database of known human protein
interactions containing 31,609 interactions amongst
7,748 proteins. By mining 753,459 human-related
abstracts from Medline with a combination of a
CRF-based protein tagger, co-citation analysis, and
automatic text classification, we extracted a set of
6,580 interactions between 3,737 proteins. By uti-
lizing information in existing knowledge bases, this
automatically extracted data was found to have an
accuracy comparable to manually developed data
sets. More details on our interaction database have
been published in the biological literature (Ramani
et al, 2005) and it is available on the web at
http://bioinformatics.icmb.utexas.edu/idserve. We
are currently exploring improvements to this
database by more accurately identifying assertions
of interactions in the text using an SVM that exploits
a relational string kernel.
9 Acknowledgements
This work was supported by grants from the N.S.F.
(IIS-0325116, EIA-0219061), N.I.H. (GM06779-
01), Welch (F1515), and a Packard Fellowship
(E.M.M.).
52
References
M. Ashburner, C. A. Ball, J. A. Blake, D. Botstein, H. Butler,
J. M. Cherry, A. P. Davis, K. Dolinski, S. S. Dwight, and J. T.
et al Eppig. 2000. Gene ontology: tool for the unification
of biology. the gene ontology consortium. Nature Genetics,
25(1):25?29.
G. D. Bader, D. Betel, and C. W. Hogue. 2003. Bind: the
biomolecular interaction network database. Nucleic Acids
Research, 31(1):248?250.
C. Blaschke and A. Valencia. 2001. Can bibliographic pointers
for known biological data be found automatically? protein
interactions as a case study. Comparative and Functional
Genomics, 2:196?206.
C. Blaschke and A. Valencia. 2002. The frame-based module
of the Suiseki information extraction system. IEEE Intelli-
gent Systems, 17:14?20.
T. Bouwmeester, A. Bauch, H. Ruffner, P. O. Angrand,
G. Bergamini, K. Croughton, C. Cruciat, D. Eberhard,
J. Gagneur, S. Ghidelli, and et al 2004. A physical and
functional map of the human tnf-alpha/nf-kappa b signal
transduction pathway. Nature Cell Biology, 6(2):97?105.
Eric Brill. 1995. Transformation-based error-driven learning
and natural language processing: A case study in part-of-
speech tagging. Computational Linguistics, 21(4):543?565.
Razvan Bunescu, Ruifang Ge, Rohit J. Kate, Edward M.
Marcotte, Raymond J. Mooney, Arun Kumar Ramani, and
Yuk Wah Wong. 2005. Comparative experiments on learn-
ing information extractors for proteins and their interactions.
Artificial Intelligence in Medicine (special issue on Sum-
marization and Information Extraction from Medical Doc-
uments), 33(2):139?155.
F. Colland, X. Jacq, V. Trouplin, C. Mougin, C. Groizeleau,
A. Hamburger, A. Meil, J. Wojcik, P. Legrain, and J. M.
Gauthier. 2004. Functional proteomics mapping of a human
signaling pathway. Genome Research, 14(7):1324?1332.
K. Franzen, G. Eriksson, F. Olsson, L. Asker, P. Liden, and
J. Coster. 2002. Protein names and how to find them. Inter-
national Journal of Medical Informatics, 67(1-3):49?61.
L. Hirschman, J. C. Park, J. Tsujii, L. Wong, and C. H. Wu.
2002. Accomplishments and challenges in literature data
mining for biology. Bioinformatics, 18(12):1553?1561.
R. Jansen, H. Yu, D. Greenbaum, Y. Kluger, N. J. Krogan,
S. Chung, A. Emili, M. Snyder, J. F. Greenblatt, and M. Ger-
stein. 2003. A bayesian networks approach for predict-
ing protein-protein interactions from genomic data. Science,
302(5644):449?453.
T. K. Jenssen, A. Laegreid, J. Komorowski, and E. Hovig. 2001.
A literature network of human genes for high-throughput
analysis of gene expression. Nature Genetics, 28(1):21?28.
G. Joshi-Tope, M. Gillespie, I. Vastrik, P. D?Eustachio,
E. Schmidt, B. de Bono, B. Jassal, G. R. Gopinath, G. R.
Wu, L. Matthews, and et al 2005. Reactome: a knowl-
edgebase of biological pathways. Nucleic Acids Research,
33 Database Issue:D428?432.
M. Kanehisa, S. Goto, S. Kawashima, Y. Okuno, and M. Hat-
tori. 2004. The kegg resource for deciphering the genome.
Nucleic Acids Research, 32 Database issue:D277?280.
John Lafferty, Andrew McCallum, and Fernando Pereira. 2001.
Conditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceedings of
18th International Conference on Machine Learning (ICML-
2001), pages 282?289, Williamstown, MA.
I. Lee, S. V. Date, A. T. Adai, and E. M. Marcotte. 2004. A
probabilistic functional network of yeast genes. Science,
306(5701):1555?1558.
B. Lehner and A. G. Fraser. 2004. A first-draft human protein-
interaction map. Genome Biology, 5(9):R63.
H. Liu and L. Wong. 2003. Data mining tools for biological se-
quences. Journal of Bioinformatics and Computational Bi-
ology, 1(1):139?167.
Huma Lodhi, Craig Saunders, John Shawe-Taylor, Nello Cris-
tianini, and Chris Watkins. 2002. Text classification us-
ing string kernels. Journal of Machine Learning Research,
2:419?444.
E. M. Marcotte, I. Xenarios, and D. Eisenberg. 2001. Min-
ing literature for protein-protein interactions. Bioinformat-
ics, 17(4):359?363.
Andrew Kachites McCallum. 2002. Mallet: A machine learn-
ing for language toolkit. http://mallet.cs.umass.edu.
S. Peri, J. D. Navarro, T. Z. Kristiansen, R. Amanchy, V. Suren-
dranath, B. Muthusamy, T. K. Gandhi, K. N. Chandrika,
N. Deshpande, S. Suresh, and et al 2004. Human protein
reference database as a discovery resource for proteomics.
Nucleic Acids Research, 32 Database issue:D497?501.
A. K. Ramani, R. C. Bunescu, R. J. Mooney, and E. M. Mar-
cotte. 2005. Consolidating the set of know human protein-
protein interactions in preparation for large-scale mapping of
the human interactome. Genome Biology, 6(5):r40.
A. Rzhetsky, I. Iossifov, T. Koike, M. Krauthammer, P. Kra,
M. Morris, H. Yu, P. A. Duboue, W. Weng, W. J. Wilbur,
V. Hatzivassiloglou, and C. Friedman. 2004. Geneways: a
system for extracting, analyzing, visualizing, and integrating
molecular pathway data. Journal of Biomedical Informatics,
37(1):43?53.
Vladimir N. Vapnik. 1998. Statistical Learning Theory. John
Wiley & Sons.
C. von Mering, R. Krause, B. Snel, M. Cornell, S. G. Oliver,
S. Fields, and P. Bork. 2002. Comparative assessment of
large-scale data sets of protein-protein interactions. Nature,
417(6887):399?403.
I. Xenarios, L. Salwinski, X. J. Duan, P. Higney, S. M. Kim, and
D. Eisenberg. 2002. Dip, the database of interacting pro-
teins: a research tool for studying cellular networks of pro-
tein interactions. Nucleic Acids Research, 30(1):303?305.
53
Proceedings of the BioNLP Workshop on Linking Natural Language Processing and Biology at HLT-NAACL 06, pages 49?56,
New York City, June 2006. c?2006 Association for Computational Linguistics
Integrating Co-occurrence Statistics with Information Extraction for
Robust Retrieval of Protein Interactions from Medline
Razvan Bunescu, Raymond Mooney
Department of Computer Sciences
University of Texas at Austin
1 University Station C0500
Austin, TX 78712
razvan@cs.utexas.edu
mooney@cs.utexas.edu
Arun Ramani, Edward Marcotte
Institute for Cellular and Molecular Biology
University of Texas at Austin
1 University Station A4800
Austin, TX 78712
arun@icmb.utexas.edu
marcotte@icmb.utexas.edu
Abstract
The task of mining relations from collec-
tions of documents is usually approached
in two different ways. One type of sys-
tems do relation extraction from individ-
ual sentences, followed by an aggrega-
tion of the results over the entire collec-
tion. Other systems follow an entirely dif-
ferent approach, in which co-occurrence
counts are used to determine whether the
mentioning together of two entities is due
to more than simple chance. We show
that increased extraction performance can
be obtained by combining the two ap-
proaches into an integrated relation ex-
traction model.
1 Introduction
Information Extraction (IE) is a natural language
processing task in which text documents are ana-
lyzed with the aim of finding mentions of relevant
entities and important relationships between them.
In many cases, the subtask of relation extraction re-
duces to deciding whether a sentence asserts a par-
ticular relationship between two entities, which is
still a difficult, unsolved problem. There are how-
ever cases where the decision whether the two enti-
ties are in a relationship is made relative to an en-
tire document, or a collection of documents. In the
biomedical domain, for example, one may be inter-
ested in finding the pairs of human proteins that are
said to be interacting in any of the Medline abstracts,
where the answer is not required to specify which
abstracts are actually describing the interaction. As-
sembling a ranked list of interacting proteins can be
very useful to biologists - based on this list, they can
make more informed decisions with respect to which
genes to focus on in their research.
In this paper, we investigate methods that use
multiple occurrences of the same pair of entities
across a collection of documents in order to boost
the performance of a relation extraction system.
The proposed methods are evaluated on the task
of finding pairs of human proteins whose interac-
tions are reported in Medline abstracts. The major-
ity of known human protein interactions are derived
from individual, small-scale experiments reported in
Medline. Some of these interactions have already
been collected in the Reactome (Joshi-Tope et al,
2005), BIND (Bader et al, 2003), DIP (Xenarios et
al., 2002), and HPRD (Peri et al, 2004) databases.
The amount of human effort involved in creating and
updating these databases is currently no match for
the continuous growth of Medline. It is therefore
very useful to have a method that automatically and
reliably extracts interaction pairs from Medline.
Systems that do relation extraction from a col-
lection of documents can be divided into two ma-
jor categories. In one category are IE systems
that first extract information from individual sen-
tences, and then combine the results into corpus-
level results (Craven, 1999; Skounakis and Craven,
2003). The second category corresponds to ap-
proaches that do not exploit much information from
the context of individual occurrences. Instead,
based on co-occurrence counts, various statistical
49
or information-theoretic tests are used to decide
whether the two entities in a pair appear together
more often than simple chance would predict (Lee
et al, 2004; Ramani et al, 2005). We believe that
a combination of the two approaches can inherit the
advantages of each method and lead to improved re-
lation extraction accuracy.
The following two sections describe the two or-
thogonal approaches to corpus-level relation extrac-
tion. A model that integrates the two approaches is
then introduced in Section 4. This is followed by a
description of the dataset used for evaluation in Sec-
tion 5, and experimental results in Section 6.
2 Sentence-level relation extraction
Most systems that identify relations between enti-
ties mentioned in text documents consider only pair
of entities that are mentioned in the same sentence
(Ray and Craven, 2001; Zhao and Grishman, 2005;
Bunescu and Mooney, 2005). To decide the exis-
tence and the type of a relationship, these systems
generally use lexico-semantic clues inferred from
the sentence context of the two entities. Much re-
search has been focused recently on automatically
identifying biologically relevant entities and their
relationships such as protein-protein interactions or
subcellular localizations. For example, the sentence
?TR6 specifically binds Fas ligand?, states an inter-
action between the two proteins TR6 and Fas ligand.
One of the first systems for extracting interactions
between proteins is described in (Blaschke and Va-
lencia, 2001). There, sentences are matched deter-
ministically against a set of manually developed pat-
terns, where a pattern is a sequence of words or Part-
of-Speech (POS) tags and two protein-name tokens.
Between every two adjacent words is a number in-
dicating the maximum number of words that can be
skipped at that position. An example is: ?interaction
of (3) <P> (3) with (3) <P>?. This approach is
generalized in (Bunescu and Mooney, 2005), where
subsequences of words (or POS tags) from the sen-
tence are used as implicit features. Their weights are
learned by training a customized subsequence ker-
nel on a dataset of Medline abstracts annotated with
proteins and their interactions.
A relation extraction system that works at the
sentence-level and which outputs normalized confi-
dence values for each extracted pair of entities can
also be used for corpus-level relation extraction. A
straightforward way to do this is to apply an aggre-
gation operator over the confidence values inferred
for all occurrences of a given pair of entities. More
exactly, if p
1
and p
2
are two entities that occur in a
total of n sentences s
1
, s
2
, ..., s
n
in the entire corpus
C , then the confidence P (R(p
1
; p
2
)jC) that they are
in a particular relationship R is defined as:
P (R(p
1
; p
2
)jC) =  (fP (R(p
1
; p
2
)js
i
)ji=1:ng)
Table 1 shows only four of the many possible
choices for the aggregation operator  .
max  
max
= max
i
P (R(p
1
; p
2
)js
i
)
noisy-or  
nor
= 1  
Y
i
(1   P (R(p
1
; p
2
)js
i
))
avg  
avg
=
X
i
P (R(p
1
; p
2
)js
i
)
n
and  
and
=
Y
i
P (R(p
1
; p
2
)js
i
)
1=n
Table 1: Aggregation Operators.
Out of the four operators in Table 1, we believe
that the max operator is the most appropriate for ag-
gregating confidence values at the corpus-level. The
question that needs to be answered is whether there
is a sentence somewhere in the corpus that asserts
the relationship R between entities p
1
and p
2
. Us-
ing avg instead would answer a different question -
whether R(p
1
; p
2
) is true in most of the sentences
containing p
1
and p
2
. Also, the and operator would
be most appropriate for finding whether R(p
1
; p
2
)
is true in all corresponding sentences in the corpus.
The value of the noisy-or operator (Pearl, 1986) is
too dependent on the number of occurrences, there-
fore it is less appropriate for a corpus where the oc-
currence counts vary from one entity pair to another
(as confirmed in our experiments from Section 6).
For examples, if the confidence threshold is set at
0:5, and the entity pair (p
1
; p
2
) occurs in 6 sentences
or less, each with confidence 0:1, then R(p
1
; p
2
) is
false, according to the noisy-or operator. However,
if (p
1
; p
2
) occur in more than 6 sentences, with the
same confidence value of 0:1, then the correspond-
ing noisy-or value exceeds 0:5, making R(p
1
; p
2
)
true.
50
3 Co-occurrence statistics
Given two entities with multiple mentions in a large
corpus, another approach to detect whether a re-
lationship holds between them is to use statistics
over their occurrences in textual patterns that are
indicative for that relation. Various measures such
as pointwise mutual information (PMI) , chi-square
(2) or log-likelihood ratio (LLR) (Manning and
Schu?tze, 1999) use the two entities? occurrence
statistics to detect whether their co-occurrence is due
to chance, or to an underlying relationship.
A recent example is the co-citation approach from
(Ramani et al, 2005), which does not try to find spe-
cific assertions of interactions in text, but rather ex-
ploits the idea that if many different abstracts refer-
ence both protein p
1
and protein p
2
, then p
1
and p
2
are likely to interact. Particularly, if the two proteins
are co-cited significantly more often than one would
expect if they were cited independently at random,
then it is likely that they interact. The model used
to compute the probability of random co-citation is
based on the hypergeometric distribution (Lee et al,
2004; Jenssen et al, 2001). Thus, if N is the total
number of abstracts, n of which cite the first protein,
m cite the second protein, and k cite both, then the
probability of co-citation under a random model is:
P (kjN;m; n) =

n
k

N   n
m  k


N
m
 (1)
The approach that we take in this paper is to con-
strain the two proteins to be mentioned in the same
sentence, based on the assumption that if there is
a reason for two protein names to co-occur in the
same sentence, then in most cases that is caused by
their interaction. To compute the ?degree of inter-
action? between two proteins p
1
and p
2
, we use the
information-theoretic measure of pointwise mutual
information (Church and Hanks, 1990; Manning
and Schu?tze, 1999), which is computed based on the
following quantities:
1. N : the total number of protein pairs co-
occurring in the same sentence in the corpus.
2. P (p
1
; p
2
) ' n
12
=N : the probability that p
1
and p
2
co-occur in the same sentence; n
12
= the
number of sentences mentioning both p
1
and
p
2
.
3. P (p
1
; p) ' n
1
=N : the probability that p
1
co-
occurs with any other protein in the same sen-
tence; n
1
= the number of sentences mention-
ing p
1
and p.
4. P (p
2
; p) ' n
2
=N : the probability that p
2
co-
occurs with any other protein in the same sen-
tence; n
2
= the number of sentences mention-
ing p
2
and p.
The PMI is then defined as in Equation 2 below:
PMI(p
1
; p
2
) = log
P (p
1
; p
2
)
P (p
1
; p)  P (p
2
; p)
' logN
n
12
n
1
 n
2
(2)
Given that the PMI will be used only for ranking
pairs of potentially interacting proteins, the constant
factor N and the log operator can be ignored. For
sake of simplicity, we use the simpler formula from
Equation 3.
sPMI(p
1
; p
2
) =
n
12
n
1
 n
2
(3)
4 Integrated model
The sPMI(p
1
; p
2
) formula can be rewritten as:
sPMI(p
1
; p
2
) =
1
n
1
 n
2

n
12
X
i=1
1 (4)
Let s
1
, s
2
, ..., s
n
12
be the sentence contexts corre-
sponding to the n
12
co-occurrences of p
1
and p
2
,
and assume that a sentence-level relation extractor
is available, with the capability of computing nor-
malized confidence values for all extractions. Then
one way of using the extraction confidence is to have
each co-occurrence weighted by its confidence, i.e.
replace the constant 1 with the normalized scores
P (R(p
1
; p
2
)js
i
), as illustrated in Equation 5. This
results in a new formula wPMI (weighted PMI),
which is equal with the product between sPMI and
the average aggregation operator  
avg
.
wPMI(p
1
; p
2
) =
1
n
1
 n
2

n
12
X
i=1
P (R(p
1
; p
2
)js
i
)
=
n
12
n
1
 n
2
  
avg
(5)
51
The operator  
avg
can be replaced with any other ag-
gregation operator from Table 1. As argued in Sec-
tion 2, we consider max to be the most appropriate
operator for our task, therefore the integrated model
is based on the weighted PMI product illustrated in
Equation 6.
wPMI(p
1
; p
2
) =
n
12
n
1
 n
2
  
max
(6)
=
n
12
n
1
 n
2
 max
i
P (R(p
1
; p
2
)js
i
)
If a pair of entities p
1
and p
2
is ranked by wPMI
among the top pairs, this means that it is unlikely
that p
1
and p
2
have co-occurred together in the en-
tire corpus by chance, and at the same time there is
at least one mention where the relation extractor de-
cides with high confidence that R(p
1
; p
2
) = 1.
5 Evaluation Corpus
Contrasting the performance of the integrated model
against the sentence-level extractor or the PMI-
based ranking requires an evaluation dataset that
provides two types of annotations:
1. The complete list of interactions reported in the
corpus (Section 5.1).
2. Annotation of mentions of genes and proteins,
together with their corresponding gene identi-
fiers (Section 5.2).
We do not differentiate between genes and their
protein products, mapping them to the same gene
identifiers. Also, even though proteins may partic-
ipate in different types of interactions, we are con-
cerned only with detecting whether they interact in
the general sense of the word.
5.1 Medline Abstracts and Interactions
In order to compile an evaluation corpus and an as-
sociated comprehensive list of interactions, we ex-
ploited information contained in the HPRD (Peri
et al, 2004) database. Every interaction listed in
HPRD is linked to a set of Medline articles where the
corresponding experiment is reported. More exactly,
each interaction is specified in the database as a tuple
that contains the LocusLink (now EntrezGene) iden-
tifiers of all genes involved and the PubMed identi-
fiers of the corresponding articles (as illustrated in
Table 2).
Interaction (XML) (HPRD)
<interaction>
<gene>2318</gene>
<gene>58529</gene>
<pubmed>10984498 11171996</pubmed>
</interaction>
Participant Genes (XML) (NCBI)
<gene id=?2318?>
<name>FLNC</name>
<description>filamin C, gamma</description>
<synonyms>
<synonym>ABPA</synonym>
<synonym>ABPL</synonym>
<synonym>FLN2</synonym>
<synonym>ABP-280</synonym>
<synonym>ABP280A</synonym>
</synonyms>
<proteins>
<protein>gamma filamin</protein>
<protein>filamin 2</protein>
<protein>gamma-filamin</protein>
<protein>ABP-L, gamma filamin</protein>
<protein>actin-binding protein 280</protein>
<protein>gamma actin-binding protein</protein>
<protein>filamin C, gamma</protein>
</proteins>
</gene>
<gene id=?58529?>
<name>MYOZ1</name>
<description>myozenin 1</description>
<synonyms> ... </synonyms>
<proteins> ... </proteins>
</gene>
Medline Abstract (XML) (NCBI)
<PMID>10984498</PMID>
<AbstractText>
We found that this protein binds to three other Z-disc pro-
teins; therefore, we have named it FATZ, gamma-filamin,
alpha-actinin and telethonin binding protein of the Z-disc.
</AbstractText>
Table 2: Interactions, Genes and Abstracts.
The evaluation corpus (henceforth referred to as
the HPRD corpus) is created by collecting the Med-
line abstracts corresponding to interactions between
human proteins, as specified in HPRD. In total,
5,617 abstracts are included in this corpus, with an
associated list of 7,785 interactions. This list is com-
prehensive - the HPRD database is based on an an-
notation process in which the human annotators re-
port all interactions described in a Medline article.
On the other hand, the fact that only abstracts are
included in the corpus (as opposed to including the
full article) means that the list may contain interac-
tions that are not actually reported in the HPRD cor-
pus. Nevertheless, if the abstracts were annotated
52
with gene mentions and corresponding GIDs, then
a ?quasi-exact? interaction list could be computed
based on the following heuristic:
[H] If two genes with identifiers gid
1
and gid
2
are
mentioned in the same sentence in an abstract with
PubMed identifier pmid, and if gid
1
and gid
2
are
participants in an interaction that is linked to pmid
in HPRD, then consider that the abstract (and con-
sequently the entire HPRD corpus) reports the inter-
action between gid
1
and gid
2
. 
An application of the above heuristic is shown at
the bottom of Table 2. The HPRD record at the
top of the table specifies that the Medline article
with ID 10984498 reports an interaction between the
proteins FATZ (with ID 58529) and gamma-filamin
(with ID 2318). The two protein names are men-
tioned in a sentence in the abstract for 10984498,
therefore, by [H], we consider that the HPRD cor-
pus reports this interaction.
This is very similar to the procedure used in
(Craven, 1999) for creating a ?weakly-labeled?
dataset of subcellular-localization relations. [H] is
a strong heuristic ? it is already known that the full
article reports an interaction between the two genes.
Finding the two genes collocated in the same sen-
tence in the abstract is very likely to be due to the
fact that the abstract discusses their interaction. The
heuristic can be made even more accurate if a pair
of genes is considered as interacting only if they co-
occur in a (predefined) minimum number of sen-
tences in the entire corpus ? with the evaluation
modified accordingly, as described later in Section 6.
5.2 Gene Name Annotation and Normalization
For the annotation of gene names and their normal-
ization, we use a dictionary-based approach similar
to (Cohen, 2005). NCBI1 provides a comprehen-
sive dictionary of human genes, where each gene is
specified by its unique identifier, and qualified with
an official name, a description, synonym names and
one or more protein names, as illustrated in Table 2.
All of these names, including the description, are
considered as potential referential expressions for
the gene entity. Each name string is reduced to a
normal form by: replacing dashes with spaces, intro-
ducing spaces between sequences of letters and se-
1URL: http://www.ncbi.nih.gov
quences of digits, replacing Greek letters with their
Latin counterparts (capitalized), substituting Roman
numerals with Arabic numerals, decapitalizing the
first word if capitalized. All names are further tok-
enized, and checked against a dictionary of close to
100K English nouns. Names that are found in this
dictionary are simply filtered out. We also ignore
all ambiguous names (i.e. names corresponding to
more than one gene identifier). The remaining non-
ambiguous names are added to the final gene dictio-
nary, which is implemented as a trie-like structure in
order to allow a fast lookup of gene IDs based on the
associated normalized sequences of tokens.
Each abstract from the HPRD corpus is tokenized
and segmented in sentences using the OpenNLP2
package. The resulting sentences are then annotated
by traversing them from left to right and finding the
longest token sequences whose normal forms match
entries from the gene dictionary.
6 Experimental Evaluation
The main purpose of the experiments in this section
is to compare the performance of the following four
methods on the task of corpus-level relation extrac-
tion:
1. Sentence-level relation extraction followed by
the application of an aggregation operator that
assembles corpus-level results (SSK.Max).
2. Pointwise Mutual Information (PMI).
3. The integrated model, a product of the two base
models (PMI.SSK.Max).
4. The hypergeometric co-citation method (HG).
7 Experimental Methodology
All abstracts, either from the HPRD corpus, or
from the entire Medline, are annotated using the
dictionary-based approach described in Section 5.2.
The sentence-level extraction is done with the sub-
sequence kernel (SSK) approach from (Bunescu and
Mooney, 2005), which was shown to give good re-
sults on extracting interactions from biomedical ab-
stracts. The subsequence kernel was trained on a
set of 225 Medline abstracts which were manually
2URL: http://opennlp.sourceforge.net
53
annotated with protein names and their interactions.
It is known that PMI gives undue importance to
low frequency events (Dunning, 1993), therefore the
evaluation considers only pairs of genes that occur at
least 5 times in the whole corpus.
When evaluating corpus-level extraction on
HPRD, because the ?quasi-exact? list of interactions
is known, we report the precision-recall (PR) graphs,
where the precision (P) and recall (R) are computed
as follows:
P =
#true interactions extracted
#total interaction extracted
R =
#true interactions extracted
#true interactions
All pairs of proteins are ranked based on each scor-
ing method, and precision recall points are com-
puted by considering the top N pairs, where N
varies from 1 to the total number of pairs.
When evaluating on the entire Medline, we used
the shared protein function benchmark described in
(Ramani et al, 2005). Given the set of interacting
pairs recovered at each recall level, this benchmark
calculates the extent to which interaction partners
in a data set share functional annotation, a measure
previously shown to correlate with the accuracy of
functional genomics data sets (Lee et al, 2004). The
KEGG (Kanehisa et al, 2004) and Gene Ontology
(Ashburner et al, 2000) databases provide specific
pathway and biological process annotations for ap-
proximately 7,500 human genes, assigning human
genes into 155 KEGG pathways (at the lowest level
of KEGG) and 1,356 GO pathways (at level 8 of the
GO biological process annotation).
The scoring scheme for measuring interaction set
accuracy is in the form of a log odds ratio of gene
pairs sharing functional annotations. To evaluate a
data set, a log likelihood ratio (LLR) is calculated as
follows:
LLR = ln
P (DjI)
P (Dj:I)
= ln
P (IjD)P (:I)
P (:IjD)P (I)
(7)
where P (DjI) and P (Dj:I) are the probability
of observing the data D conditioned on the genes
sharing benchmark associations (I) and not sharing
benchmark associations (:I). In its expanded form
(obtained by Bayes theorem), P (IjD) and P (:IjD)
are estimated using the frequencies of interactions
observed in the given data set D between annotated
genes sharing benchmark associations and not shar-
ing associations, respectively, while the priors P (I)
and P (:I) are estimated based on the total frequen-
cies of all benchmark genes sharing the same asso-
ciations and not sharing associations, respectively.
A score of zero indicates interaction partners in the
data set being tested are no more likely than random
to belong to the same pathway or to interact; higher
scores indicate a more accurate data set.
8 Experimental Results
The results for the HPRD corpus-level extraction are
shown in Figure 1. Overall, the integrated model has
a more consistent performance, with a gain in preci-
sion mostly at recall levels past 40%. The SSK.Max
and HG models both exhibit a sudden decrease in
precision at around 5% recall level. While SSK.Max
goes back to a higher precision level, the HG model
begins to recover only late at 70% recall.
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
Pr
ec
isi
on
Recall
PMI.SSK.Max
PMI
SSK.Max
HG
Figure 1: PR curves for corpus-level extraction.
A surprising result in this experiment is the be-
havior of the HG model, which is significantly out-
performed by PMI, and which does only marginally
better than a simple baseline that considers all pairs
to be interacting.
We also compared the two methods on corpus-
level extraction from the entire Medline, using the
shared protein function benchmark. As before, we
considered only protein pairs occurring in the same
54
sentence, with a minimum frequency count of 5. The
resulting 47,436 protein pairs were ranked accord-
ing to their PMI and HG scores, with pairs that are
most likely to be interacting being placed at the top.
For each ranking, the LLR score was computed for
the top N proteins, where N varied in increments of
1,000.
The comparative results for PMI and HG are
shown in Figure 2, together with the scores for three
human curated databases: HPRD, BIND and Reac-
tome. On the top 18,000 protein pairs, PMI outper-
forms HG substantially, after which both converge
to the same value for all the remaining pairs.
 2
 2.25
 2.5
 2.75
 3
 3.25
 3.5
 3.75
 4
 4.25
 4.5
 4.75
 5
 2500  5000  7500  10000 12500 15000 17500 20000 22500 25000
LL
R
Top N pairs
PMI
HG
HPRD
BIND
Reactome
Figure 2: Functional annotation benchmark.
Figure 3 shows a comparison of the four aggre-
gation operators on the same HPRD corpus, which
confirms that, overall, max is most appropriate for
integrating corpus-level results.
9 Future Work
The piece of related work that is closest to the aim of
this paper is the Bayesian approach from (Skounakis
and Craven, 2003). In their probabilistic model, co-
occurrence statistics are taken into account by using
a prior probability that a pair of proteins are inter-
acting, given the number of co-occurrences in the
corpus. However, they do not use the confidences of
the sentence-level extractions. The GeneWays sys-
tem from (Rzhetsky et al, 2004) takes a different
approach, in which co-occurrence frequencies are
simply used to re-rank the ouput from the relation
extractor.
An interesting direction for future research is to
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
Pr
ec
isi
on
Recall
Max
Noisy-Or
Avg
And
Figure 3: PR curves for aggregation operators.
design a model that takes into account both the ex-
traction confidences and the co-occurrence statis-
tics, without losing the probabilistic (or information-
theoretic) interpretation. One could investigate ways
of integrating the two orthogonal approaches to
corpus-level extraction based on other statistical
tests, such as chi-square and log-likelihood ratio.
The sentence-level extractor used in this paper
was trained to recognize relation mentions in iso-
lation. However, the trained model is later used,
through the max aggregation operator, to recognize
whether multiple mentions of the same pair of pro-
teins indicate a relationship between them. This
points to a fundamental mismatch between the train-
ing and testing phases of the model. We expect that
better accuracy can be obtained by designing an ap-
proach that is using information from multiple oc-
currences of the same pair in both training and test-
ing.
10 Conclusion
Extracting relations from a collection of documents
can be approached in two fundamentally different
ways. In one approach, an IE system extracts rela-
tion instances from corpus sentences, and then ag-
gregates the local extractions into corpus-level re-
sults. In the second approach, statistical tests based
on co-occurrence counts are used for deciding if a
given pair of entities are mentioned together more
often than chance would predict. We have described
55
a method to integrate the two approaches, and given
experimental results that confirmed our intuition that
an integrated model would have a better perfor-
mance.
11 Acknowledgements
This work was supported by grants from the N.S.F.
(IIS-0325116, EIA-0219061), N.I.H. (GM06779-
01), Welch (F1515), and a Packard Fellowship
(E.M.M.).
References
M. Ashburner, C. A. Ball, J. A. Blake, D. Botstein, H. Butler,
J. M. Cherry, A. P. Davis, K. Dolinski, S. S. Dwight, and J. T.
et al Eppig. 2000. Gene ontology: tool for the unification
of biology. the gene ontology consortium. Nature Genetics,
25(1):25?29.
G. D. Bader, D. Betel, and C. W. Hogue. 2003. Bind: the
biomolecular interaction network database. Nucleic Acids
Research, 31(1):248?250.
C. Blaschke and A. Valencia. 2001. Can bibliographic pointers
for known biological data be found automatically? protein
interactions as a case study. Comparative and Functional
Genomics, 2:196?206.
Razvan C. Bunescu and Raymond J. Mooney. 2005. Subse-
quence kernels for relation extraction. In Proceedings of the
Conference on Neural Information Processing Systems, Van-
couver, BC.
Kenneth W. Church and Patrick W. Hanks. 1990. Word associ-
ation norms, mutual information and lexicography. Compu-
tational Linguistics, 16(1):22?29.
Aaron M. Cohen. 2005. Unsupervised gene/protein named en-
tity normalization using automatically extracted dictionaries.
In Proceedings of the ACL-ISMB Workshop on Linking Bio-
logical Literature, Ontologies and Databases: Minining Bi-
ological Semantics, pages 17?24, Detroit, MI.
Mark Craven. 1999. Learning to extract relations from MED-
LINE. In Papers from the Sixteenth National Conference
on Artificial Intelligence (AAAI-99) Workshop on Machine
Learning for Information Extraction, pages 25?30, July.
Ted Dunning. 1993. Accurate methods for the statistics
of surprise and coincidence. Computational Linguistics,
19(1):61?74.
T. K. Jenssen, A. Laegreid, J. Komorowski, and E. Hovig. 2001.
A literature network of human genes for high-throughput
analysis of gene expression. Nature Genetics, 28(1):21?28.
G. Joshi-Tope, M. Gillespie, I. Vastrik, P. D?Eustachio,
E. Schmidt, B. de Bono, B. Jassal, G. R. Gopinath, G. R.
Wu, L. Matthews, and et al 2005. Reactome: a knowl-
edgebase of biological pathways. Nucleic Acids Research,
33 Database Issue:D428?432.
M. Kanehisa, S. Goto, S. Kawashima, Y. Okuno, and M. Hat-
tori. 2004. The KEGG resource for deciphering the genome.
Nucleic Acids Research, 32 Database issue:D277?280.
I. Lee, S. V. Date, A. T. Adai, and E. M. Marcotte. 2004. A
probabilistic functional network of yeast genes. Science,
306(5701):1555?1558.
Christopher D. Manning and Hinrich Schu?tze. 1999. Foun-
dations of Statistical Natural Language Processing. MIT
Press, Cambridge, MA.
Judea Pearl. 1986. Fusion, propagation, and structuring in be-
lief networks. Artificial Intelligence, 29(3):241?288.
S. Peri, J. D. Navarro, T. Z. Kristiansen, R. Amanchy, V. Suren-
dranath, B. Muthusamy, T. K. Gandhi, K. N. Chandrika,
N. Deshpande, S. Suresh, and et al 2004. Human protein
reference database as a discovery resource for proteomics.
Nucleic Acids Research, 32 Database issue:D497?501.
A. K. Ramani, R. C. Bunescu, R. J. Mooney, and E. M. Mar-
cotte. 2005. Consolidating the set of know human protein-
protein interactions in preparation for large-scale mapping of
the human interactome. Genome Biology, 6(5):r40.
Soumya Ray and Mark Craven. 2001. Representing sentence
structure in hidden Markov models for information extrac-
tion. In Proceedings of the Seventeenth International Joint
Conference on Artificial Intelligence (IJCAI-2001), pages
1273?1279, Seattle, WA.
A. Rzhetsky, T. Iossifov, I. Koike, M. Krauthammer, P. Kra,
M. Morris, H. Yu, P.A. Duboue, W. Weng, W.J. Wilbur,
V. Hatzivassiloglou, and C. Friedman. 2004. GeneWays: a
system for extracting, analyzing, visualizing, and integrating
molecular pathway data. Journal of Biomedical Informatics,
37:43?53.
Marios Skounakis and Mark Craven. 2003. Evidence combina-
tion in biomedical natural-language processing. In Proceed-
ings of the 3nd ACM SIGKDD Workshop on Data Mining in
Bioinformatics (BIOKDD 2003), pages 25?32, Washington,
DC.
I. Xenarios, L. Salwinski, X. J. Duan, P. Higney, S. M. Kim, and
D. Eisenberg. 2002. DIP, the database of interacting pro-
teins: a research tool for studying cellular networks of pro-
tein interactions. Nucleic Acids Research, 30(1):303?305.
Shubin Zhao and Ralph Grishman. 2005. Extracting relations
with integrated information using kernel methods. In Pro-
ceedings of the 43rd Annual Meeting of the Association for
Computational Linguistics (ACL?05), pages 419?426, Ann
Arbor, Michigan, June. Association for Computational Lin-
guistics.
56
 
		ffProceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 670?679,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Learning with Probabilistic Features for Improved Pipeline Models
Razvan C. Bunescu
School of EECS
Ohio University
Athens, OH 45701
bunescu@ohio.edu
Abstract
We present a novel learning framework for
pipeline models aimed at improving the com-
munication between consecutive stages in a
pipeline. Our method exploits the confidence
scores associated with outputs at any given
stage in a pipeline in order to compute prob-
abilistic features used at other stages down-
stream. We describe a simple method of in-
tegrating probabilistic features into the linear
scoring functions used by state of the art ma-
chine learning algorithms. Experimental eval-
uation on dependency parsing and named en-
tity recognition demonstrate the superiority of
our approach over the baseline pipeline mod-
els, especially when upstream stages in the
pipeline exhibit low accuracy.
1 Introduction
Machine learning algorithms are used extensively
in natural language processing. Applications range
from fundamental language tasks such as part of
speech (POS) tagging or syntactic parsing, to higher
level applications such as information extraction
(IE), semantic role labeling (SRL), or question an-
swering (QA). Learning a model for a particular lan-
guage processing problem often requires the output
from other natural language tasks. Syntactic pars-
ing and dependency parsing usually start with a tex-
tual input that is tokenized, split in sentences and
POS tagged. In information extraction, named en-
tity recognition (NER), coreference resolution, and
relation extraction (RE) have been shown to benefit
from features that use POS tags and syntactic depen-
dencies. Similarly, most SRL approaches assume
a parse tree representation of the input sentences.
The common practice in modeling such dependen-
cies is to use a pipeline organization, in which the
output of one task is fed as input to the next task
in the sequence. One advantage of this model is
that it is very simple to implement; it also allows
for a modular approach to natural language process-
ing. The key disadvantage is that errors propagate
between stages in the pipeline, significantly affect-
ing the quality of the final results. One solution
is to solve the tasks jointly, using the principled
framework of probabilistic graphical models. Sut-
ton et al (2004) use factorial Conditional Random
Fields (CRFs) (Lafferty et al, 2001) to jointly pre-
dict POS tags and segment noun phrases, improving
on the cascaded models that perform the two tasks
in sequence. Wellner et al (2004) describe a CRF
model that integrates the tasks of citation segmen-
tation and citation matching. Their empirical results
show the superiority of the integrated model over the
pipeline approach. While more accurate than their
pipeline analogues, probabilistic graphical models
that jointly solve multiple natural language tasks are
generally more demanding in terms of finding the
right representations, the associated inference algo-
rithms and their computational complexity. Recent
negative results on the integration of syntactic pars-
ing with SRL (Sutton and McCallum, 2005) provide
additional evidence for the difficulty of this general
approach. When dependencies between the tasks
can be formulated in terms of constraints between
their outputs, a simpler approach is to solve the tasks
separately and integrate the constraints in a linear
programming formulation, as proposed by Roth and
670
Yih (2004) for the simultaneous learning of named
entities and relations between them. More recently,
Finkel et al (2006) model the linguistic pipelines
as Bayesian networks on which they perform Monte
Carlo inference in order to find the most likely out-
put for the final stage in the pipeline.
In this paper, we present a new learning method
for pipeline models that mitigates the problem of er-
ror propagation between the tasks. Our method ex-
ploits the probabilities output by any given stage in
the pipeline as weights for the features used at other
stages downstream. We show a simple method of
integrating probabilistic features into linear scoring
functions, which makes our approach applicable to
state of the art machine learning algorithms such as
CRFs and Support Vector Machines (Vapnik, 1998;
Scho?lkopf and Smola, 2002). Experimental results
on dependency parsing and named entity recogni-
tion show useful improvements over the baseline
pipeline models, especially when the basic pipeline
components exhibit low accuracy.
2 Learning with Probabilistic Features
We consider that the task is to learn a mapping from
inputs x ? X to outputs y ? Y(x). Each input
x is also associated with a different set of outputs
z ? Z(x) for which we are given a probabilistic
confidence measure p(z|x). In a pipeline model, z
would correspond to the annotations performed on
the input x by all stages in the pipeline other than
the stage that produces y. For example, in the case
of dependency parsing, x is a sequence of words, y
is a set of word-word dependencies, z is a sequence
of POS tags, and p(z|x) is a measure of the confi-
dence that the POS tagger has in the output z. Let
? be a representation function that maps an exam-
ple (x, y, z) to a feature vector ?(x, y, z) ? Rd, and
w ? Rd a parameter vector. Equations (1) and (2)
below show the traditional method for computing
the optimal output y? in a pipeline model, assuming
a linear scoring function defined by w and ?.
y?(x) = argmax
y?Y(x)
w ? ?(x, y, z?(x)) (1)
z?(x) = argmax
z?Z(x)
p(z|x) (2)
The weight vector w is learned by optimizing a pre-
defined objective function on a training dataset.
In the model above, only the best annotation z?
produced by upstream stages is used for determining
the optimal output y?. However, z? may be an incor-
rect annotation, while the correct annotation may be
ignored because it was assigned a lower confidence
value. We propose exploiting all possible annota-
tions and their probabilities as illustrated in the new
model below:
y?(x) = argmax
y?Y(x)
w ? ?(x, y) (3)
?(x, y) =
?
z?Z(x)
p(z|x) ? ?(x, y, z) (4)
In most cases, directly computing ?(x, y) is unfeasi-
ble, due to a large number of annotations inZ(x). In
our dependency parsing example, Z(x) contains all
possible POS taggings of sentence x; consequently
its cardinality is exponential in the length of the sen-
tence. A more efficient way of computing ?(x, y)
can be designed based on the observation that most
components ?i of the original feature vector ? utilize
only a limited amount of evidence from the example
(x, y, z). We define (x?, y?, z?) ? Fi(x, y, z) to cap-
ture the actual evidence from (x, y, z) that is used by
one instance of feature function ?i. We call (x?, y?, z?)
a feature instance of ?i in the example (x, y, z).
Correspondingly, Fi(x, y, z) is the set of all fea-
ture instances of ?i in example (x, y, z). Usually,
?i(x, y, z) is set to be equal with the number of in-
stances of ?i in example (x, y, z), i.e. ?i(x, y, z) =
|Fi(x, y, z)|. Table 1 illustrates three feature in-
stances (x?, y?, z?) generated by three typical depen-
dency parsing features in the example from Figure 1.
Because the same feature may be instantiated multi-
?1 : DT? NN ?2 : NNS? thought ?3 : be? in
y? 10?11 2?4 7?9
z? DT10 NN11 NNS2
x? thought4 be7 in9
|Fi| O(|x|2) O(|x|) O(1)
Table 1: Feature instances.
ple times in the same example, the components of
each feature instance are annotated with their po-
sitions relative to the example. Given these defi-
nitions, the feature vector ?(x, y) from (4) can be
671
Figure 1: Dependency Parsing Example.
rewritten in a component-wise manner as follows:
?(x, y) = [?1(x, y) . . . ?d(x, y)] (5)
?i(x, y) =
?
z?Z(x)
p(z|x) ? ?i(x, y, z)
=
?
z?Z(x)
p(z|x) ? |Fi(x, y, z)|
=
?
z?Z(x)
p(z|x)
?
(x?,y?,z?)?Fi(x,y,z)
1
=
?
z?Z(x)
?
(x?,y?,z?)?Fi(x,y,z)
p(z|x)
=
?
(x?,y?,z?)?Fi(x,y,Z(x))
?
z?Z(x),z?z?
p(z|x)
where Fi(x, y,Z(x)) stands for:
Fi(x, y,Z(x)) =
?
z?Z(x)
Fi(x, y, z)
We introduce p(z?|x) to denote the expectation:
p(z?|x) =
?
z?Z(x),z?z?
p(z|x)
Then ?i(x, y) can be written compactly as:
?i(x, y) =
?
(x?,y?,z?)?Fi(x,y,Z(x))
p(z?|x) (6)
The total number of terms in (6) is equal with the
number of instantiations of feature ?i in the exam-
ple (x, y) across all possible annotations z ? Z(x),
i.e. |Fi(x, y,Z(x))|. Usually this is significantly
smaller than the exponential number of terms in (4).
The actual number of terms depends on the particu-
lar feature used to generate them, as illustrated in the
last row of Table 1 for the three features used in de-
pendency parsing. The overall time complexity for
calculating ?(x, y) also depends on the time com-
plexity needed to compute the expectations p(z?|x).
When z is a sequence, p(z?|x) can be computed ef-
ficiently using a constrained version of the forward-
backward algorithm (to be described in Section 3).
When z is a tree, p(z?|x) will be computed using a
constrained version of the CYK algorithm (to be de-
scribed in Section 4).
The time complexity can be further reduced if in-
stead of ?(x, y) we use its subcomponent ??(x, y)
that is calculated based only on instances that appear
in the optimal annotation z?:
??(x, y) = [??1(x, y) . . . ??d(x, y)] (7)
??i(x, y) =
?
(x?,y?,z?)?Fi(x,y,z?)
p(z?|x) (8)
The three models are summarized in Table 2 below.
In the next two sections we illustrate their applica-
y?(x) = argmax
y?Y(x)
w ? ?(x, y)
M1 ?(x, y) = ?(x, y, z?(x))
z?(x) = argmax
z?Z(x)
p(z|x)
y?(x) = argmax
y?Y(x)
w ? ?(x, y)
M2 ?(x, y) = [?1(x, y) . . . ?d(x, y)]
?i(x, y) =
?
(x?,y?,z?)?Fi(x,y,Z(x))
p(z?|x)
y?(x) = argmax
y?Y(x)
w ? ??(x, y)
M3 ??(x, y) = [??1(x, y) . . . ??d(x, y)]
??i(x, y) =
?
(x?,y?,z?)?Fi(x,y,z?)
p(z?|x)
Table 2: Three Pipeline Models.
tion to two common tasks in language processing:
dependency parsing and named entity recognition.
3 Dependency Parsing Pipeline
In a traditional dependency parsing pipeline (model
M1 in Table 2), an input sentence x is first aug-
672
mented with a POS tagging z?(x), and then pro-
cessed by a dependency parser in order to obtain
a dependency structure y?(x). To evaluate the new
pipeline models we use MSTPARSER1, a linearly
scored dependency parser developed by McDonald
et al (2005). Following the edge based factorization
method of Eisner (1996), the score of a dependency
tree in the first order version is defined as the sum of
the scores of all edges in the tree. Equivalently, the
feature vector of a dependency tree is defined as the
sum of the feature vectors of all edges in the tree:
M1: ?(x, y) =
?
u?v?y
?(x, u?v, z?(x))
M2: ?(x, y) =
?
u?v?y
?(x, u?v)
M3: ??(x, y) =
?
u?v?y
??(x, u?v)
For each edge u? v ? y, MSTPARSER generates
features based on a set of feature templates that take
into account the words and POS tags at positions u,
v, and their left and right neighbors u?1, v?1. For
example, a particular feature template T used inside
MSTPARSER generates the following POS bigram
features:
?i(x, u?v, z) =
{
1, if ?zu, zv? = ?t1, t2?
0, otherwise
where t1, t2 ? P are the two POS tags associated
with feature index i. By replacing y with u? v in
the feature expressions from Table 2, we obtain the
following formulations:
M1:?i(x, u?v) =
{
1, if ?z?u, z?v?=?t1, t2?
0, otherwise
M2:?i(x, u?v) = p(z?=?t1, t2?|x)
M3: ??i(x, u?v) =
{
p(z?=?t1, t2?|x), if ?z?u, z?v?=?t1, t2?
0, otherwise
where, following the notation from Section 2,
z? = ?zu, zv? is the actual evidence from z that is
used by feature i, and z? is the top scoring annotation
produced by the POS tagger. The implementation in
MSTPARSER corresponds to the traditional pipeline
model M1. Given a method for computing feature
1URL: http://sourceforge.net/projects/mstparser
probabilities p(z? = ?t1, t2?|x), it is straightforward
to modify MSTPARSER to implement models M2
and M3 ? we simply replace the feature vectors ?
with ? and ?? respectively. As mentioned in Sec-
tion 2, the time complexity of computing the fea-
ture vectors ? in model M2 depends on the com-
plexity of the actual evidence z? used by the fea-
tures. For example, the feature template T used
above is based on the POS tags at both ends of a de-
pendency edge, consequently it would generate |P|2
features in model M2 for any given edge u ? v.
There are however feature templates used in MST-
PARSER that are based on the POS tags of up to 4
tokens in the input sentence, which means that for
each edge they would generate |P|4 ? 4.5M fea-
tures. Whether using all these probabilistic features
is computationally feasible or not also depends on
the time complexity of computing the confidence
measure p(z?|x) associated with each feature.
3.1 Probabilistic POS features
The new pipeline models M2 and M3 require an
annotation model that, at a minimum, facilitates
the computation of probabilistic confidence values
for each output. We chose to use linear chain
CRFs (Lafferty et al, 2001) since CRFs can be eas-
ily modified to compute expectations of the type
p(z?|x), as needed by M2 and M3.
The CRF tagger was implemented in MAL-
LET (McCallum, 2002) using the original feature
templates from (Ratnaparkhi, 1996). The model
was trained on sections 2?21 from the English Penn
Treebank (Marcus et al, 1993). When tested on sec-
tion 23, the CRF tagger obtains 96.25% accuracy,
which is competitive with more finely tuned systems
such as Ratnaparkhi?s MaxEnt tagger.
We have also implemented in MALLET a con-
strained version of the forward-backward procedure
that allows computing feature probabilities p(z?|x).
If z? = ?ti1ti2 ...tik? specifies the tags at k positions
in the sentence, then the procedure recomputes the
? parameters for all positions between i1 and ik by
constraining the state transitions to pass through the
specified tags at the k positions. A similar approach
was used by Culotta et al in (2004) in order to asso-
ciate confidence values with sequences of contigu-
ous tokens identified by a CRF model as fields in an
information extraction task. The constrained proce-
673
dure requires (ik ? i1)|P|2 = O(N |P|2) multipli-
cations in an order 1 Markov model, where N is the
length of the sentence. Because MSTPARSER uses
an edge based factorization of the scoring function,
the constrained forward procedure will need to be
run for each feature template, for each pair of tokens
in the input sentence x. If the evidence z? required by
the feature template T constrains the tags at k posi-
tions, then the total time complexity for computing
the probabilistic features p(z?|x) generated by T is:
O(N3|P|k+2)=O(N |P|2) ?O(N2) ?O(|P|k) (9)
As mentioned earlier, some feature templates used
in the dependency parser constrain the POS tags at 4
positions, leading to a O(N3|P|6) time complexity
for a length N sentence. Experimental runs on the
same machine that was used for CRF training show
that such a time complexity is not yet feasible, espe-
cially because of the large size of P (46 POS tags).
In order to speed up the computation of probabilis-
tic features, we made the following two approxima-
tions:
1. Instead of using the constrained forward-
backward procedure, we enforce an indepen-
dence assumption between tags at different po-
sitions and rewrite p(z? = ?ti1ti2 ...tik?|x) as:
p(ti1ti2 ...tik |x) ?
k
?
j=1
p(tij |x)
The marginal probabilities p(tij |x) are easily
computed using the original forward and back-
ward parameters as:
p(tij |x) =
?ij (tij |x)?ij (tij |x)
Z(x)
This approximation eliminates the factor
O(N |P|2) from the time complexity in (9).
2. If any of the marginal probabilities p(tij |x) is
less than a predefined threshold (? |P|)?1, we
set p(z?|x) to 0. When ? ? 1, the method is
guaranteed to consider at least the most proba-
ble state when computing the probabilistic fea-
tures. Looking back at Equation (4), this is
equivalent with summing feature vectors only
over the most probable annotations z ? Z(x).
The approximation effectively replaces the fac-
tor O(|P|k) in (9) with a quasi-constant factor.
The two approximations lead to an overall time com-
plexity of O(N2) for computing the probabilistic
features associated with any feature template T , plus
O(N |P|2) for the unconstrained forward-backward
procedure. We will use M ?2 to refer to the model
M2 that incorporates the two approximations. The
independence assumption from the first approxima-
tion can be relaxed without increasing the asymp-
totic time complexity by considering as independent
only chunks of contiguous POS tags that are at least
a certain number of tokens apart. Consequently,
the probability of the tag sequence will be approxi-
mated with the product of the probabilities of the tag
chunks, where the exact probability of each chunk
is computed in constant time with the constrained
forward-backward procedure. We will use M ??2 to
refer to the resulting model.
3.2 Experimental Results
MSTPARSER was trained on sections 2?21 from the
WSJ Penn Treebank, using the gold standard POS
tagging. The parser was then evaluated on section
23, using the POS tagging output by the CRF tagger.
For model M1 we need only the best output from
the POS tagger. For models M ?2 and M ??2 we com-
pute the probability associated with each feature us-
ing the corresponding approximations, as described
in the previous section. In model M ??2 we consider
as independent only chunks of POS tags that are 4
tokens or more apart. If the distance between the
chunks is less than 4 tokens, the probability for the
entire tag sequence in the feature is computed ex-
actly using the constrained forward-backward pro-
cedure. Table 3 shows the accuracy obtained by
models M1, M ?2(?) and M ??2 (?) for various values
of the threshold parameter ? . The accuracy is com-
M1 M ?2(1) M ?2(2) M ?2(4) M ??2 (4)
88.51 88.66 88.67 88.67 88.70
Table 3: Dependency parsing results.
puted over unlabeled dependencies i.e. the percent-
age of words for which the parser has correctly iden-
tified the parent in the dependency tree. The pipeline
674
Figure 2: Named Entity Recognition Example.
model M ?2 that uses probabilistic features outper-
forms the traditional pipeline model M1. As ex-
pected, M ??2 performs slightly better than M ?2, due
to a more exact computation of feature probabilities.
Overall, only by using the probabilities associated
with the POS features, we achieve an absolute er-
ror reduction of 0.19%, in a context where the POS
stage in the pipeline already has a very high accu-
racy of 96.25%. We expect probabilistic features to
yield a more substantial improvement in cases where
the pipeline model contains less accurate upstream
stages. Such a case is that of NER based on a com-
bination of POS and dependency parsing features.
4 Named Entity Recognition Pipeline
In Named Entity Recognition (NER), the task is to
identify textual mentions of predefined types of en-
tities. Traditionally, NER is modeled as a sequence
classification problem: each token in the input sen-
tence is tagged as being either inside (I) or outside
(O) of an entity mention. Most sequence tagging
approaches use the words and the POS tags in a
limited neighborhood of the current sentence posi-
tion in order to compute the corresponding features.
We augment these flat features with a set of tree
features that are computed based on the words and
POS tags found in the proximity of the current to-
ken in the dependency tree of the sentence. We
argue that such dependency tree features are better
at capturing predicate-argument relationships, espe-
cially when they span long stretches of text. Figure 2
shows a sentence x together with its POS tagging z1,
dependency links z2, and an output tagging y. As-
suming the task is to recognize mentions of people,
the word sailors needs to be tagged as inside. If we
extracted only flat features using a symmetric win-
dow of size 3, the relationship between sailors and
thought would be missed. This relationship is use-
ful, since an agent of the predicate thought is likely
to be a person entity. On the other hand, the nodes
sailors and thought are adjacent in the dependency
tree of the sentence. Therefore, their relationship
can be easily captured as a dependency tree feature
using the same window size.
For every token position, we generate flat features
by considering all unigrams, bigrams and trigrams
that start with the current token and extend either to
the left or to the right. Similarly, we generate tree
features by considering all unigrams, bigrams and
trigrams that start with the current token and extend
in any direction in the undirected version of the de-
pendency tree. The tree features are also augmented
with the actual direction of the dependency arcs be-
tween the tokens. If we use only words to create
n-gram features, the token sailors will be associated
with the following features:
? Flat: sailors, the sailors, ?S? the sailors,
sailors mistakenly, sailors mistakenly thought.
? Tree: sailors, sailors ? the, sailors ?
thought, sailors? thought? must, sailors?
thought? mistakenly.
We also allow n-grams to use word classes such as
POS tags and any of the following five categories:
?1C? for tokens consisting of one capital letter, ?AC?
for tokens containing only capital letters, ?FC? for
tokens that start with a capital letter, followed by
small letters, ?CD? for tokens containing at least one
digit, and ?CRT? for the current token.
The set of features can then be defined as a Carte-
sian product over word classes, as illustrated in Fig-
ure 3 for the original tree feature sailors? thought
? mistakenly. In this case, instead of one com-
pletely lexicalized feature, the model will consider
12 different features such as sailors? VBD? RB,
NNS? thought? RB, or NNS? VBD? RB.
675
??
?CRT?
NNS
sailors
?
??[?]?
[
VBD
thought
]
?[?]?
[
RB
mistakenly
]
Figure 3: Dependency tree features.
The pipeline model M2 uses features that appear
in all possible annotations z = ?z1, z2?, where z1
and z2 are the POS tagging and the dependency
parse respectively. If the corresponding evidence is
z? = ?z?1, z?2?, then:
p(z?|x) = p(z?2|z?1, x)p(z?1|x)
For example, NNS2 ? thought4 ? RB3 is a feature
instance for the token sailors in the annotations from
Figure 2. This can be construed as having been gen-
erated by a feature template T that outputs the POS
tag ti at the current position, the word xj that is the
parent of xi in the dependency tree, and the POS tag
tk of another dependent of xj (i.e. ti ? xj ? tk).
The probability p(z?|x) for this type of features can
then be written as:
p(z?|x) = p(i?j?k|ti, tk, x) ? p(ti, tk|x)
The two probability factors can be computed exactly
as follows:
1. The M2 model for dependency parsing from
Section 3 is used to compute the probabilistic
features ?(x, u? v|ti, tk) by constraining the
POS annotations to pass through tags ti and tk
at positions i and k. The total time complexity
for this step is O(N3|P|k+2).
2. Having access to ?(x, u? v|ti, tk), the factor
p(i?j?k|ti, tk, x) can be computed in O(N3)
time using a constrained version of Eisner?s al-
gorithm, as will be explained in Section 4.1.
3. As described in Section 3.1, computing the
expectation p(ti, tk|x) takes O(N |P2|) time
using the constrained forward-backward algo-
rithm.
The current token position i can have a total of N
values, while j and k can be any positions other
than i. Also, ti and tk can be any POS tag from
P . Consequently, the feature template T induces
O(N3|P|2) feature instances. Overall, the time
complexity for computing the feature instances gen-
erated by T is O(N6|P|k+4), as results from:
O(N3|P|2) ? (O(N3|P|k+2) +O(N3) +O(N |P|2))
While still polynomial, this time complexity is fea-
sible only for small values ofN . In general, the time
complexity for computing probabilistic features in
the full model M2 increases with both the number
of stages in the pipeline and the complexity of the
features.
Motivated by efficiency, we decided to use the
pipeline model M3 in which probabilities are com-
puted only over features that appear in the top scor-
ing annotation z? = ?z?1, z?2?, where z?1 and z?2 repre-
sent the best POS tagging, and the best dependency
parse respectively. In order to further speed up the
computation of probabilistic features, we made the
following approximations:
1. We consider the POS tagging and the depen-
dency parse independent and rewrite p(z?|x) as:
p(z?|x) = p(z?1, z?2|x) ? p(z?1|x)p(z?2|x)
2. We enforce an independence assumption be-
tween POS tags. Thus, if z?1 = ?ti1ti2 ...tik?
specifies the tags at k positions in the sentence,
then p(z?1|x) is rewritten as:
p(ti1ti2 ...tik |x) ?
k
?
j=1
p(tij |x)
3. We also enforce a similar independence as-
sumption between dependency links. Thus, if
z?2 = ?u1 ? v1...uk ? vk? specifies k depen-
dency links, then p(z?2|x) is rewritten as:
p(u1?v1...uk?vk|x) ?
k
?
l=1
p(ul?vl|x)
For example, the probability p(z?|x) of the feature
instance NNS2 ? thought4 ? RB3 is approximated
as:
p(z?|x) ? p(z?1|x) ? p(z?2|x)
p(z?1|x) ? p(t2 =NNS|x) ? p(t3 =RB|x)
p(z?2|x) ? p(2?4|x) ? p(3?4|x)
We will use M ?3 to refer to the resulting model.
676
4.1 Probabilistic Dependency Features
The probabilistic POS features p(ti|x) are computed
using the forward-backward procedure in CRFs, as
described in Section 3.1. To completely specify the
pipeline model for NER, we also need an efficient
method for computing the probabilistic dependency
features p(u? v|x), where u? v is a dependency
edge between positions u and v in the sentence x.
MSTPARSER is a large-margin method that com-
putes an unbounded score s(x, y) for any given sen-
tence x and dependency structure y ? Y(x) using
the following edge-based factorization:
s(x, y) =
?
u?v?y
s(x, u?v) = w
?
u?v?y
?(x, u?v)
The following three steps describe a general method
for associating probabilities with output substruc-
tures. The method can be applied whenever a struc-
tured output is associated a score value that is un-
bounded in R, assuming that the score of the entire
output structure can be computed efficiently based
on a factorization into smaller substructures.
S1. Map the unbounded score s(x, y) from R
into [0, 1] using the softmax function (Bishop, 1995):
n(x, y) = e
s(x,y)
?
y?Y(x) es(x,y)
The normalized score n(x, y) preserves the ranking
given by the original score s(x, y). The normaliza-
tion constant at the denominator can be computed in
O(N3) time by replacing the max operator with the
sum operator inside Eisner?s chart parsing algorithm.
S2. Compute a normalized score for the sub-
structure by summing up the normalized scores of
all the complete structures that contain it. In our
model, dependency edges are substructures, while
dependency trees are complete structures. The nor-
malized score will then be computed as:
n(x, u?v) =
?
y?Y(x),u?v?y
n(x, y)
The sum can be computed in O(N3) time using a
constrained version of the algorithm that computes
the normalization constant in step S1. This con-
strained version of Eisner?s algorithm works in a
similar manner with the constrained forward back-
ward algorithm by restricting the dependency struc-
tures to contain a predefined edge or set of edges.
S3. Use the isotonic regression method of
Zadrozny and Elkan (2002) to map the normalized
scores n(x, u? v) into probabilities p(u? v|x). A
potential problem with the softmax function is that,
depending on the distribution of scores, the expo-
nential transform could dramatically overinflate the
higher scores. Isotonic regression, by redistributing
the normalized scores inside [0, 1], can alleviate this
problem.
4.2 Experimental Results
We test the pipeline model M ?3 versus the traditional
model M1 on the task of detecting mentions of per-
son entities in the ACE dataset2. We use the standard
training ? testing split of the ACE 2002 dataset in
which the training dataset is also augmented with the
documents from the ACE 2003 dataset. The com-
bined dataset contains 674 documents for training
and 97 for testing. We implemented the CRF model
in MALLET using three different sets of features:
Tree, Flat, and Full corresponding to the union of
all flat and tree features. The POS tagger and the de-
pendency parser were trained on sections 2-21 of the
Penn Treebank, followed by an isotonic regression
step on section 23 for the dependency parser. We
compute precision recall (PR) graphs by varying a
threshold on the token level confidence output by the
CRF tagger, and summarize the tagger performance
using the area under the curve. Table 4 shows the re-
sults obtained by the two models under the three fea-
ture settings. The model based on probabilistic fea-
Model Tree Flat Full
M ?3 76.78 77.02 77.96
M1 74.38 76.53 77.02
Table 4: Mention detection results.
tures consistently outperforms the traditional model,
especially when only tree features are used. Depen-
dency parsing is significantly less accurate than POS
tagging. Consequently, the improvement for the tree
based model is more substantial than for the flat
2URL: http://www.nist.gov/speech/tests/ace
677
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
Pr
ec
isi
on
Recall
Probabilistic
Traditional
Figure 4: PR graphs for tree features.
model, confirming our expectation that probabilis-
tic features are more useful when upstream stages in
the pipeline are less accurate. Figure 4 shows the PR
curves obtained for the tree-based models, on which
we see a significant 5% improvement in precision
over a wide range of recall values.
5 Related Work
In terms of the target task ? improving the perfor-
mance of linguistic pipelines ? our research is most
related to the work of Finkel et al (2006). In their
approach, output samples are drawn at each stage
in the pipeline conditioned on the samples drawn
at previous stages, and the final output is deter-
mined by a majority vote over the samples from
the final stage. The method needs very few sam-
ples for tasks such as textual entailment, where the
final outcome is binary, in agreement with a theo-
retical result on the rate of convergence of the vot-
ing Gibbs classifier due to Ng and Jordan (2001).
While their sampling method is inherently approx-
imate, our full pipeline model M2 is exact in the
sense that feature expectations are computed exactly
in polynomial time whenever the inference step at
each stage can be done in polynomial time, irrespec-
tive of the cardinality of the final output space. Also,
the pipeline models M2 and M3 and their more effi-
cient alternatives propagate uncertainty during both
training and testing through the vector of probabilis-
tic features, whereas the sampling method takes ad-
vantage of the probabilistic nature of the outputs
only during testing. Overall, the two approaches
can be seen as complementary. In order to be ap-
plicable with minimal engineering effort, the sam-
pling method needs NLP researchers to write pack-
ages that can generate samples from the posterior.
Similarly, the new pipeline models could be easily
applied in a diverse range of applications, assum-
ing researchers develop packages that can efficiently
compute marginals over output substructures.
6 Conclusions and Future Work
We have presented a new, general method for im-
proving the communication between consecutive
stages in pipeline models. The method relies on
the computation of probabilities for count features,
which translates in adding a polynomial factor to the
overall time complexity of the pipeline whenever the
inference step at each stage is done in polynomial
time, which is the case for the vast majority of infer-
ence algorithms used in practical NLP applications.
We have also shown that additional independence
assumptions can make the approach more practical
by significantly reducing the time complexity. Ex-
isting learning based models can implement the new
method by replacing the original feature vector with
a more dense vector of probabilistic features3. It is
essential that every stage in the pipeline produces
probabilistic features, and to this end we have de-
scribed an effective method for associating proba-
bilities with output substructures.
We have shown for NER that simply using the
probabilities associated with features that appear
only in the top annotation can lead to useful im-
provements in performance, with minimal engineer-
ing effort. In future work we plan to empirically
evaluate NER with an approximate version of the
full model M2 which, while more demanding in
terms of time complexity, could lead to even more
significant gains in accuracy. We also intend to com-
prehensively evaluate the proposed scheme for com-
puting probabilities by experimenting with alterna-
tive normalization functions.
Acknowledgements
We would like to thank Rada Mihalcea and the
anonymous reviewers for their insightful comments
and suggestions.
3The Java source code will be released on my web page.
678
References
Christopher M. Bishop. 1995. Neural Networks for Pat-
tern Recogntion. Oxford University Press.
Aron Culotta and Andrew McCallum. 2004. Confidence
estimation for information extraction. In Proceed-
ings of Human Language Technology Conference and
North American Chapter of the Association for Com-
putational Linguistics (HLT-NAACL), Boston, MA.
Jason M. Eisner. 1996. Three new probabilistic models
for dependency parsing: An exploration. In Proceed-
ings of the 16th Conference on Computational linguis-
tics, pages 340?345, Copenhagen, Denmark.
Jenny R. Finkel, Christopher D. Manning, and Andrew Y.
Ng. 2006. Solving the problem of cascading errors:
Approximate Bayesian inference for linguistic annota-
tion pipelines. In Proceedings of the 2006 Conference
on Empirical Methods in Natural Language Process-
ing, pages 618?626, Sydney, Australia.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic mod-
els for segmenting and labeling sequence data. In
Proceedings of 18th International Conference on
Machine Learning (ICML-2001), pages 282?289,
Williamstown, MA.
M. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn treebank. Computational Linguistics,
19(2):313?330.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In Proceedings of the 43rd Annual Meeting
on Association for Computational Linguistics (ACL-
05), pages 91?98, Ann Arbor, Michigan.
Andrew Y. Ng and Michael I. Jordan. 2001. Conver-
gence rates of the Voting Gibbs classifier, with appli-
cation to bayesian feature selection. In Proceedings of
18th International Conference on Machine Learning
(ICML-2001), pages 377?384, Williamstown, MA.
Adwait Ratnaparkhi. 1996. A maximum entropy model
for part of speech tagging. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP-96), pages 133?141, Philadel-
phia, PA.
D. Roth and W. Yih. 2004. A linear programming formu-
lation for global inference in natural language tasks.
In Proceedings of the Eighth Conference on Compu-
tational Natural Language Learning (CoNLL-2004),
pages 1?8, Boston, MA.
Bernhard Scho?lkopf and Alexander J. Smola. 2002.
Learning with kernels - support vector machines, regu-
larization, optimization and beyond. MIT Press, Cam-
bridge, MA.
Charles Sutton and Andrew McCallum. 2005. Joint pars-
ing and semantic role labeling. In CoNLL-05 Shared
Task.
Charles Sutton, Khashayar Rohanimanesh, and Andrew
McCallum. 2004. Dynamic conditional random
fields: Factorized probabilistic models for labeling and
segmenting sequence data. In Proceedings of 21st In-
ternational Conference on Machine Learning (ICML-
2004), pages 783?790, Banff, Canada, July.
Vladimir N. Vapnik. 1998. Statistical Learning Theory.
John Wiley & Sons.
Ben Wellner, Andrew McCallum, Fuchun Peng, and
Michael Hay. 2004. An integrated, conditional model
of information extraction and coreference with appli-
cation to citation matching. In Proceedings of 20th
Conference on Uncertainty in Artificial Intelligence
(UAI-2004), Banff, Canada, July.
Bianca Zadrozny and Charles Elkan. 2002. Trans-
forming classifier scores into accurate multiclass prob-
ability estimates. In Proceedings of the Eighth
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining (KDD-2002), Ed-
monton, Alberta.
679
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 97?107,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Learning the Relative Usefulness of Questions in Community QA
Razvan Bunescu
School of EECS
Ohio University
Athens, OH 43201, USA
bunescu@ohio.edu
Yunfeng Huang
School of EECS
Ohio University
Athens, OH 43201, USA
yh324906@ohio.edu
Abstract
We present a machine learning approach for
the task of ranking previously answered ques-
tions in a question repository with respect to
their relevance to a new, unanswered refer-
ence question. The ranking model is trained
on a collection of question groups manually
annotated with a partial order relation reflect-
ing the relative utility of questions inside each
group. Based on a set of meaning and struc-
ture aware features, the new ranking model is
able to substantially outperformmore straight-
forward, unsupervised similarity measures.
1 Introduction
Open domain Question Answering (QA) is one of
the most complex and challenging tasks in natural
language processing. In general, a question answer-
ing system may need to integrate knowledge coming
from a wide variety of linguistic processing tasks
such as syntactic parsing, semantic role labeling,
named entity recognition, and anaphora resolution
(Prager, 2006). State of the art implementations of
these linguistic analysis tasks are still limited in their
performance, with errors that compound and prop-
agate into the final performance of the QA system
(Moldovan et al, 2002). Consequently, the perfor-
mance of open domain QA systems has yet to ar-
rive at a level at which it would become a feasible
alternative to the current paradigms for information
access based on keyword searches.
Recently, community-driven QA sites such as Ya-
hoo! Answers and WikiAnswers 1 have established
1answers.yahoo.com, wiki.answers.com
a new approach to question answering that shifts
the inherent complexity of open domain QA from
the computer system to volunteer contributors. The
computer is no longer required to perform a deep
linguistic analysis of questions and generate corre-
sponding answers, and instead acts as a mediator
between users submitting questions and volunteers
providing the answers.
An important objective in community QA is to
minimize the time elapsed between the submission
of questions by users and the subsequent posting
of answers by volunteer contributors. One useful
strategy for minimizing the response latency is to
search the QA repository for similar questions that
have already been answered, and provide the cor-
responding ranked list of answers, if such a ques-
tion is found. The success of this approach de-
pends on the definition and implementation of the
question-to-question similarity function. In the sim-
plest solution, the system searches for previously
answered questions based on exact string match-
ing with the reference question. Alternatively, sites
such as WikiAnswers allow the users to mark ques-
tions they think are rephrasings (?alternate word-
ings?, or paraphrases) of existing questions. These
question clusters are then taken into account when
performing exact string matching, therefore increas-
ing the likelihood of finding previously answered
questions that are semantically equivalent to the ref-
erence question.
In order to lessen the amount of work required
from the contributors, an alternative approach is to
build a system that automatically finds rephrasings
of questions, especially since question rephrasing
97
seems to be computationally less demanding than
question answering. According to previous work in
this domain, a question is considered a rephrasing of
a reference question Q0 if it uses an alternate word-
ing to express an identical information need. For
example, Q0 and Q1 below are rephrasings of each
other, and consequently they are expected to have
the same answer.
Q0 What should I feed my turtle?
Q1 What do I feed my pet turtle?
Paraphrasings of a new question cannot always be
found in the community QA repository. We believe
that computing a ranked list of existing questions
that at least partially address the original informa-
tion need could also be useful to the user, at least
until other users volunteer to give an exact answer
to the original, unanswered reference question. For
example, in the absence of any additional informa-
tion about the reference question Q0, the expected
answers to questions Q2 and Q3 below may be seen
as partially overlapping in information content with
the expected answer for the reference question Q0.
An answer to question Q4, on the other hand, is less
likely to benefit the user, even though it has a signif-
icant lexical overlap with the reference question.
Q2 What kind of fish should I feed my turtle?
Q3 What do you feed a turtle that is the size of a
quarter?
Q4 What kind of food should I feed a turtle dove?
In this paper, we propose a supervised learning
approach to the question ranking problem, a gen-
eralization of the question paraphrasing problem in
which questions are ranked in a partial order based
on the relative information overlap between their ex-
pected answers and the expected answer of the refer-
ence question. Underlying the question ranking task
is the expectation that the user who submits a ref-
erence question will find the answers of the highly
ranked questions to be more useful than the answers
associated with the lower ranked questions. For the
reference question Q0 above, the learned ranking
model is expected to produce a partial order in which
Q1 is ranked higher than Q2, Q3 and Q4, whereas
Q2 and Q3 are ranked higher than Q4.
2 Partially Ordered Datasets for Question
Ranking
In order to enable the evaluation of question rank-
ing approaches, we have previously created a dataset
of 60 groups of questions (Bunescu and Huang,
2010b). Each group consists of a reference question
(e.g. Q0 above) that is associated with a partially or-
dered set of questions (e.g. Q1 to Q4 above). For
each reference questions, its corresponding partially
ordered set is created from questions in Yahoo! An-
swers and other online repositories that have a high
cosine similarity with the reference question. Out
of the 26 top categories in Yahoo! Answers, the 60
reference questions span a diverse set of categories.
Figure 1 lists the 20 categories covered, where each
category is shown with the number of corresponding
reference questions between parentheses.
Travel (10), Computers & Internet (6),
Beauty & Style (5), Entertainment &
Music (5), Food & Drink (5), Health (5),
Arts & Humanities (3), Cars &
Transportation (3), Consumer Electronics
(3), Pets (3), Family & Relationships
(2), Science & Mathematics (2),
Education & Reference (1), Environment
(1), Local Businesses (1), Pregnancy &
Parenting (1), Society & Culture (1),
Sports (1), Yahoo! Products (1)
Figure 1: The 20 categories represented in the dataset.
Inside each group, the questions are manually an-
notated with a partial order relation, according to
their utility with respect to the reference question.
We use the notation ?Qi ? Qj |Qr? to encode the
fact that questionQi is more useful than questionQj
with respect to the reference question Qr. Similarly,
?Qi = Qj?will be used to express the fact that ques-
tions Qi and Qj are reformulations of each other
(the reformulation relation is independent of the ref-
erence question). The partial ordering among the
questionsQ0 toQ4 above can therefore be expressed
concisely as follows: ?Q0 = Q1?, ?Q1 ? Q2|Q0?,
?Q1 ? Q3|Q0?, ?Q2 ? Q4|Q0?, ?Q3 ? Q4|Q0?.
Note that we do not explicitly annotate the rela-
tion ?Q1 ? Q4|Q0?, since it can be inferred based
on the transitivity of the more useful than relation:
?Q1 ? Q2|Q0???Q2 ? Q4|Q0? ? ?Q1 ? Q4|Q0?.
98
REFERENCE QUESTION (Qr)
Q5 What?s a nice summer camp to go to in Florida?
PARAPHRASING QUESTIONS (P )
Q6 What camps are good for a vacation during the summer in FL?
Q7 What summer camps in FL do you recommend?
USEFUL QUESTIONS (U )
Q8 Does anyone know a good art summer camp to go to in FL?
Q9 Are there any good artsy camps for girls in FL?
Q10 What are some summer camps for like singing in Florida?
Q11 What is a good cooking summer camp in FL?
Q12 Do you know of any summer camps in Tampa, FL?
Q13 What is a good summer camp in Sarasota FL for a 12 year old?
Q14 Can you please help me find a surfing summer camp for beginners in Treasure Coast, FL?
Q15 Are there any acting summer camps and/or workshops in the Orlando, FL area?
Q16 Does anyone know any volleyball camps in Miramar, FL?
Q17 Does anyone know about any cool science camps in Miami?
Q18 What?s a good summer camp you?ve ever been to?
NEUTRAL QUESTIONS (N )
Q19 What?s a good summer camp in Canada?
Q20 What?s the summer like in Florida?
Table 1: A question group.
Also note that no relation is specified between Q2
and Q3, and similarly no relation can be inferred be-
tween these two questions. This reflects our belief
that, in the absence of any additional information re-
garding the user or the ?turtle? referenced in Q0, we
cannot compare questions Q2 and Q3 in terms of
their usefulness with respect to Q0.
Table 1 shows another reference questionQ5 from
our dataset, together with its annotated group of
questions Q6 to Q20. In order to make the anno-
tation process easier and reproducible, we have di-
vided it into two levels of annotation. During the
first annotation stage, each question group is parti-
tioned manually into 3 subgroups of questions:
? P is the set of paraphrasing questions.
? U is the set of useful questions.
? N is the set of neutral questions.
A question is deemed useful if its expected answer
may overlap in information content with the ex-
pected answer of the reference question. The ex-
pected answer of a neutral question, on the other
hand, should be irrelevant with respect to the ref-
erence question. Let Qr be the reference question,
Qp ? P a paraphrasing question, Qu ? U a useful
question, and Qn ? N a neutral question. Then the
following relations are assumed to hold among these
questions:
1. ?Qp ? Qu|Qr?: a paraphrasing question is
more useful than a useful question.
2. ?Qu ? Qn|Qr?: a useful question is more use-
ful than a neutral question.
Note that as long as these relations hold between
the 3 types of questions, the names of the sub-
groups and their definitions are irrelevant with re-
spect to the implied set of more useful than rela-
tions, since only the implied ternary relations will
be used for training and evaluating question rank-
ing approaches. We also assume that, by tran-
sitivity, the following ternary relations also hold:
?Qp ? Qn|Qr?, i.e. a paraphrasing question is
more useful than a neutral question. Furthermore, if
Qp1 , Qp2 ? P are two paraphrasing questions, this
implies ?Qp1 = Qp2 |Qr?.
99
For the vast majority of questions, the first annota-
tion stage is straightforward and non-controversial.
In the second annotation stage, we perform a finer
annotation of relations between questions in the
middle group U . Table 1 shows two such relations
(using indentation): ?Q8 ? Q9|Q5? and ?Q8 ?
Q10|Q5?. Question Q8 would have been a rephras-
ing of the reference question, were it not for the
noun ?art? modifying the focus noun phrase ?sum-
mer camp?. Therefore, the information content of
the answer to Q8 is strictly subsumed in the infor-
mation content associated with the answer to Q5.
Similarly, inQ9 the focus noun phrase is further spe-
cialized through the prepositional phrase ?for girls?.
Therefore, (an answer to) Q9 is less useful to Q5
than (an answer to) Q8, i.e. ?Q8 ? Q9|Q5?. Fur-
thermore, the focus ?art summer camp? in Q8 con-
ceptually subsumes the focus ?summer camps for
singing? in Q10, therefore ?Q8 ? Q10|Q5?.
We call this dataset simple since most of the ref-
erence questions are shorter than the other questions
in their group. We have also created a complex ver-
sion of the same dataset, by selecting as the refer-
ence question in each group a longer question from
the same group. For example, ifQ0 were a reference
question, it would be replaced with a more complex
question, such as Q2, or Q3. The annotation is re-
done to reflect the relative usefulness relations with
respect to the new reference questions. We believe
that the new complex dataset is closer to the actual
distribution of questions in community QA reposi-
tories: unanswered questions tend to be more spe-
cific (longer), whereas general questions (shorter)
are more likely to have been answered already. Each
dataset is annotated by two annotators, leading to
a total of 4 datasets: Simple1, Simple2, Complex1,
and Complex2.
Table 2 presents the following statistics on the two
types of datasets (Simple, Complex) for each anno-
tator (1, 2): the total number of paraphrasings (P),
the total number of useful questions (U), the total
number of neutral questions (N ), the total number
of more useful than ordered pairs encoded in the
dataset, either explicitly or through transitivity, and
the Inter-Annotator Agreement (ITA). We compute
the ITA as the precision (P) and recall (R) with re-
spect to the more useful than ordered pairs encoded
in one annotation (Pairs1) relative to the ordered
Dataset P U N Pairs ITA
Simple1 164 775 594 11015 P: 76.6
Simple2 134 778 621 10436 R: 81.6
Complex1 103 766 664 10654 P: 71.3
Complex2 89 730 714 9979 R: 81.3
Table 2: Dataset statistics.
pairs encoded in the other annotation (Pairs2).
P = |Pairs1 ? Pairs2|
Pairs1
R = |Pairs1 ? Pairs2|
Pairs2
The statistics in Table 2 indicate that the second
annotator was in general more conservative in tag-
ging questions as paraphrases or useful questions.
3 Unsupervised Methods for Question
Ranking
An ideal question ranking method would take an ar-
bitrary triplet of questions Qr, Qi and Qj as input,
and output an ordering between Qi and Qj with re-
spect to the reference question Qr, i.e. one of ?Qi ?
Qj |Qr?, ?Qi = Qj |Qr?, or ?Qj ? Qi|Qr?. One ap-
proach is to design a usefulness function u(Qi, Qr)
that measures how useful question Qi is for the ref-
erence question Qr, and define the more useful than
(?) relation as follows:
?Qi ? Qj |Qr? ? u(Qi, Qr) > u(Qj , Qr)
If we define I(Q) to be the information need asso-
ciated with question Q, then u(Qi, Qr) could be de-
fined as a measure of the relative overlap between
I(Qi) and I(Qr). Unfortunately, the information
need is a concept that, in general, is defined only
intensionally and therefore it is difficult to measure.
For lack of an operational definition of the informa-
tion need, we will approximate u(Qi, Qr) directly
as a measure of the similarity between Qi and Qr.
The similarity between two questions can be seen as
a special case of text-to-text similarity, consequently
one possibility is to use a general text-to-text simi-
larity function such as cosine similarity in the vector
space model (Baeza-Yates and Ribeiro-Neto, 1999):
cos(Qi, Qr) =
QTi Qr
?Qi??Qr?
100
Here, Qi and Qr denote the corresponding tf?idf
vectors.
As a measure of question similarity, one major
drawback of cosine similarity is that it is oblivious
of the meanings of words in each question. This par-
ticular problem is illustrated by the three questions
below. Q22 and Q23 have the same cosine similar-
ity with Q21, they are therefore indistinguishable in
terms of their usefulness to the reference question
Q21, even though we expect Q22 to be more use-
ful than Q23 (a place that sells hydrangea often sells
other types of plants too, possibly including cacti).
Q21 Where can I buy a hydrangea?
Q22 Where can I buy a cactus?
Q23 Where can I buy an iPad?
To alleviate the lexical chasm, we can redefine
u(Qi, Qr) to be the similarity measure proposed by
(Mihalcea et al, 2006) as follows:
mcs(Qi, Qr) =
?
w?{Qi}
maxSim(w,Qr) ? idf(w)
?
w?{Qi}
idf(w)
+
?
w?{Qr}
maxSim(w,Qi) ? idf(w)
?
w?{Qr}
idf(w)
Since scaling factors are immaterial for ranking, we
have ignored the normalization constant contained
in the original measure. For each word w ? Qi,
maxSim(w,Qr) computes the maximum semantic
similarity between w and any word wr ? Qr. The
similarity scores are weighted by the correspond-
ing idf?s, and normalized. A similar score is com-
puted for each word w ? Qr. The score computed
by maxSim depends on the actual function used
to compute the word-to-word semantic similarity.
In this paper, we evaluated four of the knowledge-
based measures explored in (Mihalcea et al, 2006):
wup (Wu and Palmer, 1994), res (Resnik, 1995), lin
(Lin, 1998), and jcn (Jiang and Conrath, 1997).
4 Supervised Learning for Question
Ranking
Cosine similarity, henceforth referred as cos, treats
questions as bags-of-words. The meta-measure pro-
posed in (Mihalcea et al, 2006), henceforth called
mcs, treats questions as bags-of-concepts. Both cos
and mcs ignore the syntactic relations between the
words in a question, and therefore may miss impor-
tant structural information. In the next three sec-
tions we describe a set of structural features that we
believe are relevant for judging question similarity.
These and other types of features will be integrated
in an SVM model for ranking, as described later in
Section 4.4.
4.1 Matching the Focus Words
If we consider the question Q24 below as reference,
question Q26 will be deemed more useful than Q25
when using cos or mcs because of the higher rela-
tive lexical and conceptual overlap with Q24. How-
ever, this is contrary to the actual ordering ?Q25 ?
Q26|Q24?, which reflects the fact that Q25, which
expects the same answer type as Q24, should be
deemed more useful than Q26, which has a differ-
ent answer type.
Q24 What are some good thriller movies?
Q25 What are some thriller movies with happy end-
ing?
Q26 What are some good songs from a thriller
movie?
The analysis above shows the importance of us-
ing the answer type when computing the similar-
ity between two questions. However, instead of re-
lying exclusively on a predefined hierarchy of an-
swer types, we identify the question focus of a ques-
tion, defined as the set of maximal noun phrases in
the question that corefer with the expected answer
(Bunescu and Huang, 2010a). Focus nouns such as
movies and songs provide more discriminative in-
formation than general answer types such as prod-
ucts. We use answer types only for questions such
as Q27 or Q28 below that lack an explicit question
focus. In such cases, an artificial question focus is
created from the answer type (e.g. location for Q27,
or method for Q28).
101
Q27 Where can I buy a good coffee maker?
Q28 How do I make a pizza?
Let fi and fr be the focus words corresponding to
questions Qi and Qr. We introduce a focus feature
?f , and set its value to be equal with the similarity
between the focus words:
?f (Qi, Qr) = wsim(fi, fr) (1)
We use wsim to denote a generic word meaning sim-
ilarity measure (e.g. wup, res, lin or jcn). When
computing the focus feature, the non-focus word
?movie? in Q26 will not be compared with the fo-
cus word ?movies? in Q24, and therefore Q26 will
have a lower value for this feature than Q25, i.e.
?f (Q26, Q24) < ?f (Q25, Q24).
4.2 Matching the Main Verbs
In addition to the question focus, the main verb of
a question can also provide key information in es-
timating question-to-question similarity. We define
the main verb to be the content verb that is highest
in the dependency tree of the question, e.g. buy for
Q27, or make for Q28. If the question does not con-
tain a content verb, the main verb is defined to be the
highest verb in the dependency tree, as for example
are in Q24 to Q26. The utility of a question?s main
verb in judging its similarity to other questions can
be seen more clearly in the questions below, where
Q29 is the reference:
Q29 How can I transfer music from iTunes to my
iPod?
Q30 How can I upload music to my iPod?
Q31 How can I play music in iTunes?
The fact that upload, as the main verb of Q30, is
more semantically related to transfer is essential in
deciding that ?Q30 ? Q31|Q29?, i.e. Q30 is more
useful than Q31 to Q29.
Let vi and vr be the main verbs corresponding to
questions Qi and Qr. We introduce a main verb fea-
ture ?v as follows:
?v(Qi, Qr) = wsim(vi, vr) (2)
If Q29 is considered as reference question, it is ex-
pected that the main verb feature for question Q30
will have a higher value than the main verb feature
for Q31, i.e. ?f (Q31, Q29) < ?f (Q30, Q29).
Figure 2: Matched dependency trees.
4.3 Matching the Dependency Trees
The question focus and the main verb are only two
of the nodes in the syntactic dependency tree of a
question. In general, all the words in a question are
important when judging its semantic similarity with
another question. We therefore propose a more gen-
eral feature that exploits the dependency structure
of the question and, in doing so, it also considers
all the words in the question, like cos and mcs. For
any given question we initially ignore the direction
of the dependency arcs and change the question de-
pendency tree to be rooted at the focus word, as il-
lustrated in Figure 2 for questions Q5 and Q9. In-
terrogative patterns such as ?What is? or ?Are there
any? are automatically eliminated from the depen-
dency trees. We define the dependency tree similar-
ity between two questions Qi and Qr to be a func-
tion of similarities wsim(vi, vr) computed between
aligned nodes vi ? Qi and vr ? Qr. The nodes
of two dependency trees are aligned through a func-
tion MaxMatch(ui.C, ur.C) that takes two sets of
children nodes as arguments, one from Qi and one
from Qr, and finds the maximum weighted bipartite
matching between ui.C and ur.C. Given two chil-
dren nodes vi ? ui.C and vr ? ur.C, the weight of
a potential matching between vi and vr is defined
102
simply as wsim(vi, vr). MaxMatch(ui.C, ur.C) is
furthermore constrained to match only nodes that
have compatible part-of-speech tags (e.g. nouns
are matched to nouns, verbs are matched to verbs),
and children nodes that have the same head-modifier
relationship with their parents (i.e. they are both
heads, or they are both dependents of their par-
ents). Table 3 shows the recursive algorithm used
TreeMatch(ui, ur)
[In]: Two dependency tree nodes ui, ur.
[Out]: A set of node pairsM.
1. setM? {(ui, ur)}
2. for each (vi, vr) ? MaxMatch(ui.C, ur.C):
3. setM?M? TreeMatch(vi, vr)
4. returnM
Table 3: Dependency Tree Matching.
for finding a matching between two question depen-
dency trees rooted at the focus words. The initial
arguments of the algorithm are the two focus words
ui = fi and ur = fr. Thus, the pair (fi, fr) is
the first pair of nodes to be added to the matching
M in step 1. In the next step, we compute the maxi-
mumweighted matching between the children nodes
ui.C and ur.C, and recursively call the matching al-
gorithm on pairs of matched nodes (vi, vr) fromM.
The algorithm stops when MaxMatch returns an
empty matching, which may happen when reach-
ing leaf nodes, or when no pair of children nodes
has compatible POS tags, or child-parent dependen-
cies. Figure 2 shows the results of applying the
tree matching algorithm on questions Q5 and Q9.
Matched nodes share the same index and are shown
in circles, whereas unmatched nodes are shown in
italics.
We introduce a new feature ?t(Qi, Qr) whose
value is defined as the dependency tree similarity
between questions Qi and Qr. Once the optimum
matchingM(Qi, Qr) between dependency trees has
been found, ?t(Qi, Qr) is computed as the nor-
malized sum of the similarities between pairs of
matched nodes vi and vr, as shown in Equations 3
and 4 below. When computing the similarity be-
tween two matched nodes, we factor in the similar-
ities between corresponding pairs of words on the
paths fi ; vi, fr ; vr between the focus words fi,
fr and the nodes vi, vr, as shown in Equation 5. This
has the effect of reducing the importance of words
that are farther away from the focus word in the de-
pendency tree.
?t(Qi, Qr) =
sim(Qi, Qr)
?
sim(Qi, Qi)sim(Qr, Qr)
(3)
sim(Qi, Qr) =
?
(vi,vr)?M(Qi,Qr)
sim(fi ; vi, fr ; vr) (4)
sim(u1 ; un, v1 ; vn) =
n
?
i=1
wsim(ui, vi) (5)
If the word similarity function is normalized and
defined to return 1 for identical words, the nor-
malizer in Equation 3 becomes equivalent with
?
|Qi||Qr|. Thus, words that are left unmatched im-
plicitly decrease the dependency tree similarity.
4.4 An SVM Model for Ranking Questions
We consider learning a usefulness function
u(Qi, Qr) of the following general, linear form:
u(Qi, Qr) = wT?(Qi, Qr) (6)
The vector ?(Qi, Qr) is defined to contain the fol-
lowing generic features:
1. ?f (Qi, Qr) = the semantic similarity between
focus words, as described in Section 4.1.
2. ?v(Qi, Qr) = the semantic similarity between
main verbs, as described in Section 4.2.
3. ?t(Qi, Qr) = the semantic similarity between
the dependency trees, as described in Sec-
tion 4.3.
4. cos(Qi, Qr) = the cosine similarity between the
two questions, as described in Section 3.
5. mcs(Qi, Qr) = the bag-of-concepts similarity
between the two questions, as described in Sec-
tion 3.
Each of the generic features ?f , ?v, ?t, andmcs cor-
responds to four actual features, one for each possi-
ble choice of the word similarity functionwsim (i.e.
wup, res, lin or jcn). An additional pair of features
is targeted at questions containing locations:
103
6. ?l(Qi, Qr) = 1 if both questions contain loca-
tions, 0 otherwise.
7. ?d(Qi, Qr) = the normalized geographical dis-
tance between the locations in Qi and Qr, 0 if
?l(Qi, Qr) = 0.
Given two location names, we first find their latitude
and longitude using Google Maps, and then com-
pute the spherical distance between them using the
haversine formula.
The corresponding parameters w will be trained
on pairs from one of the partially ordered datasets
described in Section 2. We use the kernel version of
the large-margin ranking approach from (Joachims,
2002) which solves the optimization problem in Fig-
ure 3 below. The aim of this formulation is to find a
minimize:
J(w, ?) = 12?w?2 + C
?
?rij
subject to:
wT?(Qi, Qr)?wT?(Qj , Qr) ? 1? ?rij
?rij ? 0
?Qr, Qi, Qj ? D, ?Qi ? Qj |Qr?
Figure 3: SVM ranking optimization problem.
weight vector w such that 1) the number of ranking
constraints u(Qi, Qr) ? u(Qj , Qr) from the train-
ing data D that are violated is minimized, and 2) the
ranking function u(Qi, Qr) generalizes well beyond
the training data. The learned w is a linear combina-
tion of the feature vectors ?(Qi, Qr), which makes
it possible to use kernels.
5 Experimental Evaluation
We use the four question ranking datasets described
in Section 2 to evaluate the three similarity mea-
sures cos, mcs, and ?t, as well as the SVM rank-
ing model. We report one set of results for each of
the four word similarity measures wup, res, lin or
jcn. Each question similarity measure is evaluated
in terms of its accuracy on the set of ordered pairs,
and the performance is averaged between the two
annotators for the Simple and Complex datasets. If
?Qi ? Qj |Qr? is a relation specified in the anno-
tation, we consider the tuple ?Qi, Qj , Qr? correctly
classified if and only if u(Qi, Qr) > u(Qj , Qr),
where u is the question similarity measure. We used
the SVMlight 2 implementation of ranking SVMs,
with a cubic kernel and the standard parameters. The
SVM ranking model was trained and tested using
10-fold cross-validation, and the overall accuracy
was computed by averaging over the 10 folds.
We used the NLTK 3 implementation of the four
similarity measures wup, res, lin or jcn. The idf val-
ues for each word were computed from frequency
counts over the entire Wikipedia. For each ques-
tion, the focus is identified automatically by an SVM
tagger trained on a separate corpus of 2,000 ques-
tions manually annotated with focus information
(Bunescu and Huang, 2010a). The SVM tagger
uses a combination of lexico-syntactic features and
a quadratic kernel to achieve a 93.5% accuracy in
a 10-fold cross validation evaluation on the 2,000
questions. The head-modifier dependencies were
derived automatically from the syntactic parse tree
using the head finding rules from (Collins, 1999).
The syntactic tree is obtained using Spear 4, a syn-
tactic parser which comes pre-trained on an addi-
tional treebank of questions. The main verb of
a question is identified deterministically using a
breadth first traversal of the dependency tree.
The overall accuracy results presented in Table 4
show that the SVM ranking model obtains by far the
best performance on both datasets, a substantial 10%
higher than cos, which is the best performing unsu-
pervised method. The random baseline ? assigning
a random similarity value to each pair of questions ?
results in 50% accuracy. Even though its use of word
senses was expected to lead to superior results, mcs
does not perform better than cos on this dataset. Our
implementation of mcs did however perform better
than cos on the Microsoft paraphrase corpus (Dolan
et al, 2004). One possible reason for this behav-
ior is that mcs seems to be less resilient than cos
to differences in question length. Whereas the Mi-
crosoft paraphrase corpus was specifically designed
such that ?the length of the shorter of the two sen-
tences, in words, is at least 66% that of the longer?
(Dolan and Brockett, 2005), the question ranking
datasets place no constraints on the lengths of the
2svmlight.joachims.org
3www.nltk.org
4www.surdeanu.name/mihai/spear
104
Question wup res lin jcn
Dataset cos mcs ?t mcs ?t mcs ?t mcs ?t SVM
Simple 73.7 69.1 69.4 71.3 71.8 70.8 69.8 71.9 71.7 82.1
Complex 72.6 64.1 69.6 66.0 71.5 66.9 69.1 69.4 71.0 82.5
Table 4: Pairwise accuracy results.
Dataset al ??f ??v ??t ??l,d ?cos ?mcs ??f,t
Simple 82.1 79.3 82.0 80.2 81.5 80.3 81.4 78.5
Complex 82.5 81.3 81.3 78.7 81.8 79.2 81.8 77.4
Table 5: Ablation results.
questions. However, even though by themselves the
meaning aware mcs and the structure-and-meaning
aware ?t do not outperform the bag-of-words cos,
they do help in increasing the performance of the
SVM ranking model, as can be inferred from the cor-
responding columns in Table 5. The table shows the
results of ablation experiments in which all but one
type of features are used. The results indicate that
all types of features are useful, with significant con-
tributions being brought especially by cos and the
focus related features ?f,t.
The measures investigated in this paper are all
compositional and reduce the similarity computa-
tions to word level. The following question patterns
illustrate the need to design more complex similarity
measures that take into account the context of every
word in the question:
P1 Where can I find a job around ?City ??
P2 What are some famous people from ?City ??
P3 What is the population of ?City ??
Below are three instantiations of the first question
pattern:
Q32 Where can I find a job around Anaheim, CA?
Q33 Where can I find a job around Los Angeles?
Q34 Where can I find a job around Vista, CA?
If we take Q32 as reference question, the fact that
the distance between Los Angeles and Anaheim is
smaller than the distance between Vista and Ana-
heim leads the ranking system to rank Q33 as more
useful than Q34 with respect to Q32, which is the
expected result. The preposition ?around? from the
city context in the first pattern is a good indica-
tor that proximity relations are relevant in this case.
When the same three cities are used for instantiating
the other two patterns, it can be seen that the prox-
imity relations are no longer as relevant for judging
the relative usefulness of questions.
6 Future Work
We plan to integrate context dependent word sim-
ilarity measures into a more robust question util-
ity function. We also plan to make the dependency
tree matching more flexible in order to account for
paraphrase patterns that may differ in their syntactic
structure. The questions that are posted on commu-
nity QA sites often contain spelling or grammatical
errors. Consequently, we will work on interfacing
the question ranking system with a separate module
aimed at fixing orthographic and grammatical errors.
7 Related Work
The question rephrasing subtask has spawned a di-
verse set of approaches. (Hermjakob et al, 2002)
derive a set of phrasal patterns for question reformu-
lation by generalizing surface patterns acquired au-
tomatically from a large corpus of web documents.
The focus of the work in (Tomuro, 2003) is on deriv-
ing reformulation patterns for the interrogative part
of a question. In (Jeon et al, 2005), word trans-
lation probabilities are trained on pairs of seman-
tically similar questions that are automatically ex-
tracted from an FAQ archive, and then used in a
language model that retrieves question reformula-
tions. (Jijkoun and de Rijke, 2005) describe an FAQ
105
question retrieval system in which weighted com-
binations of similarity functions corresponding to
questions, existing answers, FAQ titles and pages
are computed using a vector space model. (Zhao et
al., 2007) exploit the Encarta logs to automatically
extract clusters containing question paraphrases and
further train a perceptron to recognize question para-
phrases inside each cluster based on a combination
of lexical, syntactic and semantic similarity features.
More recently, (Bernhard and Gurevych, 2008) eval-
uated various string similarity measures and vec-
tor space based similarity measures on the task of
retrieving question paraphrases from the WikiAn-
swers repository. The aim of the question search
task presented in (Duan et al, 2008) is to return
questions that are semantically equivalent or close
to the queried question, and is therefore similar to
our question ranking task. Their approach is eval-
uated on a dataset in which questions are catego-
rized either as relevant or irrelevant. Our formula-
tion of question ranking is more general, and in par-
ticular subsumes the annotation of binary question
categories such as relevant vs. irrelevant, or para-
phrases vs. non-paraphrases. Moreover, we are able
to exploit the annotated utility relations as super-
vision in a learning for ranking approach, whereas
(Duan et al, 2008) use the annotated dataset to tune
the 3 parameters of a mostly unsupervised approach.
The question ranking task was first formulated in
(Bunescu and Huang, 2010b), where an initial ver-
sion of the dataset was also described. In this pa-
per, we introduce 4 versions of the dataset, a more
general meaning and structure aware similarity mea-
sure, and a supervised model for ranking that sub-
stantially outperforms the previously proposed util-
ity measures.
8 Conclusion
We presented a supervised learning approach to the
question ranking task in which previously known
questions are ordered based on their relative util-
ity with respect to a new, reference question. We
created four versions of a dataset of 60 groups of
questions 5, each annotated with a partial order rela-
tion reflecting the relative utility of questions inside
each group. An SVM ranking model was trained
5The dataset will be made publicly available.
on the dataset and evaluated together with a set of
simpler, unsupervised question-to-question similar-
ity models. Experimental results demonstrate the
importance of using structure and meaning aware
features when computing the relative usefulness of
questions.
Acknowledgments
We would like to thank the anonymous reviewers for
their insightful comments.
References
Ricardo Baeza-Yates and Berthier Ribeiro-Neto. 1999.
Modern Information Retrieval. ACM Press, New
York.
Delphine Bernhard and Iryna Gurevych. 2008. Answer-
ing learners? questions by retrieving question para-
phrases from social Q&A sites. In EANL ?08: Pro-
ceedings of the Third Workshop on Innovative Use of
NLP for Building Educational Applications, pages 44?
52, Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Razvan Bunescu and Yunfeng Huang. 2010a. Towards a
general model of answer typing: Question focus iden-
tification. In Proceedings of The 11th International
Conference on Intelligent Text Processing and Com-
putational Linguistics (CICLing 2010), RCS Volume,
pages 231?242.
Razvan Bunescu and Yunfeng Huang. 2010b. A utility-
driven approach to question ranking in social QA.
In Proceedings of The 23rd International Conference
on Computational Linguistics (COLING 2010), pages
125?133.
Michael Collins. 1999. Head-driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
William B. Dolan and Chris Brockett. 2005. Automat-
ically constructing a corpus of sentential paraphrases.
In Proceedings of the Third International Workshop on
Paraphrasing (IWP2005), pages 9?16.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Un-
supervised construction of large paraphrase corpora:
Exploiting assively parallel news sources. In Proceed-
ings of The 20th International Conference on Compu-
tational Linguistics (COLING?04), page 350.
Huizhong Duan, Yunbo Cao, Chin-Yew Lin, and Yong
Yu. 2008. Searching questions by identifying question
topic and question focus. In Proceedings of ACL-08:
HLT, pages 156?164, Columbus, Ohio, June.
Ulf Hermjakob, Abdessamad Echihabi, and Daniel
Marcu. 2002. Natural language based reformulation
106
resource and web exploitation for question answering.
In Proceedings of TREC-2002.
Jiwoon Jeon, W. Bruce Croft, and Joon Ho Lee. 2005.
Finding similar questions in large question and an-
swer archives. In Proceedings of the 14th ACM in-
ternational conference on Information and knowledge
management (CIKM?05), pages 84?90, NewYork, NY,
USA. ACM.
J.J. Jiang and D.W. Conrath. 1997. Semantic similarity
based on corpus statistics and lexical taxonomy. In
Proceedings of the International Conference on Re-
search in Computational Linguistics, pages 19?33.
Valentin Jijkoun and Maarten de Rijke. 2005. Retrieving
answers from frequently asked questions pages on the
Web. In Proceedings of the 14th ACM international
conference on Information and knowledge manage-
ment (CIKM?05), pages 76?83, New York, NY, USA.
ACM.
Thorsten Joachims. 2002. Optimizing search engines us-
ing clickthrough data. In Proceedings of the Eighth
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining (KDD-2002), Ed-
monton, Canada.
Dekang Lin. 1998. An information-theoretic defini-
tion of similarity. In Proceedings of the Fifteenth In-
ternational Conference on Machine Learning (ICML
?98), pages 296?304, San Francisco, CA, USA. Mor-
gan Kaufmann Publishers Inc.
Rada Mihalcea, Courtney Corley, and Carlo Strappar-
ava. 2006. Corpus-based and knowledge-based mea-
sures of text semantic similarity. In Proceedings of
the 21st national conference on Artificial intelligence
(AAAI?06), pages 775?780. AAAI Press.
Dan I. Moldovan, Marius Pasca, Sanda M. Harabagiu,
and Mihai Surdeanu. 2002. Performance issues and
error analysis in an open-domain question answering
system. In Proceedings of the 40th Annual Meeting of
the Association for Computational Linguistics, pages
33?40, Philadelphia, PA, July.
John M. Prager. 2006. Open-domain question-
answering. Foundations and Trends in Information
Retrieval, 1(2):91?231.
Philip Resnik. 1995. Using information content to eval-
uate semantic similarity in a taxonomy. In IJCAI?95:
Proceedings of the 14th international joint conference
on Artificial intelligence, pages 448?453, San Fran-
cisco, CA, USA. Morgan Kaufmann Publishers Inc.
Noriko Tomuro. 2003. Interrogative reformulation pat-
terns and acquisition of question paraphrases. In Pro-
ceedings of the Second International Workshop on
Paraphrasing, pages 33?40, Morristown, NJ, USA.
Association for Computational Linguistics.
Zhibiao Wu and Martha Palmer. 1994. Verbs seman-
tics and lexical selection. In Proceedings of the 32nd
annual meeting on Association for Computational Lin-
guistics, pages 133?138, Morristown, NJ, USA. Asso-
ciation for Computational Linguistics.
Shiqi Zhao, Ming Zhou, and Ting Liu. 2007. Learn-
ing question paraphrases for QA from Encarta logs. In
Proceedings of the 20th international joint conference
on Artifical intelligence (IJCAI?07), pages 1795?1800,
San Francisco, CA, USA. Morgan Kaufmann Publish-
ers Inc.
107
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 752?762,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Learning to Grade Short Answer Questions using Semantic Similarity
Measures and Dependency Graph Alignments
Michael Mohler
Dept. of Computer Science
University of North Texas
Denton, TX
mgm0038@unt.edu
Razvan Bunescu
School of EECS
Ohio University
Athens, Ohio
bunescu@ohio.edu
Rada Mihalcea
Dept. of Computer Science
University of North Texas
Denton, TX
rada@cs.unt.edu
Abstract
In this work we address the task of computer-
assisted assessment of short student answers.
We combine several graph alignment features
with lexical semantic similarity measures us-
ing machine learning techniques and show
that the student answers can be more accu-
rately graded than if the semantic measures
were used in isolation. We also present a first
attempt to align the dependency graphs of the
student and the instructor answers in order to
make use of a structural component in the au-
tomatic grading of student answers.
1 Introduction
One of the most important aspects of the learning
process is the assessment of the knowledge acquired
by the learner. In a typical classroom assessment
(e.g., an exam, assignment or quiz), an instructor or
a grader provides students with feedback on their
answers to questions related to the subject matter.
However, in certain scenarios, such as a number of
sites worldwide with limited teacher availability, on-
line learning environments, and individual or group
study sessions done outside of class, an instructor
may not be readily available. In these instances, stu-
dents still need some assessment of their knowledge
of the subject, and so, we must turn to computer-
assisted assessment (CAA).
While some forms of CAA do not require sophis-
ticated text understanding (e.g., multiple choice or
true/false questions can be easily graded by a system
if the correct solution is available), there are also stu-
dent answers made up of free text that may require
textual analysis. Research to date has concentrated
on two subtasks of CAA: grading essay responses,
which includes checking the style, grammaticality,
and coherence of the essay (Higgins et al, 2004),
and the assessment of short student answers (Lea-
cock and Chodorow, 2003; Pulman and Sukkarieh,
2005; Mohler and Mihalcea, 2009), which is the fo-
cus of this work.
An automatic short answer grading system is one
that automatically assigns a grade to an answer pro-
vided by a student, usually by comparing it to one
or more correct answers. Note that this is different
from the related tasks of paraphrase detection and
textual entailment, since a common requirement in
student answer grading is to provide a grade on a
certain scale rather than make a simple yes/no deci-
sion.
In this paper, we explore the possibility of im-
proving upon existing bag-of-words (BOW) ap-
proaches to short answer grading by utilizing ma-
chine learning techniques. Furthermore, in an at-
tempt to mirror the ability of humans to understand
structural (e.g. syntactic) differences between sen-
tences, we employ a rudimentary dependency-graph
alignment module, similar to those more commonly
used in the textual entailment community.
Specifically, we seek answers to the following
questions. First, to what extent can machine learn-
ing be leveraged to improve upon existing ap-
proaches to short answer grading. Second, does the
dependency parse structure of a text provide clues
that can be exploited to improve upon existing BOW
methodologies?
752
2 Related Work
Several state-of-the-art short answer grading sys-
tems (Sukkarieh et al, 2004; Mitchell et al, 2002)
require manually crafted patterns which, if matched,
indicate that a question has been answered correctly.
If an annotated corpus is available, these patterns
can be supplemented by learning additional pat-
terns semi-automatically. The Oxford-UCLES sys-
tem (Sukkarieh et al, 2004) bootstraps patterns by
starting with a set of keywords and synonyms and
searching through windows of a text for new pat-
terns. A later implementation of the Oxford-UCLES
system (Pulman and Sukkarieh, 2005) compares
several machine learning techniques, including in-
ductive logic programming, decision tree learning,
and Bayesian learning, to the earlier pattern match-
ing approach, with encouraging results.
C-Rater (Leacock and Chodorow, 2003) matches
the syntactical features of a student response (i.e.,
subject, object, and verb) to that of a set of correct
responses. This method specifically disregards the
BOW approach to take into account the difference
between ?dog bites man? and ?man bites dog? while
still trying to detect changes in voice (i.e., ?the man
was bitten by the dog?).
Another short answer grading system, AutoTutor
(Wiemer-Hastings et al, 1999), has been designed
as an immersive tutoring environment with a graph-
ical ?talking head? and speech recognition to im-
prove the overall experience for students. AutoTutor
eschews the pattern-based approach entirely in favor
of a BOW LSA approach (Landauer and Dumais,
1997). Later work on AutoTutor(Wiemer-Hastings
et al, 2005; Malatesta et al, 2002) seeks to expand
upon their BOW approach which becomes less use-
ful as causality (and thus word order) becomes more
important.
A text similarity approach was taken in (Mohler
and Mihalcea, 2009), where a grade is assigned
based on a measure of relatedness between the stu-
dent and the instructor answer. Several measures are
compared, including knowledge-based and corpus-
based measures, with the best results being obtained
with a corpus-based measure using Wikipedia com-
bined with a ?relevance feedback? approach that it-
eratively augments the instructor answer by inte-
grating the student answers that receive the highest
grades.
In the dependency-based classification compo-
nent of the Intelligent Tutoring System (Nielsen et
al., 2009), instructor answers are parsed, enhanced,
and manually converted into a set of content-bearing
dependency triples or facets. For each facet of the
instructor answer each student?s answer is labelled
to indicate whether it has addressed that facet and
whether or not the answer was contradictory. The
system uses a decision tree trained on part-of-speech
tags, dependency types, word count, and other fea-
tures to attempt to learn how best to classify an an-
swer/facet pair.
Closely related to the task of short answer grading
is the task of textual entailment (Dagan et al, 2005),
which targets the identification of a directional in-
ferential relation between texts. Given a pair of two
texts as input, typically referred to as text and hy-
pothesis, a textual entailment system automatically
finds if the hypothesis is entailed by the text.
In particular, the entailment-related works that are
most similar to our own are the graph matching tech-
niques proposed by Haghighi et al (2005) and Rus
et al (2007). Both input texts are converted into a
graph by using the dependency relations obtained
from a parser. Next, a matching score is calculated,
by combining separate vertex- and edge-matching
scores. The vertex matching functions use word-
level lexical and semantic features to determine the
quality of the match while the the edge matching
functions take into account the types of relations and
the difference in lengths between the aligned paths.
Following the same line of work in the textual en-
tailment world are (Raina et al, 2005), (MacCartney
et al, 2006), (de Marneffe et al, 2007), and (Cham-
bers et al, 2007), which experiment variously with
using diverse knowledge sources, using a perceptron
to learn alignment decisions, and exploiting natural
logic.
3 Answer Grading System
We use a set of syntax-aware graph alignment fea-
tures in a three-stage pipelined approach to short an-
swer grading, as outlined in Figure 1.
In the first stage (Section 3.1), the system is pro-
vided with the dependency graphs for each pair of
instructor (Ai) and student (As) answers. For each
753
Figure 1: Pipeline model for scoring short-answer pairs.
node in the instructor?s dependency graph, we com-
pute a similarity score for each node in the student?s
dependency graph based upon a set of lexical, se-
mantic, and syntactic features applied to both the
pair of nodes and their corresponding subgraphs.
The scoring function is trained on a small set of man-
ually aligned graphs using the averaged perceptron
algorithm.
In the second stage (Section 3.2), the node simi-
larity scores calculated in the previous stage are used
to weight the edges in a bipartite graph representing
the nodes in Ai on one side and the nodes in As on
the other. We then apply the Hungarian algorithm
to find both an optimal matching and the score asso-
ciated with such a matching. In this stage, we also
introduce question demoting in an attempt to reduce
the advantage of parroting back words provided in
the question.
In the final stage (Section 3.4), we produce an
overall grade based upon the alignment scores found
in the previous stage as well as the results of several
semantic BOW similarity measures (Section 3.3).
Using each of these as features, we use Support Vec-
tor Machines (SVM) to produce a combined real-
number grade. Finally, we build an Isotonic Regres-
sion (IR) model to transform our output scores onto
the original [0,5] scale for ease of comparison.
3.1 Node to Node Matching
Dependency graphs for both the student and in-
structor answers are generated using the Stanford
Dependency Parser (de Marneffe et al, 2006) in
collapse/propagate mode. The graphs are further
post-processed to propagate dependencies across the
?APPOS? (apposition) relation, to explicitly encode
negation, part-of-speech, and sentence ID within
each node, and to add an overarching ROOT node
governing the main verb or predicate of each sen-
tence of an answer. The final representation is a
list of (relation,governor,dependent) triples, where
governor and dependent are both tokens described
by the tuple (sentenceID:token:POS:wordPosition).
For example: (nsubj, 1:provide:VBZ:4, 1:pro-
gram:NN:3) indicates that the noun ?program? is a
subject in sentence 1 whose associated verb is ?pro-
vide.?
If we consider the dependency graphs output by
the Stanford parser as directed (minimally cyclic)
graphs,1 we can define for each node x a set of nodes
Nx that are reachable from x using a subset of the
relations (i.e., edge types)2. We variously define
?reachable? in four ways to create four subgraphs
defined for each node. These are as follows:
? N0x : All edge types may be followed
? N1x : All edge types except for subject types,
ADVCL, PURPCL, APPOS, PARATAXIS,
ABBREV, TMOD, and CONJ
? N2x : All edge types except for those in N1x plus
object/complement types, PREP, and RCMOD
? N3x : No edge types may be followed (This set
is the single starting node x)
Subgraph similarity (as opposed to simple node
similarity) is a means to escape the rigidity involved
in aligning parse trees while making use of as much
of the sentence structure as possible. Humans intu-
itively make use of modifiers, predicates, and subor-
dinate clauses in determining that two sentence en-
tities are similar. For instance, the entity-describing
phrase ?men who put out fires? matches well with
?firemen,? but the words ?men? and ?firemen? have
1The standard output of the Stanford Parser produces rooted
trees. However, the process of collapsing and propagating de-
pendences violates the tree structure which results in a tree
with a few cross-links between distinct branches.
2For more information on the relations used in this experi-
ment, consult the Stanford Typed Dependencies Manual at
http://nlp.stanford.edu/software/dependencies manual.pdf
754
less inherent similarity. It remains to be determined
how much of a node?s subgraph will positively en-
rich its semantics. In addition to the complete N0x
subgraph, we chose to include N1x and N2x as tight-
ening the scope of the subtree by first removing
more abstract relations, then sightly more concrete
relations.
We define a total of 68 features to be used to train
our machine learning system to compute node-node
(more specifically, subgraph-subgraph) matches. Of
these, 36 are based upon the semantic similarity
of four subgraphs defined by N [0..3]x . All eight
WordNet-based similarity measures listed in Sec-
tion 3.3 plus the LSA model are used to produce
these features. The remaining 32 features are lexico-
syntactic features3 defined only for N3x and are de-
scribed in more detail in Table 2.
We use ?(xi, xs) to denote the feature vector as-
sociated with a pair of nodes ?xi, xs?, where xi is
a node from the instructor answer Ai and xs is a
node from the student answer As. A matching score
can then be computed for any pair ?xi, xs? ? Ai ?
As through a linear scoring function f(xi, xs) =
wT?(xi, xs). In order to learn the parameter vec-
tor w, we use the averaged version of the percep-
tron algorithm (Freund and Schapire, 1999; Collins,
2002).
As training data, we randomly select a subset of
the student answers in such a way that our set was
roughly balanced between good scores, mediocre
scores, and poor scores. We then manually annotate
each node pair ?xi, xs? as matching, i.e. A(xi, xs) =
+1, or not matching, i.e. A(xi, xs) = ?1. Overall,
32 student answers in response to 21 questions with
a total of 7303 node pairs (656 matches, 6647 non-
matches) are manually annotated. The pseudocode
for the learning algorithm is shown in Table 1. Af-
ter training the perceptron, these 32 student answers
are removed from the dataset, not used as training
further along in the pipeline, and are not included in
the final results. After training for 50 epochs,4 the
matching score f(xi, xs) is calculated (and cached)
for each node-node pair across all student answers
for all assignments.
3Note that synonyms include negated antonyms (and vice
versa). Hypernymy and hyponymy are restricted to at most
two steps).
4This value was chosen arbitrarily and was not tuned in anyway
0. set w ? 0, w? 0, n? 0
1. repeat for T epochs:
2. foreach ?Ai;As?:
3. foreach ?xi, xs? ? Ai ?As:
4. if sgn(wT?(xi, xs)) 6= sgn(A(xi, xs)):
5. set w ? w+A(xi, xs)?(xi, xs)
6. set w ? w+w, n? n+ 1
7. return w/n.
Table 1: Perceptron Training for Node Matching.
3.2 Graph to Graph Alignment
Once a score has been computed for each node-node
pair across all student/instructor answer pairs, we at-
tempt to find an optimal alignment for the answer
pair. We begin with a bipartite graph where each
node in the student answer is represented by a node
on the left side of the bipartite graph and each node
in the instructor answer is represented by a node
on the right side. The score associated with each
edge is the score computed for each node-node pair
in the previous stage. The bipartite graph is then
augmented by adding dummy nodes to both sides
which are allowed to match any node with a score of
zero. An optimal alignment between the two graphs
is then computed efficiently using the Hungarian al-
gorithm. Note that this results in an optimal match-
ing, not a mapping, so that an individual node is as-
sociated with at most one node in the other answer.
At this stage we also compute several alignment-
based scores by applying various transformations to
the input graphs, the node matching function, and
the alignment score itself.
The first and simplest transformation involves the
normalization of the alignment score. While there
are several possible ways to normalize a matching
such that longer answers do not unjustly receive
higher scores, we opted to simply divide the total
alignment score by the number of nodes in the in-
structor answer.
The second transformation scales the node match-
ing score by multiplying it with the idf 5 of the in-
structor answer node, i.e., replace f(xi, xs) with
idf(xi) ? f(xi, xs).
The third transformation relies upon a certain
real-world intuition associated with grading student
5Inverse document frequency, as computed from the British Na-
tional Corpus (BNC)
755
Name Type # features Description
RootMatch binary 5 Is a ROOT node matched to: ROOT, N, V, JJ, or Other
Lexical binary 3 Exact match, Stemmed match, close Levenshtein match
POSMatch binary 2 Exact POS match, Coarse POS match
POSPairs binary 8 Specific X-Y POS matches found
Ontological binary 4 WordNet relationships: synonymy, antonymy, hypernymy, hyponymy
RoleBased binary 3 Has as a child - subject, object, verb
VerbsSubject binary 3 Both are verbs and neither, one, or both have a subject child
VerbsObject binary 3 Both are verbs and neither, one, or both have an object child
Semantic real 36 Nine semantic measures across four subgraphs each
Bias constant 1 A value of 1 for all vectors
Total 68
Table 2: Subtree matching features used to train the perceptron
answers ? repeating words in the question is easy
and is not necessarily an indication of student under-
standing. With this in mind, we remove any words
in the question from both the instructor answer and
the student answer.
In all, the application of the three transforma-
tions leads to eight different transform combina-
tions, and therefore eight different alignment scores.
For a given answer pair (Ai, As), we assemble the
eight graph alignment scores into a feature vector
?G(Ai, As).
3.3 Lexical Semantic Similarity
Haghighi et al (2005), working on the entailment
detection problem, point out that finding a good
alignment is not sufficient to determine that the
aligned texts are in fact entailing. For instance, two
identical sentences in which an adjective from one is
replaced by its antonym will have very similar struc-
tures (which indicates a good alignment). However,
the sentences will have opposite meanings. Further
information is necessary to arrive at an appropriate
score.
In order to address this, we combine the graph
alignment scores, which encode syntactic knowl-
edge, with the scores obtained from semantic sim-
ilarity measures.
Following Mihalcea et al (2006) and Mohler
and Mihalcea (2009), we use eight knowledge-
based measures of semantic similarity: shortest path
[PATH], Leacock & Chodorow (1998) [LCH], Lesk
(1986), Wu & Palmer(1994) [WUP], Resnik (1995)
[RES], Lin (1998), Jiang & Conrath (1997) [JCN],
Hirst & St. Onge (1998) [HSO], and two corpus-
based measures: Latent Semantic Analysis [LSA]
(Landauer and Dumais, 1997) and Explicit Seman-
tic Analysis [ESA] (Gabrilovich and Markovitch,
2007).
Briefly, for the knowledge-based measures, we
use the maximum semantic similarity ? for each
open-class word ? that can be obtained by pairing
it up with individual open-class words in the sec-
ond input text. We base our implementation on
the WordNet::Similarity package provided by Ped-
ersen et al (2004). For the corpus-based measures,
we create a vector for each answer by summing
the vectors associated with each word in the an-
swer ? ignoring stopwords. We produce a score in
the range [0..1] based upon the cosine similarity be-
tween the student and instructor answer vectors. The
LSA model used in these experiments was built by
training Infomap6 on a subset of Wikipedia articles
that contain one or more common computer science
terms. Since ESA uses Wikipedia article associa-
tions as vector features, it was trained using a full
Wikipedia dump.
3.4 Answer Ranking and Grading
We combine the alignment scores ?G(Ai, As) with
the scores ?B(Ai, As) from the lexical seman-
tic similarity measures into a single feature vector
?(Ai, As) = [?G(Ai, As)|?B(Ai, As)]. The fea-
ture vector ?G(Ai, As) contains the eight alignment
scores found by applying the three transformations
in the graph alignment stage. The feature vector
?B(Ai, As) consists of eleven semantic features ?
the eight knowledge-based features plus LSA, ESA
and a vector consisting only of tf*idf weights ? both
with and without question demoting. Thus, the en-
tire feature vector ?(Ai, As) contains a total of 30
features.
6http://Infomap-nlp.sourceforge.net/
756
An input pair (Ai, As) is then associated with a
grade g(Ai, As) = uT?(Ai, As) computed as a lin-
ear combination of features. The weight vector u is
trained to optimize performance in two scenarios:
Regression: An SVM model for regression (SVR)
is trained using as target function the grades as-
signed by the instructors. We use the libSVM 7 im-
plementation of SVR, with tuned parameters.
Ranking: An SVM model for ranking (SVMRank)
is trained using as ranking pairs all pairs of stu-
dent answers (As, At) such that grade(Ai, As) >
grade(Ai, At), where Ai is the corresponding in-
structor answer. We use the SVMLight 8 implemen-
tation of SVMRank with tuned parameters.
In both cases, the parameters are tuned using a
grid-search. At each grid point, the training data is
partitioned into 5 folds which are used to train a tem-
porary SVM model with the given parameters. The
regression passage selects the grid point with the
minimal mean square error (MSE), and the SVM-
Rank package tries to minimize the number of dis-
cordant pairs. The parameters found are then used to
score the test set ? a set not used in the grid training.
3.5 Isotonic Regression
Since the end result of any grading system is to give
a student feedback on their answers, we need to en-
sure that the system?s final score has some mean-
ing. With this in mind, we use isotonic regression
(Zadrozny and Elkan, 2002) to convert the system
scores onto the same [0..5] scale used by the an-
notators. This has the added benefit of making the
system output more directly related to the annotated
grade, which makes it possible to report root mean
square error in addition to correlation. We train the
isotonic regression model on each type of system
output (i.e., alignment scores, SVM output, BOW
scores).
4 Data Set
To evaluate our method for short answer grading,
we created a data set of questions from introductory
computer science assignments with answers pro-
vided by a class of undergraduate students. The as-
signments were administered as part of a Data Struc-
7http://www.csie.ntu.edu.tw/?cjlin/libsvm/
8http://svmlight.joachims.org/
tures course at the University of North Texas. For
each assignment, the student answers were collected
via an online learning environment.
The students submitted answers to 80 questions
spread across ten assignments and two examina-
tions.9 Table 3 shows two question-answer pairs
with three sample student answers each. Thirty-one
students were enrolled in the class and submitted an-
swers to these assignments. The data set we work
with consists of a total of 2273 student answers. This
is less than the expected 31 ? 80 = 2480 as some
students did not submit answers for a few assign-
ments. In addition, the student answers used to train
the perceptron are removed from the pipeline after
the perceptron training stage.
The answers were independently graded by two
human judges, using an integer scale from 0 (com-
pletely incorrect) to 5 (perfect answer). Both human
judges were graduate students in the computer sci-
ence department; one (grader1) was the teaching as-
sistant assigned to the Data Structures class, while
the other (grader2) is one of the authors of this pa-
per. We treat the average grade of the two annotators
as the gold standard against which we compare our
system output.
Difference Examples % of examples
0 1294 57.7%
1 514 22.9%
2 231 10.3%
3 123 5.5%
4 70 3.1%
5 9 0.4%
Table 4: Annotator Analysis
The annotators were given no explicit instructions
on how to assign grades other than the [0..5] scale.
Both annotators gave the same grade 57.7% of the
time and gave a grade only 1 point apart 22.9% of
the time. The full breakdown can be seen in Table
4. In addition, an analysis of the grading patterns
indicate that the two graders operated off of differ-
ent grading policies where one grader (grader1) was
more generous than the other. In fact, when the two
differed, grader1 gave the higher grade 76.6% of the
time. The average grade given by grader1 is 4.43,
9Note that this is an expanded version of the dataset used by
Mohler and Mihalcea (2009)
757
Sample questions, correct answers, and student answers Grades
Question: What is the role of a prototype program in problem solving?
Correct answer: To simulate the behavior of portions of the desired software product.
Student answer 1: A prototype program is used in problem solving to collect data for the problem. 1, 2
Student answer 2: It simulates the behavior of portions of the desired software product. 5, 5
Student answer 3: To find problem and errors in a program before it is finalized. 2, 2
Question: What are the main advantages associated with object-oriented programming?
Correct answer: Abstraction and reusability.
Student answer 1: They make it easier to reuse and adapt previously written code and they separate complex
programs into smaller, easier to understand classes. 5, 4
Student answer 2: Object oriented programming allows programmers to use an object with classes that can be
changed and manipulated while not affecting the entire object at once. 1, 1
Student answer 3: Reusable components, Extensibility, Maintainability, it reduces large problems into smaller
more manageable problems. 4, 4
Table 3: A sample question with short answers provided by students and the grades assigned by the two human judges
while the average grade given by grader2 is 3.94.
The dataset is biased towards correct answers. We
believe all of these issues correctly mirror real-world
issues associated with the task of grading.
5 Results
We independently test two components of our over-
all grading system: the node alignment detection
scores found by training the perceptron, and the
overall grades produced in the final stage. For the
alignment detection, we report the precision, recall,
and F-measure associated with correctly detecting
matches. For the grading stage, we report a single
Pearson?s correlation coefficient tracking the anno-
tator grades (average of the two annotators) and the
output score of each system. In addition, we re-
port the Root Mean Square Error (RMSE) for the
full dataset as well as the median RMSE across each
individual question. This is to give an indication of
the performance of the system for grading a single
question in isolation.10
5.1 Perceptron Alignment
For the purpose of this experiment, the scores as-
sociated with a given node-node matching are con-
verted into a simple yes/no matching decision where
positive scores are considered a match and negative
10We initially intended to report an aggregate of question-level
Pearson correlation results, but discovered that the dataset
contained one question for which each student received full
points ? leaving the correlation undefined. We believe that
this casts some doubt on the applicability of Pearson?s (or
Spearman?s) correlation coefficient for the short answer grad-
ing task. We have retained its use here alongside RMSE for
ease of comparison.
scores a non-match. The threshold weight learned
from the bias feature strongly influences the point
at which real scores change from non-matches to
matches, and given the threshold weight learned by
the algorithm, we find an F-measure of 0.72, with
precision(P) = 0.85 and recall(R) = 0.62. However,
as the perceptron is designed to minimize error rate,
this may not reflect an optimal objective when seek-
ing to detect matches. By manually varying the
threshold, we find a maximum F-measure of 0.76,
with P=0.79 and R=0.74. Figure 2 shows the full
precision-recall curve with the F-measure overlaid.
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.2  0.4  0.6  0.8  1
Sc
or
e
Recall
Precision
F-Measure
Threshold
Figure 2: Precision, recall, and F-measure on node-level
match detection
5.2 Question Demoting
One surprise while building this system was the con-
sistency with which the novel technique of question
demoting improved scores for the BOW similarity
measures. With this relatively minor change the av-
erage correlation between the BOW methods? sim-
758
ilarity scores and the student grades improved by
up to 0.046 with an average improvement of 0.019
across all eleven semantic features. Table 5 shows
the results of applying question demoting to our
semantic features. When comparing scores using
RMSE, the difference is less consistent, yielding an
average improvement of 0.002. However, for one
measure (tf*idf), the improvement is 0.063 which
brings its RMSE score close to the lowest of all
BOW metrics. The reasons for this are not entirely
clear. As a baseline, we include here the results of
assigning the average grade (as determined on the
training data) for each question. The average grade
was chosen as it minimizes the RMSE on the train-
ing data.
? w/ QD RMSE w/ QD Med. RMSE w/ QD
Lesk 0.450 0.462 1.034 1.050 0.930 0.919
JCN 0.443 0.461 1.022 1.026 0.954 0.923
HSO 0.441 0.456 1.036 1.034 0.966 0.935
PATH 0.436 0.457 1.029 1.030 0.940 0.918
RES 0.409 0.431 1.045 1.035 0.996 0.941
Lin 0.382 0.407 1.069 1.056 0.981 0.949
LCH 0.367 0.387 1.068 1.069 0.986 0.958
WUP 0.325 0.343 1.090 1.086 1.027 0.977
ESA 0.395 0.401 1.031 1.086 0.990 0.955
LSA 0.328 0.335 1.065 1.061 0.951 1.000
tf*idf 0.281 0.327 1.085 1.022 0.991 0.918
Avg.grade 1.097 1.097 0.973 0.973
Table 5: BOW Features with Question Demoting (QD).
Pearson?s correlation, root mean square error (RMSE),
and median RMSE for all individual questions.
5.3 Alignment Score Grading
Before applying any machine learning techniques,
we first test the quality of the eight graph alignment
features ?G(Ai, As) independently. Results indicate
that the basic alignment score performs comparably
to most BOW approaches. The introduction of idf
weighting seems to degrade performance somewhat,
while introducing question demoting causes the cor-
relation with the grader to increase while increasing
RMSE somewhat. The four normalized components
of ?G(Ai, As) are reported in Table 6.
5.4 SVM Score Grading
The SVM components of the system are run on the
full dataset using a 12-fold cross validation. Each of
the 10 assignments and 2 examinations (for a total
of 12 folds) is scored independently with ten of the
remaining eleven used to train the machine learn-
Standard w/ IDF w/ QD w/ QD+IDF
Pearson?s ? 0.411 0.277 0.428 0.291
RMSE 1.018 1.078 1.046 1.076
Median RMSE 0.910 0.970 0.919 0.992
Table 6: Alignment Feature/Grade Correlations using
Pearson?s ?. Results are also reported when inverse doc-
ument frequency weighting (IDF) and question demoting
(QD) are used.
ing system. For each fold, one additional fold is
held out for later use in the development of an iso-
tonic regression model (see Figure 3). The param-
eters (for cost C and tube width ) were found us-
ing a grid search. At each point on the grid, the data
from the ten training folds was partitioned into 5 sets
which were scored according to the current param-
eters. SVMRank and SVR sought to minimize the
number of discordant pairs and the mean absolute
error, respectively.
Both SVM models are trained using a linear ker-
nel.11 Results from both the SVR and the SVMRank
implementations are reported in Table 7 along with
a selection of other measures. Note that the RMSE
score is computed after performing isotonic regres-
sion on the SVMRank results, but that it was unnec-
essary to perform an isotonic regression on the SVR
results as the system was trained to produce a score
on the correct scale.
We report the results of running the systems on
three subsets of features ?(Ai, As): BOW features
?B(Ai, As) only, alignment features ?G(Ai, As)
only, or the full feature vector (labeled ?Hybrid?).
Finally, three subsets of the alignment features are
used: only unnormalized features, only normalized
features, or the full alignment feature set.
B CA ? Ten Folds
B CA ? Ten Folds
B CA ? Ten FoldsIR Model
SVM Model
Features
Figure 3: Dependencies of the SVM/IR training stages.
11We also ran the SVR system using quadratic and radial-basis
function (RBF) kernels, but the results did not show signifi-
cant improvement over the simpler linear kernel.
759
Unnormalized Normalized Both
IAA Avg. grade tf*idf Lesk BOW Align Hybrid Align Hybrid Align Hybrid
SVMRank
Pearson?s ? 0.586 0.327 0.450 0.480 0.266 0.451 0.447 0.518 0.424 0.493
RMSE 0.659 1.097 1.022 1.050 1.042 1.093 1.038 1.015 0.998 1.029 1.021
Median RMSE 0.605 0.973 0.918 0.919 0.943 0.974 0.903 0.865 0.873 0.904 0.901
SVR
Pearson?s ? 0.586 0.327 0.450 0.431 0.167 0.437 0.433 0.459 0.434 0.464
RMSE 0.659 1.097 1.022 1.050 0.999 1.133 0.995 1.001 0.982 1.003 0.978
Median RMSE 0.605 0.973 0.918 0.919 0.910 0.987 0.893 0.894 0.877 0.886 0.862
Table 7: The results of the SVM models trained on the full suite of BOW measures, the alignment scores, and the
hybrid model. The terms ?normalized?, ?unnormalized?, and ?both? indicate which subset of the 8 alignment features
were used to train the SVM model. For ease of comparison, we include in both sections the scores for the IAA, the
?Average grade? baseline, and two of the top performing BOW metrics ? both with question demoting.
6 Discussion and Conclusions
There are three things that we can learn from these
experiments. First, we can see from the results that
several systems appear better when evaluating on a
correlation measure like Pearson?s ?, while others
appear better when analyzing error rate. The SVM-
Rank system seemed to outperform the SVR sys-
tem when measuring correlation, however the SVR
system clearly had a minimal RMSE. This is likely
due to the different objective function in the corre-
sponding optimization formulations: while the rank-
ing model attempts to ensure a correct ordering be-
tween the grades, the regression model seeks to min-
imize an error objective that is closer to the RMSE.
It is difficult to claim that either system is superior.
Likewise, perhaps the most unexpected result of
this work is the differing analyses of the simple
tf*idf measure ? originally included only as a base-
line. Evaluating with a correlative measure yields
predictably poor results, but evaluating the error rate
indicates that it is comparable to (or better than) the
more intelligent BOW metrics. One explanation for
this result is that the skewed nature of this ?natural?
dataset favors systems that tend towards scores in
the 4 to 4.5 range. In fact, 46% of the scores output
by the tf*idf measure (after IR) were within the 4 to
4.5 range and only 6% were below 3.5. Testing on
a more balanced dataset, this tendency to fit to the
average would be less advantageous.
Second, the supervised learning techniques are
clearly able to leverage multiple BOW measures to
yield improvements over individual BOW metrics.
The correlation for the BOW-only SVM model for
SVMRank improved upon the best BOW feature
from .462 to .480. Likewise, using the BOW-only
SVM model for SVR reduces the RMSE by .022
overall compared to the best BOW feature.
Third, the rudimentary alignment features we
have introduced here are not sufficient to act as a
standalone grading system. However, even with a
very primitive attempt at alignment detection, we
show that it is possible to improve upon grade learn-
ing systems that only consider BOW features. The
correlations associated with the hybrid systems (esp.
those using normalized alignment data) frequently
show an improvement over the BOW-only SVM sys-
tems. This is true for both SVM systems when con-
sidering either evaluation metric.
Future work will concentrate on improving the
quality of the answer alignments by training a model
to directly output graph-to-graph alignments. This
learning approach will allow the use of more com-
plex alignment features, for example features that
are defined on pairs of aligned edges or on larger
subtrees in the two input graphs. Furthermore, given
an alignment, we can define several phrase-level
grammatical features such as negation, modality,
tense, person, number, or gender, which make bet-
ter use of the alignment itself.
Acknowledgments
This work was partially supported by a National Sci-
ence Foundation CAREER award #0747340. Any
opinions, findings, and conclusions or recommenda-
tions expressed in this material are those of the au-
thors and do not necessarily reflect the views of the
National Science Foundation.
760
References
N. Chambers, D. Cer, T. Grenager, D. Hall, C. Kid-
don, B. MacCartney, M.C. de Marneffe, D. Ramage,
E. Yeh, and C.D. Manning. 2007. Learning align-
ments and leveraging natural logic. In Proceedings
of the ACL-PASCAL Workshop on Textual Entailment
and Paraphrasing, pages 165?170. Association for
Computational Linguistics.
M. Collins. 2002. Discriminative training methods
for hidden Markov models: Theory and experiments
with perceptron algorithms. In Proceedings of the
2002 Conference on Empirical Methods in Natural
Language Processing (EMNLP-02), Philadelphia, PA,
July.
I. Dagan, O. Glickman, and B. Magnini. 2005. The PAS-
CAL recognising textual entailment challenge. In Pro-
ceedings of the PASCAL Workshop.
M.C. de Marneffe, B. MacCartney, and C.D. Manning.
2006. Generating typed dependency parses from
phrase structure parses. In LREC 2006.
M.C. de Marneffe, T. Grenager, B. MacCartney, D. Cer,
D. Ramage, C. Kiddon, and C.D. Manning. 2007.
Aligning semantic graphs for textual inference and
machine reading. In Proceedings of the AAAI Spring
Symposium. Citeseer.
Y. Freund and R. Schapire. 1999. Large margin clas-
sification using the perceptron algorithm. Machine
Learning, 37:277?296.
E. Gabrilovich and S. Markovitch. 2007. Computing
Semantic Relatedness using Wikipedia-based Explicit
Semantic Analysis. Proceedings of the 20th Inter-
national Joint Conference on Artificial Intelligence,
pages 6?12.
A.D. Haghighi, A.Y. Ng, and C.D. Manning. 2005. Ro-
bust textual inference via graph matching. In Pro-
ceedings of the conference on Human Language Tech-
nology and Empirical Methods in Natural Language
Processing, pages 387?394. Association for Computa-
tional Linguistics.
D. Higgins, J. Burstein, D. Marcu, and C. Gentile. 2004.
Evaluating multiple aspects of coherence in student
essays. In Proceedings of the annual meeting of the
North American Chapter of the Association for Com-
putational Linguistics, Boston, MA.
G. Hirst and D. St-Onge, 1998. Lexical chains as repre-
sentations of contexts for the detection and correction
of malaproprisms. The MIT Press.
J. Jiang and D. Conrath. 1997. Semantic similarity based
on corpus statistics and lexical taxonomy. In Proceed-
ings of the International Conference on Research in
Computational Linguistics, Taiwan.
T.K. Landauer and S.T. Dumais. 1997. A solution to
plato?s problem: The latent semantic analysis theory
of acquisition, induction, and representation of knowl-
edge. Psychological Review, 104.
C. Leacock and M. Chodorow. 1998. Combining lo-
cal context and WordNet sense similarity for word
sense identification. In WordNet, An Electronic Lex-
ical Database. The MIT Press.
C. Leacock and M. Chodorow. 2003. C-rater: Auto-
mated Scoring of Short-Answer Questions. Comput-
ers and the Humanities, 37(4):389?405.
M.E. Lesk. 1986. Automatic sense disambiguation us-
ing machine readable dictionaries: How to tell a pine
cone from an ice cream cone. In Proceedings of the
SIGDOC Conference 1986, Toronto, June.
D. Lin. 1998. An information-theoretic definition of
similarity. In Proceedings of the 15th International
Conference on Machine Learning, Madison, WI.
B. MacCartney, T. Grenager, M.C. de Marneffe, D. Cer,
and C.D. Manning. 2006. Learning to recognize fea-
tures of valid textual entailments. In Proceedings of
the main conference on Human Language Technology
Conference of the North American Chapter of the As-
sociation of Computational Linguistics, page 48. As-
sociation for Computational Linguistics.
K.I. Malatesta, P. Wiemer-Hastings, and J. Robertson.
2002. Beyond the Short Answer Question with Re-
search Methods Tutor. In Proceedings of the Intelli-
gent Tutoring Systems Conference.
R. Mihalcea, C. Corley, and C. Strapparava. 2006.
Corpus-based and knowledge-based approaches to text
semantic similarity. In Proceedings of the American
Association for Artificial Intelligence (AAAI 2006),
Boston.
T. Mitchell, T. Russell, P. Broomhead, and N. Aldridge.
2002. Towards robust computerised marking of free-
text responses. Proceedings of the 6th International
Computer Assisted Assessment (CAA) Conference.
M. Mohler and R. Mihalcea. 2009. Text-to-text seman-
tic similarity for automatic short answer grading. In
Proceedings of the European Association for Compu-
tational Linguistics (EACL 2009), Athens, Greece.
R.D. Nielsen, W. Ward, and J.H. Martin. 2009. Recog-
nizing entailment in intelligent tutoring systems. Nat-
ural Language Engineering, 15(04):479?501.
T. Pedersen, S. Patwardhan, and J. Michelizzi. 2004.
WordNet:: Similarity-Measuring the Relatedness of
Concepts. Proceedings of the National Conference on
Artificial Intelligence, pages 1024?1025.
S.G. Pulman and J.Z. Sukkarieh. 2005. Automatic Short
Answer Marking. ACL WS Bldg Ed Apps using NLP.
R. Raina, A. Haghighi, C. Cox, J. Finkel, J. Michels,
K. Toutanova, B. MacCartney, M.C. de Marneffe, C.D.
Manning, and A.Y. Ng. 2005. Robust textual infer-
ence using diverse knowledge sources. Recognizing
Textual Entailment, page 57.
761
P. Resnik. 1995. Using information content to evalu-
ate semantic similarity. In Proceedings of the 14th In-
ternational Joint Conference on Artificial Intelligence,
Montreal, Canada.
V. Rus, A. Graesser, and K. Desai. 2007. Lexico-
syntactic subsumption for textual entailment. Recent
Advances in Natural Language Processing IV: Se-
lected Papers from RANLP 2005, page 187.
J.Z. Sukkarieh, S.G. Pulman, and N. Raikes. 2004. Auto-
Marking 2: An Update on the UCLES-Oxford Univer-
sity research into using Computational Linguistics to
Score Short, Free Text Responses. International Asso-
ciation of Educational Assessment, Philadephia.
P. Wiemer-Hastings, K. Wiemer-Hastings, and
A. Graesser. 1999. Improving an intelligent tu-
tor?s comprehension of students with Latent Semantic
Analysis. Artificial Intelligence in Education, pages
535?542.
P. Wiemer-Hastings, E. Arnott, and D. Allbritton. 2005.
Initial results and mixed directions for research meth-
ods tutor. In AIED2005 - Supplementary Proceedings
of the 12th International Conference on Artificial In-
telligence in Education, Amsterdam.
Z. Wu and M. Palmer. 1994. Verb semantics and lexical
selection. In Proceedings of the 32nd Annual Meeting
of the Association for Computational Linguistics, Las
Cruces, New Mexico.
B. Zadrozny and C. Elkan. 2002. Transforming classifier
scores into accurate multiclass probability estimates.
Edmonton, Alberta.
762
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 11?19,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Adaptive Clustering for Coreference Resolution with Deterministic Rules
and Web-Based Language Models
Razvan C. Bunescu
School of EECS
Ohio University
Athens, OH 45701, USA
bunescu@ohio.edu
Abstract
We present a novel adaptive clustering model
for coreference resolution in which the expert
rules of a state of the art deterministic sys-
tem are used as features over pairs of clus-
ters. A significant advantage of the new ap-
proach is that the expert rules can be eas-
ily augmented with new semantic features.
We demonstrate this advantage by incorporat-
ing semantic compatibility features for neutral
pronouns computed from web n-gram statis-
tics. Experimental results show that the com-
bination of the new features with the expert
rules in the adaptive clustering approach re-
sults in an overall performance improvement,
and over 5% improvement in F1 measure for
the target pronouns when evaluated on the
ACE 2004 newswire corpus.
1 Introduction
Coreference resolution is the task of clustering a
sequence of textual entity mentions into a set of
maximal non-overlapping clusters, such that men-
tions in a cluster refer to the same discourse entity.
Coreference resolution is an important subtask in
a wide array of natural language processing prob-
lems, among them information extraction, question
answering, and machine translation. The availabil-
ity of corpora annotated with coreference relations
has led to the development of a diverse set of super-
vised learning approaches for coreference. While
learning models enjoy a largely undisputed role in
many NLP applications, deterministic models based
on rich sets of expert rules for coreference have been
shown recently to achieve performance rivaling, if
not exceeding, the performance of state of the art
machine learning approaches (Haghighi and Klein,
2009; Raghunathan et al, 2010). In particular, the
top performing system in the CoNLL 2011 shared
task (Pradhan et al, 2011) is a multi-pass system that
applies tiers of deterministic coreference sieves from
highest to lowest precision (Lee et al, 2011). The
PRECISECONSTRUCTS sieve, for example, creates
coreference links between mentions that are found
to match patterns of apposition, predicate nomina-
tives, acronyms, demonyms, or relative pronouns.
This is a high precision sieve, correspondingly it is
among the first sieves to be applied. The PRONOUN-
MATCH sieve links an anaphoric pronoun with the
first antecedent mention that agrees in number and
gender with the pronoun, based on an ordering of the
antecedents that uses syntactic rules to model dis-
course salience. This is the last sieve to be applied,
due to its lower overall precision, as estimated on
development data. While very successful, this de-
terministic multi-pass sieve approach to coreference
can nevertheless be quite unwieldy when one seeks
to integrate new sources of knowledge in order to
improve the resolution performance. Pronoun reso-
lution, for example, was shown by Yang et al (2005)
to benefit from semantic compatibility information
extracted from search engine statistics. The seman-
tic compatibility between candidate antecedents and
the pronoun context induces a new ordering between
the antecedents. One possibility for using compat-
ibility scores in the deterministic system is to ig-
nore the salience-based ordering and replace it with
the new compatibility-based ordering. The draw-
11
back of this simple approach is that now discourse
salience, an important signal in pronoun resolution,
is completely ignored. Ideally, we would want to
use both discourse salience and semantic compat-
ibility when ranking the candidate antecedents of
the pronoun, something that can be achieved natu-
rally in a discriminative learning approach that uses
the two rankings as different, but overlapping, fea-
tures. Consequently, we propose an adaptive cluster-
ing model for coreference in which the expert rules
are successfully supplemented by semantic compat-
ibility features obtained from limited history web n-
gram statistics.
2 A Coreference Resolution Algorithm
From a machine learning perspective, the determin-
istic system of Lee et al (2011) represents a trove
of coreference resolution features. Since the de-
terministic sieves use not only information about a
pair of mentions, but also the clusters to which they
have been assigned so far, a learning model that uti-
lized the sieves as features would need to be able
to work with features defined on pairs of clusters.
We therefore chose to model coreference resolu-
tion as the greedy clustering process shown in Algo-
rithm 1. The algorithm starts by initializing the clus-
tering C with a set of singleton clusters. Then, as
long as the clustering contains more than one clus-
ter, it repeatedly finds the highest scoring pair of
clusters ?Ci, Cj?. If the score passes the threshold
? = f(?, ?), the clusters Ci, Cj are joined into one
cluster and the process continues with another high-
est scoring pair of clusters.
Algorithm 1 CLUSTER(X ,f )
Input: A set of mentions X = {x1, x2, ..., xn};
A measure f(Ci, Cj) = wT?(Ci, Cj).
Output: A greedy agglomerative clustering of X .
1: for i = 1 to n do
2: Ci ? {xi}
3: C ? {Ci}1?i?n
4: ?Ci, Cj? ? argmax
p?P(C)
f(p)
5: while |C| > 1 and f(Ci, Cj) > ? do
6: replace Ci, Cj in C with Ci ? Cj
7: ?Ci, Cj? ? argmax
p?P(C)
f(p)
8: return C
The scoring function f(Ci, Cj) is a linearly
weighted combination of features ?(Ci, Cj) ex-
tracted from the cluster pair, parametrized by a
weight vector w. The function P takes a cluster-
ing C as argument and returns a set of cluster pairs
?Ci, Cj? as follows:
P(C)={?Ci, Cj? | Ci, Cj?C, Ci 6=Cj}?{??, ??}
P(C) contains a special cluster pair ??, ??, where
?(?, ?) is defined to contain a binary feature
uniquely associated with this empty pair. Its cor-
responding weight is learned together with all other
weights and will effectively function as a clustering
threshold ? = f(?, ?).
Algorithm 2 TRAIN(C,T )
Input: A dataset of training clusterings C;
The number of training epochs T .
Output: The averaged parameters w.
1: w? 0
2: for t = 1 to T do
3: for all C ? C do
4: w? UPDATE(C,w)
5: return w
Algorithm 3 UPDATE(C,w)
Input: A gold clustering C = {C1, C2, ..., Cm};
The current parameters w.
Output: The updated parameters w.
1: X ? C1 ? C2 ? ... ? Cm = {x1, x2, ..., xn}
2: for i = 1 to n do
3: C?i ? {xi}
4: C? ? {C?i}1?i?n
5: while |C?| > 1 do
6: ?C?i, C?j? = argmax
p?P (C?)
wT?(p)
7: B ? {?C?k, C?l? ? P(C?) | g(C?k, C?l|C) >
g(C?i, C?j |C)}
8: if B 6= ? then
9: ?C?k, C?l? = argmax
p?B
wT?(p)
10: w? w + ?(C?k, C?l)? ?(Ci, Cj)
11: if ?C?i, C?j? = ??, ?? then
12: return w
13: replace C?i, C?j in C? with C?i ? C?j
14: return w
12
Algorithms 2 and 3 show an incremental learning
model for the weight vector w that is parametrized
with the number of training epochs T and a set of
training clusterings C in which each clustering con-
tains the true coreference clusters from one docu-
ment. Algorithm 2 repeatedly uses all true cluster-
ings to update the current weight vector and instead
of the last computed weights it returns an averaged
weight vector to control for overfitting, as originally
proposed by Freund and Schapire (1999). The core
of the learning model is in the update procedure
shown in Algorithm 3. Like the greedy clustering of
Algorithm 1, it starts with an initial system cluster-
ing C? that contains all singleton clusters. At every
step in the iteration (lines 5?13), it joins the high-
est scoring pair of clusters ?C?i, C?j?, computed ac-
cording to the current parameters. The iteration ends
when either the empty pair obtains the highest score
or everything has been joined into only one cluster.
The weight update logic is implemented in lines 7?
10: if a more accurate pair ?C?k, C?l? can be found,
the highest scoring such pair is used in the percep-
tron update in line 10. If multiple cluster pairs obtain
the maximum score in lines 6 and 9, the algorithm
selects one of them at random. This is useful es-
pecially in the beginning, when the weight vector is
zero and consequently all cluster pairs have the same
score of 0. We define the goodness g(C?k, C?l|C) of a
proposed pair ?C?k, C?l? with respect to the true clus-
teringC as the accuracy of the coreference pairs that
would be created if C?k and C?l were joined:
g(?) =
?
?
?{(x, y)? C?k?C?l | ?Ci?C : x, y?Ci}
?
?
?
|C?k| ? |C?l|
(1)
It can be shown that this definition of the goodness
function selects a cluster pair (lines 7?9) that, when
joined, results in a clustering with a better pairwise
accuracy. Therefore, the algorithm can be seen as
trying to fit the training data by searching for param-
eters that greedily maximize the clustering accuracy,
while overfitting is kept under control by comput-
ing an averaged version of the parameters. We have
chosen to use a perceptron update for simplicity, but
the algorithm can be easily instantiated to accommo-
date other types of incremental updates, e.g. MIRA
(Crammer and Singer, 2003).
3 Expert Rules as Features
With the exception of mention detection which is
run separately, all the remaining 12 sieves men-
tioned in (Lee et al, 2011) are used as Boolean fea-
tures defined on cluster pairs, i.e. if any of the men-
tion pairs in the cluster pair ?C?i, C?j? were linked
by sieve k, then the corresponding sieve feature
?k(C?i, C?j) = 1. We used the implementation from
the Stanford CoreNLP package1 for all sieves, with a
modification for the PRONOUNMATCH sieve which
was split into 3 different sieves as follows:
? ITPRONOUNMATCH: this sieve finds an-
tecedents only for neutral pronouns it.
? ITSPRONOUNMATCH: this sieve finds an-
tecedents only for neutral possessive pronouns
its.
? OTHERPRONOUNMATCH: this is a catch-all
sieve for the remaining pronouns.
This 3-way split was performed in order to enable
the combination of the discourse salience features
captured by the pronoun sieves with the semantic
compatibility features for neutral pronouns that will
be introduced in the next section. The OTHER-
PRONOUNMATCH sieve works exactly as the orig-
inal PRONOUNMATCH: for a given non-neutral pro-
noun, it searches in the current sentence and the pre-
vious 3 sentences for the first mention that agrees in
gender and number with the pronoun. The candi-
date antecedents for the pronoun are ordered based
on a notion of discourse salience that favors syntac-
tic salience and document proximity (Raghunathan
et al, 2010).
4 Discourse Salience Features
The IT/SPRONOUNMATCH sieves use the same im-
plementation for finding the first matching candi-
date antecedent as the original PRONOUNMATCH.
However, unlike OTHERPRONOUNMATCH and the
other sieves that generate Boolean features, the neu-
tral pronoun sieves are used to generate real valued
features. If the neutral pronoun is the leftmost men-
tion in the cluster C?j from a cluster pair ?C?i, C?j?,
the corresponding normalized feature is computed
as follows:
1http://nlp.stanford.edu/software/corenlp.shtml
13
1. Let Sj = ?S1j , S
2
j , ..., S
n
j ? be the sequence
of candidate mentions that precede the neutral
pronoun and agree in gender and number with
it, ordered from most salient to least salient.
2. Let Ai ? C?i be the set of mentions in the clus-
ter C?i that appear before the pronoun and agree
with it.
3. For each mention m ? Ai, find its rank in the
sequence Sj :
rank(m,Sj) = k ? m = S
k
j (2)
4. Find the minimum rank across all the mentions
in Ai and compute the feature as follows:
?it/s(C?i, C?j) =
(
min
m?Ai
rank(m,Sj)
)?1
(3)
If Ai is empty, set ?it/s(C?i, C?j) = 0.
The discourse salience feature described above is by
definition normalized in the interval [0, 1]. It takes
the maximum value of 1 when the most salient men-
tion in the discourse at the current position agrees
with the pronoun and also belongs to the candidate
cluster. The feature is 0 when the candidate cluster
does not contain any mention that agrees in gender
and number with the pronoun.
5 Semantic Compatibility Features
Each of the two types of neutral pronouns is associ-
ated with a new feature that computes the semantic
compatibility between the syntactic head of a candi-
date antecedent and the context of the neutral pro-
noun. If the neutral pronoun is the leftmost mention
in the cluster C?j from a cluster pair ?C?i, C?j? and cj
is the pronoun context, then the new normalized fea-
tures ?it/s(C?i, C?j) are computed as follows:
1. Compute the maximum semantic similarity be-
tween the pronoun context and any mention in
C?i that precedes the pronoun and is in agree-
ment with it:
Mj = max
m?Ai
comp(m, cj)
2. Compute the maximum and minimum seman-
tic similarity between the pronoun context and
any mention that precedes the pronoun and is
in agreement with it:
Mall = max
m?Sj
comp(m, cj)
mall = min
m?Sj
comp(m, cj)
3. Compute the semantic compatibility feature as
follows:
?it/s(C?i, C?j) =
Mj ?mall
Mall ?mall
(4)
To avoid numerical instability, if the over-
all maximum and minimum similarities are
very close (Mall ? mall < 1e?4) we set
?it/s(C?i, C?j) = 1.
Like the salience feature ?it/s, the semantic com-
patibility feature ?it/s is normalized in the interval
[0, 1]. Its definition assumes that we can compute
comp(m, cj), the semantic compatibility between a
candidate antecedent mention m and the pronoun
context cj . For the possessive pronoun its, we ex-
tract the syntactic head h of the mention m and re-
place the pronoun with the mention head h in the
possessive context. We use the resulting possessive
pronoun context pcj(h) to define the semantic com-
patibility as the following conditional probability:
comp(m, cj) = logP (pcj(h)|h) (5)
= logP (pcj(h))? logP (h)
To compute the n-gram probabilities P (pcj(h)) and
P (h) in Equation 6, we use the language mod-
els provided by the Microsoft Web N-Gram Cor-
pus (Wang et al, 2010), as described in the next sec-
tion.
Figure 1 shows an example of a possessive neu-
tral pronoun context, together with the set of can-
didate antecedents that agree in number and gender
with the pronoun, from the current and previous 3
sentences. Each candidate antecedent is given an in-
dex that reflects its ranking in the discourse salience
based ordering. We see that discourse salience does
not help here, as the most salient mention is not
the correct antecedent. The figure also shows the
14
In 1946, the nine justices dismissed a case[7] involving
the apportionment[8] of congressional districts. That
view[6] would slowly change. In 1962, the court[3]
abandoned its[5] caution[4]. Finding remedies to the
unequal distribution[1] of political power[2] was indeed
within its constitutional authority.
[3] P (court?s constitutional authority | court)
? exp(?5.91)
[5] P (court?s constitutional authority | court) (*)
? exp(?5.91)
[7] P (case?s constitutional authority | case)
? exp(?8.32)
[2] P (power?s constitutional authority | power)
? exp(?9.30)
[8] P (app-nt?s constitutional authority | app-nt)
? exp(?9.32)
[4] P (caution?s constitutional authority | caution)
? exp(?9.39)
[1] P (dist-ion?s constitutional authority | dist-ion)
? exp(?9.40)
[6] P (view?s constitutional authority | view)
? exp(?9.69)
Figure 1: Possessive neutral pronoun example.
compatibility score computed for each candidate an-
tecedent, using the formula described above. In this
example, when ranking the candidate antecedents
based on their compatibility scores, the top ranked
mention is the correct antecedent, whereas the most
salient mention is down in the list.
When the set of candidate mentions contains pro-
nouns, we require that they are resolved to a nominal
or named mention, and use the head of this mention
to instantiate the possessive context. This is the case
of the pronominal mention [5] in Figure 1, which
we assumed was already resolved to the noun court
(even if the pronoun [5] were resolved to an incor-
rect mention, the noun court would still be ranked
first due to mention [3]). This partial ordering be-
tween coreference decisions is satisfied automati-
cally by setting the semantic compatibility feature
?it/s(C?i, C?j) = 0 whenever the antecedent cluster
C?i contains only pronouns.
A similar feature is introduced for all neutral
pronouns it appearing in subject-verb-object triples.
The letter[5] appears to be an attempt[6] to calm the
concerns of the current American administration[7]. ?I
confirm my commitment[1] to the points made therein,?
Aristide said in the letter[2], ?confident that they will
help strengthen the ties between our two nations where
democracy[3] and peace[4] will flourish.? Since 1994,
when it sent 20,000 troops to restore Aristide to power,
the administration ...
[7] P (administration sent troops | administration)
? exp(?6.00)
[2] P (letter sent troops | letter)
? exp(?6.57)
[5] P (letter sent troops | letter)
? exp(?6.57)
[4] P (peace sent troops | peace)
? exp(?7.92)
[6] P (attempt sent troops | attempt)
? exp(?8.26)
[3] P (democracy sent troops | democracy)
? exp(?8.30)
[1] P (commitment sent troops | commitment)
? exp(?8.62)
Figure 2: Neutral pronoun example.
The new pronoun context pcj(h) is obtained by
replacing the pronoun it in the subject-verb-object
context cj with the head h of the candidate an-
tecedent mention. Figure 2 shows a neutral pro-
noun context, together with the set of candidate an-
tecedents that agree in number and gender with the
pronoun, from an abridged version of the original
current and previous 3 sentences. Each candidate
antecedent is given an index that reflects its ranking
in the discourse salience based ordering. Discourse
salience does not help here, as the most salient men-
tion is not the correct antecedent. The figure shows
the compatibility score computed for each candidate
antecedent, using Equation 6. In this example, the
top ranked mention in the compatibility based order-
ing is the correct antecedent, whereas the most most
salient mention is at the bottom of the list.
To summarize, in the last two sections we de-
scribed two special features for neutral pronouns:
the discourse salience feature ?it/s and the seman-
tic compatibility feature ?it/s. The two real-valued
15
Candidate mentions Original context N-gram context
capital, store, GE, side, offer with its corporate tentacles reaching GE?s corporate tentacles
AOL, Microsoft, Yahoo, product its substantial customer base AOL?s customer base
regime, Serbia, state, EU, embargo meets its international obligations Serbia?s international obligations
company, secret, internet, FBI it was investigating the incident FBI was investigating the incident
goal, team, realm, NHL, victory something it has not experienced since NHL has experienced
Onvia, line, Nasdaq, rating said Tuesday it will cut jobs Onvia will cut jobs
coalition, government, Italy but it has had more direct exposure Italy has had direct exposure
Pinochet, arrest, Chile, court while it studied a judge ?s explanation court studied the explanation
Table 1: N-gram generation examples.
features are computed at the level of cluster pairs as
described in Equations 3 and 4. Their computation
relies on the mention level rank (Equation 2) and se-
mantic compatibility (Equation 6) respectively.
6 Web-based Language Models
We used the Microsoft Web N-Gram Corpus2 to
compute the pronoun context probability P (pcj(h))
and the candidate head probability P (h). This
corpus provides smoothed back-off language mod-
els that are computed dynamically from N-gram
statistics using the CALM algorithm (Wang and Li,
2009). The N-grams are collected from the tok-
enized versions of the billions of web pages indexed
by the Bing search engine. Separate models have
been created for the document body, the document
title and the anchor text. In our experiments, we
used the April 2010 version of the document body
language models. The number of words in the pro-
noun context and the antecedent head determine the
order of the language models used for estimating the
conditional probabilities. For example, to estimate
P (administration sent troops | administration), we
used a trigram model for the context probability
P (administration sent troops) and a unigram model
for the head probability P (administration). Since
the maximum order of the N-grams available in the
Microsoft corpus is 5, we designed the context and
head extraction rules to return N-grams with size
at most 5. Table 1 shows a number of examples
of N-grams generated from the original contexts, in
which the pronoun was replaced with the correct an-
tecedent. To get a sense of the utility of each con-
text in matching the right antecedent, the table also
2http://web-ngram.research.microsoft.com
shows a sample of candidate antecedents.
For possessive contexts, the N-gram extraction
rules use the head of the NP context and its clos-
est premodifier whenever available. Using the pre-
modifier was meant to increase the discriminative
power of the context. For the subject-verb-object
N-grams, we used the verb at the same tense as in
the original context, which made it necessary to also
include the auxiliary verbs, as shown in lines 4?7 in
the table. Furthermore, in order to keep the gener-
ated N-grams within the maximum size of 5, we did
not include modifiers for the subject or object nouns,
as illustrated in the last line of the table. Some of
the examples in the table also illustrate the limits of
the context-based semantic compatibility feature. In
the second example, all three company names are
equally good matches for the possessive context. In
these situations, we expect the discourse salience
feature to provide the additional information neces-
sary for extracting the correct antecedent. This com-
bination of discourse salience with semantic com-
patibility features is done in the adaptive clustering
algorithm introduced in Section 2.
7 Experimental Results
We compare our adaptive clustering (AC) approach
with the state of the art deterministic sieves (DT)
system of Lee et al (2011) on the newswire portion
of the ACE-2004 dataset. The newswire section of
the corpus contains 128 documents annotated with
gold mentions and coreference information, where
coreference is marked only between mentions that
belong to one of seven semantic classes: person, or-
ganization, location, geo-political entity, facility, ve-
hicle, and weapon. This set of documents has been
used before to evaluate coreference resolution sys-
16
System Mentions P R F1
DT Gold, all 88.1 73.3 80.0
AC Gold, all 88.7 73.5 80.4
DT Gold, neutral 82.5 51.5 63.4
AC Gold, neutral 83.0 52.1 64.0
DT Auto, neutral 84.4 34.9 49.3
AC Auto, neutral 86.1 40.0 54.6
Table 2: B3 comparative results on ACE 2004.
tems in (Poon and Domingos, 2008; Haghighi and
Klein, 2009; Raghunathan et al, 2010), with the best
results so far obtained by the deterministic sieve sys-
tem of Lee at al. (2011). There are 11,398 annotated
gold mentions, out of which 135 are possessive neu-
tral pronouns its and 88 are neutral pronouns it in
a subject-verb-object triple. Given the very small
number of neutral pronouns, in order to obtain re-
liable estimates for the model parameters we tested
the adaptive clustering algorithm in a 16 fold cross-
validation scenario. Thus, the set of 128 documents
was split into 16 folds, where each fold contains 120
documents for training and 8 documents for testing.
The final results were pooled together from the 16
disjoint test sets. During training, the AC?s update
procedure was run for 10 epochs. Since the AC al-
gorithm does not need to tune any hyper parameters,
there was no need for development data.
Table 2 shows the results obtained by the two sys-
tems on the newswire corpus under three evaluation
scenarios. We use the B3 version of the precision
(P), recall (R), and F1 measure, computed either on
all mention pairs (all) or only on links that contain at
least one neutral pronoun (neutral) marked as a men-
tion in ACE. Furthermore, we report results on gold
mentions (Gold) as well as on mentions extracted
automatically (Auto). Since the number of neutral
pronouns marked as gold mentions is small com-
pared to the total number of mentions, the impact
on the overall performance shown in the first two
rows is small. However, when looking at corefer-
ence links that contain at least one neutral pronoun,
the improvement becomes substantial. AC increases
F1 with 5.3% when the mentions are extracted auto-
matically during testing, a setting that reflects a more
realistic use of the system. We have also evaluated
the AC approach in the Gold setting using only the
original DT sieves as features, obtaining an F1 of
80.3% for all mentions and 63.4% ? same as DT ?
for neutral pronouns.
By matching the performance of the DT system in
the first two rows of the table, the AC system proves
that it can successfully learn the relative importance
of the deterministic sieves, which in (Raghunathan
et al, 2010) and (Lee et al, 2011) have been manu-
ally ordered using a separate development dataset.
Furthermore, in the DT system the sieves are ap-
plied on mentions in their textual order, whereas the
adaptive clustering algorithm AC does not assume
a predefined ordering among coreference resolution
decisions. Thus, the algorithm has the capability to
make the first clustering decisions in any section of
the document in which the coreference decisions are
potentially easier to make. We have run experiments
in which the AC system was augmented with a fea-
ture that computed the normalized distance between
a cluster and the beginning of the document, but this
did not lead to an improvement in the results, lend-
ing further credence to the hypothesis that a strictly
left to right ordering of the coreference decisions is
not necessary, at least with the current features.
The same behavior, albeit with smaller increases
in performance, was observed when the DT and AC
approaches were compared on the newswire section
of the development dataset used in the CoNLL 2011
shared task (Pradhan et al, 2011). For these exper-
iments, the AC system was trained on all 128 docu-
ments from the newswire portion of ACE 2004. On
gold mentions, the DT and AC systems obtained a
very similar performance. When evaluated only on
links that contain at least one neutral pronoun, in a
setting where the mentions were automatically de-
tected, the AC approach improved the F1 measure
over the DT system from 58.6% to 59.1%. One rea-
son for the smaller increase in performance in the
CoNLL experiments could be given by the different
annotation schemes used in the two datasets. Com-
pared to ACE, the CoNLL dataset does not include
coreference links for appositives, predicate nomi-
nals or relative pronouns. The different annotation
schemes may have led to mismatches in the training
and test data for the AC system, which was trained
on ACE and tested on CoNLL. While we tried to
control for these conditions during the evaluation
of the AC system, it is conceivable that the differ-
17
System Mentions P R F1
DT Auto, its 86.0 46.9 60.7
AC Auto, its 91.7 47.5 62.6
Table 3: B3 comparative results on CoNLL 2011.
ences in annotation still had some effect on the per-
formance of the AC approach. Another cause for
the smaller increase in performance was that the
pronominal contexts were less discriminative in the
CoNLL data, especially for the neutral pronoun it.
When evaluated only on links that contained at least
one possessive neutral pronoun its, the improvement
in F1 increased at 1.9%, as shown in Table 3.
8 Related Work
Closest to our clustering approach from Section 2
is the error-driven first-order probabilistic model of
Culotta et al (2007). Among significant differences
we mention that our model is non-probabilistic, sim-
pler and easier to understand and implement. Fur-
thermore, the update step does not stop after the
first clustering error, instead the algorithm learns and
uses a clustering threshold ? to determine when to
stop during training and testing. This required the
design of a method to order cluster pairs in which the
clusters may not be consistent with the true coref-
erence chains, which led to the introduction of the
goodness function in Equation 1 as a new scoring
measure for cluster pairs. The strategy of contin-
uing the clustering during training as long as a an
adaptive threshold is met better matches the training
with the testing, and was observed to lead to better
performance. The cluster ranking model of Rahman
and Ng (2009) proceeds in a left-to-right fashion and
adds the current discourse old mention to the highest
scoring preceding cluster. Compared to it, our adap-
tive clustering approach is less constrained: it uses
only a weak, partial ordering between coreference
decisions, and does not require a singleton cluster at
every clustering step. This allows clustering to start
in any section of the document where coreference
decisions are easier to make, and thus create accu-
rate clusters earlier in the process.
The use of semantic knowledge for coreference
resolution has been studied before in a number of
works, among them (Ponzetto and Strube, 2006),
(Bengtson and Roth, 2008), (Lee et al, 2011), and
(Rahman and Ng, 2011). The focus in these studies
has been on the semantic similarity between a men-
tion and a candidate antecedent, or the parallelism
between the semantic role structures in which the
two appear. One of the earliest methods for using
predicate-argument frequencies in pronoun resolu-
tion is that of Dagan and Itai (1990). Closer to our
use of semantic compatibility features for pronouns
are the approaches of Kehler et al (2004) and Yang
et al (2005). The last work showed that pronoun
resolution can be improved by incorporating seman-
tic compatibility features derived from search engine
statistics in the twin-candidate model. In our ap-
proach, we use web-based language models to com-
pute semantic compatibility features for neutral pro-
nouns and show that they can improve performance
over a state-of-the-art coreference resolution system.
The use of language models instead of search engine
statistics is more practical, as they eliminate the la-
tency involved in using search engine queries. Web-
based language models can be built on readily avail-
able web N-gram corpora, such as Google?s Web 1T
5-gram Corpus (Brants and Franz, 2006).
9 Conclusion
We described a novel adaptive clustering method
for coreference resolution and showed that it can
not only learn the relative importance of the origi-
nal expert rules of Lee et al (2011), but also ex-
tend them effectively with new semantic compati-
bility features. Experimental results show that the
new method improves the performance of the state
of the art deterministic system and obtains a sub-
stantial improvement for neutral pronouns when the
mentions are extracted automatically.
Acknowledgments
We would like to thank the anonymous reviewers for
their helpful suggestions. This work was supported
by grant IIS-1018590 from the NSF. Any opinions,
findings, and conclusions or recommendations ex-
pressed in this material are those of the author and
do not necessarily reflect the views of the NSF.
18
References
Eric Bengtson and Dan Roth. 2008. Understanding the
value of features for coreference resolution. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing, pages 294?303, Hon-
olulu, Hawaii, October. Association for Computational
Linguistics.
Thorsten Brants and Alex Franz. 2006. Web 1t 5-gram
version 1.
Koby Crammer and Yoram Singer. 2003. Ultraconser-
vative online algorithms for multiclass problems. J.
Mach. Learn. Res., 3:951?991.
Aron Culotta, Michael Wick, and Andrew McCallum.
2007. First-order probabilistic models for coreference
resolution. In Human Language Technologies 2007:
The Conference of the North American Chapter of the
Association for Computational Linguistics; Proceed-
ings of the Main Conference, pages 81?88, Rochester,
New York, April. Association for Computational Lin-
guistics.
Ido Dagan and Alon Itai. 1990. Automatic processing
of large corpora for the resolution of anaphora refer-
ences. In Proceedings of the 13th conference on Com-
putational linguistics - Volume 3, COLING?90, pages
330?332.
Yoav Freund and Robert E. Schapire. 1999. Large mar-
gin classification using the perceptron algorithm. Ma-
chine Learning, 37:277?296.
Aria Haghighi and Dan Klein. 2009. Simple coreference
resolution with rich syntactic and semantic features.
In Proceedings of the 2009 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1152?1161, Singapore, August.
Andrew Kehler, Douglas Appelt, Lara Taylor, and Alek-
sandr Simma. 2004. The (non)utility of predicate-
argument frequencies for pronoun interpretation. In
HLT-NAACL 2004: Main Proceedings, pages 289?
296, Boston, Massachusetts, USA. Association for
Computational Linguistics.
Heeyoung Lee, Yves Peirsman, Angel Chang, Nathanael
Chambers, Mihai Surdeanu, and Dan Jurafsky. 2011.
Stanford?s multi-pass sieve coreference resolution sys-
tem at the conll-2011 shared task. In Proceedings of
the Fifteenth Conference on Computational Natural
Language Learning: Shared Task, pages 28?34.
Simone Paolo Ponzetto and Michael Strube. 2006. Ex-
ploiting semantic role labeling, wordnet and wikipedia
for coreference resolution. In Proceedings of the Hu-
man Language Technology Conference of the North
American Chapter of the Association of Computa-
tional Linguistics, pages 192?199.
Hoifung Poon and Pedro Domingos. 2008. Joint un-
supervised coreference resolution with markov logic.
In Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, Honolulu,
Hawaii, October.
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen Xue.
2011. Conll-2011 shared task: modeling unrestricted
coreference in ontonotes. In Proceedings of the Fif-
teenth Conference on Computational Natural Lan-
guage Learning: Shared Task, pages 1?27.
Karthik Raghunathan, Heeyoung Lee, Sudarshan Ran-
garajan, Nate Chambers, Mihai Surdeanu, Dan Juraf-
sky, and Christopher D. Manning. 2010. A multi-pass
sieve for coreference resolution. In Proceedings of
the 2010 Conference on Empirical Methods in Natural
Language Processing (EMNLP?10), pages 492?501.
Altaf Rahman and Vincent Ng. 2009. Supervised mod-
els for coreference resolution. In Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing, pages 968?977, Singapore, Au-
gust. Association for Computational Linguistics.
Altaf Rahman and Vincent Ng. 2011. Coreference res-
olution with world knowledge. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies,
pages 814?824, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Kuansan Wang and Xiaolong Li. 2009. Efficacy of a con-
stantly adaptive language modeling technique for web-
scale applications. In Proceedings of the 2009 IEEE
International Conference on Acoustics, Speech and
Signal Processing, ICASSP ?09, pages 4733?4736,
Washington, DC, USA. IEEE Computer Society.
Kuansan Wang, Christopher Thrasher, Evelyne Viegas,
Xiaolong Li, and Bo-june (Paul) Hsu. 2010. An
overview of microsoft web n-gram corpus and appli-
cations. In Proceedings of the NAACL HLT 2010
Demonstration Session, HLT-DEMO ?10, pages 45?
48, Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Xiaofeng Yang, Jian Su, and Chew Lim Tan. 2005. Im-
proving pronoun resolution using statistics-based se-
mantic compatibility information. In Proceedings of
the 43rd Annual Meeting on Association for Computa-
tional Linguistics, pages 165?172.
19
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 30?37,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Towards Building a Multilingual Semantic Network:
Identifying Interlingual Links in Wikipedia
Bharath Dandala
Dept. of Computer Science
University of North Texas
Denton, TX
BharathDandala@my.unt.edu
Rada Mihalcea
Dept. of Computer Science
University of North Texas
Denton, TX
rada@cs.unt.edu
Razvan Bunescu
School of EECS
Ohio University
Athens, Ohio
bunescu@ohio.edu
Abstract
Wikipedia is a Web based, freely available
multilingual encyclopedia, constructed in a
collaborative effort by thousands of contribu-
tors. Wikipedia articles on the same topic in
different languages are connected via interlin-
gual (or translational) links. These links serve
as an excellent resource for obtaining lexical
translations, or building multilingual dictio-
naries and semantic networks. As these links
are manually built, many links are missing
or simply wrong. This paper describes a su-
pervised learning method for generating new
links and detecting existing incorrect links.
Since there is no dataset available to evaluate
the resulting interlingual links, we create our
own gold standard by sampling translational
links from four language pairs using distance
heuristics. We manually annotate the sampled
translation links and used them to evaluate the
output of our method for automatic link detec-
tion and correction.
1 Introduction
In recent years, Wikipedia has been used as a re-
source of world knowledge in many natural lan-
guage processing applications. A diverse set of
tasks such as text categorization, information ex-
traction, information retrieval, question answering,
word sense disambiguation, semantic relatedness,
and named entity recognition have been shown to
benefit from the semi-structured text of Wikipedia.
Most approaches that use the world knowledge en-
coded in Wikipedia are statistical in nature and
therefore their performance depends significantly
on the size of Wikipedia. Currently, the English
Wikipedia alone has four million articles. However,
the combined Wikipedias for all other languages
greatly exceed the English Wikipedia in size, yield-
ing a combined total of more than 10 million arti-
cles in more than 280 languages.1 The rich hyper-
link structure of these Wikipedia corpora in different
languages can be very useful in identifying various
relationships between concepts.
Wikipedia articles on the same topic in different
languages are often connected through interlingual
links. These links are the small navigation links
that show up in the ?Languages? sidebar in most
Wikipedia articles, and they connect an article with
related articles in other languages. For instance,
the interlingual links for the Wikipedia article about
?Football? connect it to 20 articles in 20 different
languages. In the ideal case, a set of articles con-
nected directly or indirectly via such links would all
describe the same entity or concept. However, these
links are produced either by polyglot editors or by
automatic bots. Editors commonly make mistakes
by linking articles that have conceptual drift, or by
linking to a concept at a different level of granularity.
For instance, if a corresponding article in one of the
languages does not exist, a similar article or a more
general article about the concept is sometimes linked
instead. Various bots also add new interlingual links
or attempt to correct existing ones. The downside of
a bot is that an error in a translational link created
by editors in Wikipedia for one language propagates
to Wikipedias in other languages. Thus, if a bot in-
troduces a wrong link, one may have to search for
1http://en.wikipedia.org/wiki/Wikipedia:Size of Wikipedia
30
Language Code Articles Redirects Users
English en 4,674,066 4,805,557 16,503,562
French fr 3,298,615 789,408 1,250,266
German de 3,034,238 678,288 1,398,424
Italian it 2,874,747 319,179 731,750
Polish pl 2,598,797 158,956 481,079
Spanish es 2,587,613 504,062 2,162,925
Dutch nl 2,530,250 226,201 446,458
Russian ru 2,300,769 682,402 819,812
Japanese jp 1,737,565 372,909 607,152
Chinese cn 1,199,912 333,436 1,171,148
Table 1: Number of articles, redirects, and users for the top nine Wikipedia editions plus Chinese. The total number
of articles also includes the disambiguation pages.
the underlying error in a different language version
of Wikipedia.
The contributions of the research described in this
paper are two-fold. First, we describe the construc-
tion of a dataset of interlingual links that are auto-
matically sampled from Wikipedia based on a set of
distance heuristics. This dataset is manually anno-
tated in order to enable the evaluation of methods
for translational link detection. Second, we describe
an automatic model for correcting existing links and
creating new links, with the aim of obtaining a more
stable set of interlingual links. The model?s param-
eters are estimated on the manually labeled dataset
using a supervised machine learning approach.
The remaining of this paper is organized as fol-
lows: Section 2 briefly describes Wikipedia and
the relevant terminology. Section 3 introduces our
method of identifying a candidate set of translational
links based on distance heuristics, while Section 4
introduces the methodology for building a manually
annotated dataset. Section 5 describes the machine
learning experiments for detecting or correcting in-
terlingual links. Finally, we present related work in
Section 6, and concluding remarks in Section 7.
2 Wikipedia
Wikipedia is a free online encyclopedia, represent-
ing the outcome of a continuous collaborative effort
of a large number of volunteer contributors. Virtu-
ally any Internet user can create or edit a Wikipedia
webpage, and this ?freedom of contribution? has a
positive impact on both the quantity (fast-growing
number of articles) and the quality (potential errors
are quickly corrected within the collaborative envi-
ronment) of this online resource.
The basic entry in Wikipedia is an article (or
page), which defines and describes an entity or an
event, and consists of a hypertext document with hy-
perlinks to other pages within or outside Wikipedia.
The role of the hyperlinks is to guide the reader to
pages that provide additional information about the
entities or events mentioned in an article. Articles
are organized into categories, which in turn are or-
ganized into category hierarchies. For instance, the
article automobile is included in the category vehi-
cle, which in turn has a parent category named ma-
chine, and so forth.
Each article in Wikipedia is uniquely referenced
by an identifier, consisting of one or more words
separated by spaces or underscores and occasionally
a parenthetical explanation. For example, the article
for bar with the meaning of ?counter for drinks? has
the unique identifier bar (counter).
Wikipedia editions are available for more than
280 languages, with a number of entries vary-
ing from a few pages to three millions articles or
more per language. Table 1 shows the nine largest
Wikipedias (as of March 2012) and the Chinese
Wikipedia, along with the number of articles and ap-
proximate number of contributors.2
The ten languages mentioned above are also the
languages used in our experiments. Note that Chi-
2http://meta.wikimedia.org/wiki/List of Wikipedias
#Grand Total
31
Relation Exists Via
SYMMETRY
en=Ball de=Ball Yes -
en=Hentriacontane it=Entriacontano No -
TRANSITIVITY
en=Deletion (phonology) fr=Amu??ssement Yes nl=Deletie (taalkunde)
en=Electroplating fr=Galvanoplastie No -
REDIRECTIONS
en=Gun Dog de=Schiesshund Yes de=Jagdhund
en=Ball de=Ball No -
Table 2: Symmetry, transitivity, and redirections in Wikipedia
nese is the twelfth largest Wikipedia, but we decided
to include it at the cost of not covering the tenth
largest Wikipedia (Portuguese), which has close
similarities with other languages already covered
(e.g., French, Italian, Spanish).
Relevant for the work described in this paper are
the interlingual links, which explicitly connect arti-
cles in different languages. For instance, the English
article for bar (unit) is connected, among others, to
the Italian article bar (unita? di misura) and the Pol-
ish article bar (jednostka). On average, about half of
the articles in a Wikipedia version include interlin-
gual links to articles in other languages. The number
of interlingual links per article varies from an aver-
age of five in the English Wikipedia, to ten in the
Spanish Wikipedia, and as many as 23 in the Arabic
Wikipedia.
3 Identifying Interlingual Links in
Wikipedia
The interlingual links connecting Wikipedias in dif-
ferent languages should ideally be symmetric and
transitive. The symmetry property indicates that if
there is an interlingual link A? ? A? between two
articles, one in language ? and one in language ?,
then the reverse link A? ? A? should also exist
in Wikipedia. According to the transitivity property,
the presence of two links A? ? A? and A? ? A?
indicates that the link A? ? A? should also exist
in Wikipedia, where ?, ? and ? are three different
languages. While these properties are intuitive, they
are not always satisfied due to Wikipedia?s editorial
policy that accredits editors with the responsibility
of maintaining the articles. Table 2 shows actual
Link Total number Newly added
type of links links
DL 26,836,572 -
RL 26,836,572 1,277,760
DP2/RP2 25,763,689 853,658
DP3/RP3 23,383,535 693,262
DP4/RP4 21,560,711 548,354
Table 3: Number of links identified in Wikipedia, as di-
rect, symmetric, or transitional links. The number of
newly added links, not known in the previous set of links,
is also indicated (e.g., DP3/RP3 adds 693,262 new links
not found by direct or symmetric links, or by direct or
reverse paths of length two).
cases in Wikipedia where these properties fail due
to missing interlingual links. The table also shows
examples where the editors link an article from one
language to a redirect page in another language.
In order to generate a normalized set of inter-
lingual links between Wikipedias, we replace all the
redirect pages with the corresponding original arti-
cles, so that each concept in a language is repre-
sented by one unique article. We then identify the
following four types of simple interlingual paths be-
tween articles in different languages:
DL: Direct links A? ? A? between two articles.
RL: Reverse links A? ? A? between two articles.
DPk: Direct, simple paths of length k between two
articles.
RPk: Reverse, simple paths of length k between
two articles.
32
Relation Number of paths
DL
en=Ball de=Ball 1
en=Ball it=Palla (sport) 1
en=Ball fr=Boule (solide) 0
de=Ball fr=Ballon (sport) 0
RL
en=Ball de=Ball 1
en=Ball it=Palla(sport) 1
en=Ball fr=Boule (solide) 0
de=Ball fr=Ballon (sport) 0
DP2
en=Ball de=Ball 1
en=Ball it=Palla (sport) 2
en=Ball fr=Boule (solide) 1
de=Ball fr=Ballon (sport) 2
DP3
en=Ball de=Ball 1
en=Ball it=Palla (sport) 0
en=Ball fr=Boule (solide) 1
de=Ball fr=Ballon (sport) 1
DP4
en=Ball de=Ball 0
en=Ball it=Palla (sport) 0
en=Ball fr=Boule (solide) 1
de=Ball fr=Ballon (sport) 0
RP2
en=Ball de=Ball 1
en=Ball it=Palla (sport) 2
en=Ball fr=Boule (solide) 0
de=Ball fr=Ballon (sport) 2
RP3
en=Ball de=Ball 1
en=Ball it=Palla (sport) 0
en=Ball fr=Boule (solide) 0
de=Ball fr=Ballon (sport) 1
RP4
en=Ball de=Ball 0
en=Ball it=Palla (sport) 0
en=Ball fr=Boule (solide) 0
de=Ball fr=Ballon (sport) 0
Table 4: A subset of the direct links, reverse links, and
inferred direct and reverse paths for the graph in Figure 1
en=Ball
de=Ball
it=Palla(sport) fr=Boule(solide)
fr=Ballon(sport)
Figure 1: A small portion of the multilingual Wikipedia
graph.
Figure 1 shows a small portion of the Wikipedia
graph, connecting Wikipedias in four languages:
English, German, Italian, and French. Correspond-
ingly, Table 4 shows a subset of the direct links DL,
reverse links RL, direct translation paths DPk and
reverse translation paths RPk of lengths k = 2, 3, 4
for the graph in the figure.
Using these distance heuristics, we are able to
extract or infer a very large number of interlingual
links. Table 3 shows the number of direct links ex-
tracted from the ten Wikipedias we currently work
with, as well as the number of paths that we add by
enforcing the symmetry and transitivity properties.
4 Manual Evaluation of the Interlingual
Links
The translation links in Wikipedia, whether added
by the Wikipedia editors (direct links), or inferred by
the heuristics described in the previous section, are
not guaranteed for quality. In fact, previous work (de
Melo and Weikum, 2010b) has shown that a large
number of the links created by the Wikipedia users
are incorrect, connecting articles that are not transla-
tions of each other, subsections of articles, or disam-
biguation pages. We have therefore decided to run
a manual annotation study in order to determine the
quality of the interlingual links. The resulting anno-
tation can serve both as a gold standard for evaluat-
ing the quality of predicted links, and as supervision
for a machine learning model that would automati-
cally detect translation links.
33
Language pair 0 1 2 3 4
(English, German) 46 8 29 2 110
(English, Spanish) 22 19 19 13 123
(Italian, French) 30 7 19 7 132
(Spanish, Italian) 21 8 17 13 136
Table 6: Number of annotations on a scale of 0-4 for each
pair of languages
From the large pool of links directly available in
Wikipedia or inferred automatically through sym-
metry and transitivity, we sampled and then man-
ually annotated 195 pairs of articles for each of
four language pairs: (English, German), (English,
Spanish), (Italian, French), and (Spanish, Italian).
The four language pairs were determined based on
the native or near-native knowledge available in the
group of annotators in our research group. The sam-
pling of the article pairs was done such that it cov-
ers all the potentially interesting cases obtained by
combining the heuristics used to identify interlin-
gual links. The left side of Table 5 shows the com-
bination of heuristics used to select the article pairs.
For each such combination, and for each language
pair, we randomly selected 15 articles. Furthermore,
we added 15 randomly selected pairs for the highest
quality combination (Case 1).
For each language pair, the sampled links were
annotated by one human judge, with the exception of
the (English, Spanish) dataset, which was annotated
by two judges so that we could measure the inter-
annotator agreement. The annotators were asked to
check the articles in each link and annotate the link
on a scale from 0 to 4, as follows:
4: Identical concepts that are perfect translations
of each other.
3: Concepts very close in meaning, which are
good translations of each other, but a better
translation for one of the concepts in the pair
also exists. The annotators are not required to
identify a better translation in Wikipedia, they
only have to use their own knowledge of the
language, e.g. ?building? (English) may be a
good translation for ?tore? (Spanish), yet a bet-
ter translation is known to exist.
2: Concepts that are closely related but that are not
translations of each other.
1: Concepts that are remotely related and are not
translations of each other.
0: Completely unrelated concepts or links be-
tween an article and a portion of another arti-
cle.
To determine the quality of the annotations,
we ran an inter-annotator study for the (English-
Spanish) language pair. The two annotators had a
Pearson correlation of 70%, which indicates good
agreement. We also calculated their agreement
when grouping the ratings from 0 to 4 in only two
categories: 0, 1, and 2 were mapped to no transla-
tion, whereas 3 and 4 were mapped to translation.
On this coarse scale, the annotators agreed 84% of
the time, with a kappa value of 0.61, which once
again indicate good agreement.
The annotations are summarized in the right side
of Table 5. For each quality rating, the table shows
the number of links annotated with that rating. Note
that this is a summary over the annotations of five
annotators, corresponding to the four language pairs,
as well as an additional annotation for (English,
Spanish).
Not surprisingly, the links that are ?supported? by
all the heuristics considered (Case 1) are the links
with the highest quality. These are interlingual links
that are present in Wikipedia and that can also be
inferred through transitive path heuristics. Interest-
ingly, links that are only guaranteed to have a direct
link (DL) and no reverse link (RL) (Case 2) have a
rather low quality, with only 68% of the links being
considered to represent a perfect or a good transla-
tion (score of 3 or 4).
Table 6 summarizes the annotations per language
pair. There appear to be some differences in the
quality of interlingual links extracted or inferred for
different languages, with (Spanish, Italian) being the
pair with the highest quality of links (76% of the
links are either perfect or good translations), while
English to German seems to have the lowest quality
(only 57% of the links are perfect or good). For the
(English, Spanish) pair, we used the average of the
two annotators? ratings, rounded up to the nearest
integer.
34
Combinations of heuristics to extract or infer interlingual links Link quality on a 0-4 scale
Cases DL RL DP2 RP2 DP3 RP3 DP4 RP4 Samples 0 1 2 3 4
Case 1 y y y y y y y y 30 6 3 6 6 129
Case 2 y n - - - - - - 15 15 3 6 3 48
Case 3 n y - - - - - - 15 13 3 8 4 47
Case 4 n n y y - - - - 15 6 3 16 4 46
Case 5 n n - - y y - - 15 13 9 12 4 28
Case 6 n n - - - - y y 15 15 8 3 8 37
Case 7 n n n n - - - - 15 19 8 11 5 31
Case 8 n n - - n n - - 15 13 8 11 5 32
Case 9 n n - - - - n n 15 25 4 11 2 33
Case 10 y y n n - - - - 15 6 3 4 3 59
Case 11 y y - - n n - - 15 6 2 3 0 64
Case 12 y y - - - - n n 15 3 6 2 4 60
Table 5: Left side of the table: distance heuristics and number of samples based on each distance heuristic. ?y? indicates
that the corresponding path should exist, ?n? indicates that the corresponding path should not exist, ?-? indicates that
we don?t care whether the corresponding path exists or not. Right side of the table: manual annotations of the quality
of links, on a scale of 0 to 4, with 4 meaning perfect translations.
5 Machine Learning Experiments
The manual annotations described above are good
indicators of the quality of the interlingual links that
can be extracted and inferred in Wikipedia. But such
manual annotations, because of the human effort in-
volved, do not scale up, and therefore we cannot ap-
ply them on the entire interlingual Wikipedia graph
to determine the links that should be preserved or the
ones that should be removed.
Instead, we experiment with training machine
learning models that would automatically determine
the quality of an interlingual link. As features, we
use the presence or absence of direct or symmet-
ric links, along with the number of inferred paths of
length k = 2, 3, 4, as defined in Section 3. Table 7
shows the feature vectors for the same four pairs of
articles that were used in Table 4. The feature val-
ues are computed based on the sample network of
interlingual links from Figure 1. Each feature vector
is assigned a numerical class, corresponding to the
manual annotation provided by the human judges.
We conduct two experiments, at a fine-grained
and a coarse-grained level. In both experiments, we
use all the annotations for all four language pairs to-
gether (i.e., a total of 780 examples), and perform
evaluations in a ten-fold cross validation scenario.
For the fine-grained experiments, we use all five
numerical classes in a linear regression model.3 We
determine the correctness of the predictions on the
test data by calculating the Pearson correlation with
respect to the gold standard. The resulting corre-
lation was measured at 0.461. For comparison, we
also run an experiment where we only keep the pres-
ence or absence of the direct links as a feature (DL).
In this case, the correlation was measured at 0.418,
which is substantially below the correlation obtained
when using all the features. This indicates that the
interlingual links inferred through our heuristics are
indeed useful.
In the coarse-grained experiments, the quality rat-
ings 0, 1, and 2 are mapped to the no translation
label, while ratings 3 and 4 are mapped to the trans-
lation label. We used the Ada Boost classifier with
decision stumps as the binary classification algo-
rithm. When using the entire feature vectors, the
accuracy is measured at 73.97%, whereas the use
of only the direct links results in an accuracy of
69.35%. Similar to the fine-grained linear regres-
sion experiments, these coarse-grained experiments
further validate the utility of the interlingual links
inferred through the transitive path heuristics.
3We use the Weka machine learning toolkit.
35
Concept pair DL RL DP2 DP3 DP4 RP2 RP3 RP4 Class
en=Ball de=Ball 1 1 1 1 0 1 1 0 4
en=Ball it=Palla (sport) 1 1 2 0 0 2 0 0 4
en=Ball fr=Boule (solide) 0 0 1 1 1 0 0 0 1
de=Ball fr=Ballon (sport) 0 0 2 1 0 2 1 0 4
Table 7: Examples of feature vectors generated for four interlingual links, corresponding to the concept pairs listed in
Table 4
6 Related Work
The multilingual nature of Wikipedia has been al-
ready exploited to solve several number of language
processing tasks. A number of projects have used
Wikipedia to build a multilingual semantic knowl-
edge base by using the existing multilingual nature
of Wikipedia. For instance, (Ponzetto and Strube,
2007) derived a large scale taxonomy from the ex-
isting Wikipedia. In related work, (de Melo and
Weikum, 2010a) worked on a similar problem in
which they combined all the existing multilingual
Wikipedias to build a stable, large multilingual tax-
onomy.
The interlingual links have also been used for
cross-lingual information retrieval (Nguyen et al,
2009) or to generate bilingual parallel corpora (Mo-
hammadi and QasemAghaee, 2010). (Ni et al,
2011) used multilingual editions of Wikipedia to
mine topics for the task of cross lingual text clas-
sification, while (Hassan and Mihalcea, 2009) used
Wikipedias in different languages to measure cross-
lingual semantic relatedness between concepts and
texts in different languages. (Bharadwaj et al, 2010)
explored the use of the multilingual links to mine
dictionaries for under-resourced languages. They
developed an iterative approach to construct a par-
allel corpus, using the interlingual links, info boxes,
category pages, and abstracts, which they then be
used to extract a bilingual dictionary. (Navigli and
Ponzetto, 2010) explored the connections that can
be drawn between Wikipedia and WordNet. While
no attempts were made to complete the existing link
structure of Wikipedia, the authors made use of ma-
chine translation to enrich the resource.
The two previous works most closely related to
ours are the systems introduced in (Sorg and Cimi-
ano, 2008) and (de Melo and Weikum, 2010a; de
Melo and Weikum, 2010b). (Sorg and Cimiano,
2008) designed a system that predicts new interlin-
gual links by using a classification based approach.
They extract certain types of links from bilingual
Wikipedias, which are then used to create a set of
features for the machine learning system. In follow-
up work, (Erdmann et al, 2008; Erdmann et al,
2009) used an expanded set of features, which also
accounted for direct links, redirects, and links be-
tween articles in Wikipedia, to identify entries for a
bilingual dictionary. In this line of work, the focus is
mainly on article content analysis, as a way to detect
new potential translations, rather than link analysis
as done in our work.
Finally, (de Melo and Weikum, 2010b) designed
a system that detects errors in the existing interlin-
gual links in Wikipedia. They show that there are a
large number of links that are imprecise or wrong,
and propose the use of a weighted graph to produce
a more consistent set of consistent interlingual links.
Their work is focusing primarily on correcting ex-
isting links in Wikipedia, rather than inferring new
links as we do.
7 Conclusions
In this paper, we explored the identification of trans-
lational links in Wikipedia. By using a set of heuris-
tics that extract and infer links between Wikipedias
in different languages, along with a machine learn-
ing algorithm that builds upon these heuristics to
determine the quality of the interlingual links, we
showed that we can both correct existing transla-
tional links in Wikipedia as well as discover new
interlingual links. Additionally, we have also con-
structed a manually annotated dataset of interlingual
links, covering different types of links in four pairs
of languages, which can serve as a gold standard for
evaluating the quality of predicted links, and as su-
pervision for the machine learning model.
36
In future work, we plan to experiment with ad-
ditional features to enhance the performance of the
classifier. In particular, we would like to also include
content-based features, such as content overlap and
interlinking.
The collection of interlingual links for the ten
Wikipedias considered in this work, as well as the
manually annotated dataset are publicly available at
http://lit.csci.unt.edu.
Acknowledgments
This material is based in part upon work sup-
ported by the National Science Foundation IIS
awards #1018613 and #1018590 and CAREER
award #0747340. Any opinions, findings, and con-
clusions or recommendations expressed in this ma-
terial are those of the authors and do not necessarily
reflect the views of the National Science Foundation.
References
G.R. Bharadwaj, N. Tandon, and V. Varma. 2010.
An iterative approach to extract dictionaries from
Wikipedia for under-resourced languages. Kharagpur,
India.
G. de Melo and G. Weikum. 2010a. MENTA: induc-
ing multilingual taxonomies from Wikipedia. In Pro-
ceedings of the 19th ACM international conference on
Information and knowledge management, pages 1099?
1108, New York, NY, USA. ACM.
G. de Melo and G. Weikum. 2010b. Untangling the
cross-lingual link structure of Wikipedia. In Proceed-
ings of the 48th Annual Meeting of the Association for
Computational Linguistics, pages 844?853, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
M. Erdmann, K. Nakayama, T. Hara, and S. Nishio.
2008. An approach for extracting bilingual terminol-
ogy from Wikipedia. In Proceedings of the 13th In-
ternational Conference on Database Systems for Ad-
vanced Applications.
M. Erdmann, K. Nakayama, T. Hara, and S. Nishio.
2009. Improving the extraction of bilingual termi-
nology from Wikipedia. ACM Transactions on Multi-
media Computing, Communications and Applications,
5(4):31:1?31:17.
S. Hassan and R. Mihalcea. 2009. Cross-lingual seman-
tic relatedness using encyclopedic knowledge. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing (EMNLP), Suntec, Sin-
gapore.
M. Mohammadi and N. QasemAghaee. 2010. Build-
ing bilingual parallel corpora based on Wikipedia. In-
ternational Conference on Computer Engineering and
Applications, 2:264?268.
R. Navigli and S. Ponzetto. 2010. Babelnet: Building a
very large multilingual semantic network. In Proceed-
ings of the 48th Annual Meeting of the Association for
Computational Linguistics, Uppsala, Sweden.
D. Nguyen, A. Overwijk, C. Hauff, D. Trieschnigg,
D. Hiemstra, and F. De Jong. 2009. WikiTrans-
late: query translation for cross-lingual information re-
trieval using only Wikipedia. In Proceedings of the
9th Cross-language evaluation forum conference on
Evaluating systems for multilingual and multimodal
information access, pages 58?65, Berlin, Heidelberg.
Springer-Verlag.
X. Ni, J. Sun, J. Hu, and Z. Chen. 2011. Cross lingual
text classification by mining multilingual topics from
Wikipedia. In Proceedings of the fourth ACM inter-
national conference on Web search and data mining,
pages 375?384, New York, NY, USA. ACM.
S. Ponzetto and M. Strube. 2007. Deriving a large scale
taxonomy from Wikipedia. In Proceedings of the 22nd
national conference on Artificial intelligence - Volume
2, pages 1440?1445. AAAI Press.
P. Sorg and P. Cimiano. 2008. Enriching the crosslingual
link structure of Wikipedia - a classification-based ap-
proach. In Proceedings of the AAAI 2008 Workshop
on Wikipedia and Artificial Intelligence.
37
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 22?31, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
Coarse to Fine Grained Sense Disambiguation in Wikipedia
Hui Shen
School of EECS
Ohio University
Athens, OH 45701, USA
hui.shen.1@ohio.edu
Razvan Bunescu
School of EECS
Ohio University
Athens, OH 45701, USA
bunescu@ohio.edu
Rada Mihalcea
Department of CSE
University of North Texas
Denton, TX 76203, USA
rada@cs.unt.edu
Abstract
Wikipedia articles are annotated by volunteer
contributors with numerous links that connect
words and phrases to relevant titles. Links
to general senses of a word are used concur-
rently with links to more specific senses, with-
out being distinguished explicitly. We present
an approach to training coarse to fine grained
sense disambiguation systems in the presence
of such annotation inconsistencies. Experi-
mental results show that accounting for anno-
tation ambiguity in Wikipedia links leads to
significant improvements in disambiguation.
1 Introduction and Motivation
The vast amount of world knowledge available in
Wikipedia has been shown to benefit many types
of text processing tasks, such as coreference res-
olution (Ponzetto and Strube, 2006; Haghighi and
Klein, 2009; Bryl et al, 2010; Rahman and Ng,
2011), information retrieval (Milne, 2007; Li et al,
2007; Potthast et al, 2008; Cimiano et al, 2009),
or question answering (Ahn et al, 2004; Kaisser,
2008; Ferrucci et al, 2010). In particular, the user
contributed link structure of Wikipedia has been
shown to provide useful supervision for training
named entity disambiguation (Bunescu and Pasca,
2006; Cucerzan, 2007) and word sense disambigua-
tion (Mihalcea, 2007; Ponzetto and Navigli, 2010)
systems. Articles in Wikipedia often contain men-
tions of concepts or entities that already have a cor-
responding article. When contributing authors men-
tion an existing Wikipedia entity inside an article,
they are required to link at least its first mention to
the corresponding article, by using links or piped
links. Consider, for example, the following Wiki
source annotations: The [[capital city|capital]] of
Georgia is [[Atlanta]]. The bracketed strings iden-
tify the title of the Wikipedia articles that describe
the corresponding named entities. If the editor wants
a different string displayed in the rendered text, then
the alternative string is included in a piped link, af-
ter the title string. Based on these Wiki processing
rules, the text that is rendered for the aforementioned
example is: The capital of Georgia is Atlanta.
Since many words and names mentioned in
Wikipedia articles are inherently ambiguous, their
corresponding links can be seen as a useful source
of supervision for training named entity and word
sense disambiguation systems. For example,
Wikipedia contains articles that describe possible
senses of the word ?capital?, such as CAPITAL CITY,
CAPITAL (ECONOMICS), FINANCIAL CAPITAL, or
HUMAN CAPITAL, to name only a few. When dis-
ambiguating a word or a phrase in Wikipedia, a con-
tributor uses the context to determine the appropriate
Wikipedia title to include in the link. In the exam-
ple above, the editor of the article determined that
the word ?capital? was mentioned with the political
center meaning, consequently it was mapped to the
article CAPITAL CITY through a piped link.
In order to useWikipedia links for training aWSD
system for a given word, one needs first to define a
sense repository that specifies the possible meanings
for that word, and then use the Wikipedia links to
create training examples for each sense in the repos-
itory. This approach might be implemented using
the following sequence of steps:
22
In global climate models, the state and properties of the [[atmosphere]] are specified at a number of discrete locations
General = ATMOSPHERE; Specific = ATMOSPHERE OF EARTH; Label = A ? A(S) ? AE
The principal natural phenomena that contribute gases to the [[Atmosphere of Earth|atmosphere]] are emissions from volcanoes
General = ATMOSPHERE; Specific = ATMOSPHERE OF EARTH; Label = A ? A(S) ? AE
An aerogravity assist is a spacecraft maneuver designed to change velocity when arriving at a body with an [[atmosphere]]
General = ATMOSPHERE; Specific = ATMOSPHERE ? generic; Label = A ? A(G)
Assuming the planet?s [[atmosphere]] is close to equilibrium, it is predicted that 55 Cancri d is covered with water clouds
General = ATMOSPHERE; Specific = ATMOSPHERE OF CANCRI ? missing; A ? A(G)
Figure 1: Coarse and fine grained sense annotations in Wikipedia (bold). The proposed hierarchical Label (right).
A(S) = ATMOSPHERE (S), A(G) = ATMOSPHERE (G), A = ATMOSPHERE, AE = ATMOSPHERE OF EARTH.
1. Collect all Wikipedia titles that are linked from
the ambiguous anchor word.
2. Create a repository of senses from all titles that
have sufficient support in Wikipedia i.e., titles
that are referenced at least a predefined min-
imum number of times using the ambiguous
word as anchor.
3. Use the links extracted for each sense in the
repository as labeled examples for that sense
and train a WSD model to distinguish between
alternative senses of the ambiguous word.
Taking the word ?atmosphere? as an example, the
first step would result in a wide array of titles,
ranging from the general ATMOSPHERE and its in-
stantiations ATMOSPHERE OF EARTH or ATMO-
SPHERE OF MARS, to titles as diverse as ATMO-
SPHERE (UNIT), MOOD (PSYCHOLOGY), or AT-
MOSPHERE (MUSIC GROUP). In the second step,
the most frequent titles for the anchor word ?at-
mosphere? would be assembled into a repository R
= {ATMOSPHERE, ATMOSPHERE OF EARTH, AT-
MOSPHERE OF MARS, ATMOSPHERE OF VENUS,
STELLAR ATMOSPHERE, ATMOSPHERE (UNIT),
ATMOSPHERE (MUSIC GROUP)}. The classifier
trained in the third step would use features ex-
tracted from the context to discriminate between
word senses.
This Wikipedia-based approach to creating train-
ing data for word sense disambiguation has a ma-
jor shortcoming. Many of the training examples ex-
tracted for the title ATMOSPHERE could very well
belong to more specific titles such as ATMOSPHERE
OF EARTH or ATMOSPHERE OF MARS. Whenever
the word ?atmosphere? is used in a context with the
sense of ?a layer of gases that may surround a ma-
terial body of sufficient mass, and that is held in
place by the gravity of the body,? the contributor
has the option of adding a link either to the title AT-
MOSPHERE that describes this general sense of the
word, or to the title of an article that describes the
atmosphere of the actual celestial body that is re-
ferred in that particular context, as shown in the first
2 examples in Figure 1. As shown in bold in Fig-
ure 1, different occurrences of the same word may
be tagged with either a general or a specific link, an
ambiguity that is pervasive in Wikipedia for words
like ?atmosphere? that have general senses that sub-
sume multiple, popular specific senses. There does
not seem to be a clear, general rule underlying the
decision to tag a word or a phrase with a general
or specific sense link in Wikipedia. We hypothesize
that, in some cases, editors may be unaware that an
article exists in Wikipedia for the actual reference
of a word or for a more specific sense of the word,
and therefore they end up using a link to an article
describing the general sense of the word. There is
also the possibility that more specific articles are in-
troduced only in newer versions of Wikipedia, and
thus earlier annotations were not aware of these re-
cent articles. Furthermore, since annotating words
with the most specific sense available in Wikipedia
may require substantial cognitive effort, editors may
often choose to link to a general sense of the word, a
choice that is still correct, yet less informative than
the more specific sense.
2 Annotation Inconsistencies in Wikipedia
In order to get a sense of the potential magnitude
of the general vs. specific sense annotation ambi-
guity, we extracted all Wikipedia link annotations
23
for the words ?atmosphere?, ?president?, ?game?,
?dollar?, ?diamond? and ?Corinth?, and created
a special subset from those that were labeled by
Wikipedia editors with the general sense links AT-
MOSPHERE, PRESIDENT, GAME, DOLLAR, DIA-
MOND, and CORINTH, respectively. Then, for each
of the 7,079 links in this set, we used the context
to manually determine the corresponding more spe-
cific title, whenever such a title exists in Wikipedia.
The statistics in Tables 1 and 2 show a significant
overlap between the general and specific sense cate-
gories. For example, out of the 932 links from ?at-
mosphere? to ATMOSPHERE that were extracted in
total, 518 were actually about the ATMOSPHERE OF
EARTH, but the user linked them to the more general
sense category ATMOSPHERE. On the other hand,
there are 345 links to ATMOSPHERE OF EARTH that
were explicitly made by the user. We manually as-
signed general links (G) whenever the word is used
with a generic sense, or when the reference is not
available in the repository of titles collected for that
word because either the more specific title does not
exist in Wikipedia or the specific title exists, but it
does not have sufficient support ? at least 20 linked
anchors ? in Wikipedia. We grouped the more spe-
cific links for any given sense into a special cate-
gory suffixed with (S), to distinguish them from the
general links (generic use, or missing reference) that
were grouped into the category suffixed with (G).
For many ambiguous words, the annotation in-
consistencies appear when the word has senses
that are in a subsumption relationship: the ATMO-
SPHERE OF EARTH is an instance of ATMOSPHERE,
whereas a STELLAR ATMOSPHERE is a particular
type of ATMOSPHERE. Subsumed senses can be
identified automatically using the category graph in
Wikipedia. The word ?Corinth? is an interesting
case: the subsumption relationship between AN-
CIENT CORINTH and CORINTH appears because of
a temporal constraint. Furthermore, in the case of
the word ?diamond?, the annotation inconsistencies
are not caused by a subsumption relation between
senses. Instead of linking to the DIAMOND (GEM-
STONE) sense, Wikipedia contributors often link to
the related DIAMOND sense indicating the mineral
used in the gemstone.
A supervised learning algorithm that uses the ex-
tracted links for training aWSD classification model
atmosphere Size
ATMOSPHERE 932
Atmosphere (S) 559
Atmosphere of Earth 518
Atmosphere of Mars 19
Atmosphere of Venus 9
Stellar Atmosphere 13
Atmosphere (G) 373
ATMOSPHERE OF EARTH 345
ATMOSPHERE OF MARS 37
ATMOSPHERE OF VENUS 26
STELLAR ATMOSPHERE 29
ATMOSPHERE (UNIT) 96
ATMOSPHERE (MUSIC GROUP) 104
president Size
PRESIDENT 3534
President (S) 989
Chancellor (education) 326
President of the United States 534
President of the Philippines 42
President of Pakistan 27
President of France 22
President of India 21
President of Russia 17
President (G) 2545
CHANCELLOR (EDUCATION) 210
PRESIDENT OF THE UNITED STATES 5941
PRESIDENT OF THE PHILIPPINES 549
PRESIDENT OF PAKISTAN 192
PRESIDENT OF FRANCE 151
PRESIDENT OF INDIA 86
PRESIDENT OF RUSSIA 101
Table 1: Wiki (CAPS) and manual (italics) annotations.
to distinguish between categories in the sense repos-
itory assumes implicitly that the categories, and
hence their training examples, are mutually disjoint.
This assumption is clearly violated for words like
?atmosphere,? consequently the learned model will
have a poor performance on distinguishing between
the overlapping categories. Alternatively, we can
say that sense categories like ATMOSPHERE are ill
defined, since their supporting dataset contains ex-
amples that could also belong to more specific sense
categories such as ATMOSPHERE OF EARTH.
We see two possible solutions to the problem of
inconsistent link annotations. In one solution, spe-
cific senses are grouped together with the subsuming
general sense, such that all categories in the result-
ing repository become disjoint. For ?atmosphere?,
the general category ATMOSPHERE would be aug-
mented to contain all the links previously annotated
24
dollar Size
DOLLAR 379
Dollar (S) 231
United States dollar 228
Canadian dollar 3
Australian dollar 1
Dollar (G) 147
UNITED STATES DOLLAR 3516
CANADIAN DOLLAR 420
AUSTRALIAN DOLLAR 124
DOLLAR SIGN 290
DOLLAR (BAND) 30
DOLLAR, CLACKMANNANSHIRE 30
game Size
GAME 819
Game (S) 99
Video game 55
PC game 44
Game (G) 720
VIDEO GAME 312
PC GAME 24
GAME (FOOD) 232
GAME (RAPPER) 154
diamond Size
DIAMOND 716
Diamond (S) 221
Diamond (gemstone) 221
Diamond (G) 495
DIAMOND (GEMSTONE) 71
BASEBALL FIELD 36
MUSIC RECORDING SALES CERT. 36
Corinth Size
CORINTH 699
Corinth (S) 409
Ancient Corinth 409
Corinth (G) 290
ANCIENT CORINTH 92
CORINTH, MISSISSIPPI 72
Table 2: Wiki (CAPS) and manual (italics) annotations.
as ATMOSPHERE, ATMOSPHERE OF EARTH, AT-
MOSPHERE OF MARS, ATMOSPHERE OF VENUS,
or STELLAR ATMOSPHERE. This solution is
straightforward to implement, however it has the
disadvantage that the resulting WSD model will
never link words to more specific titles in Wikipedia
like ATMOSPHERE OF MARS.
Another solution is to reorganize the original
sense repository into a hierarchical classification
scheme such that sense categories at each classifi-
cation level become mutually disjoint. The resulting
WSD system has the advantage that it can make fine
grained sense distinctions for an ambiguous word,
despite the annotation inconsistencies present in the
training data. The rest of this paper describes a feasi-
ble implementation for this second solution that does
not require any manual annotation beyond the links
that are already provided by Wikipedia volunteers.
3 Learning for Coarse to Fine Grained
Sense Disambiguation
Figure 2 shows our proposed hierarchical classifica-
tion scheme for disambiguation, using ?atmosphere?
as the ambiguous word. Shaded leaf nodes show
the final categories in the sense repository for each
word, whereas the doted elliptical frames on the
second level in the hierarchy denote artificial cate-
gories introduced to enable a finer grained classifi-
cation into more specific senses. Thick dotted ar-
rows illustrate the classification decisions that are
made in order to obtain a fine grained disambigua-
tion of the word. Thus, the word ?atmosphere?
is first classified to have the general sense ATMO-
SPHERE, i.e. ?a layer of gases that may surround a
material body of sufficient mass, and that is held in
place by the gravity of the body?. In the first so-
lution, the disambiguation process would stop here
and output the general sense ATMOSPHERE. In the
second solution, the disambiguation process contin-
ues and further classifies the word to be a reference
to ATMOSPHERE OF EARTH. To get to this final
classification, the process passes through an inter-
mediate binary classification level where it deter-
mines whether the word has a more specific sense
covered in Wikipedia, corresponding to the artificial
category ATMOSPHERE (S). If the answer is no, the
system stops the disambiguation process and out-
puts the general sense category ATMOSPHERE. This
basic sense hierarchy can be replicated depending
on the existence of even finer sense distinctions in
Wikipedia. For example, Wikipedia articles describ-
ing atmospheres of particular stars could be used to
further refine STELLAR ATMOSPHERE with two ad-
ditional levels of the type Level 2 and Level 3. Over-
all, the proposed disambiguation scheme could be
used to relabel the ATMOSPHERE links in Wikipedia
with more specific, and therefore more informative,
senses such as ATMOSPHERE OF EARTH. In gen-
eral, the Wikipedia category graph could be used
to automatically create hierarchical structures for re-
25
Figure 2: Hierarchical disambiguation scheme, from coarse to fine grained senses.
lated senses of the same word.
Training word sense classifiers for Levels 1 and 3
is straightforward. For Level 1, Wikipedia links that
are annotated by users as ATMOSPHERE, ATMO-
SPHERE OF EARTH, ATMOSPHERE OF MARS, AT-
MOSPHERE OF VENUS, or STELLAR ATMOSPHERE
are collected as training examples for the general
sense category ATMOSPHERE. Similarly, links that
are annotated as ATMOSPHERE (UNIT) and ATMO-
SPHERE (MUSIC GROUP) will be used as training
examples for the two categories, respectively. A
multiclass classifier is then trained to distinguish be-
tween the three categories at this level. For Level 3,
a multiclass classifiers is trained on Wikipedia links
collected for each of the 4 specific senses.
For the binary classifier at Level 2, we could
use as training examples for the category ATMO-
SPHERE (G) all Wikipedia links that were anno-
tated as ATMOSPHERE, whereas for the category
ATMOSPHERE (S) we could use as training exam-
ples all Wikipedia links that were annotated specif-
ically as ATMOSPHERE OF EARTH, ATMOSPHERE
OF MARS, ATMOSPHERE OF VENUS, or STELLAR
ATMOSPHERE. A traditional binary classification
SVM could be trained on this dataset to distinguish
between the two categories. We call this approach
Naive SVM, since it does not account for the fact that
a significant number of the links that are annotated
by Wikipedia contributors as ATMOSPHERE should
actually belong to the ATMOSPHERE (S) category ?
about 60% of them, according to Table 1. Instead,
we propose treating all ATMOSPHERE links as unla-
beled examples. If we consider the specific links in
ATMOSPHERE (S) to be positive examples, then the
problem becomes one of learning with positive and
unlabeled examples.
3.1 Learning with positive and unlabeled
examples
This general type of semi-supervised learning has
been studied before in the context of tasks such
as text classification and information retrieval (Lee
and Liu, 2003; Liu et al, 2003), or bioinformat-
ics (Elkan and Noto, 2008; Noto et al, 2008). In
this setting, the training data consists of positive ex-
amples x ? P and unlabeled examples x ? U .
Following the notation of Elkan and Noto (2008),
we define s(x) = 1 if the example is positive and
s(x) = ?1 if the example is unlabeled. The true
label of an example is y(x) = 1 if the example
is positive and y(x) = ?1 if the example is neg-
ative. Thus, x ? P ? s(x) = y(x) = 1 and
x ? U ? s(x) = ?1 i.e., the true label y(x) of an
unlabeled example is unknown. For the experiments
reported in this paper, we use our implementation
of two state-of-the-art approaches to Learning with
Positive and Unlabeled (LPU) examples: the Biased
SVM formulation of Lee and Liu (2003) and the
Weighted Samples SVM formulation of Elkan and
Noto (2008). The original version of Biased SVM
was designed to maximize the product between pre-
cision and recall. In the next section we describe a
26
modification to the Biased SVM approach that can
be used to maximize accuracy, a measure that is of-
ten used to evaluate WSD performance.
3.1.1 The Biased SVM
In the Biased SVM formulation (Lee and Liu,
2003; Liu et al, 2003), all unlabeled examples are
considered to be negative and the decision function
f(x) = wT?(x) + b is learned using the standard
soft-margin SVM formulation shown in Figure 3.
minimize: 12?w?
2 + CP
?
x?P
?x + CU
?
x?U
?x
subject to: s(x) (wT?(x) + b) ? 1? ?x
?x ? 0, ?x ? P ? U
Figure 3: Biased SVM optimization problem.
The capacity parameters CP and CU control how
much we penalize errors on positive examples vs. er-
rors on unlabeled examples. Since not all unlabeled
examples are negative, one would want to select ca-
pacity parameters satisfying CP > CU , such that
false negative errors are penalized more than false
positive errors. In order to find the best capacity pa-
rameters to use during training, the Biased SVM ap-
proach runs a grid search on a separate development
dataset. This search is aimed at finding values for
the parameters CP and CU that maximize pr, the
product between precision p = p(y = 1|f = 1) and
recall r = p(f = 1|y = 1). Lee and Liu (2003)
show that maximizing the pr criterion is equivalent
with maximizing the objective r2/p(f = 1), where
both r = p(f = 1|y = 1) and p(f = 1) can be es-
timated using the trained decision function f(x) on
the development dataset.
Maximizing the pr criterion in the original Biased
SVM formulation was motivated by the need to opti-
mize the F measure in information retrieval settings,
where F = 2pr(p+ r). In the rest of this section we
show that classification accuracy can be maximized
using only positive and unlabeled examples, an im-
portant result for problems where classification ac-
curacy is the target performance measure.
The accuracy of a binary decision function f(x)
is, by definition, acc = p(f = 1|y = 1) + p(f =
?1|y = ?1). Since the recall is r = p(f = 1|y =
1), the accuracy can be re-written as:
acc = r + 1? p(f = 1|y = ?1) (1)
Using Bayes? rule twice, the false positive term
p(f = 1|y = ?1) can be re-written as:
p(f = 1|y = ?1) = p(f = 1)p(y = ?1|f = 1)p(y = ?1)
= p(f = 1)p(y = ?1) ? (1? p(y = 1|f = 1))
= p(f = 1)p(y = ?1) ?
p(f = 1)
p(y = ?1) ?
p(y = 1)p(f = 1|y = 1)
p(f = 1)
= p(f = 1)? p(y = 1)? rp(y = ?1) (2)
Plugging identity 2 in Equation 1 leads to:
acc = 1 + r + r ? p(y = 1)? p(f = 1)p(y = ?1)
= 1 + r ? p(f = 1)p(y = ?1) (3)
Since p(y = ?1) can be assimilated with a con-
stant, Equation 3 implies that maximizing accu-
racy is equivalent with maximizing the criterion
r ? p(f = 1), where both the recall r and p(f = 1)
can be estimated on the positive and unlabeled ex-
amples from a separate development dataset.
In conclusion, one can use the original Biased
SVM formulation to maximize r2/p(f = 1), which
has been shown by Lee and Liu (2003) to maximize
pr, a criterion that has a similar behavior with the
F-measure used in retrieval applications. Alterna-
tively, if the target performance measure is accuracy,
we can choose instead to maximize r ? p(f = 1),
which we have shown above to correspond to accu-
racy maximization.
3.1.2 The Weighted Samples SVM
Elkan and Noto (2008) introduced two ap-
proaches for learning with positive and unlabeled
data. Both approaches are based on the assumption
that labeled examples {x|s(x) = 1} are selected at
random from the positive examples {x|y(x) = 1}
i.e., p(s = 1|x, y = 1) = p(s = 1|y = 1). Their
best performing approach uses the positive and unla-
beled examples to train two distinct classifiers. First,
the dataset P ? U is split into a training set and a
validation set, and a classifier g(x) is trained on the
27
labeling s to approximate the label distribution i.e.
g(x) = p(s = 1|x). The validation set is then used
to estimate p(s = 1|y = 1) as follows:
p(s=1|y=1) = p(s=1|x, y=1) = 1|P |
?
x?P
g(x) (4)
The second and final classifier f(x) is trained on a
dataset of weighted examples that are sampled from
the original training set as follows:
? Each positive example x ? P is copied as a
positive example in the new training set with
weight p(y = 1|x, s = 1) = 1.
? Each unlabeled example x ? U is duplicated
into two training examples in the new dataset:
a positive example with weight p(y = 1|x, s =
0) and a negative example with weight p(y =
?1|x, s = 0) = 1? p(y = 1|x, s = 0).
Elkan and Noto (2008) show that the weights above
can be derived as:
p(y=1|x, s=0) = 1?p(s=1|y=1)p(s=1|y=1) ?
p(s=1|x)
1?p(s=1|x) (5)
The output of the first classifier g(x) is used to
approximate the probability p(s = 1|x), whereas
p(s = 1|y = 1) is estimated using Equation 4.
The two classifiers g and f are trained using
SVMs and a linear kernel. Platt scaling is used with
the first classifier to obtain the probability estimates
g(x) = p(s = 1|x), which are then converted into
weights following Equations 4 and 5, and used dur-
ing the training of the second classifier.
4 Experimental Evaluation
We ran disambiguation experiments on the 6 am-
biguous words atmosphere, president, dollar, game,
diamond andCorinth. The correspondingWikipedia
sense repositories have been summarized in Tables 1
and 2. All WSD classifiers used the same set of stan-
dard WSD features (Ng and Lee, 1996; Stevenson
and Wilks, 2001), such as words and their part-of-
speech tags in a window of 3 words around the am-
biguous keyword, the unigram and bigram content
words that are within 2 sentences of the current sen-
tence, the syntactic governor of the keyword, and
its chains of syntactic dependencies of lengths up to
two. Furthermore, for each example, a Wikipedia
specific feature was computed as the cosine similar-
ity between the context of the ambiguous word and
the text of the article for the target sense or reference.
The Level1 and Level3 classifiers were trained us-
ing the SVMmulti component of the SVMlight pack-
age.1 TheWSD classifiers were evaluated in a 4-fold
cross validation scenario in which 50% of the data
was used for training, 25% for tuning the capacity
parameter C, and 25% for testing. The final accu-
racy numbers, shown in Table 3, were computed by
averaging the results over the 4 folds. Since the word
president has only one sense on Level1, no classifier
needed to be trained for this case. Similarly, words
diamond andCorinth have only one sense on Level3.
atmosphere president dollar
Level1 93.1% ? 94.1%
Level3 85.6% 82.2% 90.8%
game diamond Corinth
Level1 82.9% 95.5% 92.7%
Level3 92.9% ? ?
Table 3: Disambiguation accuracy at Levels 1 & 3.
The evaluation of the binary classifiers at the sec-
ond level follows the same 4-fold cross validation
scheme that was used for Level1 and Level3. The
manual labels for specific senses and references in
the unlabeled datasets are always ignored during
training and tuning and used only during testing.
We compare the Naive SVM, Biased SVM, and
Weighted SVM in the two evaluation settings, using
for all of them the same train/development/test splits
of the data and the same features. We emphasize
that our manual labels are used only for testing pur-
poses ? the manual labels are ignored during train-
ing and tuning, when the data is assumed to contain
only positive and unlabeled examples. We imple-
mented the Biased SVM approach on top of the bi-
nary SVMlight package. TheCP andCU parameters
of the Biased SVM were tuned through the c and j
parameters of SVMlight (c = CU and j = CP /CU ).
Eventually, all three methods use the development
data for tuning the c and j parameters of the SVM.
However, whereas the Naive SVM tunes these pa-
rameters to optimize the accuracy with respect to the
noisy label s(x), the Biased SVM tunes the same pa-
rameters to maximize an estimate of the accuracy or
1http://svmlight.joachims.org
28
F-measure with respect to the true label y(x). The
Weighted SVM approach was implemented on top
of the LibSVM2 package. Even though the original
Weighted SVM method of Elkan and Noto (2008)
does not specify tuning any parameters, we noticed
it gave better results when the capacity c and weight
j parameters were tuned for the first classifier g(x).
Table 4 shows the accuracy results of the three
methods for Level2, whereas Table 5 shows the F-
measure results. The Biased SVM outperforms the
Naive SVM on all the words, in terms of both ac-
curacy and F-measure. The most dramatic increases
are seen for the words atmosphere, game, diamond,
and Corinth. For these words, the number of pos-
itive examples is significantly smaller compared to
the total number of positive and unlabeled examples.
Thus, the percentage of positive examples relative to
the total number of positive and unlabeled examples
is 31.9% for atmosphere, 29.1% for game, 9.0% for
diamond, and 11.6% for Corinth. The positive to to-
tal ratio is however significantly larger for the other
two words: 67.2% for president and 91.5% for dol-
lar. When the number of positive examples is large,
the false negative noise from the unlabeled dataset
in the Naive SVM approach will be relatively small,
hence the good performance of Naive SVM in these
cases. To check whether this is the case, we have
also run experiments where we used only half of
the available positive examples for the word presi-
dent and one tenth of the positive examples for the
word dollar, such that the positive datasets became
comparable in size with the unlabeled datasets. The
results for these experiments are shown in Tables 4
and 5 in the rows labeled presidentS and dollarS . As
expected, the difference between the performance of
Naive SVM and Biased SVM gets larger on these
smaller datasets, especially for the word dollar.
The Weighted SVM outperforms the Naive SVM
on five out of the six words, the exception being the
word president. Comparatively, the Biased SVM
has a more stable behavior and overall results in a
more substantial improvement over the Naive SVM.
Based on these initial results, we see the Biased
SVM as the method of choice for learning with pos-
itive and unlabeled examples in the task of coarse to
fine grained sense disambiguation in Wikipedia.
2http://www.csie.ntu.edu.tw/?cjlin/libsvm
Word NaiveSVM BiasedSVM WeightedSVM
atmosphere 39.9% 79.6% 75.0%
president 91.9% 92.5% 89.5%
dollar 96.0% 97.0% 97.1%
game 83.8% 87.1% 84.6%
diamond 70.2% 74.5% 75.1%
Corinth 46.2% 75.1% 51.9%
presidentS 88.1% 90.6% 87.4%
dollarS 70.3% 84.9% 70.6%
Table 4: Disambiguation accuracy at Level2.
Word NaiveSVM BiasedSVM WeightedSVM
atmosphere 30.5% 86.0% 83.2%
president 94.4% 95.0% 92.8%
dollar 97.9% 98.4% 98.5%
game 75.1% 81.8% 77.5%
diamond 8.6% 53.5% 46.3%
Corinth 15.3% 81.2% 68.0%
presidentS 90.0% 92.4% 89.5%
dollarS 77.9% 91.2% 78.2%
Table 5: Disambiguation F-measure at Level2.
In a final set of experiments, we compared the
traditional flat classification approach and our pro-
posed hierarchical classifier in terms of their over-
all disambiguation accuracy. In these experiments,
the sense repository contains all the leaf nodes as
distinct sense categories. For example, the word
atmosphere would correspond to the sense repos-
itory R = {ATMOSPHERE (G), ATMOSPHERE OF
EARTH, ATMOSPHERE OF MARS, ATMOSPHERE
OF VENUS, STELLAR ATMOSPHERE, ATMO-
SPHERE (UNIT), ATMOSPHERE (MUSIC GROUP)}.
The overall accuracy results are shown in Table 6
and confirm the utility of using the LPU framework
in the hierarchical model, which outperforms the tra-
ditional flat model, especially on words with low ra-
tio of positive to unlabeled examples.
atmosphere president dollar
Flat 52.4% 89.4% 90.0%
Hierarchical 79.7% 91.0% 90.1%
game diamond Corinth
Flat 83.6% 65.7% 42.6%
Hierarchical 87.2% 76.8% 72.1%
Table 6: Flat vs. Hierarchical disambiguation accuracy.
29
5 Future Work
Annotation inconsistencies in Wikipedia were cir-
cumvented by adapting two existing approaches that
use only positive and unlabeled data to train binary
classifiers. This binary classification constraint led
to the introduction of the artificial specific (S) cat-
egory on Level2 in our disambiguation framework.
In future work, we plan to investigate a direct exten-
sion of learning with positive and unlabeled data to
the case of multiclass classification, which will re-
duce the number of classification levels from 3 to 2.
We also plan to investigate the use of unsupervised
techniques in order to incorporate less popular refer-
ences of a word in the hierarchical classification.
Conclusion
We presented an approach to training coarse to fine
grained sense disambiguation systems that treats
annotation inconsistencies in Wikipedia under the
framework of learning with positive and unlabeled
examples. Furthermore, we showed that the true ac-
curacy of a decision function can be optimized us-
ing only positive and unlabeled examples. For test-
ing purposes, we manually annotated 7,079 links be-
longing to six ambiguous words 3. Experimental
results demonstrate that accounting for annotation
ambiguity in Wikipedia links leads to consistent im-
provements in disambiguation accuracy. The man-
ual annotations were only used for testing and were
ignored during training and development. Conse-
quently, the proposed framework of learning with
positive and unlabeled examples for sense disam-
biguation could be applied on the entire Wikipedia
without any manual annotations. By augmenting
general sense links with links to more specific ar-
ticles, such an application could have a significant
impact on Wikipedia itself.
Acknowledgments
This work was supported in part by the Na-
tional Science Foundation IIS awards #1018613 and
#1018590, and an allocation of computing time from
the Ohio Supercomputer Center.
3Data and code will be made publicly available.
References
D. Ahn, V. Jijkoun, G. Mishne, K. Muller, M. de Ri-
jke, and S. Schlobach. 2004. Using Wikipedia at the
TREC QA track. In Proceedings of the 13th Text Re-
trieval Conference (TREC 2004).
Volha Bryl, Claudio Giuliano, Luciano Serafini, and
Kateryna Tymoshenko. 2010. Using background
knowledge to support coreference resolution. In Pro-
ceedings of the 2010 conference on ECAI 2010: 19th
European Conference on Artificial Intelligence, pages
759?764, Amsterdam, The Netherlands.
Razvan Bunescu and Marius Pasca. 2006. Using ency-
clopedic knowledge for named entity disambiguation.
In Proceesings of the 11th Conference of the European
Chapter of the Association for Computational Linguis-
tics (EACL-06), pages 9?16, Trento, Italy.
Philipp Cimiano, Antje Schultz, Sergej Sizov, Philipp
Sorg, and Steffen Staab. 2009. Explicit versus la-
tent concept models for cross-language information re-
trieval. In International Joint Conference on Artificial
Intelligence (IJCAI-09, pages 1513?1518, Pasadena,
CA, july.
S. Cucerzan. 2007. Large-scale named entity disam-
biguation based on Wikipedia data. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing, pages 708?716.
Charles Elkan and Keith Noto. 2008. Learning clas-
sifiers from only positive and unlabeled data. In
Proceedings of the 14th ACM SIGKDD international
conference on Knowledge discovery and data mining,
KDD ?08, pages 213?220.
David A. Ferrucci, Eric W. Brown, Jennifer Chu-Carroll,
James Fan, David Gondek, Aditya Kalyanpur, Adam
Lally, J. William Murdock, Eric Nyberg, John M.
Prager, Nico Schlaefer, and Christopher A. Welty.
2010. Building watson: An overview of the deepqa
project. AI Magazine, 31(3):59?79.
Aria Haghighi and Dan Klein. 2009. Simple coreference
resolution with rich syntactic and semantic features.
In Proceedings of the 2009 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1152?1161, Singapore, August.
M. Kaisser. 2008. The QuALiM question answering
demo: Supplementing answers with paragraphs drawn
from Wikipedia. In Proceedings of the ACL-08 Hu-
man Language Technology Demo Session, pages 32?
35, Columbus, Ohio.
Wee Sun Lee and Bing Liu. 2003. Learning with pos-
itive and unlabeled examples using weighted logistic
regression. In Proceedings of the Twentieth Interna-
tional Conference on Machine Learning (ICML, pages
448?455, Washington, DC, August.
30
Y. Li, R. Luk, E. Ho, and K. Chung. 2007. Improv-
ing weak ad-hoc queries using Wikipedia as external
corpus. In Proceedings of the 30th Annual Interna-
tional ACM SIGIR Conference on Research and De-
velopment in Information Retrieval, pages 797?798,
Amsterdam, Netherlands.
Bing Liu, Yang Dai, Xiaoli Li, Wee Sun Lee, and
Philip S. Yu. 2003. Building text classifiers using pos-
itive and unlabeled examples. In Proceedings of the
Third IEEE International Conference on Data Mining,
ICDM ?03, pages 179?186, Washington, DC, USA.
R. Mihalcea. 2007. Using Wikipedia for automatic word
sense disambiguation. In Human Language Technolo-
gies 2007: The Conference of the North American
Chapter of the Association for Computational Linguis-
tics, pages 196?203, Rochester, New York, April.
D. Milne. 2007. Computing semantic relatedness using
Wikipedia link structure. In Proceedings of the New
Zealand Computer Science Research Student Confer-
ence, Hamilton, New Zealand.
Hwee Tou Ng and H. B. Lee. 1996. Integrating multiple
knowledge sources to disambiguate word sense: An
exemplar-based approach. In Proceedings of the 34th
Annual Meeting of the Association for Computational
Linguistics (ACL-96), pages 40?47, Santa Cruz, CA.
Keith Noto, Milton H. Saier, Jr., and Charles Elkan.
2008. Learning to find relevant biological articles
without negative training examples. In Proceedings of
the 21st Australasian Joint Conference on Artificial In-
telligence: Advances in Artificial Intelligence, AI ?08,
pages 202?213.
Simone Paolo Ponzetto and Roberto Navigli. 2010.
Knowledge-rich word sense disambiguation rivaling
supervised systems. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 1522?1531, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
Simone Paolo Ponzetto and Michael Strube. 2006. Ex-
ploiting semantic role labeling, wordnet and wikipedia
for coreference resolution. In Proceedings of the Hu-
man Language Technology Conference of the North
American Chapter of the Association of Computa-
tional Linguistics, pages 192?199.
M. Potthast, B. Stein, and M. A. Anderka. 2008.
Wikipedia-based multilingual retrieval model. In Pro-
ceedings of the 30th European Conference on IR Re-
search, Glasgow.
Altaf Rahman and Vincent Ng. 2011. Coreference res-
olution with world knowledge. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies -
Volume 1, pages 814?824, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Mark Stevenson and YorickWilks. 2001. The interaction
of knowledge sources in word sense disambiguation.
Computational Linguistics, 27(3):321?349, Septem-
ber.
31
