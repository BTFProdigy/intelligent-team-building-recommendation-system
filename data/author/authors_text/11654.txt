Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 821?831, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Type-Supervised Hidden Markov Models for Part-of-Speech Tagging
with Incomplete Tag Dictionaries
Dan Garrette
Department of Computer Science
The University of Texas at Austin
dhg@cs.utexas.edu
Jason Baldridge
Department of Linguistics
The University of Texas at Austin
jbaldrid@utexas.edu
Abstract
Past work on learning part-of-speech taggers
from tag dictionaries and raw data has re-
ported good results, but the assumptions made
about those dictionaries are often unrealistic:
due to historical precedents, they assume ac-
cess to information about labels in the raw
and test sets. Here, we demonstrate ways to
learn hidden Markov model taggers from in-
complete tag dictionaries. Taking the MIN-
GREEDY algorithm (Ravi et al2010) as a
starting point, we improve it with several intu-
itive heuristics. We also define a simple HMM
emission initialization that takes advantage of
the tag dictionary and raw data to capture both
the openness of a given tag and its estimated
prevalence in the raw data. Altogether, our
augmentations produce improvements to per-
formance over the original MIN-GREEDY al-
gorithm for both English and Italian data.
1 Introduction
Learning accurate part-of-speech (POS) taggers
based on plentiful labeled training material is gener-
ally considered a solved problem. The best taggers
obtain accuracies of over 97% for English newswire
text in the Penn Treebank, which can be consid-
ered as an upper-bound that matches human perfor-
mance on the same task (Manning, 2011). How-
ever, as Manning notes, this story changes as soon
as one is working with different assumptions and
data, including having less training data, different
kinds of training data, other languages, and other
domains. Such POS tagging work has been plen-
tiful and includes efforts to induce POS tags without
labels (Christodoulopoulos et al2010); learn from
POS-tag dictionaries (Ravi et al2010), incom-
plete dictionaries (Hasan and Ng, 2009) and human-
constructed dictionaries (Goldberg et al2008);
bootstrap taggers for a language based on knowl-
edge about other languages (Das and Petrov, 2011),
and creating supervised taggers for new, challenging
domains such as Twitter (Gimpel et al2011).
Here, we focus on learning from tag dictionar-
ies. This is often characterized as unsupervised or
weakly supervised training. We adopt the terminol-
ogy type-supervised training to distinguish it from
unsupervised training from raw text and supervised
training from word tokens labeled with their parts-
of-speech. Work on type-supervision goes back to
(Merialdo, 1994), who introduced the still standard
procedure of using a bigram Hidden Markov Model
(HMM) trained via Expectation Maximization.
Early research appeared to show that learning
from types works nearly as well as learning from
tokens, with researchers in the 1990s obtaining ac-
curacies up to 96% on English (e.g. Kupiec (1992)).
However, the tag dictionaries in these cases were ob-
tained from labeled tokens. While replicating earlier
experiments, Banko and Moore (2004) discovered
that performance was highly dependent on clean-
ing tag dictionaries using statistics gleaned from the
tokens. This greatly simplifies the job of a type-
supervised HMM: it no longer must entertain entries
for uncommon word-tag pairs (or mistaken pairs
due to annotation errors), which otherwise stand on
equal footing with the common ones. When the
full, noisy tag dictionary was employed, Banko and
Moore found accuracies dropped from 96% to 77%.
Banko and Moore?s observations spurred a new
line of research that sought to improve performance
in the face of full, noisy dictionaries; see Ravi and
821
Knight (2009) for an overview. The highest accu-
racy achieved to date under these assumptions is
91.6% (Ravi et al2010). However, as is often
noted (including by the authors themselves), many
papers that work on learning taggers from tag dic-
tionaries make unrealistic assumptions about the tag
dictionaries they use as input (Toutanova and John-
son, 2008; Ravi and Knight, 2009; Hasan and Ng,
2009). For example, tag dictionaries are typically
constructed with every token-tag pair in the data, in-
cluding those that appear only in the test set. This
means that the evaluation of these taggers does not
measure how they perform on sentences that contain
unseen words or unseen word-tag pairs, a likely oc-
currence in real use of a trained tagger.
We show that it is possible to achieve good tag-
ging accuracy using a noisy and incomplete tag dic-
tionary that has no access to the tags of the raw and
test data and no access to the tag frequency infor-
mation of the labeled training data from which the
dictionary is drawn. We build on Ravi et al (2010)
model minimization approach, which reduces dic-
tionary noise by greedily approximating the mini-
mum set of tag bigrams needed to cover the raw data
and exploits that information as a constraint on the
initialization of the model before running EM. We
extend their method in four distinct ways.
1. Enable the algorithm to be used with incomplete
dictionaries by exploiting the type-based infor-
mation provided by the tag dictionary and raw
text to initialize EM, and by training a standard
supervised HMM on the output of EM.
2. Improve the greedy procedure to find a better
minimized set of tag-tag bigrams.
3. Modify the method to return only the set of bi-
grams required to tag sentences instead of keep-
ing all bigrams chosen by minimization.
4. Exploit the paths found during minimization as a
direct initialization for EM.
Together, these improvements make it possible to
use model minimization in a realistic context, and
obtain higher performance: on English, results go
from 63.5% for a vanilla HMM to 82.1% for an
HMM that uses strategies to deal with unknowns,
then to 85.0% with Ravi and Knight?s minimization
and finally to 88.5% with our enhancements.
2 Supervision for HMMs
Hidden Markov Models (HMMs) are well-known
generative probabilistic sequence models commonly
used for POS-tagging. The probability of a tag se-
quence given a word sequence is determined from
the product of emission and transition probabilities:
P (t|w) ?
N?
i=1
P (wi|ti) ? P (ti|ti?1)
HMMs can be trained directly from labeled data by
calculating maximum likelihood estimates or from
incomplete data using Expectation Maximization
(EM) (Dempster et al1977). We use both strate-
gies in this work: EM is used to estimate models
that can automatically label raw tokens, and then a
new HMM is estimated from that auto-labeled data.
2.1 Token-supervised training
We use a simple but effective smoothing regime to
account for unknown words and unseen tag-tag tran-
sitions. For emissions:
P (wi|ti) =
C(ti, wi) + ?(ti)Puni(wi)
C(ti) + ?(ti)
where Puni(wi) is the unigram probability of wi,
and ?(ti) is a tag specific amount of mass for
smoothing. We use one-count smoothing (Chen and
Goodman, 1996), where ?(ti) is based on the num-
ber of words that occur with ti once:
?(ti) = |wi : C(ti, wi) = 1|
Since open-class tags occur more frequently with
words that appear once, they will reserve more mass
for unknown words than closed-class tags will. The
transition distributions are smoothed in a similar
fashion:
P (ti|ti?1) =
C(ti?1, ti) + ?(ti?1)Puni(ti)
C(ti?1) + ?(ti?1)
?(ti?1) = |ti : C(ti?1, ti) = 1|
This simple scheme is quite effective: an HMM
trained on the Penn Treebank sections 0-18 and eval-
uated on sections 19-21 and smoothed in this way
obtains 96.5% accuracy. We do not use gold stan-
dard labels elsewhere for this paper, but do use this
model on the output of type-supervised HMMs.
822
2.2 Type-supervised training
We are primarily interested in learning taggers from
tag dictionaries combined with unlabeled text. As is
standard, we use EM to iteratively estimate the tran-
sition and emission probability parameters to maxi-
mize the likelihood of unlabeled data. It is known,
however, that EM has particular problems learning a
good HMM for POS tagging (Johnson, 2007; Ravi
and Knight, 2009). One reason is that EM gener-
ally tries to learn probability distributions that are
fairly uniform while POS tag frequencies are quite
skewed. For example, ?a? appears in the training
data with seven different tags, but 99.9% of ?a? to-
kens are determiners. Thus, the accuracy of anything
approaching a uniform distribution for ?a? tags will
suffer greatly. In the context of unsupervised POS
tagging models, modeling this distinction greatly
improves results (Moon et al2010). Here, we can
simply exploit the tag dictionary and raw data.
An initial set of parameters for the transitions
and emissions must be supplied as the input to EM.
Given just a tag dictionary, the simplest initializa-
tion is to set alag transitions to be uniform, rang-
ing over all tag continuations, while for emissions, a
uniform distribution over all words that occur with
the tag is assigned. This may be appropriate when a
complete tag dictionary is available, including com-
plete information for words that appear only in the
test data. This is because there will never be any un-
known words during model estimation or inference.
Likewise, there will never be a situation where the
tag dictionary rules out all possible tag transitions
between two adjacent tokens in training or testing.
As a result, no smoothing is needed in this scenario.
The problem with this is that estimating a model
based on type-supervision requires raw text, and if
we have an incomplete tag dictionary, some of the
words in that text will be missing from the tag dic-
tionary. In a Bayesian setting, priors provide mass
for such tokens; models are estimated using either
Gibbs sampling or variational inference (Johnson,
2007). However, we use vanilla EM here; as a con-
sequence, once a parameter is zero, it is always zero.
We thus need to ensure that mass is reserved for
words outside the tag dictionary at the start of EM.
(For transitions, uniform distributions are sufficient
since the set of tags is closed.)
2.3 Emission probability initialization
The simplest way to initialize the emission distribu-
tions is to assign a count of one to every entry in the
tag dictionary, and one count for unknowns. Then,
during each iteration of EM, the expectation step is
able to estimate new non-zero counts for all possible
emissions encountered in the raw corpus. This basic
strategy allows one to train an HMM with EM us-
ing only an incomplete tag dictionary and raw text.
However, this basic approach for emission proba-
bilities produces bad unknown-word probabilities.
Specifically, if for each tag we simply assume one
count for each entry in the tag dictionary and one
count for unknowns and then normalize, the proba-
bility of an unknown word having a specific tag is
inversely correlated with the number of word types
associated with the tag in the tag dictionary. In other
words, a tag that appears with a smaller number of
distinct words will be seen by the HMM as being a
better candidate tag for an unknown word. Unfor-
tunately this is the opposite behavior we want since
closed-class tags like determiner and preposition are
bad candidates for tagging novel words.
For type-supervised training, we can do much bet-
ter. Note that C(w, t) comes in two varieties: w
is either found in the tag dictionary (known word
types), or it is not (unknown word types). We refer
to the later as td-unknown: these are words that oc-
cur in the raw word sequence used for EM but which
do not occur in the tag dictionary. These are thus
different unknowns from words have not been ob-
served in the dictionary or in the raw set but which
may be encountered at test time. Computing the full
C(w, t) is necessary since we want P (w|t) to cover
known and td-unknown words. We must thus deter-
mine both Cknown(w, t) and Cunktd(w, t).
First, we focus on calculating Cknown(w, t). If a
word w appears C(w) times in the raw corpus, and
is seen with |TD(w)| tags in the tag dictionary, then
assume for each t in TD(w):
Cknown(w, t) = C(w) / |TD(w)|
andCknown(w, t) = 0 for all other t. In other words,
we split C(w), the count of w tokens in the corpus,
evenly among each of w?s possible tags. This pro-
vides us with an estimate of the true C(w, t) by ap-
proximating the portion of the counts of each word
823
type that may be associated with that tag. Note that
while this will give us zeros for any words that don?t
appear in the raw corpus, this is not a problem be-
cause EM training is based only on that corpus.
Second, we look at td-unknown word types: those
in the raw data that are not found in the tag dic-
tionary. Given the value P (unktd|t) for the like-
lihood of an unknown word given a tag t, we can
compute estimated counts Cunktd(w, t) for a td-
unknown word w using
Cunktd(w, t) = C(w) ? P (unktd|t)
where C(w), again, comes from the raw corpus.
This has the effect of spreading C(w), the count of
tokens of that unknown word w, across all of the
possible tags, with each tag receiving a proportion
of the total count as determined by P (unktd|t).
The challenge, then, is to compute P (unktd|t).
For this, we have two potential sources of knowl-
edge, the tag dictionary and the raw token sequence,
each telling us complementary information.
First, the tag dictionary tells us about the openness
of a tag?the likelihood that an unseen word will
have that label?based on our previously-discussed
intuition that we are more likely to see a new word
with a tag that is known to be associated with many
words already. Thus, we can estimate Ptd(unktd|t)
by simply normalizing the |TD(t)| values:
Ptd(unktd|t) =
|TD(t)|2
?
t??Tags |TD(t
?)|2
We exaggerate the differences between tags by
squaring the |TD(t)| terms to draw an even larger
distinction between open and closed class types.
Unfortunately, if we calculate an estimated word
count directly from this using Cunktd(w, t) =
C(w) ? Ptd(unktd|t), the Cunktd(w, t) values would
be taken without any regard to the overall like-
lihood of tag t. Since Cknown(NN) is very
high, Cunktd(NN) will seem very low by compar-
ison. Likewise, since Cknown(RB) is much lower,
Cunktd(RB) will seem very high by comparison.
P (unktd|t) must account for the overall likeli-
hood of t so that the Cunktd(w, t) values will be
scaled appropriately according to the overall likeli-
hood of t. For this, we use our second knowledge
source: the raw data. Based on the Cknown(w, t)
values as given above, the raw data tells us about the
overall expectation of a word having a particular tag.
From this, we can estimate the tag distribution for
known words: Cknown(t) =
?
w??V Cknown(w
?, t)
and then normalize to get Pknown(t).
Finally, we need to combine Ptd(unktd|t) and
Pknown(t) into a single P (unktd|t) that accounts
for both the openness of a tag and its overall preva-
lence. We would like this combination to use the
high Pknown(NN) to boost P (unktd|NN) and the
low Pknown(RB) to dampen P (unktd|RB). So, we
compute and normalize:
P (unktd|t) ?
|TD(t)|2
?
t??Tags |TD(t
?)|2
? Pknown(t)
2.4 Auto-supervised post-EM smoothing
The initialization accounting for td-unknown words
given above allows EM to be run on the raw token
sequence, but it provides no probability for words
that are truly unseen (in either the tag dictionary or
the raw data). Consequently, any novel words in the
test set will have zero emission probabilities, leading
to extremely low unknown-word accuracies.
To overcome this problem, we perform a sim-
ple post-processing step after EM, which we refer
to as auto-supervised training. We take the HMM
trained by EM and use it to label the raw corpus.
This gives us an automatically-labeled corpus that
can be used for standard supervised training (with-
out EM) to produce a new HMM. The effect of this
post-processing step is to smooth the counts learned
from EM onto any new words encountered during
testing. This procedure significantly improves the
ability of the HMM to label unknown words.
As a final note, it would of course be possible to
use other models at this stage, such as a Conditional
Random Field (Lafferty et al2001).
3 Enhancing MIN-GREEDY
As was discussed above, one of the major prob-
lems for type-supervised POS-tagger training with
EM is a tag dictionary with low-frequency entries
such as the word ?a? being associated with the for-
eign word tag when nearly all of its instances are
as a determiner. To avoid the need for manually
pruning the tag dictionary, Ravi and Knight (2009)
824
?b? The boy sees a dog ?\b?
?b?
2
%%
DT
1
&&
DT
1
%%
NN
&&
NN
3

V B
&&
BB
FW
?\b?
Figure 1: MIN-GREEDY graph showing a state in the
first phase. Numbered, solid arrows: order of chosen
bigrams; dotted: potential choices.
?b? The boy sees a dog ?\b?
?b?
%%
DT
&&
DT
%%
NN
&&
NN

V B
&&
BB
FW
BB
?\b?
Figure 2: Start of the second MIN-GREEDY phase.
proposed that low-probability tags might be auto-
matically filtered from the tag dictionary through a
model minimization procedure applied to the raw
text and constrained by the full tag dictionary. Ravi
et al2010) develop a faster approach for model
minimization using a greedy algorithm that they call
MIN-GREEDY. It is this algorithm that we extend.
3.1 The original MIN-GREEDY algorithm
The MIN-GREEDY algorithm starts by initializing a
graph with a vertex for each possible tag of each to-
ken in the raw data. The set of possible tags for each
token is the set of tags associated with that word
in the tag dictionary.Special sentence start and sen-
tence end vertices are added to the graph for each
sentence to mark its beginning and end. Unlike Ravi
et al2010), we allow for an incomplete tag dic-
tionary, meaning that our scenario has the additional
complication that the tag set for some raw-corpus
?b? The boy sees a dog ?\b?
?b?
%%
DT
&&
DT
%%
NN
&&
NN

V B
&&
BB
FW
?\b?
Figure 3: Potential MIN-GREEDY conclusion.
words will not be known. For these words, the full
set of tags is used. Note that this increases the ambi-
guity and overall number of edges in the graph.
The MIN-GREEDY algorithm works in three
phases: Greedy Set Cover, Greedy Path Comple-
tion, and Iterative Model-Fitting. In the first two
phases, the algorithm chooses tag bigrams that form
the edges of the graph. The goal of these phases is to
select a set of edges that is sufficient to allow a path
through every sentence in the raw corpus. The al-
gorithm greedily selects these edges in an attempt to
quickly approximate the minimal set of tag bigrams
needed to accomplish this goal. In the final phase,
the algorithm runs several iterations of EM in order
to fit the bigram set to the raw data.
In the first phase, Greedy Set Cover, the algorithm
selects tag bigrams in an effort to cover all of the
word tokens. A word token is considered covered
if there is at least one tag bigram edge connected
to at least one of its vertices. At each iteration, the
algorithm examines the entire graph, across all sen-
tences, to find the tag bigram that, if added, would
maximize the number of newly covered words.
Consider the graph in Figure 1. Assume, for the
example, that this sentence comprises the entire raw
corpus. At the start of the first phase, no tag bigrams
are selected. On the first iteration, the algorithm
chooses the tag bigram DT?NN because this tag
bigram describes two edges for a total of four words
newly covered: The, boy, a, and dog. On the second
iteration, there are only three word tokens left un-
covered: the start symbol, sees, and the end symbol.
At this point, as the figure shows, there are five tag
bigrams that would each result in covering one addi-
825
tional token. Since there are no tag bigrams whose
choosing would result in covering more than one ad-
ditional token, the algorithm randomly chooses one
of these five. The algorithm iterates like this until all
words are covered, as in, for example, Figure 2.
The second phase of the MIN-GREEDY algorithm,
Greedy Path Completion, seeks to fill holes in the
tag paths found in the graph. A hole is a poten-
tial edge that, if added, would connect two existing
edges. At each iteration, the algorithm finds the tag
bigram that, if selected, would maximize the number
of holes that would be filled across all raw sentences.
The example graph in Figure 2 shows a potential
start of the second phase. At this point, there are
three tag bigrams that each fill one hole if selected,
and the algorithm randomly selects one. Iteration
continues until there is a complete tag path through
each sentence in the raw corpus. One potential reso-
lution for the example is given in Figure 3.
Once a set of tag bigrams has been generated that
allows for a complete tag path through every sen-
tence of the raw corpus, MIN-GREEDY begins its
final phase: Iterative Model-Fitting. In this phase,
the algorithm trains a succession of type-supervised
HMM models. Each iteration trains an HMM and
then uses it to tag the raw corpus, the result of which
is used to prepare inputs for the next iteration.
Iterative Model-Fitting begins with the minimized
set of bigrams returned from the second phase of
MIN-GREEDY. This set is used as a hard constraint
on the allowable tag bigrams during type-supervised
HMM training. While EM is running, the only tag
transitions that are counted are those that fall into the
minimized tag bigram set; all other transition counts
are ignored. Once an HMM has been trained, it is
immediately used to tag the raw corpus, producing a
set of auto-labeled sentences. For the second itera-
tion of the phase, we extract a constrained tag dictio-
nary from the auto-labeled corpus by simply taking
every word/tag pair appearing in the data. This new
tag dictionary is a subset of the original, full, tag
dictionary, and hopefully has fewer low-frequency
entries that would cause problems for EM.
We use this constrained tag dictionary to again
perform type-supervised HMM training, but without
any constraints on the allowable tag bigrams. This
produces our third HMM. Using this HMM, we can,
again, tag the raw corpus, producing another set of
auto-labeled sentences. We can then extract the set
of tag bigrams appearing in this data to produce a
new set of tag transition constraints, similar to what
was returned by the second phase. With this set of
tag transition constraints, and the full tag dictionary,
we can perform another round of type-supervised
HMM training, and repeat the entire process.
The third MIN-GREEDY phase continues iterating,
alternating between training an HMM using a con-
strained set of tag transitions and training one using
a constrained tag dictionary. The size of the set of
constrained tag bigrams produced is tracked on each
iteration, and the algorithm is considered to have
converged when this value changes by less than five
percent. The final result of the MIN-GREEDY algo-
rithm is a trained HMM.
The evaluation of the MIN-GREEDY algorithm, as
described in Ravi et al2010), was performed only
for scenarios with a complete tag dictionary (includ-
ing all raw and test word types). As such, no tech-
niques were described for handling unknown words.
Because we are interested in the more realistic sce-
nario of an incomplete tag dictionary, we augment
the original MIN-GREEDY setup with the smoothing
techniques described above.
3.2 Improving tag bigram selection
One of the major problems with the MIN-GREEDY
algorithm is that its heuristics for choosing the next
tag bigram frequently result in many-way ties. In the
first two phases of MIN-GREEDY, the greedy pro-
cedure looks for the tag bigram that will have the
most positive impact. In the Greedy Set Cover phase
this means choosing the tag bigram that would cover
the most new tokens, and in the Greedy Path Com-
pletion phase this means choosing the tag bigram
that would fill the most holes. However, it is fre-
quently the case that there are many distinct tag bi-
grams that would cover the most new tokens or fill
the most holes, leaving the MIN-GREEDY algorithm
with no choice but to randomly select from these
options. Since there are frequently cases of having
many dozens of options, it is clear that some of those
choices must be better than others, even though MIN-
GREEDY does not make a distinction and considers
them all to be equally good choices.
Consider the example in Figure 1 representing a
possible state of the minimization graph. To have
826
reached this stage, tag bigram DT?NN would have
been chosen since it covered the highest number of
tokens: four. Additionally, ?b??DT and NN??\b?
could have been chosen as the second and third tag
bigrams since they tied for the most new tokens cov-
ered: one. For the state shown in this figure, there
is only one uncovered token, sees, but three tag bi-
grams that cover it. Since each of these tag bigrams
covers exactly one new word, they are all considered
by MIN-GREEDY to be equally good choices as the
next tag bigram for inclusion, and the algorithm will
choose one at random. However, it should be clear
that the VB?FW tag bigram is wrong while the
other two would lead to a correct answer. As such,
we would like for the algorithm to avoid choosing
VB?FW, and to pick one of the others.
In order to push the algorithm into choosing the
right tag bigrams in these otherwise ambiguous sit-
uations, we have added an additional criterion to the
bigram-choosing heuristic: after narrowing down
the set of tag bigrams to those that cover the most
new tokens, we further narrow the choice of bigrams
by minimizing the number of new word-type/tag
pairs that would be added to the result. Consider
our example. If we choose the tag bigram NN?VB
or VB?DT, then exactly one new word-type/tag
pair would be added to our result: sees/VB (since
boy/NN and a/DT would already have been added
by the incorporation of previous selected tag bi-
grams). By contrast if we choose the tag bigram
VB?FW then two new word-type/tag pairs would
be added: sees/VB and a/FW.
Minimizing the number of new word/tag pairs
added by the algorithm has two main advantages.
First, it keeps the selected bigrams focused on the
same vertices, which results in fewer holes that the
Greedy Path Completion phase must deal with. Sec-
ondly, it keeps the selected bigrams focused on more
common tags for each word type, such as a/DT, and
keeps it away from rare tags, such as a/FW.
3.3 Only tag bigrams on minimization paths
As was described above, the output of MIN-
GREEDY?s second stage is a minimized set of tag
bigrams which is used as a constraint on the first
iteration of the third stage, Iterative Model-Fitting.
However, in order to determine when to stop adding
new bigrams during the first two phases, the MIN-
GREEDY algorithm must try to find complete tag
paths through each sentence in the raw corpus, stop-
ping once a tag path has been found for each one.
While the algorithm is trying to select only the tag
bigrams that are necessary for a complete tagging, it
happens frequently that bigrams are selected that are
not actually used on any tag path.
Consider the example shown in Figure 3. The
graph has a complete path through the sentence, but
also contains an extraneous edge, VB?FW, that is
not used on the path. Assuming that this tag bigram
is not used on the tag path of any other sentence, it
can safely be removed from the resultant set to pro-
duce a smaller set of tag bigrams, getting us even
closer to the minimized set that we desire.
To find the set of tag bigrams excluding these ex-
traneous edges, we modify the MIN-GREEDY algo-
rithm. During the first and second phases of the al-
gorithm, we check all raw data sentences for a com-
pleted path after each tag bigram is selected. If a
completed path is found for a sentence, we store that
path immediately. Once a path is found for every
sentence, we extract the set of bigrams used on these
paths, and pass that set, instead of the full set of se-
lected bigrams, to the third phase of the algorithm.
Note that it is important that we store the com-
pleted paths as soon as they are completed. Since
sentences are completed at different stages, and
more tag bigrams are selected after some of these
sentences are complete, it is inevitable that some
sentences will end up with multiple complete tag
paths by the end of the second phase. However, we
seek only the first such path. Tag bigrams are se-
lected in order of their impact, so bigrams selected
earlier are better and should be preferred. Consid-
ering again the example in Figure 3, based on the
frequency of the tags, it is likely that, given the
presence of other sentences in the raw corpus, the
tag path including bigrams VB?DT and DT?NN
would be found before the one including VB?FW
and FW?NN. Since they are more frequent bi-
grams, we would want to keep the first path even
if the second is completed at a later time.
The result of this improvement is a smaller,
cleaner minimized tag bigram set to be delivered to
the third phase of MIN-GREEDY.
827
Scenario Total Known Unk.
0. Random baseline (choose tag randomly from tag dictionary) 63.53 65.49 2.38
1. HMM baseline (simple EM with tag dictionary and raw text) 69.20 71.42 0.27
2. HMM baseline + auto-supervised training 82.33 83.67 40.46
3. HMM baseline + auto-supervised training + emission initialization 82.05 83.27 44.31
4. MIN-GREEDY (Ravi et al2010) with add-one smoothing 74.79 77.17 0.45
5. MIN-GREEDY with add-one smoothing + auto-supervised 86.10 87.59 39.74
6. MIN-GREEDY with add-one smoothing + auto-supervised + emission init 85.02 86.33 44.28
7. 6 + enhanced tag bigram choice heuristic 86.71 88.08 43.93
8. 6 + restrict tag bigrams to tag paths of minimization-tagged output 87.01 88.40 43.74
9. 6 + HMM initialization from minimization-tagged output 88.52 89.92 44.80
10. 6 + 7 + 8 + 9 88.51 89.92 44.80
Table 1: English tagging accuracy using PTB sections 00-15 to build the tag dictionary. Known word types
are those appearing in the tag dictionary.
3.4 EM initialization with minimization output
As a final improvement to MIN-GREEDY, we took
the set of completed tag paths returned from the sec-
ond phase of the algorithm, as described in the pre-
vious section, and used them as labeled data to ini-
tialize an HMM for EM training.
Since we modified MIN-GREEDY to produce a set
of completed tag paths for sentences, we can take
this to be a complete set of labels for the raw cor-
pus. Furthermore, since we were careful about stor-
ing paths as soon as they become completed by the
minimization process, and the tag bigrams are cho-
sen in order of frequency, there will be more high-
frequency bigrams than low-frequency. As a result,
this labeling will contain good tag transitions and
token labelings. As such, the labeled data produced
by the second phase provides useful information be-
yond a simple set of sufficient bigrams: it contains
legitimate frequency information that can be used
to initialize the HMM. We, therefore, initialize an
HMM directly from this data to start EM.
4 Evaluation1
English data. We evaluate on the Penn Treebank
(Marcus et al1993). In all cases we use the first
47,996 tokens of section 16 as our raw data, sections
19?21 as our development set, and perform the final
evaluation on sections 22?24.
1Source code, scripts, and data to reproduce the results pre-
sented here can be found at github.com/dhgarrette/
type-supervised-tagging-2012emnlp
We evaluate two differently sized tag dictionaries.
The first is extracted directly from sections 00?15
(751,059 tokens) and the second from sections 00?
07 (379,908 tokens). The former contains 39,087
word types, 45,331 word/tag entries, a per-type am-
biguity of 1.16 and yields a per-token ambiguity of
2.21 on the raw corpus (treating unknown words
as having all 45 possible tags). The latter contains
26,652 word types, 30,662 word/tag entries, a per-
type ambiguity of 1.15 and yields a per-token ambi-
guity of 2.03 on the raw corpus. In both cases, every
word/tag pair found in the relevant sections was used
in the tag dictionary: no pruning was performed.
Italian data. As a second evaluation, we use the
TUT corpus (Bosco et al2000). To verify that our
approach is language-independent without the need
for specific tuning, we executed our tests on the Ital-
ian data without any trial runs, parameter modifica-
tions, or other changes. We divided the TUT data,
taking the first half of each of the five sections as in-
put to the tag dictionary, the next quarter as raw data,
and the last quarter as test data. All together, the tag
dictionary was constructed from 41,000 tokens con-
sisting of 7,814 word types, 8,370 word/tag pairs,
per-type ambiguity of 1.07 and a per-token ambigu-
ity of 1.41 on the raw data. The raw data consisted of
18,574 tokens and the test contained 18,763 tokens.
Results We ran eleven experiments for each data
set with results shown in Tables 1 and 2. All scores
are reported as the percentage of tokens for which
the correct tag was assigned. Accuracy is shown as
828
PTB (00-07) TUT
Scenario Total Known Unk. Total Known Unk.
0. Random 64.98 68.04 2.81 62.81 76.10 1.58
1. HMM basic 69.32 72.70 0.56 60.70 73.77 0.51
2. HMM + auto-super 81.50 83.67 37.46 70.03 80.64 21.12
3. HMM + auto-super + init 81.71 83.62 42.89 70.89 80.91 24.74
4. MIN-GREEDY + add-1 68.86 72.20 0.92 53.96 65.49 0.84
5. MIN-GREEDY + add-1 + auto-super 80.78 82.88 38.02 70.85 82.41 17.60
6. MIN-GREEDY + add-1 + auto-super + init 80.92 82.80 42.64 71.52 81.56 25.28
7. 6 + enhanced bigram choice heuristic 86.69 88.83 43.07 71.48 81.57 24.98
8. 6 + restrict tag bigrams to tag paths 80.86 82.73 42.84 72.86 83.45 24.08
9. 6 + HMM init from minimization output 87.61 89.74 44.18 72.00 82.28 24.65
10. 6 + 7 + 8 + 9 87.95 90.12 43.74 71.99 82.50 23.57
Table 2: Tagging accuracy using PTB sections 00-07 and TUT to build the tag dictionary. Known word
types are those appearing in the tag dictionary. Scenario numbers correspond to Table 1.
the Total (all word types), Known (word types found
in the tag dictionary), and Unknown (word types not
found in the tag dictionary).
Experiments 1?3 evaluate our smoothing tech-
niques applied directly to the task of type-supervised
HMM training with EM, without MIN-GREEDY.
The basic HMM consistently beats the baseline ran-
dom tagger, the auto-supervision technique makes
an enormous improvement for both known and un-
known words, and the the emission initialization
yields a sizable improvement for unknown words.
Experiments 4?6 evaluated our reimplementation
of MIN-GREEDY. We start with the most basic level
of smoothing needed to work in a type-supervised
scenario. For the smaller PTB tag dictionary and
the TUT data, MIN-GREEDY actually has lower per-
formance than the HMM alone. This indicates that
if the tag dictionary has a low degree of ambigu-
ity, then MIN-GREEDY can make the situation worse.
However, with our smoothing techniques, we regain
similar improvements as with the HMM.
Finally we performed experiments evaluat-
ing combinations of our improvements to MIN-
GREEDY. Scenarios 7?9 show each improvement
taken in turn. Scenario 10 shows the results for us-
ing all three improvements. For the English data, the
best results are found when all the improvements are
used. When taken individually, the bigram choice
heuristic and HMM initialization from minimization
output each consistently outperform the improved-
MIN-GREEDY baseline on English. However, re-
stricting the tag bigrams to that in the minimization-
tagged output causes problems in the smaller PTB
scenario, presumably falling to a local maximum
like MIN-GREEDY that the other improvements are
able to help the algorithm avoid.
Though the accuracy improvements are less than
for English, the Italian results show that our MIN-
GREEDY enhancements make an appreciable differ-
ence for a language and dataset for which the ap-
proaches considered were run sight unseen.
Error analysis One of the primary goals of model
minimization is to automatically eliminate low-
probability entries from the tag dictionary that might
confuse the EM algorithm (Ravi et al2010). In or-
der to see how well our techniques are able to iden-
tify and eliminate these unlikely word/tag pairs, we
analyzed the tagging errors from each experiment.
In doing so, we discovered that the two of the most
problematic words for the EM algorithm are ?a? and
?in?. We ran further experiments explore what was
happening with those words. The results, using PTB
sections 00?07 are shown in Table 3.
In PTB sections 00-07 the word ?a? appears 7630
times and with 7 different tags. This includes 7621
occurrences with tag DT, 3 with tag SYM (symbol),
and 1 time with LS (list item marker). As such, we
would want the HMM to lean heavily toward tag DT
when tagging the token ?a?. Unfortunately, the rare
tags confuse the EM procedure and end up with dis-
829
model tokens tagged by scenario
tok output 3 6 7 8 9 10
a DT 32 4 4 4 2424 2425
LS 1531 0 0 0 0 0
SYM 731 2356 2305 2356 0 0
in IN 12 15 2024 4 2042 2047
FW 1922 1910 0 0 0 0
RP 20 27 0 2037 0 0
Table 3: Number of times, for the words ?a? and
?in?, the tagger trained by the particular scenario se-
lected the given tag. Experiments used PTB sections
00-07 for the initial tag dictionary. Scenario num-
bers correspond to Table 1.
proportionately high probabilities. Our experiment
training an HMM without minimization (scenario 3)
resulted in 1531 ?a? tokens being tagged LS, 731 as
SYM, and only 32 tagged as DT.
The situation is similar with the word ?in?, which
appears 6155 times with 5 different tags in the 8
sections. Of these, 6073 occurrences are tagged
IN (preposition), 63 are RP (particle), and 1 is FW
(foreign word). Again, EM without minimization
is confused by the rare tokens, assigning FW 1922
times and IN 12 times.
The minimization procedure attempts to over-
come this problem by removing unlikely tags from
the tag dictionary automatically. As is show in Table
3, MIN-GREEDY without our enhancements is able
to reject the problematic LS as a tag for ?a?, but
unable to do so for SYM, resulting in 2356 tokens
tagged SYM and only 4 tagged DT. Similarly, MIN-
GREEDY is unable to reject FW as a tag for ?in?.
Our enhancements to MIN-GREEDY improve the
situation. More careful choosing of bigrams during
minimization results in the avoidance of LS and FW
(but not SYM) for ?a? as well as FW and RP for
?in?. Restricting the tag bigrams output from MIN-
GREEDY to just those on tag paths avoids LS and FW
for ?a? and FW for ?in?. Finally, using the tagged
sentences from MIN-GREEDY as noisy supervision
for EM initialization eliminates all rare tags, as does
the use of all three enhancements together.
5 Conclusion
Our results show it is possible to create accurate
POS-taggers using type-supervision with incom-
plete tag dictionaries by extending the MIN-GREEDY
algorithm of Ravi et al2010). The most useful
change we made to the MIN-GREEDY procedure was
the implementation of a better heuristic for picking
tag bigrams. An intuitive and straightforward emis-
sion initialization provides the necessary basis to run
EM on a given raw token sequence. Using EM out-
put on this raw sequence as auto-labeled material
to a supervised HMM then proves highly effective
for generalization to new texts containing previously
unseen word types.
Vaswani et al010) explore the use of minimum
description length principles in a Bayesian model as
a way of capturing model minimization, inspired by
the MIN-GREEDY algorithm. The advantage there is
that only a single objective function needs to be opti-
mized, rather than having initialization followed by
an iterative back and forth with pruning of tag-tag
pairs. Our own next steps are to move in a similar
direction to explore the possibilities for encoding the
intuitions we developed for initialization and mini-
mization as a single generative model.
Goldberg et al2008) note that fixing noisy dic-
tionaries by hand is actually quite feasible, and sug-
gest that effort should focus on exploiting human
knowledge rather than just algorithmic improve-
ments. We agree; however, our ultimate motivation
is to use this work to tackle bootstrapping from very
small tag dictionaries or dictionaries obtained from
linguists or resources other than a corpus, and for
tag sets that are more ambiguous (e.g., supertagging
for CCGbank (Hockenmaier and Steedman, 2007)).
Such efforts require automatic expansion of tag dic-
tionaries, which then need be constrained based on
available raw token sequences using methods such
as those explored here. In this respect, the some-
what idiosyncratic noise in the corpus-derived dic-
tionaries used here make a good test.
Acknowledgements
We thank Yoav Goldberg, Sujith Ravi, and the re-
viewers for their feedback. This work was supported
by the U.S. Department of Defense through the U.S.
Army Research Office (grant number W911NF-10-
1-0533) and via a National Defense Science and En-
gineering Graduate Fellowship for the first author.
830
References
Michele Banko and Robert C. Moore. 2004. Part-of-
speech tagging in context. In Proceedings of COLING,
pages 556?561, Geneva, Switzerland.
Cristina Bosco, Vincenzo Lombardo, Daniela Vassallo, ,
and Leonardo Lesmo. 2000. Building a treebank for
Italian: a data-driven annotation schema. In Proceed-
ings of LREC.
Stanley F. Chen and Joshua Goodman. 1996. An empir-
ical study of smoothing techniques for language mod-
eling. In Proceedings of ACL, pages 310?318, Santa
Cruz, California, USA.
Christos Christodoulopoulos, Sharon Goldwater, and
Mark Steedman. 2010. Two decades of unsupervised
pos induction: How far have we come? In Proceed-
ings of EMNLP.
Dipanjan Das and Slav Petrov. 2011. Unsupervised part-
of-speech tagging with bilingual graph-based projec-
tions. In Proceedings of ACL-HLT, pages 600?609,
Portland, Oregon, USA.
Arthur P. Dempster, Nan M. Laird, and Donald. B. Ru-
bin. 1977. Maximum likelihood from incomplete data
via the EM algorithm. Journal of the Royal Statistical
Society: Series B (Statistical Methodology), 39:1?22.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael
Heilman, Dani Yogatama, Jeffrey Flanigan, and
Noah A. Smith. 2011. Part-of-speech tagging for twit-
ter: Annotation, features, and experiments. In Pro-
ceedings of ACL-HLT, pages 42?47, Portland, Oregon,
USA.
Yoav Goldberg, Meni Adler, and Michael Elhadad. 2008.
EM can find pretty good HMM POS-taggers (when
given a good start). In Proceedings ACL, pages 746?
754.
Kazi Saidul Hasan and Vincent Ng. 2009. Weakly super-
vised part-of-speech tagging for morphologically-rich,
resource-scarce languages. In Proceedings of EACL,
pages 363?371, Athens, Greece.
Julia Hockenmaier and Mark Steedman. 2007. Ccgbank:
A corpus of ccg derivations and dependency structures
extracted from the penn treebank. Computational Lin-
guistics, 33(3):355?396.
Mark Johnson. 2007. Why doesn?t EM find good HMM
POS-taggers? In Proceedings EMNLP-CoNLL, pages
296?305.
Julian Kupiec. 1992. Robust part-of-speech tagging us-
ing a hidden markov model. Computer Speech & Lan-
guage, 6(3):225?242.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic mod-
els for segmenting and labeling sequence data. In
Proceedings of ICML, pages 282?289. Morgan Kauf-
mann.
Christopher D. Manning. 2011. Part-of-speech tagging
from 97% to 100%: Is it time for some linguistics? In
Alexander Gelbukh, editor, Proceedings of CICLing,
volume 6608 of Lecture Notes in Computer Science,
pages 171?189.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313?330.
Bernard Merialdo. 1994. Tagging English text with
a probabilistic model. Computational Linguistics,
20(2):155?171.
Taesun Moon, Katrin Erk, and Jason Baldridge. 2010.
Crouching dirichlet, hidden markov model: Unsu-
pervised POS tagging with context local tag genera-
tion. In Proceedings of EMNLP, pages 196?206, Cam-
bridge, MA.
Sujith Ravi and Kevin Knight. 2009. Minimized models
for unsupervised part-of-speech tagging. In Proceed-
ings of ACL-AFNLP.
Sujith Ravi, Ashish Vaswani, Kevin Knight, and David
Chiang. 2010. Fast, greedy model minimization for
unsupervised tagging. In Proceedings of COLING,
pages 940?948.
Kristina Toutanova and Mark Johnson. 2008. A bayesian
lda-based model for semi-supervised part-of-speech
tagging. In Proceedings of NIPS.
Ashish Vaswani, Adam Pauls, and David Chiang. 2010.
Efficient optimization of an mdl-inspired objective
function for unsupervised part-of-speech tagging. In
Proceedings of the ACL 2010 Conference Short Pa-
pers, pages 209?214, Uppsala, Sweden.
831
Proceedings of NAACL-HLT 2013, pages 138?147,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Learning a Part-of-Speech Tagger from Two Hours of Annotation
Dan Garrette
Department of Computer Science
The University of Texas at Austin
dhg@cs.utexas.edu
Jason Baldridge
Department of Linguistics
The University of Texas at Austin
jbaldrid@utexas.edu
Abstract
Most work on weakly-supervised learning for
part-of-speech taggers has been based on un-
realistic assumptions about the amount and
quality of training data. For this paper, we
attempt to create true low-resource scenarios
by allowing a linguist just two hours to anno-
tate data and evaluating on the languages Kin-
yarwanda and Malagasy. Given these severely
limited amounts of either type supervision
(tag dictionaries) or token supervision (labeled
sentences), we are able to dramatically im-
prove the learning of a hidden Markov model
through our method of automatically general-
izing the annotations, reducing noise, and in-
ducing word-tag frequency information.
1 Introduction
The high performance achieved by part-of-speech
(POS) taggers trained on plentiful amounts of la-
beled word tokens is a success story of computa-
tional linguistics (Manning, 2011). However, re-
search on learning taggers using type supervision
(e.g. tag dictionaries or morphological transducers)
has had a more checkered history. The setting is
a seductive one: by labeling the possible parts-of-
speech for high frequency words, one might learn
accurate taggers by incorporating the type informa-
tion as constraints to a semi-supervised generative
learning model like a hidden Markov model (HMM).
Early work showed much promise for this strategy
(Kupiec, 1992; Merialdo, 1994), but successive ef-
forts in recent years have continued to peel away and
address layers of unrealistic assumptions about the
size, coverage, and quality of the tag dictionaries
that had been used (Toutanova and Johnson, 2008;
Ravi and Knight, 2009; Hasan and Ng, 2009; Gar-
rette and Baldridge, 2012). This paper attempts to
strip away further layers so we can build better intu-
itions about the effectiveness of type-supervised and
token-supervised strategies in a realistic setting of
POS-tagging for low-resource languages.
In most previous work, tag dictionaries are ex-
tracted from a corpus of annotated tokens. To ex-
plore the type-supervised scenario, these have been
used as a proxy for dictionaries produced by lin-
guists. However, this overstates their effectiveness.
Researchers have often manually pruned tag dictio-
naries by removing low-frequency word/tag pairs;
this violates the assumption that frequency informa-
tion is not available. Others have also created tag
dictionaries by extracting every word/tag pair in a
large, labeled corpus, including the test data?even
though actual applications would never have such
complete lexical knowledge. Dictionaries extracted
from corpora are also biased towards including only
the most likely tag for each word type, resulting in
a cleaner dictionary than one would find in real sce-
nario. Finally, tag dictionaries extracted from anno-
tated tokens benefit from the annotation process of
labeling and review and refinement over an extended
collaboration period. Such high quality annotations
are simply not available for most low-resource lan-
guages.
This paper describes an approach to learning
a POS-tagger that can be applied in a truly low-
resource scenario. Specifically, we discuss tech-
niques that allow us to learn a tagger given only
138
the amount of labeled data that a human annotator
could provide in two hours. Here, we evaluate on
the languages Malagasy and Kinyarwanda, as well
as English as a control language. Furthermore, we
are interested in whether type-supervision or token-
supervision is more effective, given the strict time
constraint; accordingly, we had annotators produce
both a tag dictionary and a set of labeled sentences.
The data produced under our conditions differs in
several ways from the labeled data used in previous
work. Most obviously, there is less of it. Instead
of using hundreds of thousands of labeled tokens
to construct a tag dictionary (and hundreds of thou-
sands more as unlabeled (raw) data for training), we
only use the 1k-2k labeled tokens or types provided
by our annotators within the timeframe. Our train-
ing data is also much noisier than the data from a
typical corpus: the annotations were produced by
a single non-native-speaker working alone for two
hours. Therefore, dealing with the size and quality
of training data were core challenges to our task.
To learn a POS-tagger from so little labeled data,
we developed an approach that starts by generalizing
the initial annotations to the entire raw corpus. Our
approach uses label propagation (LP) (Talukdar and
Crammer, 2009) to infer tag distributions on unla-
beled tokens. We then apply a novel weighted vari-
ant of the model minimization procedure originally
developed by Ravi and Knight (2009) to estimate se-
quence and word-tag frequency information from an
unlabeled corpus by approximating the minimal set
of tag bigrams needed to explain the data. This com-
bination of techniques turns a tiny, unweighted, ini-
tial tag dictionary into a weighted tag dictionary that
covers the entire corpus?s vocabulary. This weighted
information limits the potential damage of tag dic-
tionary noise and bootstraps frequency information
to approximate a good starting point for the learning
of an HMM using expectation-maximization (EM),
and far outperforms just using EM on the raw an-
notations themselves.
2 Data
Our experiments use Kinyarwanda (KIN), Malagasy
(MLG), and English (ENG). KIN is a Niger-Congo
language spoken in Rwanda. MLG is an Austrone-
sian language spoken in Madagascar. Both KIN and
MLG are low-resource and KIN is morphologically-
rich. For each language, the word tokens are divided
into four sets: training data to be labeled by anno-
tators, raw training data, development data, and test
data. For consistency, we use 100k raw tokens for
each language.
Data sources For ENG, we used the Penn Tree-
bank (PTB) (Marcus et al, 1993). Sections 00-04
were used as raw data, 05-14 as a dev set, and 15-24
(473K tokens) as a test set. The PTB uses 45 dis-
tinct POS tags. The KIN texts are transcripts of testi-
monies by survivors of the Rwandan genocide pro-
vided by the Kigali Genocide Memorial Center. The
MLG texts are articles from the websites1 Lakroa and
La Gazette and Malagasy Global Voices,2 a citizen
journalism site.3 Texts in both KIN and MLG were
tokenized and labeled with POS tags by two linguis-
tics graduate students, each of which was studying
one of the languages. The KIN and MLG data have
14 and 24 distinct POS tags, respectively, and were
developed by the annotators.
Time-bounded annotation One of our main goals
is to evaluate POS-tagging for low-resource lan-
guages in experiments that correspond better to a
real-world scenario than previous work. As such, we
collected two forms of annotation, each constrained
by a two-hour time limit. The annotations were done
by the same linguists who had annotated the KIN
and MLG data mentioned above. Our experiments
are thus relevant to the reasonable context in which
one has access to a linguist who is familiar with the
target language and a given set of POS tags.
The first annotation task was to directly produce a
dictionary of words to their possible POS tags?i.e.,
collecting an actual tag dictionary of the form that is
typically simulated in POS-tagging experiments. For
each language, we compiled a list of word types, or-
dered starting with most frequent, and presented it
to the annotator with a list of admissible POS tags.
The annotator had two hours to specify POS tags for
as many words as possible. The word types and fre-
quencies used for this task were taken from the raw
training data and did not include the test sets. This
1www.lakroa.mg and www.lagazette-dgi.com
2mg.globalvoicesonline.org/
3The public-domain data is available at github.com/
dhgarrette/low-resource-pos-tagging-2013
139
data is used for what will call type-supervised train-
ing. The second task was annotating full sentences
with POS tags, again for two hours. We refer to this
as token-supervised training.
Having both sets of annotations allows us to in-
vestigate the relative value of each with respect to
training taggers. Token-supervision provides valu-
able frequency and tag context information, but
type-supervision produces larger dictionaries. This
can be seen in Table 1, where the dictionary size
column in the table gives the number of unique
word/tag pairs derived from the data.
We also wanted to directly compare the two an-
notators to see how the differences in their relative
annotation speeds and quality would affect the over-
all ability to learn an accurate tagger. We thus had
them complete the same two tasks for English. As
can be seen in Table 1, there are clear differences
between the two annotators. Most notably, annota-
tor B was faster at annotating full sentences while
annotator A was faster at annotating word types.
3 Approach
Our approach to learning POS-taggers is based on
Garrette and Baldridge (2012), which properly sep-
arated test data from learning data, unlike much pre-
vious work. The input to our system is a raw cor-
pus and either a human-generated tag dictionary or
human-tagged sentences. The majority of the sys-
tem is the same for both kinds of labeled training
data, but the following description will point out dif-
ferences. The system has four main parts, in order:
1. Tag dictionary expansion
2. Weighted model minimization
3. Expectation maximization (EM) HMM training
4. MaxEnt Markov Model (MEMM) training
3.1 Tag dictionary expansion
In a low-resource setting, most word types will not
be found in the initial tag dictionary. EM-HMM train-
ing uses the tag dictionary to limit ambiguity, so a
sparse tag dictionary is problematic because it does
not sufficiently confine the parameter space.4 Small
4This is of course not the case for purely unsupervised tag-
gers, though we also note that it is not at all clear they are actu-
ally learning taggers for part-of-speech.
sent. tok. dict.
KIN human sentences A 90 1537 750
KIN human TD A 1798
MLG human sentences B 92 1805 666
MLG human TD B 1067
ENG human sentences A 86 1897 903
ENG human TD A 1644
ENG human sentences B 107 2650 959
ENG human TD B 1090
Table 1: Statistics for Kinyarwanda, Malagasy, and
English data annotated by annotators A and B.
dictionaries also interact poorly with the model min-
imization of Ravi et al (2010): if there are too many
unknown words, and every tag must be considered
for them, then the minimal model will simply be the
one that assumes that they all have the same tag.
For these reasons, we automatically expand an
initial small dictionary into one that has coverage for
most of the vocabulary. We use label propagation
(LP)?specifically, the Modified Adsorption (MAD)
algorithm (Talukdar and Crammer, 2009)5?which
is a graph-based technique for spreading labels be-
tween related items. Our graphs connect token
nodes to each other via feature nodes and are seeded
with POS-tag labels from the human-annotated data.
Defining the LP graph Our LP graph has several
types of nodes, as shown in Figure 1. The graph
contains a TOKEN node for each token of the la-
beled corpus (when available) and raw corpus. Each
word type has one TYPE node that is connected to
its TOKEN nodes. Both kinds of nodes are con-
nected with feature nodes. The PREVWORD x and
NEXTWORD x nodes represent the features of a to-
ken being preceded by or followed by word type x in
the corpus. These bigram features capture extremely
simple syntactic information. To capture shallow
morphological relatedness, we use prefix and suffix
nodes that connect word types that share prefix or
suffix character sequences up to length 5. For each
node-feature pair, the connecting edge is weighted
as 1/N where N is the number of nodes connected
to the particular feature.
5The open-source MAD implementation is provided through
Junto: github.com/parthatalukdar/junto
140
TOKEN A 1 1 TOKEN walks 2 3
SUFFIX1 e
TOKEN barks 1 3
SUFFIX1 s
PREVWORD dog
SUFFIX2 he
TYPE A
TOKEN The 2 1 TOKEN walks 3 3TOKEN The 3 1
PREVWORD manNEXTWORD .
TYPE barksTYPE The
SUFFIX2 ksDICTPOS D
NEXTWORD dog
DICTPOS N DICTPOS V
TYPE walks
NEXTWORD manPREVWORD ?b?
Figure 1: Subsets of the LP graph showing regions of connected nodes. Graph represents the sentences ?A
dog barks .?, ?The dog walks .?, and ?The man walks .?
We also explored the effectiveness of using an ex-
ternal dictionary in the graph since this is one of the
few available sources of information for many low-
resource languages. Though a standard dictionary
probably will not use the same POS tag set that we
are targeting, it nevertheless provides information
about the relatedness of various word types. Thus,
we use nodes DICTPOS p that indicate that a particu-
lar word type is listed as having POS p in the dictio-
nary. Crucially, these tags bear no particular con-
nection to the tags we are predicting: we still tar-
get the tags defined by the linguist who annotated
the types or tokens used, which may be more or
less granular than those provided in the dictionary.
As external dictionaries, we use English Wiktionary
(614k entries), malagasyworld.org (78k entries),
and kinyarwanda.net (3.7k entries).6
Seeding the graph is straightforward. With token-
supervision, labels for tokens are injected into the
corresponding TOKEN nodes with a weight of 1.0.
In the type-supervised case, any TYPE node that ap-
pears in the tag dictionary is injected with a uniform
distribution over the tags in its tag dictionary entry.
Toutanova and Johnson (2008) (also, Ravi and
Knight (2009)) use a simple method for predict-
ing possible tags for unknown words: a set of 100
most common suffixes are extracted and then mod-
els of P(tag|suffix) are built and applied to unknown
words. However, these models suffer with an ex-
tremely small set of labeled data. Our method uses
character affix feature nodes along with sequence
feature nodes in the LP graph to get distributions
over unknown words. Our technique thus subsumes
6Wiktionary (wiktionary.org) has only 3,365 en-
tries for Malagasy and 9 for Kinyarwanda.
theirs as it can infer tag dictionary entries for words
whose suffixes do not show up in the labeled data (or
with enough frequency to be reliable predictors).
Extracting a result from LP LP assigns a label
distribution to every node. Importantly, each indi-
vidual TOKEN gets its own distribution instead of
sharing an aggregation over the entire word type.
From this graph, we extract a new version of the
raw corpus that contains tags for each token. This
provides the input for model minimization.
We seek a small set of likely tags for each token,
but LP gives each token a distribution over the entire
set of tags. Most of the tags are simply noise, some
of which we remove by normalizing the weights and
excluding tags with probability less than 0.1. Af-
ter applying this cutoff, the weights of the remain-
ing tags are re-normalized. We stress that this tag
dictionary cutoff is not like those used in past re-
search, which were done with respect to frequen-
cies obtained from labeled tokens: we use either no
word-tag frequency information (type-supervision)
or very small amounts of word-tag frequency infor-
mation indirectly through LP (token-supervision).7
Some tokens might not have any associated tag
labels after LP. This occurs when there is no
path from a TOKEN node to any seeded nodes or
when all tags for the TOKEN node have weights less
than the threshold. Since we require a distribution
for every token, we use a default distribution for
such cases. Specifically, we use the unsupervised
emission probability initialization of Garrette and
Baldridge (2012), which captures both the estimated
frequency of a tag and its openness using only a
7See Banko and Moore (2004) for further discussion of these
issues.
141
?b? The man saw the saw ?b?
?b?
D
N
V
1.0
1.0
1.0 0.2
0.8
1.0
0.7
0.3
1.0
Figure 2: Weighted, greedy model minimization
graph showing a potential state between the stages
of the tag bigram choosing algorithm. Solid edges:
selected bigrams. Dotted edges: holes in the path.
small tag dictionary and unlabeled text.
Finally, we ensure that tokens of words in the
original tag dictionary are only assigned tags from
its entry. With this filter, LP of course does not add
new tags to known words (without it, we found per-
formance drops). If the intersection of the small tag
dictionary entry and the token?s resulting distribu-
tion from LP (after thresholding) is empty, we fall
back to the filtered and renormalized default distri-
bution for that token?s type.
The result of this process is a sequence of (ini-
tially raw) tokens, each associated with a distribu-
tion over a subset of tags. From this we can extract
an expanded tag dictionary for use in subsequent
stages that, crucially, provides tag information for
words not covered by the human-supplied tag dic-
tionary. This expansion is simple: an unknown word
type?s set of tags is the union of all tags assigned to
its tokens. Additionally, we add the full entries of
word types given in the original tag dictionary.
3.2 Weighted model minimization
EM-HMM training depends crucially on having a
clean tag dictionary and a good starting point for the
emission distributions. Given only raw text and a
tag dictionary, these distributions are difficult to es-
timate, especially in the presence of a very sparse
or noisy tag dictionary. Ravi and Knight (2009) use
model minimization to remove tag dictionary noise
and induce tag frequency information from raw text.
Their method works by finding a minimal set of tag
bigrams needed to explain a raw corpus.
Model minimization is a natural fit for our system
since we start with little or no frequency informa-
tion and automatic dictionary expansion introduces
noise. We extend the greedy model minimization
procedure of Ravi et al (2010), and its enhance-
ments by Garrette and Baldridge (2012), to develop
a novel weighted minimization procedure that uses
the tag weights from LP to find a minimal model
that is biased toward keeping tag bigrams that have
consistently high weights across the entire corpus.
The new weighted minimization procedure fits well
in our pipeline by allowing us to carry the tag dis-
tributions forward from LP instead of simply throw-
ing that information away and using a traditional tag
dictionary.
In brief, the procedure works by creating a graph
such that each possible tag of each raw-corpus token
is a vertex (see Figure 2). Any edge that would con-
nect two tags of adjacent tokens is a potential tag bi-
gram choice. The algorithm first selects tag bigrams
until every token is covered by at least one bigram,
then selects tag bigrams that fill gaps between exist-
ing edges until there is a complete bigram path for
every sentence in the raw corpus.8
Ravi et al (2010) select tag bigrams that cover
the most new words (stage 1) or fill the most holes
in the tag paths (stage 2). Garrette and Baldridge
(2012) introduced the tie-breaking criterion that bi-
gram choices should seek to introduce the small-
est number of new word/tag pairs possible into the
paths. Our criteria adds to this by using the tag
weights on each token: a tag bigram b is chosen by
summing up the node weights of any not-yet cov-
ered words touched by the tag bigram b, dividing
this sum by one plus the number of new word/tag
pairs that would be added by b, and choosing the b
that maximizes this value.9
Summing node weights captures the intuition of
Ravi et al (2010) that good bigrams are those which
have high coverage of new words: each newly cov-
ered node contributes additional (partial) counts.
However, by using the weights instead of full counts,
we also account for the confidence assigned by LP.
Dividing by the number of new word/tag pairs added
focuses on bigrams that reuse existing tags for words
8Ravi et al (2010) include a third phase of iterative model
fitting; however, we found this stage to be not only expensive,
but also unhelpful because it frequently yields negative results.
9In the case of token-supervision, we pre-select all tag bi-
grams appearing in the labeled corpus since these are assumed
to be known high-quality tag bigrams and word/tag pairs.
142
and thereby limits the addition of new tags for each
word type.
At the start of model minimization, there are no
selected tag bigrams, and thus no valid path through
any sentence in the corpus. As bigrams are selected,
we can begin to cover subsequences and eventually
full sentences. There may be multiple valid taggings
for a sentence, so after each new bigram is selected,
we run the Viterbi algorithm over the raw corpus us-
ing the set of selected tag bigrams as a hard con-
straint on the allowable transitions. This efficiently
identifies the highest-weight path through each sen-
tence, if one exists. If such a path is found, we re-
move the sentence from the corpus and store the tags
from the Viterbi tagging. The algorithm terminates
when a path is found for every raw corpus sentence.
The result of weighted model minimization is this
set of tag paths. Since each path represents a valid
tagging of the sentence, we use this output as a nois-
ily labeled corpus for initializing EM in stage three.
3.3 Tagger training
Stage one provides an expansion of the initial la-
beled data and stage two turns that into a corpus of
noisily labeled sentences. Stage three uses the EM
algorithm initialized by the noisy labeling and con-
strained by the expanded tag dictionary to produce
an HMM.10 The initial distributions are smoothed
with one-count smoothing (Chen and Goodman,
1996). If human-tagged sentences are available as
training data, then we use their counts to supplement
the noisy labeled text for initialization and we add
their counts into every iteration?s result.
The HMM produced by stage three is not used
directly for tagging since it will contain zero-
probabilities for test-corpus words that were unseen
during training. Instead, we use it to provide a
Viterbi labeling of the raw corpus, following the
?auto-supervision? step of Garrette and Baldridge
(2012). This material is then concatenated with the
token-supervised corpus (when available), and used
to train a Maximum Entropy Markov Model tag-
ger.11 The MEMM exploits subword features and
10An added benefit of this strategy is that the EM algorithm
with the expanded dictionary runs much more quickly than
without it since it does not have to consider every possible tag
for unknown words, averaging 20x faster on PTB experiments.
11We use OpenNLP: opennlp.apache.org.
generally produces 1-2% better results than an HMM
trained on the same material.
4 Experiments12
Experimental results are shown in Table 2. Each ex-
periment starts with an initial data set provided by
annotator A or B. Experiment (1) simply uses EM
with the initial small tag dictionary to learn a tag-
ger from the raw corpus. (2) uses LP to infer an ex-
panded tag dictionary and tag distributions over raw
corpus tokens, but then takes the highest-weighted
tag from each token for use as noisily-labeled train-
ing data to initialize EM. (3) performs greedy model-
minimization on the LP output to derive that noisily-
labeled corpus. Finally, (4) is the same as (3), but
additionally uses external dictionary nodes in the LP
graph. In the case of token-supervision, we also in-
clude (0), in which we simply used the tagged sen-
tences as supervised data for an HMM without EM
(followed by MEMM training).
The results show that performance improves with
our LP and minimization techniques compared to
basic EM-HMM training. LP gives large across-the-
board improvements over EM training with only the
original tag dictionary (compare columns 1 & 2).
Weighted model minimization further improves re-
sults for type-supervision settings, but not for token
supervision (compare 2 & 3).
Using an external dictionary in the LP graph has
little effect for KIN, probably due to the available
dictionary?s very small size. However, MLG with
its larger dictionary obtains an improvement in both
scenarios. Results on ENG are mixed; this may be
because the PTB tagset has 45 tags (far more than
the dictionary) so the external dictionary nodes in
the LP graph may consequently serve to collapse dis-
tinctions (e.g. singular and plural) in the larger set.
Our results show differences between token- and
type-supervised annotations. Tag dictionary expan-
sion is helpful no matter what the annotations look
like: in both cases, the initial dictionary is too
small for effective EM learning, so expansion is nec-
essary. However, model minimization only ben-
efits the type-supervised scenarios, leaving token-
supervised performance unchanged. This suggests
12Our code is available at github.com/dhgarrette/
low-resource-pos-tagging-2013
143
Human Annotations 0. No EM 1. EM only 2. With LP 3. LP+min 4. LP(ed)+min
Initial data T K U T K U T K U T K U T K U
KIN tokens A 72 90 58 55 82 32 71 86 58 71 86 58 71 86 58
KIN types A 63 77 32 78 83 69 79 83 70 79 83 70
MLG tokens B 74 89 49 68 87 39 74 89 49 74 89 49 76 90 53
MLG types B 71 87 46 72 81 57 74 86 56 76 86 60
ENG tokens A 63 83 38 62 83 37 72 85 55 72 85 55 72 85 56
ENG types A 66 76 37 75 81 56 76 83 56 74 81 55
ENG tokens B 70 87 44 70 87 43 78 90 60 78 90 60 78 89 61
ENG types B 69 83 38 75 82 61 78 85 61 78 86 61
Table 2: Experimental results. Three languages are shown: Kinyarwanda (KIN), Malagasy (MLG), and
English (ENG). The letters A and B refer to the annotator. LP(ed) refers to label propagation including nodes
from an external dictionary. Each result given as percentages for Total (T), Known (K), and Unknown (U).
that minimization is working as intended: it induces
frequency information when none is provided. With
token-supervision, the annotator provides some in-
formation about which tag transitions are best and
which emissions are most likely. This is miss-
ing with type-supervision, so model minimization is
needed to bootstrap word/tag frequency guesses.
This leads to perhaps our most interesting result:
in a time-critical annotation scenario, it seems better
to collect a simple tag dictionary than tagged sen-
tences. While the tagged sentences certainly contain
useful information regarding tag frequencies, our
techniques can learn this missing information auto-
matically. Thus, having wider coverage of word type
information, and having that information be focused
on the most frequent words, is more important. This
can be seen as a validation of the last two decades
of work on (simulated) type-supervision learning for
POS-tagging?with the caveat that the additional ef-
fort we do is needed to realize the benefit.
Our experiments also allow us to compare how the
data from different annotators affects the quality of
taggers learned. Looking at the direct comparison
on English data, annotator B was able to tag more
sentences than A, but A produced more tag dictio-
nary entries in the type-supervision scenario. How-
ever, it appears, based on the EM-only training, that
the annotations provided by B were of higher quality
and produced more accurate taggers in both scenar-
ios. Regardless, our full training procedure is able
to substantially improve results in all scenarios.
Table 3 gives the recall and precision of the tag
Tag Dictionary Source R P
(1) human-annotated TD 18.42 29.33
(2) LP output 35.55 2.62
(3) model min output 30.49 4.63
Table 3: Recall (R) and precision (P) for tag dictio-
naries versus the test data in a ?MLG types B? run.
dictionaries for MLG for settings 1, 2 and 3. The ini-
tial, human-provided tag dictionary unsurprisingly
has the highest precision and lowest recall. LP ex-
pands that data to greatly improve recall with a large
drop in precision. Minimization culls many entries
and improves precision with a small relative loss in
recall. Of course, this is only a rough indicator of
the quality of the tag dictionaries since the word/tag
pairs of the test set only partially overlap with the
raw training data and annotations.
Because gold-standard annotations are available
for the English sentences, we also ran oracle ex-
periments using labels from the PTB corpus (es-
sentially, the kind of data used in previous work).
We selected the same amount of labeled tokens or
word/tag pairs as were obtained by the annotators.
We found similar patterns of improved performance
by using LP expansion and model minimization,
and all accuracies are improved compared to their
human-annotator equivalents (about 2-6%). Overall
accuracy for both type and token supervision comes
to 78-80%.
144
#Errors 11k 6k 5k 4k 3k
Gold TO NNP NN JJ NNP
Model IN NN JJ NN JJ
Table 4: Top errors from an ?ENG types B? run.
Error Analysis One potential source of errors
comes directly from the annotators themselves.
Though our approach is designed to be robust to an-
notation errors, it cannot correct all mistakes. For
example, for the ?ENG types B? experiment, the an-
notator listed IN (preposition) as the only tag for
word type ?to?. However, the test set only ever as-
signs tag TO for this type. This single error accounts
for a 2.3% loss in overall tagging accuracy (Table 4).
In many situations, however, we are able to auto-
matically remove improbable tag dictionary entries,
as shown in Table 5. Consider the word type ?for?.
The annotator has listed RP (particle) as a potential
tag, but only five out of 4k tokens have this tag. With
RP included, EM becomes confused and labels a ma-
jority of the tokens as RP when nearly all should be
labeled IN. We are able to eliminate RP as a possi-
bility, giving excellent overall accuracy for the type.
Likewise for the comma type, the annotator has in-
correctly given ?:? as a valid tag, and LP, which
uses the tag dictionary, pushes this label to many to-
kens with high confidence. However, minimization
is able to correct the problem.
Finally, the word type ?opposition? provides an
example of the expected behavior for unknown
words. The type is not in the tag dictionary, so
EM assumes all tags are valid and uses many labels.
LP expands the starting dictionary to cover the type,
limiting it to only two tags. Minimization then de-
termines that NN is the best tag for each token.
5 Related work
Goldberg et al (2008) trained a tagger for Hebrew
using a manually-created lexicon which was not de-
rived from an annotated corpus. However, their lexi-
con was constructed by trained lexicographers over a
long period of time and achieves very high coverage
of the language with very good quality. In contrast,
our annotated data was created by untrained linguis-
tics students working alone for just two hours.
Cucerzan and Yarowsky (2002) learn a POS-
for *IN *RP JJ NN CD
(1) EM 1,221 2764 9 5
(2) LP 4,003
(3) min 4,004 1
gold 3,999 5
, (comma) *, *: JJS PTD VBP
(1) EM 24,708 4 3 3
(2) LP 15,505 9226 1
(3) min 24,730
gold 24,732
opposition NN JJ DT NNS VBP
(1) EM 24 4 1 4 4
(2) LP 41 4
(3) min 45
gold 45
Table 5: Tag assignments in different scenarios. A
star indicates an entry in the human-provided TD.
tagger from existing linguistic resources, namely a
dictionary and a reference grammar, but these re-
sources are not available, much less digitized, for
most under-studied languages.
Subramanya et al (2010) apply LP to the prob-
lem of tagging for domain adaptation. They con-
struct an LP graph that connects tokens in low- and
high-resource domains, and propagate labels from
high to low. This approach addresses the prob-
lem of learning appropriate tags for unknown words
within a language, but it requires that the language
have at least one high-resource domain as a source
of high quality information. For low-resource lan-
guages that have no significant annotated resources
available in any domain, this technique cannot be
applied.
Das and Petrov (2011) and Ta?ckstro?m et al
(2013) learn taggers for languages in which there
are no POS-annotated resources, but for which par-
allel texts are available between that language and a
high-resource language. They project tag informa-
tion from the high-resource language to the lower-
resource language via alignments in the parallel text.
However, large parallel corpora are not available for
most low-resource languages. These are also ex-
pensive resources to create and would take consid-
erably more effort to produce than the monolingual
resources that our annotators were able to generate
145
in a two-hour timeframe. Of course, if they are avail-
able, such parallel text links could be incorporated
into our approach.
Furthermore, their approaches require the use of
a universal tag set shared between both languages.
As such, their approach is only able to induce POS
tags for the low-resource language if the same tag
set is used to tag the high-resource language. Our
approach does not rely on any such universal tag
set; we learn whichever tags the human annotator
chooses to use when they provide their annotations.
In fact, in our experiments we learn much more de-
tailed tag sets than the fairly coarse universal tag set
used by Das and Petrov (2011) or Ta?ckstro?m et al
(2013): we learn a tagger for the full Penn Treebank
tag set of 45 tags versus the 12 tags in the universal
set.
Ding (2011) constructed an LP graph for learning
POS tags on Chinese text by propagating labels from
an initial tag dictionary to a larger set of data. This
LP graph contained Wiktionary word/POS relation-
ships as features as well as Chinese-English word
alignment information and used it to directly esti-
mate emission probabilities to initialize an EM train-
ing of an HMM.
Li et al (2012) train an HMM using EM and an
initial tag dictionary derived from Wiktionary. Like
Das and Petrov (2011), they use a universal POS tag
set, so Wiktionary can be directly applied as a wide-
coverage tag dictionary in their case. Additionally,
they evaluate their approach on languages for which
Wiktionary has high coverage?which would cer-
tainly not get far with Kinyarwanda (9 entries). Our
approach does not rely on a high-coverage tag dic-
tionary nor is it restricted to use with a small tag set.
6 Conclusions and future work
With just two hours of annotation, we obtain 71-78%
accuracy for POS-tagging across three languages us-
ing both type and token supervision. Without tag
dictionary expansion and model minimization, per-
formance is much worse, from 63-74%. We dramat-
ically improve performance on unknown words: the
range of 37-58% improves to 53-70%.
We also have a provisional answer to whether an-
notation should be on types or tokens: use type-
supervision if you also expand and minimize. These
methods can identify missing word/tag entries and
estimate frequency information, and they produce as
good or better results compared to starting with to-
ken supervision. The case of Kinyarwanda was most
dramatic: 71% accuracy for token-supervision com-
pared to 79% for type-supervision. Studies using
more annotators and across more languages would
be necessary to make any stronger claim about the
relative efficacy of the two strategies.
Acknowledgements
We thank Kyle Jerro, Vijay John, Katrin Erk,
Yoav Goldberg, Ray Mooney, Slav Petrov, Oscar
Ta?ckstro?m, and the reviewers for their assistance
and feedback. This work was supported by the U.S.
Department of Defense through the U.S. Army Re-
search Office (grant number W911NF-10-1-0533)
and via a National Defense Science and Engineer-
ing Graduate Fellowship for the first author. Experi-
ments were run on the UTCS Mastodon Cluster, pro-
vided by NSF grant EIA-0303609.
References
Michele Banko and Robert C. Moore. 2004. Part-of-
speech tagging in context. In Proceedings of COLING,
Geneva, Switzerland.
Stanley F. Chen and Joshua Goodman. 1996. An empir-
ical study of smoothing techniques for language mod-
eling. In Proceedings of ACL, Santa Cruz, California,
USA.
Silviu Cucerzan and David Yarowsky. 2002. Boot-
strapping a multilingual part-of-speech tagger in one
person-day. In Proceedings of CoNLL, Taipei, Taiwan.
Dipanjan Das and Slav Petrov. 2011. Unsupervised part-
of-speech tagging with bilingual graph-based projec-
tions. In Proceedings of ACL-HLT, Portland, Oregon,
USA.
Weiwei Ding. 2011. Weakly supervised part-of-speech
tagging for Chinese using label propagation. Master?s
thesis, University of Texas at Austin.
Dan Garrette and Jason Baldridge. 2012. Type-
supervised hidden Markov models for part-of-speech
tagging with incomplete tag dictionaries. In Proceed-
ings of EMNLP, Jeju, Korea.
Yoav Goldberg, Meni Adler, and Michael Elhadad. 2008.
EM can find pretty good HMM POS-taggers (when
given a good start). In Proceedings ACL.
Kazi Saidul Hasan and Vincent Ng. 2009. Weakly super-
vised part-of-speech tagging for morphologically-rich,
146
resource-scarce languages. In Proceedings of EACL,
Athens, Greece.
Julian Kupiec. 1992. Robust part-of-speech tagging us-
ing a hidden Markov model. Computer Speech & Lan-
guage, 6(3).
Shen Li, Joa?o Grac?a, and Ben Taskar. 2012. Wiki-ly
supervised part-of-speech tagging. In Proceedings of
EMNLP, Jeju Island, Korea.
Christopher D. Manning. 2011. Part-of-speech tagging
from 97% to 100%: Is it time for some linguistics? In
Proceedings of CICLing.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2).
Bernard Merialdo. 1994. Tagging English text with
a probabilistic model. Computational Linguistics,
20(2).
Sujith Ravi and Kevin Knight. 2009. Minimized models
for unsupervised part-of-speech tagging. In Proceed-
ings of ACL-AFNLP.
Sujith Ravi, Ashish Vaswani, Kevin Knight, and David
Chiang. 2010. Fast, greedy model minimization for
unsupervised tagging. In Proceedings of COLING.
Amarnag Subramanya, Slav Petrov, and Fernando
Pereira. 2010. Efficient graph-based semi-supervised
learning of structured tagging models. In Proceedings
EMNLP, Cambridge, MA.
Oscar Ta?ckstro?m, Dipanjan Das, Slav Petrov, Ryan Mc-
Donald, and Joakim Nivre. 2013. Token and type con-
straints for cross-lingual part-of-speech tagging. In
Transactions of the ACL. Association for Computa-
tional Linguistics.
Partha Pratim Talukdar and Koby Crammer. 2009. New
regularized algorithms for transductive learning. In
Proceedings of ECML-PKDD, Bled, Slovenia.
Kristina Toutanova and Mark Johnson. 2008. A
Bayesian LDA-based model for semi-supervised part-
of-speech tagging. In Proceedings of NIPS.
147
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 583?592,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Real-World Semi-Supervised Learning
of POS-Taggers for Low-Resource Languages
Dan Garrette1 Jason Mielens2
1Department of Computer Science 2Department of Linguistics
The University of Texas at Austin The University of Texas at Austin
dhg@cs.utexas.edu {jmielens,jbaldrid}@utexas.edu
Jason Baldridge2
Abstract
Developing natural language processing
tools for low-resource languages often re-
quires creating resources from scratch.
While a variety of semi-supervised meth-
ods exist for training from incomplete
data, there are open questions regarding
what types of training data should be used
and how much is necessary. We dis-
cuss a series of experiments designed to
shed light on such questions in the con-
text of part-of-speech tagging. We obtain
timed annotations from linguists for the
low-resource languages Kinyarwanda and
Malagasy (as well as English) and eval-
uate how the amounts of various kinds
of data affect performance of a trained
POS-tagger. Our results show that an-
notation of word types is the most im-
portant, provided a sufficiently capable
semi-supervised learning infrastructure is
in place to project type information onto
a raw corpus. We also show that finite-
state morphological analyzers are effective
sources of type information when few la-
beled examples are available.
1 Introduction
Low-resource languages present a particularly dif-
ficult challenge for natural language processing
tasks. For example, supervised learning meth-
ods can provide high accuracy for part-of-speech
(POS) tagging (Manning, 2011), but they per-
form poorly when little supervision is avail-
able. Good results in weakly-supervised tagging
have been obtained by training sequence models
such as hidden Markov models (HMM) using the
Expectation-Maximization algorithm (EM), how-
ever most work in this area has still relied on rel-
atively large amounts of data, both annotated and
unannotated, as well as an assumption that the an-
notations are very clean (Kupiec, 1992; Merialdo,
1994).
The ability to learn taggers using very little data
is enticing: only a tiny fraction of the world?s lan-
guages have enough data for standard supervised
models to work well. The collection or develop-
ment of resources is a time-consuming and expen-
sive process, creating a significant barrier for an
under-studied language where there are few ex-
perts and little funding. It is thus important to
develop approaches that achieve good accuracy
based on the amount of data that can be reasonably
obtained, for example, in just a few hours by a lin-
guist doing fieldwork on a non-native language.
Previous work explored learning taggers from
weak information, but the type, amount, quality,
and sources of data raise questions about the appli-
cability of those results to real-world low-resource
scenarios (Toutanova and Johnson, 2008; Ravi and
Knight, 2009; Hasan and Ng, 2009; Garrette and
Baldridge, 2012). Most research simulated weak
supervision with tag dictionaries extracted from
existing large, expertly-annotated corpora. These
resources have been developed over long periods
of time by trained annotators who collaborate to
produce high-quality analyses. They are also bi-
ased towards including only the most likely tag
for each word type, resulting in a cleaner dictio-
nary than one would find in a real scenario. As
such, these experiments do not reflect real-world
constraints.
One exception to this work is Goldberg et al
(2008): they use a manually-constructed lexicon
for Hebrew in order to learn an HMM tagger. How-
ever, this lexicon was constructed by trained lexi-
cographers over a long period of time and achieves
very high coverage of the language with very good
quality, much better than could be achieved by
our non-expert linguistics graduate student anno-
tators in just a few hours. Cucerzan and Yarowsky
583
(2002) learn a POS-tagger from existing linguis-
tic resources, namely a dictionary and a refer-
ence grammar, but these resources are not avail-
able, much less digitized, for most under-studied
languages. Haghighi and Klein (2006) develop a
model in which a POS-tagger is learned from a list
of POS tags and just three ?prototype? word types
for each tag, but their approach requires a vector
space to compute the distributional similarity be-
tween prototypes and other word types in the cor-
pus. Such distributional models are not feasible
for low-resource languages because they require
immense amounts of raw text, much more than is
available in these settings (Abney and Bird, 2010).
Further, they extracted their prototype lists directly
from a labeled corpus, something we are specif-
ically avoiding. Ta?ckstro?m et al (2013) evalu-
ate the use of mixed type and token constraints
generated by projecting information from a high-
resource language to a low-resource language via
a parallel corpus. However, large parallel corpora
are not available for most low-resource languages.
These are also expensive resources to create and
would take considerably more effort to produce
than the monolingual resources that our annotators
were able to generate in a four-hour timeframe.
Of course, if they are available, such parallel text
links could be incorporated into our approach.
In our previous work, we developed a differ-
ent strategy based on generalizing linguistic input
with a computational model: linguists annotated
either types or tokens for two hours, these anno-
tations are projected onto a corpus of unlabeled
tokens using label propagation and HMMs, and
a final POS-tagger is trained on this larger auto-
labeled corpus (Garrette and Baldridge, 2013).
That approach uses much more realistic types
and quantities of resources than previous work;
nonetheless, it leaves many open questions regard-
ing the effectiveness of incrementally more anno-
tation, the role of unannotated data, and whether
there is a good balance to be found using a combi-
nation of type- and token-supervision. We also did
not consider morphological analyzers as a form
of type supervision, as suggested by Merialdo
(1994).
This paper addresses these questions via a se-
ries of experiments designed to quantify the ef-
fect on performance given by the amount of time
spent finding or annotating training materials. We
specifically look at the impact of four types of data
collection:
1. Time annotating sentences (token supervision)
2. Time creating tag dictionary (type supervision)
3. Time constructing a finite state transducer
(FST) to analyze word-type morphology
4. Amount of raw data available for training
We explore these strategies in the context of POS-
tagging for Kinyarwanda and Malagasy. We also
include experiments for English, pretending as
though it is a low-resource language. The over-
whelming take away from our results is that type
supervision?when backed by an effective semi-
supervised learning approach?is the most impor-
tant source of linguistic information. Also, mor-
phological analyzers help for morphologically rich
languages when there are few labeled types or to-
kens (and, it never hurts to use them). Finally, per-
formance improves with more raw data, though we
see diminishing returns past 400,000 tokens. With
just four hours of type annotation, our system ob-
tains good accuracy across the three languages:
89.8% on English, 81.9% on Kinyarwanda, and
81.2% on Malagasy.
Our results compare favorably with previous
work despite using considerably less supervision
and a more difficult set of tags. For example, Li et
al. (2012) use the entirety of English Wiktionary
directly as a tag dictionary to obtain 87.1% accu-
racy on English, below our result. Ta?ckstro?m et al
(2013) average 88.8% across 8 major languages,
but for Turkish, a morphologically rich language,
they achieve only 65.2%, significantly below our
81.9% for morphologically-rich Kinyarwanda.
2 Data
Kinyarwanda (KIN) and Malagasy (MLG) are low-
resource, KIN is morphologically rich, and English
(ENG) is used for comparison. For each language,
sentences were divided into four sets: training data
to be labeled by annotators, raw training data, de-
velopment data, and test data.
Data sources The KIN texts are transcripts of
testimonies by survivors of the Rwandan geno-
cide provided by the Kigali Genocide Memorial
Center. The MLG texts are articles from the web-
sites1 Lakroa and La Gazette and Malagasy Global
Voices.2 Texts in both KIN and MLG were tok-
1www.lakroa.mg and www.lagazette-dgi.com
2mg.globalvoicesonline.org/
584
KIN MLG ENG - Experienced ENG - Novice
time type token type token type token type token
1:00 801 559 (1093) 660 422 (899) 910 522 (1124) 210 308 (599)
2:00 1814 948 (2093) 1363 785 (1923) 2660 1036 (2375) 631 646 (1429)
3:00 2539 1324 (3176) 2043 1082 (3064) 4561 1314 (3222) 1350 953 (2178)
4:00 3682 1651 (4119) 2773 1378 (4227) 6598 1697 (4376) 2185 1220 (2933)
Table 1: Annotations for each language and annotator as time increases. Shows the number of tag
dictionary entries from type annotation vs. token. (The count of labeled tokens is shown in parentheses).
For brevity, the table only shows hourly progress.
enized and labeled with POS tags by two linguis-
tics graduate students, each of which was studying
one of the languages. The KIN and MLG data have
12 and 23 distinct POS tags, respectively.
The Penn Treebank (PTB) (Marcus et al, 1993)
is used as ENG data. Section 01 was used for
token-supervised annotation, sections 02-14 were
used as raw data, 15-18 for development of the
FST, 19-21 as a dev set and 22-24 as a test set.
The PTB uses 45 distinct POS tags.
Collecting annotations Linguists with non-
native knowledge of KIN and MLG produced anno-
tations for four hours (in 30-minute intervals) for
two tasks. In the first task, type-supervision, the
annotator was given a list of the words in the tar-
get language (ranked from most to least frequent),
and they annotated each word type with its poten-
tial POS tags. The word types and frequencies used
for this task were taken from the raw training data
and did not include the test sets. In the second
task, token-supervision, full sentences were anno-
tated with POS tags. The 30-minute intervals allow
us to investigate the incremental benefit of addi-
tional annotation of each type as well as how both
annotation types might be combined within a fixed
annotation budget.
Baldridge and Palmer (2009) found that anno-
tator expertise greatly influences effectiveness of
active learning for morpheme glossing, a related
task. To see how differences in annotator speed
and quality impact our task, we obtained ENG data
from an experienced annotator and a novice one.
Ngai and Yarowsky (2000) investigated the ef-
fectiveness of rule-writing versus annotation (us-
ing active learning) for chunking, and found the
latter to be far more effective. While we do not
explore a rule-writing approach to POS-tagging,
we do consider the impact of rule-based morpho-
logical analyzers as a component in our semi-
supervised POS-tagging system.
ENG - Exp. ENG - Nov.
time type tok type tok
1:00 0.05 0.03 0.01 0.02
2:00 0.15 0.05 0.03 0.03
3:00 0.24 0.06 0.07 0.05
4:00 0.32 0.08 0.11 0.06
Table 2: Tag dictionary recall against the test set
for ENG annotators on type and token annotations.
Annotations Table 1 gives statistics for all lan-
guages and annotators showing progress during
the 4-hour tasks. With token-annotation, tag
dictionary growth slows because high-frequency
words are repeatedly annotated, producing only
additional frequency and sequence information.
In contrast, every type-annotation label is a new
tag dictionary entry. For types, growth increases
over time, reflecting the fact that high-frequency
words (which are addressed first) tend to be more
ambiguous and thus require more careful thought
than later words. For ENG, we can compare the
tagging speed of the experienced annotator with
the novice: 50% more tokens and 3 times as many
types. The token-tagging speed stayed fairly con-
stant for the experienced annotator, but the novice
increased his rate, showing the result of practice.
Checking the annotators? output against the
gold tags in the PTB shows that both had good
tagging accuracy on tokens: 94-95%. Comparing
the tag dictionary entries versus the test data, pre-
cision starts in the high 80%s and falls to to the
mid-70%s in all cases. However, the differences
in recall, shown in Table 2, are more interesting.
On types, the experienced annotator maxed out at
32%, but the novice only reaches 11%. More-
over, the maximum for token annotations is much
lower due to high repeat-annotation. The discrep-
ancies between experienced and novice, and be-
tween type and token recall explain a great deal of
the performance disparity seen in the experiments.
585
3 Morphological Transducers
Finite-state transducers (FSTs) accept regular lan-
guages and can be constructed easily using regu-
lar expressions, which makes them quite useful for
phonology, morphology and limited areas of syn-
tax (Karttunen, 2001). Past work has used FSTs
for direct POS-tagging (Roche and Schabes, 1995),
but this requires tight coupling between the FST
and target tagset. We use FSTs for morphologi-
cal analysis: the FST accepts a word type and pro-
duces a set of morphological features. If there are
multiple possible analyses for a given word type,
the FST returns them all. For instance the Kin-
yarwanda verb sibatarazuka ?he is not yet resur-
rected? is analyzed in several ways:
? +NEG+CL2+1PL+V+arazuk+IMP
? +NEG+CL2+NOT.YET+PRES+zuk+IMP
? +NEG+CL2+NOT.YET+razuk+IMP
FSTs are particularly valuable for their ability
to analyze out-of-vocabulary items. By looking
for known affixes, FSTs can guess the stem of
a word and produce an analysis despite not hav-
ing knowledge of that stem. For morphologically
complex languages like KIN, this ability is espe-
cially useful. Other factors, such as a large num-
ber of morphologically-conditioned phonological
changes (seen in MLG) make out-of-vocabulary
guessing more challenging because of the large
number of potential stems (high ambiguity).
Development of the FSTs for all three languages
was done by iteratively adding rules and lexical
items with the goal of increasing coverage on a
raw dataset. To accomplish this on a fixed time
budget, the most frequently occurring unanalyzed
tokens were examined, and their stems plus any
observable morphological or phonological pat-
terns were added to the transducer. Addition-
ally, developers searched for known morpholog-
ical alternations to locate instances of phonolog-
ical change for inclusion. Coverage was checked
against a raw dataset which did not include the test
data used for the POS experiments.
The KIN and MLG FSTs were created by
English-speaking linguists who were familiar with
their respective language. They also used dictio-
naries and grammars. Each FST was developed
in 10 hours. To evaluate the benefits of more de-
velopment time, a version of the English FST was
saved every 30 minutes, as shown in Table 3.
elapsed
time
tokens types
count pct count pct
2:00 130k 61% 2.1k 12%
4:00 159k 75% 4.1k 24%
6:00 170k 80% 6.7k 39%
8:00 182k 86% 7.7k 44%
10:00 192k 91% 10.7k 62%
Table 3: Coverage of the English morphological
FST during development. For brevity, showing 2-
hour increments instead of 30-minute segments.
tokens types
cov. ambig. cov. ambig.
KIN 86% 2.62 82% 5.31
MLG 78% 2.98 37% 1.13
ENG 91% 1.19 62% 1.97
Table 4: Coverage and ambiguity of the final FST
for each language.
4 Approach
Learning under low-resource conditions is more
difficult than scenarios in most previous POS work
because the vast majority of the word types in the
training and test data are not covered by the an-
notations. When most words are unknown, learn-
ing algorithms such as EM struggle (Garrette and
Baldridge, 2012). Recall that most work on learn-
ing POS-taggers from tag dictionaries used tag dic-
tionaries culled from test sets (even when consid-
ering incomplete dictionaries). We thus build on
our previous approach, which exploits extremely
sparse, human-generated annotations that are pro-
duced without knowledge of which words appear
in the test set (Garrette and Baldridge, 2013).
This approach generalizes a small initial tag dic-
tionary to include unannotated word types appear-
ing in raw data. It estimates word/tag pair and
tag-transition frequency information using model-
minimization, which also reduces noise intro-
duced by automatic tag dictionary expansion. The
approach exploits type annotations effectively to
learn parameters for out-of-vocabulary words and
infer missing frequency and sequence informa-
tion. This pipeline is described in detail in the
previous work, so we give only a brief overview
and describe our additions.
The purpose of tag dictionary expansion is to es-
timate label distributions for tokens in a raw cor-
586
pus, including words missing in the annotations.
For this, a graph connecting annotated words to
unannotated words via features is constructed and
POS labels are pushed between these items using
label propagation (LP) (Talukdar and Crammer,
2009). LP has been used successfully for extend-
ing POS labels from high-resource languages to
low via parallel corpora (Das and Petrov, 2011;
Ta?ckstro?m et al, 2013; Ding, 2011) or high- to
low-resource domains (Subramanya et al, 2010),
among other tasks. These works have typically
used n-gram features (capturing basic syntax) and
character affixes (basic morphology).
The character n-gram affix-as-morphology ap-
proach produces many features, but only a fraction
of them represent actual morphemes. Incorrect
features end up pushing noise around the graph,
so affixes can lead to more false labels that drown
out the true labels. While affixes may be suffi-
cient for languages with limited morphology, their
effectiveness diminishes for morphology-rich lan-
guages, which have much higher type-to-token ra-
tios. More types means sparser word frequency
statistics and more out-of-vocabulary items, and
thus problems for EM. Here, we modify the LP
graph by supplementing or replacing generic af-
fix features with a focused set of morphological
features produced by an FST. These targeted mor-
phological features are effective during LP because
words that share them are much more likely to ac-
tually share POS tags.
FSTs produce multiple analyses, which is actu-
ally advantageous for LP. Ambiguities need not be
resolved since we just take the union of all mor-
phological features for all analyses and use them
as features in the graph. Note that each FST pro-
duces its own POS-tags as features, but these do
not correspond to the target POS tagset used by the
tagger. This is important because it decouples FST
development and the final POS task. Thus, any FST
for the language, regardless of its provenance, can
be used with any target POS tagset.
Since the LP graph contains a node for each cor-
pus token, and each node is labeled with a distri-
bution over POS tags, the graph provides a corpus
of sentences labeled with noisy tag distributions
along with an expanded tag dictionary. This out-
put is useful as input to EM because it contains
labels for all seen word types as well as sequence
and frequency information. There is a high degree
of noise in the LP output, so we employ the model
minimization strategy of Ravi et al (2010), which
finds a minimal set of tag bigrams needed to ex-
plain the sentences in the raw corpus. It outputs
a corpus of tagged sentences, which are used as
a good starting point for EM training of an HMM.
The expanded tag dictionary constrains the EM
search space by providing a limited tagset for each
word type, steering EM towards a desirable result.
Because the HMM trained by EM will con-
tain zero-probabilities for words that did not ap-
pear in the training corpus, we use the ?auto-
supervision? step from our previous work: a Max-
imum Entropy Markov Model tagger is trained
on a corpus that is noisily labeled by the HMM
(Garrette and Baldridge, 2012). While training
an HMM before the MEMM is not strictly neces-
sary, our tests have shown that this generative-
then-discriminative combination generally results
in around 3% accuracy improvement.
5 Experiments3
To better understand the effect that each type of
supervision has on tagger accuracy, we perform a
series of experiments, with KIN and MLG as true
low-resource languages. English experiments, for
which we had both experienced and novice an-
notators, allow for further exploration into issues
concerning data collection and preparation.
The overall best accuracies achieved by lan-
guage are 81.9% for KIN using all types, 81.2% for
MLG using half types and half tokens, and 89.8%
for ENG using all types and the maximal amount
of raw data. All of these best values were achieved
using both FST and affix LP features.
All results described in this section are averaged
over five folds of raw data.
5.1 Types versus tokens
Our primary question was the relationship be-
tween annotation type and time. Annotation must
be done by someone familiar with the target lan-
guage, linguistics, and the target POS tagset. For
many low-resource languages, such people, and
the time they have to spend, are likely to be in
short supply. To make the best use of their time,
we need to know which annotations are most use-
3Code and all MLG data available at github.com/
dhgarrette/low-resource-pos-tagging-2013
We are unable to provide the KIN or ENG data for down-
load due to licensing restrictions. However, ENG data may
be shared with those holding a license for the Penn Treebank
and KIN data may be shared on a case-by-case basis.
587
(a) KIN type annotations ? Elapsed Annotation Time
Acc
ura
cy
0:30 1:00 1:30 2:00 2:30 3:00 3:30 4:00
50
55
60
65
70
75
80
No LPAffixes onlyFST onlyAffixes+FST
(b) KIN token annotations ? Elapsed Annotation Time
Acc
ura
cy
0:30 1:00 1:30 2:00 2:30 3:00 3:30 4:00
50
55
60
65
70
75
80
No LPAffixes onlyFST onlyAffixes+FST
(c) MLG type annotations ? Elapsed Annotation Time
Acc
ura
cy
0:30 1:00 1:30 2:00 2:30 3:00 3:30 4:00
65
70
75
80
No LPAffixes onlyFST onlyAffixes+FST
(d) MLG token annotations ? Elapsed Annotation Time
Acc
ura
cy
0:30 1:00 1:30 2:00 2:30 3:00 3:30 4:00
65
70
75
80
No LPAffixes onlyFST onlyAffixes+FST
Figure 1: Annotation time vs. tagger accuracy for type-only and token-only annotations.
Elapsed Annotation Time
Acc
ura
cy
0:30 1:00 1:30 2:00 2:30 3:00 3:30 4:00
65
70
75
80
85
Experienced annotator ? TypesExperienced annotator ? TokensNovice annotator ? TypesNovice annotator ? Tokens
Figure 2: Annotation time vs. tagger accuracy for
ENG type-only and token-only annotations with
affix and FST LP features.
ful so that efforts can be concentrated there. Ad-
ditionally, it is useful to identify when returns on
annotation effort diminish so that annotators do
not spend time doing work that is unlikely to add
much value.
The annotators produced four hours each of
type and token annotations, each in 30-minute in-
crements. To assess the effects of annotation time,
we trained taggers cumulatively on each increment
and determine the value of each additional half-
hour of effort. Results are shown for KIN and MLG
in Figure 1 and ENG in Figure 2. In all scenarios,
the use of LP (and model minimization) delivers
huge performance gains. Additionally, the use of
FST features, usually along with affixes, yielded
better results than without. This indicates the LP
procedure makes effective use of the morpholog-
ical features produced by the FST and that the af-
fix features are able to capture missing information
without adding too much noise to the LP graph.
Furthermore, performance is considerably bet-
ter when type annotations are used than only to-
kens. Type annotations plateau much faster, so
a shorter amount of time must be spent annotat-
ing types than if token annotations are used. For
KIN it takes approximately 1.5 hours to reach near-
maximum accuracy for types, but 2.5 hours for to-
kens. This difference is due to the fact that the type
annotations started with the most frequent words
whereas the token annotations were on random
sentences. Thus, type annotations quickly cover a
significant portion of the language?s tokens. With
annotations directly on tokens, some of the highest
588
(a) KIN ? Type/Token Annotation Mixture
Acc
ura
cy
t0/s8 t1/s7 t2/s6 t3/s5 t4/s4 t5/s3 t6/s2 t7/s1 t8/s0
60
65
70
75
80
No LPAffixes onlyFST onlyAffixes+FST
(b) MLG ? Type/Token Annotation Mixture
Acc
ura
cy
t0/s8 t1/s7 t2/s6 t3/s5 t4/s4 t5/s3 t6/s2 t7/s1 t8/s0
70
72
74
76
78
80
No LPAffixes onlyFST onlyAffixes+FST
Figure 3: Annotation mixture vs. tagger accuracy. X-axis labels give annotation proportions, e.g. ?t2/s6?
indicates 2/8 of the time (1 hour) was spent annotating types and 6/8 (3 hours), full sentences.
Type/Token Annotation Mixture
Acc
ura
cy
t0/s8 t1/s7 t2/s6 t3/s5 t4/s4 t5/s3 t6/s2 t7/s1 t8/s0
70
75
80
85
Exp. ? With LPNov. ? With LPExp. ? No LPNov. ? No LP
Figure 4: Annotation mixture vs. tagger accuracy
on ENG using affix and FST LP features for experi-
enced (Exp.) and novice (Nov.) annotators.
frequency types are covered, but annotation time
is also ineffectively used on low-frequency types
that happen to appear in those sentences.
Finally, the use of FST features yields the largest
gains for KIN, but only when small amounts of
annotation are available. This makes sense: KIN
is a morphologically rich language, so sparsity is
greater and crude affixes capture less actual mor-
phology. With little annotated data, LP relies heav-
ily on morphological features to make clean links
between words. But, with more annotations, the
gains of the FST over affix features alone dimin-
ishes: the affix features eventually capture enough
of the morphology to make up the difference.
Figure 2 shows the dramatic differences be-
tween the experienced and novice ENG annota-
tors.4 For the former, results using types and to-
4The ENG graph omits ?No LP? results since they fol-
lowed patterns similar to KIN and MLG. Additionally, the
results without FST features are not shown because they were
nearly identical (though slightly lower) than with the FST.
kens were similar after 30 minutes, but type an-
notations proved much more useful beyond that.
In contrast, the novice annotated types much more
slowly, so early on there were not enough anno-
tated types for the training to be as effective. Even
so, after three hours of annotation, type annota-
tions still win with the novice, and even beat the
experienced annotator labeling tokens.
5.2 Mixing type and token annotations
Because type and token annotations are each bet-
ter at providing different information ? a tag dic-
tionary of high-frequency words vs. sequence and
frequency information ? it is reasonable to ex-
pect that a combination of the two might yield
higher performance by each contributing differ-
ent but complementary information during train-
ing. This matters in low-resource settings because
type or token annotations will likely be produced
by the same people, so there is a tradeoff between
spending resources on one form of annotation over
the other. Understanding the best mixture of an-
notations can inform us on how to maximize the
benefit of a set annotation budget. To this end, we
ran experiments fixing the annotation time to four
hours while varying the mix of type and token an-
notations. Results are shown for KIN and MLG in
Figure 3 and ENG in Figure 4.
For KIN and ENG, tagger accuracy increases as
the proportion of type annotations increases for all
LP feature configurations. For MLG, however, as
the reliance on the FST increases, the optimal mix-
ture shifts toward higher type proportions. When
only affix features are used, the optimal mixture is
1 hour of types and 3 hours of tokens. When FST
and affix features are used, the optimum is 2 hours
589
each of types and tokens. When only FST features
are used, it is best to use 3.5 hours of types and
only 30 minutes of tokens. Because the FST op-
erates on word types, it is effective at exploiting
type annotations. Thus, when the LP focuses more
on FST features, it becomes more desirable to have
larger amounts of type annotations.
Types clearly win for ENG. The experienced an-
notator was much faster at annotating types and
the speed difference was less pronounced for to-
kens, so accuracy is most similar when only token
annotations are used. The performance disparity
grows with increasing the type proportion.
Ta?ckstro?m et al (2013) explore the use of
mixed type and token annotations in which a tag-
ger is learned by projecting information via par-
allel text. In their experiments, they?like us?
found that type information is more valuable than
token information. However, they were able to see
gains through the complementary effects of mix-
ing type and token annotations. It is likely that this
difference in our results is due to the amount of an-
notated data used. It seems that the amount of type
information collected in four hours is not sufficient
to saturate the system, meaning that switching to
annotating tokens tends to hurt performance.
5.3 FST development
The third set of experiments evaluate how the
amount of time spent developing an FST affects
the performance of trained tagger. To do this,
we had our ENG FST developer save progress af-
ter each hour (for ten hours). The results show
that, for ENG, the FST provided no value, regard-
less of how much time was spent on its develop-
ment. Moreover, since large gains in accuracy can
be achieved by spending a small amount of time
just annotating word types with POS tags, we are
led to conclude that time should be spent annotat-
ing types or tokens instead of developing an FST.
While it is likely that FST development time would
have a greater impact for morphologically rich
languages, we suspect that greater gains can still
be obtained by instead annotating types. Nonethe-
less, FSTs never seems to hurt performance, so if
one is readily available, it should be used.
5.4 The effect of more raw data
In addition to annotations, semi-supervised tagger
training requires a corpus of raw text. Raw data
can be easier to acquire since it does not need
the attention of a linguist. Even so, for many
Number of Raw Data Tokens
Acc
ura
cy
100k 200k 300k 400k 500k 600k
80
82
84
86
88
90
4hr types, FST, With LP4hr types, FST, No LP1hr types, No FST, With LP
Figure 5: Amount of raw data vs. tagger accuracy
for ENG using high vs. low amounts of annotation
and using LP vs. no LP., for experienced annotator
(novice results were similar).
low-resource languages, the amount of digitized
text, such as transcripts or websites, is very lim-
ited and may, in fact, require substantial effort
to accumulate, even with assistance from compu-
tational tools (Bird, 2011). Therefore, the col-
lection of raw data can be considered another
time-sensitive task for which the tradeoffs with
previously-discussed annotation efforts must con-
tend.
It could be the case that more raw data for train-
ing could make up for additional annotation and
FST development effort or make the LP proce-
dure unnecessary. Figure 5 shows that that in-
creased raw data does provide increasing gains,
but they diminish after 200k tokens. The best per-
formance is achieved by using more annotation
and LP. Most importantly, however, removing ei-
ther annotations or LP results in a significant de-
cline in accuracy, such that even with 600k train-
ing tokens, we are unable to achieve the results of
high annotation and LP using only 100k tokens.
5.5 Correcting existing annotations
For all of the ENG experiments, we also ran ?or-
acle? experiments using gold tags for the same
sentences or a tag dictionary containing the same
number of type/tag entries as the annotator pro-
duced, but containing only the most frequent
entries as determined by the gold-labeled cor-
pus. Using this simulated ?perfect annotator? data
shows we lose accuracy due to annotator mistakes:
for our experienced annotator and maximal FST,
using 4 hours of types the oracle accuracy is 90.5
vs. 88.5 while using only tokens we see 83.9 vs.
590
81.5. This indicates that there are gains to be made
by correcting mistakes in the annotations. This
is true even after the point of diminishing returns
on the learning curve, meaning that even when
adding more annotations no longer improves per-
formance, progress can still be made by correcting
errors, so it may be reasonable to ask annotators to
attempt to correct errors in their past annotations.
Automated techniques for facilitating error identi-
fication can be employed for this (Dickinson and
Meurers, 2003).
6 Conclusions and Future Work
Care must be taken when drawing conclusions
from small-scale annotation studies such as those
presented in this paper. Nonetheless, we have
explored realistic annotation scenarios for POS-
tagging for low-resource languages and found sev-
eral consistent patterns. Most importantly, it is
clear that type annotations are the most useful in-
put one can obtain from a linguist?provided a
semi-supervised algorithm for projecting that in-
formation reliably onto raw tokens is available. In
a sense, this result validates the research trajectory
of efforts of the past two decades put into learning
taggers from tag dictionaries: papers have succes-
sively removed layers of unrealistic assumptions,
and in doing so have produced pipelines for type-
supervision that easily beat token-supervision pre-
pared in comparable amounts of time.
The result of most immediate practical value is
that we show it is possible to train effective POS-
taggers on actual low-resource languages given
only a relatively small amount of unlabeled text
and a few hours of annotation by a non-native
linguist. Instead of having annotators label full
sentences as one might expect the natural choice
would be, it is much more effective to simply
extract a list of the most frequent word types in
the language and concentrate efforts on annotat-
ing these types with their potential parts of speech.
Furthermore, for languages with rich morphology,
a morphological transducer can yield significant
performance gains when large amounts of other
annotated resources are unavailable. (And it never
hurts performance.)
Finally, additional raw text does improve per-
formance. However, using substantial amounts of
raw text is unlikely to produce gains larger than
only a few hours spent annotating types. Thus,
when deciding whether to spend time locating
larger volumes of digitized text or to spend time
annotating types, choose types.
Despite the consistent superiority of type anno-
tations in our experiments, it of course may be the
case that techniques such as active learning may
better select sentences for token annotation, so this
should be explored in future work.
Acknowledgements
We thank Kyle Jerro, Vijay John, Jim Evans, Yoav
Goldberg, Slav Petrov, and the reviewers for their
assistance and feedback. This work was sup-
ported by the U.S. Department of Defense through
the U.S. Army Research Office (grant number
W911NF-10-1-0533) and through a National De-
fense Science and Engineering Graduate Fellow-
ship for the first author. Experiments were run
on the UTCS Mastodon Cluster, provided by NSF
grant EIA-0303609.
References
Steven Abney and Steven Bird. 2010. The human lan-
guage project: Building a universal corpus of the
worlds languages. In Proceedings of ACL.
Jason Baldridge and Alexis Palmer. 2009. How well
does active learning actually work? Time-based
evaluation of cost-reduction strategies for language
documentation. In Proceedings of EMNLP, Singa-
pore.
Steven Bird. 2011. Bootstrapping the language
archive: New prospects for natural language pro-
cessing in preserving linguistic heritage. Linguistic
Issues in Language Technology, 6.
Silviu Cucerzan and David Yarowsky. 2002. Boot-
strapping a multilingual part-of-speech tagger in one
person-day. In Proceedings of CoNLL, Taipei, Tai-
wan.
Dipanjan Das and Slav Petrov. 2011. Unsupervised
part-of-speech tagging with bilingual graph-based
projections. In Proceedings of ACL-HLT, Portland,
Oregon, USA.
Markus Dickinson and W. Detmar Meurers. 2003. De-
tecting errors in part-of-speech annotation. In Pro-
ceedings of EACL.
Weiwei Ding. 2011. Weakly supervised part-of-
speech tagging for Chinese using label propagation.
Master?s thesis, University of Texas at Austin.
Dan Garrette and Jason Baldridge. 2012. Type-
supervised hidden Markov models for part-of-
speech tagging with incomplete tag dictionaries. In
Proceedings of EMNLP, Jeju, Korea.
591
Dan Garrette and Jason Baldridge. 2013. Learning a
part-of-speech tagger from two hours of annotation.
In Proceedings of NAACL, Atlanta, Georgia.
Yoav Goldberg, Meni Adler, and Michael Elhadad.
2008. EM can find pretty good HMM POS-taggers
(when given a good start). In Proceedings ACL.
Aria Haghighi and Dan Klein. 2006. Prototype-
driven learning for sequence models. In Proceed-
ings NAACL.
Kazi Saidul Hasan and Vincent Ng. 2009.
Weakly supervised part-of-speech tagging for
morphologically-rich, resource-scarce languages. In
Proceedings of EACL, Athens, Greece.
Lauri Karttunen. 2001. Applications of finite-state
transducers in natural language processing. Lecture
Notes in Computer Science, 2088.
Julian Kupiec. 1992. Robust part-of-speech tagging
using a hidden Markov model. Computer Speech &
Language, 6(3).
Shen Li, Joa?o Grac?a, and Ben Taskar. 2012. Wiki-ly
supervised part-of-speech tagging. In Proceedings
of EMNLP, Jeju Island, Korea.
Christopher D. Manning. 2011. Part-of-speech tag-
ging from 97% to 100%: Is it time for some linguis-
tics? In Proceedings of CICLing.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2).
Bernard Merialdo. 1994. Tagging English text with
a probabilistic model. Computational Linguistics,
20(2).
Grace Ngai and David Yarowsky. 2000. Rule writing
or annotation: Cost-efficient resource usage for base
noun phrase chunking. In Proceedings ACL.
Sujith Ravi and Kevin Knight. 2009. Minimized mod-
els for unsupervised part-of-speech tagging. In Pro-
ceedings of ACL-AFNLP.
Sujith Ravi, Ashish Vaswani, Kevin Knight, and David
Chiang. 2010. Fast, greedy model minimization for
unsupervised tagging. In Proceedings of COLING.
Emmanuel Roche and Yves Schabes. 1995. Determin-
istic part-of-speech tagging with finite-state trans-
ducers. Computational Linguistics, 21(2).
Amarnag Subramanya, Slav Petrov, and Fernando
Pereira. 2010. Efficient graph-based semi-
supervised learning of structured tagging models. In
Proceedings EMNLP, Cambridge, MA.
Oscar Ta?ckstro?m, Dipanjan Das, Slav Petrov, Ryan
McDonald, and Joakim Nivre. 2013. Token and
type constraints for cross-lingual part-of-speech tag-
ging. In Transactions of the ACL. Association for
Computational Linguistics.
Partha Pratim Talukdar and Koby Crammer. 2009.
New regularized algorithms for transductive learn-
ing. In Proceedings of ECML-PKDD, Bled, Slove-
nia.
Kristina Toutanova and Mark Johnson. 2008. A
Bayesian LDA-based model for semi-supervised
part-of-speech tagging. In Proceedings of NIPS.
592
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 11?21, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
Montague Meets Markov: Deep Semantics with Probabilistic Logical Form
Islam Beltagy?, Cuong Chau?, Gemma Boleda?, Dan Garrette?, Katrin Erk?,
Raymond Mooney?
?Department of Computer Science
?Department of Linguistics
The University of Texas at Austin
Austin, Texas 78712
?{beltagy,ckcuong,dhg,mooney}@cs.utexas.edu
?gemma.boleda@utcompling.com,katrin.erk@mail.utexas.edu
Abstract
We combine logical and distributional rep-
resentations of natural language meaning by
transforming distributional similarity judg-
ments into weighted inference rules using
Markov Logic Networks (MLNs). We show
that this framework supports both judg-
ing sentence similarity and recognizing tex-
tual entailment by appropriately adapting the
MLN implementation of logical connectives.
We also show that distributional phrase simi-
larity, used as textual inference rules created
on the fly, improves its performance.
1 Introduction
Tasks in natural language semantics are very diverse
and pose different requirements on the underlying
formalism for representing meaning. Some tasks
require a detailed representation of the structure of
complex sentences. Some tasks require the ability to
recognize near-paraphrases or degrees of similarity
between sentences. Some tasks require logical infer-
ence, either exact or approximate. Often it is neces-
sary to handle ambiguity and vagueness in meaning.
Finally, we frequently want to be able to learn rele-
vant knowledge automatically from corpus data.
There is no single representation for natural lan-
guage meaning at this time that fulfills all require-
ments. But there are representations that meet some
of the criteria. Logic-based representations (Mon-
tague, 1970; Kamp and Reyle, 1993) provide an
expressive and flexible formalism to express even
complex propositions, and they come with standard-
ized inference mechanisms. Distributional mod-
hamster(gerbil(
sim( #                ?hamster, #         ?gerbil) = w
8x hamster(x) ! gerbil(x)  | f(w)
Figure 1: Turning distributional similarity into a
weighted inference rule
els (Turney and Pantel, 2010) use contextual sim-
ilarity to predict semantic similarity of words and
phrases (Landauer and Dumais, 1997; Mitchell and
Lapata, 2010), and to model polysemy (Schu?tze,
1998; Erk and Pado?, 2008; Thater et al, 2010).
This suggests that distributional models and logic-
based representations of natural language meaning
are complementary in their strengths (Grefenstette
and Sadrzadeh, 2011; Garrette et al, 2011), which
encourages developing new techniques to combine
them.
Garrette et al (2011; 2013) propose a framework
for combining logic and distributional models in
which logical form is the primary meaning repre-
sentation. Distributional similarity between pairs of
words is converted into weighted inference rules that
are added to the logical form, as illustrated in Fig-
ure 1. Finally, Markov Logic Networks (Richardson
and Domingos, 2006) (MLNs) are used to perform
weighted inference on the resulting knowledge base.
However, they only employed single-word distribu-
tional similarity rules, and only evaluated on a small
11
set of short, hand-crafted test sentences.
In this paper, we extend Garrette et al?s approach
and adapt it to handle two existing semantic tasks:
recognizing textual entailment (RTE) and seman-
tic textual similarity (STS). We show how this sin-
gle semantic framework using probabilistic logical
form in Markov logic can be adapted to support both
of these important tasks. This is possible because
MLNs constitute a flexible programming language
based on probabilistic logic (Domingos and Lowd,
2009) that can be easily adapted to support multiple
types of linguistically useful inference.
At the word and short phrase level, our approach
model entailment through ?distributional? similarity
(Figure 1). If X and Y occur in similar contexts, we
assume that they describe similar entities and thus
there is some degree of entailment between them. At
the sentence level, however, we hold that a stricter,
logic-based view of entailment is beneficial, and we
even model sentence similarity (in STS) as entail-
ment.
There are two main innovations in the formalism
that make it possible for us to work with naturally
occurring corpus data. First, we use more expres-
sive distributional inference rules based on the sim-
ilarity of phrases rather than just individual words.
In comparison to existing methods for creating tex-
tual inference rules (Lin and Pantel, 2001b; Szpek-
tor and Dagan, 2008), these rules are computed on
the fly as needed, rather than pre-compiled. Second,
we use more flexible probabilistic combinations of
evidence in order to compute degrees of sentence
similarity for STS and to help compensate for parser
errors. We replace deterministic conjunction by an
average combiner, which encodes causal indepen-
dence (Natarajan et al, 2010).
We show that our framework is able to han-
dle both sentence similarity (STS) and textual en-
tailment (RTE) by making some simple adapta-
tions to the MLN when switching between tasks.
The framework achieves reasonable results on both
tasks. On STS, we obtain a correlation of r = 0.66
with full logic, r = 0.73 in a system with weak-
ened variable binding, and r = 0.85 in an ensemble
model. On RTE-1 we obtain an accuracy of 0.57.
We show that the distributional inference rules ben-
efit both tasks and that more flexible probabilistic
combinations of evidence are crucial for STS. Al-
though other approaches could be adapted to handle
both RTE and STS, we do not know of any other
methods that have been explicitly tested on both
problems.
2 Related work
Distributional semantics Distributional models
define the semantic relatedness of words as the
similarity of vectors representing the contexts in
which they occur (Landauer and Dumais, 1997;
Lund and Burgess, 1996). Recently, such mod-
els have also been used to represent the meaning
of larger phrases. The simplest models compute
a phrase vector by adding the vectors for the indi-
vidual words (Landauer and Dumais, 1997) or by a
component-wise product of word vectors (Mitchell
and Lapata, 2008; Mitchell and Lapata, 2010).
Other approaches, in the emerging area of distribu-
tional compositional semantics, use more complex
methods that compute phrase vectors from word
vectors and tensors (Baroni and Zamparelli, 2010;
Grefenstette and Sadrzadeh, 2011).
Wide-coverage logic-based semantics Boxer
(Bos, 2008) is a software package for wide-coverage
semantic analysis that produces logical forms using
Discourse Representation Structures (Kamp and
Reyle, 1993). It builds on the C&C CCG parser
(Clark and Curran, 2004).
Markov Logic In order to combine logical and
probabilistic information, we draw on existing work
in Statistical Relational AI (Getoor and Taskar,
2007). Specifically, we utilize Markov Logic Net-
works (MLNs) (Domingos and Lowd, 2009), which
employ weighted formulas in first-order logic to
compactly encode complex undirected probabilistic
graphical models. MLNs are well suited for our ap-
proach since they provide an elegant framework for
assigning weights to first-order logical rules, com-
bining a diverse set of inference rules and perform-
ing sound probabilistic inference.
An MLN consists of a set of weighted first-order
clauses. It provides a way of softening first-order
logic by allowing situations in which not all clauses
are satisfied. More specifically, they provide a
well-founded probability distribution across possi-
ble worlds by specifying that the probability of a
12
world increases exponentially with the total weight
of the logical clauses that it satisfies. While methods
exist for learning MLN weights directly from train-
ing data, since the appropriate training data is lack-
ing, our approach uses weights computed using dis-
tributional semantics. We use the open-source soft-
ware package Alchemy (Kok et al, 2005) for MLN
inference, which allows computing the probability
of a query literal given a set of weighted clauses as
background knowledge and evidence.
Tasks: RTE and STS Recognizing Textual En-
tailment (RTE) is the task of determining whether
one natural language text, the premise, implies an-
other, the hypothesis. Consider (1) below.
(1) p: Oracle had fought to keep the forms from
being released
h: Oracle released a confidential document
Here, h is not entailed. RTE directly tests whether
a system can construct semantic representations that
allow it to draw correct inferences. Of existing RTE
approaches, the closest to ours is by Bos and Mark-
ert (2005), who employ a purely logical approach
that uses Boxer to convert both the premise and hy-
pothesis into first-order logic and then checks for
entailment using a theorem prover. By contrast, our
approach uses Markov logic with probabilistic infer-
ence.
Semantic Textual Similarity (STS) is the task of
judging the similarity of two sentences on a scale
from 0 to 5 (Agirre et al, 2012). Gold standard
scores are averaged over multiple human annota-
tions. The best performer in 2012?s competition was
by Ba?r et al (2012), an ensemble system that inte-
grates many techniques including string similarity,
n-gram overlap, WordNet similarity, vector space
similarity and MT evaluation metrics.
Weighted inference, and combined structural-
distributional representations One approach to
weighted inference in NLP is that of Hobbs et al
(1993), who proposed viewing natural language in-
terpretation as abductive inference. In this frame-
work, problems like reference resolution and syntac-
tic ambiguity resolution become inferences to best
explanations that are associated with costs. How-
ever, this leaves open the question of how costs are
assigned. Raina et al (2005) use this framework for
RTE, deriving inference costs from WordNet simi-
larity and properties of the syntactic parse.
Garrette et al (2011; 2013) proposed an approach
to RTE that uses MLNs to combine traditional log-
ical representations with distributional information
in order to support probabilistic textual inference.
This approach can be viewed as a bridge between
Bos and Markert (2005)?s purely logical approach,
which relies purely on hard logical rules and the-
orem proving, and distributional approaches, which
support graded similarity between concepts but have
no notion of logical operators or entailment.
There are also other methods that combine dis-
tributional and structured representations. Stern et
al. (2011) conceptualize textual entailment as tree
rewriting of syntactic graphs, where some rewrit-
ing rules are distributional inference rules. Socher
et al (2011) recognize paraphrases using a ?tree of
vectors,? a phrase structure tree in which each con-
stituent is associated with a vector, and overall sen-
tence similarity is computed by a classifier that inte-
grates all pairwise similarities. (This is in contrast to
approaches like Baroni and Zamparelli (2010) and
Grefenstette and Sadrzadeh (2011), who do not of-
fer a proposal for using vectors at multiple levels in
a syntactic tree simultaneously.)
3 MLN system
Our system extends that of Garrette et al (2011;
2013) to support larger-scale evaluation on standard
benchmarks for both RTE and STS. We conceptual-
ize both tasks as probabilistic entailment in Markov
logic, where STS is judged as the average degree of
mutual entailment, i.e. we compute the probability
of both S1 |= S2 and S2 |= S1 and average the re-
sults. Below are some sentence pairs that we use as
examples in the discussion below:
(2) S1: A man is slicing a cucumber.
S2: A man is slicing a zucchini.
(3) S1: A boy is riding a bicycle.
S2: A little boy is riding a bike.
(4) S1: A man is driving.
S2: A man is driving a car.
13
System overview. To compute the probability of
an entailment S1 |= S2, the system first constructs
logical forms for each sentence using Boxer and
then translates them into MLN clauses. In example
(2) above, the logical form for S1:
?x0, e1, x2
(
man(x0) ? slice(e1) ?Agent(e1, x0)?
cucumber(x2) ? Patient(e1, x2)
)
is used as evidence, and the logical form for S2 is
turned into the following formula (by default, vari-
ables are assumed to be universally quantified):
man(x) ? slice(e) ?Agent(e, x)?
zucchini(y) ? Patient(e, y)? result()
where result() is the query for which we have
Alchemy compute the probability.
However, S2 is not strictly entailed by S1 because
of the mismatch between ?cucumber? and ?zuc-
chini?, so with just the strict logical-form transla-
tions of S1 and S2, the probability of result() will
be zero. This is where we introduce distributional
similarity, in this case the similarity of ?cucumber?
and ?zucchini?, cos( #                  ?cucumber, #               ?zucchini). We cre-
ate inference rules from such similarities as a form
of background knowledge. We then treat similarity
as degree of entailment, a move that has a long tradi-
tion (e.g., (Lin and Pantel, 2001b; Raina et al, 2005;
Szpektor and Dagan, 2008)). In general, given two
words a and b, we transform their cosine similarity
into an inference-rule weight wt(a, b) using:
wt(a, b) = log( cos(
#?a , #?b )
1? cos( #?a , #?b )
)? prior (5)
Where prior is a negative weight used to initialize
all predicates, so that by default facts are assumed
to have very low probability. In our experiments,
we use prior = ?3. In the case of sentence pair
(2), we generate the inference rule:
cucumber(x)? zucchini(x) | wt(cuc., zuc.)
Such inference rules are generated for all pairs of
words (w1, w2) where w1 ? S1 and w2 ? S2.1
1We omit inference rules for words (a, b) where cos(a, b) <
? for a threshold ? set to maximize performance on the training
data. Low-similarity pairs usually indicate dissimilar words.
This removes a sizeable number of rules for STS, while for RTE
the tuned threshold was near zero.
The distributional model we use contains all lem-
mas occurring at least 50 times in the Gigaword cor-
pus (Graff et al, 2007) except a list of stop words.
The dimensions are the 2,000 most frequent of these
words, and cell values are weighted with point-wise
mutual information. 2
Phrase-based inference rules. Garrette et al only
considered distributional inference rules for pairs of
individual words. We extend their approach to dis-
tributional inference rules for pairs of phrases in or-
der to handle cases like (3). To properly estimate
the similarity between S1 and S2 in (3), we not only
need an inference rule linking ?bike? to ?bicycle?,
but also a rule estimating how similar ?boy? is to
?little boy?. To do so, we make use of existing ap-
proaches that compute distributional representations
for phrases. In particular, we compute the vector for
a phrase from the vectors of the words in that phrase,
using either vector addition (Landauer and Dumais,
1997) or component-wise multiplication (Mitchell
and Lapata, 2008; Mitchell and Lapata, 2010). The
inference-rule weight, wt(p1, p2), for two phrases
p1 and p2 is then determined using Eq. (5) in the
same way as for words.
Existing approaches that derive phrasal inference
rules from distributional similarity (Lin and Pantel,
2001a; Szpektor and Dagan, 2008; Berant et al,
2011) precompile large lists of inference rules. By
comparison, distributional phrase similarity can be
seen as a generator of inference rules ?on the fly?,
as it is possible to compute distributional phrase
vectors for arbitrary phrases on demand as they are
needed for particular examples.
Inference rules are generated for all pairs of con-
stituents (c1, c2) where c1 ? S1 and c2 ? S2, a
constituent is a single word or a phrase. The log-
ical form provides a handy way to extract phrases,
as they are generally mapped to one of two logical
constructs. Either we have multiple single-variable
predicates operating on the same variable. For ex-
ample the phrase ?a little boy? has the logical form
boy(x) ? little(x). Or we have two unary predi-
cates connected with a relation. For example, ?pizza
slice? and ?slice of pizza? are both mapped to the
2It is customary to transform raw counts in a way that cap-
tures association between target words and dimensions, for ex-
ample through point-wise mutual information (Lowe, 2001).
14
logical form, slice(x0) ? of(x0, x1) ? pizza(x1).
We consider all binary predicates as relations.
Average Combiner to determine similarity in the
presence of missing phrases. The logical forms
for the sentences in (4): are
S1: ?x0, e1
(
man(x0)?agent(x0, e1)?drive(e1)
)
S2: ?x0, e1, x2
(
man(x0) ? agent(x0, e1) ?
drive(e1) ? patient(e1, x2) ? car(x2)
)
If we try to prove S1 |= S2, the probability of
the result() will be zero: There is no evidence for
a car, and the hypothesis predicates are conjoined
using a deterministic AND. For RTE, this makes
sense: If one of the hypothesis predicates is False,
the probability of entailment should be zero. For the
STS task, this should in principle be the same, at
least if the omitted facts are vital, but it seems that
annotators rated the data points in this task more for
overall similarity than for degrees of entailment. So
in STS, we want the similarity to be a function of
the number of elements in the hypothesis that are
inferable. Therefore, we need to replace the deter-
ministic AND with a different way of combining
evidence. We chose to use the average evidence
combiner for MLNs introduced by Natarajan et al
(2010). To use the average combiner, the full logi-
cal form is divided into smaller clauses (which we
call mini-clauses), then the combiner averages their
probabilities. In case the formula is a list of con-
juncted predicates, a mini-clause is a conjunction
of a single-variable predicate with a relation predi-
cate(as in the example below). In case the logical
form contains a negated sub-formula, the negated
sub-formula is also a mini-clause. The hypothesis
above after dividing clauses for the average com-
biner looks like this:
man(x0) ? agent(x0, e1)? result(x0, e1, x2) | w
drive(e1) ? agent(x0, e1)? result(x0, e1, x2) | w
drive(e1) ? patient(e1, x2)? result(x0, e1, x2) | w
car(x2) ? patient(e1, x2)? result(x0, e1, x2) | w
where result is again the query predicate. Here,
result has all of the variables in the clause as argu-
ments in order to maintain the binding of variables
across all of the mini-clauses. The weights w are the
following function of n, the number of mini-clauses
(4 in the above example):
w = 1n ? (log(
p
1? p)? prior) (6)
where p is a value close to 1 that is set to maximize
performance on the training data, and prior is the
same negative weight as before. Setting w this way
produces a probability of p for the result() in cases
that satisfy the antecedents of all mini-clauses. For
the example above, the antecedents of the first two
mini-clauses are satisfied, while the antecedents of
the last two are not since the premise provides no
evidence for an object of the verb drive. The simi-
larity is then computed to be the maximum probabil-
ity of any grounding of the result predicate, which
in this case is around p2 .
3
An interesting variation of the average combiner
is to omit variable bindings between the mini-
clauses. In this case, the hypothesis clauses look
like this for our example:
man(x) ? agent(x, e)? result() | w
drive(e) ? agent(x, e)? result() | w
drive(e) ? patient(e, x)? result() | w
car(x) ? patient(e, x)? result() | w
This implementation loses a lot of information,
for example it does not differentiate between ?A
man is walking and a woman is driving? and ?A
man is driving and a woman is walking?. In fact,
logical form without variable binding degrades to a
representation similar to a set of independent syn-
tactic dependencies, 4 while the average combiner
with variable binding retains all of the information
in the original logical form. Still, omitting variable
binding turns out to be useful for the STS task.
It is also worth commenting on the efficiency of
the inference algorithm when run on the three dif-
ferent approaches to combining evidence. The aver-
age combiner without variable binding is the fastest
and has the least memory requirements because all
cliques in the ground network are of limited size
(just 3 or 4 nodes). Deterministic AND is much
slower than the average combiner without variable
binding, because the maximum clique size depends
on the sentence. The average combiner with vari-
able binding is the most memory intensive since the
3One could also give mini-clauses different weights depend-
ing on their importance, but we have not experimented with this
so far.
4However, it is not completely the same since we do not
divide up formulas under negation into mini-clauses.
15
number of arguments of the result() predicate can
become large (there is an argument for each individ-
ual and event in the sentence). Consequently, the
inference algorithm needs to consider a combinato-
rial number of possible groundings of the result()
predicate, making inference very slow.
Adaptation of the logical form. As discussed by
Garrette et al (2011), Boxer?s output is mapped to
logical form and augmented with additional infor-
mation to handle a variety of semantic phenomena.
However, we do not use their additional rules for
handling implicatives and factives, as we wanted to
test the system without background knowledge be-
yond that supplied by the vector space.
Unfortunately, current MLN inference algorithms
are not able to efficiently handle complex formu-
las with nested quantifiers. For that reason, we re-
placed universal quantifiers in Boxer?s output with
existentials since they caused serious problems for
Alchemy. Although this is a radical change to the
semantics of the logical form, due to the nature of
the STS and RTE data, it only effects about 5% of
the sentences, and we found that most of the uni-
versal quantifiers in these cases were actually due
to parsing errors. We are currently exploring more
effective ways of dealing with this issue.
4 Task 1: Recognizing Textual Entailment
4.1 Dataset
In order to compare directly to the logic-based sys-
tem of Bos and Markert (2005), we focus on the
RTE-1 dataset (Dagan et al, 2005), which includes
567 Text-Hypothesis (T-H) pairs in the development
set and 800 pairs in the test set. The data covers a
wide range of issues in entailment, including lexical,
syntactic, logical, world knowledge, and combina-
tions of these at different levels of difficulty. In both
development and test sets, 50% of sentence pairs are
true entailments and 50% are not.
4.2 Method
We run our system for different configurations of in-
ference rules and evidence combiners. For distri-
butional inference rules (DIR), three different lev-
els are tested: without inference rules (no DIR),
inference rules for individual words (word DIR),
and inference rules for words and phrases (phrase
DIR). Phrase vectors were built using vector addi-
tion, as point-wise multiplication performed slightly
worse. To combine evidence for the result() query,
three different options are available: without av-
erage combiner which is just using Deterministic
AND (Deterministic AND), average combiner with
variable binding (AvgComb) and average combiner
without variable binding (AvgComb w/o VarBind).
Different combinations of configurations are tested
according to its suitability for the task; RTE and
STS.
We also tested several ?distributional only? sys-
tems. The first such system builds a vector represen-
tation for each sentence by adding its word vectors,
then computes the cosine similarity between the sen-
tence vectors for S1 and S2 (VS-Add). The second
uses point-wise multiplication instead of vector ad-
dition (VS-Mul). The third scales pairwise words
similarities to the sentence level using weighted av-
erage where weights are inverse document frequen-
cies idf as suggested by Mihalcea et al (2006) (VS-
Pairwise).
For the RTE task, systems were evaluated using
both accuracy and confidence-weighted score (cws)
as used by Bos and Markert (2005) and the RTE-
1 challenge (Dagan et al, 2005). In order to map
a probability of entailment to a strict prediction of
True or False, we determined a threshold that op-
timizes performance on the development set. The
cws score rewards a system?s ability to assign higher
confidence scores to correct predictions than incor-
rect ones. For cws, a system?s predictions are sorted
in decreasing order of confidence and the score is
computed as:
cws = 1n
n?
i=1
#correct-up-to-rank-i
i
where n is the number of the items in the test set,
and i ranges over the sorted items. In our systems,
we defined the confidence value for a T-H pair as
the distance between the computed probability for
the result() predicate and the threshold.
4.3 Results
The results are shown in Table 1. They show
that the distributional only baselines perform very
poorly. In particular, they perform worse than strict
16
Method acc cws
Chance 0.50 0.50
Bos & Markert, strict 0.52 0.55
Best system in RTE-1 challenge
(Bayer et al, 2005)
0.59 0.62
VS-Add 0.49 0.53
VS-Mul 0.51 0.52
VS-Pairwise 0.50 0.50
AvgComb w/o VarBind + phrase
DIR
0.52 0.53
Deterministic AND + phrase DIR 0.57 0.57
Table 1: Results on the RTE-1 Test Set.
entailment from Bos and Markert (2005), a system
that uses only logic. This illustrates the important
role of logic-based representations for the entail-
ment task. Due to intractable memory demands of
Alchemy inference, our current system with deter-
ministic AND fails to execute on 118 of the 800 test
pairs, so, by default, the system classifies these cases
as False (non-entailing) with very low confidence.
Comparing the two configurations of our system,
using deterministic AND vs. the average combiner
without variable binding (last two lines in Table 1),
we see that for RTE, it is essential to retain the full
logical form.
Our system with deterministic AND obtains both
an accuracy and cws of 0.57. The best result in
the RTE-1 challenge by Bayer et al (2005) ob-
tained an accuracy of 0.59 and cws of 0.62. 5 In
terms of both accuracy and cws, our system outper-
forms both ?distributional only? systems and strict
logical entailment, showing again that integrating
both logical form and distributional inference rules
using MLNs is beneficial. Interestingly, the strict
entailment system of Bos and Markert incorporated
generic knowledge, lexical knowledge (from Word-
Net) and geographical knowledge that we do not
utilize. This demonstrates the advantage of us-
ing a model that operationalizes entailment between
words and phrases as distributional similarity.
5On other RTE datasets there are higher previous results.
Hickl (2008) achieves 0.89 accuracy and 0.88 cws on the com-
bined RTE-2 and RTE-3 dataset.
5 Task 2: Semantic Textual Similarity
5.1 Dataset
The dataset we use in our experiments is the MSR
Video Paraphrase Corpus (MSR-Vid) subset of the
STS 2012 task, consisting of 1,500 sentence pairs.
The corpus itself was built by asking annotators
from Amazon Mechanical Turk to describe very
short video fragments (Chen and Dolan, 2011). The
organizers of the STS 2012 task (Agirre et al, 2012)
sampled video descriptions and asked Turkers to as-
sign similarity scores (ranging from 0 to 5) to pairs
of sentences, without access to the video. The gold
standard score is the average of the Turkers? annota-
tions. In addition to the MSR Video Paraphrase Cor-
pus subset, the STS 2012 task involved data from
machine translation and sense descriptions. We do
not use these because they do not consist of full
grammatical sentences, which the parser does not
handle well. In addition, the STS 2012 data included
sentences from the MSR Paraphrase Corpus, which
we also do not currently use because some sentences
are long and create intractable MLN inference prob-
lems. This issue is discussed further in section 6.
Following STS standards, our evaluation compares
a system?s similarity judgments to the gold standard
scores using Pearson?s correlation coefficient r.
5.2 Method
Our system can be tested for different configuration
of inference rules and evidence combiners which
are explained in section 4.2. The tested systems on
the STS task are listed in table 2. Out experiments
showed that using average combiner (AvgComb) is
very memory intensive and MLN inference for 28 of
the 1,500 pairs either ran out of memory or did not
finish in reasonable time. In such cases, we back off
to AvgComb w/o VarBind.
We compare to several baselines; our MLN
system without distributional inference rules
(AvgComb + no DIR), and distributional-only
systems (VS-Add, VS-Mul, VS-Pairwise). These
are the natural baselines for our system, since they
use only one of the two types of information that
we combine (i.e. logical form and distributional
representations).
Finally, we built an ensemble that combines the
output of multiple systems using regression trained
17
Method r
AvgComb + no DIR 0.58
AvgComb + word DIR 0.60
AvgComb + phrase DIR 0.66
AvgComb w/o VarBind + no DIR 0.58
AvgComb w/o VarBind + word DIR 0.60
AvgComb w/o VarBind + phrase DIR 0.73
VS-Add 0.78
VS-Mul 0.58
VS-Pairwise 0.77
Ensemble (VS-Add + VS-Mul + VS-
Pairwise)
0.83
Ensemble ([AvgComb + phrase DIR] +
VS-Add + VS-Mul + VS-Pairwise)
0.85
Best MSR-Vid score in STS 2012 (Ba?r
et al, 2012)
0.87
Table 2: Results on the STS video dataset.
on the training data. We then compare the perfor-
mance of an ensemble with and without our sys-
tem. This is the same technique used by Ba?r et al
(2012) except we used additive regression (Fried-
man, 2002) instead of linear regression since it gave
better results.
5.3 Results
Table 2 summarizes the results of our experiments.
They show that adding distributional information
improves results, as expected, and also that adding
phrase rules gives further improvement: Using only
word distributional inference rules improves results
from 0.58 to 0.6, and adding phrase inference rules
further improves them to 0.66. As for variable bind-
ing, note that although it provides more precise in-
formation, the STS scores actually improve when it
is dropped, from 0.66 to 0.73. We offer two expla-
nations for this result: First, this information is very
sensitive to parsing errors, and the C&C parser, on
which Boxer is based, produces many errors on this
dataset, even for simple sentences. When the C&C
CCG parse is wrong, the resulting logical form is
wrong, and the resulting similarity score is greatly
affected. Dropping variable binding makes the sys-
tems more robust to parsing errors. Second, in con-
trast to RTE, the STS dataset does not really test the
important role of syntax and logical form in deter-
mining meaning. This also explains why the ?dis-
tributional only? baselines are actually doing better
than the MLN systems.
Although the MLN system on its own does not
perform better than the distributional compositional
models, it does provide complementary information,
as shown by the fact that ensembling it with the rest
of the models improves performance (0.85 with the
MLN system, compared to 0.83 without it). The per-
formance of this ensemble is close to the current best
result for this dataset (0.87).
6 Future Work
The approach presented in this paper constitutes a
step towards achieving the challenging goal of effec-
tively combining logical representations with dis-
tributional information automatically acquired from
text. In this section, we discuss some of limita-
tions of the current work and directions for future
research.
As noted before, parse errors are currently a sig-
nificant problem. We use Boxer to obtain a logi-
cal representation for a sentence, which in turn re-
lies on the C&C parser. Unfortunately, C&C mis-
parses many sentences, which leads to inaccurate
logical forms. To reduce the impact of misparsing,
we plan to use a version of C&C that can produce
the top-n parses together with parse re-ranking (Ng
and Curran, 2012). As an alternative to re-ranking,
one could obtain logical forms for each of the top-
n parses, and create an MLN that integrates all of
them (together with their certainty) as an underspec-
ified meaning representation that could then be used
to directly support inferences such as STS and RTE.
We also plan to exploit a greater variety of dis-
tributional inference rules. First, we intend to in-
corporate logical form translations of existing dis-
tributional inference rule collections (e.g., (Berant
et al, 2011; Chan et al, 2011)). Another issue
is obtaining improved rule weights based on dis-
tributional phrase vectors. We plan to experiment
with more sophisticated approaches to computing
phrase vectors such as those recently presented by
Baroni and Zamparelli (2010) and Grefenstette and
Sadrzadeh (2011). Furthermore, we are currently
deriving symmetric similarity ratings between word
pairs or phrase pairs, when really what we need is di-
18
rectional similarity. We plan to incorporate directed
similarity measures such as those of Kotlerman et al
(2010) and Clarke (2012).
A primary problem for our approach is the limita-
tions of existing MLN inference algorithms, which
do not effectively scale to large and complex MLNs.
We plan to explore ?coarser? logical representa-
tions such as Minimal Recursion Semantics (MRS)
(Copestake et al, 2005). Another potential approach
to this problem is to trade expressivity for efficiency.
Domingos and Webb (2012) introduced a tractable
subset of Markov Logic (TML) for which a future
software release is planned. Formulating the infer-
ence problem in TML could potentially allow us to
run our system on longer and more complex sen-
tences.
7 Conclusion
In this paper we have used an approach that com-
bines logic-based and distributional representations
for natural language meaning. It uses logic as
the primary representation, transforms distributional
similarity judgments to weighted inference rules,
and uses Markov Logic Networks to perform in-
ferences over the weighted clauses. This approach
views textual entailment and sentence similarity as
degrees of ?logical? entailment, while at the same
time using distributional similarity as an indicator
of entailment at the word and short phrase level. We
have evaluated the framework on two different tasks,
RTE and STS, finding that it is able to handle both
tasks given that we adapt the way evidence is com-
bined in the MLN. Even though other entailment
models could be applied to STS, given that similar-
ity can obviously be operationalized as a degree of
mutual entailment, this has not been done before to
our best knowledge. Our framework achieves rea-
sonable results on both tasks. On RTE-1 we obtain
an accuracy of 0.57. On STS, we obtain a correla-
tion of r = 0.66 with full logic, r = 0.73 in a system
with weakened variable binding, and r = 0.85 in an
ensemble model. We find that distributional word
and phrase similarity, used as textual inference rules
on the fly, leads to sizeable improvements on both
tasks. We also find that using more flexible proba-
bilistic combinations of evidence is crucial for STS.
Acknowledgements
This research was supported in part by the NSF CA-
REER grant IIS 0845925, by the DARPA DEFT
program under AFRL grant FA8750-13-2-0026, by
MURI ARO grant W911NF-08-1-0242 and by an
NDSEG grant. Any opinions, findings, and conclu-
sions or recommendations expressed in this material
are those of the author and do not necessarily reflect
the view of DARPA, AFRL, ARO, DoD or the US
government.
Some of our experiments were run on the
Mastodon Cluster supported by NSF Grant EIA-
0303609.
References
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 task 6: A pi-
lot on semantic textual similarity. In Proceedings of
SemEval.
Daniel Ba?r, Chris Biemann, Iryna Gurevych, and Torsten
Zesch. 2012. UKP: Computing semantic textual sim-
ilarity by combining multiple content similarity mea-
sures. In SemEval-2012.
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages
1183?1193, Cambridge, MA, October. Association for
Computational Linguistics.
Samuel Bayer, John Burger, Lisa Ferro, John Hender-
son, and Alexander Yeh. 2005. MITREs Submissions
to the EU Pascal RTE Challenge. In In Proceedings
of the PASCAL Challenges Workshop on Recognising
Textual Entailment, pages 41?44.
Jonathan Berant, Ido Dagan, and Jacob Goldberger.
2011. Global learning of typed entailment rules. In
Proceedings of ACL, Portland, OR.
Johan Bos and Katja Markert. 2005. Recognising tex-
tual entailment with logical inference. In Proceedings
of EMNLP 2005, pages 628?635, Vancouver, B.C.,
Canada.
Johan Bos. 2008. Wide-coverage semantic analysis with
boxer. In Johan Bos and Rodolfo Delmonte, editors,
Semantics in Text Processing. STEP 2008 Conference
Proceedings, Research in Computational Semantics,
pages 277?286. College Publications.
Tsz Ping Chan, Chris Callison-Burch, and Benjamin
Van Durme. 2011. Reranking bilingually extracted
19
paraphrases using monolingual distributional similar-
ity. In Proceedings of the GEMS 2011 Workshop on
GEometrical Models of Natural Language Semantics,
pages 33?42, Edinburgh, UK.
David L. Chen and William B. Dolan. 2011. Collect-
ing highly parallel data for paraphrase evaluation. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 190?200,
Portland, Oregon, USA, June.
Stephen Clark and James R. Curran. 2004. Parsing the
WSJ using CCG and log-linear models. In Proceed-
ings of ACL 2004, pages 104?111, Barcelona, Spain.
Daoud Clarke. 2012. A context-theoretic framework for
compositionality in distributional semantics. Compu-
tational Linguistics, 38(1).
Ann Copestake, Dan Flickinger, Carl Pollard, and Ivan A
Sag. 2005. Minimal recursion semantics: An intro-
duction. Research on Language and Computation,
3(2-3):281?332.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2005. The PASCAL Recognising Textual Entailment
Challenge. In In Proceedings of the PASCAL Chal-
lenges Workshop on Recognising Textual Entailment,
pages 1?8.
Pedro Domingos and Daniel Lowd. 2009. Markov
Logic: An Interface Layer for Artificial Intelligence.
Synthesis Lectures on Artificial Intelligence and Ma-
chine Learning. Morgan & Claypool Publishers.
Pedro Domingos and W Austin Webb. 2012. A tractable
first-order probabilistic logic. In Proceedings of the
Twenty-Sixth National Conference on Artificial Intel-
ligence.
Katrin Erk and Sebastian Pado?. 2008. A structured vec-
tor space model for word meaning in context. In Pro-
ceedings of EMNLP 2008, pages 897?906, Honolulu,
HI.
Jerome H Friedman. 2002. Stochastic gradient boosting.
Computational Statistics & Data Analysis, 38(4):367?
378.
Dan Garrette, Katrin Erk, and Raymond Mooney. 2011.
Integrating logical representations with probabilistic
information using markov logic. In Proceedings of
IWCS, Oxford, UK.
Dan Garrette, Katrin Erk, and Raymond Mooney. 2013.
A formal approach to linking logical form and vector-
space lexical semantics. In Harry Bunt, Johan Bos,
and Stephen Pulman, editors, Computing Meaning,
Vol. 4.
Lise Getoor and Ben Taskar, editors. 2007. Introduction
to Statistical Relational Learning. MIT Press, Cam-
bridge, MA.
David Graff, Junbo Kong, Ke Chen, and Kazuaki
Maeda. 2007. English Gigaword Third Edi-
tion. http://www.ldc.upenn.edu/
Catalog/CatalogEntry.jsp?catalogId=
LDC2007T07.
Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011.
Experimental support for a categorical compositional
distributional model of meaning. In Proceedings of
EMNLP, Edinburgh, Scotland, UK.
Andrew Hickl. 2008. Using Discourse Commitments
to Recognize Textual Entailment. In Proceedings of
COLING 2008, pages 337?344.
Jerry R. Hobbs, Mark Stickel, Douglas Appelt, and Paul
Martin. 1993. Interpretation as abduction. Artificial
Intelligence, 63(1?2):69?142.
Hans Kamp and Uwe Reyle. 1993. From Discourse to
Logic; An Introduction to Modeltheoretic Semantics of
Natural Language, Formal Logic and DRT. Kluwer,
Dordrecht.
Stanley Kok, Parag Singla, Matthew Richardson, and Pe-
dro Domingos. 2005. The Alchemy system for sta-
tistical relational AI. Technical report, Department
of Computer Science and Engineering, University
of Washington. http://www.cs.washington.
edu/ai/alchemy.
Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan
Zhitomirsky-Geffet. 2010. Directional distributional
similarity for lexical inference. Natural Language En-
gineering, 16(04):359?389.
Thomas Landauer and Susan Dumais. 1997. A solution
to Platos problem: the latent semantic analysis theory
of acquisition, induction, and representation of knowl-
edge. Psychological Review, 104(2):211?240.
Dekang Lin and Patrick Pantel. 2001a. DIRT - discovery
of inference rules from text. In In Proceedings of the
ACM SIGKDD Conference on Knowledge Discovery
and Data Mining, pages 323?328.
Dekang Lin and Patrick Pantel. 2001b. Discovery of
inference rules for question answering. Natural Lan-
guage Engineering, 7(4):343?360.
Will Lowe. 2001. Towards a theory of semantic space.
In Proceedings of the Cognitive Science Society, pages
576?581.
Kevin Lund and Curt Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, Instruments,
and Computers, 28:203?208.
Rada Mihalcea, Courtney Corley, and Carlo Strapparava.
2006. Corpus-based and knowledge-based measures
of text semantic similarity. In Proceedings of the na-
tional conference on artificial intelligence, volume 21,
page 775. Menlo Park, CA; Cambridge, MA; London;
AAAI Press; MIT Press; 1999.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL, pages 236?244.
20
Jeff Mitchell and Mirella Lapata. 2010. Composition in
distributional models of semantics. Cognitive Science,
34(8):1388?1429.
Richard Montague. 1970. Universal grammar. Theoria,
36:373?398. Reprinted in Thomason (1974), pp 7-27.
Sriraam Natarajan, Tushar Khot, Daniel Lowd, Prasad
Tadepalli, Kristian Kersting, and Jude Shavlik. 2010.
Exploiting causal independence in markov logic net-
works: Combining undirected and directed models.
In Proceedings of European Conference in Machine
Learning (ECML), Barcelona, Spain.
Dominick Ng and James R Curran. 2012. Dependency
hashing for n-best ccg parsing. In In Proceedings of
the 50th Annual Meeting of the Association for Com-
putational Linguistics.
Rajat Raina, Andrew Y. Ng, and Christopher D. Man-
ning. 2005. Robust textual inference via learning and
abductive reasoning. In Proceedings of AAAI.
Matthew Richardson and Pedro Domingos. 2006.
Markov logic networks. Machine Learning, 62:107?
136.
Hinrich Schu?tze. 1998. Automatic word sense discrimi-
nation. Computational Linguistics, 24(1).
Richard Socher, Eric Huang, Jeffrey Pennin, Andrew Ng,
and Christopher Manning. 2011. Dynamic pooling
and unfolding recursive autoencoders for paraphrase
detection. In J. Shawe-Taylor, R.S. Zemel, P. Bartlett,
F.C.N. Pereira, and K.Q. Weinberger, editors, Pro-
ceedings of NIPS.
Asher Stern, Amnon Lotan, Shachar Mirkin, Eyal
Shnarch, Lili Kotlerman, Jonathan Berant, and Ido Da-
gan. 2011. Knowledge and tree-edits in learnable en-
tailment proofs. In TAC, Gathersburg, MD.
Idan Szpektor and Ido Dagan. 2008. Learning entail-
ment rules for unary templates. In Proceedings of
COLING.
Stefan Thater, Hagen Fu?rstenau, and Manfred Pinkal.
2010. Contextualizing semantic representations using
syntactically enriched vector models. In Proceedings
of ACL 2010, pages 948?957, Uppsala, Sweden.
Richmond H. Thomason, editor. 1974. Formal Philoso-
phy. Selected Papers of Richard Montague. Yale Uni-
versity Press, New Haven.
Peter Turney and Patrick Pantel. 2010. From frequency
to meaning: Vector space models of semantics. Jour-
nal of Artificial Intelligence Research, 37:141?188.
21
Proceedings of the 8th International Conference on Computational Semantics, pages 116?127,
Tilburg, January 2009. c?2009 International Conference on Computational Semantics
An Extensible Toolkit for Computational Semantics
Dan Garrette
dhgarrette@gmail.com
Ewan Klein
University of Edinburgh
ewan@inf.ed.ac.uk
1 Introduction
In this paper we focus on the software for computational semantics pro-
vided by the Python-based Natural Language Toolkit (nltk). The seman-
tics modules in nltk are inspired in large part by the approach developed in
Blackburn and Bos (2005) (henceforth referred to as B&B). Since Blackburn
and Bos have also provided a software suite to accompany their excellent
textbook, one might ask what the justification is for the nltk offering, which
is similarly slanted towards teaching computational semantics.
This question can be answered in a number of ways. First, we believe
there is intrinsic merit in the availability of different software tools for se-
mantic analysis, even when there is some duplication of coverage; and this
will become more true as computational semantics starts to be as widely
studied as computational syntax. For example, one rarely hears the ob-
jection that too many implementations of syntactic parsers are available.
Moreover, the nltk software significantly goes beyond B&B in providing
an implementation of Glue Semantics.
Second, whatever the relative merits of Prolog vs. Python as program-
ming languages, there is surely an advantage in offering students and in-
structors a choice in this respect. Given that many students have either
already been exposed to Java, or else have had no programming experience
at all, Python offers them the option of accomplishing interesting results
with only a shallow learning curve.
Third, nltk is a rapidly developing, open source project
1
with a broad
coverage of natural language processing (nlp) tools; see Bird et al (2008)
for a recent overview. This wide functionality has a number of benefits,
most notably that lexical, syntactic and semantic processing can be carried
out within a uniform computational framework. As a result, nltk makes it
much easier to include some computational semantics subject matter in a
broad course on natural language analysis, rather than having to devote a
whole course exclusively to the topic.
1
See http://www.nltk.org
116
Fourth, nltk is accompanied by a substantial collection of corpora, plus
easy-to-use corpus readers. This collection, which currently stands at over
50 corpora and trained models, includes parsed, POS-tagged, plain text, cat-
egorized text, and lexicons. The availability of corpora can help encourage
students to go beyond writing toy grammars, and instead to start grappling
with the complexities of semantically analysing realistic bodies of text.
Fifth, nltk is not just for students. Although Python is slower than
languages like Java and C++, its suitability for rapid prototyping makes it
an attractive addition to the researcher?s inventory of resources. Building
an experimental set-up in nltk to test a hypothesis or explore some data is
straightforward and quick, and the rich variety of existing nlp components in
the toolkit allows rapid assembly of quite sophisticated processing pipelines.
2 Overview
Theorem Proving
Parsing
Feature-based CFG + 
Earley parser
Statistical dependency 
grammar parser
Underspecified LFs
Hole Semantics
Linear Logic Glue 
Semantics
Logical Forms
FOL + ?
DRS + ?
Model Checking
Tableau TP
Model Building
Mace
Non-monotonic TP
Prover9
Like B&B, we assume that
one of the most important
tasks for the teacher is to
ground students in the ba-
sic concepts of first order
logic and the lambda cal-
culus, model-theoretic in-
terpretation and inference.
This provides a basis for
exploring more modern ap-
proaches like Discourse Rep-
resentation Theory (drt;
Kamp and Reyle (1993))
and underspecification.
In the accompanying fig-
ure, we give a diagram-
matic overview of the main
semantics-related function-
ality that is currently avail-
able in nltk. Logical
forms (lfs) can be induced
as result of syntactic pars-
ing, using either feature-
based grammars that are
processed with an Earley
chart parser, or else by as-
sociating lfs with the output of a broad-coverage dependency parser. Our
basic lfs are expressions of first order logic, supplemented with the lambda
117
operator. However, we also admit Discourse Representation Structures
(drss) as lfs, and underspecified lfs can be built using either Hole Se-
mantics (Blackburn and Bos, 2005) or Glue Semantics (Dalrymple et al,
1999). Once we have constructed lfs, they can be evaluated in a first order
model (Klein, 2006), tested for equivalence and validity in a variety of the-
orem provers, or tested for consistency in a model builder. The latter two
tasks are aided by nltk interfaces to third-party inference tools, currently
Prover9 and Mace4 (McCune, 2008).
We do not have space in this paper to discuss all of these components,
but will try to present some of the key aspects, and along the way noting
certain points of difference vis-a`-vis B&B.
3 Logical Form
3.1 First Order Predicate Logic with Lambda Calculus
From a pedagogical point of view, it is usually important to ensure that stu-
dents have some grasp of the language of first order predicate logic (fol),
and can also manipulate ?-abstraction. The nltk.sem.logic module con-
tains an object-oriented approach to representing fol plus ?-abstraction.
Logical formulas are typically fed to the logic parser as strings, and then
represented as instances of various subclasses of Expression, as we will see
shortly.
An attractive feature of Python is its interactive interpreter, which allows
the user to enter Python expressions and statements for evaluation. In the
example below and subsequently, >>> is the Python interpreter?s prompt.
1 >>> from nltk.sem import logic
2 >>> lp = logic.LogicParser()
3 >>> e = lp.parse(?all x.(girl(x) -> exists y.(dog(y) & chase(x,y)))?)
4 >>> e
5 <AllExpression all x.(girl(x) -> exists y.(dog(y) & chase(x,y)))>
As illustrated, the result of parsing the formula at line 3 is an object e be-
longing to the class AllExpression, itself a subclass of Expression. All
such subclasses have numerous methods that implement standard logical
operations. For example, the simplify() method carries out ?-conversion;
the free() method finds all the free variables in an expression; and for quan-
tified expressions (such as AllExpressions), there is an alpha convert()
method. The logic module will ?-convert automatically when appropri-
ate to avoid name-clashes in the replace() method. Let?s illustrate these
methods with a formula involving ?-abstraction, namely \x.P(x)(y); we
use \ to represent ?. (Since \ is a special character in Python, we add the
r prefix to strings containing it to preclude additional escape characters.)
118
>>> from nltk.sem import Variable
>>> e1 = lp.parse(r?\x.P(x)(y)?)
>>> print e1.simplify()
P(y)
>>> e2 = lp.parse(?all x.P(x,a,b)?)
>>> print e2.free()
set([<Variable(?a?), Variable(?b?)])
>>> print e2.alpha_convert(Variable(?z?))
all z.P(z,a,b)
>>> e3 = lp.parse(?x?)
>>> print e2.replace(Variable(?b?), e3)
all z1.P(z1,a,x)
Allowing students to build simple first order models, and evaluate expres-
sions in those models, can be useful for helping them clarify their intuitions
about quantification. In the next example, we show one of the available
methods in nltk for specifying a model and using it to determine the set of
satisfiers of the open formula ?x.(girl(y) ? chase(x, y)).
2
,
3
>>> from nltk.sem import parse_valuation, Model, Assignment
>>> v = """
... suzie => s
... fido => f
... rover => r
... girl => {s}
... chase => {(f, s), (r, s), (s, f)}
... """
>>> val = parse_valuation(v) #create a Valuation
>>> m = Model(val.domain, val) #initialize a Model
>>> g = Assignment(val.domain) #initialize an Assignment
>>> e4 = lp.parser(?exists y. (girl(y) & chase(x, y))?)
>>> m.satisfiers(e4, ?x?, g) #check satisfiers of e4 wrt to x
set([?r?, ?f?])
In B&B, ?-abstracts are second-class citizens, used exclusively as a ?glue?
mechanism for composing meaning representations. Although we use ?-
abstracts as glue too, abstracts over individual variables are semantically
interpreted in nltk, namely as characteristic functions.
Expressions in nltk can be optionally typed (using Montague-style
types) by passing the parameter type check=True to LogicParser. Apart
from allowing the user to display the Expression?s type with type, type
checking will raise an exception for non-well typed expressions:
2
The triple quotes """ in Python allow us to break a logical line across several physical
lines.
3
Given a valuation val, the property val.domain returns the set of all domain indi-
viduals specified in the valuation.
119
>>> tlp = logic.LogicParser(type_check=True)
>>> a = tlp.parse(r?\x y.see(x,y)?)
>>> b = tlp.parse(r?\x.man(x)?)
>>> a.type, b.type
(<e,<e,t>>, <e,t>)
>>> tlp.parse(r?\x y.see(x,y)(\x.man(x))?)
Traceback (most recent call last):
. . .
TypeException: The function ?\x y.see(x,y)? is of type ?<e,<e,t>>?
and cannot be applied to ?\x.man(x)? of type ?<e,t>?. Its argument
must match type ?e?.
3.2 Discourse Representation Theory
As mentioned earlier, nltk contains an extension to the logic module for
working with Discourse Representation Theory (drt) (Kamp and Reyle,
1993). The nltk.sem.drt module introduces a DRS() constructor which
takes lists of discourse referents and conditions as initialization parameters:
(1) DRS([j,d],[John(j), dog(d), sees(j,d)])
On top of the functionality available for fol expressions, drt expres-
sions have a ?drs-concatenation? operator, represented as the + symbol. The
concatenation of two drss is a single drs containing the merged discourse
referents and the conditions from both arguments. drs-concatenation auto-
matically ?-converts bound variables to avoid name-clashes. The + symbol
is overloaded so that drt expressions can be added together easily. The
nltk.sem.drt parser allows drss to be specified succinctly as strings.
>>> from nltk.sem import drt
>>> dp = drt.DrtParser()
>>> d1 = dp.parse(?([x],[walk(x)]) + ([y],[run(y)])?)
>>> print d1
(([x],[walk(x)]) + ([y],[run(y)]))
>>> print d1.simplify()
([x,y],[walk(x), run(y)])
>>> d2 = dp.parse(?([x,y],[Bill(x), Fred(y)])?)
>>> d3 = dp.parse("""([],[([u],[Porsche(u), own(x,u)])
... -> ([v],[Ferrari(v), own(y,u)])])""")
>>> d4 = d2 + d3
>>> print d4.simplify()
([x,y],[Bill(x), Fred(y),
(([u],[Porsche(u), own(x,u)]) -> ([v],[Ferrari(v), own(y,u)]))])
drt expressions can be converted to their first order predicate logic equiva-
lents using the toFol() method and can be graphically rendered on screen
with the draw() method.
120
>>> print d1.toFol()
(exists x.walk(x) & exists y.run(y))
>>> d4.simplify().draw()
Figure 1: DRS Screenshot
Since the ? operator can be combined
with drt expressions, the nltk.sem.drt mod-
ule can be used as a plug-in replacement for
nltk.sem.logic in building compositional se-
mantics.
4 Scope Ambiguity and Underspecification
Two key questions in introducing students to computational semantics are:
Q1: How are semantic representations constructed from input sentences?
Q2: What is scope ambiguity and how is it captured?
A standard pedagogical approach is to address (Q1) with a simple syntax-
driven induction of logical forms which fails to deal with scope ambiguity,
while (Q2) is addressed by introducing underspecified representations which
are resolved to produce different readings of ambiguous sentences.
nltk includes a suite of parsing tools, amongst which is a chart parser
for context free grammars augmented with feature structures. A ?semantics?
feature sem allows us to compose the contributions of constituents to build
a logical form for a complete sentence. To illustrate, the following minimal
grammar sem1.fcfg handles quantification and intransitive verbs (where
values such as ?subj and ?vp are unification variables, while P and Q are
?-bound object language variables):
S[sem = <?subj(?vp)>] -> NP[sem=?subj] VP[sem=?vp]
VP[sem=?v] -> IV[sem=?v]
NP[sem=<?det(?n)>] -> Det[sem=?det] N[sem=?n]
Det[sem=<\P.\Q.exists x.(P(x) & Q(x))>] -> ?a?
N[sem=<\x.dog(x)>] -> ?dog?
IV[sem=<\x.bark(x)>] -> ?barks?
Using sem1.fcfg, we can parse A dog barks and view its semantics. The
load earley() method takes an optional parameter logic parser which
specifies the logic-parser for processing the value of the sem feature, thus
allowing different kinds of logical forms to be constructed.
>>> from nltk.parse import load_earley
>>> parser = load_earley(?grammars/sem1.fcfg?, trace=0)
>>> trees = parser.nbest_parse(?a dog barks?.split())
>>> print trees[0].node[?sem?].simplify()
exists x.(dog(x) & bark(x))
121
Underspecified logical forms allow us to loosen the relation between syn-
tactic and semantic representations. We consider two approaches to under-
specification, namely Hole Semantics and Glue Semantics. Since the former
will be familiar from B&B, we devote most of our attention to presenting
Glue Semantics.
4.1 Hole Semantics
Hole Semantics in nltk is handled by the nltk.sem.hole module, which
uses a context free grammar to generate an underspecified logical form.
Since the latter is itself a formula of first order logic, we can continue to use
the sem feature in the context free grammar:
N[sem=<\x h l.(PRED(l,dog,x) & LEQ(l,h) & HOLE(h) & LABEL(l))>]
-> ?dog?
The Hole Semantics module uses a standard plugging algorithm to derive
the sentence?s readings from the underspecified lf.
>>> from nltk.sem import hole
>>> readings = hole.hole_readings(?every girl chases a dog?)
>>> for r in reading: print r
exists z1.(dog(z1) & all z2.(girl(z2) -> chase(z1,z2)))
all z2.(girl(z2) -> exists z1.(dog(z1) & chase(z1,z2)))
4.2 Glue Semantics
Glue Semantics (Dalrymple et al, 1999), or Glue for short, is an approach to
compositionality that tries to handle semantic ambiguity by using resource-
sensitive logic to assemble meaning expressions. The approach builds proofs
over ?meaning constructors?; these are of the form M : G, where M is a
meaning representation and G is a term of linear logic. The linear logic
term G dictates how the meaning expression M can be combined. Each
distinct proof that can be derived reflects a different semantic reading of the
entire sentence.
The variant of linear logic that we use has (linear) implication (i.e., ()
as its only operator, so the primary operation during the proof is Modus
Ponens. Linear logic is an appropriate logic to serve as ?glue? because it
is resource-sensitive. This means that when Modus Ponens combines two
terms to create a new one, the two original terms are ?consumed?, and cannot
be used again in the proof; cf. (2) vs. (3). Additionally, every premise must
be used for the proof to be valid; cf. (4). This resource-sensitivity dictates
that each word contributes its meaning exactly once to the meaning of the
whole.
122
(2) A, (A( B) ` B
(3) A, (A( B) 0 A,B
(4) A,A, (A( B) 0 B
nltk?s nltk.gluesemantics.linearlogic module contains an implemen-
tation of linear logic.
The primary rule for composing Glue formulas is (5). Function-argument
application of meaning expressions is reflected (via the Curry-Howard iso-
morphism) by the application of Modus Ponens in a linear logic proof. Note
that A and B are meta-variables over constants of linear logic; these con-
stants represent ?attachment points? for meaning expressions in some kind of
syntactically-derived representation (such as an LFG f -structure). It is (5)
which allows Glue to guide the construction of complex meaning expressions.
(5) ? : A, ? : (A( B) ` ?(?) : B
The nltk modules gluesemantics.glue and gluesemantics.drt glue
implement Glue for fol and drt meaning expressions, respectively.
4
The
following example shows how Glue formulas are created and combined to
derive a logical form for John walks:
>>> from nltk.gluesemantics.glue import GlueFormula
>>> john = GlueFormula(?john?, ?g?)
>>> walks = GlueFormula(r?\x.walk(x)?, ?(g -o f)?)
>>> john_walks = walks.applyto(john)
>>> print john_walks.meaning.simplify()
walk(john)
Thus, the non-logical constant john is associated with the Glue term g,
while the meaning expression ?x.walk(x) is associated with (g ( f) since
it is a function that takes g as input and returns the meaning expression f ,
corresponding to the whole sentence. Consequently, a proof of f from the
premises is a derivation of a meaning representation for the sentence.
Scope ambiguity, resulting, for example, from quantifiers, requires the
use of variables in the Glue terms. Such variables may be instantiated to any
linear logic constant, so long as this is carried out uniformly. Let?s assume
that the quantified noun phrase every girl has the meaning constructor (6)
(where G is a linear logic variable):
(6) ?Q.?x.(girl(x) ? Q(x)) : ((g( G)( G)
4
See http://nltk.googlecode.com/svn/trunk/doc/contrib/sem/index.html for
more details.
123
Then the Glue derivation shown below correctly generates two readings for
the sentence Every girl chases a dog :
>>> from nltk.gluesemantics.glue import GlueFormula, Glue
>>> a = GlueFormula(r?\Q.all x.(girl(x) -> Q(x))?, ?((g -o G) -o G)?)
>>> b = GlueFormula(r?\x y.chase(x,y)?, ?(g -o (h -o f))?)
>>> c = GlueFormula(r?\Q.exists x.(dog(x)&Q(x))?, ?((h -o H) -o H)?)
>>> glue = Glue()
>>> for reading in glue.get_readings(glue.gfl_to_compiled([a,b,c])):
... print reading.simplify()
exists x.(dog(x) & all z13.(girl(z13) -> chase(z13,x)))
all x.(girl(x) -> exists z14.(dog(z14) & chase(x,z14)))
5 Inference tools
In order to perform inference over semantic representations, nltk can call
both theorem provers and model builders. The library includes a pure
Python tableau-based first order theorem prover; this is intended to allow
students to study tableau methods for theorem proving, and provides an
opportunity for experimentation. In addition, nltk provides interfaces to
two off-the-shelf tools, namely the theorem prover Prover9, and the model
builder Mace4 (McCune, 2008).
The get_prover(G, A) method by default calls Prover9, and takes as
parameters a proof goal G and a list A of assumptions. Here, we verify that
if every dog barks, and Rover is a dog, then it is true that Rover barks:
>>> from nltk.inference import inference
>>> a = lp.parse(?all x.(dog(x) -> bark(x))?)
>>> b = lp.parse(?dog(rover)?)
>>> c = lp.parse(?bark(rover)?)
>>> prover = inference.get_prover(c, [a,b])
>>> prover.prove()
True
A theorem prover can also be used to check the logical equivalence of
expressions. For two expressions A and B, we can pass (A ?? B) into
a theorem prover and know that the theorem will be proved if and only if
the expressions are logically equivalent. nltk?s standard equality operator
for Expressions (==) is able to handle situations where two expressions are
identical up to ?-conversion. However, it would be impractical for nltk
to invoke a wider range of logic rules every time we checked for equality of
two expressions. Consequently, both the logic and drt modules in nltk
have a separate method, tp equals, for checking ?equality? up to logical
equivalence.
124
>>> a = lp.parse(?all x.walk(x)?)
>>> b = lp.parse(?all y.walk(y)?)
>>> a == b
True
>>> c = lp.parse(?-(P(x) & Q(x))?)
>>> d = lp.parse(?-P(x) | -Q(x)?)
>>> c == d
False
>>> c.tp_equals(d)
True
6 Discourse Processing
nltk contains a discourse processing module, nltk.inference.discourse,
similar to the curt program presented in B&B. This module processes
sentences incrementally, keeping track of all possible threads when there is
ambiguity. For simplicity, the following example ignores scope ambiguity.
>>> from nltk.inference.discourse import DiscourseTester as DT
>>> dt = DT([?A student dances?, ?Every student is a person?])
>>> dt.readings()
s0 readings:
------------------------------
s0-r0: exists x.(student(x) & dance(x))
s1 readings:
------------------------------
s1-r0: all x.(student(x) -> person(x))
When a new sentence is added to the current discourse, setting the parameter
consistchk=True causes consistency to be checked by invoking the model
checker for each ?thread?, i.e., discourse sequence of admissible readings. In
this case, the user has the option of retracting the sentence in question.
>>> dt.add_sentence(?No person dances?, consistchk=True)
Inconsistent discourse d0 [?s0-r0?, ?s1-r0?, ?s2-r0?]:
s0-r0: exists x.(student(x) & dance(x))
s1-r0: all x.(student(x) -> person(x))
s2-r0: -exists x.(person(x) & dance(x))
>>> dt.retract_sentence(?No person dances?, quiet=False)
Current sentences are
s0: A student dances
s1: Every student is a person
In a similar manner, we use informchk=True to check whether the new sen-
tence is informative relative to the current discourse (by asking the theorem
prover to derive it from the discourse).
125
>>> dt.add_sentence(?A person dances?, informchk=True)
Sentence ?A person dances? under reading ?exists x.(person(x) &
dance(x))?:
Not informative relative to thread ?d0?
It is also possible to pass in an additional set of assumptions as background
knowledge and use these to filter out inconsistent readings.
The discourse module can accommodate semantic ambiguity and filter
out readings that are not admissable. By invoking both Glue Semantics and
drt, the following example processes the two-sentence discourse Every dog
chases a boy. He runs. As shown, the first sentence has two possible read-
ings, while the second sentence contains an anaphoric pronoun, indicated as
PRO(x).
>>> from nltk.inference.discourse import DrtGlueReadingCommand as RC
>>> dt = DT([?Every dog chases a boy?, ?He runs?], RC())
>>> dt.readings()
s0 readings:
------------------------------
s0-r0: ([],[(([x],[dog(x)]) -> ([z15],[boy(z15), chase(x,z15)]))])
s0-r1: ([z16],[boy(z16), (([x],[dog(x)]) -> ([],[chase(x,z16)]))])
s1 readings:
------------------------------
s1-r0: ([x],[PRO(x), run(x)])
When we examine the two threads d0 and d1, we see that that reading
s0-r0, where every dog out-scopes a boy, is deemed inadmissable because
the pronoun in the second sentence cannot be resolved. By contrast, in
thread d1 the pronoun (relettered to z24) has been bound via the equation
(z24 = z20).
>>> dt.readings(show_thread_readings=True)
d0: [?s0-r0?, ?s1-r0?] : INVALID: AnaphoraResolutionException
d1: [?s0-r1?, ?s1-r0?] : ([z20,z24],[boy(z20), (([x],[dog(x)]) ->
([],[chase(x,z20)])), (z24 = z20), run(z24)])
7 Conclusions and Future Work
nltk?s semantics functionality has been written with extensibility in mind.
The logic module?s LogicParser employs a basic parsing template and
contains hooks that an extending module can use to supplement or sub-
stitute functionality. Moreover, the base Expression class in logic, as
well as any derived classes, can be extended, allowing variants to reuse the
existing functionality. For example, the drt and linear logic modules are
implemented as extensions to logic.py.
126
The theorem prover and model builder code has also been carefully archi-
tected to allow extensions and the nltk.inference.api library exposes the
framework for the inference architecture. The library therefore provides a
good starting point for creating interfaces with other theorem provers and
model builders in addition to Prover9, Mace4, and the tableau prover.
nltk already includes the beginnings of a framework for ?recognizing
textual entailment?; access to the rte data sets is provided and we are in the
course of developing a few simple modules to demonstrate rte techniques.
For example, a Logical Entailment rte tagger based on Bos and Markert
(2005) begins by building a semantic representation of both the text and
the hypothesis in drt. It then runs a theorem prover with the text as the
assumption and the hypothesis as the goal in order to check whether the
text entails the hypothesis.The tagger is also capable of adding background
knowledge via an interface to the WordNet dictionary in nltk.wordnet as
a first step in making the entailment checking more robust.
References
Steven Bird, Ewan Klein, Edward Loper, and Jason Baldridge. Multidisciplinary
instruction with the Natural Language Toolkit. In Proceedings of the Third
Workshop on Issues in Teaching Computational Linguistics, Columbus, Ohio,
USA, June 2008.
Patrick Blackburn and Johan Bos. Representation and Inference for Natural Lan-
guage: A First Course in Computational Semantics. CSLI Publications, New
York, 2005.
Johan Bos and Katja Markert. Recognising textual entailment with logical infer-
ence. In Proceedings of the conference on Human Language Technology and Em-
pirical Methods in Natural Language Processing, Vancouver, British Columbia,
Canada, 2005.
Mary Dalrymple, V. Gupta, John Lamping, and V. Saraswat. Relating resource-
based semantics to categorial semantics. In Mary Dalrymple, editor, Semantics
and syntax in Lexical Functional Grammar: the resource logic approach, pages
261?280. MIT Press, Cambridge, MA, 1999.
Hans Kamp and Uwe Reyle. From Discourse to the Lexicon: Introduction to Mod-
eltheoretic Semantics of Natural Language, Formal Logic and Discourse Repre-
sentation Theory. Kluwer Academic Publishers, 1993.
Ewan Klein. Computational semantics in the Natural Language Toolkit. In Pro-
ceedings of the Australasian Language Technology Workshop, pages 26?33, 2006.
William McCune. Prover9: Automated theorem prover for first-order
and equational logic, 2008. http://www.cs.unm.edu/
~
mccune/mace4/
manual-examples.html.
127
Integrating Logical Representations with
Probabilistic Information using Markov Logic
Dan Garrette
University of Texas at Austin
dhg@cs.utexas.edu
Katrin Erk
University of Texas at Austin
katrin.erk@mail.utexas.edu
Raymond Mooney
University of Texas at Austin
mooney@cs.utexas.edu
Abstract
First-order logic provides a powerful and flexible mechanism for representing natural language
semantics. However, it is an open question of how best to integrate it with uncertain, probabilistic
knowledge, for example regarding word meaning. This paper describes the first steps of an approach
to recasting first-order semantics into the probabilistic models that are part of Statistical Relational
AI. Specifically, we show how Discourse Representation Structures can be combined with distribu-
tional models for word meaning inside a Markov Logic Network and used to successfully perform
inferences that take advantage of logical concepts such as factivity as well as probabilistic informa-
tion on word meaning in context.
1 Introduction
Logic-based representations of natural language meaning have a long history. Representing the meaning
of language in a first-order logical form is appealing because it provides a powerful and flexible way to
express even complex propositions. However, systems built solely using first-order logical forms tend
to be very brittle as they have no way of integrating uncertain knowledge. They, therefore, tend to have
high precision at the cost of low recall (Bos and Markert, 2005).
Recent advances in computational linguistics have yielded robust methods that use weighted or prob-
abilistic models. For example, distributional models of word meaning have been used successfully to
judge paraphrase appropriateness. This has been done by representing the word meaning in context as
a point in a high-dimensional semantics space (Erk and Pado?, 2008; Thater et al, 2010; Erk and Pado?,
2010). However, these models typically handle only individual phenomena instead of providing a mean-
ing representation for complete sentences. It is a long-standing open question how best to integrate the
weighted or probabilistic information coming from such modules with logic-based representations in a
way that allows for reasoning over both. See, for example, Hobbs et al (1993).
The goal of this work is to combine logic-based meaning representations with probabilities in a
single unified framework. This will allow us to obtain the best of both situations: we will have the
full expressivity of first-order logic and be able to reason with probabilities. We believe that this will
allow for a more complete and robust approach to natural language understanding. In order to perform
logical inference with probabilities, we draw from the large and active body of work related to Statistical
Relational AI (Getoor and Taskar, 2007). Specifically, we make use of Markov Logic Networks (MLNs)
(Richardson and Domingos, 2006) which employ weighted graphical models to represent first-order
logical formulas. MLNs are appropriate for our approach because they provide an elegant method of
assigning weights to first-order logical rules, combining a diverse set of inference rules, and performing
inference in a probabilistic way.
While this is a large and complex task, this paper proposes a series of first steps toward our goal.
In this paper, we focus on three natural language phenomena and their interaction: implicativity and
factivity, word meaning, and coreference. Our framework parses natural language into a logical form,
adds rule weights computed by external NLP modules, and performs inferences using an MLN. Our
end-to-end approach integrates multiple existing tools. We use Boxer (Bos et al, 2004) to parse natural
105
language into a logical form. We use Alchemy (Kok et al, 2005) for MLN inference. Finally, we use the
exemplar-based distributional model of Erk and Pado? (2010) to produce rule weights.
2 Background
Logic-based semantics. Boxer (Bos et al, 2004) is a software package for wide-coverage semantic anal-
ysis that provides semantic representations in the form of Discourse Representation Structures (Kamp
and Reyle, 1993). It builds on the C&C CCG parser (Clark and Curran, 2004). Bos and Markert (2005)
describe a system for Recognizing Textual Entailment (RTE) that uses Boxer to convert both the premise
and hypothesis of an RTE pair into first-order logical semantic representations and then uses a theorem
prover to check for logical entailment.
Distributional models for lexical meaning. Distributional models describe the meaning of a word
through the context in which it appears (Landauer and Dumais, 1997; Lund and Burgess, 1996), where
contexts can be documents, other words, or snippets of syntactic structure. Distributional models are able
to predict semantic similarity between words based on distributional similarity and they can be learned
in an unsupervised fashion. Recently distributional models have been used to predict the applicability
of paraphrases in context (Mitchell and Lapata, 2008; Erk and Pado?, 2008; Thater et al, 2010; Erk and
Pado?, 2010). For example, in ?The wine left a stain?, ?result in? is a better paraphrase for ?leave? than is
?entrust?, while the opposite is true in ?He left the children with the nurse?. Usually, the distributional
representation for a word mixes all its usages (senses). For the paraphrase appropriateness task, these
representations are then reweighted, extended, or filtered to focus on contextually appropriate usages.
Markov Logic. An MLN consists of a set of weighted first-order clauses. It provides a way of softening
first-order logic by making situations in which not all clauses are satisfied less likely but not impossible
(Richardson and Domingos, 2006). More formally, letX be the set of all propositions describing a world
(i.e. the set of all ground atoms), F be the set of all clauses in the MLN, wi be the weight associated
with clause fi ? F , Gfi be the set of all possible groundings of clause fi, and Z be the normalization
constant. Then the probability of a particular truth assignment x to the variables in X is defined as:
P (X = x) =
1
Z exp
?
?
?
fi?F
wi
?
g?Gfi
g(x)
?
? =
1
Z exp
?
?
?
fi?F
wini(x)
?
? (1)
where g(x) is 1 if g is satisfied and 0 otherwise, and ni(x) =
?
g?Gfi
g(x) is the number of groundings
of fi that are satisfied given the current truth assignment to the variables in X . This means that the
probability of a truth assignment rises exponentially with the number of groundings that are satisfied.
Markov Logic has been used previously in other NLP application (e.g. Poon and Domingos (2009)).
However, this paper marks the first attempt at representing deep logical semantics in an MLN.
While it is possible learn rule weights in anMLN directly from training data, our approach at this time
focuses on incorporating weights computed by external knowledge sources. Weights for word meaning
rules are computed from the distributional model of lexical meaning and then injected into the MLN.
Rules governing implicativity and coreference are given infinite weight (hard constraints).
3 Evaluation and phenomena
Textual entailment offers a good framework for testing whether a system performs correct analyses and
thus draws the right inferences from a given text. For example, to test whether a system correctly handles
implicative verbs, one can use the premise p along with the hypothesis h in (1) below. If the system
analyses the two sentences correctly, it should infer that h holds. While the most prominent forum using
textual entailment is the Recognizing Textual Entailment (RTE) challenge (Dagan et al, 2005), the RTE
datasets do not test the phenomena in which we are interested. For example, in order to evaluate our
system?s ability to determine word meaning in context, the RTE pair would have to specifically test word
106
sense confusion by having a word?s context in the hypothesis be different from the context of the premise.
However, this simply does not occur in the RTE corpora. In order to properly test our phenomena, we
construct hand-tailored premises and hypotheses based on real-world texts.
In this paper, we focus on three natural language phenomena and their interaction: implicativity and
factivity, word meaning, and coreference. The first phenomenon, implicativity and factivity, is concerned
with analyzing the truth conditions of nested propositions. For example, in the premise of the entailment
pair shown in example (1), ?arrange that? falls under the scope of ?forget to? and ?fail? is under the scope
of ?arrange that?. Correctly recognizing nested propositions is necessary for preventing false inferences
such as the one in example (2).
(1) p: Ed did not forget to arrange that Dave fail1
h: Dave failed
(2) p: The mayor hoped to build a new stadium2
h*: The mayor built a new stadium
For the second phenomenon, word meaning, we address paraphrasing and hypernymy. For example,
in (3) ?covering? is a good paraphrase for ?sweeping? while ?brushing? is not.
(3) p: A stadium craze is sweeping the country
h1: A stadium craze is covering the country
h2*: A stadium craze is brushing the country
The third phenomenon is coreference, as illustrated in (4). For this example, to correctly judge the
hypothesis as entailed, it is necessary to recognize that ?he? corefers with ?Christopher? and ?the new
ballpark? corefers with ?a replacement for Candlestick Park?.
(4) p: George Christopher has been a critic of the plan to build a replacement for Candlestick Park.
As a result, he won?t endorse the new ballpark.
h: Christopher won?t endorse a replacement for Candlestick Park.
Some natural language phenomena are most naturally treated as categorial, while others are more
naturally treated using weights or probabilities. In this paper, we treat implicativity and coreference as
categorial phenomena, while using a probabilistic approach to word meaning.
4 Transforming natural language text to logical form
In transforming natural language text to logical form, we build on the software package Boxer (Bos et al,
2004). We chose to use Boxer for two main reasons. First, Boxer is a wide-coverage system that can deal
with arbitrary text. Second, the DRSs that Boxer produces are close to the standard first-order logical
forms that are required for use by the MLN software package Alchemy. Our system transforms Boxer
output into a format that Alchemy can read and augments it with additional information.
To demonstrate our transformation procedure, consider again the premise of example (1). When
given to Boxer, the sentence produces the output given in Figure 1a. We then transform this output to the
format given in Figure 1b.
Flat structure. In Boxer output, nested propositional statements are represented as nested sub-DRS
structures. For example, in the premise of (1), the verbs ?forget to? and ?arrange that? both introduce
nested propositions, as is shown in Figure 1a where DRS x3 (the ?arranging that?) is the theme of ?forget
to? and DRS x5 (the ?failing?) is the theme of ?arrange that?.
In order to write logical rules about the truth conditions of nested propositions, the structure has to
be flattened. However, it is clearly not sufficient to just conjoin all propositions at the top level. Such an
approach, applied to example (2), would yield (hope(x1) ? theme(x1, x2) ? build(x2) ? . . .), leading
to the wrong inference that the stadium was built. Instead, we add a new argument to each predicate that
1Examples (1) and (16) and Figure 2 are based on examples by MacCartney and Manning (2009)
2Examples (2), (3), (4), and (18) are modified versions of sentences from document wsj 0126 from the Penn Treebank
107
x0 x1
named(x0,ed,per)
named(x1,dave,per)
?
x2 x3
forget(x2)
event(x2)
agent(x2,x0)
theme(x2,x3)
x3:
x4 x5
arrange(x4)
event(x4)
agent(x4,x0)
theme(x4,x5)
x5:
x6
fail(x6)
event(x6)
agent(x6,x1)
(a) Output from Boxer
transforms to????????
named(l0, ne per ed d s0 w0, z0)
named(l0, ne per dave d s0 w7, z1)
not(l0, l1)
pred(l1, v forget d s0 w3, e2)
event(l1, e2)
rel(l1, agent, e2, z0)
rel(l1, theme, e2, l2)
prop(l1, l2)
pred(l2, v arrange d s0 w5, e4)
event(l2, e4)
rel(l2, agent, e4, z0)
rel(l2, theme, e4, l3)
prop(l2, l3)
pred(l3, v fail d s0 w8, e6)
event(l3, e6)
rel(l3, agent, e6, z1)
(b) Canonical form
Figure 1: Converting the premise of (1) from Boxer output to MLN input
names the DRS in which the predicate originally occurred. Assigning the label l1 to the DRS containing
the predicate forget, we add l1 as the first argument to the atom pred(l1, v forget d s0 w3, e2).3 Having
flattened the structure, we need to re-introduce the information about relations between DRSs. For this
we use predicates not, imp, and or whose arguments are DRS labels. For example, not(l0, l1) states that
l1 is inside l0 and negated. Additionally, an atom prop(l0, l1) indicates that DRS l0 has a subordinate
DRS labeled l1.
One important consequence of our flat structure is that the truth conditions of our representation no
longer coincide with the truth conditions of the underlying DRS being represented. For example, we do
not directly express the fact that the ?forgetting? is actually negated, since the negation is only expressed
as a relation between DRS labels. To access the information encoded in relations between DRS labels, we
add predicates that capture the truth conditions of the underlying DRS. We use the predicates true(label)
and false(label) that state whether the DRS referenced by label is true or false. We also add rules that
govern how the predicates for logical operators interact with these truth values. For example, the rules in
(5) state that if a DRS is true, then any negated subordinate must be false and vice versa.
? p n.[not(p, n) ? (true(p) ? false(n)) ? (false(p) ? true(n))] (5)
Injecting additional information into the logical form. We want to augment Boxer output with addi-
tional information, for example gold coreference annotation for sentences that we subsequently analyze
with Boxer. In order to do so, we need to be able to tie predicates in the Boxer output back to words in
the original sentence. Fortunately, the optional ?Prolog? output format from Boxer provides the sentence
and word indices from the original sentence. When parsing the Boxer output, we extract these indices
and concatenate them to the word lemma to specific the exact occurrence of the lemma that is under
discussion. For example, the atom pred(l1, v forget d s0 w3, e2) indicates that event e2 refers to the
lemma ?forget? that appears in the 0th sentence of discourse d at word index 3.
Atomic formulas. We represent the words from the sentence as arguments instead of predicates in order
to simplify the set of inference rules we need to specify. Because our flattened structure requires that
the inference mechanism be reimplemented as a set of logical rules, it is desirable for us to be able to
write general rules that govern the interaction of atoms. With the representation we have chosen, we
can quantify over all predicates or all relations. For example, the rule in (6) states that a predicate is
accessible if it is found in an out-scoping DRS.
3The extension to the word, such as d s0 w3 for ?forget?, is an index providing the location of the original word that
triggered this atom; this is addressed in more detail shortly.
108
signature example
managed to +/- he managed to escape  he escaped
he did not manage to escape  he did not escape
refused to -/o he refused to fight  he did not fight
he did not refuse to fight 2 {he fought, he did not fight}
Figure 2: Implication Signatures
? l1 l2.[outscopes(l1, l2) ? ? p x.[pred(l1, p, x) ? pred(l2, p, x)]] (6)
We use three different predicate symbols to distinguish three types of atomic concepts: predicates,
named entities, and relations. Predicates and named entities represent words that appear in the text.
For example, named(l0, ne per ed d s0 w0, z0) indicates that variable z0 is a person named ?Ed? while
pred(l1, v forget d s0 w3, e2) says that e2 is a ?forgetting to? event. Relations capture the relationships
between words. For example, rel(l1, agent, e2, z0) indicates that z0, ?Ed?, is the ?agent? of the ?forgetting
to? event e2.
5 Handling the phenomena
Implicatives and factives
Nairn et al (2006) presented an approach to the treatment of inferences involving implicatives and fac-
tives. Their approach identifies an ?implication signature? for every implicative or factive verb that
determines the truth conditions for the verb?s nested proposition, whether in a positive or negative en-
vironment. Implication signatures take the form ?x/y? where x represents the implicativity in the the
positive environment and y represents the implicativity in the negative environment. Both x and y have
three possible values: ?+? for positive entailment, meaning the nested proposition is entailed, ?-? for
negative entailment, meaning the negation of the proposition is entailed, and ?o? for ?null? entailment,
meaning that neither the proposition nor its negation is entailed. Figure 2 gives concrete examples.
We use these implication signatures to automatically generate rules that license specific entailments
in the MLN. Since ?forget to? has implication signature ?-/+?, we generate the two rules in (7).
(7) (a) ? l1 l2 e.[(pred(l1, ?forget?, e) ? true(l1) ? rel(l1, ?theme?, e, l2) ? prop(l1, l2)) ? false(l2)]]4
(b) ? l1 l2 e.[(pred(l1, ?forget?, e) ? false(l1) ? rel(l1, ?theme?, e, l2) ? prop(l1, l2)) ? true(l2)]
To understand these rules, consider (7a). The rule says that if the atom for the verb ?forget to? appears
in a DRS that has been determined to be true, then the DRS representing any ?theme? proposition of that
verb should be considered false. Likewise, (7b) says that if the occurrence of ?forget to? appears in a
DRS determined to be false, then the theme DRS should be considered true.
Note that when the implication signature indicates a ?null? entailment, no rule is generated for that
case. This prevents the MLN from licensing entailments related directly to the nested proposition, but
still allows for entailments that include the factive verb. So he wanted to fly entails neither he flew nor he
did not fly, but it does still license he wanted to fly.
Ambiguity in word meaning
In order for our system to be able to make correct natural language inference, it must be able to handle
paraphrasing and deal with hypernymy. For example, in order to license the entailment pair in (8), the
system must recognize that ?owns? is a valid paraphrase for ?has?, and that ?car? is a hypernym of
?convertible?.
(8) p: Ed has a convertible
h: Ed owns a car
4Occurrence-indexing on the predicate ?forget? has been left out for brevity.
109
In this section we discuss our probabilistic approach to paraphrasing. In the next section we discuss
how this approach is extended to cover hypernymy. A central problem to solve in the context of para-
phrases is that they are context-dependent. Consider again example (3) and its two hypotheses. The first
hypothesis replaces the word ?sweeping? with a paraphrase that is valid in the given context, while the
second uses an incorrect paraphrase.
To incorporate paraphrasing information into our system, we first generate rules stating all paraphrase
relationships that may possibly apply to a given predicate/hypothesis pair, using WordNet (Miller, 2009)
as a resource. Then we associate those rules with weights to signal contextual adequacy. For any two
occurrence-indexed words w1, w2 occurring anywhere in the premise or hypothesis, we check whether
they co-occur in a WordNet synset. If w1, w2 have a common synset, we generate rules of the form
? l x.[pred(l, w1, x) ? pred(l, w2, x)] to connect them. For named entities, we perform a similar
routine: for each pair of matching named entities found in the premise and hypothesis, we generate a
rule ? l x.[named(l, w1, x) ? named(l, w2, x)].
We then use the distributional model of Erk and Pado? (2010) to compute paraphrase appropriateness.
In the case of (3) this means measuring the cosine similarity between the vectors for ?sweep? and ?cover?
(and between ?sweep? and ?brush?) in the sentential context of the premise. MLN formula weights are
expected to be log-odds (i.e., log(P/(1?P )) for some probability P ), so we rank all possible paraphrases
of a given word w by their cosine similarity to w and then give them probabilities that decrease by
rank according to a Zipfian distribution. So, the kth closest paraphrase by cosine similarity will have
probability Pk given by (9):
Pk ? 1/k (9)
The generated rules are given in (10) with the actual weights calculated for example (3). Note that
the valid paraphrase ?cover? is given a higher weight than the incorrect paraphrase ?brush?, which allows
the MLN inference procedure to judge h1 as a more likely entailment than h2.5 This same result would
not be achieved if we did not take context into consideration because, without context, ?brush? is a more
likely paraphrase of ?sweep? than ?cover?.
(10) (a) -2.602 ? l x.[pred(l, ?v sweep p s0 w4?, x) ? pred(l, ?v cover h s0 w4?, x)]
(b) -3.842 ? l x.[pred(l, ?v sweep p s0 w4?, x) ? pred(l, ?v brush h s0 w4?, x)]
Since Alchemy outputs a probability of entailment and not a binary judgment, it is necessary to
specify a probability threshold indicating entailment. An appropriate threshold between ?entailment?
and ?non-entailment? will be one that separates the probability of an inference with the valid rule from
the probability of an inference with the invalid rule. While we plan to automatically induce a threshold
in the future, our current implementation uses a value set manually.
Hypernymy
Like paraphrasehood, hypernymy is context-dependent: In ?A bat flew out of the cave?, ?animal? is
an appropriate hypernym for ?bat?, but ?artifact? is not. So we again use distributional similarity to
determine contextual appropriateness. However, we do not directly compute cosine similarities between
a word and its potential hypernym. We can hardly assume ?baseball bat? and ?artifact? to occur in similar
distributional contexts. So instead of checking for similarity of ?bat? and ?artifact? in a given context, we
check ?bat? and ?club?. That is, we pick a synonym or close hypernym of the word in question (?bat?)
that is also a WordNet hyponym of the hypernym to check (?artifact?).
A second problem to take into account is the interaction of hypernymy and polarity. While (8) is a
valid pair, (11) is not, because ?have a convertible? is under negation. So, we create weighted rules of
the form hypernym(w, h), along with inference rules to guide their interaction with polarity. We create
5Because weights are calculated according to the equation log(P/(1 ? P )), any paraphrase that has a probability of less
than 0.5 will have a negative weight. Since most paraphrases will have probabilities less than 0.5, most will yield negative
rule weights. However, the inferences are still handled properly in the MLN because the inference is dependent on the relative
weights.
110
these rules for all pairs of words w, h in premise and hypothesis such that h is a hypernym of w, again
using WordNet to determine potential hypernymy.
(11) p: Ed does not have a convertible
h: Ed does not own a car
Our inference rules governing the interaction of hypernymy and polarity are given in (12). The rule
in (12a) states that in a positive environment, the hyponym entails the hypernym while the rule in (12b)
states that in a negative environment, the opposite is true: the hypernym entails the hyponym.
(12) (a) ? l p1 p2 x.[(hypernym(p1, p2) ? true(l) ? pred(l, p1, x)) ? pred(l, p2, x)]]
(b) ? l p1 p2 x.[(hypernym(p1, p2) ? false(l) ? pred(l, p2, x)) ? pred(l, p1, x)]]
Making use of coreference information
As a test case for incorporating additional resources into Boxer?s logical form, we used the coreference
data in OntoNotes (Hovy et al, 2006). However, the same mechanism would allow us to transfer in-
formation into Boxer output from arbitrary additional NLP tools such as automatic coreference analysis
tools or semantic role labelers. Our system uses coreference information into two distinct ways.
The first way we make use of coreference data is to copy atoms describing a particular variable
to those variables that corefer. Consider again example (4) which has a two-sentence premise. This
inference requires recognizing that the ?he? in the second sentence of the premise refers to ?George
Christopher? from the first sentence. Boxer alone is unable to make this connection, but if we receive
this information as input, either from gold-labeled data or a third-party coreference tool, we are able to
incorporate it. Since Boxer is able to identify the index of the word that generated a particular predicate,
we can tie each predicate to any related coreference chains. Then, for each atom on the chain, we can
inject copies of all of the coreferring atoms, replacing the variables to match. For example, the word
?he? generates an atom pred(l0, male, z5)6 and ?Christopher? generates atom named(l0, christopher, x0).
So, we can create a new atom by taking the atom for ?christopher? and replacing the label and variable
with that of the atom for ?he?, generating named(l0, christopher, x5).
As a more complex example, the coreference information will inform us that ?the new ballpark?
corefers with ?a replacement for Candlestick Park?. However, our system is currently unable to handle
this coreference correctly at this time because, unlike the previous example, the expression ?a replace-
ment for Candlestick Park? results in a complex three-atom conjunct with two separate variables: pred(l2,
replacement, x6), rel(l2, for, x6, x7), and named(l2, candlestick park, x7). Now, unifying with the atom
for ?a ballpark?, pred(l0, ballpark, x3), is not as simple as replacing the variable because there are two
variables to choose from. Note that it would not be correct to replace both variables since this would
result in a unification of ?ballpark? with ?candlestick park? which is wrong. Instead we must determine
that x6 should be the one to unify with x3 while x7 is replaced with a fresh variable. The way that we can
accomplish this is to look at the dependency parse of the sentence that is produced by the C&C parser is
a precursor to running Boxer. By looking up both ?replacement? and ?Candlestick Park? in the parse, we
can determine that ?replacement? is the head of the phrase, and thus is the atom whose variable should
be unified. So, we would create new atoms, pred(l0, replacement, x3), rel(l0, for, x3, z0), and named(l0,
candlestick park, z0), where z0 is a fresh variable.
The second way that we make use of coreference information is to extend the sentential contexts
used for calculating the appropriateness of paraphrases in the distributional model. In the simplest case,
the sentential context of a word would simply be the other words in the sentence. However, consider the
context of the word ?lost? in the second sentence of (13).
(13) p1: In [the final game of the season]1, [the team]2 held on to their lead until overtime
p2: But despite that, [they]2 eventually lost [it all]1
6Atoms simplified for brevity
111
Here we would like to disambiguate ?lost?, but its immediate context, words like ?despite? and
?eventually?, gives no indication as to its correct sense. Our procedure extends the context of the sentence
by incorporating all of the words from all of the phrases that corefer with a word in the immediate
context. Since coreference chains 1 and 2 have words in p2, the context of ?lost? ends up including
?final?, ?game?, ?season?, and ?team? which give a strong indication of the sense of ?lost?. Note that
using coreference data is stronger than simply expanding the window because coreferences can cover
arbitrarily long distances.
6 Evaluation
As a preliminary evaluation of our system, we constructed a set of demonstrative examples to test our
ability to handle the previously discussed phenomena and their interactions and ran each example with
both a theorem prover and Alchemy. Note that when running an example in the theorem prover, weights
are not possible, so any rule that would be weighted in an MLN is simply treated as a ?hard clause?
following Bos and Markert (2005).
Checking the logical form. We constructed a list of 72 simple examples that exhaustively cover cases
of implicativity (positive, negative, null entailments in both positive and negative environments), hyper-
nymy, quantification, and the interaction between implicativity and hypernymy. The purpose of these
simple tests is to ensure that our flattened logical form and truth condition rules correctly maintain the
semantics of the underlying DRSs. Examples are given in (14).
(14) (a) The mayor did not manage to build a stadium 2 The mayor built a stadium
(b) Fido is a dog and every dog walks  A dog walks
Examples in previous sections. Examples (1), (2), (3), (8), and (11) all come out as expected. Each
of these examples demonstrates one of the phenomena in isolation. However, example (4) returns ?not
entailed?, the incorrect answer. As discussed previously, this failure is a result of our system?s inabil-
ity to correctly incorporate the complex coreferring expression ?a replacement for Candlestick Park?.
However, the system is able to correctly incorporate the coreference of ?he? in the second sentence to
?Christopher? in the first.
Implicativity and word sense. For example (15), ?fail to? is a negatively entailing implicative in a
positive environment. So, p correctly entails hgood in both the theorem prover and Alchemy. However,
the theorem prover incorrectly licenses the entailment of hbad while Alchemy does not. The probabilistic
approach performs better in this situation because the categorial approach does not distinguish between
a good paraphrase and a bad one. This example also demonstrates the advantage of using a context-
sensitive distributional model to calculate the probabilities of paraphrases because ?reward? is an a priori
better paraphrase than ?observe? according to WordNet since it appears in a higher ranked synset.
(15) p: The U.S. is watching closely as South Korea fails to honor U.S. patents7
hgood: South Korea does not observe U.S. patents
hbad: South Korea does not reward U.S. patents
Implicativity and hypernymy. MacCartney and Manning (2009) extended the work by Nairn et al
(2006) in order to correctly treat inference involving monotonicity and exclusion. Our approaches to
implicatives and factivity and hyper/hyponymy combine naturally to address these issues because of the
structure of our logical representations and rules. For example, no additional work is required to license
the entailments in (16).
(16) (a) John refused to dance  John didn?t tango
(b) John did not forget to tango  John danced
7Example (15) is adapted from Penn Treebank document wsj 0020 while (17) is adapted from document wsj 2358
112
Example (17) demonstrates how our system combines categorial implicativity with a probabilistic
approach to hypernymy. The verb ?anticipate that? is positively entailing in the negative environment.
The verb ?moderate? can mean ?chair? as in ?chair a discussion? or ?curb? as in ?curb spending?. Since
?restrain? is a hypernym of ?curb?, it receives a weight based on the applicability of the word ?curb? in
the context. Similarly, ?talk? receives a weight based on its hyponym ?chair?. Since our model predicts
?curb? to be a more probable paraphrase of ?moderate? in this context than ?chair? (even though the
priors according to WordNet are reversed), the system is able to infer hgood while rejecting hbad.
(17) p: He did not anticipate that inflation would moderate this year
hgood: Inflation restrained this year
hbad: Inflation talked this year
Word sense, coreference, and hypernymy. Example (18) demonstrates the interaction between para-
phrase, hypernymy, and coreference incorporated into a single entailment. The relevant coreference
chains are marked explicitly in the example. The correct inference relies on recognizing that ?he? in the
hypothesis refers to ?Joe Robbie? and ?it? to ?coliseum?, which is a hyponym of ?stadium?. Further,
our model recognizes that ?sizable? is a better paraphrase for ?healthy? than ?intelligent? even though
WordNet has the reverse order.
(18) p: [Joe Robbie]53 couldn?t persuade the mayor , so [he]53 built [[his]53 own coliseum]54.
[He]53 has used [it]54 to turn a healthy profit.8
hgood: Joe Robbie used a stadium to turn a sizable profit
hbad?1: Joe Robbie used a stadium to turn an intelligent profit
hbad?2: The mayor used a stadium to turn a healthy profit
7 Future work
The next step is to execute a full-scale evaluation of our approach using more varied phenomena and
naturally occurring sentences. However, the memory requirements of Alchemy are a limitation that
prevents us from currently executing larger and more complex examples. The problem arises because
Alchemy considers every possible grounding of every atom, even when a more focused subset of atoms
and inference rules would suffice. There is on-going work to modify Alchemy so that only the required
groundings are incorporated into the network, reducing the size of the model and thus making it possible
to handle more complex inferences. We will be able to begin using this new version of Alchemy very
soon and our task will provide an excellent test case for the modification.
Since Alchemy outputs a probability of entailment, it is necessary to fix a threshold that separates
entailment from nonentailment. We plan to use machine learning techniques to compute an appropriate
threshold automatically from a calibration dataset such as a corpus of valid and invalid paraphrases.
8 Conclusion
In this paper, we have introduced a system that implements a first step towards integrating logical seman-
tic representations with probabilistic weights using methods from Statistical Relational AI, particularly
Markov Logic. We have focused on three phenomena and their interaction: implicatives, coreference,
and word meaning. Taking implicatives and coreference as categorial and word meaning as probabilis-
tic, we have used a distributional model to generate paraphrase appropriateness ratings, which we then
transformed into weights on first order formulas. The resulting MLN approach is able to correctly solve
a number of difficult textual entailment problems that require handling complex combinations of these
important semantic phenomena.
8Only relevent coreferences have been marked
113
References
Bos, J., S. Clark, M. Steedman, J. R. Curran, and J. Hockenmaier (2004). Wide-coverage semantic
representations from a CCG parser. In Proceedings of COLING 2004, Geneva, Switzerland, pp. 1240?
1246.
Bos, J. and K. Markert (2005). Recognising textual entailment with logical inference. In Proceedings of
EMNLP 2005, pp. 628?635.
Clark, S. and J. R. Curran (2004). Parsing the WSJ using CCG and log-linear models. In Proceedings of
ACL 2004, Barcelona, Spain, pp. 104?111.
Dagan, I., O. Glickman, and B. Magnini (2005). The pascal recognising textual entailment challenge. In
In Proceedings of the PASCAL Challenges Workshop on Recognising Textual Entailment.
Erk, K. and S. Pado? (2008). A structured vector space model for word meaning in context. In Proceedings
of EMNLP 2008, Honolulu, HI, pp. 897?906.
Erk, K. and S. Pado? (2010). Exemplar-based models for word meaning in context. In Proceedings of
ACL 2010, Uppsala, Sweden, pp. 92?97.
Getoor, L. and B. Taskar (Eds.) (2007). Introduction to Statistical Relational Learning. Cambridge, MA:
MIT Press.
Hobbs, J. R., M. Stickel, D. Appelt, and P. Martin (1993). Interpretation as abduction. Artificial Intelli-
gence 63(1?2), 69?142.
Hovy, E., M. Marcus, M. Palmer, L. Ramshaw, and R. Weischedel (2006). Ontonotes: The 90% solution.
In Proceedings of HLT/NAACL 2006, pp. 57?60.
Kamp, H. and U. Reyle (1993). From Discourse to Logic; An Introduction to Modeltheoretic Semantics
of Natural Language, Formal Logic and DRT. Dordrecht: Kluwer.
Kok, S., P. Singla, M. Richardson, and P. Domingos (2005). The Alchemy system for statistical relational
AI. Technical report, Department of Computer Science and Engineering, University of Washington.
http://www.cs.washington.edu/ai/alchemy.
Landauer, T. and S. Dumais (1997). A solution to Platos problem: the latent semantic analysis theory of
acquisition, induction, and representation of knowledge. Psychological Review 104(2), 211?240.
Lund, K. and C. Burgess (1996). Producing high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, Instruments, and Computers 28, 203?208.
MacCartney, B. and C. D. Manning (2009). An extended model of natural logic. In Proceedings of the
Eighth International Conference on Computational Semantics (IWCS-8), pp. 140?156.
Miller, G. A. (2009). Wordnet - about us. http://wordnet.princeton.edu.
Mitchell, J. and M. Lapata (2008). Vector-based models of semantic composition. In Proceedings of
ACL, pp. 236?244.
Nairn, R., C. Condoravdi, and L. Karttunen (2006). Computing relative polarity for textual inference. In
Proceedings of Inference in Computational Semantics (ICoS-5), Buxton, UK.
Poon, H. and P. Domingos (2009). Unsupervised semantic parsing. In Proceedings of EMNLP 2009, pp.
1?10.
Richardson, M. and P. Domingos (2006). Markov logic networks. Machine Learning 62, 107?136.
Thater, S., H. Fu?rstenau, and M. Pinkal (2010). Contextualizing semantic representations using syntac-
tically enriched vector models. In Proceedings of ACL 2010, Uppsala, Sweden, pp. 948?957.
114
Proceedings of the Eighteenth Conference on Computational Language Learning, pages 141?150,
Baltimore, Maryland USA, June 26-27 2014.
c
?2014 Association for Computational Linguistics
Weakly-Supervised Bayesian Learning of a CCG Supertagger
Dan Garrette
?
Chris Dyer
?
Jason Baldridge
?
Noah A. Smith
?
?
Department of Computer Science, The University of Texas at Austin
?
School of Computer Science, Carnegie Mellon University
?
Department of Linguistics, The University of Texas at Austin
?
Corresponding author: dhg@cs.utexas.edu
Abstract
We present a Bayesian formulation for
weakly-supervised learning of a Combina-
tory Categorial Grammar (CCG) supertag-
ger with an HMM. We assume supervi-
sion in the form of a tag dictionary, and
our prior encourages the use of cross-
linguistically common category structures
as well as transitions between tags that
can combine locally according to CCG?s
combinators. Our prior is theoretically ap-
pealing since it is motivated by language-
independent, universal properties of the
CCG formalism. Empirically, we show
that it yields substantial improvements
over previous work that used similar bi-
ases to initialize an EM-based learner. Ad-
ditional gains are obtained by further shap-
ing the prior with corpus-specific informa-
tion that is extracted automatically from
raw text and a tag dictionary.
1 Introduction
Unsupervised part-of-speech (POS) induction is a
classic problem in NLP. Many proposed solutions
are based on Hidden Markov models (HMMs), with
various improvements obtainable through: induc-
tive bias in the form of tag dictionaries (Kupiec,
1992; Merialdo, 1994), sparsity constraints (Lee
et al., 2010), careful initialization of parameters
(Goldberg et al., 2008), feature based represen-
tations (Berg-Kirkpatrick et al., 2010; Smith and
Eisner, 2005), and priors on model parameters
(Johnson, 2007; Goldwater and Griffiths, 2007;
Blunsom and Cohn, 2011, inter alia).
When tag dictionaries are available, a situa-
tion we will call type-supervision, POS induc-
tion from unlabeled corpora can be relatively suc-
cessful; however, as the number of possible tags
increases, performance drops (Ravi and Knight,
2009). In such cases, there are a large number
of possible labels for each token, so picking the
right one simply by chance is unlikely; the pa-
rameter space tends to be large; and devising good
initial parameters is difficult. Therefore, it is un-
surprising that the unsupervised (or even weakly-
supervised) learning of a Combinatory Categorial
Grammar (CCG) supertagger, which labels each
word with one of a large (possibly unbounded)
number of structured categories called supertags,
is a considerable challenge.
Despite the apparent complexity of the task, su-
pertag sequences have regularities due to univer-
sal properties of the CCG formalism (?2) that can
be used to reduce the complexity of the problem;
previous work showed promising results by using
these regularities to initialize an HMM that is then
refined with EM (Baldridge, 2008). Here, we ex-
ploit CCG?s category structure to motivate a novel
prior over HMM parameters for use in Bayesian
learning (?3). This prior encourages (i) cross-
linguistically common tag types, (ii) tag bigrams
that can combine using CCG?s combinators, and
(iii) sparse transition distributions. We also go be-
yond the use of these universals to show how ad-
ditional, corpus-specific information can be auto-
matically extracted from a combination of the tag
dictionary and raw data, and how that information
can be combined with the universal knowledge for
integration into the model to improve the prior.
We use a blocked sampling algorithm to sam-
ple supertag sequences for the sentences in the
training data, proportional to their posterior prob-
ability (?4). We experimentally verify that
our Bayesian formulation is effective and sub-
stantially outperforms the state-of-the-art base-
line initialization/EM strategy in several languages
(?5). We also evaluate using tag dictionaries that
are unpruned and have only partial word coverage,
finding even greater improvements in these more
realistic scenarios.
141
2 CCG and Supertagging
CCG (Steedman, 2000; Steedman and Baldridge,
2011) is a grammar formalism in which each lex-
ical token is associated with a structured category,
often referred to as a supertag. CCG categories are
defined by the following recursive definition:
C ? {S, N, NP, PP, ...}
C ? {C/C,C\C}
A CCG category can either be an atomic cate-
gory indicating a particular type of basic gram-
matical phrase (S for a sentence, N for a noun,
NP for a noun phrase, etc), or a complex category
formed from the combination of two categories
by one of two slash operators. In CCG, complex
categories indicate a grammatical relationship be-
tween the two operands. For example, the cate-
gory (S\NP)/NP might describe a transitive verb,
looking first to its right (indicated by /) for an ob-
ject, then to its left (\) for a subject, to produce a
sentence. Further, atomic categories may be aug-
mented with features, such as S
dcl
, to restrict the
set of atoms with which they may unify. The task
of assigning a category to each word in a text is
called supertagging (Bangalore and Joshi, 1999).
Because they are recursively defined, there is
an infinite number of potential CCG categories
(though in practice it is limited by the number
of actual grammatical contexts). As a result, the
number of supertags appearing in a corpus far ex-
ceeds the number of POS tags (see Table 1). Since
supertags specify the grammatical context of a to-
ken, and high frequency words appear in many
contexts, CCG grammars tend to have very high
lexical ambiguity, with frequent word types asso-
ciating with a large number of categories. This
ambiguity has made type-supervised supertagger
learning very difficult because the typical ap-
proaches to initializing parameters for EM become
much less effective.
Grammar-informed supertagger learning.
Baldridge (2008) was successful in extending the
standard type-supervised tagger learning to the
task of CCG supertagging by setting the initial
parameters for EM training of an HMM using
two intrinsic properties of the CCG formalism:
the tendency for adjacent tags to combine, and
the tendency to use less complex tags. These
properties are explained in detail in the original
work, but we restate the ideas briefly throughout
this paper for completeness.
X/Y Y ? X (>)
Y X\Y ? X (<)
X/Y Y/Z ? X/Z (>B)
Y \Z X\Y ? X\Z (<B)
Y/Z X\Y ? X/Z (<B
?
)
Figure 1: Combination rules used by CCGBank.
S
NP
NP/N
N
S\NP
(S\NP)/NP
NP
NP/N
N
The
man
walks
a
dog
Figure 2: CCG parse for ?The man walks a dog.?
Tag combinability. A CCG parse of a sentence is
derived by recursively combining the categories of
sub-phrases. Category combination is performed
using only a small set of generic rules (see Fig-
ure 1). In the tree in Figure 2, we can see that
a and dog can combine via Forward Application
(>), with NP/N and N combining to produce NP.
The associativity engendered by CCG?s compo-
sition rules means that most adjacent lexical cate-
gories may be combined. In the Figure 2 tree, we
can see that instead of combining (walks?(a?dog)),
we could have combined ((walks?a)?dog) since
(S\NP)/NP and NP/N can combine using >B.
3 Model
In this section we define the generative process
we use to model a corpus of sentences. We begin
by generating the model parameters: for each
supertag type t in the tag set T , the transition
probabilities to the next state (pi
t
) and the emis-
sion probabilities (?
t
) are generated by draws
from Dirichlet distributions parameterized with
per-tag mean distributions (pi
0
t
and ?
0
t
, respec-
tively) and concentration parameters (?
pi
and
?
?
). By setting ?
pi
close to zero, we can encode
our prior expectation that transition distributions
should be relatively peaked (i.e., that each tag
type should be followed by relatively few tag
types). The prior means, discussed below, encode
both linguistic intuitions about expected tag-tag
transition behavior and automatically-extracted
corpus information. Given these parameters, we
next generate the sentences of the corpus. This
process is summarized as follows:
142
Parameters:
?
t
? Dirichlet(?
?
, ?
0
t
) ?t ? T
pi
t
? Dirichlet(?
pi
, pi
0
t
) ?t ? T
Sentence:
y
1
? Categorical(pi
?S?
)
for i ? {1, 2, . . .}, until y
i
= ?E?
x
i
| y
i
? Categorical(?
y
i
)
y
i+1
| y
i
? Categorical(pi
y
i
)
This model can be understood as a Bayesian
HMM (Goldwater and Griffiths, 2007). We next
discuss how the prior distributions are constructed
to build in additional inductive bias.
3.1 Transition Prior Means (pi
0
t
)
We use the prior mean for each tag?s transition dis-
tribution to build in two kinds of bias. First, we
want to favor linguistically probable tags. Second,
we want to favor transitions that result in a tag
pair that combines according to CCG?s combina-
tors. For simplicity, we will define pi
0
t
as a mixture
of two components, the first, P
pi
(u) is an (uncon-
ditional) distribution over category types u that fa-
vors cross-linguistically probable categories. The
second component, P
pi
(u | t), conditions on the
previous tag type, t, and assigns higher probabil-
ity to pairs of tags that can be combined. That is,
the probability of transitioning from t to u in the
Dirichlet mean distribution is given by
1
pi
0
t
(u) = ? ? P
pi
(u) + (1? ?) ? P
pi
(u | t).
We discuss the two mixture components in turn.
3.1.1 Unigram Category Generator (P
pi
(u))
In this section, we define a CCG category gener-
ator that generates cross-linguistically likely cat-
egory types. Baldridge?s approach estimated the
likelihood of a category using the inverse number
of sub-categories: P
CPLX
(u) ? 1/complexity(u).
We propose an improvement, P
G
, expressed as a
probabilistic grammar:
2
C ? a p
term
?p
atom
(a)
C ? A/A p
term
?p
fw
?p
mod
?P
G
(A)
C ? A/B, A 6=B p
term
?p
fw
?p
mod
?P
G
(A) ?P
G
(B)
C ? A\A p
term
?p
fw
?p
mod
?P
G
(A)
C ? A\B, A 6=B p
term
?p
fw
?p
mod
?P
G
(A) ?P
G
(B)
1
Following Baldridge (2008), we fix ? = 0.5 for our ex-
periments.
2
For readability, we use the notation p = (1? p).
where A,B,C are categories and a is an atomic
category (and terminal): a ? {S, N, NP, ...}.
3
We have designed this grammar to capture sev-
eral important CCG characteristics. In particular
we encode four main ideas, each captured through
a different parameter of the grammar and dis-
cussed in greater detail below:
1. Simpler categories are more likely: e.g. N/N is
a priori more likely than (N/N)/(N/N).
2. Some atoms are more likely than others: e.g.
NP is more likely than S, much more than NP
nb
.
3. Modifiers are more likely: e.g. (S\NP)/(S\NP)
is more likely than (S\NP)/(NP\NP).
4. Operators occur with different frequencies.
The first idea subsumes the complexity measure
used by Baldridge, but accomplishes the goal nat-
urally by letting the probabilities decrease as the
category grows. The rate of decay is governed
by the p
term
parameter: the marginal probability
of generating a terminal (atomic) category in each
expansion. A higher p
term
means a stronger em-
phasis on simplicity. The probability distribution
over categories is guaranteed to be proper so long
as p
term
>
1
2
since the probability of the depth of a
tree will decrease geometrically (Chi, 1999).
The second idea is a natural extension of the
complexity concept and is particularly relevant
when features are used. The original complex-
ity measure treated all atoms uniformly, but e.g.
we would expect NP
expl
/N to be less likely than
NP/N since it contains the more specialized, and
thus rarer, atom NP
expl
. We define the distribution
p
atom
(a) as the prior over atomic categories.
Due to our weak, type-only supervision, we
have to estimate p
atom
from just the tag dictionary
and raw corpus, without frequency data. Our goal
is to estimate the number of each atom in the su-
pertags that should appear on the raw corpus to-
kens. Since we don?t know what the correct su-
pertags are, we first estimate counts of supertags,
from which we can extract estimated atom counts.
Our strategy is to uniformly distribute each raw
corpus token?s counts over all of its possible su-
pertags, as specified in the tag dictionary. Word
types not appearing in the tag dictionary are ig-
3
While very similar to standard probabilistic context-free
grammars seen in NLP work, this grammar is not context-free
because modifier categories must have matching operands.
However, this is not a problem for our approach since the
grammar is unambiguous, defines a proper probability distri-
bution, and is only used for modeling the relative likelihoods
of categories (not parsing categories).
143
nored for the purposes of these estimates. Assum-
ing that C(w) is the number of times that word
type w is seen in the raw corpus, atoms(a, t) is the
number of times atom a appears in t, TD(w) is the
set of tags associated with w, and TD(t) is the set
of word types associated with t:
C
supertag
(t) =
?
w?TD(t)
(C(w)+?)/|TD(w)|
C
atom
(a) =
?
t?T
atoms(a, t) ? C
supertag
(t)
p
atom
(a) ? C
atom
(a) + ?
Adding ? smooths the estimates.
Using the raw corpus and tag dictionary data to
set p
atom
allows us to move beyond Baldridge?s
work in another direction: it provides us with a
natural way to combine CCG?s universal assump-
tions with corpus-specific data.
The third and fourth ideas pertain only to com-
plex categories. If the category is complex, then
we consider two additional parameters. The pa-
rameter p
fw
is the marginal probability that the
complex category?s operator specifies a forward
argument. The parameter p
mod
gives the amount
of marginal probability mass that is allocated for
modifier categories. Note that it is not necessary
for p
mod
to be greater than
1
2
to achieve the de-
sired result of making modifier categories more
likely than non-modifier categories: the number
of potential modifiers make up only a tiny fraction
of the space of possible categories, so allocating
more than that mass as p
mod
will result in a cate-
gory grammar that gives disproportionate weight
to modifiers, increasing the likelihood of any par-
ticular modifier from what it would otherwise be.
3.1.2 Bigram Category Generator (P
pi
(u | t))
While the above processes encode important prop-
erties of the distribution over categories, the in-
ternal structure of categories is not the full story:
cross-linguistically, the categories of adjacent to-
kens are much more likely to be combinable via
some CCG rule. This is the second component of
our mixture model.
Baldridge derives this bias by allocating the ma-
jority of the transition probability mass from each
tag t to tags that can follow t according to some
combination rule. Let ?(t,u) be an indicator of
whether t connects to u; for ? ? [0, 1]:
4
P
?
(u | t) =
{
? ? uniform(u) if ?(t,u)
(1? ?) ? uniform(u) otherwise
4
Again, following Baldridge (2008), we fix ? = 0.95 for
our experiments.
There are a few additional considerations that
must be made in defining ?, however. In assum-
ing the special tags ?S? and ?E? for the start and
end of the sentence, respectively, we can define
?(?S?,u) = 1 when u seeks no left-side argu-
ments (since there are no tags to the left with
which to combine) and ?(t, ?E?) = 1 when t seeks
no right-side arguments. So ?(?S?, NP/N) = 1, but
?(?S?, S\NP) = 0. If atoms have features asso-
ciated, then the atoms are allowed to unify if the
features match, or if at least one of them does
not have a feature. So ?(NP
nb
, S\NP) = 1, but
?(NP
nb
, S\NP
conj
) = 0. In defining ?, it is also im-
portant to ignore possible arguments on the wrong
side of the combination since they can be con-
sumed without affecting the connection between
the two. To achieve this for ?(t,u), it is assumed
that it is possible to consume all preceding argu-
ments of t and all following arguments of u. So
?(NP, (S\NP)/NP) = 1. This helps to ensure the
associativity discussed earlier. Finally, the atom
NP is allowed to unify with N if N is the argument.
So ?(N, S\NP) = 1, but ?(NP/N, NP) = 0. This is
due to the fact that CCGBank assumes that N can
be rewritten as NP.
Type-supervised initialization. As above, we
want to improve upon Baldridge?s ideas by en-
coding not just universal CCG knowledge, but
also automatically-induced corpus-specific infor-
mation where possible. To that end, we can de-
fine a conditional distribution P
tr
(u | t) based on
statistics from the raw corpus and tag dictionary.
We use the same approach as we did above for set-
ting p
atom
(and the definition of ?
0
t
below): we esti-
mate by evenly distributing raw corpus counts over
the tag dictionary entries. Assume that C(w
1
, w
2
)
is the (?-smoothed) count of times word type w
1
was directly followed byw
2
in the raw corpus, and
ignoring any words not found in the tag dictionary:
C(t,u) = ?+
?
w
1
?TD(t), w
2
?TD(u)
C(w
1
, w
2
)
|TD(w
1
)| ? |TD(w
2
)|
P
tr
(u | t) = C(t,u)/
?
u
?
C(t,u
?
)
Then the alternative definition of the compatibility
distribution is as follows:
P
tr
?
(u | t) =
{
? ? P
tr
(u | t) if ?(t,u)
(1??) ? P
tr
(u | t) otherwise
144
Our experiments compare performance when
pi
0
t
is set using P
pi
(u)=P
CPLX
(experiment 3) ver-
sus our category grammar P
G
(4?6), and using
P
pi
(u | t) = P
?
as the compatibility distribution
(3?4) versus P
tr
?
(5?6).
3.2 Emission Prior Means (?
0
t
)
For each supertag type t, ?
0
t
is the mean distri-
bution over words it emits. While Baldridge?s
approach used a uniform emission initialization,
treating all words as equally likely, we can,
again, induce token-level corpus-specific informa-
tion:
5
To set ?
0
t
, we use a variant and simplifica-
tion of the procedure introduced by Garrette and
Baldridge (2012) that takes advantage of our prior
over categories P
G
.
Assuming that C(w) is the count of word type
w in the raw corpus, TD(w) is the set of supertags
associated with word type w in the tag dictionary,
and TD(t) is the set of known word types associ-
ated with supertag t, the count of word/tag pairs
for known words (words appearing in the tag dic-
tionary) is estimated by uniformly distributing a
word?s (?-smoothed) raw counts over its tag dic-
tionary entries:
C
known
(t, w) =
{
C(w)+?
|TD(w)|
if t ? TD(w)
0 otherwise
For unknown words, we first use the idea of tag
?openness? to estimate the likelihood of a partic-
ular tag t applying to an unknown word: if a tag
applies to many word types, it is likely to apply to
some new word type.
P (unk | t) ? |known words w s.t. t ? TD(w)|
Then, we apply Bayes? rule to get P (t | unk), and
use that to estimate word/tag counts for unknown
words:
P (t | unk) ? P (unk | t) ? P
G
(t)
C
unk
(t, w) = C(w) ? P (t | unk)
Thus, with the estimated counts for all words:
P
em
(w | t) =
C
known
(t, w) + C
unk
(t, w)
?
w
?
C
known
(t, w
?
) + C
unk
(t, w
?
)
We perform experiments comparing perfor-
mance when ?
0
t
is uniform (3?5) and when
?
0
t
(w) = P
em
(w | t) (6).
5
Again, without gold tag frequencies.
4 Posterior Inference
We wish to find the most likely supertag of each
word, given the model we just described and a cor-
pus of training data. Since there is exact inference
with these models is intractable, we resort to Gibbs
sampling to find an approximate solution. At a
high level, we alternate between resampling model
parameters (?
t
, pi
t
) given the current tag sequence
and resampling tag sequences given the current
model parameters and observed word sequences.
It is possible to sample a new tagging from the
posterior distribution over tag sequences for a sen-
tence, given the sentence and the HMM parameters
using the forward-filter backward-sample (FFBS)
algorithm (Carter and Kohn, 1996). To effi-
ciently sample new HMM parameters, we exploit
Dirichlet-multinomial conjugacy. By repeating
these alternating steps and accumulating the num-
ber of times each supertag is used in each position,
we obtain an approximation of the required poste-
rior quantities.
Our inference procedure takes as input the tran-
sition prior means pi
0
t
, the emission prior means
?
0
t
, and concentration parameters ?
pi
and ?
?
,
along with the raw corpus and tag dictionary. The
set of supertags associated with a word w will be
known as TD(w). We will refer to the set of word
types included in the tag dictionary as ?known?
words and others as ?unknown? words. For sim-
plicity, we will assume that TD(w), for any un-
known word w, is the full set of CCG categories.
During sampling, we always restrict the possible
tag choices for a word w to the categories found in
TD(w). We refer to the sequence of word tokens
as x and tags as y.
We initialize the sampler by setting pi
t
= pi
0
t
and ?
t
= ?
0
t
and then sampling tagging sequences
using FFBS.
To sample a tagging for a sentence x, the strat-
egy is to inductively compute, for each token x
i
starting with i = 0 and going ?forward?, the prob-
ability of generating x
0
, x
1
, . . . , x
i
via any tag se-
quence that ends with y
i
= u:
p(y
i
= u | x
0:i
) =
?
u
(x
i
) ?
?
t?T
pi
t
(u) ? p(y
i?1
= t | x
0:i?1
)
We then pass through the sequence again, this time
?backward? starting at i = |x| ? 1 and sampling
y
i
| y
i+1
? p(y
i
= t | x
0:i
) ? pi
t
(y
i+1
).
145
num. raw TD TD ambiguity dev test
Corpus tags tokens tokens entries type token tokens tokens
English
CCGBank POS 50
158k 735k
45k 3.75 13.11 ? ?
CCGBank 1,171 65k 56.98 296.18 128k 127k
Chinese CTB-CCG 829 99k 439k 60k 96.58 323.37 59k 85k
Italian CCG-TUT 955 6k 27k 9k 178.88 426.13 5k 5k
Table 1: Statistics for the various corpora used. CCGBank is English, CCG-CTB is Chinese, and TUT
is Italian. The number of tags includes only those tags found in the tag dictionary (TD). Ambiguity rates
are the average number of entries in the unpruned tag dictionary for each word in the raw corpus. English
POS statistics are shown only for comparison; only CCG experiments were run.
The block-sampling approach of choosing new
tags for a sentence all at once is particularly ben-
eficial given the sequential nature of the model of
the HMM. In an HMM, a token?s adjacent tags tend
to hold onto its current tag due to the relation-
ships between the three. Resampling all tags at
once allows for more drastic changes at each it-
eration, providing better opportunities for mixing
during inference. The FFBS approach has the ad-
ditional advantage that, by resampling the distri-
butions only once per iteration, we are able to re-
sample all sentences in parallel. This is not strictly
true of all HMM problems with FFBS, but because
our data is divided by sentence, and each sentence
has a known start and end tag, the tags chosen dur-
ing the sampling of one sentence cannot affect the
sampling of another sentence in the same iteration.
Once we have sampled tags for the entire cor-
pus, we resample pi and ?. The newly-sampled
tags y are used to compute C(w, t), the count of
tokens with word type w and tag t, and C(t,u),
the number of times tag t is directly followed by
tag u. We then sample, for each t ? T where T is
the full set of valid CCG categories:
pi
t
? Dir
(
??
pi
? pi
0
t
(u) + C(t,u)?
u?T
)
?
t
? Dir
(
??
?
? ?
0
t
(w) + C(w, t)?
w?V
)
It is important to note that this method of re-
sampling allows the draws to incorporate both the
data, in the form of counts, and the prior mean,
which includes all of our carefully-constructed bi-
ases derived from both the intrinsic, universal CCG
properties as well as the information we induced
from the raw corpus and tag dictionary.
With the distributions resampled, we can con-
tinue the procedure by resampling tags as above,
and then resampling distributions again, until a
maximum number of iterations is reached.
5 Experiments
6
To evaluate our approach, we used CCGBank
(Hockenmaier and Steedman, 2007), which is
a transformation of the English Penn Treebank
(Marcus et al., 1993); the CTB-CCG (Tse and
Curran, 2010) transformation of the Penn Chinese
Treebank (Xue et al., 2005); and the CCG-TUT
corpus (Bos et al., 2009), built from the TUT cor-
pus of Italian text (Bosco et al., 2000). Statistics
on the size and ambiguity of these datasets are
shown in Table 1.
For CCGBank, sections 00?15 were used for
extracting the tag dictionary, 16?18 for the raw
corpus, 19?21 for development data, and 22?24
for test data. For TUT, the first 150 sentences of
each of the CIVIL LAW and NEWSPAPER sections
were used for raw data, the next sentences 150?
249 of each was used for development, and the
sentences 250?349 were used for test; the remain-
ing data, 457 sentences from CIVIL LAW and 548
from NEWSPAPER, plus the much smaller 132-
sentence JRC ACQUIS data, was used for the tag
dictionary. For CTB-CCG, sections 00?11 were
used for the tag dictionary, 20?24 for raw, 25?27
for dev, and 28?31 for test.
Because we are interested in showing the rel-
ative gains that our ideas provide over Baldridge
(2008), we reimplemented the initialization pro-
cedure from that paper, allowing us to evaluate
all approaches consistently. For each dataset, we
ran a series of experiments in which we made fur-
ther changes from the original work. We first ran
a baseline experiment with uniform transition and
emission initialization of EM (indicated as ?1.? in
Table 2) followed by our reimplementation of the
initialization procedure by Baldridge (2). We then
6
All code and experimental scripts are available
at http://www.github.com/dhgarrette/
2014-ccg-supertagging
146
Corpus English Chinese Italian
TD cutoff 0.1 0.01 0.001 no 0.1 0.01 0.001 no 0.1 0.01 0.001 no
1. uniform EM 77 62 47 38 64 39 30 26 51 32 30 30
2. init (Baldridge) EM 78 67 55 41 66 43 33 28 54 36 33 32
3. init Bayes 74 68 56 42 65 56 47 37 52 46 40 40
4. P
G
Bayes 74 70 59 42 64 57 47 36 52 40 39 40
5. P
G
, P
tr
?
Bayes 75 72 61 50 66 58 49 44 52 44 41 43
6. P
G
, P
tr
?
, P
em
Bayes 80 80 73 51 69 62 56 49 53 47 45 46
Table 2: Experimental results: test-set per-token supertag accuracies. ?TD cutoff? indicates the level of
tag dictionary pruning; see text. (1) is uniform EM initialization. (2) is a reimplementation of (Baldridge,
2008). (3) is Bayesian formulation using only the ideas from Baldridge: P
CPLX
, P
?
, and uniform emis-
sions. (4?6) are our enhancements to the prior: using our category grammar in P
G
instead of P
CPLX
, using
P
tr
?
instead of P
?
, and using P
em
instead of uniform.
experimented with the Bayesian formulation, first
using the same information used by Baldridge, and
then adding our enhancements: using our category
grammar in P
G
, using P
tr
?
as the transition com-
patability distribution, and using P
em
as ?
0
t
(w).
For each dataset, we ran experiments using four
different levels of tag dictionary pruning. Prun-
ing is the process of artificially removing noise
from the tag dictionary by using token-level anno-
tation counts to discard low-probability tags; for
each word, for cutoff x, any tag with probability
less than x is excluded. Tag dictionary pruning
is a standard procedure in type-supervised train-
ing, but because it requires information that does
not truly conform to the type-supervised scenario,
we felt that it was critical to demonstrate the per-
formance of our approach under situations of less
pruning, including no artificial pruning at all.
We emphasize that unlike in most previous
work, we use incomplete tag dictionaries. Most
previous work makes the unrealistic assumption
that the tag dictionary contains an entry for ev-
ery word that appears in either the training or test-
ing data. This is a poor approximation of a real
tagging system, which will never have complete
lexical knowledge about the test data. Even work
that only assumes complete knowledge of the tag-
ging possibilities for the lexical items in the train-
ing corpus is problematic (Baldridge, 2008; Ravi
et al., 2010). This still makes learning unrealisti-
cally easy since it dramatically reduces the ambi-
guity of words that would have been unseen, and,
in the case of CCG, introduces additional tags that
would not have otherwise been known. To ensure
that our experiments are more realistic, we draw
our tag dictionary entries from data that is totally
disjoint from both the raw and test corpora. Dur-
ing learning, any unknown words (words not ap-
pearing in the tag dictionary) are unconstrained so
that they may take any tag, and are, thus, maxi-
mally ambiguous.
We only performed minimal parameter tuning,
choosing instead to stay consistent with Baldridge
(2008) and simply pick reasonable-seeming val-
ues for any additional parameters. Any tuning that
was performed was done with simple hill-climbing
on the development data of English CCGBank.
All parameters were held consistent across exper-
iments, including across languages. For EM, we
used 50 iterations; for FFBS we used 100 burn-
in iterations and 200 sampling iterations.
7
For
all experiments, we used ? = 0.95 for P
(tr)
?
and
? = 0.5 for pi
0
t
to be consistent with previous
work, ?
pi
= 3000, ?
?
= 7000, p
term
= 0.6,
p
fw
= 0.5, p
mod
= 0.8, and ? = 1000 for p
atom
.
Test data was run only once, for the final figures.
The final results reported were achieved by us-
ing the following training sequence: initialize pa-
rameters according to the scenario, train an HMM
using EM or FFBS starting with that set of parame-
ters, tag the raw corpus with the trained HMM, add-
0.1 smooth counts from the now-tagged raw cor-
pus, and train a maximum entropy Markov model
(MEMM) from this ?auto-supervised? data.
8
Results are shown in Table 2. Most notably, the
contributions described in this paper improve re-
sults in nearly every experimental scenario. We
can see immediate, often sizable, gains in most
7
Final counts are averaged across the sampling iterations.
8
Auto-supervised training of an MEMM increases accu-
racy by 1?3% on average (Garrette and Baldridge, 2013). We
use the OpenNLP MEMM implementation with its standard
set of features: http://opennlp.apache.org
147
cases simply by using the Bayesian formulation.
Further gains are seen from adding each of the
other various contributions of this paper. Perhaps
most interestingly, the gains are only minimal with
maximum pruning, but the gains increase as the
pruning becomes less aggressive ? as the scenar-
ios become more realistic. This indicates that our
improvements make the overall procedure more
robust.
Error Analysis Like POS-taggers, the learned
supertagger frequently confuses nouns (N) and
their modifiers (N/N), but the most frequent er-
ror made by the English (6) experiment was
(((S\NP)\(S\NP))/N) instead of (NP
nb
/N). How-
ever, these are both determiner types, indicating an
interesting problem for the supertagger: it often
predicts an object type-raised determiner instead
of the vanilla NP/N, but in many contexts, both cat-
egories are equally valid. (In fact, for parsers that
use type-raising as a rule, this distinction in lexical
categories does not exist.)
6 Related Work
Ravi et al. (2010) also improved upon the work by
Baldridge (2008) by using integer linear program-
ming to find a minimal model of supertag transi-
tions, thereby generating a better starting point for
EM than the grammatical constraints alone could
provide. This approach is complementary to the
work presented here, and because we have shown
that our work yields gains under tag dictionaries
of various levels of cleanliness, it is probable that
employing minimization to set the base distribu-
tion for sampling could lead to still higher gains.
On the Bayesian side, Van Gael et al. (2009)
used a non-parametric, infinite HMM for truly un-
supervised POS-tagger learning (Van Gael et al.,
2008; Beal et al., 2001). While their model is not
restricted to the standard set of POS tags, and may
learn a more fine-grained set of labels, the induced
labels are arbitrary and not grounded in any gram-
matical formalism.
Bisk and Hockenmaier (2013) developed an ap-
proach to CCG grammar induction that does not
use a tag dictionary. Like ours, their procedure
learns from general properties of the CCG formal-
ism. However, while our work is intended to pro-
duce categories that match those used in a partic-
ular training corpus, however complex they might
be, their work produces categories in a simplified
form of CCG in which N and S are the only atoms
and no atoms have features. Additionally, they as-
sume that their training corpus is annotated with
POS tags, whereas we assume truly raw text.
Finally, we find the task of weakly-supervised
supertagger learning to be particularly relevant
given the recent surge in popularity of CCG.
An array of NLP applications have begun using
CCG, including semantic parsing (Zettlemoyer and
Collins, 2005) and machine translation (Weese et
al., 2012). As CCG finds more applications, and
as these applications move to lower-resource do-
mains and languages, there will be increased need
for the ability to learn without full supervision.
7 Conclusion and Future Work
Standard strategies for type-supervised HMM es-
timation are less effective as the number of cat-
egories increases. In contrast to POS tag sets,
CCG supertags, while quite numerous, have struc-
tural clues that can simplify the learning prob-
lem. Baldridge (2008) used this formalism-
specific structure to inform an initialization pro-
cedure for EM. In this work, we have shown that
CCG structure can instead be used to motivate an
effective prior distribution over the parameters of
an HMM supertagging model, allowing our work
to outperform Baldridge?s previously state-of-the-
art approach, and to do so in a principled manner
that lends itself better to future extensions such as
incorporation in more complex models.
This work also improves on Baldridge?s simple
?complexity? measure, developing instead a prob-
abilistic category grammar over supertags that al-
lows our prior to capture a wider variety of inter-
esting and useful properties of the CCG formalism.
Finally, we were able to achieve further gains
by augmenting the universal CCG knowledge with
corpus-specific information that could be automat-
ically extracted from the weak supervision that is
available: the raw corpus and the tag dictionary.
This allows us to combine the cross-linguistic
properties of the CCG formalism with corpus- or
language-specific information in the data into a
single, unified Bayesian prior.
Our model uses a relatively large number of pa-
rameters, e.g., p
term
, p
fw
, p
mod
, p
atom
, in the prior.
Here, we fixed each to a single value (i.e., a ?fully
Bayesian? approach). Future work might explore
sensitivity to these choices, or empirical Bayesian
or maximum a posteriori inference for their values
(Johnson and Goldwater, 2009).
148
In this work, as in most type-supervised work,
the tag dictionary was automatically extracted
from an existing tagged corpus. However, a tag
dictionary could instead be automatically induced
via multi-lingual transfer (Das and Petrov, 2011)
or generalized from human-provided information
(Garrette and Baldridge, 2013; Garrette et al.,
2013). Again, since the approach presented here
has been shown to be somewhat robust to tag dic-
tionary noise, it is likely that the model would
perform well even when using an automatically-
induced tag dictionary.
Acknowledgements
This work was supported by the U.S. Department
of Defense through the U.S. Army Research Of-
fice (grant number W911NF-10-1-0533). Exper-
iments were run on the UTCS Mastodon Cluster,
provided by NSF grant EIA-0303609.
References
Jason Baldridge. 2008. Weakly supervised supertag-
ging with grammar-informed initialization. In Pro-
ceedings of COLING.
Srinivas Bangalore and Aravind K. Joshi. 1999. Su-
pertagging: An approach to almost parsing. Com-
putational Linguistics, 25(2).
Matthew J. Beal, Zoubin Ghahramani, and Carl Ed-
ward Rasmussen. 2001. The innite hidden Markov
model. In NIPS.
Taylor Berg-Kirkpatrick, Alexandre Bouchard-C?ot?e,
John DeNero, and Dan Klein. 2010. Painless un-
supervised learning with features. In Proceedings of
NAACL.
Yonatan Bisk and Julia Hockenmaier. 2013. An HDP
model for inducing combinatory categorial gram-
mars. Transactions of the Association for Compu-
tational Linguistics, 1.
Phil Blunsom and Trevor Cohn. 2011. A hierarchical
Pitman-Yor process HMM for unsupervised part of
speech induction. In Proceedings of ACL.
Johan Bos, Cristina Bosco, and Alessandro Mazzei.
2009. Converting a dependency treebank to a cat-
egorial grammar treebank for Italian. In M. Pas-
sarotti, Adam Przepi?orkowski, S. Raynaud, and
Frank Van Eynde, editors, Proceedings of the Eighth
International Workshop on Treebanks and Linguistic
Theories (TLT8).
Cristina Bosco, Vincenzo Lombardo, Daniela Vassallo,
and Leonardo Lesmo. 2000. Building a treebank
for Italian: a data-driven annotation schema. In Pro-
ceedings of LREC.
Christopher K. Carter and Robert Kohn. 1996. On
Gibbs sampling for state space models. Biometrika,
81(3):341?553.
Zhiyi Chi. 1999. Statistical properties of probabilistic
context-free grammars. Computational Linguistics,
25(1).
Dipanjan Das and Slav Petrov. 2011. Unsupervised
part-of-speech tagging with bilingual graph-based
projections. In Proceedings of ACL-HLT.
Dan Garrette and Jason Baldridge. 2012. Type-
supervised hidden Markov models for part-of-
speech tagging with incomplete tag dictionaries. In
Proceedings of EMNLP.
Dan Garrette and Jason Baldridge. 2013. Learning a
part-of-speech tagger from two hours of annotation.
In Proceedings of NAACL.
Dan Garrette, Jason Mielens, and Jason Baldridge.
2013. Real-world semi-supervised learning of POS-
taggers for low-resource languages. In Proceedings
of ACL.
Yoav Goldberg, Meni Adler, and Michael Elhadad.
2008. EM can find pretty good HMM POS-taggers
(when given a good start). In Proceedings of ACL.
Sharon Goldwater and Thomas L. Griffiths. 2007.
A fully Bayesian approach to unsupervised part-of-
speech tagging. In Proceedings of ACL.
Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: A corpus of CCG derivations and dependency
structures extracted from the Penn Treebank. Com-
putational Linguistics, 33(3).
Mark Johnson and Sharon Goldwater. 2009. Im-
proving nonparameteric Bayesian inference: Ex-
periments on unsupervised word segmentation with
adaptor grammars. In Proceedings of NAACL.
Mark Johnson. 2007. Why doesn?t EM find good
HMM POS-taggers? In Proceedings of EMNLP-
CoNLL.
Julian Kupiec. 1992. Robust part-of-speech tagging
using a hidden Markov model. Computer Speech &
Language, 6(3).
Yoong Keok Lee, Aria Haghighi, and Regina Barzilay.
2010. Simple type-level unsupervised pos tagging.
In Proceedings of EMNLP.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2).
Bernard Merialdo. 1994. Tagging English text with
a probabilistic model. Computational Linguistics,
20(2).
149
Sujith Ravi and Kevin Knight. 2009. Minimized mod-
els for unsupervised part-of-speech tagging. In Pro-
ceedings of ACL-AFNLP.
Sujith Ravi, Jason Baldridge, and Kevin Knight. 2010.
Minimized models and grammar-informed initial-
ization for supertagging with highly ambiguous lex-
icons. In Proceedings of ACL, pages 495?503.
Noah A. Smith and Jason Eisner. 2005. Contrastive
estimation: Training log-linear models on unlabeled
data. In Proceedings of ACL.
Mark Steedman and Jason Baldridge. 2011. Combina-
tory categorial grammar. In Robert Borsley and Ker-
sti Borjars, editors, Non-Transformational Syntax:
Formal and Explicit Models of Grammar. Wiley-
Blackwell.
Mark Steedman. 2000. The Syntactic Process. MIT
Press.
Daniel Tse and James R. Curran. 2010. Chinese CCG-
bank: Extracting CCG derivations from the Penn
Chinese treebank. In Proceedings of COLING.
Jurgen Van Gael, Yunus Saatci, Yee Whye Teh, and
Zoubin Ghahramani. 2008. Beam sampling for the
infinite hidden Markov model. In Proceedings of
ICML.
Jurgen Van Gael, Andreas Vlachos, and Zoubin
Ghahramani. 2009. The infinite HMM for unsu-
pervised PoS tagging. In Proceedings of EMNLP.
Jonathan Weese, Chris Callison-Burch, and Adam
Lopez. 2012. Using categorial grammar to label
translation rules. In Proceedings of WMT.
Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The Penn Chinese TreeBank: Phrase
structure annotation of a large corpus. Natural Lan-
guage Engineering, 11(2):207?238.
Luke S. Zettlemoyer and Michael Collins. 2005.
Learning to map sentences to logical form: Struc-
tured classification with probabilistic categorial
grammars. In Proceedings of UAI.
150
