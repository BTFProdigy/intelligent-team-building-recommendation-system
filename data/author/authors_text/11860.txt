Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 852?860,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
SMS based Interface for FAQ Retrieval
Govind Kothari
IBM India Research Lab
gokothar@in.ibm.com
Sumit Negi
IBM India Research Lab
sumitneg@in.ibm.com
Tanveer A. Faruquie
IBM India Research Lab
ftanveer@in.ibm.com
Venkatesan T. Chakaravarthy
IBM India Research Lab
vechakra@in.ibm.com
L. Venkata Subramaniam
IBM India Research Lab
lvsubram@in.ibm.com
Abstract
Short Messaging Service (SMS) is popu-
larly used to provide information access to
people on the move. This has resulted in
the growth of SMS based Question An-
swering (QA) services. However auto-
matically handling SMS questions poses
significant challenges due to the inherent
noise in SMS questions. In this work we
present an automatic FAQ-based question
answering system for SMS users. We han-
dle the noise in a SMS query by formu-
lating the query similarity over FAQ ques-
tions as a combinatorial search problem.
The search space consists of combinations
of all possible dictionary variations of to-
kens in the noisy query. We present an ef-
ficient search algorithm that does not re-
quire any training data or SMS normaliza-
tion and can handle semantic variations in
question formulation. We demonstrate the
effectiveness of our approach on two real-
life datasets.
1 Introduction
The number of mobile users is growing at an
amazing rate. In India alone a few million sub-
scribers are added each month with the total sub-
scriber base now crossing 370 million. The any-
time anywhere access provided by mobile net-
works and portability of handsets coupled with the
strong human urge to quickly find answers has fu-
eled the growth of information based services on
mobile devices. These services can be simple ad-
vertisements, polls, alerts or complex applications
such as browsing, search and e-commerce. The
latest mobile devices come equipped with high
resolution screen space, inbuilt web browsers and
full message keypads, however a majority of the
users still use cheaper models that have limited
screen space and basic keypad. On such devices,
SMS is the only mode of text communication.
This has encouraged service providers to build in-
formation based services around SMS technology.
Today, a majority of SMS based information ser-
vices require users to type specific codes to re-
trieve information. For example to get a duplicate
bill for a specific month, say June, the user has
to type DUPBILLJUN. This unnecessarily con-
straints users who generally find it easy and intu-
itive to type in a ?texting? language.
Some businesses have recently allowed users to
formulate queries in natural language using SMS.
For example, many contact centers now allow cus-
tomers to ?text? their complaints and requests for
information over SMS. This mode of communica-
tion not only makes economic sense but also saves
the customer from the hassle of waiting in a call
queue. Most of these contact center based services
and other regular services like ?AQA 63336?1 by
Issuebits Ltd, GTIP2 by AlienPant Ltd., ?Tex-
perts?3 by Number UK Ltd. and ?ChaCha?4 use
human agents to understand the SMS text and re-
spond to these SMS queries. The nature of tex-
ting language, which often as a rule rather than ex-
ception, has misspellings, non-standard abbrevia-
tions, transliterations, phonetic substitutions and
omissions, makes it difficult to build automated
question answering systems around SMS technol-
ogy. This is true even for questions whose answers
are well documented like a FAQ database. Un-
like other automatic question answering systems
that focus on generating or searching answers, in
a FAQ database the question and answers are al-
ready provided by an expert. The task is then
to identify the best matching question-answer pair
for a given query.
In this paper we present a FAQ-based ques-
tion answering system over a SMS interface. Our
1http://www.aqa.63336.com/
2http://www.gtip.co.uk/
3http://www.texperts.com/
4http://www.chacha.com/
852
system allows the user to enter a question in
the SMS texting language. Such questions are
noisy and contain spelling mistakes, abbrevia-
tions, deletions, phonetic spellings, translitera-
tions etc. Since mobile handsets have limited
screen space, it necessitates that the system have
high accuracy. We handle the noise in a SMS
query by formulating the query similarity over
FAQ questions as a combinatorial search prob-
lem. The search space consists of combinations
of all possible dictionary variations of tokens in
the noisy query. The quality of the solution, i.e.
the retrieved questions is formalized using a scor-
ing function. Unlike other SMS processing sys-
tems our model does not require training data or
human intervention. Our system handles not only
the noisy variations of SMS query tokens but also
semantic variations. We demonstrate the effective-
ness of our system on real-world data sets.
The rest of the paper is organized as follows.
Section 2 describes the relevant prior work in this
area and talks about our specific contributions.
In Section 3 we give the problem formulation.
Section 4 describes the Pruning Algorithm which
finds the best matching question for a given SMS
query. Section 5 provides system implementation
details. Section 6 provides details about our exper-
iments. Finally we conclude in Section 7.
2 Prior Work
There has been growing interest in providing ac-
cess to applications, traditionally available on In-
ternet, on mobile devices using SMS. Examples
include Search (Schusteritsch et al, 2005), access
to Yellow Page services (Kopparapu et al, 2007),
Email 5, Blog 6 , FAQ retrieval 7 etc. As high-
lighted earlier, these SMS-based FAQ retrieval ser-
vices use human experts to answer questions.
There are other research and commercial sys-
tems which have been developed for general ques-
tion and answering. These systems generally
adopt one of the following three approaches:
Human intervention based, Information Retrieval
based, or Natural language processing based. Hu-
man intervention based systems exploit human
communities to answer questions. These sys-
tems 8 are interesting because they suggest simi-
lar questions resolved in the past. Other systems
5http://www.sms2email.com/
6http://www.letmeparty.com/
7http://www.chacha.com/
8http://www.answers.yahoo.com/
like Chacha and Askme9 use qualified human ex-
perts to answer questions in a timely manner. The
information retrieval based system treat question
answering as an information retrieval problem.
They search large corpus of text for specific text,
phrases or paragraphs relevant to a given question
(Voorhees, 1999). In FAQ based question answer-
ing, where FAQ provide a ready made database of
question-answer, the main task is to find the clos-
est matching question to retrieve the relevant an-
swer (Sneiders, 1999) (Song et al, 2007). The
natural language processing based system tries to
fully parse a question to discover semantic struc-
ture and then apply logic to formulate the answer
(Molla et al, 2003). In another approach the ques-
tions are converted into a template representation
which is then used to extract answers from some
structured representation (Sneiders, 2002) (Katz et
al., 2002). Except for human intervention based
QA systems most of the other QA systems work
in restricted domains and employ techniques such
as named entity recognition, co-reference resolu-
tion, logic form transformation etc which require
the question to be represented in linguistically cor-
rect format. These methods do not work for SMS
based FAQ answering because of the high level of
noise present in SMS text.
There exists some work to remove noise from
SMS (Choudhury et al, 2007) (Byun et al, 2007)
(Aw et al, 2006) (Kobus et al, 2008). How-
ever, all of these techniques require aligned cor-
pus of SMS and conventional language for train-
ing. Building this aligned corpus is a difficult task
and requires considerable human effort. (Acharya
et al, 2008) propose an unsupervised technique
that maps non-standard words to their correspond-
ing conventional frequent form. Their method can
identify non-standard transliteration of a given to-
ken only if the context surrounding that token is
frequent in the corpus. This might not be true in
all domains.
2.1 Our Contribution
To the best of our knowledge we are the first to
handle issues relating to SMS based automatic
question-answering. We address the challenges
in building a FAQ-based question answering sys-
tem over a SMS interface. Our method is unsu-
pervised and does not require aligned corpus or
explicit SMS normalization to handle noise. We
propose an efficient algorithm that handles noisy
9http://www.askmehelpdesk.com/
853
lexical and semantic variations.
3 Problem Formulation
We view the input SMS S as a sequence of tokens
S = s1, s2, . . . , sn. Let Q denote the set of ques-
tions in the FAQ corpus. Each question Q ? Q
is also viewed as a sequence of terms. Our goal
is to find the question Q? from the corpus Q that
best matches the SMS S. As mentioned in the in-
troduction, the SMS string is bound to have mis-
spellings and other distortions, which needs to be
taken care of while performing the match.
In the preprocessing stage, we develop a Do-
main dictionary D consisting of all the terms that
appear in the corpusQ. For each term t in the dic-
tionary and each SMS token si, we define a simi-
larity measure ?(t, si) that measures how closely
the term t matches the SMS token si. We say that
the term t is a variant of si, if ?(t, si) > 0; this is
denoted as t ? si. Combining the similarity mea-
sure and the inverse document frequency (idf) of t
in the corpus, we define a weight function ?(t, si).
The similarity measure and the weight function are
discussed in detail in Section 5.1.
Based on the weight function, we define a scor-
ing function for assigning a score to each question
in the corpus Q. The score measures how closely
the question matches the SMS string S. Consider
a question Q ? Q. For each token si, the scor-
ing function chooses the term from Q having the
maximum weight; then the weight of the n chosen
terms are summed up to get the score.
Score(Q) =
n?
i=1
[
max
t:t?Q and t?si
?(t, si)
]
(1)
Our goal is to efficiently find the question Q? hav-
ing the maximum score.
4 Pruning Algorithm
We now describe algorithms for computing the
maximum scoring question Q?. For each token
si, we create a list Li consisting of all terms from
the dictionary that are variants of si. Consider a
token si. We collect all the variants of si from the
dictionary and compute their weights. The vari-
ants are then sorted in the descending order of
their weights. At the end of the process we have n
ranked lists. As an illustration, consider an SMS
query ?gud plc buy 10s strng on9?. Here, n = 6
and six lists of variants will be created as shown
Figure 1: Ranked List of Variations
in Figure 1. The process of creating the lists is
speeded up using suitable indices, as explained in
detail in Section 5.
Now, we assume that the lists L1, L2, . . . , Ln
are created and explain the algorithms for com-
puting the maximum scoring question Q?. We de-
scribe two algorithms for accomplishing the above
task. The two algorithms have the same function-
ality i.e. they compute Q?, but the second algo-
rithm called the Pruning algorithm has a better
run time efficiency compared to the first algorithm
called the naive algorithm. Both the algorithms re-
quire an index which takes as input a term t from
the dictionary and returns Qt, the set of all ques-
tions in the corpus that contain the term t. We
call the above process as querying the index on
the term t. The details of the index creation is dis-
cussed in Section 5.2.
Naive Algorithm: In this algorithm, we scan
each list Li and query the index on each term ap-
pearing in Li. The returned questions are added to
a collection C. That is,
C =
n?
i=1
?
?
?
t?Li
Qt
?
?
The collection C is called the candidate set. No-
tice that any question not appearing in the candi-
date set has a score 0 and thus can be ignored. It
follows that the candidate set contains the maxi-
mum scoring question Q?. So, we focus on the
questions in the collection C, compute their scores
and find the maximum scoring question Q?. The
scores of the question appearing in C can be com-
puted using Equation 1.
The main disadvantage with the naive algorithm
is that it queries each term appearing in each list
and hence, suffers from high run time cost. We
next explain the Pruning algorithm that avoids this
pitfall and queries only a substantially small subset
of terms appearing in the lists.
Pruning Algorithm: The pruning algorithm
854
is inspired by the Threshold Algorithm (Fagin et
al., 2001). The Pruning algorithm has the prop-
erty that it queries fewer terms and ends up with
a smaller candidate set as compared to the naive
algorithm. The algorithm maintains a candidate
set C of questions that can potentially be the max-
imum scoring question. The algorithm works in
an iterative manner. In each iteration, it picks
the term that has maximum weight among all the
terms appearing in the lists L1, L2, . . . , Ln. As
the lists are sorted in the descending order of the
weights, this amounts to picking the maximum
weight term amongst the first terms of the n lists.
The chosen term t is queried to find the setQt. The
set Qt is added to the candidate set C. For each
question Q ? Qt, we compute its score Score(Q)
and keep it along with Q. The score can be com-
puted by Equation 1 (For each SMS token si, we
choose the term from Q which is a variant of si
and has the maximum weight. The sum of the
weights of chosen terms yields Score(Q)). Next,
the chosen term t is removed from the list. Each
iteration proceeds as above. We shall now develop
a thresholding condition such that when it is sat-
isfied, the candidate set C is guaranteed to contain
the maximum scoring questionQ?. Thus, once the
condition is met, we stop the above iterative pro-
cess and focus only on the questions in C to find
the maximum scoring question.
Consider end of some iteration in the above pro-
cess. Suppose Q is a question not included in C.
We can upperbound the score achievable by Q, as
follows. At best, Q may include the top-most to-
ken from every list L1, L2, . . . , Ln. Thus, score of
Q is bounded by
Score(Q) ?
n?
i=0
?(Li[1]).
(Since the lists are sorted Li[1] is the term having
the maximum weight in Li). We refer to the RHS
of the above inequality as UB.
Let Q? be the question in C having the maximum
score. Notice that if Q? ? UB, then it is guaranteed
that any question not included in the candidate set
C cannot be the maximum scoring question. Thus,
the condition ?Q? ? UB? serves as the termination
condition. At the end of each iteration, we check
if the termination condition is satisfied and if so,
we can stop the iterative process. Then, we simply
pick the question in C having the maximum score
and return it. The algorithm is shown in Figure 2.
In this section, we presented the Pruning algo-
Procedure Pruning Algorithm
Input: SMS S = s1, s2, . . . , sn
Output: Maximum scoring question Q?.
Begin
Construct lists L1, L2, . . . , Ln //(see Section 5.3).
// Li lists variants of si in descending
//order of weight.
Candidate list C = ?.
repeat
j? = argmaxi?(Li[1])
t? = Lj? [1]
// t? is the term having maximum weight among
// all terms appearing in the n lists.
Delete t? from the list Lj? .
Query the index and fetch Qt?
// Qt? : the set of all questions inQ
//having the term t?
For each Q ? Qt?
Compute Score(Q) and
add Q with its score into C
UB =
?n
i=1 ?(Li[1])
Q? = argmaxQ?CScore(Q).
if Score(Q?) ? UB, then
// Termination condition satisfied
Output Q? and exit.
forever
End
Figure 2: Pruning Algorithm
rithm that efficiently finds the best matching ques-
tion for the given SMS query without the need to
go through all the questions in the FAQ corpus.
The next section describes the system implemen-
tation details of the Pruning Algorithm.
5 System Implementation
In this section we describe the weight function,
the preprocessing step and the creation of lists
L1, L2, . . . , Ln.
5.1 Weight Function
We calculate the weight for a term t in the dic-
tionary w.r.t. a given SMS token si. The weight
function is a combination of similarity measure
between t and si and Inverse Document Frequency
(idf) of t. The next two subsections explain the
calculation of the similarity measure and the idf in
detail.
5.1.1 Similarity Measure
Let D be the dictionary of all the terms in the cor-
pus Q. For term t ? D and token si of the SMS,
the similarity measure ?(t, si) between them is
855
?(t, si) =
?
????
????
LCSRatio(t,si)
EditDistanceSMS(t,si)
if t and si share same
starting character *
0 otherwise
(2)
where LCSRatio(t, si) =
length(LCS(t,si))
length(t) and LCS(t, si) is
the Longest common subsequence between t and si.
* The rationale behind this heuristic is that while typing a SMS, people
typically type the first few characters correctly. Also, this heuristic helps limit
the variants possible for a given token.
The Longest Common Subsequence Ratio
(LCSR) (Melamed, 1999) of two strings is the ra-
tio of the length of their LCS and the length of the
longer string. Since in SMS text, the dictionary
term will always be longer than the SMS token,
the denominator of LCSR is taken as the length of
the dictionary term. We call this modified LCSR
as the LCSRatio.
Procedure EditDistanceSMS
Input: term t, token si
Output: Consonant Skeleton Edit distance
Begin
return LevenshteinDistance(CS(si), CS(t)) + 1
// 1 is added to handle the case where
// Levenshtein Distance is 0
End
Consonant Skeleton Generation (CS)
1. remove consecutive repeated characters
// (call? cal)
2. remove all vowels
//(waiting ? wtng, great? grt)
Figure 3: EditDistanceSMS
The EditDistanceSMS shown in Figure 3
compares the Consonant Skeletons (Prochasson et
al., 2007) of the dictionary term and the SMS to-
ken. If the consonant keys are similar, i.e. the Lev-
enshtein distance between them is less, the simi-
larity measure defined in Equation 2 will be high.
We explain the rationale behind using the
EditDistanceSMS in the similarity measure
?(t, si) through an example. For the SMS
token ?gud? the most likely correct form is
?good?. The two dictionary terms ?good? and
?guided? have the same LCSRatio of 0.5 w.r.t
?gud?, but the EditDistanceSMS of ?good? is
1 which is less than that of ?guided?, which has
EditDistanceSMS of 2 w.r.t ?gud?. As a result
the similarity measure between ?gud? and ?good?
will be higher than that of ?gud? and ?guided?.
5.1.2 Inverse Document Frequency
If f number of documents in corpus Q contain a
term t and the total number of documents in Q is
N, the Inverse Document Frequency (idf) of t is
idf(t) = log
N
f
(3)
Combining the similarity measure and the idf
of t in the corpus, we define the weight function
?(t, si) as
?(t, si) = ?(t, si) ? idf(t) (4)
The objective behind the weight function is
1. We prefer terms that have high similarity
measure i.e. terms that are similar to the
SMS token. Higher the LCSRatio and lower
the EditDistanceSMS , higher will be the
similarity measure. Thus for example, for a
given SMS token ?byk?, similarity measure
of word ?bike? is higher than that of ?break?.
2. We prefer words that are highly discrimi-
native i.e. words with a high idf score.
The rationale for this stems from the fact
that queries, in general, are composed of in-
formative words. Thus for example, for a
given SMS token ?byk?, idf of ?bike? will
be more than that of commonly occurring
word ?back?. Thus, even though the similar-
ity measure of ?bike? and ?back? are same
w.r.t. ?byk?, ?bike? will get a higher weight
than ?back? due to its idf.
We combine these two objectives into a single
weight function multiplicatively.
5.2 Preprocessing
Preprocessing involves indexing of the FAQ cor-
pus, formation of Domain and Synonym dictionar-
ies and calculation of the Inverse Document Fre-
quency for each term in the Domain dictionary.
As explained earlier the Pruning algorithm re-
quires retrieval of all questions Qt that contains a
given term t. To do this efficiently we index the
FAQ corpus using Lucene10. Each question in the
FAQ corpus is treated as a Document; it is tok-
enized using whitespace as delimiter and indexed.
10http://lucene.apache.org/java/docs/
856
The Domain dictionaryD is built from all terms
that appear in the corpus Q.
The weight calculation for Pruning algorithm
requires the idf for a given term t. For each term t
in the Domain dictionary, we query the Lucene in-
dexer to get the number of Documents containing
t. Using Equation 3, the idf(t) is calculated. The
idf for each term t is stored in a Hashtable, with t
as the key and idf as its value.
Another key step in the preprocessing stage is
the creation of the Synonym dictionary. The Prun-
ing algorithm uses this dictionary to retrieve se-
mantically similar questions. Details of this step is
further elaborated in the List Creation sub-section.
The Synonym Dictionary creation involves map-
ping each word in the Domain dictionary to it?s
corresponding Synset obtained from WordNet11.
5.3 List Creation
Given a SMS S, it is tokenized using white-spaces
to get a sequence of tokens s1, s2, . . . , sn. Digits
occurring in SMS token (e.g ?10s? , ?4get?) are re-
placed by string based on a manually crafted digit-
to-string mapping (?10? ? ?ten?). A list Li is
setup for each token si using terms in the domain
dictionary. The list for a single character SMS to-
ken is set to null as it is most likely to be a stop
word . A term t from domain dictionary is in-
cluded in Li if its first character is same as that of
the token si and it satisfies the threshold condition
length(LCS(t, si)) > 1.
Each term t that is added to the list is assigned a
weight given by Equation 4.
Terms in the list are ranked in descending or-
der of their weights. Henceforth, the term ?list?
implies a ranked list.
For example the SMS query ?gud plc 2 buy 10s
strng on9? (corresponding question ?Where is a
good place to buy tennis strings online??), is to-
kenized to get a set of tokens {?gud?, ?plc?, ?2?,
?buy?, ?10s?, ?strng?, ?on9?}. Single character to-
kens such as ?2? are neglected as they are most
likely to be stop words. From these tokens cor-
responding lists are setup as shown in Figure 1.
5.3.1 Synonym Dictionary Lookup
To retrieve answers for SMS queries that are
semantically similar but lexically different from
questions in the FAQ corpus we use the Synonym
dictionary described in Section 5.2. Figure 4 illus-
trates some examples of such SMS queries.
11http://wordnet.princeton.edu/
Figure 4: Semantically similar SMS and questions
Figure 5: Synonym Dictionary LookUp
For a given SMS token si, the list of variations
Li is further augmented using this Synonym dic-
tionary. For each token si a fuzzy match is per-
formed between si and the terms in the Synonym
dictionary and the best matching term from the
Synonym dictionary, ? is identified. As the map-
pings between the Synonym and the Domain dic-
tionary terms are maintained, we obtain the corre-
sponding Domain dictionary term ? for the Syn-
onym term ? and add that term to the list Li. ? is
assigned a weight given by
?(?, si) = ?(?, si) ? idf(?) (5)
It should be noted that weight for ? is based on
the similarity measure between Synonym dictio-
nary term ? and SMS token si.
For example, the SMS query ?hw2 countr quik
srv?( corresponding question ?How to return a
very fast serve??) has two terms ?countr? ?
?counter? and ?quik? ? ?quick? belonging to
the Synonym dictionary. Their associated map-
pings in the Domain dictionary are ?return? and
?fast? respectively as shown in Figure 5. During
the list setup process the token ?countr? is looked
857
up in the Domain dictionary. Terms from the Do-
main dictionary that begin with the same character
as that of the token ?countr? and have a LCS > 1
such as ?country?,?count?, etc. are added to the
list and assigned a weight given by Equation 4.
After that, the token ?countr? is looked up in the
Synonym dictionary using Fuzzy match. In this
example the term ?counter? from the Synonym
dictionary fuzzy matches the SMS token. The Do-
main dictionary term corresponding to the Syn-
onym dictionary term ?counter? is looked up and
added to the list. In the current example the cor-
responding Domain dictionary term is ?return?.
This term is assigned a weight given by Equation
5 and is added to the list as shown in Figure 5.
5.4 FAQ retrieval
Once the lists are created, the Pruning Algorithm
as shown in Figure 2 is used to find the FAQ ques-
tionQ? that best matches the SMS query. The cor-
responding answer to Q? from the FAQ corpus is
returned to the user.
The next section describes the experimental
setup and results.
6 Experiments
We validated the effectiveness and usability of
our system by carrying out experiments on two
FAQ data sets. The first FAQ data set, referred
to as the Telecom Data-Set, consists of 1500 fre-
quently asked questions, collected from a Telecom
service provider?s website. The questions in this
data set are related to the Telecom providers prod-
ucts or services. For example queries about call
rates/charges, bill drop locations, how to install
caller tunes, how to activate GPRS etc. The sec-
ond FAQ corpus, referred to as the Yahoo DataSet,
consists of 7500 questions from three Yahoo!
Answers12 categories namely Sports.Swimming,
Sports.Tennis, Sports.Running.
To measure the effectiveness of our system, a
user evaluation study was performed. Ten human
evaluators were asked to choose 10 questions ran-
domly from the FAQ data set. None of the eval-
uators were authors of the paper. They were pro-
vided with a mobile keypad interface and asked to
?text? the selected 10 questions as SMS queries.
Through that exercise 100 relevant SMS queries
per FAQ data set were collected. Figure 6 shows
sample SMS queries. In order to validate that the
system was able to handle queries that were out of
12http://answers.yahoo.com/
Figure 6: Sample SMS queries
Data Set Relevant Queries Irrelevant Queries
Telecom 100 50
Yahoo 100 50
Table 1: SMS Data Set.
the FAQ domain, we collected 5 irrelevant SMS
queries from each of the 10 human-evaluators for
both the data sets. Irrelevant queries were (a)
Queries out of the FAQ domain e.g. queries re-
lated to Cricket, Billiards, activating GPS etc (b)
Absurd queries e.g. ?ama ameyu tuem? (sequence
of meaningless words) and (c) General Queries
e.g. ?what is sports?. Table 1 gives the number
of relevant and irrelevant queries used in our ex-
periments.
The average word length of the collected SMS
messages for Telecom and Yahoo datasets was 4
and 7 respectively. We manually cleaned the SMS
query data word by word to create a clean SMS
test-set. For example, the SMS query ?h2 mke a
pdl bke fstr? was manually cleaned to get ?how
to make pedal bike faster?. In order to quantify
the level of noise in the collected SMS data, we
built a character-level language model(LM)13 us-
ing the questions in the FAQ data-set (vocabulary
size is 44 characters) and computed the perplex-
ity14 of the language model on the noisy and the
cleaned SMS test-set. The perplexity of the LM on
a corpus gives an indication of the average num-
ber of bits needed per n-gram to encode the cor-
pus. Noise will result in the introduction of many
previously unseen n-grams in the corpus. Higher
number of bits are needed to encode these improb-
able n-grams which results in increased perplexity.
From Table 2 we can see the difference in perplex-
ity for noisy and clean SMS data for the Yahoo
and Telecom data-set. The high level of perplexity
in the SMS data set indicates the extent of noise
present in the SMS corpus.
To handle irrelevant queries the algorithm de-
scribed in Section 4 is modified. Only if the
Score(Q?) is above a certain threshold, it?s answer
is returned, else we return ?null?. The threshold
13http://en.wikipedia.org/wiki/Language model
14bits = log2(perplexity)
858
Cleaned SMS Noisy SMS
Yahoo bigram 14.92 74.58trigram 8.11 93.13
Telecom bigram 17.62 59.26trigram 10.27 63.21
Table 2: Perplexity for Cleaned and Noisy SMS
Figure 7: Accuracy on Telecom FAQ Dataset
was determined experimentally.
To retrieve the correct answer for the posed
SMS query, the SMS query is matched against
questions in the FAQ data set and the best match-
ing question(Q?) is identified using the Pruning al-
gorithm. The system then returns the answer to
this best matching question to the human evalua-
tor. The evaluator then scores the response on a bi-
nary scale. A score of 1 is given if the returned an-
swer is the correct response to the SMS query, else
it is assigned 0. The scoring procedure is reversed
for irrelevant queries i.e. a score of 0 is assigned
if the system returns an answer and 1 is assigned
if it returns ?null? for an ?irrelevant? query. The
result of this evaluation on both data-sets is shown
in Figure 7 and 8.
Figure 8: Accuracy on Yahoo FAQ Dataset
In order to compare the performance of our sys-
tem, we benchmark our results against Lucene?s
15 Fuzzy match feature. Lucene supports fuzzy
searches based on the Levenshtein Distance, or
Edit Distance algorithm. To do a fuzzy search
15http://lucene.apache.org
we specify the ? symbol at the end of each to-
ken of the SMS query. For example, the SMS
query ?romg actvt? on the FAQ corpus is refor-
mulated as ?romg? 0.3 actvt? 0.3?. The param-
eter after the ? specifies the required similarity.
The parameter value is between 0 and 1, with a
value closer to 1 only terms with higher similar-
ity will be matched. These queries are run on the
indexed FAQs. The results of this evaluation on
both data-sets is shown in Figure 7 and 8. The
results clearly demonstrate that our method per-
forms 2 to 2.5 times better than Lucene?s Fuzzy
match. It was observed that with higher values
of similarity parameter (? 0.6, ? 0.8), the num-
ber of correctly answered queries was even lower.
In Figure 9 we show the runtime performance of
the Naive vs Pruning algorithm on the Yahoo FAQ
Dataset for 150 SMS queries. It is evident from
Figure 9 that not only does the Pruning Algorithm
outperform the Naive one but also gives a near-
constant runtime performance over all the queries.
The substantially better performance of the Prun-
ing algorithm is due to the fact that it queries much
less number of terms and ends up with a smaller
candidate set compared to the Naive algorithm.
Figure 9: Runtime of Pruning vs Naive Algorithm
for Yahoo FAQ Dataset
7 Conclusion
In recent times there has been a rise in SMS based
QA services. However, automating such services
has been a challenge due to the inherent noise in
SMS language. In this paper we gave an efficient
algorithm for answering FAQ questions over an
SMS interface. Results of applying this on two
different FAQ datasets shows that such a system
can be very effective in automating SMS based
FAQ retrieval.
859
References
Rudy Schusteritsch, Shailendra Rao, Kerry Rodden.
2005. Mobile Search with Text Messages: Design-
ing the User Experience for Google SMS. CHI,
Portland, Oregon.
Sunil Kumar Kopparapu, Akhilesh Srivastava and Arun
Pande. 2007. SMS based Natural Language Inter-
face to Yellow Pages Directory, In Proceedings of
the 4th International conference on mobile technol-
ogy, applications, and systems and the 1st Interna-
tional symposium on Computer human interaction
in mobile technology, Singapore.
Monojit Choudhury, Rahul Saraf, Sudeshna Sarkar, Vi-
jit Jain, and Anupam Basu. 2007. Investigation and
Modeling of the Structure of Texting Language, In
Proceedings of IJCAI-2007 Workshop on Analytics
for Noisy Unstructured Text Data, Hyderabad.
E. Voorhees. 1999. The TREC-8 question answering
track report.
D. Molla. 2003. NLP for Answer Extraction in Tech-
nical Domains, In Proceedings of EACL, USA.
E. Sneiders. 2002. Automated question answering
using question templates that cover the conceptual
model of the database, In Proceedings of NLDB,
pages 235?239.
B. Katz, S. Felshin, D. Yuret, A. Ibrahim, J. Lin, G.
Marton, and B. Temelkuran. 2002. Omnibase: Uni-
form access to heterogeneous data for question an-
swering, Natural Language Processing and Infor-
mation Systems, pages 230?234.
E. Sneiders. 1999. Automated FAQ Answering: Con-
tinued Experience with Shallow Language Under-
standing, Question Answering Systems. Papers from
the 1999 AAAI Fall Symposium. Technical Report
FS-99?02, November 5?7, North Falmouth, Mas-
sachusetts, USA, AAAI Press, pp.97?107
W. Song, M. Feng, N. Gu, and L. Wenyin. 2007.
Question similarity calculation for FAQ answering,
In Proceeding of SKG 07, pages 298?301.
Aiti Aw, Min Zhang, Juan Xiao, and Jian Su. 2006.
A phrase-based statistical model for SMS text nor-
malization, In Proceedings of COLING/ACL, pages
33?40.
Catherine Kobus, Franois Yvon and Graldine Damnati.
2008. Normalizing SMS: are two metaphors bet-
ter than one?, In Proceedings of the 22nd Inter-
national Conference on Computational Linguistics,
pages 441?448 Manchester.
Jeunghyun Byun, Seung-Wook Lee, Young-In Song,
Hae-Chang Rim. 2008. Two Phase Model for SMS
Text Messages Refinement, Association for the Ad-
vancement of Artificial Intelligence. AAAI Workshop
on Enhanced Messaging
Ronald Fagin , Amnon Lotem , Moni Naor. 2001.
Optimal aggregation algorithms for middleware, In
Proceedings of the 20th ACM SIGMOD-SIGACT-
SIGART symposium on Principles of database sys-
tems.
I. Dan Melamed. 1999. Bitext maps and alignment via
pattern recognition, Computational Linguistics.
E. Prochasson, Christian Viard-Gaudin, Emmanuel
Morin. 2007. Language Models for Handwritten
Short Message Services, In Proceedings of the 9th
International Conference on Document Analysis and
Recognition.
Sreangsu Acharya, Sumit Negi, L. V. Subramaniam,
Shourya Roy. 2008. Unsupervised learning of mul-
tilingual short message service (SMS) dialect from
noisy examples, In Proceedings of the second work-
shop on Analytics for noisy unstructured text data.
860
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 87?96,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Handling Noisy Queries In Cross Language FAQ Retrieval
Danish Contractor Govind Kothari Tanveer A. Faruquie
L. Venkata Subramaniam Sumit Negi
IBM Research India
Vasant Kunj, Institutional Area
New Delhi, India
{dcontrac,govkotha,ftanveer,lvsubram,sumitneg}@in.ibm.com
Abstract
Recent times have seen a tremendous growth
in mobile based data services that allow peo-
ple to use Short Message Service (SMS) to
access these data services. In a multilin-
gual society it is essential that data services
that were developed for a specific language
be made accessible through other local lan-
guages also. In this paper, we present a ser-
vice that allows a user to query a Frequently-
Asked-Questions (FAQ) database built in a lo-
cal language (Hindi) using Noisy SMS En-
glish queries. The inherent noise in the SMS
queries, along with the language mismatch
makes this a challenging problem. We handle
these two problems by formulating the query
similarity over FAQ questions as a combina-
torial search problem where the search space
consists of combinations of dictionary varia-
tions of the noisy query and its top-N transla-
tions. We demonstrate the effectiveness of our
approach on a real-life dataset.
1 Introduction
There has been a tremendous growth in the number
of new mobile subscribers in the recent past. Most
of these new subscribers are from developing coun-
tries where mobile is the primary information de-
vice. Even for users familiar with computers and the
internet, the mobile provides unmatched portability.
This has encouraged the proliferation of informa-
tion services built around SMS technology. Several
applications, traditionally available on Internet, are
now being made available on mobile devices using
SMS. Examples include SMS short code services.
Short codes are numbers where a short message in
a predesignated format can be sent to get specific
information. For example, to get the closing stock
price of a particular share, the user has to send a
message IBMSTOCKPR. Other examples are search
(Schusteritsch et al, 2005), access to Yellow Page
services (Kopparapu et al, 2007), Email 1, Blog 2 ,
FAQ retrieval 3 etc. The SMS-based FAQ retrieval
services use human experts to answer SMS ques-
tions.
Recent studies have shown that instant messag-
ing is emerging as the preferred mode of commu-
nication after speech and email.4 Millions of users
of instant messaging (IM) services and short mes-
sage service (SMS) generate electronic content in a
dialect that does not adhere to conventional gram-
mar, punctuation and spelling standards. Words are
intentionally compressed by non-standard spellings,
abbreviations and phonetic transliteration are used.
Typical question answering systems are built for use
with languages which are free from such errors. It
is difficult to build an automated question answer-
ing system around SMS technology. This is true
even for questions whose answers are well docu-
mented like in a Frequently-Asked-Questions (FAQ)
database. Unlike other automatic question answer-
ing systems that focus on searching answers from
a given text collection, Q&A archive (Xue et al,
2008) or the Web (Jijkoun et al, 2005), in a FAQ
database the questions and answers are already pro-
1http://www.sms2email.com/
2http://www.letmeparty.com/
3http://www.chacha.com/
4http://www.whyconverge.com/
87
Figure 1: Sample SMS queries with Hindi FAQs
vided by an expert. The main task is then to iden-
tify the best matching question to retrieve the rel-
evant answer (Sneiders, 1999) (Song et al, 2007).
The high level of noise in SMS queries makes this a
difficult problem (Kothari et al, 2009). In a multi-
lingual setting this problem is even more formidable.
Natural language FAQ services built for users in one
language cannot be accessed in another language.
In this paper we present a FAQ-based question an-
swering system over a SMS interface that solves this
problem for two languages. We allow the FAQ to be
in one language and the SMS query to be in another.
Multi-lingual question answering and information
retrieval has been studied in the past (Sekine and
Grishman, 2003)(Cimiano et al, 2009). Such sys-
tems resort to machine translation so that the search
can be performed over a single language space. In
the two language setting, it involves building a ma-
chine translation system engine and using it such
that the question answering system built for a sin-
gle language can be used.
Typical statistical machine translation systems
use large parallel corpora to learn the translation
probabilities (Brown et al, 2007). Traditionally
such corpora have consisted of news articles and
other well written articles. Since the translation sys-
tems are not trained on SMS language they perform
very poorly when translating noisy SMS language.
Parallel corpora comprising noisy sentences in one
language and clean sentences in another language
are not available and it would be hard to build such
large parallel corpora to train a machine translation
system. There exists some work to remove noise
from SMS (Choudhury et al, 2007) (Byun et al,
2008) (Aw et al, 2006) (Neef et al, 2007) (Kobus
et al, 2008). However, all of these techniques re-
quire an aligned corpus of SMS and conventional
language for training. Such data is extremely hard
to create. Unsupervised techniques require huge
amounts of SMS data to learn mappings of non-
standard words to their corresponding conventional
form (Acharyya et al, 2009).
Removal of noise from SMS without the use of
parallel data has been studied but the methods used
are highly dependent on the language model and the
degree of noise present in the SMS (Contractor et
al., 2010). These systems are not very effective if
the SMSes contain grammatical errors (or the sys-
tem would require large amounts of training data in
the language model to be able to deal with all pos-
sible types of noise) in addition to misspellings etc.
Thus, the translation of a cleaned SMS, into a second
language, will not be very accurate and it would not
give good results if such a translated SMS is used to
query an FAQ collection.
Token based noise-correction techniques (such as
those using edit-distance, LCS etc) cannot be di-
rectly applied to handle the noise present in the SMS
query. These noise-correction methods return a list
of candidate terms for a given noisy token (E.g.
?gud? ? > ?god?,?good?,?guide? ) . Considering all
these candidate terms and their corresponding trans-
lations drastically increase the search space for any
multi-lingual IR system. Also , naively replacing the
noisy token in the SMS query with the top matching
candidate term gives poor performance as shown by
our experiments. Our algorithm handles these and
related issues in an efficient manner.
In this paper we address the challenges arising
when building a cross language FAQ-based ques-
tion answering system over an SMS interface. Our
method handles noisy representation of questions in
a source language to retrieve answers across target
languages. The proposed method does not require
hand corrected data or an aligned corpus for explicit
SMS normalization to mitigate the effects of noise.
It also works well with grammatical noise. To the
best of our knowledge we are the first to address
issues in noisy SMS based cross-language FAQ re-
trieval. We propose an efficient algorithm that can
handle noise in the form of lexical and semantic cor-
ruptions in the source language.
2 Problem formulation
Consider an input SMS Se in a source language
e. We view Se as a sequence of n tokens Se =
s1, s2, . . . , sn. As explained in the introduction, the
input is bound to have misspellings and other lexical
and semantic distortions. Also let Qh denote the set
88
of questions in the FAQ corpus of a target language
h. Each question Qh ? Qh is also viewed as a se-
quence of tokens. We want to find the question Q?h
from the corpus Qh that best matches the SMS Se.
The matching is assisted by a source dictionary
De consisting of clean terms in e constructed from
a general English dictionary and a domain dictio-
nary of target language Dh built from all the terms
appearing in Qh. For a token si in the SMS in-
put, term te in dictionary De and term th in dictio-
nary Dh we define a cross-lingual similarity mea-
sure ?(th, te, si) that measures the extent to which
term si matches th using the clean term te. We con-
sider th a cross lingual variant of si if for any te the
cross language similarity measure ?(th, te, si) > .
We denote this as th ? si.
We define a weight function ?(th, te, si) using the
cross lingual similarity measure and the inverse doc-
ument frequency (idf) of th in the target language
FAQ corpus. We also define a scoring function to as-
sign a score to each question in the corpusQh using
the weight function. Consider a question Qh ? Qh.
For each token si, the scoring function chooses the
term from Qh having the maximum weight using
possible clean representations of si; then the weight
of the n chosen terms are summed up to get the
score. The score measures how closely the question
in FAQ matches the noisy SMS string Se using the
composite weights of individual tokens.
Score(Qh) =
n?
i=1
max
th?Qh,te?De & th?si
?(th, te, si)
Our goal is to efficiently find the question Q?h having
the maximum score.
3 Noise removal from queries
In order to process the noisy SMS input we first have
to map noisy tokens in Se to the possible correct lex-
ical representations. We use a similarity measure to
map the noisy tokens to their clean lexical represen-
tations.
3.1 Similarity Measure
For a term te ? De and token si of the SMS input
Se, the similarity measure ?(te, si) between them is
?(te, si) =
?
???????
???????
LCSRatio(te,si)
EditDistanceSMS(te,si)
if te and si share
same starting
character *
0 otherwise
(1)
Where LCSRatio(te, si) =
length(LCS(te,si))
length(te)
and LCS(te, si)
is the Longest common subsequence between te and si.
* The intuition behind this measure is that people typically type the
first few characters of a word in an SMS correctly. This way we limit
the possible variants for a particular noisy token
The Longest Common Subsequence Ratio (LC-
SRatio) (Melamed et al, 1999) of two strings is the
ratio of the length of their LCS and the length of the
longer string. Since in the SMS scenario, the dictio-
nary term will always be longer than the SMS token,
the denominator of LCSRatio is taken as the length
of the dictionary term.
The EditDistanceSMS (Figure 2) compares the
Consonant Skeletons (Prochasson et al, 2007) of the
dictionary term and the SMS token. If the Leven-
shtein distance between consonant skeletons is small
then ?(te, si) will be high. The intuition behind us-
ing EditDistanceSMS can be explained through
an example. Consider an SMS token ?gud? whose
most likely correct form is ?good?. The longest
common subsequence for ?good? and ?guided? with
?gud? is ?gd?. Hence the two dictionary terms
?good? and ?guided? have the same LCSRatio of 0.5
w.r.t ?gud?, but the EditDistanceSMS of ?good?
is 1 which is less than that of ?guided?, which has
EditDistanceSMS of 2 w.r.t ?gud?. As a result the
similarity measure between ?gud? and ?good? will
be higher than that of ?gud? and ?guided?. Higher
the LCSRatio and lower the EditDistanceSMS ,
higher will be the similarity measure. Hence, for
a given SMS token ?byk?, the similarity measure of
word ?bike? is higher than that of ?break?.
4 Cross lingual similarity
Once we have potential candidates which are the
likely disambiguated representations of the noisy
89
Procedure EditDistanceSMS(te, si)
Begin
return LevenshteinDistance(CS(si), CS(te)) + 1
End
Procedure CS (t): // Consonant Skeleton Generation
Begin
Step 1. remove consecutive repeated characters in t
// (fall? fal)
Step 2. remove all vowels in t
//(painting ? pntng, threat? thrt)
return t
End
Figure 2: EditDistanceSMS
term, we map these candidates to appropriate terms
in the target language. We use a statistical dictionary
to achieve this cross lingual mapping.
4.1 Statistical Dictionary
In order to build a statistical dictionary we use
the statistical translation model proposed in (Brown
et al, 2007). Under IBM model 2 the transla-
tion probability of source language sentence e? =
{t1e, . . . , t
j
e, . . . , t
m
e } and a target language sentence
h? = {t1h, . . . , t
i
h, . . . , t
l
e} is given by
Pr(h?|e?) = ?(l|m)
l?
i=1
m?
j=0
?(tih|t
j
e)a(j|i,m, l).
(2)
Here the word translation model ?(th|te) gives the
probability of translating the source term to target
term and the alignment model a(j|i,m, l) gives the
probability of translating the source term at position
i to a target position j. This model is learnt using an
aligned parallel corpus.
Given a clean term tie in source language we get
all the corresponding terms T = {t1h, . . . , t
k
h, . . .}
from the target language such that word translation
probability ?(tkh|t
i
e) > ?. We rank these terms ac-
cording to the probability given by the word trans-
lation model ?(th|te) and consider only those tar-
get terms that are part of domain dictionary i.e.
tkh ? D
h.
4.2 Cross lingual similarity measure
For each term si in SMS input query, we find all
the clean terms te in source dictionary De for which
similarity measure ?(te, si) > ?. For each of these
term te, we find the cross lingual similar terms Tte
using the word translation model. We compute the
cross lingual similarity measure between these terms
as
?(si, te, th) = ?(te, si).?(th, te) (3)
The measure selects those terms in target lan-
guage that have high probability of being translated
from a noisy term through one or more valid clean
terms.
4.3 Cross lingual similarity weight
We combine the idf and the cross lingual similarity
measure to define the cross lingual weight function
?(th, te, si) as
?(th, te, si) = ?(th, te, si).idf(th) (4)
By using idf we give preference to terms that are
highly discriminative. This is necessary because
queries are distinguished from each other using in-
formative words. For example for a given noisy
token ?bck? if a word translation model produces
a translation output ?wapas? (as in came back) or
?peet? or ?qamar? (as in back pain) then idf will
weigh ?peet? more as it is relatively more discrim-
inative compared to ?wapas? which is used fre-
quently.
5 Pruning and matching
In this section we describe our search algorithm and
the preprocessing needed to find the best question
Q?h for a given SMS query.
5.1 Indexing
Our algorithm operates at a token level and its corre-
sponding cross lingual variants. It is therefore nec-
essary to be able to retrieve all questions Qhth that
contain a given target language term th. To do this
efficiently we index the questions in FAQ corpus us-
ing Lucene5. Each question in FAQ is treated as a
document. It is tokenized using whitespace as de-
limiter before indexing.
5http://lucene.apache.org/java/docs/
90
The cross lingual similarity weight calculation re-
quires the idf for a given term th. We query on this
index to determine the number of documents f that
contain th. The idf of each term in Dh is precom-
puted and stored in a hashtable with th as the key.
The cross lingual similarity measure calculation re-
quires the word translation probability for a given
term te. For every te in dictionary De, we store
Tte in a hashmap that contains a list of terms in the
target language along with their statistically deter-
mined translation probability ?(th|te) > ?, where
th ? Dh.
Since the query and the FAQs use terms from dif-
ferent languages, the computation of IDF becomes a
challenge (Pirkola, 1998) (Oard et al, 2007). Prior
work uses a bilingual dictionary for translations for
calculating the IDF. We on the other hand rely on
a statistical dictionary that has translation probabil-
ities. Applying the method suggested in the prior
work on a statistical dictionary leads to errors as the
translations may themselves be inaccurate.
We therefore calculate IDFs for target language
term (translation) and use it in the weight measure
calculation. The method suggested by Oard et al
(Oard et al, 2007) is more useful in retrieval tasks
for multiple documents, while in our case we need
to retrieve a specific document (FAQ).
5.2 List Creation
Given an SMS input string Se, we tokenize it on
white space and replace any occurrence of digits to
their string based form (e.g. 4get, 2day) to get a se-
ries of n tokens s1, s2, . . . , sn. A list Lei is created
for each token si using terms in the monolingual dic-
tionary De. The list for a single character SMS to-
ken is set to null as it is most likely to be a stop word.
A term te from De is included in Lei if it satisfies the
threshold condition
?(te, si) > ? (5)
The threshold value ? is determined experimen-
tally. For every te ? Lei we retrieve Tte and then
retrieve the idf scores for every th ? Tte . Using the
word translation probabilities and the idf score we
compute the cross lingual similarity weight to create
a new list Lhi . A term th is included in the list only
if
?(th|te) > 0.1 (6)
This probability cut-off is used to prevent poor
quality translations from being included in the list.
If more than one term te has the same transla-
tion th, then th can occur more than once in a given
list. If this happens, then we remove repetitive oc-
currences of th and assign it a weight equal to the
maximum weight amongst all occurrences in the list,
multiplied by the number of times it occurs. The
terms th in Lhi are sorted in decreasing order of their
similarity weights. Henceforth, the term ?list? im-
plies a sorted list.
For example given a SMS query ?hw mch ds it cst
to stdy in india? as shown in Fig. 3, for each token
we create a list of possible correct dictionary words
by dictionary look up. Thus for token ?cst? we get
dictionary words lik ?cost, cast, case, close?. For
each dictionary word we get a set of possible words
in Hindi by looking at statistical translation table.
Finally we merged the list obtained to get single list
of Hindi words. The final list is ranked according to
their similarity weights.
5.3 Search algorithm
Given Se containing n tokens, we create n sorted
lists Lh1 , L
h
2 , . . . , L
h
n containing terms from the do-
main dictionary and sorted according to their cross
lingual weights as explained in the previous section.
A naive approach would be to query the index using
each term appearing in all Lhi to build a Collection
set C of questions. The best matching question Q?h
will be contained in this collection. We compute the
score of each question in C using Score(Q) and the
question with highest score is treated as Q?h. How-
ever the naive approach suffers from high runtime
cost.
Inspired by the Threshold Algorithm (Fagin et
al., 2001) we propose using a pruning algorithm
that maintains a much smaller candidate set C of
questions that can potentially contain the maximum
scoring question. The algorithm is shown in Fig-
ure 4. The algorithm works in an iterative manner.
In each iteration, it picks the term that has maxi-
mum weight among all the terms appearing in the
lists Lh1 , L
h
2 , . . . , L
h
n. As the lists are sorted in the
descending order of the weights, this amounts to
picking the maximum weight term amongst the first
terms of the n lists. The chosen term th is queried to
find the set Qth . The set Qth is added to the candi-
91
Figure 3: List creation
date set C. For each question Q ? Qth , we compute
its score Score(Q) and keep it along with Q. After
this the chosen term th is removed from the list and
the next iteration is carried out. We stop the iterative
process when a thresholding condition is met and fo-
cus only on the questions in the candidate set C. The
thresholding condition guarantees that the candidate
set C contains the maximum scoring question Q?h.
Next we develop this thresholding condition.
Let us consider the end of an iteration. Sup-
pose Q is a question not included in C. At
best, Q will include the current top-most tokens
Lh1 [1], L
h
2 [1], . . . , L
h
n[1] from every list. Thus, the
upper bound UB on the score of Q is
Score(Q) ?
n?
i=0
?(Lhi [1]).
Let Q? be the question in C having the maximum
score. Notice that if Q? ? UB, then it is guaranteed
that any question not included in the candidate set C
cannot be the maximum scoring question. Thus, the
condition ?Q? ? UB? serves as the termination cri-
terion. At the end of each iteration, we check if the
termination condition is satisfied and if so, we can
stop the iterative process. Then, we simply pick the
question in C having the maximum score and return
it.
Procedure Search Algorithm
Input: SMS S = s1, s2, . . . , sn
Output: Maximum scoring question Q?h.
Begin
?si, construct Lei for which ?(si, te) > 
// Li lists variants of si
Construct lists Lh1 , L
h
2 , . . . , L
h
n //(see Section 5.2).
// Lhi lists cross lingual variants of si in decreasing
//order of weight.
Candidate list C = ?.
repeat
j? = argmaxi?(L
h
i [1])
t?h = L
h
j? [1]
// t?h is the term having maximum weight among
// all terms appearing in the n lists.
Delete t?h from the list L
h
j? .
Retrieve Qt?h using the index
// Qt?h : the set of all questions in Q
h
//having the term t?h
For each Q ? Qt?h
Compute Score(Q) and
add Q with its score into C
UB =
?n
i=1 ?(L
h
i [1])
Q? = argmaxQ?CScore(Q).
if Score(Q?) ? UB, then
// Termination condition satisfied
Output Q? and exit.
forever
End
Figure 4: Search Algorithm with Pruning
6 Experiments
To evaluate our system we used noisy English SMS
queries to query a collection of 10, 000 Hindi FAQs.
These FAQs were collected from websites of vari-
ous government organizations and other online re-
sources. These FAQs are related to railway reser-
vation, railway enquiry, passport application and
health related issues. For our experiments we asked
6 human evaluators, proficient in both English and
Hindi, to create English SMS queries based on the
general topics that our FAQ collection dealt with.
We found 60 SMS queries created by the evaluators,
had answers in our FAQ collection and we desig-
nated these as the in-domain queries. To measure
the effectiveness of our system in handling out of
domain queries we used a total of 380 SMSes part of
which were taken from the NUS corpus (How et al,
92
whch metro statn z nr pragati maidan ?
dus metro goes frm airpot 2 new delhi rlway statn?
is dere any special metro pas 4 delhi uni students?
whn is d last train of delhi metro?
whr r d auto stands N delhi?
Figure 5: Sample SMS queries
2005) and the rest from the ?out-of-domain? queries
created by the human evaluators. Thus the total SMS
query data size was 440. Fig 5 shows some of the
sample queries.
Our objective was to retrieve the correct Hindi
FAQ response given a noisy English SMS query. A
given English SMS query was matched against the
list of indexed FAQs and the best matching FAQ was
returned by the Pruning Algorithm described in Sec-
tion 5. A score of 1 was assigned if the retrieved
answer was indeed the response to the posed SMS
query else we assigned a score of 0. In case of out
of domain queries a score of 1 was assigned if the
output was NULL else we assigned a score of 0.
6.1 Translation System
We used the Moses toolkit (Koehn et al, 2007) to
build an English-Hindi statistical machine transla-
tion system. The system was trained on a collec-
tion of 150, 000 English and Hindi parallel sentences
sourced from a publishing house. The 150, 000 sen-
tences were on a varied range of subjects such as
news, literature, history etc. Apart from this the
training data also contained an aligned parallel cor-
pus of English and Hindi FAQs. The FAQs were
collected from government websites on topics such
as health, education, travel services etc.
Since an MT system trained solely on a collection
of sentences would not be very accurate in translat-
ing questions, we trained the system on an English-
Hindi parallel question corpus. As it was difficult
to find a large collection of parallel text consisting
of questions, we created a small collection of par-
allel questions using 240 FAQs and multiplied them
to create a parallel corpus of 50, 000 sentences. This
set was added to the training data and this helped fa-
miliarize the language model and phrase tables used
by the MT systems to questions. Thus in total the
MT system was trained on a corpus of 200, 000 sen-
tences.
Experiment 1 and 2 form the baseline against
which we evaluated our system. For our experi-
ments the lexical translation probabilities generated
by Moses toolkit were used to build the word trans-
lation model. In Experiment 1 the threshold ? de-
scribed in Equation 5 is set to 1. In Experiment 2
and 3 this is set to 0.5. The Hindi FAQ collection
was indexed using Lucene and a domain dictionary
Dh was created from the Hindi words in the FAQ
collection.
6.2 System Evaluation
We perform three sets of experiments to show how
each stage of the algorithm contributes in improving
the overall results.
6.2.1 Experiment 1
For Experiment 1 the threshold ? in Equation 5
is set to 1 i.e. we consider only those tokens in the
query which belong to the dictionary. This setup il-
lustrates the case when no noise handling is done.
The results are reported in Figure 6.
6.2.2 Experiment 2
For Experiment 2 the noisy SMS query was
cleaned using the following approach. Given a noisy
token in the SMS query it?s similarity (Equation 1)
with each word in the Dictionary is calculated. The
noisy token is replaced with the Dictionary word
with the maximum similarity score. This gives us
a clean English query.
For each token in the cleaned English SMS query,
we create a list of possible Hindi translations of the
token using the statistical translation table. Each
Hindi word was assigned a weight according to
Equation 4. The Pruning algorithm in Section 5 was
then applied to get the best matching FAQ.
6.2.3 Experiment 3
In this experiment, for each token in the noisy En-
glish SMS we obtain a list of possible English vari-
ations. For each English variation a corresponding
set of Hindi words from the statistical translation ta-
ble was obtained. Each Hindi word was assigned
a weight according to Equation 4. As described in
Section 5.2, all Hindi words obtained from English
variations of a given SMS token are merged to create
93
Experiment 1 Experiment 2 Experiment 3
MRR Score 0.41 0.68 0.83
Table 1: MRR Scores
F1 Score
Expt 1 (Baseline 1) 0.23
Expt 2 (Baseline 2) 0.68
Expt 3 (Proposed Method) 0.72
Table 2: F1 Measure
a list of Hindi words sorted in terms of their weight.
The Pruning algorithm as described in Section 5 was
then applied to get the best matching FAQ.
We evaluated our system using two different cri-
teria. We used MRR (Mean reciprocal rank) and
the best matching accuracy. Mean reciprocal rank
is used to evaluate a system by producing a list of
possible responses to a query, ordered by probabil-
ity of correctness. The reciprocal rank of a query
response is the multiplicative inverse of the rank of
the first correct answer. The mean reciprocal rank
is the average of the reciprocal ranks of results for a
sample of queries Q.
MRR = 1/|Q|
Q?
i=1
1/ranki (7)
Best match accuracy can be considered as a spe-
cial case of MRR where the size of the ranked list is
1. As the SMS based FAQ retrieval system will be
used via mobile phones where screen size is a ma-
jor constraint it is crucial to have the correct result
on the top. Hence in our settings the best match ac-
curacy is a more relevant and stricter performance
evaluation measure than MRR.
Table 1 compares the MRR scores for all three
experiments. Our method reports the highest MRR
of 0.83. Figure 6 shows the performance using the
strict evaluation criterion of the top result returned
being correct.
We also experimented with different values of
the threshold for Score(Q) (Section 5.3). The ROC
curve for various threshold is shown in Figure 7. The
result for both in-domain and out-of-domain queries
for the three experiments are shown in Figure 6 for
Score(Q) = 8. The F1 Score for experiments 1, 2 and
3 are shown in Table 2.
Figure 6: Comparison of results
Figure 7: ROC Curve for Score(Q)
6.3 Measuring noise level in SMS queries
In order to quantify the level of noise in the col-
lected SMS data, we built a character-level language
model(LM) using the questions in the FAQ data-set
(vocabulary size is 70) and computed the perplexity
of the language model on the noisy and the cleaned
SMS test-set. The perplexity of the LM on a cor-
pus gives an indication of the average number of bits
needed per n-gram to encode the corpus. Noise re-
Cleaned SMS Noisy SMS
English FAQ collection
bigram 16.64 55.19
trigram 9.75 69.41
Table 3: Perplexity for Cleaned and Noisy SMS
94
sults in the introduction of many previously unseen
n-grams in the corpus. Higher number of bits are
needed to encode these improbable n-grams which
results in increased perplexity. From Table 3 we can
see the difference in perplexity for noisy and clean
SMS data for the English FAQ data-set. Large per-
plexity values for the SMS dataset indicates a high
level of noise.
For each noisy SMS query e.g. ?hw 2 prvnt ty-
phd? we manually created a clean SMS query ?how
to prevent typhoid?. A character level language
model using the questions in the clean English FAQ
dataset was created to quantify the level of noise in
our SMS dataset. We computed the perplexity of the
language model on clean and noisy SMS queries.
7 Conclusion
There has been a tremendous increase in information
access services using SMS based interfaces. How-
ever, these services are limited to a single language
and fail to scale for multilingual QA needs. The
ability to query a FAQ database in a language other
than the one for which it was developed is of great
practical significance in multilingual societies. Au-
tomatic cross-lingual QA over SMS is challenging
because of inherent noise in the query and the lack
of cross language resources for noisy processing. In
this paper we present a cross-language FAQ retrieval
system that handles the inherent noise in source lan-
guage to retrieve FAQs in a target language. Our sys-
tem does not require an end-to-end machine transla-
tion system and can be implemented using a sim-
ple dictionary which can be static or constructed
statistically using a moderate sized parallel corpus.
This side steps the problem of building full fledged
translation systems but still enabling the system to
be scaled across multiple languages quickly. We
present an efficient algorithm to search and match
the best question in the large FAQ corpus of tar-
get language for a noisy input question. We have
demonstrated the effectiveness of our approach on a
real life FAQ corpus.
References
Sreangsu Acharyya, Sumit Negi, L Venkata Subrama-
niam, Shourya Roy. 2009. Language independent
unsupervised learning of short message service di-
alect. International Journal on Document Analysis
and Recognition, pp. 175-184.
Aiti Aw, Min Zhang, Juan Xiao, and Jian Su. 2006. A
phrase-based statistical model for SMS text normaliza-
tion. In Proceedings of COLING-ACL, pp. 33-40.
Peter F. Brown, Vincent J.Della Pietra, Stephen A. Della
Pietra, Robert. L. Mercer 1993. The Mathematics of
Statistical Machine Translation: Parameter Estimation
Computational Linguistics, pp. 263-311.
Jeunghyun Byun, Seung-Wook Lee, Young-In Song,
Hae-Chang Rim. 2008. Two Phase Model for SMS
Text Messages Refinement. AAAI Workshop on En-
hanced Messaging.
Monojit Choudhury, Rahul Saraf, Vijit Jain, Animesh
Mukherjee, Sudeshna Sarkar, Anupam Basu. 2007.
Investigation and modeling of the structure of texting
language. International Journal on Document Analy-
sis and Recognition, pp. 157-174.
Philipp Cimiano, Antje Schultz, Sergej Sizov, Philipp
Sorg, Steffen Staab. 2009. Explicit versus latent con-
cept models for cross-language information retrieval.
In Proceeding of IJCAI, pp. 1513-1518.
Danish Contractor, Tanveer A. Faruquie, L. Venkata Sub-
ramaniam. 2010. Unsupervised cleansing of noisy
text. In Proceeding of COLING 2010: Posters, pp.
189-196.
R. Fagin, A. Lotem, and M. Naor. 2001. Optimal aggre-
gation algorithms for middleware. In Proceedings of
the 20th ACM SIGMOD-SIGACT-SIGART symposium
on Principles of database systems, pp. 102-113.
Yijue How and Min-Yen Kan. 2005. Optimizing pre-
dictive text entry for short message service on mobile
phones. In M. J. Smith and G. Salvendy (Eds.) Proc. of
Human Computer Interfaces International,Lawrence
Erlbaum Associates
Valentin Jijkoun and Maarten de Rijke. 2005. Retrieving
answers from frequently asked questions pages on the
web. In Proceedings of the Tenth ACM Conference on
Information and Knowledge Management,CIKM, pp.
76-83.
Catherine Kobus, Francois Yvon and Grraldine Damnati.
2008. Normalizing SMS: Are two metaphors better
than one? In Proceedings of COLING, pp. 441-448.
Philipp Koehn, Hieu Hoang, Alexandra Birch Mayne,
Christopher Callison-Burch, Marcello Federico,
Nicola Bertoldi, Brooke Cowan, Wade Shen, Chris-
tine Moran, Richard Zens, Chris Dyer, Ondrej Bojar,
Alexandra Constantin, Evan Herbst 2007. Moses:
Open source toolkit for statistical machine translation.
Annual Meeting of the Association for Computation
Linguistics (ACL), Demonstration Session .
Sunil Kumar Kopparapu, Akhilesh Srivastava and Arun
Pande. 2007. SMS based Natural Language Interface
95
to Yellow Pages Directory. In Proceedings of the 4th
international conference on mobile technology, appli-
cations, and systems and the 1st international sympo-
sium on Computer human interaction in mobile tech-
nology, pp. 558-563 .
Govind Kothari, Sumit Negi, Tanveer Faruquie, Venkat
Chakravarthy and L V Subramaniam 2009. SMS
based Interface for FAQ Retrieval. Annual Meeting
of the Association for Computation Linguistics (ACL).
I. D. Melamed. 1999. Bitext maps and alignment via pat-
tern recognition. Computational Linguistics, pp. 107-
130.
Guimier de Neef, Emilie, Arnaud Debeurme, and
Jungyeul Park. 2007. TILT correcteur de SMS : Eval-
uation et bilan quantitatif. In Actes de TALN, pp. 123-
132.
Douglas W. Oard, Funda Ertunc. 2002. Translation-
Based Indexing for Cross-Language Retrieval In Pro-
ceedings of the ECIR, pp. 324-333.
A. Pirkola 1998. The Effects of Query Structure
and Dictionary Setups in Dictionary-Based Cross-
Language Information Retrieval SIGIR ?98: Proceed-
ings of the 21st Annual International ACM SIGIR Con-
ference on Research and Development in Information
Retrieval , pp. 55-63.
E. Prochasson, C. Viard-Gaudin, and E. Morin. 2007.
Language models for handwritten short message ser-
vices. In Proceedings of the 9th International Confer-
ence on Document Analysis and Recognition, pp. 83-
87.
Rudy Schusteritsch, Shailendra Rao, Kerry Rodden.
2005. Mobile Search with Text Messages: Designing
the User Experience for Google SMS. In Proceedings
of ACM SIGCHI, pp. 1777-1780.
Satoshi Sekine, Ralph Grishman. 2003. Hindi-English
cross-lingual question-answering system. ACM Trans-
actions on Asian Language Information Processing,
pp. 181-192.
E. Sneiders. 1999. Automated FAQ Answering: Contin-
ued Experience with Shallow Language Understand-
ing Question Answering Systems. Papers from the
1999 AAAI Fall Symposium. Technical Report FS-99-
02, AAAI Press, pp. 97-107.
W. Song, M. Feng, N. Gu, and L. Wenyin. 2007. Ques-
tion similarity calculation for FAQ answering. In Pro-
ceeding of SKG 07, pp. 298-301.
X. Xue, J. Jeon, and W.B Croft. 2008. Retrieval Models
for Question and Answer Archives. In Proceedings of
SIGIR, pp. 475-482.
96
