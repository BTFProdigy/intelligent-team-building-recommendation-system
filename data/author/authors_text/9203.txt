Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 1049?1056
Manchester, August 2008
Chinese Dependency Parsing with Large Scale Automatically 
Constructed Case Structures 
Kun Yu 
Graduate School of Infor-
matics, 
Kyoto University, Japan 
kunyu@nlp.kuee.kyo
to-u.ac.jp 
Daisuke Kawahara 
National Institute of Informa-
tion and Communications 
Technology, Japan 
dk@nict.go.jp 
Sadao Kurohashi 
Graduate School of Infor-
matics, 
Kyoto University, Japan 
kuro@i.kyoto-
u.ac.jp 
 
Abstract 
This paper proposes an approach using 
large scale case structures, which are 
automatically constructed from both a 
small tagged corpus and a large raw cor-
pus, to improve Chinese dependency 
parsing. The case structure proposed in 
this paper has two characteristics: (1) it 
relaxes the predicate of a case structure to 
be all types of words which behaves as a 
head; (2) it is not categorized by semantic 
roles but marked by the neighboring 
modifiers attached to a head. Experimen-
tal results based on Penn Chinese Tree-
bank show the proposed approach 
achieved 87.26% on unlabeled attach-
ment score, which significantly outper-
formed the baseline parser without using 
case structures. 
1 Introduction 
Case structures (i.e. predicate-argument struc-
tures) represent what arguments can be attached 
to a predicate, which are very useful to recognize 
the meaning of natural language text. Research-
ers have applied case structures to Japanese syn-
tactic analysis and improved parsing accuracy 
successfully (Kawahara and Kurohashi, 2006(a); 
Abekawa and Okumura, 2006). However, few 
works focused on using case structures in Chi-
nese parsing. Wu (2003) proposed an approach 
to learn the relations between verbs and nouns 
and applied these relations to a Chinese parser. 
Han et al (2004) presented a method to acquire 
                                                 
? 2008. Licensed under the Creative Commons Attri-
bution-Noncommercial-Share Alike 3.0 Unported 
license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved. 
the sub-categorization of Chinese verbs and used 
them in a PCFG parser. 
Normally, case structures are categorized by 
semantic roles for verbs. For example, Kawahara 
and Kurohashi (2006(b)) constructed Japanese 
case structures which were marked by post posi-
tions. Wu (2003) classified the Chinese verb-
noun relations as ?verb-object? and ?modifier-
head?. In this paper, we propose a new type of 
Chinese case structure, which is different from 
those presented in previous work (Wu, 2003; 
Han et al, 2004; Kawahara and Kurohashi, 
2006(a); Abekawa and Okumura, 2006) in two 
aspects:  
(1) It relaxes the predicate of a case structure 
to be all types of words which behaves as a head; 
(2) It is not categorized by semantic roles 
but marked by the neighboring modifiers at-
tached to a head. The sibling modification infor-
mation remembers the parsing history of a head 
node, which is useful to correct the parsing error 
such as a verb ? (see) is modified by two nouns 
?? (film) and ?? (introduction) as objects 
(see Figure 1). 
 
Figure 1. Dependency trees of an example sen-
tence (I see the introduction of a film). 
We automatically construct large scale case 
structures from both a small tagged corpus and a 
large raw corpus. Then, we apply the large scale 
case structures to a Chinese dependency parser to 
improve parsing accuracy.  
The Chinese dependency parser using case 
structures is evaluated by Penn Chinese Tree-
bank 5.1 (Xue et al, 2002). Results show that the 
1049
automatically constructed case structures helped 
increase parsing accuracy by 2.13% significantly.  
The rest of this paper is organized as follows: 
Section 2 describes the proposed Chinese case 
structure and the construction method in detail; 
Section 3 describes a Chinese dependency parser 
using constructed case structures; Section 4 lists 
the experimental results with a discussion in sec-
tion 5; Related work is introduced in Section 6; 
Finally, Section 7 gives a brief conclusion and 
the direction of future work. 
2 Chinese Case Structure and its Con-
struction 
2.1 A New Type of Chinese Case Structure 
We propose a new type of Chinese case structure 
in this paper, which is represented as the combi-
nation of a case pattern and a case element (see 
Figure 2). Case element remembers the bi-lexical 
dependency relation between all types of head-
modifier pairs, which is also recognized in previ-
ous work (Wu, 2003; Han et al, 2004; Kawahara 
and Kurohashi, 2006(a); Abekawa and Okumura, 
2006). Case pattern keeps the pos-tag sequence 
of all the modifiers attached to a head to remem-
ber the parsing history of a head node.  
 
Figure 2. An example of constructed case 
structure. 
2.2 Construction Corpus 
We use 9,684 sentences from Penn Chinese 
Treebank 5.1 as the tagged corpus, and 7,338,028 
sentences written in simplified Chinese from 
Chinese Gigaword (Graff et al, 2005) as the raw 
corpus for Chinese case structure construction.  
Before constructing case structures from the 
raw corpus, we need to get the syntactic analysis 
of it. First, we do word segmentation and pos-
tagging for the sentences in Chinese Gigaword 
by a Chinese morphological  analyzer 
(Nakagawa and Uchimoto, 2007). Then a 
Chinese deterministic syntactic analyzer (Yu et 
al., 2007) is used to parse the whole corpus. 
To guarantee the accuracy of constructed case 
structures, we only use the sentences with less 
than k words from Chinese Gigaword. It is based 
on the assumption that parsing short sentences is 
more accurate than parsing long sentences. The 
performance of the deterministic parser used for 
analyzing Chinese Gigaword (see Figure 3) 
shows smaller k ensures better parsing quality 
but suffers from lower sentence coverage. 
Referring to Figure 2, we set k as 30. 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 3. Performance of the deterministic parser 
with different k on 1,800 sentences2. 
2.3 Case Pattern Construction 
A case pattern consists of a sequence of pos-tags 
indicating the order of all the modifiers attached 
to a head (see Figure 1), which can be repre-
sented as following.  
>=< ?? rnnlmmi posposposposposposcp ],,...,[,],...,,[ 1111
Here, 
lmm pospospos ],...,,[ 11? means the pos-tag 
sequence of the modifiers attached to a head 
from the left side, and 
rnn pospospos ],,...,[ 11 ? means 
the pos-tag sequence of the modifiers attached to 
a head from the right side. 
We use the 33 pos-tags defined in Penn Chi-
nese Treebank (Xue et al, 2002) to describe a 
case pattern, and make following modifications: 
? group common noun, proper noun and 
pronoun together and mark them as ?noun?; 
? group predicative adjective and all the 
other verbs together and mark them as ?verb?; 
? only regard comma, pause, colon and 
semi-colon as punctuations and mark them as 
?punc?, and neglect other punctuations. 
                                                 
2 UAS means unlabeled attachment score (Buchholz and 
Marsi, 2006). The sentences used for this evaluation are 
from Penn Chinese Treebank with gold word segmentation 
and pos-tag. 
0 10 20 30 40 50 60 70 80 90 100 110
20
30
40
50
60
70
80
90
100
%
k
 UAS
 Sentence Coverage
1050
? group cardinal number and ordinal number 
together and mark them as ?num?; 
? keep the original definition for other pos-
tags but label them by new tags, such as labeling 
?P? as ?prep? and labeling ?AD? as ?adv?. 
The task of case pattern construction is to ex-
tract cpi for each head from both the tagged cor-
pus and the raw corpus. As we will introduce 
later, the Chinese dependency parser using case 
structures applies CKY algorithm for decoding. 
Thus the following substrings of cpi are also ex-
tracted for each head as horizontal Markoviza-
tion during case pattern construction. 
],1[],,1[                                                
],,...,[,],...,,[ 1111
njmk
pospospospospospos rjjlkk
???
>< ??  
],1[  ,],...,,[ 11 mkpospospos lkk ??>< ?  
],1[  ,],,...,[ 11 njpospospos rjj ??>< ?  
2.4 Case Element Construction 
As introduced in Section 2.1, a case element 
keeps the lexical preference between a head and 
its modifier. Therefore, the task of case element 
construction is to extract head-modifier pairs 
from both the tagged corpus and the raw corpus.  
Although only the sentences with less than k 
(k=30) words from Chinese Gigaword are used 
as raw corpus to guarantee the accuracy, there 
still exist some dependency relations with low 
accuracy in these short sentences because of the 
non-perfect parsing quality. Therefore, we apply 
a head-modifier (HM) classifier to the parsed 
sentences from Chinese Gigaword to further ex-
tract head-modifier pairs with high quality. This 
HM classifier is based on SVM classification. 
Table 1 lists the features used in this classifier.  
Feature Description 
Poshead/ 
Posmod 
Pos-tag pair of head and modifier 
Distance Distance between head and modifier 
HasComma If there exists comma between head and  modifier, set as 1; otherwise as 0 
HasColon If there exists colon between head and modifier, set as 1; otherwise as 0 
HasSemi If there exists semi-colon between head and modifier, set as 1; otherwise as 0 
Table 1. Features for HM classifier. 
The HM classifier is trained on 3500 sentences 
from Penn Chinese Treebank with gold-standard 
word segmentation and pos-tag. All the sentences 
are parsed by the same Chinese deterministic 
parser used for Chinese Gigaword analysis. The 
correct dependency relations created by the 
parser are looked as positive examples and the 
left dependency relations are used as negative 
examples. TinySVM 3  is selected as the SVM 
toolkit. A polynomial kernel is used and degree 
is set as 2. Tested on 346 sentences, which are 
from Penn Chinese Treebank and parsed by the 
same deterministic parser with gold standard 
word segmentation and pos-tag, this HM classi-
fier achieved 96.77% on precision and 46.35% 
on recall. 
3 A Chinese Dependency Parser Using 
Case Structures 
3.1 Parsing Model 
We develop a lexicalized Chinese dependency 
parser to use constructed case structures. This 
parser gives a probability P(T|S) to each possible 
dependency tree T of an input sentence 
S=w1,w2,?,wn (wi is a node representing a word 
with its pos-tag), and outputs the dependency 
tree T* that maximizes P(T|S) (see equation 1). 
CKY algorithm is used to decode the dependency 
tree from bottom to up. 
)|(maxarg STPT
T
=?  (1)
To use case structures, P(T|S) is divided into 
two parts (see equation 2): the probability of a 
sentence S generating a root node wROOT, and the 
product of the probabilities of a node wi generat-
ing a case structure CSi. 
?
=
?=
m
i
iiROOT wCSPSwPSTP
1
)|()|()|(  (2)
As introduced in Section 2, a case structure 
CSi is composed of a case pattern cpi and a case 
element cmi. Thus 
),|()|(                  
)|,()|(
iiiii
iiiii
cpwcmPwcpP
wcmcpPwCSP
?=
=  
(3)
A case element cmi consists of a set of de-
pendencies {Dj}, in which each Dj is a tuple <wj, 
disj, commaj>. Here wj means a modifier node, 
disj means the distance between wj and its head, 
and commaj means the number of commas be-
tween wj and its head. Assuming any Dj and Dk 
are independent of each other when they belong 
to the same case element, P(cmi|wi,cpi) can be 
written as  
),|,,(                        
),|(),|(
iij
j
jj
ii
j
jiii
cpwcommadiswP
cpwDPcpwcmP
?
?
=
=
(4)
                                                 
3 http://chasen.org/~taku/software/TinySVM/ 
1051
Finally, P(wj,disj,commaj | wi,cpi) is divided as 
),,|,(),|(
),|,,(
ijijjiij
iijjj
cpwwcommadisPcpwwP
cpwcommadiswP
?=  (5) 
Maximum likelihood estimation is used to es-
timate P(wROOT|S) on training data set with the 
smoothing method used in (Collins, 1996). The 
estimation of P(cpi|wi), P(wj|wi,cpi), and P(disj, 
commaj |wi,wj,cpi) will be introduced in the fol-
lowing subsections. 
3.2 Estimating P(cpi|wi) by Case Patterns 
Three steps are used to estimate P(cpi|wi) by 
maximum likelihood estimation using the con-
structed case patterns: 
? Estimate P(cpi|wi) only by the case pat-
terns from the tagged corpus and represent it as 
)|(? iitagged wcpP ; 
? Estimate P(cpi|wi) only by the case pat-
terns from the raw corpus and represent it as 
)|(? iiraw wcpP ; 
? Estimate P(cpi|wi) by equation 6, in which 
?pattern is calculated by equation 7 to set proper 
ratio for the probabilities estimated by the case 
patterns from different corpora.  
)|(?)1()|(?
)|(?
iirawpatterniitaggedpattern
ii
wcpPwcpP
wcpP
??+?= ?? (6)
1++++
+++=
rawrawtaggedtagged
rawrawtaggedtagged
pattern ????
?????  
(7)
In equation 7, ?tagged and ?raw mean the occur-
rence of a lexicalized node wi=<lexi, posi> gener-
ating cpi in the tagged or raw corpus, ?tagged and 
?raw mean the occurrence of a back-off node 
wi=<posi> generating cpi in the tagged or raw 
corpus. To overcome the data sparseness prob-
lem, we not only apply the smoothing method 
used in (Collins, 1996) for a lexicalized head to 
back off it to its part-of-speech, but also assign a 
very small value to P(cpi|wi) when there is no cpi 
modifying wi in the constructed case patterns. 
3.3 Estimating P(wj|wi,cpi) and P(disj, com-
maj |wi,wj,cpi) by Case Elements 
To estimate P(wj|wi,cpi) and P(disj, commaj 
|wi,wj,cpi) by maximum likelihood estimation, we 
also use three steps: 
? Estimate the two probabilities only by the 
case elements from the tagged corpus and repre-
sent them as ),|(? iijtagged cpwwP  and 
),,|,(? ijijjtagged cpwwcommadisP ; 
? Estimate the two probabilities only by the 
case elements from the raw corpus, and represent 
them as ),|(? iijraw cpwwP  and 
),,|,(? ijijjraw cpwwcommadisP ; 
? Estimate P(wj|wi,cpi) and P(disj, commaj 
|wi,wj,cpi) by equation 8 and equation 9. 
),|(?)1(   
 ),|(?
),|(?
iijrawelement
iijtaggedelement
iij
cpwwP
cpwwP
cpwwP
??
+?=
?
?  
(8)
),,|,(?)1(   
),,|,(?
),,|,(?
ijijjrawelement
ijijjtaggedelement
ijijj
cpwwcommadisP
cpwwcommadisP
cpwwcommadisP
??
+?=
?
?  
(9)
The smoothing method used in (Collins, 1996) 
is applied during estimation.  
 
 
 
 
 
 
 
 
 
 
 
 
Figure 4. Parsing accuracy with different ?element 
on development data set. 
In order to set proper ratio for the probabilities 
estimated by the case elements from different 
corpora, we use a parameter ?element in equation 8 
and 9. The appropriate setting (?element =0.4) is 
learned by a development data set (see Figure 4). 
4 Evaluation Results 
4.1 Experimental Setting 
We use Penn Chinese Treebank 5.1 as data set to 
evaluate the proposed approach. 9,684 sentences 
from Section 001-270 and 400-931, which are 
also used for constructing case structures, are 
used as training data. 346 sentences from Section 
271-300 are used as testing data. 334 sentences 
from Section 301-325 are used as development 
data. Penn2Malt4 is used to transfer the phrase 
structure of Penn Chinese Treebank to depend-
ency structure. Gold-standard word segmentation 
and pos-tag are applied in all the experiments. 
                                                 
4 http://w3.msi.vxu.se/~nivre/research/Penn2Malt.html 
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.1
86.0
86.5
87.0
87.5
88.0
88.5
 
 
U
A
S 
(%
)
?
element
1052
Unlabeled attachment score (UAS) (Buchholz 
and Marsi, 2006) is used as evaluation metric. 
Because of the difficulty of assigning correct 
head to Chinese punctuation, we calculate UAS 
only on the dependency relations in which the 
modifier is not punctuation. 
4.2 Results 
Three parsers were evaluated in this experiment: 
? ?baseline?: a parser not using case struc-
tures, where P(T|S) is calculated by equation 11 
and P(wj|wi) and P(disj, commaj |wi,wj) are esti-
mated by training data set only.  
?
?
??
??
??=
?=
jinji
jijjijROOT
jinji
ijjjROOT
wwcommadisPwwPSwP
wcommadiswPSwP
STP
],,1[,
],,1[,
)),|,()|(()|(
)|,,()|(
)|(
(11) 
? ?w/ case elem?: a parser only using case 
element, which also calculates P(T|S) by equa-
tion 11 but estimates P(wj|wi) and P(disj, commaj 
|wi,wj) by constructed case elements.  
? ?proposed?: the parser introduced in Sec-
tion 3, which uses both case elements and case 
patterns. 
The evaluation results on testing data set (see 
Table 2) shows the proposed parser achieved 
87.26% on UAS, which was 2.13% higher than 
that of the baseline parser. This improvement is 
regarded as statistically significant (McNemar?s 
test: p<0.0005). Besides, Table 2 shows only us-
ing case elements increased parsing accuracy by 
1.30%. It means both case elements and case pat-
terns gave help to parsing accuracy, and case 
elements contributed more in the proposed ap-
proach. 
Parsing 
model baseline w/ case elem proposed 
UAS (%) 85.13 86.43 (+1.30) 87.26 (+2.13)
Table 2. Parsing accuracy of different parsing 
models. 
Figure 5 and Figure 6 show the dependency 
trees of two example sentences created by both 
the baseline parser and the proposed parser. In 
Figure 5, the baseline parser incorrectly assigned 
??/NN (signing) as the head of ??/NN (co-
operation). However, after using the case ele-
ment ??/NN (project) ? ??/NN, the cor-
rect head of ??/NN was found by the pro-
posed parser. Figure 6 shows the baseline parser 
recognized ??/VV (opening) as the head of ?
/P (as) incorrectly. But in the proposed parser, 
the probability of ??/VV generating the case 
pattern ?[prep, punc, prep]l? was much lower 
than that of ??/VV  generating the case pattern 
?[prep]l?. Therefore, the proposed parser rejected 
the incorrect dependency that?/P modified?
?/VV  and got the correct head of ?/P as ??
/VV (show) successfully. 
5 Discussion 
5.1 Influence of the Number of Case Struc-
tures on Parsing Accuracy 
During case structure construction, we only used 
the sentences with less than k (k=30) words from 
Chinese Gigaword as the raw corpus. Enlarging k 
will introduce more sentences from Chinese Gi-
gaword and increase the number of case struc-
tures. Table 4 lists the number of case structures 
and parsing accuracy of the proposed parser on 
testing data set with different k5. It shows enlarg-
ing the number of case structures is a possible 
way to increase parsing accuracy. But simply 
setting larger k did not help parsing, because it 
decreased the parsing accuracy of Chinese Gi-
gaword and consequently decreased the accuracy 
of constructed case structures. Using good parse 
selection (Reichart and Rappoport, 2007; Yates 
et al, 2006) on the syntactic analysis of Chinese 
Gigaword is a probable way to construct more 
case structures without decreasing their accuracy. 
We will consider about it in the future. 
k 10 20 30 40 
# of Case Ele-
ment (M) 0.66 1.14 1.81 2.75
# of Case Pat-
tern (M) 0.57 1.55 3.91 8.48
UAS (%) 85.16 86.42 87.26 87.07
Table 4. Case structure number and parsing 
accuracy with different k. 
5.2 Influence of the Case Structure Con-
struction Corpus on Parsing Accuracy 
We also evaluated the proposed parser on testing 
data set using case structures constructed from 
different corpora.  
Results (see Table 5) show that parsing accu-
racy was improved greatly only when using case 
structures constructed from both the two corpora. 
The case structures constructed from either of a 
                                                 
5 Considering about the time expense of case structure con-
struction, we only did test for k ?40. 
1053
single corpus only gave a little help to parsing. It 
is because among all the case structures used 
during testing (see Table 6), 19.57% case ele-
ments were constructed from the tagged corpus 
only and 54.18% case patterns were constructed 
from the raw corpus only. The incorrect head-
modifier pairs extracted from Chinese Gigaword 
is a possible reason for the fact that some case 
elements only existing in the tagged corpus. En-
hancing good parse selection on Chinese Giga-
word could improve the quality of extracted 
head-modifier pairs and solve this problem. In 
addition, the strict definition of case pattern is a 
probable reason that makes more than half of the 
case patterns only exist in the raw corpus and 
18.18% case patterns exist in neither of the two 
corpora. We will modify the representation of 
case pattern to make it more flexible to the num-
ber of modifiers to resolve this issue in the future. 
Corpus Tagged Raw Tagged+Raw
UAS (%) 85.25 85.90 87.26 
Table 5. Parsing accuracy with case structures 
constructed from different corpora. 
Corpus Tagged Raw Tagged+Raw None
% of case 
element  19.57 8.95 68.07 3.41
% of case 
pattern 0.03 54.18 27.61 18.18
Table 6. Ratio of case structures constructed 
from different corpora. 
 
Figure 5. Dependency trees of an example sentence (The signing of China-US cooperation high tech 
project ?). 
?/P ??/NN  ?/PU  ?/P  ???/NN  ??/VV ?/DEC  ???/NN  ?/AD  ??/VV  ... 
(a) dependency tree created by the baseline parser
(b) dependency tree created by the proposed parser
?/P ??/NN  ?/PU  ?/P  ???/NN  ??/VV  ?/DEC  ???/NN  ?/AD  ??/VV ... 
 
Figure 6. Dependency trees of an example sentence (As introduced, the exhibition opening in the 
stadium will show?). 
5.3 Parsing Performance with Real Pos-tag 
Gold standard word segmentation and pos-tag 
are applied in previous experiments. However, 
parsing accuracy will be affected by the incorrect 
word segmentation and pos-tag in the real appli-
cations. Currently, the best performance of Chi-
nese word segmentation has achieved 99.20% on 
F-score, but the best accuracy of Chinese pos-
tagging was 96.89% (Jin and Chen, 2008). 
Therefore, we think pos-tagging is more crucial 
for applying parser in real task compared with 
word segmentation. Considering about this, we 
evaluated the parsing models introduced in Sec-
tion 4 with real pos-tag in this experiment. 
 
 
 
Parsing model baseline proposed 
UAS (%) 80.91 82.90 (+1.99) 
Table 7. Parsing accuracy of different parsing 
models with real pos-tag. 
An HMM-based pos-tagger is used to get pos-
tag for testing sentences with gold word segmen-
tation. The pos-tagger was trained on the same 
training data set described in Section 4.1 and 
achieved 93.70% F-score on testing data set. Re-
sults (see Table 7) show that even if with real 
pos-tags, the proposed parser still outperformed 
the baseline parser significantly. However, the 
results in Table 7 indicate that incorrect pos-tag 
affected the parsing accuracy of the proposed 
parser greatly. Some researchers integrated pos-
1054
tagging into parsing and kept n-best pos-tags to 
reduce the effect of pos-tagging errors on parsing 
accuracy (Cao et al, 2007). We will also con-
sider about this in our future work. 
6 Related Work 
To our current knowledge, there were few works 
about using case structures in Chinese parsing, 
except for the work of Wu (2003) and Han et al 
(2004). Compared with them, our proposed ap-
proach presents a new type of case structures for 
all kinds of head-modifier pairs, which not only 
recognizes bi-lexical dependency but also re-
members the parsing history of a head node. 
Parsing history has been used to improve pars-
ing accuracy by many researchers (Yamada and 
Matsumoto, 2003; McDonald and Pereira, 2006). 
Yamada and Matsumoto (2003) showed that 
keeping a small amount of parsing history was 
useful to improve parsing performance in a shift-
reduce parser. McDonald and Pereira (2006) ex-
panded their first-order spanning tree model to be 
second-order by factoring the score of the tree 
into the sum of adjacent edge pair scores. In our 
proposed approach, the case patterns remember 
the neighboring modifiers for a head node like 
McDonald and Pereira?s work. But it keeps all 
the parsing histories of a head, which is different 
from only keeping adjacent two modifiers in 
(McDonald and Pereira, 2006). Besides, to use 
the parsing histories in CKY decoding, our ap-
proach applies horizontal Markovization during 
case pattern construction. In general, the success 
of using case patterns in Chinese parsing in his 
paper proves again that keeping parsing history is 
crucial to improve parsing performance, no mat-
ter in which way and to which parsing model it is 
applied. 
There were also some works that handled lexi-
cal preference for Chinese parsing in other ways. 
For example, Cheng et al (2006) and Hall et al 
(2007) applied shift-reduce deterministic parsing 
to Chinese. Sagae and Tsujii (2007) generalized 
the standard deterministic framework to prob-
abilistic parsing by using a best-first search strat-
egy. In these works, lexical preferences were 
introduced as features for predicting parsing ac-
tion. Besides, Bikel and Chiang (2000) applied 
two lexicalized parsing models developed for 
English to Penn Chinese Treebank. Wang et al 
(2005) proposed a completely lexicalized bot-
tom-up generative parsing model to parse Chi-
nese, in which a word-similarity-based smooth-
ing was introduced to replace part-of-speech 
smoothing. 
7 Conclusion and Future Work 
This paper proposes an approach to use large 
scale case structures, which are automatically 
constructed from both a small tagged corpus and 
the syntactic analysis of a large raw corpus, to 
improve Chinese dependency parsing. The pro-
posed case structures not only recognize the lexi-
cal preference between all types of head-modifier 
pairs, but also keep the parsing history of a head 
word. Experimental results show the proposed 
approach improved parsing accuracy signifi-
cantly. Besides, although we only apply the pro-
posed approach to Chinese dependency parsing 
currently, the same idea could be adapted to 
other languages easily because it doesn?t use any 
language specific knowledge. 
There are several future works under consid-
eration, such as modifying the representation of 
case patterns to make it more robust, enhancing 
good parse selection on the analysis of raw cor-
pus, and integrating pos-tagging into parsing 
model.  
References 
T.Abekawa and M.Okumura. 2006. Japanese De-
pendency Parsing Using Co-occurrence Informa-
tion and a Combination of Case Elements. In Pro-
ceedings of   the joint conference of the 
International Committee on Computational Lin-
guistics and the Association for Computational 
Linguistics 2006. pp. 833-840. 
D.Bikel. 2004. Intricacies of Collins? Parsing Model. 
Computational Linguistics, 30(4): 479-511. 
D.Bikel and D.Chiang. 2000. Two Statistical Parsing 
Models Applied to the Chinese Treebank. In Pro-
ceedings of the 2nd Chinese Language Processing 
Workshop. pp. 1-6. 
S.Buchholz and E.Marsi. 2006. CoNLL-X Shared 
Task on Multilingual Dependency Parsing. In Pro-
ceedings of the 10th Conference on Computational 
Natural Language Learning.  
H.Cao et al. 2007. Empirical Study on Parsing Chi-
nese Based on Collins? Model. In Proceedings of 
the 10th Conference of the Pacific Association for 
Computational Linguisitcs. pp. 113-119. 
Y.Cheng, M.Asahara and Y.Matsumoto. 2006. Multi-
lingual Dependency Parsing at NAIST. In Proceed-
ings of the 10th Conference on Computational 
Natural Language Learning. pp. 191-195. 
1055
M.Collins. 1996. A New Statistical Parser Based on 
Bigram Lexical Dependencies. In Proceedings of 
the 34th Annual Meeting of the Association for 
Computational Linguistics. pp. 184-191. 
M.Collins. 1999. Head-Driven Statistical Models for 
Natural Language Parsing. Ph.D Thesis. University 
of Pennsylvania. 
D.Graff et al. 2005. Chinese Gigaword Second Edi-
tion. Linguistic Data Consortium, Philadelphia. 
J.Hall et al 2007. Single Malt or Blended? A Study in 
Multilingual Parser Optimization. In Proceedings 
of the shared task at the Conference on Computa-
tional Natural Language Learning 2007. pp. 933-
939. 
X.Han et al. 2004. Subcategorization Acquisition and 
Evaluation for Chinese Verbs. In Proceedings of 
the 20th International Conference on Computa-
tional Linguistics.  
G.Jin and X.Chen. 2008. The Fourth International 
Chinese Language Processing Bakeoff: Chinese 
Word Segmentation, Named Entity Recognition 
and Chinese Pos Tagging. In Proceedings of the 6th 
SIGHAN Workshop on Chinese Language Process-
ing. 
D.Kawahara and S.Kurohashi. 2006 (a). A Fully-
lexicalized Probabilistic Model for Japanese Syn-
tactic and Case frame Analysis. In Proceedings of 
the Human Language Technology conference - 
North American chapter of the Association for 
Computational Linguistics annual meeting 2006. 
pp. 176-183. 
D.Kawahara and S.Kurohashi. 2006 (b). Case Frame 
Compilation from the Web Using High-
performance Computing. In Proceedings of the 5th 
International Conference on Language Resources 
and Evaluation. 
T.Kudo and Y.Matsumoto. 2002. Japanese Depend-
ency Analysis Using Cascaded Chunking. In Pro-
ceedings of the Conference on Natural Language 
Learning. pp. 29-35. 
R.McDonald and F.Pereira. 2006. Online Learning of 
Approximate Dependency Parsing Algorithm. In 
Proceedings of the 11th Conference of the Euro-
pean Chapter of the Association for Computational 
Linguistics. 
R.McDonald and J.Nivre. 2007. Characterizing the 
Errors of Data-driven Dependency Parsing Models. 
In Proceedings of the Conference on Empirical 
Methods in Natural Language Processing Confer-
ence on Computational Natural Language Learn-
ing 2007. 
T.Nakagawa and K.Uchimoto. 2007. A Hybrid Ap-
proach to Word Segmentation and POS Tagging. 
In Proceedings of the 45th Annual Meeting of the 
Association for Computational Linguistics. pp. 
217-220. 
R.Reichart and A.Rappoport. 2007. An Ensemble 
Method for Selection of High Quality Parses. In 
Proceedings of the 45th Annual Meeting of the As-
sociation for Computational Linguistics. pp. 408-
415. 
K.Sagae and J.Tsujii. 2007. Dependency Parsing and 
Domain Adaptation with LR Models and Parser 
Ensembles. In Proceedings of the shared task at 
the Conference on Computational Natural Lan-
guage Learning 2007. pp. 1044-1050. 
Q.Wang, D.Schuurmans, and D.Lin. 2005. Strictly 
Lexical Dependency Parsing. In Proceedings of the 
9th International Workshop on Parsing Technolo-
gies. pp. 152-159.  
A.Wu. 2003. Learning Verb-Noun Relations to Im-
prove Parsing. In Proceedings of the 2nd SIGHAN 
Workshop on Chinese Language Processing. pp. 
119-124. 
N.Xue, F.Chiou and M.Palmer. 2002. Building a 
Large-Scale Annotated Chinese Corpus. In Pro-
ceedings of the 18th International Conference on 
Computational Linguistics. 
N.Xue and M.Palmer. 2003. Annotating the Proposi-
tions in the Penn Chinese Treebank. In Proceed-
ings of the 2nd SIGHAN Workshop on Chinese Lan-
guage Processing. 
N.Xue and M.Palmer. 2005. Automatic Semantic 
Rule Labeling for Chinese Verbs. In Proceedings 
of the 19th International Joint Conference on Artifi-
cial Intelligence.  
H.Yamada and Y.Matsumoto. 2003. Statistical De-
pendency Analysis with Support Vector Machines. 
In Proceedings of the 7th International Workshop 
on Parsing Technologies. 
A.Yates, S.Schoenmackers, and O.Etzioni. 2006. De-
tecting Parser Errors Using Web-based Semantic 
Filters. In Proceedings of the 2006 Conference on 
Empirical Methods in Natural Language Process-
ing. pp. 27-34. 
J.You and K.Chen. 2004. Automatic Semantic Role 
Assignment for a Tree Structure. In Proceedings of 
the 3rd SIGHAN Workshop on Chinese Language 
Processing. 
K.Yu, S.Kurohashi, and H.Liu. 2007. A Three-step 
Deterministic Parser for Chinese Dependency Pars-
ing. In Proceedings of the Human Language Tech-
nologies: the Annual Conference of the North 
American Chapter of the Association for Computa-
tional Linguistics 2007. pp. 201-204. 
 
1056
Proceedings of NAACL HLT 2007, Companion Volume, pages 201?204,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
A Three-step Deterministic Parser for Chinese Dependency Parsing 
 
Kun Yu Sadao Kurohashi Hao Liu 
Graduate School of Informatics Graduate School of Informatics Graduate School of Information 
Science and Technology 
Kyoto University Kyoto University The University of Tokyo 
kunyu@nlp.kuee.kyoto-u.ac.jp kuro@i.kyoto-u.ac.jp liuhao@kc.t.u-tokyo.ac.jp 
 
Abstract 
  This paper presents a three-step dependency 
parser to parse Chinese deterministically. By divid-
ing a sentence into several parts and parsing them 
separately, it aims to reduce the error propagation 
coming from the greedy characteristic of determi-
nistic parsing. Experimental results showed that 
compared with the deterministic parser which 
parsed a sentence in sequence, the proposed parser 
achieved extremely significant improvement on 
dependency accuracy.  
1 Introduction 
Recently, as an attractive alternative to probabilistic 
parsing, deterministic parsing (Yamada and Matsumoto, 
2003; Nivre and Scholz, 2004) has drawn great attention 
with its high efficiency, simplicity and good accuracy 
comparable to the state-of-the-art generative probabilis-
tic models. The basic idea of deterministic parsing is 
using a greedy parsing algorithm that approximates a 
globally optimal solution by making a sequence of lo-
cally optimal choices (Hall et al, 2006). This greedy 
idea guarantees the simplicity and efficiency, but at the 
same time it also suffers from the error propagation 
from the previous parsing choices to the left decisions.  
For example, given a Chinese sentence, which means 
Paternity test is a test that gets personal identity 
through DNA analysis, and it brings proof for finding 
lost children, the correct dependency tree is shown by 
solid line  (see Figure 1). But, if word ??(through) is 
incorrectly parsed as depending on word ?(is) (shown 
by dotted line), this error will result in the incorrect 
parse of word??(a test) as depending on word ??
(brings) (shown by dotted line).  
This problem exists not only in Chinese, but also in 
other languages. Some efforts have been done to solve 
this problem. Cheng et al (2005) used a root finder to 
divide one sentence into two parts by the root word and 
parsed them separately. But the two-part division is not 
enough when a sentence is composed of several coordi-
nating sub-sentences. Chang et al (2006) applied a 
pipeline framework in their dependency parser to make 
the local predictions more robust. While it did not show 
great help for stopping the error propagation between 
different parsing stages.  
 
Figure 1. Dependency tree of a sentence  (word sequence is top-down) 
This paper focuses on resolving this issue for Chi-
nese. After analyzing the dependency structure of sen-
tences in Penn Chinese Treebank 5.1 (Xue et al, 2002), 
we found an interesting phenomenon: if we define a 
main-root as the head of a sentence, and define a sub-
sentence as a sequence of words separated by punctua-
tions, and the head1 of these words is the child of main-
root or main-root itself, then the punctuations that de-
pend on main-root can be a separator of sub-sentences.  
For example, in the example sentence there are three 
punctuations marked as PU_A, PU_B and PU_C, in 
which PU_B and PU_C depends on main-root but 
PU_A depends on word ??(gets). According to our 
observation, PU_B and PU_C can be used for segment-
ing this sentence into two sub-sentences A and B (cir-
cled by dotted line in Figure 2), where the sub-root of A 
is main-root and the sub-root of B depends on main-root.  
This phenomenon gives us a useful clue: if we divide 
a sentence by the punctuations whose head is main-root, 
then the divided sub-sentences are basically independ-
ent of each other, which means we can parse them sepa-
rately. The shortening of sentence length and the recog-
nition of sentence structure guarantee the robustness of 
deterministic parsing. The independent parsing of each 
sub-sentence also prevents the error-propagation. In 
                                                 
1
 The head of sub-sentence is defined as a sub-root. 
201
addition, because the sub-root depends on main-root or 
is main-root itself, it is easy to combine the dependency 
structure of each sub-sentence to create the final de-
pendency tree. 
 
Figure 2. A segmentation of the sentence in Figure 1 
Based on above analyses, this paper proposes a three-
step deterministic dependency parser for Chinese, which 
works as: 
Step1(Sentence Segmentation): Segmenting a sen-
tence into sub-sentences by punctuations (sub-sentences 
do not contain the punctuations for segmentation); 
Step2(Sub-sentence Parsing): Parsing each sub-
sentence deterministically; 
Step3(Parsing Combination): Finding main-root 
among all the sub-roots, then combining the dependency 
structure of sub-sentences by making main-root as the 
head of both the left sub-roots and the punctuations for 
sentence segmentation. 
2 Sentence Segmentation 
As mentioned in section 1, the punctuations depending 
on main-root can be used to segment a sentence into 
several sub-sentences, whose sub-root depends on main-
root or is main-root. But by analysis, we found only 
several punctuations were used as separator commonly. 
To ensure the accuracy of sentence segmentation, we 
first define the punctuations which are possible for seg-
mentation as valid punctuation, which includes comma, 
period, colon, semicolon, question mark, exclamatory 
mark and ellipsis. Then the task in step 1 is to find 
punctuations which are able to segment a sentence from 
all the valid punctuations in a sentence, and use them to 
divide the sentence into two or more sub-sentences. 
We define a classifier (called as sentence seg-
menter) to classify the valid punctuations in a sentence 
to be good or bad for sentence segmentation. SVM (Se-
bastiani, 2002) is selected as classification model for its 
robustness to over-fitting and high performance.  
Table 1 shows the binary features defined for sen-
tence segmentation. We use a lexicon consisting of all 
the words in Penn Chinese Treebank 5.1 to lexicalize 
word features. For example, if word ? (for) is the 
27150th word in the lexicon, then feature Word1 of 
PU_B (see Figure 2) is ?27150:1?. The pos-tag features 
are got in the same way by a pos-tag list containing 33 
pos-tags, which follow the definition in Penn Chinese 
Treebank. Such method is also used to get word and 
pos-tag features in other modules. 
Table 1. Features for sentence segmenter 
Feature Description 
Wordn/Posn word/pos-tag in different position, n=-2,-1,0,1,2 
Word_left/ 
Pos_left 
word/pos-tag between the first left valid punctua-
tion and current punctuation 
Word_right/ 
Pos_right 
word/pos-tag between current punctuation and 
the first right valid punctuation 
#Word_left/ 
#Word_right 
if the number of words between the first left/right 
valid punctuation and current punctuation is 
higher than 2, set as 1; otherwise set as 0 
V_left/ 
V_right 
if there is a verb between the first left/right valid 
punctuation and current punctuation, set as 1; 
otherwise set as 0 
N_leftFirst/ 
N_rightFirst 
if the left/right neighbor word is a noun, set as 1; 
otherwise set as 0 
P_rightFirst/ 
CS_rightFirst 
if the right neighbor word is a preposi-
tion/subordinating conjunction, set as 1; other-
wise set as 0 
3 Sub-sentence Parsing  
3.1 Parsing Algorithm 
The parsing algorithm in step 2 is a shift-reduce parser 
based on (Yamada and Matsumoto, 2003). We call it as 
sub-sentence parser. 
Two stacks P and U are defined, where stack P keeps 
the words under consideration and stack U remains all 
the unparsed words. All the dependency relations cre-
ated by the parser are stored in queue A.  
At start, stack P and queue A are empty and stack U 
contains all the words. Then word on the top of stack U 
is pushed into stack P, and a trained classifier finds 
probable action for word pair <p,u> on the top of the 
two stacks. After that, according to different actions, 
dependency relations are created and pushed into queue 
A, and the elements in the two stacks move at the same 
time. Parser stops when stack U is empty and the de-
pendency tree can be drawn according to the relations 
stored in queue A.  
Four actions are defined for word pair <p, u>: 
LEFT: if word p modifies word u, then push pu 
into A and push u into P. 
RIGHT: if word u modifies word p, then push up 
into A and pop p. 
REDUCE: if there is no word u? (u??U and u??u) 
which modifies p, and word next to p in stack P is p?s 
head, then pop p. 
SHIFT: if there is no dependency relation between p 
and u, and word next to p in stack P is not p?s head, then 
push u into stack P. 
202
We construct a classifier for each action separately, 
and classify each word pair by all the classifiers. Then 
the action with the highest classification score is se-
lected. SVM is used as the classifier, and One vs. All 
strategy (Berger, 1999) is applied for its good efficiency 
to extend binary classifier to multi-class classifier. 
3.2 Features 
Features are crucial to this step. First, we define some 
features based on local context (see Flocal in Table 2), 
which are often used in other deterministic parsers 
(Yamada and Matsumoto, 2003; Nivre et al, 2006). 
Then, to get top-down information, we add some global 
features (see Fglobal in Table 2). All the features are bi-
nary features, except that Distance is normalized be-
tween 0-1 by the length of sub-sentence.  
Before parsing, we use a root finder (i.e. the sub-
sentence root finder introduced in Section 4) to get 
Rootn feature, and develop a baseNP chunker to get 
BaseNPn feature. In the baseNP chunker, IOB represen-
tation is applied for each word, where B means the word 
is the beginning of a baseNP, I means the word is inside 
of a baseNP, and O means the word is outside of a 
baseNP. Tagging is performed by SVM with One vs. All 
strategy. Features used in baseNP chunking are current 
word, surrounding words and their corresponding pos-
tags. Window size is 5. 
Table 2. Features for sub-sentence parser 
Feature Description 
Wordn/ 
Posn 
word/pos-tag in different position, 
n= P0, P1, P2, U0, U1, U2 (Pi/Ui mean 
the ith position from top in stack P/U) 
Word_childn/ 
Pos_childn 
the word/pos-tag of the children of 
Wordn, n= P0, P1, P2, U0, U1, U2 
Local 
Feature 
(Flocal) 
Distance distance between p and u in sentence 
Rootn 
if Wordn is the sub-root of this sub-
sentence, set as 1; otherwise set as 0 
Global 
Feature 
(Fglobal) BaseNPn baseNP tag of Wordn 
Table 3. Features for sentence/sub-sentence root finder 
Feature Description 
Wordn/Posn words in different position, n=-2,-1,0,1,2 
Word_left/Pos_left wordn/posn where n<-2 
Word_right/Pos_right wordn/posn where n>2 
#Word_left/ 
#Word_right 
if the number of words between the 
start/end of sentence and current word is 
higher than 2, set as 1; otherwise set as 0 
V_left/V_right 
if there is a verb between the start/end of 
sentence and current word, set as 1; oth-
erwise set as 0 
Nounn/Verbn/Adjn 
if the word in different position is a 
noun/verb/adjective, set as 1; otherwise 
set as 0. n=-2,-1,1,2 
Dec_right if the word next to current word in right 
side is ?(of), set as 1; otherwise set as 0 
CC_left 
if there is a coordinating conjunction 
between the start of sentence and current 
word, set as 1; otherwise set as 0 
BaseNPn baseNP tag of Wordn 
4 Parsing Combination 
A root finder is developed to find main-root for parsing 
combination. We call it as sentence root finder. We 
also retrain the same module to find the sub-root in step 
2, and call it as sub-sentence root finder. 
We define the root finding problem as a classification 
problem. A classifier, where we still select SVM, is 
trained to classify each word to be root or not. Then the 
word with the highest classification score is chosen as 
root. All the binary features for root finding are listed in 
Table 3. Here the baseNP chunker introduced in section 
3.2 is used to get the BaseNPn feature. 
5 Experimental Results 
5.1 Data Set and Experimental Setting 
We use Penn Chinese Treebank 5.1 as data set. To 
transfer the phrase structure into dependency structure, 
head rules are defined based on Xia?s head percolation 
table (Xia and Palmer, 2001). 16,984 sentences and 
1,292 sentences are used for training and testing. The 
same training data is also used to train the sentence 
segmenter, the baseNP chunker, the sub-sentence root 
finder, and the sentence root finder. During both train-
ing and testing, the gold-standard word segmentation 
and pos-tag are applied. 
TinySVM is selected as a SVM toolkit. We use a 
polynomial kernel and set the degree as 2 in all the ex-
periments.  
5.2 Three-step Parsing vs. One-step Parsing 
First, we evaluated the dependency accuracy and root 
accuracy of both three-step parsing and one-step parsing. 
Three-step parsing is the proposed parser and one-step 
parsing means parsing a sentence in sequence (i.e. only 
using step 2). Local and global features are used in both 
of them. 
Results (see Table 4) showed that because of the 
shortening of sentence length and the prevention of er-
ror propagation three-step parsing got 2.14% increase 
on dependency accuracy compared with one-step pars-
ing. Based on McNemar?s test (Gillick and Cox, 1989), 
this improvement was considered extremely statistically 
significant (p<0.0001).  In addition, the proposed parser 
got 1.01% increase on root accuracy.  
Table 4. Parsing result of three-step and one-step parsing 
Parsing Strategy Dep.Accu. (%) 
Root Accu. 
(%) 
Avg. Parsing 
Time (sec.) 
One-step Parsing 82.12 74.92 22.13 
Three-step Parsing 84.26 (+2.14) 
75.93 
(+1.01) 
24.27 
(+2.14) 
Then we tested the average parsing time for each sen-
tence to verify the efficiency of proposed parser. The 
average sentence length is 21.68 words. Results (see 
Table 4) showed that compared with one-step parsing, 
the proposed parser only used 2.14 more seconds aver-
203
agely when parsing one sentence, which did not affect 
efficiency greatly. 
To verify the effectiveness of proposed parser on 
complex sentences, which contain two or more sub-
sentences according to our definition, we selected 665 
such sentences from testing data set and did evaluation 
again. Results (see Table 5) proved that our parser 
outperformed one-step parsing successfully.  
Table 5. Parsing result of complex sentence 
Parsing Strategy Dep.Accu. (%) Root Accu. (%) 
One-step Parsing 82.56 78.95 
Three-step Parsing 84.94 (+2.38) 79.25 (+0.30) 
5.3 Comparison with Others? Work 
At last, we compare the proposed parser with Nivre?s 
parser (Hall et al, 2006). We use the same head rules 
for dependency transformation as what were used in 
Nivre?s work. We also used the same training (section 
1-9) and testing (section 0) data and retrained all the 
modules. Results showed that the proposed parser 
achieved 84.50% dependency accuracy, which was 
0.20% higher than Nivre?s parser (84.30%).  
6 Discussion 
In the proposed parser, we used five modules: sentence 
segmenter (step1); sub-sentence root finder (step2); 
baseNP chunker (step2&3); sub-sentence parser (step2); 
and sentence root finder (step3).  
The robustness of the modules will affect parsing ac-
curacy. Thus we evaluated each module separately. Re-
sults (see Table 6) showed that all the modules got rea-
sonable accuracy except for the sentence root finder. 
Considering about this, in step 3 we found main-root 
only from the sub-roots created by step 2. Because the 
sub-sentence parser used in step 2 had good accuracy, it 
could provide relatively correct candidates for main-root 
finding. Therefore it helped decrease the influence of 
the poor sentence root finding to the proposed parser. 
Table 6. Evaluation result of each module 
Module F-score(%) Dep.Accu(%) 
Sentence Segmenter (M1) 88.04 --- 
Sub-sentence Root Finder (M2) 88.73 --- 
BaseNP Chunker (M3) 89.25 --- 
Sub-sentence Parser (M4) --- 85.56 
Sentence Root Finder (M5) 78.01 --- 
Then we evaluated the proposed parser assuming us-
ing gold-standard modules (except for sub-sentence 
parser) to check the contribution of each module to 
parsing. Results (see Table 7) showed that (1) the accu-
racy of current sentence segmenter was acceptable be-
cause only small increase on dependency accuracy and 
root accuracy was got by using gold-standard sentence 
segmentation; (2) the correct recognition of baseNP 
could help improve dependency accuracy but gave a 
little contribution to root accuracy; (3) the accuracy of 
both sub-sentence root finder and sentence root finder 
was most crucial to parsing. Therefore improving the 
two root finders is an important task in our future work. 
Table 7. Parsing result with gold-standard modules 
Gold-standard Module Dep.Accu(%) Root.Accu(%) 
w/o 84.26 75.93 
M1 84.51 76.24 
M1+M2 86.57 80.34 
M1+M2+M3 88.63 80.57 
M1+M2+M3+M5 91.25 91.02 
7 Conclusion and Future Work 
We propose a three-step deterministic dependency 
parser for parsing Chinese. It aims to solve the error 
propagation problem by dividing a sentence into inde-
pendent parts and parsing them separately. Results 
based on Penn Chinese Treebank 5.1 showed that com-
pared with the deterministic parser which parsed a sen-
tence in sequence, the proposed parser achieved ex-
tremely significant increase on dependency accuracy. 
Currently, the proposed parser is designed only for 
Chinese. But we believe it can be easily adapted to other 
languages because no language-limited information is 
used. We will try this work in the future. In addition, 
improving sub-sentence root finder and sentence root 
finder will also be considered in the future. 
Acknowledgement 
We would like to thank Dr. Daisuke Kawahara and Dr. Eiji Aramaki 
for their helpful discussions. We also thank the three anonymous 
reviewers for their valuable comments. 
Reference 
A.Berger. Error-correcting output coding for text classification. 1999. 
In Proceedings of the IJCAI-99 Workshop on Machine Learning 
for Information Filtering. 
M.Chang, Q.Do and D.Roth. 2006. A Pipeline Framework for De-
pendency Parsing. In Proceedings of Coling-ACL 2006. 
Y.Cheng, M.Asahara and Y.Matsumoto. 2005. Chinese Deterministic 
Dependency Analyzer: Examining Effects of Global Features and 
Root Node Finder. In Proceedings of IJCNLP 2005.  
L.Gillick and S.J.Cox. 1989. Some Statistical Issues in the Compari-
son of Speech Recognition Algorithms. In Proceedings of ICASSP.  
J.Hall, J.Nivre and J.Nilsson. 2006. Discriminative Classifiers for 
Deterministic Dependency Parsing. In Proceedings of Coling-ACL 
2006. pp. 316-323. 
J.Nivre and M.Scholz. 2004. Deterministic Dependency Parsing of 
English Text. In Proceedings of Coling 2004. pp. 64-70. 
F.Sebastiani. 2002. Machine learning in automated text categorization. 
ACM Computing Surveys, 34(1): 1-47. 
F.Xia and M.Palmer. 2001. Converting Dependency Structures to 
Phrase Structures. In HLT-2001. 
N.Xue, F.Chiou and M.Palmer. 2002. Building a Large-Scale Anno-
tated Chinese Corpus. In Proceedings of COLING 2002. 
H.Yamada and Y.Matsumoto. 2003. Statistical Dependency Analysis 
with Support Vector Machines. In Proceedings of IWPT. 2003. 
204
Proceedings of NAACL HLT 2009: Short Papers, pages 121?124,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Extracting Bilingual Dictionary from Comparable Corpora with 
Dependency Heterogeneity 
 
Kun Yu Junichi Tsujii 
Graduate School of Information Science and Technology 
The University of Tokyo 
Hongo 7-3-1, Bunkyo-ku, Tokyo, Japan 
{kunyu, tsujii}@is.s.u-tokyo.ac.jp 
 
Abstract 
This paper proposes an approach for bilingual 
dictionary extraction from comparable corpora. 
The proposed approach is based on the obser-
vation that a word and its translation share 
similar dependency relations. Experimental re-
sults using 250 randomly selected translation 
pairs prove that the proposed approach signifi-
cantly outperforms the traditional context-
based approach that uses bag-of-words around 
translation candidates. 
1 Introduction 
Bilingual dictionary plays an important role in many 
natural language processing tasks. For example, ma-
chine translation uses bilingual dictionary to reinforce 
word and phrase alignment (Och and Ney, 2003), cross-
language information retrieval uses bilingual dictionary 
for query translation (Grefenstette, 1998). The direct 
way of bilingual dictionary acquisition is aligning trans-
lation candidates using parallel corpora (Wu, 1994). But 
for some languages, collecting parallel corpora is not 
easy. Therefore, many researchers paid attention to bi-
lingual dictionary extraction from comparable corpora 
(Fung, 2000; Chiao and Zweigenbaum, 2002; Daille and 
Morin, 2008; Robitaille et al, 2006; Morin et al, 2007; 
Otero, 2008), in which texts are not exact translation of 
each other but share common features. 
Context-based approach, which is based on the ob-
servation that a term and its translation appear in similar 
lexical contexts (Daille and Morin, 2008), is the most 
popular approach for extracting bilingual dictionary 
from comparable corpora and has shown its effective-
ness in terminology extraction (Fung, 2000; Chiao and 
Zweigenbaum, 2002; Robitaille et al, 2006; Morin et al, 
2007). But it only concerns about the lexical context 
around translation candidates in a restricted window. 
Besides, in comparable corpora, some words may appear 
in similar context even if they are not translation of each 
other. For example, using a Chinese-English comparable 
corpus from Wikipedia and following the definition in 
(Fung, 1995), we get context heterogeneity vector of 
three words (see Table 1). The Euclidean distance be-
tween the vector of  ????(economics)? and ?econom-
ics? is 0.084. But the Euclidean distance between the 
vector of  ????? and ?medicine? is 0.075. In such 
case, the incorrect dictionary entry ????/medicine? 
will be extracted by context-based approach. 
Table 1. Context heterogeneity vector of words. 
Word Context Heterogeneity Vector
???(economics) (0.185, 0.006) 
economics (0.101, 0.013) 
medicine (0.113,0.028) 
To solve this problem, we investigate a comparable 
corpora from Wikipedia and find the following phe-
nomenon: if we preprocessed the corpora with a de-
pendency syntactic analyzer, a word in source language 
shares similar head and modifiers with its translation in 
target language, no matter whether they occur in similar 
context or not. We call this phenomenon as dependency 
heterogeneity. Based on this observation, we propose an 
approach to extract bilingual dictionary from compara-
ble corpora. Not like only using bag-of-words around 
translation candidates in context-based approach, the 
proposed approach utilizes the syntactic analysis of 
comparable corpora to recognize the meaning of transla-
tion candidates. Besides, the lexical information used in 
the proposed approach does not restrict in a small win-
dow, but comes from the entire sentence. 
We did experiments with 250 randomly selected 
translation pairs. Results show that compared with the 
approach based on context heterogeneity, the proposed 
approach improves the accuracy of dictionary extraction 
significantly. 
2 Related Work  
In previous work about dictionary extraction from com-
parable corpora, using context similarity is the most 
popular one.  
At first, Fung (1995) utilized context heterogeneity 
for bilingual dictionary extraction. Our proposed ap-
proach borrows Fung?s idea but extends context hetero-
geneity to dependency heterogeneity, in order to utilize 
rich syntactic information other than bag-of-words.  
After that, researchers extended context heterogeneity 
vector to context vector with the aid of an existing bilin-
gual dictionary (Fung, 2000; Chiao and Zweigenbaum, 
2002; Robitaille et al, 2006; Morin et al, 2007; Daille 
and Morin, 2008). In these works, dictionary extraction 
121
is fulfilled by comparing the similarity between the con-
text vectors of words in target language and the context 
vectors of words in source language using an external 
dictionary. The main difference between these works 
and our approach is still our usage of syntactic depend-
ency other than bag-of-words. In addition, except for a 
morphological analyzer and a dependency parser, our 
approach does not need other external resources, such as 
the external dictionary. Because of the well-developed 
morphological and syntactic analysis research in recent 
years, the requirement of analyzers will not bring too 
much burden to the proposed approach. 
Besides of using window-based contexts, there were 
also some works utilizing syntactic information for bi-
lingual dictionary extraction. Otero (2007) extracted 
lexico-syntactic templates from parallel corpora first, 
and then used them as seeds to calculate similarity be-
tween translation candidates. Otero (2008) defined syn-
tactic rules to get lexico-syntactic contexts of words, and 
then used an external bilingual dictionary to fulfill simi-
larity calculation between the lexico-syntactic context 
vectors of translation candidates. Our approach differs 
from these works in two ways: (1) both the above works 
defined syntactic rules or templates by hand to get syn-
tactic information. Our approach uses data-driven syn-
tactic analyzers for acquiring dependency relations 
automatically. Therefore, it is easier to adapt our ap-
proach to other language pairs. (2) the types of depend-
encies used for similarity calculation in our approach are 
different from Otero?s work. Otero (2007; 2008) only 
considered about the modification dependency among 
nouns, prepositions and verbs, such as the adjective 
modifier of nouns and the object of verbs. But our ap-
proach not only uses modifiers of translation candidates, 
but also considers about their heads. 
3 Dependency Heterogeneity of Words in 
Comparable Corpora 
Dependency heterogeneity means a word and its trans-
lation share similar modifiers and head in comparable 
corpora. Namely, the modifiers and head of unrelated 
words are different even if they occur in similar context. 
Table 2. Frequently used modifiers (words are not ranked). 
???(economics) economics medicine 
??/micro keynesian physiology
??/macro new Chinese 
??/computation institutional traditional
?/new positive biology 
??/politics classical internal 
??/university labor science 
???/classicists development clinical 
??/development engineering veterinary 
 ??/theory finance western 
??/demonstration international agriculture
For example, Table 2 collects the most frequently 
used 10 modifiers of the words listed in Table 1. It 
shows there are 3 similar modifiers (italic words) be-
tween ????(economics)? and ?economics?. But there 
is no similar word between the modifiers of ????? 
and that of ?medicine?. Table 3 lists the most frequently 
used 10 heads (when a candidate word acts as subject) 
of the three words. If excluding copula, ????? and 
?economics? share one similar head (italic words). But 
????? and ?medicine? shares no similar head.  
Table 3. Frequently used heads  
(the predicate of subject, words are not ranked). 
???(economics) economics medicine 
?/is is is 
??/average has tends 
??/graduate was include 
??/admit emphasizes moved 
?/can non-rivaled means 
??/split became requires 
??/leave assume includes 
?/compare relies were 
??/become can has 
??/emphasize replaces may 
4 Bilingual Dictionary Extraction with De-
pendency Heterogeneity   
Based on the observation of dependency heterogeneity 
in comparable corpora, we propose an approach to ex-
tract bilingual dictionary using dependency heterogene-
ity similarity.  
4.1 Comparable Corpora Preprocessing 
Before calculating dependency heterogeneity similarity, 
we need to preprocess the comparable corpora. In this 
work, we focus on Chinese-English bilingual dictionary 
extraction for single-nouns. Therefore, we first use a 
Chinese morphological analyzer (Nakagawa and Uchi-
moto, 2007) and an English pos-tagger (Tsuruoka et al, 
2005) to analyze the raw corpora. Then we use Malt-
Parser (Nivre et al, 2007) to get syntactic dependency of 
both the Chinese corpus and the English corpus. The 
dependency labels produced by MaltParser (e.g. SUB) 
are used to decide the type of heads and modifiers.  
After that, the analyzed corpora are refined through 
following steps: (1) we use a stemmer1 to do stemming 
for the English corpus. Considering that only nouns are 
treated as translation candidates, we use stems for trans-
lation candidate but keep the original form of their heads 
and modifiers in order to avoid excessive stemming. (2) 
stop words are removed. For English, we use the stop 
word list from (Fung, 1995). For Chinese, we remove 
??(of)? as stop word. (3) we remove the dependencies 
including punctuations and remove the sentences with 
                                                          
1 http://search.cpan.org/~snowhare/Lingua-Stem-0.83/  
122
more than k (set as 30 empirically) words from both 
English corpus and Chinese corpus, in order to reduce 
the effect of parsing error on dictionary extraction.  
4.2 Dependency Heterogeneity Vector Calculation 
Equation 1 shows the definition of dependency hetero-
geneity vector of a word W. It includes four elements. 
Each element represents the heterogeneity of a depend-
ency relation. ?NMOD? (noun modifier), ?SUB? (sub-
ject) and ?OBJ? (object) are the dependency labels 
produced by MaltParser.  
(HNMODHead ,HSUBHead ,HOBJHead ,HNMODMod )  (1)
HNMODHead (W ) = number of different heads of W with NMOD labeltotal number of heads of W with NMOD label
  
HSUBHead (W ) = number of different heads of W with SUB labeltotal number of heads of W with SUB label
 
 
HOBJHead (W ) = number of different heads of W with OBJ labeltotal number of heads of W with OBJ label
 
 
HNMODMod (W ) = number of different modifiers of W with NMOD labeltotal number of modifiers of W with NMOD label  
4.3 Bilingual Dictionary Extraction  
After calculating dependency heterogeneity vector of 
translation candidates, bilingual dictionary entries are 
extracted according to the distance between the vector of 
Ws in source language and the vector of Wt in target lan-
guage. We use Euclidean distance (see equation 2) for 
distance computation. The smaller distance between the 
dependency heterogeneity vectors of Ws and Wt, the 
more likely they are translations of each other. 
DH (Ws,Wt ) = DNMODHead 2 + DSUBHead 2 + DOBJHead 2 + DNMODMod 2 (2)
            DNMODHead = HNMODHead(Ws)?HNMODHead(Wt )  
            DSUBHead = HSUBHead (W s) ? HSUBHead (W t )   
            DOBJHead = HOBJHead (Ws)?HOBJHead (Wt )  
            DNMODMod = HNMODMod (Ws) ?HNMODMod (Wt )  
For example, following above definitions, we get de-
pendency heterogeneity vector of the words analyzed 
before (see Table 4). The distances between these vec-
tors are DH(???, economics) = 0.222,  DH(???, 
medicine) = 0.496. It is clear that the distance between 
the vector of ????(economics)? and ?economics? is 
much smaller than that between ????? and ?medi-
cine?. Thus, the pair ????/economics? is extracted 
successfully. 
Table 4. Dependency heterogeneity vector of words. 
Word Dependency Heterogeneity Vector
???(economics) (0.398, 0.677, 0.733, 0.471) 
economics (0.466, 0.500, 0.625, 0.432) 
medicine (0.748, 0.524, 0.542, 0.220) 
5 Results and Discussion  
5.1 Experimental Setting 
We collect Chinese and English pages from Wikipedia2 
with inter-language link and use them as comparable 
corpora. After corpora preprocessing, we get 1,132,492 
                                                          
2 http://download.wikimedia.org 
English sentences and 665,789 Chinese sentences for 
dependency heterogeneity vector learning. To evaluate 
the proposed approach, we randomly select 250 Chi-
nese/English single-noun pairs from the aligned titles of 
the collected pages as testing data, and divide them into 
5 folders. Accuracy (see equation 3) and MMR (Voor-
hees, 1999) (see equation 4) are used as evaluation met-
rics. The average scores of both accuracy and MMR 
among 5 folders are also calculated. 
Accuracy = ti
i=1
N? N  (3)
ti = 1, if there exists correct translation in top n ranking0, otherwise
? ? ? 
  
MMR = 1
N
1
rankii=1
N? ,     ranki = ri,  if ri < n0, otherwise
? ? ? 
 (4)
       n means top n evaluation,  
       ri means the rank of the correct translation in top n ranking 
      N means the total number of words for evaluation 
 
5.2 Results of Bilingual Dictionary Extraction 
Two approaches were evaluated in this experiment. One 
is the context heterogeneity approach proposed in (Fung, 
1995) (context for short). The other is our proposed ap-
proach (dependency for short). 
The average results of dictionary extraction are listed 
in Table 5. It shows both the average accuracy and aver-
age MMR of extracted dictionary entries were improved 
significantly (McNemar?s test, p<0.05) by the proposed 
approach. Besides, the increase of top5 evaluation was 
much higher than that of top10 evaluation, which means 
the proposed approach has more potential to extract pre-
cise bilingual dictionary entries.  
Table 5. Average results of dictionary extraction. 
context dependency  
ave.accu ave.MMR ave.accu ave.MMR 
Top5 0.132 0.064 0.208(?57.58%) 0.104(?62.50%)
Top10 0.296 0.086 0.380(?28.38%) 0.128(?48.84%)
5.3 Effect of Dependency Heterogeneity Vector 
Definition 
In the proposed approach, a dependency heterogeneity 
vector is defined as the combination of head and modi-
fier heterogeneities. To see the effects of different de-
pendency heterogeneity on dictionary extraction, we 
evaluated the proposed approach with different vector 
definitions, which are 
only-head: (HNMODHead ,HSUBHead ,HOBJHead )
only-mod: (HNMODMod ) 
only-NMOD: (HNMODHead ,HNMODMod )  
Table 6. Average results with different vector definitions. 
Top5 Top10  
ave.accu ave.MMR ave.accu ave.MMR
context 0.132 0.064 0.296 0.086 
dependency 0.208 0.104 0.380 0.128 
only-mod 0.156 0.080 0.336 0.103 
only-head 0.176 0.077 0.336 0.098 
only-NMODs 0.200 0.094 0.364 0.115 
123
The results are listed in Table 6. It shows with any 
types of vector definitions, the proposed approach out-
performed the context approach. Besides, if comparing 
the results of dependency, only-mod, and only-head, a 
conclusion can be drawn that head dependency hetero-
geneities and modifier dependency heterogeneities gave 
similar contribution to the proposed approach. At last, 
the difference between the results of dependency and 
only-NMOD shows the head and modifier with NMOD 
label contributed more to the proposed approach. 
5.4 Discussion 
To do detailed analysis, we collect the dictionary entries 
that are not extracted by context approach but extracted 
by the proposed approach (good for short), and the en-
tries that are extracted by context approach but not ex-
tracted by the proposed approach (bad for short) from 
top10 evaluation results with their occurrence time (see 
Table 7). If neglecting the entries ???/passports? and 
???/shanghai?, we found that the proposed approach 
tended to extract correct bilingual dictionary entries if 
both the two words occurred frequently in the compara-
ble corpora, but failed if one of them seldom appeared.   
Table 7. Good and bad dictionary entries. 
Good Bad 
Chinese English Chinese English 
???/262 jew/122 ???/53 crucifixion/19 
??/568 velocity/175 ???/6 aquarium/31 
??/2298 history/2376 ???/47 mixture/179 
??/1775 organizations/2194 ?/17 brick/66 
??/1534 movement/1541 ??/23 quantification/31
??/76 passports/80 ??/843 shanghai/1247 
But there are two exceptions: (1) although ???
(shanghai)? and ?shanghai? appeared frequently, the pro-
posed approach did not extract them correctly; (2) both 
???(passport)? and ?passports? occurred less than 100 
times, but they were recognized successfully by the pro-
posed approach. Analysis shows the cleanliness of the 
comparable corpora is the most possible reason. In the 
English corpus we used for evaluation, many words are 
incorrectly combined with ?shanghai? by ?br? (i.e. line 
break), such as ?airportbrshanghai?. These errors af-
fected the correctness of dependency heterogeneity vec-
tor of ?shanghai? greatly. Compared with the dirty 
resource of ?shanghai?, only base form and plural form 
of ?passport? occur in the English corpus. Therefore, the 
dependency heterogeneity vectors of ???? and ?pass-
ports? were precise and result in the successful extrac-
tion of this dictionary entry. We will clean the corpora to 
solve this problem in our future work. 
6 Conclusion and Future Work  
This paper proposes an approach, which not uses the 
similarity of bag-of-words around translation candidates 
but considers about the similarity of syntactic dependen-
cies, to extract bilingual dictionary from comparable 
corpora. Experimental results show that the proposed 
approach outperformed the context-based approach sig-
nificantly. It not only validates the feasibility of the pro-
posed approach, but also shows the effectiveness of 
applying syntactic analysis in real application.  
There are several future works under consideration 
including corpora cleaning, extending the proposed ap-
proach from single-noun dictionary extraction to multi-
words, and adapting the proposed approach to other lan-
guage pairs. Besides, because the proposed approach is 
based on the syntactic analysis of sentences with no 
more than k words (see Section 4.1), the parsing accu-
racy and the setting of threshold k will affect the cor-
rectness of dependency heterogeneity vector learning. 
We will try other thresholds and syntactic parsers to see 
their effects on dictionary extraction in the future. 
Acknowledgments 
This research is sponsored by Microsoft Research Asia 
Web-scale Natural Language Processing Theme. 
References  
Y.Chiao and P.Zweigenbaum. 2002. Looking for Candidate Transla-
tional Equivalents in Specialized, Comparable Corpora. Proceed-
ings of LREC 2002. 
B.Daille and E.Morin. 2008. An Effective Compositional Model for 
Lexical Alignment. Proceedings of IJCNLP-08. 
P.Fung. 1995. Compiling Bilingual Lexicon Entries from a Non-
parallel English-Chinese Corpus. Proceedings of the 3rd Annual 
Workshop on Very Large Corpora. pp. 173-183.  
P.Fung. 2000. A Statistical View on Bilingual Lexicon Extraction 
from Parallel Corpora to Non-parallel Corpora. Parallel Text Proc-
essing: Alignment and Use of Translation Corpora. Kluwer Aca-
demic Publishers. 
G.Grefenstette. 1998. The Problem of Cross-language Information 
Retrieval. Cross-language Information Retrieval. Kluwer Aca-
demic Publishers. 
E.Morin et al. 2007. Bilingual Terminology Mining ? Using Brain, 
not Brawn Comparable Corpora. Proceedings of ACL 2007. 
T.Nakagawa and K.Uchimoto. 2007. A Hybrid Approach to Word 
Segmentation and POS Tagging. Proceedings of ACL 2007. 
J.Nivre et al. 2007. MaltParser: A Language-independent System for 
Data-driven Dependency Parsing. Natural Language Engineering. 
13(2): 95-135. 
F.Och and H.Ney. 2003. A Systematic Comparison of Various Statis-
tical Alignment Models. Computational Linguistics, 29(1): 19-51. 
P.Otero. 2007. Learning Bilingual Lexicons from Comparable English 
and Spanish Corpora. Proceedings of MT Summit XI. pp. 191-198.  
P.Otero. 2008. Evaluating Two Different Methods for the Task of 
Extracting Bilingual Lexicons from Comparable Corpora. Proceed-
ings of LREC 2008 Workshop on Comparable Corpora. pp. 19-26. 
X.Robitaille et al. 2006. Compiling French Japanese Terminologies 
from the Web. Proceedings of EACL 2006. 
Y.Tsuruoka et al. 2005. Developing a Robust Part-of-speech Tagger 
for Biomedical Text. Advances in Informatics ? 10th Panhellenic 
Conference on Informationcs. LNCS 3746. pp. 382-392. 
E.M.Voorhees. 1999. The TREC-8 Question Answering Track Report. 
Proceedings of the 8th Text Retrieval Conference.  
D.Wu. 1994. Learning an English-Chinese Lexicon from a Parallel 
Corpus. Proceedings of the 1st Conference of the Association for 
Machine Translation in the Americas.
124
Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 146?149,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Chinese Word Segmentation and Named Entity Recognition by 
Character Tagging 
 
Kun Yu1      Sadao Kurohashi2     Hao Liu1     Toshiaki Nakazawa1 
 Graduate School of Information Science and Technology, The University of 
Tokyo, Tokyo, Japan, 113-86561 
Graduate School of Informatics, Kyoto University, Kyoto, Japan, 606-85012 
 {kunyu, liuhao, nakazawa}@kc.t.u-tokyo.ac.jp1 
kuro@i.kyoto-u.ac.jp2 
  
Abstract 
This paper describes our word segmenta-
tion system and named entity recognition 
(NER) system for participating in the 
third SIGHAN Bakeoff. Both of them are 
based on character tagging, but use dif-
ferent tag sets and different features. 
Evaluation results show that our word 
segmentation system achieved 93.3% and 
94.7% F-score in UPUC and MSRA open 
tests, and our NER system got 70.84% 
and 81.32% F-score in LDC and MSRA 
open tests. 
1 Introduction 
Dealing with word segmentation as character 
tagging showed good results in last SIGHAN 
Bakeoff (J.K.Low et al,2005). It is good at un-
known word identification, but only using char-
acter-level features sometimes makes mistakes 
when identifying known words (T.Nakagawa, 
2004). Researchers use word-level features 
(J.K.Low et al,2005) to solve this problem. 
Based on this idea, we develop a word segmenta-
tion system based on character-tagging, which 
also combine character-level and word-level fea-
tures. In addition, a character-based NER module 
and a rule-based factoid identification module 
are developed for post-processing.  
Named entity recognition based on character-
tagging has shown better accuracy than word-
based methods (H.Jing et al,2003). But the small 
window of text makes it difficult to recognize the 
named entities with many characters, such as 
organization names (H.Jing et al,2003). Consid-
ering about this, we developed a NER system 
based on character-tagging, which combines 
word-level and character-level features together. 
In addition, in-NE probability is defined in this 
system to remove incorrect named entities and 
create new named entities as post-processing. 
2 Character Tagging for Word 
Segmentation and NER 
2.1 Basic Model 
We look both word segmentation and NER as 
character tagging, which is to find the tag se-
quence T* with the highest probability given a 
sequence of characters S=c1c2?cn.  
)|(maxarg* STPT
T
=  (1) 
Then we assume that the tagging of one char-
acter is independent of each other, and modify 
formula 1 as 
?
=
=
=
=
=
n
i
ii
tttT
nn
tttT
ctP
ccctttPT
n
n
1...
2121
...
*
)|(maxarg     
)...|...(maxarg
21
21
 (2) 
 Beam search (n=3) (Ratnaparkhi,1996) is ap-
plied for tag sequence searching, but we only 
search the valid sequences to ensure the validity 
of searching result. SVM is selected as the basic 
classification model for tagging because of its 
robustness to over-fitting and high performance 
(Sebastiani, 2002). To simplify the calculation, 
the output of SVM is regarded as P(ti|ci). 
2.2 Tag Definition 
Four tags ?B, I, E, S? are defined for the word 
segmentation system, in which ?B? means the 
character is the beginning of one word, ?I? means 
the character is inside one word, ?E? means the 
character is at the end of one word and ?S? means 
the character is one word by itself. 
For the NER system, different tag sets are de-
fined for different corpuses. Table 1 shows the 
146
tag set defined for MSRA corpus. It is the prod-
uct of Segment-Tag set and NE-Tag set, because 
not only named entities but also words are seg-
mented in this corpus. Here NE-Tag ?O? means 
the character does not belong to any named enti-
ties. For LDC corpus, because there is no seg-
mentation information, we delete NE-Tag ?O? 
but add tag ?NONE? to indicate the character 
does not belong to any named entities (Table 2). 
Table 1 Tags of NER for MSRA corpus 
Segment-Tag NE-Tag 
B, I, E, S ? PER, LOC, ORG, O 
Table 2 Tags of NER for LDC corpus 
Segment Tag NE Tag 
B, I, E, S ? PER, LOC, ORG, GPE + NONE 
2.3 Feature Definition 
First, some features based on characters are 
defined for the two tasks, which are: 
(a) Cn (n=-2,-1,0,1,2) 
(b) Pu(C0) 
Feature Cn (n=-2,-1,0,1,2) mean the Chinese 
characters appearing in different positions (the 
current character and two characters to its left 
and right), and they are binary features. A char-
acter list, which contains all the characters in the 
lexicon introduced later, is used to identify them.
 
Besides of that, feature Pu(C0) means whether C0 
is in a punctuation character list. It is also binary 
feature and all the punctuations in the punctua-
tion character list come from Penn Chinese Tree-
bank 5.1 (N.Xue et al,2002). 
In addition, we define some word-level fea-
tures based on a lexicon to enlarge the window 
size of text in the two tasks, which are:  
(c) Wn (n=-1,0,1) 
Feature Wn (n=-1,0,1) mean the lexicon words 
in different positions (the word containing C0 
and one word to its left and right) and they are 
also binary features. We select all the possible 
words in the lexicon that satisfy the requirements, 
not like only selecting the longest one in 
(J.K.Low et al,2005). To create the lexicon, we 
use following steps. First, a lexicon from NICT 
(National Institute of Information and Communi-
cations Technology, Japan) is used as the basic 
lexicon, which is extracted from Peking Univer-
sity Corpus of the second SIGHAN Bakeoff 
(T.Emerson, 2005), Penn Chinese Treebank 4.0 
(N.Xue et al,2002), a Chinese-to-English Word-
list1  and part of NICT corpus (K.Uchimoto et 
al.,2004; Y.J.Zhang et al,2005). Then, all the 
words containing digits and letters are removed 
                                                 
1
 http://projects.ldc.upenn.edu/Chinese/  
from this lexicon. At last, all the punctuations in 
Penn Chinese Treebank 5.1 (N.Xue et al,2002) 
and all the words in the training data of UPUC 
and MSRA corpuses are added into the lexicon.  
Besides of above features, some extra features 
are defined only for NER task. 
First, we add some character-based features to 
improve the accuracy of person name recognition, 
which are CNn (n=-2,-1,0,1,2). They mean 
whether C
 n (n=-2,-1,0,1,2) belong to a Chinese 
surname list. All of them are binary features. The 
Chinese surname list contains the most famous 
100 Chinese surnames, such as ?, ?, ?, ? 
(Zhao, Qian, Sun, Li). 
Then, we add some word-based features to 
help identify the organization name, which are 
WORGn (n=-1,0,1). They mean whether W n (n= 
-1,0,1) belong to an organization suffix list. All 
of them are also binary features. The organiza-
tion suffix list is created by extracting the last 
word from all the organization names in the 
training data of both MSRA and LDC corpuses. 
3 Post-processing 
Besides of the basic model, a NER module 
and a factoid identification module are developed 
in our word segmentation system for post-
processing. In addition, we define in-NE prob-
ability to delete the incorrect named entities and 
identify new named entities in the post-
processing phrase of our NER system. 
3.1 Named Entity Recognition for Word 
Segmentation 
In this module, if two or more segments in the 
outputs of basic model are recognized as one 
named entity, we combine them as one segment.  
This module uses the same basic NER model 
as what we introduced in the previous section. 
But it only identifies person and location names, 
because organization names often contain more 
than one word. In addition, to keep the high ac-
curacy of person name recognition, the features 
about organization suffixes are not used here.  
3.2 Factoid Identification for Word Seg-
mentation 
Rules are used to identify the following fac-
toids among the segments from the basic word 
segmentation model:  
NUMBER: Integer, decimal, Chinese number 
PERCENT: Percentage and fraction 
DATE: Date 
FOREIGN: English words 
147
Table 3 shows some rules defined here. 
Table 3 Some Rules for Factoid Identification 
Factoid Rule 
NUMBER If previous segment ends with DIGIT and current 
segment starts with DIGIT, then combine them. 
PERCENT If previous segment is composed of DIGIT and 
current segment equals ?%?, then combine them. 
DATE 
If previous segment is composed of DIGIT and 
current segment is in the list of ??, ?, ?, ? 
(Year, Month, Day, Day)?, then combine them. 
FOREIGN Combine the consequent letters as one segment. 
(DIGIT means both Arabic and Chinese numerals) 
3.3 NER Deletion and Creation 
In-word probability has been used in unknown 
word identification successfully (H.Q.Li et al, 
2004). Accordingly, we define in-NE probability 
to help delete and create named entities (NE). 
Formula 3 shows the definition of in-NE prob-
ability for character sequence cici+1?ci+n. Here ?# 
of cici+1?ci+n as NE? is defined as TimeInNE and 
the occurrence of cici+1?ci+n in different type of 
NE is treated differently. 
data in testing ... of #
NE as ... of #)...(
1
1
1
niii
niii
niiiInNE
ccc
ccc
cccP
++
++
++ =
 (3) 
Then, we use some criteria to delete the incor-
rect NE and create new possible NE, in which 
different thresholds are set for different tasks. 
Criterion 1: If PInNE(cici+1?ci+n) of one NE 
type is lower than TDel, and TimeInNE(cici+1?ci+n) 
of the same NE type is also lower than TTime, then 
delete this type of NE composed of cici+1?ci+n.  
Criterion 2: If PInNE(cici+1?ci+n) of one NE 
type is higher than TCre, and in other places the 
character sequence cici+1?ci+n does not belong to 
any NE, then create a new NE containing 
cici+1?ci+n with this NE type.  
4 Evaluation Results and Discussion 
4.1 Evaluation Setting 
SVMlight (T.Joachims, 1999) was used as 
SVM tool. In addition, we used the MSRA train-
ing corpus of NER task in this Bakeoff to train 
our NER post-processing module. 
4.2 Results of Word Segmentation 
We attended the open track of word segmenta-
tion task for two corpuses: UPUC and MSRA. 
Table 4 shows the evaluation results. 
Table 4 Results of Word Segmentation Task (in percentage %) 
Corpus Pre. Rec. F-score Roov Riv 
UPUC 94.4 92.2 93.3 68.0 97.0 
MSRA 94.0 95.3 94.7 50.3 96.9 
The F-score of our word segmentation system 
in UPUC corpus ranked 4th (same as that of the 
3rd group) among all the 8 participants. And it 
was only 1.1% lower than the highest one and 
0.2% lower than the second one. It showed that 
our character-tagging approach was feasible. But 
the F-score of MSRA corpus was only higher 
than one participant in all the 10 groups (the 
highest one was 97.9%). Error analysis shows 
that there are two main reasons.  
First, in MSRA corpus, they tend to segment 
one organization name as one word, such as ?
?????(China Chamber of Commerce in 
USA). But our basic segmentation model seg-
mented such word into several words, e.g. ??/
??/??(USA/China/Chamber of Commerce), 
and our post-processing NER module does not 
consider about organization names.  
Second, our factoid identification rule did not 
combine the consequent DATE factoids into one 
word, but they are combined in MSRA corpus. 
For example, our system segmented the word?
? 9?? (9 o?clock in the evening) into three 
parts ??/9 ?/? (Evening/9 o?clock/Exact). 
This error can be solved by revising the rules for 
factoid identification. 
Besides of that, we also found although our 
large lexicon helped identify the known word 
successfully, it also decreased the recall of OOV 
words (our Riv of UPUC corpus ranked 2nd, with 
only 0.6% decrease than the highest one, but 
Roov ranked 4th, with 8.8% decrease than the 
highest one). The large size of this lexicon is 
looked as the main reason.  
Our lexicon contains 221,407 words, in which 
6,400 words are single-character words. It made 
our system easy to segment one word into sev-
eral words, for example word ??? (Economy 
Group) in UPUC corpus was segmented into?
?  (Economy) and ? (Group). Moreover, the 
large size of this lexicon also brought errors of 
combining two words into one word if the word 
was in the lexicon. For example, words ? (Only) 
and ? (Have) in MSRA corpus were identified 
as one word because there existed the word?? 
(Only) in our lexicon. We will reduce our lexi-
con to a reasonable size to solve these problems. 
4.3 Results of NER 
We also attended the open track of NER task 
for both LDC corpus and MSRA corpus. Table 5 
and Table 6 give the evaluation results.  
There were only 3 participants in the open 
track of LDC corpus and our group got the best 
F-score. In addition, among all the 11 partici-
pants for MSRA corpus, our system ranked 6th 
148
by F-score. It showed the validity of our charac-
ter-tagging method for NER. But for location 
name (LOC) in LDC corpus, both the precision 
and recall of our NER system were very low. It 
was because there were too few location names 
in the training data (there were only 476 LOC in 
the training data, but 5648 PER, 5190 ORG and 
9545 GPE in the same data set). 
Table 5 Results of NER Task for LDC corpus (in percentage %) 
 PER LOC ORG GPE Overall 
Pre. 83.29 58.52 61.48 78.66 76.16 
Rec. 66.93 18.87 45.19 79.94 66.21 
F-score 74.22 28.57 52.09 79.30 70.84 
Table 6 Results of NER Task for MSRA corpus (in percentage %) 
 PER LOC ORG Overall 
Pre. 90.76 85.62 73.90 84.68 
Rec. 76.13 85.41 65.74 78.22 
F-score 82.80 85.52 69.58 81.32 
Besides of that, error analysis shows there are 
four types of main errors in the NER results. 
First, some organization names were very long 
and can be divided into several words, in which 
parts of them can also be looked as named enti-
ties. In such case, our system only recognized the 
small parts as named entities. For example,  ??
???????????  (Fei Zhengqing 
Eastern Asia Research Center of Harvard Univ.) 
was an organization name. But our system rec-
ognized it as????(Harvard Univ.)/ORG+?
? ? (Fei Zheng Qing)/PER+ ? ? (Eastern 
Asia)/LOC+ ????(Research Center)/ORG. 
Adding more context features may be useful to 
resolve this issue. 
In addition, our system was not good at recog-
nizing foreign person names, such as ??? 
(Riordan), and abbreviations, such as ?? (Los 
Angeles), if they seldom or never appeared in 
training corpus. It is because the use of the large 
lexicon decreased the unknown word identifica-
tion ability of our NER system simultaneously. 
Third, the in-NE probability used in post-
processing is helpful to identify named entities 
which cannot be recognized by the basic model. 
But it also recognized some words which can 
only be regarded as named entities in the local 
context incorrectly. For example, our system 
recognized?? (Najing) as GPE in ?????
? (Send to Najing for remedy) in LDC corpus. 
We will consider about adding the in-NE prob-
ability as one feature into the basic model to 
solve this problem. 
At last, in LDC corpus, they combine the at-
tributive of one named entity (especially person 
and organization names) with the named entity 
together. But our system only recognized the 
named entity by itself. For example, our system 
only recognized ??? (Liu Gui Fang) as PER 
in the reference person name ???????? 
(Liu Gui Fang who does not know the inside). 
5 Conclusion and Future Work 
Through the participation of the third 
SIGHAN Bakeoff, we found that tagging charac-
ters with both character-level and word-level fea-
tures was effective for both word segmentation 
and NER. While, this work is only our 
preliminary attempt and there are still many 
works needed to do in the future, such as the 
control of lexicon size, the use of extra 
knowledge (e.g. pos-tag), the feature definition, 
and so on. In addition, our word segmentation 
system only combined the NER module as post-
processing, which resulted in that lots of infor-
mation from NER module cannot be used by the 
basic model. We will consider about combining 
the NER and factoid identification modules into 
the basic word segmentation model by defining 
new tag sets in our future work. 
Acknowledgement 
We would like to thank Dr. Kiyotaka Uchi-
moto for providing the NICT lexicon. 
Reference 
T.Emerson. 2005. The Second International Chinese Word Seg-
mentation Bakeoff. In the 4th SIGHAN Workshop. pp. 123-133. 
H.Jing et al 2003. HowtogetaChineseName(Entity): Segmentation 
and Combination Issues. In EMNLP 2003. pp. 200-207. 
T.Joachims. 1999. Making large-scale SVM learning practical. 
Advances in Kernel Methods - Support Vector Learning. MIT-
Press. 
H.Q.Li et al 2004. The Use of SVM for Chinese New Word Identi-
fication. In IJCNLP 2004. pp. 723-732. 
J.K.Low et al 2005. A Maximum Entropy Approach to Chinese 
Word Segmentation. In the 4th SIGHAN Workshop. pp. 161-164. 
T.Nakagawa. 2004. Chinese and Japanese Word Segmentation 
Using Word-level and Character-level Information. In COLING 
2004. pp. 466-472. 
A.Ratnaparkhi. 1996. A Maximum Entropy Model for Part-of-
Speech Tagging. In EMNLP 1996. 
F.Sebastiani. 2002. Machine learning in automated text categoriza-
tion. ACM Computing Surveys. 34(1): 1-47. 
K.Uchimoto et al 2004. Multilingual Aligned Parallel Treebank 
Corpus Reflecting Contextual Information and its Applications. 
In Proceedings of the MLR 2004. pp. 63-70. 
N.Xue et al 2002. Building a Large-Scale Annotated Chinese Cor-
pus. In COLING 2002. 
Y.J.Zhang et al 2005. Building an Annotated Japanese-Chinese 
Parallel Corpus ? A part of NICT Multilingual Corpora. In Pro-
ceedings of the MT SummitX. pp. 71-78. 
149
Proceedings of the 43rd Annual Meeting of the ACL, pages 499?506,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Resume Information Extraction with Cascaded Hybrid Model 
 
Kun Yu Gang Guan Ming Zhou 
Department of Computer Science 
and Technology 
Department of Electronic 
Engineering Microsoft Research Asia 
University of Science and 
Technology of China Tsinghua University 
5F Sigma Center, No.49 Zhichun 
Road, Haidian 
Hefei, Anhui, China, 230027 Bejing, China, 100084 Bejing, China, 100080 
yukun@mail.ustc.edu.cn guangang@tsinghua.org.cn mingzhou@microsoft.com 
 
Abstract 
This paper presents an effective approach 
for resume information extraction to 
support automatic resume management 
and routing. A cascaded information 
extraction (IE) framework is designed. In 
the first pass, a resume is segmented into 
a consecutive blocks attached with labels 
indicating the information types. Then in 
the second pass, the detailed information, 
such as Name and Address, are identified 
in certain blocks (e.g. blocks labelled 
with Personal Information), instead of 
searching globally in the entire resume. 
The most appropriate model is selected 
through experiments for each IE task in 
different passes. The experimental results 
show that this cascaded hybrid model 
achieves better F-score than flat models 
that do not apply the hierarchical 
structure of resumes. It also shows that 
applying different IE models in different 
passes according to the contextual 
structure is effective. 
1 Introduction 
Big enterprises and head-hunters receive 
hundreds of resumes from job applicants every day.  
Automatically extracting structured information 
from resumes of different styles and formats is 
needed to support the automatic construction of 
database, searching and resume routing.  The 
definition of resume information fields varies in 
different applications. Normally, resume 
information is described as a hierarchical structure 
                                                 
 The research was carried out in Microsoft Research Asia. 
with two layers. The first layer is composed of 
consecutive general information blocks such as 
Personal Information, Education etc. Then within 
each general information block, detailed 
information pieces can be found, e.g., in Personal 
Information block, detailed information such as 
Name, Address, Email etc. can be further extracted.  
Info Hierarchy Info Type (Label) 
General Info 
Personal Information(G1); 
Education(G2); Research 
Experience(G3); Award(G4); 
Activity(G5); Interests(G6); 
Skill(G7) 
Personal 
Detailed Info 
(Personal 
Information)
Name(P1); Gender(P2); 
Birthday(P3); Address(P4); Zip 
code(P5); Phone(P6); 
Mobile(P7); Email(P8); 
Registered Residence(P9); 
Marriage(P10); Residence(P11); 
Graduation School(P12); 
Degree(P13); Major(P14) 
Detailed 
Info 
Educational 
Detailed Info 
(Education) 
Graduation School(D1); 
Degree(D2); Major(D3); 
Department(D4) 
Table 1. Predefined information types. 
Based on the requirements of an ongoing 
recruitment management system which 
incorporates database construction with IE 
technologies and resume recommendation 
(routing), as shown in Table 1, 7 general 
information fields are defined. Then, for Personal 
Information, 14 detailed information fields are 
designed; for Education, 4 detailed information 
fields are designed. The IE task, as exemplified in 
Figure 1, includes segmenting a resume into 
consecutive blocks labelled with general 
information types, and further extracting the 
detailed information such as Name and Address 
from certain blocks. 
Extracting information from resumes with high 
precision and recall is not an easy task. In spite of  
499
 
Figure 1. Example of a resume and the extracted information.
constituting a restricted domain, resumes can be 
written in multitude of formats (e.g. structured 
tables or plain texts), in different languages (e.g. 
Chinese and English) and in different file types 
(e.g. Text, PDF, Word etc.). Moreover, writing 
styles could be very diversified. 
Among the methods in IE, Hidden Markov 
modelling has been widely used (Freitag and 
McCallum, 1999; Borkar et al, 2001). As a state-
based model, HMMs are good at extracting 
information fields that hold a strong order of 
sequence. Classification is another popular method 
in IE. By assuming the independence of 
information types, it is feasible to classify 
segmented units as either information types to be 
extracted (Kushmerick et al, 2001; Peshkin and 
Pfeffer, 2003; Sitter and Daelemans, 2003), or 
information boundaries (Finn and Kushmerick, 
2004). This method specializes in settling the 
extraction problem of independent information 
types.  
Resume shares a document-level hierarchical 
contextual structure where the related information 
units usually occur in the same textual block, and 
text blocks of different information categories 
usually occur in a relatively fixed order. Such 
characteristics have been successfully used in the 
categorization of multi-page documents by 
Frasconi et al (2001).  
In this paper, given the hierarchy of resume 
information, a cascaded two-pass IE framework is 
designed. In the first pass, the general information 
is extracted by segmenting the entire resume into 
consecutive blocks and each block is annotated 
with a label indicating its category. In the second 
pass, detailed information pieces are further 
extracted within the boundary of certain blocks. 
Moreover, for different types of information, the 
most appropriate extraction method is selected 
through experiments. For the first pass, since there 
exists a strong sequence among blocks, a HMM 
model is applied to segment a resume and each 
block is labelled with a category of general 
information. We also apply HMM for the 
educational detailed information extraction for the 
same reason. In addition, classification based 
method is selected for the personal detailed 
information extraction where information items 
appear relatively independently.  
Tested with 1,200 Chinese resumes, 
experimental results show that exploring the 
hierarchical structure of resumes with this 
proposed cascaded framework improves the 
average F-score of detailed information extraction 
500
greatly, and combining different IE models in 
different layer properly is effective to achieve 
good precision and recall.  
The remaining part of this paper is structured as 
follows. Section 2 introduces the related work. 
Section 3 presents the structure of the cascaded 
hybrid IE model and introduces the HMM model 
and SVM model in detail. Experimental results 
and analysis are shown in Section 4. Section 5 
provides a discussion of our cascaded hybrid 
model. Section 6 is the conclusion and future work. 
2 Related Work 
As far as we know, there are few published 
works on resume IE except some products, for 
which there is no way to determine the technical 
details. One of the published results on resume IE 
was shown in Ciravegna and Lavelli (2004). In 
this work, they applied (LP)2 , a toolkit of IE, to 
learn information extraction rules for resumes 
written in English. The information defined in 
their task includes a flat structure of Name, Street, 
City, Province, Email, Telephone, Fax and Zip 
code. This flat setting is not only different from 
our hierarchical structure but also different from 
our detailed information pieces.   
Besides, there are some applications that are 
analogous to resume IE, such as seminar 
announcement IE (Freitag and McCallum, 1999), 
job posting IE (Sitter and Daelemans, 2003; Finn 
and Kushmerick, 2004) and address segmentation 
(Borkar et al, 2001; Kushmerick et al, 2001). 
Most of the approaches employed in these 
applications view a text as flat and extract 
information from all the texts directly (Freitag and 
McCallum, 1999; Kushmerick et al, 2001; 
Peshkin and Pfeffer, 2003; Finn and Kushmerick, 
2004). Only a few approaches extract information 
hierarchically like our model. Sitter and 
Daelemans (2003) present a double classification 
approach to perform IE by extracting words from 
pre-extracted sentences. Borkar et al (2001) 
develop a nested model, where the outer HMM 
captures the sequencing relationship among 
elements and the inner HMMs learn the finer 
structure within each element. But these 
approaches employ the same IE methods for all 
the information types. Compared with them, our 
model applies different methods in different sub-
tasks to fit the special contextual structure of 
information in each sub-task well. 
3 Cascaded Hybrid Model 
Figure 2 is the structure of our cascaded hybrid 
model. The first pass (on the left hand side) 
segments a resume into consecutive blocks with a 
HMM model. Then based on the result, the second 
pass (on the right hand side) uses HMM to extract 
the educational detailed information and SVM to 
extract the personal detailed information, 
respectively. The block selection module is used to 
decide the range of detailed information extraction 
in the second pass. 
 
 
Figure 2. Structure of cascaded hybrid model. 
3.1 HMM Model 
3.1.1 Model Design 
For general information, the IE task is viewed as 
labelling the segmented units with predefined class 
labels. Given an input resume T which is a 
sequence of words w1,w2,?,wk, the result of 
general information extraction is a sequence of 
blocks in which some words are grouped into a 
certain block T = t1, t2,?, tn, where ti is a block. 
Assuming the expected label sequence of T is L=l1, 
l2,?, ln,  with each block being assigned a label li, 
we get the sequence of block and label pairs Q=(t1, 
l1), (t2, l2),?,(tn, ln). In our research, we simply 
assume that the segmentation is based on the 
natural paragraph of T. 
Table 1 gives the list of information types to be 
extracted, where general information is 
represented as G1~G7. For each kind of general 
information, say Gi, two labels are set: Gi-B means 
the beginning of Gi, Gi-M means the remainder 
part of Gi. In addition, label O is defined to 
represent a block that does not belong to any 
general information types. With these positional 
information labels, general information can be 
obtained. For instance, if the label sequence Q for 
501
a resume with 10 paragraphs is Q=(t1, G1-B), (t2, 
G1-M) , (t3, G2-B) , (t4, G2-M) , (t5, G2-M) , (t6, O) , 
(t7, O) , (t8, G3-B) , (t9, G3-M) , (t10, G3-M), three 
types of general information can be extracted as 
follows: G1:[t1, t2], G2:[t3, t4, t5], G3:[t8, t9, t10].  
Formally, given a resume T=t1,t2,?,tn, seek a 
label sequence L*=l1,l2,?,ln, such that the 
probability of the sequence of labels is maximal. 
   )|(maxarg* TLPL
L
=  (1)
According to Bayes? equation, we have 
       )()|(maxarg* LPLTPL
L
?=  (2)
If we assume the independent occurrence of 
blocks labelled as the same information types, we 
have 
     ?
=
=
n
i
ii ltPLTP
1
)|()|(  (3)
We assume the independence of words 
occurring in ti and use a unigram model, which 
multiplies the probabilities of these words to get 
the probability of ti.  
},...,{   where), |()|( 21
1
mii
m
r
rii wwwtlwPltP ==?
=
 (4) 
If a tri-gram model is used to estimate P(L), we 
have 
         ?
=
???=
n
i
iii lllPllPlPLP
3
21121 ),|()|()()( (5)
To extract educational detailed information 
from Education general information, we use 
another HMM. It also uses two labels Di-B and Di-
M to represent the beginning and remaining part of 
Di, respectively. In addition, we use label O to 
represent that the corresponding word does not 
belong to any kind of educational detailed 
information. But this model expresses a text T as 
word sequence T=w1,w2,?,wn. Thus in this model, 
the probability P(L) is calculated with Formula 5 
and the probability P(T|L) is calculated by 
?
=
=
n
i
ii lwPLTP
1
)|()|(  (6)
Here we assume the independent occurrence of 
words labelled as the same information types.  
3.1.2 Parameter Estimation 
Both words and named entities are used as 
features in our HMMs. A Chinese resume C= 
c1?,c2?,?,ck? is first tokenized into C= w1,w2,?,wk 
with a Chinese word segmentation system LSP 
(Gao et al, 2003). This system outputs predefined 
features, including words and named entities in 8 
types (Name, Date, Location, Organization, Phone, 
Number, Period, and Email). The named entities 
of the same type are normalized into single ID in 
feature set.  
In both HMMs, fully connected structure with 
one state representing one information label is 
applied due to its convenience. To estimate the 
probabilities introduced in 3.1.1, maximum 
likelihood estimation is used, which are 
       
),(
),,(),|(
21
21
21
??
??
?? =
ii
iii
iii llcount
lllcountlllP  (7) 
      
)(
),(
)|(
1
1
1
?
?
? =
i
ii
ii lcount
llcount
llP  (8) 
       
ordsdistinct w mcontainsistatewhere
 ,
),(
),()|(
1
?
=
= m
r
ir
ir
ir
lwcount
lwcountlwP
 
 
(9) 
3.1.3 Smoothing 
Short of training data to estimate probability is a 
big problem for HMMs. Such problems may occur 
when estimating either P(T|L) with unknown word 
wi or P(L) with unknown events.  
Bikel et al (1999) mapped all unknown words 
to one token _UNK_ and then used a held-out data 
to train the bi-gram models where unknown words 
occur. They also applied a back-off strategy to 
solve the data sparseness problem when estimating 
the context model with unknown events, which 
interpolates the estimation from training corpus 
and the estimation from the back-off model with 
calculated parameter ? (Bikel et al, 1999). Freitag 
and McCallum (1999) used shrinkage to estimate 
the emission probability of unknown words, which 
combines the estimates from data-sparse states of 
the complex model and the estimates in related 
data-rich states of the simpler models with a 
weighted average.  
In our HMMs, we first apply Good Turing 
smoothing (Gale, 1995) to estimate the probability 
P(wr|li) when training data is sparse. For word wr 
seen in training data, the emission probability is 
P(wr|li)?(1-x), where P(wr|li) is the emission 
probability calculated with Formula 9 and x=Ei/Si 
(Ei is the number of words appearing only once in 
state i and Si is the total number of words 
occurring in state i). For unknown word wr, the 
emission probability is x/(M-mi), where M is the 
number of all the words appearing in training data, 
502
and mi is the number of distinct words occurring in 
state i. Then, we use a back-off schema (Katz, 
1987) to deal with the data sparseness problem 
when estimating the probability P(L) (Gao et al, 
2003). 
3.2 SVM Model 
3.2.1 Model Design 
We convert personal detailed information 
extraction into a classification problem. Here we 
select SVM as the classification model because of 
its robustness to over-fitting and high performance 
(Sebastiani, 2002). In the SVM model, the IE task 
is also defined as labelling segmented units with 
predefined class labels. We still use two labels to 
represent personal detailed information Pi: Pi-B 
represents the beginning of Pi and Pi-M represents 
the remainder part of Pi. Besides of that, label O 
means that the corresponding unit does not belong 
to any personal detailed information boundaries 
and information types. For example, for part of a 
resume ?Name:Alice (Female)?, we got three units 
after segmentation with punctuations, i.e. ?Name?, 
?Alice?, ?Female?. After applying SVM 
classification, we can get the label sequence as P1-
B,P1-M,P2-B. With this sequence of unit and label 
pairs, two types of personal detailed information 
can be extracted as P1: [Name:Alice] and P2: 
[Female]. 
Various ways can be applied to segment T. In 
our work, segmentation is based on the natural 
sentence of T. This is based on the empirical 
observation that detailed information is usually 
separated by punctuations (e.g. comma, Tab tag or 
Enter tag). 
The extraction of personal detailed information 
can be formally expressed as follows: given a text 
T=t1,t2,?,tn, where ti is a unit defined by the 
segmenting method mentioned above, seek a label 
sequence L* = l1,l2,?,ln, such that the probability 
of the sequence of labels is maximal. 
    )|(maxarg* TLPL
L
=  (10) 
The key assumption to apply classification in IE 
is the independence of label assignment between 
units. With this assumption, Formula 10 can be 
described as 
   ?
==
=
n
i
ii
lllL
tlPL
n 1...,
* )|(maxarg
21
 (11) 
Thus this probability can be maximized by 
maximizing each term in turn. Here, we use the 
SVM score of labelling ti with li to replace P(li|ti). 
3.2.2 Multi-class Classification 
SVM is a binary classification model. But in our 
IE task, it needs to classify units into N classes, 
where  N is two times of the number of personal 
detailed information types. There are two popular 
strategies to extend a binary classification task to 
N classes (A.Berger, 1999). The first is One vs. All 
strategy, where N classifiers are built to separate 
one class from others. The other is Pairwise 
strategy, where N?(N-1)/2 classifiers considering 
all pairs of classes are built and final decision is 
given by their weighted voting. In our model, we 
apply the One vs. All strategy for its good 
efficiency in classification. We construct one 
classifier for each type, and classify each unit with 
all these classifiers. Then we select the type that 
has the highest score in classification. If the 
selected score is higher than a predefined threshold, 
then the unit is labelled as this type. Otherwise it is 
labelled as O. 
3.2.3 Feature Definition 
Features defined in our SVM model are 
described as follows: 
Word: Words that occur in the unit. Each word 
appearing in the dictionary is a feature. We use 
TF?IDF as feature weight, where TF means word 
frequency in the text, and IDF is defined as: 
wN
NLogwIDF 2)( =  (12) 
N: the total number of training examples;  
Nw: the total number of positive examples that contain word w 
Named Entity: Similar to the HMM models, 8 
types of named entities identified by LSP, i.e., 
Name, Date, Location, Organization, Phone, 
Number, Period, Email, are selected as binary 
features. If any one type of them appears in the 
text, then the weight of this feature is 1, otherwise 
is 0. 
3.3 Block Selection 
Block selection is used to select the blocks 
generated from the first pass as the input of the 
second pass for detailed information extraction.  
Error analysis of preliminary experiments shows 
that the majority of the mistakes of general 
information extraction resulted from labelling non- 
503
Personal Detailed Info (SVM) Educational Detailed Info (HMM) Model Avg.P (%) Avg.R (%) Avg.F (%) Avg.P (%) Avg.R (%) Avg.F (%) 
Flat 77.49 82.02 77.74 58.83 77.35 66.02 
Cascaded 86.83 (+9.34) 76.89 (-5.13) 80.44 (+2.70) 70.78 (+11.95) 76.80 (-0.55) 73.40 (+7.38)
Table 2. IE results with cascaded model and flat model.
boundary blocks as boundaries in the first pass. 
Therefore we apply a fuzzy block selection 
strategy, which not only selects the blocks labelled 
with target general information, but also selects 
their neighboring two blocks, so as to enlarge the 
extracting range. 
4 Experiments and Analysis 
4.1 Data and Experimental Setting 
We evaluated this cascaded hybrid model with 
1,200 Chinese resumes. The data set was divided 
into 3 parts: training data, parameter tuning data 
and testing data with the proportion of 4:1:1. 6-
folder cross validation was conducted in all the 
experiments. We selected SVMlight (Joachims, 
1999) as the SVM classifier toolkit and LSP (Gao 
et al, 2003) for Chinese word segmentation and 
named entity identification. Precision (P), recall (R) 
and F-score (F=2PR/(P+R)) were used as the basic 
evaluation metrics and macro-averaging strategy 
was used to calculate the average results. For the 
special application background of our resume IE 
model, the ?Overlap? criterion (Lavelli et al, 2004) 
was used to match reference instances and 
extracted instances. We define that if the 
proportion of the overlapping part of extracted 
instance and reference instance is over 90%, then 
they match each other. 
A set of experiments have been designed to 
verify the effectiveness of exploring document-
level hierarchical structure of resume and choose 
the best IE models (HMM vs. classification) for 
each sub-task. 
z Cascaded model vs. flat model 
Two flat models with different IE methods 
(SVM and HMM) are designed to extract personal 
detailed information and educational detailed 
information respectively. In these models, no 
hierarchical structure is used and the detailed 
information is extracted from the entire resume 
texts rather than from specific blocks. These two 
flat models will be compared with our proposed 
cascaded model. 
z Model selection for different IE tasks 
Both SVM and HMM are tested for all the IE 
tasks in first pass and in second pass.  
4.2 Cascaded Model vs. Flat Model 
We tested the flat model and cascaded model 
with detailed information extraction to verify the 
effectiveness of exploring document-level 
hierarchical structure. Results (see Table 2) show 
that with the cascaded model, the precision is 
greatly improved compared with the flat model 
with identical IE method, especially for 
educational detailed information. Although there is 
some loss in recall, the average F-score is still 
largely improved in the cascaded model.  
4.3 Model Selection for Different IE Tasks 
Then we tested different models for the general 
information and detailed information to choose the 
most appropriate IE model for each sub-task.  
Model Avg.P (%) Avg.R (%) 
SVM 80.95 72.87 
HMM 75.95 75.89 
Table 3. General information extraction with 
different models. 
Personal Detailed 
Info 
Educational 
Detailed Info Model Avg.P 
(%) 
Avg.R 
(%) 
Avg.P 
(%) 
Avg.R 
(%) 
SVM 86.83 76.89 67.36 66.21 
HMM 79.64 60.16 70.78 76.80 
Table 4. Detailed information extraction with 
different models. 
Results (see Table 3) show that compared with 
SVM, HMM achieves better recall. In our 
cascaded framework, the extraction range of 
detailed information is influenced by the result of 
general information extraction. Thus better recall 
of general information leads to better recall of 
detailed information subsequently. For this reason, 
504
we choose HMM in the first pass of our cascaded 
hybrid model. 
Then in the second pass, different IE models are 
tested in order to select the most appropriate one 
for different sub-tasks. Results (see Table 4) show 
that HMM performs much better in both precision 
and recall than SVM for educational detailed 
information extraction. We think that this is 
reasonable because HMM takes into account the 
sequence constraints among educational detailed 
information types. Therefore HMM model is 
selected to extract educational detailed information 
in our cascaded hybrid model. While for the 
personal detailed information extraction, we find 
that the SVM model gets better precision and 
recall than HMM model. We think that this is 
because of the independent occurrence of personal 
detailed information. Therefore, we select SVM to 
extract personal detailed information in our 
cascaded model. 
5 Discussion 
Our cascaded framework is a ?pipeline? 
approach and it may suffer from error propagation. 
For instance, the error in the first pass may be 
transferred to the second pass when determining 
the extraction range of detailed information. 
Therefore the precision and recall of detailed 
information extraction in the second pass may be 
decreased subsequently. But we are not sure 
whether N-Best approach (Zhai et al, 2004) would 
be helpful. Because our cascaded hybrid model 
applies different IE methods for different sub-tasks, 
it is difficult to incorporate the N-best strategy by 
either simply combining the scores of the first pass 
and the second pass, or using the scores of the 
second pass to do re-ranking to select the best 
results. Instead of using N-best, we apply a fuzzy 
block selection strategy to enlarge the search scope. 
Experimental results of personal detailed 
information extraction show that compared with 
the exact block selection strategy, this fuzzy 
strategy improves the average recall of personal 
detailed information from 68.48% to 71.34% and 
reduce the average precision from 83.27% to 
81.71%. Therefore the average F-score is 
improved by the fuzzy strategy from 75.15% to 
76.17%.  
Features are crucial to our SVM model. For 
some fields (such as Name, Address and 
Graduation School), only using words as features 
may result in low accuracy in IE. The named 
entity (NE) features used in our model enhance the 
accuracy of detailed information extraction. As 
exemplified by the results (see Table 5) on 
personal detailed information extraction, after 
adding named entity features, the F-score are 
improved greatly.  
Field Word +NE (%)  Word  (%)
Name 90.22 3.11 
Birthday 87.31 84.82 
Address 67.76 49.16 
Phone 81.57 75.31 
Mobile 70.64 58.01 
Email 88.76 85.96 
Registered Residence 75.97 72.73 
Residence 51.61 42.86 
Graduation School 40.96 15.38 
Degree 73.20 63.16 
Major 63.09 43.24 
Table 5. Personal detailed information extraction 
with different features (Avg.F). 
In our cascaded hybrid model, we apply HMM 
and SVM in different pass separately to explore 
the contextual structure of information types. It 
guarantees the simplicity of our hybrid model. 
However, there are other ways to combine state-
based and discriminative ideas. For example, Peng 
and McCallum (2004) applied Conditional 
Random Fields to extract information, which 
draws together the advantages of both HMM and 
SVM. This approach could be considered in our 
future experiments. 
Some personal detailed information types do not 
achieve good average F-score in our model, such 
as Zip code (74.50%) and Mobile (73.90%). Error 
analysis shows that it is because these fields do not 
contain distinguishing words and named entities. 
For example, it is difficult to extract Mobile from 
the text ?Phone: 010-62617711 (13859750123)?. 
But these fields can be easily distinguished with 
their internal characteristics. For example, Mobile 
often consists of certain length of digital figures. 
To identify these fields, the Finite-State 
Automaton (FSA) that employs hand-crafted 
grammars is very effective (Hsu and Chang, 1999). 
Alternatively, rules learned from annotated data 
are also very promising in handling this case 
(Ciravegna and Lavelli, 2004).  
We assume the independence of words 
occurring in unit ti to calculate the probability 
505
P(ti|li) in HMM model. While in Bikel et al (1999), 
a bi-gram model is applied where each word is 
conditioned on its immediate predecessor when 
generating words inside the current name-class. 
We will compare this method with our current 
method in the future.  
6 Conclusions and Future Work 
We have shown that a cascaded hybrid model 
yields good results for the task of information 
extraction from resumes. We tested different 
models for the first pass and the second pass, and 
for different IE tasks. Our experimental results 
show that the HMM model is effective in handling 
the general information extraction and educational 
detailed information extraction, where there exists 
strong sequence of information pieces. And the 
SVM model is effective for the personal detailed 
information extraction.  
We hope to continue this work in the future by 
investigating the use of other well researched IE 
methods. As our future works, we will apply FSA 
or learned rules to improve the precision and recall 
of some personal detailed information (such as Zip 
code and Mobile). Other smoothing methods such 
as (Bikel et al 1999) will be tested in order to 
better overcome the data sparseness problem. 
7 Acknowledgements 
The authors wish to thank Dr. JianFeng Gao, Dr. 
Mu Li, Dr. Yajuan Lv for their help with the LSP 
tool, and Dr. Hang Li, Yunbo Cao for their 
valuable discussions on classification approaches. 
We are indebted to Dr. John Chen for his 
assistance to polish the English.  We want also 
thank Long Jiang for his assistance to annotate the 
training and testing data. We also thank the three 
anonymous reviewers for their valuable comments. 
References 
A.Berger. Error-correcting output coding for text 
classification. 1999. In Proceedings of the IJCAI-99 
Workshop on Machine Learning for Information Filtering. 
D.M.Bikel, R.Schwartz, R.M.Weischedel. 1999. An algorithm 
that learns what?s in a name. Machine Learning, 
34(1):211-231. 
V.Borkar, K.Deshmukh and S.Sarawagi. 2001. Automatic 
segmentation of text into structured records. In 
Proceedings of ACM SIGMOD Conference. pp.175-186. 
F.Ciravegna, A.Lavelli. 2004. LearningPinocchio: adaptive 
information extraction for real world applications. Journal 
of Natural Language Engineering, 10(2):145-165. 
A.Finn and N.Kushmerick. 2004. Multi-level boundary 
classification for information extraction. In Proceedings of 
ECML04. 
P.Frasconi, G.Soda and A.Vullo. 2001. Text categorization 
for multi-page documents: a hybrid Na?ve Bayes HMM 
approach. In Proceedings of the 1st ACM/IEEE-CS Joint 
Conference on Digital Libraries. pp.11-20. 
D.Freitag and A.McCallum. 1999. Information extraction 
with HMMs and shrinkage. In AAAI99 Workshop on 
Machine Learning for Information Extraction. pp.31-36. 
W.Gale. 1995. Good-Turing smoothing without tears. Journal 
of Quantitative Linguistics, 2:217-237. 
J.F.Gao, M.Li and C.N.Huang. 2003. Improved source-
channel models for Chinese word segmentation. In 
Proceedings of ACL03. pp.272-279. 
C.N.Hsu and C.C.Chang. 1999.  Finite-state transducers for 
semi-structured text mining. In Proceedings of IJCAI99 
Workshop on Text Mining: Foundations, Techniques and 
Applications. pp.38-49. 
T.Joachims. 1999. Making large-scale SVM learning practical. 
Advances in Kernel Methods - Support Vector Learning. 
MIT-Press. 
S.M.Katz. 1987. Estimation of probabilities from sparse data 
for the language model component of a speech recognizer. 
IEEE ASSP, 35(3):400-401. 
N.Kushmerick, E.Johnston and S.McGuinness. 2001. 
Information extraction by text classification. In IJCAI01 
Workshop on Adaptive Text Extraction and Mining. 
A.Lavelli, M.E.Califf, F.Ciravegna, D.Freitag, C.Giuliano, 
N.Kushmerick and L.Romano. 2004. A critical survey of 
the methodology for IE evaluation. In Proceedings of the 
4th International Conference on Language Resources and 
Evaluation.  
F.Peng and A.McCallum. 2004. Accurate information 
extraction from research papers using conditional random 
fields. In Proceedings of HLT/NAACL-2004. pp.329-336. 
L.Peshkin and A.Pfeffer. 2003. Bayesian information 
extraction network. In Proceedings of IJCAI03. pp.421-
426. 
F.Sebastiani. 2002. Machine learning in automated text 
categorization. ACM Computing Surveys, 34(1):1-47. 
A.D.Sitter and W.Daelemans. 2003. Information extraction 
via double classification. In Proceedings of ATEM03. 
L.Zhai, P.Fung, R.Schwartz, M.Carpuat and D.Wu. 2004. 
Using N-best lists for named entity recognition from 
Chinese speech. In Proceedings of HLT/NAACL-2004. 
506
Coling 2010: Poster Volume, pages 1417?1425,
Beijing, August 2010
Semi-automatically Developing Chinese HPSG Grammar from the Penn Chinese Treebank for Deep Parsing 
Kun Yu1 Yusuke Miyao2 Xiangli Wang1 Takuya Matsuzaki1 Junichi Tsujii1,3 1. The University of Tokyo 2. National Institute of Informatics yusuke@nii.ac.jp {kunyu, xiangli, matuzaki, tsujii} @is.s.u-tokyo.ac.jp 3. The University of Manchester  Abstract In this paper, we introduce our recent work on Chinese HPSG grammar development through treebank conversion. By manually defining grammatical constraints and anno-tation rules, we convert the bracketing trees in the Penn Chinese Treebank (CTB) to be an HPSG treebank. Then, a large-scale lexi-con is automatically extracted from the HPSG treebank. Experimental results on the CTB 6.0 show that a HPSG lexicon was successfully extracted with 97.24% accu-racy; furthermore, the obtained lexicon achieved 98.51% lexical coverage and 76.51% sentential coverage for unseen text, which are comparable to the state-of-the-art works for English. 1 Introduction Precise, in-depth syntactic and semantic analysis has become important in many NLP applications. Deep parsing provides a way of simultaneously obtaining both the semantic relation and syntac-tic structure. Thus, the method has become more popular among researchers recently (Miyao and Tsujii, 2006; Matsuzaki et al, 2007; Clark and Curran, 2004; Kaplan et al, 2004).  This paper introduces our recent work on deep parsing for Chinese, specifically focusing on the development of a large-scale grammar, based on the HPSG theory (Pollard and Sag, 1994). Be-cause it takes a decade to manually develop an HPSG grammar that achieves sufficient coverage for real-world text, we use a semi-automatic ap-proach, which has successfully been pursued for English (Miyao, 2006; Miyao et al, 2005; Xia, 1999; Hockenmaier and Steedman, 2002; Chen and Shanker, 2000; Chiang, 2000) and other lan-guages (Guo et al, 2007; Cramer and Zhang, 2009; Hockenmaier, 2006; Rehbei and Genabith, 2009; Schluter and Genabith, 2009).  The following lists our method of approach: (1) define a skeleton of the grammar (in this 
work, the structure of sign, grammatical princi-ples and schemas), (2) convert the CTB (Xue et al, 2002) into an HPSG-style treebank, (3) automatically extract a large-scale lexicon from the obtained treebank. Experiments were performed to evaluate the quality of the grammar developed from the CTB 6.0. More than 95% of the sentences in the CTB could be successfully converted, and the ex-tracted lexicon was 97.24% accurate. The ex-tracted lexicon achieved 98.51% lexical coverage and 76.51% sentential coverage for unseen text, which are comparable to the state-of-the-art works for English. Since grammar engineering has many specific problems in each language, although we used the similar method applied in other languages to de-velop a Chinese HPSG grammar, it is very dif-ferent from applying, such as statistical parsing models, to a new language. Lots of efforts have been done for the specific characteristics of Chi-nese. The contribution of our work is to describe these issues. As a result, a skeleton design of Chinese HPSG is proposed, and for the first time, a robust and wide-coverage Chinese HPSG grammar is developed from real-world text.  2 Design of Grammatical Constraints for Chinese HPSG Because of the lack of a comprehensive HPSG-based syntactic theory for Chinese, we extended the original HPSG (Pollard and Sag, 1994) to analyze the specific linguistic phenomena in Chinese. Due to space limitations, we will pro-vide a brief sampling of our extensions, and dis-cuss several selected constructions.  2.1 Sign, Principles, and Schemas Sign, which is a data structure to express gram-matical constraints of words/phrases, is modified and extended for the analysis of Chinese specific constructions, as shown in Figure 1. PHON, MOD, SPEC, SUBJ, MARKING, and SLASH are 
1417
features defined in the original HPSG, and they represent the phonological information of a word, the constraints on the modifiee, the speci-fiee, the subject, the marker, and the long-distance dependency, respectively. COMPS, which represents the constraints on comple-ments, is divided into LCOMPS and RCOMPS, to distinguish between left and right comple-ments. Aspect, question, and negation particles are treated as markers as done in (Gao, 2000), which are distinguished by ASPECT, QUESTION, and NEGATION. CONT is also originated from Pollard and Sag (1994), although it is used to represent semantic structures with predicate-argument dependencies. TOPIC and CONJ are extended features that represent the constraints on the topic and the conjuncts of co-ordination. FILLER is another extended feature that records the grammatical function of the moved argument in a long-distance dependency. 
 Figure 1. HPSG sign for Chinese. The principles, including Phonology Princi-ple, Valence Principle, Head Feature Principle, and Nonlocal Feature Principle, are imple-mented in our Chinese HPSG grammar as de-fined in (Pollard and Sag, 1994). Semantic Principle is slightly modified so that it composes predicate-argument structures. 14 schemas are defined in our grammar, among which the Coord-Empty-Conj Schema, Relative-Head Schema, Empty-Relativizer Schema, and Topic-Head Schema are designed specifically for Chinese. The other 10 schemas are borrowed from the original HPSG theory. 15 Chinese constructions are considered in our current grammar (refer to Table 1). A detailed description of some particular constructions will be provided in the following subsection.  2.2 An HPSG Analysis for Chinese  2.2.1 BA Construction The BA construction moves the object of a verb to the pre-verbal position. For example, the sen-
tence in Figure 2 with the original word order is ??/I ?/read ? ?/book?. There were three popular ways to address the BA construction: as a verb (Huang, 1991; Bender, 2000), preposition (Gao, 1992), and case marker (Gao, 2000). Since the aspect markers, such as ???, cannot attach to BA, we exclude the analysis of treating BA as a verb. Because BA, like prepositions, always ap-pears before a noun phrase, we therefore follow the analysis in Gao (1992), and treat BA as a preposition. As shown in Figure 2, BA takes a moved object as a complement, and attaches to the verb as a left-complement. 
 (I read the book.) Figure 21. Analysis of BA construction. 2.2.2 BEI Construction The BEI construction is used to make the passive voice of a sentence. Because the aspect marker also cannot attach to BEI, we do not treat BEI as a verb, as done in the CTB. Similar to the analy-sis of BA construction, we regard BEI as a preposition that attaches to the verb as a left-complement. Additionally, because we can insert a clause ???/Li ?/send ?/person? between the moved object ??/he? and the verb ??/beat?, as is the case for ??/he ?/BEI ??/Li ?/send ?/person ?/beat ? (He was beaten by the person that is sent by Li)?, we treat the relation between the moved object and the verb as a long-distance dependency. Figure 3 exem-plifies our analysis of the BEI construction, in which the Filler-Head Schema is used to handle the long-distance dependency, and the FILLER feature is used to record that the role of the moved argument. 
 (The book is read by me.)  Figure 3. Analysis of BEI construction.  2.2.3 Topic Construction As indicated in Li and Thompson (1989), a topic refers to the theme of a sentence, which always                                                            1 In the figures in this paper, we will show only selected features that are relevant to the explanation.  
1418
appears before the subject. The difference be-tween the topic and subject is the subject must always have a direct semantic relationship with the verb in a sentence, whereas the topic does not. There are two types of topic constructions. In the first type, the topic does not fill any argu-ment slots of the verb, such as the topic ???/elephant? in Figure 4. In the second type, the topic has a semantic relationship with the verb. For example, in the sentence ??/he ?/I ??/like (I like him)?, the topic ??/he? is also an object of ???/like?. For the first type, we define the Topic-Head Schema to describe the topic construction (refer to Figure 4). For the second type, we follow the same analysis as in English, and use the Filler-Head Schema.   
 (The nose of an elephant is long.) Figure 4. Analysis of topic construction. 2.2.4 Serial Verb Construction In contrast to the definition of serial verb con-struction in Li and Thompson (1989), we specify a serial verb construction as a special type of verb phrase coordination, which describes sev-eral separate events with no conjunctions inside. Similar to ordinary coordination, the verb phrases in a serial verb construction share the same syntactic subject (Muller and Lipenkova, 2009), topic, and left-complement. We define Coord-Empty-Conj Schema to deal with it. Fig-ure 5 shows an example analysis. 
 (I go to the book store and buy a book.) Figure 5. Analysis of serial verb construction. 2.2.5 Relative Clause In Chinese, a relative clause is marked by a rela-tivizer ??? and exists in the left of the head noun. Because Chinese noun phrases are right-headed in general, we analyze a relative clause as a nominalization that modifies a head noun (Li and Thompson, 1989). Inside of a relative clause, the relativizer is treated as head. When the rela-tivizer is omitted, we define a unary schema, Empty-Relativizer Schema, which functions by combining a relative clause with an empty rela-
tivizer. Furthermore, we introduce a Relative-Head Schema to handle the long-distance de-pendency for the extracted argument2 (refer to Figure 6).  
 (the book that I buy) Figure 6. Analysis of relative clause. 3 Converting the CTB into an HPSG Treebank 3.1 Partially-specified Derivation Tree Annotation In order to convert the CTB into an HPSG tree-bank, we first annotate the bracketing trees in the CTB to be partially-specified derivation trees3, which conform to the grammatical constraints designed in Section 2. Three types of rules are defined to fulfill this annotation. 
 (I read the book that he wrote.) Figure 7. The CTB annotation for a sentence. 
 Figure 8. Partially-specified derivation tree for Figure 7. For example, Figure 7 shows the bracketing tree of a sentence in the CTB, while Figure 8 shows the partially-specified derivation tree after re-annotation.                                                            2 The extracted adjunct is not treated as a long-distance dependency in our current grammar. 3 Partially-specified derivation tree means a tree structure that is annotated with schema names and some features of the HPSG signs (Miyao, 2006). 
1419
3.1.1 Rules for Annotation Conversion  In the CTB, there exist some annotations that do not coincide with our HPSG analysis for Chi-nese. Therefore, we define pattern rules to con-vert the annotations in the CTB to fit with our HPSG analysis. 76 annotation rules are defined for 15 Chinese constructions (refer to Table 2). Due to page constraints, we focus on the constructions that we discussed in Section 2. Construction Rule # Relative clause 20 BEI construction 21 Coordination 7 Subject/object control 5 Non-verbal predicate 4 Logical subject 3 Right node raising 3 Parenthesis 3 BA construction 3 Aspect/question/negation particle 2 Subordination 1 Serial Verb construction 1 Modal verb 1 Topic construction 1 Apposition 1 Table 1. Chinese constructions and annotation rules. Rules for BA and BEI Construction As analyzed in Section 2, we treat BA and BEI as prepositions that attach to the verb as left-complements. However, in the CTB, BA and BEI are annotated as verbs that take a sentential complement (Xue and Xia, 2000). By applying the annotation rules, the BA/BEI and the subject of the sentential complement of BA/BEI are re-annotated as a prepositional phrase (as indicated in the dash-boxed part in Figure 9). 
 (I read the book.) Figure 9. Conversion of BA construction. 
 (He is regarded as a friend by me.) Figure 10. Verb division in BEI construction. In addition, in the CTB, some BA/BEI con-structions are not annotated with trace, which 
makes it difficult to retrieve the semantic relation between the verb and the moved object. The principal reason for this is that the moved object in these constructions has a semantic relation with only part of the verb. For example, in Fig-ure 10, the moved noun ??/he? is the object of ??/regard?, but not for ???/regard as?. Analy-sis shows that only a closed set of characters (e.g. ??/as?)  can be attached to verbs in such a case. Therefore, we manually collect these char-acters from the CTB, and then define pattern rules to automatically split the verb, which ends with the collected characters, in the BA and BEI construction. Finally, we annotate trace for the split verb. Figure 10 exemplifies the conversion of an example sentence. Rules for Topic Construction In the CTB, a functional tag ?TPC? is used to indicate a topic (Xue and Xia, 2000). Therefore, we use this functional tag to detect topic phrases during conversion. Rules for Serial Verb Construction We define pattern rules to detect the parallel verb phrases with no conjunction inside (as shown in Figure 11), and treat these verb phrases as a se-rial verb construction. However, when the verb in the first phrase is a modal verb, such as the case of ??/I ?/want to ??/sing (I want to sing)?, the parallel verb phrases should not be treated as a serial verb construction. Therefore, a list of modal verbs is manually collected from the CTB to filter out these exceptional cases dur-ing conversion.  
 (go downstairs and eat meal) Figure 11. An example of parallel verb phrases. Rules for Relative Clause  
 (the book that he wrote) Figure 12. Conversion of relative clause. We define annotation rules to slightly modify the annotation of a relative clause in CTB, as shown in Figure 12, to make the tree structure easy to be 
1420
analyzed. Furthermore, in CTB, relative clauses are annotated with both extracted arguments and extracted adjuncts. But in our grammar, we only deal with extracted arguments, and the gap in a relative clause (as indicated in the dash-boxed part in Figure 12). When the extracted phrase is an adjunct of the relative clause, we simply view the clause as a modifier of the extracted phrase. 3.1.2 Rules for Correcting Inconsistency  There are some inconsistencies in the annotation of the CTB, which presents difficulties for per-forming the derivation tree annotation. There-fore, we define 49 rules, as done in (Hockenmaier and Steedman, 2002) for English, to mitigate inconsistencies before annotation (re-fer to Table 3).  3.1.3 Rules for Assisting Annotation We also define 48 rules (refer to Table 2), which are similar to the rules used in (Miyao, 2006) for English, to help the derivation tree annotation. For example, 12 pattern rules are defined to as-sign the schemas to corresponding constituents. Rule Type Rule Description Rule # Fix tree annotation 37 Fix phrase tag annotation 5 Fix functional tag annotation 5 Rules for correcting inconsistent annotation Fix POS tag annotation 2 Slash recognization 27 Schema assignment 12 Head/Argument/Modifier marking 8 Rules for assisting  annotation Binarization 1 Table 2. Rules for correcting inconsistency and assisting annotation. 3.2 HPSG Treebank Acquisition In this phase, the schemas and principles are ap-plied to the annotated partially-specified trees, in order to fill out unspecified constraints and vali-date the consistency of the annotated constraints. In effect, an HPSG treebank is obtained. For instance, by applying the Head-Complement Schema to the dash-boxed nodes in Figure 8, the constraints of the right daughter are percolated to RCOMPS of the left daughter (as indicated as 4 in Figure 13). After applying the schemas and the principles to the whole tree in Figure 8, a HPSG derivation tree is acquired (re-fer to Figure 13).  3.3 Lexicon Extraction  With the HPSG treebank acquired in Section 3.2, we automatically collect lexical entries as the combination of words and lexical entry templates from the terminal nodes of the derivation trees. For example, from the HPSG derivation tree 
shown in Figure 13, we obtain a lexical entry for the word ??/write? as shown in Figure 14. 
 Figure 13. HPSG derivation tree for Figure 8. 
 Figure 14. Lexical entry extracted for the word ??/write?. 3.3.1 Lexical Entry Template Expansion 
 (a) Lexical entry template for the verb in BEI construction 
 (b) Lexical entry template for the verb in original word order Figure 15. Application of a lexical rule. Some Chinese constructions change the word order of sentences, such as the BA/BEI construc-tions. Therefore, we apply lexical rules (Naka-nishi et al, 2004) to the lexical entry templates to convert them into those for the original word order, and expand the lexical entry templates consequently. 18 lexical rules are defined for the verbs in the BA/BEI constructions. For example, by applying a lexical rule to the lexical entry template in Figure 15(a), the moved object indi-
1421
cated by SLASH is restored into RCOMPS, and the subject introduced by BEI in LCOMPS is restored into SUBJ (refer to Figure 15(b)). 3.3.2 Mapping of Semantics In our grammar, we use predicate-argument de-pendencies for semantic representation. 44 types of predicate-argument relations are defined to represent the semantic structures of 13 classes of words. For example, we define a predicate-argument relation ?verb_arg12?, in which a verb takes two arguments ?ARG1? and ?ARG2?, to ex-press the semantics of transitive verbs. 72 se-mantics mapping rules are defined to associate these predicate-argument relations with the lexi-cal entry templates. Figure 16 exemplifies a se-mantics mapping rule. The input of this rule is the lexical entry template (as shown in the left part), and the output is a predicate-argument rela-tion ?verb_arg12? (as shown in the right part), which associates the syntactic arguments SUBJ and SLASH with the semantic arguments ARG1 and ARG2 (as indicated by 1 and 2 in Figure 16). 
 Figure 16. A semantics mapping rule. 4 Evaluation 4.1 Experimental Setting We used the CTB 6.0 for HPSG grammar devel-opment and evaluation. We split the corpus into development, testing, and training data sets, fol-lowing the recommendation from the corpus author. The development data was used to tune the design of grammar constraints and the anno-tation rules. However, the testing data set was reserved for further evaluation on parsing. Thus, the training data was further divided into two parts for training and testing in this work. During the evaluation, unknown words were handled in the same way as done in (Hockenmaier and Steedman, 2002).  4.2 Evaluation Metrics In order to verify the quality of the grammar de-veloped in our work, we evaluated the extracted lexicon by the accuracy for assessing the semi-automatic conversion process, and the coverage for quantifying the upper-bound coverage of the future HPSG parser based on this grammar.  The accuracy of the extracted lexicon was evaluated by lexical accuracy, which counts the 
number of the correct lexical entries among all the obtained lexical entries.  In addition, two evaluation metrics as used in (Hockenmaier and Steedman, 2002; Xia, 1999; Miyao, 2006) were used to evaluate the coverage of the obtained lexicon. The first one is lexical coverage (Hockenmaier and Steedman, 2002; Xia, 1999), which means that the percentage that the lexical entries extracted from the testing data are covered by the lexical entries acquired from the training data. The second one is sentential coverage (Miyao, 2006): a sentence is consid-ered to be covered only when the lexical entries of all the words in this sentence are covered.  4.3 Results of Accuracy Since there was no gold standard data for the automatic evaluation of accuracy, we randomly selected 100 sentences from the testing data, and manually checked the lexical entries extracted from these sentences. Results show that 1,558 lexical entries were extracted at 97.24% (1,515/1,558) accuracy.  Error analysis shows all the incorrect lexical entries came from the error in the derivation tree annotation. For example, our current design failed to find the correct boundary of coordinated noun phrases when the word ??/etc? was at-tached at the end, such as ???/property right ??/selling ? ??/assets ??/renting ?/etc (property right selling and assets renting etc.)?. We will improve the derivation tree anno-tation to solve this issue. 4.4 Results of Coverage Table 3 shows the coverage of the extracted lexi-cal entries, which indicates that a large HPSG lexicon was successfully extracted from the CTB for unseen text, with reasonable coverage. The statistics of the HPSG lexicon extraction in our experiments (refer to Table 4) also indicates that we successfully extracted lexical entries from more than 95% of the sentences in the CTB.  Among all the uncovered lexical entries, 78.55% are for content words, such as verb and noun. In addition, the classification of uncovered lexical entries in Table 4 indicates that about 1/3 of the uncovered lexical entries came from the unknown lexical entry templates (?+w/-t?). We analyzed the 193 ?+w/-t? failures in the testing data, among which 169 failures resulted from the shortage of training data, which indicated that the correct lexical entry template did not appear in 
1422
the training data. The learning curve in Figure 17 shows that we can resolve this issue by enlarging the training data. The other 24 failures came from the error in the derivation tree annotation. For example, our current grammar failed at de-tecting the coordinated clauses when they were separated by a colon. We will be able to reduce this type of failure by improving the derivation tree annotation. Uncovered Lexical Entries Sent. Cov. Lex. Cov. +w/+t +w/-t 76.51% 98.51% 1.05% 0.43% Table 34. Coverage of extracted HPSG lexicon. Data Set Total Sent # Succeed Sent # Word # Lexical Entry Template # Training 20,230 19,257(95.19%) 510,815 4,836 Develop 2,067 2,009(97.19%) 55,714 1,582 Testing 2,000 1,941(97.05%) 44,924 1,163 Table 4. Statistics of HPSG lexicon extraction.  
 Figure 17. Lexical coverage (Y axis) vs. corpus size (X axis). 
 Figure 18. A lexical entry template extracted from testing data. The other type of failures (?+w/+t?) indicate that a word was incorrectly associated with a lexical entry template, even though both of them existed in the training data. Error analysis shows that 64.39% of failures were related to verbs. For example, for a relative clause ???/invest ??/Taiwan ? ??/businessman (the busi-nessman that invests Taiwan)? in the testing data, we associated a lexical entry template as shown in Figure 18 with the verb ???/invest?. In the training data, however, the lexical entry template shown in Figure 18 cannot be extracted for ???/invest?, since this word never appears in a relative clause with an extracted subject. Intro-ducing lexical rules to expand the lexical entry template of verbs in a relative clause is a possible way to solve this problem. 4.5 Comparison with Previous Work Guo?s work (Guo et al, 2007; Guo, 2009) is the only previous work on Chinese lexicalized                                                            4 ?+w/+t? means both the word and lexical entry template have been seen in the lexicon. ?+w/-t? means only the word has been seen in the lexicon (Hockenmaier and Steedman, 2002). 
grammar development from the CTB, which in-duced wide-coverage LFG resources from the CTB. By using the hand-made gold-standard f-structures of 200 sentences from the CTB 5.1, the LFG f-structures developed in Guo?s work achieved 96.34% precision and 96.46% recall for unseen text (Guo, 2009). In our work, we applied the similar strategy in evaluating the accuracy of the developed Chinese HPSG grammar, which achieved 97.24% lexical accuracy on 100 unseen sentences from the CTB 6.0. When evaluating the coverage of our grammar, we used a much larger data set (including 2,000 unseen sen-tences), and achieved 98.51% lexical coverage. Although these results cannot be compared to Guo?s work directly because of the different size and content of data set, it indicates that the Chi-nese HPSG grammar developed in our work is comparable in quality with Guo?s work. In addition, there were previous works about developing lexicalized grammar for English. Considering the small size of the CTB, in com-parison to the Penn Treebank used in the previ-ous works, the results listed in Table 5 verify that, the quality of the Chinese HPSG grammar developed in our work is comparable to these previous works.  Previous Work Sent. Cov. Lex. Cov. Miyao (2006) 82.50% 98.97% Hockenmaier and Steedman (2002) - 98.50% Xia (1999) - 96.20% Table 5. Evaluation results of previous work.  4.6 Discussion There are still some sentences in the CTB from which we failed to extract lexical entries. We analyzed the 59 failed sentences in the testing data and listed the reasons in Table 6.  Reason Sent # Error in the derivation tree annotation 31 Short of semantics mapping rule 23 Inconsistent annotation in the CTB 5 Table 6. Reasons for lexicon extraction failures. The principal reason for 31 sentence failures, is the error in the derivation tree annotation. For instance, our current annotation rules could con-vert the regular relative clause shown in Figure 12. Nonetheless, when the relative clause is in-side of a parenthesis, such as ?? ??/primitive ? ???/method (the method that is primi-tive)?, the annotation rules failed at finding the extracted head noun to create a derivation tree. This type of failure can be reduced by improving the annotation rules. 
1423
The second reason, for which 23 sentences failed, is the shortage of the semantics mapping rules. For example, we did not define semantics mapping rule for a classifier that acts as a predi-cate with two topics. This type of failure can be reduced by adding semantic mapping rules.  The last reason for sentence failures is incon-sistencies in the CTB annotation. In our future work, these inconsistencies will be collected to enrich our inconsistency correction rules.  In addition to the reasons above, some sen-tences with special constructions in the devel-opment and training data also could not be analyzed by our current grammar, since the spe-cial construction is difficult for the current HPSG to analyze. The special constructions include the argument-cluster coordination shown in Figure 19. Introducing the similar rules used in CCG (Hockenmaier and Steedman, 2002) could be a possible solution to this problem. 
 (have 177 intrant projects and 6.4 billion investments) Figure 19. An argument-cluster coordination in CTB. 5 Related Work  To the extent of our knowledge, the only previ-ous work about developing Chinese lexicalized grammar from treebanks is Guo?s work (Guo et al, 2007; Guo, 2009). An LFG-based parsing using wide-coverage LFG approximations in-duced from the CTB was done in this work. However, they did not train a deep parser based on the LFG resources obtained in their work, but relied on an external PCFG parser to create c-structure trees, and then mapped the c-structure trees into f-structures using their annotation rules (Guo, 2009). In contrast to Guo?s work, we paid particular attention to a different grammar framework, i.e. HPSG, with the analysis of more Chinese constructions, such as the serial verb construction. In addition, in our on-going deep parsing work, we use the developed Chinese HPSG grammar, i.e. the lexical entries, to train a full-fledged HPSG parser directly. Additionally, there are some works that induce lexicalized grammar from corpora for other lan-guages. For example, by using the Penn Tree-bank, Miyao et al (2005) automatically extracted a large HPSG lexicon, Xia (1999), Chen and Shanker (2000), Hockenmaier and Steedman (2002), and Chiang (2000) invented LTAG/CCG 
specific procedures for lexical entry extraction. From the German Tiger corpus, Cramer and Zhang (2009) constructed a German HPSG grammar; Hockenmaier (2006) created a German CCGbank; and Rehbei and Genabith (2009) ac-quired LFG resources. In addition, Schluter and Genabith (2009) automatically obtained wide-coverage LFG resources from a French Tree-bank. Our work implements a similar idea to these works, but we apply different grammar design and annotation rules, which are specific to Chinese. Furthermore, we obtained a compara-tive result to state-of-the-art works for English.  There are some researchers who worked on Chinese HPSG grammar development manually. Zhang (2004) implemented a Chinese HPSG grammar using the LinGO Grammar matrix (Bender et al, 2002). Only a few basic construc-tions were considered, and a small lexicon was constructed in this work. Li (1997) and Wang et al (2009) designed frameworks for Chinese HPSG grammar; however, only small grammars were implemented in these works. Furthermore, some linguistic works focused mainly on the discussion of specific Chinese constructions in the HPSG or LFG framework, without implementing a grammar for real-world text (Bender, 2000; Gao, 2000; Li and McFe-tridge, 1995; Li, 1995; Xue and McFetridge, 1995; Wang and Liu, 2007; Ng, 1997; Muller and Lipenkova, 2009; Liu, 1996; Kit, 1998). 6 Conclusion and Future Work  In this paper, we described the semi-automatic development of a Chinese HPSG grammar from the CTB. Grammatical constraints are first de-signed by hand. Then, we convert the bracketing trees in the CTB into an HPSG treebank, by us-ing pre-defined annotation rules. Lastly, we automatically extract lexical entries from the HPSG treebank. We evaluated our work on the CTB 6.0. Results indicated that a large HPSG lexicon was successfully extracted with a 97.24% accuracy. Furthermore, our grammar achieved 98.51% lexical coverage and 76.51% sentential coverage for unseen text.  This is an ongoing work, and there are some future works under consideration, including en-riching the design of annotation rules, introduc-ing more semantics mapping rules, and adding lexical rules. In addition, the work on Chinese HPSG parsing is on-going, within which the Chinese HPSG grammar developed in this work will be available soon. 
1424
References  Emily Bender. 2000. The Syntax of Madarin Ba: Reconsid-ering the Verbal Analysis. Journal of East Asian Lin-guistics. 9(2): 105-145. Emily Bender, Dan Flickinger, and Stephan Oepen. 2002. The Grammar Matrix: An Open-source Starter-lit for the Rapid Development of Cross-linguistically Consistent Broad-coverage Precision Grammars. Procedings of the Workshop on Grammar Engineering and Evaluation. John Chen and Vijay K. Shanker. 2004. Automated Extrac-tion of TAGs from the Penn Treebank. Proceedings of the 6th IWPT. David Chiang. 2000. Statistical Parsing with an Automati-cally-extracted Tree Adjoining Grammar. Proceedings of the 38th ACL. 456-463. Stephen Clark and James R. Curran. 2004. Parsing the WSJ Using CCG and Log-linear Models. Proceedings of the 42nd ACL. Bart Cramer and Yi Zhang. 2009. Construction of a German HPSG Grammar from a Detailed Treebank. Proceedings of the 2009 Workshop on Grammar Engineering Across Frameworks. Qian Gao. 1992. Chinese Ba Construction: its Syntax and Semantics. Technical report.  Qian Gao. 2000. Argument Structure, HPSG and Chinese Grammar. Ph.D. Thesis. Ohio State University. Yuqing Guo. 2009. Treebank-based acquisition of Chinese LFG Resources for Parsing and Generation. Ph.D. The-sis. Dublin City University. Yuqing Guo, Josef van Genabith and Haifeng Wang. 2007. Acquisition of Wide-Coverage, Robust, Probabilistic Lexical-Functional Grammar Resources for Chinese. Proceedings of the 12th International Lexical Functional Grammar Conference (LFG 2007). 214-232. Julia Hockenmaier. 2006. Creating a CCGbank and a wide-coverage CCG lexicon for German Proceedings of COLING/ACL 2006. Julia Hockenmaier and Mark Steedman. 2002. Acquiring Compact Lexicalized Grammars from a Cleaner Tree-bank. Proceedings of the 3rd LREC. C-R Huang. 1991. Madarin Chinese and the Lexical Map-ping Theory: A Study of the Interaction of Morphology and Argument Changing. Bulletin of the Institute of His-tory and Philosophy 62. Ronald M. Kaplan et al 2004. Speed and Accuracy in Shal-low and Deep Stochastic Parsing. Proceedings of HLT/NAACL 2004. Chunyu Kit. 1998. Ba and Bei as Multi-valence Preposi-tions in Chinese. Studia Linguistica Sinica: 497-522.  Wei Li. 1995. Esperanto Inflection and its Interface in HPSG. Proceedings of the 11th North West Linguistics Conference. Wei Li. 1997. Outline of an HPSG-style Chinese Reversible Grammar. Proceedings of the 13th North West Linguis-tics Conference. Wei Li and Paul McFetridge. 1995. Handling Chinese NP Predicate in HPSG. Proceedings of PACLING-II. Charles N. Li and Sandra A. Thompson. 1989. Mandarin Chinese: A Functional Reference Grammar. University of California Press, London, England. 
Takuya Matsuzaki, Yusuke Miyao, and Junichi Tsujii. 2007. Efficient HPSG Parsing with Supertagging and CFG-filtering. Proceedings of the 20th IJCAI. Yusuke Miyao. 2006. From Linguistic Theory to Syntactic Analysis: Corpus-oriented Grammar Development and Feature Forest Model. Ph.D. Thesis. The University of Tokyo. Yusuke Miyao, Takashi Ninomiya and Junichi Tsujii. 2005. Corpus-oriented Grammar Development for Acquiring a Head-driven Phrase Structure Grammar from the Penn Treebank. Natural Language Processing - IJCNLP 2005: 684-693. Yusuke Miyao and Junichi Tsujii. 2008. Feature Forest Models for Probabilistic HPSG Parsing. Computational Linguistics. 34(1): 35-80. Stefan Muller and Janna Lipenkova. 2009. Serial Verb Con-structions in Chinese: A HPSG Account. Proceedings of the 16th International Conference on Head-Driven Phrase Structure Grammar. 234-254. Hiroko Nakanishi, Yusuke Miyao and Junichi Tsujii. 2004. An Empirical Investigation of the Effect of Lexical Rules on Parsing with a Treebank Grammar. Proceed-ings of the 3rd TLT. 103-114. Say K. Ng. 1997. A Double-specifier Account of Chinese NPs Using Head-driven Phrase Structure Grammar. Master Thesis. Department of Linguistics, University of Edinburgh. Carl Pollard and Ivan A. Sag. 1994. Head-Driven Phrase Structure Grammar. The University of Chicago Press and CSLI Publications, Chicago, IL and Stanford, CA. Ines Rehbein and Josef van Genabith. 2009. Automatic Acquisition of LFG Resources for German ? As Good as it Gets. Proceedings of the 14th International Lexical Functional Grammar Conference (LFG 2009). Natalie Schluter and Josef van Genabith. 2008. Treebank-based Acquisition of LFG Parsing Resources for French. Proceedings of the 6th LREC. Mark Steedman. 2000. The Syntactic Process. The MIT Press. Xiangli Wang et al 2009. Design of Chinese HPSG Frame-work for Data-driven Parsing. Proceedings of the 23rd Pacific Asia Conference on Language, Information and Computation. Lulu Wang and Haitao Liu. 2007. A Description of Chinese NPs Using Head-driven Phrase Structure Grammar. Pro-ceedings of the 14th International Conference on Head-Driven Phrase Structure Grammar. 287-305. Fei Xia. 1999. Extracting Tree Adjoining Grammars from Bracketed Corpora. Proceedings of the 5th NLPRS. Nianwen Xue, Fudong Chiou, and Martha Palmer. 2002. Building a Large-scale Annotated Chinese Corpus. Pro-ceedings of COLING 2002. Ping Xue and Paul McFetridge. 1995. DP Structure, HPSG, and the Chinese NP. Proceedings of the 14th Annual Conference of Canadian Linguistics Association. Nianwen Xue and Fei Xia. 2000. The Bracketing Guidelines for the Penn Chinese Treebank. Yi Zhang. 2004. Starting to Implement Chinese Resource Grammar using LKB and LinGO Grammar Matrix. Technical report.   
1425
Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 123?126,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
The Deep Re-annotation in a Chinese Scientific Treebank  
Kun Yu1 Xiangli Wang1 Yusuke Miyao2 Takuya Matsuzaki1 Junichi Tsujii1,3 1. The University of Tokyo, Hongo 7-3-1, Bunkyo-ku, Tokyo, 113-0033, Japan {kunyu, xiangli, matuzaki, tsujii}@is.s.u-tokyo.ac.jp 2. National Institute of Informatics, Hitotsubashi 2-1-2, Chiyoda-ku, Tokyo, 101-8430, Japan yusuke@nii.ac.jp 3. The University of Manchester, Oxford Road, Manchester, M13 9PL, UK  
Abstract 
In this paper, we introduce our recent work on re-annotating the deep information, which includes both the grammatical functional tags and the traces, in a Chinese scientific tree-bank. The issues with regard to re-annotation and its corresponding solutions are discussed. Furthermore, the process of the re-annotation work is described. 1 Introduction A Chinese scientific Treebank (called the NICT Chinese Treebank) has been developed by the National Institute of Information and Communi-cations Technology of Japan (NICT). This tree-bank annotates the word segmentation, pos-tags, and bracketing structures according to the anno-tation guideline of the Penn Chinese Treebank (Xia, 2000(a); Xia, 2000(b); Xue and Xia, 2000). Contrary to the Penn Chinese Treebank in news domain, the NICT Chinese Treebank includes sentences that are manually translated from Japanese scientific papers. Currently, the NICT Chinese Treebank includes around 8,000 Chinese sentences. The annotation of more sen-tences in the science domain is ongoing.  The current annotation of the NICT Chinese Treebank is informative for some language analysis tasks, such as syntactic parsing and word segmentation. However, the deep informa-tion, which includes both the grammatical func-tional tags and the traces, are omitted in the an-notation. Without grammatical functions, the simple bracketing structure is not informative enough to represent the semantics for Chinese. Furthermore, the traces are critical elements in detecting long-distance dependencies.  Gabbard et al (2006) and Blaheta and Charniak (2000) applied machine learning mod-els to automatically assign the empty categories and functional tags to an English treebank.  
However, considering about the different do-mains that the Penn Chinese Treebank and the NICT Chinese Treebank belong to, the machine learning model trained on the Penn Chinese Treebank may not work successfully on the NICT Chinese Treebank. In order to guarantee the high annotation quality, in our work, we manually re-annotate both the grammatical functional tags and the traces to the NICT Chi-nese Treebank. With the deep re-annotation, the NICT Chinese Treebank could be used not only for the shallow natural language processing tasks, but also as a resource for deep applica-tions, such as the lexicalized grammar develop-ment from treebanks (Miyao 2006; Guo 2009; Xia 1999; Hockenmaier and Steedman 2002).  Considering that the translation quality of the sentences in the NICT Chinese Treebank may affect the quality of re-annotation, in the current phase, we only selected 2,363 sentences that are of good translation quality, for re-annotation. In the future, with the expansion of the NICT Chi-nese Treebank, we will continue this re-annotation work on large-scale sentences.  2 Content of Re-annotation Because the NICT Chinese Treebank follows the annotation guideline of the Penn Chinese Treebank, our re-annotation uses similar annota-tion criteria in the Penn Chinese Treebank.  Figure 1 exemplifies our re-annotation to a sentence in the NICT Chinese Treebank. In this example, we first re-annotate the trace (as indi-cated by the italicized part in Figure 1(b)) for the extracted head noun ??/word?. Furthermore, we re-annotate the functional tag of the trace (as indicated by the dashed-box in Figure 1(b)), to indicate that the extracted head noun should be restored into the relative clause as a topic. There are 26 functional tags in the Penn Chi-nese Treebank (Xue and Xia, 2000), in which seven functional tags describe the grammatical 
123
roles and one functional tag (i.e. LGS) indicates a logical subject. Since the eight functional tags are crucial for obtaining the grammatical func-tion of constituents, we re-annotate the eight functional tags (refer to Table 1) to the NICT Chinese Treebank. (NP (CP (IP (NP (NN ??)                             (NN ???))                               (VP (VA ?)))                                  (DEC ?))                           (NP (NN ?))) (the word of which the word cohesion is high) (a) A relative clause in the NICT Chinese Treebank          (NP (CP (WHNP-1 (-NONE- *OP*)      (CP (IP (NP-TPC (-NONE- *T*-1))                   (NP (NN ??)                           (NN ???))                   (VP (VA ?)))                                (DEC ?)))                (NP (NN ?))) (b) The relative clause after re-annotation Figure 1.  Our re-annotation to a relative clause. Functional Tag Description IO indirect object OBJ direct object EXT post-verbal complement that describes the extent, frequency, or quantity FOC object fronted to a pre-verbal but post-subject position PRD non-verbal predicate SBJ surface subject TPC topic LGS logical subject Table 1. Functional tags that we re-annotate.                 (IP (NP-TPC-1 (NN ??))                 (VP (ADVP (AD ??))                        (VP (ADVP (AD ??))                               (VP (VV ??)                                      (NP-OBJ (-NONE- *T*-1))))))                            (It is easier to obtain information.) (a) A topic construction with long-distance dependency after re-annotation of functional tag and trace          (IP (NP-TPC (DP (DT ?))                                (NP (NN ??)))                (NP-SBJ (NP (PN ?))                               (NP (NN ???)))                (VP (ADVP (AD ?))                        (VP (VV ??)                               (VV ??))))                (The rationality of this algorithm has been verified.)  (b) A topic construction without long-distance dependency after re-annotation of functional tag Figure 2. Our re-annotation to topic constructions. In addition, in the annotation guideline of the Penn Chinese Treebank, four constructions are annotated with traces: BA-construction, BEI-construction, topic construction and relative clause. The BEI-construction and relative 
clause introduce long-distance dependency. Therefore, we re-annotate the traces for the two constructions. The topic construction introduces the topic phrase. For the topic constructions that contain long-distance dependency, we re-annotate both the traces and the functional tags (refer to the italicized part in Figure 2(a)). Some topic constructions, however, do not include long-distance dependency. In such cases, we only re-annotate the functional tag to indicate that it is a topic (refer to the italicized part in Figure 2(b)). In addition, the BA-construction moves the object to a pre-verbal position. Al-though the BA-construction does not contain long-distance dependency, we still re-annotate the trace to acquire the original position of the moved object in the sentence. 3 Issues and Solutions 3.1 Trace re-annotation in the BA/BEI construction The NICT Chinese Treebank follows the word segmentation and pos-tag annotation guideline of the Penn Chinese Treebank. Therefore, there are some BA-constructions and BEI-constructions that cannot be re-annotated with traces. The principle reason for this is that the moved object has semantic relations with only part of the verb. For example, in the sentence shown in Figure 3(a), the moved head noun ???/hometown? is the object of ??/construct?, but not for ???/construct to be?.  (VP (BA ?)                (IP (NP (NN ??))                      (VP (VV ??)                             (NP (NN ??))))) (construct the hometown to be a garden) (a) The annotation in the NICT Chinese Treebank  (VP (BA ?)                            (IP (NP-SBJ-1 (NN ??))                                  (VP (VV ?)                                         (NP-OBJ (-NONE- *-1))                                         (AM ?)                                         (NP (NN ??))))) (b) Our proposed re-annotation of functional tag and trace Figure 3.  Our re-annotation to a BA construction with split verb. Our analysis of the Penn Chinese Treebank shows that only a closed list of characters (such as ??/to be?) can be attached to verbs in such a case. Therefore, we solve the problem by fol-lowing four steps (for an example, refer to Fig-ure 3(b)): 
124
(1) A linguist manually collects the characters that can be attached to verbs in such a case from the Penn Chinese Treebank and assigns them a new pos-tag ?AM (argument marker)?.  (2) The annotators use the character list as a reference during the re-annotation. When the verb in a BA/BEI construction ends with a char-acter in the list, and the annotators think the verb should be split, the annotators record the sentence ID without performing any re-annotation.  (3) The linguist collects all of the recorded sentences, and defines pattern rules to automati-cally split the verbs in the BA/BEI construc-tions. (4) The annotators annotate trace for the sen-tences with the split verbs. This step will be fin-ished in our future work. 3.2 Topic detection In the annotation guideline of the Penn Chinese Treebank, a topic is defined as ?the element that appears before the subject in a declarative sen-tence?. However, the NICT Chinese Treebank does not annotate the omitted subject. Therefore, we could not use the position of the subject as a criterion for topic detection.  In order to resolve this issue, we define some heuristic rules based on both the meaning and the bracketing structure of phrases, to help de-tect the topic phrase. Only the phrase that satis-fies all the rules will be re-annotated as a topic. The following exemplifies some rules: (1) If there is a phrase before a subject, the phrase is probably a topic. (2) A topic phrase must be parallel to the fol-lowing verb phrase. (3) The preposition phrase and localization phrase describing the location or time are not topics. 3.3 Inconsistent annotation in the NICT Chinese Treebank There are some inconsistent annotations in the NICT Chinese Treebank, which makes our re-annotation work difficult.  These inconsistencies include: (1) Inconsistent word segmentation, such as segmenting the word ???? /corresponding? into two words ???/opposite? and ??/ought?. (2) Inconsistent pos-tag annotation. For ex-ample, when the word  ???  exists between two noun phrases, it should be tagged as an associa-tive marker (i.e. DEG), according to the guide-
line of the Penn Chinese Treebank. However, in the NICT Chinese Treebank, sometimes it is tagged as a nominalizer (i.e. DEC).  (3) Inconsistent bracketing annotation. Fig-ure 4(a) shows the annotation of a relative clause in the NICT Chinese Treebank. In this annotation, the noun phrase ???/Osaka ??/subway? is incorrectly treated as the extracted head; furthermore, the adverb ???/by hand? that modifies the verb ???/make? is incor-rectly annotated as an adjective that modifies the noun ????/deformation graph?. After cor-recting these inconsistencies, the relative clause should be annotated as shown in Figure 4(b). (NP (QP (CD ??))              (ADJP (JJ ??))              (DNP (NP (CP (IP (VP (VV ??)))                                       (DEC ?))                                (NP (NR ??)                                       (NN ??)))                         (DEG ?))              (NP (NN ???))) (many deformation graphs of Osaka subway that are made by hand)  (a) The inconsistent annotation of a relative clause (NP (QP (CD ??))        (NP (CP (IP (VP (ADVP (AD ??))                                     (VP (VV ??))))                       (DEC ?))                (NP (DNP (NP (NR ??)                                         (NN ??))                                  (DEG ?))                       (NP (NN ???)))))  (b) The annotation after correcting the inconsistencies Figure 4. An inconsistent annotation in the NICT Chinese Treebank and its correction. In our re-annotation, these inconsistently an-notated sentences in the NICT Chinese Tree-bank were recorded by the annotators. We then sent them back to NICT for further verification. 4 Process of Re-annotation 4.1 Annotation Guideline  During the re-annotation, we basically follow the annotation guideline of the Penn Chinese Treebank (Xue and Xia, 2000). However, in order to fit with the characteristics of scientific sentences in the NICT Chinese Treebank, some constraints are added to the guideline.  For example, in the science domain, the rela-tive clause is often used to describe a phenome-non, in which the extracted head noun is usually an abstract noun, and the relative clause is an appositive of the extracted head noun. Figure 5 shows an example in which the relative clause ???/system ??/stop ??/working? is a de-
125
scription of the extracted head noun ???/phenomenon?. In such a case, the head noun cannot be restored into the clause. Therefore, we add the following restriction in our re-annotation guideline: Do not re-annotate the trace when the head noun of a relative clause is an abstract noun and it is an appositive of the relative clause.         (NP (CP (IP (NP (NN ??))                              (VP (VV ??)                                      (NP (NN ??))))                        (DEC ?))                 (NP (NN ??))) (the phenomenon that the system stops working) Figure 5. A relative clause in the NICT Chinese Treebank. 4.2 Quality Control Several processes were undertaken to guarantee the quality of our re-annotation:  (1) We chose graduate students who major in Chinese for all of the annotators.  (2) A visualization tool - XConc Suite (Kim et al, 2008) was used as assistance during the re-annotation.  (3) Only 2,363 sentences with good transla-tion quality in the NICT Chinese Treebank were chosen for re-annotation in the current phase.   (4) Before starting the re-annotation, a lin-guist selected 200 representative sentences, which contain all the linguistic phenomena that we want to re-annotate, from among the 2,363 sentences in the NICT Chinese Treebank. The selected 200 sentences were manually re-annotated by the linguist, and were split into two sets for training the annotators sequentially. We evaluated the annotation quality of the anno-tators during training. The average annotation quality of all the annotators after training is shown in Table 2. Annotation Quality Inter-annotator Consistency Precision Recall Precision Recall 70.71% 70.75% 61.59% 61.59% Table 2. The average annotation quality of the annotators after training.      (5) After training, the remaining sentences were split into several parts and assigned to the annotators for re-annotation. In each part, there were around 20% sentences that were shared by all of the annotators. These shared sentences were used to check and guarantee inter-annotator consistency during the re-annotation.  5 Conclusion and Future Work  We re-annotated the deep information, which includes eight types of grammatical functional 
tags and the traces in four constructions, to a Chinese scientific treebank, i.e. the NICT Chi-nese Treebank. Since the NICT Chinese Tree-bank is based on manually translated sentences, only 2,363 sentences with good translation qual-ity were re-annotated in the current phase to guarantee the re-annotation quality.  In the future, we will finish the trace annota-tion for the BA and BEI constructions with split verbs. Furthermore, we will continue our re-annotation on more sentences in the NICT Chi-nese Treebank. Acknowledgments We would like to thank Dr. Kiyotaka Uchimoto and Dr. Junichi Kazama for providing the NICT Chinese Treebank. References  Don Blaheta and Eugene Charniak. 2000. Assigning Func-tion Tags to Parsed Text. Proceedings of NAACL 2000. Ryan Gabbard, Seth Kulick and Mitchell Marcus. 2006. Fully Parsing the Penn Treebank. Proceedings of HLT-NAACL 2006. Yuqing Guo. 2009. Treebank-based acquisition of Chinese LFG Resources for Parsing and Generation. Ph.D. Thesis. Dublin City University. Julia Hockenmaier and Mark Steedman. 2002. Acquiring Compact Lexicalized Grammars from a Cleaner Tree-bank. Proceedings of the 3rd LREC. Jindong Kim, Tomoko Ohta, and Junichi Tsujii. 2008. Corpus Annotation for Mining Biomedical Events from Literature. BMC Bioinformatics, 9(10).  Yusuke Miyao. 2006. From Linguistic Theory to Syntactic Analysis: Corpus-oriented Grammar Development and Feature Forest Model. Ph.D Thesis. The University of Tokyo. Fei Xia. 1999. Extracting Tree Adjoining Grammars from Bracketed Corpora. Proceedings of the 5th NLPRS. Fei Xia. 2000 (a). The Segmentation Guidelines for the Penn Chinese Treebank (3.0). Fei Xia. 2000 (b). The Part-of-speech Tagging Guidelines for the Penn Chinese Treebank (3.0). Nianwen Xue, Fudong Chiou, and Martha Palmer. 2002. Building a Large-Scale Annotated Chinese Corpus. Proceedings of COLING 2002. Nianwen Xue and Fei Xia. 2000. The Bracketing Guide-lines for the Penn Chinese Treebank. Shiwen Yu et al 2002. The Basic Processing of Contempo-rary Chinese Corpus at Peking University Specification. Journal of Chinese Information Processing, 16 (5). Qiang Zhou. 2004. Annotation Scheme for Chinese Tree-bank. Journal of Chinese Information Processing, 18 (4). 
126
