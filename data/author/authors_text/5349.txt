Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 329?336
Manchester, August 2008
Evaluating Unsupervised Part-of-Speech Tagging for Grammar Induction
William P. Headden III, David McClosky, Eugene Charniak
Brown Laboratory for Linguistic Information Processing (BLLIP)
Brown University
Providence, RI 02912
{headdenw,dmcc,ec}@cs.brown.edu
Abstract
This paper explores the relationship be-
tween various measures of unsupervised
part-of-speech tag induction and the per-
formance of both supervised and unsuper-
vised parsing models trained on induced
tags. We find that no standard tagging
metrics correlate well with unsupervised
parsing performance, and several metrics
grounded in information theory have no
strong relationship with even supervised
parsing performance.
1 Introduction
There has been a great deal of recent interest in
the unsupervised discovery of syntactic structure
from text, both parts-of-speech (Johnson, 2007;
Goldwater and Griffiths, 2007; Biemann, 2006;
Dasgupta and Ng, 2007) and deeper grammatical
structure like constituency and dependency trees
(Klein and Manning, 2004; Smith, 2006; Bod,
2006; Seginer, 2007; Van Zaanen, 2001). While
some grammar induction systems operate on raw
text, many of the most successful ones presume
prior part-of-speech tagging. Meanwhile, most re-
cent work in part-of-speech induction focuses on
increasing the degree to which their tags match
hand-annotated ones such as those in the Penn
Treebank.
In this work our goal is to evaluate how im-
provements in part-of-speech tag induction affects
grammar induction. Using several different unsu-
pervised taggers, we induce tags and train three
grammar induction systems on the results.
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
We then explore the relationship between the
performance on common unsupervised tagging
metrics and the performance of resulting grammar
induction systems. Disconcertingly we find that
they bear little to no relationship.
This paper is organized as follows. In Section 2
we discuss unsupervised part-of-speech induction
systems and common methods of evaluation. In
Section 3, we describe grammar induction in gen-
eral and discuss the systems with which we evalu-
ate taggings. We present our experiments in Sec-
tion 4, and finally conclude in Section 5.
2 Part-of-speech Tag Induction
Part-of-speech tag induction can be thought of as a
clustering problem where, given a corpus of words,
we aim to group word tokens into syntactic classes.
Two tasks are commonly labeled unsupervised
part-of-speech induction. In the first, tag induction
systems are allowed the use of a tagging dictionary,
which specifies for each word a set of possible
parts-of-speech (Merialdo, 1994; Smith and Eis-
ner, 2005; Goldwater and Griffiths, 2007). In the
second, only the word tokens and sentence bound-
aries are given. In this work we focus on this latter
task to explore grammar induction in a maximally
unsupervised context.
Tag induction systems typically focus on two
sorts of features: distributional and morphologi-
cal. Distributional refers to what sorts of words
appear in close proximity to the word in question,
while morphological refers to modeling the inter-
nal structure of a word. All the systems below
make use of distributional information, whereas
only two use morphological features.
We primarily focus on the metrics used to evalu-
ate induced taggings. The catalogue of recent part-
of-speech systems is large, and we can only test
329
the tagging metrics using a few systems. Recent
work that we do not explore explicitly includes
(Biemann, 2006; Dasgupta and Ng, 2007; Freitag,
2004; Smith and Eisner, 2005). We have selected
a few systems, described below, that represent a
broad range of features and techniques to make our
evaluation of the metrics as broad as possible.
2.1 Clustering using SVD and K-means
Schu?tze (1995) presents a series of part-of-speech
inducers based on distributional clustering. We
implement the baseline system, which Klein and
Manning (2002) use for their grammar induction
experiments with induced part-of-speech tags. For
each word type w in the vocabulary V , the system
forms a feature row vector consisting of the num-
ber of times each of the F most frequent words oc-
cur to the left of w and to the right of w. It normal-
izes these row vectors and assembles them into a
|V |?2F matrix. It then performs a Singular Value
Decomposition on the matrix and rank reduces it to
decrease its dimensionality to d principle compo-
nents (d < 2F ). This results in a representation
of each word as a point in a d dimensional space.
We follow Klein and Manning (2002) in using K-
means to cluster the d dimensional word vectors
into parts-of-speech. We use the F = 500 most
frequent words as left and right context features,
and reduce to a dimensionality of d = 50. We re-
fer to this system as SVD in our experiments.
The other systems described in Schu?tze (1995)
make use of more complicated feature models. We
chose the baseline system primarily to match pre-
vious evaluations of grammar induction using in-
duced tags (Klein and Manning, 2002).
2.2 Hidden Markov Models
One simple family of models for part-of-speech in-
duction are the Hidden Markov Models (HMMs),
in which there is a sequence of hidden state vari-
ables t
1
...t
n
(for us, the part-of-speech tags). Each
state t
i
is conditioned on the previous n ? 1 states
t
i?1
...t
i?n+1
, and every t
i
emits an observed word
w
i
conditioned on t
i
. There is a single start state
that emits nothing, as well as a single stop state,
which emits an end-of-sentence marker with prob-
ability 1 and does not transition further. In our ex-
periments we use the bitag HMM, in which each
state t
i
depends only on state t
i?1
.
The classic method of training HMMs for part-
of-speech induction is the Baum-Welch (Baum,
1972) variant of the Expectation-Maximization
(EM) algorithm, which searches for a local max-
imum in the likelihood of the observed words.
Other methods approach the problem from
a Bayesian perspective. These methods place
Dirichlet priors over the parameters of each transi-
tion and emission multinomial. For an HMM with
a set of states T and a set of output symbols V :
?t ? T ?
t
? Dir(?
1
, ...?
|T |
) (1)
?t ? T ?
t
? Dir(?
1
, ...?
|V |
) (2)
t
i
|t
i?1
, ?
t
i
?1
? Multi(?
t
i?1
) (3)
w
i
|t
i
, ?
t
i
? Multi(?
t
i
) (4)
One advantage of the Bayesian approach is that
the prior allows us to bias learning toward sparser
structures, by setting the Dirichlet hyperparame-
ters ?, ? to a value less than one (Johnson, 2007;
Goldwater and Griffiths, 2007). This increases the
probability of multinomial distributions which put
most of their mass on a few events, instead of dis-
tributing them broadly across many events. There
is evidence that this leads to better performance
on some part-of-speech induction metrics (John-
son, 2007; Goldwater and Griffiths, 2007).
There are both MCMC and variational ap-
proaches to estimating HMMs with sparse Dirich-
let priors; we chose the latter (Variational Bayes
or VB) due to its simple implementation as a
minor modification to Baum-Welch. Johnson
(2007) evaluates both estimation techniques on the
Bayesian bitag model; Goldwater and Griffiths
(2007) emphasize the advantage in the MCMC ap-
proach of integrating out the HMM parameters in a
tritag model, yielding a tagging supported by many
different parameter settings.
Following the setup in Johnson (2007), we ini-
tialize the transition and emission distributions to
be uniform with a small amount of noise, and run
EM and VB for 1000 iterations. We label these
systems as HMM-EM and HMM-VB respectively
in our experiments. In our VB experiments we set
?
i
= ?
j
= 0.1,?i ? {1, ..., |T |} , j ? {1, ..., |V |},
which yielded the best performance on most re-
ported metrics in Johnson (2007). We use max-
imum marginal decoding, which Johnson (2007)
reports performs better than Viterbi decoding.
2.3 Systems with Morphology
Clark (2003) presents several part-of-speech in-
duction systems which incorporate morphological
as well as distributional information. We use the
330
implementation found on his website.1
2.3.1 Ney-Essen with Morphology
The simplest model is based on work by (Ney et
al., 1994). It uses a bitag HMM, with the restric-
tion that each word type in the vocabulary can only
be generated by a single part-of-speech. Thus the
tag induction task here reduces to finding a multi-
way partition of the vocabulary. The learning al-
gorithm greedily reassigns each word type to the
part-of-speech that results in the greatest increase
in likelihood.
In order to incorporate morphology, Clark
(2003) associates with each part-of-speech a HMM
with letter emissions. The vocabulary is gener-
ated by generating a series of word types from
the letter HMM of each part-of-speech. These can
model very basic concatenative morphology. The
parameters of the HMMs are estimated by running
a single iteration of Forward-Backward after each
round of reassigning words to tags. In our exper-
iments we evaluate both the model without mor-
phology (NE in our experiments), and the morpho-
logical model, trying both 5 and 10 states in the let-
ter HMM (NEMorph5, NEMorph10 respectively).
2.3.2 Two-Level HMM
The final part-of-speech inducer we try from
Clark (2003) is a two-level HMM. This is similar
to the previous model, except it lifts the restriction
that a word appear under only one part-of-speech.
Alternatively, one could think of this model as a
standard HMM, whose emission distributions in-
corporate a mixture of a letter HMM and a stan-
dard multinomial. Training uses a simple variation
of Forward-Backward. In the experiments in this
paper, we initialize the mixture parameters to .5,
and try 5 states in the letter HMM. We refer to this
model as 2HMM.
2.4 Tag Evaluation
Objective evaluation in any clustering task is al-
ways difficult, since there are many ways to de-
fine good clusters. Typically it involves a mix-
ture of subjective evaluation and a comparison of
the clusters to those found by human annotators.
In the realm of part-of-speech induction, there are
several common ways of doing the latter. These
split into two groups: accuracy and information-
theoretic criteria.
1http://www.cs.rhul.ac.uk/home/alexc/pos.tar.gz
Accuracy, given some mapping between the set
of induced classes and the gold standard labels, is
the number of words in the corpus that have been
marked with the correct gold label divided by the
total number of word tokens. The main challenge
facing these metrics is deciding how to to map each
induced part-of-speech class to a gold tag. One
option is what Johnson (2007) calls ?many-to-one?
(M-to-1) accuracy, in which each induced tag is
labeled with its most frequent gold tag. Although
this results in a situation where multiple induced
tags may share a single gold tag, it does not punish
a system for providing tags of a finer granularity
than the gold standard.
In contrast, ?one-to-one? (1-to-1) accuracy re-
stricts each gold tag to having a single induced
tag. The mapping typically is made to try to give
the most favorable mapping in terms of accuracy,
typically using a greedy assignment (Haghighi and
Klein, 2006). In cases where the number of gold
tags is different than the number of induced tags,
some must necessarily remain unassigned (John-
son, 2007).
In addition to accuracy, there are several infor-
mation theoretic criteria presented in the literature.
These escape the problem of trying to find an ap-
propriate mapping between induced and gold tags,
at the expense of perhaps being less intuitive.
Let T
I
be the tag assignments to the words
in the corpus created by an unsupervised tag-
ger, and let T
G
be the gold standard tag as-
signments. Clark (2003) uses Shannon?s condi-
tional entropy of the gold tagging given the in-
duced tagging H(T
G
|T
I
). Lower entropy indi-
cates less uncertainty in the gold tagging if we al-
ready know the induced tagging. Freitag (2004)
uses the similar ?cluster-conditional tag perplex-
ity? which is merely exp(H(T
G
|T
I
))
2
. Since
cluster-conditional tag perplexity is a monotonic
function of H(T
G
|T
I
), we only report the latter.
Goldwater and Griffiths (2007) propose using
the Variation of Information of Meila? (2003):
V I(T
G
;T
I
) = H(T
G
|T
I
) + H(T
I
|T
G
)
VI represents the change in information when go-
ing from one clustering to another. It holds the
nice properties of being nonnegative, symmetric,
as well as fulfilling the triangle inequality.
2Freitag (2004) measures entropy in nats, while we use
bits. The difference is a constant factor.
331
3 Grammar Induction
In addition to parts-of-speech, we also want to dis-
cover deeper syntactic relationships. Grammar in-
duction is the problem of determining these re-
lationships in an unsupervised fashion. This can
be thought of more concretely as an unsupervised
parsing task. As there are many languages and do-
mains with few treebank resources, systems that
can learn syntactic structure from unlabeled data
would be valuable. Most work on this problem has
focused on either dependency induction, which we
discuss in Section 3.2, or on constituent induction,
which we examine in the next section.
The Grammar Induction systems we use to eval-
uate the above taggers are the Constituent-Context
Model (CCM), the Dependency Model with Va-
lence (DMV), and a model which combines the
two (CCM+DMV) outlined in (Klein and Man-
ning, 2002; Klein and Manning, 2004).
3.1 Constituent Grammar Induction
Klein and Manning (2002) present a generative
model for inducing constituent boundaries from
part-of-speech tagged text. The model first gener-
ates a bracketing B = {B
ij
}
1?i?j?n
, which spec-
ifies whether each span (i, j) in the sentence is a
constituent or a distituent. Next, given the con-
stituency or distituency of the span B
ij
, the model
generates the part-of-speech yield of the span
t
i
...t
j
, and the one-tag context window of the span
t
i?1
, t
j+1
. P (t
i
...t
j
|B
ij
) and P (t
i?1
, t
j+1
|B
ij
)
are multinomial distributions. The model is trained
using EM.
We evaluate induced constituency trees against
those of the Penn Treebank using the versions of
unlabeled precision, recall, and F-score used by
Klein and Manning (2002). These ignore triv-
ial brackets and multiple constituents spanning the
same bracket. They evaluate their CCM system
on the Penn Treebank WSJ sentences of length 10
or less, using part-of-speech tags induced by the
baseline system of Schu?tze (1995). They report
that switching to induced tags decreases the overall
bracketing F-score from 71.1 to 63.2, although the
recall of VP and S constituents actually improves.
Additionally, they find that NP and PP recall de-
creases substantially with induced tags. They at-
tribute this to the fact that nouns end up in many
induced tags.
There has been quite a bit of other work on
constituency induction. Smith and Eisner (2004)
present an alternative estimation technique for
CCM which uses annealing to try to escape local
maxima. Bod (2006) describes an unsupervised
system within the Data-Oriented-Parsing frame-
work. Several approaches try to learn structure
directly from raw text. Seginer (2007) has an in-
cremental parsing approach using a novel repre-
sentation called common-cover-links, which can
be converted to constituent brackets. Van Zaanen
(2001)?s ABL attempts to align sentences to deter-
mine what sequences of words are substitutable.
The work closest in spirit to this paper is Cramer
(2007), who evaluates several grammar induction
systems on the Eindhoven corpus (Dutch). One
of his experiments compares the grammar induc-
tion performance of these systems starting with
tags induced using the system described by Bie-
mann (2006), to the performance of the systems
on manually-marked tags. However he does not
evaluate to what degree better tagging performance
leads to improvement in these systems.
3.2 Dependency Grammar Induction
A dependency tree is a directed graph whose nodes
are words in the sentence. A directed edge exists
between two words if the target word (argument) is
a dependent of the source word (head). Each word
token may be the argument of only one head, but a
head may have several arguments. One word is the
head of the sentence, and is often thought of as the
argument of a virtual ?Root? node.
Klein and Manning (2004) present their Depen-
dency Model with Valence (DMV) for the un-
supervised induction of dependencies. Like the
constituency model, DMV works from parts-of-
speech. Under this model, for a given head, h,
they first generate the parts-of-speech of the argu-
ments to the right of h, and then those to the left.
Generating the arguments in a particular direction
breaks down into two parts: deciding whether to
stop generating in this direction, and if not, what
part-of-speech to generate as the argument. The ar-
gument decision conditions on h and the direction.
The stopping decision conditions on this and also
on whether h has already generated an argument in
this direction, thereby capturing the limited notion
of valence from which the model takes its name.
It is worth noting that this model can only repre-
sent projective dependency trees, i.e. those without
crossing edges.
Dependencies are typically evaluated using di-
332
Tagging Metrics Grammar Induction Metrics
Tagger No. Tags CCM CCM+DMV DMV
1-to-1 H(T
G
|T
I
) M-to-1 VI UF1 DA UA UF1 DA UA
Gold 1.00 0.00 1.00 0.00 71.50 52.90 67.60 56.50 45.40 63.80
HMM-EM 10 0.39 2.67 0.41 4.39 58.89 40.12 59.26 59.43 36.77 57.37
HMM-EM 20 0.43 2.28 0.48 4.54 57.31 51.16 64.66 61.33 38.65 58.57
HMM-EM 50 0.36 1.83 0.58 4.92 56.56 48.03 63.84 58.02 39.30 58.84
HMM-VB 10 0.40 2.75 0.41 4.42 39.05 27.72 52.84 58.64 23.94 51.64
HMM-VB 20 0.40 2.63 0.43 4.65 37.60 33.77 55.97 40.30 30.36 51.53
HMM-VB 50 0.38 2.70 0.42 5.01 34.68 37.29 57.72 39.82 29.03 50.50
NE 10 0.34 2.74 0.40 4.32 28.80 20.70 50.60 32.70 26.20 48.90
NE 20 0.48 2.02 0.55 3.76 32.50 36.00 59.30 40.60 32.80 54.00
NEMorph10 10 0.44 2.46 0.47 3.74 29.03 25.99 53.80 34.58 26.98 48.72
NEMorph10 20 0.48 1.94 0.56 3.65 31.95 35.85 57.93 38.22 30.45 50.72
NEMorph10 50 0.47 1.24 0.72 3.60 31.07 36.29 57.76 39.28 31.50 52.83
NEMorph5 10 0.45 2.50 0.47 3.76 29.04 22.72 51.58 32.67 23.62 47.89
NEMorph5 20 0.44 2.02 0.56 3.80 31.94 24.17 52.43 32.90 22.41 47.17
NEMorph5 50 0.47 1.27 0.72 3.64 31.39 38.63 59.44 40.23 34.26 54.63
2HMM 10 0.38 2.78 0.41 4.55 31.63 36.35 58.87 44.97 28.43 49.32
2HMM 20 0.41 2.35 0.48 4.71 42.39 43.91 60.74 50.85 29.32 50.69
2HMM 50 0.37 1.92 0.58 5.11 41.18 49.94 64.87 57.84 39.24 59.14
SVD 10 0.31 3.07 0.34 4.99 37.77 27.64 49.56 36.46 20.74 45.52
SVD 20 0.33 2.73 0.40 4.99 37.17 30.14 51.66 37.66 22.24 46.25
SVD 50 0.34 2.37 0.47 5.18 36.87 37.66 56.49 52.83 22.50 46.52
SVD 100 0.34 2.03 0.53 5.37 45.46 41.68 58.83 64.20 20.81 44.36
SVD 200 0.32 1.72 0.59 5.59 61.90 34.79 52.25 59.93 22.66 42.30
Table 1: The performance of the taggers regarding both tag and grammar induction metrics on WSJ
sections 0-10, averaged over 10 runs. Bold indicates the result was within 10 percent of the best-scoring
induced system for a given metric.
rected and undirected accuracy. These are the to-
tal number of proposed edges that appear in the
gold tree divided by the total number of edges (the
number of words in the sentence). Directed accu-
racy gives credit to a proposed edge if it is in the
gold tree and is in the correct direction, while undi-
rected accuracy ignores the direction.
Klein and Manning (2004) also present a model
which combines CCM and DMV into a single
model, which we show as CCM+DMV. In their
experiments, this model performed better on both
the constituency and dependency induction tasks.
As with CCM, Klein and Manning (2004) simi-
larly evaluate the combined CCM+DMV system
using tags induced with the same method. Again
they find that overall bracketing F-score decreases
from 77.6 to 72.9 and directed dependency accu-
racy measures decreases from 47.5 to 42.3 when
switching to induced tags from gold. However for
each metric, the systems still do quite well with
induced tags.
As in the constituency case, Smith (2006)
presents several alternative estimation procedures
for DMV, which try to minimize the local maxi-
mum problems inherent in EM. It is thus possible
these methods might yield better performance for
the models when run off of induced tags.
4 Experiments
We induce tags with each system on the Penn Tree-
bank Wall Street Journal (Marcus et al, 1994), sec-
tions 0-10, which contain 20,260 sentences. We
vary the number of tags (10, 20, 50) and run each
system 10 times for a given setting. The result of
each run is used as the input to the CCM, DMV,
and CCM+DMV systems. While the tags are in-
duced from all sentences in the section, following
the practice in (Klein and Manning, 2002; Klein
and Manning, 2004), we remove punctuation, and
consider only sentences of length not greater than
10 in our grammar induction experiments. Tag-
gings are evaluated after punctuation is removed,
but before filtering for length.
To explore the relationship between tagging
metrics and the resulting performance of grammar
induction systems, we examine each pair of tag-
ging and grammar induction metrics. Consider the
following two examples: DMV directed accuracy
vs. H(T
G
|T
I
) (Figure 1), and CCM f-score vs.
variation of information (Figure 2). These were se-
lected because they have relatively high magnitude
?s. From these plots it is clear that although there
333
may be a slight correspondence, the relationships
are weak at best.
Each tagging and grammar induction metric
gives us a ranking over the set of taggings of the
data generated over the course of our experiments.
These are ordered from best to worst according to
the metric, so for instance H(T
G
|T
I
) would give
highest rank to its lowest value. We can com-
pare the two rankings using Kendall?s ? (see Lap-
ata (2006) for an overview), a nonparametric mea-
sure of correspondence for rankings. ? measures
the difference between the number of concordant
pairs (items the two rankings place in the same or-
der) and discordant pairs (those the rankings place
in opposite order), divided by the total number of
pairs. A value of 1 indicates the rankings have per-
fect correspondence, -1 indicates they are in the
opposite order, and 0 indicates they are indepen-
dent. The ? values are shown in Table 2. The
scatter-plot in Figure 1 shows the ? with the great-
est magnitude. However, we can see that even
these rankings have barely any relationship.
An objection one might raise is that the lack of
correspondence reflects poorly not on these met-
rics, but upon the grammar induction systems we
use to evaluate them. There might be something
about these models in particular which yields these
low correlations. For instance these grammar in-
ducers all estimate their models using EM, which
can get caught easily in a local maximum.
To this possibility, we respond by pointing to
performance on gold tags, which is consistently
high for all grammar induction metrics. There is
clearly some property of the gold tags which is ex-
ploited by the grammar induction systems even in
the absence of better estimation procedures. This
property is not reflected in the tagging metrics.
The scores for each system for tagging and
grammar induction, averaged over the 10 runs, are
shown in Table 1. Additionally, we included runs
of the SVD-tagger for 100 and 200 tags, since run-
ning this system is still practical with these num-
bers of tags. The Ney-Essen with Morphology tag-
gers perform at or near the top on the various tag-
ging metrics, but not well on the grammar induc-
tion tasks on average. HMM-EM seems to perform
on average quite well on all the grammar induction
tasks, while the SVD-based systems yield the top
bracketing F-scores, making use of larger numbers
of tags.
Grammar Induction Metrics
Tagging CCM CCM+DMV DMV
Metrics UF1 DA UA UF1 DA UA
1-to-1 -0.22 -0.04 0.05 -0.13 0.13 0.12
M-to-1 -0.09 0.17 0.24 0.03 0.26 0.25
H(T
G
|T
I
) 0.01 0.21 0.27 0.07 0.29 0.28
VI -0.25 -0.17 -0.06 -0.20 0.07 0.07
Table 2: Kendall?s ? , between tag and grammar
induction criteria.
4.1 Supervised Experiments
One question we might ask is whether these tag-
ging metrics capture information relevant to any
parsing task. We explored this by experimenting
with a supervised parser, training off trees where
the gold parts-of-speech have been removed and
replaced with induced tags. Our expectation was
that the brackets, the head propagation paths, and
the phrasal categories in the training trees would
be sufficient to overcome any loss in information
that the gold tags might provide. Additionally it
was possible the induced tags would ignore rare
parts-of-speech such as FW, and make better use
of the available tags, perhaps using new distribu-
tional clues not in the original tags.
To this end we modified the Charniak Parser
(Charniak, 2000) to train off induced parts-of-
speech. The Charniak parser is a lexicalized PCFG
parser for which the part-of-speech of a head word
is a key aspect of its model. During training, the
head-paths from the gold part-of-speech tags are
retained, but we replace the tags themselves.
We ran experiments using the bitag HMM from
Section 2.2 trained using EM, as well as with the
Schu?tze SVD tagger from Section 2.1. The parser
was trained on sections 2-21 of the Penn Treebank
for training and section 24 was used for evaluation.
As before we calculated ? scores between each
tagging metric and supervised f-score. Unlike the
unsupervised evaluation where we used the metric
UF1, we use the standard EVALB calculation of
unlabeled f-score. The results are shown in Table
3.
The contrast with the unsupervised case is vast,
with very high ?s for both accuracy metrics. Con-
sider f-score vs. many-to-one, plotted in Figure 3.
The correspondence here is very clear: taggings
with high accuracy do actually reflect on better
parser performance. Note, however, that the corre-
spondence between the information theoretic mea-
sures and parsing performance is still rather weak.
334
0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5
 
15
20
25
30
35
40
45
50
55
D
M
V
 
D
A
NEMorph
HMM-VB
2HMM
NE
SVD
HMM-EM
Gold
Figure 1: DMV Directed Accuracy vs. H(T
G
|T
I
)
0 1 2 3 4 5 6
 VI
20
30
40
50
60
70
80
C
C
M
 
U
F
1
NEMorph
HMM-VB
2HMM
NE
SVD
HMM-EM
Gold
Figure 2: CCM fscore vs. tagging variation of in-
formation.
0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
 M-to-1
0.78
0.80
0.82
0.84
0.86
0.88
0.90
C
h
a
r
n
i
a
k
 
p
a
r
s
e
r
 
F
1
SVD 10 tags
SVD 50 tags
Gold
HMM-EMbitag 10 tags
HMM-EMbitag 50 tags
Figure 3: Supervised parsing f-score vs. tagging
many-to-one accuracy.
Tagging Metric Supervised F1
1-to-1 0.62
M-to-1 0.83
H(T
G
|T
I
) -0.19
VI 0.25
Table 3: Kendall?s ? , between tag induction cri-
teria and supervised parsing unlabeled bracketing
F-score.
Interestingly, parsing performance and speed
does degrade considerably when training off in-
duced tags. We are not sure what causes this. One
possibility is in the lexicalized stage of the parser,
where the probability of a head word is smoothed
primarily by its part-of-speech tag. This requires
that the tag be a good proxy for the syntactic role
of the head. In any case this warrants further in-
vestigation.
5 Conclusion and Future Work
In this work, we found that none of the most com-
mon part-of-speech tagging metrics bear a strong
relationship to good grammar induction perfor-
mance. Although our experiments only involve
English, the poor correspondence we find between
the various tagging metrics and grammar induc-
tion performance raises concerns about their re-
lationship more broadly. We additionally found
that while tagging accuracy measures do corre-
late with better supervised parsing, common infor-
mation theoretic ones do not strongly predict bet-
ter performance on either task. Furthermore, the
supervised experiments indicate that informative
part-of-speech tags are important for good parsing.
The next step is to explore better tagging met-
rics that correspond more strongly to better gram-
mar induction performance. A good metric should
use all the information we have, including the gold
trees, to evaluate. Finally, we should explore gram-
mar induction schemes that do not rely on prior
parts-of-speech, instead learning them from raw
text at the same time as deeper structure.
Acknowledgments
We thank Dan Klein for his grammar induction
code, as well as Matt Lease and other members of
BLLIP for their feedback. This work was partially
supported by DARPA GALE contract HR0011-06-
2-0001 and NSF award 0631667.
335
References
Baum, L.E. 1972. An inequality and associated maxi-
mization techniques in statistical estimation of prob-
abilistic functions of Markov processes. Inequali-
ties, 3:1?8.
Biemann, Chris. 2006. Unsupervised part-of-speech
tagging employing efficient graph clustering. In
Proceedings of the COLING/ACL 2006 Student Re-
search Workshop, pages 7?12, Sydney, Australia.
Bod, Rens. 2006. An all-subtrees approach to unsuper-
vised parsing. In Proceedings of Coling/ACL 2006,
pages 865?872.
Charniak, Eugene. 2000. A maximum-entropy-
inspired parser. In Proceedings of the North Ameri-
can Chapter of the ACL 2000, pages 132?139.
Clark, Alexander. 2003. Combining distributional and
morphological information for part of speech induc-
tion. In Proceedings of EACL 2003, pages 59?66,
Budapest, Hungary.
Cramer, Bart. 2007. Limitations of current grammar
induction algorithms. In Proceedings of the ACL
2007 Student Research Workshop, pages 43?48.
Dasgupta, Sajib and Vincent Ng. 2007. Unsupervised
part-of-speech acquisition for resource-scarce lan-
guages. In Proceedings of the EMNLP/CoNLL 2007,
pages 218?227.
Freitag, Dayne. 2004. Toward unsupervised whole-
corpus tagging. In Proceedings of Coling 2004,
pages 357?363, Aug 23?Aug 27.
Goldwater, Sharon and Tom Griffiths. 2007. A fully
bayesian approach to unsupervised part-of-speech
tagging. In Proceedings of the ACL 2007, pages
744?751.
Haghighi, Aria and Dan Klein. 2006. Prototype-driven
learning for sequence models. In Proceedings of
HLT/NAACL 2006, pages 320?327, New York, USA.
Johnson, Mark. 2007. Why doesn?t EM find
good HMM POS-taggers? In Proceedings of the
EMNLP/CoNLL 2007, pages 296?305.
Klein, Dan and Christopher Manning. 2002. A gener-
ative constituent-context model for improved gram-
mar induction. In Proceedings of ACL 2002.
Klein, Dan and Christopher Manning. 2004. Corpus-
based induction of syntactic structure: Models of de-
pendency and constituency. In Proceedings of ACL
2004, pages 478?485, Barcelona, Spain, July.
Lapata, Mirella. 2006. Automatic evaluation of infor-
mation ordering: Kendall?s tau. Computational Lin-
guistics, 32(4):1?14.
Marcus, Mitchell, Grace Kim, Mary Ann
Marcinkiewicz, Robert MacIntyre, Ann Bies,
Mark Ferguson, Karen Katz, and Britta Schasberger.
1994. The Penn Treebank: Annotating Predicate
Argument Structure. In Proceedings of the 1994
ARPA Human Language Technology Workshop.
Meila?, Marina. 2003. Comparing clusterings. Pro-
ceedings of the Conference on Computational Learn-
ing Theory (COLT).
Merialdo, Bernard. 1994. Tagging english text with
a probabilistic model. Computational Linguistics,
20(2):154?172.
Ney, Herman, Ute Essen, and Renhard Knesser. 1994.
On structuring probabilistic dependencies in stochas-
tic language modelling. Computer Speech and Lan-
guage, 8:1?38.
Schu?tze, Hinrich. 1995. Distributional part-of-speech
tagging. In Proceedings of the 7th conference of the
EACL, pages 141?148.
Seginer, Yoav. 2007. Fast unsupervised incremental
parsing. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 384?391, Prague, Czech Republic.
Smith, Noah A. and Jason Eisner. 2004. Anneal-
ing techniques for unsupervised statistical language
learning. In Proceedings of ACL 2004, pages 487?
494, Barcelona, Spain.
Smith, Noah A. and Jason Eisner. 2005. Contrastive
estimation: Training log-linear models on unlabeled
data. In Proceedings of ACL 2005, pages 354?362,
Ann Arbor, Michigan.
Smith, Noah A. 2006. Novel Estimation Methods for
Unsupervised Discovery of Latent Structure in Nat-
ural Language Text. Ph.D. thesis, Department of
Computer Science, Johns Hopkins University, Octo-
ber.
Van Zaanen, Menno M. 2001. Bootstrapping Structure
into Language: Alignment-Based Learning. Ph.D.
thesis, University of Leeds, September.
336
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 561?568
Manchester, August 2008
When is Self-Training Effective for Parsing?
David McClosky, Eugene Charniak, and Mark Johnson
Brown Laboratory for Linguistic Information Processing (BLLIP)
Brown University
Providence, RI 02912
{dmcc|ec|mj}@cs.brown.edu
Abstract
Self-training has been shown capable of
improving on state-of-the-art parser per-
formance (McClosky et al, 2006) despite
the conventional wisdom on the matter and
several studies to the contrary (Charniak,
1997; Steedman et al, 2003). However, it
has remained unclear when and why self-
training is helpful. In this paper, we test
four hypotheses (namely, presence of a
phase transition, impact of search errors,
value of non-generative reranker features,
and effects of unknown words). From
these experiments, we gain a better un-
derstanding of why self-training works for
parsing. Since improvements from self-
training are correlated with unknown bi-
grams and biheads but not unknown words,
the benefit of self-training appears most in-
fluenced by seeing known words in new
combinations.
1 Introduction
Supervised statistical parsers attempt to capture
patterns of syntactic structure from a labeled set of
examples for the purpose of annotating new sen-
tences with their structure (Bod, 2003; Charniak
and Johnson, 2005; Collins and Koo, 2005; Petrov
et al, 2006; Titov and Henderson, 2007). These
annotations can be used by various higher-level ap-
plications such as semantic role labeling (Pradhan
et al, 2007) and machine translation (Yamada and
Knight, 2001).
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
However, labeled training data is expensive to
annotate. Given the large amount of unlabeled text
available for many domains and languages, tech-
niques which allow us to use both labeled and
unlabeled text to train our models are desirable.
These methods are called semi-supervised. Self-
training is a specific type of semi-supervised learn-
ing. In self-training, first we train a model on the
labeled data and use that model to label the unla-
beled data. From the combination of our original
labeled data and the newly labeled data, we train a
second model ? our self-trained model. The pro-
cess can be iterated, where the self-trained model
is used to label new data in the next iteration. One
can think of self-training as a simple case of co-
training (Blum and Mitchell, 1998) using a single
learner instead of several. Alternatively, one can
think of it as one step of the Viterbi EM algorithm.
Studies prior to McClosky et al (2006) failed to
show a benefit to parsing from self-training (Char-
niak, 1997; Steedman et al, 2003). While the re-
cent success of self-training has demonstrated its
merit, it remains unclear why self-training helps in
some cases but not others. Our goal is to better un-
derstand when and why self-training is beneficial.
In Section 2, we discuss the previous applica-
tions of self-training to parsing. Section 3 de-
scribes our experimental setup. We present and
test four hypotheses of why self-training helps in
Section 4 and conclude with discussion and future
work in Section 5.
2 Previous Work
To our knowledge, the first reported uses of self-
training for parsing are by Charniak (1997). He
used his parser trained on the Wall Street Journal
(WSJ, Mitch Marcus et al (1993)) to parse 30 mil-
lion words of unparsed WSJ text. He then trained
561
a self-trained model from the combination of the
newly parsed text with WSJ training data. How-
ever, the self-trained model did not improve on the
original model.
Self-training and co-training were subsequently
investigated in the 2002 CLSP Summer Work-
shop at Johns Hopkins University (Steedman et
al., 2003). They considered several different pa-
rameter settings, but in all cases, the number of
sentences parsed per iteration of self-training was
relatively small (30 sentences). They performed
many iterations of self-training. The largest seed
size (amount of labeled training data) they used
was 10,000 sentences from WSJ, though many ex-
periments used only 500 or 1,000 sentences. They
found that under these parameters, self-training did
not yield a significant gain.
Reichart and Rappoport (2007) showed that one
can self-train with only a generative parser if the
seed size is small. The conditions are similar to
Steedman et al (2003), but only one iteration of
self-training is performed (i.e. all unlabeled data is
labeled at once).1 In this scenario, unknown words
(words seen in the unlabeled data but not in train-
ing) were a useful predictor of when self-training
improves performance.
McClosky et al (2006) showed that self-training
improves parsing accuracy when the two-stage
Charniak and Johnson (2005) reranking parser is
used. Using both stages (a generative parser and
discriminative reranker) to label the unlabeled data
set is necessary to improve performance. Only re-
training the first stage had a positive effect. How-
ever, after retraining the first stage, both stages pro-
duced better parses. Unlike Steedman et al (2003),
the training seed size is large and only one itera-
tion of self-training is performed. Error analysis
revealed that most improvement comes from sen-
tences with lengths between 20 and 40 words. Sur-
prisingly, improvements were also correlated with
the number of conjunctions but not with the num-
ber of unknown words in the sentence.
To summarize, several factors have been iden-
tified as good predictors of when self-training im-
proves performance, but a full explanation of why
self-training works is lacking. Previous work es-
tablishes that parsing all unlabeled sentences at
once (rather than over many iterations) is impor-
tant for successful self-training. The full effect of
1Performing multiple iterations presumably fails because
the parsing models become increasingly biased. However,
this remains untested in the large seed case.
seed size and the reranker on self-training is not
well understood.
3 Experimental Setup
We use the Charniak and Johnson reranking parser
(outlined below), though we expect many of these
results to generalize to other generative parsers
and discriminative rerankers. Our corpora consist
of WSJ for labeled data and NANC (North Amer-
ican News Text Corpus, Graff (1995)) for unla-
beled data. We use the standard WSJ division for
parsing: sections 2-21 for training (39,382 sen-
tences) and section 24 for development (1,346 sen-
tences). Given self-training?s varied performance
in the past, many of our experiments use the con-
catenation of sections 1, 22, and 24 (5,039 sen-
tences) rather than the standard development set
for more robust testing.
A full description of the reranking parser can be
found in Charniak and Johnson (2005). Briefly
put, the reranking parser consists of two stages:
A generative lexicalized PCFG parser which pro-
poses a list of the n most probable parses (n-best
list) followed by a discriminative reranker which
reorders the n-best list. The reranker uses about
1.3 million features to help score the trees, the
most important of which is the first stage parser?s
probability. In Section 4.3, we mention two classes
of reranker features in more depth. Since some of
experiments rely on details of the first stage parser,
we present a summary of the parsing model.
3.1 The Parsing Model
The parsing model assigns a probability to a parse
? by a top-down process of considering each con-
stituent c in ? and, for each c, first guessing the
preterminal of c, t(c) then the lexical head of c,
h(c), and then the expansion of c into further con-
stituents e(c). Thus the probability of a parse is
given by the equation
p(?) =
?
c??
p(t(c) | l(c),R(c))
?p(h(c) | t(c), l(c),R(c))
?p(e(c) | l(c), t(c), h(c),R(c))
where l(c) is the label of c (e.g., whether it is a
noun phrase (np), verb phrase, etc.) and R(c) is
the relevant history of c ?- information outside c
that the probability model deems important in de-
termining the probability in question.
562
For each expansion e(c) we distinguish one of
the children as the ?middle? child M(c). M(c) is
the constituent from which the head lexical item
h is obtained according to deterministic rules that
pick the head of a constituent from among the
heads of its children. To the left of M is a sequence
of one or more left labels L
i
(c) including the spe-
cial termination symbol ? and similarly for the la-
bels to the right, R
i
(c). Thus an expansion e(c)
looks like:
l ? ?L
m
...L
1
MR
1
...R
n
?. (1)
The expansion is generated by guessing first M ,
then in order L
1
through L
m+1
(= ?), and simi-
larly for R
1
through R
n+1
.
So the parser assigns a probability to the parse
based upon five probability distributions, T (the
part of speech of the head), H (the head), M (the
child constituent which includes the head), L (chil-
dren to the left of M ), and R (children to the right
of M ).
4 Testing the Four Hypotheses
The question of why self-training helps in some
cases (McClosky et al, 2006; Reichart and Rap-
poport, 2007) but not others (Charniak, 1997;
Steedman et al, 2003) has inspired various the-
ories. We investigate four of these to better un-
derstand when and why self-training helps. At
a high level, the hypotheses are (1) self-training
helps after a phase transition, (2) self-training re-
duces search errors, (3) specific classes of reranker
features are needed for self-training, and (4) self-
training improves because we see new combina-
tions of words.
4.1 Phase Transition
The phase transition hypothesis is that once a
parser has achieved a certain threshold of perfor-
mance, it can label data sufficiently accurately.
Once this happens, the labels will be ?good
enough? for self-training.
To test the phase transition hypothesis, we use
the same parser as McClosky et al (2006) but train
on only a fraction of WSJ to see if self-training is
still helpful. This is similar to some of the ex-
periments by Reichart and Rappoport (2007) but
with the use of a reranker and slightly larger seed
sizes. The self-training protocol is the same as
in (Charniak, 1997; McClosky et al, 2006; Re-
ichart and Rappoport, 2007): we parse the entire
unlabeled corpus in one iteration. We start by tak-
ing a random subset of the WSJ training sections
(2-21), accepting each sentence with 10% proba-
bility. With the sampled training section and the
standard development data, we train a parser and a
reranker. In Table 1, we show the performance of
the parser with and without the reranker. For ref-
erence, we show the performance when using the
complete training division as well. Unsurprisingly,
both metrics drop as we decrease the amount of
training data. These scores represent our baselines
for this experiment.
Using this parser model, we parse one million
sentences from NANC, both with and without the
reranker. We combine these one million sentences
with the sampled subsets of WSJ training and train
new parser models from them.2
Finally, we evaluate these self-trained models
(Table 2). The numbers in parentheses indicate the
change from the corresponding non-self-trained
model. As in Reichart and Rappoport (2007), we
see large improvements when self-training on a
small seed size (10%) without using the reranker.
However, using the reranker to parse the self-
training and/or evaluation sentences further im-
proves results. From McClosky et al (2006), we
know that when 100% of the training data is used,
self-training does not improve performance with-
out a reranker.
From this we conclude that there is no such
threshold phase transition in this case. High per-
formance is not a requirement to successfully use
self-training for parsing, since there are lower per-
forming parsers which can self-train and higher
performing parsers which cannot. The higher per-
forming Charniak and Johnson (2005) parser with-
out reranker achieves an f -score of 89.0 on section
24 when trained on all of WSJ. This parser does
not benefit from self-training unless paired with a
reranker. Contrast this with the same parser trained
on only 10% of WSJ, where it gets an f -score of
85.8 (Table 2) or the small seed models of Reichart
and Rappoport (2007). Both of these lower per-
forming parsers can successfully self-train. Ad-
ditionally, we now know that while a reranker is
not required for self-training when the seed size is
small, it still helps performance considerably (f -
score improves from 87.7 to 89.0 in the 10% case).
2We do not weight the original WSJ data, though our ex-
pectation is that performance would improve if WSJ were
given a higher relative weight. This is left as future work.
563
% WSJ # sentences Parser f -score Reranking parser f -score
10 3,995 85.8 87.0
100 39,832 89.9 91.5
Table 1: Parser and reranking parser performance on sentences ? 100 words in sections 1, 22, and 24
when trained on different amounts of training data. % WSJ is the percentage of WSJ training data trained
on (sampled randomly). Note that the full amount of development data is still used as held out data.
Parsed NANC with reranker? Parser f -score Reranking parser f -score
No 87.7 (+1.9) 88.7 (+1.7)
Yes 88.4 (+2.6) 89.0 (+2.0)
Table 2: Effect of self-training using only 10% of WSJ as labeled data. The parser model is trained from
one million parsed sentences from NANC + WSJ training. The first column indicates whether the million
NANC sentences were parsed by the parser or reranking parser. The second and third columns differ in
whether the reranker is used to parse the test sentences (WSJ sections 1, 22, and 24, sentences 100 words
and shorter). Numbers in parentheses are the improvements over the corresponding non-self-trained
parser.
4.2 Search Errors
Another possible explanation of self-training?s im-
provements is that seeing newly labeled data re-
sults in fewer search errors (Daniel Marcu, per-
sonal communication). A search error would in-
dicate that the parsing model could have produced
better (more probable) parses if not for heuristics
in the search procedure. The additional parse trees
may help produce sharper distributions and reduce
data sparsity, making the search process easier. To
test this, first we present some statistics on the n-
best lists (n = 50) from the baseline WSJ trained
parser and self-trained model3 from McClosky et
al. (2006). We use each model to parse sentences
from held-out data (sections 1, 22, and 24) and ex-
amine the n-best lists.
We compute statistics of the WSJ and self-
trained n-best lists with the goal of understand-
ing how much they intersect and whether there are
search errors. On average, the n-best lists over-
lap by 66.0%. Put another way, this means that
about a third of the parses from each model are
unique, so the parsers do find a fair number of dif-
ferent parses in their search. The next question
is where the differences in the n-best lists lie ?
if all the differences were near the bottom, this
would be less meaningful. Let W and S repre-
sent the n-best lists from the baseline WSJ and self-
trained parsers, respectively. The top
m
(?) func-
tion returns the highest scoring parse in the n-best
list ? according to the reranker and parser model
3http://bllip.cs.brown.edu/selftraining/
m.
4 Almost 40% of the time, the top parse in
the self-trained model is not in the WSJ model?s
n-best list, (top
s
(S) /? W ) though the two mod-
els agree on the top parse roughly 42.4% of the
time (top
s
(S) = top
w
(W )). Search errors can
be formulated as top
s
(S) /? W ? top
s
(S) =
top
w
(W ? S). This captures sentences where the
parse that the reranker chose in the self-trained
model is not present in the WSJ model?s n-best list,
but if the parse were added to the WSJ model?s list,
the parse?s probability in the WSJ model and other
reranker features would have caused it to be cho-
sen. These search errors occur in only 2.5% of
the n-best lists. At first glance, one might think
that this could be enough to account for the differ-
ences, since the self-trained model is only several
tenths better in f -score. However, we know from
McClosky et al (2006) that on average, parses do
not change between the WSJ and self-trained mod-
els and when they do, they only improve slightly
more than half the time. For this reason, we run a
second test more focused on performance.
For our second test we help the WSJ trained
model find the parses that the self-trained model
found. For each sentence, we start with the n-best
list (n = 500 here) from the WSJ trained parser,
W . We then consider parses in the self-trained
parser?s n-best list, S, that are not present in W
(S ? W ). For each of these parses, we deter-
mine its probability under the WSJ trained parsing
4Recall that the parser?s probability is a reranker feature
so the parsing model influences the ranking.
564
Model f -score
WSJ 91.5
WSJ & search help 91.7
Self-trained 92.0
Table 3: Test of whether ?search help? from the
self-trained model impacts the WSJ trained model.
WSJ + search help is made by adding self-trained
parses not proposed by the WSJ trained parser but
to which the parser assigns a positive probability.
The WSJ reranker is used in all cases to select the
best parse for sections 1, 22, and 24.
model. If the probability is non-zero, we add the
parse to the n-best list W , otherwise we ignore the
parse. In other words, we find parses that the WSJ
trained model could have produced but didn?t due
to search heuristics. In Table 3, we show the per-
formance of the WSJ trained model, the model with
?search help? as described above, and the self-
trained model on WSJ sections 1, 22, and 24. The
WSJ reranker is used to pick the best parse from
each n-best list. WSJ with search help performs
slightly better than WSJ alone but does not reach
the level of the self-trained model. From these ex-
periments, we conclude that reduced search errors
can only explain a small amount of self-training?s
improvements.
4.3 Non-generative reranker features
We examine the role of specific reranker features
by training rerankers using only subsets of the fea-
tures. Our goal is to determine whether some
classes of reranker features benefit self-training
more than others. We hypothesize that features
which are not easily captured by the generative
first-stage parser are the most beneficial for self-
training. If we treat the parser and reranking parser
as different (but clearly dependent) views, this is a
bit like co-training. If the reranker uses features
which are captured by the first-stage, the views
may be too similar for there to be an improvement.
We consider two classes of features (GEN and
EDGE) and their complements (NON-GEN and
NON-EDGE).5 GEN consists of features that
are roughly captured by the first-stage generative
parser: rule rewrites, head-child dependencies, etc.
EDGE features describe items across constituent
boundaries. This includes the words and parts of
5A small number of features overlap hence these sizes do
not add up.
Feature set # features f -score
GEN 448,349 90.4
NON-GEN 885,492 91.1
EDGE 601,578 91.0
NON-EDGE 732,263 91.1
ALL 1,333,519 91.3
Table 4: Sizes and performance of reranker feature
subsets. Reranking parser f -scores are on all sen-
tences in section 24.
speech of the tokens on the edges between con-
stituents and the labels of these constituents. This
represents a specific class of features not captured
by the first-stage. These subsets and their sizes are
shown in Table 4. For comparison, we also include
the results of experiments using the full feature set,
as in McClosky et al (2006), labeled ALL. The
GEN features are roughly one third the size of the
full feature set.
We evaluate the effect of these new reranker
models on self-training (Table 4). For each fea-
ture set, we do the following: We parse one million
NANC sentences with the reranking parser. Com-
bining the parses with WSJ training data, we train
a new first-stage model. Using this new first-stage
model and the reranker subset, we evaluate on sec-
tion 24 of WSJ. GEN?s performance is weaker
while the other three subsets achieve almost the
same score as the full feature set. This confirms
our hypothesis that when the reranker helps in self-
training it is due to features which are not captured
by the generative first-stage model.
4.4 Unknown Words
Given the large size of the parsed self-training cor-
pus, it contains an immense number of parsing
events which never occur in the training corpus.
The most obvious of these events is words ? the
vocabulary grows from 39,548 to 265,926 words
as we transition from the WSJ trained model to
the self-trained model. Slightly less obvious is bi-
grams. There are roughly 330,000 bigrams in WSJ
training data and approximately 4.8 million new
bigrams in the self-training corpus.
One hypothesis (Mitch Marcus, personal com-
munication) is that the parser is able to learn a lot
of new bilexical head-to-head dependencies (bi-
heads) from self-training. The reasoning is as fol-
lows: Assume the self-training corpus is parsed in
a mostly correct manner. If there are not too many
565
new pairs of words in a sentence, there is a de-
cent chance that we can tag these words correctly
and bracket them in a reasonable fashion from con-
text. Thus, using these parses as part of the train-
ing data improves parsing because should we see
these pairs of words together in the future, we will
be more likely to connect them together properly.
We test this hypothesis in two ways. First, we
perform an extension of the factor analysis simi-
lar to that in McClosky et al (2006). This is done
via a generalized linear regression model intended
to determine which features of parse trees can pre-
dict when the self-training model will perform bet-
ter. We consider many of the same features (e.g.
bucketed sentence length, number of conjunctions,
and number of unknown words) but also consider
two new features: unknown bigrams and unknown
biheads. Unknown items (words, bigrams, bi-
heads) are calculated by counting the number of
items which have never been seen in WSJ train-
ing but have been seen in the parsed NANC data.
Given these features, we take the f -scores for each
sentence when parsed by the WSJ and self-trained
models and look at the differences. Our goal is to
find out which features, if any, can predict these f -
score differences. Specifically, we ask the question
of whether seeing more unknown items indicates
whether we are more likely to see improvements
when self-training.
The effect of unknown items on self-training?s
relative performance is summarized in Figure 1.
For each item, we show the total number of incor-
rect parse nodes in sentences that contain the item.
We also show the change in the number of correct
parse nodes in these sentences between the WSJ
and self-trained models. A positive change means
that performance improved under self-training. In
other words, looking at Figure 1a, the greatest per-
formance improvement occurs, perhaps surpris-
ingly, when we have seen no unknown words.
As we see more unknown words, the improve-
ment from self-training decreases. This is a pretty
clear indication that unknown words are not a good
predictor of when self-training improves perfor-
mance.
A possible objection that one might raise is that
using unknown biheads as a regression feature will
bias our results if they are counted from gold trees
instead of parsed trees. Seeing a bihead in train-
ing will cause the otherwise sparse biheads dis-
tribution to be extremely peaked around that bi-
f -score Model
89.8 ? WSJ (baseline)
89.8 ? WSJ+NANC M
89.9 ? WSJ+NANC T
89.9 ? WSJ+NANC L
90.0 ? WSJ+NANC R
90.0 WSJ+NANC MT
90.1 WSJ+NANC H
90.2 WSJ+NANC LR
90.3 WSJ+NANC LRT
90.4 WSJ+NANC LMRT
90.4 WSJ+NANC LMR
90.5 WSJ+NANC LRH
90.7 ? WSJ+NANC LMRH
90.8 ? WSJ+NANC (fully self-trained)
Table 5: Performance of the first-stage parser
on various combinations of distributions WSJ and
WSJ+NANC (self-trained) models on sections 1,
22, and 24. Distributions are L (left expansion), R
(right expansion), H (head word), M (head phrasal
category), and T (head POS tag). ? and ? indicate
the model is not significantly different from base-
line and self-trained model, respectively.
head. If we see the same pair of words in testing,
we are likely to connect them in the same fash-
ion. Thus, if we count unknown biheads from gold
trees, this feature may explain away other improve-
ments: When gold trees contain a bihead found in
our self-training data, we will almost always see an
improvement. However, given the similar trends in
Figures 1b and 1c, we propose that unknown bi-
grams can be thought of as a rough approximation
of unknown biheads.
The regression analysis reveals that unknown bi-
grams and unknown biheads are good predictors of
f -score improvements. The significant predictors
from McClosky et al (2006) such as the number
of conjunctions or sentence length continue to be
helpful whereas unknown words are a weak pre-
dictor at best. These results are apparent in Figure
1: as stated before, seeing more unknown words
does not correlate with improvements. However,
seeing more unknown bigrams and biheads does
predict these changes fairly well. When we have
seen zero or one new bigrams or biheads, self-
training negatively impacts performance. After
seeing two or more, we see positive effects until
about six to ten after which improvements taper
off.
566
To see the effect of biheads on performance
more directly, we also experiment by interpolat-
ing between the WSJ and self-trained models on a
distribution level. To do this, we take specific dis-
tributions (see Section 3.1) from the self-trained
model and have them override the corresponding
distributions in a compatible WSJ trained model.
From this we hope to show which distributions
self-training boosts. According to the biheads hy-
pothesis, the H distribution (which captures infor-
mation about head-to-head dependencies) should
account for most of the improvement.
The results of moving these distributions is
shown in Table 5. For each new model, we show
whether the model?s performance is not signifi-
cantly different than the baseline model (indicated
by ?) or not significantly different than the self-
trained model (?). H (biheads) is the strongest sin-
gle feature and the only one to be significantly bet-
ter than the baseline. Nevertheless, it is only 0.3%
higher, accounting for 30% of the full self-training
improvement. In general, the performance im-
provements from distributions are additive (+/?
0.1%). Self-training improves all distributions, so
biheads are not the full picture. Nevertheless, they
remain the strongest single feature.
5 Discussion
The experiments in this paper have clarified many
details about the nature of self-training for parsing.
We have ruled out the phase transition hypothe-
sis entirely. Reduced search errors are responsible
for some, but not all, of the improvements in self-
training. We have confirmed that non-generative
reranker features are more beneficial than genera-
tive reranker features since they make the rerank-
ing parser more different from the base parser. Fi-
nally, we have found that while unknown bigrams
and biheads are a significant source of improve-
ment, they are not the sole source of it. Since
unknown words do not correlate well with self-
training improvements, there must be something
about the unknown bigrams and biheads which are
aid the parser. Our belief is that new combinations
of words we have already seen guides the parser in
the right direction. Additionally, these new combi-
nations result in more peaked distributions which
decreases the number of search errors.
However, while these experiments and others
get us closer to understanding self-training, we still
lack a complete explanation. Naturally, the hy-
0 1 2 3 4 5 6 7 10 11 12
To
ta
l n
um
be
r o
f
in
co
rre
ct
 n
od
es
0
60
00
0 1 2 3 4 5 6 7 10 11 12
Number of unknown words in tree
R
ed
uc
tio
n 
in
in
co
rre
ct
 n
od
es
0
30
0
(a) Effect of unknown words on performance
0 2 4 6 8 10 12 14 16 18 20
To
ta
l n
um
be
r o
f
in
co
rre
ct
 n
od
es
0
10
00
0 2 4 6 8 10 12 14 16 18 20
Number of unknown bigrams in tree
R
ed
uc
tio
n 
in
in
co
rre
ct
 n
od
es
?
10
0
10
0
(b) Effect of unknown bigrams on performance
0 2 4 6 8 10 12 14 16 18 20 25
To
ta
l n
um
be
r o
f
in
co
rre
ct
 n
od
es
0
10
00
0 2 4 6 8 10 12 14 16 18 20 25
Number of unknown biheads in tree
R
ed
uc
tio
n 
in
in
co
rre
ct
 n
od
es
?
50
10
0
(c) Effect of unknown biheads on performance
Figure 1: Change in the number of incorrect parse
tree nodes between WSJ and self-trained models
as a function of number of unknown items. See-
ing any number of unknown words results in fewer
errors on average whereas seeing zero or one un-
known bigrams or biheads is likely to hurt perfor-
mance.
567
potheses tested are by no means exhaustive. Addi-
tionally, we have only considered generative con-
stituency parsers here and a good direction for fu-
ture research would be to see if self-training gener-
alizes to a broader class of parsers. We suspect that
using a generative parser/discriminative reranker
paradigm should allow self-training to extend to
other parsing formalisms.
Recall that in Reichart and Rappoport (2007)
where only a small amount of labeled data was
used, the number of unknown words in a sen-
tence was a strong predictor of self-training ben-
efits. When a large amount of labeled data is avail-
able, unknown words are no longer correlated with
these gains, but unknown bigrams and biheads are.
When using a small amount of training data, un-
known words are useful since we have not seen
very many words yet. As the amount of train-
ing data increases, we see fewer new words but
the number of new bigrams and biheads remains
high. We postulate that this difference may help
explain the shift from unknown words to unknown
bigrams and biheads. We hope to further inves-
tigate the role of these unknown items by seeing
how our analyses change under different amounts
of labeled data relative to unknown item rates.
Acknowledgments
This work was supported by DARPA GALE contract
HR0011-06-2-0001. We would like to thank Matt Lease, the
rest of the BLLIP team, and our anonymous reviewers for
their comments. Any opinions, findings, and conclusions or
recommendations expressed in this paper are those of the au-
thors and do not necessarily reflect the views of DARPA.
References
Blum, Avrim and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. In Pro-
ceedings of the 11th Annual Conference on Compu-
tational Learning Theory (COLT-98).
Bod, Rens. 2003. An efficient implementation of a
new DOP model. In 10th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics, Budapest, Hungary.
Charniak, Eugene and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and MaxEnt discriminative
reranking. In Proc. of the 2005 Meeting of the Assoc.
for Computational Linguistics (ACL), pages 173?
180.
Charniak, Eugene. 1997. Statistical parsing with a
context-free grammar and word statistics. In Proc.
AAAI, pages 598?603.
Collins, Michael and Terry Koo. 2005. Discriminative
Reranking for Natural Language Parsing. Computa-
tional Linguistics, 31(1):25?69.
Graff, David. 1995. North American News Text Cor-
pus. Linguistic Data Consortium. LDC95T21.
McClosky, David, Eugene Charniak, and Mark John-
son. 2006. Effective self-training for parsing. In
Proceedings of the Human Language Technology
Conference of the NAACL, Main Conference, pages
152?159.
Mitch Marcus et al 1993. Building a large annotated
corpus of English: The Penn Treebank. Comp. Lin-
guistics, 19(2):313?330.
Petrov, Slav, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of the 21st
International Conference on Computational Linguis-
tics and 44th Annual Meeting of the Association for
Computational Linguistics, pages 433?440, Sydney,
Australia, July. Association for Computational Lin-
guistics.
Pradhan, Sameer, Wayne Ward, and James Martin.
2007. Towards robust semantic role labeling. In Hu-
man Language Technologies 2007: The Conference
of the North American Chapter of the Association for
Computational Linguistics; Proceedings of the Main
Conference, pages 556?563, Rochester, New York,
April. Association for Computational Linguistics.
Reichart, Roi and Ari Rappoport. 2007. Self-training
for enhancement and domain adaptation of statistical
parsers trained on small datasets. Proceedings of the
45th Annual Meeting of the Association of Computa-
tional Linguistics, pages 616?623.
Steedman, Mark, Steven Baker, Jeremiah Crim,
Stephen Clark, Julia Hockenmaier, Rebecca Hwa,
Miles Osborne, Paul Ruhlen, and Anoop Sarkar.
2003. CLSP WS-02 Final Report: Semi-Supervised
Training for Statistical Parsing. Technical report,
Johns Hopkins University.
Titov, Ivan and James Henderson. 2007. Constituent
parsing with incremental sigmoid belief networks.
In Proceedings of the 45th Annual Meeting of the As-
sociation of Computational Linguistics, pages 632?
639, Prague, Czech Republic, June. Association for
Computational Linguistics.
Yamada, Kenji and Kevin Knight. 2001. A syntax-
based statistical translation model. In Proceedings
of the 39th Annual Meeting of the Association for
Computational Linguistics, pages 523?529.
568
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 676?683, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Improving Statistical MT through Morphological Analysis
Sharon Goldwater
Dept. of Cognitive and Linguistic Sciences
Brown University
sharon goldwater@brown.edu
David McClosky
Dept. of Computer Science
Brown University
dmcc@cs.brown.edu
Abstract
In statistical machine translation, estimat-
ing word-to-word alignment probabilities
for the translation model can be difficult
due to the problem of sparse data: most
words in a given corpus occur at most a
handful of times. With a highly inflected
language such as Czech, this problem can
be particularly severe. In addition, much
of the morphological variation seen in Czech
words is not reflected in either the morphol-
ogy or syntax of a language like English. In
this work, we show that using morphologi-
cal analysis to modify the Czech input can
improve a Czech-English machine transla-
tion system. We investigate several differ-
ent methods of incorporating morphological
information, and show that a system that
combines these methods yields the best re-
sults. Our final system achieves a BLEU
score of .333, as compared to .270 for the
baseline word-to-word system.
1 Introduction
In a statistical machine translation task, the goal is
to find the most probable translation of some foreign
language text f into the desired language e. That is,
the system seeks to maximize P (e|f). Rather than
maximizing P (e|f) directly, the standard noisy chan-
nel approach to translation uses Bayes inversion to
split the problem into two separate parts:
argmax
e
P(e|f) = argmax
e
P(e)P(f |e) (1)
where P (e) is known as the language model and
P (f |e) is known as the translation model. The limit-
ing factor in machine translation is usually the qual-
ity of the translation model, since the monolingual
resources needed for training the language model are
generally more available than the parallel corpora
needed for training the translation model.
Due to the difficulty in obtaining large parallel cor-
pora, sparse data is a serious issue when estimating
the parameters of the translation model. This prob-
lem is compounded when one or both of the lan-
guages involved is a highly inflected language. In this
paper, we present a series of experiments suggesting
that morphological analysis can be used to reduce
data sparseness and increase similarity between lan-
guages, thus improving the quality of machine trans-
lation for highly inflected languages. Our work is on
a language pair in which the input language (Czech)
is highly inflected, and the output language (English)
is not. We discuss in Section 5 how our methods
might be generalized to pairs where both languages
are highly inflected.
The plan of this paper is as follows: In Section
2, we review previous work on using morphologi-
cal analysis for statistical machine translation. In
Section 3, we describe several methods for utilizing
morphological information in a statistical translation
model. Section 4 presents the results of our experi-
ments using these methods. Sections 5 and 6 discuss
the results of our experiments and conclude the pa-
per.
2 Previous Work
Until recently, most machine translation projects in-
volved translating between languages with relatively
little morphological structure. Nevertheless, a few
research projects have investigated the use of mor-
phology to improve translation quality. Niessen and
Ney (2000; 2004) report work on German-English
translation, where they investigate various types
of morphosyntactic restructuring, including merging
German verbs with their detached prefixes, annotat-
ing a handful of frequent ambiguous German words
with POS tags, combining idiomatic multi-word ex-
pressions into single words, and undoing question in-
676
version and do-insertion in both German and En-
glish. In addition, Niessen and Ney (2004) decom-
pose German words into a hierarchical representa-
tion using lemmas and morphological tags, and use
a MaxEnt model to combine the different levels of
representation in the translation model. The results
from these papers indicate that on corpus sizes up
to 60,000 parallel sentences, the restructuring op-
erations yielded a large improvement in translation
quality, but the morphological decomposition pro-
vided only a slight additional benefit. However, since
German is not as morphologically complex as Czech,
we might expect a larger benefit from morphological
analysis in Czech.
Another project utilizing morphological analysis
for statistical machine translation is described by Lee
(2004). Lee?s system for Arabic-English translation
takes as input POS-tagged English and Arabic text,
where the Arabic words have been pre-segmented
into stems and affixes. The system performs an ini-
tial alignment of the Arabic morphemes to the En-
glish words. Based on the consistency of the English
POS tag that each Arabic morpheme aligns to, the
system determines whether to keep that morpheme
as a separate item, merge it back onto the stem,
or delete it altogether. In addition, multiple occur-
rences of the determiner Al within a single Arabic
noun phrase are deleted (i.e. only one occurrence
is allowed). Using a phrase-based translation model,
Lee found that Al-deletion was more helpful than the
rest of the morphological analysis. Also, Al-deletion
helped for training corpora up to 3.3 million sen-
tences, but the other morphological analysis helped
only on the smaller corpus sizes (up to 350,000 paral-
lel sentences). This result is consistent with anecdo-
tal evidence suggesting that morphological analysis
becomes less helpful as corpus sizes increase. How-
ever, since parallel corpora of hundreds of thousands
of sentences or more are often difficult to obtain, it
would still be worthwhile to develop a method for
improving systems trained on smaller corpora.
Previous results on Czech-English machine trans-
lation suggest that morphological analysis may be
quite productive for this highly inflected language
where there is only a small amount of closely trans-
lated material. C?mejrek et al (2003), while not fo-
cusing on the use of morphology, give results indicat-
ing that lemmatization of the Czech input improves
BLEU score relative to baseline. These results sup-
port the earlier findings of Al-Onaizan et al (1999),
who used subjective scoring measures. Al-Onaizan
et al measured translation accuracy not only for
lemmatized input, but for an input form they re-
fer to as Czech?. Czech? is intended to capture many
of the morphological distinctions of English, while
discarding those distinctions that are Czech-specific.
The Czech? input was created by distinguishing the
Czech lemmas for singular and plural nouns, differ-
ent verb tenses, and various inflections on pronouns.
Artificial words were also added automatically in
cases where syntactic information in the Czech parse
trees indicated that articles, pronouns, or preposi-
tions might be expected in English. The transforma-
tion to Czech? provided a small additional increase
in translation quality over basic lemmatization.
The experiments described here are similar to
those performed by Al-Onaizan et al (1999), but
there are several important differences. First, we use
no syntactic analysis of the Czech input. Our intent
is to determine how much can be gained by a purely
morphological approach to translation. Second, we
present some experiments in which we modify the
translation model itself to take advantage of morpho-
logical information, rather than simply transforming
the input. Finally, our use of BLEU scores rather
than subjective measurements allows us to perform
more detailed evaluation. We examine the effects of
each type of morphological information separately.
3 Morphology for MT
Morphological variations in Czech are reflected in
several different ways in English. In some cases, such
as verb past tenses or noun plurals, morphological
distinctions found in Czech are also found in English.
In other instances, English may use function words
to express a meaning that occurs as a morphological
variant in Czech. For example, genitive case marking
can often be translated as of and instrumental case
as by or with. In still other instances, morphologi-
cal distinctions made in Czech are either completely
absent in English (e.g. gender on common nouns)
or are reflected in English syntax (e.g. many case
markings). Handling these correspondences between
morphology and syntax requires analysis above the
lexical level and is therefore beyond the scope of this
paper. However, morphological analysis of the Czech
input can potentially be used to improve the trans-
lation model by exploiting the other types of corre-
spondences we have mentioned.
Before we describe how this can be done, it is im-
portant to clarify the kind of morphological anal-
ysis we assume in our input. Our data comes
from the Prague Czech-English Dependency Tree-
bank (PCEDT) (Hajic?, 1998; C?mejrek et al, 2004),
the Czech portion of which has been fully annotated
with morphological information. Each Czech word in
the corpus is associated with an analysis containing
the word?s lemma and a sequence of morphological
677
Pro/pro/RR--4----------
ne?koho/ne?kdo/PZM-4----------
by/by?t/Vc-X---3-------
jej??/jeho/PSZS1FS3-------
proveden??/proveden??/NNNS4-----A----
me?lo/m??t/VpNS---XR-AA---
smysl/smysl/NNIS4-----A----
././Z:-------------
Figure 1: A sentence from the PCEDT corpus. Each
token is followed by its lemma and a string giving
the values of up to 15 morphological tags. Dashes
indicates tags that are not applicable for a particu-
lar token. This sentence corresponds to the English
sentence It would make sense for somebody to do it.
tags. These tags provide values along several mor-
phological dimensions, such as part of speech, gen-
der, number, tense, and negation. There are a total
of 15 dimensions along which words may be charac-
terized, although most words have a number of di-
mensions unspecified. An example sentence from the
Czech corpus is shown in Figure 1.
In what follows, we describe four different ways
that the Czech lemma and tag information can be
used to modify the parameters of the translation
model. The first three of these are similar to the work
of Al-Onaizan et al (1999) and involve transforma-
tions to the input data only. The assumptions un-
derlying the word alignment model P (fj|ei) (where
fj and ei are individual words in an aligned sen-
tence pair) are maintained. The fourth method of
incorporating morphological information is novel and
changes the alignment model itself.
3.1 Lemmas
A very simple way to modify the input data us-
ing morphological information is by replacing each
wordform with its associated lemma (see Figure 2).
Based on previous results (Al-Onaizan et al, 1999;
C?mejrek et al, 2003), we expected that this trans-
formation would lead to an improvement in trans-
lation quality due to reduction of data sparseness.
However, since lemmatization does remove some use-
ful information from the Czech wordforms, we also
tried two alternative lemmatization schemes. First,
we tried lemmatizing only certain parts of speech,
leaving other parts of speech alone. We reasoned
that nouns, verbs, and pronouns all carry inflectional
morphology in English, so by lemmatizing only the
other parts of speech, we might retain some of the
benefits of full lemmatization without losing as much
information. We also tried lemmatizing all parts of
speech except pronouns, which are very common and
therefore should be less affected by sparse data prob-
lems.
As a second alternative to full lemmatization, we
experimented with lemmatizing only the less fre-
quent wordforms in the corpus. This allows the
translation system to use the full wordform infor-
mation from more frequent forms, where sparse data
is less of a problem.
To determine whether knowledge of lemmas was
actually necessary, we compared lemmatization with
word truncation. We truncated each wordform in the
data after a fixed number of characters, as suggested
by Och (1995).
3.2 Pseudowords
As discussed earlier, much of the information en-
coded in Czech morphology is encoded as function
words in English. One way to reintroduce some of
the information lost during Czech lemmatization is
by using some of the morphological tags to add ex-
tra ?words? to the Czech input. In many cases,
these pseudowords will also increase the correspon-
dence of English function words to items in the Czech
input. In our system, each pseudoword encodes a
single morphological tag (feature/value pair), such
as PER 1 (?first person?) or TEN F (?future tense?).
Figure 2 shows a Czech input sentence after gener-
ating pseudowords for the person feature on verbs.
We expected that the class of tags most likely to
be useful as pseudowords would be the person tags,
because Czech is a pro-drop language. Using the
person tags as pseudowords should simulate the ex-
istence of pronouns for the English pronouns to align
to. We also expected that negation (which is ex-
pressed on verbs in Czech) would be a useful pseu-
doword, and that case markings might also be helpful
since they sometimes correspond to prepositions in
English, such as of, with, or to.
3.3 Modified Lemmas
In some cases, such as the past tense, Czech mor-
phology is likely to correspond not to a function
word in English, but rather to English inflectional
morphology. In order to capture this kind of phe-
nomenon, we experimented with concatenating the
Czech morphological tags onto their lemmas instead
of inserting them as separate input tokens. See Fig-
ure 2 for an example. This concatenation creates
distinctions between some lemmas, which will ide-
ally correspond to morphological distinctions made
in English. Although this transformation splits the
Czech data (relative to pure lemmatization), it still
suppresses many of the distinctions made in the full
Czech wordforms. We expected that number mark-
678
Words: Pro ne?koho by jej?? proveden?? me?lo smysl .
Lemmas: pro ne?kdo by?t jeho proveden?? m??t smysl .
Lemmas+Pseudowords: pro ne?kdo by?t PER 3 jeho proveden?? m??t PER X smysl .
Modified Lemmas: pro ne?kdo by?t+PER 3 jeho proveden?? m??t+PER X smysl .
Figure 2: Various transformations of the Czech sentence from Figure 1. The pseudowords and modified
lemmas encode the verb person feature, with the values 3 (third person) and X (?any? person).
ing on nouns and tense marking on verbs would be
the tags best treated in this way.
3.4 Morphemes
Our final set of experiments used the same input for-
mat as the Modified Lemma experiments. However,
in this set of experiments, we changed the model used
to calculate the word-to-word alignment probabili-
ties. In the standard system, the alignment model
parameters P (fj |ei) are found using maximum like-
lihood estimation based on the expected number of
times fj aligns to ei in the parallel corpus. Our new
model assumes a compositional structure for fj , so
that fj = fj0 . . . fjK , where fj0 is the lemma of
fj , and fj1 . . . fjK are morphemes generated from
the tags associated with fj . We assume that every
word contains exactly K morphemes, and that the
kth morpheme in each word is used to encode the
value for the kth class of morphological tag, where
the classes (e.g. person or tense) are assigned an or-
dering beforehand. fjk is assigned a null value if the
value of the kth tag class is unspecified for fj .
Given this decomposition of words into mor-
phemes, and a generative model in which each mor-
pheme in fj is generated independently conditioned
on ei, we have
P(fj|ei) =
K
?
k=0
P(fjk|ei) (2)
We can now estimate P(fj |ei) using a slightly
modified version of the standard EM algorithm for
learning alignment probabilities. During the E step,
we calculate the expected alignment counts between
Czech morphemes and English words based on the
current word alignments, and revise our estimate of
P(fj|ei) using Equation 2. The M step of the algo-
rithm remains the same.
The morpheme-based model in Equation 2 is sim-
ilar to the modified lemma model in that it removes
much of the differentiation between Czech word-
forms, but leaves the differences that are most likely
to appear as inflection on English words. However,
it also performs an additional smoothing function.
The model assumes that, in the absence of other in-
formation, an English word that has aligned mostly
to Czech words with a particular morphological tag
is more likely to align to another word with this tag
than to a Czech word with a different tag. For ex-
ample, an English word aligned to mostly past tense
forms is more likely to align to another past tense
form than to a present or future tense form.
4 Experiments
In order to evaluate the effectiveness of the tech-
niques described in the previous section, we ran a
number of experiments using data from the PCEDT
corpus. The English portion of this corpus (used to
train the language model) contains the same material
as the Penn WSJ corpus, but with a different divi-
sion into training, development, and test sets. About
250 sentences each for development and test were
translated once into Czech and then back into En-
glish by five different translators. These translations
are used to calculate BLEU scores. The remainder
of the corpus (about 50,000 sentences) is used for
training. About 21,000 of the training sentences have
been translated into Czech and morphologically an-
notated for use as a parallel corpus.
Some statistics on the parallel corpus are shown
in the graph in Figure 3. This graph illustrates the
sparse data problem in Czech that our morpholog-
ical analysis is intended to address. Although the
number of infrequently occurring lemmas is about
the same in both English and Czech, the number of
infrequently occurring inflected wordforms is approx-
imately twice as high in Czech.1
For all of our experiments, we used the same lan-
guage model, trained with the CMU Statistical Lan-
guage Modelling Toolkit (Clarkson and Rosenfeld,
1997). Our translation models were trained using
GIZA++ (Och and Ney, 2003), which we modi-
1Although we did not use it for the experiments in
this paper, the PCEDT corpus does contain lemma in-
formation for the English data. There is a slight discrep-
ancy between the English and Czech data in the lemma
information for pronouns, in that English pronouns (in-
cluding accusitive, possessive, and other forms) are as-
signed themselves as lemmas, whereas Czech pronouns
are reduced to uninflected forms. Given that pronouns
generally have many tokens, this discrepancy should not
affect the data in Figure 3.
679
1 2 3 4 5 6 7 8 9 10
0
0.5
1
1.5
2
2.5
3
3.5
x 104
Token count
Ite
m
 c
ou
nt
English Wordforms
Czech Wordforms
English Lemmas
Czech Lemmas
Figure 3: The number of items (full wordforms or
lemmas) y appearing in the parallel corpus with a
token count of x.
fied as necessary for the morpheme-based experi-
ments. We used the ISI ReWrite Decoder (Marcu
and Germann, 2005) for producing translations. Be-
fore beginning our experiments, we obtained a base-
line BLEU score by training a standard word-to-word
translation model. Our baseline results indicate that
the test set for this corpus is considerably more diffi-
cult than the development set: word-to-word scores
were .311 (development) and .270 (test).
4.1 Lemmas
As Figure 3 shows, lemmatization of the Czech cor-
pus cuts the number of unique items by more than
half, and the number of items with no more than
ten occurrences by nearly half. The lemmatization
BLEU scores in Table 1 indicate that this has a large
impact on the quality of translation. As expected,
full lemmatization performed better than word-to-
word translation, with an an improvement of about
.04 in the development set BLEU score and .03 in
the test set. (In this and the following experiments,
BLEU score differences of .009 or more are signifi-
cant at the .05 level.) Experiments on the develop-
ment set showed that leaving certain parts of speech
unlemmatized did not improve results, but lemma-
tizing only low-frequency words did. A frequency
cutoff of 50 worked best on the development set (i.e.
only words with frequency less than 50 were lemma-
tized). Despite the improvement on the development
set, using this cutoff with the test set yielded only a
non-significant improvement over full lemmatization.
The results of these lemmatization experiments
support the argument that lemmatization improves
translation quality by reducing data sparseness, but
also removes potentially useful information. Our re-
Dev Test
word-to-word .311 .270
lemmatize all .355 .299
except Pro .350
except Pro, V, N .346
lemmatize n < 50 .370 .306
truncate all .353 .283
Table 1: BLEU scores for the word-to-word baseline,
lemmatization, and word truncation experiments.
sults suggest that lemmatizing only infrequent words
may, in some cases, work better than lemmatizing all
words.
As Table 1 indicates, it is possible to get some
of the benefits of lemmatization without using any
morphological knowledge at all. For both dev and
test sets, truncating words to 6 characters (the best
length on the dev set) provided a significant im-
provement over word-to-word translation, but was
also significantly worse than the best lemmatization
scores. Changing the frequency cutoff for trunca-
tion did not produce any significant differences in
the BLEU score.
4.2 Pseudowords
Results for the pseudoword experiments on the devel-
opment set are shown in the first column of Table 2.
Note that in these (and the following) experiments,
we treated all words the same way regardless of their
frequency, so the effects of adding morphological in-
formation are in comparison to the full lemmatiza-
tion scheme. In most of our experiments, we added
morphological information for only a single class of
tags at a time in order to determine the effects of
each class individually. The classes we used were
verb person (PER), verb tense (TEN), noun number
(NUM), noun case (CASE), and negation (NEG).
Most of the results of the pseudoword experiments
confirm our expectations. Adding the verb person
tags was helpful, and examination of the alignments
revealed that they did indeed align to English pro-
nouns with high probability. The noun number tags
did not help, since plurality is expressed as an affix
in English. Negation tags helped slightly, though the
improvement was not significant. This is probably
because negation tags are relatively infrequent, as
can be seen in Table 3. The addition of pseudowords
for case did not yield an improvement, probably be-
cause these pseudowords were so frequent. The ad-
ditional ambiguity caused by so many extra words
likely overwhelmed any positive effect.
A somewhat puzzling result is the behavior of the
680
Tag type Pseudo Mod-Lem Morph
PER .365 .356 .356
TEN .365 .361 .364
PER,TEN .355 .362 .355
NUM .354 .367 .361
CASE .353 .340 .337
NEG .357 .356 .353
Table 2: BLEU scores indicating the results of in-
corporating the information from different classes
of morphological tags in the the experiments us-
ing pseudowords (Pseudo), modified lemmas (Mod-
Lem), and morphemes (Morph). Scores are from the
development set. Differences of .009 are significant
(p < .05).
Tag class Count Avg/sentence
PER 49700 2.35
TEN 47744 2.26
past 22544 1.07
pres 20291 0.96
fut 1707 0.08
?any? 3202 0.15
NUM 151646 7.17
CASE 151646 7.17
NEG 3326 0.16
Table 3: Number of occurrences of each class of tags
in the Czech training data.
verb tense tags. With the exception of future tense,
English generally does not mark tense with an aux-
iliary. Yet Table 3 shows that only a very small per-
centage of sentences have a future tense marker, so
it seems unlikely that this explains the positive ef-
fects of the tense pseudowords. In fact, we tried
adding only future tense pseudowords to the lem-
matized Czech data, and found that the results were
no better than basic lemmatization.
The other unusual behavior we see with pseu-
dowords is that when verb person and tense tags are
combined, they seem to cancel each other out, result-
ing in a score that is no better than lemmatization
alone. Examination of the alignments did not reveal
any obvious reason for this effect.
4.3 Modified Lemmas
As shown in the second column of Table 2, the num-
ber and tense tags yield an improvement under the
modified lemma transformation, while the person
tags do not. Again, this confirms our predictions
based on the morphology of English.
Our results using the case tags under this model
actually decreased performance, but this is not
surprising given that differentiating Czech lemmas
based on case marking creates as much as a 7-way
split of the data (there are seven cases in Czech),
without adding much information that would be use-
ful in English.
4.4 Morphemes
BLEU scores for the morpheme-based model are
given in the third column of Table 2. None of the
differences in scores between this model and the mod-
ified lemma model are significant, although the trend
for most of the tag classes is for this model to per-
form slightly worse. This suggests that the type of
smoothing induced by the morpheme-based model
may not be as helpful as simply attempting to cre-
ate Czech words that reflect the same morphological
distinctions as the English words. In Section 5, we
propose a generalized version of the morpheme model
that might be an improvement.
4.5 Combined Model
In the experiments described so far, we used only
a single method at a time of incorporating mor-
phological information into the translation process.
However, it is straightforward to combine the pseu-
doword method with either the modified-lemma or
morpheme-based methods by using pseudowords for
certain tags and attaching others to the Czech lem-
mas. The experiments described above allowed us to
confirm our intuitions about how each class of tags
should be treated under such a combined model. We
then created a model using the pseudoword treat-
ment of the person and negation tags, and the mod-
ified lemma treatment of number and tense. We did
not use the case tags in this model, since they did
not seem to yield an improvement in any of the three
basic morphological models.
Our combined model achieved a BLEU score of
.390 (development) and .333 (test), outperforming
the models in all of our previous experiments.
5 Discussion
The results of our experiments provide additional
support for the findings of previous researchers that
using morphological analysis can improve the quality
of statistical machine translation for highly-inflected
languages. While human judgment is probably the
best metric for evaluating translation quality, our use
of the automatically-derived BLEU score allowed us
to easily compare many different translation models
and evaluate the effects of each one individually. We
found that simple lemmatization, by significantly re-
ducing the sparse data problem, was quite effective
681
despite the loss of information involved. Lemmatiz-
ing the less frequent words in the corpus seemed to
increase performance slightly, but these results were
inconclusive. Word truncation, which requires no
morphological information at all, was effective at in-
creasing scores over the word-to-word baseline, but
did not perform quite as well as lemmatization. This
result conflicts with Och?s (Och, 1995), and is likely
due to the much smaller size of our corpus. In any
case, our results suggest that lemmatization or word
truncation could yield a significant improvement in
the quality of translation from a highly-inflected to
a less-inflected language, even when limited morpho-
logical information is available.
Our primary results concern the use of full mor-
phological information. We found that certain tags
were more useful when we treated them as discrete
input words, while others provided a greater benefit
when attached directly to their lemmas. The best
choice of which method to use for each class of tags
seems to correspond closely with how that class of in-
formation is expressed in English (either using func-
tion words or inflection). In a sense, the goal of the
morphological analysis is to make the Czech input
data more English-like by suppressing unnecessary
morphological distinctions and expressing necessary
distinctions in ways that are similar to English. This
sort of procedure could be taken further by incorpo-
rating syntactic information as well, but as we stated
earlier, our goal was to determine exactly how much
benefit we could derive from a strictly morphological
approach.
In the work we have presented, the output lan-
guage (English) is low in inflection. We therefore
considered it less important to perform morphologi-
cal analysis on the English data. However, we expect
that the work described here could be generalized to
highly inflected output languages by doing morpho-
logical analysis on both the input and output lan-
guages. The most promising way to do this seems
to be by extending the morpheme-based translation
model in Equation 2 to incorporate morphemes in
both languages, so that
P(fj|ei) =
K
?
k=0
P(fjk|eik) (3)
where fjk are the morphemes in the input language,
and eik are the corresponding morphemes in the out-
put language. This extended model may also prove
a benefit to Czech-English translation; we are cur-
rently investigating this possibility.
In this work, we used a word-based translation sys-
tem due to the availability of source code that could
be modified for our morph experiments. An obvious
extension to the current work would be to move to a
phrase-based translation system. One advantage of
phrase-based models is their ability to align phrases
in one language to morphologically complex words in
the other language. However, this feature still suffers
from the same sparse data problems as a word-based
system: if a morphologically complex word only ap-
pears a handful of times in the training corpus, the
system will have difficulty determining its (phrasal
or word) alignment. We expect that morphological
analysis would still be helpful in this situation, at the
very least because it can be used to remove distinc-
tions that appear in only one language.
6 Conclusion
In this paper we used morphological analysis of
Czech to improve a Czech-English statistical machine
translation system. We have argued that this im-
provement was primarily due to a reduction of the
sparse data problem caused by the highly inflected
nature of Czech. An alternative method for reducing
sparse data is to use a larger parallel corpus; however,
it is often easier to obtain additional monolingual re-
sources, such as a morphological analyzer or tagged
corpus, than additional parallel data for a specific
language pair. For that reason, we believe that the
approach taken here is a promising one.
We have described several different ways of using
morphological information for machine translation,
and have shown how these can be combined to yield
an improved translation model. In general, we would
not expect the exact combination of techniques that
yielded our best results for Czech-English to be op-
timal for other language pairs. Rather, we have sug-
gested that these techniques should be combined in
a way that makes the input language more similar
to the output language. Although this combination
will need to be determined for each language pair, the
general approach outlined here should provide ben-
efits for any MT system involving a highly inflected
language.
Acknowledgements
We would like to thank Eugene Charniak and the
members of BLLIP for their encouragement and
helpful suggestions. This research was partially sup-
ported by NSF awards IGERT 9870676 and ITR
0085940.
682
References
Y. Al-Onaizan, J. Cur?in, M. Jahr, K. Knight, J. Laf-
ferty, D. Melamed, F. Och, D. Purdy, N. Smith,
and D. Yarowsky. 1999. Statistical machine trans-
lation. Final Report, JHU Summer Workshop
1999.
P. Clarkson and R. Rosenfeld. 1997. Sta-
tistical language modeling using the CMU-
Cambridge Toolkit. In Proceedings of ESCA
Eurospeech. Current version available at
http://mi.eng.cam.ac.uk/?prc14/toolkit.html.
J. Hajic?. 1998. Building a Syntactically Anno-
tated Corpus: The Prague Dependency Treebank.
In Eva Hajic?ova?, editor, Issues of Valency and
Meaning. Studies in Honor of Jarmila Panevova?,
pages 12?19. Prague Karolinum, Charles Univer-
sity Press.
Y. Lee. 2004. Morphological analysis for statistical
machine translation. In Proceedings NAACL.
D. Marcu and U. Germann. 2005. The
ISI ReWrite Decoder 1.0.0a. Available at
http://www.isi.edu/licensed-sw/rewrite-decoder/.
S. Niessen and H. Ney. 2000. Improving SMT quality
with morpho-syntactic analysis. In Proceedings of
COLING.
S. Niessen and H. Ney. 2004. Statistical machine
translation with scarce resources using morpho-
syntactic analysis. Computational Linguistics,
30(2):181?204.
F. J. Och and H. Ney. 2003. A systematic compari-
son of various statistical alignment models. Com-
putational Linguistics, 29(1):19?51.
F. Och. 1995. Statistical machine translation: The
fabulous present and future. Invited talk at the
Workshop on Building and Using Parallel Texts
at ACL?05.
M. C?mejrek, J. Cur???n, and J. Havelka. 2003. Czech-
english dependency-based machine translation. In
Proceedings of EACL.
M. C?mejrek, J. Cur???n, J. Havelka, J. Hajic?, and
V. Kubon?. 2004. Prague czech-english dependecy
treebank: Syntactically annotated resources for
machine translation. In 4th International Confer-
ence on Language Resources and Evaluation, Lis-
bon, Portugal.
683
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 152?159,
New York, June 2006. c?2006 Association for Computational Linguistics
Effective Self-Training for Parsing
David McClosky, Eugene Charniak, and Mark Johnson
Brown Laboratory for Linguistic Information Processing (BLLIP)
Brown University
Providence, RI 02912
{dmcc|ec|mj}@cs.brown.edu
Abstract
We present a simple, but surprisingly ef-
fective, method of self-training a two-
phase parser-reranker system using read-
ily available unlabeled data. We show
that this type of bootstrapping is possible
for parsing when the bootstrapped parses
are processed by a discriminative reranker.
Our improved model achieves an f -score
of 92.1%, an absolute 1.1% improvement
(12% error reduction) over the previous
best result for Wall Street Journal parsing.
Finally, we provide some analysis to bet-
ter understand the phenomenon.
1 Introduction
In parsing, we attempt to uncover the syntactic struc-
ture from a string of words. Much of the challenge
of this lies in extracting the appropriate parsing
decisions from textual examples. Given sufficient
labelled data, there are several ?supervised? tech-
niques of training high-performance parsers (Char-
niak and Johnson, 2005; Collins, 2000; Henderson,
2004). Other methods are ?semi-supervised? where
they use some labelled data to annotate unlabeled
data. Examples of this include self-training (Char-
niak, 1997) and co-training (Blum and Mitchell,
1998; Steedman et al, 2003). Finally, there are ?un-
supervised? strategies where no data is labeled and
all annotations (including the grammar itself) must
be discovered (Klein and Manning, 2002).
Semi-supervised and unsupervised methods are
important because good labeled data is expensive,
whereas there is no shortage of unlabeled data.
While some domain-language pairs have quite a bit
of labelled data (e.g. news text in English), many
other categories are not as fortunate. Less unsuper-
vised methods are more likely to be portable to these
new domains, since they do not rely as much on ex-
isting annotations.
2 Previous work
A simple method of incorporating unlabeled data
into a new model is self-training. In self-training,
the existing model first labels unlabeled data. The
newly labeled data is then treated as truth and com-
bined with the actual labeled data to train a new
model. This process can be iterated over different
sets of unlabeled data if desired. It is not surprising
that self-training is not normally effective: Charniak
(1997) and Steedman et al (2003) report either mi-
nor improvements or significant damage from using
self-training for parsing. Clark et al (2003) applies
self-training to POS-tagging and reports the same
outcomes. One would assume that errors in the orig-
inal model would be amplified in the new model.
Parser adaptation can be framed as a semi-
supervised or unsupervised learning problem. In
parser adaptation, one is given annotated training
data from a source domain and unannotated data
from a target. In some cases, some annotated data
from the target domain is available as well. The goal
is to use the various data sets to produce a model
that accurately parses the target domain data despite
seeing little or no annotated data from that domain.
Gildea (2001) and Bacchiani et al (2006) show that
out-of-domain training data can improve parsing ac-
152
curacy. The unsupervised adaptation experiment by
Bacchiani et al (2006) is the only successful in-
stance of parsing self-training that we have found.
Our work differs in that all our data is in-domain
while Bacchiani et al uses the Brown corpus as la-
belled data. These correspond to different scenarios.
Additionally, we explore the use of a reranker.
Co-training is another way to train models from
unlabeled data (Blum and Mitchell, 1998). Unlike
self-training, co-training requires multiple learners,
each with a different ?view? of the data. When one
learner is confident of its predictions about the data,
we apply the predicted label of the data to the train-
ing set of the other learners. A variation suggested
by Dasgupta et al (2001) is to add data to the train-
ing set when multiple learners agree on the label. If
this is the case, we can be more confident that the
data was labelled correctly than if only one learner
had labelled it.
Sarkar (2001) and Steedman et al (2003) inves-
tigated using co-training for parsing. These studies
suggest that this type of co-training is most effec-
tive when small amounts of labelled training data is
available. Additionally, co-training for parsing can
be helpful for parser adaptation.
3 Experimental Setup
Our parsing model consists of two phases. First, we
use a generative parser to produce a list of the top n
parses. Next, a discriminative reranker reorders the
n-best list. These components constitute two views
of the data, though the reranker?s view is restricted
to the parses suggested by the first-stage parser. The
reranker is not able to suggest new parses and, more-
over, uses the probability of each parse tree accord-
ing to the parser as a feature to perform the rerank-
ing. Nevertheless, the reranker?s value comes from
its ability to make use of more powerful features.
3.1 The first-stage 50-best parser
The first stage of our parser is the lexicalized proba-
bilistic context-free parser described in (Charniak,
2000) and (Charniak and Johnson, 2005). The
parser?s grammar is a smoothed third-order Markov
grammar, enhanced with lexical heads, their parts
of speech, and parent and grandparent informa-
tion. The parser uses five probability distributions,
one each for heads, their parts-of-speech, head-
constituent, left-of-head constituents, and right-of-
head constituents. As all distributions are condi-
tioned with five or more features, they are all heavily
backed off using Chen back-off (the average-count
method from Chen and Goodman (1996)). Also,
the statistics are lightly pruned to remove those that
are statistically less reliable/useful. As in (Char-
niak and Johnson, 2005) the parser has been mod-
ified to produce n-best parses. However, the n-best
parsing algorithm described in that paper has been
replaced by the much more efficient algorithm de-
scribed in (Jimenez and Marzal, 2000; Huang and
Chang, 2005).
3.2 The MaxEnt Reranker
The second stage of our parser is a Maximum En-
tropy reranker, as described in (Charniak and John-
son, 2005). The reranker takes the 50-best parses
for each sentence produced by the first-stage 50-
best parser and selects the best parse from those
50 parses. It does this using the reranking method-
ology described in Collins (2000), using a Maxi-
mum Entropy model with Gaussian regularization
as described in Johnson et al (1999). Our reranker
classifies each parse with respect to 1,333,519 fea-
tures (most of which only occur on few parses).
The features consist of those described in (Char-
niak and Johnson, 2005), together with an additional
601,577 features. These features consist of the parts-
of-speech, possibly together with the words, that
surround (i.e., precede or follow) the left and right
edges of each constituent. The features actually used
in the parser consist of all singletons and pairs of
such features that have different values for at least
one of the best and non-best parses of at least 5 sen-
tences in the training data. There are 147,456 such
features involving only parts-of-speech and 454,101
features involving parts-of-speech and words. These
additional features are largely responsible for im-
proving the reranker?s performance on section 23
to 91.3% f -score (Charniak and Johnson (2005) re-
ported an f -score of 91.0% on section 23).
3.3 Corpora
Our labeled data comes from the Penn Treebank
(Marcus et al, 1993) and consists of about 40,000
sentences from Wall Street Journal (WSJ) articles
153
annotated with syntactic information. We use the
standard divisions: Sections 2 through 21 are used
for training, section 24 is held-out development, and
section 23 is used for final testing. Our unlabeled
data is the North American News Text corpus, NANC
(Graff, 1995), which is approximately 24 million un-
labeled sentences from various news sources. NANC
contains no syntactic information. Sentence bound-
aries in NANC are induced by a simple discrimina-
tive model. We also perform some basic cleanups on
NANC to ease parsing. NANC contains news articles
from various news sources including the Wall Street
Journal, though for this paper, we only use articles
from the LA Times.
4 Experimental Results
We use the reranking parser to produce 50-best
parses of unlabeled news articles from NANC. Next,
we produce two sets of one-best lists from these 50-
best lists. The parser-best and reranker-best lists
represent the best parse for each sentence accord-
ing to the parser and reranker, respectively. Fi-
nally, we mix a portion of parser-best or reranker-
best lists with the standard Wall Street Journal train-
ing data (sections 2-21) to retrain a new parser (but
not reranker1) model. The Wall Street Journal train-
ing data is combined with the NANC data in the
following way: The count of each parsing event is
the (optionally weighted) sum of the counts of that
event in Wall Street Journal and NANC. Bacchiani
et al (2006) show that count merging is more effec-
tive than creating multiple models and calculating
weights for each model (model interpolation). Intu-
itively, this corresponds to concatenating our train-
ing sets, possibly with multiple copies of each to ac-
count for weighting.
Some notes regarding evaluations: All numbers
reported are f -scores2. In some cases, we evaluate
only the parser?s performance to isolate it from the
reranker. In other cases, we evaluate the reranking
parser as a whole. In these cases, we will use the
term reranking parser.
Table 1 shows the difference in parser?s (not
reranker?s) performance when trained on parser-best
1We attempted to retrain the reranker using the self-trained
sentences, but found no significant improvement.
2The harmonic mean of labeled precision (P) and labeled
recall (R), i.e. f = 2?P?RP+R
Sentences added Parser-best Reranker-best
0 (baseline) 90.3
50k 90.1 90.7
250k 90.1 90.7
500k 90.0 90.9
750k 89.9 91.0
1,000k 90.0 90.8
1,500k 90.0 90.8
2,000k ? 91.0
Table 1: f -scores after adding either parser-best or
reranker-best sentences from NANC to WSJ training
data. While the reranker was used to produce the
reranker-best sentences, we performed this evalua-
tion using only the first-stage parser to parse all sen-
tences from section 22. We did not train a model
where we added 2,000k parser-best sentences.
output versus reranker-best output. Adding parser-
best sentences recreates previous self-training ex-
periments and confirms that it is not beneficial.
However, we see a large improvement from adding
reranker-best sentences. One may expect to see a
monotonic improvement from this technique, but
this is not quite the case, as seen when we add
1,000k sentences. This may be due to some sec-
tions of NANC being less similar to WSJ or contain-
ing more noise. Another possibility is that these
sections contains harder sentences which we can-
not parse as accurately and thus are not as useful
for self-training. For our remaining experiments, we
will only use reranker-best lists.
We also attempt to discover the optimal number
of sentences to add from NANC. Much of the im-
provement comes from the addition of the initial
50,000 sentences, showing that even small amounts
of new data can have a significant effect. As we add
more data, it becomes clear that the maximum ben-
efit to parsing accuracy by strictly adding reranker-
best sentences is about 0.7% and that f -scores will
asymptote around 91.0%. We will return to this
when we consider the relative weightings of WSJ and
NANC data.
One hypothesis we consider is that the reranked
NANC data incorporated some of the features from
the reranker. If this were the case, we would not see
an improvement when evaluating a reranking parser
154
Sentences added 1 22 24
0 (baseline) 91.8 92.1 90.5
50k 91.8 92.4 90.8
250k 91.8 92.3 91.0
500k 92.0 92.4 90.9
750k 92.0 92.4 91.1
1,000k 92.1 92.2 91.3
1,500k 92.1 92.1 91.2
1,750k 92.1 92.0 91.3
2,000k 92.2 92.0 91.3
Table 2: f -scores from evaluating the rerank-
ing parser on three held-out sections after adding
reranked sentences from NANC to WSJ training.
These evaluations were performed on all sentences.
on the same models. In Table 2, we see that the new
NANC data contains some information orthogonal to
the reranker and improves parsing accuracy of the
reranking parser.
Up to this point, we have only considered giving
our true training data a relative weight of one. In-
creasing the weight of the Wall Street Journal data
should improve, or at least not hurt, parsing perfor-
mance. Indeed, this is the case for both the parser
(figure not shown) and reranking parser (Figure 1).
Adding more weight to the Wall Street Journal data
ensures that the counts of our events will be closer
to our more accurate data source while still incorpo-
rating new data from NANC. While it appears that
the performance still levels off after adding about
one million sentences from NANC, the curves cor-
responding to higher WSJ weights achieve a higher
asymptote. Looking at the performance of various
weights across sections 1, 22, and 24, we decided
that the best combination of training data is to give
WSJ a relative weight of 5 and use the first 1,750k
reranker-best sentences from NANC.
Finally, we evaluate our new model on the test
section of Wall Street Journal. In Table 3, we note
that baseline system (i.e. the parser and reranker
trained purely on Wall Street Journal) has improved
by 0.3% over Charniak and Johnson (2005). The
92.1% f -score is the 1.1% absolute improvement
mentioned in the abstract. The improvement from
self-training is significant in both macro and micro
tests (p < 10?5).
 91.7
 91.8
 91.9
 92
 92.1
 92.2
 92.3
 92.4
 0  5  10  15  20  25  30  35  40
f-s
co
re
NANC sentences added (units of 50k sentences)
WSJ x1
WSJ x3
WSJ x5
Figure 1: Effect of giving more relative weight to
WSJ training data on reranking parser f -score. Eval-
uations were done from all sentences from section
1.
Model fparser freranker
Charniak and Johnson (2005) ? 91.0
Current baseline 89.7 91.3
WSJ + NANC 91.0 92.1
Table 3: f -scores on WSJ section 23. fparser and
freranker are the evaluation of the parser and rerank-
ing parser on all sentences, respectively. ?WSJ +
NANC? represents the system trained on WSJ train-
ing (with a relative weight of 5) and 1,750k sen-
tences from the reranker-best list of NANC.
5 Analysis
We performed several types of analysis to better un-
derstand why the new model performs better. We
first look at global changes, and then at changes at
the sentence level.
5.1 Global Changes
It is important to keep in mind that while the
reranker seems to be key to our performance im-
provement, the reranker per se never sees the extra
data. It only sees the 50-best lists produced by the
first-stage parser. Thus, the nature of the changes to
this output is important.
We have already noted that the first-stage parser?s
one-best has significantly improved (see Table 1). In
Table 4, we see that the 50-best oracle rate also im-
155
Model 1-best 10-best 50-best
Baseline 89.0 94.0 95.9
WSJ?1 + 250k 89.8 94.6 96.2
WSJ?5 + 1,750k 90.4 94.8 96.4
Table 4: Oracle f -scores of top n parses produced
by baseline, a small self-trained parser, and the
?best? parser
proves from 95.5% for the original first-stage parser,
to 96.4% for our final model. We do not show it here,
but if we self-train using first-stage one-best, there is
no change in oracle rate.
The first-stage parser also becomes more ?deci-
sive.? The average (geometric mean) of log2(Pr(1-
best) / Pr(50th-best)) (i.e. the ratios between the
probabilities in log space) increases from 11.959 for
the baseline parser, to 14.104 for the final parser. We
have seen earlier that this ?confidence? is deserved,
as the first-stage one-best is so much better.
5.2 Sentence-level Analysis
To this point we have looked at bulk properties of the
data fed to the reranker. It has higher one best and
50-best-oracle rates, and the probabilities are more
skewed (the higher probabilities get higher, the lows
get lower). We now look at sentence-level proper-
ties. In particular, we analyzed the parsers? behav-
ior on 5,039 sentences in sections 1, 22 and 24 of
the Penn treebank. Specifically, we classified each
sentence into one of three classes: those where the
self-trained parser?s f -score increased relative to the
baseline parser?s f -score, those where the f -score
remained the same, and those where the self-trained
parser?s f -score decreased relative to the baseline
parser?s f -score. We analyzed the distribution of
sentences into these classes with respect to four fac-
tors: sentence length, the number of unknown words
(i.e., words not appearing in sections 2?21 of the
Penn treebank) in the sentence, the number of coor-
dinating conjunctions (CC) in the sentence, and the
number of prepositions (IN) in the sentence. The
distributions of classes (better, worse, no change)
with respect to each of these factors individually are
graphed in Figures 2 to 5.
Figure 2 shows how the self-training affects f -
score as a function of sentence length. The top line
0 10 20 30 40 50 60
20
40
60
80
10
0
Sentence length
N
um
be
r o
f s
en
te
nc
es
 (s
mo
oth
ed
)
Better
No change
Worse
Figure 2: How self-training improves performance
as a function of sentence length
shows that the f -score of most sentences remain un-
changed. The middle line is the number of sentences
that improved their f -score, and the bottom are those
which got worse. So, for example, for sentences of
length 30, about 80 were unchanged, 25 improved,
and 22 worsened. It seems clear that there is no
improvement for either very short sentences, or for
very long ones. (For long ones the graph is hard
to read. We show a regression analysis later in this
section that confirms this statement.) While we did
not predict this effect, in retrospect it seems reason-
able. The parser was already doing very well on
short sentences. The very long ones are hopeless,
and the middle ones are just right. We call this the
Goldilocks effect.
As for the other three of these graphs, their stories
are by no means clear. Figure 3 seems to indicate
that the number of unknown words in the sentence
does not predict that the reranker will help. Figure 4
might indicate that the self-training parser improves
prepositional-phrase attachment, but the graph looks
suspiciously like that for sentence length, so the im-
provements might just be due to the Goldilocks ef-
fect. Finally, the improvement in Figure 5 is hard to
judge.
To get a better handle on these effects we did a
factor analysis. The factors we consider are number
of CCs, INs, and unknowns, plus sentence length.
As Figure 2 makes clear, the relative performance
of the self-trained and baseline parsers does not
156
0 1 2 3 4 5
0
50
0
10
00
15
00
20
00
Unknown words
N
um
be
r o
f s
en
te
nc
es
Better
No change
Worse
Figure 3: How self-training improves performance
as a function of number of unknown words
Estimate Pr(> 0)
(Intercept) -0.25328 0.3649
BinnedLength(10,20] 0.02901 0.9228
BinnedLength(20,30] 0.45556 0.1201
BinnedLength(30,40] 0.40206 0.1808
BinnedLength(40,50] 0.26585 0.4084
BinnedLength(50,200] -0.06507 0.8671
CCs 0.12333 0.0541
Table 5: Factor analysis for the question: does the
self-trained parser improve the parse with the high-
est probability
vary linearly with sentence length, so we introduced
binned sentence length (with each bin of length 10)
as a factor.
Because the self-trained and baseline parsers pro-
duced equivalent output on 3,346 (66%) of the sen-
tences, we restricted attention to the 1,693 sentences
on which the self-trained and baseline parsers? f -
scores differ. We asked the program to consider the
following factors: binned sentence length, number
of PPs, number of unknown words, and number of
CCs. The results are shown in Table 5. The factor
analysis is trying to model the log odds as a sum of
linearly weighted factors. I.e,
log(P (1|x)/(1 ? P (1|x))) = ?0 +
m
?
j=1
?jfj(x)
In Table 5 the first column gives the name of the fac-
0 2 4 6 8 10
20
0
40
0
60
0
Number of INs
N
um
be
r o
f s
en
te
nc
es
Better
No change
Worse
Figure 4: How self-training improves performance
as a function of number of prepositions
tor. The second the change in the log-odds resulting
from this factor being present (in the case of CCs
and INs, multiplied by the number of them) and the
last column is the probability that this factor is really
non-zero.
Note that there is no row for either PPs or un-
known words. This is because we also asked the pro-
gram to do a model search using the Akaike Infor-
mation Criterion (AIC) over all single and pairwise
factors. The model it chooses predicts that the self-
trained parser is likely produce a better parse than
the baseline only for sentences of length 20?40 or
sentences containing several CCs. It did not include
the number of unknown words and the number of
INs as factors because they did not receive a weight
significantly different from zero, and the AIC model
search dropped them as factors from the model.
In other words, the self-trained parser is more
likely to be correct for sentences of length 20?
40 and as the number of CCs in the sentence in-
creases. The self-trained parser does not improve
prepositional-phrase attachment or the handling of
unknown words.
This result is mildly perplexing. It is fair to say
that neither we, nor anyone we talked to, thought
conjunction handling would be improved. Conjunc-
tions are about the hardest things in parsing, and we
have no grip on exactly what it takes to help parse
them. Conversely, everyone expected improvements
on unknown words, as the self-training should dras-
157
0 1 2 3 4 5
0
50
0
10
00
15
00
20
00
Number of CCs
N
um
be
r o
f s
en
te
nc
es
Better
No change
Worse
Figure 5: How self-training improves performance
as a function of number of conjunctions
tically reduce the number of them. It is also the case
that we thought PP attachment might be improved
because of the increased coverage of preposition-
noun and preposition-verb combinations that work
such as (Hindle and Rooth, 1993) show to be so im-
portant.
Currently, our best conjecture is that unknowns
are not improved because the words that are un-
known in the WSJ are not significantly represented
in the LA Times we used for self-training. CCs
are difficult for parsers because each conjunct has
only one secure boundary. This is particularly the
case with longer conjunctions, those of VPs and Ss.
One thing we know is that self-training always im-
proves performance of the parsing model when used
as a language model. We think CC improvement is
connected with this fact and our earlier point that
the probabilities of the 50-best parses are becoming
more skewed. In essence the model is learning, in
general, what VPs and Ss look like so it is becom-
ing easier to pull them out of the stew surrounding
the conjunct. Conversely, language modeling has
comparatively less reason to help PP attachment. As
long as the parser is doing it consistently, attaching
the PP either way will work almost as well.
6 Conclusion
Contrary to received wisdom, self-training can im-
prove parsing. In particular we have achieved an ab-
solute improvement of 0.8% over the baseline per-
formance. Together with a 0.3% improvement due
to superior reranking features, this is a 1.1% im-
provement over the previous best parser results for
section 23 of the Penn Treebank (from 91.0% to
92.1%). This corresponds to a 12% error reduc-
tion assuming that a 100% performance is possible,
which it is not. The preponderance of evidence sug-
gests that it is somehow the reranking aspect of the
parser that makes this possible, but given no idea of
why this should be, so we reserve final judgement
on this matter.
Also contrary to expectations, the error analy-
sis suggests that there has been no improvement in
either the handing of unknown words, nor prepo-
sitional phrases. Rather, there is a general im-
provement in intermediate-length sentences (20-50
words), but no improvement at the extremes: a phe-
nomenon we call the Goldilocks effect. The only
specific syntactic phenomenon that seems to be af-
fected is conjunctions. However, this is good news
since conjunctions have long been considered the
hardest of parsing problems.
There are many ways in which this research
should be continued. First, the error analysis needs
to be improved. Our tentative guess for why sen-
tences with unknown words failed to improve should
be verified or disproven. Second, there are many
other ways to use self-trained information in pars-
ing. Indeed, the current research was undertaken
as the control experiment in a program to try much
more complicated methods. We still have them
to try: restricting consideration to more accurately
parsed sentences as training data (sentence selec-
tion), trying to learn grammatical generalizations di-
rectly rather than simply including the data for train-
ing, etc.
Next there is the question of practicality. In terms
of speed, once the data is loaded, the new parser is
pretty much the same speed as the old ? just un-
der a second a sentence on average for treebank sen-
tences. However, the memory requirements are lar-
gish, about half a gigabyte just to store the data. We
are making our current best self-trained parser avail-
able3 as machines with a gigabyte or more of RAM
are becoming commonplace. Nevertheless, it would
be interesting to know if the data can be pruned to
3ftp://ftp.cs.brown.edu/pub/nlparser
158
make the entire system less bulky.
Finally, there is also the nature of the self-trained
data themselves. The data we use are from the LA
Times. Those of us in parsing have learned to expect
significant decreases in parsing accuracy even when
moving the short distance from LA Times to Wall
Street Journal. This seemingly has not occurred.
Does this mean that the reranking parser somehow
overcomes at least small genre differences? On this
point, we have some pilot experiments that show
great promise.
Acknowledgments
This work was supported by NSF grants LIS9720368, and
IIS0095940, and DARPA GALE contract HR0011-06-2-0001.
We would like to thank Michael Collins, Brian Roark, James
Henderson, Miles Osborne, and the BLLIP team for their com-
ments.
References
Michiel Bacchiani, Michael Riley, Brian Roark, and
Richard Sproat. 2006. MAP adaptation of stochas-
tic grammars. Computer Speech and Language,
20(1):41?68.
Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. In Proceed-
ings of the 11th Annual Conference on Computational
Learning Theory (COLT-98).
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and MaxEnt discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics (ACL?05),
pages 173?180, Ann Arbor, Michigan, June. Associa-
tion for Computational Linguistics.
Eugene Charniak. 1997. Statistical parsing with a
context-free grammar and word statistics. In Proceed-
ings of the Fourteenth National Conference on Artifi-
cial Intelligence, Menlo Park. AAAI Press/MIT Press.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In 1st Annual Meeting of the NAACL.
Stanley F. Chen and Joshua Goodman. 1996. An empir-
ical study of smoothing techniques for language mod-
eling. In Arivind Joshi and Martha Palmer, editors,
Proceedings of the Thirty-Fourth Annual Meeting of
the Association for Computational Linguistics.
Stephen Clark, James Curran, and Miles Osborne. 2003.
Bootstrapping POS-taggers using unlabelled data. In
Proceedings of CoNLL-2003.
Michael Collins. 2000. Discriminative reranking for nat-
ural language parsing. In Machine Learning: Pro-
ceedings of the 17th International Conference (ICML
2000), pages 175?182, Stanford, California.
Sanjoy Dasgupta, M.L. Littman, and D. McAllester.
2001. PAC generalization bounds for co-training. In
Advances in Neural Information Processing Systems
(NIPS), 2001.
Daniel Gildea. 2001. Corpus variation and parser perfor-
mance. In Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP).
David Graff. 1995. North American News Text Corpus.
Linguistic Data Consortium. LDC95T21.
James Henderson. 2004. Discriminative training of a
neural network statistical parser. In Proc. 42nd Meet-
ing of Association for Computational Linguistics (ACL
2004), Barcelona, Spain.
Donald Hindle and Mats Rooth. 1993. Structural ambi-
guity and lexical relations. Computational Linguistics,
19(1):103?120.
Liang Huang and David Chang. 2005. Better k-best pars-
ing. Technical Report MS-CIS-05-08, Department of
Computer Science, University of Pennsylvania.
Victor M. Jimenez and Andres Marzal. 2000. Computa-
tion of the n best parse trees for weighted and stochas-
tic context-free grammars. In Proceedings of the Joint
IAPR International Workshops on Advances in Pattern
Recognition. Springer LNCS 1876.
Mark Johnson, Stuart Geman, Stephen Canon, Zhiyi Chi,
and Stefan Riezler. 1999. Estimators for stochas-
tic ?unification-based? grammars. In The Proceedings
of the 37th Annual Conference of the Association for
Computational Linguistics, pages 535?541, San Fran-
cisco. Morgan Kaufmann.
Dan Klein and Christopher Manning. 2002. A genera-
tive constituent-context model for improved grammar
induction. In Proceedings of the 40th Annual Meeting
of the ACL.
Michell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313?330.
Anoop Sarkar. 2001. Applying cotraining methods to
statistical parsing. In Proceedings of the 2001 NAACL
Conference.
Mark Steedman, Miles Osborne, Anoop Sarkar, Stephen
Clark, Rebecca Hwa, Julia Hockenmaier, Paul Ruhlen,
Steven Baker, and Jeremiah Crim. 2003. Bootstrap-
ping statistical parsers from small datasets. In Pro-
ceedings of EACL 03.
159
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 101?109,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Improving Unsupervised Dependency Parsing
with Richer Contexts and Smoothing
William P. Headden III, Mark Johnson, David McClosky
Brown Laboratory for Linguistic Information Processing (BLLIP)
Brown University
Providence, RI 02912
{headdenw,mj,dmcc}@cs.brown.edu
Abstract
Unsupervised grammar induction models tend
to employ relatively simple models of syntax
when compared to their supervised counter-
parts. Traditionally, the unsupervised mod-
els have been kept simple due to tractabil-
ity and data sparsity concerns. In this paper,
we introduce basic valence frames and lexi-
cal information into an unsupervised depen-
dency grammar inducer and show how this
additional information can be leveraged via
smoothing. Our model produces state-of-the-
art results on the task of unsupervised gram-
mar induction, improving over the best previ-
ous work by almost 10 percentage points.
1 Introduction
The last decade has seen great strides in statisti-
cal natural language parsing. Supervised and semi-
supervised methods now provide highly accurate
parsers for a number of languages, but require train-
ing from corpora hand-annotated with parse trees.
Unfortunately, manually annotating corpora with
parse trees is expensive and time consuming so for
languages and domains with minimal resources it is
valuable to study methods for parsing without re-
quiring annotated sentences.
In this work, we focus on unsupervised depen-
dency parsing. Our goal is to produce a directed
graph of dependency relations (e.g. Figure 1) where
each edge indicates a head-argument relation. Since
the task is unsupervised, we are not given any ex-
amples of correct dependency graphs and only take
words and their parts of speech as input. Most
of the recent work in this area (Smith, 2006; Co-
hen et al, 2008) has focused on variants of the
The big dog barks
Figure 1: Example dependency parse.
Dependency Model with Valence (DMV) by Klein
and Manning (2004). DMV was the first unsu-
pervised dependency grammar induction system to
achieve accuracy above a right-branching baseline.
However, DMV is not able to capture some of the
more complex aspects of language. Borrowing some
ideas from the supervised parsing literature, we
present two new models: Extended Valence Gram-
mar (EVG) and its lexicalized extension (L-EVG).
The primary difference between EVG and DMV is
that DMV uses valence information to determine the
number of arguments a head takes but not their cat-
egories. In contrast, EVG allows different distri-
butions over arguments for different valence slots.
L-EVG extends EVG by conditioning on lexical in-
formation as well. This allows L-EVG to potentially
capture subcategorizations. The downside of adding
additional conditioning events is that we introduce
data sparsity problems. Incorporating more valence
and lexical information increases the number of pa-
rameters to estimate. A common solution to data
sparsity in supervised parsing is to add smoothing.
We show that smoothing can be employed in an un-
supervised fashion as well, and show that mixing
DMV, EVG, and L-EVG together produces state-of-
the-art results on this task. To our knowledge, this is
the first time that grammars with differing levels of
detail have been successfully combined for unsuper-
vised dependency parsing.
A brief overview of the paper follows. In Section
2, we discuss the relevant background. Section 3
presents how we will extend DMV with additional
101
features. We describe smoothing in an unsupervised
context in Section 4. In Section 5, we discuss search
issues. We present our experiments in Section 6 and
conclude in Section 7.
2 Background
In this paper, the observed variables will be a corpus
of n sentences of text s = s1 . . . sn, and for each
word sij an associated part-of-speech ?ij . We denote
the set of all words as Vw and the set of all parts-of-
speech as V? . The hidden variables are parse trees
t = t1 . . . tn and parameters ?? which specify a dis-
tribution over t. A dependency tree ti is a directed
acyclic graph whose nodes are the words in si. The
graph has a single incoming edge for each word in
each sentence, except one called the root of ti. An
edge from word i to word j means that word j is
an argument of word i or alternatively, word i is the
head of word j. Note that each word token may be
the argument of at most one head, but a head may
have several arguments.
If parse tree ti can be drawn on a plane above the
sentence with no crossing edges, it is called projec-
tive. Otherwise it is nonprojective. As in previous
work, we restrict ourselves to projective dependency
trees. The dependency models in this paper will be
formulated as a particular kind of Probabilistic Con-
text Free Grammar (PCFG), described below.
2.1 Tied Probabilistic Context Free Grammars
In order to perform smoothing, we will find useful a
class of PCFGs in which the probabilities of certain
rules are required to be the same. This will allow
us to make independence assumptions for smooth-
ing purposes without losing information, by giving
analogous rules the same probability.
Let G = (N ,T , S,R, ?) be a Probabilistic Con-
text Free Grammar with nonterminal symbols N ,
terminal symbols T , start symbol S ? N , set of
productions R of the form N ? ?, N ? N , ? ?
(N ? T )?. Let RN indicate the subset of R whose
left-hand sides are N . ? is a vector of length |R|, in-
dexed by productions N ? ? ? R. ?N?? specifies
the probability that N rewrites to ?. We will let ?N
indicate the subvector of ? corresponding to RN .
A tied PCFG constrains a PCFG G with a tying
relation, which is an equivalence relation over rules
that satisfies the following properties:
1. Tied rules have the same probability.
2. Rules expanding the same nonterminal are
never tied.
3. If N1 ? ?1 and N2 ? ?2 are tied then the ty-
ing relation defines a one-to-one mapping be-
tween rules in RN1 and RN2 , and we say that
N1 and N2 are tied nonterminals.
As we see below, we can estimate tied PCFGs using
standard techniques. Clearly, the tying relation also
defines an equivalence class over nonterminals. The
tying relation allows us to formulate the distribu-
tions over trees in terms of rule equivalence classes
and nonterminal equivalence classes. Suppose R? is
the set of rule equivalence classes and N? is the set
of nonterminal equivalence classes. Since all rules
in an equivalence class r? have the same probability
(condition 1), and since all the nonterminals in an
equivalence class N? ? N? have the same distribu-
tion over rule equivalence classes (condition 1 and
3), we can define the set of rule equivalence classes
R?N? associated with a nonterminal equivalence class
N? , and a vector ?? of probabilities, indexed by rule
equivalence classes r? ? R? . ??N? refers to the sub-
vector of ?? associated with nonterminal equivalence
class N? , indexed by r? ? R?N? . Since rules in the
same equivalence class have the same probability,
we have that for each r ? r?, ?r = ??r?.
Let f(t, r) denote the number of times rule r ap-
pears in tree t, and let f(t, r?) = ?r?r? f(t, r). We
see that the complete data likelihood is
P (s, t|?) = ?
r??R?
?
r?r?
?f(t,r)r =
?
r??R?
??f(t,r?)r?
That is, the likelihood is a product of multinomi-
als, one for each nonterminal equivalence class, and
there are no constraints placed on the parameters of
these multinomials besides being positive and sum-
ming to one. This means that all the standard es-
timation methods (e.g. Expectation Maximization,
Variational Bayes) extend directly to tied PCFGs.
Maximum likelihood estimation provides a point
estimate of ??. However, often we want to incorpo-
rate information about ?? by modeling its prior distri-
bution. As a prior, for each N? ? N? we will specify a
102
Dirichlet distribution over ??N? with hyperparameters
?N? . The Dirichlet has the density function:
P (??N? |?N? ) =
?(?r??R?N? ?r?)?
r??R?N? ?(?r?)
?
r??R?N?
???r??1r? ,
Thus the prior over ?? is a product of Dirichlets,which
is conjugate to the PCFG likelihood function (John-
son et al, 2007). That is, the posterior P (??|s, t, ?)
is also a product of Dirichlets, also factoring into a
Dirichlet for each nonterminal N? , where the param-
eters ?r? are augmented by the number of times rule
r? is observed in tree t:
P (??|s, t, ?) ? P (s, t|??)P (??|?)
?
?
r??R?
??f(t,r?)+?r??1r?
We can see that ?r? acts as a pseudocount of the num-
ber of times r? is observed prior to t.
To make use of this prior, we use the Variational
Bayes (VB) technique for PCFGs with Dirichlet Pri-
ors presented by Kurihara and Sato (2004). VB es-
timates a distribution over ??. In contrast, Expec-
tation Maximization estimates merely a point esti-
mate of ??. In VB, one estimates Q(t, ??), called
the variational distribution, which approximates the
posterior distribution P (t, ??|s, ?) by minimizing the
KL divergence of P from Q. Minimizing the KL
divergence, it turns out, is equivalent to maximiz-
ing a lower bound F of the log marginal likelihood
log P (s|?).
log P (s|?) ? ?
t
?
??
Q(t, ??) log P (s, t, ??|?)Q(t, ??) = F
The negative of the lower bound, ?F , is sometimes
called the free energy.
As is typical in variational approaches, Kuri-
hara and Sato (2004) make certain independence as-
sumptions about the hidden variables in the vari-
ational posterior, which will make estimating it
simpler. It factors Q(t, ??) = Q(t)Q(??) =?n
i=1 Qi(ti)
?
N??N? Q(??N? ). The goal is to recover
Q(??), the estimate of the posterior distribution over
parameters and Q(t), the estimate of the posterior
distribution over trees. Finding a local maximum of
F is done via an alternating maximization of Q(??)
and Q(t). Kurihara and Sato (2004) show that each
Q(??N? ) is a Dirichlet distribution with parameters
??r = ?r + EQ(t)f(t, r).
2.2 Split-head Bilexical CFGs
In the sections that follow, we frame various de-
pendency models as a particular variety of CFGs
known as split-head bilexical CFGs (Eisner and
Satta, 1999). These allow us to use the fast Eisner
and Satta (1999) parsing algorithm to compute the
expectations required by VB in O(m3) time (Eis-
ner and Blatz, 2007; Johnson, 2007) where m is the
length of the sentence.1
In the split-head bilexical CFG framework, each
nonterminal in the grammar is annotated with a ter-
minal symbol. For dependency grammars, these
annotations correspond to words and/or parts-of-
speech. Additionally, split-head bilexical CFGs re-
quire that each word sij in sentence si is represented
in a split form by two terminals called its left part
sijL and right part sijR. The set of these parts con-
stitutes the terminal symbols of the grammar. This
split-head property relates to a particular type of de-
pendency grammar in which the left and right depen-
dents of a head are generated independently. Note
that like CFGs, split-head bilexical CFGs can be
made probabilistic.
2.3 Dependency Model with Valence
The most successful recent work on dependency
induction has focused on the Dependency Model
with Valence (DMV) by Klein and Manning (2004).
DMV is a generative model in which the head of
the sentence is generated and then each head recur-
sively generates its left and right dependents. The
arguments of head H in direction d are generated
by repeatedly deciding whether to generate another
new argument or to stop and then generating the
argument if required. The probability of deciding
whether to generate another argument is conditioned
on H , d and whether this would be the first argument
(this is the sense in which it models valence). When
DMV generates an argument, the part-of-speech of
that argument A is generated given H and d.
1Efficiently parsable versions of split-head bilexical CFGs
for the models described in this paper can be derived using the
fold-unfold grammar transform (Eisner and Blatz, 2007; John-
son, 2007).
103
Rule Description
S ? YH Select H as root
YH ? LH RH Move to split-head representation
LH ? HL STOP | dir = L, head = H,val = 0
LH ? L1H CONT | dir = L, head = H, val = 0
L?H ? HL STOP | dir = L, head = H,val = 1
L?H ? L1H CONT | dir = L, head = H, val = 1
L1H ? YA L?H Arg A | dir = L, head = H
Figure 2: Rule schema for DMV. For brevity, we omit
the portion of the grammar that handles the right argu-
ments since they are symmetric to the left (all rules are
the same except for the attachment rule where the RHS is
reversed). val ? {0, 1} indicates whether we have made
any attachments.
The grammar schema for this model is shown in
Figure 2. The first rule generates the root of the sen-
tence. Note that these rules are for ?H,A ? V? so
there is an instance of the first schema rule for each
part-of-speech. YH splits words into their left and
right components. LH encodes the stopping deci-
sion given that we have not generated any arguments
so far. L?H encodes the same decision after generat-
ing one or more arguments. L1H represents the distri-
bution over left attachments. To extract dependency
relations from these parse trees, we scan for attach-
ment rules (e.g., L1H ? YA L?H) and record that
A depends on H . The schema omits the rules for
right arguments since they are symmetric. We show
a parse of ?The big dog barks? in Figure 3.2
Much of the extensions to this work have fo-
cused on estimation procedures. Klein and Manning
(2004) use Expectation Maximization to estimate
the model parameters. Smith and Eisner (2005) and
Smith (2006) investigate using Contrastive Estima-
tion to estimate DMV. Contrastive Estimation max-
imizes the conditional probability of the observed
sentences given a neighborhood of similar unseen
sequences. The results of this approach vary widely
based on regularization and neighborhood, but often
outperforms EM.
2Note that our examples use words as leaf nodes but in our
unlexicalized models, the leaf nodes are in fact parts-of-speech.
S
Ybarks
Lbarks
L1barks
Ydog
Ldog
L1dog
YThe
LThe
TheL
RThe
TheR
L?dog
L1dog
Ybig
Lbig
bigL
Rbig
bigR
L?dog
dogL
Rdog
dogR
L?barks
barksL
Rbarks
barksR
Figure 3: DMV split-head bilexical CFG parse of ?The
big dog barks.?
Smith (2006) also investigates two techniques for
maximizing likelihood while incorporating the lo-
cality bias encoded in the harmonic initializer for
DMV. One technique, skewed deterministic anneal-
ing, ameliorates the local maximum problem by flat-
tening the likelihood and adding a bias towards the
Klein and Manning initializer, which is decreased
during learning. The second technique is structural
annealing (Smith and Eisner, 2006; Smith, 2006)
which penalizes long dependencies initially, grad-
ually weakening the penalty during estimation. If
hand-annotated dependencies on a held-out set are
available for parameter selection, this performs far
better than EM; however, performing parameter se-
lection on a held-out set without the use of gold de-
pendencies does not perform as well.
Cohen et al (2008) investigate using Bayesian
Priors with DMV. The two priors they use are the
Dirichlet (which we use here) and the Logistic Nor-
mal prior, which allows the model to capture correla-
tions between different distributions. They initialize
using the harmonic initializer of Klein and Manning
(2004). They find that the Logistic Normal distri-
bution performs much better than the Dirichlet with
this initialization scheme.
Cohen and Smith (2009), investigate (concur-
104
Rule Description
S ? YH Select H as root
YH ? LH RH Move to split-head representation
LH ? HL STOP | dir = L, head = H,val = 0
LH ? L?H CONT | dir = L, head = H, val = 0
L?H ? L1H STOP | dir = L, head = H,val = 1
L?H ? L2H CONT | dir = L, head = H, val = 1
L2H ? YA L?H Arg A | dir = L, head = H,val = 1
L1H ? YA HL Arg A | dir = L, head = H,val = 0
Figure 4: Extended Valence Grammar schema. As be-
fore, we omit rules involving the right parts of words. In
this case, val ? {0, 1} indicates whether we are generat-
ing the nearest argument (0) or not (1).
rently with our work) an extension of this, the
Shared Logistic Normal prior, which allows differ-
ent PCFG rule distributions to share components.
They use this machinery to investigate smoothing
the attachment distributions for (nouns/verbs), and
for learning using multiple languages.
3 Enriched Contexts
DMV models the distribution over arguments iden-
tically without regard to their order. Instead, we
propose to distinguish the distribution over the argu-
ment nearest the head from the distribution of sub-
sequent arguments. 3
Consider the following changes to the DMV
grammar (results shown in Figure 4). First, we will
introduce the rule L2H ? YA L?H to denote the deci-
sion of what argument to generate for positions not
nearest to the head. Next, instead of having L?H ex-
pand to HL or L1H , we will expand it to L1H (attach
to nearest argument and stop) or L2H (attach to non-
nearest argument and continue). We call this the Ex-
tended Valence Grammar (EVG).
As a concrete example, consider the phrase ?the
big hungry dog? (Figure 5). We would expect that
distribution over the nearest left argument for ?dog?
to be different than farther left arguments. The fig-
3McClosky (2008) explores this idea further in an un-
smoothed grammar.
.
.
.
Ldog
L1dog
YThe
TheL TheR
L?dog
L1dog
Ybig
bigL bigR
L?dog
dogL
.
.
.
Ldog
L?dog
L2dog
YThe
TheL TheR
L?dog
L1dog
Ybig
bigL bigR
dogL
Figure 5: An example of moving from DMV to EVG
for a fragment of ?The big dog.? Boxed nodes indicate
changes. The key difference is that EVG distinguishes
between the distributions over the argument nearest the
head (big) from arguments farther away (The).
ure shows that EVG allows these two distributions to
be different (nonterminals L2dog and L1dog) whereas
DMV forces them to be equivalent (both use L1dog as
the nonterminal).
3.1 Lexicalization
All of the probabilistic models discussed thus far
have incorporated only part-of-speech information
(see Footnote 2). In supervised parsing of both de-
pendencies and constituency, lexical information is
critical (Collins, 1999). We incorporate lexical in-
formation into EVG (henceforth L-EVG) by extend-
ing the distributions over argument parts-of-speech
A to condition on the head word h in addition to the
head part-of-speech H , direction d and argument po-
sition v. The argument word a distribution is merely
conditioned on part-of-speech A; we leave refining
this model to future work.
In order to incorporate lexicalization, we extend
the EVG CFG to allow the nonterminals to be anno-
tated with both the word and part-of-speech of the
head. We first remove the old rules YH ? LH RH
for each H ? V? . Then we mark each nonter-
minal which is annotated with a part-of-speech as
also annotated with its head, with a single excep-
tion: YH . We add a new nonterminal YH,h for each
H ? V? , h ? Vw, and the rules YH ? YH,h and
YH,h ? LH,h RH,h. The rule YH ? YH,h cor-
responds to selecting the word, given its part-of-
speech.
105
4 Smoothing
In supervised estimation one common smoothing
technique is linear interpolation, (Jelinek, 1997).
This section explains how linear interpolation can
be represented using a PCFG with tied rule proba-
bilities, and how one might estimate smoothing pa-
rameters in an unsupervised framework.
In many probabilistic models it is common to esti-
mate the distribution of some event x conditioned on
some set of context information P (x|N(1) . . . N(k))
by smoothing it with less complicated condi-
tional distributions. Using linear interpolation
we model P (x|N(1) . . . N(k)) as a weighted aver-
age of two distributions ?1P1(x|N(1), . . . , N(k)) +
?2P2(x|N(1), . . . , N(k?1)), where the distribution
P2 makes an independence assumption by dropping
the conditioning event N(k).
In a PCFG a nonterminal N can encode a collec-
tion of conditioning events N(1) . . . N(k), and ?N de-
termines a distribution conditioned on N(1) . . . N(k)
over events represented by the rules r ? RN . For
example, in EVG the nonterminal L1NN encodes
three separate pieces of conditioning information:
the direction d = left , the head part-of-speech
H = NN , and the argument position v = 0;
?L1NN?YJJ NNL represents the probability of gener-
ating JJ as the first left argument of NN . Sup-
pose in EVG we are interested in smoothing P (A |
d,H, v) with a component that excludes the head
conditioning event. Using linear interpolation, this
would be:
P (A | d,H, v) = ?1P1(A | d,H, v)+?2P2(A | d, v)
We will estimate PCFG rules with linearly interpo-
lated probabilities by creating a tied PCFG which
is extended by adding rules that select between the
main distribution P1 and the backoff distribution P2,
and also rules that correspond to draws from those
distributions. We will make use of tied rule proba-
bilities to make the independence assumption in the
backoff distribution.
We still use the original grammar to parse the sen-
tence. However, we estimate the parameters in the
extended grammar and then translate them back into
the original grammar for parsing.
More formally, suppose B ? N is a set of non-
terminals (called the backoff set) with conditioning
events N(1) . . . N(k?1) in common (differing in a
conditioning event N(k)), and with rule sets of the
same cardinality. If G is our model?s PCFG, we can
define a new tied PCFG G? = (N ?,T , S,R?, ?),
where N ? = N ? {N b? | N ? B, ? ? {1, 2}},
meaning for each nonterminal N in the backoff
set we add two nonterminals N b1 , N b2 represent-
ing each distribution P1 and P2. The new rule
set R? = (?N?N ?R?N ) where for all N ? B
rule set R?N =
{
N ? N b? | ? ? {1, 2}}, mean-
ing at N in G? we decide which distribution P1, P2
to use; and for N ? B and ? ? {1, 2} ,
R?Nb? =
{
N b? ? ? | N ? ? ? RN
}
indicating a
draw from distribution P?. For nonterminals N 6? B,
R?N = RN . Finally, for each N,M ? B we
specify a tying relation between the rules in R?Nb2
and R?Mb2 , grouping together analogous rules. This
has the effect of making an independence assump-
tion about P2, namely that it ignores the condition-
ing event N(k), drawing from a common distribution
each time a nonterminal N b2 is rewritten.
For example, in EVG to smooth P (A = DT |
d = left ,H = NN , v = 0) with P2(A = DT |
d = left , v = 0) we define the backoff set to
be
{
L1H | H ? V?
}
. In the extended grammar we
define the tying relation to form rule equivalence
classes by the argument they generate, i.e. for each
argument A ? V? , we have a rule equivalence class{
L1b2H ? YA HL | H ? V?
}
.
We can see that in grammar G? each N ? B even-
tually ends up rewriting to one of N ?s expansions ?
in G. There are two indirect paths, one through N b1
and one through N b2 . Thus this defines the proba-
bility of N ? ? in G, ?N??, as the probability of
rewriting N as ? in G? via N b1 and N b2 . That is:
?N?? = ?N?Nb1?Nb1?? + ?N?Nb2?Nb2??
The example in Figure 6 shows the probability that
L1dog rewrites to Ybig dogL in grammar G.
Typically when smoothing we need to incorporate
the prior knowledge that conditioning events that
have been seen fewer times should be more strongly
smoothed. We accomplish this by setting the Dirich-
let hyperparameters for each N ? N b1 , N ? N b2
decision to (K, 2K), where K = |RNb1 | is the num-
ber of rewrite rules for A. This ensures that the
model will only start to ignore the backoff distribu-
106
PG
0
B
B
@
L1dog
Ybig dogL
1
C
C
A
= PG?
0
B
B
B
B
B
B
B
@
L1dog
L1b1dog
Ybig dogL
1
C
C
C
C
C
C
C
A
+ PG?
0
B
B
B
B
B
B
B
@
L1dog
L1b2dog
Ybig dogL
1
C
C
C
C
C
C
C
A
Figure 6: Using linear interpolation to smooth L1dog ?
Ybig dogL: The first component represents the distri-
bution fully conditioned on head dog, while the second
component represents the distribution ignoring the head
conditioning event. This later is accomplished by tying
the rule L1b2dog ? Ybig dogL to, for instance, L1b2cat ?
Ybig catL, L1b2fish ? Ybig fishL etc.
tion after having seen a sufficiently large number of
training examples. 4
4.1 Smoothed Dependency Models
Our first experiments examine smoothing the dis-
tributions over an argument in the DMV and EVG
models. In DMV we smooth the probability of argu-
ment A given head part-of-speech H and direction d
with a distribution that ignores H . In EVG, which
conditions on H , d and argument position v we back
off two ways. The first is to ignore v and use back-
off conditioning event H, d. This yields a backoff
distribution with the same conditioning information
as the argument distribution from DMV. We call this
EVG smoothed-skip-val.
The second possibility is to have the backoff
distribution ignore the head part-of-speech H and
use backoff conditioning event v, d. This assumes
that arguments share a common distribution across
heads. We call this EVG smoothed-skip-head. As
we see below, backing off by ignoring the part-of-
speech of the head H worked better than ignoring
the argument position v.
For L-EVG we smooth the argument part-of-
speech distribution (conditioned on the head word)
with the unlexicalized EVG smoothed-skip-head
model.
5 Initialization and Search issues
Klein and Manning (2004) strongly emphasize the
importance of smart initialization in getting good
performance from DMV. The likelihood function is
full of local maxima and different initial parameter
values yield vastly different quality solutions. They
offer what they call a ?harmonic initializer? which
4We set the other Dirichlet hyperparameters to 1.
initializes the attachment probabilities to favor ar-
guments that appear more closely in the data. This
starts EM in a state preferring shorter attachments.
Since our goal is to expand the model to incor-
porate lexical information, we want an initializa-
tion scheme which does not depend on the details
of DMV. The method we use is to create M sets of
B random initial settings and to run VB some small
number of iterations (40 in all our experiments) for
each initial setting. For each of the M sets, the
model with the best free energy of the B runs is
then run out until convergence (as measured by like-
lihood of a held-out data set); the other models are
pruned away. In this paper we use B = 20 and
M = 50.
For the bth setting, we draw a random sample
from the prior ??(b). We set the initial Q(t) =
P (t|s, ??(b)) which can be calculated using the
Expectation-Maximization E-Step. Q(??) is then ini-
tialized using the standard VB M-step.
For the Lexicalized-EVG, we modify this proce-
dure slightly, by first running MB smoothed EVG
models for 40 iterations each and selecting the best
model in each cohort as before; each L-EVG dis-
tribution is initialized from its corresponding EVG
distribution. The new P (A|h,H, d, v) distributions
are set initially to their corresponding P (A|H, d, v)
values.
6 Results
We trained on the standard Penn Treebank WSJ cor-
pus (Marcus et al, 1993). Following Klein and Man-
ning (2002), sentences longer than 10 words after
removing punctuation are ignored. We refer to this
variant as WSJ10. Following Cohen et al (2008),
we train on sections 2-21, used 22 as a held-out de-
velopment corpus, and present results evaluated on
section 23. The models were all trained using Varia-
tional Bayes, and initialized as described in Section
5. To evaluate, we follow Cohen et al (2008) in us-
ing the mean of the variational posterior Dirichlets
as a point estimate ???. For the unsmoothed models
we decode by selecting the Viterbi parse given ???, or
argmaxtP (t|s, ???).
For the smoothed models we find the Viterbi parse
of the unsmoothed CFG, but use the smoothed prob-
abilities. We evaluate against the gold standard
107
Model Variant Dir. Acc.
DMV harmonic init 46.9*
DMV random init 55.7 (8.0)
DMV log normal-families 59.4*
DMV shared log normal-families 62.4?
DMV smoothed 61.2 (1.2)
EVG random init 53.3 (7.1)
EVG smoothed-skip-val 62.1 (1.9)
EVG smoothed-skip-head 65.0 (5.7)
L-EVG smoothed 68.8 (4.5)
Table 1: Directed accuracy (DA) for WSJ10, section 23.
*,? indicate results reported by Cohen et al (2008), Co-
hen and Smith (2009) respectively. Standard deviations
over 10 runs are given in parentheses
dependencies for section 23, which were extracted
from the phrase structure trees using the standard
rules by Yamada and Matsumoto (2003). We mea-
sure the percent accuracy of the directed dependency
edges. For the lexicalized model, we replaced all
words that were seen fewer than 100 times with
?UNK.? We ran each of our systems 10 times, and
report the average directed accuracy achieved. The
results are shown in Table 1. We compare to work
by Cohen et al (2008) and Cohen and Smith (2009).
Looking at Table 1, we can first of all see the
benefit of randomized initialization over the har-
monic initializer for DMV. We can also see a large
gain by adding smoothing to DMV, topping even
the logistic normal prior. The unsmoothed EVG ac-
tually performs worse than unsmoothed DMV, but
both smoothed versions improve even on smoothed
DMV. Adding lexical information (L-EVG) yields a
moderate further improvement.
As the greatest improvement comes from moving
to model EVG smoothed-skip-head, we show in Ta-
ble 2 the most probable arguments for each val, dir,
using the mean of the appropriate variational Dirich-
let. For d = right, v = 1, P (A|v, d) largely seems
to acts as a way of grouping together various verb
types, while for d = left, v = 0 the model finds
that nouns tend to act as the closest left argument.
Dir,Val Arg Prob Dir,Val Arg Prob
left, 0 NN 0.65 right, 0 NN 0.26
NNP 0.18 RB 0.23
DT 0.12 NNS 0.12
IN 0.11
left, 1 CC 0.35 right, 1 IN 0.78
RB 0.27
IN 0.18
Table 2: Most likely arguments given valence and direc-
tion, according to smoothing distributionP (arg|dir, val)
in EVG smoothed-skip-head model with lowest free en-
ergy.
7 Conclusion
We present a smoothing technique for unsupervised
PCFG estimation which allows us to explore more
sophisticated dependency grammars. Our method
combines linear interpolation with a Bayesian prior
that ensures the backoff distribution receives proba-
bility mass. Estimating the smoothed model requires
running the standard Variational Bayes on an ex-
tended PCFG. We used this technique to estimate a
series of dependency grammars which extend DMV
with additional valence and lexical information. We
found that both were helpful in learning English de-
pendency grammars. Our L-EVG model gives the
best reported accuracy to date on the WSJ10 corpus.
Future work includes using lexical information
more deeply in the model by conditioning argument
words and valence on the lexical head. We suspect
that successfully doing so will require using much
larger datasets. We would also like to explore us-
ing our smoothing technique in other models such
as HMMs. For instance, we could do unsupervised
HMM part-of-speech induction by smooth a tritag
model with a bitag model. Finally, we would like to
learn the parts-of-speech in our dependency model
from text and not rely on the gold-standard tags.
Acknowledgements
This research is based upon work supported by
National Science Foundation grants 0544127 and
0631667 and DARPA GALE contract HR0011-06-
2-0001. We thank members of BLLIP for their feed-
back.
108
References
Shay B. Cohen and Noah A. Smith. 2009. Shared lo-
gistic normal distributions for soft parameter tying in
unsupervised grammar induction. In Proceedings of
NAACL-HLT 2009.
Shay B. Cohen, Kevin Gimpel, and Noah A. Smith.
2008. Logistic normal priors for unsupervised prob-
abilistic grammar induction. In Advances in Neural
Information Processing Systems 21.
Michael Collins. 1999. Head-driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, The Uni-
versity of Pennsylvania.
Jason Eisner and John Blatz. 2007. Program transforma-
tions for optimization of parsing algorithms and other
weighted logic programs. In Proceedings of the 11th
Conference on Formal Grammar.
Jason Eisner and Giorgio Satta. 1999. Efficient pars-
ing for bilexical context-free grammars and head-
automaton grammars. In Proceedings of ACL 1999.
Frederick Jelinek. 1997. Statistical Methods for Speech
Recognition. The MIT Press, Cambridge, Mas-
sachusetts.
Mark Johnson, Thomas L. Griffiths, and Sharon Goldwa-
ter. 2007. Bayesian inference for PCFGs via Markov
chain Monte Carlo. In Proceedings of NAACL 2007.
Mark Johnson. 2007. Transforming projective bilexical
dependency grammars into efficiently-parsable CFGs
with unfold-fold. In Proceedings of ACL 2007.
Dan Klein and Christopher Manning. 2002. A genera-
tive constituent-context model for improved grammar
induction. In Proceedings of ACL 2002.
Dan Klein and Christopher Manning. 2004. Corpus-
based induction of syntactic structure: Models of de-
pendency and constituency. In Proceedings of ACL
2004, July.
Kenichi Kurihara and Taisuke Sato. 2004. An applica-
tion of the variational bayesian approach to probabilis-
tics context-free grammars. In IJCNLP 2004 Work-
shop Beyond Shallow Analyses.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313?330.
David McClosky. 2008. Modeling valence effects in un-
supervised grammar induction. Technical Report CS-
09-01, Brown University, Providence, RI, USA.
Noah A. Smith and Jason Eisner. 2005. Guiding unsu-
pervised grammar induction using contrastive estima-
tion. In International Joint Conference on Artificial
Intelligence Workshop on Grammatical Inference Ap-
plications.
Noah A. Smith and Jason Eisner. 2006. Annealing struc-
tural bias in multilingual weighted grammar induction.
In Proceedings of COLING-ACL 2006.
Noah A. Smith. 2006. Novel Estimation Methods for
Unsupervised Discovery of Latent Structure in Natural
Language Text. Ph.D. thesis, Department of Computer
Science, Johns Hopkins University.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical
dependency analysis with support vector machines. In
In Proceedings of the International Workshop on Pars-
ing Technologies.
109
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 337?344,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Reranking and Self-Training for Parser Adaptation
David McClosky, Eugene Charniak, and Mark Johnson
Brown Laboratory for Linguistic Information Processing (BLLIP)
Brown University
Providence, RI 02912
{dmcc|ec|mj}@cs.brown.edu
Abstract
Statistical parsers trained and tested on the
Penn Wall Street Journal (WSJ) treebank
have shown vast improvements over the
last 10 years. Much of this improvement,
however, is based upon an ever-increasing
number of features to be trained on (typi-
cally) the WSJ treebank data. This has led
to concern that such parsers may be too
finely tuned to this corpus at the expense
of portability to other genres. Such wor-
ries have merit. The standard ?Charniak
parser? checks in at a labeled precision-
recall f -measure of 89.7% on the Penn
WSJ test set, but only 82.9% on the test set
from the Brown treebank corpus.
This paper should allay these fears. In par-
ticular, we show that the reranking parser
described in Charniak and Johnson (2005)
improves performance of the parser on
Brown to 85.2%. Furthermore, use of the
self-training techniques described in (Mc-
Closky et al, 2006) raise this to 87.8%
(an error reduction of 28%) again with-
out any use of labeled Brown data. This
is remarkable since training the parser and
reranker on labeled Brown data achieves
only 88.4%.
1 Introduction
Modern statistical parsers require treebanks to
train their parameters, but their performance de-
clines when one parses genres more distant from
the training data?s domain. Furthermore, the tree-
banks required to train said parsers are expensive
and difficult to produce.
Naturally, one of the goals of statistical parsing
is to produce a broad-coverage parser which is rel-
atively insensitive to textual domain. But the lack
of corpora has led to a situation where much of
the current work on parsing is performed on a sin-
gle domain using training data from that domain
? the Wall Street Journal (WSJ) section of the
Penn Treebank (Marcus et al, 1993). Given the
aforementioned costs, it is unlikely that many sig-
nificant treebanks will be created for new genres.
Thus, parser adaptation attempts to leverage ex-
isting labeled data from one domain and create a
parser capable of parsing a different domain.
Unfortunately, the state of the art in parser
portability (i.e. using a parser trained on one do-
main to parse a different domain) is not good. The
?Charniak parser? has a labeled precision-recall
f -measure of 89.7% on WSJ but a lowly 82.9%
on the test set from the Brown corpus treebank.
Furthermore, the treebanked Brown data is mostly
general non-fiction and much closer to WSJ than,
e.g., medical corpora would be. Thus, most work
on parser adaptation resorts to using some labeled
in-domain data to fortify the larger quantity of out-
of-domain data.
In this paper, we present some encouraging re-
sults on parser adaptation without any in-domain
data. (Though we also present results with in-
domain data as a reference point.) In particular we
note the effects of two comparatively recent tech-
niques for parser improvement.
The first of these, parse-reranking (Collins,
2000; Charniak and Johnson, 2005) starts with a
?standard? generative parser, but uses it to gener-
ate the n-best parses rather than a single parse.
Then a reranking phase uses more detailed fea-
tures, features which would (mostly) be impossi-
ble to incorporate in the initial phase, to reorder
337
the list and pick a possibly different best parse.
At first blush one might think that gathering even
more fine-grained features from a WSJ treebank
would not help adaptation. However, we find that
reranking improves the parsers performance from
82.9% to 85.2%.
The second technique is self-training ? pars-
ing unlabeled data and adding it to the training
corpus. Recent work, (McClosky et al, 2006),
has shown that adding many millions of words
of machine parsed and reranked LA Times arti-
cles does, in fact, improve performance of the
parser on the closely related WSJ data. Here we
show that it also helps the father-afield Brown
data. Adding it improves performance yet-again,
this time from 85.2% to 87.8%, for a net error re-
duction of 28%. It is interesting to compare this to
our results for a completely Brown trained system
(i.e. one in which the first-phase parser is trained
on just Brown training data, and the second-phase
reranker is trained on Brown 50-best lists). This
system performs at a 88.4% level ? only slightly
higher than that achieved by our system with only
WSJ data.
2 Related Work
Work in parser adaptation is premised on the as-
sumption that one wants a single parser that can
handle a wide variety of domains. While this is the
goal of the majority of parsing researchers, it is not
quite universal. Sekine (1997) observes that for
parsing a specific domain, data from that domain
is most beneficial, followed by data from the same
class, data from a different class, and data from
a different domain. He also notes that different
domains have very different structures by looking
at frequent grammar productions. For these rea-
sons he takes the position that we should, instead,
simply create treebanks for a large number of do-
mains. While this is a coherent position, it is far
from the majority view.
There are many different approaches to parser
adaptation. Steedman et al (2003) apply co-
training to parser adaptation and find that co-
training can work across domains. The need to
parse biomedical literature inspires (Clegg and
Shepherd, 2005; Lease and Charniak, 2005).
Clegg and Shepherd (2005) provide an extensive
side-by-side performance analysis of several mod-
ern statistical parsers when faced with such data.
They find that techniques which combine differ-
Training Testing f -measureGildea Bacchiani
WSJ WSJ 86.4 87.0
WSJ Brown 80.6 81.1
Brown Brown 84.0 84.7
WSJ+Brown Brown 84.3 85.6
Table 1: Gildea and Bacchiani results on WSJ and
Brown test corpora using different WSJ and Brown
training sets. Gildea evaluates on sentences of
length ? 40, Bacchiani on all sentences.
ent parsers such as voting schemes and parse se-
lection can improve performance on biomedical
data. Lease and Charniak (2005) use the Charniak
parser for biomedical data and find that the use of
out-of-domain trees and in-domain vocabulary in-
formation can considerably improve performance.
However, the work which is most directly com-
parable to ours is that of (Ratnaparkhi, 1999; Hwa,
1999; Gildea, 2001; Bacchiani et al, 2006). All
of these papers look at what happens to mod-
ern WSJ-trained statistical parsers (Ratnaparkhi?s,
Collins?, Gildea?s and Roark?s, respectively) as
training data varies in size or usefulness (because
we are testing on something other than WSJ). We
concentrate particularly on the work of (Gildea,
2001; Bacchiani et al, 2006) as they provide re-
sults which are directly comparable to those pre-
sented in this paper.
Looking at Table 1, the first line shows us
the standard training and testing on WSJ ? both
parsers perform in the 86-87% range. The next
line shows what happens when parsing Brown us-
ing a WSJ-trained parser. As with the Charniak
parser, both parsers take an approximately 6% hit.
It is at this point that our work deviates from
these two papers. Lacking alternatives, both
(Gildea, 2001) and (Bacchiani et al, 2006) give
up on adapting a pure WSJ trained system, instead
looking at the issue of how much of an improve-
ment one gets over a pure Brown system by adding
WSJ data (as seen in the last two lines of Table 1).
Both systems use a ?model-merging? (Bacchiani
et al, 2006) approach. The different corpora are,
in effect, concatenated together. However, (Bac-
chiani et al, 2006) achieve a larger gain by weight-
ing the in-domain (Brown) data more heavily than
the out-of-domain WSJ data. One can imagine, for
instance, five copies of the Brown data concate-
nated with just one copy of WSJ data.
338
3 Corpora
We primarily use three corpora in this paper. Self-
training requires labeled and unlabeled data. We
assume that these sets of data must be in similar
domains (e.g. news articles) though the effective-
ness of self-training across domains is currently an
open question. Thus, we have labeled (WSJ) and
unlabeled (NANC) out-of-domain data and labeled
in-domain data (BROWN). Unfortunately, lacking
a corresponding corpus to NANC for BROWN, we
cannot perform the opposite scenario and adapt
BROWN to WSJ.
3.1 Brown
The BROWN corpus (Francis and Kuc?era, 1979)
consists of many different genres of text, intended
to approximate a ?balanced? corpus. While the
full corpus consists of fiction and nonfiction do-
mains, the sections that have been annotated in
Treebank II bracketing are primarily those con-
taining fiction. Examples of the sections annotated
include science fiction, humor, romance, mystery,
adventure, and ?popular lore.? We use the same
divisions as Bacchiani et al (2006), who base
their divisions on Gildea (2001). Each division of
the corpus consists of sentences from all available
genres. The training division consists of approx-
imately 80% of the data, while held-out develop-
ment and testing divisions each make up 10% of
the data. The treebanked sections contain approx-
imately 25,000 sentences (458,000 words).
3.2 Wall Street Journal
Our out-of-domain data is the Wall Street Journal
(WSJ) portion of the Penn Treebank (Marcus et al,
1993) which consists of about 40,000 sentences
(one million words) annotated with syntactic in-
formation. We use the standard divisions: Sec-
tions 2 through 21 are used for training, section 24
for held-out development, and section 23 for final
testing.
3.3 North American News Corpus
In addition to labeled news data, we make use
of a large quantity of unlabeled news data. The
unlabeled data is the North American News Cor-
pus, NANC (Graff, 1995), which is approximately
24 million unlabeled sentences from various news
sources. NANC contains no syntactic information
and sentence boundaries are induced by a simple
discriminative model. We also perform some basic
cleanups on NANC to ease parsing. NANC contains
news articles from various news sources including
the Wall Street Journal, though for this paper, we
only use articles from the LA Times portion.
To use the data from NANC, we use self-training
(McClosky et al, 2006). First, we take a WSJ
trained reranking parser (i.e. both the parser and
reranker are built from WSJ training data) and
parse the sentences from NANC with the 50-best
(Charniak and Johnson, 2005) parser. Next, the
50-best parses are reordered by the reranker. Fi-
nally, the 1-best parses after reranking are com-
bined with the WSJ training set to retrain the first-
stage parser.1 McClosky et al (2006) find that the
self-trained models help considerably when pars-
ing WSJ.
4 Experiments
We use the Charniak and Johnson (2005) rerank-
ing parser in our experiments. Unless mentioned
otherwise, we use the WSJ-trained reranker (as op-
posed to a BROWN-trained reranker). To evaluate,
we report bracketing f -scores.2 Parser f -scores
reported are for sentences up to 100 words long,
while reranking parser f -scores are over all sen-
tences. For simplicity and ease of comparison,
most of our evaluations are performed on the de-
velopment section of BROWN.
4.1 Adapting self-training
Our first experiment examines the performance
of the self-trained parsers. While the parsers are
created entirely from labeled WSJ data and unla-
beled NANC data, they perform extremely well on
BROWN development (Table 2). The trends are the
same as in (McClosky et al, 2006): Adding NANC
data improves parsing performance on BROWN
development considerably, improving the f -score
from 83.9% to 86.4%. As more NANC data is
added, the f -score appears to approach an asymp-
tote. The NANC data appears to help reduce data
sparsity and fill in some of the gaps in the WSJ
model. Additionally, the reranker provides fur-
ther benefit and adds an absolute 1-2% to the f -
score. The improvements appear to be orthogonal,
as our best performance is reached when we use
the reranker and add 2,500k self-trained sentences
from NANC.
1We trained a new reranker from this data as well, but it
does not seem to get significantly different performance.
2The harmonic mean of labeled precision (P) and labeled
recall (R), i.e. f = 2?P?RP+R
339
Sentences added Parser Reranking Parser
Baseline BROWN 86.4 87.4
Baseline WSJ 83.9 85.8
WSJ+50k 84.8 86.6
WSJ+250k 85.7 87.2
WSJ+500k 86.0 87.3
WSJ+750k 86.1 87.5
WSJ+1,000k 86.2 87.3
WSJ+1,500k 86.2 87.6
WSJ+2,000k 86.1 87.7
WSJ+2,500k 86.4 87.7
Table 2: Effects of adding NANC sentences to WSJ
training data on parsing performance. f -scores
for the parser with and without the WSJ reranker
are shown when evaluating on BROWN develop-
ment. For this experiment, we use the WSJ-trained
reranker.
The results are even more surprising when we
compare against a parser3 trained on the labeled
training section of the BROWN corpus, with pa-
rameters tuned against its held-out section. De-
spite seeing no in-domain data, the WSJ based
parser is able to match the BROWN based parser.
For the remainder of this paper, we will refer
to the model trained on WSJ+2,500k sentences of
NANC as our ?best WSJ+NANC? model. We also
note that this ?best? parser is different from the
?best? parser for parsing WSJ, which was trained
on WSJ with a relative weight4 of 5 and 1,750k
sentences from NANC. For parsing BROWN, the
difference between these two parsers is not large,
though.
Increasing the relative weight of WSJ sentences
versus NANC sentences when testing on BROWN
development does not appear to have a significant
effect. While (McClosky et al, 2006) showed that
this technique was effective when testing on WSJ,
the true distribution was closer to WSJ so it made
sense to emphasize it.
4.2 Incorporating In-Domain Data
Up to this point, we have only considered the sit-
uation where we have no in-domain data. We now
3In this case, only the parser is trained on BROWN. In sec-
tion 4.3, we compare against a fully BROWN-trained rerank-
ing parser as well.
4A relative weight of n is equivalent to using n copies of
the corpus, i.e. an event that occurred x times in the corpus
would occur x?n times in the weighted corpus. Thus, larger
corpora will tend to dominate smaller corpora of the same
relative weight in terms of event counts.
explore different ways of making use of labeled
and unlabeled in-domain data.
Bacchiani et al (2006) applies self-training to
parser adaptation to utilize unlabeled in-domain
data. The authors find that it helps quite a bit when
adapting from BROWN to WSJ. They use a parser
trained from the BROWN train set to parse WSJ and
add the parsed WSJ sentences to their training set.
We perform a similar experiment, using our WSJ-
trained reranking parser to parse BROWN train and
testing on BROWN development. We achieved a
boost from 84.8% to 85.6% when we added the
parsed BROWN sentences to our training. Adding
in 1,000k sentences from NANC as well, we saw a
further increase to 86.3%. However, the technique
does not seem as effective in our case. While the
self-trained BROWN data helps the parser, it ad-
versely affects the performance of the reranking
parser. When self-trained BROWN data is added to
WSJ training, the reranking parser?s performance
drops from 86.6% to 86.1%. We see a similar
degradation as NANC data is added to the train-
ing set as well. We are not yet able to explain this
unusual behavior.
We now turn to the scenario where we have
some labeled in-domain data. The most obvious
way to incorporate labeled in-domain data is to
combine it with the labeled out-of-domain data.
We have already seen the results (Gildea, 2001)
and (Bacchiani et al, 2006) achieve in Table 1.
We explore various combinations of BROWN,
WSJ, and NANC corpora. Because we are
mainly interested in exploring techniques with
self-trained models rather than optimizing perfor-
mance, we only consider weighting each corpus
with a relative weight of one for this paper. The
models generated are tuned on section 24 from
WSJ. The results are summarized in Table 3.
While both WSJ and BROWN models bene-
fit from a small amount of NANC data, adding
more than 250k NANC sentences to the BROWN
or combined models causes their performance to
drop. This is not surprising, though, since adding
?too much? NANC overwhelms the more accurate
BROWN or WSJ counts. By weighting the counts
from each corpus appropriately, this problem can
be avoided.
Another way to incorporate labeled data is to
tune the parser back-off parameters on it. Bac-
chiani et al (2006) report that tuning on held-out
BROWN data gives a large improvement over tun-
340
ing on WSJ data. The improvement is mostly (but
not entirely) in precision. We do not see the same
improvement (Figure 1) but this is likely due to
differences in the parsers. However, we do see
a similar improvement for parsing accuracy once
NANC data has been added. The reranking parser
generally sees an improvement, but it does not ap-
pear to be significant.
4.3 Reranker Portability
We have shown that the WSJ-trained reranker is
actually quite portable to the BROWN fiction do-
main. This is surprising given the large number
of features (over a million in the case of the WSJ
reranker) tuned to adjust for errors made in the 50-
best lists by the first-stage parser. It would seem
the corrections memorized by the reranker are not
as domain-specific as we might expect.
As further evidence, we present the results of
applying the WSJ model to the Switchboard cor-
pus ? a domain much less similar to WSJ than
BROWN. In Table 4, we see that while the parser?s
performance is low, self-training and reranking
provide orthogonal benefits. The improvements
represent a 12% error reduction with no additional
in-domain data. Naturally, in-domain data and
speech-specific handling (e.g. disfluency model-
ing) would probably help dramatically as well.
Finally, to compare against a model fully
trained on BROWN data, we created a BROWN
reranker. We parsed the BROWN training set with
20-fold cross-validation, selected features that oc-
curred 5 times or more in the training set, and
fed the 50-best lists from the parser to a numeri-
cal optimizer to estimate feature weights. The re-
sulting reranker model had approximately 700,000
features, which is about half as many as the WSJ
trained reranker. This may be due to the smaller
size of the BROWN training set or because the
feature schemas for the reranker were developed
on WSJ data. As seen in Table 5, the BROWN
reranker is not a significant improvement over the
WSJ reranker for parsing BROWN data.
5 Analysis
We perform several types of analysis to measure
some of the differences and similarities between
the BROWN-trained and WSJ-trained reranking
parsers. While the two parsers agree on a large
number of parse brackets (Section 5.2), there are
categorical differences between them (as seen in
Parser model Parser f -score Reranker f -score
WSJ 74.0 75.9
WSJ+NANC 75.6 77.0
Table 4: Parser and reranking parser performance
on the SWITCHBOARD development corpus. In
this case, WSJ+NANC is a model created from WSJ
and 1,750k sentences from NANC.
Model 1-best 10-best 25-best 50-best
WSJ 82.6 88.9 90.7 91.9
WSJ+NANC 86.4 92.1 93.5 94.3
BROWN 86.3 92.0 93.3 94.2
Table 6: Oracle f -scores of top n parses pro-
duced by baseline WSJ parser, a combined WSJ and
NANC parser, and a baseline BROWN parser.
Section 5.3).
5.1 Oracle Scores
Table 6 shows the f -scores of an ?oracle reranker?
? i.e. one which would always choose the parse
with the highest f -score in the n-best list. While
the WSJ parser has relatively low f -scores, adding
NANC data results in a parser with comparable ora-
cle scores as the parser trained from BROWN train-
ing. Thus, the WSJ+NANC model has better oracle
rates than the WSJ model (McClosky et al, 2006)
for both the WSJ and BROWN domains.
5.2 Parser Agreement
In this section, we compare the output of the
WSJ+NANC-trained and BROWN-trained rerank-
ing parsers. We use evalb to calculate how sim-
ilar the two sets of output are on a bracket level.
Table 7 shows various statistics. The two parsers
achieved an 88.0% f -score between them. Ad-
ditionally, the two parsers agreed on all brackets
almost half the time. The part of speech tagging
agreement is fairly high as well. Considering they
were created from different corpora, this seems
like a high level of agreement.
5.3 Statistical Analysis
We conducted randomization tests for the signifi-
cance of the difference in corpus f -score, based on
the randomization version of the paired sample t-
test described by Cohen (1995). The null hypoth-
esis is that the two parsers being compared are in
fact behaving identically, so permuting or swap-
ping the parse trees produced by the parsers for
341
WSJ tuned parser
BROWN tuned parser
WSJ tuned reranking parser
BROWN tuned reranking parser
NANC sentences added
f-
sc
o
re
2000k1750k1500k1250k1000k750k500k250k0k
87.8
87.0
86.0
85.0
83.8
Figure 1: Precision and recall f -scores when testing on BROWN development as a function of the number
of NANC sentences added under four test conditions. ?BROWN tuned? indicates that BROWN training data
was used to tune the parameters (since the normal held-out section was being used for testing). For ?WSJ
tuned,? we tuned the parameters from section 24 of WSJ. Tuning on BROWN helps the parser, but not for
the reranking parser.
Parser model Parser alone Reranking parser
WSJ alone 83.9 85.8
WSJ+2,500k NANC 86.4 87.7
BROWN alone 86.3 87.4
BROWN+50k NANC 86.8 88.0
BROWN+250k NANC 86.8 88.1
BROWN+500k NANC 86.7 87.8
WSJ+BROWN 86.5 88.1
WSJ+BROWN+50k NANC 86.8 88.1
WSJ+BROWN+250k NANC 86.8 88.1
WSJ+BROWN+500k NANC 86.6 87.7
Table 3: f -scores from various combinations of WSJ, NANC, and BROWN corpora on BROWN develop-
ment. The reranking parser used the WSJ-trained reranker model. The BROWN parsing model is naturally
better than the WSJ model for this task, but combining the two training corpora results in a better model
(as in Gildea (2001)). Adding small amounts of NANC further improves the models.
Parser model Parser alone WSJ-reranker BROWN-reranker
WSJ 82.9 85.2 85.2
WSJ+NANC 87.1 87.8 87.9
BROWN 86.7 88.2 88.4
Table 5: Performance of various combinations of parser and reranker models when evaluated on BROWN
test. The WSJ+NANC parser with the WSJ reranker comes close to the BROWN-trained reranking parser.
The BROWN reranker provides only a small improvement over its WSJ counterpart, which is not statisti-
cally significant.
342
Bracketing agreement f -score 88.03%
Complete match 44.92%
Average crossing brackets 0.94
POS Tagging agreement 94.85%
Table 7: Agreement between the WSJ+NANC
parser with the WSJ reranker and the BROWN
parser with the BROWN reranker. Complete match
is how often the two reranking parsers returned the
exact same parse.
the same test sentence should not affect the cor-
pus f -scores. By estimating the proportion of per-
mutations that result in an absolute difference in
corpus f -scores at least as great as that observed
in the actual output, we obtain a distribution-
free estimate of significance that is robust against
parser and evaluator failures. The results of this
test are shown in Table 8. The table shows that
the BROWN reranker is not significantly different
from the WSJ reranker.
In order to better understand the difference be-
tween the reranking parser trained on Brown and
the WSJ+NANC/WSJ reranking parser (a reranking
parser with the first-stage trained on WSJ+NANC
and the second-stage trained on WSJ) on Brown
data, we constructed a logistic regression model
of the difference between the two parsers? f -
scores on the development data using the R sta-
tistical package5. Of the 2,078 sentences in the
development data, 29 sentences were discarded
because evalb failed to evaluate at least one of
the parses.6 A Wilcoxon signed rank test on the
remaining 2,049 paired sentence level f -scores
was significant at p = 0.0003. Of these 2,049
sentences, there were 983 parse pairs with the
same sentence-level f -score. Of the 1,066 sen-
tences for which the parsers produced parses with
different f -scores, there were 580 sentences for
which the BROWN/BROWN parser produced a
parse with a higher sentence-level f -score and 486
sentences for which the WSJ+NANC/WSJ parser
produce a parse with a higher f -score. We
constructed a generalized linear model with a
binomial link with BROWN/BROWN f -score >
WSJ+NANC/WSJ f -score as the predicted variable,
and sentence length, the number of prepositions
(IN), the number of conjunctions (CC) and Brown
5http://www.r-project.org
6This occurs when an apostrophe is analyzed as a posses-
sive marker in the gold tree and a punctuation symbol in the
parse tree, or vice versa.
Feature Estimate z-value Pr(> |z|)
(Intercept) 0.054 0.3 0.77
IN -0.134 -4.4 8.4e-06 ***
ID=G 0.584 2.5 0.011 *
ID=K 0.697 2.9 0.003 **
ID=L 0.552 2.3 0.021 *
ID=M 0.376 0.9 0.33
ID=N 0.642 2.7 0.0055 **
ID=P 0.624 2.7 0.0069 **
ID=R 0.040 0.1 0.90
Table 9: The logistic model of BROWN/BROWN
f -score > WSJ+NANC/WSJ f -score identified by
model selection. The feature IN is the num-
ber prepositions in the sentence, while ID identi-
fies the Brown subcorpus that the sentence comes
from. Stars indicate significance level.
subcorpus ID as explanatory variables. Model
selection (using the ?step? procedure) discarded
all but the IN and Brown ID explanatory vari-
ables. The final estimated model is shown in Ta-
ble 9. It shows that the WSJ+NANC/WSJ parser
becomes more likely to have a higher f -score
than the BROWN/BROWN parser as the number
of prepositions in the sentence increases, and that
the BROWN/BROWN parser is more likely to have
a higher f -score on Brown sections K, N, P, G
and L (these are the general fiction, adventure and
western fiction, romance and love story, letters and
memories, and mystery sections of the Brown cor-
pus, respectively). The three sections of BROWN
not in this list are F, M, and R (popular lore, sci-
ence fiction, and humor).
6 Conclusions and Future Work
We have demonstrated that rerankers and self-
trained models can work well across domains.
Models self-trained on WSJ appear to be better
parsing models in general, the benefits of which
are not limited to the WSJ domain. The WSJ-
trained reranker using out-of-domain LA Times
parses (produced by the WSJ-trained reranker)
achieves a labeled precision-recall f -measure of
87.8% on Brown data, nearly equal to the per-
formance one achieves by using a purely Brown
trained parser-reranker. The 87.8% f -score on
Brown represents a 24% error reduction on the
corpus.
Of course, as corpora differences go, Brown is
relatively close to WSJ. While we also find that our
343
WSJ+NANC/WSJ BROWN/WSJ BROWN/BROWN
WSJ/WSJ 0.025 (0) 0.030 (0) 0.031 (0)
WSJ+NANC/WSJ 0.004 (0.1) 0.006 (0.025)
BROWN/WSJ 0.002 (0.27)
Table 8: The difference in corpus f -score between the various reranking parsers, and the significance of
the difference in parentheses as estimated by a randomization test with 106 samples. ?x/y? indicates that
the first-stage parser was trained on data set x and the second-stage reranker was trained on data set y.
?best? WSJ-parser-reranker improves performance
on the Switchboard corpus, it starts from a much
lower base (74.0%), and achieves a much less sig-
nificant improvement (3% absolute, 11% error re-
duction). Bridging these larger gaps is still for the
future.
One intriguing idea is what we call ?self-trained
bridging-corpora.? We have not yet experimented
with medical text but we expect that the ?best?
WSJ+NANC parser will not perform very well.
However, suppose one does self-training on a bi-
ology textbook instead of the LA Times. One
might hope that such a text will split the differ-
ence between more ?normal? newspaper articles
and the specialized medical text. Thus, a self-
trained parser based upon such text might do much
better than our standard ?best.? This is, of course,
highly speculative.
Acknowledgments
This work was supported by NSF grants LIS9720368, and
IIS0095940, and DARPA GALE contract HR0011-06-2-
0001. We would like to thank the BLLIP team for their com-
ments.
References
Michiel Bacchiani, Michael Riley, Brian Roark, and
Richard Sproat. 2006. MAP adaptation of stochas-
tic grammars. Computer Speech and Language,
20(1):41?68.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and MaxEnt discriminative
reranking. In Proc. of the 2005 Meeting of the
Assoc. for Computational Linguistics (ACL), pages
173?180.
Andrew B. Clegg and Adrian Shepherd. 2005. Evalu-
ating and integrating treebank parsers on a biomedi-
cal corpus. In Proceedings of the ACL Workshop on
Software.
Paul R. Cohen. 1995. Empirical Methods for Artifi-
cial Intelligence. The MIT Press, Cambridge, Mas-
sachusetts.
Michael Collins. 2000. Discriminative reranking
for natural language parsing. In Machine Learn-
ing: Proceedings of the Seventeenth International
Conference (ICML 2000), pages 175?182, Stanford,
California.
W. Nelson Francis and Henry Kuc?era. 1979. Manual
of Information to accompany a Standard Corpus of
Present-day Edited American English, for use with
Digital Computers. Brown University, Providence,
Rhode Island.
Daniel Gildea. 2001. Corpus variation and parser per-
formance. In Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 167?202.
David Graff. 1995. North American News Text Cor-
pus. Linguistic Data Consortium. LDC95T21.
Rebecca Hwa. 1999. Supervised grammar induction
using training data with limited constituent infor-
mation. In Proceedings of the 37th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 72?80, University of Maryland.
Matthew Lease and Eugene Charniak. 2005. Parsing
biomedical literature. In Second International Joint
Conference on Natural Language Processing (IJC-
NLP?05).
Michell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn Treebank. Comp. Lin-
guistics, 19(2):313?330.
David McClosky, Eugene Charniak, and Mark John-
son. 2006. Effective self-training for parsing. In
Proceedings of HLT-NAACL 2006.
Adwait Ratnaparkhi. 1999. Learning to parse natural
language with maximum entropy models. Machine
Learning, 34(1-3):151?175.
Satoshi Sekine. 1997. The domain dependence of
parsing. In Proc. Applied Natural Language Pro-
cessing (ANLP), pages 96?102.
Mark Steedman, Miles Osborne, Anoop Sarkar,
Stephen Clark, Rebecca Hwa, Julia Hockenmaier,
Paul Ruhlen, Steven Baker, and Jeremiah Crim.
2003. Bootstrapping statistical parsers from small
datasets. In Proc. of European ACL (EACL), pages
331?338.
344
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 101?104,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Self-Training for Biomedical Parsing
David McClosky and Eugene Charniak
Brown Laboratory for Linguistic Information Processing (BLLIP)
Brown University
Providence, RI 02912
{dmcc|ec}@cs.brown.edu
Abstract
Parser self-training is the technique of
taking an existing parser, parsing extra
data and then creating a second parser
by treating the extra data as further
training data. Here we apply this tech-
nique to parser adaptation. In partic-
ular, we self-train the standard Char-
niak/Johnson Penn-Treebank parser us-
ing unlabeled biomedical abstracts. This
achieves an f -score of 84.3% on a stan-
dard test set of biomedical abstracts from
the Genia corpus. This is a 20% error re-
duction over the best previous result on
biomedical data (80.2% on the same test
set).
1 Introduction
Parser self-training is the technique of taking an
existing parser, parsing extra data and then cre-
ating a second parser by treating the extra data
as further training data. While for many years it
was thought not to help state-of-the art parsers,
more recent work has shown otherwise. In this
paper we apply this technique to parser adap-
tation. In particular we self-train the standard
Charniak/Johnson Penn-Treebank (C/J) parser
using unannotated biomedical data. As is well
known, biomedical data is hard on parsers be-
cause it is so far from more ?standard? English.
To our knowledge this is the first application of
self-training where the gap between the training
and self-training data is so large.
In section two, we look at previous work. In
particular we note that there is, in fact, very
little data on self-training when the corpora for
self-training is so different from the original la-
beled data. Section three describes our main
experiment on standard test data (Clegg and
Shepherd, 2005). Section four looks at some
preliminary results we obtained on development
data that show in slightly more detail how self-
training improved the parser. We conclude in
section five.
2 Previous Work
While self-training has worked in several do-
mains, the early results on self-training for pars-
ing were negative (Steedman et al, 2003; Char-
niak, 1997). However more recent results have
shown that it can indeed improve parser perfor-
mance (Bacchiani et al, 2006; McClosky et al,
2006a; McClosky et al, 2006b).
One possible use for this technique is for
parser adaptation ? initially training the parser
on one type of data for which hand-labeled trees
are available (e.g., Wall Street Journal (M. Mar-
cus et al, 1993)) and then self-training on a sec-
ond type of data in order to adapt the parser
to the second domain. Interestingly, there is lit-
tle to no data showing that this actually works.
Two previous papers would seem to address this
issue: the work by Bacchiani et al (2006) and
McClosky et al (2006b). However, in both cases
the evidence is equivocal.
Bacchiani and Roark train the Roark parser
(Roark, 2001) on trees from the Brown treebank
and then self-train and test on data from Wall
Street Journal. While they show some improve-
ment (from 75.7% to 80.5% f -score) there are
several aspects of this work which leave its re-
101
sults less than convincing as to the utility of self-
training for adaptation. The first is the pars-
ing results are quite poor by modern standards.1
Steedman et al (2003) generally found that self-
training does not work, but found that it does
help if the baseline results were sufficiently bad.
Secondly, the difference between the Brown
corpus treebank and the Wall Street Journal
corpus is not that great. One way to see this
is to look at out-of-vocabulary statistics. The
Brown corpus has an out-of-vocabulary rate of
approximately 6% when given WSJ training as
the lexicon. In contrast, the out-of-vocabulary
rate of biomedical abstracts given the same lex-
icon is significantly higher at about 25% (Lease
and Charniak, 2005). Thus the bridge the self-
trained parser is asked to build is quite short.
This second point is emphasized by the sec-
ond paper on self-training for adaptation (Mc-
Closky et al, 2006b). This paper is based on the
C/J parser and thus its results are much more
in line with modern expectations. In particu-
lar, it was able to achieve an f -score of 87% on
Brown treebank test data when trained and self-
trained on WSJ-like data. Note this last point.
It was not the case that it used the self-training
to bridge the corpora difference. It self-trained
on NANC, not Brown. NANC is a news corpus,
quite like WSJ data. Thus the point of that
paper was that self-training a WSJ parser on
similar data makes the parser more flexible, not
better adapted to the target domain in particu-
lar. It said nothing about the task we address
here. Thus our claim is that previous results are
quite ambiguous on the issue of bridging corpora
for parser adaptation.
Turning briefly to previous results on Medline
data, the best comparative study of parsers is
that of Clegg and Shepherd (2005), which eval-
uates several statistical parsers. Their best re-
sult was an f -score of 80.2%. This was on the
Lease/Charniak (L/C) parser (Lease and Char-
niak, 2005).2 A close second (1% behind) was
1This is not a criticism of the work. The results are
completely in line with what one would expect given the
base parser and the relatively small size of the Brown
treebank.
2This is the standard Charniak parser (without
the parser of Bikel (2004). The other parsers
were not close. However, several very good cur-
rent parsers were not available when this paper
was written (e.g., the Berkeley Parser (Petrov
et al, 2006)). However, since the newer parsers
do not perform quite as well as the C/J parser
on WSJ data, it is probably the case that they
would not significantly alter the landscape.
3 Central Experimental Result
We used as the base parser the standardly avail-
able C/J parser. We then self-trained the parser
on approximately 270,000 sentences ? a ran-
dom selection of abstracts from Medline.3 Med-
line is a large database of abstracts and citations
from a wide variety of biomedical literature. As
we note in the next section, the number 270,000
was selected by observing performance on a de-
velopment set.
We weighted the original WSJ hand anno-
tated sentences equally with self-trained Med-
line data. So, for example, McClosky et al
(2006a) found that the data from the hand-
annotated WSJ data should be considered at
least five times more important than NANC
data on an event by event level. We did no tun-
ing to find out if there is some better weighting
for our domain than one-to-one.
The resulting parser was tested on a test cor-
pus of hand-parsed sentences from the Genia
Treebank (Tateisi et al, 2005). These are ex-
actly the same sentences as used in the com-
parisons of the last section. Genia is a corpus
of abstracts from the Medline database selected
from a search with the keywords Human, Blood
Cells, and Transcription Factors. Thus the Ge-
nia treebank data are all from a small domain
within Biology. As already noted, the Medline
abstracts used for self-training were chosen ran-
domly and thus span a large number of biomed-
ical sub-domains.
The results, the central results of this paper,
are shown in Figure 1. Clegg and Shepherd
(2005) do not provide separate precision and
recall numbers. However we can see that the
reranker) modified to use an in-domain tagger.
3http://www.ncbi.nlm.nih.gov/PubMed/
102
System Precision Recall f -score
L/C ? ? 80.2%
Self-trained 86.3% 82.4% 84.3%
Figure 1: Comparison of the Medline self-trained
parser against the previous best
Medline self-trained parser achieves an f -score
of 84.3%, which is an absolute reduction in er-
ror of 4.1%. This corresponds to an error rate
reduction of 20% over the L/C baseline.
4 Discussion
Prior to the above experiment on the test data,
we did several preliminary experiments on devel-
opment data from the Genia Treebank. These
results are summarized in Figure 2. Here we
show the f -score for four versions of the parser
as a function of number of self-training sen-
tences. The dashed line on the bottom is the
raw C/J parser with no self-training. At 80.4, it
is clearly the worst of the lot. On the other hand,
it is already better than the 80.2% best previous
result for biomedical data. This is solely due to
the introduction of the 50-best reranker which
distinguishes the C/J parser from the preceding
Charniak parser.
The almost flat line above it is the C/J parser
with NANC self-training data. As mentioned
previously, NANC is a news corpus, quite like
the original WSJ data. At 81.4% it gives us a
one percent improvement over the original WSJ
parser.
The topmost line, is the C/J parser trained
on Medline data. As can be seen, even just a
thousand lines of Medline is already enough to
drive our results to a new level and it contin-
ues to improve until about 150,000 sentences at
which point performance is nearly flat. How-
ever, as 270,000 sentences is fractionally better
than 150,000 sentences that is the number of
self-training sentences we used for our results
on the test set.
Lastly, the middle jagged line is for an inter-
esting idea that failed to work. We mention it
in the hope that others might be able to succeed
where we have failed.
We reasoned that textbooks would be a par-
ticularly good bridging corpus. After all, they
are written to introduce someone ignorant of
a field to the ideas and terminology within it.
Thus one might expect that the English of a Bi-
ology textbook would be intermediate between
the more typical English of a news article and
the specialized English native to the domain.
To test this we created a corpus of seven texts
(?BioBooks?) on various areas of biology that
were available on the web. We observe in Fig-
ure 2 that for all quantities of self-training data
one does better with Medline than BioBooks.
For example, at 37,000 sentences the BioBook
corpus is only able to achieve and an f-measure
of 82.8% while the Medline corpus is at 83.4%.
Furthermore, BioBooks levels off in performance
while Medline has significant improvement left
in it. Thus, while the hypothesis seems reason-
able, we were unable to make it work.
5 Conclusion
We self-trained the standard C/J parser on
270,000 sentences of Medline abstracts. By do-
ing so we achieved a 20% error reduction over
the best previous result for biomedical parsing.
In terms of the gap between the supervised data
and the self-trained data, this is the largest that
has been attempted.
Furthermore, the resulting parser is of interest
in its own right, being as it is the most accurate
biomedical parser yet developed. This parser is
available on the web.4
Finally, there is no reason to believe that
84.3% is an upper bound on what can be
achieved with current techniques. Lease and
Charniak (2005) achieve their results using small
amounts of hand-annotated biomedical part-of-
speech-tagged data and also explore other pos-
sible sources or information. It is reasonable to
assume that its use would result in further im-
provement.
Acknowledgments
This work was supported by DARPA GALE con-
tract HR0011-06-2-0001. We would like to thank the
BLLIP team for their comments.
4http://bllip.cs.brown.edu/biomedical/
103
0 25000 50000 75000 100000 125000 150000 175000 200000 225000 250000 275000
Number of sentences added
80.0
80.2
80.4
80.6
80.8
81.0
81.2
81.4
81.6
81.8
82.0
82.2
82.4
82.6
82.8
83.0
83.2
83.4
83.6
83.8
84.0
84.2
84.4
R
e
r
a
n
k
i
n
g
 
p
a
r
s
e
r
 
f
-
s
c
o
r
e
WSJ+Medline
WSJ+BioBooks
WSJ+NANC
WSJ (baseline)
Figure 2: Labeled Precision-Recall results on development data for four versions of the parser as a function
of number of self-training sentences
References
Michiel Bacchiani, Michael Riley, Brian Roark, and
Richard Sproat. 2006. MAP adaptation of
stochastic grammars. Computer Speech and Lan-
guage, 20(1):41?68.
Daniel M. Bikel. 2004. Intricacies of collins parsing
model. Computational Linguistics, 30(4).
Eugene Charniak. 1997. Statistical parsing with
a context-free grammar and word statistics. In
Proc. AAAI, pages 598?603.
Andrew B. Clegg and Adrian Shepherd. 2005.
Evaluating and integrating treebank parsers on
a biomedical corpus. In Proceedings of the ACL
Workshop on Software.
Matthew Lease and Eugene Charniak. 2005. Pars-
ing biomedical literature. In Second International
Joint Conference on Natural Language Processing
(IJCNLP?05).
M. Marcus et al 1993. Building a large annotated
corpus of English: The Penn Treebank. Comp.
Linguistics, 19(2):313?330.
David McClosky, Eugene Charniak, and Mark John-
son. 2006a. Effective self-training for parsing.
In Proceedings of the Human Language Technol-
ogy Conference of the NAACL, Main Conference,
pages 152?159.
David McClosky, Eugene Charniak, and Mark John-
son. 2006b. Reranking and self-training for
parser adaptation. In Proceedings of COLING-
ACL 2006, pages 337?344, Sydney, Australia,
July. Association for Computational Linguistics.
Slav Petrov, Leon Barrett, Romain Thibaux, and
Dan Klein. 2006. Learning accurate, compact,
and interpretable tree annotation. In Proceed-
ings of COLING-ACL 2006, pages 433?440, Syd-
ney, Australia, July. Association for Computa-
tional Linguistics.
Brian Roark. 2001. Probabilistic top-down parsing
and language modeling. Computational Linguis-
tics, 27(2):249?276.
Mark Steedman, Miles Osborne, Anoop Sarkar,
Stephen Clark, Rebecca Hwa, Julia Hockenmaier,
Paul Ruhlen, Steven Baker, and Jeremiah Crim.
2003. Bootstrapping statistical parsers from small
datasets. In Proc. of European ACL (EACL),
pages 331?338.
Y. Tateisi, A. Yakushiji, T. Ohta, and J. Tsujii.
2005. Syntax Annotation for the GENIA corpus.
Proc. IJCNLP 2005, Companion volume, pages
222?227.
104
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 873?882, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Learning Constraints for Consistent Timeline Extraction
David McClosky and Christopher D. Manning
Natural Language Processing Group
Computer Science Department
Stanford University, Stanford, CA, USA
{mcclosky,manning}@stanford.edu
Abstract
We present a distantly supervised system for
extracting the temporal bounds of fluents (re-
lations which only hold during certain times,
such as attends school). Unlike previous
pipelined approaches, our model does not as-
sume independence between each fluent or
even between named entities with known con-
nections (parent, spouse, employer, etc.). In-
stead, we model what makes timelines of flu-
ents consistent by learning cross-fluent con-
straints, potentially spanning entities as well.
For example, our model learns that someone
is unlikely to start a job at age two or to marry
someone who hasn?t been born yet. Our sys-
tem achieves a 36% error reduction over a
pipelined baseline.
1 Introduction
Many information extraction (IE) systems tradition-
ally extracted just relations, but a great many real
world relations such as attends school or has spouse
vary over time. To capture this, some recent IE
systems have extended their focus from relations to
fluents (relations combined with temporal bounds).
This can be seen in the temporal slot filling track in
the TAC-KBP 2011 shared task (Ji et al2011). A
direct application of this work is the automatic im-
provement of online resources such as Freebase and
Wikipedia infoboxes. Indirect applications include
question answering systems.
Fluents can be grouped together to form time-
lines (see Figure 1 for an example) and provide eas-
ily capturable consistency constraints. Our goal is
Figure 1: A timeline of two named entities. Each time
span represents a fluent (a relation with temporal bounds).
Temporal bounds are denoted by spans on the timeline.
Fluents can create links between entities (e.g., marriage).
to learn these constraints and use them to produce
more accurate timelines of significant events for
people and organizations. For example, it is com-
mon knowledge that someone cannot attend a school
if they haven?t been born yet. Constraints on con-
sistent timelines do not need to be hard constraints,
though: it is rare, although possible, to become the
CEO of a company at the age of 21.
Despite the rich constraints on valid timelines,
there is relatively little work on exploiting these con-
straints for mutual disambiguation. Many existing
systems extract different parts of a timeline sepa-
rately and use heuristics to combine them. These
heuristics tend to optimize only local consistency
(within a single fluent) but ignore more global con-
straints across fluents (e.g., attending a school be-
fore being born) or across fluents of two linked
entities (e.g., attending a school before the school
was founded). In this work, we explore using joint
inference to enforce these constraints. We show
that these techniques can yield substantial improve-
ments. Additionally, our general approach is not
specific to extracting temporal boundaries of fluents.
It could easily be applied to other IE systems which
873
employ independent extractions followed by heuris-
tics to improve consistency.
2 The timelining task
As a basis for our task, we first describe the Tempo-
ral KBP task (Ji et al2011). As input, one is given
a list of queries, a database of example fluents, and
source documents. Queries are named entities (peo-
ple or organizations) with their gold relations but no
temporal bounds. The database consists of training
entities with their fluents, including known tempo-
ral bounds for each fluent. Example fluents can be
seen in Table 1. Note that the database may be in-
complete. In addition to missing fluents for an en-
tity, some temporal bounds may be missing from
the database; missing bounds are unfortunately in-
distinguishable from unbounded ranges. As a result,
we can only trust concrete temporal boundaries in
the database. Source documents consist of raw text
from news, blogs, and Wikipedia articles. For each
fluent, systems must output their predicted temporal
bounds, along with references to source documents
to provide provenance.
Our task is a variation of the Temporal KBP task.
In our case, the database is a collection of Freebase1
entities and their fluents, merged with Wikipedia in-
foboxes. Each entity has a unique ID, allowing us
to avoid some coreference issues (though there can
still be issues in document retrieval). In Temporal
KBP, the temporal representation allows for upper
and lower bounds on both the event start and end:
?sl, su, el, eu? where sl ? start ? su, el ? end ?
eu. However, it is difficult to obtain these bounds
without manual annotation. As a result, we opted for
the simpler representation which can be easily found
in databases like Freebase. Our temporal represen-
tation is limited to bounds of the form ?start, end?
where either can be unbounded or unknown (both
represented as ??).
Our set of fluents is closely related to those in
the Temporal KBP task. Our goal was to use
as much temporal information as possible, with
the hope of each fluent providing additional poten-
tial constraints. While we omit the resides in and
member of fluents,2 we add several others. For
1http://freebase.org
2This is because these fluents are rarely present in Freebase
people and organizations, we add a special fluent,
lifespan, which doesn?t take a slot value.3 A list of
fluents we use are listed in Table 3.
3 Model
To operate on a set of queries, we first collect can-
didate temporal expression mentions for each fluent
from our source documents. This limits us to us-
ing temporal expression mentions which appear near
fluent mentions in text. It also ensures that we can
provide provenance for each temporal boundary as-
sertion. This process is described in ?3.1.
Our model contains two components, both of
which assign probabilities to timelines. The clas-
sifier component determines how each candidate
temporal expression mention connects to its fluent
(?3.2). For example, the mention may indicate the
START of the fluent, the END, both its START AND
END (for instantaneous events), or be UNRELATED.
These connections involve relations between tempo-
ral expression mentions and relations and we refer to
them as metarelations.4 For features, the classifier
uses the surrounding textual and syntactic context of
temporal expression and fluent mentions. Each clas-
sification decision is made independently, allowing
for inconsistency at multiple levels (within a fluent,
across fluents, or across entities). However, using
joint inference, the classifier component can deter-
mine the best overall span for each fluent.
The consistency component learns what makes
timelines consistent (?3.3). It is similar in nature to
a language model for timelines instead of sentences.
Given a candidate timeline, the consistency compo-
nent estimates its probability of occurring. This is
done by decomposing timelines into a series of ques-
tions (such as ?did the entity go to school before
starting a job??) and learning the probabilities of
different answers from training data.
Unlike the classifier component, the consistency
component is blind to the underlying text in the
source documents. The two components work to-
gether to find a global timeline that is both based on
textual evidence and coherent across entities using
with temporal bounds.
3Note that this is a relation in the non-temporal KBP task.
4Other metarelations are possible under more complex tem-
poral representations. For example, Artiles et al2011) uses
the HOLDS metarelation.
874
Entity Relation Slot value Temporal bounds
Jon Stewart lifespan ? [1962-11-28, +?)/en/jon stewart
Jon Stewart has parent Donald Leibowitz [1962-11-28, +?)/en/jon stewart /en/donald leibowitz
Jon Stewart
attends school College of William and Mary (??, 1984]/en/jon stewart /en/college of william and mary
Jon Stewart has spouse Tracey McShane [2000-11, +?)/en/jon stewart /en/tracy mcshane
Table 1: Example relations with their temporal bounds. Freebase IDs are shown in monospace. Note that temporal
bounds differ in their resolution (some are days of the year, others are only years). Some bounds are unknown (e.g.,
the start of the attends school fluent) and indistinguishable from unbounded. The lifespan fluent is a unary relation.
joint inference (note that they are trained indepen-
dently). The inference process is described in ?3.4.
3.1 Temporal expression retrieval
Given a fluent, we search for all textual mentions
of the fluent and collect nearby temporal expression
mentions. These temporal expressions are used as
candidate boundaries for the fluent in later steps.
The search process assumes that if a fluent?s entity
and slot value co-occur in a sentence,5 that sentence
is typically a positive example of the fluent.6 This
is sometimes known as distant supervision (Craven
and Kumlien, 1999; Mintz et al2009). We use
the Stanford Core NLP suite (Toutanova et al2003;
Finkel et al2005; Klein and Manning, 2003; Lee et
al., 2011) to annotate each document with POS and
NER tags, parse trees, and coreference chains. On
top of this, we apply a rule-based temporal expres-
sion extractor (Chang and Manning, 2012). Since
we have coreference links, we also search docu-
ments for anything coreferent with the fluent?s en-
tity.
The temporal expression extractor handles most
standard date and time formats. For each document,
one can provide an optional reference time. For
underspecified dates, the reference time is used to
5While we limit our scope to sentences in this work, it is
trivial to extend this to larger regions such as paragraphs.
6The lifespan fluent requires special handling. Ideally, its
candidates would be provided by a relation extraction mention
detector (e.g., a KBP system). For this work, we use the gold
lifespan bounds as slot values for the purpose of document re-
trieval. While this does heavily bias the system towards using
gold bounds, the system still must predict the correct associa-
tions (START, END, etc.) making the lifespan fluent non-trivial.
resolve these dates to full expressions if possible.
Some of our documents are news articles, where we
use the publication date as the reference time. Other
documents, e.g., Wikipedia articles, are undated and
we typically omit a reference time for these. We ex-
clude dates which are not uniquely resolvable (e.g.,
?September 15th,? when the reference date is un-
known) since our task requires us to output unam-
biguous dates.
We create training datums by computing the
metarelation between each temporal expression and
its gold fluent. For example, for the temporal
expression mention ?September 15th, 1981? and
gold lifespan relation that spans [1981-09-15,
+?), we would assign the START metarelation. As
a heuristic, we allow for underspecified matches.
Thus, both ?1981? and ?September 1981? would
have the START metarelation but ?September 2nd,
1981? would be assigned UNRELATED.
3.2 Classifier component
We use a classifier to determine the nature of the
link between fluents and candidate temporal expres-
sion mentions. Our classifier (a standard multi-
class maximum entropy classifier) learns a function
C : (t, f) ? M where t is a temporal expression
mention, f = ?entity, relation name, slot value? is
a fluent from the database, and M is the set of the
four possible metarelations.
Features for the classifier include many of those
in Artiles et al2011). These include standard re-
lation extraction features such as the dependency
paths between the temporal expression and the en-
tity or slot value. We use both the original depen-
875
dency paths and their collapsed Stanford Dependen-
cies forms (de Marneffe and Manning, 2008). We
include the lengths of each path and, if the path is
shorter than four edges, the grammatical relations,
words, POS tags, and NER labels along the path.
We extract the same sorts of features from surface
paths (i.e., the words and tags between the entity and
the temporal expression) if the path is five tokens or
shorter. For temporal expressions, we include their
century and decade as features. These features act as
a crude prior over when valid temporal expressions
occur. There are also features for the precision of
the temporal expression (year only, has month, and
has day). Lastly, we include the relation name itself
as a feature.
Previous work (Artiles et al2011) heuristically
aggregates the hard decisions from their classifier to
create a locally consistent span. The basic aggre-
gation model (described in ?4.2) is similar to their
method. In contrast, our method uses the likeli-
hood of complete spans to ensure both boundaries
are consistent with the text.
To calculate the likelihood of a specific temporal
span for a fluent f , we represent the span as a
series of metarelations and take the product of their
probabilities. For example, if the candidate span is
[1981-09-15, +?) and we have two temporal
expressions, ?September 15th, 1981? and ?2012?:
P
(
span(f) = [1981-09-15, +?) | f
)
=
P
(
C(?September 15th, 1981?, f) = START
)
?
P
(
C(?2012?, f) = UNRELATED
)
This can easily be extended to calculating the joint
probability of an entire timeline, represented as a list
of ?fluent , span? pairs:
PCC
(
?f1, s1?, . . .
)
=
?
i
P
(
span(fi) = si | fi
)
We refer to this model as the Combined Classifier
(CC) since it uses the probabilities of all timelines
boundaries rather than aggregating hard local deci-
sions.
3.3 Consistency component
While distant supervision can be used to create im-
plicit negative examples for the classifier component
(time expressions marked as UNRELATED), we do
not have an equivalent technique to reliably create
negative examples for the consistency component
(examples of inconsistent timelines). Instead, we
only have positive examples of consistent timelines
from the database. As a result, we must treat predict-
ing consistency as a density estimation rather than a
classification problem.
Our consistency component is designed to be as
general as possible ? it does not even include basic
constraints about timelines such as ?starts are before
ends.? Instead, we provide several simple templates
for temporal constraints to allow it to learn these ba-
sic tendencies as well as more complex ones. Ex-
amples include whether one typically goes to school
first or starts their first job, how many jobs people
typically have at one time, or if it is possible to marry
someone who hasn?t been born yet.
We achieve this by decomposing timelines
into a series of probabilistic events, or ques-
tions. As an example, one question about
the timeline shown in Table 1 is whether Jon
Stewart graduated from the College of William
and Mary BEFORE marrying Tracey McShane,
i.e., end(attends school) < start(has spouse). In this
case, the answer is ?yes.? More generally, we
can apply the BEFORE template to all bound-
aries of all fluents: boundary1(fluent1) <
boundary2(fluent2). We use templates like these
(denoted by SMALL CAPS) to generate all possible
questions to ask about a specific entity.
Other questions can be asked at the fluent level
rather than the boundary level (Allen, 1983). One
set of fluent level questions asks whether two flu-
ents? spans OVERLAP. For example, in Table 1, Jon
Stewart?s lifespan OVERLAPs with the span of his
has spouse fluent. Other sets of fluent level ques-
tions ask whether the span of a fluent completely
CONTAINS the span of another one, whether a flu-
ent is COMPLETELY BEFORE another fluent, and
whether two fluents TOUCH (the start of one fluent
is the same as the end of another).
Since all of these questions involve ordering but
ignore the actual differences in time, we create one
more set of questions asking whether two bound-
aries are WITHIN a certain number of years:
|boundary1(fluent1)? boundary2(fluent2)| ? K
876
for K ? {1, 2, 4, 8, 16}. The aim is to approxi-
mate the typical lengths of a single fluent or amount
of time between boundaries from different fluents.
There is nothing which requires that the flu-
ents in question come from a single entity. Thus,
we can trivially ask questions about two entities
which are linked by a fluent. For example, since
Jon Stewart is linked to Tracey McShane by the
has spouse fluent (Table 1), we could ask the ques-
tion of whether Jon Stewart?s lifespan OVERLAPS
Tracey McShane?s lifespan. We can ask any type
of question about two linked entities and distinguish
the questions by prefixing them with the nature of
the link (has spouse in this case).
Note that not all questions can be answered since
they may rely on comparing unknown values. This
is because (for our setup) infinite values are indistin-
guishable from unknown values. For example, the
start of the Jon Stewart?s attends school fluent is un-
defined in the database, but clearly not actually ??.
Thus, we add a third possible answer to each ques-
tion: unknown. The answers to boundary level ques-
tions are defined only if both boundaries are finite.
Fluent level questions have known answers as long
as both fluents have at least one finite value.
To train our model, we gather the answers to ques-
tions over all the fluents from training entities. Each
question forms a multinomial over the three possible
values (yes, no, unknown). To determine the proba-
bility of a complete timeline:
Pconsistency(timeline) =
?
(q,a)?Q(timeline)
{
(1? c)P?(a | q) q is old
c q is new
where Q(?) generates all possible
?question, answer? pairs which are consistent
with the fluents in the timeline, ? is a vector of the
model parameters, and c is a smoothing parameter
(described below).
To learn the model parameters, we start by us-
ing maximum-likelihood estimation for these multi-
nomials from training entities. However, some
smoothing is required since new entities may con-
tain previously unseen answers to existing ques-
tions. To address this, we apply add-? smoothing
to each multinomial, P?(a | q). Additionally, it is
possible to see entirely new questions when we see
a new combination of fluent types. We reserve an
amount of probability mass for new questions, c. c
and ? are estimated in turn by picking the value that
maximizes the likelihood of the timeline made by
the development entities.
To adjust the weight of the consistency compo-
nent relative to the classifier component, we take
the geometric mean of the likelihood using the to-
tal number of questions, |Q(t)|, as the exponent and
raise the resulting mean to an exponent, ?. This is
necessary since the two components essentially op-
erate on different scales. The Joint Classifier with
Consistency (JCC) model calculates the score of a
timeline, t, according to both components:
scoreJCC (t) = PCC (t)
[
Pconsistency(t)
?
|Q(t)|
]
3.4 Inference
Inference for the CC model is relatively simple:
Simply pick the most likely span for each fluent.
Since it assumes all fluents are independent, the
bounds for each fluent can be inferred separately.
To perform inference on a specific fluent, we con-
sider all of its possible temporal spans, limited by
the temporal expression mentions found by the re-
trieval system (?3.1). Each possible span assigns one
of the four metarelations to each candidate temporal
expression for the fluent. For example, if we found
only the temporal expression mention ?1981? for a
specific fluent, there are four possible spans:
UNRELATED: (??, +?)
START: [1981-01-01, +?)
END: (??, 1981-12-31]
START AND END: [1981-01-01, 1981-12-31]
Note that when we assign ?1981? as a start, we
use the earliest possible time (January 1st) while
when we assign it as an end, we use the latest pos-
sible time (December 31st). Of course, we typi-
cally have multiple candidate temporal expressions
and thus potentially many more than four possible
spans. All temporal expression mentions that re-
solve to the same time are grouped together, since it
wouldn?t make sense to assign ?August 28th, 2010?
one metarelation and a different one to ?8/28/2010.?
Joint inference for the JCC model is a little more
involved since the consistency model does not as-
sume independence across fluents. Thus, we need
877
to apply techniques like Gibbs sampling or random-
restart hillclimbing (RRHC) to determine the opti-
mal temporal spans for each fluent. For our task,
the two methods obtain similar performance while
RRHC requires many fewer iterations so our discus-
sion focuses on the latter. RRHC involves looping
over all fluents in our testing entities, shuffling the
order of the fluents at the beginning of each pass.
We maintain a working timeline, t, with our current
guesses of the spans for each fluent. For each fluent
and span ?f, s? ? t, we pick the optimal span for f :
s? = argmax
s??S(f)
scoreJCC (ts?)
where S(f) determines all possible temporal
spans for the fluent f and ts? = (t? ?f, s??)? ?f, s?
is a copy of t where s? is the span for f instead
of s. After selecting s?, we add it to our timeline:
tnew = (t ? ?f, s??) ? ?f, s?. Rather than calculat-
ing the score of the full timeline, we can save time by
using only the relevant fluents in ts? . For example,
if our fluent is the has spouse fluent for Jon Stew-
art, we include all the fluents involving Jon Stewart
and any relevant linked entities. In this case, we also
include all the fluents for Tracey McShane.
Each round of RRHC consists of two passes
through the fluents we are inferring: An argmax
pass followed by a randomization pass where we
randomly choose spans for a random fraction of the
fluents. When finished, we return the highest scor-
ing timeline seen during either of these passes.
4 Experiments
We evaluate our models (CC and JCC) according to
their ability to predict the temporal bounds of flu-
ents from Freebase. This is similar to the Diagnostic
Track in the Temporal KBP task, where gold rela-
tions are provided as inputs. We provide three base-
lines for comparison, discussed further in ?4.2. To
form our database, we scraped a random sample of
people and organization entities from Freebase us-
ing their API. Since our consistency model has lim-
ited effect if entities do not have any links to other
entities, we restrict our attention to entities linked
to at least one other entity ? this eliminates a large
portion of possible entities. Our corpus7 consists of
8,450 entities for training, 1,072 for development,
and 1,067 for test. Entities have approximately 2.0
fluents on average.
From experiments on the development set, we set
the relative strength of the consistency component
? = 10. For the JCC model, we perform three runs
for each experiment with different random seeds.
Each experiment performs 10 rounds of RRHC,8 ini-
tializing from an empty timeline.
4.1 Evaluation metric
Our evaluation metric is adapted from the Temporal
KBP metric (Ji et al2011) to work with 2-tuples
for temporal representations rather than the 4-tuples
in Temporal KBP. The metric favors tighter bounds
on fluents while giving partial credit. All dates need
to be given at day resolution. Thus, for gold fluents
with only year- or month-level resolution, we treat
them as their earliest (for starts) or latest (for ends)
possible day. To score a boundary, we take the dif-
ference between the predicted and gold values: If
they?re both unbounded (??), the boundary?s score
is 1. If only one is unbounded, the score is 0. If
both are finite, the score is 1/(1 + |d|) where d is
the difference between the values in years. To score
a fluent, we average the scores of its start and end
boundaries. In rare cases, we have multiple spans
for the same relation (e.g., Elizabeth Taylor married
Richard Burton twice). In these cases, we give sys-
tems the benefit of the doubt and greedily align flu-
ents in such a way as to maximize the metric. The
total metric computes the score of each fluent di-
vided by the number of fluents. The official metric
includes precision and recall components, but since
our setup provides gold relations, our precision and
recall are be equal. This allows us to report a single
number.
4.2 Baselines and oracle
The simplest baseline is the null baseline, proposed
in Surdeanu et al2011). This baseline assumes that
all fluents are unbounded in their spans. The purpose
7http://nlp.stanford.edu/
?
mcclosky/data/
freebase-temporal-relations.tar.gz
8There was no significant difference in accuracy between
running 10 and 200 rounds of RRHC.
878
Figure 2: Performance of models and baselines on devel-
opment data while varying amount of training data. Not
pictured: The null baseline at 58.8%.
of this baseline is primarily to show the approximate
minimal value for the temporal metric.
We provide two other baselines to describe heuris-
tic methods of aggregating the hard decisions from
the classifier function C learned in ?3.2. These are
unlike the CC model which uses the soft decisions
of C. Both of these baselines maintain lists of pos-
sible starts and ends for each fluent. If the classifier
assigns START AND END, we add the candidate tem-
poral expression to both. The first baseline, basic
aggregation, is along the same lines as the aggrega-
tion method used in Artiles et al2011), a state-of-
the-art system. Our baseline assigns the earliest start
and the latest end as the bounds for each fluent, as-
signing ?? for empty lists. The second baseline,
basic aggregation (modes), is the same except that it
uses the mode from each list.
To determine the best possible score given our
temporal expression retrieval system, we calculate
the oracle score by assigning each fluent the span
which maximizes the temporal metric. The oracle
score can differ from a perfect score since we can
only use candidate temporal expressions as values
for a fluent if (a) mentions of the fluent are retriev-
able in our source documents, (b) the temporal ex-
pression mention appears nearby, and (c) our tem-
poral expression extractor is able to recognize it cor-
rectly. Nevertheless, it is still a reasonable upper
bound in our setting.
Model Dev Test
Oracle 78.1 75.2
Joint Classifier with Consistency 76.1 72.2
Combined Classifier 75.8 71.5
Basic aggregation (modes) 75.3 71.2
Basic aggregation 74.7 70.5
null baseline 58.8 55.6
Table 2: Performance of systems on development and test
divisions. The Joint classifier with Consistency is the av-
erage of three runs with negligible variance (? ? 0.02).
4.3 Results
We present the performance of our models, base-
lines, and the oracle in Figure 2 while varying the
percentage of training entities. The JCC model
(76.1% on development with 100% training enti-
ties) is consistently the best non-oracle system. Its
gains are larger when the amount of training data is
low. This is presumably because the classifier suf-
fers from insufficient data and the consistency com-
ponent is able to learn consistency rules to recover
from this. Both the CC and JCC models outperform
the basic aggregation models. This shows the value
of incorporating all marginal probabilities. On the
test set (Table 2), the JCC model performs even bet-
ter in comparison to the simple models, despite the
test set being clearly more difficult than the develop-
ment set. In this case, the JCC achieves a 36% error
reduction over the basic aggregation model.9 On the
official KBP entities, the oracle score is 92%. Since
we use a different set of entities, there is a mismatch
between our entities and the source documents re-
sulting in a lower oracle score. Addressing this is
future work.
5 Discussion
Table 3 shows the performance of four systems
and baselines on individual fluent types. The JCC
model derives most of its improvement from the
two lifespan fluents and other high frequency flu-
ents. The lifespan fluents provide the most room
for improvement since they tend to contain non-null
values a reasonable amount of the time (note how
these relations have a large gap between their ora-
9This counts errors relative to the oracle score since we treat
the retrieval system as fixed in this work.
879
Model
Fluent Count null Basic Basic (modes) CC JCC Oracle
organization: lifespan 266 49.2 71.0 70.7 71.1 71.7 73.4
organization: top employees 150 88.0 88.0 88.0 88.0 88.0 88.3
organization: founders 31 0.0 5.4 5.4 10.8 11.1 16.3
organization: acquires company 14 21.4 21.4 21.4 21.4 21.4 38.5
person: lifespan 806 28.6 63.1 64.6 65.6 66.1 69.1
person: has spouse 582 92.2 92.1 92.1 92.2 92.3 93.1
person: attends school 107 97.7 97.7 97.7 97.7 98.1 98.1
person: has job 85 78.8 79.4 79.4 78.8 78.8 80.3
person: holds government position 45 16.7 19.7 19.7 19.7 19.7 25.1
person: romantic partner 5 50.0 52.9 52.9 52.9 52.9 71.2
Table 3: Fluent-level performance of models and baselines on development data. Scores are calculated with the
temporal metric. CC stands for Combined Classifier and JCC for Joint Classifier with Consistency. The JCC model
obtains most of its benefits on the two lifespan relations. For attends school, it is the only system able to achieve
oracle-level performance. The null baseline is especially strong for several fluents since these tend to be unbounded or
(more likely) missing their values in Freebase. The two basic aggregation models differ primarily on their predictions
for the lifespan fluents.
cle and null scores). Additionally, the lifespan fluent
is always present for entities while other fluents are
sparser. For attends school, JCC is the only system
able to achieve oracle-level performance. No system
improves on the null baseline for acquires company.
This is likely due to its sparsity.
Inspecting the multinomials in the consistency
component, we can see that the model learns reason-
able answers to questions such as whether an entity
?was born before getting married?? (yes: 14.8%,
no: 0.04%),10 ?died before their parents were born??
(yes: 0.3%, no: 53.7%) and ?finished a job before
starting a job (not necessarily the same one)?? (yes:
72.5%, no: 20.5%). Despite some unavoidable noise
in the data, it is clear these constraints are useful.
6 Related work
There is a large body of related work that focuses
on ordering events or classifying temporal relations
between them (Ling and Weld, 2010; Yoshikawa et
al., 2009; Chambers and Jurafsky, 2008; Mani et
al., 2006, inter alia). Much of this work uses the
Allen interval relations (Allen, 1983) which richly
describe partial orderings of fluents. We use several
of these as fluent-level question templates.
Joint inference has been applied successfully
10Percentages for ?unknown? are omitted here.
to other NLP problems (Roth and Yih, 2004;
Toutanova et al2008; Martins et al2009; Chang
et al2010; Koo et al2010; Berant et al
2011). Two recent examples in information ex-
traction include using Markov Logic for temporal
ordering (Ling and Weld, 2010) and using dual-
decomposition for event extraction (Riedel and Mc-
Callum, 2011).
Our work is closest to Temporal KBP slot filling
systems. The CUNY and UNED systems (Artiles
et al2011; Garrido et al2011) for this task used
classifiers to determine the relation between tempo-
ral expressions and fluents. These systems use the
hard decisions from the classifier and combine the
decisions by finding a span that includes all temporal
expressions. In contrast, our system uses the classi-
fier?s marginal probabilities along with the consis-
tency component to incorporate global consistency
constraints. Other participants used rule-based and
pattern matching approaches (Byrne and Dunnion,
2011; Surdeanu et al2011; Burman et al2011).
Outside of Temporal KBP, there are several works
on the task of extracting fluents from text. Wang
et al2011) which uses label propagation, a graph-
based semi-supervised method to extend positive
and negative seed examples over the graph. Taluk-
dar et al2012) apply a similar approach by ag-
gregating local classification decisions using tempo-
880
ral constraints (e.g., mutual exclusion, containment,
and succession) and joint inference. One key dif-
ference is that their constraints are included as input
rather than learned by the system.
7 Conclusion and future Work
Joint inference can be effectively applied to the task
of inferring timelines about named entities. Rather
than using hard coded heuristics, our model learns
and applies consistency constraints which capture
inter-entity and cross-entity rules. Simple inference
techniques such as random-restart hillclimbing score
well and run efficiently. Both of our models (CC and
JCC) obtain a substantial error reductions over sim-
pler heuristics-based consistency approaches.
The overall framework can easily be applied to
other information extraction tasks. Rather than list-
ing rules for consistency, these can be learned and
enforced via joint inference. While simple joint in-
ference methods such as random-restart hillclimb-
ing and Gibbs sampling worked well in our case,
more complex inference methods may be required
with more elaborate constraints.
A prime direction for future work is combining
our model with a probabilistic relation extraction
system. This could be accomplished by using the
marginal probabilities on the extracted relations and
multiplying them with the probabilities from the
classifier and consistency components. Inference
would require an additional step which could add or
drop candidate fluents. Furthermore, the consistency
component can be extended with new question types
to incorporate non-temporal constraints as well.
Acknowledgments
The authors would like to thank the Stanford NLP
group (with special thanks to Gabor Angeli and
Mihai Surdeanu), William Headden, Micha Elsner,
Pontus Stenetorp, and our anonymous reviewers for
their helpful comments and feedback.
We gratefully acknowledge the support of
Defense Advanced Research Projects Agency
(DARPA) Machine Reading Program under Air
Force Research Laboratory (AFRL) prime contract
no. FA8750-09-C-0181. Any opinions, findings,
and conclusion or recommendations expressed in
this material are those of the author(s) and do not
necessarily reflect the view of the DARPA, AFRL,
or the US government.
References
James F. Allen. 1983. Maintaining knowledge about
temporal intervals. Communications of the ACM,
26(11):832?843.
Javier Artiles, Qi Li, Taylor Cassidy, Suzanne Tamang,
and Heng Ji. 2011. CUNY BLENDER TAC-
KBP2011 Temporal Slot Filling System Description.
In Proceedings of Text Analysis Conference (TAC),
November.
Jonathan Berant, Ido Dagan, and Jacob Goldberger.
2011. Global learning of typed entailment rules. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 610?619, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.
Amev Burman, Arun Jayapal, Sathish Kannan, Madhu
Kavilikatta, Ayman Alhelbawy, Leon Derczynski, and
Robert Gaizauskas. 2011. USFD at KBP 2011: Entity
Linking, Slot Filling and Temporal Bounding. In Pro-
ceedings of Text Analysis Conference (TAC), Novem-
ber.
Lorna Byrne and John Dunnion. 2011. UCD IIRG at
TAC 2011. In Proceedings of Text Analysis Confer-
ence (TAC), November.
Nathanael Chambers and Dan Jurafsky. 2008. Jointly
combining implicit constraints improves temporal or-
dering. In Proceedings of the Conference on Empir-
ical Methods in Natural Language Processing, pages
698?706. Association for Computational Linguistics.
Angel X. Chang and Christopher D. Manning. 2012.
SUTIME: A library for recognizing and normalizing
time expressions. In 8th International Conference on
Language Resources and Evaluation (LREC 2012),
May.
Ming-Wei Chang, Dan Goldwasser, Dan Roth, and Vivek
Srikumar. 2010. Discriminative learning over con-
strained latent representations. In Human Language
Technologies: The 2010 Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics, pages 429?437. Association for
Computational Linguistics.
Mark Craven and Johan Kumlien. 1999. Constructing
biological knowledge bases by extracting information
from text sources. In Proceedings of the Seventh Inter-
national Conference on Intelligent Systems for Molec-
ular Biology, pages 77?86. Heidelberg, Germany.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The Stanford typed dependencies repre-
sentation. In Proceedings of the COLING Workshop
881
on Cross-framework and Cross-domain Parser Evalu-
ation.
Jenny R. Finkel, Teg Grenager, and Christopher D. Man-
ning. 2005. Incorporating non-local information into
information extraction systems by Gibbs sampling. In
Proceedings of the 43rd Annual Meeting on Associ-
ation for Computational Linguistics, pages 363?370.
Association for Computational Linguistics.
Guillermo Garrido, Bernardo Cabaleiro, Anselmo Pe nas,
Alvaro Rodrigo, and Damiano Spina. 2011. A distant
supervised learning system for the TAC-KBP Slot Fill-
ing and Temporal Slot Filling Tasks. In Proceedings of
Text Analysis Conference (TAC), November.
Heng Ji, Ralph Grishman, and Hoa Trang Dang. 2011.
Overview of the TAC 2011 Knowledge Base Popula-
tion track. In Proceedings of Text Analysis Conference
(TAC), November.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of the 41st An-
nual Meeting on Association for Computational Lin-
guistics, pages 423?430. Association for Computa-
tional Linguistics.
Terry Koo, Alexander M. Rush, Michael Collins, Tommi
Jaakkola, and David Sontag. 2010. Dual decompo-
sition for parsing with non-projective head automata.
In Proceedings of the 2010 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1288?1298. Association for Computational Linguis-
tics.
Heeyoung Lee, Yves Peirsman, Angel X. Chang,
Nathanael Chambers, Mihai Surdeanu, and Dan Juraf-
sky. 2011. Stanford?s Multi-Pass Sieve Coreference
Resolution System at the CoNLL-2011 Shared Task.
In CoNLL 2011, page 28.
Xiao Ling and Daniel S. Weld. 2010. Temporal infor-
mation extraction. In Proceedings of the Twenty Fifth
National Conference on Artificial Intelligence.
Inderjeet Mani, Marc Verhagen, Ben Wellner, Chong Min
Lee, and James Pustejovsky. 2006. Machine learning
of temporal relations. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and the 44th Annual Meeting of the Association for
Computational Linguistics, pages 753?760. Associa-
tion for Computational Linguistics.
Andre? F. T. Martins, Noah A. Smith, and Eric P. Xing.
2009. Concise integer linear programming formula-
tions for dependency parsing. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP, pages
342?350. Association for Computational Linguistics.
Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky.
2009. Distant supervision for relation extraction with-
out labeled data. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the 4th
International Joint Conference on Natural Language
Processing of the AFNLP, pages 1003?1011. Associa-
tion for Computational Linguistics.
Sebastian Riedel and Andrew McCallum. 2011. Fast and
robust joint models for biomedical event extraction. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing (EMNLP ?11), July.
Dan Roth and Wen-tau Yih. 2004. A linear program-
ming formulation for global inference in natural lan-
guage tasks. In Hwee Tou Ng and Ellen Riloff, editors,
HLT-NAACL 2004 Workshop: Eighth Conference on
Computational Natural Language Learning (CoNLL-
2004), pages 1?8, Boston, Massachusetts, USA, May
6 - May 7. Association for Computational Linguistics.
Mihai Surdeanu, Sonal Gupta, John Bauer, David Mc-
Closky, Angel X. Chang, Valentin I. Spitkovsky, and
Christopher D. Manning. 2011. Stanford?s Distantly-
Supervised Slot-Filling System. In Proceedings of
Text Analysis Conference (TAC), November.
Partha Pratim Talukdar, Derry Wijaya, and Tom Mitchell.
2012. Coupled temporal scoping of relational facts. In
Proceedings of the Fifth ACM International Confer-
ence on Web Search and Data Mining, pages 73?82.
ACM.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Pro-
ceedings of the 2003 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics on Human Language Technology, pages 173?
180. Association for Computational Linguistics.
Kristina Toutanova, Aria Haghighi, and Christopher D.
Manning. 2008. A global joint model for semantic
role labeling. Computational Linguistics, 34(2):161?
191.
Yafang Wang, Bing Yang, Lizhen Qu, Marc Spaniol, and
Gerhard Weikum. 2011. Harvesting facts from textual
web sources by constrained label propagation. In Pro-
ceedings of the 20th ACM International Conference on
Information and Knowledge Management, pages 837?
846. ACM.
Katsumasa Yoshikawa, Sebastian Riedel, Yuji Mat-
sumoto, and Masayuki Asahara. 2009. Jointly iden-
tifying temporal relations with Markov Logic. In Pro-
ceedings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the
AFNLP, pages 405?413. Association for Computa-
tional Linguistics.
882
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 28?36,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Automatic Domain Adaptation for Parsing
David McCloskya,b
aStanford University
Stanford, CA, USA
mcclosky@stanford.edu
Eugene Charniakb
bBrown University
Providence, RI, USA
ec@cs.brown.edu
Mark Johnsonc,b
cMacquarie University
Sydney, NSW, Australia
mjohnson@science.mq.edu.au
Abstract
Current statistical parsers tend to perform well
only on their training domain and nearby gen-
res. While strong performance on a few re-
lated domains is sufficient for many situations,
it is advantageous for parsers to be able to gen-
eralize to a wide variety of domains. When
parsing document collections involving het-
erogeneous domains (e.g. the web), the op-
timal parsing model for each document is typ-
ically not obvious. We study this problem as
a new task ? multiple source parser adapta-
tion. Our system trains on corpora from many
different domains. It learns not only statistics
of those domains but quantitative measures of
domain differences and how those differences
affect parsing accuracy. Given a specific tar-
get text, the resulting system proposes linear
combinations of parsing models trained on the
source corpora. Tested across six domains,
our system outperforms all non-oracle base-
lines including the best domain-independent
parsing model. Thus, we are able to demon-
strate the value of customizing parsing models
to specific domains.
1 Introduction
In statistical parsing literature, it is common to see
parsers trained and tested on the same textual do-
main (Charniak and Johnson, 2005; McClosky et
al., 2006a; Petrov and Klein, 2007; Carreras et al,
2008; Suzuki et al, 2009, among others). Unfor-
tunately, the performance of these systems degrades
on sentences drawn from a different domain. This
issue can be seen across different parsing models
(Sekine, 1997; Gildea, 2001; Bacchiani et al, 2006;
McClosky et al, 2006b). Given that some aspects of
syntax are domain dependent (typically at the lexi-
cal level), single parsing models tend to not perform
well across all domains (see Table 1). Thus, statis-
tical parsers inevitably learn some domain-specific
properties in addition to the more general properties
of a language?s syntax. Recently, Daume? III (2007)
and Finkel and Manning (2009) showed techniques
for training models that attempt to separate domain-
specific and general properties. However, even when
given models for multiple training domains, it is not
straightforward to determine which model performs
best on an arbitrary piece of novel text.
This problem comes to the fore when one wants
to parse document collections where each document
is potentially its own domain. This shows up par-
ticularly when parsing the web. Recently, there
has been much interest in applying parsers to the
web for the purposes of information extraction and
other forms of analysis (c.f. the CLSP 2009 summer
workshop ?Parsing the Web: Large-Scale Syntactic
Processing?). The scale of the web demands an au-
tomatic solution to the domain detection and adap-
tation problems. Furthermore, it is not obvious that
human annotators can determine the optimal parsing
models for each web page.
Our goal is to study this exact problem. We create
a new parsing task, multiple source parser adapta-
tion, designed to capture cross-domain performance
along with evaluation metrics and baselines. Our
new task involves training parsing models on labeled
and unlabeled corpora from a variety of domains
(source domains). This is in contrast to standard do-
main adaptation tasks where there is a single source
domain. For evaluation, one is given a text (target
text) but not the identity of its domain. The chal-
lenge is determining how to best use the available
28
Test
Train BNC GENIA BROWN SWBD ETT WSJ Average
GENIA 66.3 83.6 64.6 51.6 69.0 66.6 67.0
BROWN 81.0 71.5 86.3 79.0 80.9 80.6 79.9
SWBD 70.8 62.9 75.5 89.0 75.9 69.1 73.9
ETT 72.7 65.3 75.4 75.2 81.9 73.2 73.9
WSJ 82.5 74.9 83.8 78.5 83.4 89.0 82.0
Table 1: Cross-domain f-score performance of the Charniak (2000) parser. Averages are macro-averages.
Performance drops as training and test domains diverge. On average, the WSJ model is the most accurate.
resources from training to maximize accuracy across
multiple target texts.
Broadly put, we model how domain differences
influence parsing accuracy. This is done by taking
several computational measures of domain differ-
ences between the target text and each source do-
main. We use these features in a simple linear re-
gression model which is trained to predict the accu-
racy of a parsing model (or, more generally, a mix-
ture of parsing models) on a target text. To parse
the target text, one simply uses the mixture of pars-
ing models with the highest predicted accuracy. We
show that our method is able to predict these accu-
racies quite well and thus effectively rank parsing
models formed from mixtures of labeled and auto-
matically labeled corpora.
In Section 2, we detail recent work on similar
tasks. Our regression-based approach is covered in
Section 3. We describe an evaluation strategy in Sec-
tion 4. Section 5 presents new baselines which are
intended to give a sense of current approaches and
their limitations. The results of our experiments are
detailed in Section 6 where we show that our system
outperforms all non-oracle baselines. We conclude
with a discussion and future work (Section 7).
2 Related work
The closest work to ours is Plank and Sima?an
(2008), where unlabeled text is used to group sen-
tences from WSJ into subdomains. The authors cre-
ate a model for each subdomain which weights trees
from its subdomain more highly than others. Given
the domain specific models, they consider different
parse combination strategies. Unfortunately, these
methods do not yield a statistically significant im-
provement.
Multiple source domain adaptation has been done
for other tasks (e.g. classification in (Blitzer et
al., 2007; Daume? III, 2007; Dredze and Cram-
mer, 2008)) and is related to multitask learning.
Daume? III (2007) shows that an extremely sim-
ple method delivers solid performance on a num-
ber of domain adaptation classification tasks. This is
achieved by making a copy of each feature for each
source domain plus the ?general? pseudodomain
(for capturing domain independent features). This
allows the classifier to directly model which features
are domain-specific. Finkel and Manning (2009)
demonstrate the hierarchical Bayesian extension of
this where domain-specific models draw from a gen-
eral base distribution. This is applied to classifica-
tion (named entity recognition) as well as depen-
dency parsing. These works describe how to train
models in many different domains but sidestep the
problem of domain detection. Thus, our work is or-
thogonal to theirs.
Our domain detection strategy draws on work in
parser accuracy prediction (Ravi et al, 2008; Kawa-
hara and Uchimoto, 2008). These works aim to pre-
dict the parser performance on a given target sen-
tence. Ravi et al (2008) frame this as a regression
problem. Kawahara and Uchimoto (2008) treat it
as a binary classification task and predict whether
a specific parse is at a certain level of accuracy or
higher. Ravi et al (2008) show that their system
can be used to return a ranking over different parsing
models which we extend to the multiple domain set-
ting. They also demonstrate that training their model
on WSJ allows them to accurately predict parsing
accuracy on the BROWN corpus. In contrast, our
models are trained over multiple domains to model
which factors influence cross-domain performance.
29
3 Approach
We start with the assumption that all target domains
are mixtures of our source domains.1 Intuitively,
these mixtures should give higher probability mass
to more similar source domains. This raises the
question of how to measure the similarity between
domains. Our method uses multiple complemen-
tary similarity measures between the target and each
source. We feed these similarity measures into a re-
gression model which learns how domain dissimi-
larities hurt parse accuracy. Thus, to parse a target
domain, we need only find the input that maximizes
the regression function ? that is, the highest scoring
mixture of source domains. Our system is similar to
Ravi et al (2008) in that both use regression to pre-
dict f-scores and some of the features are related.
3.1 Features
Our features are designed to help the regression
model determine if a particular source domain mix-
ture is well suited for a target domain as well as the
quality of a source domain mixture. While we ex-
plored a large number of features, we present here
only the three that were chosen by our feature selec-
tion method (Section 6.2).
Two of our features, COSINETOP50 and UN-
KWORDS, are designed to approximate how simi-
lar the target domain is to a specific source domain.
Only the surface form of the target text and auto-
matic analyses are available (e.g. we can tag or parse
the target text, but cannot use gold tags or trees).
Relative word frequencies are an important in-
dicator of domain. Cosine similarity uses a spa-
tial representation to summarize the word frequen-
cies in a corpus as a single vector. A common
method is to represent each corpus as a vector of
frequencies of the k most frequent words (Schu?tze,
1995). This method assigns high similarity to do-
mains with a large amount of overlap in the high-
frequency vocabulary items. We experimented with
several orders of magnitude for k (our feature selec-
tion method later chose k = 50 ? see Section 6.2).
Our second feature for comparing domains, UN-
1This may seem like a major limitation, but as we will show
later, our method works quite well at incorporating self-trained
(automatically parsed) corpora which can typically be obtained
for any domain.
KWORDS, returns the percentage of words in one
domain which never appear in the other domain.
This can be done on the word type or token level.
We opt for tokens since unknown words pose prob-
lems for parsing each time they occur. UNKWORDS
provides the percentage of words in the source
domain that are never seen in the target domain.
Whereas COSINETOP50 examines how similar the
high frequency words are from one domain, UN-
KWORDS tends to focus on the overlap of low fre-
quency words.
As described, COSINETOP50 and UNKWORDS
are functions only of two source domains and do not
take the mixing weights of source domains into ac-
count. We experimented with several methods of in-
corporating mixing weights into the feature value.
In practice, the one which worked best for us is to
divide the mixture weight of the source domain by
the raw feature value. This has the nice property that
when a source is not used, the adjusted feature value
is zero regardless of the raw feature value.
From pilot studies, we learned that a uniform mix-
ture of available source domains gave strong results
(further details on this in Section 5). Our last feature,
ENTROPY, is intended to let the regression system
leverage this and measures the entropy of the distri-
bution over source domains. This provides a sense
of uniformity.
3.2 Predicting cross-domain accuracy
For a given source domain mixture, we can create
a parsing model by linearly interpolating the pars-
ing model statistics from each source domain. The
key component of our approach is a domain-aware
linear regression model which predicts how well a
specific parsing model will do on a given target text.
The linear regressor is given values from the three
features from the previous section (COSINETOP50,
UNKWORDS, and ENTROPY) and returns an esti-
mate of the f-score the parsing model would achieve
the target text.
Training data for the regressor consists of ex-
amples of source domain mixtures and their ac-
tual f-scores on target texts. To produce this, we
randomly sampled source domain mixtures, created
parsing models for those mixtures, and then evalu-
ated the parsing models on all of our target texts.
We used a simple technique for randomly sam-
30
0 200 400 600 800 1000
Number of mixed parsing model samples
84.0
84.5
85.0
85.5
86.0
86.5
87.0
87.5
o
r
a
c
l
e
 
f
-
s
c
o
r
e
Figure 1: Cumulative oracle f-score (averaged over
all target domains) as more models are randomly
sampled. Most of the improvement comes the first
200 samples indicating that our samples seem to be
sufficient to cover the space of good source domain
mixtures.
pling source domain mixtures. First, we sample the
number of source domains to use. We draw values
from an exponential distribution and take their inte-
ger value until we obtain a number between two and
the number of source domains. This is parametrized
so that we typically only use a few corpora but still
have some chance of using all of them. Once we
know the number of source domains, we sample
their identities uniformly at random without replace-
ment from the list of all source domains. Finally,
we sample the weights for the source domains uni-
formly from a simplex. The dimension of the sim-
plex is the same as the number of source domains
so we end up with a probability distribution over the
sampled source domains.
In total, we sampled 1,040 source domain mix-
tures. We evaluated each of these source domain
mixtures on the six target domains giving us 6,240
data points in total. One may be concerned that
this is insufficient to cover the large space of source
domain mixtures. However, we show in Figure 1
that only about 200 samples are sufficient to achieve
good oracle performance2 in practice.
2We calculate this by picking the best available model for
each target domain and taking the average of their f-scores.
Train Test
Source Target Source Target
C \ {t} C \ {t} C \ {t} {t}
(a) Out-of-domain evaluation
Train Test
Source Target Source Target
C C \ {t} C {t}
(b) In-domain evaluation
Table 2: List of domains allowed in single round of
evaluation. In each round, the evaluation corpus is t.
C is the set of all target domains.
4 Evaluation
Multiple-source domain adaptation is a new task for
parsing and thus some thought must be given to eval-
uation methodology. We describe two evaluation
scenarios which differ in how foreign the target text
is from our source domains. Schemas for these eval-
uation scenarios are shown in Table 2. Note that
training and testing here refer to training and testing
of our regression model, not the parsing models.
In the first scenario, out-of-domain evaluation,
one target domain is completely removed from con-
sideration and only used to evaluate proposed mod-
els at test time. The regressor is trained on training
points that use any of the remaining corpora, C\{t},
as sources or targets. For example, if t = WSJ, we
can train the regressor on all data points which don?t
use WSJ (or any self-trained corpora derived from
WSJ) as a source or target domain. At test time, we
are given the text of WSJ?s test set. From this, our
system creates a parsing model using the remaining
available corpora for parsing the raw WSJ text.
This evaluation scenario is intended to evaluate
how well our system can adapt to an entirely new
domain with only raw text from the new domain
(for example, parsing biomedical text when none
is available in our list of source domains). Ide-
ally, we would have a large number of web pages
or other documents from other domains which we
could use solely for evaluation. Unfortunately, at
this time, only a handful of domains have been an-
notated with constituency structures under the same
This can pick different models for each target domain.
31
annotation guidelines. Instead, we hold out each
hand-annotated domain, t, (including any automat-
ically parsed corpora derived from that source do-
main) as a test set in a round-robin fashion.3 For
each round of the round robin we obtain an f-score
and we report the mean and variance of the f-scores
for each model.
The second scenario, in-domain evaluation, al-
lows the target domain, t, to be used as a source
domain in training but not as a target domain. This
is intended to evaluate the situation where the target
domain is not actually that different from our source
domains. The in-domain evaluation can approxi-
mate how our system would perform when, for ex-
ample, we have WSJ as a source domain and the tar-
get text is news from a source other than WSJ. Thus,
our model still has to learn that WSJ and the North
American News Text corpus (NANC) are good for
parsing news text like WSJ without seeing any direct
evaluations of the sort (WSJ and NANC can be used
in models which are evaluated on all other corpora,
though).
5 Baselines
Given that this is a new task for parsing, we needed
to create baselines which demonstrate the current
approaches to multiple-source domain adaptation.
One approach is to take all available corpora and
mix them together uniformly.4 The UNIFORM base-
line does exactly this using the available hand-built
training corpora. SELF-TRAINED UNIFORM uses
self-trained corpora as well. In the out-of-domain
scenario, these exclude the held out domain, but in
the in-domain setting, the held out domain is in-
cluded. These baselines are similar to the ALL and
WEIGHTED baselines in Daume? III (2007).
Another simple baseline is to use the same pars-
ing model regardless of target domain. This is how
large heterogeneous document collections are typi-
cally parsed currently. We use the WSJ corpus since
it is the best single corpus for parsing all six target
domains (see Table 1). We refer to this baseline as
FIXED SET: WSJ. In the out-of-domain scenario,
we fall back to SELF-TRAINED UNIFORM when the
3Thus, the schemas in Table 2 are schemas for each round.
4Accounting for size so that the larger corpora don?t over-
whelm the smaller ones.
target domain is WSJ while the in-domain scenario
uses the WSJ model throughout.
There are several interesting oracle baselines as
well which serve to measure the limits of our ap-
proach. These baselines examine the resulting
f-scores of models and pick the best model accord-
ing to some criteria. The first oracle baseline is
BEST SINGLE CORPUS which parses each corpus
with the source domain that maximizes performance
on the target domain. In almost all cases, this base-
line selects each corpus to parse itself.
Our second oracle baseline, BEST SEEN, chooses
the best parsing model from all those explored for
each test set. Recall that while training the regres-
sion model in Section 3.2, we needed to explore
many possible source domain mixtures to approxi-
mate the complete space of mixed parsing models.
To the extent that we can fully explore the space of
mixed parsing models, this baseline represents an
upper bound for model mixing approaches. Since
fully exploring the space of possible weightings is
intractable, it is not a true upper bound. While it
is theoretically possible to beat this pseudo-upper
bound, (indeed, this is the mark of a good domain
detection system) it is far from easy. We provide
BEST SINGLE CORPUS and BEST SEEN for both
in-domain and out-of-domain scenarios. The out-of-
domain scenario restricts the set of possible models
to those not including the target domain.
Finally, we searched for the BEST OVERALL
MODEL. This is the model with the highest aver-
age f-score across all six target domains. This base-
line can be thought of as an oracle version of FIXED
SET: WSJ and demonstrates the limit of using a sin-
gle parsing model regardless of target domain. Natu-
rally, the very nature of this baseline places it only in
the in-domain evaluation scenario. Since it was able
to select the model according to f-scores on our six
target domains, its performance on domains outside
that set is not guaranteed.
To provide a better sense of the space of mixed
parsing models, we also provide the WORST SEEN
baseline which picks the worst model available for a
specific target corpus.5
5This turns out to be GENIA for all corpora other than GENIA
and SWBD when the target domain is GENIA.
32
6 Experiments
Our experiments use the Charniak (2000) generative
parser. We describe the corpora used in our nine
source and six target domains in Section 6.1. In Sec-
tion 6.2, we provide a greedy strategy for picking
features to include in our regression model. The re-
sults of our experiments are in Section 6.3.
6.1 Corpora
We aimed to include as many different domains as
possible annotated under compatible schemes. We
also tried to include human-annotated corpora and
automatically labeled corpora (self-trained corpora
as in McClosky et al (2006a) which have been
shown to work well across domains). Our final
set includes text from news (WSJ, NANC), broad-
cast news (ETT), literature (BROWN, GUTENBERG),
biomedical (GENIA, MEDLINE), spontaneous speech
(SWBD), and the British National Corpus (BNC). In
our experiments, self-trained corpora cannot be used
as target domains since we lack gold annotations and
BNC is not used as a source domain due to its size.
An overview of our corpora is shown in Table 3.
We use news articles portion of the Wall Street
Journal corpus (WSJ) from the Penn Treebank (Mar-
cus et al, 1993) in conjunction with the self-trained
North American News Text Corpus (NANC, Graff
(1995)). The English Translation Treebank, ETT
(Bies, 2007), is the translation6 of broadcast news
in Arabic. For literature, we use the BROWN cor-
pus (Francis and Kuc?era, 1979) and the same di-
vision as (Gildea, 2001; Bacchiani et al, 2006;
McClosky et al, 2006b). We also use raw sen-
tences which we downloaded from Project Guten-
berg7 as a self-trained corpus. The Switchboard cor-
pus (SWBD) consists of transcribed telephone con-
versations. While the original trees include disflu-
ency information, we assume our speech corpora
have had speech repairs excised (e.g. using a sys-
tem such as Johnson et al (2004)). Our biomedi-
cal data comes from the GENIA treebank8 (Tateisi
et al, 2005), a corpus of abstracts from the Med-
line database.9 We downloaded additional sentences
6The transcription and translation were done by humans.
7http://gutenberg.org/
8http://www-tsujii.is.s.u-tokyo.ac.jp/
GENIA/
9http://www.ncbi.nlm.nih.gov/PubMed/
from Medline for our self-trained MEDLINE corpus.
Unlike the other two self-trained corpora, we include
two versions of MEDLINE. These differ on whether
they were parsed using GENIA or WSJ as a base
model to study the effect on cross-domain perfor-
mance. Finally, we use a small number of sentences
from the British National Corpus (BNC) (Foster and
van Genabith, 2008).10 The sentences were chosen
randomly, so each one is potentially from a different
domain. On the other hand, BNC can be thought of
as its own domain in that it contains significant lex-
ical differences from the American English used in
our other corpora.
We preprocessed the corpora to standardize many
of the annotation differences. Thus, our results on
them may be slightly different than other works on
these corpora. Nevertheless, these changes should
not significantly impact overall the performance.
6.2 Feature selection
While our final model uses only three features, we
considered many other possible features (not de-
scribed due to space constraints). In order to explore
these without hill climbing on our test data, we cre-
ated a round-robin tuning scenario. Since the out-
of-domain evaluation scenario holds out one target
domain, this gives us six test evaluation rounds. For
each of these six rounds, we hold out one of the re-
maining five target domains for tuning. This gives
us 30 tuning evaluation rounds and we pick our fea-
tures to optimize our aggregate performance over all
of them. A model that performs well in this situation
has proven that it has useful features which transfer
to unknown target domains.
The next step is to determine the loss function
to minimize. Our primary guide is oracle f-score
loss which we determine as follows. We take all
test data points (i.e. mixed parsing models evalu-
ated on the target domain) and predict their f-scores
with our model. In particular for this measure, we
are interested in the point with the highest predicted
f-score. We take its actual f-score which we call
the candidate f-score. When tuning, we know the
true f-scores of all test points. The difference be-
tween the highest f-score (the oracle f-score for
10http://nclt.computing.dcu.ie/
?
jfoster/
resources/, downloaded January 8th, 2009.
33
Corpus Source? Target? Average length Train Tune Test
BNC ? 28.3 ? ? 1,000
BROWN ? ? 20.0 19,786 2,082 2,439
ETT ? ? 25.6 2,639 1,029 1,166
GENIA ? ? 27.5 14,326 1,361 1,360
MEDLINE ? 27.2 278,192 ? ?
SWBD ? ? 9.2 92,536 5,895 6,051
WSJ ? ? 25.5 39,832 1,346 2,416
NANC ? 23.2 915,794 ? ?
GUTENBERG ? 26.2 689,782 ? ?
MEDLINE ? 27.2 278,192 ? ?
Table 3: List of source and target domains, sizes of each division in trees, and average sentence length.
Indented rows indicate self-trained corpora parsed using the non-indented row as a base parser.
this dataset) and the candidate f-score is the oracle
f-score loss. Ties need to be handled correctly to
avoid degenerate models.11 If there is a tie for high-
est predicted f-score, the candidate f-score is the
one with the lowest actual f-score. This approach
is conservative but ensures that regression models
which give everything the same predicted f-score do
not receive zero oracle f-score loss.
Armed with a tuning regime and a loss function,
we created a procedure to pick the combination of
features to use. We used a parallelized best-first
search procedure. At each round, it expanded the
current best set of features by adding or removing
each feature where ?best? was determined by the loss
function. We explored over 6,000 settings, though
the best setting of (UNKWORDS, COSINETOP50,
ENTROPY) was found within the first 200 settings
explored. The best setting obtains an oracle f-score
loss of 0.37 and a root mean squared error of 0.48
? these numbers are quite low and show the high
accuracy of our regression model (similar to those
in Ravi et al (2008)). Additionally, the features are
complementary in that UNKWORDS focuses on low
frequency words whereas COSINETOP50 looks only
at high frequency words and ENTROPY functions as
a regularizer.
6.3 Results
We present an overview of our final results for out-
of-domain and in-domain evaluation in Table 4. The
11For example, regression models which assign every parsing
model the same f-score.
results include the f-score macro-averaged over the
six target domains and their standard deviation.
In both situations, the FIXED SET: WSJ baseline
performs fairly poorly. Not surprisingly, assuming
all of our target domains are close enough to WSJ
works badly for our set of target domains and it
does particularly poorly on SWBD and GENIA. On
average, the UNIFORM baseline does slightly bet-
ter for out-of-domain and over 3% better for in-
domain. UNIFORM actually does fairly well on out-
of-domain except on GENIA. In general, using more
source domains is better which partially explains the
success of UNIFORM. This seems to be the case
since even if a source domain is terribly mismatched
with the target domain, it may still be able to fill
in some holes left by the other source domains. Of
course, if it overpowers more relevant domains, per-
formance may suffer. The SELF-TRAINED UNI-
FORM baseline uses even more source domains as
well as the largest ones. In both scenarios, this dra-
matically improves performance and is the second
best non-oracle system. This baseline provides more
evidence as to the power of self-training for improv-
ing parser adaptation. If we excluded all self-trained
corpora, our performance on this task would be sub-
stantially worse. We believe the self-trained cor-
pora are beneficial in this task since they help reduce
data sparsity of smaller corpora. The BEST SINGLE
CORPUS baseline is poor in the out-of-domain sce-
nario primarily because the actual best single corpus
is excluded by the task specification in most cases.
When we move to in-domain, this baseline improves
34
Oracle Baseline or model Average f-score
? Worst seen 62.0 ? 6.1
? Best single corpus 81.0 ? 2.9
Fixed set: WSJ 81.0 ? 3.5
Uniform 81.4 ? 3.6
Self-trained uniform 83.4 ? 2.5
Our model 84.0 ? 2.5
? Best seen 84.3 ? 2.6
(a) Out-of-domain evaluation
Oracle Baseline or model Average f-score
Fixed set: WSJ 82.0 ? 4.8
Uniform 85.4 ? 2.4
? Best single corpus 85.6 ? 2.9
Self-trained uniform 86.1 ? 2.0
? Best overall model 86.2 ? 1.9
Our model 86.9 ? 2.4
? Best seen 87.5 ? 2.1
(b) In-domain evaluation
Table 4: Baselines and final results for the two multiple-source domain adaptation evaluation scenarios.
Results include f-scores, macro-averaged over all six target domains and their standard deviations.
but is still worse than SELF-TRAINED UNIFORM on
average. It beats SELF-TRAINED UNIFORM primar-
ily on WSJ, SWBD, and GENIA indicating that these
three domains are best when not diluted by others.
By definition, the WORST SEEN baseline does terri-
bly, almost 20% worse then BEST SINGLE CORPUS.
Our model is the best non-oracle system for both
evaluation scenarios. For out-of-domain evaluation,
our system is only 0.3% worse than the BEST SEEN
models for each target domain. For the in-domain
scenario, we are within 0.6% of the BEST SEEN
models. For a sense of scale, our out-of-domain and
in-domain f-scores on WSJ are 83.1% and 89.8%
respectively. Both numbers are quite close to the
BEST SEEN baseline. Additionally, our model is
0.7% better than the BEST OVERALL MODEL. Re-
call that the BEST OVERALL MODEL is the single
model with the best performance across all six tar-
get domains.12 By beating this baseline, we show
that there is value in customizing parsing models
to the target domain. It is also interesting that the
BEST OVERALL MODEL is only marginally better
than SELF-TRAINED UNIFORM. Without any fur-
ther information about the target corpus, an unin-
formed prior appears best.
7 Discussion
We have shown that for both out-of-domain and in-
domain evaluations, our system is well adapted to
predicting the effects of domain divergence on pars-
12Somewhat surprisingly, the best overall model uses almost
entirely self-trained corpora consisting of 9.5% GUTENBERG,
60.3% NANC, 26.0% MEDLINE (by GENIA), and 4.2% SWBD.
ing accuracy. Using the parsing model with the
highest predicted f-score leads to great performance
in practice. There is a substantial benefit to doing
this over existing approaches (using the same model
for all domains or mixing all training data together
uniformly). Creating a number of domain-specific
models and mixing them together as needed is a vi-
able approach.
One can think of our system as trying to esti-
mate document-level context. Our representation of
this context is simply a distribution over our source
domains, but one can imagine more complex op-
tions such as a high-dimensional vector space. Ad-
ditionally, our model separates domain and syntax
estimation, but a future direction is to learn these
jointly. This would combine our work with (Daume?
III, 2007; Finkel and Manning, 2009).
We have focused on the Charniak (2000) parser,
the first stage in the two stage Charniak and John-
son (2005) reranking parser. Applying our methods
to other generative parsers (such as (Collins, 1999;
Petrov and Klein, 2007)) is trivial, but it is less clear
how our methods can be applied to the discrimina-
tive reranker component of the two stage parser. One
avenue of approach is to incorporate the domain rep-
resentation into the feature space, as in Daume? III
(2007) but with more complex domain information.
Acknowledgments
This work was performed while the first author was
at Brown and supported by DARPA GALE contract
HR0011-06-2-0001. We would like to thank the BLLIP
team and our anonymous reviewers for their comments.
35
References
Michiel Bacchiani, Michael Riley, Brian Roark, and
Richard Sproat. 2006. MAP adaptation of stochas-
tic grammars. Computer Speech and Language,
20(1):41?68.
Ann Bies. 2007. GALE Phase 3 Release 1 - English
Translation Treebank. Linguistic Data Consortium.
LDC2007E105.
John Blitzer, Mark Dredze, and Fernando Pereira. 2007.
Biographies, bollywood, boom-boxes and blenders:
Domain adaptation for sentiment classification. In
Association for Computational Linguistics, Prague,
Czech Republic.
Xavier Carreras, Michael Collins, and Terry Koo. 2008.
TAG, dynamic programming, and the perceptron for
efficient, feature-rich parsing. In Proceedings of
CoNLL 2008, pages 9?16, Manchester, England, Au-
gust.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and MaxEnt discriminative rerank-
ing. In Proceedings of the ACL 2005, pages 173?180.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the North American Chapter
of the ACL (NAACL), pages 132?139.
Michael Collins. 1999. Head-driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, The Uni-
versity of Pennsylvania.
Hal Daume? III. 2007. Frustratingly easy domain adap-
tation. In Proceedings of ACL 2007, Prague, Czech
Republic.
Mark Dredze and Koby Crammer. 2008. Online methods
for multi-domain learning and adaptation. In Proceed-
ings of the EMNLP 2008, pages 689?697, Honolulu,
Hawaii, October.
Jenny Rose Finkel and Christopher D. Manning. 2009.
Hierarchical bayesian domain adaptation. In Proceed-
ings of HLT-NAACL 2009, pages 602?610, Boulder,
Colorado, June.
Jennifer Foster and Josef van Genabith. 2008. Parser
evaluation and the bnc: Evaluating 4 constituency
parsers with 3 metrics. In Proceedings LREC 2008,
Marrakech, Morocco, May.
W. Nelson Francis and Henry Kuc?era. 1979. Manual
of Information to accompany a Standard Corpus of
Present-day Edited American English, for use with
Digital Computers. Brown University, Providence,
Rhode Island.
Daniel Gildea. 2001. Corpus variation and parser per-
formance. In Empirical Methods in Natural Language
Processing (EMNLP), pages 167?202.
David Graff. 1995. North American News Text Corpus.
Linguistic Data Consortium. LDC95T21.
Mark Johnson, Eugene Charniak, and Matthew Lease.
2004. An improved model for recognizing disfluen-
cies in conversational speech. In Proc. of the Rich Text
2004 Fall Workshop (RT-04F).
Daisuke Kawahara and Kiyotaka Uchimoto. 2008.
Learning reliability of parses for domain adaptation
of dependency parsing. In Third International Joint
Conference on Natural Language Processing (IJCNLP
?08).
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Comp. Linguis-
tics, 19(2):313?330.
David McClosky, Eugene Charniak, and Mark Johnson.
2006a. Effective self-training for parsing. In Proceed-
ings of HLT-NAACL 2006, pages 152?159.
David McClosky, Eugene Charniak, and Mark John-
son. 2006b. Reranking and self-training for parser
adaptation. In Proceedings of COLING-ACL 2006,
pages 337?344, Sydney, Australia, July. Association
for Computational Linguistics.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Human Language Tech-
nologies 2007: The Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics; Proceedings of the Main Conference, pages
404?411, Rochester, New York, April. Association for
Computational Linguistics.
Barbara Plank and Khalil Sima?an. 2008. Subdomain
sensitive statistical parsing using raw corpora. In
Proceedings of the Sixth International Language Re-
sources and Evaluation (LREC?08), Marrakech, Mo-
rocco, May.
Sujith Ravi, Kevin Knight, and Radu Soricut. 2008. Au-
tomatic prediction of parser accuracy. In Proceedings
of the 2008 Conference on Empirical Methods in Nat-
ural Language Processing, pages 887?896, Honolulu,
Hawaii, October. Association for Computational Lin-
guistics.
Hinrich Schu?tze. 1995. Distributional part-of-speech
tagging. In Proceedings of the 7th conference of the
EACL, pages 141?148.
Satoshi Sekine. 1997. The domain dependence of pars-
ing. In Proc. Applied Natural Language Processing
(ANLP), pages 96?102.
Jun Suzuki, Hideki Isozaki, Xavier Carreras, and Michael
Collins. 2009. An empirical study of semi-supervised
structured conditional models for dependency parsing.
In Proceedings EMNLP 2009, pages 551?560, Singa-
pore, August.
Yuka Tateisi, Akane Yakushiji, Tomoko Ohta, and
Jun?ichi Tsujii. 2005. Syntax Annotation for the GE-
NIA corpus. Proceedings of IJCNLP 2005, Compan-
ion volume, pages 222?227.
36
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1626?1635,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Event Extraction as Dependency Parsing
David McClosky, Mihai Surdeanu, and Christopher D. Manning
Department of Computer Science
Stanford University
Stanford, CA 94305
{mcclosky,mihais,manning}@stanford.edu
Abstract
Nested event structures are a common occur-
rence in both open domain and domain spe-
cific extraction tasks, e.g., a ?crime? event
can cause a ?investigation? event, which can
lead to an ?arrest? event. However, most cur-
rent approaches address event extraction with
highly local models that extract each event and
argument independently. We propose a simple
approach for the extraction of such structures
by taking the tree of event-argument relations
and using it directly as the representation in a
reranking dependency parser. This provides a
simple framework that captures global prop-
erties of both nested and flat event structures.
We explore a rich feature space that models
both the events to be parsed and context from
the original supporting text. Our approach ob-
tains competitive results in the extraction of
biomedical events from the BioNLP?09 shared
task with a F1 score of 53.5% in development
and 48.6% in testing.
1 Introduction
Event structures in open domain texts are frequently
highly complex and nested: a ?crime? event can
cause an ?investigation? event, which can lead to an
?arrest? event (Chambers and Jurafsky, 2009). The
same observation holds in specific domains. For ex-
ample, the BioNLP?09 shared task (Kim et al, 2009)
focuses on the extraction of nested biomolecular
events, where, e.g., a REGULATION event causes a
TRANSCRIPTION event (see Figure 1a for a detailed
example). Despite this observation, many state-
of-the-art supervised event extraction models still
extract events and event arguments independently,
ignoring their underlying structure (Bjo?rne et al,
2009; Miwa et al, 2010b).
In this paper, we propose a new approach for su-
pervised event extraction where we take the tree of
relations and their arguments and use it directly as
the representation in a dependency parser (rather
than conventional syntactic relations). Our approach
is conceptually simple: we first convert the origi-
nal representation of events and their arguments to
dependency trees by creating dependency arcs be-
tween event anchors (phrases that anchor events in
the supporting text) and their corresponding argu-
ments.1 Note that after conversion, only event an-
chors and entities remain. Figure 1 shows a sentence
and its converted form from the biomedical do-
main with four events: two POSITIVE REGULATION
events, anchored by the phrase ?acts as a costim-
ulatory signal,? and two TRANSCRIPTION events,
both anchored on ?gene transcription.? All events
take either protein entity mentions (PROT) or other
events as arguments. The latter is what allows for
nested event structures. Existing dependency pars-
ing models can be adapted to produce these seman-
tic structures instead of syntactic dependencies. We
built a global reranking parser model using multiple
decoders from MSTParser (McDonald et al, 2005;
McDonald et al, 2005b). The main contributions of
this paper are the following:
1. We demonstrate that parsing is an attractive ap-
proach for extracting events, both nested and
otherwise.
1While our approach only works on trees, we show how we
can handle directed acyclic graphs in Section 5.
1626
(a) Original sentence with nested events (b) After conversion to event dependencies
Figure 1: Nested events in the text fragment: ?. . . the HTLV-1 transactivator protein, tax, acts as a costim-
ulatory signal for GM-CSF and IL-2 gene transcription . . . ? Throughout this paper, bold text indicates
instances of event anchors and italicized text denotes entities (PROTEINs in the BioNLP?09 domain). Note
that in (a) there are two copies of each type of event, which are merged to single nodes in the dependency
tree (Section 3.1).
2. We propose a wide range of features for event
extraction. Our analysis indicates that fea-
tures which model the global event structure
yield considerable performance improvements,
which proves that modeling event structure
jointly is beneficial.
3. We evaluate on the biomolecular event corpus
from the the BioNLP?09 shared task and show
that our approach obtains competitive results.
2 Related Work
The pioneering work of Miller et al (1997) was
the first, to our knowledge, to propose parsing as
a framework for information extraction. They ex-
tended the syntactic annotations of the Penn Tree-
bank corpus (Marcus et al, 1993) with entity and
relation mentions specific to the MUC-7 evalua-
tion (Chinchor et al, 1997) ? e.g., EMPLOYEE OF
relations that hold between person and organization
named entities ? and then trained a generative pars-
ing model over this combined syntactic and seman-
tic representation. In the same spirit, Finkel and
Manning (2009) merged the syntactic annotations
and the named entity annotations of the OntoNotes
corpus (Hovy et al, 2006) and trained a discrimina-
tive parsing model for the joint problem of syntac-
tic parsing and named entity recognition. However,
both these works require a unified annotation of syn-
tactic and semantic elements, which is not always
feasible, and focused only on named entities and bi-
nary relations. On the other hand, our approach fo-
cuses on event structures that are nested and have
an arbitrary number of arguments. We do not need
a unified syntactic and semantic representation (but
we can and do extract features from the underlying
syntactic structure of the text).
Finkel and Manning (2009b) also proposed a
parsing model for the extraction of nested named en-
tity mentions, which, like this work, parses just the
corresponding semantic annotations. In this work,
we focus on more complex structures (events instead
of named entities) and we explore more global fea-
tures through our reranking layer.
In the biomedical domain, two recent papers pro-
posed joint models for event extraction based on
Markov logic networks (MLN) (Riedel et al, 2009;
Poon and Vanderwende, 2010). Both works propose
elegant frameworks where event anchors and argu-
ments are jointly predicted for all events in the same
sentence. One disadvantage of MLN models is the
requirement that a human expert develop domain-
specific predicates and formulas, which can be a
cumbersome process because it requires thorough
domain understanding. On the other hand, our ap-
proach maintains the joint modeling advantage, but
our model is built over simple, domain-independent
features. We also propose and analyze a richer fea-
ture space that captures more information on the
global event structure in a sentence. Furthermore,
since our approach is agnostic to the parsing model
used, it could easily be tuned for various scenarios,
e.g., models with lower inference overhead such as
shift-reduce parsers.
Our work is conceptually close to the recent
CoNLL shared tasks on semantic role labeling,
where the predicate frames were converted to se-
1627
Events	 ?to	 ?	 ?
Dependencies	 ?
Parser	 ?1	 ? ?	 ?	 ?
Reranker	 ?
Dependencies	 ?	 ?
to	 ?Events	 ?
Parser	 ?k	 ?
Dependencies	 ?	 ?
to	 ?Events	 ?
Event	 ?	 ?
Trigger	 ?
Recognizer	 ?
En?ty	 ?
Recognizer	 ?
Figure 2: Overview of the approach. Rounded rect-
angles indicate domain-independent components;
regular rectangles mark domain-specific modules;
blocks in dashed lines surround components not nec-
essary for the domain presented in this paper.
mantic dependencies between predicates and their
arguments (Surdeanu et al, 2008; Hajic et al, 2009).
In this representation the dependency structure is a
directed acyclic graph (DAG), i.e., the same node
can be an argument to multiple predicates, and there
are no explicit dependencies between predicates.
Due to this representation, all joint models proposed
for semantic role labeling handle semantic frames
independently.
3 Approach
Figure 2 summarizes our architecture. Our approach
converts the original event representation to depen-
dency trees containing both event anchors and entity
mentions, and trains a battery of parsers to recognize
these structures. The trees are built using event an-
chors predicted by a separate classifier. In this work,
we do not discuss entity recognition because in
the BioNLP?09 domain used for evaluation entities
(PROTEINs) are given (but including entity recog-
nition is an obvious extension of our model). Our
parsers are several instances of MSTParser2 (Mc-
Donald et al, 2005; McDonald et al, 2005b) con-
figured with different decoders. However, our ap-
proach is agnostic to the actual parsing models used
and could easily be adapted to other dependency
parsers. The output from the reranking parser is
2http://sourceforge.net/projects/mstparser/
converted back to the original event representation
and passed to a reranker component (Collins, 2000;
Charniak and Johnson, 2005), tailored to optimize
the task-specific evaluation metric.
Note that although we use the biomedical event
domain from the BioNLP?09 shared task to illustrate
our work, the core of our approach is almost do-
main independent. Our only constraints are that each
event mention be activated by a phrase that serves as
an event anchor, and that the event-argument struc-
tures be mapped to a dependency tree. The conver-
sion between event and dependency structures and
the reranker metric are the only domain dependent
components in our approach.
3.1 Converting between Event Structures and
Dependencies
As in previous work, we extract event structures at
sentence granularity, i.e., we ignore events which
span sentences (Bjo?rne et al, 2009; Riedel et al,
2009; Poon and Vanderwende, 2010). These form
approximately 5% of the events in the BioNLP?09
corpus. For each sentence, we convert the
BioNLP?09 event representation to a graph (repre-
senting a labeled dependency tree) as follows. The
nodes in the graph are protein entity mentions, event
anchors, and a virtual ROOT node. Thus, the only
words in this dependency tree are those which par-
ticipate in events. We create edges in the graph in
the following way. For each event anchor, we cre-
ate one link to each of its arguments labeled with the
slot name of the argument (for example, connecting
gene transcription to IL-2 with the label THEME in
Figure 1b). We link the ROOT node to each entity
that does not participate in an event using the ROOT-
LABEL dependency label. Finally, we link the ROOT
node to each top-level event anchor, (those which do
not serve as arguments to other events) again using
the ROOT-LABEL label. We follow the convention
that the source of each dependency arc is the head
while the target is the modifier.
The output of this process is a directed graph,
since a phrase can easily play a role in two or more
events. Furthermore, the graph may contain self-
referential edges (self-loops) due to related events
sharing the same anchor (example below). To guar-
antee that the output of this process is a tree, we
must post-process the above graph with the follow-
1628
ing three heuristics:
Step 1: We remove self-referential edges. An exam-
ple of these can be seen in the text ?the domain in-
teracted preferentially with underphosphorylated
TRAF2,? there are two events anchored by the same
underphosphorylated phrase, a NEGATIVE REGU-
LATION and a PHOSPHORYLATION event, and the
latter serves as a THEME argument for the former.
Due to the shared anchor, our conversion compo-
nent creates an self-referential THEME dependency.
By removing these edges, 1.5% of the events in the
training arguments are left without arguments, so we
remove them as well.
Step 2: We break structures where one argument par-
ticipates in multiple events, by keeping only the de-
pendency to the event that appears first in text. For
example, in the fragment ?by enhancing its inactiva-
tion through binding to soluble TNF-alpha receptor
type II,? the protein TNF-alpha receptor type II is
an argument in both a BINDING event (binding) and
in a NEGATIVE REGULATION event (inactivation).
As a consequence of this step, 4.7% of the events in
training are removed.
Step 3: We unify events with the same types an-
chored on the same anchor phrase. For example,
for the fragment ?Surface expression of intercellu-
lar adhesion molecule-1, P-selectin, and E-selectin,?
the BioNLP?09 annotation contains three distinct
GENE EXPRESSION events anchored on the same
phrase (expression), each having one of the proteins
as THEMEs. In such cases, we migrate all arguments
to one of the events, and remove the empty events.
21.5% of the events in training are removed in this
step (but no dependencies are lost).
Note that we do not guarantee that the resulting
tree is projective. In fact, our trees are more likely
to be non-projective than syntactic dependency trees
of English sentences, because in our representation
many nodes can be linked directly to the ROOT node.
Our analysis indicates that 2.9% of the dependencies
generated in the training corpus are non-projective
and 7.9% of the sentences contain at least one non-
projective dependency (for comparison, these num-
bers for the English Penn Treebank are 0.3% and
6.7%, respectively).
After parsing, we implement the inverse process,
i.e., we convert the generated dependency trees to
the BioNLP?09 representation. In addition to the
obvious conversions, this process implements the
heuristics proposed by Bjo?rne et al (2009), which
reverse step 3 above, e.g., we duplicate GENE EX-
PRESSION events with multiple THEME arguments.
The heuristics are executed sequentially in the given
order:
1. Since all non-BINDING events can have at
most one THEME argument, we duplicate non-
BINDING events with multiple THEME argu-
ments by creating one separate event for each
THEME.
2. Similarly, since REGULATION events accepts
only one CAUSE argument, we duplicate REG-
ULATION events with multiple CAUSE argu-
ments, obtaining one event per CAUSE.
3. Lastly, we implement the heuristic of Bjo?rne et
al. (2009) to handle the splitting of BINDING
events with multiple THEME arguments. This is
more complex because these events can accept
one or more THEMEs. In such situations, we
first group THEME arguments by the label of the
first Stanford dependency (Marneffe and Man-
ning, 2008) from the head word of the anchor
to this argument. Then we create one event for
each combination of THEME arguments in dif-
ferent groups.
3.2 Recognition of Event Anchors
For anchor detection, we used a multiclass classifier
that labels each token independently.3 Since over
92% of the anchor phrases in our evaluation domain
contain a single word, we simplify the task by re-
ducing all multi-word anchor phrases in the training
corpus to their syntactic head word (e.g., ?acts? for
the anchor ?acts as a costimulatory signal?).
We implemented this model using a logistic re-
gression classifier with L2 regularization over the
following features:
3We experimented with using conditional random fields as a
sequence labeler but did not see improvements in the biomed-
ical domain. We hypothesize that the sequence tagger fails to
capture potential dependencies between anchor labels ? which
are its main advantage over an i.i.d. classifier ? because anchor
words are typically far apart in text. This result is consistent
with observations in previous work (Bjo?rne et al, 2009).
1629
? Token-level: The form, lemma, and whether
the token is present in a gazetteer of known an-
chor words.4
? Surface context: The above token features ex-
tracted from a context of two words around the
current token. Additionally, we build token bi-
grams in this context window, and model them
with similar features.
? Syntactic context: We model all syntactic de-
pendency paths up to depth two starting from
the token to be classified. These paths are built
from Stanford syntactic dependencies (Marn-
effe and Manning, 2008). We extract token
features from the first and last token in these
paths. We also generate combination features
by concatenating: (a) the last token in each path
with the sequence of dependency labels along
the corresponding path; and (b) the word to be
classified, the last token in each path, and the
sequence of dependency labels in that path.
? Bag-of-word and entity count: Extracted
from (a) the entire sentence, and (b) a window
of five words around the token to be classified.
3.3 Parsing Event Structures
Given the entities and event anchors from the pre-
vious stages in the pipeline, the parser generates la-
beled dependency links between them. Many de-
pendency parsers are available and we chose MST-
Parser for its ability to produce non-projective and
n-best parses directly. MSTParser frames parsing
as a graph algorithm. To parse a sentence, MST-
Parser finds the tree covering all the words (nodes)
in the sentence (graph) with the largest sum of edge
weights, i.e., the maximum weighted spanning tree.
Each labeled, directed edge in the graph represents a
possible dependency between its two endpoints and
has an associated score (weight). Scores for edges
come from the dot product between the edge?s corre-
sponding feature vector and learned feature weights.
As a result, all features for MSTParser must be edge-
factored, i.e., functions of both endpoints and the la-
bel connecting them. McDonald et al (2006) ex-
tends the basic model to include second-order de-
pendencies (i.e., two adjacent sibling nodes and their
4These are automatically extracted from the training corpus.
parent). Both first and second-order modes include
projective and non-projective decoders.
Our features for MSTParser use both the event
structures themselves as well as the surrounding
English sentences which include them. By map-
ping event anchors and entities back to the original
text, we can incorporate information from the orig-
inal English sentence as well its syntactic tree and
corresponding Stanford dependencies. Both forms
of context are valuable and complementary. MST-
Parser comes with a large number of features which,
in our setup, operate on the event structure level
(since this is the ?sentence? from the parser?s point
of view). The majority of additional features that
we introduced take advantage of the original text as
context (primarily its associated Stanford dependen-
cies). Our system includes the following first-order
features:
? Path: Syntactic paths in the original sentence
between nodes in an event dependency (as in
previous work by Bjo?rne et al (2009)). These
have many variations including using Stanford
dependencies (?collapsed? and ?uncollapsed?)
or constituency trees as sources, optionally lex-
icalizing the path, and using words or relation
names along the path. Additionally, we include
the bucketed length of the paths.
? Original sentence words: Words from the full
English sentence surrounding and between the
nodes in event dependencies, and their buck-
eted distances. This additional context helps
compensate for how our anchor detection pro-
vides only the head word of each anchor, which
does not necessarily provide the full context for
event disambiguation.
? Graph: Parents, children, and siblings of
nodes in the Stanford dependencies graph
along with label of the edge. This provides ad-
ditional syntactic context.
? Consistency: Soft constraints on edges be-
tween anchors and their arguments (e.g., only
regulation events can have edges labeled with
CAUSE). These features fire if their constraints
are violated.
? Ontology: Generalized types of the end-
points of edges using a given type hierar-
chy (e.g., POSITIVE REGULATION is a COM-
1630
PLEX EVENT5 is an EVENT). Values of
this feature are coded with the types of each
of the endpoints on an edge, running over
the cross-product of types for each endpoint.
For instance, an edge between a BINDING
event anchor and a POSITIVE REGULATION
could cause this feature to fire with the val-
ues [head:EVENT, child:COMPLEX EVENT] or
[head:SIMPLE EVENT, child:EVENT].6 The lat-
ter feature can capture generalizations such as
?simple event anchors cannot take other events
as arguments.?
Both Consistency and Ontology feature classes in-
clude domain-specific information but can be used
on other domains under different constraints and
type hierarchies. When using second-order de-
pendencies, we use additional Path and Ontol-
ogy features. We include the syntactic paths be-
tween sibling nodes (adjacent arguments of the same
event anchor). These Path features are as above
but differentiated as paths between sibling nodes.
The second-order Ontology features use the type
hierarchy information on both sibling nodes and
their parent. For example, a POSITIVE REGULA-
TION anchor attached to a PROTEIN and a BINDING
event would produce an Ontology feature with the
value [parent:COMPLEX EVENT, child1:PROTEIN,
child2:SIMPLE EVENT] (among several other possi-
ble combinations).
To prune the number of features used, we employ
a simple entropy-based measure. Our intuition is
that good features should typically appear with only
one edge label.7 Given all edges enumerated during
training and their gold labels, we obtain a distribu-
tion over edge labels (df ) for each feature f . Given
this distribution and the frequency of a feature, we
can score the feature with the following:
score(f) = ?? log2
(
freq(f)
)
?H(df )
The ? parameter adjusts the relative weight of the
two components. The log frequency component fa-
vors more frequent features while the entropy com-
ponent favors features with low entropy in their edge
5We define complex events are those which can accept other
events are arguments. Simple events can only take PROTEINs.
6We omit listing the other two combinations.
7Labels include ROOT-LABEL, THEME, CAUSE, and NULL.
We assign the NULL label to edges which aren?t in the gold data.
label distribution. Features are pruned by accepting
all features with a score above a certain threshold.
3.4 Reranking Event Structures
When decoding, the parser finds the highest scoring
tree which incorporates global properties of the sen-
tence. However, its features are edge-factored and
thus unable to take into account larger contexts. To
incorporate arbitrary global features, we employ a
two-step reranking parser. For the first step, we ex-
tend our parser to output its n-best parses instead
of just its top scoring parse. In the second step, a
discriminative reranker rescores each parse and re-
orders the n-best list. Rerankers have been success-
fully used in syntactic parsing (Collins, 2000; Char-
niak and Johnson, 2005; Huang, 2008) and semantic
role labeling (Toutanova et al, 2008).
Rerankers provide additional advantages in our
case due to the mismatch between the dependency
structures that the parser operates on and their cor-
responding event structures. We convert the out-
put from the parser to event structures (Section 3.1)
before including them in the reranker. This al-
lows the reranker to capture features over the ac-
tual event structures rather than their original de-
pendency trees which may contain extraneous por-
tions.8 Furthermore, this lets the reranker optimize
the actual BioNLP F1 score. The parser, on the other
hand, attempts to optimize the Labeled Attachment
Score (LAS) between the dependency trees and con-
verted gold dependency trees. LAS is approximate
for two reasons. First, it is much more local than
the BioNLP metric.9 Second, the converted gold de-
pendency trees lose information that doesn?t transfer
to trees (specifically, that event structures are really
multi-DAGs and not trees).
We adapt the maximum entropy reranker from
Charniak and Johnson (2005) by creating a cus-
tomized feature extractor for event structures ? in
all other ways, the reranker model is unchanged. We
use the following types of features in the reranker:
? Source: Score and rank of the parse from the
8For instance, event anchors with no arguments could be
proposed by the parser. These event anchors are automatically
dropped by the conversion process.
9As an example, getting an edge label between an anchor
and its argument correct is unimportant if the anchor is missing
other arguments.
1631
Unreranked Reranked
Decoder(s) R P F1 R P F1
1P 65.6 76.7 70.7 68.0 77.6 72.5
2P 67.4 77.1 71.9 67.9 77.3 72.3
1N 67.5 76.7 71.8 ? ? ?
2N 68.9 77.1 72.7 ? ? ?
1P, 2P, 2N ? ? ? 68.5 78.2 73.1
(a) Gold event anchors
Unreranked Reranked
Decoder(s) R P F1 R P F1
1P 44.7 62.2 52.0 47.8 59.6 53.1
2P 45.9 61.8 52.7 48.4 57.5 52.5
1N 46.0 61.2 52.5 ? ? ?
2N 38.6 66.6 48.8 ? ? ?
1P, 2P, 2N ? ? ? 48.7 59.3 53.5
(b) Predicted event anchors
Table 1: BioNLP recall, precision, and F1 scores of individual decoders and the best decoder combination
on development data with the impact of event anchor detection and reranking. Decoder names include the
features order (1 or 2) followed by the projectivity (P = projective, N = non-projective).
decoder; number of different decoders produc-
ing the parse (when using multiple decoders).
? Event path: Path from each node in the event
tree up to the root. Unlike the Path features
in the parser, these paths are over event struc-
tures, not the syntactic dependency graphs from
the original English sentence. Variations of the
Event path features include whether to include
word forms (e.g., ?binds?), types (BINDING),
and/or argument slot names (THEME). We also
include the path length as a feature.
? Event frames: Event anchors with all their ar-
guments and argument slot names.
? Consistency: Similar to the parser Consis-
tency features, but capable of capturing larger
classes of errors (e.g., incorrect number or
types of arguments). We include the number of
violations from four different classes of errors.
To improve performance and robustness, features
are pruned as in Charniak and Johnson (2005): se-
lected features must distinguish a parse with the
highest F1 score in a n-best list, from a parse with a
suboptimal F1 score at least five times.
Rerankers can also be used to perform model
combination (Toutanova et al, 2008; Zhang et al,
2009; Johnson and Ural, 2010). While we use a sin-
gle parsing model, it has multiple decoders.10 When
combining multiple decoders, we concatenate their
n-best lists and extract the unique parses.
10We only have n-best versions of the projective decoders.
For the non-projective decoders, we use their 1-best parse.
4 Experimental Results
Our experiments use the BioNLP?09 shared task
corpus (Kim et al, 2009) which includes 800
biomedical abstracts (7,449 sentences, 8,597 events)
for training and 150 abstracts (1,450 sentences,
1,809 events) for development. The test set includes
260 abstracts, 2,447 sentences, and 3,182 events.
Throughout our experiments, we report BioNLP F1
scores with approximate span and recursive event
matching (as described in the shared task definition).
For preprocessing, we parsed all documents us-
ing the self-trained biomedical McClosky-Charniak-
Johnson reranking parser (McClosky, 2010). We
bias the anchor detector to favor recall, allowing the
parser and reranker to determine which event an-
chors will ultimately be used. When performing n-
best parsing, n = 50. For parser feature pruning,
? = 0.001.
Table 1a shows the performance of each of the de-
coders when using gold event anchors. In both cases
where n-best decoding is available, the reranker im-
proves performance over the 1-best parsers. We also
present the results from a reranker trained from mul-
tiple decoders which is our highest scoring model.11
In Table 1b, we present the output for the predicted
anchor scenario. In the case of the 2P decoder,
the reranker does not improve performance, though
the drop is minimal. This is because the reranker
chose an unfortunate regularization constant during
crossvalidation, most likely due to the small size of
the training data. In later experiments where more
11Including the 1N decoder as well provided no gains, possi-
bly because its outputs are mostly subsumed by the 2N decoder.
1632
data is available, the reranker consistently improves
accuracy (McClosky et al, 2011). As before, the
reranker trained from multiple decoders outperforms
unreranked models and reranked single decoders.
All in all, our best model in Table 1a scores 1 F1
point higher than the best system at the BioNLP?09
shared task, and the best model in Table 1b performs
similarly to the best shared task system (Bjo?rne et
al., 2009), which also scores 53.5% on development.
We show the effects of each system component
in Table 2. Note how our upper limit is 87.1%
due to our conversion process, which enforces the
tree constraint, drops events spanning sentences, and
performs approximate reconstruction of BINDING
events. Given that state-of-the-art systems on this
task currently perform in the 50-60% range, we are
not troubled by this number as it still allows for
plenty of potential.12 Bjo?rne et al (2009) list 94.7%
as the upper limit for their system. Considering
this relatively large difference, we find the results
in the previous table very encouraging. As in other
BioNLP?09 systems, our performance drops when
switching from gold to predicted anchor informa-
tion. Our decrease is similar to the one seen in
Bjo?rne et al (2009).
To show the potential of reranking, we provide or-
acle reranker scores in Table 3. An oracle reranker
picks the highest scoring parse from the available
parses. We limit the n-best lists to the top k parses
where k ? {1, 2, 10,All}. For single decoders,
?All? uses the entire 50-best list. For multiple de-
coders, the n-best lists are concatenated together.
The oracle score with multiple decoders and gold
anchors is only 0.4% lower than our upper limit (see
Table 2). This indicates that parses which could have
achieved that limit were nearly always present. Im-
proving the features in the reranker as well as the
original parsers will help us move closer to the limit.
With predicated anchors, the oracle score is about
13% lower but still shows significant potential.
Our final results on the test set, broken down by
class, are shown in Table 4. As with other systems,
complex events (e.g., REGULATION) prove harder
than simple events. To get a complex event cor-
rect, one must correctly detect and parse all events in
12Additionally, improvements such as document-level pars-
ing and DAG parsing would eliminate the need for much of the
approximate and lossy portions of the conversion process.
AD Parse RR Conv R P F1
X X X 45.9 61.8 52.7
X X X X 48.7 59.3 53.5
G X X 68.9 77.1 72.7
G X X X 68.5 78.2 73.1
G G G X 81.6 93.4 87.1
Table 2: Effect of each major component to the over-
all performance in the development corpus. Compo-
nents shown: AD ? event anchor detection; Parse
? best individual parsing model; RR ? reranking
multiple parsers; Conv ? conversion between the
event and dependency representations. ?G? indicates
that gold data was used; ?X? indicates that the actual
component was used.
n-best parses considered
Anchors Decoder(s) 1 2 10 All
Gold
1P 70.7 76.6 84.0 85.7
2P 71.8 77.5 84.8 86.2
1P, 2P, 2N ? ? ? 86.7
Predicted
1P 52.0 60.3 69.9 72.5
2P 52.7 60.7 70.1 72.5
1P, 2P, 2N ? ? ? 73.4
Table 3: Oracle reranker BioNLP F1 scores for
our n-best decoders and their combinations before
reranking on the development corpus.
the event subtree allowing small errors to have large
effects. Top systems on this task obtain F1 scores
of 52.0% at the shared task evaluation (Bjo?rne et
al., 2009) and 56.3% post evaluation (Miwa et al,
2010a). However, both systems are tailored to the
biomedical domain (the latter uses multiple syntac-
tic parsers), whereas our system has a design that is
virtually domain independent.
5 Discussion
We believe that the potential of our approach is
higher than what the current experiments show. For
example, the reranker can be used to combine not
only several parsers but also multiple anchor rec-
ognizers. This passes the anchor selection decision
to the reranker, which uses global information not
available to the current anchor recognizer or parser.
Furthermore, our approach can be adapted to parse
event structures in entire documents (instead of in-
1633
Event Class Count R P F1
Gene Expression 722 68.6 75.8 72.0
Transcription 137 42.3 51.3 46.4
Protein Catabolism 14 64.3 75.0 69.2
Phosphorylation 135 80.0 82.4 81.2
Localization 174 44.8 78.8 57.1
Binding 347 42.9 51.7 46.9
Regulation 291 23.0 36.6 28.3
Positive Regulation 983 28.4 42.5 34.0
Negative Regulation 379 29.3 43.5 35.0
Total 3,182 42.6 56.6 48.6
Table 4: Results in the test set broken by event class;
scores generated with the main official metric of ap-
proximate span and recursive event matching.
dividual sentences) by using a representation with a
unique ROOT node for all event structures in a doc-
ument. This representation has the advantage that
it maintains cross-sentence events (which account
for 5% of BioNLP?09 events), and it allows for
document-level features that model discourse struc-
ture. We plan to explore these ideas in future work.
One current limitation of the proposed model is
that it constrains event structures to map to trees. In
the BioNLP?09 corpus this leads to the removal of
almost 5% of the events, which generate DAGs in-
stead of trees. Local event extraction models (Bjo?rne
et al, 2009) do not have this limitation, because
their local decisions are blind to (and hence not
limited by) the global event structure. However,
our approach is agnostic to the actual parsing mod-
els used, so we can easily incorporate models that
can parse DAGs (Sagae and Tsujii, 2008). Addi-
tionally, we are free to incorporate any new tech-
niques from dependency parsing. Parsing using
dual-decomposition (Rush et al, 2010) seems espe-
cially promising in this area.
6 Conclusion
In this paper we proposed a simple approach for the
joint extraction of event structures: we converted
the representation of events and their arguments to
dependency trees with arcs between event anchors
and event arguments, and used a reranking parser to
parse these structures. Despite the fact that our ap-
proach has very little domain-specific engineering,
we obtain competitive results. Most importantly, we
showed that the joint modeling of event structures is
beneficial: our reranker outperforms parsing models
without reranking in five out of the six configura-
tions investigated.
Acknowledgments
The authors would like to thank Mark Johnson for
helpful discussions on the reranker component and
the BioNLP shared task organizers, Sampo Pyysalo
and Jin-Dong Kim, for answering questions. We
gratefully acknowledge the support of the Defense
Advanced Research Projects Agency (DARPA) Ma-
chine Reading Program under Air Force Research
Laboratory (AFRL) prime contract no. FA8750-09-
C-0181. Any opinions, findings, and conclusion
or recommendations expressed in this material are
those of the author(s) and do not necessarily reflect
the view of DARPA, AFRL, or the US government.
References
Jari Bjo?rne, Juho Heimonen, Filip Ginter, Antti Airola,
Tapio Pahikkala, and Tapio Salakoski. 2009. Ex-
tracting Complex Biological Events with Rich Graph-
Based Feature Sets. Proceedings of the Workshop on
BioNLP: Shared Task.
Nate Chambers and Dan Jurafsky. 2009. Unsupervised
Learning of Narrative Schemas and their Participants.
Proceedings of ACL.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and MaxEnt discriminative rerank-
ing. In Proceedings of the 2005 Meeting of the As-
sociation for Computational Linguistics (ACL), pages
173?180
Nancy Chinchor. 1997. Overview of MUC-7. Pro-
ceedings of the Message Understanding Conference
(MUC-7).
Michael Collins. 2000. Discriminative reranking for nat-
ural language parsing. In Machine Learning: Pro-
ceedings of the Seventeenth International Conference
(ICML 2000), pages 175?182.
Jenny R. Finkel and Christopher D. Manning. 2009.
Joint Parsing and Named Entity Recognition. Pro-
ceedings of NAACL.
Jenny R. Finkel and Christopher D. Manning. 2009b.
Nested Named Entity Recognition. Proceedings of
EMNLP.
Jan Hajic?, Massimiliano Ciaramita, Richard Johansson,
Daisuke Kawahara, Maria A. Marti, Lluis Marquez,
Adam Meyers, Joakim Nivre, Sebastian Pado, Jan
Stepanek, Pavel Stranak, Mihai Surdeanu, Nianwen
1634
Xue, and Yi Zhang. 2009. The CoNLL-2009 Shared
Task: Syntactic and Semantic Dependencies in Multi-
ple Languages. Proceedings of CoNLL.
Eduard Hovy, Mitchell P. Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. OntoNotes:
The 90% Solution. Proceedings of the NAACL-HLT.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of
ACL-08: HLT, pages 586?594, Association for Com-
putational Linguistics.
Mark Johnson and Ahmet Engin Ural. 2010. Rerank-
ing the Berkeley and Brown Parsers. Proceedings of
NAACL.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview
of the BioNLP?09 Shared Task on Event Extrac-
tion. Proceedings of the NAACL-HLT 2009 Work-
shop on Natural Language Processing in Biomedicine
(BioNLP?09).
Mitchell P. Marcus, Beatrice Santorini, and Marry Ann
Marcinkiewicz. 1993. Building a Large Annotated
Corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. Stanford Typed Hierarchies Represen-
tation. Proceedings of the COLING Workshop on
Cross-Framework and Cross-Domain Parser Evalua-
tion.
David McClosky. 2010. Any Domain Parsing: Auto-
matic Domain Adaptation for Natural Language Pars-
ing. PhD thesis, Department of Computer Science,
Brown University.
David McClosky, Mihai Surdeanu, and Christopher D.
Manning. 2011. Event extraction as dependency pars-
ing in BioNLP 2011. In BioNLP 2011 Shared Task
(submitted).
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online Large-Margin Training of Dependency
Parsers. Proceedings of ACL.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005b. Non-projective Dependency Pars-
ing using Spanning Tree Algorithms. Proceedings of
HLT/EMNLP.
Ryan McDonald and Fernando Pereira. 2006. Online
Learning of Approximate Dependency Parsing Algo-
rithms. Proceedings of EACL.
Scott Miller, Michael Crystal, Heidi Fox, Lance
Ramshaw, Richard Schwartz, Rebecca Stone, and
Ralph Weischedel. 1997. BBN: Description of the
SIFT System as Used for MUC-7. Proceedings of the
Message Understanding Conference (MUC-7).
Makoto Miwa, Sampo Pyysalo, Tadayoshi Hara, and
Jun?ichi Tsujii. 2010a. A Comparative Study of Syn-
tactic Parsers for Event Extraction. Proceedings of
the 2010 Workshop on Biomedical Natural Language
Processing.
Makoto Miwa, Rune Saetre, Jin-Dong Kim, and Jun?ichi
Tsujii. 2010b. Event Extraction with Complex Event
Classification Using Rich Features. Journal of Bioin-
formatics and Computational Biology, 8 (1).
Hoifung Poon and Lucy Vanderwende. 2010. Joint Infer-
ence for Knowledge Extraction from Biomedical Liter-
ature. Proceedings of NAACL.
Sebastian Riedel, Hong-Woo Chun, Toshihisa Takagi,
and Jun?ichi Tsujii. 2009. A Markov Logic Approach
to Bio-Molecular Event Extraction. Proceedings of the
Workshop on BioNLP: Shared Task.
Alexander M. Rush, David Sontag, Michael Collins, and
Tommi Jaakkola. 2010. On dual decomposition and
linear programming relaxations for natural language
processing. Proceedings of EMNLP.
Kenji Sagae and Jun?ichi Tsujii. 2008. Shift-Reduce De-
pendency DAG Parsing. Proceedings of the COLING.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Lluis Marquez, and Joakim Nivre. 2008. The CoNLL-
2008 Shared Task on Joint Parsing of Syntactic and
Semantic Dependencies. Proceedings of CoNLL.
Kristina Toutanova, Aria Haghighi, and Christopher D.
Manning. 2008. A Global Joint Model for Semantic
Role Labeling. Computational Linguistics 34(2).
Zhang, H. and Zhang, M. and Tan, C.L. and Li, H. 2009.
K-best combination of syntactic parsers. Proceedings
of Empirical Methods in Natural Language Process-
ing.
1635
Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 55?60,
Baltimore, Maryland USA, June 23-24, 2014.
c?2014 Association for Computational Linguistics
The Stanford CoreNLP Natural Language Processing Toolkit
Christopher D. Manning
Linguistics & Computer Science
Stanford University
manning@stanford.edu
Mihai Surdeanu
SISTA
University of Arizona
msurdeanu@email.arizona.edu
John Bauer
Dept of Computer Science
Stanford University
horatio@stanford.edu
Jenny Finkel
Prismatic Inc.
jrfinkel@gmail.com
Steven J. Bethard
Computer and Information Sciences
U. of Alabama at Birmingham
bethard@cis.uab.edu
David McClosky
IBM Research
dmcclosky@us.ibm.com
Abstract
We describe the design and use of the
Stanford CoreNLP toolkit, an extensible
pipeline that provides core natural lan-
guage analysis. This toolkit is quite widely
used, both in the research NLP community
and also among commercial and govern-
ment users of open source NLP technol-
ogy. We suggest that this follows from
a simple, approachable design, straight-
forward interfaces, the inclusion of ro-
bust and good quality analysis compo-
nents, and not requiring use of a large
amount of associated baggage.
1 Introduction
This paper describe the design and development of
Stanford CoreNLP, a Java (or at least JVM-based)
annotation pipeline framework, which provides
most of the common core natural language pro-
cessing (NLP) steps, from tokenization through to
coreference resolution. We describe the original
design of the system and its strengths (section 2),
simple usage patterns (section 3), the set of pro-
vided annotators and how properties control them
(section 4), and how to add additional annotators
(section 5), before concluding with some higher-
level remarks and additional appendices. While
there are several good natural language analysis
toolkits, Stanford CoreNLP is one of the most
used, and a central theme is trying to identify the
attributes that contributed to its success.
2 Original Design and Development
Our pipeline system was initially designed for in-
ternal use. Previously, when combining multiple
natural language analysis components, each with
their own ad hoc APIs, we had tied them together
with custom glue code. The initial version of the
Tokeniza)on*
Sentence*Spli0ng*
Part4of4speech*Tagging*
Morphological*Analysis*
Named*En)ty*Recogni)on*
Syntac)c*Parsing*
Other*Annotators*
Coreference*Resolu)on**
Raw*text*
Execu)
on*Flow
* Annota)on*Object*
Annotated*text*
(tokenize)*
(ssplit)*
(pos)*
(lemma)*
(ner)*
(parse)*
(dcoref)*
(gender, sentiment)!
Figure 1: Overall system architecture: Raw text
is put into an Annotation object and then a se-
quence of Annotators add information in an analy-
sis pipeline. The resulting Annotation, containing
all the analysis information added by the Annota-
tors, can be output in XML or plain text forms.
annotation pipeline was developed in 2006 in or-
der to replace this jumble with something better.
A uniform interface was provided for an Annota-
tor that adds some kind of analysis information to
some text. An Annotator does this by taking in an
Annotation object to which it can add extra infor-
mation. An Annotation is stored as a typesafe het-
erogeneous map, following the ideas for this data
type presented by Bloch (2008). This basic archi-
tecture has proven quite successful, and is still the
basis of the system described here. It is illustrated
in figure 1. The motivations were:
? To be able to quickly and painlessly get linguis-
tic annotations for a text.
? To hide variations across components behind a
common API.
? To have a minimal conceptual footprint, so the
system is easy to learn.
? To provide a lightweight framework, using plain
Java objects (rather than something of heav-
ier weight, such as XML or UIMA?s Common
Analysis System (CAS) objects).
55
In 2009, initially as part of a multi-site grant
project, the system was extended to be more easily
usable by a broader range of users. We provided
a command-line interface and the ability to write
out an Annotation in various formats, including
XML. Further work led to the system being re-
leased as free open source software in 2010.
On the one hand, from an architectural perspec-
tive, Stanford CoreNLP does not attempt to do ev-
erything. It is nothing more than a straightforward
pipeline architecture. It provides only a Java API.
1
It does not attempt to provide multiple machine
scale-out (though it does provide multi-threaded
processing on a single machine). It provides a sim-
ple concrete API. But these requirements satisfy
a large percentage of potential users, and the re-
sulting simplicity makes it easier for users to get
started with the framework. That is, the primary
advantage of Stanford CoreNLP over larger frame-
works like UIMA (Ferrucci and Lally, 2004) or
GATE (Cunningham et al., 2002) is that users do
not have to learn UIMA or GATE before they can
get started; they only need to know a little Java.
In practice, this is a large and important differ-
entiator. If more complex scenarios are required,
such as multiple machine scale-out, they can nor-
mally be achieved by running the analysis pipeline
within a system that focuses on distributed work-
flows (such as Hadoop or Spark). Other systems
attempt to provide more, such as the UIUC Cu-
rator (Clarke et al., 2012), which includes inter-
machine client-server communication for process-
ing and the caching of natural language analyses.
But this functionality comes at a cost. The system
is complex to install and complex to understand.
Moreover, in practice, an organization may well
be committed to a scale-out solution which is dif-
ferent from that provided by the natural language
analysis toolkit. For example, they may be using
Kryo or Google?s protobuf for binary serialization
rather than Apache Thrift which underlies Cura-
tor. In this case, the user is better served by a fairly
small and self-contained natural language analysis
system, rather than something which comes with
a lot of baggage for all sorts of purposes, most of
which they are not using.
On the other hand, most users benefit greatly
from the provision of a set of stable, robust, high
1
Nevertheless, it can call an analysis component written in
other languages via an appropriate wrapper Annotator, and
in turn, it has been wrapped by many people to provide Stan-
ford CoreNLP bindings for other languages.
quality linguistic analysis components, which can
be easily invoked for common scenarios. While
the builder of a larger system may have made over-
all design choices, such as how to handle scale-
out, they are unlikely to be an NLP expert, and
are hence looking for NLP components that just
work. This is a huge advantage that Stanford
CoreNLP and GATE have over the empty tool-
box of an Apache UIMA download, something
addressed in part by the development of well-
integrated component packages for UIMA, such
as ClearTK (Bethard et al., 2014), DKPro Core
(Gurevych et al., 2007), and JCoRe (Hahn et al.,
2008). However, the solution provided by these
packages remains harder to learn, more complex
and heavier weight for users than the pipeline de-
scribed here.
These attributes echo what Patricio (2009) ar-
gued made Hibernate successful, including: (i) do
one thing well, (ii) avoid over-design, and (iii)
up and running in ten minutes or less! Indeed,
the design and success of Stanford CoreNLP also
reflects several other of the factors that Patricio
highlights, including (iv) avoid standardism, (v)
documentation, and (vi) developer responsiveness.
While there are many factors that contribute to the
uptake of a project, and it is hard to show causal-
ity, we believe that some of these attributes ac-
count for the fact that Stanford CoreNLP is one of
the more used NLP toolkits. While we certainly
have not done a perfect job, compared to much
academic software, Stanford CoreNLP has gained
from attributes such as clear open source licens-
ing, a modicum of attention to documentation, and
attempting to answer user questions.
3 Elementary Usage
A key design goal was to make it very simple to
set up and run processing pipelines, from either
the API or the command-line. Using the API, run-
ning a pipeline can be as easy as figure 2. Or,
at the command-line, doing linguistic processing
for a file can be as easy as figure 3. Real life is
rarely this simple, but the ability to get started us-
ing the product with minimal configuration code
gives new users a very good initial experience.
Figure 4 gives a more realistic (and complete)
example of use, showing several key properties of
the system. An annotation pipeline can be applied
to any text, such as a paragraph or whole story
rather than just a single sentence. The behavior of
56
Annotator pipeline = new StanfordCoreNLP();
Annotation annotation = new Annotation(
"Can you parse my sentence?");
pipeline.annotate(annotation);
Figure 2: Minimal code for an analysis pipeline.
export StanfordCoreNLP_HOME /where/installed
java -Xmx2g -cp $StanfordCoreNLP_HOME/*
edu.stanford.nlp.StanfordCoreNLP
-file input.txt
Figure 3: Minimal command-line invocation.
import java.io.*;
import java.util.*;
import edu.stanford.nlp.io.*;
import edu.stanford.nlp.ling.*;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.trees.*;
import edu.stanford.nlp.trees.TreeCoreAnnotations.*;
import edu.stanford.nlp.util.*;
public class StanfordCoreNlpExample {
public static void main(String[] args) throws IOException {
PrintWriter xmlOut = new PrintWriter("xmlOutput.xml");
Properties props = new Properties();
props.setProperty("annotators",
"tokenize, ssplit, pos, lemma, ner, parse");
StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
Annotation annotation = new Annotation(
"This is a short sentence. And this is another.");
pipeline.annotate(annotation);
pipeline.xmlPrint(annotation, xmlOut);
// An Annotation is a Map and you can get and use the
// various analyses individually. For instance, this
// gets the parse tree of the 1st sentence in the text.
List<CoreMap> sentences = annotation.get(
CoreAnnotations.SentencesAnnotation.class);
if (sentences != null && sentences.size() > 0) {
CoreMap sentence = sentences.get(0);
Tree tree = sentence.get(TreeAnnotation.class);
PrintWriter out = new PrintWriter(System.out);
out.println("The first sentence parsed is:");
tree.pennPrint(out);
}
}
}
Figure 4: A simple, complete example program.
annotators in a pipeline is controlled by standard
Java properties in a Properties object. The most
basic property to specify is what annotators to run,
in what order, as shown here. But as discussed be-
low, most annotators have their own properties to
allow further customization of their usage. If none
are specified, reasonable defaults are used. Run-
ning the pipeline is as simple as in the first exam-
ple, but then we show two possibilities for access-
ing the results. First, we convert the Annotation
object to XML and write it to a file. Second, we
show code that gets a particular type of informa-
tion out of an Annotation and then prints it.
Our presentation shows only usage in Java, but
the Stanford CoreNLP pipeline has been wrapped
by others so that it can be accessed easily from
many languages, including Python, Ruby, Perl,
Scala, Clojure, Javascript (node.js), and .NET lan-
guages, including C# and F#.
4 Provided annotators
The annotators provided with StanfordCoreNLP
can work with any character encoding, making use
of Java?s good Unicode support, but the system
defaults to UTF-8 encoding. The annotators also
support processing in various human languages,
providing that suitable underlying models or re-
sources are available for the different languages.
The system comes packaged with models for En-
glish. Separate model packages provide support
for Chinese and for case-insensitive processing of
English. Support for other languages is less com-
plete, but many of the Annotators also support
models for French, German, and Arabic (see ap-
pendix B), and building models for further lan-
guages is possible using the underlying tools. In
this section, we outline the provided annotators,
focusing on the English versions. It should be
noted that some of the models underlying annota-
tors are trained from annotated corpora using su-
pervised machine learning, while others are rule-
based components, which nevertheless often re-
quire some language resources of their own.
tokenize Tokenizes the text into a sequence of to-
kens. The English component provides a PTB-
style tokenizer, extended to reasonably handle
noisy and web text. The corresponding com-
ponents for Chinese and Arabic provide word
and clitic segmentation. The tokenizer saves the
character offsets of each token in the input text.
cleanxml Removes most or all XML tags from
the document.
ssplit Splits a sequence of tokens into sentences.
truecase Determines the likely true case of tokens
in text (that is, their likely case in well-edited
text), where this information was lost, e.g., for
all upper case text. This is implemented with
a discriminative model using a CRF sequence
tagger (Finkel et al., 2005).
pos Labels tokens with their part-of-speech (POS)
tag, using a maximum entropy POS tagger
(Toutanova et al., 2003).
lemma Generates the lemmas (base forms) for all
tokens in the annotation.
gender Adds likely gender information to names.
ner Recognizes named (PERSON, LOCATION,
ORGANIZATION, MISC) and numerical
(MONEY, NUMBER, DATE, TIME, DU-
RATION, SET) entities. With the default
57
annotators, named entities are recognized
using a combination of CRF sequence taggers
trained on various corpora (Finkel et al., 2005),
while numerical entities are recognized using
two rule-based systems, one for money and
numbers, and a separate state-of-the-art system
for processing temporal expressions (Chang
and Manning, 2012).
regexner Implements a simple, rule-based NER
over token sequences building on Java regular
expressions. The goal of this Annotator is to
provide a simple framework to allow a user to
incorporate NE labels that are not annotated in
traditional NL corpora. For example, a default
list of regular expressions that we distribute
in the models file recognizes ideologies (IDE-
OLOGY), nationalities (NATIONALITY), reli-
gions (RELIGION), and titles (TITLE).
parse Provides full syntactic analysis, including
both constituent and dependency representa-
tion, based on a probabilistic parser (Klein and
Manning, 2003; de Marneffe et al., 2006).
sentiment Sentiment analysis with a composi-
tional model over trees using deep learning
(Socher et al., 2013). Nodes of a binarized tree
of each sentence, including, in particular, the
root node of each sentence, are given a senti-
ment score.
dcoref Implements mention detection and both
pronominal and nominal coreference resolution
(Lee et al., 2013). The entire coreference graph
of a text (with head words of mentions as nodes)
is provided in the Annotation.
Most of these annotators have various options
which can be controlled by properties. These can
either be added to the Properties object when cre-
ating an annotation pipeline via the API, or spec-
ified either by command-line flags or through a
properties file when running the system from the
command-line. As a simple example, input to the
system may already be tokenized and presented
one-sentence-per-line. In this case, we wish the
tokenization and sentence splitting to just work by
using the whitespace, rather than trying to do any-
thing more creative (be it right or wrong). This can
be accomplished by adding two properties, either
to a properties file:
tokenize.whitespace: true
ssplit.eolonly: true
in code:
/** Simple annotator for locations stored in a gazetteer. */
package org.foo;
public class GazetteerLocationAnnotator implements Annotator {
// this is the only method an Annotator must implement
public void annotate(Annotation annotation) {
// traverse all sentences in this document
for (CoreMap sentence:annotation.get(SentencesAnnotation.class)) {
// loop over all tokens in sentence (the text already tokenized)
List<CoreLabel> toks = sentence.get(TokensAnnotation.class);
for (int start = 0; start < toks.size(); start++) {
// assumes that the gazetteer returns the token index
// after the match or -1 otherwise
int end = Gazetteer.isLocation(toks, start);
if (end > start) {
for (int i = start; i < end; i ++) {
toks.get(i).set(NamedEntityTagAnnotation.class,"LOCATION");
}
}
}
}
}
}
Figure 5: An example of a simple custom anno-
tator. The annotator marks the words of possibly
multi-word locations that are in a gazetteer.
props.setProperty("tokenize.whitespace", "true");
props.setProperty("ssplit.eolonly", "true");
or via command-line flags:
-tokenize.whitespace -ssplit.eolonly
We do not attempt to describe all the properties
understood by each annotator here; they are avail-
able in the documentation for Stanford CoreNLP.
However, we note that they follow the pattern of
being x.y, where x is the name of the annotator
that they apply to.
5 Adding annotators
While most users work with the provided annota-
tors, it is quite easy to add additional custom an-
notators to the system. We illustrate here both how
to write an Annotator in code and how to load it
into the Stanford CoreNLP system. An Annotator
is a class that implements three methods: a sin-
gle method for analysis, and two that describe the
dependencies between analysis steps:
public void annotate(Annotation annotation);
public Set<Requirement> requirementsSatisfied();
public Set<Requirement> requires();
The information in an Annotation is updated in
place (usually in a non-destructive manner, by
adding new keys and values to the Annotation).
The code for a simple Annotator that marks loca-
tions contained in a gazetteer is shown in figure 5.
2
Similar code can be used to write a wrapper Anno-
tator, which calls some pre-existing analysis com-
ponent, and adds its results to the Annotation.
2
The functionality of this annotator is already provided by
the regexner annotator, but it serves as a simple example.
58
While building an analysis pipeline, Stanford
CoreNLP can add additional annotators to the
pipeline which are loaded using reflection. To pro-
vide a new Annotator, the user extends the class
edu.stanford.nlp.pipeline.Annotator
and provides a constructor with the signature
(String, Properties). Then, the user adds
the property
customAnnotatorClass.FOO: BAR
to the properties used to create the pipeline. If
FOO is then added to the list of annotators, the
class BAR will be loaded to instantiate it. The
Properties object is also passed to the constructor,
so that annotator-specific behavior can be initial-
ized from the Properties object. For instance, for
the example above, the properties file lines might
be:
customAnnotatorClass.locgaz: org.foo.GazetteerLocationAnnotator
annotators: tokenize,ssplit,locgaz
locgaz.maxLength: 5
6 Conclusion
In this paper, we have presented the design
and usage of the Stanford CoreNLP system, an
annotation-based NLP processing pipeline. We
have in particular tried to emphasize the proper-
ties that we feel have made it successful. Rather
than trying to provide the largest and most engi-
neered kitchen sink, the goal has been to make it
as easy as possible for users to get started using
the framework, and to keep the framework small,
so it is easily comprehensible, and can easily be
used as a component within the much larger sys-
tem that a user may be developing. The broad us-
age of this system, and of other systems such as
NLTK (Bird et al., 2009), which emphasize acces-
sibility to beginning users, suggests the merits of
this approach.
A Pointers
Website: http://nlp.stanford.edu/software/
corenlp.shtml
Github: https://github.com/stanfordnlp/CoreNLP
Maven: http://mvnrepository.com/artifact/edu.
stanford.nlp/stanford-corenlp
License: GPL v2+
Stanford CoreNLP keeps the models for ma-
chine learning components and miscellaneous
other data files in a separate models jar file. If you
are using Maven, you need to make sure that you
list the dependency on this models file as well as
the code jar file. You can do that with code like the
following in your pom.xml. Note the extra depen-
dency with a classifier element at the bottom.
<dependency>
<groupId>edu.stanford.nlp</groupId>
<artifactId>stanford-corenlp</artifactId>
<version>3.3.1</version>
</dependency>
<dependency>
<groupId>edu.stanford.nlp</groupId>
<artifactId>stanford-corenlp</artifactId>
<version>3.3.1</version>
<classifier>models</classifier>
</dependency>
B Human language support
We summarize the analysis components supported
for different human languages in early 2014.
Annotator Ara- Chi- Eng- Fre- Ger-
bic nese lish nch man
Tokenize X X X X X
Sent. split X X X X X
Truecase X
POS X X X X X
Lemma X
Gender X
NER X X X
RegexNER X X X X X
Parse X X X X X
Dep. Parse X X
Sentiment X
Coref. X
C Getting the sentiment of sentences
We show a command-line for sentiment analysis.
$ cat sentiment.txt
I liked it.
It was a fantastic experience.
The plot move rather slowly.
$ java -cp "*" -Xmx2g edu.stanford.nlp.pipeline.StanfordCoreNLP -annotators
tokenize,ssplit,pos,lemma,parse,sentiment -file sentiment.txt
Adding annotator tokenize
Adding annotator ssplit
Adding annotator pos
Reading POS tagger model from edu/stanford/nlp/models/pos-tagger/
english-left3words/english-left3words-distsim.tagger ... done [1.0 sec].
Adding annotator lemma
Adding annotator parse
Loading parser from serialized file edu/stanford/nlp/models/lexparser/
englishPCFG.ser.gz ... done [1.4 sec].
Adding annotator sentiment
Ready to process: 1 files, skipped 0, total 1
Processing file /Users/manning/Software/stanford-corenlp-full-2014-01-04/
sentiment.txt ... writing to /Users/manning/Software/
stanford-corenlp-full-2014-01-04/sentiment.txt.xml {
Annotating file /Users/manning/Software/stanford-corenlp-full-2014-01-04/
sentiment.txt [0.583 seconds]
} [1.219 seconds]
Processed 1 documents
Skipped 0 documents, error annotating 0 documents
Annotation pipeline timing information:
PTBTokenizerAnnotator: 0.0 sec.
WordsToSentencesAnnotator: 0.0 sec.
POSTaggerAnnotator: 0.0 sec.
59
MorphaAnnotator: 0.0 sec.
ParserAnnotator: 0.4 sec.
SentimentAnnotator: 0.1 sec.
TOTAL: 0.6 sec. for 16 tokens at 27.4 tokens/sec.
Pipeline setup: 3.0 sec.
Total time for StanfordCoreNLP pipeline: 4.2 sec.
$ grep sentiment sentiment.txt.xml
<sentence id="1" sentimentValue="3" sentiment="Positive">
<sentence id="2" sentimentValue="4" sentiment="Verypositive">
<sentence id="3" sentimentValue="1" sentiment="Negative">
D Use within UIMA
The main part of using Stanford CoreNLP within
the UIMA framework (Ferrucci and Lally, 2004)
is mapping between CoreNLP annotations, which
are regular Java classes, and UIMA annotations,
which are declared via XML type descriptors
(from which UIMA-specific Java classes are gen-
erated). A wrapper for CoreNLP will typically de-
fine a subclass of JCasAnnotator ImplBase whose
process method: (i) extracts UIMA annotations
from the CAS, (ii) converts UIMA annotations to
CoreNLP annotations, (iii) runs CoreNLP on the
input annotations, (iv) converts the CoreNLP out-
put annotations into UIMA annotations, and (v)
saves the UIMA annotations to the CAS.
To illustrate part of this process, the ClearTK
(Bethard et al., 2014) wrapper converts CoreNLP
token annotations to UIMA annotations and saves
them to the CAS with the following code:
int begin = tokenAnn.get(CharacterOffsetBeginAnnotation.class);
int end = tokenAnn.get(CharacterOffsetEndAnnotation.class);
String pos = tokenAnn.get(PartOfSpeechAnnotation.class);
String lemma = tokenAnn.get(LemmaAnnotation.class);
Token token = new Token(jCas, begin, end);
token.setPos(pos);
token.setLemma(lemma);
token.addToIndexes();
where Token is a UIMA type, declared as:
<typeSystemDescription>
<name>Token</name>
<types>
<typeDescription>
<name>org.cleartk.token.type.Token</name>
<supertypeName>uima.tcas.Annotation</supertypeName>
<features>
<featureDescription>
<name>pos</name>
<rangeTypeName>uima.cas.String</rangeTypeName>
</featureDescription>
<featureDescription>
<name>lemma</name>
<rangeTypeName>uima.cas.String</rangeTypeName>
</featureDescription>
</features>
</typeDescription>
</types>
</typeSystemDescription>
References
Steven Bethard, Philip Ogren, and Lee Becker. 2014.
ClearTK 2.0: Design patterns for machine learning
in UIMA. In LREC 2014.
Steven Bird, Ewan Klein, and Edward Loper.
2009. Natural Language Processing with Python.
O?Reilly Media.
Joshua Bloch. 2008. Effective Java. Addison Wesley,
Upper Saddle River, NJ, 2nd edition.
Angel X. Chang and Christopher D. Manning. 2012.
SUTIME: A library for recognizing and normalizing
time expressions. In LREC 2012.
James Clarke, Vivek Srikumar, Mark Sammons, and
Dan Roth. 2012. An NLP Curator (or: How I
learned to stop worrying and love NLP pipelines).
In LREC 2012.
Hamish Cunningham, Diana Maynard, Kalina
Bontcheva, and Valentin Tablan. 2002. GATE:
an architecture for development of robust HLT
applications. In ACL 2002.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
LREC 2006, pages 449?454.
David Ferrucci and Adam Lally. 2004. UIMA: an
architectural approach to unstructured information
processing in the corporate research environment.
Natural Language Engineering, 10:327?348.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by Gibbs
sampling. In ACL 43, pages 363?370.
I. Gurevych, M. M?uhlh?auser, C. M?uller, J. Steimle,
M. Weimer, and T. Zesch. 2007. Darmstadt knowl-
edge processing repository based on UIMA. In
First Workshop on Unstructured Information Man-
agement Architecture at GLDV 2007, T?ubingen.
U. Hahn, E. Buyko, R. Landefeld, M. M?uhlhausen,
Poprat M, K. Tomanek, and J. Wermter. 2008. An
overview of JCoRe, the Julie lab UIMA component
registry. In LREC 2008.
Dan Klein and Christopher D. Manning. 2003. Fast
exact inference with a factored model for natural
language parsing. In Suzanna Becker, Sebastian
Thrun, and Klaus Obermayer, editors, Advances in
Neural Information Processing Systems, volume 15,
pages 3?10. MIT Press.
Heeyoung Lee, Angel Chang, Yves Peirsman,
Nathanael Chambers, Mihai Surdeanu, and Dan Ju-
rafsky. 2013. Deterministic coreference resolu-
tion based on entity-centric, precision-ranked rules.
Computational Linguistics, 39(4).
Anthony Patricio. 2009. Why this project is success-
ful? https://community.jboss.org/wiki/
WhyThisProjectIsSuccessful.
Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Ng, and
Christopher Potts. 2013. Recursive deep models
for semantic compositionality over a sentiment tree-
bank. In EMNLP 2013, pages 1631?1642.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In NAACL 3, pages 252?259.
60
Proceedings of the ACL 2011 Workshop on Relational Models of Semantics (RELMS 2011), pages 2?10,
Portland, Oregon, USA, June 23, 2011. c?2011 Association for Computational Linguistics
Customizing an Information Extraction System to a New Domain
Mihai Surdeanu, David McClosky, Mason R. Smith, Andrey Gusev,
and Christopher D. Manning
Department of Computer Science
Stanford University
Stanford, CA 94305
{mihais,mcclosky,mrsmith,manning}@stanford.edu
agusev@cs.stanford.edu
Abstract
We introduce several ideas that improve the
performance of supervised information ex-
traction systems with a pipeline architecture,
when they are customized for new domains.
We show that: (a) a combination of a se-
quence tagger with a rule-based approach for
entity mention extraction yields better perfor-
mance for both entity and relation mention
extraction; (b) improving the identification of
syntactic heads of entity mentions helps rela-
tion extraction; and (c) a deterministic infer-
ence engine captures some of the joint domain
structure, even when introduced as a post-
processing step to a pipeline system. All in all,
our contributions yield a 20% relative increase
in F1 score in a domain significantly differ-
ent from the domains used during the devel-
opment of our information extraction system.
1 Introduction
Information extraction (IE) systems generally con-
sist of multiple interdependent components, e.g., en-
tity mentions predicted by an entity mention detec-
tion (EMD) model connected by relations via a re-
lation mention detection (RMD) component (Yao et
al., 2010; Roth and Yih, 2007; Surdeanu and Cia-
ramita, 2007). Figure 1 shows a sentence from a
sports domain where both entity and relation men-
tions are annotated. When training data exists, the
best performance in IE is generally obtained by su-
pervised machine learning approaches. In this sce-
nario, the typical approach for domain customiza-
tion is apparently straightforward: simply retrain
on data from the new domain (and potentially tune
model parameters). In this paper we argue that, even
when considerable training data is available, this is
not sufficient to maximize performance. We apply
several simple ideas that yield a significant perfor-
mance boost, and can be implemented with minimal
effort. In particular:
? We show that a combination of a conditional
random field model (Lafferty et al, 2001) with
a rule-based approach that is recall oriented
yields better performance for EMD and for
the downstream RMD component. The rule-
based approach includes gazetteers, which have
been shown to be important by Mikheev et al
(1999), among others.
? We improve the unification of the predicted se-
mantic annotations with the syntactic analy-
sis of the corresponding text, i.e., finding the
syntactic head of a given semantic constituent.
Since many features in an IE system depend on
syntactic analysis, this leads to more consistent
features and better extraction models.
? We add a simple inference engine that gener-
ates additional relation mentions based solely
on the relation mentions extracted by the RMD
model. This engine mitigates some of the limi-
tations of a text-based RMD model, which can-
not extract relations not explicitly stated in text.
We investigate these ideas using an IE system that
performs recognition of entity mentions followed by
extraction of binary relations between these men-
tions. We used as target a sports domain that is sig-
nificantly different from the corpora previously used
with this IE system. The target domain is also sig-
nificantly different from the dataset used to train the
2
Rookie	 ?Mike	 ?Anderson	 ?scored	 ?two	 ?second-??half	 ?touchdowns,	 ?	 ?
leading	 ?the	 ?Broncos	 ?to	 ?their	 ?sixth	 ?straight	 ?victory,	 ?31	 ?-??	 ?24	 ?	 ?
over	 ?the	 ?Sea?le	 ?Seahawks	 ?on	 ?Sunday.	 ?
ScoreType-??2	 ?
FinalScore	 ? FinalScore	 ?NFLGame	 ?NFLTeam	 ?
NFLTeam	 ? Date	 ?
teamInGame	 ?
gameWinner	 ?
touchdownPar?alCount	 ?
teamScoringAll	 ?
teamInGame	 ?
gameDate	 ?
gameLoser	 ?
teamScoringAll	 ?
teamFinalScore	 ?teamFinalScore	 ?
Figure 1: Sample sentence from the NFL domain. The domain contains entity mentions (underlined with entity types
in bold) and binary relations between entity mentions (indicated by arrows; relation types are italicized).
supporting natural language processing tools (e.g.,
syntactic parser). Our investigation shows that, de-
spite their simplicity, all our proposals help, yielding
a 20% relative improvement in RMD F1 score.
The paper is organized as follows: Section 2 sur-
veys related work. Section 3 describes the IE system
used. We cover the target domain that serves as use
case in this paper in Section 4. Section 5 introduces
our ideas and evaluates their impact in the target do-
main. Finally, Section 6 concludes the paper.
2 Related Work
Other recent works have analyzed the robustness of
information extraction systems. For example, Flo-
rian et al (2010) observed that EMD systems per-
form badly on noisy inputs, e.g., automatic speech
transcripts, and propose system combination (sim-
ilar to our first proposal) to increase robustness in
such scenarios. Ratinov and Roth (2009) also in-
vestigate design challenges for named entity recog-
nition, and showed that other design choices, such
as the representation of output labels and using fea-
tures built on external knowledge, are more impor-
tant than the learning model itself. These works are
conceptually similar to our paper, but we propose
several additional directions to improve robustness,
and we investigate their impact in a complete IE sys-
tem instead of just EMD.
Several of our lessons are drawn from the BioCre-
ative challenge1 and the BioNLP shared task (Kim
1http://biocreative.sourceforge.net/
et al, 2009). These tasks have shown the impor-
tance of high quality syntactic annotations and using
heuristic fixes to correct systematic errors (Schuman
and Bergler, 2006; Poon and Vanderwende, 2010,
among others). Systems in the latter task have also
shown the importance of high recall in the earlier
stages of pipeline system.
3 Description of the Generic IE System
We illustrate our proposed ideas using a simple IE
system that implements a pipeline architecture: en-
tity mention extraction followed by relation men-
tion extraction. Note however that the domain cus-
tomization discussion in Section 5 is independent of
the system architecture or classifiers used for EMD
and RMD, and we expect the proposed ideas to ap-
ply to other IE approaches as well.
We performed all pre-processing (tokenization,
part-of-speech (POS) tagging) with the Stanford
CoreNLP toolkit.2 For EMD we used the Stanford
named entity recognizer (Finkel et al, 2005). In all
our experiments we used a generic set of features
(?macro?) and the IO notation3 for entity mention la-
bels (e.g., the labels for the tokens ?over the Seattle
Seahawks on Sunday? (from Figure 1) are encoded
as ?O O NFLTEAM NFLTEAM O DATE?).
2http://nlp.stanford.edu/software/
corenlp.shtml
3The IO notation facilitates faster inference than the IOB
or IOB2 notations with minimal impact on performance, when
there are fewer adjacent mentions with the same type.
3
Argument
Features
? Head words of the two arguments
and their combination
? Entity mention labels of the two
arguments and their combination
Syntactic
Features
? Sequence of dependency labels
in the dependency path linking the
heads of the two arguments
? Lemmas of all words in the de-
pendency path
? Syntactic path in the constituent
parse tree between the largest con-
stituents headed by the same words
as the two arguments (similar
to Gildea and Jurafsky (2002))
Surface
Features
? Concatenation of POS tags be-
tween arguments
? Binary indicators set to true if
there is an entity mention with a
given type between the two argu-
ments
Table 1: Feature set used for RMD.
The RMD model was built from scratch as a
multi-class classifier that extracts binary relations
between entity mentions in the same sentence. Dur-
ing training, known relation mentions become pos-
itive examples for the corresponding label and all
other possible combinations between entity men-
tions in the same sentence become negative exam-
ples. We used a multiclass logistic regression classi-
fier with L2 regularization. Our feature set is taken
from (Yao et al, 2010; Mintz et al, 2009; Roth and
Yih, 2007; Surdeanu and Ciaramita, 2007) and mod-
els the relation arguments, the surface distance be-
tween the relation arguments, and the syntactic path
between the two arguments, using both constituency
and dependency representations. For syntactic in-
formation, we used the Stanford parser (Klein and
Manning, 2003) and the Stanford dependency repre-
sentation (de Marneffe et al, 2006).
For RMD, we implemented an additive feature se-
lection algorithm similar to the one in (Surdeanu
et al, 2008), which iteratively adds the feature
with the highest improvement in F1 score to the
current feature set, until no improvement is seen.
The algorithm was configured to select features
that yielded the best combined performance on the
dataset from Roth and Yih (2007) and the training
partition of ACE 2007.4 We used ten-fold cross val-
4LDC catalog numbers LDC2006E54 and LDC2007E11
Documents Words Entity Relation
Mentions Mentions
110 70,119 2,188 1,629
Table 2: Summary statistics of the NFL corpus, after our
conversion to binary relations.
idation on both datasets. We decided to use a stan-
dard F1 score to evaluate RMD performance rather
than the more complex ACE score because we be-
lieve that the former is more interpretable. We used
gold entity mentions for the feature selection pro-
cess. Table 1 summarizes the final set of features
selected.
Despite its simplicity, our approach achieves
comparable performance with other state-of-the-art
results reported on these datasets (Roth and Yih,
2007; Surdeanu and Ciaramita, 2007). For exam-
ple, Surdeanu and Ciaramita report a RMD F1 score
of 59.4 for ACE relation types (i.e., ignoring sub-
types) when gold entity mentions are used. Under
the same conditions, our RMD model obtains a F1
score of 59.2.
4 Description of the Target Domain
In this paper we report results on the ?Machine
Reading NFL Scoring? corpus.5 This corpus was
developed by LDC for the DARPA Machine Read-
ing project. The corpus contains 110 newswire arti-
cles on National Football League (NFL) games. The
annotations cover game information, such as partici-
pating teams, winners and losers, partial (e.g., a sin-
gle touchdown or three field goals) and final scores.
Most of the annotated relations in the original corpus
are binary (e.g. GAMEDATE(NFLGAME, DATE))
but some are n-ary relations or include other at-
tributes in addition of the relation type. We reduce
these to annotations compatible with our RMD ap-
proach as follows:
? We concatenate the cardinality of each scoring
event (i.e. how many scoring events are be-
ing talked about) to the corresponding SCORE-
TYPE entity label. Thus SCORETYPE-2 in-
dicates that there were two of a given type
of scoring event (touchdown, field goal, etc.).
This operation is necessary because the cardi-
nality of scoring events is originally annotated
as an additional attribute of the SCORETYPE
5LDC catalog number LDC2009E112
4
Entity Mentions Correct Predicted Actual P R F1
Date 141 190 174 74.2 81.0 77.5
FinalScore 299 328 347 91.2 86.2 88.6
NFLGame 71 109 147 65.1 48.3 55.5
NFLPlayoffGame 8 25 38 32.0 21.1 25.4
NFLTeam 651 836 818 77.9 79.6 78.7
ScoreType-1 329 479 525 68.7 62.7 65.5
ScoreType-2 49 68 79 72.1 62.0 66.7
ScoreType-3 17 26 36 65.4 47.2 54.8
ScoreType-4 6 11 14 54.5 42.9 48.0
Total 1571 2076 2188 75.7 71.8 73.7
Relation Mentions Correct Predicted Actual P R F1
fieldGoalPartialCount 33 41 101 80.5 32.7 46.5
gameDate 32 36 115 88.9 27.8 42.4
gameLoser 22 44 124 50.0 17.7 26.2
gameWinner 6 15 123 40.0 4.9 8.7
teamFinalScore 95 101 232 94.1 40.9 57.1
teamInGame 49 105 257 46.7 19.1 27.1
teamScoringAll 202 232 321 87.1 62.9 73.1
touchDownPartialCount 156 191 322 81.7 48.4 60.8
Total 595 766 1629 77.7 36.5 49.7
Table 3: Baseline results: stock system without any domain customization. Correct/Predicted/Actual indicate the num-
ber of mentions (entities or relations) that are correctly predicted/predicted/gold. P/R/F1 indicate precision/recall/F1
scores for the corresponding label.
entity and our EMD approach does not model
mention attributes.
? We split all n-ary relations into several new
binary relations. For example, the original
TEAMFINALSCORE(NFLTEAM, NFLGAME,
FINALSCORE) relation is split into three binary
relations: TEAMSCORINGALL(NFLTEAM,
FINALSCORE), TEAMINGAME(NFLGAME,
NFLTEAM), and TEAMFINALSCORE(NFL-
GAME, FINALSCORE).
Figure 1 shows an example annotated sentence af-
ter the above conversion and Table 2 lists the corpus
summary statistics for the new binary relations.
The purpose behind this corpus is to encourage
the development of systems that answer structured
queries that go beyond the functionality of informa-
tion retrieval engines, e.g.:
?For each NFL game, identify the win-
ning and losing teams and each team?s fi-
nal score in the game.?
?For each team losing to the Green Bay
Packers, tell us the losing team and the
number of points they scored.?6
6These queries would be written in a formal language but
5 Domain Customization
Table 3 lists the results of the generic IE system de-
scribed in Section 3 on the NFL domain. Through-
out this paper we will report results using ten-fold
cross-validation on all 110 documents in the cor-
pus.7 We consider an entity mention as correct if
both its boundaries and label match exactly the gold
mention. We consider a relation mention correct if
both its arguments and label match the gold relation
mention. For RMD, we report results using the ac-
tual mentions predicted by our EMD model (instead
of using gold entity mentions for RMD). For clar-
ity, we do not show in the tables some labels that are
highly uncommon in the data (e.g., SCORETYPE-5
appears only four times in the entire corpus); but the
?Total? results include all entity and relation men-
tions.
Table 3 shows that the stock IE system obtains an
are presented here in English for clarity.
7Generally, we do not condone reporting results using cross-
validation because it may be a recipe for over-fitting on the
corresponding corpus. However, all our domain customization
ideas were developed using outside world and domain knowl-
edge and were not tuned on this data, so we believe that there is
minimal over-fitting in this case.
5
Entity Mentions P R F1
Date 74.2 81.0 77.5
FinalScore 91.3 87.3 89.2
NFLGame 61.2 48.3 54.0
NFLPlayoffGame 33.3 21.1 25.8
NFLTeam 77.9 81.3 79.5
ScoreType-1 68.8 62.3 65.4
ScoreType-2 72.1 62.0 66.7
ScoreType-3 65.4 47.2 54.8
ScoreType-4 54.5 42.9 48.0
Total 75.6 72.5 74.0
Relation Mentions P R F1
fieldGoalPartialCount 78.0 31.7 45.1
gameDate 91.4 27.8 42.7
gameLoser 50.0 18.5 27.1
gameWinner 40.0 4.9 8.7
teamFinalScore 94.1 40.9 57.1
teamInGame 45.9 19.5 27.3
teamScoringAll 87.0 64.8 74.3
touchDownPartialCount 82.4 49.4 61.7
Total 77.6 37.1 50.2
Table 4: Performance after gazetteer-based features were
added to the EMD model.
EMD F1 score of 73.7 and a RMD F1 score of 49.7.
These are respectable results, in line with state-of-
the-art results in other domains.8 However, there
are some obvious areas for improvement. For exam-
ple, the score for a few relations (e.g., GAMELOSER
and GAMEWINNER) is quite low. This is caused by
the fact that these relations are often not explicitly
stated in text but rather implied (e.g., based on team
scores). Furthermore, the low recall of entity types
that are crucial for all relations (e.g., NFLTEAM and
NFLGAME) negatively impacts the overall recall of
RMD.
5.1 Combining a Rule-based Model with
Conditional Random Fields for EMD
A straightforward way to improve EMD perfor-
mance is to construct domain-specific gazetteers and
include gazetteer-based features in the model. We
constructed a NFL-specific gazetteer as follows: (a)
we included all 32 NFL team names; (b) we built a
lexicon for NFLGame nouns and verbs that included
game types (e.g., ?semi-final?, ?quarter-final?) and
8As a comparison, the best RMD system in ACE 2007 ob-
tained an ACE score of less than 35%, even though the ACE
score gives credit for approximate matches of entity mention
boundaries (Surdeanu and Ciaramita, 2007).
Entity Mentions P R F1
Date 74.2 81.0 77.5
FinalScore 91.3 87.3 89.2
NFLGame 61.2 48.3 54.0
NFLPlayoffGame 33.3 21.1 25.8
NFLTeam 71.4 96.9 82.3
ScoreType-1 68.8 62.3 65.4
ScoreType-2 72.1 62.0 66.7
ScoreType-3 65.4 47.2 54.8
ScoreType-4 54.5 42.9 48.0
Total 72.8 78.4 75.5
Relation Mentions P R F1
fieldGoalPartialCount 81.2 38.6 52.3
gameDate 93.9 27.0 41.9
gameLoser 51.1 19.4 28.1
gameWinner 38.9 5.7 9.9
teamFinalScore 94.1 40.9 57.1
teamInGame 47.4 24.5 32.3
teamScoringAll 87.0 68.8 76.9
touchDownPartialCount 81.6 56.5 66.8
Total 77.2 40.6 53.2
Table 5: Performance after gazetteer-based features were
added to the EMD model, and NFLTeam entity mentions
were extracted using the rule-based model rather than
classification.
typical game descriptors. The game descriptors
were manually bootstrapped from three seed words
(?victory?, ?loss?, ?game?) using Dekang Lin?s
dependency-based thesaurus.9 This process added
other relevant game descriptors such as ?triumph?,
?defeat?, etc. All in all, our gazetteer includes 32
team names and 50 game descriptors. The gazetteer
was built in less than four person hours.
We added features to our EMD model to indi-
cate if a sequence of words matches a gazetteer en-
try, allowing approximate matches (e.g., ?Cowboys?
matches ?Dallas Cowboys?). Table 4 lists the results
after this change. The improvements are modest: 0.3
for both EMD and RMD, caused by a 0.8 improve-
ment for NFLTEAM. The score for NFLGAME suf-
fers a loss of 1.5 F1 points, probably caused by the
fact that our NFLGAME gazetteer is incomplete.
These results are somewhat disappointing: even
though our gazetteer contains an exhaustive list of
NFL team names, the EMD recall for NFLTEAM
is still relatively low. This happens because city
9http://webdocs.cs.ualberta.ca/?lindek/
Downloads/sim.tgz
6
names that are not references to team names are rela-
tively common in this corpus, and the CRF model fa-
vors the generic city name interpretation. However,
since the goal is to answer structured queries over
the extracted relations, we would prefer a model
that favors recall for EMD, to avoid losing candi-
dates for RMD. While this can be achieved in dif-
ferent ways (Minkov et al, 2006), in this paper we
implement a very simple approach: we recognize
NFLTEAM mentions with a rule-based system that
extracts all token sequences that begin, end, or are
equal to a known team name. For example, ?Green
Bay? and ?Packers? are marked as team mentions,
but not ?Bay?. Note that this approach is prone to in-
troducing false positives, e.g., ?Green? in the above
example. For all other entity types we use the CRF
model with gazetteer-based features. Table 5 lists
the results for this model combination. The table
shows that the RMD performance is improved by 3
F1 points. The F1 score for NFLTEAM mentions is
also improved by 3 points, due to a significant in-
crease in recall (from 81% to 97%).
Of course, this simple idea works only for en-
tity types with low ambiguity. In fact, it does not
improve results if we apply it to NFLGAME or
SCORETYPE-*. However, low ambiguity entities
are common in many domains (e.g., medical). In
such domains, our approach offers a straightforward
way to address potential recall errors of a machine
learned model.
5.2 Improving Head Identification for Entity
Mentions
Table 1 indicates that most RMD features (e.g., lex-
ical information on arguments, dependency paths
between arguments) depend on the syntactic heads
of entity mentions. This observation applies to
other natural language processing (NLP) tasks as
well, e.g., semantic role labeling or coreference res-
olution (Gildea and Jurafsky, 2002; Haghighi and
Klein, 2009). It is thus crucial that syntactic heads
of mentions be correctly identified. Originally we
employed a common heuristic: we first try to find a
constituent with the exact same span as the given en-
tity mention in the parse tree of the entire sentence,
and extract its head. If no such constituent exists,
we parse only the text corresponding to the mention
and return the head of the generated tree (Haghighi
Entity Mentions P R F1
Date 69.5 75.9 72.5
FinalScore 90.9 88.8 89.8
NFLGame 60.5 51.0 55.4
NFLPlayoffGame 37.0 26.3 30.8
NFLTeam 72.4 98.3 83.4
ScoreType-1 69.7 62.1 65.7
ScoreType-2 76.9 63.3 69.4
ScoreType-3 64.3 50.0 56.3
ScoreType-4 72.7 57.1 64.0
Total 73.2 79.2 76.1
Relation Mentions P R F1
fieldGoalPartialCount 81.2 55.4 65.9
gameDate 93.9 27.0 41.9
gameLoser 51.2 17.7 26.3
gameWinner 50.0 8.9 15.2
teamFinalScore 96.5 47.4 63.6
teamInGame 48.3 33.5 39.5
teamScoringAll 86.7 72.9 79.2
touchDownPartialCount 89.1 61.2 72.6
Total 78.5 45.9 57.9
Table 6: Performance with the improved syntactic head
identification rules.
and Klein, 2009). Here we argue that the last step of
this heuristic is flawed: since most parsers are heav-
ily context dependent, they are likely to not parse
correctly arbitrarily short text fragments. For exam-
ple, the Stanford parser generates the incorrect parse
tree:
The syntactic head is ?5? for the mention ?a 5-yard
scoring pass? instead of ?pass.?10 This problem is
exacerbated out of domain, where the parse tree of
the entire sentence is likely to be incorrect, which
will often trigger the parsing of the isolated men-
tion text. For example, in the NFL domain, more
than 25% of entity mentions cannot be matched to
a constituent in the parse tree of the corresponding
sentence.
10We tokenize around dashes in this domain because scores
are often dash separated. However, this mention is incorrectly
parsed even when ?5-yard? is a single token.
7
teamFinalScore(G, S) :- teamInGame(T, G), teamScoringAll(T, S).
teamFinalScore(G, S) :- gameWinner(T, G), teamScoringAll(T, S).
teamFinalScore(G, S) :- gameLoser(T, G), teamScoringAll(T, S).
teamInGame(G, T) :- teamScoringAll(T, S), teamFinalScore(G, S).
gameWinner(G, T1) :- teamInGame(G, T1), teamInGame(G, T2),
teamFinalScore(G, S1), teamFinalScore(G, S2),
teamScoringAll(T1, S1), teamScoringAll(T2, S2),
greaterThan(S1, S2).
gameLoser(G, T1) :- teamInGame(G, T1), teamInGame(G, T2),
teamFinalScore(G, S1), teamFinalScore(G, S2),
teamScoringAll(T1, S1), teamScoringAll(T2, S2),
lessThan(S1, S2).
Table 7: Deterministic inference rules for the NFL domain as first-order Horn clauses. G, T, and S indicate game,
team, and score variables.
In this work, we propose several simple heuristics
that improve the parsing of isolated mention texts:
? We append ?It was ? to the beginning of the text
to be parsed. Since entity mentions are noun
phrases (NP), the new text is guaranteed to be
a coherent sentence. A similar heuristic was
used by Moldovan and Rus for the parsing of
WordNet glosses (2001).
? Because dashes are uncommon in the Penn
Treebank, we remove them from the text before
parsing.
? We guide the Stanford parser such that the final
tree contains a constituent with the same span
as the mention text.11
After implementing these heuristics, the Stanford
parser correctly parses the mention in the above ex-
ample as a NP headed by ?pass?. Table 6 lists
the overall extraction scores after deploying these
heuristics. The table shows that the RMD F1 score
is a considerable 4.7 points higher than before this
change (Table 5).
5.3 Deterministic Inference for RMD
Figure 1 underlines the fact that relations in the NFL
domain are highly inter-dependent. This is a com-
mon occurrence in many extraction tasks and do-
mains (Poon and Vanderwende, 2010; Carlson et
al., 2010). The typical way to address these situa-
tions is to jointly model these relations, e.g., using
Markov logic networks (MLN) (Poon and Vander-
wende, 2010). However, this implies a complete
redesign of the corresponding IE system, which
would essentially ignore all the effort behind exist-
ing pipeline systems.
11This is supported by the parser API.
Relation Mentions P R F1
fieldGoalPartialCount 81.2 55.4 65.9
gameDate 93.9 27.0 41.9
gameLoser 45.9 27.4 34.3
gameWinner 45.6 25.2 32.5
teamFinalScore 96.5 47.4 63.6
teamInGame 48.1 44.7 46.4
teamScoringAll 86.7 72.9 79.2
touchDownPartialCount 89.1 61.2 72.6
Total 74.2 49.6 59.5
Table 8: Performance after adding deterministic infer-
ence. The EMD scores are not affected by this change,
so they are not listed here.
In this work, we propose a simple method that
captures some of the joint domain structure indepen-
dently of the IE architecture and the EMD and RMD
models. We add a deterministic inference compo-
nent that generates new relation mentions based on
the data already extracted by the pipeline model. Ta-
ble 7 lists the rules of this inference component that
were developed for the NFL domain. These rules
are domain-dependent, but they are quite simple: the
first four rules implement transitive-closure rules for
relation mentions centered around the same NFL-
GAME mention; the last two add domain knowledge
that is not captured by the text extractors, e.g., the
game winner is the team with the higher score. Ta-
ble 8, which lists the RMD scores after inference, in-
dicates that the inference component is responsible
for an increase of approximately 2 F1 points, caused
by a recall boost of approximately 4%.
Table 9 lists the results of a post-hoc experiment,
where we removed several relation types from the
RMD classifier (the ones predicted with poor perfor-
mance) and let the deterministic inference compo-
nent generate them instead. This experiment shows
8
Without Inference With Inference
P R F1 P R F1
Skip gameWinner, gameLoser 78.6 45.6 57.7 75.1 48.4 58.8
Skip teamInGame 77.0 43.6 55.7 71.7 49.4 58.5
Skip teamInGame, teamFinalScore 74.5 37.1 49.6 70.9 47.6 56.9
Skip nothing 78.5 45.9 57.9 74.2 49.6 59.5
Table 9: Analysis of different combination strategies between the RMD classifier and inference: the RMD model skips
the relation types listed in the first column; the inference component generates all relation types. The other columns
show relation mention scores under the various configurations.
EMD RMD
F1 F1
Baseline 73.7 49.7
+ gazetteer features 74.0 50.2
+ rule-based model for NFLTeam 75.5 53.2
+ improved head identification 76.1 57.9
+ inference 76.1 59.5
Table 10: Summary of domain customization results.
that inference helps in all configurations, and, most
importantly, it is robust: even though the RMD score
without inference decreases by up to 8 F1 points
as relations are removed, the score after inference
varies by less than 3 F1 points (from 56.9 to 59.5
F1). This proves that deterministic inference is ca-
pable of generating relation mentions that are either
missed or cannot be modeled by the RMD classifier.
Finally, Table 10 summarizes the experiments
presented in this paper. It is clear that, despite their
simplicity, all our proposed ideas help. All in all,
our contributions yielded an improvement of 9.8 F1
points (approximately 20% relative) over the stock
IE system without these changes. Our best IE sys-
tem was used in a blind evaluation within the Ma-
chine Reading project. In this evaluation, systems
were required to answer 50 queries similar to the
examples in Section 4 and were evaluated on the
correctness of the individual facts extracted. Note
that this evaluation is more complex than the exper-
iments reported until now, because the correspond-
ing IE system requires additional components, e.g.,
the normalization of all DATE mentions and event
coreference (i.e., are two different game mentions
referring to the same real-world game?). For this
evaluation, we used an internal script for date nor-
malization and we did not implement event corefer-
ence. This system was evaluated at 46.7 F1 (53.7
precision and 41.2 recall), a performance that was
approximately 80% of the F1 score obtained by hu-
man annotators. This further highlights that strong
IE performance can be obtained with simple models.
6 Conclusions
This paper introduces a series of simple ideas that
improve the performance of IE systems when they
are customized to new domains. We evaluated our
contributions on a sports domain (NFL game sum-
maries) that is significantly different from the do-
mains used to develop our IE system or the language
processors used by our system.
Our analysis revealed several interesting and non-
obvious facts. First, we showed that accurate identi-
fication of syntactic heads of entity mentions, which
has received little attention in IE literature, is cru-
cial for good performance. Second, we showed that
a deterministic inference component captures some
of the joint domain structure, even when the under-
lying system follows a pipeline architecture. Lastly,
we introduced a simple way to tune precision and
recall by combining our entity mention extractor
with a rule-based system. Overall, our contributions
yielded a 20% improvement in the F1 score for rela-
tion mention extraction.
We believe that our contributions are model inde-
pendent and some, e.g., the better head identifica-
tion, even task independent. Some of our ideas re-
quire domain knowledge, but they are all very sim-
ple to implement. We thus expect them to impact
other problems as well, e.g., coreference resolution,
semantic role labeling.
Acknowledgments
We thank the reviewers for their detailed comments.
This material is based upon work supported by the Air
Force Research Laboratory (AFRL) under prime contract
no. FA8750-09-C-0181. Any opinions, findings, and
conclusion or recommendations expressed in this mate-
rial are those of the authors and do not necessarily reflect
the view of the Air Force Research Laboratory (AFRL).
9
References
Andrew Carlson, Justin Betteridge, Richard C. Wang, Es-
tevam R. Hruschka Jr., and Tom M. Mitchell. 2010.
Coupled semi-supervised learning for information ex-
traction. In Proceedings of the Third ACM Interna-
tional Conference on Web Search and Data Mining
(WSDM).
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
LREC.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local information
into information extraction systems by gibbs sampling.
In Proceedings of the 43nd Annual Meeting of the As-
sociation for Computational Linguistics (ACL 2005).
Radu Florian, John Pitrelli, Salim Roukos, and Imed Zi-
touni. 2010. Improving mention detection robustness
to noisy input. In Proc. of Empirical Methods in Nat-
ural Language Processing (EMNLP).
Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguistics,
28(3).
Aria Haghighi and Dan Klein. 2009. Simple coreference
resolution with rich syntactic and semantic features.
In Proc. of Empirical Methods in Natural Language
Processing (EMNLP).
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview of
BioNLP?09 shared task on event extraction. In Pro-
ceedings of the Workshop on BioNLP: Shared Task,
pages 1?9. Association for Computational Linguistics.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of the 41st
Meeting of the Association for Computational Linguis-
tics.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional Random Fields: Probabilistic mod-
els for segmenting and labeling sequence data. In
Proc. of the International Conference on Machine
Learning (ICML).
Andrei Mikheev, Marc Moens, and Claire Grover. 1999.
Named entity recognition without gazetteers. In
EACL, pages 1?8.
Einat Minkov, Richard C. Wang, Anthony Tomasic, and
William W. Cohen. 2006. Ner systems that suit user?s
preferences: Adjusting the recall-precision trade-off
for entity extraction. In Proc. of HLT/NAACL.
M. Mintz, S. Bills, R. Snow, and D. Jurafsky. 2009. Dis-
tant supervision for relation extraction without labeled
data. In Proc. of the Conference of the Association for
Computational Linguistics (ACL-IJCNLP).
Dan I. Moldovan and Vasile Rus. 2001. Logic form
transformation of wordnet and its applicability to ques-
tion answering. In Proceedings of the Annual Meeting
of the Association for Computational Linguistics.
Hoifung Poon and Lucy Vanderwende. 2010. Joint in-
ference for knowledge extraction from biomedical lit-
erature. In Proceedings of the North American Chap-
ter of the Association for Computational Linguistics -
Human Language Technologies Conference (NAACL-
HLT).
Lev Ratinov and Dan Roth. 2009. Design challenges and
misconceptions in named entity recognition. In Proc.
of the Annual Conference on Computational Natural
Language Learning (CoNLL).
D. Roth and W. Yih. 2007. Global inference for entity
and relation identification via a linear programming
formulation. In Introduction to Statistical Relational
Learning. MIT Press.
Jonathan Schuman and Sabine Bergler. 2006. Postnom-
inal prepositional phrase attachment in proteomics. In
Proceedings of the HLT-NAACL BioNLP Workshop on
Linking Natural Language and Biology, pages 82?89.
Association for Computational Linguistics, June.
Mihai Surdeanu and Massimiliano Ciaramita. 2007. Ro-
bust information extraction with perceptrons. In Pro-
ceedings of the NIST 2007 Automatic Content Extrac-
tion Workshop (ACE07).
Mihai Surdeanu, Massimiliano Ciaramita, and Hugo
Zaragoza. 2008. Learning to rank answers on large
online qa collections. In Proceedings of the 46th An-
nual Meeting of the Association for Computational
Linguistics (ACL 2008).
Limin Yao, Sebastian Riedel, and Andrew McCallum.
2010. Collective cross-document relation extraction
without labelled data. In Proc. of Empirical Methods
in Natural Language Processing (EMNLP).
10
Proceedings of BioNLP Shared Task 2011 Workshop, pages 41?45,
Portland, Oregon, USA, 24 June, 2011. c?2011 Association for Computational Linguistics
Event Extraction as Dependency Parsing for BioNLP 2011
David McClosky, Mihai Surdeanu, and Christopher D. Manning
Department of Computer Science
Stanford University
Stanford, CA 94305
{mcclosky,mihais,manning}@stanford.edu
Abstract
We describe the Stanford entry to the BioNLP
2011 shared task on biomolecular event ex-
traction (Kim et al, 2011a). Our framework is
based on the observation that event structures
bear a close relation to dependency graphs.
We show that if biomolecular events are cast
as these pseudosyntactic structures, standard
parsing tools (maximum-spanning tree parsers
and parse rerankers) can be applied to per-
form event extraction with minimum domain-
specific tuning. The vast majority of our
domain-specific knowledge comes from the
conversion to and from dependency graphs.
Our system performed competitively, obtain-
ing 3rd place in the Infectious Diseases track
(50.6% f-score), 5th place in Epigenetics and
Post-translational Modifications (31.2%), and
7th place in Genia (50.0%). Additionally, this
system was part of the combined system in
Riedel et al (2011) to produce the highest
scoring system in three out of the four event
extraction tasks.
1 Introduction
The distinguishing aspect of our approach is that by
casting event extraction as a dependency parsing, we
take advantage of standard parsing tools and tech-
niques rather than creating special purpose frame-
works. In this paper, we show that with minimal
domain-specific tuning, we are able to achieve com-
petitive performance across the three event extrac-
tion domains in the BioNLP 2011 shared task.
At the heart of our system1 is an off-the-shelf
1nlp.stanford.edu/software/eventparser.shtml
dependency parser, MSTParser2 (McDonald et al,
2005; McDonald and Pereira, 2006), extended with
event extraction-specific features and bookended by
conversions to and from dependency trees. While
features in MSTParser must be edge-factored and
thus fairly local (e.g., only able to examine a portion
of each event at once), decoding is performed glob-
ally allowing the parser to consider trade-offs. Fur-
thermore, as MSTParser can use n-best decoders,
we are able to leverage a reranker to capture global
features to improve accuracy.
In ?2, we provide a brief overview of our frame-
work. We describe specific improvements for the
BioNLP 2011 shared task in ?3. In ?4, we present
detailed results of our system. Finally, in ?5 we give
some directions for future work.
2 Event Parsing
Our system includes three components: (1) anchor
detection to identify and label event anchors, (2)
event parsing to form candidate event structures by
linking entities and event anchors, and (3) event
reranking to select the best candidate event structure.
As the full details on our approach are described in
McClosky et al (2011), we will only provide an out-
line of our methods here along with additional im-
plementation notes.
Before running our system, we perform basic
preprocessing on the corpora. Sentences need
to be segmented, tokenized, and parsed syntacti-
cally. We use custom versions of these (except
for Infectious Diseases where we use those from
Stenetorp et al (2011)). To ease event parsing, our
2http://sourceforge.net/projects/mstparser/
41
tokenizations are designed to split off suffixes which
are often event anchors. For example, we split the
token RelA-induced into the two tokens RelA and in-
duced3 since RelA is a protein and induced an event
anchor. If this was a single token, our event parser
would be unable to link them since it cannot pre-
dict self-loops in the dependency graph. For syntac-
tic parsing, we use the self-trained biomedical pars-
ing model from McClosky (2010) with the Charniak
and Johnson (2005) reranking parser. We use its ac-
tual constituency tree, the dependency graph created
by applying head percolation rules, and the Stanford
Dependencies (de Marneffe and Manning, 2008) ex-
tracted from the tree (collapsed and uncollapsed).
Anchor detection uses techniques inspired from
named entity recognition to label each token with
an event type or none. The features for this stage
are primarily drawn from Bjo?rne et al (2009). We
reduce multiword event anchors to their syntactic
head.4 We classify each token independently using a
logistic regression classifier with L2 regularization.
By adjusting a threshold parameter, we can adjust
the balance between precision and recall. We choose
to heavily favor recall (i.e., overgenerate event an-
chors) as the event parser can drop extraneous an-
chors by not attaching any arguments to them.
The event anchors from anchor detection and
the included entities (.t1 files) form a ?reduced?
sentence, which becomes the input to event pars-
ing. Thus, the only words in the reduced sentence
are tokens believed to directly take part in events.
Note, though, that we use the original ?full? sen-
tence (including the various representations of its
syntactic parse) for feature generation. For full de-
tails on this process, see McClosky et al (2011).
As stated before, this stage consists of MSTParser
with additional event parsing features. There are
four decoding options for MSTParser, depending
on (a) whether features are first- or second-order
and (b) whether graphs produced are projective or
non-projective. The projective decoders have com-
plete n-best implementations whereas their non-
projective counterparts are approximate. Neverthe-
3The dash is removed since a lone dash would further con-
fuse the syntactic parser.
4This does not affect performance if the approximate scorer
is used, but it does impact scores if exact matching of anchor
boundaries is imposed.
less, these four decoders constitute slightly different
views of the same data and can be combined inside
the reranking framework. After decoding, we con-
vert parses back to event structures. Details on this
critical step are given in McClosky et al (2011).
Event reranking, the final stage of our system, re-
ceives an n-best list of event structures from each
decoder in the event parsing step. The reranker
can use any global features of an event structure to
rescore it and outputs the highest scoring structure.
This is based on parse reranking (Ratnaparkhi, 1999;
Collins, 2000) but uses features on event structures
instead of syntactic constituency structures. We
used Mark Johnson?s cvlm estimator5 (Charniak
and Johnson, 2005) when learning weights for the
reranking model. Since the reranker can incorporate
the outputs from multiple decoders, we use it as an
ensemble technique as in Johnson and Ural (2010).
3 Extensions for BioNLP 2011
This section outlines the changes between our
BioNLP 2011 shared task submission and the sys-
tem described in McClosky et al (2011). The main
differences are that all dataset-specific portions of
the model have been factored out to handle the ex-
panded Genia (GE) dataset (Kim et al, 2011b) and
the new Epigenetics and Post-translational Modifi-
cations (EPI) and Infectious Diseases (ID) datasets
(Ohta et al, 2011; Pyysalo et al, 2011, respec-
tively). Other changes are relatively minor but doc-
umented here as implementation notes.
Several improvements were made to anchor de-
tection, improving its accuracy on all three do-
mains. The first is the use of distributional sim-
ilarity features. Using a large corpus of abstracts
from PubMed (30,963,886 word tokens of 335,811
word types), we cluster words by their syntactic con-
texts and morphological contents (Clark, 2003). We
used the Ney-Essen clustering model with morphol-
ogy to produce 45 clusters. Using these clusters, we
extended the feature set for anchor detection from
McClosky et al (2011) as follows: for each lexical-
ized feature we create an equivalent feature where
the corresponding word is replaced by its cluster ID.
This yielded consistent improvements of at least 1
percentage point in both anchor detection and event
5http://github.com/BLLIP/bllip-parser
42
extraction in the development partition of the GE
dataset.
Additionally, we improved the head percolation
rules for selecting the head of each multiword event
anchor. The new rules prohibit determiners and
prepositions from being heads, instead preferring
verbs, then nouns, then adjectives. There is also
a small stop list to prohibit the selection of certain
verbs (?has?, ?have?, ?is?, ?be?, and ?was?).
In event parsing, we used the morpha lemma-
tizer (Minnen et al, 2001) to stem words instead
of simply lowercasing them. This generally led to
a small but significant improvement in event extrac-
tion across the three domains. Additionally, we do
not use the feature selection mechanism described
in McClosky et al (2011) due to time restrictions.
It requires running all parsers twice which is espe-
cially cumbersome when operating in a round-robin
frame (as is required to train the reranker).
Also, note that our systems were only trained to
do Task 1 (or ?core?) roles for each dataset. This was
due to time restrictions and not system limitations.
3.1 Adapting to the Epigenetics track
For the EPI dataset, we adjusted our postprocessing
rules to handle the CATALYSIS event type. Similar
to REGULATION events in GE, CATALYSIS events do
not accept multiple CAUSE arguments. We handle
this by replicating such CATALYSIS events and as-
signing each new event a different CAUSE argument.
To adapt the ontology features in the parser (Mc-
Closky et al, 2011, ?3.3), we created a supertype for
all non-CATALYSIS events since they behave simi-
larly in many respects.
There are several possible areas for improvement
in handling this dataset. First, our internal imple-
mentation of the evaluation criteria differed from
the online scorer, sometimes by up to 6% f-score.
As a result, the reranker optimized a noisy version
of the evaluation criteria and potentially could have
performed better. It is unclear why our evaluator
scored EPI structures differently (it replicated the
scores for GE) but it is worthy of investigation. Sec-
ond, due to time constraints, we did not transfer the
parser or reranker consistency features (e.g., non-
REGULATION events should not take events as argu-
ments) or the type ontology in the reranker to the EPI
dataset. As a result, our results describe our system
with incomplete domain-specific knowledge.
3.2 Adapting to the Infectious Diseases track
Looking only at event types and their arguments, ID
is similar to GE. As a result, much of our domain-
specific processing code for this dataset is based on
code for GE. The key difference is that the GE post-
processing code removes event anchors with zero ar-
guments. Since ID allows PROCESS events to have
zero or one anchors, we added this as an exception.
Additionally, the ID dataset includes many nested
entities, e.g., two-component system entities contain
two other entities within their span. In almost all of
these cases, only the outermost entity takes part in
an event. To simplify processing, we removed all
nested entities. Any events attaching to a nested en-
tity were reattached to its outermost entity.
Given the similarities with GE, we explored sim-
ple domain adaptation by including the gold data
from GE along with our ID training data. To en-
sure that the GE data did not overwhelm the ID data,
we tried adding multiple copies of the ID data (see
Table 1 and the next section).
As in EPI, we adjusted the type ontology in the
parser for this dataset. This included ?core enti-
ties? (as defined by the task) and a ?PROTEIN-or-
REGULON-OPERON? type (the type of arguments for
GENE EXPRESSION and TRANSCRIPTION events).
Also as in EPI, the reranker did not use the updated
type ontology.
4 Results
For ID, we present experiments on merging GE with
ID data (Table 1). Since GE is much larger than
ID, we experimented with replicating the ID training
partition. Our best performance came from train-
ing on three copies of the ID data and the training
and development sections of GE. However, as the ta-
ble shows, performance is stable for more than two
copies of the ID data. Note that for this shared task
we simply merged the two domains. We did not
implement any domain adaptation techniques (e.g.,
labeling features based on the domain they come
from (Daume? III, 2007)).
Table 2 shows the performance of the various
parser decoders and their corresponding rerankers.
The last line in each domain block lists the score of
the reranker that uses candidates produced by all de-
43
coders. This reranking model always outperforms
the best individual parser. Furthermore, the rerank-
ing models on top of individual decoders help in all
but one situation (ID ? 2N decoder). To our knowl-
edge, our approach is the first to show that reranking
with features generated from global event structure
helps event extraction. Note that due to approximate
2N decoding in MSTParser, this decoder does not
produce true n-best candidates and generally out-
puts only a handful of unique parses. Because of
this, the corresponding rerankers suffer from insuf-
ficient training data and hurt performance in ID.
Finally, in Table 3, we give our results and rank-
ing on the official test sets. Our results are 6 f
points lower than the best submission in GE and EPI
and 5 points lower in ID. Considering that the we
used generic parsing tools with minimal customiza-
tion (e.g., our parsing models cannot extract directed
acyclic graph structures, which are common in this
data), we believe these results are respectable.
5 Conclusion
Our participation in the BioNLP shared task proves
that standard parsing tools (i.e., maximum-spanning
tree parsers, parse rerankers) can be successfully
used for event extraction. We achieved this by con-
verting the original event structures to a pseudo-
syntactic representation, where event arguments ap-
pear as modifiers to event anchors. Our analysis in-
dicates that reranking always helps, which proves
that there is merit in modeling non-local information
in biomolecular events. To our knowledge, our ap-
proach is the first to use parsing models for biomed-
ical event extraction.
During the shared task, we adapted our system
previously developed for the 2009 version of the
Genia dataset. This process required minimal ef-
fort: we did not add any new features to the pars-
ing model; we added only two domain-specific post-
processing steps (i.e., we allowed events without ar-
guments in ID and we replicated CATALYSIS events
with multiple CAUSE arguments in EPI). Our sys-
tem?s robust performance in all domains proves that
our approach is portable.
A desired side effect of our effort is that we
can easily incorporate any improvements to parsing
models (e.g., parsing of directed acyclic graphs, dual
decomposition, etc.) in our event extractor.
Model Prec Rec f-score
ID 59.3 38.0 46.3
(ID?1) + GE 52.0 40.2 45.3
(ID?2) + GE 52.4 41.7 46.4
(ID?3) + GE 54.8 45.0 49.4
(ID?4) + GE 55.2 43.8 48.9
(ID?5) + GE 55.1 44.7 49.4
Table 1: Impact of merging several copies of ID
training with GE training and development. Scores
on ID development data (2N parser only).
Decoder(s) Parser Reranker
1P 49.0 49.4
2P 49.5 50.5
1N 49.9 50.2
2N 46.5 47.9
All ? 50.7 ?
(a) Genia results (task 1)
Decoder(s) Parser Reranker
1P 62.3 63.3
2P 62.2 63.3
1N 62.9 64.6 ?
2N 60.8 63.8
All ? 64.1
(b) Epigenetics results (core task)
Decoder(s) Parser Reranker
1P 46.0 48.5
2P 47.8 49.8
1N 48.5 49.4
2N 49.4 48.8
All ? 50.2 ?
(c) Infectious Diseases results (core task)
Table 2: Results on development sections in
BioNLP f-scores. ??? indicates the submission
model for each domain.
Domain (task) Prec Rec f-score Ranking
GE (task 1) 61.1 42.4 50.0 7th
EPI (core) 70.2 56.9 62.8 5th
ID (core) 55.9 46.3 50.6 3rd
Table 3: BioNLP f-scores on the final test set.
44
Acknowledgments
We would like to thank the BioNLP shared task or-
ganizers for an enjoyable and interesting task and
their quick responses to questions. We would also
like to thank Sebastian Riedel for many interesting
discussions. We gratefully acknowledge the sup-
port of the Defense Advanced Research Projects
Agency (DARPA) Machine Reading Program under
Air Force Research Laboratory (AFRL) prime con-
tract no. FA8750-09-C-0181.
References
Jari Bjo?rne, Juho Heimonen, Filip Ginter, Antti Airola,
Tapio Pahikkala, and Tapio Salakoski. 2009. Extract-
ing complex biological events with rich graph-based
feature sets. In Proceedings of the BioNLP 2009Work-
shop Companion Volume for Shared Task, pages 10?
18, Boulder, Colorado, June. Association for Compu-
tational Linguistics.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-Fine n-Best Parsing and MaxEnt Discriminative
Reranking. In ACL. The Association for Computer
Linguistics.
Alexander Clark. 2003. Combining distributional and
morphological information for part of speech induc-
tion. In Proceedings of the tenth Annual Meeting of
the European Association for Computational Linguis-
tics (EACL), pages 59?66.
Michael Collins. 2000. Discriminative reranking for nat-
ural language parsing. In Machine Learning: Pro-
ceedings of the Seventeenth International Conference
(ICML 2000), pages 175?182, Stanford, California.
Hal Daume? III. 2007. Frustratingly easy domain adap-
tation. In Conference of the Association for Computa-
tional Linguistics (ACL), Prague, Czech Republic.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. Stanford typed hierarchies representation.
In Proceedings of the COLING Workshop on Cross-
Framework and Cross-Domain Parser Evaluation.
Mark Johnson and Ahmet Engin Ural. 2010. Rerank-
ing the berkeley and brown parsers. In Proceedings of
the HLT: North American Chapter of the ACL (HLT-
NAACL), pages 665?668. Association for Computa-
tional Linguistics, June.
Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert
Bossy, and Jun?ichi Tsujii. 2011a. Overview
of BioNLP Shared Task 2011. In Proceedings of
the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
Jin-Dong Kim, Yue Wang, Toshihisa Takagi, and Aki-
nori Yonezawa. 2011b. Overview of the Genia Event
task in BioNLP Shared Task 2011. In Proceedings
of the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
David McClosky, Mihai Surdeanu, and Chris Manning.
2011. Event extraction as dependency parsing. In
Proceedings of the Association for Computational Lin-
guistics: Human Language Technologies 2011 Confer-
ence (ACL-HLT?11), Main Conference, Portland, Ore-
gon, June.
David McClosky. 2010. Any Domain Parsing: Auto-
matic Domain Adaptation for Parsing. Ph.D. thesis,
Computer Science Department, Brown University.
Ryan T. McDonald and Fernando C. N. Pereira. 2006.
Online learning of approximate dependency parsing
algorithms. In Proceedings of EACL. The Association
for Computer Linguistics.
Ryan T. McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings
of HLT/EMNLP. The Association for Computational
Linguistics.
Guido Minnen, John Carroll, and Darren Pearce. 2001.
Applied morphological processing of English. Natu-
ral Language Engineering, 7(03):207?223.
Tomoko Ohta, Sampo Pyysalo, and Jun?ichi Tsujii. 2011.
Overview of the Epigenetics and Post-translational
Modifications (EPI) task of BioNLP Shared Task
2011. In Proceedings of the BioNLP 2011 Workshop
Companion Volume for Shared Task, Portland, Oregon,
June. Association for Computational Linguistics.
Sampo Pyysalo, Tomoko Ohta, Rafal Rak, Dan Sul-
livan, Chunhong Mao, Chunxia Wang, Bruno So-
bral, Jun?ichi Tsujii, and Sophia Ananiadou. 2011.
Overview of the Infectious Diseases (ID) task of
BioNLP Shared Task 2011. In Proceedings of
the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
Adwait Ratnaparkhi. 1999. Learning to parse natural
language with maximum entropy models. Machine
Learning, 34(1-3):151?175.
Sebastian Riedel, David McClosky, Mihai Surdeanu, An-
drew McCallum, and Christopher D. Manning. 2011.
Model Combination for Event Extraction in BioNLP
2011. In BioNLP 2011 Shared Task.
Pontus Stenetorp, Goran Topic?, Sampo Pyysalo, Tomoko
Ohta, Jin-Dong Kim, and Jun?ichi Tsujii. 2011.
BioNLP Shared Task 2011: Supporting Resources. In
Proceedings of the BioNLP 2011 Workshop Compan-
ion Volume for Shared Task, Portland, Oregon, June.
Association for Computational Linguistics.
45
Proceedings of BioNLP Shared Task 2011 Workshop, pages 51?55,
Portland, Oregon, USA, 24 June, 2011. c?2011 Association for Computational Linguistics
Model Combination for Event Extraction in BioNLP 2011
Sebastian Riedela, David McCloskyb, Mihai Surdeanub,
Andrew McCalluma, and Christopher D. Manningb
a Department of Computer Science, University of Massachusetts at Amherst
b Department of Computer Science, Stanford University
{riedel,mccallum}@cs.umass.edu
{mcclosky,mihais,manning}@stanford.edu
Abstract
We describe the FAUST entry to the BioNLP
2011 shared task on biomolecular event ex-
traction. The FAUST system explores sev-
eral stacking models for combination using
as base models the UMass dual decomposi-
tion (Riedel and McCallum, 2011) and Stan-
ford event parsing (McClosky et al, 2011b)
approaches. We show that using stacking is
a straightforward way to improving perfor-
mance for event extraction and find that it is
most effective when using a small set of stack-
ing features and the base models use slightly
different representations of the input data. The
FAUST system obtained 1st place in three out
of four tasks: 1st place in Genia Task 1 (56.0%
f-score) and Task 2 (53.9%), 2nd place in the
Epigenetics and Post-translational Modifica-
tions track (35.0%), and 1st place in the In-
fectious Diseases track (55.6%).
1 Introduction
To date, most approaches to the BioNLP event ex-
traction task (Kim et al, 2011a) use a single model
to produce their output. However, model combina-
tion techniques such as voting, stacking, and rerank-
ing have been shown to consistently produce higher
performing systems by taking advantage of multi-
ple views of the same data. The Netflix Prize (Ben-
nett et al, 2007) is a prime example of this. System
combination essentially allows systems to regular-
ize each other, smoothing over the artifacts of each
(c.f. Nivre and McDonald (2008), Surdeanu and
Manning (2010)). To our knowledge, the only previ-
ous example of model combination for the BioNLP
shared task was performed by Kim et al (2009). Us-
ing a weighted voting scheme to combine the out-
puts from the top six systems, they obtained a 4%
absolute f-score improvement over the best individ-
ual system.
This paper shows that using a straightforward
model combination strategy on two competitive
systems produces a new system with substantially
higher accuracy. This is achieved with the frame-
work of stacking: a stacking model uses the output
of a stacked model as additional features.
While we initially considered voting and rerank-
ing model combination strategies, it seemed that
given the performance gap between the UMass and
Stanford systems that the best option was to in-
clude the predictions from the Stanford system into
the UMass system (e.g., as in Nivre and McDon-
ald (2008)). This has the advantage that one model
(Umass) determines how to integrate the outputs of
the other model (Stanford) into its own structure,
whereas in reranking, for example, the combined
model is required to output a complete structure pro-
duced by only one of the input models.
2 Approach
In the following we briefly present both the stacking
and the stacked model and some possible ways of
integrating the stacked information.
2.1 Stacking Model
As our stacking model, we employ the UMass ex-
tractor (Riedel and McCallum, 2011). It is based on
a discriminatively trained model that jointly predicts
trigger labels, event arguments and protein pairs in
51
binding. We will briefly describe this model but first
introduce three types of binary variables that will
represent events in a given sentence. Variables ei,t
are active if and only if the token at position i has
the label t. Variables ai,j,r are active if and only if
there is an event with trigger i that has an argument
with role r grounded at token j. In the case of an
entity mention this means that the mention?s head is
j. In the case of an event j is the position of its trig-
ger. Finally, variables bp,q indicate whether or not
two entity mentions at p and q appear as arguments
in the same binding event.
Two parts form our model: a scoring function, and
a set of constraints. The scoring function over the
trigger variables e, argument variables a and binding
pair variables b is
s (e,a,b) def=
?
ei,t=1
sT (i, t) +
?
ai,j,r=1
sR (i, j, r)+
?
bp,q=1
sB (p, q)
with local scoring functions sT (i, t)
def=
?wT, fT (i, t)?, sR (i, j, r)
def= ?wR, fR (i, j, r)?
and sB (p, q)
def= ?wB, fB (p, q)?.
Our model scores all parts of the structure in iso-
lation. It is a joint model due to the nature of the
constraints we enforce: First, we require that each
active event trigger must have at least one Theme ar-
gument; second, only regulation events (or Catalysis
events for the EPI track) are allowed to have Cause
arguments; third, any trigger that is itself an argu-
ment of another event has to be labelled active, too;
finally, if we decide that two entities p and q are part
of the same binding (as indicated by bp,q = 1), there
needs to be a binding event at some trigger i that
has p and q as arguments. We will denote the set of
structures (e,a,b) that satisfy these constraints as
Y .
Stacking with this model is simple: we only
need to augment the local feature functions fT (i, t),
fR (i, j, r) and fB (p, q) to include predictions from
the systems to be stacked. For example, for every
system S to be stacked and every pair of event types
(t?, tS) we add the features
fS,t? ,tS (i, t) =
{
1 hS (i) = tS ? t? = t
0 otherwise
to fT (i, t). Here hS (i) is the event label given to to-
ken i according to S. These features allow different
weights to be given to each possible combination of
type t? that we want to assign, and type tS that S
predicts.
Inference in this model amounts to maximizing
s (e,a,b) over Y . Our approach to solving this
problem is dual decomposition (Komodakis et al,
2007; Rush et al, 2010). We divide the problem into
three subproblems: (1) finding the best trigger label
and set of outgoing edges for each candidate trigger;
(2) finding the best trigger label and set of incoming
edges for each candidate trigger; (3) finding the best
pairs of entities to appear in the same binding. Due
to space limitations we refer the reader to Riedel and
McCallum (2011) for further details.
2.2 Stacked Model
For the stacked model, we use a system based on an
event parsing framework (McClosky et al, 2011a)
referred to as the Stanford model in this paper. This
model converts event structures to dependency trees
which are parsed using MSTParser (McDonald et
al., 2005).1 Once parsed, the resulting dependency
tree is converted back to event structures. Using the
Stanford model as the stacked model is helpful since
it captures tree structure which is not the focus in
the UMass model. Of course, this is also a limita-
tion since actual BioNLP event graphs are DAGs,
but the model does well considering these restric-
tions. Additionally, this constraint encourages the
Stanford model to provide different (and thus more
useful for stacking) results.
Of particular interest to this paper are the four
possible decoders in MSTParser. These four de-
coders come from combinations of feature order
(first or second) and whether the resulting depen-
dency tree is required to be projective.2 Each de-
coder presents a slightly different view of the data
and thus has different model combination proper-
ties. Projectivity constraints are not captured in the
UMass model so these decoders incorporate novel
information.
To produce stacking output from the Stanford sys-
tem, we need its predictions on the training, devel-
1http://sourceforge.net/projects/mstparser/
2For brevity, the second-order non-projective decoder is ab-
breviated as 2N, first-order projective as 1P, etc.
52
UMass FAUST+All
R P F1 R P F1
GE T1 48.5 64.1 55.2 49.4 64.8 56.0
GE T2 43.9 60.9 51.0 46.7 63.8 53.9
EPI (F) 28.1 41.6 33.5 28.9 44.5 35.0
EPI (C) 57.0 73.3 64.2 59.9 80.3 68.6
ID (F) 46.9 62.0 53.4 48.0 66.0 55.6
ID (C) 49.5 62.1 55.1 50.6 66.1 57.3
Table 1: Results on test sets of all tasks we submitted to.
T1 and T2 stand for task 1 and 2, respectively. C stands
for CORE metric, F for FULL metric.
opment and test sets. For predictions on test and de-
velopment sets we used models learned from the the
complete training set. Predictions over training data
were produced using crossvalidation. This helps to
avoid a scenario where the stacking model learns to
rely on high accuracy at training time that cannot be
matched at test time.
Note that, unlike Stanford?s individual submission
in this shared task, the stacked models in this paper
do not include the Stanford reranker. This is because
it would have required making a reranker model for
each crossvalidation fold.
We made 19 crossvalidation training folds for Ge-
nia (GE) (Kim et al, 2011b), 12 for Epigenetics
(EPI), and 17 for Infectious Diseases (ID) (Kim et
al., 2011b; Ohta et al, 2011; Pyysalo et al, 2011,
respectively). Note that while ID is the smallest and
would seem like it would have the fewest folds, we
combined the training data of ID with the training
and development data from GE. To produce predic-
tions over the test data, we combined the training
folds with 6 development folds for GE, 4 for EPI,
and 1 for ID.
3 Experiments
Table 1 gives an overview of our results on the test
sets for all four tasks we submitted to. Note that
for the EPI and ID tasks we show the CORE metric
next to the official FULL metric. The former is suit-
able for our purposes because it does not measure
performance for negations, speculations and cellular
locations?all of these we did not attempt to predict.
We compare the UMass standalone system to the
FAUST+All system which stacks the Stanford 1N,
1P, 2N and 2P predictions. For all four tasks we
System SVT BIND REG TOTAL
UMass 74.7 47.7 42.8 54.8
Stanford 1N 71.4 38.6 32.8 47.8
Stanford 1P 70.8 35.9 31.1 46.5
Stanford 2N 69.1 35.0 27.8 44.3
Stanford 2P 72.0 36.2 32.2 47.4
FAUST+All 76.9 43.5 44.0 55.9
FAUST+1N 76.4 45.1 43.8 55.6
FAUST+1P 75.8 43.1 44.6 55.7
FAUST+2N 74.9 42.8 43.8 54.9
FAUST+2P 75.7 46.0 44.1 55.7
FAUST+All 76.4 41.2 43.1 54.9
(triggers)
FAUST+All 76.1 41.7 43.6 55.1
(arguments)
Table 2: BioNLP f-scores on the development section of
the Genia track (task 1) for several event categories.
observe substantial improvements due to stacking.
The increase is particular striking for the EPI track,
where stacking improves f-score by more than 4.0
points on the CORE metric.
To analyze the impact of stacking further, Ta-
ble 2 shows a breakdown of our results on the Ge-
nia development set. Presented are f-scores for sim-
ple events (SVT), binding events (BIND), regulation
events (REG) and the set of all event types (TOTAL).
We compare the UMass standalone system, various
Stanford-standalone models and stacked versions of
these (FAUST+X).
Remarkably, while there is a 7 point gap between
the best individual Stanford system and the stand-
alone UMass systems, integrating the Stanford pre-
diction still leads to an f-score improvement of 1.
This can be seen when comparing the UMass, Stan-
ford 1N and FAUST+All results, where the latter
stacks 1N, 1P, 2N and 2P. We also note that stack-
ing the projective 1P and 2P systems helps almost
as much as stacking all Stanford systems. Notably,
both 1P and 2P do not do as well in isolation when
compared to the 1N system. When stacked, how-
ever, they do slightly better. This suggests that pro-
jectivity is a missing aspect in the UMass standalone
system.
The FAUST+All (triggers) and FAUST+All (ar-
guments) lines represent experiments to determine
whether it is useful to incorporate only portions of
53
the stacking information from the Stanford system.
Given the small gains over the original UMass sys-
tem, it is clear that stacking information is only use-
ful when attached to triggers and arguments. Our
theory is that most of our gains come from when the
UMass and Stanford systems disagree on triggers
and the Stanford system provides not only its trig-
gers but also their attached arguments to the UMass
system. This is supported by a pilot experiment
where we trained the Stanford model to use the
UMass triggers and saw no benefit from stacking
(even when both triggers and arguments were used).
Table 3 shows our results on the development set
of the ID task, this time in terms of recall, precision
and f-score. Here the gap between Stanford-only
results, and the UMass results, is much smaller. This
seems to lead to more substantial improvements for
stacking: FAUST+All obtains a f-score 2.2 points
larger than the standalone UMass system. Also note
that, similarly to the previous table, the projective
systems do worse on their own, but are more useful
when stacked.
Another possible approach to stacking conjoins
all the original features of the stacking model with
the predicted features of the stacked model. The
hope is that this allows the learner to give differ-
ent weights to the stacked predictions in different
contexts. However, incorporating Stanford predic-
tions by conjoining them with all features of the
UMass standalone system (FAUST+2P-Conj in Ta-
ble 3) does not help here.
We note that for our results on the ID task we
augment the training data with events from the GE
training set. Merging both training sets is reasonable
since there is a significant overlap between both in
terms of events as well as lexical and syntactic pat-
terns to express these. When building our training
set we add each training document from GE once,
and each ID training document twice?this lead to
substantially better results than including ID data
only once.
4 Discussion
Generally stacking has led to substantial improve-
ments across the board. There are, however, some
exceptions. One is binding events for the GE task.
Here the UMass model still outperforms the best
System Rec Prec F1
UMass 46.2 51.1 48.5
Stanford 1N 43.1 49.1 45.9
Stanford 1P 40.8 46.7 43.5
Stanford 2N 41.6 53.9 46.9
Stanford 2P 42.8 48.1 45.3
FAUST+All 47.6 54.3 50.7
FAUST+1N 45.8 51.6 48.5
FAUST+1P 47.6 52.8 50.0
FAUST+2N 45.4 52.4 48.6
FAUST+2P 49.1 52.6 50.7
FAUST+2P-Conj 48.0 53.2 50.4
Table 3: Results on the development set for the ID track.
stacked system (see Table 2). Likewise, for full pa-
pers in the Genia test set, the UMass model still does
slightly better with 53.1 f-score compared to 52.7
f-score. This suggests that a more informed com-
bination of our systems (e.g., metaclassifiers) could
lead to better performance.
5 Conclusion
We have presented the FAUST entry to the BioNLP
2011 shared task on biomolecular event extraction.
It is based on stacking, a simple approach for model
combination. By using the predictions of the Stan-
ford entry as features of the UMass model, we sub-
stantially improved upon both systems in isolation.
This helped us to rank 1st in three of the four tasks
we submitted results to. Remarkably, in some cases
we observed improvements despite a 7.0 f-score
margin between the models we combined.
In the future we would like to investigate alter-
native means for model combination such as rerank-
ing, union, intersection, and other voting techniques.
We also plan to use dual decomposition to encourage
models to agree. In particular, we will seek to incor-
porate an MST component into the dual decomposi-
tion algorithm used by the UMass system.
Acknowledgments
We thank the BioNLP shared task organizers for setting this
up and their quick responses to questions. This work was sup-
ported in part by the Center for Intelligent Information Re-
trieval. We gratefully acknowledge the support of the Defense
Advanced Research Projects Agency (DARPA) Machine Read-
ing Program under Air Force Research Laboratory (AFRL)
prime contract no. FA8750-09-C-0181.
54
References
James Bennett, Stan Lanning, and Netflix. 2007. The
netflix prize. In KDD Cup and Workshop in conjunc-
tion with KDD.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview of
BioNLP?09 shared task on event extraction. In Pro-
ceedings of the Workshop on BioNLP: Shared Task,
pages 1?9. Association for Computational Linguistics.
Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert
Bossy, and Jun?ichi Tsujii. 2011a. Overview
of BioNLP Shared Task 2011. In Proceedings of
the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
Jin-Dong Kim, Yue Wang, Toshihisa Takagi, and Aki-
nori Yonezawa. 2011b. Overview of the Genia Event
task in BioNLP Shared Task 2011. In Proceedings
of the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
Nikos Komodakis, Nikos Paragios, and Georgios Tziri-
tas. 2007. MRF optimization via dual decomposition:
Message-passing revisited. In ICCV.
David McClosky, Mihai Surdeanu, and Chris Manning.
2011a. Event extraction as dependency parsing. In
Proceedings of the Association for Computational Lin-
guistics: Human Language Technologies 2011 Confer-
ence (ACL-HLT?11), Main Conference, Portland, Ore-
gon, June.
David McClosky, Mihai Surdeanu, and Christopher D.
Manning. 2011b. Event extraction as dependency
parsing in BioNLP 2011. In BioNLP 2011 Shared
Task.
Ryan T. McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings
of HLT/EMNLP. The Association for Computational
Linguistics.
Joakim Nivre and Ryan McDonald. 2008. Integrating
graph-based and transition-based dependency parsers.
In Proceedings of ACL-08: HLT, pages 950?958,
Columbus, Ohio, June. Association for Computational
Linguistics.
Tomoko Ohta, Sampo Pyysalo, and Jun?ichi Tsujii. 2011.
Overview of the Epigenetics and Post-translational
Modifications (EPI) task of BioNLP Shared Task
2011. In Proceedings of the BioNLP 2011 Workshop
Companion Volume for Shared Task, Portland, Oregon,
June. Association for Computational Linguistics.
Sampo Pyysalo, Tomoko Ohta, Rafal Rak, Dan Sul-
livan, Chunhong Mao, Chunxia Wang, Bruno So-
bral, Jun?ichi Tsujii, and Sophia Ananiadou. 2011.
Overview of the Infectious Diseases (ID) task of
BioNLP Shared Task 2011. In Proceedings of
the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
Sebastian Riedel and Andrew McCallum. 2011. Ro-
bust biomedical event extraction with dual decomposi-
tion and minimal domain adaptation. In BioNLP 2011
Shared Task.
Alexander M. Rush, David Sontag, Michael Collins, and
Tommi Jaakkola. 2010. On dual decomposition and
linear programming relaxations for natural language
processing. In Proc. EMNLP.
Mihai Surdeanu and Christopher D. Manning. 2010. En-
semble models for dependency parsing: Cheap and
good? In Proceedings of the North American Chapter
of the Association for Computational Linguistics Con-
ference (NAACL-2010), Los Angeles, CA, June.
55
