Ext ract ing  the Names  of Genes  and Gene Products  w i th  a 
H idden Markov  Mode l  
Nige l  Co l l i e r ,  Ch ikash i  Nobata  and J un - i ch i  Tsu j i i  
l )el)artm(mt of Information Science 
(h'aduate School of Science 
University of Tokyo, Hongo-7-3-1 
Bunkyo-ku,  Tokyo 113, .Japan 
E-maih {n ige l ,  nova, t su j  ??}@?s. s. u - tokyo ,  ac. jp 
Abst ract  
\~e report the results of a study into the use 
of a linear interpolating hidden Marker model 
(HMM) for the task of extra.('ting lxw\]mi(:al |;er- 
minology fl:om MEDLINE al)stra('ts and texl;s 
in the molecular-bioh)gy domain. Tiffs is the 
first stage isl a. system that will exl;ra('l; evenl; 
information for automatically ut)da.ting 1)ioh)gy 
databases. We trained the HMM entirely with 
1)igrams based (m lexical and character fea- 
tures in a relatively small corpus of 100 MED- 
LINE abstract;s that were ma.rked-ul) l)y (lo- 
main experts wil;h term (:lasses u(:h as t)rol;eins 
and DNA. I.Jsing cross-validation methods we 
a(:\]fieved a,n \].e-score of 0.73 and we (',xmnine the 
('ontrilmtion made by each 1)art of the interl)o- 
lation model to overconfing (la.ta Sl)arsen('.ss. 
1 In t roduct ion  
Ill the last few ye~trs there has t)een a great in- 
vestment in molecula.r-l)iology resear(:h. This 
has yielded many results l;\]la.1;, 1;ogel;her wil;h 
a migration of m:c\]fival mal;erial to the inter- 
net, has resulted in an exl)losion in l;tm nuns- 
\])el7 of research tmbli('ations aa~ailat)le in online 
databases. The results in these 1)al)ers how- 
ever arc not available ill a structured fornmt and 
have to 1)e extracted and synthesized mammlly. 
Updating databases such as SwissProt (Bairoch 
mid Apweiler, 1.997) this way is time (:onsmning 
and nmans l;h~tt he resull;s are not accessible so 
conveniently to he11) researchers in their work. 
Our research is aimed at autonmti(:ally ex- 
tra(:ting facts Kern scientific abstracts and flfll 
papers ill the molecular-biology domain and us- 
ing these to update databases. As the tirst stage 
in achieving this goal we have exl)lored th(; use 
of a generalisable, supervised training method 
based on hidden Markov models (ItMMs) (Ra- 
biner and .\]uang, 1986) fbr tim identification mid 
classitieation of technical expressions ill these 
texts. This task can 1)e considered to be similar 
to the named c.ntity task in the MUC evaluation 
exercises (MUC, 1995). 
In our current work we are using abstracts 
available fl:om PubMed's MEDLINE (MED- 
\],INE, 1999). The MEDLINE (lnta.l)ase is an 
online collection of al)straets for pul)lished jour- 
nal articles in biology mid medicine and con- 
tains more than nine million articles. 
With the rapid growth in the mlmbcr of tmb- 
\]ished l)al)ers in the field of moh;('ular-biolog 3, 
there has been growing interest in the at)pli- 
cation of informa.tion extra(:tion, (Sekimizu et 
al., 1998) (Collier et al, 1999)(Thomas et al, 
1999) (Craven and Kmnlien, 1999), to help solve 
souse (sf the t)robhmss that are associated with 
information overload. 
In the remainder of this i)aper we will first 
of all (ratline the t)ackground to the task and 
then d(~s('ril)e t;hc basics of ItMMs and the fi)r- 
real model wc are using. The following sections 
give an outline of a. lse\v tagged ('orlms (Ohta et 
al., 1999) thnt our team has deveh)i)ed using al)- 
stra('ts taken from a sub-domain of MEDLINF, 
and the results of our experinmnts on this cor- 
lmS. 
2 Background 
Ileeent studies into the use of SUl)ervised 
learning-t)ased models for the n~mled entity task 
in the miero-lsioh)gy domain have. shown that 
lnodels based on HMMs and decision trees such 
as (Nol)al;~t et al, 1999) ~,r(; much more gener- 
alisable and adaptable to slew classes of words 
than systems based on traditional hand-lmilt 
1)attexns a.nd domain specific heuristic rules 
such as (Fukuda et al, 1998), overcoming the 
1)rol)lems associated with data sparseness with 
the help of sophisticated smoothing algorithms 
201 
(Chen and Goodman, 1996). 
HMMs can be considered to be stochastic fi- 
nite state machines and have enjoyed success 
in a number of felds including speech recogni- 
tion and part-of-speech tagging (Kupiec, 1992). 
It has been natural therefore that these mod- 
els have been adapted tbr use in other word- 
class prediction tasks such as the atoned-entity 
task in IE. Such models are often based on n- 
grams. Although the assumption that a word's 
part-of speech or name class can be predicted 
by the previous n-1 words and their classes is 
counter-intuitive to our understanding of lin- 
guistic structures and long distance dependen- 
cies, this simple method does seem to be highly 
effective ill I)ractice. Nymble (Bikel et al, 
1997), a system which uses HMMs is one of the 
most successflfl such systems and trains on a 
corpus of marked-up text, using only character 
features in addition to word bigrams. 
Although it is still early days for the use of 
HMMs for IE, we can see a number of trends 
in the research. Systems can be divided into 
those which use one state per class such as 
Nymble (at the top level of their backoff model) 
and those which automatically earn about the 
model's tructure such as (Seymore t al., 1999). 
Additionally, there is a distinction to be made 
in the source of the knowledge for estimating 
transition t)robabilities between models which 
are built by hand such as (Freitag and McCal- 
lure, 1999) and those which learn fl'om tagged 
corpora in the same domain such as the model 
presented in this paper, word lists and corpora 
in different domains - so-called distantly-labeled 
data (Seymore t al., 1999). 
2.1 Challenges of name finding in 
molecu lar -b io logy  texts  
The names that we are trying to extract fall into 
a number of categories that are often wider than 
the definitions used for the traditional named- 
entity task used in MUC and may be considered 
to share many characteristics of term recogni- 
tion. 
The particular difficulties with identit)dng 
and elassit~qng terms in the molecular-biology 
domain are all open vocabulary and irrgeular 
naming conventions as well as extensive cross- 
over in vocabulary between classes. The irreg- 
ular naming arises in part because of the num- 
ber of researchers from difli;rent fields who are 
TI - Activation of <PROTEIN> JAK kinases 
</PROTEIN> and <PROTEIN>STAT pTvteins 
</PR, OTEIN> by <PROTEIN> interlcukin - 2 
</PROTEIN> and <PROTEIN> intc~fc~vn alph, a
</PROTEIN> , but not the <PROTEIN> T cell 
antigen receptor <~PROTEIN> , in <SOURCE.ct> 
h, uman T lymphoeytes </SOURCE.et> . 
AB The activation of <PROTEIN> Janus 
protein t,.flvsine kinascs </PROTEIN> ( 
<PROTEIN> JAI(s </PROTEIN> ) and 
<PROTEIN> signal transducer and ac- 
tivator of transcription </PROTEIN> ( 
<PROTEIN> STAT </PROTEIN> ) pro- 
reins by <PROTEIN> intcrIcukin ( IL ) 2 
</PROTEIN> , thc  <PROTEIN> T cell antigen 
receptor </PROTEIN> ( <PROTEIN> TCR 
</PROTEIN> ) and <PROTEIN> intc~fcrvn 
( IFN)  alpha </PROTEIN> was czplorcd in 
<SOURCE.ct> human periph, cral blood- derived 
T cclls </SOURCE.et> and the <SOURCE.el> 
leukemic T cell line Kit225 </SOURCE.el> .
Figure 1: Example MEDLINE sentence marked 
up in XML for lfiochemical named-entities. 
working on the same knowledge discovery area 
as well as the large number of substances that 
need to be named. Despite the best, etforts of 
major journals to standardise the terminology, 
there is also a significant problem with syn- 
onymy so that often an entity has more tlm.n 
one name that is widely used. The class cross- 
over of terms arises because nla l ly  prot(:ins are 
named after DNA or RNA with which they re- 
act. 
All of the names which we mark up must be- 
long to only one of the name classes listed in 
Table 1. We determined that all of these name 
classes were of interest o domain experts and 
were essential to our domain model for event 
extraction. Example sentences from a nmrked 
ut) abstract are given in Figure 1. 
We decided not to use separate states ibr 
pre- and post-class words as had been used in 
some other systems, e.g. (Freitag and McCal- 
lure, 1999). Contrary to our expectations, we 
observed that our training data provided very 
poor maximum-likelihood probabilities for these 
words as class predictors. 
We found that protein predictor words had 
the only significant evidence and even this was 
quite weak, except in tlm case of post-class 
words which included a mmfi)er of head nouns 
such as "molecules" or "heterodimers". In our 
202 
Class ~/: Examl)le l)escription 
P1K)TEIN 21.25 .MK ki'n,a.se 
\])NA 358 IL-2 \]rlvmotcr 
\]{NA 30 771I?, 
S()UI{CF,.cl 93 le'ukemic T cell line Kit225 
S()UI\],CE.(:t 417 h,'wm, an T lymphocytes 
SOURCE.too 21 ,%hizosacch, aromyces pombc 
S()URCE.mu 64 mice 
SOURCE.vi 90 ItJV-1 
S()UI{CE.sl 77 membrane 
S()UI{CE.ti 37 central 'ner,vo'us system 
UNK t,y~vsine ph, osphovylal, ion 
t)ro{xfiils~ protein groups, 
families~ cOral)loxes and Slll)Sl;I'llCI;lll'eS. 
I)NAs I)NA groups, regions and genes 
RNAs I~NA groups, regions and genes 
cell line 
(:ell type 
lllOll()-organism 
multiorganism 
viruses 
sublocat;ion 
tissue 
lmckground words 
Table l: Named (mtilsy (:lasses. ~/: indi(:at(ts tsfic ~mmt)cr of XMI, tagged terms in our (:orpus of 100 
abstracts. 
early experiments using I IMMs that in(:orpo- 
rated pro- and 1)ost-c\]ass tates we \[imnd tha.t 
pcrforlnance was signiticantly worse than wil;h- 
Ollt; sll(;h si;at;cs an(l st) w('. formulated the ~uodcl 
as g,~ivcll i l S(;(;\[;iOll :/. 
~,.f(Qi,..~,l < _,Ffi,..~.,, >) + 
(1) 
and for all other words and their name classes 
as tbllows: 
3 Mx.'tzho d 
The lmrl)osc of our mod(;1 is Io lind t;hc n,osl: 
likely so(tilth, liCe of name classes (C) lbr a given 
se(tucncc of wor(ls (W). The set of name ('lasses 
inchutcs the 'Unk' name (:lass whi('h we use li)r 
1)ackgromM words not 1)elonging to ally ()\[ the 
interesting name classes given in Tal)lc 1 and 
t;hc given st;qu(m(:e of words which w(~ ,>('. spans 
a single s(,Jd;cn('c. The task is thcrcfor(~ 1(} max- 
intize Pr((TIH:). \?c iml)lem(mt a I \ ]MM to es- 
t imate this using th('. Markov assuml)tion that 
P r (C I I?  ) can be t'(mnd from t)igrams of ha.me 
classes. 
In th('. following model we (:onsid(u" words to 
1)c ordered pairs consisting of a. surface word, 
W, and a. word tbature, 1", given as < W, F >. 
The word features thcms('Jvcs arc discussed in 
Section 3.1. 
As is common practice, we need to (:alculatc 
the 1)rol)abilities for a word sequence for the 
first; word's name class and every other word 
diflbrently since we have no initial nalnt>class 
to make a transit ion frolll. Accordingly we use 
l;he R)llowing equation to (:alculatc the ilfitial 
name (:lass probability, 
~,,J'(Cz,..~,,I < wi~,..~, 19~,.,~,, >) + 
I',,.( G 
)~o.1' ( G 
A ,./' (G 
;v~.f (G 
5:I./'(G 
), ~./' (G 
),,~.I(G) 
< Wt,l,} >,< l lS,_,, l , i  ~ >,G J) :- 
< 1'15., I ~,, >, < I,V~_~, l )_j >, G.-~) + 
< _, l'i >, < 115_ l, Ft,- ~ >, Ct-., ) + 
< 115, Fi >, < _, P~,_~ >, G ~) + 
< _, l,) >, < ._, 1% ~ >, C~__~) + 
(2) 
whc,:c f(I) is ('alculatcd with nmxinluln- 
likelihood estimates from counts on training 
data, so that tbr example, 
.f(G,I < 1,~5,1,i >,< I,t,~_,, F~_~ >,G-~)  - 
T(< I lS, 1,~ >, G., < 1'15_,, 1~}_~ >, G.-@, 
T(< l'lZj,,l~J >,< \ [ 'Vt- l ,Ft- I  >,Ct- l )  ~3) 
Where T() has been found from counting the 
events in thc training cortms. In our current 
sysl;oln \vc SC\[; t;tlc C()llSt;&lltS ~i }lJld o- i \])y halld 
all(l let ~ ai = 1.0, ~ Ai = 1.0, a0 > al k O-2, 
A0 > A I . . .  _> As. Tile current name-class Ct 
is conditioned oil the current word and fea- 
t;llrc~ thc I)rcviolls name-class, ~*t--l: and t)rc- 
vious word an(t tbaturc. 
Equations 1 and 2 implement a linear- 
interpolating HMM that  incorporates a mmfl)cr 
203 
of sub-models (rethrred to fl'om now by their 
A coefficients) designed to reduce the effects of 
data sparseness. While we hope to have enough 
training data to provide estimates tbr all model 
parameters, in reality we expect to encounter 
highly fl'agmented probability distributions. In 
the worst case, when even a name class pair 
has not been observed beibre in training, the 
model defaults at A5 to an estimate of name 
class unigrams. We note here that the bigram 
language model has a non-zero probability asso- 
ciated with each bigram over the entire vocal)- 
ulary. 
Our model differs to a backoff ormulation be- 
cause we tbund that this model tended to suffer 
fl'om the data sparseness problem on our small 
training set. Bikel et alfor example consid- 
ers each backoff model to be separate models, 
starting at the top level (corresl)onding approx- 
imately to our Ao model) and then falling back 
to a lower level model when there not enough 
evidence. In contrast, we have combined these 
within a single 1)robability calculation tbr state 
(class) transitions. Moreover, we consider that 
where direct bigram counts of 6 or more occur 
in the training set, we can use these directly to 
estimate the state transition probability and we 
nse just the ,~0 model in this case. For counts 
of less than 6 we smooth using Equation 2; this 
can be thought of as a simt)le form of q)nck- 
eting'. The HMM models one state per name 
(:lass as well as two special states tbr the start 
and end o fa  sentence. 
Once the state transition l)rol)abilities have 
been calcnlated according to Equations 1 and 2, 
the Viterbi algorithm (Viterbi, 1967) is used to 
search the state space of 1)ossible name class as- 
signments. This is done in linear time, O(MN 2) 
for 54 the nunfl)er of words to be classified and 
N the number of states, to find the highest prob- 
ability path, i.e. to maxinfise Pr(W,  C). In our 
exl)eriments 5/i is the length of a test sentence. 
The final stage of our algorithm that is used 
after name-class tagging is complete is to use 
~ clean-up module called Unity. This creates a 
frequency list of words and name-classes tbr a 
docmnent and then re-tags the document using 
the most frequently nsed name class assigned by 
the HMM. We have generally tbund that this 
improves F-score performance by al)out 2.3%, 
both tbr re-tagging spuriously tagged words and 
Word Feature Exmnl)le 
DigitNmnber 15 
SingleCap M 
GreekLetter alpha 
CapsAndDigits I2 
TwoCaps RalGDS 
LettersAndDigits p52 
hfitCap Interleukin 
LowCaps ka,t)paB 
Lowercase kinases 
IIyphon 
Backslash / 
OpenSquare \[ 
CloseSquare \] 
Colon 
SemiColon 
Percent % 
Oi) enParen ( 
CloseParen ) 
Comma 
FullStop 
Deternliner the 
Conjmmtion and 
Other * + 
Table 2: Word tbatures with examples 
tbr finding untagged words in mlknown contexts 
that had been correctly tagged elsewhere in the 
text. 
3.1 Word  features  
Table 2 shows the character t'eatnres that we 
used which are based on those given for Nymble 
and extended to give high pertbrmance in both 
molecular-biology and newswire domains. The 
intnition is that such features provide evidence 
that helps to distinguish nmne classes of words. 
Moreover we hyt)othesize that such featnres 
will help the model to find sinfilarities between 
known words that were tbnnd in the training 
set and unknown words (of zero frequency in 
the training set) and so overcome the unknown 
word t)rol)lem. To give a simple example: if we 
know that LMP - 1 is a member of PROTEIN  
and we encounter AP - 1 for the first time in 
testing, we can make a fairly good guess about 
the category of the unknown word 'LMP' based 
on its sharing the same feature TwoCaps  with 
the known word 'AP' and 'AP's known relation- 
ship with '- 1'. 
Such unknown word evidence is captured in 
submodels A1 through ),3 in Equation 2. \?e 
204 
consider that character information 1)rovides 
more mealfingflll distinctions between name 
(;\]asses than for examI)le part-of-speech (POS), 
since POS will 1)redominmltly 1)e noun fi)r all 
name-class words. The t'catures were chosen 
to be as domain independent as possit)le, with 
the exception of I lyphon and Greel,:Letter which 
have t)articular signitieance for the terminology 
in this dolnain. 
4 Exper iments  
4.1 Tra in ing  and  tes t ing  set 
The training set we used in our experiments 
('onsisted of 100 MEI)II, INI~ al)stra(:ts, marked 
Ul) ill XS/\[L l)y a (lonmin ext)ert for the name 
('lasses given in Tal)le 1. The mmfl)er of NEs 
that were marked u 1) by class are also given in 
Tfl)le 1 and the total lmmber of words in the 
corlms is 299/\]:0. The al)stracts were chosen from 
a sul)(lomain of moleeular-1)iology that we for- 
mulated by s(',ar(;hing under the terms h/uman, 
blood cell, trav,.scription ,/'actor in the 1)utiMed 
datal)asc, This yiel(l('.(t al)t)roximately 33(10 al/- 
stracts.  
4.2 Resu l ts  
The results are given as F-scores, a (;Ollllll()ll 
measurement for a(:(:ura(:y in tlw, MUC con- 
ferences that eonfl)ines r(;(:all and 1)re(:ision. 
These are eah:ulated using a standard MUC tool 
(Chinchor, 1995). F-score is d('.iin(~d as 
'2 x lS"(eci.sion x l~cc, ll 
F - .~cor.  = (4) 
l)'rccisio~, + \]?,cc(dl 
The tirst set ot7 experiments we did shows the 
effectiveness of the mode.1 for all name (:lasses 
and is smnmarized in Table 3. We see that data 
sparseness does have an etfe('t~ with 1)roteins - 
the most mlmerous (;lass in training - getting 
the best result and I/,NA - the snmllc, st training 
(:lass - getting the worst result. The tal)le also 
shows the ett'eetiveness of the character feature 
set, whi('h in general adds 10.6% to the F-score. 
This is mainly due to a t)ositive effect on words 
in the 1)R,OTEIN and DNA elases, but we also 
see that memt)ers of all SOURCE sul)-('lasses 
sufl'er from featurization. 
We have atteml)ted to incorl)orate generali- 
sation through character t'eatm:es and linear in- 
teri)olation, which has generally \])een quite su(:- 
cessful. Nevertheless we were (:urious to see just 
Class Base llase-l'eatures 
PROTEIN 0.759 0.670 (-11.7%) 
DNA 0.472 0.376 (-20.3%) 
\]~NA 0.025 0.OOO (-leo.o%) 
SOURCE(all) 0.685 0.697 (+1.8%) 
S()UI{CE.cl 0.478 0.503 (+5.2%) 
SOURCE.el 0.708 0.752 (+6.2%) 
SOURCE.me 0.200 0.311 (+55.5%) 
SOURCE.mu 0.396 0.402 (+1.5%) 
SOURCE.vi 0.676 0.713 (+5.5%) 
S()URCI,Lsl 0.540 0.549 (+1.7%) 
SOURCE.ti 0.206 0.216 (+4.9%) 
All classes 0.728 0.651 (-10.6%) 
q)d)le 3: Named entity acquisition results us- 
ing 5-fi)ld cross validation on 100 XML tagged 
MEI)I~INE al/stra(:ts, 80 for training and 20 fin. 
testing, l\]ase-J'(',at'urc.s u es no character feature 
inibrmation. 
)~ Mode\[ No. 
# Texts 0 1 2 3 4 5 
80 
40 
20 
10 
5 
0.06 0.22 0.10 0.67 0.93 1.0 
0.06 0.19 0.10 0.63 0.94 1.0 
().()~l 0.15 0.09 0.59 0.89 1.0 
0.03 0.12 0.08 0.52 0.83 1.0 
0.02 0.09 0.06 0.41 0.68 1.0 
Tal)le 4: M(',an lmml)er of successflll calls to sul)- 
m(i(t(;ls during testing as a fl'aetion of total mnn- 
1)er (If stale transitions in the Viterl)i latti(:e, g/: 
T(!xis indicates the mmfl)er of al)stra(:ts used ill 
training. 
whi(:h t)arts of the model were contributing to 
the bigram s(:ores. Table 4 shows the l)ercent- 
age of bigranls which could be mat('hed against 
training t)igrams. The result indicate tha~ a 
high 1)ereentage of dire(:t bigrams in the test 
eorl)uS never al)t)(;ar in the training (:oft)us and 
shows tha, t our HMM model is highly depel> 
(l(mt on smoothing through models ~kl and )~:~. 
\?e can take another view of the training data 
1)y 'salalni-slieing' the model so that only evi- 
(tenee from 1)art of the model is used. Results 
are shown in Tat)le 5 and support the eonchl- 
sion that models Al, A2 and Aa are. crucial at 
this sir,(; of training data, although we would 
expect their relative ilnportance to fifil as we 
have more (tircct observations of bigrams with 
larger training data sets. 
Tal)le 6 shows the rolmstness of the model 
205 
I Backoff models 
\[ F-score (all classes) 0.728 0.722 0.644 0.572 0.576 \] 
Table 5: F-scores using different nfixtures of models tested on 100 abstracts, 80 training and 20 
testing. 
I # Texts 80 40 20 10 5 \] 
I F-score 0.728 0.705 0.647 0.594 0.534\] 
Table 6: 
training 
stracts). 
F-score for all classes agMnst size of 
corpus (in number of MEDLINE ab- 
for data sparseness, so that even with only 10 
training texts the model can still make sensible 
decisions about term identification and classi- 
fication. As we would expect;, the table ;flso 
clearly shows that more training data is better, 
and we have not yet reached a peak in pertbr- 
i nance .  
5 Conc lus ion  
HMMs are proving their worth for various 
tasks in inibrmation extraction and the results 
here show that this good performance can be 
achieved across domains, i.e. in molecular- 
biology as well as rising news paper reports. The 
task itself', while being similar to named entity 
in MUC, is we believe more challenging due to 
the large nunfl)er of terms which are not proper 
nouns, such as those in the source  sub-classes as 
well as the large lexieal overlap between classes 
such as PROTEIN  and DNA. A usefifl line of 
work in the future would be to find empirical 
methods for comparing difficulties of domains. 
Unlike traditional dictionary-based lnethods, 
the method we have shown has the advantage of 
being portable and no hand-made patterns were 
used. Additiolmlly, since the character tbatures 
are quite powerful, yet very general, there is lit- 
tle need for intervention to create domain spe- 
cific features, although other types of features 
could be added within the interpolation frame- 
work. Indeed the only thing that is required is 
a quite small corpus of text containing entities 
tagged by a domain expert. 
Currently we have optinfized the ,k constants 
by hand but clearly a better way would be to do 
this antomatically. An obvious strategy to use 
would be to use some iterative learning method 
such as Expectation Maximization (Dempster 
et al, 1977). 
The model still has limitations, most obvi- 
ously when it needs to identity, term boundaries 
for phrases containing potentially ambiguous lo- 
cal structures uch as coordination and pa.ren- 
theses. For such cases we will need to add post- 
processing rules. 
There are of course many NF, models that 
are not based on HMMs that have had suc- 
cess in the NE task at the MUC conferences. 
Our main requirement in implementing a model 
for the domain of molecular-biology has been 
ease of development, accuracy and portability 
to other sub-domains since molecular-biology it-
self is a wide field. HMMs seemed to be the 
most favourable option at this time. Alterna- 
tives that have also had considerable success 
are decision trees, e.g. (Nobata et al, 1.999) 
and maximum-entropy. The maximum entropy 
model shown in (Borthwick et al, 1998) in par- 
ticular seems a promising approach because of 
its ability to handle overlapping and large fea- 
ture sets within n well founded nmthenmtical 
ti'amework. However this implementation of the 
method seems to incorporate a number of hand- 
coded domain specitic lexical Datures and dic- 
tionary lists that reduce portability. 
Undoubtedly we could incorporate richer tba- 
tures into our model and based on the evidence 
of others we would like to add head nouns as 
one type of feature in the future. 
Acknowledgements  
We would like to express our gratitude to Yuka 
Tateishi and Tomoko Ohta of the Tsujii labora- 
tory for their efforts to produce the tagged cor- 
tins used in these experiments and to Sang-Zoo 
Lee also of the Tsujii laboratory tbr his com- 
ments regarding HMMs. We would also like to 
thank the anonymous retirees tbr their helpflfl 
comments. 
206 
\]~{,eferences 
A. Bairoch and R. Apweiler. 1997. The SWISS- 
PF\[OT 1)r{)t{~in sequence data bank and its 
new SUl)l)lement 15:EMBL. Nucleic Acids Re- 
search, 25:31-36. 
D. Bikel, S. Miller, I:L Schwartz, and 
R. Wesichedel. 1997. Nymble: a high- 
t)ertbrmanee l arning \]mlne-tin(ler. In Pro- 
ceedings of the Fifth Co~@rcrcncc on Applied 
Natural Langua9 e \])~vcessi'n,g, pages 194 201. 
A. Borthwick, J. Sterling, E. Agichtein, and 
ll,. Grishman. 1998. Ext}l{}iting div(:rse 
knowledge sour(:es via lllaXillllllll (mtrol}y in 
named entity recogniti{}n. In P'mcccdings 
of the Worlcshop on Very Lar.qc Corpora 
(WVLC'98). 
S. Chert and J. Goodman. 1996. An empirical 
study of smoothing te{:hmfiques tbr language 
motleling. 3/tst Annual Meeting of tlt,(: Associ- 
ation of Computational Linguistics, Calffof 
nia, USA, 24-27 .hme. 
N. Chin{:h{}r. 1995. MUC-5 ewduati{m etrics. 
In In Pwcecdings of th, c i"ffl, h, Mc.ss(u.le Un- 
dcrstandin 9 Cou:fe'rencc (MUC-5), Baltimore,, 
Maryland, USA., 1)ages 69 78. 
N. Collier, It.S. Park, N. Ogata, Y. Tateishi, 
C. Nol}ata, 'F. Ohta, T. Sekimizu, H. \]mai, 
and J. Tsujii. 1999. The GENIA 1}r{)je(:t: 
corlms-1)ascd kn(}wlcdge acquisitio\], and in- 
forlnal, ion extra('tion f\]'Olll genome r{',sear(:h 
t)al)ers, in Proccediu, fl.s of the A n',,'aal M(',eting 
of the European ch, aptcr of the Association for 
Computational Lingu'istic,s (EA (/\]3 '99), 3 uuc. 
M. Craven and 3, Kumlien. 1999. Construct- 
ing bioh}gical knowh;{tg{; t}ases t)y extracting 
information from text sour(:es. In \]}~vc(:(,Aings 
of the 7th, hl, tcrnational CoTff(:rence on Intelli- 
gent Systcmps for Molecular Biology (ISMB- 
99), Heidellmrg, Germmly, August 6 10. 
A.P. Dempster, N.M. Laird, and D.B. Rubins. 
1977. Maximmn likelihood from incoml)lete 
data via the EM algorithm. ,\]ou'rnal of the 
Royal Statistical Society (B), 39:1-38. 
l). Freitag and A. McCMlum. 1999. Intbrma- 
tion extraction with HMMs and shrinkage. 
In Proceedings of the AAAl'99 Worl~.~h, op ou, 
Machine Learning for IT~:formation Extrac- 
tion, Orlando, Florida, July 19th. 
K. Fuku(la, T. Tsunoda, A. 2)mmra, and 
T. Takagi. 1998. ~12)ward intbrmation extrac- 
tion: identifying l)rotein names from biologi- 
eal papers. Ill PTvcccdings of thc Pac'lific Sym- 
posium on Biocomp'uting'98 (PSB'98), .Jan- 
1uAYy. 
.1. Kupiec. 1992. l/obust Imrt-ofspeech tag- 
ging using a hidden markov model. Computer 
Speech and Lang'aagc, 6:225-242. 
MEI)LINE. 1999. The PubMed 
datal)ase can be t'(mnd at:. 
httt)://www.ncbi.nhn.nih.gov/Pul}Med/. 
DAIIPA. 1995. l}roceeding.s o.fl th, c Sixth, 
Message Understanding Cou:fcrcnce(MUC-6), 
Cohmdfia, MI), USA, Nove, nfl}er. Morgan 
Nail\['\] l lal l l l .  
C. Nobata, N. Collier, and J. Tsu.iii. 1999. Au- 
tomatic term identification and classification 
in 1}iology texts. In Proceeding.s" of the Nat- 
u'ral Lang,lmgc Pacific Rim Symposium (NL- 
PRS'2000), November. 
Y. Ohta, Y. Tateishi, N. Collie'r, C. No- 
1)ata, K. II}ushi, and J. Tsujii. 1999. A 
senmntieally annotated cort)us from MED- 
L\]\[NE al)sl;ra{:l;s. In l}'rocccd,bu.l s of th.c ~:nth. 
Workshop on Go'home I~fformatics. Universal 
A{:ademy Press, Inc., 14 15 Deccntl)er. 
l~. llabiner and B..\]uang. 1!)86. An intro{tu{:- 
ti(m to hidden Markov too(Ms. H'2EE ASSP 
Magazi',,(',, 1}ages d 16, Jammry. 
T. Sekilnizu, H. Park, and J. 'l'sujii. 1998. 
I{lenti\[ying l;he interaction 1)etween genes an{1 
gOlle i}ro(lucts \]}ase(l on f\]'e(lue\]My seen verbs 
in n\]e{tline al)si;rael;s. Ill ~(:'li,()?ll,('~ \]~ffor'm, al, ics'. 
Univcrsa,1 Academy Press, Inc. 
K. Seymore, A. MeCallum, and l{. I{oscnfeld. 
1999. Learning hidden Markove strucl:ure 
for informati{m (,xtraction. In \])wcccdings of 
the AAAl'99 Workshop on Macfli'n,(: Lcarni'n 9 
for l',fo'rmation E:draction, Orland{}, Flori{ta., 
July 19th. 
.J. Thomas, D. Milward, C. Ouzounis, S. Pul- 
man, and M. Carroll. 1999. Automatic ex- 
traction of 1)rotein interactions fl'om s{'ien- 
tific abstracts. In Proceedings of the I}ac'll/ic 
Symposium on Biocomputing'99 (PSB'99), 
Hawaii, USA, Jmmary 4-9. 
A. 3. Vit(;rbi. 1967. Error l){mnds for {:onvolu- 
tions e{}{les and an asyml)totically optimum 
deco(ling algorithm. IEEE Tran,s'actiou,.s' on
I~formation Theory, IT-13(2):260 269. 
207 
Comparison between Tagged Corpora for the Named Entity 
Task 
Chi~sh i  NOBATA N ige l  COLL IER  and Jun ' i ch i  TSUJ I I  
Kansa i  Advanced Research Center  Depar tment  of  In format ion  Science 
Communicat ions  Research Laboratory  Graduate  School of  Science 
588-2 Iwaoka, Iwaoka-cho, Nishi-ku University of  Tokyo, Hongo 7-3-1 
Kobe,  Hyogo, 65\].-2492 JAPAN Bunkyo-ku,  Tokyo,  113-0033 JAPAN 
nova@crl, go. j p {nigel, tsuj ii}@is, s. u-tokyo, ac. jp 
Abst rac t  
We present two measures for compar- 
ing corpora based on infbrmation the- 
ory statistics uch as gain ratio as well 
as simple term-class ~equency counts. 
We tested the predictions made by these 
measures about corpus difficulty in two 
domains - -  news and molecular biol- 
ogy - -  using the result of two well-used 
paradigms for NE, decision trees and 
HMMs and found that gain ratio was the 
more reliable predictor. 
made by these measures against actual system 
performance. 
Recently IE systems based on supervised learn- 
ing paradigms uch as hidden Markov models 
(Bikel et al, 1997), maximum entropy (Borth- 
wick et al, 1998) and decision trees (Sekine et 
al., 1998) have emerged that should be easier to 
adapt to new domains than the dictionary-based 
systems of the past. Much of this work has taken 
advantage of smoothing techniques to overcome 
problems associated with data sparseness (Chen 
and Goodman, 1996). 
The two corpora we use in our NE experiments 
represent the following domains: 
1 In t roduct ion  
With the advent of the information society and 
increasing availability of large mounts  of infor- 
mation in electronic form, new technologies such 
as information extraction are emerging to meet 
user's information access needs. Recent evalu- 
ation conferences such as TREC (Voorhees and 
Harman, 2000) showed the feasibility of this task 
and highlighted the need to combine information 
ret r ied  (m) and extraction (IE) to go beyond 
simply offering the user a long ranked list of in- 
teresting documents to providing facts for user's 
questions. 
The problem of domain dependence r mains a 
serious one and in fact there has been very little 
work so far to compare the difllculty of IE tasks for 
different domaln~ and their corpora. Such knowl- 
edge is useful for developing IE systems that are 
portable between domains. This paper begins to 
address this issue, in particular the lowest level of 
IE task, defined in the TIPSTER sponsored MUC- 
6 conference (MUC, 1995) as named entity (NE). 
This is emerging as a key technology in several 
other IF-related tasks such as question answer- 
ing. We seek here to show theoretically motivated 
measures for comparing the ditficulty of corpora 
for the NE task in two domains, newswire and 
molecular-biology. We then test the predictions 
? Newswire: acquisition of names of people, or- 
ganizations and monetary units etc., from the 
MUC-6 data set. 
? Molecular-biology: acquisition of proteins, 
DNAs, RNAs etc. from a subset of the MED- 
LINE database (MEDLINE, 1999). 
Information extraction in the molecular-biology 
domain (Seldmlzu et al, 1998) (Craven and Kum- 
lien, 1999) (Rindflesch et al, 2000) has recently 
become a topic of interest o the NLP community. 
This is a result of the need to formalise the huge 
number of research results that appear in free-text 
form in online collections of journal abstracts and 
papers such as MEDLINE for databases such as 
Swissprot (Ban:och and Apwefler, 1997) and also 
to search such collections for facts in an intelligent 
way. 
The purpose of our study is not to show a high 
level of absolute system performance. In fact since 
we use only the MUC-6 executive succession data 
set of 60 articles and a new MEDLINE data set 
of 100 articles we cannot hope to achieve perfor- 
mance limits. What we aim to do is to compare 
model performance against he predictions of cor- 
pus difficulty made by two different methods. In 
the rest of this paper we firstly introduce the NE 
models used for evaluation, the two corpora we 
20 
examined and then the difficulty comparison met- 
rics. Predictive scores from the metrics are ex- 
amined against he actual performance of the NE 
models. 
2 Mode ls  
Recent studies into the use of supervised learning- 
based modeels for the NE task in the molecular- 
biology domain have shown that models based on 
hidden Markov models (HMMs) (Collier et al, 
2000) and decision trees (Nobata et al, 1999) are 
not only adaptable to this highly technical do- 
main, but are also much more generalizable to new 
classes of words than systems based on traditional 
hand-built heuristic rules such as (Fukuda et al, 
1998). We now describe two models used in our 
experiments based on the decision trees package 
C4.5 (Quiuian, 1993) and HMMs (Rabiner and 
Juang, 1986). 
2.1 Decision tree named entity 
recogniser:NE-DT 
A decision tree is a type of classifier which 
has "leaf nodes" indicating classes and "decision 
nodes" that specify some test to be carried out, 
with one branch or subtree for each possible out- 
come of the test. A decision tree can be used 
to classify an object by starting at the root of 
the tree and moving through it until a leaf is en- 
countered. When we can define suitable features 
for the decision tree, the system can achieve good 
performance with only a small amount of training 
data. 
The system we used is based on one that was 
originally created for Japanese documents (Seine 
et al, 1998). It has two phases, one for creating 
the decision tree from training data and the other 
for generating the class-tagged text based on the 
decision tree. When generating decision trees, tri- 
grams of words were used. For this system, words 
are considered to be quadruple features. The fol- 
lowing features are used to generate conditions in 
the decision tree: 
Par t -o f -speech in format ion:  There are 45 
part-of-speech categories, whose definitions 
are based on Pennsylvania Treebank's cat- 
egories. We use a tagger based on Adwait 
Ratnaparkhi's method (Ratnaparkhi, 1996). 
Character type in format ion:  Orthographic 
information is considered such as upper case, 
lower case, capitalization, numerical expres- 
sions, symbols. These character features 
are the same as those used by NEHMM 
described in the next section and shown in 
Table 1. 
Word  l ists specif ic to  the  domain :  Word 
lists are made from the training corpus. 
Only the 200 highest fxequency words are 
used. 
2.2 H idden Markov  mode l  named ent i ty  
reco~. i ser :  NEHMM 
HMMs are a widely u~d class of learning algo- 
rithms and can be considered to be stochastic fi- 
nite state machines. In the following model, sum- 
marized here from the full description given in 
(Collier et al, 2000), we consider words to be or- 
dered pairs consisting of a surface word, W, and 
a word feature, F ,  given as < W, F >. The word 
features themselves are discussed below. As is 
common practice, we need to calculate the prob- 
abilities for a word sequence for the first word's 
name class and every other word differently since 
we have no initial name-class to make a transition 
from. Accordingly we use the following equation 
to calculate the initial name class probability, 
Pr(NC~\[ < Wf~,t , Flli,,~ >)= 
aof(NC$,,s,\[ < Wf,,,,,Ffi,,t >)+ 
o~f(gcs~,,,I < -,Ff~,,, >) + 
a~f(NCfi,.,,) (i) 
and for all other words and their name classes 
as follows: 
Fr(NCT~ I < Wt,Ft >,< W~-,,Ft-, >,NC~-i) = 
Aof(NGtl < W~,F~ >,< Wt-,,Ft-1 >,NG~-,) + 
Alf(NCtI < .,F~ >,< W~-I,F~-i >,NC~- i )+ 
A2f(NC~I < W,,F~ >, < .. F,-, >,NCt-x) + 
AsI(NG, I < .,Ft >,< _, F~-, >,NG,- ,)+ 
A4f(NC, INC,-,) + 
Asf(NC,) (2) 
where f(I) is calculated with maximum- 
likelihood estimates from counts on training data. 
In our current system we set the constants Ai 
and al by hand and let ~ ai = 1.0, ~ Ai = 1.0, 
ao _> al  > ~,  ~o >_ A , . . .  >_ As. The cur- 
rent name-class NCt is conditioned on the cur- 
rent word and feature, the previous name-class, 
NCt-1, and previous word and feature. 
Equations 1 and 2 implement a linear- 
interpolating HMM that incorporates a number of 
sub-models designed to reduce the effects of data 
sparseness. 
Table 1: Word features v~ith examples 
Word Feature Example 
TwoDig i tN~ 25 
FourDigitNumber 2000 
DigitNumber 15012 
SingleCap M 
GreekLetter alpha 
CapsAndDigits 12 
TwoCaps RalGDS 
LettersAnd.Digits p52 
In i tCap Interleukin 
LowCaps kappaB 
Lowercase kinases 
Hyphon 
Backslash / 
Feature Ex. 
CloseSquare \] 
Colon 
SemiColon ; 
Percent % 
OpenParen ( 
CloseParen ) 
Comma 
FullStop . 
Determiner the 
Conjunction and 
Other *+~ 
Once the state transition probabilities have 
been calculated according to Equations 1 and 2, 
the Viterbi algorithm (Viterbi, 1967) is used to 
search the state space of possible name class as- 
signments in linear time to find the highest prob- 
ability path, i.e. to maximise Pr(W, NC). The fi- 
nal stage of our algorithm that is used after narae- 
class tagging is complete is to use a clean-up mod- 
ule called Unity. This creates a frequency list 
of words and name-classes and then re-tags the 
text using the most frequently used name class 
assigned by the HMM. We have generally found 
that this improves F-score performance by be- 
tween 2 and 4%, both for re-tagging spuriously 
tagged words and for finding untagged words in 
unknown contexts that had been correctly tagged 
elsewhere in the text. 
Table 1 shows the char~ter  features that we 
used in both NEHMM and NE-DT. Our intuition 
is that such features will help the model to find 
similarities between known words that were found 
in the training set and unknown words and so 
overcome the unknown word problem. 
3 Corpora  
We used two corpora in our experiments repre- 
senting two popular domains in IE, molecular- 
biology (from MEDLINE) and newswire texts 
(from MUC-6). These are now described. 
3.1 MUC-6  
The corpus for MUC-6 (MUC, 1995) contains 60 
articles, from the test corpus for the dry and for- 
malruns. An example canbe seenin Figure 1. We 
can see several interesting features of the domain 
such as the focus of NF.,s on people and organiza- 
tion profiles. Moreover we see that there are many 
pre-name clue words such as "Ms." or "Rep." indi- 
cating that a Republican politician's name should 
follow. 
3.2 Biology 
In our tests in the domain of molecular-biology 
we are using abstracts available from PubMed's 
MEDLIhrE. The MEDLINE database is an online 
collection of abstracts for published journal arti- 
cles in biology and medicine and contains more 
than nine million articles. Currently we have ex- 
tracted a subset of MEDLINE based on a search 
using the keywords human AND blood cell AND 
transcription .factor yielding about 3650 abstracts. 
Of these 100 docmnents were NE tagged for our 
experiments using a human domain expert. An 
example of the annotated abstracts is shown in 
Figure 2. In contrast o MUC-6 each article is 
quite short and there are few pre-class clue words 
making the task much more like terminology iden- 
tification and classification than pure name find- 
ing. 
4 A f i r s t  a t tempt  a t  corpus  
compar i son  based  on  s imple  
token  f requency  
A simple and intuitive approach to NE task dif- 
ficulty comparison used in some previous tudies 
such as (palmer and Day, 1997) who studied cor- 
pora in six different languages, compares class to 
term-token ratios on the assumption that rarer 
classes are more difficult to acquire. The relative 
frequency counts from these ratios also give an in- 
direct measure of the granularity of a class, i.e. 
how wide it is. While this is appealing, we show 
that this approach does not necessarily give the 
best metric for comparison. 
Tables 2 and 3 show the ratio of the number of 
different words used in NEs to the total number 
of words in the NE  class vocabulary. The num- 
ber of different tokens is influenced by the corpus 
size and is not a suitable index that can uniformly 
show the difficulty for different NE tasks, there- 
fore it should be normalized. Here we use words 
as tokens. A value close to zero indicates little 
variation within the class and should imply that 
the class is easier to acquire. We see that the NEs 
in the biology domain seem overall to be easier 
to acquire than those in the MUC-6 domain given 
hxical variation. 
The figures in the second columns of Tables 2 
and 3 are normalized so that all numerals are re- 
placed by a single token. It still seems though 
that MUC-6 is a considerably more eheJlenging 
domain than biology. This is despite the fact that 
the ratios for ENAMEX expressions such as Date, 
22 
A graduate of <ENAMEX TYPE=" ORGANIZATION" >Harvard Law SChooI</ENAMEX>, Ms. 
<ENAMEX TYPE="PERSON'>Washington</ENAMEX> worked as a laywer for the corporate fi- 
nance division of the <ENAMEX TYPE='ORGANIZATION~>SEC</ENAMEX> in the late <TIMEX 
TYPE='DATE">1970s</TIMEX>. She has been a congressional staffer since <TIMEX TYPE= 
"DATE'>1979</TIMEX>. Separately, <ENAMEX TYPE='PERSON'>Clintou</ENAMEX> transi- 
tion officials said that <ENAMEX TYPE='PERSON">Frank Newman</ENAMEX>, 50, vice chairman 
and chief financial officer of <ENAMEX TYPE=" ORGANIZATION" >BankAmerica Corp.</ENAMEX>, 
is expected to be nominated as assistant <ENAMEX TYPE="ORGANIZATION~>Treasury</ENAMEX> 
secretary for domestic finance. 
Figure 1: Example sentences taken from the annotated MUC-6 NE text 
<PROTEIN>SOX-4</PROTEIN>, an <PROTEIN>Sty-like HMG box protein</PROTEIN>, is 
a transcriptional activator in <SOLrRCE.cell-type>lymphocytes</SOUl:tCE>. Previous studies in 
<SOURCE.cell-type>lymphocytes</SOUB.CE> have described two DNA-binding <PROTEIN>HMG 
bax proteins</PROTEIN>, <PROTEIN>TCF-I</PROTEIN> and <PROTEIN>LEF-I</PROTEIN>, 
with affinity for the <DNA>A/TA/TCAAAG motif</DNA> found in several <SOURCE.cell-type>T 
cell</SOUl~CE>-specific enhancers. Evaluation of cotransfection experiments in <SOURCE.cell-type>non- 
T cells</SOURCE> and the observed inactivity of an <DNA>AACAAAG concatamer</DNA> in the 
<PROTEIN>TCF-1 </PROTEIN> / <PROTEIN>LEF-1 </PROTEIN>-expressing <SOURCE.cell-line>T 
cell line BW5147</SOURCE>, led us to conclude that these two proteins did not mediate the observed 
enhancer effect. 
Figure 2: Example sentences taken from the annotated biology text 
Table 2: Frequency values for words in the MUC-6 
test corpus 
Class 
Org. 
Person 
Loc. 
Date 
Time 
Money 
Percent 
Al l  
Original 
0.28(=507 / 1783) 
0.45(=381 / 838) 
0.38(=148 / 390) 
0.23(=123 / 542) 
1.00(= 3 / 3) 
0.33(=138 / 423) 
0.39(= 42 / 108) 
0.33(=1342/4087) 
Table 3: Frequency values for words in the biology 
corpus 
Norm. numerals Class Original 
0.28(=507 / 1783) DNA 0.21(=245 / 1140) 
0.45(=381 / 838) Protein 0.15(=631 / 4125) 
0.38(=148 / 390) RNA 0.43(= 30 / 70) 
0.11(= 60 / 542) Source 0.16(=248 / 1533) 
1.00(= 3 / 3) All 0.17(=1'154/6868) 
0.05(= 20 / 423) 
0.03(= 3 / 108) 
0.27(=1122/4087) 
Money and Percent all fall significantly. Expres- 
sions in the Time class are so rare however that it 
is di~cult o make any sort of meaningftfl compar- 
ison. In the biology corpus, the ratios are not sig- 
nificantly changed and the NE classes defined for 
biology documents eem to have the same chuj-- 
acteristics as non-numeric ENAMEX classes in 
MUCC-6 documents. 
Comparing between the biology documents and 
the MUC-6 documents, we may say that identify- 
ing entities in biology docmnents is easier than 
identifying ENAMEX entities in MUC-6 docu- 
ments. 
5 Exper iments  
We evaluated the performance ofour two systems 
using a cross validation method. For the MUC- 
6 corpus, 6-fold cross validation was performed 
on the 60 texts and 5-fold cross validation was 
performed for the 100 texts in the biology corpus. 
Norm. numerals 
0.20(=228 / 1140) 
0.13(=540 / 4125) 
0.43(= 30 / 70) 
0.16(=242 / 1833). 
0.15(=I040/6868) 
We use "F-scores ~for evaluation of our experi- 
ments (Van Rijsbergen, 1979). "F-score" is a mea- 
surement combining "Recall" and "Predsion" and 
defined in Equation 3. "Recall" is the percent- 
age of answers proposed by the system that corre- 
spond to those in the human-made key set. "Pre- 
cision" is the percentage of correct answers among 
the answers proposed by the system. The F-scores 
presented here are automatically calculated using 
a scoring program (Chinchor, 1995). 
2 x Precision x Recall 
F-score = Precision + Recall (3) 
In Table 4 we show the actual performance 
of our term recognition systems, NE-DT and 
NEHMM. We can see that corpus comparisons 
based only on class-token ratios are inadequate o 
explain why both systems' performance was about 
the same in both domains or why NEHMM did 
better in both test corpora than NE-DT. The dif- 
ference in performance is despite there being more 
training examples in biology (3301 NEs) than in 
MUC-6 (2182 NEs). Part of the reason for this is 
97 
Table 4: Performance of the NE systems 
NEHMM with Unity 7&4 75.0 
NEHMM w/o Unity 74.2: 73.1 
NE-DT 68:~-" 69.4 
that the class-token ratios ignore individual sys- 
tem knowledge, i.e. the types of features that 
can be captured and useful in the corpus domain. 
Among other considerations they also fail to con- 
sider the overlap of words and features between 
classes in the same corpus domain. 
6 Corpus  compar i son  based  on  
in fo rmat ion  theoret i ca l  measures  
In this section we attempt o present measures 
that overcome some of the limitations of the class- 
token method. We evaluate tbe contribution from 
each feature used in our NE recognition systems 
by calculating its entropy. There are thee  types of 
feature information used by our two systems: lexo 
ical information, character type information, and 
part-of-speech information. 
The entropy for NE classes H(C) is defined by 
= - E p(c) log 2 p(c) H(C) 
cEC 
where: 
n(O 
p(c) = "N 
n(c): the number of words in class c 
N: the total number of words in text 
We can calculate the entropy for features in the 
same way. 
When a feature F is given, the conditional en- 
tropy for NE classes H(CIF) is defined by 
- ~ ~ p(~, f) logs p(cll) H(C\]F) 
cEC fEF  
where: 
p(c, I) = .(c, I) 
N 
n(c, I) p(cll) = n(l) 
n(c, f):  the number of words in class c 
with the feature value f 
n(/): the number of words 
with the feature value f 
Using these entropies, we can calculate infor- 
mation gain (Breiman et al, 1984) and gain ra- 
tio (Quinlan, 1990). Information gain for NE 
classes and a feature I(C; F) is given as follows: 
I(C; F) = H(C) - H(CIF ) 
The information gain I(C; F) shows how the fea- 
ture F is related with NE classes C. When F is 
completely independent ofC, the value of I(C; F) 
becomes the minimum value O. The maximum 
value of I(C;_F) is equivalent to that of H(C), 
when the feature F gives sufficient information to 
recognize named entities. Information gain can 
also be calculated by: 
I(C; F) = H(C) + H(F) - H(C, F) 
We show the values of the above three entropies 
in Table 5,6, and 7. In these tables, F is replaced 
with single letters which represent each of the 
model's features, i.e. character types (T), part- 
of-speech (P), and hxical information (W). 
Gain ratio is the normalized value of in.forma- 
tion gain. The gain ratio GR(C; F) is defined by 
GR(C; F) = I(C; F) 
H(C) 
The range of the gain ratio GR(C; F) is 0 < 
GR(C; F) _~ 1 even when the class entropy is 
different in various corpora, so we can compare 
the values directly in the different NE recognition 
tasks. 
6.1 Character types 
Character type features are used to identify 
named entities in the MUCC-6 and biology corpus. 
However, the distribution of the character types 
are quite different between these two types of doc- 
uments as we can see in Table 5. We see through 
the gain-ratio score that character type informa- 
tion has a greater predictive power for classes in 
MUC~ than biology due to the higher entropy 
of character type and class sequences in the bi- 
ology corpus, i.e. the greater disorder of this in- 
formation. The result partially shows why iden- 
tification and classification is harder in biological 
documents than in newspaper articles such as the 
MUC-6  corpus. 
6.2  Part-of -speech 
Table 6 shows the entropy scores for part-of- 
speech (POS) sequences in the two corpora. We 
see through the gain ratio scores that POS infor- 
mation is not so powerful for acquiring NEs in the 
biology domain compared to the MUC-6 domain. 
24 
Table 5: Values of Entropy for character type 
Entropy MUC-6 Biology 
H(T) \[\[ 1.880 2.013 
H(C) II 0.890 1.264 
H(C,T) II 2.345 2.974 
I(C;T) \[I .0.425 0.302 
GR(C;T) H 0.478 0.239 
Table 6: Values of Entropy for POSs 
Entropy MUC-6 Biology 
"H(P) 4.287 4.037 
H(C) 0.890 1.264 
H(C,P) 4.750 5.029 
I(C;P) 0.426 0.272 
GR(C;P) 0.479 0.216 
In fact POS information for biology is far less use- 
ful than character information when we compare 
the results in Tables 5 and 6, whereas POS has 
about the same predictive power as character in- 
formation in the MUC-6 domain. One likely ex- 
planation for this is that the POS tagger we use in 
NE-DT is trained on a corpus based on newspaper 
articles, therefore the assigned POS tags are often 
incorrect in biology documents. 
6.3 Lexical information 
Table 7 shows the entropy statistics for the two 
domains. Although entropy for words in biology 
is lower than MUC-6, the entropy for classes is 
higher leading to a lower gain ratio in biology. We 
also note that, as we would expect, in comparison 
to the other two types of knowledge, surface word 
forms are by far the most useful type of knowledge 
with a gain ratio in MUC-6 of 0.897 compared to 
0.479 for POS and 0.478 for character types in the 
same domain. However, such knowledge is also 
the least generalizable and runs the risk of data- 
sparseness. It therefore has to be complemented 
by more generalizable knowledge such as character 
features and POS. 
Table 7: Values of Entropy for words 
--Entropy MUC-6 Biology 
H(W) 9.570 8.89O 
H(C) 0.890 1.264 
H(C,W) 9.662 9.232 
I(C;W) 0.798 0.921 
~R(C;W) 0.897 0.729 
Table 8: Values of Entropy for NEHMM features 
in the MUC-6 corpus 
GR 
0.994 
0.898 
0.967 
0.798 
0.340 
0.806 
0.461 
0.558 
0.221 
0.806 
0.563 
0.971 
0.633 
Cross Entropy 
5.38(4.08-9.68) 
7.69(6.97-9.32) 
7.73(7.07-9.30) 
4.38(4.12-.-4.82) 
1.62(1.32-1.90) 
7.65(7.11-8.65) 
2.64(2.41-2.97) 
7.91(7.25--8.99) 
2.94(2.70-3.25) 
7.65(7.11-6.65) 
7.92(7.26-9.03) 
5.42(4.10-9.70) 
4.18(3.91-4.60) 
Coverage 
o.44(o.34-o.78) 
O. 77(0.72-0.90) 
0.79(0.73-0.90) 
0.99(0.98-1.00) 
L00(1.00-L00) 
0.65(0.81-0.93) 
1.00(0.99-1.00) 
0.83(0.79-0.92) 
1.00(1.00-1.00) 
0.85(0.81,-0.93) 
0.83(0.79-0.92) 
0.44 (0.34-O.75) 
0.99(0.99--1.00) 
Features. 
for A0 
for Al 
for A2 
for As 
Ct-1 
Wt 
Ft 
Wt- I  
F~-x 
Wt Fz 
W~-l F=-i 
Wt-l,~ 
F~-Lt 
Table 
in the biology corpus 
GR Cross Entropy 
0.977 5.83(5.66-6.14) 
0.793 7.93(7.77-8.08) 
0.929 7.79(7.65-7.85) 
0.643 5.07(4.95-5.21) 
0.315 2.26(2.24--2.28) 
0.694 7.64(7.52-7.78) 
0.257 3.12(3.06--3.19) 
0.423 7.99(7.62-8.08) 
0.093 3.33(3.27-3,43) 
0.694 7.64(7.52-7.78) 
0.424 7.98(7.82-8.04) 
0.904 5.96(5.78-6.24) 
0.339 4.66(4.53-4,78) 
9: Values of Entropy for NEHMM features 
Coverage 
0.49(0.48--0.52) 
o.6o(o.79-o.61) 
o.so(o.70-o.81) 
0.98(0.98-0.98) 
1.00(1.00-I.00) 
0.89(0.87-0.89) 
1.oo(1.OO-l.OO) 
0.87(0.86-0.88) 
1.00(1.00-1.00) 
0.89(0.87-0,89) 
o.87(0.85-0.86) 
0.50(0.49-0.52) 
0.99(0.98-0.99) 
Features 
for ~to 
for A1 
for ~t2 
for As 
Ct- I 
W= 
Fe 
Wt  Ft 
Wt-1 F,-z 
Wz-l,t 
F~-l,t 
6.4 Compar i son  between the 
comblnutlon of features 
In this section we show a comparison of gain ra- 
tio for the features used by both systems in each 
corpus. Values of gain ratio for each feature set 
are shown on the 'GR' column in Tables 8, 9, 10 
and 111. The values of GR show that surface 
words have the best contribution in both corpora 
for both systems. We can see that gain ratio for 
all features in NE-DT is actually lower than the 
top level model for NEHMM in biology, reflecting 
the actual system performance that we observed. 
We also see that in the biology corpus, the com- 
bination of all features in NE-DT has a lower con- 
tribution than in the MUC-6  corpus. This indi- 
cates the limitation of the current feature set for 
the biology corpus and shows that we need to uti- 
lize other types of features in this domain. 
Values for cross entropy between training and 
test sets are shown in Tables 8, 9, 10 and 11 to- 
IOn the 'Features' col, mn~ "(Features) for A#" 
means the features used in each HMM sub- 
model which corresponds with the A# in Eclua- 
tion 2. And also, 'ALL' in Tables 10 and 11 
means all the features used in decision tree, i.e. 
{P~-l,~,,+l,F~-l,t,t+l,W,-1,~,~+l). 
Table 10: Values of Entropy for NE-DT features 
in the MUC-6 corpus 
0.G91~8 ! Cross Entropy 
1.59(1.38-1.77) 
0.402 5.22(5.09..-5.32) 
0.4681 2.66(2.51-2.87) 
0.844 7.36(7.19-7.57) 
0.670 7.89(7.81-7.97) 
0.6691 3.87(3.67-4.07) 
0.977 4.42(4.10-4.88) 
0.822 9.25(9.10-9.40) 
0.807 4.92(4.72-5.08) 
0.998 1.89(1.67-2.16) 
Coverage 
0.12(0.10-0.13) 
1.00(0.99-:t.00) 
L00(0.99-1.00) 
o.81(o.8o~.83) 
0.98(0.96--0.98) 
0.99(0.98-1.00) 
0.36(0.34--0.40) 
0.89(0.87~0.91) 
0.96(0.95--0.96) 
0.15(0.13-9.17) 
Features 
ALL 
Pt 
Ft 
Wt 
Pt-l,$ 
Ft- l . t  
Wt--l,t 
Pt-l ,t,t+l 
F?-1.:.~+1 
W~-l.t.t+l 
Table 11: Values of Entropy for NE-DT features 
in the biology corpus 
GR Cross Entropy 
0.937 2.31(2.00-2.50) 
0.23"/ 5.31(5.21-5.38) 
0.262 3.27(3.14-3.41) 
0.416 7.63(7.50-7.79) 
0.370 7.78(7.69.-7.86) 
0.363 4.57(4.38-4.67) 
0.586 5.71(5.37-5.93) 
0.541 8.92(8.82-9.02) 
0.502 5.46(5.26-5.64) 
0.764 2.56(2.25-2.76) 
Coverage Features 
0.18(0.15-0.19) ALL 
1.00(0.99-1.00) P, 
1.00(1.00-1.00) Ft 
0.87(0.85--0.68) wt 
0.97(0.96-0.97) P~-a.= 
0.98(0.98-.0.99) F~-I,~ 
0.48(0.45--0.50) Wt-  s,~ 
0.88(0.87--0.89) Pt-x.~t.t +a 
0.96(0.94--0,96) Ft-l.t.~+a 
0.20(0.17--0.21) Wt_L?,t+t 
gether with error bounds in parentheses. These 
values are calculated for pairs of an NE class and 
features, and averaged for the n-fold experiments. 
In the MUC-6 corpus, 60 texts are separated into 
6 subsets, and one of them is used as the test set 
and the others are put together to form a train- 
ing set. Similarly, 100 texts are separated into 5 
subsets in the biology corpus. We also show the 
coverage of the pairs on the 'Coverage' col,,mn. 
Coverage means that how many pairs which ap- 
peared in a test set alo appear in a trainlug set. 
In these columns, the greater the cross entropy 
between features and a class, the more different 
their occurrences between tr~iuing and test sets. 
On the other hand, as the coverage for class- 
features pairs increases, so does the part of the 
test set that is covered with the given feature set. 
The results in both corpora for both systems 
show a drawback of surface words, since their cov- 
erage for a test set is lower than that of features 
like POSs and character types in both corpora 
Also, the coverage of surface words in the biol- 
ogy corpus is higher than in the MUC6 corpus 
as opposed to other features. The result matches 
our intuition that vocabulary inthe biology corpus 
is relatively restricted but has a variety of types 
other than normal English words. 
7 Conc lus ion  
The need for soundly-motivated metrics to com- 
pare the usefulness of corpora for specific tasks 
and systems is dearly necessary for the develop- 
ment of robust and portable information extrac- 
tion systems. 
In this paper we have shown that measures for 
comparing corpora based just on class-token ratios 
have difficulty predicting system performance and 
cannot adequately explain the difficulty of the NE 
task either generally or for specific systems. 
While we should be cautious in ma~ng sweep- 
ing conclusions due to the small size of corpora in 
our study, our results from gain ratio and cross 
entropy indicate that counts from the features of 
both systems will be more useful in the MUC6 cor- 
pus than in the biology corpus. We can also see 
that while the coverage is limited, surface words 
play a leading role for both systems. Gain ra- 
tio statistics for surface words in the two domains 
were far closer than for any other type of feature, 
and given that this is also the dominant knowl- 
edge type this seems to be one likely reason that 
the performance of systems is about the same in 
both domains. 
We have presented the results of applying two 
supervised learning based models to the named 
entity task in two widely different domains and 
explained the performance through class-token ra- 
tios, entropy and gain ratio. Measures such as 
entropy and gain ratio have been found to have 
the best predictive power, although the features 
used to calculate gain ratio are not sufficient o 
describe all the information that is necessary for 
the named entity task. In future work we intend 
to extend our study to new and larger NE corpora 
in various domains and to try to reduce the error 
factor in our calculations that is a result of corpus 
size. 
Re ferences  
A. Bairoch and 1t. Apweiler. 1997. The SWISS- 
PROT protein sequence data bank and its new 
supplement TrEMBL. Nucleic Acids Research, 
25:31-36. 
D. Bikel, S. Miller, R. Schwartz, and 
11. Wesichedel. 1997. Nymble: a high- 
performance learning name-finder. In Pro- 
ceedings of the Fifth Con/ererenee on Applied 
Natural Language Processing, pages 194--201. 
A. Borthwiek, J. Sterling, E. Agichtein, and 
11. Grishman. 1998. Exploiting diverse knowl- 
edge sources via maximum entropy in named 
entity recognition. In Proceedings of the Work- 
shop on Very Large Corpora (WYLC'98). 
L. Breiman, It. Friedman, A. Olshen, and 
C. Stone. 1984. Classification and regressiwa 
26 
trees. Belmont CA: Wadsworth International 
Group. 
S. Chen and J. Goodman. 1996. An empiri- 
cal study of smoothing techniques for language 
modeling. 3gst Annual Meeting of the Associ- 
ation of Computational Linguistics, California, 
USA, 24-27 3tree. 
N. Chinchor. 1995. MUC-5 evaluation metrics. 
In In Proceedings of the Fifth Message Un- 
derstanding Conference (MUC-5), Baltimore, 
Maryland, USA., pages 69-78. 
N. Collier, C. Nobata, and J. Tsujii. 2000. Ex- 
tracting the names of genes and gene products 
with a hidden Markov model. In Proceedings 
of the 18th International Conference on Com- 
putational Linguistics (COLING'2000), Saar- 
bruchen, Germany, July 31st-August 4th. 
M. Craven and J. Kumlien. 1999. Constructing 
biological knowledge bases by extracting infor- 
mation from text sources. In Proceedings ofthe 
7th International Conference on Intelligent Sys- 
temps for Molecular Biology (ISMB-99), Hei- 
delburg, Germany, August 6--10. 
K. Fukuda, T. Tsunoda, A. Tamura, and T. Tak- 
ag i. 1998. Toward information extraction: 
identifying protein names from biological pa- 
pers. In Proceedings of the Pacific Symposium 
on Biocomputin9'98 (PSB'98), January. 
MEDLINE. 1999. The PubMed 
database can be found at:. 
http://www.ncbi.nlm.nih.gov/PubMed/. 
DARPA. 1995. Proceedings ofthe Sixth Message 
Understanding Conference(MUC-6), Columbia, 
MD, USA, November. Morgan Kaufmann. 
C. Nobata, N. Collier, and I. Tsujii. 1999. Au- 
tomatic term identification and classification 
in biology texts. In Proceedings of the Nat- 
ural Language Pacific Rim Symposium (NL- 
PRS'gO00), November. 
D. Palmer and D. Day. 1997. A statistical 
profile of the named entity task. In Proceed- 
ings of the Fifth Conference on Applied Natural 
Language Processing (ANLP'97), Washington 
D.C., USA., 31 March - 3 April. 
J.R. Quinlan. 1990. Introduction to Decision 
Trees. In J.W. Shavlik and T.G. Dietterich, ed- 
itors, Readings in Machine Learning. Morgan 
Kauf:marm Publishers, Inc., San Mateo, Cali- 
fornia. 
J.R. Quinlan. 1993. cJ.5 Programs for Machine 
Learning. Morgan Kaufmann Publishers, Inc., 
San Mateo, California. 
L. Rabiner and B. Juang. 1986. An introduction 
to bidden Markov models. 1EEE ASSP Maga- 
zine, pages 4-16, January. 
A. Ratnaparkhi. 1996. A maximum entropy 
model for part-of-speech tagging. In Uon\]er- 
ence on Empirical Methods in Natural Language 
Processing, pages 133-142, University of Penn- 
sylvania, May. 
T. Rindflesch, L. Tanabe, N. Weinstein, and L.. 
Hunter. 2000. EDGAR: Extraction of drugs, 
genes and relations from the biomedical litera- 
ture. In Pacific Symposium on Bio-inforraaties 
(PSB '2000), Hawai 'i, USA, January. 
T. Sekimizu, H. Park, and J. Tsujii. 1998. Iden- 
tifying the interaction between genes and gene 
products based on frequently seen verbs in reed- 
line abstracts. In Genome Informatics. Univer- 
sal Academy Press, Inc. 
Satosbi Sekine, Ralph Grishman, and Hiroyuki 
Sbinnou. 1998. A Decision Tree Method for 
Finding and Classifying Names in Japanese 
Texts. In Proceedings o\] the Sixth Workshop 
on Very Large Corpora, Montreal, Canada, Au- 
gust. 
C. Van Rijsbergen. 1979. Information Retrieval. 
Butterworths, London. 
A. J. Viterbi. 1967. Error bounds for convolutions 
codes and an asymptotically optimum decoding 
algorithm. IEEE Transactions on Information 
Theory, IT-13(2):260-269. 
E.M. Voorhees and D.K. Harman, editors. 
2000. The Eighth Text REtrieval Confer- 
ence (TREC-8), Electronic version available at 
http://trec.nist.gov/pubs.html. 
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 449?456
Manchester, August 2008
The Choice of Features for Classification of Verbs in Biomedical Texts
Anna Korhonen
University of Cambridge
Computer Laboratory
15 JJ Thomson Avenue
Cambridge CB3 0FD, UK
alk23@cl.cam.ac.uk
Yuval Krymolowski
Dept. of Computer Science
Haifa University
Israel
yuvalkry@gmail.com
Nigel Collier
National Institute of Informatics
Hitotsubashi 2-1-2
Chiyoda-ku, Tokyo 101-8430
Japan
collier@nii.ac.jp
Abstract
We conduct large-scale experiments to in-
vestigate optimal features for classification
of verbs in biomedical texts. We intro-
duce a range of feature sets and associated
extraction techniques, and evaluate them
thoroughly using a robust method new to
the task: cost-based framework for pair-
wise clustering. Our best results compare
favourably with earlier ones. Interestingly,
they are obtained with sophisticated fea-
ture sets which include lexical and seman-
tic information about selectional prefer-
ences of verbs. The latter are acquired au-
tomatically from corpus data using a fully
unsupervised method.
1 Introduction
Recent years have seen a massive growth in the
scientific literature in the domain of biomedicine.
Because future research in the biomedical sciences
depends on making use of all this existing knowl-
edge, there is a strong need for the development of
natural language processing (NLP) tools which can
be used to automatically locate, organize and man-
age facts related to published experimental results.
Major progress has been made on information
retrieval and on the extraction of specific rela-
tions (e.g. between proteins and cell types) from
biomedical texts (Ananiadou et al, 2006). Other
tasks, such as the extraction of factual information,
remain a bigger challenge.
Researchers have recently begun to use deeper
NLP techniques (e.g. statistical parsing) for im-
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
PROTEINS: p53
p53
Tp53
Dmp53
...
ACTIVATE
suggestsdemonstrates
indicatesimplies...
GENES: WAF1
WAF1CIP1p21
...
It
INDICATE
that
activates
up-regulates
induces
stimulates...
...
Figure 1: Sample lexical classes
proved processing of the challenging linguistic
structures (e.g. complex nominals, modal subordi-
nation, anaphoric links) in biomedical texts. For
optimal performance, many of these techniques
require richer syntactic and semantic informa-
tion than is provided by existing domain lexicons
(e.g. UMLS metathesaurus and lexicon
1
). This par-
ticularly applies to verbs, which are central to the
structure and meaning of sentences.
Where the information is absent, lexical classi-
fication can compensate for it, or aid in obtaining
it. Lexical classes which capture the close rela-
tion between the syntax and semantics of verbs
provide generalizations about a range of linguis-
tic properties (Levin, 1993). For example, con-
sider the INDICATE and ACTIVATE verb classes in
Figure 1. Their members have similar subcatego-
rization frames SCFs (e.g. activate / up-regulate /
induce / stimulate NP) and selectional preferences
(e.g. activate / up-regulate / induce / stimulate
GENES:WAF1), and they can be used to make sim-
ilar statements describing similar events (e.g. PRO-
TEINS:P53 ACTIVATE GENES:WAF1).
Lexical classes can be used to abstract away
from individual words, or to build a lexical or-
ganization which predicts much of the behaviour
of a new word by associating it with an appro-
priate class. They have proved useful for various
NLP application tasks, e.g. parsing, word sense dis-
1
http://www.nlm.nih.gov/research/umls
449
ambiguation, semantic role labeling, information
extraction, question-answering, machine transla-
tion (Dorr, 1997; Prescher et al, 2000; Swier
and Stevenson, 2004; Dang, 2004; Shi and Mi-
halcea, 2005). A large-scale classification spe-
cific to the biomedical data could support key BIO-
NLP tasks such as anaphora resolution, predicate-
argument identification, event extraction and the
identification of biomedical (e.g. interaction) rela-
tions. However, no such classification is available.
Recent research shows that it is possible to auto-
matically induce lexical classes from corpora with
promising accuracy (Schulte im Walde, 2006; Joa-
nis et al, 2007; Sun et al, 2008). A number of
machine learning (ML) methods have been applied
to classify mainly syntactic features (e.g. subcat-
egorization frames (SCFs)) extracted from cross-
domain corpora using e.g. part-of-speech tagging
or robust statistical parsing techniques. Korho-
nen et al (2006) have recently applied such an
approach to biomedical texts. Their preliminary
experiment shows encouraging results but further
work is required before such an approach can be
used to benefit practical BIO-NLP.
We conduct a large-scale investigation to find
optimal features for biomedical verb classification.
We introduce a range of theoretically-motivated
feature sets and evaluate them thoroughly using
a robust method new to the task: a cost-based
framework for pairwise clustering. Our best re-
sults compare favourably with earlier ones. Inter-
estingly, they are obtained using feature sets which
have proved challenging in general language verb
classification: ones which incorporate information
about selectional preferences of verbs. Unlike in
earlier work, we acquire the latter from corpus data
using a fully unsupervised method.
We present our lexical classification approach in
section 2 and data in section 3. Experimental eval-
uation is reported in section 4. Section 5 provides
discussion and section 6 concludes.
2 Approach
Our lexical classification approach involves (i) ex-
tracting features from corpus data and (ii) cluster-
ing them. These steps are described in the follow-
ing two sections, respectively.
2.1 Features
Lexical classifications are based on diathesis alter-
nations which manifest in alternating sets of syn-
tactic frames (Levin, 1993). Most verb classifi-
cation approaches have therefore employed shal-
low syntactic slots or SCFs as basic features. Some
have supplemented them with further information
about verb tense, voice, and/or semantic selec-
tional preferences on argument heads.
2
The preliminary experiment on biomedical verb
classification (Korhonen et al, 2006) employed
basic syntactic features only: SCFs extracted
from corpus data using the system of Briscoe
and Carroll (1997) which operates on the output
of a domain-independent robust statistical parser
(RASP) (Briscoe and Carroll, 2002). Because such
deep syntactic features seem ideally suited for
challenging biomedical data, we adopted the same
basic approach, but we designed and extracted a
range of novel feature sets which include addi-
tional syntactic and semantic information.
The SCF extraction system assigns each occur-
rence of a verb in the parsed data as a member of
one of the 163 verbal SCFs, builds a lexical entry
for each verb (type) and SCF combination, and fil-
ters noisy entries out of the lexicon. We do not
employ the filter in our work because its primary
aim is to filter out SCFs containing adjuncts (as op-
posed to arguments). Adjuncts have been shown
to be beneficial for general language verb classifi-
cation (Sun et al, 2008; Joanis et al, 2007) and
particularly meaningful in biomedical texts (Co-
hen and Hunter, 2006).
The lexical entries provide various information
useful for verb classification, including e.g. the fre-
quency of the entry in the data, the part-of-speech
(POS) tags of verb tokens, the argument heads in
argument positions, the prepositions in PP frames,
and the number of verbal occurrences in active and
passive. Making use of this information we de-
signed ten feature sets for experimentation.
The first three feature sets F1-F3 include basic
SCF frequency information for each verb:
F1: SCFs and their relative frequencies. The SCFs
abstract over lexically governed particles and
prepositions.
F2: F1 with two high frequency PP frames pa-
rameterized for prepositions: the simple PP
and NP-PP frames refined according to the
prepositions provided in the lexical entries
(e.g. PP at, PP on, PP in).
2
See section 5 for discussion on previous work.
450
F3: F2 with 13 additional high frequency PP
frames parameterized for prepositions.
Although prepositions are an important part of
the syntactic description of lexical classes and
therefore F3 should be the most informative fea-
ture set, we controlled the number of PP frames
parameterized for prepositions to examine the ef-
fect of sparse data in automatic classification.
F4-F7 build on the most refined SCF-based fea-
ture set F3, supplementing it with information
about verb tense (F4-F5) and voice (F6-F7):
F4: The frequencies of POS tags (e.g. VVD for
activated) calculated over all the SCFs of the
verb.
F5: The frequencies of POS tags calculated spe-
cific to each SCF of the verb.
F6: The frequency of the active and passive oc-
currences of the verb (calculated over all the
SCFs of the verb).
F7: The frequency of the active and passive occur-
rences of the verb (calculated specific to each
SCF of the verb).
Also F8-F10 build on feature set F3. They sup-
plement it with information about lexical or se-
mantic selectional preferences (SPs) of the verbs
in the following slots: subject, direct object, sec-
ond object, and the NP within the PP complement.
The SPs are acquired using argument head data in
the ten most frequent SCFs. We use two baseline
methods (F8 and F9) which employ raw data and
one method based on clustering (F10):
F8: The raw argument head types are considered
as SP classes.
F9: Only those raw argument head types which
occur with four or more verbs with frequency
of ? 3 are considered as SP classes.
F10: SPs are acquired by clustering those argu-
ment heads which occur with ten or more
verbs with frequency of ? 3. We used the PC
clustering method described below in section
2. The number of clusters K
np
was set to 10,
20, and 50 to produce SP classes. We call the
feature sets corresponding to these different
values of K
np
F10A, F10B and F10C, respec-
tively. Since the clustering algorithms have
an element of randomness, clustering was ran
100 times. The output is a result of voting
among the outputs of the runs.
F3-F10 are entirely novel feature sets in biomed-
ical verb classification. Variations of some of them
have been used in earlier work on general language
classification (see section 5 for details).
2.2 Classification
The clustering method which proved the best in the
preliminary experiment on biomedical verb classi-
fication was Information Bottleneck (IB) (Tishby
et al, 1999). We compare this method against a
probabilistic method: a cost-based framework for
pairwise clustering (PC) (Puzicha et al, 2000).
2.2.1 Information Bottleneck
IB is an information-theoretic method which
controls the balance between: (i) the loss of
information by representing verbs as clusters
(I(Clusters;V erbs)), which has to be min-
imal, and (ii) the relevance of the output
clusters for representing the SCF distribution
(I(Clusters; SCFs)) which has to be maximal.
The balance between these two quantities ensures
optimal compression of data through clusters. The
trade-off between the two constraints is realized
through minimising the cost function:
L
IB
= I(Clusters;V erbs)
? ?I(Clusters; SCFs) ,
where ? is a parameter that balances the con-
straints. IB takes three inputs: (i) SCF-verb -based
distributions, (ii) the desired number of clusters K,
and (iii) the initial value of ?. It then looks for the
minimal ? that decreases L
IB
compared to its value
with the initial ?, using the given K. IB delivers as
output the probabilities p(K|V ).
2.2.2 Pairwise Clustering
PC is a method where a cost criterion guides
the search for a suitable clustering configuration.
This criterion is realized through a cost function
H(S,M) where
(i) S = {sim(a, b)}, a, b ? A : a collection of pairwise
similarity values, each of which pertains to a pair of
data elements a, b ? A.
(ii) M = (A
1
, . . . , A
k
) : a candidate clustering configu-
ration, specifying assignments of all elements into the
disjoint clusters (that is ?A
j
= A and A
j
? A
j
?
= ?
for every 1 ? j < j
?
? k).
451
1 Have an effect on activity (BIO/29) 9 Report (GEN/30)
1.1 Activate / Inactivate 9.1 Investigate
1.1.1 Change activity: activate, inhibit 9.1.1 Examine: evaluate, analyze
1.1.2 Suppress: suppress, repres s 9.1.2 Establish: test, investigate
1.1.3 Stimulate: stimulate 9.1.3 Confirm: verify, determine
1.1.4 Inactivate: delay, diminish 9.2 Suggest
1.2 Affect 9.2.1 Presentational:
1.2.1 Modulate: stabilize, modulate hypothesize, conclude
1.2.2 Regulate: control, support 9.2.2 Cognitive:
1.3 Increase / decrease: increase, decrease consider, believe
1.4 Modify: modify, catalyze 9.3 Indicate: demonstrate, imply
Table 1: Sample classes from the gold standard
Journal Years Words
Genes & Development 2003-5 4.7M
Journal of Biological Chemistry 2004 5.2M
(Vol.1-9)
The Journal of Cell Biology 2003-5 5.6M
Cancer Research 2005 6.5M
Carcinogenesis 2003-5 3.4M
Nature Immunology 2003-5 2.3M
Drug Metabolism and Disposition 2003-5 2.3M
Toxicological Sciences 2003-5 3.1M
Total: 33.1M
Table 2: Data from MEDLINE
The cost function is defined as follows:
H = ?
P
n
j
?Avgsim
j
,
Avgsim
j
=
1
n
j
?(n
j
?1)
P
{a,b?A
j
}
sim(a, b)
where n
j
is the size of the j
th
cluster and Avgsim
j
is the average similarity between cluster members.
We used the Jensen-Shannon divergence (JS) as the
similarity measure.
3 Data
3.1 Test Verbs and Gold Standard
We employed in our experiments the same gold
standard as earlier employed by Korhonen et al
(2006). This three level gold standard was created
by a team of human experts: 4 domain experts and
2 linguists. It includes 192 test verbs (typically fre-
quent verbs in biomedical journal articles) classi-
fied into 16, 34 and 50 classes, respectively. The
classes created by domain experts are labeled as
BIO and those created by linguists as GEN. BIO
classes include 116 verbs whose analysis required
domain knowledge (e.g. activate, solubilize, har-
vest). GEN classes include 76 general or scientific
text verbs (e.g. demonstrate, hypothesize, appear).
Each class is associated with 1-30 member verbs.
Table 1 illustrates two of the gold standard classes
with 1-2 example verbs per (sub-)class.
3.2 Test Data
We downloaded the data from the MEDLINE
database, from eight journals covering various ar-
SCF F1 98 39
F2 247 64
F3 486 75
F3 + tense F4 490 79
F5 920 176
F3 + voice F6 488 77
F7 682 153
F3 + SP F8 150407 2112
F9 13352 344
F10A 110280 2091
F10B 115208 2091
F10C 114793 2091
Table 3: (i) The total number of features and (ii)
the average per verb for all the feature sets
eas of biomedicine. The first column in table 2
lists each journal, the second shows the years from
which the articles were downloaded, and the third
indicates the size of the data. We experimented
with two test sets: 1) The 15.5M word sub-set
shown in the first three rows of the table (this was
used for creating the gold standard). 2) All the
data: this new larger data was necessary for exper-
iments with new feature sets as the most refined
ones do not appear in 1) with sufficient frequency.
4 Experimental Evaluation
4.1 Processing the Data
The data was first processed using the feature ex-
traction module. Table 3 shows (i) the total num-
ber of features in each feature set and (ii) the av-
erage per verb in the resulting lexicon. The clas-
sification module was then applied. We requested
K = 2 to 60 clusters from both clustering meth-
ods. We did not want to enforce the actual num-
ber of classes but preferred to let the class hierar-
chy emerge from the clustering results. In order
to find the values of K where the clustering output
might correspond to a level in the class hierarchy
we used the relevance criterion. For each method
(clustering method and feature set combination)
we choose as informative K?s the values for which
the relevance information I(Clusters; SCFs)) in-
creases more sharply between K?1 and K clusters
than between K and K+1. We then chose for eval-
uation the outputs corresponding only to informa-
tive values of K. The clustering was run 50 times
for each method. The output is a result of voting
among the outputs of the runs.
4.2 Measures
The clusters were evaluated against the gold stan-
dard using four methods. The first measure, the
452
adjusted pairwise precision, evaluates clusters in
terms of verb pairs:
APP =
1
K
K
P
i=1
num. of correct pairs in k
i
num. of pairs in k
i
?
|k
i
|?1
|k
i
|+1
APP is the average proportion of all within-
cluster pairs that are correctly co-assigned. Mul-
tiplied by a factor that increases with cluster size it
compensates for a bias towards small clusters.
The second measure is modified purity, a global
measure which evaluates the mean precision of
clusters. Each cluster is associated with its preva-
lent class. The number of verbs in a cluster K that
take this class is denoted by n
prevalent
(K). Verbs
that do not take it are considered as errors. Clusters
where n
prevalent
(K) = 1 are disregarded as not to
introduce a bias towards singletons:
mPUR =
P
n
prevalent
(k
i
)?2
n
prevalent
(k
i
)
number of verbs
The third measure is the weighted class accu-
racy, the proportion of members of dominant clus-
ters DOM-CLUST
i
within all classes c
i
.
ACC =
C
P
i=1
verbs in DOM-CLUST
i
number of verbs
mPUR can be seen to measure the precision of
clusters and ACC the recall. We define an F mea-
sure as the harmonic mean of mPUR and ACC:
F =
2 ?mPUR ? ACC
mPUR + ACC
The experiments were run 50 times on each in-
put to get the distribution of performance due to
the randomness in the initial clustering. We calcu-
lated the average performance and standard devia-
tion from the results of these runs.
4.3 Results for Test Set 1
We first compared IB and PC on the smaller test set
1 using feature set F2. We chose for evaluation the
outputs corresponding to the most informative val-
ues of K: 20, 33, 53 for IB, and 19, 26, 51 for PC.
In the results included in table 4 IB shows slightly
better performance than PC, but the difference is
not significant for K=34 and 50. We decided to use
PC for larger experiments because it has two ad-
vantages over IB: 1) It can cluster the large test set
2 with K = 10 ? 60 in minutes, while IB requires
a day for this. 2) It can deal with (and combine)
different feature sets, while IB runs into numeri-
cal problems. Due to its speed and flexibility PC
is thus more suitable for larger-scale experiments
involving comparison of complex feature sets.
4.4 Results for Test Set 2
Tables 5 and 6 include the PC results on the larger
test set 2. Table 5 shows the results for each in-
dividual feature set (indicated in the second col-
umn). It shows also the standard deviations (?
avg
)
of the four performance measures averaged across
all the runs. These are very similar for 16, 34, and
50 classes and hence only included in one of the
columns. In addition, ?
diff
is indicated. This is
?
2 ? ?
avg
and used for calculating the significance
of the performance differences. In the following
discussion we consider a difference of more than
2?
diff
(p > 97.7%) as significant.
The first feature sets F1-F3 include basic SCF
(frequency) information for each verb, F2-F3 re-
fined with prepositions. F2 shows clearly better
results than F1 (over 10 F-measure) at all the levels
of gold standard. This demonstrates the usefulness
of prepositions for the task. When moving to F3
the performance decreases for 34 and 50 classes,
while improving for 16 classes, but these differ-
ences are not statistically significant.
Feature sets F4-F10 build on F3. F4-F5 include
information about verb tense. This information
proves quite useful for verb classification, partic-
ularly when specific to individual SCFs. When
compared against the baseline featureset F3, F5
is clearly better - particularly at 50 classes where
the difference is 3.9 in F-measure (2?
diff
). Verb
voice information is not equally helpful: F6-F7 are
not better than F3. In some comparisons they are
worse, e.g. F7 vs. F3 at 16 classes.
F8-F10 supplement F3 with information about
SPs. Surprisingly, these lexical and semantic fea-
tures prove the most useful for our task. At the
level of 34 and 50 classes, the best SP features are
even better than the best tense features (the dif-
ference is statistically significant), and they yield
notable improvement over the baseline features
(e.g. 6.8 difference in F-measure between F9 and
F3). The performance is not equally good at 16
classes. This makes perfect sense because class
members are unlikely to have similar SPs at such a
coarse level of semantic classification.
When comparing the five sets of SPs features
against each other, F9 and F10C produce the best
results at 34 and 50 classes. F9 uses raw (filtered)
argument head data for SP acquisition while F10C
uses clustering. It is interesting that the differ-
ence between these two very different methods is
not statistically significant. Whether one employs
453
16 Classes 34 Classes 50 Classes
APP mPUR ACC F APP mPUR ACC F APP mPUR ACC F
IB 74 77 66 71 69 75 81 77 54 72 79 75
PC 71 78 58 67 64 71 81 75 63 71 73 72
? 1.1 1.0 1.0 0.8 1.8 1.6 1.3 1.4 2.1 1.5 1.6 1.1
Table 4: Performance on test set 1
16 Classes 34 Classes 50 Classes
APP mPUR ACC F APP mPUR ACC F APP mPUR ACC F
SCF F1 62.7 68.2 54.6 60.6 50.4 58.4 53.4 55.8 41.5 50.3 55.7 52.9
F2 68.7 76.4 66.4 71.1 61.9 65.5 65.8 65.6 53.9 61.2 65.4 63.2
F3 69.3 77.7 67.6 72.3 61.6 66.0 64.0 65.0 53.7 60.2 65.9 62.9
F3 + tense F4 70.1 77.5 65.5 71.0 62.0 70.3 69.4 69.8 53.3 60.6 68.0 64.1
F5 68.5 75.4 71.7 73.5 61.9 67.8 68.2 68.0 58.2 62.7 71.7 66.8
F3 + voice F6 70.6 78.1 64.0 70.4 61.2 66.0 65.8 65.9 54.3 59.6 70.1 64.4
F7 74.0 79.5 59.7 68.2 62.6 65.4 65.1 65.2 55.1 60.9 69.2 64.7
F3 + SP F8 77.1 78.2 61.6 68.9 69.6 69.3 71.2 70.2 61.3 62.7 71.1 66.6
F9 72.4 77.1 64.0 69.9 72.2 72.0 71.6 71.8 62.3 65.6 72.4 68.8
F10A 75.6 80.0 63.2 70.6 66.1 69.2 70.6 69.9 59.4 63.5 69.0 66.2
F10B 68.8 77.1 69.2 72.9 65.3 67.2 69.8 68.5 59.9 61.9 70.5 65.9
F10C 74.1 78.9 65.7 71.7 68.8 71.7 69.7 70.7 59.8 63.4 71.1 67.0
?
avg
2.2 1.5 1.8 1.4
?
diff
3.1 2.1 2.5 2.0
Table 5: Performance on test set 2: PC clustering results for individual feature sets at the three levels of
gold standard. ?
avg
and ?
diff
were calculated across all the three classification levels.
16 CL. F5+F9 F4+ F10C F5 F5+ F8
APP 72.3 68.2 68.5 72.2
mPUR 76.4 77.0 75.4 76.5
ACC 73.6 70.9 71.7 69.9
F 75.0 73.8 73.5 73.0
34 CL. F5+ F9 F5+ F8 F9 F4+ F10A
APP 68.7 71.0 72.2 62.9
mPUR 70.1 71.0 72.0 68.4
ACC 74.8 73.4 71.6 75.0
F 72.4 72.2 71.8 71.5
50 CL. F9 F5+ F9 F5+ F8 F4+ F9
APP 62.3 59.8 62.8 59.7
mPUR 65.6 63.8 64.1 63.1
ACC 72.4 72.7 71.0 71.8
F 68.8 68.0 67.4 67.1
Table 6: Results for the top four feature set combi-
nations. All the feature sets build on F3.
fine grained clusters (F10C) or coarse-grained ones
(F10A) as SPs does not make much difference.
We next combined various feature sets. Table 6
shows the performance for the top four combina-
tions. Comparing these results against the ones in
Table 5, (see the ?
diff
values in Table 5) we can see
that combining feature sets does not result in better
performance
3
. The only exception is the difference
in APP and mPUR between F9 and F4 + F10A at
N=34. However, these results show similar ten-
dencies as the earlier ones: at 16 classes the most
3
Recall that all F4-F10 are actually already ?combined?
with F3 - we do not refer to this combination here.
useful features are based on verb tense, while at 34
and 50 classes they are based on SPs.
5 Discussion
The results presented in the previous section are
in interesting contrast with those reported in ear-
lier work. In previous work on general lan-
guage verb classification, syntactic features (slots
or SCFs) have proved generally the most help-
ful features, e.g. (Schulte im Walde, 2006; Joa-
nis et al, 2007). The preliminary experiment on
biomedical verb classification (Korhonen et al,
2006) experimented only with them. In our ex-
periments, SCFs proved useful baseline features.
When we refined them further, we faced sparse
data problems: considerable improvement was ob-
tained when moving from F1 to F2, but not when
moving to F3. Although many verb classes are
sensitive to preposition types, many of the types
are low in frequency. Future work could address
this problem by employing smoothing techniques,
or backing off to preposition classes.
Joanis et al (2007) experimented with tense
and voice -based features in general English verb
classification. They offered no significant im-
provement over basic syntactic features. Also in
our experiments, we obtained little improvement
with voice features. This could be due to the
454
un-distinctiveness of passive in biomedical texts
where it is used typically with high frequency.
However, tense-based features clearly improved
the baseline performance in our experiments. This
could be partly because we ?parameterize? POS in-
formation for SCFs, and partly because semanti-
cally similar verbs in biomedical language tend to
behave similarly also in terms of tense (Friedman
et al, 2002).
Joanis (2002) and Schulte im Walde (2006) used
SP-based features in general English and German
verb classifications, respectively. The former ac-
quired them from WordNet (Miller, 1990) and
the latter from GermaNet (Kunze, 2000). Joa-
nis (2002) obtained no improvement over syntactic
features while Schulte im Walde (2006) obtained,
but the improvement was not significant. In our
experiments, SP features gave the best results and
the clearest improvement over the baseline features
at the finer-grained levels of classification where
class members are indeed likely to be the most uni-
form in terms of their SPs.
We obtained this improvement despite using
a fully unsupervised approach to SP acquisition.
We did not exploit lexical resources like Joa-
nis (2002) and Schulte im Walde (2006) because
it would have required combining general re-
sources (e.g. WordNet) with domain specific ones
(e.g. UMLS). We opted for a simpler approach in
this initial work ? using raw argument heads and
clustering ? and obtained surprisingly good results.
In our experiments filtering of raw argument heads
and clustering with N=50 produced equivalent re-
sults, suggesting that relatively fine-grained clus-
ters are optimal. Future work will require quali-
tative analysis of noun clusters and comparison of
these against classes in lexical resources to deter-
mine an optimal method for SP acquisition.
Does the fact that we obtain good results with
features which have not proved helpful in general
language classification indicate a need for domain-
specific feature engineering? We do not believe
so. The feature sets we experimented with are the-
oretically well-motivated and should, in principle,
also aid general language verb classification. We
believe they proved helpful in our experiments be-
cause being domain-specific, biomedical language
is conventionalised and therefore less varied in
terms of verb sense and usage than general lan-
guage. For example, verbs have stronger SPs for
their argument heads when many of their corpus
occurrences are of similar sense. This renders SP-
based features more useful for classification.
Due to differences in the data, methods, and ex-
perimental setup, direct comparison of our perfor-
mance figures with previously published ones is
difficult. The closest comparison point with gen-
eral language is (Korhonen et al, 2003) which re-
ported 59% mPUR using IB to assign 110 polyse-
mous English verbs into 34 classes. Our best re-
sults are substantially better (72-80% mPUR). It
is encouraging that we obtained such good results
despite focusing on a linguistically challenging do-
main.
In addition to the points mentioned earlier, our
future plans include seeding automatic classifica-
tion with more sophisticated information acquired
automatically from domain-specific texts (e.g. us-
ing named entity recognition and anaphoric link-
ing (Vlachos et al, 2006)). We will also explore
semi-automatic ML technology and active learn-
ing in aiding the classification. Finally, we plan to
conduct a bigger experiment with a larger number
of verbs, make the resulting classification publicly
available, and demonstrate its usefulness for prac-
tical BIO-NLP application tasks.
6 Conclusion
We reported large-scale experiments to investigate
the optimal characteristics of features required for
biomedical verb classification. A range of feature
sets and associated extraction methods were intro-
duced for this work, along with a robust cluster-
ing method capable of dealing with large data and
complex feature sets. A number of experiments
were reported. The best performing feature sets
proved to be the ones which include information
about SCFs supplemented with information about
verb tense and SPs in particular. The latter were
acquired automatically from corpus data using an
unsupervised method. Similar feature sets have
not proved equally useful in earlier work in gen-
eral language verb classification. We discussed
reasons for this and highlighted several areas for
future work.
Acknowledgement
Work on this paper was funded by the Royal So-
ciety, EPSRC (?ACLEX? project, GR/T19919/01)
and MRC (?CRAB? project, G0601766), UK.
455
References
Ananiadou, S., B. D. Kell, and J. Tsujii. 2006. Text
mining and its potential applications in systems biol-
ogy. Trends in Biotechnology, 24(12):571?579.
Briscoe, E. J. and J. Carroll. 1997. Automatic extrac-
tion of subcategorization from corpora. In 5
th
ACL
Conference on Applied Natural Language Process-
ing, pages 356?363, Washington DC.
Briscoe, E. J. and J. Carroll. 2002. Robust accurate
statistical annotation of general text. In 3
rd
Interna-
tional Conference on Language Resources and Eval-
uation, pages 1499?1504, Las Palmas, Gran Canaria.
Cohen, K. B. and L. Hunter. 2006. A critical review of
PASBio?s argument structures for biomedical verbs.
BMC Bioinformatics, 7(3).
Dang, H. T. 2004. Investigations into the Role of Lexi-
cal Semantics in Word Sense Disambiguation. Ph.D.
thesis, CIS, University of Pennsylvania.
Dorr, B. J. 1997. Large-scale dictionary construction
for foreign language tutoring and interlingual ma-
chine translation. Machine Translation, 12(4):271?
322.
Friedman, C., P. Kra, and A. Rzhetsky. 2002. Two
biomedical sublanguages: a description based on the
theories of zellig harris. Journal of Biomedical In-
formatics, 35(4):222?235.
Joanis, E., S. Stevenson, and D. James. 2007. A gen-
eral feature space for automatic verb classification.
Natural Language Engineering.
Joanis, E. 2002. Automatic verb classification using a
general feature space. Master?s thesis, University of
Toronto.
Korhonen, A., Y. Krymolowski, and N. Collier. 2006.
Automatic classification of verbs in biomedical texts.
In ACL-COLING, Sydney, Australia.
Kunze, C. 2000. Extension and use of germanet,
a lexical-semantic database. In 2nd International
Conference on Language Resources and Evaluation,
Athens, Greece.
Levin, B. 1993. English Verb Classes and Alterna-
tions. Chicago University Press, Chicago.
Miller, G. A. 1990. WordNet: An on-line lexical
database. International Journal of Lexicography,
3(4):235?312.
Prescher, D., S. Riezler, and M. Rooth. 2000. Using
a probabilistic class-based lexicon for lexical am-
biguity resolution. In 18th International Confer-
ence on Computational Linguistics, pages 649?655,
Saarbr?ucken, Germany.
Puzicha, J., T. Hofmann, and J. M. Buhmann. 2000.
A theory of proximity-based clustering: structure
detection by optimization. Pattern Recognition,
33(4):617?634.
Schulte im Walde, S. 2006. Experiments on the au-
tomatic induction of german semantic verb classes.
Computational Linguistics, 32(2):159?194.
Shi, L. and R. Mihalcea. 2005. Putting pieces to-
gether: Combining FrameNet, VerbNet and Word-
Net for robust semantic parsing. In Proceedings of
the Sixth International Conference on Intelligent Text
Processing and Computational Linguistics, Mexico
City, Mexico.
Sun, L., A. Korhonen, and Y. Krymolowski. 2008.
Verb class discovery from rich syntactic data. In
9th International Conference on Intelligent Text Pro-
cessing and Computational Linguistics, Haifa, Is-
rael.
Swier, R. and S. Stevenson. 2004. Unsupervised se-
mantic role labelling. In Proceedings of the 2004
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 95?102, Barcelona, Spain,
August.
Tishby, N., F. C. Pereira, and W. Bialek. 1999. The
information bottleneck method. In Proc. of the
37
th
Annual Allerton Conference on Communica-
tion, Control and Computing, pages 368?377.
Vlachos, A., C. Gasperin, I. Lewin, and E. J. Briscoe.
2006. Bootstrapping the recognition and anaphoric
linking of named entitites in drosophila articles. In
Pacific Symposium in Biocomputing, Maui, Hawaii.
456
Global Health Monitor - A Web-based System for Detecting and 
Mapping Infectious Diseases
Son Doan*, QuocHung-Ngo?, Ai Kawazoe*, Nigel Collier*
* National Institute of Informatics,
2-1-2 Hitotsubashi, Chiyoda-ku, Tokyo, 
Japan
{doan,zoeai,collier}@nii.ac.jp
? University of Information Technology, 
Vietnam National University (HCM), 
Vietnam
hungnq@uit.edu.vn
Abstract
We present the Global Health Monitor, an 
online Web-based system for 
detecting and mapping infectious disease 
outbreaks that appear in news 
stories. The system analyzes English news 
stories from news feed providers, 
classifies them for topical relevance and 
plots them onto a Google map using 
geo-coding information, helping public 
health workers to monitor the spread 
of diseases in a geo-temporal context. The 
background knowledge for the 
system is contained in the BioCaster ontol-
ogy (BCO) (Collier et al, 2007a) 
which includes both information on infec-
tious diseases as well as 
geographical locations with their lati-
tudes/longitudes. The system consists 
of four main stages: topic classification, 
named entity recognition (NER), 
disease/location detection and visualiza-
tion. Evaluation of the system shows 
that it achieved high accuracy on a gold 
standard corpus. The system is now 
in practical use. Running on a cluster-
computer, it monitors more than 1500 
news feeds 24/7, updating the map every 
hour.
1 Introduction
Information concerning disease outbreak events is 
published in various news outlets on the World 
Wide Web, in many different languages.  Identify-
ing early news stories about disease outbreaks
automatically is important for a bio-surveillance 
system that is designed to inform health profes-
sionals. Currently, there are several systems avail-
able for the disease detection and tracking task. For 
example, ProMED-mail (2001) or MedISys (2007)
(Medical Intelligence System). ProMED-mail is an 
Internet-based system that provides reports by pub-
lic health experts concerning outbreak diseases 
(that is, the system is not automatic but rather hu-
man curated). In contrast to ProMED-mail, MedI-
Sys is an automatic system working on multilin-
gual languages, but it mainly focuses on analyzing 
news stories based on the country level. Another 
system which is close to the one we present is 
HealthMap (Brownstein and Freifeld, 
2007). HealthMap automatically collects news 
from the Internet about human and animal health 
and plots the data on a Google Maps mashup. Data 
is aggregated by disease and location. Unlike 
HealthMap, our system takes an ontology-centred 
approach to knowledge understanding and linkage 
to external resources. For annotation of topics and 
entities we also exploit a range of linguistic re-
sources within a machine learning framework.
There are several challenges in geo-coding when 
dealing with news stories. The two main chal-
lenges are disease/location extraction and geo-
disambiguation. The former is concerned with how 
to determine disease and location names for dis-
ease-related news stories. The latter is concerned 
with how to solve geo-disambiguation. For exam-
ple, if there is a news story about equine influenza 
in Camden, the system should detect that the dis-
ease name is ?equine influenza? and the location 
name is ?Camden?. However, there are two loca-
tions named Camden: One in Australia and one in 
951
London, UK. The problem is that only one location 
should be chosen for plotting into a map. In our 
opinion, current systems lack the advanced natural 
language processing and text mining techniques 
that would enable the automatic extraction of such 
disease/location event information.
BioCaster is a project working towards the detec-
tion and tracking of infectious diseases using text 
mining techniques. One of the main components is 
the BioCaster Ontology (BCO), which includes 50
infectious diseases with links to external vocabu-
lary resources, and a geographical ontology of a 
total of 243 countries and 4025 sub-countries 
(province and cities) with their lati-
tudes/longitudes. We also now automatically link 
news on outbreaks to academic sources such as
Stanford university?s Highwire and NCBI?s Pub-
Med using search terms Disease name + Location
name (Country) + ?case?. This is to ensure a focus 
on recent case report relevant to the news items.
The system includes four main stages: topic classi-
fication, named entity recognition (NER), dis-
ease/location detection, and visualization. The cur-
rent version of the system (English only) can be 
found at http://biocaster.nii.ac.jp.
The remainder of this paper is organized as fol-
lows. Section 2 outlines the BioCaster Ontology 
(BCO). Section 3 describes some features of the 
system (modules, functionality and algorithms).  
Section 4 is concerned with system evaluation. 
Finally, Section 5 outlines conclusions and pre-
sents possible future work.
2 Overview of BCO
BCO is one of the main components in the Bio-
Caster project. It includes an ontology of 50 infec-
tious diseases and a geographical ontology (243 
countries and 4,025 sub-countries). The infectious 
disease ontology was built by a team consisting of
a linguist, an epidemiologist, a geneticist, and a 
medical anthropologist. A disease in BCO has a 
root name which is unique identifier and also other 
properties relating to synonyms, symptoms, asso-
ciated syndromes, hosts, etc. The ontology is mul-
tilingual, supporting six languages (English, Japa-
nese, Vietnamese, Thai, Korean, and Chinese); and 
has links to external ontologies (such as MeSH,
SNOMED and ICD9/10) and resources (like 
Wikipedia). The geographical part is built from 
Wikipedia1. The BCO is available on the Web at 
http://biocaster.nii.ac.jp. For a fuller description of 
the BCO, see Collier et al (2007a) and Kawazoe et 
al. (2007).
3 The System
3.1 Overview of the system
The Global Health Monitor system runs on a clus-
ter machine with 16 nodes under the Linux operat-
ing system. The code was written in PHP, Perl, C, 
and Java and the number of input news feeds is 
about 1,500. The system has a crawler that collects 
news every hour. These collected news stories are 
then processed and analyzed step-by-step in four
main phases: topic classification, named entity 
recognition (NER), disease/location detection, and 
visualization. Each of the four phases is managed 
by a distinct module. These components are de-
picted in Figure 1. The first three modules are run 
inside the system and the visualization module ?
the Google Map ? can be seen at the BioCaster 
portal. Figure 2 shows a screenshot of the system. 
                                                
1 http://www.wikipedia.org.
Figure 1. Stages of the system.
952
We will now describe each of the four modules in 
turn: 
* Topic classification.  This module identifies 
news stories with disease-related topics and retains 
relevant ones for later processing. The module uses
ontology-supported text classification with na?ve 
Bayes as the classification algorithm and the Bio-
Caster gold standard corpus as the training data set
(Doan et al, 2007). In this module, we used the 
Rainbow toolkit.2
* NER. Disease-related news stories are automati-
cally analyzed and tagged with NEs like PERSON, 
ORGANIZATION, DISEASE, LOCATION. This 
module is implemented by SVM classification al
                                                
2 Rainbow toolkit, available at 
http://www.cs.umass.edu/~mccallum/bow/rainbow
gorithm3. For a more detailed description of the 
schema and NER module, see Kawazoe et al 
(2006).
* Disease/location detection. This module extracts 
disease and location information. Details are given
in Section 3.2.
* Visualization. The detected locations are plotted 
onto a Google map with ontology links to associ-
ated diseases and news stories.
3.2 Disease/location detection algorithm
The disease/location detection algorithm is based 
on a statistical model of the LOCATION and DIS
EASE Named Entities (NEs). The algorithm can be 
described as follows:
                                                
3 TinySVM, available at 
http://chasen.org/~taku/software/TinySVM.
Figure 2.  The Global Health Monitor system, showing disease events from the last 30 days. The main 
screen is a Google Map. Selected headline reports run along the bottom of the screen and link to biomedi-
cal reference on PubMed, HighWire and Google Scholar.  Symbol    links to disease names in the BCO 
and symbol  stands for disease name not in the BCO. The right of the screen shows various user options 
to filter the news.
953
Input: A set of news stories tagged with NEs.
Output: A set of disease/location pairs.
Step 1: Detect LOCATION-DISEASE pairs in 
each news story by corresponding NEs, and calcu-
late their frequency in a news story.
Step 2: Calculate the frequency of LOCATION-
DISEASE pairs in a corpus.
Step 3: Rank LOCATION ? DISEASE pairs by 
the frequencies calculated in Step 2. Use a thresh-
old to choose top LOCATION - DISEASE names4. 
Step 4: Map disease and location names: If DIS-
EASE matches to a synonym in BCO then DIS-
EASE was assigned to that disease name. This 
process of matching (grounding the terms) allows
us to provide extra information from the ontology
and to remove variant forms of terms from the map
                                                
4 In the current system, we set the threshold value to 40.
? thereby aiding readability. Similarly, if LOCA-
TION matches to a location in BCO then LOCA-
TION was assigned to that location name. 
Step 5: Re-map into news stories: Match detected 
diseases and locations within the first half of each 
news story. If both disease and location are
matched then they are stored; otherwise, skip.
This five step process is repeated every hour, for 
each news article that is less than 1 day (24 hours) 
old.
3.3 Capabilities of the system
The following lists some capabilities of the current 
Global Health Monitor system.
* Date range: The system shows the dis-
ease/location and news stories within a specific
date range. Current implemented date ranges are:
30 days ago, 3 weeks ago, 2 weeks ago, 1 week 
ago, this week and today.
Figure 3.  The Global Health Monitor with the Respiratory Syndrome  selected. The time span selected is 
the current week.
954
* Genre filter: The system can show news stories 
by publication type. There are four genres of news: 
Press news (like Google News, Yahoo News), Of-
ficial news (like ProMED, WHO reports), Business 
news, and Mixed news (like Topix.com).
* Similar stories: The system currently uses a sim-
ple method to remove duplicate news stories. Users 
can use the ?Initial headline only? option to acti-
vate this function.
* Syndrome filter: There are six syndromes in 
BCO: Dermatological, Gastrointestinal, Hemor-
harrgic fever, Musculoskeletal, Neurological, and 
Respiratory. A syndrome can be associated with
several diseases included in BCO. The system can 
show news stories related to these syndromes.
* Agent option: This option allows users to view 
lists of infectious diseases which come from BCO. 
Some diseases though are not in the BCO. Users 
can choose some, all, or no diseases using a check-
box style interface.
Figure 3 shows the interface when users choose 
Syndromes as Respiratory for this week at the cur-
rent view. 
4 Evaluation
To evaluate any bio-surveillance system is very 
challenging (Morse, 2007). Our system is an inte-
gration of several modules, e.g., classification, 
NER and other algorithms. The evaluation proc-
esses for these modules are briefly described be-
low:
4.1 Topic classification
Evaluation of topic classification is presented in 
Doan et al (2007). The system used the BioCaster 
gold standard corpus which includes 1,000 anno-
tated news stories as training data. The classifica-
tion model is na?ve Bayes with features as raw text, 
NEs, and Roles (Doan et al, 2007). The system 
achieved an accuracy score of 88.10%.
4.2 NER evaluation
The evaluation of the NER system module is re-
ported in Kawazoe et al (2006). We used an anno-
tated corpus of 200 corpus news articles as training 
data. The NER system achieved an F-score of 
76.97% for all NE classes. 
4.3 Disease/location detection
For the preliminary evaluation of disease/location 
detection, we used data from a one-month period
(from October 12 to November 11, 2007). 
In our observations, the system detects about 25-30 
locations a day, an average of 40 infectious dis-
eases and 950 detected pairs of diseases/locations 
per month (A news story can contain multiple loca-
tions and diseases). The main news resources
mostly come from Google News (251 pairs, about 
26.4%), Yahoo News (288 pairs, about 30.3%), 
ProMED-mail (180 pairs, about 18.9%), and the 
remaining 24.3% for others. The running time for 
updating disease/location takes about 5 minutes.
In order to evaluate the performance of dis-
ease/location detection, we define the Precision 
and Recall as follows:
pairsRetrieved#
pairsRetrieved#pairsRelevant #
Precision
??
,
pairsRelevant #
pairsRetrieved#pairsRelevant #
Recall
??
Where #Relevant pairs is the number of dis-
ease/location pairs that human found, and #Re-
trieved pairs is the number of disease/location pairs 
that the system detected.
The Precision can be calculated based on our re-
trieved pairs detected by the system, however the 
Recall is under estimated as it does not measure 
pairs missed by  the system in the topic classifica-
tion stage. 
We evaluate the Precision of disease/location de-
tection on 950 pairs of location/disease. The sys-
tem correctly detected 887 pairs, taking 
887/950=93.4% Precision. 
4.4 Limitations of the system
There are some limitations of the system. The first 
limitation is there are several cases of ambiguity. 
For example, news stories about ?A team at Peking 
University in Beijing studied tissue taken from 2 
955
people killed by H5N1 in China? or ?A meeting on 
foot and mouth disease (FMD) was held in Brus-
sels on 17th October, 2007?. The system incorrectly
detects the location as Beijing in the first story, and 
Brussels in the second one. Another hard case is 
location disambiguation, e.g., news about ?Rabies 
in Isle of Wight? in which in the main body does 
not mention anything about country and sub-
country. There are two locations named ?Isle of 
Wight? in our geo-ontology: one in Virginia, USA 
and one in the UK. In the future, we will look at 
the country-level information of new providers (by 
checking domain names) to solve this problem. For 
example, if a news story mentions the Isle of 
Wight, and the news story originates from the UK,
then it will be taken to refer to the Isle of Wight in 
the UK.
The second limitation is the ability to detect new 
diseases or locations that are not in the ontology. In 
the future work, we will augment newly detected 
diseases as well as improve the geographical on-
tology.
5 Conclusion
We presented the Global Health Monitor - a Web-
based system for detecting and mapping infectious 
diseases from Web. The system collects news from 
news feed providers, analyzes news and plots dis-
ease relevant data onto a Google map. Preliminary
evaluations show that our system works efficiently
with real data.
In the future, we will develop more efficient algo-
rithms for detecting diseases/locations based on 
relation identification. Named relation will be de-
scribed in the BCO event taxonomy (Kawazoe et 
al., 2007). Extra capabilities will be added to the 
system like classifying outbreak of disease by 
countries, detecting new diseases that are not in out 
ontology, and showing timeline of news stories.
Evaluation of the timelineness system against hu-
man curated sources like ProMED-mail will be 
implemented. Working versions for other lan-
guages like Vietnamese, Japanese, and Thai are 
also being considered, using the existing BioCaster 
disease ontology.
Acknowledgements
The authors wish to thank Mike Conway at the 
National Institute of Informatics for revising the 
manuscript, and both Mika Shigematsu and Kiyosu 
Taniguchi at the National Institute of Infectious 
Diseases for useful discussions. This work was 
supported by Grants-in-Aid from the Japan Society 
for the Promotion of Science (grant no. 18049071).
References
J. Brownstein and C. Freifeld. 2007. HealthMap ?
Global Disease Alert Mapping System.
http://www.healthmap.org. 
N. Collier, A. Kawazoe, L. Jin, M. Shigematsu, D.
Dien, R. Barrero, K. Takeuchi, A. Kawtrakul. 2007a. 
A multilingual ontology for infectious disease out-
break surveillance: rationale, design and challenges.
Journal of Language Resources and Evaluation. DOI: 
10.1007/s10579-007-9019-7.
N. Collier, A.Kawazoe, S. Doan, M. Shigematsu, K. 
Taniguchi, L. Jin, J. McCrae, H. Chanlekha, D. Dien, 
Q. Hung, V.C. Nam, K. Takeuchi, A. Kawtrakul. 
2007b. Detecting Web Rumors with a Multilingual 
Ontology - Supported Text Classification System.
Advances in Disease Surveillance, pp.242, vol.4, 
2007.
S. Doan, A. Kawazoe, and N.Collier. 2007. The Roles of 
Roles in Classifying Annotated Biomedical Text. 
Proc. of BioNLP - Biological, translational, and clini-
cal language processing 2007, pp.17-24, 2007.
A. Kawazoe, L. Jin, M. Shigematsu, R. Barrero, K. Ta-
niguchi and N. Collier. 2006. The development of a 
schema for the annotation of terms in the BioCaster 
disease detection/tracking system. Proc. of the Int?l 
Workshop on Biomedical Ontology in Action (KR-
MED 2006), Baltimore, Maryland, USA, November 
8, pp. 77-85, 2006.
A. Kawazoe, H. Chanlekha, M. Shigematsu and N. Col-
lier. 2007. Structuring an event ontology for disease 
outbreak detection. The 2nd International Sympo-
sium on Languages in Biology and Medicine (LBM) 
(accepted to appear).
MedISys. 2007. Medical Intelligence System.
http://medusa.jrc.it/medisys. 
S. Morse S. 2007. Global Infectious Disease Surveil-
lance And Health
 Intelligence. Health Affairs, 26(4):1069-1077, 2007.
ProMED-mail. 2001. The Program for Monitoring 
Emerging Diseases. http://www.promedmail.org.
956
Incorporating topic information into sentiment analysis models
Tony Mullen
National Institute of Informatics (NII)
Hitotsubashi 2-1-2, Chiyoda-ku
Tokyo 101-8430,
Japan,
mullen@nii.ac.jp
Nigel Collier
National Institute of Informatics (NII)
Hitotsubashi 2-1-2, Chiyoda-ku
Tokyo 101-8430,
Japan,
collier@nii.ac.jp
Abstract
This paper reports experiments in classifying texts based upon their favorability towards the subject of the
text using a feature set enriched with topic information on a small dataset of music reviews hand-annotated
for topic. The results of these experiments suggest ways in which incorporating topic information into such
models may yield improvement over models which do not use topic information.
1 Introduction
There are a number of challenging aspects in recognizing the favorability of opinion-based texts, the task
known as sentiment analysis. Opinions in natural language are very often expressed in subtle and complex
ways, presenting challenges which may not be easily addressed by simple text categorization approaches
such as n-gram or keyword identification approaches. Although such approaches have been employed ef-
fectively (Pang et al, 2002), there appears to remain considerable room for improvement. Moving beyond
these approaches can involve addressing the task at several levels. Negative reviews may contain many ap-
parently positive phrases even while maintaining a strongly negative tone, and the opposite is also common.
This paper attempts to address this issue using Support Vector Machines (SVMs), a well-known and
powerful tool for classification of vectors of real-valued features (Vapnik, 1998). The present approach
emphasizes the use of a variety of diverse information sources. In particular, several classes of features based
upon the proximity of the topic with phrases which have been assigned favorability values are described in
order to take advantage of situations in which the topic of the text may be explicitly identified.
2 Motivation
In the past, work has been done in the area of characterizing words and phrases according to their emotive
tone (Turney and Littman, 2003; Turney, 2002; Kamps et al, 2002; Hatzivassiloglou and Wiebe, 2000;
Hatzivassiloglou and McKeown, 2002; Wiebe, 2000), but in many domains of text, the values of individual
phrases may bear little relation to the overall sentiment expressed by the text. Pang et al (2002)?s treatment
of the task as analogous to topic-classification underscores the difference between the two tasks. A number
of rhetorical devices, such as the drawing of contrasts between the reviewed entity and other entities or ex-
pectations, sarcasm, understatement, and digressions, all of which are used in abundance in many discourse
domains, create challenges for these approaches. It is hoped that incorporating topic information along the
lines suggested in this paper will be a step towards solving some of these problems.
3 Methods
3.1 Semantic orientation with PMI
Here, the term semantic orientation (SO) (Hatzivassiloglou and McKeown, 2002) refers to a real number
measure of the positive or negative sentiment expressed by a word or phrase. In the present work, the
approach taken by Turney (2002) is used to derive such values for selected phrases in the text. For the
purposes of this paper, these phrases will be referred to as value phrases, since they will be the sources of
SO values. Once the desired value phrases have been extracted from the text, each one is assigned an SO
value. The SO of a phrase is determined based upon the phrase?s pointwise mutual information (PMI) with
the words ?excellent? and ?poor?. PMI is defined by Church and Hanks (1989) as follows:
 
	
ffProceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 345?352,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Automatic Classification of Verbs in Biomedical Texts
Anna Korhonen
University of Cambridge
Computer Laboratory
15 JJ Thomson Avenue
Cambridge CB3 0GD, UK
alk23@cl.cam.ac.uk
Yuval Krymolowski
Dept. of Computer Science
Technion
Haifa 32000
Israel
yuvalkr@cs.technion.ac.il
Nigel Collier
National Institute of Informatics
Hitotsubashi 2-1-2
Chiyoda-ku, Tokyo 101-8430
Japan
collier@nii.ac.jp
Abstract
Lexical classes, when tailored to the appli-
cation and domain in question, can provide
an effective means to deal with a num-
ber of natural language processing (NLP)
tasks. While manual construction of such
classes is difficult, recent research shows
that it is possible to automatically induce
verb classes from cross-domain corpora
with promising accuracy. We report a
novel experiment where similar technol-
ogy is applied to the important, challeng-
ing domain of biomedicine. We show that
the resulting classification, acquired from
a corpus of biomedical journal articles,
is highly accurate and strongly domain-
specific. It can be used to aid BIO-NLP
directly or as useful material for investi-
gating the syntax and semantics of verbs
in biomedical texts.
1 Introduction
Lexical classes which capture the close relation
between the syntax and semantics of verbs have
attracted considerable interest in NLP (Jackendoff,
1990; Levin, 1993; Dorr, 1997; Prescher et al,
2000). Such classes are useful for their ability to
capture generalizations about a range of linguis-
tic properties. For example, verbs which share the
meaning of ?manner of motion? (such as travel,
run, walk), behave similarly also in terms of
subcategorization (I traveled/ran/walked, I trav-
eled/ran/walked to London, I traveled/ran/walked
five miles). Although the correspondence between
the syntax and semantics of words is not perfect
and the classes do not provide means for full se-
mantic inferencing, their predictive power is nev-
ertheless considerable.
NLP systems can benefit from lexical classes
in many ways. Such classes define the mapping
from surface realization of arguments to predicate-
argument structure, and are therefore an impor-
tant component of any system which needs the
latter. As the classes can capture higher level
abstractions they can be used as a means to ab-
stract away from individual words when required.
They are also helpful in many operational contexts
where lexical information must be acquired from
small application-specific corpora. Their predic-
tive power can help compensate for lack of data
fully exemplifying the behavior of relevant words.
Lexical verb classes have been used to sup-
port various (multilingual) tasks, such as compu-
tational lexicography, language generation, ma-
chine translation, word sense disambiguation, se-
mantic role labeling, and subcategorization acqui-
sition (Dorr, 1997; Prescher et al, 2000; Korho-
nen, 2002). However, large-scale exploitation of
the classes in real-world or domain-sensitive tasks
has not been possible because the existing classi-
fications, e.g. (Levin, 1993), are incomprehensive
and unsuitable for specific domains.
While manual classification of large numbers of
words has proved difficult and time-consuming,
recent research shows that it is possible to auto-
matically induce lexical classes from corpus data
with promising accuracy (Merlo and Stevenson,
2001; Brew and Schulte im Walde, 2002; Ko-
rhonen et al, 2003). A number of ML methods
have been applied to classify words using features
pertaining to mainly syntactic structure (e.g. sta-
tistical distributions of subcategorization frames
(SCFs) or general patterns of syntactic behaviour,
e.g. transitivity, passivisability) which have been
extracted from corpora using e.g. part-of-speech
tagging or robust statistical parsing techniques.
345
This research has been encouraging but it has
so far concentrated on general language. Domain-
specific lexical classification remains unexplored,
although it is arguably important: existing clas-
sifications are unsuitable for domain-specific ap-
plications and these often challenging applications
might benefit from improved performance by uti-
lizing lexical classes the most.
In this paper, we extend an existing approach
to lexical classification (Korhonen et al, 2003)
and apply it (without any domain specific tun-
ing) to the domain of biomedicine. We focus on
biomedicine for several reasons: (i) NLP is criti-
cally needed to assist the processing, mining and
extraction of knowledge from the rapidly growing
literature in this area, (ii) the domain lexical re-
sources (e.g. UMLS metathesaurus and lexicon1)
do not provide sufficient information about verbs
and (iii) being linguistically challenging, the do-
main provides a good test case for examining the
potential of automatic classification.
We report an experiment where a classifica-
tion is induced for 192 relatively frequent verbs
from a corpus of 2230 biomedical journal articles.
The results, evaluated with domain experts, show
that the approach is capable of acquiring classes
with accuracy higher than that reported in previous
work on general language. We discuss reasons for
this and show that the resulting classes differ sub-
stantially from those in extant lexical resources.
They constitute the first syntactic-semantic verb
classification for the biomedical domain and could
be readily applied to support BIO-NLP.
We discuss the domain-specific issues related to
our task in section 2. The approach to automatic
classification is presented in section 3. Details of
the experimental evaluation are supplied in sec-
tion 4. Section 5 provides discussion and section
6 concludes with directions for future work.
2 The Biomedical Domain and Our Task
Recent years have seen a massive growth in the
scientific literature in the domain of biomedicine.
For example, the MEDLINE database2 which cur-
rently contains around 16M references to journal
articles, expands with 0.5M new references each
year. Because future research in the biomedical
sciences depends on making use of all this existing
knowledge, there is a strong need for the develop-
1http://www.nlm.nih.gov/research/umls
2http://www.ncbi.nlm.nih.gov/PubMed/
ment of NLP tools which can be used to automat-
ically locate, organize and manage facts related to
published experimental results.
In recent years, major progress has been made
on information retrieval and on the extraction of
specific relations e.g. between proteins and cell
types from biomedical texts (Hirschman et al,
2002). Other tasks, such as the extraction of fac-
tual information, remain a bigger challenge. This
is partly due to the challenging nature of biomedi-
cal texts. They are complex both in terms of syn-
tax and semantics, containing complex nominals,
modal subordination, anaphoric links, etc.
Researchers have recently began to use deeper
NLP techniques (e.g. statistical parsing) in the do-
main because they are not challenged by the com-
plex structures to the same extent than shallow
techniques (e.g. regular expression patterns) are
(Lease and Charniak, 2005). However, deeper
techniques require richer domain-specific lexical
information for optimal performance than is pro-
vided by existing lexicons (e.g. UMLS). This is
particularly important for verbs, which are central
to the structure and meaning of sentences.
Where the lexical information is absent, lexical
classes can compensate for it or aid in obtaining
it in the ways described in section 1. Consider
e.g. the INDICATE and ACTIVATE verb classes in
Figure 1. They capture the fact that their members
are similar in terms of syntax and semantics: they
have similar SCFs and selectional preferences, and
they can be used to make similar statements which
describe similar events. Such information can be
used to build a richer lexicon capable of support-
ing key tasks such as parsing, predicate-argument
identification, event extraction and the identifica-
tion of biomedical (e.g. interaction) relations.
While an abundance of work has been con-
ducted on semantic classification of biomedical
terms and nouns, less work has been done on the
(manual or automatic) semantic classification of
verbs in the biomedical domain (Friedman et al,
2002; Hatzivassiloglou and Weng, 2002; Spasic et
al., 2005). No previous work exists in this domain
on the type of lexical (i.e. syntactic-semantic) verb
classification this paper focuses on.
To get an initial idea about the differences be-
tween our target classification and a general lan-
guage classification, we examined the extent to
which individual verbs and their frequencies dif-
fer in biomedical and general language texts. We
346
PROTEINS: p53
p53Tp53Dmp53...
ACTIVATE
suggestsdemonstratesindicatesimplies...
GENES: WAF1
WAF1CIP1p21...
It
INDICATE
that
activatesup-regulatesinducesstimulates...
...
Figure 1: Sample lexical classes
BIO BNC
show do
suggest say
use make
indicate go
contain see
describe take
express get
bind know
require come
observe give
find think
determine use
demonstrate find
perform look
induce want
Table 1: The 15 most frequent verbs in the
biomedical data and in the BNC
created a corpus of 2230 biomedical journal arti-
cles (see section 4.1 for details) and compared the
distribution of verbs in this corpus with that in the
British National Corpus (BNC) (Leech, 1992). We
calculated the Spearman rank correlation between
the 1165 verbs which occurred in both corpora.
The result was only a weak correlation: 0.37 ?
0.03. When the scope was restricted to the 100
most frequent verbs in the biomedical data, the
correlation was 0.12 ? 0.10 which is only 1.2?
away from zero. The dissimilarity between the
distributions is further indicated by the Kullback-
Leibler distance of 0.97. Table 1 illustrates some
of these big differences by showing the list of 15
most frequent verbs in the two corpora.
3 Approach
We extended the system of Korhonen et al (2003)
with additional clustering techniques (introduced
in sections 3.2.2 and 3.2.4) and used it to ob-
tain the classification for the biomedical domain.
The system (i) extracts features from corpus data
and (ii) clusters them using five different methods.
These steps are described in the following two sec-
tions, respectively.
3.1 Feature Extraction
We employ as features distributions of SCFs spe-
cific to given verbs. We extract them from cor-
pus data using the comprehensive subcategoriza-
tion acquisition system of Briscoe and Carroll
(1997) (Korhonen, 2002). The system incorpo-
rates RASP, a domain-independent robust statis-
tical parser (Briscoe and Carroll, 2002), which
tags, lemmatizes and parses data yielding com-
plete though shallow parses and a SCF classifier
which incorporates an extensive inventory of 163
verbal SCFs3. The SCFs abstract over specific
lexically-governed particles and prepositions and
specific predicate selectional preferences. In our
work, we parameterized two high frequency SCFs
for prepositions (PP and NP + PP SCFs). No filter-
ing of potentially noisy SCFs was done to provide
clustering with as much information as possible.
3.2 Classification
The SCF frequency distributions constitute the in-
put data to automatic classification. We experi-
ment with five clustering methods: the simple hard
nearest neighbours method and four probabilis-
tic methods ? two variants of Probabilistic Latent
Semantic Analysis and two information theoretic
methods (the Information Bottleneck and the In-
formation Distortion).
3.2.1 Nearest Neighbours
The first method collects the nearest neighbours
(NN) of each verb. It (i) calculates the Jensen-
Shannon divergence (JS) between the SCF distri-
butions of each pair of verbs, (ii) connects each
verb with the most similar other verb, and finally
(iii) finds all the connected components. The NN
method is very simple. It outputs only one clus-
tering configuration and therefore does not allow
examining different cluster granularities.
3.2.2 Probabilistic Latent Semantic Analysis
The Probabilistic Latent Semantic Analysis
(PLSA, Hoffman (2001)) assumes a generative
model for the data, defined by selecting (i) a verb
verbi, (ii) a semantic class classk from the dis-
tribution p(Classes | verbi), and (iii) a SCF scfj
from the distribution p(SCFs | classk). PLSA uses
Expectation Maximization (EM) to find the dis-
tribution p?(SCFs |Clusters, V erbs) which max-
imises the likelihood of the observed counts. It
does this by minimising the cost function
F = ?? log Likelihood(p? | data) +H(p?) .
3See http://www.cl.cam.ac.uk/users/alk23/subcat/subcat.html
for further detail.
347
For ? = 1 minimising F is equivalent to the stan-
dard EM procedure while for ? < 1 the distri-
bution p? tends to be more evenly spread. We use
? = 1 (PLSA/EM) and ? = 0.75 (PLSA?=0.75).
We currently ?harden? the output and assign each
verb to the most probable cluster only4.
3.2.3 Information Bottleneck
The Information Bottleneck (Tishby et al,
1999) (IB) is an information-theoretic method
which controls the balance between: (i) the
loss of information by representing verbs as
clusters (I(Clusters;V erbs)), which has to be
minimal, and (ii) the relevance of the output
clusters for representing the SCF distribution
(I(Clusters; SCFs)) which has to be maximal.
The balance between these two quantities ensures
optimal compression of data through clusters. The
trade-off between the two constraints is realized
through minimising the cost function:
LIB = I(Clusters;V erbs)
? ?I(Clusters; SCFs) ,
where ? is a parameter that balances the con-
straints. IB takes three inputs: (i) SCF-verb dis-
tributions, (ii) the desired number of clusters K,
and (iii) the initial value of ?. It then looks for
the minimal ? that decreases LIB compared to its
value with the initial ?, using the given K. IB de-
livers as output the probabilities p(K|V ). It gives
an indication for the most informative number of
output configurations: the ones for which the rele-
vance information increases more sharply between
K ? 1 and K clusters than between K and K + 1.
3.2.4 Information Distortion
The Information Distortion method (Dimitrov
and Miller, 2001) (ID) is otherwise similar to IB
but LID differs from LIB by an additional term that
adds a bias towards clusters of similar size:
LID = ?H(Clusters |V erbs)
? ?I(Clusters; SCFs)
= LIB ?H(Clusters) .
ID yields more evenly divided clusters than IB.
4 Experimental Evaluation
4.1 Data
We downloaded the data for our experiment from
the MEDLINE database, from three of the 10 lead-
4The same approach was used with the information theo-
retic methods. It made sense in this initial work on biomedi-
cal classification. In the future we could use soft clustering a
means to investigate polysemy.
ing journals in biomedicine: 1) Genes & Devel-
opment (molecular biology, molecular genetics),
2) Journal of Biological Chemistry (biochemistry
and molecular biology) and 3) Journal of Cell Bi-
ology (cellular structure and function). 2230 full-
text articles from years 2003-2004 were used. The
data included 11.5M words and 323,307 sentences
in total. 192 medium to high frequency verbs (with
the minimum of 300 occurrences in the data) were
selected for experimentation5. This test set was
big enough to produce a useful classification but
small enough to enable thorough evaluation in this
first attempt to classify verbs in the biomedical do-
main.
4.2 Processing the Data
The data was first processed using the feature ex-
traction module. 233 (preposition-specific) SCF
types appeared in the resulting lexicon, 36 per verb
on average.6 The classification module was then
applied. NN produced Knn = 42 clusters. From
the other methods we requested K = 2 to 60 clus-
ters. We chose for evaluation the outputs corre-
sponding to the most informative values of K: 20,
33, 53 for IB, and 17, 33, 53 for ID.
4.3 Gold Standard
Because no target lexical classification was avail-
able for the biomedical domain, human experts (4
domain experts and 2 linguists) were used to cre-
ate the gold standard. They were asked to examine
whether the test verbs similar in terms of their syn-
tactic properties (i.e. verbs with similar SCF distri-
butions) are similar also in terms of semantics (i.e.
they share a common meaning). Where this was
the case, a verb class was identified and named.
The domain experts examined the 116 verbs
whose analysis required domain knowledge
(e.g. activate, solubilize, harvest), while the lin-
guists analysed the remaining 76 general or scien-
tific text verbs (e.g. demonstrate, hypothesize, ap-
pear). The linguists used Levin (1993) classes as
gold standard classes whenever possible and cre-
ated novel ones when needed. The domain ex-
perts used two purely semantic classifications of
biomedical verbs (Friedman et al, 2002; Spasic et
al., 2005)7 as a starting point where this was pos-
5230 verbs were employed initially but 38 were dropped
later so that each (coarse-grained) class would have the min-
imum of 2 members in the gold standard.
6This number is high because no filtering of potentially
noisy SCFs was done.
7See http://www.cbr-masterclass.org.
348
1 Have an effect on activity (BIO/29) 8 Physical Relation
1.1 Activate / Inactivate Between Molecules (BIO/20)
1.1.1 Change activity: activate, inhibit 8.1 Binding: bind, attach
1.1.2 Suppress: suppress, repress 8.2 Translocate and Segregate
1.1.3 Stimulate: stimulate 8.2.1 Translocate: shift, switch
1.1.4 Inactivate: delay, diminish 8.2.2 Segregate: segregate, export
1.2 Affect 8.3 Transmit
1.2.1 Modulate: stabilize, modulate 8.3.1 Transport: deliver, transmit
1.2.2 Regulate: control, support 8.3.2 Link: connect, map
1.3 Increase / decrease: increase, decrease 9 Report (GEN/30)
1.4 Modify: modify, catalyze 9.1 Investigate
2 Biochemical events (BIO/12) 9.1.1 Examine: evaluate, analyze
2.1 Express: express, overexpress 9.1.2 Establish: test, investigate
2.2 Modification 9.1.3 Confirm: verify, determine
2.2.1 Biochemical modification: 9.2 Suggest
dephosphorylate, phosphorylate 9.2.1 Presentational:
2.2.2 Cleave: cleave hypothesize, conclude
2.3 Interact: react, interfere 9.2.2 Cognitive:
3 Removal (BIO/6) consider, believe
3.1 Omit: displace, deplete 9.3 Indicate: demonstrate, imply
3.2 Subtract: draw, dissect 10 Perform (GEN/10)
4 Experimental Procedures (BIO/30) 10.1 Quantify
4.1 Prepare 10.1.1 Quantitate: quantify, measure
4.1.1 Wash: wash, rinse 10.1.2 Calculate: calculate, record
4.1.2 Mix: mix 10.1.3 Conduct: perform, conduct
4.1.3 Label: stain, immunoblot 10.2 Score: score, count
4.1.4 Incubate: preincubate, incubate 11 Release (BIO/4): detach, dissociate
4.1.5 Elute: elute 12 Use (GEN/4): utilize, employ
4.2 Precipitate: coprecipitate 13 Include (GEN/11)
coimmunoprecipitate 13.1 Encompass: encompass, span
4.3 Solubilize: solubilize,lyse 13.2 Include: contain, carry
4.4 Dissolve: homogenize, dissolve 14 Call (GEN/3): name, designate
4.5 Place: load, mount 15 Move (GEN/12)
5 Process (BIO/5): linearize, overlap 15.1 Proceed:
6 Transfect (BIO/4): inject, microinject progress, proceed
7 Collect (BIO/6) 15.2 Emerge:
7.1 Collect: harvest, select arise, emerge
7.2 Process: centrifuge, recover 16 Appear (GEN/6): appear, occur
Table 2: The gold standard classification with a
few example verbs per class
sible (i.e. where they included our test verbs and
also captured their relevant senses)8.
The experts created a 3-level gold standard
which includes both broad and finer-grained
classes. Only those classes / memberships were
included which all the experts (in the two teams)
agreed on.9 The resulting gold standard includ-
ing 16, 34 and 50 classes is illustrated in table 2
with 1-2 example verbs per class. The table in-
dicates which classes were created by domain ex-
perts (BIO) and which by linguists (GEN). Each
class was associated with 1-30 member verbs10.
The total number of verbs is indicated in the table
(e.g. 10 for PERFORM class).
4.4 Measures
The clusters were evaluated against the gold stan-
dard using measures which are applicable to all the
8Purely semantic classes tend to be finer-grained than lex-
ical classes and not necessarily syntactic in nature. Only these
two classifications were found to be similar enough to our tar-
get classification to provide a useful starting point. Section 5
includes a summary of the similarities/differences between
our gold standard and these other classifications.
9Experts were allowed to discuss the problematic cases
to obtain maximal accuracy - hence no inter-annotator agree-
ment is reported.
10The minimum of 2 member verbs were required at the
coarser-grained levels of 16 and 34 classes.
classification methods and which deliver a numer-
ical value easy to interpret.
The first measure, the adjusted pairwise preci-
sion, evaluates clusters in terms of verb pairs:
APP = 1K
K?
i=1
num. of correct pairs in ki
num. of pairs in ki ?
|ki|?1
|ki|+1
APP is the average proportion of all within-
cluster pairs that are correctly co-assigned. Multi-
plied by a factor that increases with cluster size it
compensates for a bias towards small clusters.
The second measure is modified purity, a global
measure which evaluates the mean precision of
clusters. Each cluster is associated with its preva-
lent class. The number of verbs in a cluster K that
take this class is denoted by nprevalent(K). Verbs
that do not take it are considered as errors. Clus-
ters where nprevalent(K) = 1 are disregarded as
not to introduce a bias towards singletons:
mPUR =
?
nprevalent(ki)?2
nprevalent(ki)
number of verbs
The third measure is the weighted class accu-
racy, the proportion of members of dominant clus-
ters DOM-CLUSTi within all classes ci.
ACC =
C?
i=1
verbs in DOM-CLUSTi
number of verbs
mPUR can be seen to measure the precision of
clusters and ACC the recall. We define an F mea-
sure as the harmonic mean of mPUR and ACC:
F = 2 ?mPUR ? ACCmPUR + ACC
The statistical significance of the results is mea-
sured by randomisation tests where verbs are
swapped between the clusters and the resulting
clusters are evaluated. The swapping is repeated
100 times for each output and the average avswaps
and the standard deviation ?swaps is measured.
The significance is the scaled difference signif =
(result? avswaps)/?swaps .
4.5 Results from Quantitative Evaluation
Table 3 shows the performance of the five clus-
tering methods for K = 42 clusters (as produced
by the NN method) at the 3 levels of gold stan-
dard classification. Although the two PLSA vari-
ants (particularly PLSA?=0.75) produce a fairly ac-
curate coarse grained classification, they perform
worse than all the other methods at the finer-
grained levels of gold standard, particularly ac-
cording to the global measures. Being based on
349
16 Classes 34 Classes 50 Classes
APP mPUR ACC F APP mPUR ACC F APP mPUR ACC F
NN 81 86 39 53 64 74 62 67 54 67 73 69
IB 74 88 47 61 61 76 74 75 55 69 87 76
ID 79 89 37 52 63 78 65 70 53 70 77 73
PLSA/EM 55 72 49 58 43 53 61 57 35 47 66 55
PLSA?=0.75 65 71 68 70 53 48 76 58 41 34 77 47
Table 3: The performance of the NN, PLSA, IB and ID methods with Knn = 42 clusters
16 Classes 34 Classes 50 Classes
K APP mPUR ACC F APP mPUR ACC F APP mPUR ACC F
20 IB 74 77 66 71 60 56 86 67 54 48 93 63
17 ID 67 76 60 67 43 56 81 66 34 46 91 61
33 IB 78 87 52 65 69 75 81 77 61 67 93 77
ID 81 88 43 57 65 75 70 72 54 67 82 73
53 IB 71 87 41 55 61 78 66 71 54 72 79 75
ID 79 89 33 48 66 79 55 64 53 72 68 69
Table 4: The performance of IB and ID for the 3 levels of class hierarchy for informative values of K
pairwise similarities, NN shows mostly better per-
formance than IB and ID on the pairwise measure
APP but the global measures are better for IB and
ID. The differences are smaller in mPUR (yet sig-
nificant: 2? between NN and IB and 3? between
NN and ID) but more notable in ACC (which is
e.g. 8 ? 12% better for IB than for NN). Also
the F results suggest that the two information the-
oretic methods are better overall than the simple
NN method.
IB and ID also have the advantage (over NN) that
they can be used to produce a hierarchical verb
classification. Table 4 shows the results for IB and
ID for the informative values of K. The bold font
indicates the results when the match between the
values of K and the number of classes at the par-
ticular level of the gold standard is the closest.
IB is clearly better than ID at all levels of gold
standard. It yields its best results at the medium
level (34 classes) with K = 33: F = 77 and APP
= 69 (the results for ID are F = 72 and APP =
65). At the most fine-grained level (50 classes),
IB is equally good according to F with K = 33,
but APP is 8% lower. Although ID is occasion-
ally better than IB according to APP and mPUR
(see e.g. the results for 16 classes with K = 53)
this never happens in the case where the corre-
spondence between the number of gold standard
classes and the values of K is the closest. In other
words, the informative values of K prove really
informative for IB. The lower performance of ID
seems to be due to its tendency to create evenly
sized clusters.
All the methods perform significantly better
than our random baseline. The significance of the
results with respect to two swaps was at the 2?
level, corresponding to a 97% confidence that the
results are above random.
4.6 Qualitative Evaluation
We performed further, qualitative analysis of clus-
ters produced by the best performing method IB.
Consider the following clusters:
A: inject, transfect, microinfect, contransfect (6)
B: harvest, select, collect (7.1)
centrifuge, process, recover (7.2)
C: wash, rinse (4.1.1)
immunoblot (4.1.3)
overlap (5)
D: activate (1.1.1)
When looking at coarse-grained outputs, in-
terestingly, K as low as 8 learned the broad
distinction between biomedical and general lan-
guage verbs (the two verb types appeared only
rarely in the same clusters) and produced large se-
mantically meaningful groups of classes (e.g. the
coarse-grained classes EXPERIMENTAL PROCE-
DURES, TRANSFECT and COLLECT were mapped
together). K = 12 was sufficient to iden-
tify several classes with very particular syntax
One of them was TRANSFECT (see A above)
whose members were distinguished easily be-
cause of their typical SCFs (e.g. inject /trans-
fect/microinfect/contransfect X with/into Y).
On the other hand, even K = 53 could not iden-
tify classes with very similar (yet un-identical)
syntax. These included many semantically similar
sub-classes (e.g. the two sub-classes of COLLECT
350
shown in B whose members take similar NP and
PP SCFs). However, also a few semantically dif-
ferent verbs clustered wrongly because of this rea-
son, such as the ones exemplified in C. In C, im-
munoblot (from the LABEL class) is still somewhat
related to wash and rinse (the WASH class) because
they all belong to the larger EXPERIMENTAL PRO-
CEDURES class, but overlap (from the PROCESS
class) shows up in the cluster merely because of
syntactic idiosyncracy.
While parser errors caused by the challeng-
ing biomedical texts were visible in some SCFs
(e.g. looking at a sample of SCFs, some adjunct
instances were listed in the argument slots of the
frames), the cases where this resulted in incorrect
classification were not numerous11.
One representative singleton resulting from
these errors is exemplified in D. Activate ap-
pears in relatively complicated sentence struc-
tures, which gives rise to incorrect SCFs. For ex-
ample, MECs cultured on 2D planar substrates
transiently activate MAP kinase in response to
EGF, whereas... gets incorrectly analysed as SCF
NP-NP, while The effect of the constitutively ac-
tivated ARF6-Q67L mutant was investigated... re-
ceives the incorrect SCF analysis NP-SCOMP. Most
parser errors are caused by unknown domain-
specific words and phrases.
5 Discussion
Due to differences in the task and experimental
setup, direct comparison of our results with pre-
viously published ones is impossible. The clos-
est possible comparison point is (Korhonen et al,
2003) which reported 50-59% mPUR and 15-19%
APP on using IB to assign 110 polysemous (gen-
eral language) verbs into 34 classes. Our results
are substantially better, although we made no ef-
fort to restrict our scope to monosemous verbs12
and although we focussed on a linguistically chal-
lenging domain.
It seems that our better result is largely due
to the higher uniformity of verb senses in the
biomedical domain. We could not investigate this
effect systematically because no manually sense
11This is partly because the mistakes of the parser are
somewhat consistent (similar for similar verbs) and partly be-
cause the SCFs gather data from hundreds of corpus instances,
many of which are analysed correctly.
12Most of our test verbs are polysemous according to
WordNet (WN) (Miller, 1990), but this is not a fully reliable
indication because WN is not specific to this domain.
annotated data (or a comprehensive list of verb
senses) exists for the domain. However, exami-
nation of a number of corpus instances suggests
that the use of verbs is fairly conventionalized in
our data13. Where verbs show less sense varia-
tion, they show less SCF variation, which aids the
discovery of verb classes. Korhonen et al (2003)
observed the opposite with general language data.
We examined, class by class, to what extent our
domain-specific gold standard differs from the re-
lated general (Levin, 1993) and domain classifica-
tions (Spasic et al, 2005; Friedman et al, 2002)
(recall that the latter were purely semantic clas-
sifications as no lexical ones were available for
biomedicine):
33 (of the 50) classes in the gold standard are
biomedical. Only 6 of these correspond (fully or
mostly) to the semantic classes in the domain clas-
sifications. 17 are unrelated to any of the classes in
Levin (1993) while 16 bear vague resemblance to
them (e.g. our TRANSPORT verbs are also listed
under Levin?s SEND verbs) but are too different
(semantically and syntactically) to be combined.
17 (of the 50) classes are general (scientific)
classes. 4 of these are absent in Levin (e.g. QUAN-
TITATE). 13 are included in Levin, but 8 of them
have a more restricted sense (and fewer members)
than the corresponding Levin class. Only the re-
maining 5 classes are identical (in terms of mem-
bers and their properties) to Levin classes.
These results highlight the importance of build-
ing or tuning lexical resources specific to different
domains, and demonstrate the usefulness of auto-
matic lexical acquisition for this work.
6 Conclusion
This paper has shown that current domain-
independent NLP and ML technology can be used
to automatically induce a relatively high accu-
racy verb classification from a linguistically chal-
lenging corpus of biomedical texts. The lexical
classification resulting from our work is strongly
domain-specific (it differs substantially from pre-
vious ones) and it can be readily used to aid BIO-
NLP. It can provide useful material for investigat-
ing the syntax and semantics of verbs in biomed-
ical data or for supplementing existing domain
lexical resources with additional information (e.g.
13The different sub-domains of the biomedical domain
may, of course, be even more conventionalized (Friedman et
al., 2002).
351
semantic classifications with additional member
verbs). Lexical resources enriched with verb class
information can, in turn, better benefit practical
tasks such as parsing, predicate-argument identifi-
cation, event extraction, identification of biomedi-
cal relation patterns, among others.
In the future, we plan to improve the accu-
racy of automatic classification by seeding it with
domain-specific information (e.g. using named en-
tity recognition and anaphoric linking techniques
similar to those of Vlachos et al (2006)). We also
plan to conduct a bigger experiment with a larger
number of verbs and demonstrate the usefulness of
the bigger classification for practical BIO-NLP ap-
plication tasks. In addition, we plan to apply sim-
ilar technology to other interesting domains (e.g.
tourism, law, astronomy). This will not only en-
able us to experiment with cross-domain lexical
class variation but also help to determine whether
automatic acquisition techniques benefit, in gen-
eral, from domain-specific tuning.
Acknowledgement
We would like to thank Yoko Mizuta, Shoko
Kawamato, Sven Demiya, and Parantu Shah for
their help in creating the gold standard.
References
C. Brew and S. Schulte im Walde. 2002. Spectral
clustering for German verbs. In Conference on Em-
pirical Methods in Natural Language Processing,
Philadelphia, USA.
E. J. Briscoe and J. Carroll. 1997. Automatic extrac-
tion of subcategorization from corpora. In 5th ACL
Conference on Applied Natural Language Process-
ing, pages 356?363, Washington DC.
E. J. Briscoe and J. Carroll. 2002. Robust accurate
statistical annotation of general text. In 3rd Interna-
tional Conference on Language Resources and Eval-
uation, pages 1499?1504, Las Palmas, Gran Ca-
naria.
A. G. Dimitrov and J. P. Miller. 2001. Neural coding
and decoding: communication channels and quanti-
zation. Network: Computation in Neural Systems,
12(4):441?472.
B. Dorr. 1997. Large-scale dictionary construction for
foreign language tutoring and interlingual machine
translation. Machine Translation, 12(4):271?325.
C. Friedman, P. Kra, and A. Rzhetsky. 2002. Two
biomedical sublanguages: a description based on the
theories of Zellig Harris. Journal of Biomedical In-
formatics, 35(4):222?235.
V. Hatzivassiloglou and W. Weng. 2002. Learning an-
chor verbs for biological interaction patterns from
published text articles. International Journal of
Medical Inf., 67:19?32.
L. Hirschman, J. C. Park, J. Tsujii, L. Wong, and C. H.
Wu. 2002. Accomplishments and challenges in lit-
erature data mining for biology. Journal of Bioin-
formatics, 18(12):1553?1561.
T. Hoffman. 2001. Unsupervised learning by proba-
bilistic latent semantic analysis. Machine Learning,
42(1):177?196.
R. Jackendoff. 1990. Semantic Structures. MIT Press,
Cambridge, Massachusetts.
A. Korhonen, Y. Krymolowski, and Z. Marx. 2003.
Clustering polysemic subcategorization frame distri-
butions semantically. In Proceedings of the 41st An-
nual Meeting of the Association for Computational
Linguistics, pages 64?71, Sapporo, Japan.
A. Korhonen. 2002. Subcategorization Acquisition.
Ph.D. thesis, University of Cambridge, UK.
M. Lease and E. Charniak. 2005. Parsing biomedical
literature. In Second International Joint Conference
on Natural Language Processing, pages 58?69.
G. Leech. 1992. 100 million words of English:
the British National Corpus. Language Research,
28(1):1?13.
B. Levin. 1993. English Verb Classes and Alterna-
tions. Chicago University Press, Chicago.
P. Merlo and S. Stevenson. 2001. Automatic verb
classification based on statistical distributions of
argument structure. Computational Linguistics,
27(3):373?408.
G. A. Miller. 1990. WordNet: An on-line lexical
database. International Journal of Lexicography,
3(4):235?312.
D. Prescher, S. Riezler, and M. Rooth. 2000. Using
a probabilistic class-based lexicon for lexical am-
biguity resolution. In 18th International Confer-
ence on Computational Linguistics, pages 649?655,
Saarbru?cken, Germany.
I. Spasic, S. Ananiadou, and J. Tsujii. 2005. Master-
class: A case-based reasoning system for the clas-
sification of biomedical terms. Journal of Bioinfor-
matics, 21(11):2748?2758.
N. Tishby, F. C. Pereira, and W. Bialek. 1999. The
information bottleneck method. In Proc. of the
37th Annual Allerton Conference on Communica-
tion, Control and Computing, pages 368?377.
A. Vlachos, C. Gasperin, I. Lewin, and E. J. Briscoe.
2006. Bootstrapping the recognition and anaphoric
linking of named entitites in drosophila articles. In
Pacific Symposium in Biocomputing.
352
            	 
   	                          Bio-Medical Entity Extraction using Support Vector Machines
Koichi Takeuchi and Nigel Collier
National Institute of Informatics
2-1-2 Hitotsubashi, Chiyoda-ku
Tokyo 101-8430, Japan
{koichi,collier}@nii.ac.jp
Abstract
Support Vector Machines have achieved
state of the art performance in several clas-
sification tasks. In this article we apply
them to the identification and semantic an-
notation of scientific and technical termi-
nology in the domain of molecular biol-
ogy. This illustrates the extensibility of
the traditional named entity task to spe-
cial domains with extensive terminologies
such as those in medicine and related dis-
ciplines. We illustrate SVM?s capabilities
using a sample of 100 journal abstracts
texts taken from the {human, blood cell,
transcription factor} domain of MED-
LINE. Approximately 3400 terms are an-
notated and the model performs at about
74% F-score on cross-validation tests. A
detailed analysis based on empirical ev-
idence shows the contribution of various
feature sets to performance.
1 Introduction
With the rapid growth in the number of published
papers in the scientific fields such as medicine there
has been growing interest in the application of In-
formation Extraction (IE), (Thomas et al, 1999)
(Craven and Kumlien, 1999), to help solve some
of the problems that are associated with informa-
tion overload. IE can benefit the medical sciences
by enabling the automatic extraction of facts related
to prototypical events such as those contained in pa-
tient records or research articles regarding molecular
processes and their affect on human health. These
facts can then be used to populate databases, aid in
searching or document summarization and a variety
of tasks which require the computer to have an in-
telligent understanding of the contents inside a doc-
ument.
Our aim here is to show a state of the art method
for identifying and classifying technical terminol-
ogy. This task is an extension of the named entity
task defined by the DARPA-sponsored Message Un-
derstanding Conferences (MUCs) (MUC, 1995) and
is aimed at acquiring the shallow semantic building
blocks that contribute to a high level understanding
of the text. Although our study here looks at shallow
semantics that can be captured using IE our basic
goal is to join this with deep semantic representa-
tions so that computers can obtain a full understand-
ing of the facts in a text using logical inference and
reasoning. The scenario is that human experts will
create taxonomies and axioms (ontologies) and by
providing a small set of annotated examples, ma-
chine learning can take over the role of instance cap-
turing though information extraction technology.
Recent studies into the use of supervised learning-
based models for the named entity task have
shown that models based on hidden Markov mod-
els (HMMs) (Bikel et al, 1997), and decision trees
(Sekine et al, 1998), and maximum entropy (Borth-
wick et al, 1998) are much more generalisable and
adaptable to new classes of words than systems
based on hand-built patterns (including wrappers)
and domain specific heuristic rules such as (Herzig
and Johns, 1997).
The method we use is based on support vec-
tor machines (SVMs)(Vapnik, 1995), a state of the
art model that has achieved new levels of perfor-
mance in many classification tasks. In previous
work we have shown SVMs to be superior to sev-
eral other commonly used machine learning meth-
ods for named entity in previous experiments such
as HMMs and C4.5 (citations omitted). This pa-
per explores the underlying SVM model and shows
through detailed empirical analysis the key features
and parameter settings.
To show the application of SVMs to term ex-
traction in unstructured texts related to the medi-
cal sciences we are using a collection of abstracts
from PubMed?s MEDLINE (MEDLINE, 1999). The
MEDLINE database is an online collection of ab-
stracts for published journal articles in biology and
medicine and contains more than nine million arti-
cles. The collection we use in our tests is a con-
trolled subset of MEDLINE obtained using three
search keywords in the domain of molecular biol-
ogy. From the retrieved abstracts 100 were ran-
domly chosen for annotation by a human expert ac-
cording to classes in a small top-level ontology.
In the remainder of this paper in Section (2) we
outline the background to the task and the data set
we are using; in Section (3) we described the basic
advantages of SVMs and the formal model we are
using as well as implementation specific issues such
as the choice of feature set and report experimental
results. In Section (4) we provide extensive results
and a discussion of four sets of experiments we con-
ducted that show the best feature sets and parameter
settings in our sample domain.
2 Background
The names that we are trying to extract fall into
a number of categories that are outside the defini-
tions used for the traditional named-entity task used
in MUC. For this reason we consider the task of
term identification and classification to be an ex-
tended named entity task (NE+) in which the goal
is to find types as well as individuals and where the
term classes belong to an explicitly defined ontol-
ogy. The use of an ontology allows us to associate
human-readable terms in the domain with a set of
computer-readable classes, relations, properties and
axioms (Gruber, 1993).
The particular difficulties with identifying and
classifying terms in scientific and technical domains
are the size of the vocabulary (Lindberg et al,
1993), an open growing vocabulary (Lovis et al,
1995), irregular naming conventions as well as ex-
tensive cross-over in vocabulary between named en-
tity classes. The irregular naming arises in part be-
cause of the number of researchers and practitioners
from different fields who are working on the same
knowledge discovery area as well as the large num-
ber of entities that need to be named. Despite the
best efforts of major journals to standardize the ter-
minology, there is also a significant problem with
synonymy so that often an entity has more than
one name that is widely used. In molecular bi-
ology for example class cross-over of terms may
arise because many DNA and RNA are named af-
ter the protein with which they transcribe. This se-
mantic ambiguity which is dependent on often com-
plex contextual conditions is one of the main rea-
sons why we need learnable models and why it is
difficult to re-use existing term lists and vocabular-
ies such as MeSH(NLM, 1997), UMLS (Lindberg et
al., 1993) or those found in databases such as Swis-
sProt (Bairoch and Apweiler, 1997). An additional
obstacle to re-use is that the classification scheme
used within an existing thesaurus or database may
not be the same as the one in the users? ontology
which may change from time to time as the consen-
sus view of the structure of knowledge is refined.
Our work has focussed on identifying names be-
longing to the classes shown in Table 1 which are all
taken from the domain of molecular biology . Exam-
ple sentences from a marked up abstract are given in
Figure 1. The ontology (Tateishi et al, 2000) that
underlies this classification scheme describes a sim-
ple top-level model which is almost flat except for
the source class which shows places where genetic
activity occurs and has a number of sub-types. Fur-
ther discussion of our use of deep semantic struc-
tures in the ontology is given elsewhere1 and we
will now focus our attention on the machine learning
model used to capture low level semantics.
The training set we used in our experiments called
Bio1 consists of 100 MEDLINE abstracts, marked
up in XML by a doctoral-qualified domain expert
1Now being submitted for publication
Class # Description
PROTEIN 2125 proteins, protein groups,
families, complexes and
substructures.
DNA 358 DNAs, DNA groups,
regions and genes
RNA 30 RNAs, RNA groups,
regions and genes
SOURCE.cl 93 cell line
SOURCE.ct 417 cell type
SOURCE.mo 21 mono-organism
SOURCE.mu 64 multiorganism
SOURCE.vi 90 virus
SOURCE.sl 77 sublocation
SOURCE.ti 37 tissue
Table 1: Markup classes used in Bio1 with the num-
ber of word tokens for each class.
TI - Differential interactions of <NAME cl=?PROTEIN?
>Rel </NAME >- <NAME cl=?PROTEIN? >NF-kappa B
</NAME > complexes with <NAME cl=?PROTEIN? >I
kappa B alpha </NAME > determine pools of constitutive and
inducible <NAME cl=?PROTEIN? >NF-kappa B </NAME >
activity.
AB - The <NAME cl=?PROTEIN? >Rel </NAME >-
<NAME cl=?PROTEIN? >NF-kappa B </NAME > fam-
ily of transcription factors plays a crucial role in the regula-
tion of genes involved in inflammatory and immune responses.
We demonstrate that in vivo, in contrast to the other mem-
bers of the family, <NAME cl=?PROTEIN? >RelB </NAME
>associates efficiently only with <NAME cl=?PROTEIN?
>NF-kappa B1 </NAME > ( <NAME cl=?PROTEIN?
>p105-p50 </NAME >) and <NAME cl=?PROTEIN? >NF-
kappa B2 </NAME > ( <NAME cl=?PROTEIN? >p100-p52
</NAME >), but not with <NAME cl=?PROTEIN? >cRel
</NAME > or <NAME cl=?PROTEIN? >p65 </NAME >.
The <NAME cl=?PROTEIN? >RelB </NAME >- <NAME
cl=?PROTEIN? >p52 </NAME >heterodimers display a
much lower affinity for <NAME cl=?PROTEIN? >I kappa
B alpha </NAME > than <NAME cl=?PROTEIN? >RelB
</NAME >- <NAME cl=?PROTEIN? >p50 </NAME >
heterodimers or <NAME cl=?PROTEIN? >p65 </NAME >
complexes.
Figure 1: Example MEDLINE sentence marked up
in XML for molecular biology named-entities.
for the name classes given in Table 1. The number
of named entities that were marked up by class are
also given in Table 1 and the total number of words
in the corpus is 29940. The abstracts were chosen
from a sub-domain of molecular biology that we for-
mulated by searching under the terms human, blood
cell, transcription factor in the PubMed database.
An example can be seen in Figure 1
3 Method
3.1 Basic model
The named entity task can be formulated as a type of
classification task. In the supervised machine learn-
ing approach which we adopt here we aim to esti-
mate a classification function f ,
f : ?N ? {?1} (1)
so that error on unseen examples is minimized,
using training examples that are N dimensional vec-
tors xi with class labels yi. The sample set S with
m examples is
S = (x1, y1), (x2, y2), . . . , (xm, ym) ? ?N ? {?1}
(2)
The classification function returns either +1 if the
test data is a member of the class, or ?1 if it is not.
SVMs use linear models to discriminate between
two classes. This raises the question of how can they
be used to capture non-linear classification func-
tions? The answer to this is by the use of a non-
linear mapping function called a kernel,
? : ?N ? ? (3)
which maps the input space ?N into a feature
space ?. The kernel function k requires the evalu-
ation of a dot product
k(xi, xj) = (?(xi) ? ?(xj)) (4)
Clearly the complexity of data being classified de-
termines which particular kernel should be used and
of course more complex kernels require longer train-
ing times.
By substituting ?(xi) for each training example
in S we derive the final form of the optimal decision
function f ,
f(x) = sgn(
m?
i
yi?ik(x, xi) + b) (5)
where b ? R is the bias and the Lagrange pa-
rameters ?i (?i ? 0) are estimated using quadratic
optimization to maximize the following function
w(?) =
m?
i=1
?i ? 12
m?
i,j
?i?jyiyjk(xi, xj) (6)
under the constraints that
m?
i=1
?iyi = 0 (7)
and
0 ? ?i ? C (8)
for i = 1, . . . ,m. C is a constant that controls the
ratio between the complexity of the function and the
number of misclassified training examples.
The number of parameters to be estimated in ?
therefore never exceeds the number of examples.
The influence of ?i basically means that training
examples with ?i > 0 define the decision func-
tion (the support vectors) and those examples with
?i = 0 have no influence, making the final model
very compact and testing (but not training) very fast.
The point x is classified as positive (or negative) if
f(x) > 0 (or f(x) < 0).
The kernel function we explored in our exper-
iments was the polynomial function k(xi, xj) =
(xi ? xj + 1)d for d = 2 which was found to be the
best by (Takeuchi and Collier, 2002). Once input
vectors have been mapped to the feature space the
linear discrimination function which is found is the
one which gives the maximum the geometric margin
between the two classes in the feature space.
Besides efficiency of representation, SVMs are
known to maximize their generalizability, making
them an ideal model for the NE+ task. Generaliz-
ability in SVMs is based on statistical learning the-
ory and the observation that it is useful sometimes
to misclassify some of the training data so that the
margin between other training points is maximized.
This is particularly useful for real world data sets
that often contain inseparable data points.
We implemented our method using the Tiny SVM
package from NAIST2 which is an implementation
of Vladimir Vapnik?s SVM combined with an op-
timization algorithm (Joachims, 1999). The multi-
class model is built up from combining binary clas-
sifiers and then applying majority voting.
3.2 Generalising with features
In order for the model to be successful it must recog-
nize regularities in the training data that relate pre-
classified examples of terms with unseen terms that
will be encountered in testing.
Following on from previous studies in named en-
tity we chose a set of linguistically motivated word-
level features that include surface word forms, part
of speech tags using the Brill tagger (Brill, 1992)
and orthographic features. Additionally we used
head-noun features that were obtained from pre-
analysis of the training data set using the FDG shal-
low parser from Conexor (Tapanainen and Ja?rvinen,
1997). A significant proportion of the terms in
our corpus undergo a local syntactic transforma-
tions such as coordination which introduces ambi-
guity that needs to be resolved by shallow parsing.
For example the c- and v-rel (proto) oncogenes and
NF-kappaB and I kappa B protein families. In these
cases the head noun features oncogene and fam-
ily would be added to each word in the constituent
phrase. Head information is also needed when de-
ciding the semantic category of a long term such as
tumor necrosis factor-alpha which should be a PRO-
TEIN, whereas tumor necrosis factor (TNF) gene
and tumor necrosis factor promoter region should
both be types of DNA.
Table 2 shows the orthographic features that we
used. We hypothesize that such features will help the
model to find similarities between known words that
were found in the training set and unknown words
(of zero frequency in the training set) and so over-
come the unknown word problem.
In the experiments we report below we use feature
vectors consisting of differing amounts of ?context?
by varying the window around the focus word which
is to be classified into one of the semantic classes.
The full window of context considered in these ex-
periments is ?3 about the focus word.
2Tiny SVM is available from http:// http://cl.aist-nara.ac.jp/
taku-ku/software/ TinySVM/
Feature Example Feature Example
DigitNumber 15 CloseSquare ]
SingleCap M Colon :
GreekLetter alpha SemiColon ;
CapsAndDigits I2 Percent %
TwoCaps RalGDS OpenParen (
LettersAndDigits p52 CloseParen )
InitCap Interleukin Comma ,
LowCaps kappaB FullStop .
Lowercase kinases Determiner the
Hyphon - Conjunction and
Backslash / Other * + #
OpenSquare [
Table 2: Orthographic features with examples
4 Experiment and Discussion
Results are given as F-scores (van Rijsbergen, 1979)
using the CoNLL evaluation script and are defined
as F = (2PR)/(P+R). where P denotes Precision
and R Recall. P is the ratio of the number of cor-
rectly found NE chunks to the number of found NE
chunks, and R is the ratio of the number of correctly
found NE chunks to the number of true NE chunks.
All results are calculated using 10-fold cross valida-
tion.
4.1 Experiment 1: Effect of Training Set Size
The effect of context window size is shown along
the top column of Tables 3 and 4. It can be seen
that without exception more training data results in
higher overall F-scores except at 10 per cent. where
the result seems to be biased by the small sample,
perhaps because one abstract is partly included in
the training and testing sets. As we would expect
larger training sets reduce the effects of data sparse-
ness and allow more accurate models to be induced.
The rate of increase in improvement however is
not uniform according to the feature sets that are
used. For surface word features and head noun
features the improvement in performance is consis-
tently increasing whereas the improvement for using
orthographic and part of speech features is quite er-
ratic. This may be an effect of the small sample of
training data that we used and we could not find any
consistent explanation why this occurred.
As we observed before, the best overall result
comes from using Or hd, i.e. surface words, or-
thographic and head features. However the to-
tal score hides the fact that three classes, i.e.
SOURCE.mo, SOURCE.mu and SOURCE.ti actu-
ally perform worse when using anything but sur-
face word forms (shown in Table 5). One possi-
ble explanation for this is that all of these classes
have very small numbers of samples and the effect
of adding features may be to blur the distinction be-
tween these and other more numerous classes in the
model. However it is interesting to note that this
does not happen with the RNA class which is also
very small.
4.2 Experiment 2: Effect of Feature Sets
The effects of feature sets is of major importance in
modelling named entity. In general we would like
to identify only the necessary features that are re-
quired and to remove those that do not contribute to
an increase in performance. This also saves time in
training and testing.
The results from Tables 3 and 4 at 100 per cent.
training data are summarized in Table 5 and clearly
illustrate the value of surface word level features
combined with orthographic and head noun features.
Orthographic features allow us to capture many gen-
eralities that are not obvious at the surface word
level such as IkappaB alpha and IkappaB beta both
being PROTEINs and IL-10 and IL-2 both being
PROTEINs.
The orthographic-head noun feature combination
(Or hd) gives the best combined-class performance
of 74.23 at 100 per cent. training data on a -2+2 win-
dow. Overall orthographic features combined with
surface word features gave an improvement of be-
tween 4.9 and 22.0 per cent. at 100 per cent. data
depending on window size over surface words alone.
This was the biggest contribution by any feature ex-
cept the surface words. Head information for exam-
ple allowed us to correctly capture the fact that in
the phrase NF-kappaB consensus site the whole of
it is a DNA, whereas using orthographic informa-
tion alone the SVM could only say that NF-kappaB
was a PROTEIN and ignoring consensus site. We
see a similar case in the phrase primary NK cells
which is correctly classified as SOURCE.ct using
head noun and orthographic features but only NK
cells are found using orthographic features. This
mistake is a natural consequence of a limited con-
textual view which the head noun feature helped to
rectify.
Part of speech (POS) when combined with sur-
face word features gave an improvement of between
7.9 and 11.7 per cent. at 100 per cent. data. The
influence of POS though does not appear to be sus-
tained when combined with other features and we
found that it actually degraded performance slightly
in many cases. This may possibly be due to ei-
ther overlapping knowledge or more likely subtle
inconsistencies between POS features and say, or-
thographic features. This could have occurred dur-
ing training when the POS tagger was trained on an
out of domain (news) text collection. It is possible
that if the POS tagger was trained on in-domain texts
it would make a greater and more consistent con-
tribution. An example where orthographic features
allowed correct classification but adding POS fea-
tures resulted in failure is p50 in the phrase consist-
ing of 50 (p50) - and 65 (p65) -kDa proteins. Also
in the phrase c-Jun transactivation domain where
only c-Jun should be tagged as a protein, by using
orthographic features and POS the model tags the
whole phrase as a PROTEIN. This is probably be-
cause POS tagging gives a NN feature value (com-
mon noun) to each word. This is very general and
does not allow the model to discriminate between
them.
The fourth feature we investigated is related to
syntactic rather than lexical knowledge. We felt
though that there should exist a strong semantic re-
lation between a word in a term and the head noun
of that term. The results in Table 5 show that while
the overall contribution of the Head feature is quite
small, it is consistent for almost all classes.
5 Conclusion
The method we have shown for identifying and clas-
sifying technical terms has the advantage of be-
ing portable, not requiring large domain dependent
dictionaries and no hand-made patterns were used.
Additionally, since all the word level features are
found automatically there is no need for interven-
tion to create domain specific features. Indeed the
only thing that is required is a quite small corpus of
text containing entities tagged by a domain expert.
For future work we are now looking at how to bal-
ance the scores from SVM for each word-class over
the whole of a sentence using dynamic program-
ming. Theoretically the existing SVM model cannot
consider evidence from outside the context window,
in particular evidence related to named entity class
scores in the history and later in the sentence.
References
A. Bairoch and R. Apweiler. 1997. The SWISS-PROT
protein sequence data bank and its new supplement
TrEMBL. Nucleic Acids Research, 25:31?36.
D. Bikel, S. Miller, R. Schwartz, and R. Wesichedel.
1997. Nymble: a high-performance learning name-
finder. In Proceedings of the Fifth Conference on Ap-
plied Natural Language Processing (ANLP?97), Wash-
ington D.C., USA., pages 194?201, 31 March ? 3
April.
A. Borthwick, J. Sterling, E. Agichtein, and R. Grishman.
1998. Exploiting diverse knowledge sources via max-
imum entropy in named entity recognition. In Pro-
ceedings of the Sixth Workshop on Very Large Corpora
(WVLC?98), Montreal, Canada, pages 152?160.
E. Brill. 1992. A simple rule-based part of speech tagger.
In Third Conference on Applied Natural Language
Processing ? Association for Computational Linguis-
tics, Trento, Italy, pages 152?155, 31st March ? 3rd
April.
M. Craven and J. Kumlien. 1999. Constructing biolog-
ical knowledge bases by extracting information from
text sources. In Proceedings of the 7th International
Conference on Intelligent Systemps for Molecular Bi-
ology (ISMB-99), pages 77?86, Heidelburg, Germany,
August 6?10.
T. R. Gruber. 1993. A translation approach to
portable ontology specifications. Knowledge Acqui-
sition, 6(2):199?221.
T. Herzig and M. Johns. 1997. Extraction of medical
information from textual sources: a statistical vari-
ant of the boundary word method. In Proceedings of
the American Medical Informatics Association (AMIA)
1997 Annual Fall Symposium, Nashville, USA, 25?29
October.
T. Joachims. 1999. Making large-scale SVM learning
practical. In B. Scholkopf, C. Burges, and A. Smola,
editors, Advances in Kernel Methods - Support Vector
Learning. MIT Press.
Donald A.B. Lindberg, L. Humphreys, Betsy, and T. Mc-
Cray, Alexa. 1993. The unified medical language
system. Methods of Information in Medicine, 32:281?
291.
Feature Set & Percentage of data used in experiment
Window Size 10 20 30 40 50 60 70 80 90 100
Wd -10 58.52 47.30 51.44 52.40 52.37 52.30 51.29 53.24 55.57 56.06
Wd -1+1 55.35 48.15 53.91 54.50 56.02 55.30 55.92 58.98 60.28 61.55
Wd -2+2 46.87 40.73 47.92 49.64 53.31 53.20 55.01 56.95 59.40 62.04
Wd -3+2 46.12 38.55 44.19 47.93 49.50 50.50 51.21 54.76 56.66 60.25
Wd -3+3 44.83 35.37 42.67 45.24 46.78 49.10 49.66 54.01 55.59 58.83
Or -10 60.33 55.08 63.49 63.41 64.09 63.04 62.97 62.64 64.59 65.63
Or -1+1 65.35 58.69 66.63 68.18 69.20 68.74 69.55 69.32 71.02 72.13
Or -2+2 60.84 58.90 66.44 67.17 69.88 68.81 69.68 69.62 71.41 72.12
Or -3+2 62.48 59.21 65.64 66.69 67.56 67.25 68.37 68.94 69.92 71.69
Or -3+3 59.61 58.65 64.95 65.68 67.11 66.65 67.85 68.84 69.54 71.78
Head -10 58.51 47.10 51.99 52.74 52.44 52.01 53.09 53.79 55.97 57.01
Head -1+1 57.50 50.00 55.81 57.88 58.03 57.84 58.81 61.08 62.64 63.93
Head -2+2 49.43 45.92 53.40 53.75 57.52 56.94 59.33 61.29 63.36 64.67
Head -3+2 46.51 39.42 49.39 49.75 54.54 54.81 56.95 58.13 59.25 61.96
Head -3+3 45.79 40.81 47.52 48.11 53.58 53.50 55.95 57.02 59.06 61.52
POS -10 61.62 52.89 61.14 62.04 62.62 61.51 61.05 60.78 62.71 62.63
POS -1+1 61.24 57.25 63.83 62.94 65.35 64.82 67.40 66.47 67.43 68.37
POS -2+2 57.52 53.11 59.39 59.98 62.86 62.16 63.72 64.17 64.56 66.92
POS -3+2 56.81 54.55 56.53 56.26 59.60 59.40 61.42 61.86 63.41 64.90
POS -3+3 54.76 53.28 56.79 55.02 57.46 57.66 59.60 59.89 62.39 63.50
Table 3: F-scores on Bio1 showing the effects of training set size, feature sets, and context window sizes.
Wd: surface word level features; Or: Orthographic features; Head: Head noun features; POS: part of speech
features.
Feature Set & Percentage of data used in experiment
Window Size 10 20 30 40 50 60 70 80 90 100
Or hd -10 62.16 57.80 64.31 65.70 65.20 63.84 64.90 64.73 66.46 67.31
Or hd -1+1 64.84 60.52 68.42 68.25 68.82 69.34 71.31 71.88 72.60 73.38
Or hd -2+2 61.16 61.10 68.06 67.42 69.32 69.62 70.91 71.31 72.31 74.23
Or hd -3+2 61.54 60.06 65.87 66.33 67.43 68.36 70.28 70.15 70.81 72.95
Or hd -3+3 59.68 57.03 64.58 65.76 66.84 67.16 69.07 69.22 70.73 72.12
Or POS -10 61.48 54.04 63.20 63.92 64.11 64.74 63.23 63.62 64.87 66.28
Or POS -1+1 64.57 58.89 66.52 66.77 67.83 67.90 69.32 69.07 70.84 71.70
Or POS -2+2 61.48 58.56 63.37 65.44 67.01 66.74 68.21 68.55 70.09 71.87
Or POS -3+2 61.08 57.14 64.23 63.39 65.53 65.11 67.31 67.78 68.64 71.54
Or POS -3+3 57.92 57.12 62.86 62.36 65.48 64.41 66.10 66.64 68.22 70.46
POS hd -10 64.90 55.39 61.14 61.65 61.91 61.29 61.88 60.51 63.27 63.82
POS hd -1+1 62.25 57.25 63.66 64.81 64.64 65.57 67.78 67.63 68.69 69.68
POS hd -2+2 58.08 53.23 58.91 60.28 62.55 62.06 64.19 64.51 66.18 67.66
POS hd -3+2 57.09 53.20 56.58 57.75 59.34 59.14 62.19 62.93 64.23 65.41
POS hd -3+3 54.69 51.09 55.67 55.46 58.31 58.28 60.88 61.17 62.94 64.31
Or POS hd -10 63.70 56.63 63.29 65.11 64.72 64.14 64.40 64.04 66.01 67.41
Or POS hd -1+1 66.20 59.65 66.49 67.91 68.44 68.14 70.01 70.61 71.80 72.95
Or POS hd -2+2 61.62 58.03 64.76 65.16 66.45 67.26 69.00 69.86 70.83 72.56
Or POS hd -3+2 62.06 57.28 63.74 64.50 66.10 66.25 68.01 69.05 69.44 71.59
Or POS hd -3+3 59.12 56.51 62.43 62.61 65.37 65.09 66.89 67.80 69.36 71.25
Table 4: F-scores on Bio1 showing the effects of training set size, feature sets, and context window sizes.
Wd: surface word level features; Or: Orthographic features; Head: Head noun features; POS: part of speech
features.
NE+ Class Feature Set
Wd Or Head POS Or hd Or POS POS hd Or POS hd
DNA 44.53 56.49 50.88 47.33 62.78 58.12 47.30 59.19
PROTEIN 65.07 77.50 67.96 72.10 78.99 77.03 72.89 77.58
RNA 12.12 42.11 12.90 24.24 43.24 37.84 6.67 29.41
SOURCE.cl 52.63 57.14 51.52 54.79 59.21 55.90 56.94 59.87
SOURCE.ct 65.83 66.39 66.22 63.70 69.32 67.03 65.65 68.94
SOURCE.mo 32.00 16.67 9.09 17.39 17.39 16.67 17.39 17.39
SOURCE.mu 61.02 58.41 55.24 57.14 51.92 54.55 53.33 51.92
SOURCE.sl 55.22 62.86 62.69 51.20 68.53 62.41 54.84 63.38
SOURCE.ti 23.26 18.18 0.00 14.63 5.00 14.29 0.00 0.00
SOURCE.vi 76.54 75.16 79.50 73.68 80.25 74.84 75.00 73.33
Table 5: Class by class performance using a -2+2 window shown against feature sets. Wd: surface word
level features; Or: Orthographic features; Head: Head noun features; POS: part of speech features.
C. Lovis, P. Michel, R. Baud, and J. Scherrer. 1995.
Word segmentation processing: a way to exponentially
extend medical dictionaries. Medinfo, 8:28?32.
MEDLINE. 1999. The PubMed database can be found
at:. http://www.ncbi.nlm.nih.gov/PubMed/.
DARPA. 1995. Proceedings of the Sixth Message Under-
standing Conference(MUC-6), Columbia, MD, USA,
November. Morgan Kaufmann.
NLM. 1997. Medical subject headings, bethesda, MD.
National Library of Medicine.
Satoshi Sekine, Ralph Grishman, and Hiroyuki Shinnou.
1998. A Decision Tree Method for Finding and Clas-
sifying Names in Japanese Texts. In Proceedings of
the Sixth Workshop on Very Large Corpora, Montreal,
Canada, August.
K. Takeuchi and N. Collier. 2002. Use of support vec-
tor machines in extended named entity recognition. In
Proceedings of the 6th Conference on Natural Lan-
guage Learning 2002 (CoNLL-2002), Roth, D. and
van den Bosch, A. (eds), pages 119?125, August 31st.
P. Tapanainen and T. Ja?rvinen. 1997. A non-projective
dependency parser. In Proceedings of the 5th Confer-
ence on Applied Natural Language Processing, Wash-
ington D.C., Association of Computational Linguis-
tics, pages 64?71.
Y. Tateishi, T. Ohta, N. Collier, C. Nobata, K. Ibushi, and
J. Tsujii. 2000. Building an annotated corpus in the
molecular-biology domain. In COLING?2000 Work-
shop on Semantic Annotation and Intelligent Content,
Luxemburg, 5th?6th August.
J. Thomas, D. Milward, C. Ouzounis, S. Pulman, and
M. Carroll. 1999. Automatic extraction of protein in-
teractions from scientific abstracts. In Proceedings of
the Pacific Symposium on Biocomputing?99 (PSB?99),
pages 1?12, Hawaii, USA, January 4?9.
C. J. van Rijsbergen. 1979. Information Retrieval. But-
terworths, London.
V. N. Vapnik. 1995. The Nature of Statistical Learning
Theory. Springer-Verlag, New York.
Zone Identification in Biology Articles as a Basis for Information Extraction 
Yoko MIZUTA and Nigel COLLIER 
National Institute of Informatics 
2-1-2 Hitotsubashi, Chiyoda-ku,  
Tokyo, Japan, 101-8430  
{ymizuta, collier}@nii.ac.jp 
 
Abstract 
Information extraction (IE) in the biomedical 
domain is now regarded as an essential 
technique for the dynamic management of 
factual information contained in archived journal 
articles and abstract collections. We aim to 
provide a technique serving as a basis for pin-
pointing and organizing factual information 
related to experimental results. In this paper, we 
enhance the idea proposed in (Mizuta and 
Collier, 2004); annotating articles in terms of 
rhetorical zones with shallow nesting. We give a 
qualitative analysis of the zone identification 
(ZI) process in biology articles. Specifically, we 
illustrate the linguistic and other features of each 
zone based on our investigation of articles 
selected from four major online journals. We 
also discuss controversial cases and nested zones, 
and ZI using multiple features. In doing so, we 
provide a stronger theoretical and practical 
support for our framework toward automatic ZI. 
1 Introduction 
Information extraction (IE) in the biomedical 
domain is now regarded as an essential technique 
for utilizing information contained in archived 
journal articles and abstract collections such as 
MEDLINE. Major domain databases often contain 
incomplete and inconsistent results. Also, a 
majority of the reported experimental results are 
only available in unstructured full-text format. 
These being combined, scientists need to check 
with source journal articles to obtain and confirm 
factual information. Furthermore, they often need 
to start with document retrieval and face an 
overwhelming number of candidate articles. Thus, 
the significance of dynamic management of factual 
information, specifically an integration and update 
of experimental results, is self-evident. It would 
not only save researchers much time used for 
retrieval and redundant experiments but also help 
them use the information more effectively. Given 
the limitations of manual work in terms of both 
efficiency and accuracy, IE focusing on factual 
information is of critical importance. 
Researches in bioNLP have made major 
progress mainly in the extraction of bio-named 
entity biological interactions (e.g. Craven et al 
1999; Humphreys et al, 2000; Tanabe et al, 2002). 
But further progress aimed at pin-pointing and 
organizing factual information remains a challenge. 
We aim to provide a basis for this purpose. As 
the first step, we proposed in (Mizuta and Collier, 
2004) annotating biology texts in terms of 
rhetorical zones with a shallow nesting, and 
provided an annotation scheme. In this paper, we 
explore a qualitative analysis of zone identification 
(ZI) in biology articles and provide stronger 
support for our framework toward automatic 
annotation of zones. Specifically we; 1) illustrate 
the linguistic and other features of each zone, 
which have been extracted through our pilot study 
of a total of 20 articles randomly selected from 
four major online journals (EMBO, PNAS, NAR 
and JCB), 2) discuss controversial cases for ZI and 
nested annotation to elaborate the scheme, 3) 
discuss multiple features relevant to ZI, and 4) 
summarize the investigation and outline future 
steps related to machine learning and applications. 
Previous work on rhetorical analysis of scientific 
articles focus on either; 1) hierarchical discourse 
relations between sentences (e.g. Mann and 
Thompson, 1987), 2) genre analysis within a 
descriptive framework (e.g. Swales 1990), or 3) ZI 
in a flat structure and a statistical evaluation of the 
annotation scheme from a machine learning 
perspective (e.g. Teufel and Moens, 2002). We 
follow the lines of (Teufel and Moens, 2002) and 
apply ZI to the domain of biology. But our 
approach is unique in that we focus on 
experimental results and on a qualitative analysis 
of ZI as a basis for automatic ZI. 
2 Overview of the framework 
2.1 The need for zone identification (ZI) 
We discuss below the critical issues in bioNLP 
involved in pin-pointing and organizing factual 
information and show how ZI can be applied. 
First, articles provide information in various 
rhetorical statuses (e.g. new vs. old results; own vs. 
previous work). Current IE relies on surface lexical 
29
and syntactic patterns, neglecting the rhetorical 
status of information. Thus, we are in danger of 
extracting old results mixed with new ones. 
(1) Recent data suggest that ? ~ is involved in 
DPC removal in mammalian cells (ref.), ? 
?The data presented here suggest that ? 
The data  (1) provide statements in different 
rhetorical statuses (boldfaced by us). Preprocessing 
the text in terms of such information helps filter 
out old results (i.e. the first statement). 
Secondly, so far the scope of bioNLP largely 
bear on abstracts. But arguably, the final goal 
should be full texts, given their much richer 
sources of information and the increasing ease of 
access (e.g.  open access to collections such as 
PUBMED-central; online journals such as EMBO, 
PNAS, and JCB). This involves exploring new 
techniques because there are some essential 
differences from abstracts. Among others, full texts 
present much more complexity in the sentence 
structure and vocabulary (e.g. inserted phrases, 
embedded sentences, nominalization of verbs, 
more anaphoric expressions). Thus, we expect that 
the analysis of the whole text requires a much 
more complex set of patterns and algorithms, 1 
resulting in errors. A solution to this problem is to 
identify the subset of the article relevant to further 
analysis at issue. For example, in order to extract 
certain kinds of biological interactions found by 
the author, we could skip statements about 
previous work as seen in the Introduction section. 
Thirdly, experimental results make sense in their 
relation to the experimental goal and procedure. 
Also, there are usually a sequence of experiments 
performed, each of which obtains complex results. 
Therefore, it is important to extract a set of 
experimental results in an organized manner. This 
also helps identify the reference of demonstratives 
(e.g. this) and pronouns (e.g. it). 
From these points of view, ZI in articles plays an 
essential role in extracting factual information of 
different sorts from different zone classes. 
2.2 Characteristics of the framework 
The idea underlying ZI in our sense contrasts 
with other, discourse relations-based notions (e.g. 
Mann et al 1987; Kando 1999; van Dijk, 1980); 
we focus on the global type of information. For 
example, in our ZI, reference to previous work as 
background information remains as such whether it 
is supported or refuted by the author later in the 
article, whereas this difference plays an essential 
role in discourse relations-based analyses. 
                                                     
1 A. Koike (at AVIRG 2004) reported that to extract 
the interactions between two biological elements from 
PUBMED abstracts, about 400 patterns were necessary. 
The larger picture we have consists of 2 levels;  
1) ZI, and 2a) analysis of zone interactions (e.g. 
discourse relations), or 2b) analysis on specific 
zones (i.e. extraction of biological interactions). In 
this paper we focus on the first step. 
2.3 Annotation scheme 
Our annotation scheme is proposed in (Mizuta et 
al., 2004), based on Teufel et al?s (2002) scheme. 
Three major modifications are made; 1) a fine-
grained OWN class based on the model of an 
experimental procedure which we identified across 
journals, 2) CNN and DFF classes to cover the 
relations between data/findings, and 3) nested 
annotation. The set of zone classes is as follows: 
? BKG (Background): given information 
(reference to previous work or a generally 
accepted fact) 
? PBM (Problem-setting): the  problem to be 
solved; the goal of the present work/paper. 
? OTL (Outline): a characterization/ summary of 
the content of the paper. 
? TXT (Textual): section organization of the paper 
(e.g. ?Section 3 describes our method?). 
? OWN: the author?s own work: 
? MTH (Method): experimental procedure; 
? RSL (Result): the results of the experiment; 
? INS (Insight): the author?s insights and findings 
obtained from experimental results (including 
the interpretation) or from previous work 
? IMP (Implication): the implications of 
experimental results (e.g. conjectures, 
assessment, applications, future work) or those 
of previous work 
? ELS (Else): anything else within OWN. 
? CNN (Connection): correlation or consistency 
between data and/or findings.  
? DFF (Difference): a contrast or inconsistency 
between data and/or findings. 
The basic annotation unit is a sentence, but in 
some cases it may be a phrase. In light of those 
cases which fit into multiple zones, we employ 2-
level annotation. Empirical analysis indicates that 
even though zone classes are conceptually non-
overlapping, an annotation unit may fit into 
multiple classes. That is, a linguistic unit (e.g. a 
sentence) may well represent complex concepts. 
Therefore, we consider that nested annotation is 
necessary, even though it complicates annotation. 
3 Zone identification -1: Main features of 
each zone 
Based on our sample annotation of full texts, we 
discuss the major features extracted from the data 
30
of each zone class. Complex cases and the location 
of zones will be discussed in later sections. 
3.1 BACKGROUND (BKG) 
(1) In cells, DNA is tightly associated with ? 
(2) Ref. suggested/ suggests that ~  
(3) A wide variety of restriction-modification (R-
M) systems have been discovered ?. 
BKG has three tense variations; 1) simple 
present for a generic statement about background 
information (e.g. biological facts; reference to 
previous work), 2) simple past, and 3) present 
perfect, to mention the current relevance of 
previous work. A wider range of verbs are used to 
cover both biological and bibliographical facts. 
Citations in the sentence-final position having as 
its scope the whole sentence signal BKG, but inter-
sentential citations having a smaller scope do not. 
3.2 PROBLEM SETTING (PBM )  
There are two types of PBM. 
(2) X has not been established/addressed 
there has been no study on X 
little is currently known about ~ 
there are very limited data concerning X 
X remain unclear 
The first type as illustrated above is observed in 
the I-section2; it addresses the problem to solve. It 
has a ?negative polarity? in that it mentions 
something missing in the current situation (e.g. 
knowledge, study, a research question). It contains 
vocabulary expressing negation or incompleteness 
(boldfaced). Tense variation is either simple 
present or present perfect, depending on the 
temporal interval referred to. The range of verbs 
used has not been analyzed yet. 
(3) To test {whether ~ / this hypothesis/?},  
To evaluate X; To address the question of X  
The second type of PBM is observed in the R-
section. As illustrated in (3), it corresponds to a to-
phrase appearing sentence-initially or finally. It is 
combined with a description of experimental 
procedure, as illustrated in (8).  
The two types of PBM are both related to a goal 
description. The first type concerns the whole work 
and the second type its subset (i.e. an experiment). 
3.3 OUTLINE (OTL) 
(4) We report here the results of experiments?. In 
brief, we have asked, ? To address the first 
question, we utilized ? We found ? Together, 
these results not only confirm that ?. but also 
that? (End of the I-section) 
                                                     
                                                     2 In what follows, I-, M-, R-, and D- section stand for 
Introduction, Method and Materials, Results, and 
Discussion sections, respectively  
OTL provides a concise characterization of (or 
an ?excerpts? from) the work as an abstract does. 
(5)  [Introduction Body Conclusion]full-text article 
The rhetorical scheme of the whole article is 
analyzed as (5). OTL has as its scope ?Body?, and 
thus it is expected to appear either in Introduction 
or Conclusion. This conforms to our investigation. 
Tense choices are between simple present and 
future (in Introduction), and between present 
perfect and simple past (in Conclusion). 
The first element of (4) signals the beginning of 
an OTL zone. By itself it would fit into AIM (of 
the paper) employed in (Teufel et al, 2002). It 
contains certain kind of linguistic signals such as:3 
(6) Indexicals:  
e.g. in this paper; in the present study; here 
?Reporting verbs? or verbs for presentation:  
e.g. we show/ demonstrate/ present/ report 
However, OTL consists of a wider range of 
sentences. As illustrated in (4), OTL also contains 
those elements which provide information relevant 
to other zones (e.g. PBM, MTH and RSL). We 
consider that the whole sequence of sentences in 
(4) deserve an independent class from both 
theoretical and practical perspectives. That is, it is 
embedded in a reporting context, and provides 
abstract-like information. Thus, we propose OTL. 
3.4 TEXTUAL (TXT) 
TXT zones were not observed in our sample. 
This makes sense because the journals investigated 
provide a rigid section format. However, we retain 
this class for future application to other journals 
which may provide a more flexible section format. 
3.5 METHOD (MTH) 
(7) we performed X , using ?; we exploited the 
presence of ~; we utilized sucrose-gradient 
fractionation; X was normalized 
MTH takes the form of an event description in 
the past tense, using matrix verbs expressing the 
experimental procedure (e.g. perform, examine, 
use, collect, purify). Either a passive or an active 
form (with we as its semantic subject) is used. 
(8) [To test ~,] PBM  [we performed ~] MTH. 
We observed that a paragraph in the R-section 
starts with a combination of PBM and MTH as 
illustrated in (8). It is much more common for 
PBM to come first. This can be explained in terms 
of ?iconicity?, the phenomenon that the conceptual 
and/or the real world ordering of elements is often 
reflected in linguistic expressions. In (8), the PBM 
3 For a more comprehensive set of expressions, see, 
for example, (Swales, 1990) and (Teufel et al, 2002). 
31
portion (to-phrase) is preposed conforming to the 
fact that the author first had the experimental goal. 
3.6 RESULT (RSL) 
(9) the distribution of ~ was shifted from ?; 
no significant change was seen; 
cells ? demonstrated an enrichment in ~ 
RSL usually describes an event in the past tense, 
as MTH does, using a certain set of verbs 
expressing; 1) phenomena (e.g. represent, show 
and demonstrate,  having as its subject the material 
used), 2) observations (e.g. observe, recognize and 
see, having we as its subject, or in the passive 
form), or 3)  biological processes (e.g. mutate, 
translate, express, often in the passive form).  
(10) the distribution of ~ is shifted from ? 
no significant change is seen 
cells devoid of Scp160p demonstrates ~ 
~ are presented in Table 2. 
As illustrated above, RSL, unlike MTH, may 
also be written in the present tense to create a 
context in which the author observes and presents 
the results real-time,  referring to figures. 
In the R-section, RSL zones were observed to 
follow MTH with no discourse connectives. 
However, the boundary was rather easy to identify, 
by virtue of a cause-effect relation identified. 
Specifically, matrix verbs used in these zones 
played a critical role; some of them present a rather 
complementary distribution. This feature is useful 
for machine learning too.4 
MTH and RSL may be combined by resulted in: 
(11) [Parallel ? transcription reactions using?] MTH 
resulted in [? strong smears. ] RSL 
However, result in is usually observed in relating 
biological events, and the above usage relating a 
method and results is found uncommon. Also, the 
explicit use of result(s) as below is uncommon: 
(12) The results, ??, were striking. First, ?  
Given these, keyword searches using result(s) do 
not work for the purpose of identifying 
experimental results. In contrast, RSL zones can be 
identified using features such as matrix verbs and 
location. Thus, annotating RSL zones is important. 
(13) Interestingly/ Surprisingly/ Noticeably/?,  
In a RSL zone, empathetic expressions as in (13) 
may be used, often sentence-initially, to call the 
reader?s attention. The adjective version (e.g. 
striking in (12)) is also used. 
                                                     
                                                     4 The occurrences in MTH/ RSL in our sample were: 
perform 38/2, use 181/12, collect 10/1, purify 23/2, 
observe 1/43, reduce 1/15, affect 1/15, associate 6/25. 
However, some verbs had a rather neutral distribution 
(e.g. detect 11/13, follow 26/8). Such cases require the 
use of other features too, as we will discuss later on. 
3.6.1 INSIGHT (INS) 
We have identified three major patterns for INS. 
The examples below illustrate the first pattern: 
(14) [As can be seen in Figure 2C, ? was not 
significantly different compared with that in 
Figure 2A,]RSL [indicating that ? had no 
appreciable effect on ?.] INS 
(15) [Interestingly, central ZYG-9 was significantly 
reduced in ? embryos ??In the converse 
experiment, ? was observed in embryos?. ]RSL 
[These results suggest that ?-tublin is required 
to assemble centrosomes ??] INS 
These are conventionalized forms which the author 
uses in stating his/her interpretation of the results 
with respect to a biological process behind the 
observed results. A generalization is:5 
(16) X indicate Y        (a variant: X, indicating Y  ) 
X: results/experiments/studies, 
Y: biological statement or model,  
Verb variations from our sample: 
indicate/suggest/demonstrate/represent/reveal. 
The second pattern is a sentence using the verb 
seem/ appear or consider such as: 
(17) X seem/appear to V (It seems/ appears that ~) 
X is considered to V 
The third pattern is the use of confirm/ support: 
(18) This was confirmed, as shown in Figure 3. 
Here, this refers to the author?s hypothesis. 
Although (18) refers to a figure which shows the 
result, the sentence does not fit into RSL but into 
INS. We consider that it describes the author?s 
interpretation of the result and that the hypothesis 
is now licensed as an insight. A generalization is: 
(19) X confirm that Y;  Y was confirmed. 
X: results/experiments/studies 
Y: proposition (hypothesis or prediction). 
As we will discuss later, confirm also signals 
CNN, relating two things (X and Y). Therefore, it 
triggers a nested annotation for INS and CNN. 
3.7 IMPLICATION (IMP) 
The IMP class is used as a cover category for the 
author?s ?weaker? insights from experimental 
results and for other kinds of implication of the 
work (e.g. assessment, applications, future work). 
(20) Fusion of ?of type III enzymes, ?, would 
result in  type IIG enzymes? 
(21) We speculate that as ~  lose ?, ~ increases. 
?Weaker? insights (vs. ?regular? insights fitting 
into INS) are signaled by; 1) modal expressions 
5 In our data, suggest occurred mainly in INS (63%) 
and BKG (23%), and indicate in INS(55%), RSL(20%) 
and MTH (10%). This means that these verbs strongly 
signal INS but other features are also needed for ZI (e.g. 
location, zone sequence, and the subject of the verb). 
32
(e.g. could, may, might, be possible, one possibility 
is that) and 2) verbs related to conjecture (e.g. 
speculate, hypothesize), as in the examples above. 
(22) These data are significant because ? 
(23) This approach has the potential to increase ? 
(24) ~ provides structural insights into ~ 
Assessment is signaled by weak linguistic clues 
as illustrated in (22) - (24) above. 
(25) Potential targets also remain to be studied; 
we do not yet know 
(26) Further experiments will focus on ~; 
a future study/work/challenge? 
Taken out of context, IMP mentioning future 
work look very similar to PBM as in (25), unless it 
contains key words such as future and further, as in 
(26). The critical feature for the distinction 
between them is the section in which they appear. 
3.8 ELSE (ELS) 
We found only few cases of ELS in our data. 
The following is an example (a naming statement). 
(27)  ?, we refer to this gene as gip-1 and ~ as ? 
The lack of ELS zone in our data indicates that 
the domain of experimental biology has a more 
established methodology and that the focus is on 
the experiments and the findings obtained. In other 
domains where the methodology is less 
standardized (e.g. computer science), we would 
expect some essential cases fitting into ELS (e.g. 
the author?s proposal and invention) and thus 
further elaboration of classes would be needed. 
3.9 DIFFERENCE (DFF) 
(28) [ [ These effects are significantly different 
from the effects caused by ?]DFF ] RSL 
(29) [ [ Our structural results differ somewhat from 
the previous proposal (ref.) and ?]DFF ]INS 
As in (28) and (29), DFF is signalled by a limited 
set of vocabulary (mainly, different and contrast 
and their variants). Also, as illustrated above, DFF 
often overlaps with other classes (e.g. INS, IMP, 
RSL), and therefore involves nested annotation. 
3.10 CONNECTION (CNN) 
(30) This conservation further supports their 
putative regulatory role in exon skipping. 
(31) this peroxide treatment experiment was 
consistent with previous data 
(32) The results also confirm the recent discovery of 
MntH ? (ref). 
(33) This conclusion was supported not only by 
? but also by ? 
The CNN class covers statements mentioning 
consistency (i.e. some sort of positive relation) 
between data/findings. A generalization is: 
(34) X is consistent with Y ; X conform to Y 
X is {similar to/ same as?Y ; X support Y  
X/Y: previous work, the author?s observation,  
model, hypothesis, insight, etc. 
(35) X. Similarly, Y. (X/Y: a proposition) 
The specific relation mentioned shows a variety 
(e.g. correlation or similarity; support for the 
author?s own or other?s data/ idea/ findings). 6 
Interestingly, we observed more CNN zones 
than DFF zones in our sample (Mizuta et al, 2004), 
and we consider that this is not accidental; this 
asymmetry indicates that biologists put more focus 
on correlation between two elements.7  
4 Zone identification -2: elaboration 
4.1 Nested zones for complex concepts 
The following examples illustrate complex zones 
motivating nested annotation: 
(36)  [ [Similar DNA links were also observed in the 
complexes with ? (ref.), which show structural 
similarities with?.] CNN  ] RSL 
(37) [ [Previous 113Cd NMR studies on ? indicated 
that zinc plays a catalytic role.]BKG [According 
to the mechanism we propose, Zn2+ plays a 
crucial role only in?.]INS ]DFF [Another 
difference from the previous proposal is?]DFF 
Sentence (36) provides a result and compares it 
with other results (boldfaced). Thus, the sentence 
fits into RSL and CNN simultaneously; it is a case 
of combined zones, conceptually distinct from 
indeterminacy between two zones. Sentence (37) 
illustrates an example of nested zones. The first 
two sentences fit into BKG and INS respectively. 
Also, they contrast with each other, with respect to 
the role which zinc is claimed to play, deserving of 
DFF as a whole (but there is no explicit clue at this 
point). The key word in the third sentence, another 
difference (boldfaced)  licenses the sentence to 
DFF and also indicates an element referring to a 
difference already mentioned. Accordingly the first 
two sentences will be annotated for DFF. 
Precisely speaking, combined zones and nested 
zones are not identical. But we treat combined 
zones as a special case of nesting, as two zones 
having the same scope and an arbitrary ordering. 
Importantly, nested zones (in a wider sense) are 
conceptually distinct from ambiguity between two 
zones; the sentences simultaneously fit into 
                                                     
6  DFF and CNN classes cover a wide range of 
relations between data and findings. 
7  This insight was checked with a biologist. This 
asymmetry also suggests the essential difference 
between the biology and the computer science domains. 
In the scheme by (Teufel et al, 2002) focusing on 
computer science articles, CONTRAST seems to be 
more important than BASIS. 
33
multiple zones. In fact, in our sample, most CNN 
and DFF zones overlap with another zone such as 
INS and IMP. Since CNN and DFF zones are 
important for our purpose, we consider that nested 
annotation is necessary. 
4.2 Controversial cases  
(38) However, it was not evident whether DPCs 
composed of ? were ? or protelytic 
degradation was involved in the process. 
A PBM zone (in I-section) and an IMP zone 
describing future work (or limitations) often look 
very similar on the surface, as illustrated in (38), 
which is the last sentence in the article describing 
the limitation of the work presented. A critical 
feature is the location; PBM in this use is located 
in the I-section, whereas IMP in other sections. 
A PBM zone in I-section (e.g. X remains 
unclear) is considered to be a subset of a larger 
BKG zone when the problem mentioned is a 
generally accepted fact. However, we chose to 
avoid nested annotation in this case, because; 1) 
the situation above is rather common, and yet 2) 
we identify the significance of PBM zone in its 
own. In case a single sentence consists of a clause 
fitting into BKG and another fitting into PBM, 
then it will result in a complex annotation. That is, 
we annotate the sentence as both BKG and PBM. 
5 Zone identification -3: location 
We now analyze the zones appearing in each 
section and their sequence, to try to describe the 
locations where a specific zone class may appear. 
The section organization of the sample articles is 
mapped onto the scheme shown in (5) as follows:8 
(39) [IIntro [M R D(non-final)]Body D(final) Conc] 
In what follows, I, M, R, and D stand for the 
corresponding section. 
5.1 I-section and M-section 
Common to all sample articles, the I-section 
consists of a large number of BKG zones with a 
few PBM zones inserted in it, which is then 
followed by an OTL zone. The OTL zone may or 
may not constitute a separate paragraph. 
The M-section focuses on methodological details, 
and thus consists of MTH zones possibly with an 
ignorable number of other zones (e.g. BKG, INS).  
5.2 R-section 
The R-section consists of ?problem-solving? 
units following the experimental procedure. The 
main elements of each unit are PBM, MTH, and 
RSL zones, which are often then followed by an 
                                                     
                                                     
8  Or, [IIntro [R D(non-final) M ]Body D(final) Conc] 
INS zone. There are also some optional elements. 
A generalization of the zone patterns is as follows. 
For practical reasons, we use the regular 
expression style; superscripts + and * stand for the 
occurrence of one (+) / zero (*) or more times. 
Brackets represent OR-relation. 
(40) ( X*  PandM  MTH+  (RSL  INS*  IMP*)* )+ 
X:  an arbitrary zone, and 
PandM = [ (PBM MTH)  (MTH PBM) ] 
Below are examples of an optional zone (X) 
placed at the beginning a problem-solving unit:9 
(41) [It is possible that ? ]IMP [To test this 
possibility,]PBM [we examined ?] MTH  
(42) [... has revealed two motifs (Fig. 1). As can be 
seen in Figure 1A, ??.]RSL [To ascertain that 
?.]PBM [ we aligned their weight ?]MTH 
5.3 D-section 
The D-section is much more complex and 
flexible, but some generalization is possible.  
First, the essential components of D-section, 
both quantitatively and qualitatively, are INS and 
IMP zones. This indicates that the focus of D-
section is on obtaining deeper insights. In contrast 
with the zone sequence in the R-section, INS and 
IMP often precede, or even lack, RSL and BKG 
zones related to them. A closer look at examples 
explains the apparent lack of  RSL/BKG: 
(43) The data within this report demonstrate? 
(44) As for the C-rich element, its comparison with 
the PTB binding motif has shown that these are 
different motifs. 
(45) Similarly, the failure of ... protein (Fig. 7) 
suggests that... 
The italicized elements in (43) - (45) would fit into 
RSL or MTH, but are too small constituents to be 
annotated. As a result, only the whole sentence  
gets annotated as INS. A similar tendency holds 
also for BKG (e.g. since-clause), but less 
frequently. We may consider extracting these cases 
in future work. Usually D-section ends with OTL 
(summary) or IMP (assessment or future work). 
6 Zone identification using multiple features 
Table 1 illustrates multiple features contributing 
to ZI, as we identified them through our manual 
annotation. We observed that certain pairs of zone 
classes  present similar distribution of key features, 
with the same primary feature, and that BKG lacks 
a key feature, indicating its neutral nature. Using 
multiple features is critical in ZI. We intend to 
9  We observe that these paragraph-initial zones 
trigger the PBM zone. For example, this in (41) refers to 
the preceding IMP zone, and the RSL in (42) mentions 
the results of a preceding experiment. 
34
improve our insight shown here through 
quantitative analysis (cf. fn. 3 and 4). It then better 
helps determine the right set of features and their 
relative priority to be used in machine learning. 
 
Feature\ Zone  B   P  O   M R  INS IMP  CNN DFF
lexical/syntactic -   ??  -  -  ?   ?     ?   ? 
matrix verb -   ?? ?? -      -        -     -  
location -   ?? ???   ?      ?  ? 
zone sequence -   ?? ???   ?      ?  ? 
reference to Fig x   -   -   ?? x    x        x    x 
citation ? -   -    -  x   x    x       ?  ? 
Table 1: Multiple features for ZI  
Explanatory notes on the priority of features: 
?: primary feature (with specific clues); 
?: major feature;  ?: secondary feature 
x:  negative feature;  -: non-/less informative 
7 Conclusion 
We have provided a qualitative analysis of the 
process and results of ZI based on our hand-
annotated sample, with a view to strengthening the 
basis for the annotation scheme. We are now 
starting to use our sample as training data for 
machine learning, as well as creating more data in 
a systematic way, toward automatic annotation. 
We are also considering to use our ontology 
management tool (Open Ontology Forge, 
http://research.nii.ac.jp/~collier/resources/OOF/ind
ex.htm) for these purposes; 1) to define zone 
classes as ontology classes; zone annotation is then 
expected to be a variant of named entity annotation, 
which we are familiar with, and 2) to link between 
expressions referring to results (e.g. these results/ 
our results) and their antecedent (i.e. the RSL zone 
providing a concrete description of the 
experimental results), using the coreference tool. 
Applications include full color representation of 
annotated texts; a sample is available at: 
http://research.nii.ac.jp/~collier/projects/ZAISA/in
dex.htm. Also, IR focusing on particular zone 
classes should improve the quality of retrieval. 
Specifically, the goal of the experiment (a PBM 
zone) is expected to be used as an index for the 
organization and retrieval of experimental results. 
Acknowledgements 
We gratefully acknowledge the kind support of 
our colleague Tony Mullen with the quantitative 
analysis of the data, the generous support of 
Professor Asao Fujiyama (NII) and the funding 
from the BioPortal project, and the very helpful 
comments from the three anonymous reviewers. 
References 
M. Craven and J. Kumlien. 1999. Constructing 
biological knowledge bases by extracting 
information from text sources. In Proceedings of 
the 7th Intl. Conference on Intelligent Systems 
for Molecular Biology (ISMB?99). 
D.K. Farkas. 1999. The logical and rhetorical 
construction of procedural discourse. Technical 
Communications, 43(1): 42-53. 
K. Humphreys, G. Demetriou and R. Gaizauskas. 
2000. Two applications of information extraction 
to biological science journal articles: Enzyme 
interactions and protein structures. In 
Proceedings of the 5th Pacific Symposium on 
Biocomputing (PSB2000). 
A. Lehman. 1999. Text structuration leading to an 
automatic summary system. Information 
Processing and Management, 35(2):181-191. 
W.C. Mann and S.A. Thompson. 1987. Rhetorical 
structure theory: toward a functional theory of 
text organization. Text, 8(3):243-281. 
D. Marcu and A. Echihabi. 2002. An unsupervised 
approach to recognizing discourse relations. In 
Proceedings of the 40th Annual Meeting of the 
Association for Computational Linguistics. 
Y. Mizuta and N. Collier. 2004. An Annotation 
Scheme for a Rhetorical Analysis of Biology 
Articles. In Proceedings of the Fourth Intl. 
Conference on Language Resources and 
Evaluation (LREC2004). 
C.D. Paice and P.A. Jones. 1993. The 
identification of important concepts in highly 
structured technical papers. In Proceedings of the 
16th Intl. ACM-SIGIR Conference on Research 
and Development in Information Retrieval. 
J. Swales. 1990. Genre analysis. Cambridge UP. 
L. Tanabe and W. Wilbur. 2002. Tagging gene and 
protein names in biomedical text. Bioinformatics, 
18:1124-1132. 
S. Teufel, J. Carletta and M. Moens. 1999. An 
annotation scheme for discourse-level 
argumentation in research articles. In 
Proceedings of the 9th EACL Conference. 
S. Teufel and M. Moens. 1999. Argumentative 
classification of extracted sentences as a first 
step towards flexible abstracting. In ?Advances 
in automatic text summarization?, Mani, I. and 
Maybury, M.T, eds. Cambridge, MA: MIT Press. 
S. Teufel and M. Moens. 2002. Summarizing 
Scientific Articles: Experiments with Relevance 
and Rhetorical Status. Computational Linguistics,  
28(4):409-445. 
T. A. van Dijk. 1980. Macrostructures. Hillsdale, 
NJ: Lawrence Erlbaum. 
35
Sentiment analysis using support vector machines with diverse information
sources
Tony Mullen and Nigel Collier
National Institute of Informatics (NII)
Hitotsubashi 2-1-2, Chiyoda-ku
Tokyo 101-8430
Japan
 
mullen,collier  @nii.ac.jp
Abstract
This paper introduces an approach to sentiment
analysis which uses support vector machines
(SVMs) to bring together diverse sources of po-
tentially pertinent information, including several fa-
vorability measures for phrases and adjectives and,
where available, knowledge of the topic of the
text. Models using the features introduced are fur-
ther combined with unigram models which have
been shown to be effective in the past (Pang et
al., 2002) and lemmatized versions of the unigram
models. Experiments on movie review data from
Epinions.com demonstrate that hybrid SVMs which
combine unigram-style feature-based SVMs with
those based on real-valued favorability measures
obtain superior performance, producing the best re-
sults yet published using this data. Further experi-
ments using a feature set enriched with topic infor-
mation on a smaller dataset of music reviews hand-
annotated for topic are also reported, the results of
which suggest that incorporating topic information
into such models may also yield improvement.
1 Introduction
Recently an increasing amount of research has been
devoted to investigating methods of recognizing fa-
vorable and unfavorable sentiments towards specific
subjects within natural language texts. Areas of ap-
plication for such analysis are numerous and varied,
ranging from newsgroup flame filtering and infor-
mative augmentation of search engine responses to
analysis of public opinion trends and customer feed-
back. For many of these tasks, classifying the tone
of the communication as generally positive or nega-
tive is an important step.
There are a number of challenging aspects of this
task. Opinions in natural language are very of-
ten expressed in subtle and complex ways, present-
ing challenges which may not be easily addressed
by simple text categorization approaches such as
n-gram or keyword identification approaches. Al-
though such approaches have been employed effec-
tively (Pang et al, 2002), there appears to remain
considerable room for improvement. Moving be-
yond these approaches can involve addressing the
task at several levels. Recognizing the semantic im-
pact of words or phrases is a challenging task in it-
self, but in many cases the overarching sentiment
of a text is not the same as that of decontextualized
snippets. Negative reviews may contain many ap-
parently positive phrases even while maintaining a
strongly negative tone, and the opposite is also com-
mon.
This paper introduces an approach to classify-
ing texts as positive or negative using Support Vec-
tor Machines (SVMs), a well-known and powerful
tool for classification of vectors of real-valued fea-
tures (Vapnik, 1998). The present approach em-
phasizes the use of a variety of diverse information
sources, and SVMs provide the ideal tool to bring
these sources together. We describe the methods
used to assign values to selected words and phrases,
and we introduce a method of bringing them to-
gether to create a model for the classification of
texts. In addition, several classes of features based
upon the proximity of the topic with phrases which
have been assigned favorability values are described
in order to take further advantage of situations in
which the topic of the text may be explicitly iden-
tified. The results of a variety of experiments are
presented, using both data which is not topic anno-
tated and data which has been hand annotated for
topic. In the case of the former, the present approach
is shown to yield better performance than previous
models on the same data. In the case of the latter,
results indicate that our approach may allow for fur-
ther improvements to be gained given knowledge of
the topic of the text.
2 Motivation
A continual challenge in the task of sentiment anal-
ysis of a text is to home in on those aspects of
the text which are in some way representative of
the tone of the whole text. In the past, work has
been done in the area of characterizing words and
phrases according to their emotive tone (Turney
and Littman, 2003; Turney, 2002; Kamps et al,
2002; Hatzivassiloglou and Wiebe, 2000; Hatzi-
vassiloglou and McKeown, 2002; Wiebe, 2000),
but in many domains of text, the values of indi-
vidual phrases may bear little relation to the over-
all sentiment expressed by the text. Pang et al
(2002)?s treatment of the task as analogous to topic-
classification underscores the difference between
the two tasks. Sources of misleading phrases in-
clude what Pang et al (2002) refer to as ?thwarted
expectations? narrative, where emotive effect is at-
tained by emphasizing the contrast between what
the reviewer expected and the actual experience.
For example, in the record review data used in
the present experiments, the sentence, ?How could
they not be the most unimaginative, bleak,
whiny emo band since...? occurs in one of the
most highly rated reviews, describing the reviewer?s
initial misgivings about the record under review
based on its packaging, followed immediately by
?I don?t know. But it?s nothing like you?d imag-
ine. Not even almost.? Clearly, the strongly pos-
itive sentiment conveyed by these four sentences is
much different from what we would expect from the
sum of its parts. Likewise, another exceptionally
highly rated review contains the quote: ?This was a
completely different band, defeated, miserable,
and exhausted, absolutely, but not hopeless:
they had somehow managed to succeed where
every other band in their shoes had failed.?
Other rhetorical devices which tend to widen the
gap in emotional tone between what is said locally
in phrases and what is meant globally in the text in-
clude the drawing of contrasts between the reviewed
entity and other entities, sarcasm, understatement,
and digressions, all of which are used in abundance
in many discourse domains.
The motivation of the present research has been
to incorporate methods of measuring the favorabil-
ity content of phrases into a general classification
tool for texts.
3 Methods
3.1 Semantic orientation with PMI
Here, the term semantic orientation (SO) (Hatzi-
vassiloglou and McKeown, 2002) refers to a real
number measure of the positive or negative senti-
ment expressed by a word or phrase. In the present
work, the approach taken by Turney (2002) is used
to derive such values for selected phrases in the text.
This approach is simple and surprisingly effective.
Moreover, is not restricted to words of a particular
part of speech, nor even restricted to single words,
but can be used with multiple word phrases. In
general, two word phrases conforming to particular
part-of-speech templates representing possible de-
scriptive combinations are used. The phrase pat-
terns used by Turney can be seen in figure 1. In
some cases, the present approach deviates from this,
utilizing values derived from single words. For the
purposes of this paper, these phrases will be referred
to as value phrases, since they will be the sources
of SO values. Once the desired value phrases have
been extracted from the text, each one is assigned
an SO value. The SO of a phrase is determined
based upon the phrase?s pointwise mutual informa-
tion (PMI) with the words ?excellent? and ?poor?.
PMI is defined by Church and Hanks (1989) as fol-
lows:
 
	
ffBioNLP 2007: Biological, translational, and clinical language processing, pages 17?24,
Prague, June 2007. c?2007 Association for Computational Linguistics
The Role of Roles in Classifying Annotated Biomedical Text
Son Doan, Ai Kawazoe, Nigel Collier
National Institute of Informatics
2-1-2 Hitotsubashi, Chiyoda-ku, Tokyo, Japan
{doan,zoeai,collier}@nii.ac.jp
Abstract
This paper investigates the roles of named
entities (NE?s) in annotated biomedical text
classification. In the annotation schema of
BioCaster, a text mining system for pub-
lic health protection, important concepts that
reflect information about infectious diseases
were conceptually analyzed with a formal
ontological methodology. Concepts were
classified as Types, while others were iden-
tified as being Roles. Types are specified
as NE classes and Roles are integrated into
NEs as attributes. We focus on the Roles
of NEs by extracting and using them in
different ways as features in the classifier.
Experimental results show that: 1) Roles
for each NE greatly helped improve perfor-
mance of the system, 2) combining informa-
tion about NE classes with their Roles con-
tribute significantly to the improvement of
performance. We discuss in detail the effect
of each Role on the accuracy of text classifi-
cation.
1 Introduction
Today, the Internet is a powerful tool for discov-
ering novel information via news feed providers.
This is becoming increasingly important for the
public health domain because it can help to de-
tect emerging and re-emerging diseases. In infec-
tious disease surveillance systems such as the Global
Public Health Intelligence Network (GPHIN) sys-
tem (Public Health Agency of Canada, 2004) and
ProMed-Mail (International Society for Infectious
Diseases, 2001), the detection and tracking of out-
breaks using the Internet has been proven to be a
key source of information for public health work-
ers, clinicians, and researchers interested in com-
municable diseases. The basis for such systems is
the monitoring of a large number of news articles
simultaneously. The classification of news articles
into disease-related or none disease-related classes
is the first stage in any automated approach to this
task. In practice though there are a large number of
news articles whose main subject is related to dis-
eases but which should not necessarily be notified
to users together with a relatively small number of
high priority articles that experts should be actively
alerted to. Alerting criteria broadly include news re-
lated to newly emerging diseases, the spread of dis-
eases across international borders, the deliberate re-
lease of a human or engineered pathogen, etc. The
use of only raw text in the classification process in-
evitably fails to resolve many subtle ambiguities, for
example semantic class ambiguities in polysemous
words like ?virus?, ?fever?, ?outbreak?, and ?con-
trol? which all exhibit a variety of senses depending
on context. These different senses appear with rela-
tively high frequency in the news especially in head-
lines. A further challenge is that diseases can be de-
noted by many variant forms. Therefore we consider
that the use of advanced natural language process-
ing (NLP) techniques like named entity recognition
(NER) and anaphora resolution are needed in order
to achieve high classification accuracy.
Text classification is defined as the task of as-
signing documents into one or more predefined cat-
17
egories. As shown by (Cohen and Hersh, 2005),
an accurate text classification system can be espe-
cially valuable to database curators. A document in
the biomedical domain can be annotated using NER
techniques with enriched semantic information in
the form of NEs such as the disease, pathogen, loca-
tion, and time. NER and term identification in gen-
eral have been recognized as an important research
topic both in the NLP and biomedical communities
(Krauthammer and Nenadic, 2004). However, an in-
vestigation into the contribution of NEs on the per-
formance of annotated biomedical text classification
has remained an open question until now. There are
two main reasons for this: Firstly there are a small
number of open annotation schema for biomedical
text, and secondly there is no benchmark annotated
data for testing.
The BioCaster project (Collier, 2006) is working
towards the detection and tracking of disease out-
breaks from Internet news articles. Although there
are several schema for biomedical text (Wilbur et al,
2006), little work has been done on developing one
specifically for public health related text. BioCaster
therefore provides an annotation schema that can fill
this gap. Our schema, which is based on discussions
with biologists, computational linguists and public
health experts, helps identify entities related to in-
fectious diseases which are then used to build up a
detailed picture of events in later stages of text min-
ing. One significant aspect of the schema is that it
is based on conceptual analysis with a formal on-
tological methodology. As discussed in (Kawazoe
et al, 2006), by applying meta-properties (Guarino
and Welty, 2000a; Guarino and Welty, 2000b), our
?markable? concepts are classified into ?Type? and
?Role?. Information about Role concepts is inte-
grated into the schema as attributes on NEs. This
work takes the investigation one step forward by
showing empirical evidence for the usefulness of
Role concepts in a practical application.
In this paper, we focus on the task of text classifi-
cation, proceeding under the simplifying assumption
that given enough annotated training data for NEs
and their Roles both can be automatically tagged
with high accuracy. In recent years there have been
many studies on text classification using general
methods (Sebastiani, 2002; Yang and Liu, 1999)
semi-structured texts (Kudo and Matsumoto, 2004),
and XML classification (Zaki and Aggarwal, 2003).
Other research has investigated the contribution of
semantic information in the form of synonyms, syn-
tax, etc. in text representation (Bloehdorn and
Hotho, 2004; Hotho et al, 2003; Fru?rnkranz et al,
1998). Feature selection (Scott and Matwin, 1999)
has also been studied. The contribution of this paper
is to provide an analysis and evaluation on the Roles
of NEs in annotated text classification.
The rest of this paper is organized as follows: in
Section 2, we outline the BioCaster schema for the
annotation of terms in biomedical text; Section 3
presents a description of the BioCaster gold standard
corpus; Section 4 provides details of the method
and experimental results of classification on the gold
standard corpus. Finally we draw some conclusions
in Section 5.
2 BioCaster Schema for Annotation of
Terms in Biomedical Text
The BioCaster annotation schema is a component of
the BioCaster text mining project. We have iden-
tified several important concepts that reflect infor-
mation about infectious diseases, and created guide-
lines for annotating them as target entity classes
in texts. Based on the conceptual analysis using
meta-properties (rigidity, identity, and dependency)
developed by Guarino and Welty (2000a; 2000b),
categories of important concepts were classified as
Types, i.e., properties which are rigid1 and supply
identity conditions, while others were identified as
being Roles, properties which are anti-rigid2 and
dependent. The 18 categories of Type concepts
are specified as NE classes which we denote here
in upper case. These include PERSON, LOCA-
TION, ORGANIZATION, TIME, DISEASE, CON-
DITION (status of patient such as ?hospitalized?
or ?in stable condition?), OUTBREAK (event of
group infection), VIRUS, ANATOMY (body part),
PRODUCT (biological product such as ?vaccine?),
NONHUMAN (animals), DNA, RNA, PROTEIN,
CONTROL (control measures to contain the dis-
ease), BACTERIA, CHEMICAL and SYMPTOM.
The three Role concepts we explore are case (dis-
1A property is rigid if every instance of that property neces-
sarily has the property, i.e. in every possible world.
2A property is anti-rigid if no instance of that property nec-
essarily has the property.
18
eased person), transmission (source of infection)
and therapeutic (therapeutic agent). These are inte-
grated into the annotation schema as XML attributes
which are associated with some XML elements de-
noting Type concepts. PERSON takes a case at-
tribute, NONHUMAN and ANATOMY take trans-
mission, PRODUCT takes transmission and thera-
peutic and CHEMICAL takes therapeutic. For PER-
SON we added another attribute number (number
of people). Each attribute has only one value, the
value of number is one or many, and the value of
case, transmission, therapeutic is true or false. This
is summarized in Table 1. In the rest of this paper,
we call case, transmission, and therapeutic ?Role at-
tributes? (or ?Role? for short) and number a ?Qual-
ity attributes? (or ?Quality? for short).
A NE in a biomedical text is annotated following
the BioCaster annotation schema in XML format as
follows,
<NAME cl="Named Entity"
attribute1="value1" attribute2="value2"
... </NAME>,
where "Named Entity" is one of the names for the
18 BioCaster NEs and attribute1, attribute2,
... are the names of the NE?s Role/Quality at-
tributes, "value1", "value2", ... are values cor-
responding to Role/Quality attributes. Further de-
tails of the annotation guidelines are discussed in
(Kawazoe et al, 2006).
3 BioCaster Gold Standard Data Corpus
The BioCaster gold standard corpus was collected
from Internet news and manually annotated by two
doctoral students. The annotation of a news article
proceeded as follows. Firstly, NEs are annotated fol-
lowing the BioCaster schema and guidelines. Sec-
ondly, each annotated article is manually assigned
into one of four relevancy categories: alert, publish,
check, and reject. The assignment is based on guide-
lines that we made following discussions with epi-
demiologists and a survey of World Health Organi-
zation (WHO) reports (World Health Organization,
2004). These categories are currently being used op-
erationally by the GPHIN system which is used by
the WHO and other public health agencies. Where
there were major differences of opinion in NE anno-
tation or relevancy assignment between the two an-
notators, we consulted a public health expert in order
to decide the most appropriate assignment. Finally
we had a total of 500 articles that were fully anno-
tated. While this is small compared to other data
sets in text classification, we consider that it is large
enough to obtain a preliminary indication about the
usefulness of Role attributes.
The following is an example of an annotated arti-
cle in the BioCaster gold standard corpus.
Example.
<DOC id="000125" language="en-us"
source="WHO" domain="health"
subdomain="disease"
date published="2005-03-17"
relevancy="alert"> <NAME cl="DISEASE">
Acute fever </NAME> and <NAME
cl="DISEASE"> rash syndrome </NAME> in
<NAME cl="LOCATION">Nigeria</NAME> <NAME
cl="TIME"> 17 March 2005 </NAME><NAME
cl="ORGANIZATION"> WHO</NAME> has received
reports of <NAME cl="PERSON" case="true"
number="many"> 1118 cases </NAME>
including <NAME cl="PERSON" case="true"
number="many">76 deaths</NAME>case
fatality rate, 6.8% reported in 12
Local Government Areas (LGAs) of <NAME
cl="LOCATION">damawa </NAME> state, <NAME
cl="LOCATION"> Nigeria</NAME> as of <NAME
cl="TIME">28 February 2005</NAME>. The
cases have been clinically diagnosed
as <NAME cl="DISEASE"> measles </NAME>
but no laboratory diagnosis has been
made to date. Other states, including
<NAME cl="LOCATION">Gombe</NAME>,
<NAME cl="LOCATION">Jigawa</NAME>,<NAME
cl="LOCATION">Kaduna</NAME>, <NAME
cl="LOCATION">Kano</NAME>, and <NAME
cl="LOCATION">Kebbi</NAME> have all
reported <NAME cl="OUTBREAK"> outbreaks
</NAME> of <NAME cl="DISEASE"> measles
</NAME>... </DOC>
We grouped the 500 articles into 2 categories: re-
ject and relevant. The reject category corresponds
simply to articles with label reject while the relevant
category includes articles with labels alert, pub-
lish, and check. We conflated the alert, publish and
check categories because we hypothesized that dis-
tinguishing between non-reject (relevant) categories
19
Named entity Role/Quality attributes Named entity Role/Quality attributes
PERSON case, number ANATOMY transmission
ORGANIZATION none SYMPTOM none
LOCATION none CONTROL none
TIME none CHEMICAL therapeutic
DISEASE none BACTERIA none
CONDITION none PRODUCT transmission, therapeutic
NONHUMAN transmission DNA none
VIRUS none RNA none
OUTBREAK none PROTEIN none
Table 1: Lists of Named entity classes and their Role/Quality attributes in BioCaster annotation schema.
would require higher level semantic knowledge such
as pathogen infectivity and previous occurrence his-
tory which is the job of the text mining system and
the end user. Finally we had a total of 269 news
articles belong to the reject category and 231 news
articles belong to the relevant category. The statis-
tical information about NEs is shown in Table 2. In
the table, ?+? stands for the frequency of NEs in the
relevant category and ?-? stands for the frequency of
NEs in the reject category.
4 Experiments
4.1 Method
We used the BioCaster gold standard corpus to in-
vestigate the effect of NE classes and their Role at-
tributes on performance of classification. In order
to avoid unnecessary data, we removed the first line
containing DOC tag of all article in the corpus. The
validation is as follows. We randomly divided the
data set into 10 parts. Each of the first 9 parts has 23
articles belonging to the relevant category and 27 ar-
ticles belonging to the reject category; the 10th part
has 24 articles belonging to the relevant and 26 arti-
cles belonging to the reject categories. Then, we im-
plemented 10-fold cross validation: 9 parts for train-
ing and 1 part for testing sets. For the training set we
extracted NEs classes and their Roles as features to
build a classifier. The remaining part was used for
testing.
The classifier we use in this paper is the standard
Na?ive Bayes classifier (Mitchell, 1997). In the pre-
processing we did not use a stop list and no word
stemming. The experiments were implemented in
Linux OS, using the Bow toolkit (McCallum, 1996).
The details of extracting NEs and their Roles
from annotated texts are the followings. For the
sake of convenience, we divided features into 3
groups: Features for each NE, features for NEs with
Role/Quality, and features for combined NEs with
Role/Quality.
1. Features for each NE: Each NE is extracted and
used with raw text as features. We denoted NE1
as features extracted from named entity NE1.
For example, DISEASE1 means features are
raw text and DISEASE class, VIRUS1 means
features are raw text and VIRUS class. An ex-
ample of features for PERSON1 is shown in
Table 3.
2. Features for NEs with Role/Quality: We inves-
tigated the effect of NEs with Roles/Qualities,
i.e., case, number, therapeutic, and transmis-
sion. Features are chosen as follows.
- PERSON+case+number: Raw text and
PERSON class with both Role case and
Quality number are used as features.
- PERSON+case: Raw text and PERSON
class with Role case are used as features.
- PERSON+number: Raw text and PER-
SON class and Quality number are used
as features.
- NONHUMAN+trans: Raw text and
NONHUMAN class and Role transmis-
sion are used as features.
- ANATOMY+trans: Raw text and
ANATOMY class and Role transmission
are used as features.
20
NE class Frequency Total NE class Frequency Total
PERSON +3291/-4978 8269 ANATOMY +263/-224 487
ORGANIZATION +1405/-3460 4865 SYMPTOM +293/-105 398
LOCATION +2432/-2409 4841 CONTROL +282/-87 369
TIME +1159/-1518 2677 CHEMICAL +108/-185 293
DISEASE +1164/-456 1620 BACTERIA +136/-103 239
CONDITION +689/-206 895 PRODUCT +124/-74 198
NONHUMAN +393/-344 737 DNA +8/-55 63
VIRUS +428/-127 555 RNA +0/-55 55
OUTBREAK +460/-75 535 PROTEIN +5/-32 37
Table 2: The frequency of NE classes in the BioCaster gold standard corpus, ?+? denotes the frequency in
the relevant category and ?-? denotes the frequency in the reject category.
Example of <NAME cl="ORGANIZATION"> WHO</NAME> has
annotated text received reports of <NAME cl="PERSON" case="true"
number="many"> 1118 cases </NAME>
Text only ?WHO?, ?has?, ?received?, ?reports?, ?of?, ?1118?, ?cases?
PERSON1 ?WHO?, ?has?, ?received?, ?reports?, ?of?, ?1118?, ?cases?, ?PERSON?
PERSON+case+number ?WHO?, ?has?, ?received?, ?reports?, ?of?, ?1118?, ?cases?, ?PERSON?,
?case?, ?number?
PERSON+case ?WHO?, ?has?, ?received?, ?reports?, ?of?, ?1118?, ?cases?, ?PERSON?,
?case?
PERSON+number ?WHO?, ?has?, ?received?, ?reports?, ?of?, ?1118?, ?cases?, ?PERSON?,
?number?
Table 3: An example of using different features for PERSON class as training data.
- PRODUCT+trans+thera: Raw text and
PRODUCT class and both Roles transmis-
sion and therapeutic are used as features.
- PRODUCT+trans: Raw text and PROD-
UCT class and Role transmission are used
as features.
- PRODUCT+thera: Raw text and PROD-
UCT class and Role therapeutic are used
as features.
- CHEMICAL+thera: Raw text and
CHEMICAL class and Role therapeutic
are used as features.
3. Features for combined NEs with Roles. We
investigate features for disease-related NEs
which include DISEASE, VIRUS, BACTE-
RIA, SYMPTOM, CONDITION, CONTROL,
DNA, PROTEIN, RNA, OUTBREAK, PROD-
UCT, ANATOMY, NONHUMAN, CHEMI-
CAL and features for all NEs with their Roles,
i.e., therapeutic and transmission. We investi-
gated 5 different features as follows:
- Text only: Only raw text is used as fea-
tures.
- Text+DiseaseNEs: Raw text and all 14
NEs disease-related classes are used as
features.
- Text+DiseaseNEs+Roles: Raw text and
all 14 NEs disease-related classes with
Roles are used as features. We note that
there are two Roles therapeutic and trans-
mission in this case.
- Text+AllNEs: Raw text and all NE classes
are used as features.
- Text+AllNEs+Roles: Raw text and all NE
classes with Roles are used as features. In
this case we have all 3 Roles case, thera-
peutic and transmission.
An example of using different features for PER-
21
YES is correct NO is correct
Assigned YES a b
Assigned NO c d
Table 4: A contingency table.
SON class is shown in Table 3.
4.2 Results and Discussions
The details of experimental results are shown in the
following sections. We use two performance mea-
sures, standard Precision/Recall and accuracy. They
are calculated based on the two-way contingency ta-
ble in Table 4. In the table, a counts the assigned
and correct cases, b counts the assigned and incor-
rect cases, c counts the not assigned but incorrect
cases, and d counts the not assigned and correct
cases (Yang, 1999). Then,
Precision = aa + b , and Recall =
a
a + c .
Accuracy is defined as accuracy=(a + d)/(a + b +
c + d).
4.2.1 Effectiveness of Each NE Class
In order to investigate the effect of NEs on per-
formance, we consider the baseline as the method
using text only. In experiment the baseline achieved
a performance of 74.40% accuracy and 64.35% Pre-
cision, 100% Recall. We can see that Recall always
achieves 100% in all cases. This may be due to the
small size of data. However it is interesting that we
can observe the change of Precision measure - an
important measure in our case. Hereafter we discuss
accuracy and Precision only.
The effectiveness of each NE class is shown in Ta-
ble 5. The results show that each NE does not have
the same effect. Compared to the baseline, nearly
half the total NEs (7/18) help improve performance
while the others do not have a significant affect.
Looking at the distribution of NE frequency in Ta-
ble 2, it seems that the higher the frequency of the
NE class, the better the performance it provides. For
example, PERSON achieved the best of all (76.80%
accuracy, 66.57% Precision compared to 74.40% ac-
curacy and 64.35% Precision when using raw text).
However this trend is not always followed, for ex-
ample, the TIME class tends to reduce performance
when compared to raw text. This is natural as there
is no obvious correlation between time and rele-
vancy. From the result tables we can conclude that
the effectiveness of each NE on the performance of
classification in our corpus is decreased in the fol-
lowing order.
PERSON > LOCATION > ORGANIZATION >
DISEASE > CONDITION = VIRUS = OUT-
BREAK > NONHUMAN = ANATOMY = SYMP-
TOM = CONTROL = BACTERIA = PRODUCT =
PROTEIN > CHEMICAL = DNA = RNA > TIME
In particular, 7 NEs, i.e., PERSON, LO-
CATION, ORGANIZATION, DISEASE, CONDI-
TION, VIRUS, OUTBREAK improve performance,
while TIME significantly reduces it. Two NEs DNA
and RNA that have low frequency weakly reduce
performance.
4.2.2 Effectiveness of Roles on Classification
In this Section we investigate the effect of each
Role on performance. The experimental results are
shown in Table 6. We can easily observe that Roles
in NEs improved both the accuracy and Precision
significantly.
We first consider the Role case. This Role is as-
sociated to PERSON which has highest frequency
in the corpus. Role case helped improve the ac-
curacy from 76.8% to 80.60%, and Precision from
66.57% to 74.43% for PERSON. This is significant
when we compare to the baseline with 74.4% ac-
curacy and 64.35% Precision. We note that PER-
SON has another attribute, the Quality number. Role
case helps PERSON with Quality number improve
the accuracy from 78.00% to 81.80% and Precision
from 67.74% to 71.74%. Moreover, we can obvi-
ously draw the relative comparison about effective-
ness between Role case and Quality number from
these results, it yields that case > number.
We proceed to investigate the effect of Roles ther-
apeutic and transmission. Obviously we see that
their effects on performance are positive. Specifi-
cally, transmission help NONHUMAN improve the
accuracy from 74.40% to 74.60%, therapeutic helps
CHEMICAL improve the accuracy from 74.20% to
74.40%. They both have not effects on some mi-
nor NE classes like ANATOMY and PRODUCT. If
we had more training data with more of these mi-
nor NE classes we hope to see a positive effect from
22
Named entity Accuracy Pre/Rec Named entity Accuracy Pre/Rec
PERSON1 76.80 66.57/100 ANATOMY1 74.40 64.35/100
ORGANIZATION1 75.40 65.25/100 SYMPTOM1 74.40 64.35/100
LOCATION1 75.60 65.44/100 CONTROL1 74.40 64.35/100
TIME1 73.00 63.11/100 CHEMICAL1 74.20 64.17/100
DISEASE1 75.00 64.89/100 BACTERIA1 74.40 64.35/100
CONDITION1 74.60 64.53/100 PRODUCT1 74.40 64.35/100
NONHUMAN1 74.40 64.35/100 DNA1 74.20 64.17/100
VIRUS1 74.60 64.53/100 RNA1 74.20 64.17/100
OUTBREAK1 74.60 64.53/100 PROTEIN1 74.40 64.35/100
Table 5: Performance of each NE class in which features of NEs in bold text have Role attributes.
FEATURES Accuracy Pre/Rec
Baseline 74.40 64.35/100
PERSON1 76.80 66.57/100
PERSON+number 78.00 67.74/100
PERSON+case 80.60 74.43/100
PERSON+case+number 81.80 71.74/100
NONHUMAN1 74.40 64.35/100
NONHUMAN+trans 74.60 64.53/100
ANATOMY1 74.40 64.35/100
ANATOMY+trans 74.40 64.35/100
PRODUCT1 74.40 64.35/100
PRODUCT+trans 74.40 64.35/100
PRODUCT+therapeutic 74.40 64.35/100
PRODUCT+trans+thera 74.40 64.35/100
CHEMICAL1 74.20 64.17/100
CHEMICAL+therapeutic 74.40 64.35/100
Table 6: Performance of Role attributes with their
NEs.
Roles on them. Interestingly, while NEs associated
to Roles do not improve the accuracy like NONHU-
MAN and CHEMICAL, their Roles helped improve
the accuracy. Based on the improvements of trans-
mission and therapeutic in Table 6, we can draw
their effectiveness are the same on their NEs, that
is therapeutic = transmission.
When we compare the effect of all Roles on per-
formance, we can see that the improvements of Role
case and also Quality number are much higher than
the improvements of Roles therapeutic and trans-
mission. We think this is because the frequency of
PERSON (NE associated to Role case and Quality
number) is higher than the frequency of NEs which
FEATURES Accuracy Pre/Rec
Baseline 74.40 64.35/100
Text+DiseaseNEs 75.80 65.63/100
Text+DiseaseNEs+Roles 76.20 66.00/100
Text+AllNEs 79.40 69.16/100
Text+AllNEs+Roles 84.40 74.76/100
Table 7: The performance of combined NEs with
their Roles.
are associated to Roles therapeutic and transmis-
sion in the corpus. Then, we can have the effect
of Roles/Qualities is in the order case > number >
therapeutic = transmission.
4.2.3 Effectiveness of Combined NEs with
Roles
We continue to investigate the effectiveness of
Roles for combined NEs. The experimental re-
sults are given in Table 7. We note that there are
two Roles therapeutic and transmission in disease-
related NE classes, and all 3 Roles case, therapeutic
and transmission in all NE classes.
We can easily see that Roles improved perfor-
mance of text classification significantly. In de-
tails, for disease-related NE classes, Roles thera-
peutic and transmission helped to improve the ac-
curacy from 74.40% to 76.20%, and Precision from
64.35% to 66.% compared to the baseline. For all
NE classes, all 3 Roles case, therapeutic, and trans-
mission help to improve the accuracy from 74.40%
to 84.40% and Precision from 64.35% to 74.76%.
We conclude that all 3 Roles achieved the best re-
sults in performance.
23
5 Conclusion
This paper has focused on the contribution of Roles
in biomedical annotated text classification. The ex-
perimental results indicated that:
1. Roles of each NE greatly help improve perfor-
mance of the system.
2. The effect of Role/Quality attributes on classi-
fication was decreased in the order as follows:
case > number > therapeutic = transmission.
3. Combined NE classes with Roles contribute
significantly to the improvement of perfor-
mance.
Acknowledgments
The authors wish to thank Mika Shigematsu and
Kiyosu Taniguchi at the National Institute of Infec-
tious Diseases for useful discussions. This work was
supported by Grants-in-Aid from the Japan Society
for the Promotion of Science (grant no. 18049071).
References
S. Bloehdorn and A. Hotho. 2004. Boosting for text
classification with semantic features. In Proc. of the
Workshop on Mining for and from the Semantic Web
at the 10th ACM SIGKDD 2004, pages 70?87.
A.M. Cohen and W.R. Hersh. 2005. A survey of current
work in biomedical text mining. Briefing in BioInfor-
matics, 6(3):57?71.
N. Collier. 2006. BioCaster text mining project. http:
//biocaster.nii.ac.jp.
J. Fru?rnkranz, T. Mitchell, and E. Riloff. 1998. A case
study in using linguistic phrases for text categorization
on the WWW. In Working Notes of the AAAI/ICML
Workshop on Learning for Text Categorization, pages
5?13.
N. Guarino and C. Welty. 2000a. A formal ontology of
properties. In Proceedings of the 2000 Conference on
Knowledge Engineering and Knowledge Management
(EKAW-2000), pages 97?112.
N. Guarino and C. Welty. 2000b. Ontological analysis
of taxonomic relations. In Proceedings of the Inter-
national Conference on Conceptual Modeling, pages
210?224.
A. Hotho, S. Staab, and G. Stumme. 2003. WordNet im-
proves text document clustering. In Proc. of the SIGIR
2003 Semantic Web Workshop, 2003.
International Society for Infectious Diseases. 2001.
Promed mail. http://www.promedmail.org.
A. Kawazoe, L. Jin, M. Shigematsu, R. Barrero, K.
Taniguchi, and N. Collier. 2006. The development of
a schema for the annotation of terms in the BioCaster
disease detection/tracking system. In Proceedings of
the International Workshop on Biomedical Ontology
in Action (KR-MED 2006), pages 77?85.
M. Krauthammer and G. Nenadic. 2004. Term identifi-
cation in the biomedical literature. Journal of Biomed-
ical Informatics, 37(6):512?526.
T. Kudo and Y. Matsumoto. 2004. A boosting algorithm
for classification of semi-structured text. In Proceed-
ings of the 2004 Conference on Empirical Methods in
NLP, pages 301?308.
A.K. McCallum. 1996. Bow: A toolkit for sta-
tistical language modeling, text retrieval, classifica-
tion and clustering. http://www.cs.cmu.edu/
?mccallum/bow.
T.M. Mitchell. 1997. Machine Learning. McGraw-Hill.
Public Health Agency of Canada. 2004. Global Pub-
lic Heath Intelligence Network (GPHIN). http:
//www.gphin.org.
S. Scott and S. Matwin. 1999. Feature engineering for
text classification. In Proc. of International Confer-
ence on Machine Learning 1999, pages 379?388.
F. Sebastiani. 2002. Machine learning in automated text
categorization. ACM computing survey, 34(1):1?47.
W. J. Wilbur, A. Rzhetsky, and H. Shatkay. 2006. New
directions in biomedical text annotation: definition,
guidelines and corpus construction. BMC Bioinfor-
matics, 7(356):1471?2105.
World Health Organization. 2004. ICD10, Interna-
tional Statistical Classification of Diseases and Related
Health Problems, Tenth Revision.
Y. Yang and X. Liu. 1999. A re-examination of text
categorization methods. In Proc. of 22th ACM Int?l.
Conf. on Research and Development in Information
Retrieval, pages 42?49.
Y. Yang. 1999. An evaluation of statistical approaches
to text categorization. Information Retrieval Journal,
1:69?90.
M.J. Zaki and C.C. Aggarwal. 2003. XRules: an effec-
tive structural classifier for XML data. In Proceedings
of the ninth ACM SIGKDD International Conference,
2003, pages 316?325.
24
Proceedings of the Workshop on BioNLP, pages 142?143,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Using Hedges to Enhance a Disease Outbreak Report Text Mining System
Mike Conway, Nigel Collier
National Institute of Informatics
2-1-2 Hitotsubashi, Chiyoda-ku
Tokyo 101-8430, Japan
{mike|collier}@nii.ac.jp
Son Doan
Vanderbilt University Medical Center
2525 West End Ave., Suite 800
Nashville, TN 37235, USA
son.doan@vanderbilt.edu
1 Introduction
Identifying serious infectious disease outbreaks in
their early stages is an important task, both for na-
tional governments and international organizations
like the World Health Organization. Text mining
and information extraction systems can provide an
important, low cost and timely early warning sys-
tem in these circumstances by identifying the first
signs of an outbreak automatically from online tex-
tual news. One interesting characteristic of disease
outbreak reports ? which to the best of our knowl-
edge has not been studied before ? is their use of
speculative language (hedging) to describe uncertain
situations. This paper describes two uses of hedging
to enhance the BioCaster disease outbreak report
text mining system.
Following a brief description of the BioCaster
system and corpus (section 2), we discuss in section
3 previous uses of hedging in NLP and the meth-
ods used to identify hedges in the current work. In
section 4 we describe some initial classification ex-
periments using hedge features. Section 5 describes
a ?speculative? method of tagging disease outbreak
reports with a metric designed to aid users of the
BioCaster system in identifying articles of inter-
est.
2 BioCaster System & Corpus
The BioCaster system scans online news reports
for stories concerning infectious disease outbreaks
(e.g. H5N1, Ebola) and makes its results available to
registered users as email alerts (Collier et al, 2008).
In addition to this email service, data that has been
filtered through a topic classifier but which is still
uninterpreted is used to populate a Google Map ap-
plication called the Global Health Monitor.1
The BioCaster corpus consists of 1000 news
articles downloaded from the WWW and then man-
ually categorized and annotated with Named Entities
by two PhD students. Articles were collected from
various news sources (e.g. BBC, New York Times
and ProMED-Mail2). Each document is classified
as either relevant (350) or reject (650).3
The corpus is designed to include difficult border-
line cases where more advanced understanding of
the context is required. For example, an article may
be about, say, polio, but not centrally concerned with
specific outbreaks of that disease. Instead, the arti-
cle could report a vaccination campaign or research
breakthrough.
3 Hedges
According to Hyland (1998), in an extensive study
of speculative language in science writing, hedges
?are the means by which writers can present a propo-
sition as an opinion rather than a fact.? More re-
cently, Kilicoglu and Bergler (2008) have presented
a method for automatically identifying hedges in the
biomedical domain. In the current work, we used a
science orientated hedge lexicon derived from Mer-
cer et al (2004). The lexicon consisted of 72 verbs
(including appear, appears, appeared, appearing,
indicate, indicates, indicated, indicating, and so on)
and 32 non-verbs (including, about, quite, poten-
1www.biocaster.org
2ProMED-Mail is a human curated service for monitoring
disease outbreak reports (www.promedmail.org.)
3For copyright reasons, the BioCaster corpus is not pub-
licly available.
142
Rank Hedge Rank Hedge
1 reported 9 suggests
2 suspected 10 estimated
3 probable 11 appeared
4 suspect 12 appearing
5 usually 13 mostly
6 see 14 assumes
7 reports 15 predicted
8 sought 16 suggested
Table 1: Statistically Significant Hedges
Features Naive Bayes SVMAcc F Acc F
9000 ?2 94.8 0.93 92.2 0.89
Unigram 88.4 0.85 90.9 0.87
Unigram+hedge 88.0 0.85 91.7 0.89
Table 2: Classification Results
tially, likely and so on). Preliminary work showed
that the frequency of hedge words differs in the two
categories of the BioCaster corpus (relevant and
reject) at a highly significant level using the ?2 test
(P < 0.01). Table 1 shows the 16 most discriminat-
ing hedge words in the BioCaster corpus (identi-
fied using the ?2 feature selection method.)
4 Classification Experiment
The current BioCaster system uses n-gram based
text classification to identify disease outbreak re-
ports, and reject other online news. We used hedg-
ing features to augment this classifier, and evaluated
the results using a subset of the BioCaster cor-
pus. One binary hedging feature was used. The fea-
ture was ?true? if and only if one of the 105 hedge
lexemes identified by Mercer et al (2004) occurred
in the input document within 5 words of a disease
named entity. Results are shown in Table 2, where it
can be seen that the addition of a single binary hedge
feature to the unigram feature set increases accuracy
by 0.8%. The performance does not however reach
the level achieved by the ?2 9000 n-gram feature set
described in Conway et al (2008).
5 Towards a ?Speculative? Metric
Users of the BioCaster system would benefit
from an indicator of how ?speculative? each news
article is, as breaking news regarding disease out-
breaks is characterized by uncertainty, which is en-
coded using hedging. We use the Mercer list of 105
hedging words as described above, in conjunction
with statistics derived from a 10,000 document sec-
Accept (%) Reject (%)
High 64.2 48.3
Medium 29.5 36.7
Low 6.3 15.0
Table 3: Proportion of Articles in Each Category
tion of the Reuters corpus to provide a ?speculative?
metric.4 We calculated total frequencies for all 105
hedge words in each of the 10,000 Reuters docu-
ments ? that is, the total number of hedge words
per document ? then ranked these frequencies (af-
ter normalizing the frequencies to take account of
document length). The bottom third of documents
had hedge percentages in the range 0% - 0.2544%
(LOW). The middle third had hedge percentages in
the range 0.2545% - 1.0574 (MEDIUM). The range
for the top third was 1.0575% - 100% (HIGH). Doc-
uments inputted to the BioCaster system auto-
matically have their proportion of hedge words cal-
culated and are assigned a value according to their
position on the scale (LOW, MEDIUM or HIGH). Ta-
ble 3 shows that a majority of the documents in the
accept segment of the BioCaster corpus can be
tagged as highly speculative using this method.
References
N. Collier, S. Doan, A. Kawazoe, R. Matsuda-Goodwin,
M. Conway, Y. Tateno, Q-H. Ngo, D. Dien, A. Kaw-
trakul, K. Takeuchi, M. Shigematsu, and K. Tanigu-
ichi. 2008. BioCaster: Detecting Public Health Ru-
mors with a Web-based Text Mining System. Bioin-
formatics, 24(24):2940?2941.
M. Conway, S. Doan, A. Kawazoe, and N. Collier.
2008. Classifying Disease Outbreak Reports Using
N-grams and Semantic Features. Proceedings of the
Third International Symposium on Semantic Mining in
Biomedicine (SMBM 2008), Turku, Finland, pages 29?
36.
K. Hyland. 1998. Hedging in Scientific Research Articles.
John Benjamins, Amsterdam.
H. Kilicoglu and S. Bergler. 2008. Recognizing Spec-
ulative Language in Biomedical Research Articles: a
Linguistically Motivated Perspective. BMC Bioinfor-
matics, 9(Suppl 11):S10.
R. Mercer, C. DiMarco, and F. Kroon. 2004. The Fre-
quency of Hedging Cues in Citation Contexts in Sci-
entific Writing. In Proceedings of the Canadian Con-
ference on AI, pages 75?88.
4Reuters Corpus, Volume 1, English language, 1996-08-20
to 1997-08-19 (Release date 2000-11-03, Format version 1, cor-
rection level 0).
143
Introduction to the Bio-Entity Recognition Task at JNLPBA
Jin-Dong KIM, Tomoko OHTA, Yoshimasa TSURUOKA, Yuka TATEISI
CREST, Japan Science and Technology Agency, and
Department of Computer Science, University of Tokyo,
7-3-1 Hongo, Bunkyo-ku, Tokyo 113-0033, Japan?
Nigel COLLIER
National Institute of Informatics,
2-1-2 Hitotsubashi, Chiyoda-ku, Tokyo 101-8430, Japan?
Abstract
We describe here the JNLPBA shared task of
bio-entity recognition using an extended version
of the GENIA version 3 named entity corpus of
MEDLINE abstracts. We provide background
information on the task and present a general
discussion of the approaches taken by partici-
pating systems.
1 Introduction
Bio-entity recognition aims to identify and clas-
sify technical terms in the domain of molecu-
lar biology that correspond to instances of con-
cepts that are of interest to biologists. Exam-
ples of such entities include the names of pro-
teins, genes and their locations of activity such
as cells or organism names as shown in Figure 1.
Entity recognition is a core component tech-
nology in several higher level information access
tasks such as information extraction (template
filling), summarization and question answering.
These tasks aim to help users find structure in
unstructured text data and aid in finding rele-
vant factual information. This is becoming in-
creasingly important with the massive increase
in reported results due to high throughput ex-
perimental methods.
Bio-entity recognition by computers remains
a significantly challenging task. Despite good
progress in newswire entity recognition (e.g.
(MUC, 1995; Tjong Kim Sang and De Meul-
der, 2003)) that has led to ?near human? levels
of performance, measured in the high 90s for F-
score (van Rijsbergen, 1979), similar methods
have not performed so well in the bio-domain
leaving an accuracy gap of some 30 points of F-
score. Challenges occur for example due to am-
biguity in the left boundary of entities caused
by descriptive naming, shortened forms due to
abbreviation and aliasing, the difficulty of creat-
? {jdkim,yucca,okap,tsuruoka}@is.s.u-tokyo.ac.jp
? collier@nii.ac.jp
We have shown that <cons
sem=?G#protein?>interleukin-1</cons>
(<cons sem=?G#protein?>IL-1</cons>)
and <cons sem=?G#protein?>IL-2</cons>
control <cons sem=?G#DNA?>IL-2 receptor
alpha (IL-2R alpha) gene</cons> transcription
in <cons sem=?G#cell line?>CD4-CD8-
murine T lymphocyte precursors</cons>.
Figure 1: Example MEDLINE sentence marked
up in XML for molecular biology named-
entities.
ing consistently annotated human training data
with a large number of classes, etc. In or-
der to make progress it is becoming clear that
several points need to be considered: (1) ex-
tension of feature sets beyond the lexical level
(part of speech, orthography etc.) and use of
higher-levels of linguistic knowledge such as de-
pendency relations, (2) potential for re-use of
external domain knowledge resources such as
gazetteers and ontologies, (3) improved quality
control methods for building annotation collec-
tions, (4) fine grained error analysis beyond the
F-score statistics.
The JNLPBA shared task 1 is an open chal-
lenge task and as such we allowed participants
to use whatever methodology and knowledge
sources they liked in the bio-entity task. The
systems were evaluated on a common bench-
mark data set using a common evaluation
method. Although it is not directly possible
to compare systems due to the diversity of re-
sources used the F-score results provide an ap-
proximate indication of how useful each method
is.
2 Data
The training data used in the task came from
the GENIA version 3.02 corpus (Kim et al,
1http://research.nii.ac.jp/
?collier/workshops/JNLPBA04st.htm
70
2003). This was formed from a controlled search
on MEDLINE using the MeSH terms ?human?,
?blood cells? and ?transcription factors?. From
this search 2,000 abstracts were selected and
hand annotated according to a small taxon-
omy of 48 classes based on a chemical classi-
fication. Among the classes, 36 terminal classes
were used to annotate the GENIA corpus.
The GENIA corpus is important for two ma-
jor reasons: the first is that it provides the
largest single source of annotated training data
for the NE task in molecular biology and the
second is in the breadth of classification. Al-
though 36 classes is a fraction of the classes con-
tained in major taxonomies it is still the largest
class set that has been attempted so far for the
NE task. In this respect it is an important test
of the limits of human and machine annotation
capability. For the shared task we decided how-
ever to simplify the 36 classes and used only the
classes protein, DNA, RNA, cell line and cell
type. The first three incorporate several sub-
classes from the original taxonomy while the
last two are interesting in order to make the
task realistic for post-processing by a potential
template filling application.
For testing purposes we used a newly anno-
tated collection of MEDLINE abstracts from
the GENIA project. 404 abstracts were used
that were annotated for the same classes of en-
tities: Half of them were from the same domain
as the training data and the other half of them
were from the super-domain of ?blood cells? and
?transcription factors?. Our hope was that this
should provide an important test of generaliz-
ability in the methods used.
3 Evaluation
The 2,000 abstracts of the GENIA corpus ver-
sion 3.02 which had already been made publicly
available were formatted for IOB2 notation and
made available as training materials. For test-
ing, additional 404 abstracts were randomly se-
lected from an unpublished set of the GENIA
corpus and the annotations were re-checked by a
biologist. The training set consists of abstracts
retrieved from the MEDLINE database with
MeSH terms ?human?, ?blood cells? and ?tran-
scription factors?, and their publication year
ranges over 1990?1999. Most parts of the test
set include abstracts retrieved with the same
set of MeSH terms, and their publication year
ranges over 1978?2001. To see the effect of pub-
lication year, the test set was roughly divided
into four subsets: 1978-1989 set (which rep-
resents an old age from the viewpoint of the
models that will be trained using the training
set), 1990-1999 set (which represents the same
age as the training set), 2000-2001 set (which
represents a new age compared to the training
set) and S/1998-2001 set (which represents
roughly a new age in a super domain). The last
subset represents a super domain and the ab-
stracts was retrieved with MeSH terms, ?blood
cells? and ?transcription factors? (without ?hu-
man?)2. Table 1 illustrates the size of the data
sets
Table 2 shows the number of entities anno-
tated in each data set3. As seen in the ta-
ble, the annotation density of proteins increases
over the ages significantly, whereas the anno-
tation density of DNAs and RNAs increases in
the 1990-1999 set and slightly decreases in the
2000-2001 set. This tendency roughly corre-
sponds to the expansion in the subject area as a
whole that can be estimated from statistics on
the MeSH terms introduced in each age shown
in Table 3. This observation suggests that the
density of mention of a class of entities in aca-
demic papers is affected by the amount of inter-
est the entity receives in each age.
Figure 2 shows the ratio of annotated struc-
tures in each set. In accordance with our expec-
tation, the 1990-1999 set has the most simi-
lar annotation trait with the training set. The
2000-2001 set is also similar to the training
set, but the 1978-1989 set had quite a differ-
ent distribution of entity classes. The variation
of domain does not seem to make any signif-
icant difference to the distribution of entities
mentioned. One reason may be the large frac-
tion of abstracts from the same domain in the
super domain set. In fact, among 206 abstracts
in the super domain set, 140 abstracts (69%)
are also from the same domain. It also corre-
sponds to the fraction in the whole MEDLINE
database: among 9,362 abstracts that can be
retrieved with MeSH terms, ?blood cells? and
?transcription factors?, 6,297 abstracts (67%)
can also be retrieved with MeSH terms ?human?,
?blood cells? and ?transcription factors?.
To simplify the annotation task to a simple
linear sequential analysis problem, embedded
structures have been removed leaving only the
2The S/1998-2001 set includes the whole 2000-
2001 set.
3The figures in the parenthesis are the average num-
ber of entities per an abstract in each set.
71
# abs # sentences # words
Training Set 2,000 20,546 (10.27/abs) 472,006 (236.00/abs) (22.97/sen)
Test Set 404 4,260 (10.54/abs) 96,780 (239.55/abs) (22.72/sen)
1978-1989 104 991 ( 9.53/abs) 22,320 (214.62/abs) (22.52/sen)
1990-1999 106 1,115 (10.52/abs) 25,080 (236.60/abs) (22.49/sen)
2000-2001 130 1,452 (11.17/abs) 33,380 (256.77/abs) (22.99/sen)
S/1998-2001 206 2,270 (11.02/abs) 51,957 (252.22/abs) (22.89/sen)
Table 1: Basic statistics for the data sets
protein DNA RNA cell type cell line ALL
Training Set 30,269 (15.1) 9,533 (4.8) 951 (0.5) 6,718 (3.4) 3,830 (1.9) 51,301 (25.7)
Test Set 5,067 (12.5) 1,056 (2.6) 118 (0.3) 1,921 (4.8) 500 (1.2) 8,662 (21.4)
1978-1989 609 ( 5.9) 112 (1.1) 1 (0.0) 392 (3.8) 176 (1.7) 1,290 (12.4)
1990-1999 1,420 (13.4) 385 (3.6) 49 (0.5) 459 (4.3) 168 (1.6) 2,481 (23.4)
2000-2001 2,180 (16.8) 411 (3.2) 52 (0.4) 714 (5.5) 144 (1.1) 3,501 (26.9)
S/1998-2001 3,186 (15.5) 588 (2.9) 70 (0.3) 1,138 (5.5) 170 (0.8) 5,152 (25.0)
Table 2: Absolute (and relative) frequencies for NEs in each data set. Figures for the test set are
broken down according to the age of the data.
Figure 2: Ratio of annotated NEs
          
 	 
         
       
         
        
    
         
    Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 215?222,
Beijing, August 2010
An ontology-driven system for detecting global health events
Nigel Collier
National Inst. Informatics
collier@nii.ac.jp
Reiko Matsuda Goodwin
Fordham University
reikogoodwin@gmail.com
John McCrae
Bielefeld University
johnmccrae@gmail.com
Son Doan
Vanderbilt University
son.doan@vanderbilt.edu
Ai Kawazoe
Tsuda College
zoeai@tsuda.ac.jp
Mike Conway
University of Pittsburgh
conwaym@pitt.edu
Asanee Kawtrakul
Kasetart University
ak@ku.ac.th
Koichi Takeuchi
Okayama University
koichi@cs.okayama-u.ac.jp
Dinh Dien
VietNam National University
ddien66@yahoo.com
Abstract
Text mining for global health surveillance
is an emerging technology that is gaining
increased attention from public health or-
ganisations and governments. The lack
of multilingual resources such as Word-
Nets specifically targeted at this task have
so far been a major bottleneck. This pa-
per reports on a major upgrade to the
BioCaster Web monitoring system and
its freely available multilingual ontology;
improving its original design and extend-
ing its coverage of diseases from 70 to 336
in 12 languages.
1 Introduction
The number of countries who can sustain teams
of experts for global monitoring of human/animal
health is limited by scarce national budgets.
Whilst some countries have advanced sensor net-
works, the world remains at risk from the health
impacts of infectious diseases and environmen-
tal accidents. As seen by the recent A(H5N1),
A(H1N1) and SARS outbreaks, a problem in one
part of the world can be rapidly exported, leading
to global hardship.
The World Health Organization (WHO) esti-
mates that in the future, between 2 to 7.4 mil-
lion people could be at risk worldwide from a
highly contageous avian flu virus that spreads
rapidly through the international air travel net-
work (WHO, 2005). Pandemics of novel
pathogens have the capacity to overwhelm health-
care systems, leading to widespread morbidity,
mortality and socio-economic disruption (Cox
et al, 2003). Furthermore, outbreaks of live-
stock diseases, such as foot-and-mouth disease or
equine influenza can have a devastating impact on
industry, commerce and human health (Blake et
al., 2003). The challenge is to enhance vigilance
and control the emergence of outbreaks. Whilst
human analysis remains essential to spot complex
relationships, automated analysis has a key role
to play in filtering the vast volume of data in real
time and highlighting unusual trends using reli-
able predictor indicators.
BioCaster (http://born.nii.ac.jp) (Collier et al,
2008) is a Web 2.0 monitoring station for the early
detection of infectious disease events. The sys-
tem exploits a high-throughput semantic process-
ing pipeline, converting unstructured news texts
to structured records, alerting events based on
time-series analysis and then sharing this informa-
tion with users via geolocating maps (Fig. 1(a)),
graphs (Fig. 1(b)) and alerts. Underlying the sys-
tem is a publicly available multilingual applica-
tion ontology. Launched in 2006 (Collier et al,
2006) the BioCaster Ontology (BCO) has been
downloaded by over 70 academic and industrial
groups worldwide. This paper reports on a ma-
jor upgrade to the system and the ontology - ex-
panding the number of languages from 6 to 12,
redefining key relations and extending coverage in
the number of diseases from 70 to 336, including
many veterinary diseases.
215
(a) Bio-geographic map (b) Trend graph analyser
(c) BioCaster processes
Figure 1: (a)BioCaster?s bio-geographic map for a suspected foot-and-mouth outbreak on 22nd March,
2010 with links to the multilingual ontology, NCBI, HighWire, GoPubMed and Google Scholar; (b)
The trends analyser showing aggregated document counts for health events in China between 13nd
March and 12th April, 2010; (c) The system?s pipeline of processes with example semantic markup.
216
2 Background
As the world becomes more interconnected and
urbanized and animal production becomes in-
creasingly intensive, the speed with which epi-
demics spread becomes faster, adding to pressure
on biomedical experts and governments to make
quick decisions. Traditional validation methods
such as field investigations or laboratory analysis
are the mainstay of public health but can require
days or weeks to issue reports. The World Wide
Web with its economical and real time delivery of
information represents a new modality in health
surveillance (Wagner and Johnson, 2006) and has
been shown to be an effective source by the World
Health Organization (WHO) when Public Health
Canada?s GPHIN system detected the SARS out-
break in southern China from news reports dur-
ing November 2002. The recent A(H1N1) ?swine
flu? pandemic highlighted the trend towards agen-
cies using unvalidated sources. The technologi-
cal basis for such systems can be found in sta-
tistical classification approaches and light weight
ontological reasoning. For example, Google Flu
Trends (Ginsberg et al, 2009) is a system that de-
pends almost entirely on automatic statistical clas-
sification of user queries; MedISys-PULS (Yan-
garber et al, 2008), HealthMap (Freifeld et al,
2008) and BioCaster use a mixture of statisti-
cal and ontological classification; and GPHIN
(Mawudeku and Blench, 2006) and Argus (Wil-
son, 2007) rely on a mixture of ontological classi-
fication and manual analysis.
Compared to other similar systems BioCaster
is characterized by its richly featured and pub-
licly downloadable ontology and emphasizes crit-
ical evaluation of its text mining modules. Em-
pirical results have included: topic classification,
named entity recognition, formal concept anal-
ysis and event recognition. In the absence of
a community gold standard, task performance
was assessed on the best available ?silver? stan-
dard - the ProMED-mail network (Madoff and
Woodall, 2005), achieving F-score of 0.63 on 14
disease-country pairs over a 365-day period (Col-
lier, 2010).
Despite initial skepticism within the public
health community, health surveillance systems
based on NLP-supported human analysis of me-
dia reports are becoming firmly established in
Europe, North America and Japan as sources of
health information available to governments and
the public (Hartley et al, 2010). Whilst there is no
substitute for trained human analysts, automated
filtering has helped experts save time by allow-
ing them to sift quickly through massive volumes
of media data. It has also enabled them to sup-
plement traditional sources with a broader base of
information.
In comparison with other areas of biomedical
NLP such as the clinical and genetics? domains, a
relative lack of building block resources may have
hindered the wider participation of NLP groups
in public health applications. It is hoped that the
provision of common resources like the BCO can
help encourage further development and bench-
marking.
3 Method
BioCaster performs analysis of over 9000 news ar-
ticles per day using the NPACI Rocks cluster mid-
dleware (http://www.rockcsclusters.org) on a plat-
form of 48 3.0GHz Xeon cores. Data is ingested
24/7 into a semantic processing pipeline in a short
1 hour cycle from over 1700 public domain RSS
feeds such as Google news, the European Media
Monitor and ProMED-mail. Since 2009, news has
also being gathered under contract from a com-
mercial news aggregation company, providing ac-
cess to over 80,000 sources across the world?s lan-
guages.
The new 2010 version of BioCaster uses ma-
chine translation into English (eleven languages)
to source news stories related to currently oc-
curring infectious and environmental disease out-
breaks in humans, animals and plants.
Access to the site is freely available but lo-
gin registration applies to some functions such as
email alerts. Processing is totally automatic, but
we have the potential within the login system to
enable human moderated alerts which broadcast
to Twitter and RSS.
Below we describe in detail two key aspects of
the system that have been significantly upgraded:
the BCO and the event detection system.
217
3.1 Ontology
3.1.1 Aim
The BioCaster Ontology aims:
? To describe the terms and relations necessary
to detect and risk assess public health events
in the grey literature;
? To bridge the gap between (multilingual)
grey literature and existing standards in
biomedicine;
? To mediate integration of content across lan-
guages;
? To be freely available.
The central knowledge source for BioCaster
is the multilingual ontology containing domain
terms such as diseases, agents, symptoms, syn-
dromes and species as well as domain sensitive
relations such as a disease causing symptoms or
an agent affecting particular host species. This al-
lows the text mining system to have a basic un-
derstanding of the key concepts and relationships
within the domain to fill in gaps not mentioned
explicitly in the news reports. To the best of our
knowledge the BCO is unique as an application
ontology, providing freely available multilingual
support to system developers interested in out-
break surveillance in the language of the open me-
dia.
The BCO however has little to say outside of
its application domain, e.g. in disease-gene in-
teraction or for supporting automatic diagnosis.
As discussed in Grey Cowell and Smith (2010),
there are many other resources available that have
the potential to support applications for infec-
tious disease analysis including controlled vocab-
ularies and ontologies such as the the Unified
Medical Language System (UMLS) (Lindberg et
al., 1993), International Classification of Diseases
(ICD-10) (WHO, 2004), SNOMED CT (Stearns
et al, 2001), Medical Subject Headings (MeSH)
(Lipscomb, 2000) and the Infectious Disease On-
tology (IDO) (Grey Cowell and Smith, 2010). In
(Collier et al, 2006) we discussed how BCO com-
pared to such ontologies so we will focus from
now on the implication of the extensions.
3.1.2 Scope
The new version of the BCO now covers 12 lan-
guages including all the United Nation?s official
languages: Arabic (968 terms), English (4113),
French (1281), Indonesian (1081), Japanese
(2077), Korean (1176), Malaysian (1001), Rus-
sian (1187), Spanish (1171), Thai (1485), Viet-
namese (1297) and Chinese (1142). The multi-
lingual ontology can be used as a direct knowl-
edge source in language-specific text mining mod-
ules, as an indexing resource for searching across
concepts in various languages and as a dictionary
for future translation modules. Currently news in
all 12 languages is available via the Web portal
but news in additional languages such as German,
Italian and Dutch are being added using machine
translation.
3.1.3 Design
Like EuroWordNet (Vossen, 1998), on which
it is loosely based, the BCO adopts a thesaurus-
like structure with synonym sets linking to-
gether terms across languages with similar mean-
ing. Synonym sets are referred to using root
terms. Root terms themselves are fully defined in-
stances that provide bridges to external classifica-
tion schemes and nomenclatures such as ICD10,
MeSH, SNOMED CT and Wikipedia. The central
backbone taxonomy is deliberately shallow and
taken from the ISO?s Suggested Upper Merged
Ontology (Niles and Pease, 2001). To maintain
consistency and computability we kept a single
inheritance structure throughout. 18 core domain
concepts corresponding to named entities in the
text mining system such as DISEASE and SYMP-
TOM were the results of analysis using a formal
theory (Guarino and Welty, 2000).
We have endeavoured to construct definitions
for root terms along Aristotelean principles by
specifying the difference to the parent. For ex-
ample in the case of Eastern encephalitis virus:
Eastern equine encephalitis virus is a
species of virus that belongs to the
genus Alphavirus of the family Togaviri-
dae (order unassigned) of the group
IV ((+)ssRNA) that possesses a positive
single stranded RNA genome. It is the
218
etiological agent of the eastern equine
encephalitis.
We are conscious though that terms used in
the definitions still require more rigorous control
to be considered useful for machine reasoning.
To aid both human and machine analysis root
terms are linked by a rich relational structure
reflecting domain sensitive relations such as
causes(virus,disease), has symptom(disease,
symptom), has associated syndrome(disease,
syndrome), has reservoir(virus, organism).
In such a large undertaking, the order of work
was critical. We proceeded by collecting a list of
notifiable diseases from national health agencies
and then grouped the diseases according to per-
ceived relevance to the International Health Reg-
ulations 2005 (Lawrence and Gostin, 2004). In
this way we covered approximately 200 diseases,
and then explored freely available resources and
the biomedical literature to find academic and lay-
man?s terminology to describe their agents, af-
fected hosts, vector species, symptoms, etc. We
then expanded the coverage to less well known
human diseases, zoonotic diseases, animal dis-
eases and diseases caused by toxic substances
such as sarin, hydrogen sulfide, sulfur dioxide and
ethylene. At regular stages we checked and val-
idated terms against those appearing in the news
media.
As we expanded the number of conditions to in-
clude veterinary diseases we found a major struc-
tural reorganization was needed to support animal
symptoms. For example, a high temperature in
humans would not be the same as one in bovids.
This prompted us in the new version to group dis-
eases and symptoms around major animal familes
and related groups, e.g. high temperature (human)
and high temperature (bovine).
A second issue that we encountered was the
need to restructure the hierarchy under Organi-
cObject which was divided between MicroOrgan-
ism and Animal. The structure of the previous
version meant that the former were doing dou-
ble duty as infecting agents and the later were af-
fected hosts. The MicroOrganism class contained
bacterium, helminth, protozoan, fungus and virus,
which then became the domain in a relation ?x
causes y?. Expansion forced us to accomodate the
fact that some animals such as worms and mites
(e.g. scabies) also infect humans as well as ani-
mals. The result was a restructuring of the organic
classes using the Linnean taxonomy as a guide-
line, although this is probably not free from errors
(e.g. virus is typically not considered to be an or-
ganism).
3.2 Event alerting system
Figure 1(c) shows a schematic of the modular de-
sign used by the BioCaster text mining system.
Following on from machine translation and topic
classification is named entity recognition and tem-
plate recognition which we describe in more detail
below. The final structured event frames include
slot values normalized to ontology root terms for
disease, pathogen (virus or bacterium), country
and province. Additionally we also identify 15 as-
pects of public health events critical to risk assess-
ment such as: spread across international borders,
hospital worker infection, accidental or deliberate
release, food contamination and vaccine contami-
nation.
Latitude and longitude of events down to the
province level are found in two ways: using the
Google API up to a limit of 15000 lookups per
day, and then using lookup on the BCO taxonomy
of 5000 country and province names derived from
open sources such as Wikipedia.
Each hour events are automatically alerted to
a Web portal page by comparing daily aggre-
gated event counts against historical norms (Col-
lier, 2010). Login users can also sign up to receive
emails on specific topics. A topic would normally
specify a disease or syndrome, a country or region
and a specific risk condition.
In order to extract knowledge from docu-
ments, BioCaster maintains a collection of rule
patterns in a regular expression language that
converts surface expressions into structured in-
formation. For example the surface phrase
?man exposes airline passengers to measles?
would be converted into the three templates
?species(human); disease(measles); interna-
tional travel(true)?. Writing patterns to produce
such templates can be very time consuming and
so the BioCaster project has developed its own
219
D3: :- name(disease){ list(@undiagnosed) words(,1) list(@disease) }
S2: :- name(symptom) { list(@severity) list(@symptom)}
CF1: contaminated food(?true?) :- ?caused? ?by? list(@contaminate verbs past)
list(@injested material)
SP4: species(?animal?) :- name(animal,A) words(,3) list(@cull verbs past)
Table 1: Examples of SRL rules for named entity and template recognition. Template rules contain
a label, a head and a body, where the head specifies the template pattern to be output if the body
expression matches. The body can contain word lists, literals, and wild cards. Various conditions can
be placed on each of these such as orthographic matching.
light weight rule language - called the Simple
Rule Language (SRL) and a pattern building inter-
face for maintaining the rule base (McCrae et al,
2009). Both are freely available to the research
community under an open source license. Cur-
rently BioCaster uses approximately 130 rules for
entity recognition, 1000 word lists and 3200 tem-
plate rules (of which half are for location recogni-
tion) to identify events of interest in English. Us-
ing SRL allows us to quickly adapt the system to
newly emerging terminology such as the 11+ des-
ignations given to A(H1N1) during the first stages
of the 2009 pandemic.
The SRL rulebook for BioCaster can recognize
a range of entities related to the task of disease
surveillance such as bacteria, chemicals, diseases,
countries, provinces, cities and major airports.
Many of these classes are recognized using terms
imported from the BCO. The rule book also con-
tains specialised thesauri to recognize subclasses
of entities such as locations of habitation, eater-
ies and medical service centres. Verb lists are
maintained for lexical classes such as detection,
mutation, investigation, causation, contamination,
culling, blaming, and spreading.
Some examples of SRL rules for named entity
recognition are shown in Table 1 and described
below:
Rule D3 in the rulebook tags phrases like ?mys-
tery illness? or ?unknown killer bug? by matching
on strings contained within two wordlists, @un-
diagnosed and @disease, separated by up to one
word.
Rule S2 allows severity indicators such as ?se-
vere? or ?acute? to modify a list of known symp-
toms in order to identify symptom entities.
Rule CF1 is an example of a template rule. If
the body of the rule matches by picking out ex-
pressions such as ?was caused by tainted juice?,
this triggers the head to output an alert for con-
taminated food.
Rule SP4 identifies the victim species as ?ani-
mal? in contexts like ?250 geese were destroyed?.
The rulebook also supports more complex in-
ferences such as the home country of national
public health organizations.
Since BioCaster does not employ systematic
manual checking of its reports, it uses a number of
heuristic filters to increase specificity (the propor-
tion of correctly identified negatives) for reports
that appear on the public Web portal pages. For
example, reports with no identified disease and
country are rejected. Since these heuristics may
reduce sensitivity they are not applied to news that
appears on the user login portal pages.
4 Results and Discussion
Version 3 of the ontology represents a significant
expansion in the coverage of diseases, symptoms
and pathogens on version 2. Table 2 summarizes
the number of root terms for diseases classified by
animal familes.
The thesaurus like structure of the BCO is com-
patible in many respects to the Simple Knowledge
Organization System (SKOS) (Miles et al, 2005).
In order to extend exchange and re-use we have
produced a SKOS version of the BCO which is
available from the BCO site. We have also con-
verted the BCO terms into 12 SRL rule books (1
for each language) for entity tagging. These too
are freely available from the BCO site.
As the ontology expands we will consider
adopting a more detailed typing of diseases such
as hasInfectingPart to indicate the organ affected
220
Species N Example
Avian 22 Fowl pox
Bee 6 Chalk brood
disease
Bovine 24 Bluetongue
Canine 4 Blastomycosis
(Canine)
Caprine 14 Contagious
agalactia
Cervine 2 Chronic wasting
disease
Equine 17 Strangles
Feline 4 Feline AIDS
Fish 2 Viral hemorr
hagic septicemia
Human 216 Scarlet fever
Lagomorph 2 Myxomatosis
Non-human 16 Sylvan
primate yellow fever
Other 2 Crayfish plague
Rodent 8 Colorado tick
fever (Rodent)
Swine 12 Swine erysipelas
Table 2: Major disease groups organized by af-
fected animal family. N represents the number of
root terms.
or hasProtectionMethod to indicate broad classes
of methods used to prevent or treat a condition.
The typology of diseases could also be extended
in a more fine grained manner to logically group
conditions, e.g. West Nile virus encephalitis,
Powassan encephalitis and the Japanese B en-
cephalitis could be connected through a hasType
relation on encephalitis.
5 Conclusion
Multilingual resources specifically targeted at the
task of global health surveillance have so far been
very rare. We hope that the release of version 3
can be used to support a range of applications such
as text classification, cross language search, ma-
chine translation, query expansion and so on.
The BCO has been constructed to provide core
vocabulary and knowledge support to the Bio-
Caster project but it has also been influential
in the construction of other public health ori-
ented application ontologies such as the Syn-
dromic Surveillance Ontology (Okhamatovskaia
et al, 2009). The BCO is freely available from
http://code.google.com/p/biocaster-ontology/ un-
der a Creative Commons license.
Acknowledgements
The authors greatly acknowledge the many co-
workers who have provided comments and feed-
back on BioCaster. Funding support was pro-
vided in part by the Japan Science and Technology
Agency under the PRESTO programme.
References
Blake, A., M. T. Sinclair, and G. Sugiyarto. 2003.
Quantifying the impact of foot and mouth disease on
tourism and the UK economy. Tourism Economics,
9(4):449?465.
Collier, N., A. Kawazoe, L. Jin, M. Shigematsu,
D. Dien, R. Barrero, K. Takeuchi, and A. Kaw-
trakul. 2006. A multilingual ontology for infectious
disease surveillance: rationale, design and chal-
lenges. Language Resources and Evaluation, 40(3?
4). DOI: 10.1007/s10579-007-9019-7.
Collier, N., S. Doan, A. Kawazoe, R. Matsuda Good-
win, M. Conway, Y. Tateno, Q. Ngo, D. Dien,
A. Kawtrakul, K. Takeuchi, M. Shigematsu, and
K. Taniguchi. 2008. BioCaster:detecting public
health rumors with a web-based text mining sys-
tem. Bioinformatics, 24(24):2940?1, December.
doi:10.1093/bioinformatics/btn534.
Collier, N. 2010. What?s unusual in online dis-
ease outbreak news? Biomedical Semantics, 1(1),
March. doi:10.1186/2041-1480-1-2.
Cox, N., S. Temblyn, and T. Tam. 2003. Influenza
pandemic planning. Vaccine, 21(16):1801?1803.
Freifeld, C., K. Mandl, B. Reis, and J. Brownstein.
2008. Healthmap: global infectious disease mon-
itoring through automated classification and visual-
ization of internet media reports. J. American Med-
ical Informatics Association, 15:150?157.
Ginsberg, J., M. Mohebbi, R. Patel, L. Brammer,
M. Smolinski, and L. Brilliant. 2009. Detecting
influenza epidemics using search engine query data.
Nature, 457:1012?1014.
Grey Cowell, L. and B. Smith. 2010. Infectious dis-
ease informatics. In Sintchenko, V., editor, Infec-
tious Disease Informatics, pages 373?395. Springer
New York.
221
Guarino, N. and C. Welty. 2000. A formal ontology
of properties. In Dieng, R. and O. Corby, editors,
EKAW-2000: Proc. 12th Int. Conf. on Knowledge
Engineering and Knowledge Management, pages
97?112.
Hartley, D., N. Nelson, R. Walters, R. Arthur, R. Yan-
garber, L. Madoff, J. Linge, A. Mawudeku, N. Col-
lier, J. Brownstein, G. Thinus, and N. Lightfoot.
2010. The landscape of international biosurveil-
lance. Emerging Health Threats J., 3(e3), January.
doi:10.1093/bioinformatics/btn534.
Lawrence, O. and J. Gostin. 2004. International
infectious disease law - revision of the World
Health Organization?s international health regula-
tions. J. American Medical Informatics Associa-
tion, 291(21):2623?2627.
Lindberg, Donald A.B., L. Humphreys, Betsy, and
T. McCray, Alexa. 1993. The unified medical lan-
guage system. Methods of Information in Medicine,
32:281?291.
Lipscomb, C. 2000. Medical subject headings
(MeSH). Bulletin of the Medical Library Assoca-
tion, 88:256?266.
Madoff, Lawrence C. and John P. Woodall. 2005. The
internet and the global monitoring of emerging dis-
eases: Lessons from the first 10 years of promed-
mail. Archives of Medical Research, 36(6):724 ?
730. Infectious Diseases: Revisiting Past Problems
and Addressing Future Challenges.
Mawudeku, A. and M. Blench. 2006. Global pub-
lic health intelligence network (gphin). In Proc. 7th
Int. Conf. of the Association for Machine Transla-
tion in the Americas, Cambridge, MA, USA, August
8?12.
McCrae, J., M. Conway, and N. Collier. 2009. Simple
rule language editor. Google code project, Septem-
ber. Available from: http://code.google.com/p/srl-
editor/.
Miles, A., B. Matthews, and M. Wilson. 2005. SKOS
Core: Simple knowledge organization for the web.
In Proc. Int. Conf. on Dublin Core and Metadata
Applications, Madrid, Spain, 12?15 September.
Niles, I. and A. Pease. 2001. Towards a standard up-
per ontology. In Welty, C. and B. Smith, editors,
2nd Int. Conf. on Formal Ontology in Information
Systems FOIS-2001, Maine, USA, October 17?19.
Okhamatovskaia, A., W. Chapman, N. Collier, J. Es-
pino, and D. Buckeridge. 2009. SSO: The syn-
dromic surveillance ontology. In Proc. Int. Soc. for
Disease Surveillance, Miami, USA, December 3?4.
Stearns, M. Q., C. Price, K. A. Spackman, and A. Y.
Wang. 2001. SNOMED clinical terms: overview of
the development process and project status. In Proc.
American Medical Informatics Association (AMIA)
Symposium, pages 662?666.
Vossen, P. 1998. Introduction to EuroWordNet. Com-
puters and the Humanities, 32:73?89.
Wagner, M. and H. Johnson. 2006. The internet as
sentinel. In Wagner, M. et al, editor, The Hand-
book of Biosurveillance, pages 375?385. Academic
Press.
WHO. 2004. ICD-10, International Statistical Classi-
fication of Diseases and Related Health Problems,
Tenth Revision. World Health Organization, De-
cember.
WHO. 2005. Avian influenza: assessing the pandemic
threat. Technical Report WHO/CDS/2005.29,
World Health Organization, Geneva, January.
Wilson, J. 2007. Argus: a global detection and track-
ing system for biological events. Advances in Dis-
ease Surveillance, 4.
Yangarber, R., P. von Etter, and R. Steinberger. 2008.
Content collection and analysis in the domain of
epidemiology. In Proc. Int. Workshop on Describ-
ingMedical Web Resources (DRMED 2008), Goten-
burg, Sweden, May 27th.
222
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 560?568,
Gothenburg, Sweden, April 26-30 2014. c?2014 Association for Computational Linguistics
 Discriminating Rhetorical Analogies in Social Media 
 
Christoph Lofi Christian Nieke Nigel Collier 
National Inst. of Informatics TU Braunschweig National Inst. of Informatics 
2-1-2 Hitotsubashi, 
Chiyoda-ku, Tokyo 
101-8430, Japan 
M?hlenpfordtstr. 23 
38106 Braunschweig 
Germany  
2-1-2 Hitotsubashi, 
Chiyoda-ku, Tokyo  
101-8430, Japan 
lofi@nii.ac.jp nieke@ifis.cs.tu-bs.de       collier@nii.ac.jp 
Abstract 
Analogies are considered to be one of the core 
concepts of human cognition and communica-
tion, and are very efficient at encoding com-
plex information in a natural fashion. How-
ever, computational approaches towards large-
scale analysis of the semantics of analogies are 
hampered by the lack of suitable corpora with 
real-life example of analogies. In this paper we 
therefore propose a workflow for discriminat-
ing and extracting natural-language analogy 
statements from the Web, focusing on analo-
gies between locations mined from travel re-
ports, blogs, and the Social Web. For realizing 
this goal, we employ feature-rich supervised 
learning models which we extensively evalu-
ate. We also showcase a crowd-supported 
workflow for building a suitable Gold dataset 
used for this purpose. The resulting system is 
able to successfully learn to identify analogies 
to a high degree of accuracy (F-Score 0.9) by 
using a high-dimensional subsequence feature 
space. 
1 Introduction 
Analogies are one of the core concepts of human 
cognition (Hofstadter, 2001), and it has been sug-
gested that analogical inference is  the ?thing that 
makes us smart? (Gentner, 2003). An analogy can 
be seen as a pattern of speech leading to a cogni-
tive process that transfers some high-level mean-
ing from one particular subject (often called the 
analogue or the source) to another subject, usually 
called the target. When using analogies, one em-
phasizes that the ?essence? of source and target is 
similar, i.e. their most discriminating and proto-
typical processes and properties are perceived in a 
similar way. 
The nature of analogies has been discussed and 
studied since the ancient Greeks, however compu-
tational approaches are still rather limited and in 
their infancy. One reason for this is that text cor-
pora containing analogies are crucial to study the 
syntactic and semantic patterns of analogies in or-
der to make progress on automated understanding 
techniques. For example, to learn about their dis-
tribution and the attribute-value pairs that are 
compared. However, to the best of our knowledge, 
no such corpus is freely available. We will there-
fore in this paper present a method for creating 
such a corpus in an efficient fashion, and make our 
corpus available for further research efforts. 
As an example, consider this brief statement: 
?West Shinjuku (a Tokyo district) is like Lower Manhat-
tan? It allows readers who know New York, but 
not Tokyo, to infer some of the more significant 
properties of the unknown district (e.g., it is an im-
portant business district, hosts the headquarters of 
many companies, features many skyscrapers, 
etc.). However, automatically understanding anal-
ogies is surprisingly hard due to the extensive do-
main knowledge required in order to perform ana-
logical reasoning. For example, an analogy repos-
itory containing such domain knowledge has to 
provide information on which attributes of source 
and target are generally considered comparable. In 
contrast to Linked Open Data or typical ontolo-
gies, such analogical knowledge is consensual, i.e. 
there is no undisputable truth to analogical infor-
mation, but a statement can be considered ?good? 
analogical knowledge if its? semantics are per-
ceived similarly by enough people (Lofi & Nieke, 
2013). For example, while many properties of 
West Shinjuku and Lower Manhattan are dissimi-
lar, nonetheless most people will immediately rec-
ognize dominant similarities. 
In order to build an analogy repositories, a large 
number of actual analogy statements reflecting the 
diversity of people?s opinions are required for 
analysis. In this paper, we make a start on this task 
by proposing a workflow for reliably extracting 
such statements by using feature-rich supervised 
560
learning models, and demonstrate its effectiveness 
for analogies between different places. Our contri-
butions in this paper are as follows: 
? First, we build a suitable Gold corpus for train-
ing and testing supervised learning models, fo-
cusing on analogies between places. This cor-
pus will be based upon content mined from 
search engines and social media.  
? We show the effectiveness, but also the chal-
lenges of crowd-sourcing as a technique for 
screening and refining potential Gold corpus 
documents. This process results in multi-sen-
tence text snippets containing an analogy ex-
tracted from these documents.  
? We design and evaluate supervised learning 
models with rich feature sets to recognize anal-
ogy statements automatically, allowing us to 
substitute crowd-sourcing with automated 
techniques for further expanding the corpus.  
? We extensively evaluate our models, and dis-
cuss their strengths and shortcomings. 
2 Processing Analogies 
There exist several approaches for modeling and 
capturing the semantics of analogies, among them 
many formal ones relying, for example, on struc-
tural mapping (Gentner, 1983). These types of ap-
proaches aim at mapping characteristics and rela-
tionships of source and target, usually relying on 
factual domain knowledge given in propositional 
networks. One example typically used in this con-
text is the Rutherford analogy ?Atoms are like the So-
lar System?, which can be derived by outlining sim-
ilarities between the nucleus and the sun, which 
are both heavy masses in the center of their respec-
tive system, and electrons and planets, which re-
volve around the center attracted by a strong force 
(here, the coulomb force is analog to the gravita-
tional force). This model resulted in several theo-
retical computational models (e.g. (Gentner & 
Gunn, 2001)).  
The most extensively researched subset of analo-
gies are 4-term analogies between two word pairs 
(mason, stone)::(carpenter, wood). Here, processing 
analogies boils down to measuring the relational 
similarity of the word pairs, i.e. a mason works 
with stone as a carpenter works with wood.  
However, measuring the similarity between enti-
ties or relationships is a difficult task. While most 
structure-mapping approaches rely on processing 
facts, e.g. as extracted from ontologies or 
knowledge networks, supporters of perceptual 
analogies claim that this similarity has to be meas-
ured on a high perceptional level (Chalmers, 
French, & Hofstadter, 1992), i.e. there can be an 
analogy if people perceive or believe relations or 
properties to be as similar even if there are no hard 
facts supporting it (Kant, 1790), or even when 
facts oppose it. More formally, two entities ? and 
? can be seen as being analogous (written as ? ?
?) when their relevant relationships and properties 
are perceived sufficiently similar (Lofi & Nieke, 
2013). This type of consensual analogy is of high 
relevance in natural communication (in fact, most 
analogies we discovered in our data are of this 
type), but very hard to learn as there are no corpora 
for studying analogies readily available. Further-
more, this definition opens up other challenges: 
What are the relevant characteristics between two 
entities? When are they perceived as being simi-
lar? And when does an analogy hold true?  
With this work, we aim at paving the way for fu-
ture research on this challenging set of problems 
by providing a workflow for mining analogy ex-
amples from the Web and Social Media. To illus-
trate this, consider the following example ex-
tracted from our Gold corpus:   
?Tokyo, like Disneyland, is sterile. It?s too clean and 
really safe, which are admirable traits, but also unre-
alistic. Tokyo is like a bubble where people can live 
their lives in a very naive and enchanted way because 
real problems do not exist.?  
(No. 5310 in corpus) 
This perceptual analogy between Tokyo and Dis-
neyland is hard to explain when only relying on 
typical structured knowledge like Linked Open 
Data or ontologies, and thus requires specialized 
data repositories which can be built up using real-
world examples as provided by our approach.   
Unfortunately, actually detecting the use of an 
analogies in natural text, a requirement for build-
ing sufficiently large test corpora, is not an easy 
task, as there are only subtle syntactic and mor-
phological differences between an analogy and a 
simple comparison. These differences cannot be 
grasped by simple classification models. For ex-
ample, while many rhetorical analogies contain 
phrases as ?is like? or ?, like?, as for example in 
?West Shinjuku is like Lower Manhattan? or ?Tokyo is 
like Disneyland as it is very sterile? there is a plethora 
of very similar sentences which do not express an 
analogy (?Shinjuku is like this: ?? or ?Tokyo, like the 
rest of Japan, ??). These subtle differences, which 
are hard to grasp with handcrafted patterns and are 
often found in the surrounding context, can be 
modeled by our approach as outlined in section 5. 
561
3 Related Work 
There exist several works on the semantics of 
analogies from a cognitive, philosophical, or lin-
guistic perspective, such as (Dedre Gentner, Keith 
J. Holyoak, & Boicho N. Kokinov, 2001), 
(Itkonen, 2005), or (Shelley, 2003).   
Hearst-like patterns (Hearst, 1992), which we use 
as a first and very crude filter during the construc-
tion of the Gold dataset, have frequently been em-
ployed in recent years, especially in the area of ex-
tracting hyponyms, e.g., (Snow, Jurafsky, & Ng, 
2004) which also aims at learning new extraction 
patterns based on word dependency trees. But also 
approaches for dealing with analogies are fre-
quently based on patterns applied to text corpora. 
Most of these approaches are tailored for solving 
general analogy challenges given in a 4-term mul-
tiple-choice format, and are usually evaluated on 
the US-based SAT challenge dataset (part of the 
standardized aptitude test for college admission). 
SAT challenges are in 4-term analogy form, e.g. 
?ostrich is to bird AS a) cub is to bear OR b) lion is to cat?, 
and the focus of those approaches is on heuristi-
cally assessing similarity of two given words 
pairs, to find the statistically more plausible an-
swer. For example, (Bollegala, Matsuo, & 
Ishizuka, 2009), (Nakov & Hearst, 2008), or 
(Turney, 2008) approach this challenge by using 
pattern-based Web search and subsequent analysis 
of the resulting snippets. In contrast to these ap-
proaches, we do not focus on word pair similarity, 
but given one entity, we aim at finding other enti-
ties which are seen as analogous in a specific do-
main (in our case analogies between locations and 
places). Being focused on a special domain often 
renders approaches relying on thesauri like Word-
Net or CoreLex unusable, as many of the words 
relevant to the domain are simply not contained. 
Closely related to analogy processing is the detec-
tion of metaphors or metonyms, which are a spe-
cial form of analogy. Simplified, a metaphor is an 
analogy between two entities with the additional 
semantics that one entity can substitute the other 
and vice versa). While early approaches to meta-
phor identification relied on hand-crafted patterns 
(Wilks, 1978), newer ones therefore heavily ex-
ploit the interchangeability of the entities (Beust, 
Ferrari, & Perlerin, 2003) or (Shutova, 2010), and 
cannot be used for general analogy processing 
without extensive adoption. These approaches of-
ten also rely on some reasoning techniques based 
on thesauri, but also other approaches based on 
                                                   
1 http://data.l3s.de/dataset/analogy-text-snippets 
mining and corpus analysis became popular. For 
example in (Shutova, Sun, & Korhonen, 2010) a 
system is presented which, starting from a small 
seed set of manually annotated metaphorical ex-
pressions, is capable of harvesting a large number 
of metaphors of similar syntactic structure from a 
corpus. 
Detecting analogies also has some similarities 
with relation extraction, e.g. (Bunescu & Mooney, 
2006) using Subsequence Kernels. However, the 
task is slightly more difficult than simply mining 
for a ?similar_to? relation, which is addressed by 
our approach in section 5. 
4 Building the Gold Dataset 
As the goal of this paper is to supply the tools for 
creating a large corpus of analogies from the Web, 
we require a reliable mechanism for automatically 
classifying if a text snippet contains an analogy or 
not. Such classification requires a Gold dataset 
which we construct in this section and which we 
make available to the community for download1. 
As we expect the number of analogies in a com-
pletely random collection of web documents to be 
extremely low, we first start by collecting a set of 
web documents that are likely to contain an anal-
ogy by applying some easy-to-implement but ra-
ther coarse techniques as follows: 
In order to obtain a varied set of text snippets (i.e. 
short excerpts from larger Web documents), we 
first used a Web search engine (Google Search 
API) with simple Hearst-like patterns for crawling 
potentially relevant websites. These patterns were 
selected manually based on analysis of sample 
Web data by three experts. In contrast to other ap-
proaches relying on extraction patters, e.g. 
(Turney, 2008) or (Bollegala et al., 2009), our pat-
terns are semi-open, e.g. ?# * similar to * as?, where 
# is replaced by one of 19 major cities we used for 
corpus extraction. * is a wildcard, therefore only 
one entity of the analogy is fixed by the pattern. 
Each pattern is created by combining one base part 
(in this case, ?# * similar to *?) with an extension 
part (?as?). We used 17 different base parts, and 14 
different extensions, resulting in 238 different ex-
traction patterns before inserting the city names. 
Using Web search, we initially obtained 109,121 
search results and used them to crawl 22,360 doc-
uments, for which we extracted the text snippets 
surrounding the occurrence of the pattern (2 pre-
ceding and 2 succeeding sentences). The intention 
of our open Hearst-like patterns is to obtain a wide 
562
variety of text snippets which are not limited to 
simple analogy cases, so most snippets obtained 
will actually not be analogies at all. Therefore, ad-
ditional filtering is required to find those which do 
actually contain an analogy between places. Un-
like e.g. (Turney, 2008) where patterns of the form 
?[0..1] X [0..3] Y [0..1]?, with X and Y two given en-
tities, are used, we chose a more general approach 
and filtered out all snippets not containing at least 
two different locations (and hence no place anal-
ogy, locations provided by Stanford CoreNLP 
NER tagger), which left 14,141 snippets.  
Since we lacked the means to manually classify all 
of these snippets as a Gold set, we randomly se-
lected a subset of 8000 snippets, and performed a 
crowd-sourcing based filtering to detect potential 
analogies, as described in the following.  
 Crowd-Sourcing-Based Filtering 
Under certain circumstances, crowd-sourcing can 
be very effective for handling large tasks requiring 
human intelligence without relying on expensive 
experts. In contrast to using expert annotators, 
crowd-workers are readily and cheaply available 
even for ad-hoc tasks. In this paper, we used mi-
cro-task crowd-sourcing, i.e. a central platform 
like for example Amazon Mechanical Turk2  or 
CrowdFlower3 assigns small tasks (called HITs, 
human-intelligence tasks) to workers for monetary 
compensation. HITs usually consist of multiple 
work units taking only a few minutes to process, 
and therefore pay few cents.   
Crowd-sourcing has been shown to be effective 
for language processing related tasks, e.g. in 
(Snow, O?Connor, Jurafsky, & Ng, 2008)  it was 
used to annotate text corpora, and the authors 
found that for this task, the combination of three 
crowd judgments roughly provides the quality of 
one expert worker. However, the quality can vary 
due to potential incompetence and maliciousness 
of workers, making quality control mandatory. 
The two basic tools for quality control in crowd-
sourcing are majority votes and Gold units, which 
are both used in our process. Gold units are tasks 
for which the correct answer is known, and they 
are transparently mixed into normal HITs distrib-
uted to workers. If workers repeatedly provide an 
incorrect judgment for gold units, they are consid-
ered malicious, are not paid, and their judgments 
are excluded from the results.  
Therefore, we continued to classify the selected 
8,000 snippets using 90 gold units. 5 snippets are 
grouped within each HIT, for which we pay USD 
                                                   
2 https://www.mturk.com/  
$0.04. For each snippet, 3 judgments are elicited. 
In total, 336 workers participated in categorizing 
87 snippets on average (some top contributors cat-
egorized up to 1,975 snippets). As a result 895 
snippets are classified as containing an analogy 
with a confidence of over 90% (confidence is 
computed as a weighted majority vote of worker 
judgments and worker reliability; with worker re-
liability resulting from workers failing or passing 
gold units in previous tasks).  
A brief manual inspection showed that these re-
sults cannot be trusted blindly (a correctness of 
78% compared to an expert judgment was meas-
ured in a small sample), so we performed an expert 
inspection on all potential analogy snippets, revis-
ing the crowd judgments where necessary. Fur-
thermore, we manually tagged the names of the 
analogous locations. This resulted in 542 snippets 
which are now manually judged as analogies and 
353 snippets that were manually judged as not be-
ing an analogy. For this task, worker performance 
is extremely asymmetrical as it is much easier for 
crowd-workers to reach an agreement for negative 
examples than for positive ones, and there were 
3,023 snippets classified as no analogies with 
100% confidence. This intuition was supported by 
a short evaluation in which we sampled 314 
(10.3%) random snippets from this set and found 
none that had been misclassified. Therefore, the 
negative examples of our Gold set consist of the 
snippets manually re-classified by our expert an-
notators, and the snippets which had been classi-
fied with 100% confidence by the crowd-workers. 
This leaves out 4,082 snippets for which no clear 
consensus could be reached, and which are thus 
excluded from the Gold set.  
5 Classifiers and Feature Extraction 
Using crowd-sourcing for finding analogy state-
ments is a tedious and still quite expensive task. 
Therefore, we aim at automating the processes of 
detecting analogies in a given text snippet by de-
signing multiple rich feature sets for machine 
learning-based classification models, allowing us 
to discover new analogies quicker and cheaper. 
 Dataset Description 
Our complete Gold dataset of 3,918 text snippets 
shows a ratio of positive to negative examples of 
roughly 1:8. For training and evaluation, we per-
form four stratified random selections on the Gold 
set to obtain 4 training sets with 2/3 of the overall 
3 http://crowdflower.com/ 
563
size (2,611), and respective test sets with 1/3 size 
(1,307). In each set, the original ratio between pos-
itive example (analogies) and negative examples 
(not analogies) is retained. We prefer this ap-
proach over n-fold cross-validation as some of our 
models are expensive to train.  
All snippets in the Gold set consist of 5 sentences, 
with 105 words per snippet on average. This aver-
age does not significantly vary between positively 
and negatively classified snippets (94 vs. 106). 
The overall vocabulary contains 31,878 unique 
words, with 6,960 words in the positive and 
30,234 in the negative subset. 5,316 of these 
words are shared between both sets (76% of those 
in the Gold set). This observation implies that the 
language in our snippets is highly varied and far 
from saturated (for the significantly smaller posi-
tive set, 12.84 new words per snippet are added to 
the vocabulary on average, while for the larger 
negative subset, this value only drops to 8.95). 
This situation looks similar for locations, which 
play a central role in this classification task: the 
overall number of different locations encountered 
in all snippets is 2,631, with 0.86 new locations 
per snippet in the positive set and 0.73 in the neg-
ative set. On average, there are 3.18 locations 
mentioned in a given snippet, again with no sig-
nificant differences in the positive and negative 
subset (3.67 vs. 3.10). Please refer to Table 1 for 
exhaustive statistics.  
 Unigram (Bag-of-Word) Feature Model 
As our evaluation baseline, we use a straight-for-
ward unigram (bag-of-word) feature model for 
training a support vector machine. No stop words 
are removed, and the feature vectors are normal-
ized to the average length of training snippets. 
Furthermore, we only retain the 5000 most fre-
quent features, and skip any which occur only in a 
single snippet. For this experiments (and all other 
later experiments using a SVM), we used the 
LibSVM implementation (Chang & Lin, 2011) 
with a linear kernel due to the size of the feature 
space. 
  N-Gram-based Feature Model  
Our first approach to increasing classification 
quality of the baseline is expanding the feature 
space to also include n-grams. We tested different 
versions of this model with lexical word-level n-
grams, part-of-speech n-grams, and both of them 
simultaneously. In all cases, we include n-grams 
with a length of 1 to 4 words, and similar to the 
                                                   
4 http://nlp.stanford.edu/software/corenlp.shtml 
baseline, the top-5000 features are retained and 
values are normalized to the training snippet 
length, with a minimal frequency of 2. The re-
quired part-of-speech labels are obtained by using 
the Stanford CoreNLP library4.The three resulting 
feature models have been trained and evaluated 
with three classification algorithms which are 
known to provide good performance in similar 
classification tasks: a support vector machine clas-
sifier (as in 5.2), a Na?ve Bayes classifier (from the 
Weka library5), and Weka?s J48 implementation 
of the C4.5 classifier (Quinlan, 1993) (with prun-
ing confidence 0.25 and min. leaf distance 2). 
 Shortest Path Feature Model 
In this subsection we design the Shortest Path fea-
ture model, a model aiming at exploiting some of 
the specific properties of place analogies. By def-
inition, only text snippets featuring two different 
places can be a place analogy. The Shortest Path 
model furthermore assumes that both these loca-
tions occur in a single sentence (which is tested in 
6.3), and that there is a meaningful lexical or 
grammatical dependency between these occur-
rences. For actually building our feature space, we 
rely on typed dependency parses (Marneffe, 
MacCartney, & Manning, 2006) of the snippets, 
and extract the shortest path in the resulting de-
pendency tree between both locations (also using 
Stanford CoreNLP). This path represents the col-
lapsed and propagated dependencies between both 
locations, i.e. basic tokens as ?on? or ?by? are inte-
grated in the edge labels and don?t appear as 
nodes. We considered three variations of this ap-
proach: paths built using lexical labels, path with 
part-of-speech labels, and a combination of both. 
During the construction of our Gold set, we man-
ually annotated the two relevant places for all 
analogies. Therefore this approach can be applied 
5 http://www.cs.waikato.ac.nz/ml/weka/ 
Table 1: Characteristics of Gold Data 
characteristic all  positive negative 
# of snippets 3,918 542 3,376 
# of snippets in 
training set 
2,611 361 2,250 
# of snippets in test 
set 
1,307 181 1,126 
vocabulary size 31,878 6,960 30,234 
voc. / #snippets 8.14 12.84 8.95 
location   
vocabulary size 
2,631 468 2,459 
loc.voc. / #snipts. 0.67 0.86 0.73 
# words / s.       + 105 94 106 
# locations / s. 3.18 3.67 3.10 
+ #/s.: average count per snippet  
564
directly for positive training examples. However, 
for negative snippets, no relevant locations have 
been annotated. Hence, for all negative snippets in 
training and all snippets in the test set, we assume 
that all locations which appear in a snippet (as de-
termined by a NER tagger) are relevant, and we 
extract all shortest paths between any of them. On 
average this results in 5.6 paths extracted from any 
given snippet. The extracted paths are generalized 
by replacing the locations with a generic, and the 
final feature model results from constructing a bi-
nary feature representing whether a given path oc-
curs or not. 
As with the n-gram-based feature model, we train 
and evaluate SVM, Na?ve Bayes, and J48 classifi-
ers with our feature vector (parameters as in 5.3). 
Please note that building this model is computa-
tionally significantly more expensive than the n-
gram-based approach as it requires named entity 
recognition, and typed dependency parsing (we re-
quired roughly 30 minutes per training / test set on 
our Intel i7 laptop). 
 Subsequence Pattern Feature Model 
Basically, this approach aims at creating some-
thing similar to the most common sub forests of 
all snippets, or skip-grams (Guthrie, Allison, Liu, 
Guthrie, & Wilks, 2006), i.e. results can be seen as 
a hybrid between ?tree patterns? (as e.g. the Short-
est Path) and n-grams. The intention is to avoid the 
problem of overly local patterns, allowing the pat-
terns to work even in the presence of fill words and 
subsequences added to a sentence. For this, we uti-
lize the PrefixSpan algorithm (Pei et al., 2001) to 
detect common, reappearing subsequence in the 
training set, i.e. sequences of words that appear in 
a given order, ignoring anything in-between. In 
contrast to the shortest path approach, this model 
focuses on multiple sentences simultaneously, and 
therefore is a significant contribution over state-
of-the-art techniques. 
As before, we used lexical, part-of-speech, and 
combined features. The general idea of this ap-
proach is to use the PrefixSpan algorithm to mine 
subsequence patterns from positive gold snippets 
(the primitives), and use these as binary features in 
a classification step, for which we trained three 
classifiers as described in 5.3.  
In case of the lexical labels, we use the PrefixSpan 
algorithm to return all subsequences that appear at 
least 10 times (this value is dependent on charac-
teristics of the dataset and has to be tuned manu-
ally) in the relevant part (i.e. the minimal set of 
consecutive sentences that include both locations) 
of the positive training set snippets. Depending on 
the training set used, this resulted in about 40k 
common subsequences. To avoid unspecific pat-
terns, we filtered out all sequences that did not 
contain both locations, which reduces the number 
to about 15k in average. We then replaced the ac-
tual locations with a generic, which allows build-
ing a regular expression from the pattern that al-
lows any number of words in-between each part of 
the sequence. Before applying a pattern to an un-
known snippet, we also replace all (NER tagged) 
locations with a generic. For example, ?LOCA-
TION * is * like * LOCATION? would match ?Tokyo 
is also a lot like Seoul? using regular expressions. 
The part-of-speech version is similar to the lexical 
one, but tries to create more generic and open pat-
terns by mining subsequences from the POS rep-
resentation of the relevant snippet part. For filter-
ing, all patterns that do not contain two ?NNP? tags 
and appear less than 60 times are removed (the fil-
ter threshold is increased as POS patterns are more 
generic). We get around 60k to 80k patterns be-
fore, and ~10k to 20k primitive patterns after fil-
tering which are used as binary features. Finally, 
we merged lexical and POS patterns and thus al-
lowed the classifiers to use any of the features. A 
strongly truncated version of a rule tree created us-
ing J48 classification with POS subsequence prim-
itives is shown in Figure 1. Please note that due to 
the open nature of the primitives and their inde-
pendence, combining several of them in a feature 
vector will create extremely complex patterns 
quite easily. Even a vector that contains only the 
patterns *A*B* and *A*C* would create matches 
for ABC, ACB, ABAC, ACAB, AACB, AABC and allow 
any kind of additional fill words in between. How-
ever, this approach is computationally expensive 
(testing/training was around 6 hours on average). 
6 Evaluation 
In the following we evaluate the effectiveness of 
our analogy classifiers and models. We primarily 
rely on the informedness measure (Powers, 2007) 
for quantifying performance. In contrast to using 
only precision, recall, or F-Measure, it respects all 
 
Figure 1: Example Classification Tree 
*NNP*NNP* * *NNP*NNP*NN*NN* *
*NNP*NNP*.* *
*NN*NN*NN*NNP*IN*NNP* *
NOT-ANALOGY 
(243.0/8.0 correct)
ANALOGY 
(281.0/3.0 correct)
NO MATCH
NO MATCH
MATCH
MATCH
NO MATCH
NOT-ANALOGY 
(1938.0/22.0 correct)
*NNP*IN*NNP*NN* *
MATCH
MATCH
565
error types, false positives (FP) and false negatives 
(FN), but also true positives (TP) and true nega-
tives (TN), making it a fair and unbiased measure 
for classification. Furthermore, it compensates bi-
ased class distributions in datasets, e.g. as in our 
dataset the ratio of positive to negative snippets is 
1:8, even an ?always no? classifier has a correct-
ness of 85%, but will have an informedness of 0. 
Informedness is given by: 
???????????? = ?????? + ????????? ? 1 
with: ?????? =  
??
??+??
   and  ????????????? =  
??
??+??
 
In Table 2, we provide the average informedness, 
the percentage of correctly classified snippets, F-
measure, precision, recall, and inverse recall (true 
negative rate) for all experiments. A discussion of 
these results follows in the next section. 
 Classifier Performance 
Our straight-forward baseline approach, using uni-
grams and an SVM classifier results in a reasona-
ble informedness of 0.5. Expanding the feature 
space to lexical n-grams slightly increases perfor-
mance, while using more generic part-of-speech 
n-grams results in weaker results. Combining 
both, however, generally leads to better classifica-
tion results. When comparing different classifica-
tion algorithms, it shows that SVMs are most in-
formed when classifying n-grams-based features, 
followed by J48. Both techniques will result in 
moderate recall values around 0.5 and precision 
around 0.6, with a rather high true negative (inv. 
Recall rate) of 0.9. This changes quite signifi-
cantly for Na?ve Bayes, which is more likely to 
classify a snippet as positive, therefore leading to 
higher recall values, but also much lower in-
formedness, precision, and inverse recall. Conse-
quently, the best approach is using SVM with a 
lexical-POS combined feature space, leading to an 
informedness of 0.55. 
Shortest Path was intended to achieve higher pre-
cision results by exploiting additional semantic 
knowledge of the underlying problem. Unfortu-
nately, it performs poorly if not used with a SVM, 
but even then it achieves inferior overall results 
than the best n-gram approach (informedness 0.4). 
This is due to some of its necessary assumptions 
not holding true (see section 6.4). 
In contrast, our subsequence-based model 
achieves a higher informedness score of 0.85 and 
0.87 in the best cases. While the lexical variants 
perform not as well, the more generic variants us-
ing POS allow for reliable classification. Combin-
ing the lexical and the POS features does unfortu-
nately not increase the performance further (quite 
contrary, the scores generally decrease for com-
bined features). A possible explanation is overfit-
ting caused by the increased feature space.  
  Significance Tests 
As our Gold set is of limited size, we performed 
statistical tests to investigate whether the differ-
ences reported in the last subsection are actually 
Table 2: Classifier Result Comparison with respect to the Gold classification 
Classifier Informed. % Correct F-Measure Precision Recall Inv. Recall 
Always No  0.00 0.85 - - 0 1 
Unigram Lexical SVM 0.50 0.88 0.59 0.63 0.55 0.94 
n-Gram Lexical SVM 0.53 0.89 0.63 0.68 0.58 0.95 
n-Gram  POS SVM 0.42 0.87 0.52 0.58 0.48 0.94 
n-Gram Lex & POS SVM 0.55 0.90 0.65 0.73 0.59 0.96 
n-Gram  Lexical Na?ve Bayes 0.33 0.48 0.36 0.22 0.93 0.41 
n-Gram  POS Na?ve Bayes 0.38 0.61 0.39 0.26 0.81 0.58 
n-Gram  Lex & POS Na?ve Bayes 0.48 0.75 0.47 0.35 0.73 0.75 
n-Gram  Lexical J48 (C4.5) 0.45 0.87 0.55 0.58 0.52 0.93 
n-Gram  POS J48 (C4.5) 0.37 0.85 0.47 0.51 0.45 0.92 
n-Gram  Lex & POS J48 (C4.5) 0.44 0.87 0.54 0.57 0.52 0.93 
Shortest Path SVM 0.40  0.90 0.53 0.71 0.43 0.97 
Shortest Path Na?ve Bayes 0.27  0.87 0.40 0.55 0.32 0.96 
Shortest Path J48 (C4.5) 0.26 0.89 0.40 0.77 0.27 0.99 
Subseq. Lexical  SVM 0.39 0.87 0.24 0.51 0.46 0.94 
Subseq. Lexical Na?ve Bayes 0.53 0.79 0.49 0.36 0.73 0.80 
Subseq. Lexical J48 (C4.5) 0.34 0.86 0.44 0.47 0.41 0.93 
Subseq. POS SVM 0.84 0.97 0.87 0.89 0.85 0.98 
Subseq. POS Na?ve Bayes 0.72 0.81 0.57 0.41 0.93 0.79 
Subseq. POS J48 (C4.5) 0.85 0.97 0.90 0.93 0.86 0.99 
Subseq. Lex & POS SVM 0.77 0.95 0.83 0.87 0.79 0.98 
Subseq. Lex & POS Na?ve Bayes 0.70 0.80 0.56 0.41 0.91 0.79 
Subseq. Lex & POS J48 (C4.5) 0.87 0.97 0.90 0.92 0.88 0.98 
 
 
566
significant or result from noise. We used an in-
stance-based test relying on the theory of approx-
imate randomization (Noreen, 1989)6 to perform 
100k iterations of randomized testing of the hy-
pothesis that the pairwise performance differences 
of selected approaches are actually significant (ex-
cluding those pairs where the significance is obvi-
ous). First, we compared our baseline, lexical uni-
grams with SVM to using lexical n-grams to test 
whether using n-grams actually contributed to the 
quality, and found the difference to be significant 
(sign-test p<0.024). However, for SVM-based 
classification, the higher reported performance for 
also including POS features in addition to lexical 
n-grams could not be shown to be significant 
(p>0.4). Finally, we tested if the choice between 
SVM or J48 is significant for our two best subse-
quence-based approaches, and confirmed this 
clearly (sign-test: p<0.006). According to the re-
ported subsequence results, combining lexical fea-
tures with part-of-speech features counter-intui-
tively lowers the performance when using SVM or 
Na?ve Bayes and the positive effect on J48 was 
shown to be insignificant (p>0.68). Therefore, we 
assume that lexical features don?t make a substan-
tial contribution when POS features are present. 
 Error Analysis 
For only 2,845 of all 3,918 snippets, two different 
locations (regardless of their relevance to the anal-
ogy) are mentioned in the same sentence. This se-
verely hampers the effectiveness of our Shortest 
Path approach, which is limited to cases where 
both locations appear in the same sentence. Those 
snippets (344 on average / test set) are then classi-
fied as ?not analogy?, decreasing the recall. The 
overall impact of this shortcoming is still low, as 
only 4% of these snippets are analogies. Our other 
approaches are unaffected. 
Interestingly, we see what one might call the ?in-
verse problem? when using the other two models 
(n-gram and subsequence) that search for the pres-
ence of certain terms or sequences, but do not ex-
plicitly connect them to the locations. They tend 
to create false positives by detecting statements 
that contain 2 locations and an analogy, but not 
between these locations. Consider: 
?They say New York is the City of Dreams. I say Lon-
don is the theatre where it all happens?  
(No. 5627 in corpus). 
Another source for false positives is when an anal-
ogy is not stated, but is requested: 
                                                   
6 Implementation at: http://www.clips.ua.ac.be/scripts/art 
?What districts of Paris are similar to Shepherd's 
Bush or Ealing (both in West London??  
(No. 8505 in corpus) 
7 Summary and Outlook 
We demonstrated approaches for discriminating 
analogy statements from the Web and Social Me-
dia. Our two major contributions are: a) We cre-
ated a Gold dataset containing 3,918 example text 
snippets, of which 542 are positively identified as 
analogies. This dataset was extracted from 109k 
potential documents resulting from a Web search 
with manually crafted Hearst-like patterns. The 
dataset was consequently refined by using a com-
bination of filters, crowd-sourcing, and expert 
judgments. We also discussed the challenges aris-
ing from a crowd-sourcing in such a setting.b) Us-
ing the Gold dataset, we designed and evaluated a 
set of machine learning models for classifying text 
snippets automatically with respect to containing 
place analogies. Besides more traditional n-gram 
based models, we also designed novel models re-
lying on feature spaces resulting from shortest 
path analysis of the typed dependency tree, and 
high-dimensional feature spaces built from fil-
tered subsequence patterns mined using the Pre-
fixSpan algorithm. In an exhaustive evaluation, 
the latter approach, which bridges between lexical 
and structural features, could be shown to provide 
significantly superior performance with a maxi-
mal informedness of 0.87 compared to 0.55 for the 
next best approach.  
In future work, classification performance can be 
further increased by better handling of current 
problem cases, e.g. analogies with out-of-domain 
targets (analogies between locations and other en-
tity classes, analogies between other entities but 
unrelated locations nearby, etc.) or ambiguous 
sentence constructions. Also, our approach can be 
adopted to other domains relevant to Web-based 
information systems like movies, cars, books, or 
e-commerce products in general. 
However, the more challenging next step is actu-
ally analyzing the semantics of the retrieved anal-
ogies, i.e. extracting the triggers of why people 
chose to compare the source and target. Achieving 
this challenge will allow building analogy reposi-
tories containing perceived similarities between 
entities and is a mandatory building block for ac-
tually implementing an analogy-enabled infor-
mation system.  
567
References  
Beust, P., Ferrari, S., & Perlerin, V. (2003). NLP model 
and tools for detecting and interpreting 
metaphors in domain-specific corpora. In Conf. 
on Corpus Linguistics. Lancaster, UK. 
Bollegala, D. T., Matsuo, Y., & Ishizuka, M. (2009). 
Measuring the similarity between implicit 
semantic relations from the web. In 18th Int. Conf. 
on World Wide Web (WWW). Madrid, Spain. 
doi:10.1145/1526709.1526797 
Bunescu, R. C., & Mooney, R. J. (2006). Subsequence 
Kernels for Relation Extraction. In Conf. on 
Advances in Neural Information Processing 
Systems (NIPS). Vancouver, Canada. 
Chalmers, D. J., French, R. M., & Hofstadter, D. R. 
(1992). High-level perception, representation, 
and analogy: A critique of artificial intelligence 
methodology. Journal of Experimental & 
Theoretical Artificial Intelligence, 4(3), 185?211. 
doi:10.1080/09528139208953747 
Chang, C.-C., & Lin, C.-J. (2011). LIBSVM: A library 
for support vector machines. ACM Transactions 
on Intelligent Systems and Technology (TIST), 
2(3). 
Dedre Gentner, Keith J. Holyoak, & Boicho N. 
Kokinov (Eds.). (2001). The analogical mind: 
perspectives from cognitive science (Vol. 0, p. 
541). MIT Press. 
Gentner, D. (1983). Structure-mapping: A theoretical 
framework for analogy. Cognitive science, 7, 
155?170. 
Gentner, D. (2003). Why We?re So Smart. In Language 
in Mind: Advances in the Study of Language and 
Thought (pp. 195?235). MIT Press. 
Gentner, D., & Gunn, V. (2001). Structural alignment 
facilitates the noticing of differences. Memory & 
Cognition, 29(4), 565?77. 
Guthrie, D., Allison, B., Liu, W., Guthrie, L., & Wilks, 
Y. (2006). A closer look at skip-gram modelling. 
In Int. Conf. on Language Resources and 
Evaluation (LREC). Genoa, Italy. 
Hearst, M. A. (1992). Automatic acquisition of 
hyponyms from large text corpora. In Int. Conf. 
on Computational Linguistics (COLING). Nantes, 
France. 
Hofstadter, D. R. (2001). Analogy as the Core of 
Cognition. In The Analogical Mind (pp. 499?
538). 
Itkonen, E. (2005). Analogy as structure and process: 
Approaches in linguistics, cognitive psychology 
and philosophy of science. John Benjamins Pub 
Co. 
Kant, I. (1790). Critique of Judgement. 
Lofi, C., & Nieke, C. (2013). Modeling Analogies for 
Human-Centered Information Systems. In 5th Int. 
Conf. On Social Informatics (SocInfo). Kyoto, 
Japan. 
Marneffe, M.-C. de, MacCartney, B., & Manning, C. D. 
(2006). Generating typed dependency parses 
from phrase structure parses. In Int. Conf. on 
Language Resources and Evaluation (LREC). 
Genoa, Italy. 
Nakov, P., & Hearst, M. A. (2008). Solving relational 
similarity problems using the web as a corpus. In 
Proc. of ACL:HLT. Columbus, USA. 
Noreen, E. W. (1989). Computer-intensive Methods for 
Testing Hypotheses: An Introduction. John Wiley 
& Sons, New York, NY, USA. 
Pei, J., Han, J., Mortazavi-asl, B., Pinto, H., Chen, Q., 
Dayal, U., & Hsu, M. (2001). PrefixSpan: Mining 
Sequential Patterns Efficiently by Prefix-
Projected Pattern Growth. IEEE Computer 
Society. 
Powers, D. M. W. (2007). Evaluation: From Precision, 
Recall and F-Factor to ROC, Informedness, 
Markedness & Correlation. Flinders University 
Adelaide Technical Report SIE07001. 
Quinlan, R. (1993). C4.5: Programs for Machine 
Learning. San Mateo, USA: Morgan Kaufmann 
Publishers, Inc. 
Shelley, C. (2003). Multiple Analogies In Science And 
Philosophy. John Benjamins Pub. 
Shutova, E. (2010). Models of metaphor in NLP. In 
Annual Meeting of the Association for 
Computational Linguistics (ACL). 
Shutova, E., Sun, L., & Korhonen, A. (2010). Metaphor 
identification using verb and noun clustering. In 
Int. Conf. on Computational Linguistics 
(COLING). Beijing, China. 
Snow, R., Jurafsky, D., & Ng, A. (2004). Learning 
syntactic patterns for automatic hypernym 
discovery. In Advances in Neural Information 
Processing Systems (NIPS). Vancouver, Canada. 
Snow, R., O?Connor, B., Jurafsky, D., & Ng, A. (2008). 
Cheap and fast---but is it good? Evaluating non-
expert annotations for natural language tasks. In 
Empirical Methods in Natural Language 
Processing (EMNLP). Honolulu, USA. 
Turney, P. (2008). A uniform approach to analogies, 
synonyms, antonyms, and associations. In Int. 
Conf. on Computational Linguistics (COLING). 
Manchester, UK. 
Wilks, Y. (1978). Making preferences more active. 
Artificial Intelligence, 11(3), 197?223. 
 
568
Proceedings of the BioNLP Shared Task 2013 Workshop, pages 130?134,
Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational Linguistics
Exploring a Probabilistic Earley Parser for  
Event Composition in Biomedical Texts 
 
Mai-Vu Tran1 Hoang-Quynh Le1  Van-Thuy Phi1  Thanh-Binh Pham1 Nigel Collier2,3    
1University of Engineering and Technology, VNU, Hanoi, Vietnam 
2National Institute of Informatics, Tokyo, Japan 
3European Bioinformatics Institute, Cambridge, UK 
{vutm,lhquynh,thuypv,binhpt}@vnu.edu.vn, collier@ebi.ac.uk 
  
Abstract 
We describe a high precision system for ex-
tracting events of biomedical significance that 
was developed during the BioNLP shared task 
2013 and tested on the Cancer Genetics data 
set. The system achieved an F-score on the de-
velopment data of 73.67 but was ranked 5th out 
of six with an F-score of 29.94 on the test data. 
However, precision was the second highest 
ranked on the task at 62.73. Analysis suggests 
the need to continue to improve our system for 
complex events particularly taking into ac-
count cross-domain differences in argument 
distributions.   
1 Introduction 
In this paper we present our approach to the Bi-
oNLP 2013 shared task on Cancer Genetics (CG) 
(Pyysalo et al, 2013, Pyysalo et al, 2012), 
aimed at identifying biomedical relations of sig-
nificance in the development and progress of 
cancer. Our system explored a multi-stage ap-
proach including trigger detection, edge detec-
tion and event composition. After trigger edge 
detection is finished we are left with a semantic 
graph from which we must select the optimal 
subset that is consistent with the semantic frames 
for each event type. Previous approaches have 
derived sub-graph matching rules using heuris-
tics (Jari Bj?rne et al 2009) or machine learning 
using graph kernels (Liu et al, 2013). Based on 
McClosky et al (2011)?s observation that event 
structures have a strong similarity to dependency 
graphs, we proposed a novel method for the 
composition of ambiguous events used a proba-
bilistic variation of the Earley chart parsing algo-
rithm (Stolcke 1995) for finding best derived 
trigger-argument candidates. Our method uses 
the event templates and named entity classes as 
grammar rules. As an additional novel step our 
chart parsing approach incorporates a linear in-
terpolation mechanism for cross-domain adaptiv-
ity between the training and testing (develop-
ment)  data.   
2 Approach 
The system consists of five main modules: pre-
processing, trigger detection, edge detection, 
simple event extraction, complex event extrac-
tion. Each of these is described below with an 
emphasis on event composition where we ap-
plied a probabilistic variation on the Earley par-
ser.   
2.1 Experimental Setting 
As our team?s first attempt at the BioNLP shared 
task we decided to focus our attention on the 
Cancer Genetic Task. The CG Task aims to ex-
tract events related to the development and pro-
gression of cancer.  
A characteristic feature of the CG Task is that 
there are a large number of entity and event 
types: 18 entity classes, 40 types of event and 8 
types of arguments. Among these events, there 
are 7 that may have no arguments: Blood vessel 
development, Cell death, Carcinogenesis, Metas-
tasis, Infection, Amino acid catabolism and Gly-
colysis. On the other hand, some events may 
have more than one argument: Binding and Gene 
Expression may have more than one Theme ar-
gument, and Planned process may have more 
than one Instrument argument. 
We divided events into two groups based on 
definitions of Miwa et al(2010) : simple and 
complex events. Simple events include 36 events 
whose arguments must be entities. Complex 
events include 4 event types whose arguments 
may be other events. 
2.2 Pre-processing 
Pre-processing conventionally made use of the 
GeniaTagger (Tsuruoka and Tsujii, 2005) for 
sentence splitting and tokenizing, and the HPSG 
130
parser Enju1 (Miyao and Tsujii, 2008).  Both of 
these were provided in the supporting resources 
by the task organisers. Gold standard named enti-
ty annotations were also provided.  
2.3 Trigger Detection 
In the CG Task dataset, 95% of the triggers 
that indicate events are single token. We there-
fore treated trigger detection as a token labeling 
problem in a similar way to Bj?rne et al (2009). 
Here the system has to classify whether a token 
acts as a trigger for one of the forty event types 
or not.  We used the Liblinear-java library2 (Fan 
et al, 2008) with the L2-regularized logistic re-
gression method for both trigger detection and 
edge detection. We performed a manual grid 
search to select a C-value parameter of 0.5. This 
parameter value is same from that of the Turku 
system (Bj?rne et al (2009), in which the C-
values were tuned for all of their detectors. 
The major features used are primarily based 
on Miwa, et al (2012) and shown in Table 1. In 
our experiments this led to a relatively large 
number of features: about 500k features for the 
trigger detection model, 900k features in the T-E 
model and 600k features in the EV-EV model. 
Our choice of the Liblinear library was partly 
motivated by its efficient performance with large 
feature sets. 
 
Feature Target 
Token feature - Current token 
Neighbouring word feature - Current token 
Word n-gram feature - Current token 
Trigger dictionary feature - Current token 
Pair n-gram feature - Between current token and 
named entities 
Parse tree shortest path 
feature 
- Between current token and 
named entities 
Table 1: Features in the trigger detection module. 
2.4 Event edge detection 
For edge detection, we used Liblinear to con-
struct two models: one model is designed primar-
ily to extract trigger-entity edges (T-E model), 
while the other system is designed primarily to 
extract event-event edges (EV-EV model). 
The T-E model classifies edge candidates to 
one of the 8 argument roles (theme, cause, site, 
atloc, toloc, fromloc, instrument, participant) 
and a negative argument class. Relation pairs are 
identified through the simple event extraction 
module (cf Section 2.5). 
                                                 
1 http://www-tsujii.is.s.u-tokyo.ac.jp/enju/ 
2 http://www.bwaldvogel.de/liblinear-java/ 
The EV-EV model identifies relations in the 
sentences between 4 types of complex events 
(Regulation, Negative regulation, Positive Regu-
lation and Planned process) and other events 
(including events belonging to the 4 complex 
events). The relations are classified into three 
classes: the two argument roles (theme or cause) 
or NOT. 
The features used in these two models are 
mostly the same as those used in the earlier trig-
ger detection module. Table 2 shows features and 
their applied target objects used in T-E model, 
Table 3 shows features and target objects for 
each feature of EV-EV model.  
 
Feature Target  
Token feature - Current trigger 
- Trigger argument entity 
Class feature - Current trigger 
- Trigger argument entity 
Neighbouring word 
feature 
- Current trigger 
- Trigger argument entity 
Word n-gram feature - Current trigger 
- Trigger argument entity 
Pair n-gram feature - Between current trigger and 
argument entity 
Parse tree shortest 
path feature 
- Between current trigger and 
rigger argument entity 
Table 2: Features in the T-E model. 
 
Feature Target 
Token feature Current trigger, target trigger, cur-
rent arguments, target arguments 
Class feature Current trigger, target trigger, cur-
rent arguments, target arguments 
Neighbouring word 
feature 
Current trigger, target trigger, cur-
rent arguments, target arguments 
Word n-gram feature Current trigger, target trigger, cur-
rent arguments, target arguments 
Pair n-gram feature Between current trigger and target 
trigger, between current trigger and 
target arguments, between current 
arguments and target trigger, be-
tween current arguments and target 
arguments 
Parse tree shortest 
path feature 
Between current trigger and target 
trigger, between current trigger and 
target arguments, between current 
arguments and target trigger, be-
tween current arguments and target 
arguments 
Table 3: Features in the EV-EV model. 
2.5 Simple event extraction 
In order to minimise the incorrect combination 
of arguments and triggers it seemed natural to try 
and solve the edge classification problem first 
between triggers and entities (simple edge detec-
tion) and then apply these as features in a stacked 
model to the complex event recogniser (cf Sec-
tion 2.6). In the simple event extraction module, 
131
 
Figure 1: An example of representing two complex events as two event trees. 
 
we combined edge candidates identified in the T-
E model into complete simple events. After this 
step, we had the results which belong to the 36 
simple event types and relations between 4 com-
plex events and entities. 
In order to select the edge candidates for each 
trigger, we used event-argument pattern based 
probabilities derived from the training set. An 
example of a Development event-arguments pat-
tern is:  
Development ? Theme(Gene_expression) + At-
Loc(Cancer) 
In practice there are several problems that 
arose when opting for this simple strategy: 
 - Firstly, there may be multiple candidates 
with the same argument role label linking to a 
trigger (such triggers do not belong to Binding, 
Gene Expression and Planned process). We used 
the output probability from the logistic regres-
sion event edge classifiers to select the best can-
didate in these cases. 
- Secondly, there are triggers whose candidate 
edge types link to entities that do not match pat-
terns observed in the training set or do not have 
any relation. We introduced a rule-based seman-
tic post-processing step: triggers are checked to 
see if they belong to the 7 event types which 
have no argument; if they do not, we rejected 
these from the results. 
- Thirdly, there may be an imbalance between 
the argument distribution in the training and test-
ing data (development data). In the development 
data, we observed some event-argument patterns 
which do not occur in training set, this problem 
may lead to false negatives. For example: 
Cell_transformation ? Theme(Cell) + At-
Loc(Cell) or Mutation ? 
Site(DNA_domain_or_region). This was one 
cause of false negatives in our system?s perfor-
mance (cf Section 3). 
2.6 Complex event extraction with proba-
bilistic Earley Parser 
For complex event extraction, based on the 
idea of McClosky et al (2011) that treats event 
extraction as dependency parsing, we represent 
complex events in the form of event trees which 
are similar to dependency trees. Our idea differs 
from McClosky et al in that they represented all 
events in a sentences within a single tree, where-
as we build a tree for each complex event. This 
solution helps avoid the problem of directed cy-
cles if there are two complex event that relate to 
the same entity or event. 
Figure 1 shows an example of representing 
two complex events as two event trees. To build 
the event tree, we create a virtual ROOT node; 
the complex event target will be linked directly 
to this ROOT node, and triggers and entities that 
do not belong to sub-structure of the target event 
will also have links to ROOT node, too. In the 
event tree, labels of entity classes and event 
types are retained while terms of triggers and 
entities are removed. 
For event tree parsing, we used the Earley 
parsing algorithm proposed by Jay Earley (1970) 
to find alternative structures. The event tree is 
stored in memory in the form of Earley rules. 
The inputs to the parser are the entities and trig-
gers which have been identified in the trigger 
detection module, and the outputs are the event 
tree candidates.  
To choose the best event tree candidates, we 
built a probabilistic Earley parser which devel-
oped from the idea of Hale (2001). As a first at-
tempt at introducing robustness for edge classifi-
er error our parser used linear interpolation on 
the probability from the edge detection module 
and the prior edge probabilities to calculate a 
score for each event tree candidate. The interpo-
lation parameter ? was set using a manual grid 
132
search and reflects the confidence we have in the 
generalisability of the edge detection module on 
the testing (development) data.   
The scoring function for each node is: 
Occurrence
(edge | argrument)
(node) (arguments | node)(edges)
edges node
P
Score Pnum
?? ?
? 
where, 
? num(edge) is the number of edges that 
have a link to the node 
? POccurence(arguments|node) is a distribu-
tion which represents the co-occurrence of 
entity/trigger labels in the arguments of an 
event type. 
? (edge | argrument) (edge | argument)ClassifierP P? ?? ?
        (1 )* (edge | argument)PriorP??  
? ? is a linear interpolation parameter in 
the range of [0,1]  
? PClassifier(edge|argument) is the probabil-
ity obtained from the edge classifier. 
? PPrior(edge|argument) is the training set?s 
prior probability for the edge. 
Edges that linked directly to ROOT and did 
not relate to the target complex event had a de-
fault value of zero. The final score of an event 
tree candidate was calculated as ROOT?s value. 
We used a filter_threshold parameter to re-
move event tree candidates which had an edge 
with P(edge|argument) less than filter_threshold. 
On the other hand, we used a cut-off_threshold 
parameter to choose event tree candidates which 
have highest value. Event tree candidates which 
are sub-structure of other event tree candidates 
were removed from the final results. 
3 Results and Discussion 
We evaluated each component of the system 
on the training and held out data sets. The opti-
mal configuration of parameters was then used 
on the shared task test data. We set these as fol-
lows:?=0.5;filter_threshold=0.2;cutoff_threshol
d=0.45.  
Table 4 shows F-score performance for event 
composition on the development data set. We 
found that complex events such as regulation and 
planned process performed at the lower end of 
accuracy due to their high complexity. This re-
sulted in relatively low recall compared to preci-
sion (figures not shown). The three Regulation 
events in particular are very productive in terms 
of the variety of named entities and triggers they 
take as arguments and their distribution in the 
development data was quite different to the train-
ing data. 
Event F1 Event F1 
Development 86.67 Phosphorylation 68.45 
Blood vessel 
development 
84.15 Dephosphorylation 66.67 
Growth 76.77 DNA methylation 85.71 
Death 61.95 DNA demethyla-
tion 
- 
Cell death 53.06 Pathway 61.81 
Breakdown 77.68 Localization 66.11 
    
Cell proliferation 59.82 Binding 70.68 
Cell division 100.00 Dissociation 100.00 
Remodeling 60.00 Regulation 69.55 
Reproduction - Positive regulation 68.13 
Mutation 78.74 Negative regula-
tion 
68.57 
Carcinogenesis 60.67 Planned process 49.99 
Metastasis 74.39 Acetylation  100.00 
Metabolism 62.50 Glycolysis  69.89 
Synthesis 52.63 Glycosylation - 
Catabolism 59.27 Cell transformation  66.67 
Gene expression 79.18 Cell differentiation  71.18 
Transcription 75.00 Ubiquitination 75.00 
Translation 80.00 Amino acid ca-
tabolism 
100.00 
Protein pro-
cessing 
100.00 Infection  75.86 
  Total  73.67 
Table 4: Baseline results for event composi-
tion on the development data. 
 
From our analysis on the development set we 
found that trigger detection was performing well 
overall with F-scores in the range 78 to 80. We 
choose 50 false negative events at random for 
error analysis. There are 29 triggers and 21 
events missing. Table 5 shows a stratified analy-
sis by major error type (we note that errors may 
of course have multiple causes). 
Cause Trigger Event 
Ambiguity in event class 9  
Co-reference 6  
Do not match with any event argument 
patterns 
7  
No training instance 7 4 
Choose best argument entity in simple 
event extraction   
 5 
No argument  4 
No Earley parser rule  8 
Total 29 21 
Table 5: Error classification of 50 missing 
false negatives. 
133
Performance on the shared task testing set was 
overall disappointing with an F-score of 29.94 
(Recall = 19.66, Precision = 62.73, F-score of 
simple event extraction = 47.96 and F-score of 
complex event extraction = 12.49) indicating low 
coverage caused by severe over-fitting issues. 
Analysis revealed that one cause of this was the 
imbalance in the distribution of arguments be-
tween training and testing sets. 
4 Conclusion  
We presented a system built on supervised 
machine learning with rich features, semantic 
post-processing rules and the dynamic program-
ming Earley parser. The system achieved an F-
score of 29.94 on the CG task with high preci-
sion of 62.73. Future work will focus on extend-
ing recall for complex events and looking at how 
we can avoid over-fitting to benefit cross-domain 
adaptivity.   
Acknowledgements 
We thank the shared task organisers for sup-
porting this community evaluation and to the 
supporting resource providers. Nigel Collier also 
gratefully acknowledges funding support from 
the European Commission through the Marie 
Curie International Incoming Fellowship (IIF) 
programme (Project: Phenominer, Ref: 301806). 
References  
David McClosky, Mihai Surdeanu, and Chris Man-
ning. 2011. Event extraction as dependency pars-
ing. In Proceedings of the BioNLP Shared Task 
2011 Workshop at the Association for Computa-
tional Linguistics Conference, pp. 41-45. 
Jari Bj?rne, Juho Heimonen, Filip Ginter, Antti Airo-
la, Tapio Pahikkala, and Tapio Salakoski. 2009. 
Extracting complex biological events with rich 
graph-based feature sets. In Proceedings of the Bi-
oNLP 2009 Shared Task Workshop at the Associa-
tion for Computational Linguistics Conference, pp. 
10?18. 
Jari Bj?rne, Filip Ginter, Tapio Salakoski: University 
of Turku in the BioNLP'11 Shared Task. BMC Bi-
oinformatics 13(S-11): S4 (2012) 
Fan R-E, Chang K-W, Hsieh C-J, Wang X-R, Lin C-
J. 2008. LIBLINEAR: A library for large linear 
classification. J Machine Learn Res 9:1871?1874. 
Miwa, M., Thompson, P., McNaught, J., Kell, D., 
Ananiadou, S. 2012. Extracting semantically en-
riched events from biomedical literature. BMC Bi-
oinformatics 13, 108. 
Makoto Miwa, Rune S?tre, Jin-Dong Kim, and 
Jun?ichi Tsujii. 2010. Event extraction with com-
plex event classification using rich features. Jour-
nal of Bioinformatics and Computational Biology 
(JBCB), 8(1):131?146. 
Earley, Jay (1970). An efficient context-free parsing 
algorithm. Communications of the ACM 13 (2): 
94?102 
Andreas Stolcke (1995). An efficient probabilistic 
context-free parsing algorithm that computes pre-
fix probabilities. Journal Computational Linguis-
tics (1995) Volume 21 Issue 2: 165-201 
Sampo Pyysalo, Tomoko Ohta and Sophia Anani-
adou. (2013). Overview of the Cancer Genetics 
(CG) task of BioNLP Shared Task 2013. Proceed-
ings of BioNLP Shared Task 2013 Workshop at the 
Association for Computational Linguistics Confer-
ence, (in press). 
Sampo Pyysalo, Tomoko Ohta, Makoto Miwa, Han-
Cheol Cho, Jun'ichi Tsujii and Sophia Ananiadou. 
(2012). Event extraction across multiple levels of 
biological organization. Bioinformatics, 
28(18):i575-i581. 
Haibin Liu, Lawrence Hunter, Vlado Ke?elj, Karin 
Verspoor (2013). Approximate Subgraph Match-
ing-Based Literature Mining for Biomedical Events 
and Relations. PLOS ONE, 2013. 
134
Proceedings of the 5th International Workshop on Health Text Mining and Information Analysis (Louhi) @ EACL 2014, pages 11?20,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
The impact of near domain transfer on biomedical named entity
recognition
Nigel Collier
?
European Bioinformatics Institute
Hinxton, Cambridge, UK, and
National Institute of Informatics, Tokyo, Japan
Ferdinand Paster
University of Applied Sciences
Upper Austria
Hagenberg Campus, Austria
Mai-vu Tran
University of Engineering and Technology - VNU
Hanoi, Vietnam
Abstract
Current research in fully supervised
biomedical named entity recognition
(bioNER) is often conducted in a setting
of low sample sizes. Whilst experi-
mental results show strong performance
in-domain it has been recognised that
quality suffers when models are applied to
heterogeneous text collections. However
the causal factors have until now been
uncertain. In this paper we describe a con-
trolled experiment into near domain bias
for two Medline corpora on hereditary
diseases. Five strategies are employed
for mitigating the impact of near domain
transference including simple transfer-
ence, pooling, stacking, class re-labeling
and feature augmentation. We measure
their effect on f-score performance against
an in domain baseline. Stacking and
feature augmentation mitigate f-score loss
but do not necessarily result in superior
performance except for selected classes.
Simple pooling of data across domains
failed to exploit size effects for most
classes. We conclude that we can expect
lower performance and higher annotation
costs if we do not adequately compensate
for the distributional dissimilarities of
domains during learning.
1 Introduction
Model and feature selection are important exper-
imental tasks in supervised machine learning for
suggesting approaches that will generalise well on
real world data. Research in biomedical named en-
tity recognition (bioNER) often displays two fea-
tures: (1) small samples of labeled data, and (2)
an implicit assumption that the future data will be
?
collier@ebi.ac.uk
drawn from a similar distribution to the labeled
data and hence that minimising expected predic-
tion error on held out data will minimise actual fu-
ture loss. Since expert labeling is time consuming
and expensive, labeled data sets tend to be rela-
tively small, e.g. (Kim et al., 2003; Tanabe et al.,
2005; Pyysalo et al., 2007), in the region of a few
hundred or thousand Medline abstracts. Despite
the danger of intrinsic idiosyncracies such corpora
are often used to demonstrate putative prediction
error across the heterogeneous collection of 22
million Medline abstracts. Once this assumption
is made explicit it is of interest to both researchers
and users that the implications and limitations of
such experimental settings are explored.
Cross domain studies have indicated an ad-
vantage for mechanisms that compensate for do-
main bias. For fully supervised learning, which
is the scenario we explore here, recent methods
include: feature augmentation (Daum?e III, 2007;
Arnold et al., 2008; McClosky et al., 2010), in-
stance weighting (Jiang and Zhai, 2007; Foster
et al., 2010), schema harmonisation (Wang et al.,
2010) and semi-supervised/lightly supervised ap-
proaches (Sagae and Tsujii, 2007; Liu et al., 2011;
Pan et al., 2013). More generally there is a wide
body of work in transfer learning (also known as
domain adaptation) that tries to handle discrep-
ancies between training and testing distributions
(Pan and Yang, 2010).
As an illustration of near domain bias consider
the list of high frequency named entities in Ta-
ble 1 drawn from two sub-domains in the research
literature of hereditary diseases. A domain ex-
pert in hereditary diseases would have no diffi-
culty in dividing them into two non-overlapping
sets corresponding to the two near domains with
one term t
5
patients shared by both: {t
1
,t
6
,t
8
,t
9
}
and {t
2
,t
3
,t
4
,t
7
,t
10
}.
Previous studies have shown what happens
when you radically change the domain and/or the
11
t1
rheumatoid t
6
human leukocyte
arthritis antigen
t
2
lupus t
7
coronary heart
erythematosus disease
t
3
leopard syndrome t
8
type 1 diabetes
t
4
Omapatrilat t
9
T1D
t
5
patients t
10
hypertension
Table 1: High frequency entities in the hereditory
disease literature for auto-immune and cardio-
vascular diseases.
annotation schema, e.g. from newswire to Med-
line or Web pages. But what happens when the
annotation schema, the annotator and the primary
domain stay the same? Although the notion of
domain is difficult to formalise in the context of
research literature, this study explores the con-
dition where the variable factor is a shift to a
near domain of literature as defined by biocura-
tors and illustrated in the previous example. Our
contribution to biomedical named entity recogni-
tion (bioNER) is in five areas:
1. We compare four data combination strate-
gies for mitigating the impact of near domain
transference and measure their effect on f-
score performance against an in domain base-
line.
2. We provide additional evidence for the effec-
tiveness of (Daum?e III, 2007)?s frustratingly
simple strategy which provides both general
and domain-specific features; in effect a joint
learning model.
3. Expectedly, but not trivially, we show that
a general loss of f-score occurs on bioNER
when transfering to near domains. This loss
is not uniform across all classes. We provide
class-by-class drill down analysis to the un-
derlying causal factors which make some en-
tities more robust to near domain transference
in biomedicine than others.
4. Our results challenge the notion that pool-
ing small corpora, even when guideline dif-
ferences are reconciled, leads to improved
f-score performance (Wang et al., 2010;
Wagholikar et al., 2013).
5. In addition to the usual biomedical entity
types we introduce the class of phenotypes
which are valued as indicators of genetic mal-
function and characteristic of diseases. The
phenotype class incorporates a complex de-
pendency between classes, notably anatomi-
cal entities and genes.
This paper is organised as follows: Section 2
describes related work in cross domain transfer
for biomedical NER, Section 3 discusses our ap-
proach including the two data sets used in our ex-
periments, CRF model, feature choices and evalu-
ation framework. In Section 4 we outline our ex-
perimental design. Finally in Section 5 we com-
pare the performance of six data selection strate-
gies that try to maximise f-score performance on
domain entity classes in the target corpus.
2 Related work
It is surprising that there exists, to the best of our
knowledge, no controlled study that has shed light
on the issue of near domain transfer for bioNER
in a straightforward manner. The closest approach
to our investigation in the biomedical domain is
(Wang et al., 2009). Wang et al. explore potential
sources of incompatibility across major bioNER
corpora with different annotation schema (GENIA
- 2000 Medline abstracts, GENETAG - approx-
imately 20,000 Medline sentences and AIMed -
225 Medline abstracts). They focus exclusively
on protein name recognition and observe a drop in
performance of 12% f-score when combining data
from different corpora. Various reasons are put
forwards such as differences in entity boundary
conventions, the scope of the entity class defini-
tions, distributional properties of the entity classes
and the degree of overlap between corpora.
A follow up study by the authors (Wang et
al., 2010) looked at increasing compatibility be-
tween the GENIA and GENETAG corpora by re-
organising the annotation schema to unify pro-
tein, DNA and RNA NER under a new label GGP
(Gene and Gene Product). However the best per-
formance from the coarse grained annotations still
do not improve on the intra-corpus data.
In earlier work, (Tsai et al., 2006) looked at
schema differences between the JNLPBA corpus
of 2000 Medline abstracts (Kim et al., 2004) and
the BioCreative corpus of 15,000 Medline sen-
tences (Yeh et al., 2005) and tried to harmonise
matching criteria. They demonstrated that relax-
ing the boundary matching criteria was helpful in
maximising the cross-domain performance.
12
In the clinical domain (Wagholikar et al.,
2013), explore the effect of harmonising annota-
tion guidelines on the 2010 i2b2 challenge with
Mayo Clinic Rochester (MCR) electronic patient
records. They concluded that the effectiveness of
pooling - i.e. merging of corpora by ensuring a
common format and harmonised semantics - is de-
pendent on several factors including compatibility
between the annotation schema and differences in
size. Again they noticed that simple pooling re-
sulted in a loss of f-score, 12% for MCR and 4%
for i2b2. They concluded that the asymmetry was
likely due to size effects of the corpora, i.e. MCR
being smaller suffered a greater loss due to the
classifier being biased towards i2b2.
Due to the formulation of these studies and their
limited scope it has previously been difficult to un-
derstand the precise causual factors affecting per-
formance. Our study sheds light on the expected
level of loss under different combination strategies
and more importantly highlights the non-uniform
nature of that loss.
3 Approach
We assume two small labeled data sets D
S
=
d
s
1
..d
s
n
and D
T
= d
t
1
..d
t
m
. d
s
i
= ?x
i
? X, y
i
?
Y ? is drawn from an unknown distribution P
s
and represents the source document examples.
Similarly,d
t
i
= ?x
i
? X, y
i
? Y ? is also drawn
from an unknown distribution P
t
and represents
the target document examples. We assume that
D
S
has N examples and D
T
has M examples
where N ? M . x
i
represents a covariate or fea-
ture vector and y
i
is a target or label that can take
multiple discrete values. We have a learning al-
gorithm that learns a function h : X ? Y with
minimal loss on the portion of D
T
used for test-
ing. Any combination of D
S
and D
T
which are
not used in testing can be used to learn h. Our task
is to explore various strategies for data selection
and re-factoring labels/features in order to max-
imise held out performance.
3.1 Data
In this paper we aim to empirically test domain
transferrence for bioNER under the condition that
the test and training data are relatively small and
drawn from near domains, i.e. from studies on
different types of heritable diseases. To do this
we selected Medline abstracts from PubMed that
were cited by biocuration experts in the canon-
ical database on heritable diseases, the Online
Mendelian Inheritance of Man (OMIM) (Hamosh
et al., 2005). We selected auto-immune diseases
and cardio-vascular diseases for our two corpora
which we denote as C1 and C2 respectively. By
comparing performance of a single model, a single
annotator and a single annotation scheme with a
range of sampling techniques we hope to quantify
the effects of domain transferrence in isolation.
The target classes for the entities are as follows:
ANA Anatomical structures in the body. e.g.
liver, heart.
CHE A chemical or drug. e.g. pristane, his-
tamine, S-nitrosoglutathione.
DIS Diseases. e.g. end stage renal disease, mitral
valve prolapse.
GGP Genes and gene products. e.g. KLKB1
gene, highly penetrant recessive major gene.
PHE Phenotype entities describing observable
and measurable characteristic of an organism.
e.g. cardiovascular abnormalities, abundant
ragged-red fibers, elevated IgE levels.
ORG A living organism. e.g.first-degree rela-
tives, mice.
The two corpora were annotated by a single
experienced annotator who had participated in
the GENIA entity and event corpus annotation.
We developed detailed guidelines for single span
none-nested entities before conducting a training
and feedback session. Feedback was conducted
over two weeks by email and direct meetings with
the annotator and then annotation took approxi-
mately two months. The characteristics of the two
corpora are shown in Table 2. Because annotation
was carried out by only one person we do not pro-
vide inter-annotator scores.
Importantly, we note four points at this stage:
(1) We incorporate a new named entity type, phe-
notype, which is aligned with investigations into
heritable diseases. Semantically it is interesting
because phenotypes annotated in the auto-immune
literature pertain more often to sub-cellular pro-
cesses and those in the cardiovascular domain per-
tain more often to cells, tissues and organs; (2)
It can be seen that two NE classes fall well be-
low 500 instances - what we might arbitarily con-
sider the necessary level of support for high lev-
els of performance. These are ANA and CHE;
13
C1 C2 a b
Abstracts 110 80 - -
Tokens 27,421 26,578 - -
Av. length 32.57 29.93 - -
ANA 194 195 0.33 0.26
(138) (133)
CHE 44 147 0.08 0.07
(33) (75)
DIS 892 955 0.39 0.27
(282) (442)
GGP 1663 754 0.41 0.45
(928) (511)
ORG 799 770 0.56 0.67
(429) (323)
PHE 507 1430 0.52 0.33
(423) (1113)
Table 2: Characteristics of the C1 auto-immune
and C2 cardiovascular corpora: number of ab-
stracts, number of tokens, average sentence length,
frequency of each entity type. Figures in parenthe-
ses represent counts after removing duplication. a:
probability that a word in an entity class X in C1
is also a word in entity class X in C2. b: probabil-
ity that a word in an entity class X in C2 is also a
word in entity class X in C1
(3) We calculated from Table 2 the average num-
ber of mentions for each entity form by class and
noted that this is relatively stable across corpora,
except for DIS which has less variation in C2 than
C1 and CHE which has more variation in C2 than
C1. When combining evidence from both cor-
pora the approximate order of type/token ratio are
PHE < ANA < CHE,GGP < ORG < DIS
indicating that on average PHE entities have the
greatest variation. Average entity lengths in to-
kens (not shown) indicate that PHE are signifi-
cantly longer than other entity mentions; and (4)
We calculated the probability that a word token in
an entity class from one corpus would appear in
an instance of the same entity class in the other
corpus, reported as columns a and b. Although the
probability of an exact match in instances between
entities in the two corpora is generally quite low
(below 20% - data not shown) there appears to be
significant vocabulary overlap in most classes ex-
cept for chemicals.
3.2 Conditional Random Fields
As in (Finkel and Manning, 2009) we apply our
approach to a linear chain conditional random field
(CRF) model (Lafferty et al., 2001; McCallum
and Wei, 2003; Settles, 2004; Doan et al., 2012)
using the Mallet toolkit
1
with default parameters.
CRFs have been shown consistently to be among
the highest performing bioNER learners. The data
selection strategies employed here though are neu-
tral and could have been applied to any other fully
supervised learner model.
3.3 Features
We made use of a wide range of features, both
conventional features such as word or part of
speech, as well as gazetteers derived from ex-
ternal classification schemes that have been hand
crafted by experts. These are shown in Ta-
ble 3. Previous studies such as (Ratinov and
Roth, 2009) have noted that domain gazetteer
features play a critical role in aiding classifi-
cation. In order to show realistic model be-
haviour consistent with state-of-the-art techniques
we have included gazetteers derived from: the Hu-
man Phenotype Ontology (HPO: 15,800 terms),
the Mammalian Phenotype Ontology (MP: 23,700
terms), the Phenotypic Attribute and Trait On-
tology (PATO: 2,200 synonyms), the Brenda
Tissue Ontology (BTO: 9,600 synonyms), the
Foundation Model of Anatomy (FMA: 120,000
terms), National Library of Medicine gene list
(NLM: 9 million terms), UMLS disease terms
(UMLS: 275,000 terms), Jochem chemical terms
(JOCHEM: 320,000 terms).
The feature set is quite large and therefore there
is a danger that the learner will be hindered. For
feature selection, we conducted baseline test runs
under the same experimental conditions as those
reported here using a grid search on features F1
to F11 and found that f-score performance was
uniformly lower when removing any feature (data
not shown but available as supplementary material
from the first author).
In order to characterise the contribution each
feature is making in label prediction we wanted to
provide a measure of similarity between the fea-
ture and the class label probability distributions.
Here we use the Gain Ratio (GR) to estimate intra-
corpus class prediction performance by each fea-
ture. GR was used as a splitting function in C4.5
1
http://mallet.cs.umass.edu/
14
(Quinlan, 1993) and is defined as
GR(C,F ) = IG(C,F )/H(F ) (1)
where C represents a class label and F repre-
sents a feature type. IG is information gain and
defined as,
IG(C,F ) = H(C)?H(C|X) (2)
H is entropy and defined for feature types as,
H(F ) = ?
n
?
i=1
p(f
i
)log
2
(p(f
i
)) (3)
for n feature types f
i
? F . Further informa-
tion can be found in (Quinlan, 1993). GR is used
in C4.5 in preference to IG because of its ability
to normalise for the biases in IG. Generally this
results in GR having greater predictive accuracy
than IR since it takes into account the number of
feature values. Note that GR is undefined when
the denominator is zero.
Several points emerge from looking at GR and
IG values in Table 3:
? C1 (auto-immune) and C2 (cardio-vascular)
have about the same information gain con-
tribution from most features but C1 seems
to benefit more from GENIA named entity
tagging, Human Phenotype Ontology (HPO),
Foundation Model of Anatomy (FMA) and
Gene Ontology (GO) terms whereas C2 ben-
efits more from the UMLS diseases and
ChEBI terms.
? GO, containing terms about genetic pro-
cesses, has a higher GR in C1 than C2. This
supports what we already expected - that
auto-immune diseases contain a higher pro-
portion of information about genetic process
phenotypes than cardiovascular.
? The GENIA POS tags seem to provide a
slightly higher GR in C2 than in C1.
? Despite its large size, UMLS has a smaller
GR on both corpora compared to some other
resources like HPO or GO or MA. This is de-
spite its high IG value.
3.4 Evaluation
Traditional re-sampling using k-fold cross valida-
tion (k-CV) divides the n labelled documents into
k disjoint subsets of approximately equal size des-
ignated as D
i
for i = 1, .., k. The NER learner
is trained successively on k ? 1 folds from D and
tested on a held out fold over k iterations. In or-
der to preserve independence between contexts in
training and held out data we assume here that the
unit of division is the document, i.e. a single Med-
line abstract. Estimated prediction error is calcu-
lated based on the learner?s labels on the k held
out folds. Whilst k-CV is known to be nearly un-
biased it is a highly variable estimator. Several
studies have looked at k-CV for small sample sets.
For example, (Braga-Neto and Dougherty, 2004)
found on classifier experiments for small microar-
ray samples (20 <= n <= 120) that whilst k-
CV showed low bias they suffered from excessive
variance compared to bootstrap or resubstitution
estimators.
One cause of variance has been identified as
within-block and between-block training errors
arising from the disproportionate effects of a sin-
gle abstract appearing in the training set of many
folds. In order to reduce this effect Monte Carlo
cross validation was used (also called CV with rep-
etition). 100 iterations were used to randomly re-
order the documents in the corpora before 10-fold
CV sampling was run (cv10r100). Sampling of
documents is done without replacement so that the
independence between training and testing sets are
maintained. Stratification was not applied. Mi-
cro averaged f-scores for labeling accuracy were
calculated based on the 1000 test folds for each
model. Evaluation was done in both directions
(training and testing) for each corpus C1 and C2
to show any asymmetrical effects. To minimse the
time taken for each experiment a cluster computer
was used with 48 nodes.
The matching criteria we employ is the exact
match - i.e. the span of the system labeling and
the held out data labels should be exactly the same.
Although this is not a necessary criteria for some
applications such as database curation we used it
here as it is widely applied in shared evaluations
and shows the clearest effects of modeling choice.
We evaluate using the named entity precision,
recall and F-score calculated using the CoNLL
2003 Perl script. This was calculated as,
f ? score =
(2? precision? recall)
(precision + recall)
(4)
where,
15
Feature IG(C1, F
i
) GR(C1, F
i
) IG(C2, F
i
) GR(C2, F
i
)
F
1
Word 1.17 0.13 1.20 0.13
F
2
Lemma 1.15 0.13 1.18 0.13
F
3
POS tag 0.36 0.09 1.18 0.13
F
4
Chunk tag 0.22 0.12 0.26 0.10
F
5
GENIA NE
a
0.20 0.35 0.14 0.27
F
6
Orthography 0.15 0.08 0.16 0.08
F
7
Domain prefix 0.11 0.11 0.11 0.10
F
8
Domain suffix 0.08 0.11 0.08 0.11
F
9
Word length 0.13 0.05 0.16 0.06
F
10
Parenthesis 0.04 0.20 0.04 0.23
F
11
Abbreviation 0.08 0.22 0.06 0.24
F
12
HPO
b
0.07 0.41 0.09 0.33
F
13
MP
c
0.03 0.33 0.06 0.33
F
14
PATO
d
0.01 0.03 0.02 0.04
F
15
BTO
e
0.03 0.32 0.03 0.29
F
16
FMA
f
0.05 0.28 0.05 0.23
F
17
MA
g
0.02 0.31 0.02 0.29
F
18
PRO
h
0.02 0.12 0.03 0.15
F
19
ChEBI
i
0.01 0.15 0.03 0.20
F
20
JOCHEM
j
0.01 0.15 0.01 0.14
F
21
NCBI
k
0.01 0.14 0.01 0.14
F
22
UMLS
l
disease 0.01 0.14 0.03 0.24
F
23
NCBI gene 0.02 0.18 0.02 0.19
F
24
GO
m
0.13 0.38 0.05 0.28
F
25
UMLS
n
0.48 0.12 0.52 0.11
F
26
45CLUSTERS
o
0.50 0.10 0.47 0.10
Table 3: Features used in the experiments.
a
The GENIA named entity tagger (Kim et al., 2003),
b
(Robinson et al., 2008),
c
(Smith et al., 2004),
d
(Gkoutos et al., 2005),
e
(Gremse et al., 2011),
f
(Rosse
andMejino, 2003),
g
(Hayamizu et al., 2005),
h
(Natale et al., 2011) ,
i
(Degtyarenko et al., 2008),
j
(Hettne
et al., 2009),
k
(Federhen, 2012),
l
(Lindberg et al., 1993),
m
(Gene Ontology Consortium, 2000),
n
133 cat-
egories from the UMLS,
o
45 cluster classes derived by Richard Socher and Christoph Manning PubMed
available at http://nlp.stanford.edu/software/bionlp2011-distsim-clusters-v1.tar.gz
precision = TP/(TP + FP ) (5)
and,
recall = TP/(TP + FN) (6)
A true positive (TP) is a gold standard NE
tagged by the system as an NE. A true negative
(TN) is a gold standard none-NE tagged by the
system as a none-NE. A false positive (FP) is a
gold standard none-NE tagged by the system as
an NE. Evaluation is based on correctly marked
whole entities rather than tokens.
4 Experimental design
In this section we present the experimental condi-
tions we used, starting with a description of the
models which we designate M1 to M6 and de-
scribe below. All methods made use of 100 iter-
ations of Monte Carlo 10-fold cross validation.
M1: IN DOMAIN We trained and tested on only
the data for the source domain. This methods
forms our baseline and represents the stan-
dard experimental setting.
M2: OUT DOMAIN We trained on the source
domain and tested on the target domain. This
method shows expected loss on near domain
transferrence and represents the standard op-
erational setting for users.
16
M3: MIX-IN We trained on 100% of the source
domain and unified this with 90% of the
folded in target domain data, leaving 10%
for testing. This method reflects the pooling
technique typically employed in corpus con-
struction for bioNER.
M4: STACK We trained a CRF model on 100%
of the source domain and stacked it with
another CRF trained on 90% of the folded
in target domain data. Stacking employs a
meta-classifier and is a popular method for
constructing high performance ensembles of
classifiers (Ekbal and Saha, 2013). In this
case we collected the output labels from the
source domain-trained CRF on target sen-
tences and added them as features for the tar-
get domain trained CRF.
M5: BINARY CLASS We re-labeled the com-
plex class PHE as PHE-C1 in C1 and PHE-
C2 in C2 and repeated M3. Afterwards we
recombined PHE-C1 and PHE-C2 into PHE.
M6: FRUSTRATINGLY SIMPLE We fol-
lowed the feature augmentation approach of
(Daum?e III, 2007). This method effectively
provides a joint learning model on C1 and
C2 by splitting each feature into three parts:
one for sharing cross domain values and one
for each domain specific value. We evaluated
using the same regime as M3.
5 Experimental results and discussion
In Table 4 we show f-score performance from near
biomedical domains with our six strategies. This
section now tries to draw together an interpretation
for the performance trends that we see and to drill
down to some of the causal factors.
Held out tests performed in-domain (M1) on
both corpora C1 and C2 indicate a relatively high
level of performance, conservatively in line with
state-of-the-art estimates. The broad trend in per-
formance is for entity classes with more instances
to out perform others with lower numbers. The
class which most obviously breaks this trend is
the complex entity type of PHE. To understand
this consider that PHE is defined as an observable
property on an organism and as such tends to be
formed from a quality such as malformed that de-
scribes a structural entity such as valve. To see
closer what is happening we looked at the confu-
sion matrices for M1 on both corpora. For both
C1 and C2 we observed that a substantial pro-
portion of words inside PHE sequences were con-
fused with GGP, DIS or ANA entities. Similarly
a high proportion of words inside ANA sequences
were confused with PHE entities. This indicates
that dependencies within complex biomedical en-
tities like PHE might better be modeled explicitly
using tree-structures in a manner similar to events
rather than using n-gram relations.
In the M2 out of domain experiments we see
a generally severe loss of f-score performance
across most classes. Training on C2 and testing
on C1 results in a 19.1% loss (F1 69.9 to 50.8)
and training on C1 and testing on C2 results in
a 11.9% loss overall (F1 58.5 to 46.6). The re-
sults agree with Wang et al.?s experience on het-
erogeneous Medline corpora and extend the upper
limit on all-class loss due to domain transferrence
to 19%. The only NE class where we see a sym-
metric benefit from pooling entities in M3 is for
ORG (F1 68.4 to 72.2, F1 73.2 to 77.4). Intrigu-
ingly the data from Tables 2 and 4 hint at a correla-
tion between the success of M3 pooling for ORG
and broad cross-domain compatibility on the vo-
cabulary (over 50% of ORG vocabulary is shared
across corpora). However this is not supported
in the low sharing case for CHE where we see
increased performance from pooling (F1 31.3 to
38.7) when the target is C2 but decreased perfor-
mance when the target is C1 (F1 29.5 to 20.0).
When we look at the pooling method (M3) and
compare to the in-domain method (M1) no obvi-
ous size effect occurs for the number of entities
in each class. To see this we can examine entity
classes with an imbalanced number of instances
in C1 and C2 such as CHE, GGP and PHE. Con-
sider the following three cases: (1) Adding 147
instances of CHE from C2 to 44 instances from
C1 is associated with CHE performance dropping
from M1:29.5 to M3:20.0 when tested on C1; (2)
Similarly adding 1430 instances of PHE from C2
to 507 instances from C1 is associated with PHE
performance dropping from 46.0 in M1 to 39.7 in
M3 when tested on C1; (3) But adding 1663 in-
stances of GGP from C1 to 754 from C2 is asso-
ciated with GGP rising from 57.2 in M1 to 61.1 in
M3. If simply pooling more entities was impor-
tant to improved f-score we would expect to see a
clearer pattern of improvement but we do not.
The overall pooling loss for all classes on M3
is within 3% in both directions and within the
17
Model Target ANA CHE DIS GGP PHE ORG ALL
M1 C1 57.1 29.5 80.4 74.0 46.0 68.4 69.9
M2 C1 34.3 26.9 57.7 55.6 26.9 64.0 50.8
M3 C1 50.8 20.0 77.9 71.7 39.7 72.2 67.3
M4 C1 56.3 17.4 79.0 74.1 44.1 70.8 69.8
M5 C1 56.7 29.6 77.3 72.7 41.5 72.8 68.3
M6 C1 57.1 27.7 79.0 73.4 44.9 69.9 69.5
M1 C2 37.2 31.3 72.9 57.2 46.5 73.2 58.5
M2 C2 21.2 20.2 57.0 52.3 24.4 68.5 46.6
M3 C2 36.8 38.7 72.3 61.1 44.0 77.4 59.7
M4 C2 34.8 34.4 72.5 57.5 45.9 74.7 58.5
M5 C2 34.1 41.6 73.6 58.9 43.2 78.5 59.6
M6 C2 39.9 35.0 73.3 56.4 46.6 75.0 59.1
Table 4: Named entity recognition f-scores using Methods 1 to 6. All methods were tested using 100
iterations of Monte Carlo 10-fold cross validation. Figures in bold show best in class scores. Figures in
italics show scores above the M1 baseline.
bounds observed by (Wang et al., 2009) and
(Wagholikar et al., 2013) for their pooling of het-
erogeneous Medline corpora. Except for the ORG
class which we higlighted above, we might cau-
tiously quantify the loss of pooled entity mentions
as being in the range up to 9.5% for CHE but more
typically below 4%. The majority of the differ-
ences they observed - which are not present in our
data - are most likely due to concept definition dif-
ferences and annotation conventions.
In contrast to our expectations the M4 experi-
ments showed very mild benefits for stacking and
these were mixed across entity types. M4 tests
on C2 showed no general improvement but some
improvement in CHE and ORG. M4 tests on C1
resulted again in no overall improvement except
for some gain for ORG, supporting our hypothesis
that there is greater compatibility in ORG across
domains.
The M5 approach of splitting the PHE labels for
the two corpora resulted in a noticable improve-
ment over M3 on the C1 test but unfortunately this
was not sustained when testing on C2.
It is striking that in the M6 experiments the fea-
ture augmentation method only just meets the in-
domain f-score on C1 and mildly exceeds it on C2.
One explanation is that the corpora are so small
that a richer feature set has only marginal effects
on performance. Table 3 certainly indicates that
many of the features have low predictive capac-
ity (gain ratio values below 0.1) in an intra-corpus
setting but this is not the case for others such as
GENIA NE tags or HPO gazzetteer terms.
Overall when we average the f-scores across
models for C1 and C2 we see that there is a
marginal benefit to the M1, M4 and M6 strategies
over M3 and M5 with M2 suffering the greatest
loss in performance.
6 Conclusion
In this paper we have provided evidence that trans-
ference even to closely related domains in biomed-
ical NER incurs a severe loss in f-score. We
have demonstrated empirically that strategies that
make use of multi-domain corpora such as stack-
ing learners and feature augmentation mitigate the
accuracy loss but do not necessarily result in supe-
rior performance except for selected classes such
as organisms where there appears to be broad
terminology consensus. Simple pooling of data
across domains failed to exploit size effects espe-
cially for the complex class of phenotypes. The
list of strategies employed has not been exhaus-
tive and it is possible that others such as feature
hierarchies (Arnold et al., 2008) might yield better
results.
BioNER is complicated by various factors such
as descriptive names, polysemous terms, conjuc-
tions, nested constructions and a high quantity of
abbreviations. We have shown that performance is
also held back by not considering document level
properties related to domain such as topicality. We
can expect lower performance and higher annota-
tion costs if we do not adequately allow for the dis-
tributional dissimilarities of domains during learn-
ing, even in closely related topical settings.
18
Acknowledgments
The authors gratefully acknowledge the many
helpful comments from the anonymous review-
ers of this paper. Nigel Collier?s research is
supported by the European Commission through
the Marie Curie International Incoming Fellow-
ship (IIF) programme (Project: Phenominer, Ref:
301806).
References
A. Arnold, N. Nallapati, and W. Cohen. 2008. Exploit-
ing feature hierarchy for transfer learning in named
entity recognition. In Annual meeting of the Asso-
ciation for Computational Linguistics (ACL 2008),
pages 245?253.
U. Braga-Neto and E. Dougherty. 2004. Is cross-
validation valid for small-sample microarray classi-
fication? Bioinformatics, 20(3):374?380.
H. Daum?e III. 2007. Frustratingly easy domain adap-
tation. In Annual meeting of the Association for
Computational Linguistics (ACL 2007), pages 256?
263.
K. Degtyarenko, P. de Matos, M. Ennis, J. Hastings,
M. Zbinden, A. McNaught, R. Alc?antara, M. Dar-
sow, M. Guedj, and M. Ashburner. 2008. ChEBI:
a database and ontology for chemical entities of bi-
ological interest. Nucleic acids research, 36(suppl
1):D344?D350.
S. Doan, N. Collier, H. Xu, P. Duy, and T. Phuong.
2012. Recognition of medication information from
discharge summaries using ensembles of classifiers.
BMC Medical Informatics and Decision Making,
12(1):36.
A. Ekbal and S. Saha. 2013. Stacked ensemble cou-
pled with feature selection for biomedical entity ex-
traction. Knowledge-Based Systems.
S. Federhen. 2012. The NCBI taxonomy database.
Nucleic acids research, 40(D1):D136?D143.
J. Finkel and C. Manning. 2009. Hierarchical bayesian
domain adaptation. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, pages 602?610.
G. Foster, C. Goutte, and R. Kuhn. 2010. Discrim-
inative instance weighting for domain adaptation in
statistical machine translation. In Proceedings of the
2010 Conference on Empirical Methods in Natural
Language Processing (EMNLP 2010), pages 451?
459.
Gene Ontology Consortium. 2000. Gene ontology:
tool for the unification of biology. Nature Genetics,
25:19?29.
G. Gkoutos, E. Green, A. Mallon, J. Hancock, and
D. Davidson. 2005. Using ontologies to describe
mouse phenotypes. Genome Biology, 6:R8.
M. Gremse, A. Chang, I. Schomburg, A. Grote,
M. Scheer, C. Ebeling, and D. Schomburg. 2011.
The BRENDA tissue ontology (BTO): the first
all-integrating ontology of all organisms for en-
zyme sources. Nucleic Acids Research, 39(suppl
1):D507?D513.
A. Hamosh, A. F. Scott, J. S. Amberger, and C. A. Boc-
chini. 2005. Online mendelian inheritance of man
(OMIM), a knowledgebase of human genes and ge-
netic disorders. Nucleic Acids Research, 33(suppl
1):D514?D517.
T. Hayamizu, M. Mangan, J. Corradi, J. Kadin,
M. Ringwald, et al. 2005. The adult mouse anatom-
ical dictionary: a tool for annotating and integrating
data. Genome Biol, 6(3):R29.
K. Hettne, R. Stierum, M. Schuemie, P. Hendriksen,
B. Schijvenaars, E. van Mulligen, J. Kleinjans, and
J. Kors. 2009. A dictionary to identify small
molecules and drugs in free text. Bioinformatics,
25(22):2983?2991.
J. Jiang and C. Zhai. 2007. Instance weighting for
domain adaptation in NLP. In Annual meeting of
the Association for Computational Linguistics (ACL
2007), volume 2007, page 22.
J. D. Kim, T. Ohta, Y. Tateishi, and J. Tsujii. 2003.
GENIA corpus - a semantically annotated corpus for
bio-textmining. Bioinformatics, 19(Suppl.1):180?
182.
J. Kim, T. Ohta, Y. Tsuruoka, Y. Tateisi, and N. Col-
lier. 2004. Introduction to the bio-entity recog-
nition task at JNLPBA. In N. Collier, P. Ruch,
and A. Nazarenko, editors, Proceedings of the In-
ternational Joint Workshop on Natural Language
Processing in Biomedicine and its Applications
(JNLPBA), Geneva, Switzerland, pages 70?75, Au-
gust 28?29. held in conjunction with COL-
ING?2004.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: probabilistic models for seg-
menting and labeling sequence data. In Proceed-
ings of the Eighteenth International Conference on
Machine Learning, Massachusetts, USA, pages 282?
289, June 28th ? July 1st.
Donald A.B. Lindberg, L. Humphreys, Betsy, and
T. McCray, Alexa. 1993. The unified medical lan-
guage system. Methods of Information in Medicine,
32:281?291.
Xiaohua Liu, Shaodian Zhang, Furu Wei, and Ming
Zhou. 2011. Recognizing named entities in tweets.
In Annual meeting of the Association for Computa-
tional Linguistics (ACL 2011), pages 359?367.
19
A. McCallum and L. Wei. 2003. Early results for
named entity recognition with conditional random
fields, feature induction and web-enhanced lexicons.
In Proc. Seventh Conference on Natural language
learning at HLT-NAACL 2003 - Volume 4, CONLL
?03, pages 188?191.
D.McClosky, E. Charniak, andM. Johnson. 2010. Au-
tomatic domain adaptation for parsing. In Human
Language Technologies: The 2010 Annual Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics, pages 28?36.
Association for Computational Linguistics.
D. Natale, C. Arighi, W. Barker, J. Blake, C. Bult,
M. Caudy, H. Drabkin, P. DEustachio, A. Evsikov,
H. Huang, et al. 2011. The protein ontology: a
structured representation of protein forms and com-
plexes. Nucleic acids research, 39(suppl 1):D539?
D545.
S. Pan and Q. Yang. 2010. A survey on transfer learn-
ing. Knowledge and Data Engineering, IEEE Trans-
actions on, 22(10):1345?1359.
S. Pan, Z. Toh, and J. Su. 2013. Transfer joint em-
bedding for cross-domain named entity recognition.
ACM Transactions on Information Systems (TOIS),
31(2):7.
S. Pyysalo, F. Ginter, J. Heimonen, J. Bj?orne,
J. Boberg, J. J?arvinen, and T. Salakoski. 2007.
Bioinfer: a corpus for information extraction in the
biomedical domain. BMC bioinformatics, 8(1):50.
J. Quinlan. 1993. C4. 5: programs for machine learn-
ing, volume 1. Morgan kaufmann.
L. Ratinov and D. Roth. 2009. Design challenges and
misconceptions in named entity recognition. In Pro-
ceedings of the Thirteenth Conference on Computa-
tional Natural Language Learning (CoNLL), pages
147?155.
P. N. Robinson, S. Kohler, S. Bauer, D. Seelow,
D. Horn, and S. Mundlos. 2008. The human pheno-
type ontology: a tool for annotating and analyzing
human hereditary disease. The American Journal of
Human Genetics, 83(5):610?615.
C. Rosse and J. L. V. Mejino. 2003. A reference on-
tology for bioinformatics: the Foundational Model
of Anatomy. Journal of Biomedical Informatics,
36(6):478?500, December. PMID: 14759820.
K. Sagae and J. Tsujii. 2007. Dependency parsing
and domain adaptation with lr models and parser en-
sembles. In Conference on Empirical Methods in
Natural Language Processing Conference on Com-
putational Natural Language Learning (EMNLP-
CoNLL), volume 2007, pages 1044?1050.
B. Settles. 2004. Biomedical named entity recognition
using conditional random fields. In Proceedings of
the International Joint Workshop on Natural Lan-
guage Processing in Biomedicine and its Applica-
tions (JNLPBA) at COLING?2004, Geneva, Switzer-
land, pages 104?107, August 28?29.
C. L. Smith, C. W. Goldsmith, and J. T. Eppig. 2004.
The mammalian phenotype ontology as a tool for an-
notating, analyzing and comparing phenotypic infor-
mation. Genome Biology, 6:R7.
L. Tanabe, N. Xie, L. H. Thom, W. Matten, and W. J.
Wilbur. 2005. GENETAG: a tagged corpus for
gene/protein named entity recognition. BMC Bioin-
formatics, 6(Suppl 1):S3.
R. Tsai, S. Wu, W. Chou, Y. Lin, D. He, J. Hsiang,
T. Sung, and W. Hsu. 2006. Various criteria in the
evaluation of biomedical named entity recognition.
BMC bioinformatics, 7(1):92.
K. Wagholikar, M. Torii, S. Jonnalagadda, H. Liu, et al.
2013. Pooling annotated corpora for clinical con-
cept extraction. J. Biomedical Semantics, 4:3.
Y. Wang, J. Kim, R. S?tre, S. Pyysalo, and J. Tsujii.
2009. Investigating heterogeneous protein annota-
tions toward cross-corpora utilization. BMC bioin-
formatics, 10(1):403.
Y. Wang, J. Kim, R. S?tre, S Pyysalo, T. Ohta, and
J. Tsujii. 2010. Improving the inter-corpora com-
patibility for protein annotations. Journal of bioin-
formatics and computational biology, 8(05):901?
916.
A. Yeh, A. Morgan, M. Colosimo, and L. Hirschman.
2005. Biocreative task 1a: gene mention finding
evaluation. BMC bioinformatics, 6(Suppl 1):S2.
20
