Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 648?653,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Comparative News Summarization Using Linear Programming
Xiaojiang Huang Xiaojun Wan? Jianguo Xiao
Institute of Computer Science and Technology, Peking University, Beijing 100871, China
Key Laboratory of Computational Linguistic (Peking University), MOE, China
{huangxiaojiang, wanxiaojun, xiaojianguo}@icst.pku.edu.cn
Abstract
Comparative News Summarization aims to
highlight the commonalities and differences
between two comparable news topics. In
this study, we propose a novel approach to
generating comparative news summaries. We
formulate the task as an optimization problem
of selecting proper sentences to maximize the
comparativeness within the summary and the
representativeness to both news topics. We
consider semantic-related cross-topic concept
pairs as comparative evidences, and con-
sider topic-related concepts as representative
evidences. The optimization problem is
addressed by using a linear programming
model. The experimental results demonstrate
the effectiveness of our proposed model.
1 Introduction
Comparative News Summarization aims to highlight
the commonalities and differences between two
comparable news topics. It can help users to analyze
trends, draw lessons from the past, and gain insights
about similar situations. For example, by comparing
the information about mining accidents in Chile and
China, we can discover what leads to the different
endings and how to avoid those tragedies.
Comparative text mining has drawn much atten-
tion in recent years. The proposed works differ
in the domain of corpus, the source of comparison
and the representing form of results. So far, most
researches focus on comparing review opinions of
products (Liu et al, 2005; Jindal and Liu, 2006a;
?Corresponding author
Jindal and Liu, 2006b; Lerman and McDonald,
2009; Kim and Zhai, 2009). A reason is that the
aspects in reviews are easy to be extracted and the
comparisons have simple patterns, e.g. positive
vs. negative. A few other works have also
tried to compare facts and views in news article
(Zhai et al, 2004) and Blogs (Wang et al, 2009).
The comparative information can be extracted from
explicit comparative sentences (Jindal and Liu,
2006a; Jindal and Liu, 2006b; Huang et al, 2008),
or mined implicitly by matching up features of
objects in the same aspects (Zhai et al, 2004; Liu
et al, 2005; Kim and Zhai, 2009; Sun et al,
2006). The comparisons can be represented by
charts (Liu et al, 2005), word clusters (Zhai et al,
2004), key phrases(Sun et al, 2006), and summaries
which consist of pairs of sentences or text sections
(Kim and Zhai, 2009; Lerman and McDonald,
2009; Wang et al, 2009). Among these forms,
the comparative summary conveys rich information
with good readability, so it keeps attracting interest
in the research community. In general, document
summarization can be performed by extraction or
abstraction (Mani, 2001). Due to the difficulty
of natural sentence generation, most automatic
summarization systems are extraction-based. They
select salient sentences to maximize the objective
functions of generated summaries (Carbonell and
Goldstein, 1998; McDonald, 2007; Lerman and
McDonald, 2009; Kim and Zhai, 2009; Gillick et al,
2009). The major difference between the traditional
summarization task and the comparative summa-
rization task is that traditional summarization task
places equal emphasis on all kinds of information in
648
the source, while comparative summarization task
only focuses on the comparisons between objects.
News is one of the most important channels for
acquiring information. However, it is more difficult
to extract comparisons in news articles than in
reviews. The aspects are much diverse in news.
They can be the time of the events, the person
involved, the attitudes of participants, etc. These
aspects can be expressed explicitly or implicitly in
many ways. For example, ?storm? and ?rain? both
talk about ?weather?, and thus they can form a
potential comparison. All these issues raise great
challenges to comparative summarization in the
news domain.
In this study, we propose a novel approach for
comparative news summarization. We consider
comparativeness and representativeness as well as
redundancy in an objective function, and solve the
optimization problem by using linear programming
to extract proper comparable sentences. More
specifically, we consider a pair of sentences
comparative if they share comparative concepts;
we also consider a sentence representative if it
contains important concepts about the topic. Thus
a good comparative summary contains important
comparative pairs, as well as important concepts
about individual topics. Experimental results
demonstrate the effectiveness of our model, which
outperforms the baseline systems in quality of
comparison identification and summarization.
2 Problem Definition
2.1 Comparison
A comparison identifies the commonalities or
differences among objects. It basically consists
of four components: the comparee (i.e. what is
compared), the standard (i.e. to what the compare
is compared), the aspect (i.e. the scale on which
the comparee and standard are measured), and the
result (i.e. the predicate that describes the positions
of the comparee and standard). For example, ?Chile
is richer than Haiti.? is a typical comparison, where
the comparee is ?Chile?; the standard is ?Haiti?; the
comparative aspect is wealth, which is implied by
?richer?; and the result is that Chile is superior to
Haiti.
A comparison can be expressed explicitly in a
comparative sentence, or be described implicitly
in a section of text which describes the individual
characteristics of each object point-by-point. For
example, the following text
Haiti is an extremely poor country.
Chile is a rich country.
also suggests that Chile is richer than Haiti.
2.2 Comparative News Summarization
The task of comparative news summarization is to
briefly sum up the commonalities and differences
between two comparable news topics by using
human readable sentences. The summarization
system is given two collections of news articles,
each of which is related to a topic. The system
should find latent comparative aspects, and generate
descriptions of those aspects in a pairwise way, i.e.
including descriptions of two topics simultaneously
in each aspect. For example, when comparing
the earthquake in Haiti with the one in Chile,
the summary should contain the intensity of each
temblor, the damages in each disaster area, the
reactions of each government, etc.
Formally, let t1 and t2 be two comparable news
topics, and D1 and D2 be two collections of
articles about each topic respectively. The task of
comparative summarization is to generate a short
abstract which conveys the important comparisons
{< t1, t2, r1i, r2i >}, where r1i and r2i are
descriptions about topic t1 and t2 in the same
latent aspect ai respectively. The summary can be
considered as a combination of two components,
each of which is related to a news topic. It can also
be subdivided into several sections, each of which
focuses on a major aspect. The comparisons should
have good quality, i.e., be clear and representative to
both topics. The coverage of comparisons should be
as wide as possible, which means the aspects should
not be redundant because of the length limit.
3 Proposed Approach
It is natural to select the explicit comparative
sentences as comparative summary, because they
express comparison explicitly in good qualities.
However, they do not appear frequently in regular
news articles so that the coverage is limited. Instead,
649
it is more feasible to extract individual descriptions
of each topic over the same aspects and then
generate comparisons.
To discover latent comparative aspects, we
consider a sentence as a bag of concepts, each of
which has an atom meaning. If two sentences have
same concepts in common, they are likely to discuss
the same aspect and thus they may be comparable
with each other. For example,
Lionel Messi named FIFA Word Player of
the Year 2010.
Cristiano Ronalo Crowned FIFA Word
Player of the Year 2009.
The two sentences compare on the ?FIFA Word
Player of the Year?, which is contained in both
sentences. Furthermore, semantic related concepts
can also represent comparisons. For example,
?snow? and ?sunny? can indicate a comparison
on ?weather?; ?alive? and ?death? can imply a
comparison on ?rescue result?. Thus the pairs
of semantic related concepts can be considered as
evidences of comparisons.
A comparative summary should contain as many
comparative evidences as possible. Besides, it
should convey important information in the original
documents. Since we model the text with a
collection of concept units, the summary should
contain as many important concepts as possible.
An important concept is likely to be mentioned
frequently in the documents, and thus we use the
frequency as a measure of a concept?s importance.
Obviously, the more accurate the extracted
concepts are, the better we can represent the
meaning of a text. However, it is not easy to extract
semantic concepts accurately. In this study, we
use words, named entities and bigrams to simply
represent concepts, and leave the more complex
concept extraction for future work.
Based on the above ideas, we can formulate
the summarization task as an optimization problem.
Formally, letCi = {cij} be the set of concepts in the
document set Di, (i = 1, 2). Each concept cij has a
weight wij ? R. ocij ? {0, 1} is a binary variable
indicating whether the concept cij is presented in the
summary. A cross-topic concept pair < c1j , c2k >
has a weight ujk ? R that indicates whether it
implies a important comparison. opjk is a binary
variable indicating whether the pair is presented in
the summary. Then the objective function score of a
comparative summary can be estimated as follows:
?
|C1|
?
j=1
|C2|
?
k=1
ujk ?opjk +(1??)
2
?
i=1
|Ci|
?
j=1
wij ?ocij (1)
The first component of the function estimates the
comparativeness within the summary and the second
component estimates the representativeness to both
topics. ? ? [0, 1] is a factor that balances these two
factors. In this study, we set ? = 0.55.
The weights of concepts are calculated as follows:
wij = tfij ? idfij (2)
where tfij is the term frequency of the concept cij
in the document set Di, and idfij is the inverse
document frequency calculated over a background
corpus.
The weights of concept pairs are calculated as
follows:
ujk =
{
(w1j + w2k)/2, if rel(c1j , c2k) > ?
0, otherwise
(3)
where rel(c1j , c2k) is the semantic relevance be-
tween two concepts, and it is calculated using the
algorithms basing on WordNet (Pedersen et al,
2004). If the relevance is higher than the threshold
? (0.2 in this study), then the concept pair is
considered as an evidence of comparison.
Note that a concept pair will not be presented in
the summary unless both the concepts are presented,
i.e.
opjk ? oc1j (4)
opjk ? oc2k (5)
In order to avoid bias towards the concepts which
have more related concepts, we only count the most
important relation of each concept, i.e.
?
k
opjk ? 1, ?j (6)
?
j
opjk ? 1, ?k (7)
The algorithm selects proper sentences to max-
imize the objective function. Formally, let Si =
650
{sik} be the set of sentences in Di, ocsijk be
a binary variable indicating whether concept cij
occurs in sentence sik, and osik be a binary variable
indicating whether sik is presented in the summary.
If sik is selected in the summary, then all the
concepts in it are presented in the summary, i.e.
ocij ? ocsijk ? osik, ?1 ? j ? |Ci| (8)
Meanwhile, a concept will not be present in the
summary unless it is contained in some selected
sentences, i.e.
ocij ?
|Si|
?
k=1
ocsijk ? osik (9)
Finally, the summary should satisfy a length
constraint:
2
?
i=1
|Si|
?
k=1
lik ? osik ? L (10)
where lik is the length of sentence sik, and L is the
maximal summary length.
The optimization of the defined objective function
under above constraints is an integer linear program-
ming (ILP) problem. Though the ILP problems
are generally NP-hard, considerable works have
been done and several software solutions have been
released to solve them efficiently.1
4 Experiment
4.1 Dataset
Because of the novelty of the comparative news
summarization task, there is no existing data set
for evaluating. We thus create our own. We first
choose five pairs of comparable topics, then retrieve
ten related news articles for each topic using the
Google News2 search engine. Finally we write the
comparative summary for each topic pair manually.
The topics are showed in table 1.
4.2 Evaluation Metrics
We evaluate the models with following measures:
Comparison Precision / Recall / F-measure:
let aa and am be the numbers of all aspects
1We use IBM ILOG CPLEX optimizer to solve the problem.
2http://news.google.com
ID Topic 1 Topic 2
1 Haiti Earth quake Chile Earthquake
2 Chile Mining Acci-
dent
New Zealand Mining
Accident
3 Iraq Withdrawal Afghanistan
Withdrawal
4 Apple iPad 2 BlackBerry Playbook
5 2006 FIFAWorld Cup 2010 FIFAWorld Cup
Table 1: Comparable topic pairs in the dataset.
involved in the automatically generated summary
and manually written summary respectively; ca
be the number of human agreed comparative
aspects in the automatically generated summary.
The comparison precision (CP ), comparison recall
(CR) and comparison F-measure (CF ) are defined
as follows:
CP = ca
aa
; CR = ca
am
; CF = 2 ? CP ? CR
CP + CR
ROUGE: the ROUGE is a widely used metric
in summarization evaluation. It measures summary
quality by counting overlapping units between the
candidate summary and the reference summary (Lin
and Hovy, 2003). In the experiment, we report
the f-measure values of ROUGE-1, ROUGE-2 and
ROUGE-SU4, which count overlapping unigrams,
bigrams and skip-4-grams respectively. To evaluate
whether the summary is related to both topics,
we also split each comparative summary into two
topic-related parts, evaluate them respectively, and
report the mean of the two ROUGE values (denoted
as MROUGE).
4.3 Baseline Systems
Non-Comparative Model (NCM): The
non-comparative model treats the task as a
traditional summarization problem and selects the
important sentences from each document collection.
The model is adapted from our approach by setting
? = 0 in the objection function 1.
Co-Ranking Model (CRM): The co-ranking
model makes use of the relations within each
topic and relations across the topics to reinforce
scores of the comparison related sentences. The
model is adapted from (Wan et al, 2007). The
651
SS, WW and SW relationships are replaced by
relationships between two sentences within each
topic and relationships between two sentences from
different topics.
4.4 Experiment Results
We apply all the systems to generate comparative
summaries with a length limit of 200 words. The
evaluation results are shown in table 2. Compared
with baseline models, our linear programming based
comparative model (denoted as LPCM) achieves
best scores over all metrics. It is expected to find
that the NCM model does not perform well in this
task because it does not focus on the comparisons.
The CRM model utilizes the similarity between
two topics to enhance the score of comparison
related sentences. However, it does not guarantee
to choose pairwise sentences to form comparisons.
The LPCM model focus on both comparativeness
and representativeness at the same time, and thus
it achieves good performance on both comparison
extraction and summarization. Figure 1 shows
an example of comparative summary generated by
using the CLPM model. The summary describes
several comparisons between two FIFA World Cups
in 2006 and 2010. Most of the comparisons are clear
and representative.
5 Conclusion
In this study, we propose a novel approach to
summing up the commonalities and differences
between two news topics. We formulate the
task as an optimization problem of selecting
sentences to maximize the score of comparative and
representative evidences. The experiment results
show that our model is effective in comparison
extraction and summarization.
In future work, we will utilize more semantic
information such as localized latent topics to help
capture comparative aspects, and use machine
learning technologies to tune weights of concepts.
Acknowledgments
This work was supported by NSFC (60873155),
Beijing Nova Program (2008B03) and NCET
(NCET-08-0006).
Model CP CR CF ROUGE-1 ROUGE-2 ROUGE-su4 MROUGE-1 MROUGE-2 MROUGE-su4
NCM 0.238 0.262 0.247 0.398 0.146 0.174 0.350 0.122 0.148
CRM 0.313 0.285 0.289 0.426 0.194 0.226 0.355 0.146 0.175
LPCM 0.359 0.419 0.386 0.427 0.205 0.234 0.380 0.171 0.192
Table 2: Evaluation results of systems
World Cup 2006 World Cup 2010
The 2006 Fifa World Cup drew to a close on Sunday
with Italy claiming their fourth crown after beating
France in a penalty shoot-out.
Spain have won the 2010 FIFA World Cup South Africa
final, defeating Netherlands 1-0 with a wonderful goal
from Andres Iniesta deep into extra-time.
Zidane won the Golden Ball over Italians Fabio
Cannavaro and Andrea Pirlo.
Uruguay star striker Diego Forlan won the Golden
Ball Award as he was named the best player of the
tournament at the FIFA World Cup 2010 in South
Africa.
Lukas Podolski was named the inaugural Gillette Best
Young Player.
German youngster Thomas Mueller got double delight
after his side finished third in the tournament as he was
named Young Player of the World Cup
Germany striker Miroslav Klose was the Golden Shoe
winner for the tournament?s leading scorer.
Among the winners were goalkeeper and captain Iker
Casillas who won the Golden Glove Award.
England?s fans brought more colour than their team. Only four of the 212 matches played drew more that
40,000 fans.
Figure 1: A sample comparative summary generated by using the LPCM model
652
References
Jaime Carbonell and Jade Goldstein. 1998. The use of
MMR, diversity-based reranking for reordering docu-
ments and producing summaries. In Proceedings of
the 21st annual international ACM SIGIR conference
on Research and development in information retrieval,
pages 335?336. ACM.
Dan Gillick, Korbinian Riedhammer, Benoit Favre, and
Dilek Hakkani-Tur. 2009. A global optimization
framework for meeting summarization. In Proceed-
ings of the 2009 IEEE International Conference on
Acoustics, Speech and Signal Processing, ICASSP
?09, pages 4769?4772, Washington, DC, USA. IEEE
Computer Society.
Xiaojiang. Huang, Xiaojun. Wan, Jianwu. Yang, and
Jianguo. Xiao. 2008. Learning to Identify
Comparative Sentences in Chinese Text. PRICAI
2008: Trends in Artificial Intelligence, pages 187?198.
Nitin Jindal and Bing Liu. 2006a. Identifying compar-
ative sentences in text documents. In Proceedings of
the 29th annual international ACM SIGIR conference
on Research and development in information retrieval,
pages 244?251. ACM.
Nitin Jindal and Bing Liu. 2006b. Mining comparative
sentences and relations. In proceedings of the 21st
national conference on Artificial intelligence - Volume
2, pages 1331?1336. AAAI Press.
Hyun Duk Kim and ChengXiang Zhai. 2009. Generating
comparative summaries of contradictory opinions in
text. In Proceeding of the 18th ACM conference
on Information and knowledge management, pages
385?394. ACM.
Kevin Lerman and Ryan McDonald. 2009. Contrastive
summarization: an experiment with consumer reviews.
In Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, Companion Volume: Short Papers, pages
113?116. Association for Computational Linguistics.
Chin-Yew Lin and Eduard Hovy. 2003. Automatic
evaluation of summaries using n-gram co-occurrence
statistics. In Proceedings of the 2003 Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology - Volume 1, NAACL ?03, pages 71?78,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Bing Liu, Minqing Hu, and Junsheng Cheng. 2005.
Opinion observer: analyzing and comparing opinions
on the Web. In Proceedings of the 14th international
conference on World Wide Web, pages 342?351. ACM.
Inderjeet Mani. 2001. Automatic summarization. Natu-
ral Language Processing. John Benjamins Publishing
Company.
Ryan McDonald. 2007. A study of global inference
algorithms in multi-document summarization. In
Proceedings of the 29th European conference on IR re-
search, ECIR?07, pages 557?564, Berlin, Heidelberg.
Springer-Verlag.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. WordNet:: Similarity: measuring the
relatedness of concepts. In Demonstration Papers at
HLT-NAACL 2004 on XX, pages 38?41. Association
for Computational Linguistics.
Jian-Tao Sun, Xuanhui Wang, Dou Shen, Hua-Jun Zeng,
and Zheng Chen. 2006. CWS: a comparative
web search system. In Proceedings of the 15th
international conference on World Wide Web, pages
467?476. ACM.
Xiaojun Wan, Jianwu Yang, and Jianguo Xiao. 2007.
Towards an iterative reinforcement approach for
simultaneous document summarization and keyword
extraction. In Proceedings of the 45th Annual Meeting
of the Association of Computational Linguistics, pages
552?559, Prague, Czech Republic, June. Association
for Computational Linguistics.
Dingding Wang, Shenghuo Zhu, Tao Li, and Yihong
Gong. 2009. Comparative document summarization
via discriminative sentence selection. In Proceeding
of the 18th ACM conference on Information and
knowledge management, pages 1963?1966. ACM.
ChengXiang Zhai, Atulya Velivelli, and Bei Yu. 2004.
A cross-collection mixture model for comparative text
mining. In Proceedings of the tenth ACM SIGKDD
international conference on Knowledge discovery and
data mining, pages 743?748. ACM.
653
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 380?390,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Collective Tweet Wikification based on Semi-supervised Graph
Regularization
Hongzhao Huang
1
, Yunbo Cao
2
, Xiaojiang Huang
2
, Heng Ji
1
, Chin-Yew Lin
2
1
Computer Science Department, Rensselaer Polytechnic Institute, Troy, NY 12180, USA
2
Microsoft Research Asia, Beijing 100080, P.R.China
{huangh9,jih}@rpi.edu
1
,
{yunbo.cao,xiaojih,cyl}@microsoft.com
2
Abstract
Wikification for tweets aims to automat-
ically identify each concept mention in a
tweet and link it to a concept referent in
a knowledge base (e.g., Wikipedia). Due
to the shortness of a tweet, a collective
inference model incorporating global ev-
idence from multiple mentions and con-
cepts is more appropriate than a non-
collecitve approach which links each men-
tion at a time. In addition, it is chal-
lenging to generate sufficient high quality
labeled data for supervised models with
low cost. To tackle these challenges, we
propose a novel semi-supervised graph
regularization model to incorporate both
local and global evidence from multi-
ple tweets through three fine-grained re-
lations. In order to identify semantically-
related mentions for collective inference,
we detect meta path-based semantic rela-
tions through social networks. Compared
to the state-of-the-art supervised model
trained from 100% labeled data, our pro-
posed approach achieves comparable per-
formance with 31% labeled data and ob-
tains 5% absolute F1 gain with 50% la-
beled data.
1 Introduction
With millions of tweets posted daily, Twitter en-
ables both individuals and organizations to dis-
seminate information, from current affairs to
breaking news in a timely fashion. In this
work, we study the wikification (Disambiguation
to Wikipedia) task (Mihalcea and Csomai, 2007)
for tweets, which aims to automatically identify
each concept mention in a tweet, and link it to a
concept referent in a knowledge base (KB) (e.g.,
Wikipedia). For example, as shown in Figure 1,
Hawks is an identified mention, and its correct ref-
erent concept in Wikipedia is Atlanta Hawks. An
end-to-end wikification system needs to solve two
sub-problems: (i) concept mention detection, (ii)
concept mention disambiguation.
Wikification is a particularly useful task for
short messages such as tweets because it allows
a reader to easily grasp the related topics and en-
riched information from the KB. From a system-
to-system perspective, wikification has demon-
strated its usefulness in a variety of applica-
tions, including coreference resolution (Ratinov
and Roth, 2012) and classification (Vitale et al,
2012).
Sufficient labeled data is crucial for supervised
models. However, manual wikification annota-
tion for short documents is challenging and time-
consuming (Cassidy et al, 2012). The challenges
are: (i) unlinkability, a valid concept may not ex-
ist in the KB. (ii) ambiguity, it is impossible to
determine the correct concept due to the dearth
of information within a single tweet or multiple
correct answer. For instance, it would be diffi-
cult to determine the correct referent concept for
?Gators? in t
1
in Figure 1. Linking ?UCONN?
in t
3
to University of Connecticut may also be ac-
ceptable since Connecticut Huskies is the athletic
team of the university. (iii) prominence, it is chal-
lenging to select a set of linkable mentions that
are important and relevant. It is not tricky to select
?Fans?, ?slump?, and ?Hawks? as linkable men-
tions, but other mentions such as ?stay up? and
?stay positive? are not prominent. Therefore, it
is challenging to create sufficient high quality la-
beled tweets for supervised models and worth con-
sidering semi-supervised learning with the explo-
ration of unlabeled data.
380
    
Stay up Hawk Fans. We are going 
through a slump now, but we have to 
stay positive. Go Hawks!
Congrats to UCONN and Kemba Walker. 
5 wins in 5 days, very impressive...
Just getting to the Arena, we play the 
Bucks tonight. Let's get it!
Fan (person); Mechanical fan
Slump (geology);  Slump (sports)
Atlanta Hawks;  Hawks (film)
University of Connecticut; Connecticut Huskies
Kemba Walker
Arena; Arena (magazine); Arena (TV series)
Bucks County, Pennsylvania; Milwaukee Bucks
Tweets Concept Candidates
Go Gators!!! Florida Gators football; Florida Gators men's basketballt1
t2
t3
t4
Figure 1: An illustration of Wikification Task for Tweets. Concept mentions detected in tweets are
marked as bold, and correctly linked concepts are underlined. The concept candidates are ranked by
their prior popularity which will be explained in section 4.1, and only top 2 ranked concepts are listed.
However, when selecting semi-supervised
learning frameworks, we noticed another unique
challenge that tweets pose to wikification due
to their informal writing style, shortness and
noisiness. The context of a single tweet usually
cannot provide enough information for prominent
mention detection and similarity computing for
disambiguation. Therefore, a collective inference
model over multiple tweets in the semi-supervised
setting is desirable. For instance, the four tweets
in Figure 1 are posted by the same author within
a short time period. If we perform collective
inference over them we can reliably link am-
biguous mentions such as ?Gators?, ?Hawks?,
and ?Bucks? to basketball teams instead of other
concepts such as the county Bucks County.
In order to address these unique challenges
for wikification for the short tweets, we employ
graph-based semi-supervised learning algorithms
(Zhu et al, 2003; Smola and Kondor, 2003; Blum
et al, 2004; Zhou et al, 2004; Talukdar and
Crammer, 2009) for collective inference by ex-
ploiting the manifold (cluster) structure in both
unlabeled and labeled data. These approaches
normally assume label smoothness over a defined
graph, where the nodes represent a set of labeled
and unlabeled instances, and the weighted edges
reflect the closeness of each pair of instances. In
order to construct a semantic-rich graph capturing
the similarity between mentions and concepts for
the model, we introduce three novel fine-grained
relations based on a set of local features, social
networks and meta paths.
The main contributions of this paper are sum-
marized as follows:
? To the best of our knowledge, this is the first
effort to explore graph-based semi-supervised
learning algorithms for the wikification task.
? We propose a novel semi-supervised graph reg-
ularization model performing collective infer-
ence for joint mention detection and disam-
biguation. Our approach takes advantage of
three proposed principles to incorporate both lo-
cal and global evidence from multiple tweets.
? We propose a meta path-based unified frame-
work to detect both explicitly and implicitly rel-
evant mentions.
2 Preliminaries
Concept and Concept Mention We define a con-
cept c as a Wikipedia article (e.g., Atlanta Hawks),
and a concept mentionm as an n-gram from a spe-
cific tweet. Each concept has a set of textual repre-
sentation fields (Meij et al, 2012), including title
(the title of the article), sentence (the first sentence
of the article), paragraph (the first paragraph of
the article), content (the entire content of the arti-
cle), and anchor (the set of all anchor texts with
incoming links to the article).
Wikipedia Lexicon Construction We first
construct an offline lexicon with each entry as
?m, {c
1
, ..., c
k
}?, where {c
1
, ..., c
k
} is the set of
possible referent concepts for the mention m.
Following the previous work (Bunescu, 2006;
Cucerzan, 2007; Hachey et al, 2013), we extract
the possible mentions for a given concept c using
the following resources: the title of c; the aliases
appearing in the introduction and infoboxes of c
(e.g., The Evergreen State is an alias of Wash-
ington state); the titles of pages redirecting to c
(e.g., State of Washington is a redirecting page of
Washington (state)); the titles of the disambigua-
381
tion pages containing c; and all the anchor texts
appearing in at least 5 pages with hyperlinks to c
(e.g., WA is a mention for the concept Washing-
ton (state) in the text ?401 5th Ave N [[Seattle]],
[[Washington (state)?WA]] 98109 USA?. We also
propose three heuristic rules to extract mentions
(i.e., different combinations of the family name
and given name for a person, the headquarters of
an organization, and the city name for a sports
team).
Concept Mention Extraction Based on the
constructed lexicon, we then consider all n-grams
of size? n (n=7 in this paper) as concept mention
candidates if their entries in the lexicon are not
empty. We first segment @usernames and #hash-
tags into regular tokens (e.g., @amandapalmer is
segmented as amanda palmer and #WorldWater-
Day is split as World Water Day) using the ap-
proach proposed by (Wang et al, 2011). Segmen-
tation assists finding concept candidates for these
non-regular mentions.
3 Principles and Approach Overview
    
R elational Graph Construction
Knowledge Base 
(Wikipedia)
Labeled and 
Unlabeled Tweets
Wikipedia Lex icon Construction
Concept Mention and 
Concept Candidate E x traction
Local Compatibility
(local features, 
cosine similarity)
Coreference
(meta path,
mention 
similarity)
Semantic R elatedness
(meta path, concept 
semantic relatedness)
Semi- Supervised Graph R egularization
<Mention, Concept>
Pairs
Figure 2: Approach Overview.
3.1 Principles
A single tweet may not provide enough evidence
to identify prominent mentions and infer their cor-
rect referent concepts due to the lack of contextual
information. To tackle this problem, we propose to
incorporate global evidence from multiple tweets
and performing collective inference for both men-
tion identification and disambiguation. We first in-
troduce the following three principles that our ap-
proach relies on.
Principle 1 (Local compatibility): Two pairs
of ?m, c? with strong local compatibility tend to
have similar labels. Mentions and their correct
referent concepts usually tend to share a set of
characteristics such as string similarity betweenm
and c (e.g., ?Chicago, Chicago? and ?Facebook,
Facebook?). We define the local compatibility to
model such set of characteristics.
Principle 2 (Coreference): Two coreferential
mentions should be linked to the same concept.
For example, if we know ?nc? and ?North Car-
olina? are coreferential, then they should both be
linked to North Carolina.
Principle 3 (Semantic Relatedness): Two
highly semantically-related mentions are more
likely to be linked to two highly semantically-
related concepts. For instance, when ?Sweet 16?
and ?Hawks? often appear together within rel-
evant contexts, they can be reliably linked to
two baseketball-related concepts NCAA Men?s Di-
vision I Basketball Championship and Atlanta
Hawks, respectively.
3.2 Approach Overview
Given a set of tweets ?t
1
, ..., t
|T |
?, our system first
generates a set of candidate concept mentions, and
then extracts a set of candidate concept referents
for each mention based on the Wikipedia lexicon.
Given a pair of mention and its candidate referent
concept ?m, c?, the remaining task of wikification
is to assign either a positive label if m should be
selected as a prominently linkable mention and c
is its correct referent concept, or otherwise a neg-
ative label. The label assignment is obtained by
our semi-supervised graph regularization frame-
work based on a relational graph, which is con-
structed from local compatibility, coreference, and
semantic relatedness relations. The overview of
our approach is as illustrated in Figure 2.
4 Relational Graph Construction
We first construct the relational graphG = ?V,E?,
where V = {v
1
, ..., v
n
} is a set of nodes and E =
{e
1
, ..., e
m
} is a set of edges. Each v
i
= ?m
i
, c
i
?
represents a tuple of mention m
i
and its referent
concept candidate c
i
. An edge is added between
two nodes v
i
and v
j
if there is a proposed rela-
tion based on the three principles described in sec-
tion 3.1.
4.1 Local Compatibility
We first compute local compatibility (Principle 1)
by considering a set of novel local features to cap-
382
ture the importance and relevance of a mention m
to a tweet t, as well as the correctness of its link-
age to a concept c. We have designed a number
of features which are similar to those commonly
used in wikification and entity linking work (Meij
et al, 2012; Guo et al, 2013; Mihalcea and Cso-
mai, 2007).
Mention Features We define the following fea-
tures based on information from mentions.
? IDF
f
(m) = log(
|C|
df(m)
), where |C| is the total
number of concepts in Wikipedia and df(m) is
the total number of concepts in whichm occurs,
and f indicates the field property, including ti-
tle, content, and anchor.
? Keyphraseness(m) =
|C
a
(m)|
df(m)
to measure
how likely m is used as an anchor in Wikipedia,
where C
a
(m) is the set of concepts where m
appears as an anchor.
? LinkProb(m) =
?
c?C
a
(m)
count(m,c)
?
c?C
count(m,c)
, where
count(m, c) indicates the number of occurrence
of m in c.
? SNIL(m) and SNCL(m) to count the number
of concepts that are equal to or contain a sub-n-
gram of m, respectively (Meij et al, 2012).
Concept Features The concept features are
solely based on Wikipedia, including the number
of incoming and outgoing links for c, and the num-
ber of words and characters in c.
Mention + Concept Features This set of fea-
tures considers information from both mentions
and concepts:
? prior popularity prior(m, c) =
count(m,c)?
c
?
count(m,c
?
)
, where count(m, c) mea-
sures the frequency of the anchor links from m
to c in Wikipedia.
? TF
f
(m, c) =
count
f
(m,c)
|f |
to measure the rela-
tive frequency of m in each field representation
f of c, normalized by the length of f . The fields
include title, sentence, paragraph, content and
anchor.
? NCT (m, c), TCN(m, c), and TEN(m, c) to
measure whether m contains the title of c,
whether the title of c contains m, and whether
m equals to the title of c, respectively.
Context Features This set of features include
(i) Context Capitalization features, which indicate
whether the current mention, the token before, and
the token after are capitalized. (ii) tf-idf based fea-
tures, which include the dot product of two word
vectors v
c
and v
t
, and the average tf-idf value of
common items in v
c
and v
t
, where v
c
and v
t
are
the top 100 tf-idf word vectors in c and t.
Local Compatibility Computation For each
node v
i
= ?m
i
, c
i
?, we collect its local features
as a feature vector F
i
= ?f
1
, f
2
, ..., f
d
?. To avoid
features with large numerical values that domi-
nate other features, the value of each feature is
re-scaled using feature standardization approach.
The cosine similarity is then adopted to compute
the local compatibility of two nodes and construct
a k nearest neighbor (kNN) graph, where each
node is connected to its k nearest neighboring
nodes. We compute the weight matrix that rep-
resents the local compatibility relation as:
W
loc
ij
=
{
cosine(F
i
, F
j
) j ? kNN(i)
0 Otherwise
4.2 Meta Path
    
Mention
Hashtag
Tweet User
post - 1
post
contain - 1
contain
contain - 1 contain
Figure 3: Schema of the Twitter network.
In this subsection, we introduce the concept
meta path which will be used to detect corefer-
ence (section 4.3) and semantic relatedness rela-
tions (section 4.4).
A meta-path is a path defined over a network
and composed of a sequence of relations between
different object types (Sun et al, 2011b). In our
experimental setting, we can construct a natu-
ral Twitter network summarized by the network
schema in Figure 3. The network contains four
types of objects: Mention (M), Tweet (T), User
(U), and Hashtag (H). Tweets and mentions are
connected by links ?contain? and ?contained by?
(denoted as ?contain
?1
?); and other linked rela-
tionships can be described similarly.
We then define the following five types of meta
paths to connect two mentions as:
? ?M - T - M?,
? ?M - T - U - T - M?,
? ?M - T - H - T - M?,
? ?M - T - U - T - M - T - H - T - M?,
? ?M - T - H - T - M - T - U - T - M?.
383
Each meta path represents one particular seman-
tic relation. For instance, the first three paths are
basic ones expressing the explicit relations that
two mentions are from the same tweet, posted by
the same user, and share the same #hashtag, re-
spectively. The last two paths are concatenated
ones which are constructed by concatenating the
first three simple paths to express the implicit rela-
tions that two mentions co-occur with a third men-
tion sharing either the same authorship or #hash-
tag. Such complicated paths can be exploited to
detect more semantically-related mentions from
wider contexts. For example, the relational link
between ?narita airport? and ?Japan? would be
missed without using the path ?narita airport - t
1
- u
1
- t
2
- american - t
3
- h
1
- t
4
- Japan? since they
don?t directly share any authorships or #hashtags.
4.3 Coreference
A coreference relation (Principle 2) usually occurs
across multiple tweets due to the highly redundant
information in Twitter. To ensure high precision,
we propose a simple yet effective approach utiliz-
ing the rich social network relations in Twitter.
We consider two mentions m
i
and m
j
corefer-
ential if m
i
and m
j
share the same surface form
or one is an abbreviation of the other, and at least
one meta path exists betweenm
i
andm
j
. Then we
define the weight matrix representing the corefer-
ential relation as:
W
coref
ij
=
?
?
?
1.0 if m
i
and m
j
are coreferential,
and c
i
= c
j
0 Otherwise
4.4 Semantic Relatedness
Ensuring topical coherence (Principle 3) has been
beneficial for wikification on formal texts (e.g.,
News) by linking a set of semantically-related
mentions to a set of semantically-related concepts
simultaneously (Han et al, 2011; Ratinov et al,
2011; Cheng and Roth, 2013). However, the short-
ness of a single tweet means that it may not pro-
vide enough topical clues. Therefore, it is impor-
tant to extend this evidence to capture semantic re-
latedness information from multiple tweets.
We define the semantic relatedness score be-
tween two mentions as SR(m
i
,m
j
) = 1.0 if at
least one meta path exists between m
i
and m
j
,
otherwise SR(m
i
,m
j
) = 0. In order to compute
the semantic relatedness of two concepts c
i
and
c
j
, we adopt the approach proposed by (Milne and
Witten, 2008a):
SR(c
i
, c
j
) = 1?
logmax(|C
i
|, |C
j
|)? log |C
i
? C
j
|
log(|C|)? logmin(|C
i
|, |C
j
|)
,
where |C| is the total number of concepts in
Wikipedia, and C
i
and C
j
are the set of concepts
that have links to c
i
and c
j
, respectively.
Then we compute a weight matrix representing
the semantic relatedness relation as:
W
rel
ij
=
{
SR(N
i
, N
j
) if SR(N
i
, N
j
) ? ?
0 Otherwise
where SR(N
i
, N
j
) = SR(m
i
,m
j
) ? SR(c
i
, c
j
)
and ? = 0.3, which is optimized from a develop-
ment set.
4.5 The Combined Relational Graph
    
hawks, 
 Atlanta Hawks
uconn, 
Connecticut 
Huskies
bucks, 
Milwaukee 
Bucks
kemba walker, 
Kemba Walker
0 .40 4
gators, 
Florida Gators 
men's basketball
now, 
N ow
days, 
D ay
tonight, 
Tonight
0 .9 32
0 .7 6 4
0 .6 6 5
0 .46 7
0 .56 3 0 .538
0 .447
Figure 4: A example of the relational graph con-
structed for the example tweets in Figure 1. Each
node represents a pair of ?m, c?, separated by a
comma. The edge weight is obtained from the lin-
ear combination of the weights of the three pro-
posed relations. Not all mentions are included due
to the space limitations.
Based on the above three weight matricesW
loc
,
W
coref
, and W
rel
, we first obtain their corre-
sponding transition matrices P
loc
, P
coref
, and
P
rel
, respectively. The entry P
ij
of the transition
matrix P for a weight matrix W is computed as
P
ij
=
W
ij?
k
W
ik
such that
?
k
P
ik
= 1. Then we
obtain the combined graph G with weight matrix
W , where W
ij
= ?P
loc
ij
+ ?P
coref
ij
+ ?P
rel
ij
. ?,
?, and ? are three coefficients between 0 and 1
with the constraint that ?+ ? + ? = 1. They con-
trol the contributions of these three relations in our
semi-supervised graph regularization model. We
choose transition matrix to avoid the domination
of one relation over others. An example graph of
G is shown in Figure 4. Compared to the referent
graph which considers each mention or concept
as a node in previous graph-based re-ranking ap-
proaches (Han et al, 2011; Shen et al, 2013), our
384
novel graph representation has two advantages: (i)
It can easily incorporate more features related to
both mentions and concepts. (ii) It is more appro-
priate for our graph-based semi-supervised model
since it is difficult to assign labels to a pair of men-
tion and concept in the referent graph.
5 Semi-supervised Graph Regularization
Given the constructed relational graph with the
weighted matrix W and the label vector Y of all
nodes, we assume the first l nodes are labeled as
Y
l
and the remaining u nodes (u = n? l) are ini-
tialized with labels Y
0
u
. Then our goal is to refine
Y
0
u
and obtain the final label vector Y
u
.
Intuitively, if two nodes are strongly connected,
they tend to hold the same label. We propose a
novel semi-supervised graph regularization frame-
work based on the graph-based semi-supervised
learning algorithm (Zhu et al, 2003):
Q(Y) = ?
n
?
i=l+1
(y
i
?y
0
i
)
2
+
1
2
?
i,j
W
ij
(y
i
?y
j
)
2
.
The first term is a loss function that incorporates
the initial labels of unlabeled examples into the
model. In our method, we adopt prior popular-
ity (section 4.1) to initialize the labels of the un-
labeled examples. The second term is a regular-
izer that smoothes the refined labels over the con-
structed graph. ? is a regularization parameter that
controls the trade-off between initial labels and the
consistency of labels on the graph. The goal of the
proposed framework is to ensure that the refined
labels of unlabeled nodes are consistent with their
strongly connected nodes, as well as not too far
away from their initial labels.
The above optimization problem can be solved
directly since Q(Y) is convex (Zhu et al, 2003;
Zhou et al, 2004). Let I be an identity matrix
and D
W
be a diagonal matrix with entries D
ii
=
?
j
W
ij
. We can split the weighted matrix W into
four blocks as W =
[
W
ll
W
lu
W
ul
W
uu
]
, where W
mn
is
anm?nmatrix. D
w
is split similarly. We assume
that the vector of the labeled examples Y
l
is fixed,
so we only need to infer the refined label vector of
the unlabeled examples Y
u
. In order to minimize
Q(Y), we need to find Y
?
u
such that
?Q
?Y
u
?
?
?
?
Y
u
=Y
?
u
= (D
uu
+ ?I
uu
)Y
u
?W
uu
Y
u
?
W
ul
Y
l
? ?Y
0
u
= 0.
Therefore, a closed form solution can be derived
as Y
?
u
= (D
uu
+ ?I
uu
?W
uu
)
?1
(W
ul
Y
l
+ ?Y
0
u
).
However, for practical application to a large-
scale data set, an iterative solution would be more
efficient to solve the optimization problem. Let
Y
t
u
be the refined labels after the t
th
iteration, the
iterative solution can be derived as:
Y
t+1
u
= (D
uu
+?I
uu
)
?1
(W
uu
Y
t
u
+W
ul
Y
l
+?Y
0
u
).
The iterative solution is more efficient since
(D
uu
+ ?I
uu
) is a diagonal matrix and its inverse
is very easy to compute.
6 Experiments
In this section we compare our approach with
state-of-the-art methods as shown in Table 1.
6.1 Data and Scoring Metric
For our experiments we use a public data set (Meij
et al, 2012) including 502 tweets posted by 28
verified users. The data set was annotated by two
annotators. We randomly sample 102 tweets for
development and the remaining for evaluation. We
use a Wikipedia dump on May 3, 2013 as our
knowledge base, which includes 30 million pages.
For computational efficiency, we also filter some
mention candidates by applying the preprocess-
ing approach proposed in (Ferragina and Scaiella,
2010), and remove all the concepts with prior pop-
ularity less than 2% from an mention?s concept set
for each mention, similar to (Guo et al, 2013).
A mention and concept pair ?m, c? is judged as
correct if and only if m is linkable and c is the
correct referent concept for m. To evaluate the
performance of a wikification system, we use the
standard precision, recall and F1 measures.
6.2 Experimental Results
The overall performance of various approaches
is shown in Table 2. The results of the super-
vised method proposed by (Meij et al, 2012) are
obtained from 5-fold cross validation. For our
semi-supervised setting, we experimentally sam-
ple 200 tweets for training and use the remain-
ing set as unlabeled and testing sets. In our semi-
supervised regularization model, the matrix W
loc
is constructed by a kNN graph (k = 20). The reg-
ularization parameter ? is empirically set to 0.1,
and the coefficients ?, ?, and ? are learnt from the
development set by considering all the combina-
385
Methods Descriptions
TagMe The same approach that is described in (Ferragina and Scaiella, 2010), which aims to annotate short
texts based on prior popularity and semantic relatedness of concepts. It is basically an unsupervised
approach, except that it needs a development set to tune the probability threshold for linkable mentions.
Meij A state-of-the-art system described in (Meij et al, 2012), which is a supervised approach based on the
random forest model. It performs mention detection and disambiguation jointly, and it is trained from
400 labeled tweets.
SSRegu
1
Our proposed model based on Principle 1, using 200 labeled tweets.
SSRegu
12
Our proposed model based on Principle 1 and 2, using 200 labeled tweets.
SSRegu
13
Our proposed model based on Principle 1 and 3, using 200 labeled tweets.
SSRegu
123
Our proposed full model based on Principle 1, 2 and 3, using 200 labeled tweets.
Table 1: Description of Methods.
Methods Precision Recall F1
TagMe 0.329 0.423 0.370
Meij 0.393 0.598 0.475
SSRegu
1
0.538 0.435 0.481
SSRegu
12
0.638 0.438 0.520
SSRegu
13
0.541 0.457 0.495
SSRegu
123
0.650 0.441 0.525
Table 2: Overall Performance.
tions of values from 0 to 1 at 0.1 intervals
1
. In
order to randomize the experiments and make the
comparison fair, we conduct 20 test runs for each
method and report the average scores across the 20
trials.
The relatively low performance of the baseline
system TagMe demonstrates that only relying on
prior popularity and topical information within a
single tweet is not enough for an end-to-end wik-
ification system for the short tweets. As an exam-
ple, it is difficult to obtain topical clues in order
to link the mention ?Clinton? to Hillary Rodham
Clinton by relying on the single tweet ?wolfblitzer-
cnn: Behind the scenes on Clinton?s Mideast trip
#cnn?. Therefore, the system mistakenly links it
to the most popular concept Bill Clinton.
In comparision with the supervised baseline
proposed by (Meij et al, 2012), our model
SSRegu
1
relying on local compatibility already
achieves comparable performance with 50% of
labeled data. This is because that our model
performs collective inference by making use of
the manifold (cluster) structure of both labeled
and unlabeled data, and that the local compat-
ibility relation is detected with high precision
2
(89.4%). For example, the following three pairs
of mentions and concepts ?pelosi, Nancy Pelosi?,
?obama, Barack Obama?, and ?gaddafi, Muam-
1
These three coefficients are slightly different with differ-
ent training data, a sample of them is: ? = 0.4, ? = 0.5, and
? = 0.1
2
Here we define precision as the percentage of links that
holds the same label.
mar Gaddafi? have strong local compatibility with
each other since they share many similar char-
acteristics captured by the local features such as
string similarity between the mention and the con-
cept. Suppose the first pair is labeled, then its pos-
itive label will be propagated to other unlabeled
nodes through the local compatibility relation, and
correctly predict the labels of other nodes.
Incorporating coreferential or semantic related-
ness relation into SSRegu
1
provides further gains,
demonstrating the effectiveness of these two re-
lations. For instance, ?wh? is correctly linked to
White House by incorporating evidence from its
coreferential mention ?white house?. The corefer-
ential relation (Principle 2) is demonstrated to be
more beneficial than the semantic relatedness re-
lation (Principle 3) because the former is detected
with much higher precision (99.7%) than the latter
(65.4%).
Our full model SSRegu
123
achieves significant
improvement over the supervised baseline (5% ab-
solute F1 gain with 95.0% confidence level by
the Wilcoxon Matched-Pairs Signed-Ranks Test),
showing that incorporating global evidence from
multiple tweets with fine-grained relations is ben-
eficial. For instance, the supervised baseline fails
to link ?UCONN? and ?Bucks? in our examples
to Connecticut Huskies and Milwaukee Bucks, re-
spectively. Our full model corrects these two
wrong links by propagating evidence through the
semantic links as shown in Figure 4 to obtain mu-
tual ranking improvement. The best performance
of our full model also illustrates that the three re-
lations complement each other.
We also study the disambiguation performance
for the annotated mentions, as shown in Table 3.
We can easily see that our proposed approach
using 50% labeled data achieves similar perfor-
mance with the state-of-the-art supervised model
with 100% labeled data. When the mentions are
given, the unpervised approach TagMe has already
386
Methods TagMe Meij SSRegu
123
Accuracy 0.710 0.779 0.772
Table 3: Disambiguation Performance.
Methods Precision Recall F1
SSRegu
12
0.644 0.423 0.510
SSRegu
13
0.543 0.441 0.486
SSRegu
123
0.657 0.419 0.512
Table 4: The Performance of Systems Without Us-
ing Concatenated Meta Paths.
achieved reasonable performance. In fact, mention
detection actually is the performance bottleneck of
a tweet wikification system (Guo et al, 2013). Our
system performs better in identifying the promi-
nent mention.
6.3 Effect of Concatenated Meta Paths
In this work, we propose a unified framework uti-
lizing meta path-based semantic relations to ex-
plore richer relevant context. Beyond the basic
meta paths, we introduce concatenated ones by
concatenating the basic ones. The performance of
the system without using the concatenated meta
paths is shown in Table 4. In comparison with
the system based on all defined meta paths, we
can clearly see that the systems using concate-
nated ones outperform those relying on the sim-
ple ones. This is because the concatenated meta
paths can incorporate more relevant information
with implicit relations into the models by increas-
ing 1.6% coreference links and 9.3% semantic re-
latedness links. For example, the mention ?narita
airport? is correctly disambiguated to the concept
?Narita International Airport? with higher confi-
dence since its semantic relatedness relation with
?Japan? is detected with the concatenated meta
path as described in section 4.2.
6.4 Effect of Labeled Data Size
5 0 1 0 0 1 5 0 2 0 0 2 5 0 3 0 0 3 5 0 4 0 00 . 3 00 . 3 50 . 4 00 . 4 5
0 . 5 00 . 5 50 . 6 0F1 L a b e l e d  T w e e t  S i z e S S R e g u 1 2 3 M e i j
Figure 5: The effect of Labeled Tweet Size.
In previous experiments, we experimentally set
the number of labeled tweets to be 200 for over-
all performance comparision with the baselines.
In this subsection, we study the effect of labeled
data size on our full model. We randomly sam-
ple 100 tweets as testing data, and randomly se-
lect 50, 100, 150, 200, 250, and 300 tweets as
labeled data. 20 test runs are conducted and the
average results are reported across the 20 trials,
as shown in Figure 5. We find that as the size
of the labeled data increases, our proposed model
achieves better performance. It is encouraging to
see that our approach, with only 31.3% labeled
tweets (125 out of 400), already achieves a perfor-
mance that is comparable to the state-of-the-art su-
pervised model trained from 100% labeled tweets.
6.5 Parameter Analysis
0 . 1 0 . 5 1 2 5 1 0 2 0 3 0 4 0 5 00 . 3 00 . 3 50 . 4 00 . 4 5
0 . 5 00 . 5 50 . 6 0F1 R e g u l a r i z a t i o n  P a r a m e t e r  ? S S R e g u 1 2 3
Figure 6: The effect of parameter ?.
In previous experiments, we empirically set the
parameter ? = 0.1. ? is the regularization pa-
rameter that controls the trade-off between initial
labels and the consistency of labels on the graph.
When ? increases, the model tends to trust more in
the initial labels. Figure 6 shows the performance
of our models by varying ? from 0.02 to 50. We
can easily see that the system performce is stable
when ? < 0.4. However, when ? ? 0.4, the sys-
tem performance dramatically decreases, showing
that prior popularity is not enough for an end-to-
end wikification system.
7 Related Work
The task of linking concept mentions to a knowl-
edge base has received increased attentions over
the past several years, from the linking of concept
mentions in a single text (Mihalcea and Csomai,
2007; Milne and Witten, 2008b; Milne and Witten,
2008a; Kulkarni et al, 2009; He et al, 2011; Rati-
nov et al, 2011; Cassidy et al, 2012; Cheng and
Roth, 2013), to the linking of a cluster of corefer-
387
ent named entity mentions spread throughout dif-
ferent documents (Entity Linking) (McNamee and
Dang, 2009; Ji et al, 2010; Zhang et al, 2010; Ji et
al., 2011; Zhang et al, 2011; Han and Sun, 2011;
Han et al, 2011; Gottipati and Jiang, 2011; He et
al., 2013; Li et al, 2013; Guo et al, 2013; Shen et
al., 2013; Liu et al, 2013).
A significant portion of recent work considers
the two sub-problems mention detection and men-
tion disambiguation separately and focus on the
latter by first defining candidate concepts for a
deemed mention based on anchor links. Men-
tion disambiguation is then formulated as a rank-
ing problem, either by resolving one mention at
each time (non-collective approaches), or by dis-
ambiguating a set of relevant mentions simulta-
neously (collective approaches). Non-collective
methods usually rely on prior popularity and con-
text similarity with supervised models (Mihalcea
and Csomai, 2007; Milne and Witten, 2008b; Han
and Sun, 2011), while collective approaches fur-
ther leverage the global coherence between con-
cepts normally through supervised or graph-based
re-ranking models (Cucerzan, 2007; Milne and
Witten, 2008b; Han and Zhao, 2009; Kulkarni et
al., 2009; Pennacchiotti and Pantel, 2009; Ferrag-
ina and Scaiella, 2010; Fernandez et al, 2010;
Radford et al, 2010; Cucerzan, 2011; Guo et al,
2011; Han and Sun, 2011; Han et al, 2011; Rati-
nov et al, 2011; Chen and Ji, 2011; Kozareva et
al., 2011; Cassidy et al, 2012; Shen et al, 2013;
Liu et al, 2013). Especially note that when apply-
ing the collective methods to short messages from
social media, evidence from other messages usu-
ally needs to be considered (Cassidy et al, 2012;
Shen et al, 2013; Liu et al, 2013). Our method
is a collective approach with the following novel
advancements: (i) A novel graph representation
with fine-grained relations, (ii) A unified frame-
work based on meta paths to explore richer rele-
vant context, (iii) Joint identification and linking
of mentions under semi-supervised setting.
Two most similar methods to ours were pro-
posed by (Meij et al, 2012; Guo et al, 2013)
by performing joint detection and disambiguation
of mentions. (Meij et al, 2012) studied several
supervised machine learning models, but without
considering any global evidence either from a sin-
gle tweet or other relevant tweets. (Guo et al,
2013) explored second order entity-to-entity rela-
tions but did not incorporate evidence from multi-
ple tweets.
This work is also related to graph-based semi-
supervised learning (Zhu et al, 2003; Smola
and Kondor, 2003; Zhou et al, 2004; Talukdar
and Crammer, 2009), which has been success-
fully applied in many Natural Language Process-
ing tasks (Niu et al, 2005; Chen et al, 2006).
We introduce a novel graph that incorporates three
fine-grained relations. Our work is further re-
lated to meta path-based heterogeneous informa-
tion network analysis (Sun et al, 2011b; Sun et
al., 2011a; Kong et al, 2012; Huang et al, 2013),
which has demonstrated advantages over homoge-
neous information network analysis without dif-
ferentiating object types and relational links.
8 Conclusions
We have introduced a novel semi-supervised graph
regularization framework for wikification to si-
multaneously tackle the unique challenges of an-
notation and information shortage in short tweets.
To the best of our knowledge, this is the first work
to explore the semi-supervised collective inference
model to jointly perform mention detection and
disambiguation. By studying three novel fine-
grained relations, detecting semantically-related
information with semantic meta paths, and ex-
ploiting the data manifolds in both unlabeled and
labeled data for collective inference, our work can
dramatically save annotation cost and achieve bet-
ter performance, thus shed light on the challenging
wikification task for tweets.
Acknowledgments
This work was supported by the U.S. Army Re-
search Laboratory under Cooperative Agreement
No. W911NF-09-2-0053 (NS-CTA), U.S. NSF
CAREER Award under Grant IIS-0953149, U.S.
DARPA Award No. FA8750-13-2-0041 in the
Deep Exploration and Filtering of Text (DEFT)
Program, IBM Faculty Award, Google Research
Award and RPI faculty start-up grant. The views
and conclusions contained in this document are
those of the authors and should not be inter-
preted as representing the official policies, either
expressed or implied, of the U.S. Government.
The U.S. Government is authorized to reproduce
and distribute reprints for Government purposes
notwithstanding any copyright notation here on.
388
References
A. Blum, J. Lafferty, M. Rwebangira, and R. Reddy.
2004. Semi-supervised learning using randomized
mincuts. In Proceedings of the Twenty-first Interna-
tional Conference on Machine Learning, ICML ?04.
Razvan Bunescu. 2006. Using encyclopedic knowl-
edge for named entity disambiguation. In EACL,
pages 9?16.
T. Cassidy, H. Ji, L. Ratinov, A. Zubiaga, and
H. Huang. 2012. Analysis and enhancement of wik-
ification for microblogs with context expansion. In
Proceedings of COLING 2012.
Z. Chen and H. Ji. 2011. Collaborative ranking: A
case study on entity linking. In Proc. EMNLP2011.
J. Chen, D. Ji, C Tan, and Z. Niu. 2006. Rela-
tion extraction using label propagation based semi-
supervised learning. In Proceedings of the 21st In-
ternational Conference on Computational Linguis-
tics and 44th Annual Meeting of the Association for
Computational Linguistics.
X. Cheng and D. Roth. 2013. Relational inference
for wikification. In Proceedings of the 2013 Con-
ference on Empirical Methods in Natural Language
Processing.
Silviu Cucerzan. 2007. Large-scale named entity dis-
ambiguation based on wikipedia data. In EMNLP-
CoNLL 2007.
S. Cucerzan. 2011. Tac entity linking by performing
full-document entity extraction and disambiguation.
In Proc. TAC 2011 Workshop.
N. Fernandez, J. A. Fisteus, L. Sanchez, and E. Mar-
tin. 2010. Webtlab: A cooccurence-based approach
to kbp 2010 entity-linking task. In Proc. TAC 2010
Workshop.
P. Ferragina and U. Scaiella. 2010. Tagme: on-the-
fly annotation of short text fragments (by wikipedia
entities). In Proceedings of the 19th ACM inter-
national conference on Information and knowledge
management, CIKM ?10.
S. Gottipati and J. Jiang. 2011. Linking entities to a
knowledge base with query expansion. In Proceed-
ings of the 2011 Conference on Empirical Methods
in Natural Language Processing.
Y. Guo, W. Che, T. Liu, and S. Li. 2011. A graph-
based method for entity linking. In Proc. IJC-
NLP2011.
S. Guo, M. Chang, and E. Kiciman. 2013. To link
or not to link? a study on end-to-end tweet entity
linking. In Proceedings of the 2013 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies.
B. Hachey, W. Radford, J. Nothman, M. Honnibal, and
J. Curran. 2013. Evaluating entity linking with
wikipedia. Artif. Intell.
X. Han and L. Sun. 2011. A generative entity-mention
model for linking entities with knowledge base. In
Proc. ACL2011.
X. Han and J. Zhao. 2009. Named entity disam-
biguation by leveraging wikipedia semantic knowl-
edge. In Proceedings of the 18th ACM conference
on Information and knowledge management, CIKM
2009.
X. Han, L. Sun, and J. Zhao. 2011. Collective entity
linking in web text: A graph-based method. In Proc.
SIGIR2011.
J. He, M. de Rijke, M. Sevenster, R. van Ommering,
and Y. Qian. 2011. Generating links to background
knowledge: A case study using narrative radiology
reports. In Proceedings of the 20th ACM inter-
national conference on Information and knowledge
management. ACM.
Z. He, S. Liu, Y. Song, M. Li, M. Zhou, and H. Wang.
2013. Efficient collective entity linking with stack-
ing. In Proceedings of the 2013 Conference on Em-
pirical Methods in Natural Language Processing.
H. Huang, Z. Wen, D. Yu, H. Ji, Y. Sun, J. Han, and
H. Li. 2013. Resolving entity morphs in censored
data. In Proceedings of the 51st Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers).
H. Ji, R. Grishman, H.T. Dang, K. Griffitt, and J. El-
lis. 2010. Overview of the tac 2010 knowledge base
population track. In Text Analysis Conference (TAC)
2010.
H. Ji, R. Grishman, and H.T. Dang. 2011. Overview
of the tac 2011 knowledge base population track. In
Text Analysis Conference (TAC) 2011.
X. Kong, P. Yu, Y. Ding, and J. Wild. 2012. Meta
path-based collective classification in heterogeneous
information networks. In Proceedings of the 21st
ACM International Conference on Information and
Knowledge Management, CIKM ?12.
Z. Kozareva, K. Voevodski, and S. Teng. 2011. Class
label enhancement via related instances. In Proc.
EMNLP2011.
S. Kulkarni, A. Singh, G. Ramakrishnan, and
S. Chakrabarti. 2009. Collective annotation of
wikipedia entities in web text. In KDD.
Y. Li, C. Wang, F. Han, J. Han, D. Roth, and X. Yan.
2013. Mining evidences for named entity dis-
ambiguation. In Proceedings of the 19th ACM
SIGKDD International Conference on Knowledge
Discovery and Data Mining, KDD ?13.
389
X. Liu, Y. Li, H. Wu, M. Zhou, F. Wei, and Y. Lu.
2013. Entity linking for tweets. In Proceedings of
the 51st Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers).
P. McNamee and H.T. Dang. 2009. Overview of the
tac 2009 knowledge base population track. In Text
Analysis Conference (TAC) 2009.
E. Meij, W. Weerkamp, and M. de Rijke. 2012.
Adding semantics to microblog posts. In Proceed-
ings of the fifth ACM international conference on
Web search and data mining, WSDM ?12.
R. Mihalcea and A. Csomai. 2007. Wikify!: linking
documents to encyclopedic knowledge. In Proceed-
ings of the sixteenth ACM conference on Conference
on information and knowledge management, CIKM
?07.
D. Milne and I.H. Witten. 2008a. Learning to link
with wikipedia. In An effective, low-cost measure of
semantic relatedness obtained from wikipedia links.
the Wikipedia and AI Workshop of AAAI.
D. Milne and I.H. Witten. 2008b. Learning to link
with wikipedia. In Proceeding of the 17th ACM con-
ference on Information and knowledge management,
pages 509?518. ACM.
Z. Niu, D. Ji, and C. Tan. 2005. Word sense dis-
ambiguation using label propagation based semi-
supervised learning. In Proceedings of the 43rd An-
nual Meeting of the Association for Computational
Linguistics (ACL?05).
M. Pennacchiotti and P. Pantel. 2009. Entity extraction
via ensemble semantics. In Proc. EMNLP2009.
W. Radford, B. Hachey, J. Nothman, M. Honnibal, and
J. R. Curran. 2010. Cmcrc at tac10: Document-
level entity linking with graph-based re-ranking. In
Proc. TAC 2010 Workshop.
L. Ratinov and D. Roth. 2012. Learning-based multi-
sieve co-reference resolution with knowledge. In
EMNLP.
L. Ratinov, D. Roth, D. Downey, and M. Anderson.
2011. Local and global algorithms for disambigua-
tion to wikipedia. In Proc. of the Annual Meeting of
the Association of Computational Linguistics (ACL).
W. Shen, J. Wang, P. Luo, and M. Wang. 2013. Link-
ing named entities in tweets with knowledge base
via user interest modeling. In Proceedings of the
19th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, KDD ?13.
A. Smola and R. Kondor. 2003. Kernels and regular-
ization on graphs. COLT.
Y. Sun, R. Barber, M. Gupta, C. Aggarwal, and J. Han.
2011a. Co-author relationship prediction in hetero-
geneous bibliographic networks. In Proceedings of
the 2011 International Conference on Advances in
Social Networks Analysis and Mining, ASONAM
?11.
Y. Sun, J. Han, X. Yan, P. Yu, and T. Wu. 2011b. Path-
sim: Meta path-based top-k similarity search in het-
erogeneous information networks. PVLDB, 4(11).
P. Talukdar and K. Crammer. 2009. New regularized
algorithms for transductive learning. In Proceed-
ings of the European Conference on Machine Learn-
ing and Knowledge Discovery in Databases: Part II,
ECML PKDD ?09.
D. Vitale, P. Ferragina, and U. Scaiella. 2012. Clas-
sification of short texts by deploying topical annota-
tions. In ECIR, pages 376?387.
K. Wang, C. Thrasher, and B. Hsu. 2011. Web scale
nlp: A case study on url word breaking. In Proceed-
ings of the 20th International Conference on World
Wide Web, WWW ?11.
W. Zhang, J. Su, C. Tan, and W. Wang. 2010. En-
tity linking leveraging automatically generated an-
notation. In Proceedings of the 23rd International
Conference on Computational Linguistics (Coling
2010).
W. Zhang, J. Su, and C. L. Tan. 2011. A wikipedia-lda
model for entity linking with batch size changing. In
Proc. IJCNLP2011.
D. Zhou, O. Bousquet, T. Lal, J. Weston, and
B. Sch?olkopf. 2004. Learning with local and global
consistency. In Advances in Neural Information
Processing Systems 16.
X. Zhu, Z. Ghahramani, and J. Lafferty. 2003. Semi-
supervised learning using gaussian fields and har-
monic functions. In ICML.
390
