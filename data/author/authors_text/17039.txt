Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 814?820,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Cascading Collective Classification for Bridging Anaphora Recognition
Using a Rich Linguistic Feature Set
Yufang Hou1, Katja Markert2, Michael Strube1
1 Heidelberg Institute for Theoretical Studies gGmbH, Heidelberg, Germany
(yufang.hou|michael.strube)@h-its.org
2School of Computing, University of Leeds, UK
scskm@leeds.ac.uk
Abstract
Recognizing bridging anaphora is difficult due
to the wide variation within the phenomenon,
the resulting lack of easily identifiable surface
markers and their relative rarity. We develop
linguistically motivated discourse structure,
lexico-semantic and genericity detection fea-
tures and integrate these into a cascaded mi-
nority preference algorithm that models bridg-
ing recognition as a subtask of learning fine-
grained information status (IS). We substan-
tially improve bridging recognition without
impairing performance on other IS classes.
1 Introduction
In bridging or associative anaphora (Clark, 1975;
Prince, 1981; Gundel et al, 1993), the antecedent
and anaphor are not coreferent but are linked via a
variety of contiguity relations.1 In Example 1, the
phrases a resident, the stairs and the lobby are bridg-
ing anaphors with the antecedent One building.2
(1) One building was upgraded to red status while peo-
ple were taking things out, and a resident called up the
stairs to his girlfriend, telling her to keep sending things
down to the lobby.
Bridging is an important problem as it affects lin-
guistic theory and applications alike. For exam-
ple, without bridging resolution, entity coherence
between the first and second coordinated clause in
1We exclude comparative anaphora where anaphor and an-
tecedent are in a similarity/exclusion relation, indicated by ana-
phor modifiers such as other or similar (Modjeska et al, 2003).
2Examples are from OntoNotes (Weischedel et al, 2011).
Bridging anaphora are set in boldface; antecedents in italics.
Example 1 cannot be established. This is a prob-
lem both for coherence theories such as Centering
(Grosz et al, 1995) (where bridging is therefore in-
corporated as an indirect realization of previous en-
tities) as well as applications relying on entity co-
herence modelling, such as readability assessment
or sentence ordering (Barzilay and Lapata, 2008).
Full bridging resolution needs (i) recognition that
a bridging anaphor is present and (ii) identification
of the antecedent and contiguity relation. In re-
cent work, these two tasks have been tackled sep-
arately, with bridging recognition handled as part of
information status (IS) classification (Markert et al,
2012; Cahill and Riester, 2012; Rahman and Ng,
2012). Each mention in a text gets assigned one IS
class that describes its accessibility to the reader at
a given point in a text, bridging being one possible
class. We stay within this framework.
Bridging recognition is a difficult task, so that we
had to report very low results on this IS class in pre-
vious work (Markert et al, 2012). This is due to the
phenomenon?s variety, leading to a lack of clear sur-
face features for recognition. Instead, we formulate
in this paper novel discourse structure and lexico-
semantic features as well as features that distinguish
bridging from generics (see Section 3). In addition,
making up between 5% and 20% of definite descrip-
tions (Gardent and Manue?lian, 2005; Caselli and
Prodanof, 2006) and around 6% of all NPs (Mark-
ert et al, 2012), bridging is still less frequent than
many other IS classes and recognition of minority
classes is well known to be more difficult. We there-
fore use a cascaded classification algorithm to ad-
dress this problem (Omuya et al, 2013).
814
2 Related Work
Most bridging research concentrates on antecedent
selection only (Poesio and Vieira, 1998; Poesio et
al., 2004a; Markert et al, 2003; Lassalle and De-
nis, 2011; Hou et al, 2013), assuming that bridg-
ing recognition has already been performed. Previ-
ous work on recognition is either limited to definite
NPs based on heuristics evaluated on small datasets
(Hahn et al, 1996; Vieira and Poesio, 2000), or
models it as a subtask of learning fine-grained IS
(Rahman and Ng, 2012; Markert et al, 2012; Cahill
and Riester, 2012). Results within this latter frame-
work for bridging have been mixed: We reported in
Markert et al (2012) low results for bridging in writ-
ten news text whereas Rahman and Ng (2012) re-
port high results for the four subcategories of bridg-
ing annotated in the Switchboard dialogue corpus by
Nissim et al (2004). We believe this discrepancy to
be due to differences in corpus size and genre as well
as in bridging definition. Bridging in Switchboard
includes non-anaphoric, syntactically linked part-of
and set-member relationships (such as the building?s
lobby), as well as comparative anaphora, the latter
being marked by surface indicators such as other,
another etc. Both types are much easier to identify
than anaphoric bridging cases.3 In addition, many
non-anaphoric lexical cohesion cases have been an-
notated as bridging in Switchbard as well.
We also separate bridging recognition and ante-
cedent selection. One could argue that a joint model
is more attractive as potential antecedents such as
building ?trigger? subsequent bridging cases such as
stairs (Example 1). However, bridging can be indi-
cated by referential patterns without world knowl-
edge about the anaphor/antecedent NPs, as the non-
sense example 2 shows: the wug is clearly a bridging
anaphor although we do not know the antecedent.4
(2) The blicket couldn?t be connected to the dax. The
wug failed.
Similarly, Clark (1975) distinguishes between
bridging via necessary, probable and inducible
parts/roles and argues that only in the first and
maybe the second case the antecedent triggers the
3See also the high results for our specific category for com-
parative anaphora (Markert et al, 2012).
4We thank an anonymous reviewer for pointing this out.
bridging anaphor in the sense that we already spon-
taneously think of the anaphor when we read the an-
tecedent. Also, bridging recognition on its own can
be valuable for applications: for example, prosody is
influenced by IS status without needing antecedent
knowledge (Baumann and Riester, 2013).
3 Characterizing Bridging Anaphora for
Automatic Recognition
3.1 Properties of bridging anaphora
Bridging anaphors are rarely marked by surface fea-
tures. Indeed, even the common practice (Vieira and
Poesio, 2000; Lassalle and Denis, 2011; Cahill and
Riester, 2012) to limit bridging to definite NPs does
not seem to be correct: We report in previous work
(Hou et al, 2013) that less than 40% of the bridg-
ing anaphora in our corpus are definites. Instead,
bridging is diverse with regard to syntactic form
and function: bridging anaphora can be definite NPs
(Examples 4 and 6), indefinite NPs (Example 5) or
bare NPs (Examples 3, 8 and 9). The only frequent
syntactic property shared is that bridging NPs tend
to have a simple internal structure with regards to
modification. Bridging is also easily confused with
generics: friends is used as bridging anaphor in Ex-
ample 9 but generically in Example 10.
(3) . . . meat . . . The Communists froze prices instead.
(4) . . . the fund?s building . . . The budget was only
$400,000.
(5) . . . employees . . . A food caterer stashed stones in the
false bottom of a milk pail.
(6) . . . his truck . . . The farmer at the next truck shouts,
?Wheat!?
(7) . . . the firms . . . Crime was the reason that 26% re-
ported difficulty recruiting personnel and that 19% said
they were considering moving.
(8) . . . the company . . . His father was chairman and
chief executive until his death in an accident five years
ago.
(9) . . . Josephine Baker . . . Friends pitched in.
(10) Friends are part of the glue that holds life and faith
together.
Bridging anaphora can have almost limitless varia-
tion. However, we observe that bridging anaphors
are often licensed because of discourse structure
815
Markert et al (2012) local feature set
f1 FullPrevMention (b) f2 FullPreMentionTime (n)
f3 PartialPreMention (b) f4 ContentWordPreMention (b)
f5 Determiner (n) f6 NPtype (n)
f7 NPlength (int) f8 GrammaticalRole (n)
f9 NPNumber (n) f10 PreModByCompMarker (b)
f11 SemanticClass (n)
Markert et al (2012) relational feature set
f12 HasChild (r) f13 Precedes (r)
Table 1: Markert et al?s (2012) feature set, b indi-
cates binary, n nominal, r relational features.
and/or lexical or world knowledge. With regard to
discourse structure, Grosz et al (1995) observe that
bridging is often needed to establish entity coher-
ence between two adjacent sentences (Examples 1,
2, 4, 5, 6, 7 and 9). With regard to lexical and world
knowledge, relational noun phrases (Examples 3, 4,
8 and 9), building parts (Example 1), set member-
ship elements (Example 7), or, more rarely, tem-
poral/spatial modification (Example 6) may favor a
bridging reading. Motivated by these observations,
we develop discourse structure and lexico-semantic
features indicating bridging anaphora as well as fea-
tures designed to separate genericity from bridging.
3.2 Features
In Markert et al (2012) we classify eight fine-
grained IS categories for NPs in written text: old,
new and 6 mediated categories (syntactic, world-
Knowledge, bridging, comparative, aggregate and
function). This feature set (Table 1, f1-f13) works
well to identify old, new and several mediated cate-
gories. However, it fails to recognize most bridging
anaphora which we try to remedy in this work by
including more diverse features.
Discourse structure features (Table 2, f1-f3).
Bridging occurs frequently in sentences where oth-
erwise there would no entity coherence to previous
sentences/clauses (see Grosz et al (1995) and Poe-
sio et al (2004b) for discussions about bridging, en-
tity coherence and centering transitions in the Cen-
tering framework). This is especially true for topic
NPs (Halliday and Hasan, 1976) in such sentences.
We follow these insights by identifying coherence
gap sentences (see Examples 1, 4, 5, 6, 7, 9 and also
2): a sentence has a coherence gap (f1) if it has none
new local features for bridging
discourse f1 IsCoherenceGap (b)
structure f2 IsSentFirstMention (b)
f3 IsDocFirstMention (b)
semantics f4 IsWordNetRelationalNoun (b)
f5 IsInquirerRoleNoun (b)
f6 IsBuildingPart (b)
f7 IsSetElement (b)
f8 PreModSpatialTemporal (b)
f9 IsYear (b)
f10 PreModifiedByCountry (b)
generic f11 AppearInIfClause (b)
NP f12 VerbPosTag (l)
features f13 IsFrequentGenericNP (b)
f14 WorldKnowledgeNP (l)
f15 PreModByGeneralQuantifier (b)
other features f16 Unigrams (l)
f17 BridgingHeadNP (l)
f18 HasChildNP (b)
new features for other mediated categories
aggregate f19 HasChildCoordination (r)
function f20 DependOnChangeVerb (b)
worldKnowledge f21 IsFrequentProperName (b)
Table 2: New feature set, l indicates lexical features.
of the following three coherence elements: (1) entity
coreference to previous sentences, as approximated
via string match or presence of pronouns, (2) com-
parative anaphora approximated by mentions modi-
fied via a small set of comparative markers (see also
Table 1, f10 PreModByCompMarker), or (3) proper
names. We approximate the topic of a sentence via
the first mention (f2).
f3 models that bridging anaphors do not appear
at the beginning of a text.
Semantic features (Table 2, f4-f10). In contrast
to generic patterns, our semantic features capture
lexical properties of nouns that make them more
likely to be the head of a bridging NP. We create
f4-f8 to capture four kinds of bridging anaphora.
Lo?bner (1985) distinguishes between relational
nouns that take on at least one obligatory semantic
role (such as friend) and sortal nouns. It is likely that
relational nouns are more frequently used as bridg-
ing than sortal nouns (see Examples 3, 4, 8 and 9).
We extract a list containing around 4,000 relational
nouns from WordNet and a list containing around
500 nouns that specify professional roles from the
General Inquirer lexicon (Stone et al, 1966), then
determine whether the NP head appears in these lists
816
or not (f4 and f5). The obligatory semantic role for
a relational noun can of course also be filled NP in-
ternally instead of anaphorically and we use the fea-
tures f10 (for instances such as the Egyptian presi-
dent) and f18 (for complex NPs that are likely to fill
needed roles NP internally) to address this.
Because part-of relations are typical bridging re-
lations (see Example 1 and Clark (1975)), we use f6
to determine whether the NP is a part of the building
or not, using again a list extracted from Inquirer.
f7 is used to identify set membership bridging
cases (see Example 7), by checking whether the
NP head is a number or indefinite pronoun (such as
none, one, some) or modified by each, one. How-
ever, not all numbers are bridging cases (such as
1976) and we use f9 to exclude such cases.
Lassalle and Denis (2011) note that some bridging
anaphors are indicated by spatial or temporal modi-
fications (see Example 6). We use f8 to detect this
by compiling 20 such adjectives from Inquirer.
Features to detect generic nouns (Table 2, f11-
f15). Generic NPs (Example 10) are easily con-
fused with bridging anaphora. Inspired by Reiter
and Frank (2010) who build on linguistic research,
we develop features (f11-f15) to exclude generics.
First, hypothetical entities are likely to refer to
generic entities (Mitchell et al, 2002), We approx-
imate this by determining whether the NP appears
in an if-clause (f11). Also the clause tense and
mood may play a role to decide genericity (Reiter
and Frank, 2010). This is often reflected by the main
verb of a clause, so we extract its POS tag (f12).
Some NPs are commonly used generically, such
as children, men, or the dollar. The ACE-2 corpus
(distinct from our corpus) contains generic annota-
tion . We collect all NPs from ACE-2 that are always
used generically (f13). We also try to learn NPs that
are uniquely identifiable without further description
or anaphoric links such as the sun or the pope. We
do this by extracting common nouns which are an-
notated as worldKnowledge from the training part of
our corpus5 and use these as lexical features (f14).
Finally, motivated by the ACE-2 annotation
guidelines, we identify six quantifiers that may in-
dicate genericity, such as all, no, neither (f15).
5This list varies for each run of our algorithm in 10-fold
cross validation.
Other features for bridging (Table 2, f16-f18).
Following Rahman and Ng (2012), we use unigrams
(f16). We also extract heads of bridging anaphors
from the training data as lexical features (f17) to
learn typical nouns used for bridging that we did not
cover in lexicon extraction (f4 to f6).
Feature f18 models that bridging anaphora most
often have a simple internal structure and usually do
not contain any other NPs.
Features for other IS categories (Table 2, f19-
f21). We propose three features to improve other
IS categories. In the relational feature f19, we sep-
arate coordination parent-child from other parent-
child relations to help with the class aggregate. f20
determines whether a number is the object of an in-
crease/decrease verb (using a list extracted from In-
quirer) and therefore likely to be the IS class func-
tion. Frequent proper names are more likely to be
hearer old and hence of the class worldKnowledge.
f21 extracts proper names that occur in at least 100
documents in the Tipster corpus to approximate this.
4 Experiments and Results
Experimental setup. We perform experiments on
the corpus provided in Markert et al (2012)6. It con-
sists of 50 texts taken from the WSJ portion of the
OntoNotes corpus (Weischedel et al, 2011) with al-
most 11,000 NPs annotated for information status
including 663 bridging NPs and their antecedents.
All experiments are performed via 10-fold cross-
validation on documents. We use gold standard
mentions and the OntoNotes named entity and syn-
tactic annotation layers for feature extraction.
Reimplemented baseline system (rbls). rbls uses
the same features as Markert et al (2012) (Table 1)
but replaces the local decision tree classifier with
LibSVM as we will need to include lexical features.
rbls + Table 2 feature set (rbls+newfeat). Based
on rbls, all the new features from Table 2 are added.
Cascading minority preference system (cmps).
Minority classes such as bridging suffer during stan-
dard multi-class classification. Inspired by Omuya
6http://www.h-its.org/nlp/download/
isnotes.php
817
collective cascade + collective
markert 12 rbls rbls+newfeat cmps cmps?newfeat
R P F R P F R P F R P F R P F
old 84.1 85.2 84.6 84.6 85.5 85.1 84.4 86.0 85.2 82.2 87.2 84.7 78.9 89.5 83.8
med/worldKnowledge 60.6 70.0 65.0 65.9 69.6 67.7 67.4 77.3 72.0 67.2 77.2 71.9 67.5 66.7 67.1
med/syntactic 75.7 80.1 77.9 77.8 81.2 79.4 82.2 81.9 82.0 81.6 82.5 82.0 73.9 81.7 77.6
med/aggregate 43.1 55.8 48.7 47.9 58.0 52.5 64.5 79.5 71.2 63.5 77.9 70.0 46.9 60.0 52.7
med/function 35.4 53.5 48.7 33.8 56.4 42.3 67.7 72.1 69.8 67.7 72.1 69.8 41.5 50.0 45.4
med/comparative 81.4 82.0 81.7 81.8 82.5 82.1 81.8 82.1 82.0 86.6 78.2 82.2 86.2 78.7 82.3
med/bridging 12.2 41.7 18.9 10.7 36.6 16.6 19.3 39.0 25.8 44.9 39.8 42.2 31.8 23.9 27.3
new 87.7 73.3 79.8 87.5 74.8 80.7 86.5 76.1 81.0 83.0 78.1 80.5 82.4 76.1 79.1
acc 76.8 77.6 78.9 78.6 75.0
Table 3: Experimental results
et al (2013), we develop a cascading minority pref-
erence system for fine-grained IS classification. For
the five minority classes (function, aggregate, com-
parative, bridging and worldKnowledge) that each
make up less than the expected 18 of the data set, we
develop five binary classifiers with LibSVM7 using
all features from Tables 1 and 2 and apply them in
order from rarest to more frequent category. When-
ever a minority classifier predicts true, this class is
assigned. When all minority classifiers say false, we
back off to the multiclass rbls+newfeat system.
cmps ? Table 2 feature set (cmps?newfeat). To
test the effect of using the minority preference sys-
tem without additional features, we employ a cmps
system with baseline features from Table 1 only.
Results and Discussion (Table 3). Our novel
features in rbls+newfeat show improvements for
worldKnowledge, aggregate and function as well as
bridging categories compared to both baseline sys-
tems, although the performance for bridging is still
low. In addition, the overall accuracy is significantly
better than the two baseline systems (at the level of
1% using McNemar?s test). Using the cascaded mi-
nority preference system cmps in addition improves
bridging results substantially while the performance
on other categories does not worsen. The algorithm
needs both our novel feature classes as well as cas-
caded modelling to achieve this improvement as the
comparison to cmps?newfeat shows: the latter low-
ers overall accuracy as it tends to overgenerate rare
7Parameter against data imbalance is set according to the
ratio between positive and negative instances in the training set.
classes (including bridging) with low precision if the
features are not strong enough. Our novel features
(addressing linguistic properties of bridging) and the
cascaded algorithm (addressing data sparseness) ap-
pear to be complementary.
To look at the impact of features in our best sys-
tem, we performed an ablation study. Lexical fea-
tures as well as semantic ones have the most impact.
Discourse structure and genericity information fea-
tures have less of an impact. We believe the latter to
be due to noise involved in extracting these features
(such as approximating coreference for the coher-
ence gap feature) as well as genericity recognition
still being in its infancy (Reiter and Frank, 2010).
5 Conclusions
This paper aims to recognize bridging anaphora in
written text. We develop discourse structure, lexico-
semantic and genericity features based on linguis-
tic intuition and corpus research. By using a cas-
cading minority preference system, we show that
our approach outperforms the bridging recognition
in Markert et al (2012) by a large margin without
impairing the performance on other IS classes.
Acknowledgements. Yufang Hou is funded by a PhD
scholarship from the Research Training Group Coher-
ence in Language Processing at Heidelberg University.
Katja Markert receives a Fellowship for Experienced Re-
searchers by the Alexander-von-Humboldt Foundation.
We thank HITS gGmbH for hosting Katja Markert and
funding the annotation.
818
References
Regina Barzilay and Mirella Lapata. 2008. Modeling
local coherence: An entity-based approach. Computa-
tional Linguistics, 34(1):1?34.
Stefan Baumann and Arndt Riester. 2013. Coreference,
lexical givenness and prosody in German. Lingua.
Accepted.
Aoife Cahill and Arndt Riester. 2012. Automatically ac-
quiring fine-grained information status distinctions in
German. In Proceedings of the SIGdial 2012 Confer-
ence: The 13th Annual Meeting of the Special Interest
Group on Discourse and Dialogue, Seoul, Korea, 5?6
July 2012, pages 232?236.
Tommaso Caselli and Irina Prodanof. 2006. Annotat-
ing bridging anaphors in Italian: In search of reliabil-
ity. In Proceedings of the 5th International Conference
on Language Resources and Evaluation, Genoa, Italy,
22?28 May 2006.
Herbert H. Clark. 1975. Bridging. In Proceedings of the
Conference on Theoretical Issues in Natural Language
Processing, Cambridge, Mass., June 1975, pages 169?
174.
Claire Gardent and He?le`ne Manue?lian. 2005. Cre?ation
d?un corpus annote? pour le traitement des descrip-
tions de?finies. Traitement Automatique des Langues,
46(1):115?140.
Barbara J. Grosz, Aravind K. Joshi, and Scott Weinstein.
1995. Centering: A framework for modeling the lo-
cal coherence of discourse. Computational Linguis-
tics, 21(2):203?225.
Jeanette K. Gundel, Nancy Hedberg, and Ron Zacharski.
1993. Cognitive status and the form of referring ex-
pressions in discourse. Language, 69:274?307.
Udo Hahn, Michael Strube, and Katja Markert. 1996.
Bridging textual ellipses. In Proceedings of the 16th
International Conference on Computational Linguis-
tics, Copenhagen, Denmark, 5?9 August 1996, vol-
ume 1, pages 496?501.
M. A. K. Halliday and Ruqaiya Hasan. 1976. Cohesion
in English. London, U.K.: Longman.
Yufang Hou, Katja Markert, and Michael Strube. 2013.
Global inference for bridging anaphora resolution. In
Proceedings of the 2013 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
Atlanta, Georgia, 9?14 June 2013, pages 907?917.
Emmanuel Lassalle and Pascal Denis. 2011. Leverag-
ing different meronym discovery methods for bridging
resolution in French. In Proceedings of the 8th Dis-
course Anaphora and Anaphor Resolution Colloquium
(DAARC 2011), Faro, Algarve, Portugal, 6?7 October
2011, pages 35?46.
Sebastian Lo?bner. 1985. Definites. Journal of Seman-
tics, 4:279?326.
Katja Markert, Malvina Nissim, and Natalia N. Mod-
jeska. 2003. Using the web for nominal anaphora
resolution. In Proceedings of the EACL Workshop on
the Computational Treatment of Anaphora. Budapest,
Hungary, 14 April 2003, pages 39?46.
Katja Markert, Yufang Hou, and Michael Strube. 2012.
Collective classification for fine-grained information
status. In Proceedings of the 50th Annual Meeting of
the Association for Computational Linguistics, Jeju Is-
land, Korea, 8?14 July 2012, pages 795?804.
Alexis Mitchell, Stephanie Strassel, Mark Przybocki,
JK Davis, George Doddington, Ralph Grishman,
Adam Meyers, Ada Brunstain, Lisa Ferro, and Beth
Sundheim. 2002. ACE-2 Version 1.0. LDC2003T11,
Philadelphia, Penn.: Linguistic Data Consortium.
Natalia M. Modjeska, Katja Markert, and Malvina Nis-
sim. 2003. Using the web in machine learning for
other-anaphora resolution. In Proceedings of the 2003
Conference on Empirical Methods in Natural Lan-
guage Processing, Sapporo, Japan, 11?12 July 2003,
pages 176?183.
Malvina Nissim, Shipara Dingare, Jean Carletta, and
Mark Steedman. 2004. An annotation scheme for in-
formation status in dialogue. In Proceedings of the 4th
International Conference on Language Resources and
Evaluation, Lisbon, Portugal, 26?28 May 2004, pages
1023?1026.
Malvina Nissim. 2006. Learning information status of
discourse entities. In Proceedings of the 2006 Con-
ference on Empirical Methods in Natural Language
Processing, Sydney, Australia, 22?23 July 2006, pages
94?012.
Adinoyi Omuya, Vinodkumar Prabhakaran, and Owen
Rambow. 2013. Improving the quality of minority
class identification in dialog act tagging. In Proceed-
ings of the 2013 Conference of the North American
Chapter of the Association for Computational Linguis-
tics: Human Language Technologies, Atlanta, Geor-
gia, 9?14 June 2013, pages 802?807.
Massimo Poesio and Renata Vieira. 1998. A corpus-
based investigation of definite description use. Com-
putational Linguistics, 24(2):183?216.
Massimo Poesio, Rahul Mehta, Axel Maroudas, and
Janet Hitzeman. 2004a. Learning to resolve bridging
references. In Proceedings of the 42nd Annual Meet-
ing of the Association for Computational Linguistics,
Barcelona, Spain, 21?26 July 2004, pages 143?150.
Massimo Poesio, Rosemary Stevenson, Barbara Di Euge-
nio, and Janet Hitzeman. 2004b. Centering: A para-
metric theory and its instantiations. Computational
Linguistics, 30(3). 309-363.
819
Ellen F. Prince. 1981. Towards a taxonomy of given-new
information. In P. Cole, editor, Radical Pragmatics,
pages 223?255. Academic Press, New York, N.Y.
Altaf Rahman and Vincent Ng. 2012. Learning the fine-
grained information status of discourse entities. In
Proceedings of the 13th Conference of the European
Chapter of the Association for Computational Linguis-
tics, Avignon, France, 23?27 April 2012, pages 798?
807.
Nils Reiter and Anette Frank. 2010. Identifying generic
noun phrases. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, Uppsala, Sweden, 11?16 July 2010, pages 40?49.
Philip J. Stone, Dexter C. Dunphy, Marshall S. Smith,
Daniel M. Ogilvie, and Cambridge Computer Asso-
ciates. 1966. General Inquirer: A Computer Ap-
proach to Content Analysis. MIT Press, Cambridge,
Mass.
Renata Vieira and Massimo Poesio. 2000. An
empirically-based system for processing definite de-
scriptions. Computational Linguistics, 26(4):539?
593.
Ralph Weischedel, Martha Palmer, Mitchell Marcus, Ed-
uard Hovy, Sameer Pradhan, Lance Ramshaw, Ni-
anwen Xue, Ann Taylor, Jeff Kaufman, Michelle
Franchini, Mohammed El-Bachouti, Robert Belvin,
and Ann Houston. 2011. OntoNotes release 4.0.
LDC2011T03, Philadelphia, Penn.: Linguistic Data
Consortium.
820
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2082?2093,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
A Rule-Based System for Unrestricted Bridging Resolution:
Recognizing Bridging Anaphora and Finding Links to Antecedents
Yufang Hou
1
, Katja Markert
2
, Michael Strube
1
1
Heidelberg Institute for Theoretical Studies gGmbH, Heidelberg, Germany
(yufang.hou|michael.strube)@h-its.org
2
School of Computing, University of Leeds, UK
scskm@leeds.ac.uk
Abstract
Bridging resolution plays an important
role in establishing (local) entity coher-
ence. This paper proposes a rule-based
approach for the challenging task of unre-
stricted bridging resolution, where bridg-
ing anaphors are not limited to defi-
nite NPs and semantic relations between
anaphors and their antecedents are not re-
stricted to meronymic relations. The sys-
tem consists of eight rules which target
different relations based on linguistic in-
sights. Our rule-based system significantly
outperforms a reimplementation of a pre-
vious rule-based system (Vieira and Poe-
sio, 2000). Furthermore, it performs better
than a learning-based approach which has
access to the same knowledge resources
as the rule-based system. Additionally,
incorporating the rules and more features
into the learning-based system yields a mi-
nor improvement over the rule-based sys-
tem.
1 Introduction
Bridging resolution recovers the various non-
identity relations between anaphora and an-
tecedents. It plays an important role in establish-
ing entity coherence in a text. In Example 1, the
links between the bridging anaphors (The five as-
tronauts and touchdown) and the antecedent (The
space shuttle Atlantis) establish (local) entity co-
herence.
1
(1) The space shuttle Atlantis landed at a desert
air strip at Edwards Air Force Base, Calif.,
ending a five-day mission that dispatched
the Jupiter-bound Galileo space probe. The
1
Examples are from OntoNotes (Weischedel et al., 2011).
Bridging anaphora are typed in boldface; antecedents in ital-
ics.
five astronauts returned to Earth about three
hours early because high winds had been pre-
dicted at the landing site. Fog shrouded the
base before touchdown.
Bridging or associative anaphora has been
widely discussed in the linguistic literature (Clark,
1975; Prince, 1981; Gundel et al., 1993;
L?obner, 1998). Poesio and Vieira (1998) and
Bunescu (2003) include cases where antecedent
and anaphor are coreferent but do not share the
same head noun (different-head coreference). We
follow our previous work (Hou et al., 2013b) and
restrict bridging to non-coreferential cases. We
also exclude comparative anaphora (Modjeska et
al., 2003).
Bridging resolution includes two subtasks: (1)
recognizing bridging anaphors and (2) finding the
correct antecedent among candidates. In recent
empirical work, these two subtasks have been
tackled separately: (Markert et al., 2012; Cahill
and Riester, 2012; Rahman and Ng, 2012; Hou et
al., 2013a) handle bridging recognition as part of
information status (IS) classification, while (Poe-
sio et al., 1997; Poesio et al., 2004; Markert et
al., 2003; Lassalle and Denis, 2011; Hou et al.,
2013b) concentrate on antecedent selection only,
assuming that bridging recognition has already
been performed. One exception is Vieira and Poe-
sio (2000). They propose a rule-based system for
processing definite NPs. However, they include
different-head coreference into bridging. They re-
port results for the whole anaphora resolution but
do not report results for bridging resolution only.
Another exception is R?osiger and Teufel (2014).
They apply a coreference resolution system with
several additional semantic features to find bridg-
ing links in scientific text where bridging anaphors
are limited to definite NPs. They report prelim-
inary results using the CoNLL scorer. However,
we think the coreference resolution system and the
evaluation metric for coreference resolution are
2082
not suitable for bridging resolution since bridging
is not a set problem.
Another vein of research for bridging resolu-
tion focuses on formal semantics. Asher and Las-
carides (1998) and Cimiano (2006) model bridg-
ing by integrating discourse structure and seman-
tics from a formal semantics viewpoint. However,
the implementation of such a theoretical frame-
work is beyond the current capabilities of NLP
since it depends heavily on commonsense entail-
ment.
In this paper, we propose a rule-based system
for unrestricted bridging resolution. The system
consists of eight rules which we carefully design
based on linguistic intuitions, i.e., how the nature
of bridging is reflected by various lexical, syntac-
tic and semantic features. We evaluate our rule-
based system on a corpus where bridging is reli-
ably annotated. We find that our rule-based sys-
tem significantly outperforms a reimplementation
of a previous rule-based system (Vieira and Poe-
sio, 2000). We further notice that our rule-based
system performs better than a learning-based ap-
proach which has access to the same knowledge
resources as the rule-based system. Surprisingly,
incorporating the rules and more features into the
learning-based approach only yields a minor im-
provement over the rule-based system. We ob-
serve that diverse bridging relations and relatively
small-scale data for each type of relations make
generalization difficult for the learning-based ap-
proach. This work is ? to the best of our
knowledge ? the first system recognizing bridging
anaphora and finding links to antecedents for unre-
stricted phenomenon where bridging anaphors are
not limited to definite NPs and semantic relations
between anaphors and their antecedents are not re-
stricted to meronymic relations.
2 Data
All the data used throughout the paper come
from the ISNotes corpus
2
released by Hou et al.
(2013b). This corpus contains around 11,000 NPs
annotated for information status including 663
bridging NPs and their antecedents in 50 texts
taken from the WSJ portion of the OntoNotes cor-
pus (Weischedel et al., 2011). ISNotes is reli-
ably annotated for bridging: for bridging anaphor
recognition, ? is over 60 for all three possible an-
2
http://www.h-its.org/english/research/nlp/download/
isnotes.php
notator pairings (? is over 70 for two expert anno-
tators); for selecting bridging antecedents, agree-
ment is around 80% for all annotator pairings.
It is notable that bridging anaphors in ISNotes
are not limited to definite NPs as in previous work
(Poesio et al., 1997; Poesio et al., 2004; Lassalle
and Denis, 2011). Table 1 shows the bridging
Bridging Anaphors 663
Non-determiner 44.9%
Definite 38.5%
Indefinite 15.4%
Other-determiner 1.2%
Table 1: Bridging anaphora distribution w.r.t. de-
terminers in ISNotes.
anaphora distribution with regard to determiners in
ISNotes: only around 38% of bridging anaphors
are definite NPs (NPs modified by the); 15.4%
of bridging anaphors are modified by determiners
such as a, an or one which normally indicate in-
definite NPs. Most bridging anaphors (43%) are
not modified by any determiners, such as touch-
down in Example 1. A small fraction of bridging
anaphors (1.2%) are modified by other determin-
ers, such as demonstratives.
The semantic relations between anaphor and
antecedent in the corpus are extremely diverse:
only 14% of anaphors have a part-of/attribute-
of relation with the antecedent (see Example 2)
and only 7% of anaphors stand in a set relation-
ship to the antecedent (see Example 3). 79%
of anaphors have ?other? relations with their an-
tecedents (without further distinction), including
encyclopedic relations such as The space shut-
tle Atlantis-The five astronauts (see Example 1)
as well as context-specific relations such as The
space shuttle Atlantis-touchdown (Example 1).
(2) At age eight, Josephine Baker was sent by
her mother to a white women?s house to do
chores in exchange for meals and a place to
sleep ? a place in the basement with coal.
(3) This creates several problems. One is that
there are not enough police to satisfy small
businesses.
In ISNotes, bridging anaphora with distant an-
tecedents are common when the antecedent is the
global focus of a document. 29% of the anaphors
in the corpus have antecedents that are three or
more sentences away.
2083
Bridging resolution is an extremely challeng-
ing task in ISNotes. In contrast with surface clues
for coreference resolution, there are no clear sur-
face clues for bridging resolution. In Example 4,
the bridging anaphor low-interest disaster loans
associates to the antecedent the Carolinas and
Caribbean, whereas in Example 5 the NP loans is
a generic use. In Example 6, the bridging anaphor
The opening show associates to the antecedent
Mancuso FBI, whereas the NP the show is coref-
erent with its antecedent Mancuso FBI.
(4) The $2.85 billion measure comes on top of
$1.1 billion appropriated after Hugo stuck
the Carolinas and Caribbean last month, and
these totals don?t reflect the additional benefit
of low-interest disaster loans.
(5) Many states already have Enterprise Zones
and legislation that combines tax incentives,
loans, and grants to encourage investment in
depressed areas.
(6) Over the first few weeks, Mancuso FBI has
sprung straight from the headlines. The
opening show featured a secretary of defense
designate accused of womanizing (a la John
Tower).
. . .
Most of all though, the show is redeemed
by the character of Mancuso.
Our previous work on bridging resolution on
this corpus only focuses on its subtasks. In
Hou et al. (2013a) we model bridging anaphora
recognition as a subtask of learning fine-grained
information status. We report an F-measure
of 0.42 for bridging anaphora recognition. In
Hou et al. (2013b) we propose a joint inference
framework for antecedent selection by exploring
Markov logic networks. We report an accuracy
of 0.41 for antecedent selection given gold bridg-
ing anaphora. In this paper, we aim to solve these
two substasks together, i.e., recognizing bridging
anaphora and finding links to antecedents.
3 Method
In this section, we describe our rule-based system
for unrestricted bridging resolution. We choose
ten documents randomly from the corpus as the
development set. Then we carefully design rules
for finding ?bridging links? among all NPs in a
document based on the generalizations of bridg-
ing in the linguistic literature as well as our in-
spections of bridging annotations in the develop-
ment set. The system consists of two components:
bridging link prediction and post processing.
3.1 Bridging Link Prediction
The bridging link prediction component consists
of eight rules. L?obner (1985; 1998) interprets
bridging anaphora as a particular kind of func-
tional concept, which in a given situation assign
a necessarily unique correlate to a (implicit) pos-
sessor argument. He distinguishes between rela-
tional nouns (e.g. parts terms, kinship terms, role
terms) and sortal nouns and points out that rela-
tional nouns are more frequently used as bridg-
ing anaphora than sortal nouns. Rule1 to Rule4 in
our system aim to resolve such relational nouns.
We design Rule5 and Rule6 to capture set bridg-
ing. Finally, Rule7 and Rule8 are motivated by
previous work on implicit semantic role labeling
(Laparra and Rigau, 2013) which focuses on few
predicates.
For all NPs in a document, each rule r is applied
separately to predict a set of potential bridging
links. Every rule has its own constraints on bridg-
ing anaphora and antecedents respectively. Bridg-
ing anaphors are diverse with regard to syntactic
form and function: they can be modified by def-
inite or indefinite determiners (Table 1), further-
more they can take the subject (e.g. Example 3
and Example 6) or other positions (e.g. Example
2 and Example 4) in sentences. The only fre-
quent syntactic property shared is that bridging
anaphors most often have a simple internal struc-
ture concerning modification. Therefore we first
create an initial list of potential bridging anaphora
A which excludes NPs which have a complex syn-
tactic structure. An NP is added to A if it does
not contain any other NPs and do not have modifi-
cations strongly indicating comparative NPs (such
as other symptoms)
3
. Since head match is a strong
indicator of coreference anaphora for definite NPs
(Vieira and Poesio, 2000; Soon et al., 2001), we
further exclude definite NPs from A if they have
the same head as a previous NP. Then a set of
potential bridging anaphors A
r
is chosen from A
based on r?s constraints on bridging anaphora. Fi-
nally, for each potential bridging anaphor ana ?
3
A small list of 10 markers such as such, another . . . and
the presence of adjectives or adverbs in the comparative form
are used to predict comparative NPs.
2084
Ar
, a single best antecedent ante from a list of
candidate NPs (C
ana
) is chosen if the rule?s con-
straint on antecedents is applied successfully.
Every rule has its own scope to form the
antecedent candidate set C
ana
. Instead of using
a static sentence window to construct the list of
antecedent candidates like most previous work for
resolving bridging anaphora (Poesio et al., 1997;
Markert et al., 2003; Poesio et al., 2004; Lassalle
and Denis, 2011), we use the development set
to estimate the proper scope for each rule. The
scope is influenced by the following factors: (1)
the nature of the target bridging link (e.g., set
bridging is a local coherence phenomenon where
the antecedent often occurs in the same or up
to two sentences prior to the anaphor); and (2)
the strength of the rule?s constraint to select the
correct antecedent (e.g., in Rule8, the ability
to select the correct antecedent decreases with
increasing the scope to contain more antecedent
candidates). In the following, we describe the mo-
tivation for each rule and their constraints in detail.
Rule1: building part NPs. To capture typical
part-of bridging (see Example 2), we extract a
list of 45 nouns which specify building parts (e.g.
room or roof ) from the General Inquirer lexicon
(Stone et al., 1966). A common noun phrase from
A is added to A
r1
if: (1) its head appears in the
building part list; and (2) it does not contain any
nominal pre-modifications. Then for each poten-
tial bridging anaphor ana ? A
r1
, the NP with
the strongest semantic connectivity to the potential
anaphor ana among all NPs preceding ana from
the same sentence as well as from the previous two
sentences is predicted to be the antecedent.
The semantic connectivity of an NP to a po-
tential anaphor is measured via the hit counts of
the preposition pattern query (anaphor preposi-
tion NP) in big corpora
4
. An initial effort to ex-
tract partOf relations using WordNet yields low
recall on the development set. Therefore we use
semantic connectivity expressed by prepositional
patterns (e.g. the basement of the house) to cap-
ture underlying semantic relations. Such syntactic
patterns are also explored in Poesio et al. (2004) to
resolve meronymy bridging.
4
We use Gigaword (Parker et al., 2011) with automatic
POS tag and NP chunk information.
Rule2: relative person NPs. This rule is used
to capture the bridging relation between a relative
(e.g. The husband) and its antecedent (e.g. She).
A list of 110 such relative nouns is extracted from
WordNet. However, some relative nouns are fre-
quently used generically instead of being bridging,
such as children. To exclude such cases, we com-
pute the argument taking ratio ? for an NP using
NomBank (Meyers et al., 2004). For each NP, ? is
calculated via its head frequency in the NomBank
annotation divided by the head?s total frequency
in the WSJ corpus in which the NomBank anno-
tation is conducted. The value of ? reflects how
likely an NP is to take arguments. For instance,
the value of ? is 0.90 for husband but 0.31 for
children. To predict bridging anaphora more ac-
curately, a conservative constraint is used. An NP
from A is added to A
r2
if: (1) its head appears in
the relative person list; (2) its argument taking ra-
tio ? is bigger than 0.5; and (3) it does not contain
any nominal or adjective pre-modifications. Then
for each potential bridging anaphor ana ? A
r2
,
the closest non-relative person NP among all NPs
preceding ana from the same sentence as well as
from the previous two sentences is chosen as its
antecedent.
Rule3: GPE job title NPs. In news articles, it is
common that a globally salient geo-political entity
(hence GPE, e.g. Japan or U.S.) is introduced in
the beginning, then later a related job title NP (e.g.
officials or the prime minister) is used directly
without referring to this GPE explicitly. To resolve
such bridging cases accurately, we compile a list
of twelve job titles which are related to GPEs (e.g.
mayor or official). An NP from A is added to A
r3
if its head appears in this list and does not have a
country pre-modification (e.g. the Egyptian pres-
ident). Then for each potential bridging anaphor
ana ? A
r3
, the most salient GPE NP among all
NPs preceding ana is predicted as its antecedent.
We use the NP?s frequency in the whole document
to measure its salience throughout the paper. In
case of a tie, the closest one is chosen to be the
predicted antecedent.
Rule4: role NPs. Compared to Rule3, Rule4
is designed to resolve more general role NPs to
their implicit possessor arguments. We extract a
list containing around 100 nouns which specify
professional roles from WordNet (e.g. chairman,
president or professor). An NP from A is added to
2085
Ar4
if its head appears in this list. Then for each
potential bridging anaphor ana ? A
r4
, the most
salient proper name NP which stands for an orga-
nization among all NPs preceding ana from the
same sentence as well as from the previous four
sentences is chosen as its antecedent (if such an
NP exists). Recency is again used to break ties.
Rule5: percentage NPs. In set bridging as
shown in Example 7, the anaphor (Seventeen per-
cent) is indicated by a percentage expression from
A, which is often in the subject position. The an-
tecedent (the firms) is predicted to be the closest
NP which modifies another percentage NP via the
preposition ?of? among all NPs occurring in the
same or up to two sentences prior to the potential
anaphor.
(7) 22% of the firms said employees or owners
had been robbed on their way to or from
work. Seventeen percent reported their cus-
tomers being robbed.
Rule6: other set member NPs. In set bridg-
ing, apart from percentage expressions, numbers
or indefinite pronouns are also good indicators for
bridging anaphora. For such cases, the anaphor
is predicted if it is: (1) a number expression (e.g.
One in Example 3) or an indefinite pronoun(e.g.
some, as shown in Example 8) from A; and (2) a
subject NP. The antecedent is predicted to be the
closest NP among all plural, subject NPs preced-
ing the potential anaphor from the same sentence
as well as from the previous two sentences (e.g.
Reds and yellows in Example 8). If such an NP
does not exist, the closest NP among all plural, ob-
ject NPs preceding the potential anaphor from the
same sentence as well as from the previous two
sentences is chosen to be the predicted antecedent
(e.g. several problems in Example 3).
(8) Reds and yellows went about their business
with a kind of measured grimness. Some
frantically dumped belongings into pillow-
cases.
Rule7: argument-taking NPs I. Laparra and
Rigau (2013) found that different instances of the
same predicate in a document likely maintain the
same argument fillers. Here we follow this as-
sumption but apply it to nouns and their nomi-
nal modifiers only: different instances of the same
noun predicate likely maintain the same argument
fillers indicated by nominal modifiers. First, a
common noun phrase from A is added to A
r7
if:
(1) its argument taking ratio ? is bigger than 0.5;
(2) it does not contain any nominal or adjective
pre-modifications; and (3) it is not modified by in-
definite determiners
5
which usually introduce new
discourse referents (Hawkins, 1978). Then for
each potential bridging anaphor ana ? A
r7
, we
choose the antecedent by performing the follow-
ing steps:
1. We take ana?s head lemma form ana
h
and collect all its syntactic modifications in
the document. We consider nominal pre-
modification, possessive modification as well
as prepositional post-modification. All real-
izations of these modifications which precede
ana form the antecedent candidates setC
ana
.
2. We choose the most recent NP from C
ana
as the predicted antecedent for the potential
bridging anaphor ana.
In Example 9, we first predict the two occur-
rences of residents as bridging anaphors. Since
in the text, other occurrences of the lemma ?res-
ident? are modified by ?Marina? (supported by
Marina residents) and ?buildings? (supported by
some residents of badly damaged buildings), we
collect all NPs whose syntactic head is ?Ma-
rina? or ?buildings? in C
ana
(i.e. Marina, badly
damaged buildings and buildings with substan-
tial damage). Then among all NPs in C
ana
, the
most recent NP is chosen to be the antecedent (i.e.
buildings with substantial damage).
(9) She finds the response of Marina residents to
the devastation of their homes ?incredible?.
. . .
Out on the streets, some residents of badly
damaged buildings were allowed a 15 minute
scavenger hunt through their possessions.
. . .
After being inspected, buildings with sub-
stantial damage were color - coded.
Green allowed residents to re-enter; red
allowed residents one last entry to gather
everything they could within 15 minutes.
Rule8: argument-taking NPs II. Prince (1992)
found that discourse-old entities are more likely
5
We compile a list of 17 such determiners, such as a, an
or one.
2086
to be represented by NPs in subject position.
Although she could not draw a similar conclu-
sion when collapsing Inferrable (= bridging) with
Discourse-old Nonpronominal, we find that in the
development set, an argument-taking NP in the
subject position is a good indicator for bridging
anaphora (e.g. participants in Example 10). A
common noun phrase from A is collected in A
r8
if: (1) its argument taking ratio ? is bigger than
0.5; (2) it does not contain any nominal or adjec-
tive pre-modifications; and (3) it is in the subject
position. Semantic connectivity again is used as
the criteria to choose the antecedent: for each po-
tential bridging anaphor ana ? A
r8
, the NP with
the strongest semantic connectivity to ana among
all NPs preceding ana from the same sentence as
well as from the previous two sentences is pre-
dicted to be the antecedent.
(10) Initial steps were taken at Poland?s first in-
ternational environmental conference, which
I attended last month. . . . While Polish data
have been freely available since 1980, it was
no accident that participants urged the free
flow of information.
3.2 Post-processing
In the bridging link prediction component, each
rule is applied separately. To resolve the conflicts
between different rules (e.g., two rules predict dif-
ferent antecedents for the same potential anaphor),
a post processing step is applied. We first order
the rules according to their precision for predicting
bridging pairs (i.e., recognizing bridging anaphors
and finding links to antecedents) in the develop-
ment set. When a conflict happens, the rule with
the highest order has the priority to decide the an-
tecedent. Table 2 summarizes the rules described
in Section 3.1, the numbers in square brackets in
the first column indicate the order of the rules. Ta-
ble 3 shows the precisions of bridging anaphora
recognition and bridging pairs prediction for each
rule in the development set. Firing rate is the
proportion of bridging links predicted by rule r
among all predicted links.
4 Experiments and Results
4.1 Experimental Setup
We conduct all experiments on the ISNotes cor-
pus. We use the OntoNotes named entity and syn-
tactic annotations to extract features. Ten doc-
uments containing 113 bridging anaphors from
the ISNotes corpus are set as the development set
to estimate parameters for the rule-based system.
The remaining 40 documents are used as the test
set. In order to compare the results of different
systems directly, we evaluate all systems on the
test set.
4.2 Evaluation Metric
In ISNotes, bridging is annotated mostly between
an NP (anaphor) and an entity (antecedent)
6
, so
that a bridging anaphor could have multiple links
to different instantiations of the same entity (entity
information is based on the Ontonotes coreference
annotation). For bridging resolution, we use an
evaluation metric based on bridging anaphors in-
stead of all links between bridging anaphors and
their antecedent instantiations. A link predicted by
the system is counted as correct if it recognizes the
bridging anaphor correctly and links the anaphor
to any instantiation of the right antecedent entity
preceding the anaphor.
In the evaluation metric, recall is calculated
via the number of the correct links predicted by
the system (one unique link per each predicted
anaphor) divided by the total number of the gold
bridging anaphors, precision is calculated via the
number of the correct links predicted by the sys-
tem divided by the total links predicted by the sys-
tem.
4.3 A Learning-based Approach
To compare our rule-based system (hence ruleSys-
tem, described in Section 3) with other ap-
proaches, we implement a learning-based system
for unrestricted bridging resolution. We adapt the
pairwise model which is widely used in corefer-
ence resolution (Soon et al., 2001). Similar to
the rule-based system, we first create an initial list
of possible bridging anaphora A
ml
with one more
constraint. The purpose is to exclude as many ob-
vious non-bridging anaphoric NPs from the list
as possible. An NP is added to A
ml
if: (1) it
does not contain any other NPs; (2) it is not mod-
ified by pre-modifications which strongly indicate
comparative NPs; and (3) it is not a pronoun or a
proper name. Then for each NP a ? A
ml
, a list
of antecedent candidates C
a
is created by includ-
ing all NPs preceding a from the same sentence
6
There are a few cases where bridging is annotated be-
tween an NP and a non-NP antecedent (e.g. verbs or clauses).
2087
antecedent
rule anaphor antecedent
candidates scope
rule1 [2] building part NPs the NP with the strongest semantic connectivity to the two
potential anaphor
rule2 [5] relative person NPs the closest person NP which is not a relative NP two
rule3 [6] GPE job title NPs the most salient GPE NP all
rule4 [7] role NPs the most salient organization NP four
rule5 [1] percentage NPs the closest NP which modifies another percentage NP two
via the preposition ?of?
rule6 [3] other set member NPs the closest subject, plural NP; two
otherwise the closest object, plural NP
rule7 [4] argument-taking NPs I the closest NP whose head is an unfilled role of the potential all
anaphor (such a role is predicted via syntactic modifications of NPs
which have the same head as the potential anaphor)
rule8 [8] argument-taking NPs II the NP with the strongest semantic connectivity to the two
potential anaphor
Table 2: Rules for unrestricted bridging resolution. Antecedent candidates scope are verified in the
development set: ?all? represents all NPs preceding the potential anaphor from the whole document,
?four? NPs occurring in the same or up to four sentences prior to the potential anaphor, ?two? NPs
occurring in the same or up to two sentences prior to the potential anaphor.
anaphora recognition bridging pairs prediction
rule anaphora
precision precision
firing rate
rule1 [2] building part NPs 75.0% 50.0% 6.1%
rule2 [5] relative person NPs 69.2% 46.2% 6.1%
rule3 [6] GPE job title NPs 52.6% 44.7% 19.4%
rule4 [7] role NPs 61.7% 32.1% 28.6%
rule5 [1] percentage NPs 100.0% 100.0% 2.6%
rule6 [3] other set member NPs 66.7% 46.7% 7.8%
rule7 [4] argument-taking NPs I 53.8% 46.4% 6.1%
rule8 [8] argument-taking NPs II 64.5% 25.0% 25.5%
Table 3: Precision of bridging anaphora recognition and bridging pairs prediction for each rule in the
development set. The numbers in square brackets in the first column indicate the order of the rules.
as well as from the previous two sentences
7
. We
create a pairwise instance (a, c) for every c ? C
a
.
We also add extra pairwise instances from the pre-
diction of ruleSystem to the learning-based sys-
tem. In the decoding stage, the best first strat-
egy (Ng and Cardie, 2002) is used to predict the
bridging links. Specifically, for each a ? A
ml
, we
predict the bridging link to be the most confident
pair (a, c
ante
) among all instances with the posi-
tive prediction. We use SVM
light
to conduct the
experiments
8
. All experiments are conducted via
10-fold cross-validation on the whole corpus
9
.
7
In ISNotes, 71% of NP antecedents occur in the same
or up to two sentences prior to the anaphor. Initial experi-
ments show that increasing the window size more than two
sentences decreases the performance.
8
To deal with data imbalance, the SVM
light
parameter
is set according to the ratio between positive and negative
instances in the training set.
9
To compare the learning-based approach to the rule-
based system described in Section 3 directly, we report the
mlSystem ruleFeats We provide mlSys-
tem ruleFeats with the same knowledge resources
as the rule-based system. All rules from the
rule-based system are incorporated into mlSys-
tem ruleFeats as the features.
mlSystem ruleFeats + atomFeats We augment
mlSystem ruleFeats with more features from our
previous work (Markert et al., 2012; Hou et al.,
2013a; Hou et al., 2013b) on bridging anaphora
recognition and antecedent selection. Some of
these features overlap with the atomic features
used in the rule-based system.
Table 4 shows all the features we use for rec-
ognizing bridging anaphora. ??? indicates the re-
sources are used in the rule-based system. We ap-
ply them to the first element a of a pairwise in-
stance (a, c). Markert et al. (2012) and Hou et
results of learning-based approaches on the same test set as
the rule-based system.
2088
Markert et al. local feature set
f1 FullPrevMention (b) ? f2 FullPreMentionTime (n) f3 PartialPreMention (b)
f4 ContentWordPreMention (b) f5 Determiner (n) ? f6 NPtype (n) ?
f7 NPlength (int) f8 GrammaticalRole (n) ? f9 NPNumber (n) ?
f10 PreModByCompMarker (b) ?
Hou et al. local feature set
features to identify bridging anaphora
f1 IsCoherenceGap (b) f2 IsSentFirstMention (b) f3 IsDocFirstMention (b)
f4 IsWordNetRelationalNoun (b) ? f5 IsInquirerRoleNoun (b) f6 IsBuildingPart (b) ?
f7 IsSetElement (b) ? f8 PreModSpatialTemporal (b) f9 IsYearExpression (b)
f10 PreModifiedByCountry (b) ? f11 AppearInIfClause (b) f12 VerbPosTag (l)
f13 IsFrequentGenericNP (b) f14 WorldKnowledgeNP (l) f15 Unigrams (l)
f16 PreModByGeneralQuantifier (b) f17 BridgingHeadNP (l) f18 HasChildNP (b) ?
features to identify function and worldKnowledge NPs
f20 DependOnChangeVerb (b) f21 IsFrequentProperName (b)
Table 4: Features for bridging anaphora recognition from Markert et al. (2012) and Hou et al. (2013a).
?b? indicates binary, ?n? nominal, ?l? lexical features, ??? resources used in the rule-based system.
Group Feature Value
semantic f1 preposition pattern ? the normalized hit counts of the preposition pattern query
a prep. c (e.g. participants of the conference) in big corpora
f2 verb pattern the normalized hit counts of the verb pattern query c verb
a
or
verb
a
c in big corpora (for set bridging in Example 7, the
pattern query is the firms reported)
f3 WordNet partOf whether a partOf relation holds between a and c in WordNet
f4 semantic class ? 16 classes, e.g. location, organization, GPE, rolePerson,
relativePerson, product, date, money, percent
salience f5 document span the normalized value of the span of text in which c is mentioned
f6 utterance distance the sentence distance between a and c
f7 local first mention whether c is the first mention within the previous five sentences
f8 global first mention whether c is the first mention in the whole document
syntactic f9 isSameHead whether a and c share the same head
& (exclude coreferent antecedent candidates)
lexical f10 isWordOverlap whether a is prenominally modified by the head of c (for
bridging where the anaphor is a compound noun, such as
the mine-mine security)
f11 isCoArgument whether subject c and object a are dependent on the same verb
(the subject can not be the bridging antecedent of the object
in the same clause)
f12 WordNet distance the inverse value of the shortest path length between a and c
in WordNet
Table 5: Features for antecedent selection from Hou et al. (2013b). ??? indicates resources used in the
rule-based system.
al. (2013a) classify eight fine-grained information
status (IS) categories for NPs: old, new and 6
mediated categories (syntactic, worldKnowledge,
bridging, comparative, aggregate and function).
Features from Markert et al. (2012) work well to
identify old, new and several mediated categories
but fail to recognize most bridging anaphora. Hou
et al. (2013a) remedy this by adding discourse
structure features (f1-f3), semantic features (f4-
f10) and features to detect generic nouns (f11-
2089
Feature Value
for anaphor candidate a
f1 preModByNominal whether a contains any nominal pre-modifications
f2 preModByAdj whether a contains any adjective modifications
f3 isGPEJobTitle whether a is a job title about GPE (e.g. mayor or official)
f4 isArgumentTakingNP whether the argument taking ratio of a is bigger than 0.5
for antecedent candidate c
f5 fullMentionTime the normalized value of the frequency of c in the whole document
for pairwise instance (a, c)
f6 word distance the token distance between a and c
Table 6: Additional atomic features from the rule-based system.
f14 and f16).
Table 5 shows all features we use for selecting
antecedents for bridging anaphora. ??? indicates
the resources that are used in the rule-based sys-
tem. These features are from Hou et al. (2013b)?s
local pairwise model. They try to model: (1) the
semantic relations between bridging anaphors and
their antecedents (f1 to f4); (2) the salience of
an antecedent from different perspectives (f5 to
f8); and (3) the syntactic and lexical constraints
between anaphor and antecedent (f9 to f12).
Apart from the features shown in Table
4 and Table 5, we further enrich mlSys-
tem ruleFeats+atomFeats with additional atomic
features used in the rule-based system (Table 6).
mlSystem atomFeats Based on mlSys-
tem ruleFeats+atomFeats, the rule features
from the rule-based system are removed.
4.4 Baseline
We also reimplement the rule-based system from
Vieira and Poesio (2000) as a baseline. The origi-
nal algorithm focuses on processing definite NPs.
It classifies four categories for the definite NPs:
discourse new, direct anaphora (same-head coref-
erent anaphora), lenient bridging and Unknown.
This algorithm also finds antecedents for NPs
which belong to direct anaphora or lenient bridg-
ing.
Since Vieira and Poesio (2000) include
different-head coreference into their lenient
bridging category, we further divide their le-
nient bridging category into two subcategories:
different-head coreference and bridging. Figure
1 shows the details of the division after failing
to classify an NP as discourse new or direct
anaphora. For more details about the whole
system, see Vieira and Poesio (2000). We then
apply this slightly revised algorithm to process
all NPs in the initial list of potential bridging
anaphoraA from ruleSystem (described in Section
3.1).
4.5 Results and Discussion
Table 7 shows the results on the same test set of
different approaches for unrestricted bridging res-
olution. The results reveal the difficulty of the
task, when evaluating on a realistic scenario with-
out constraints on types of bridging anaphora and
bridging relations.
Both our rule-based system and all learning-
based approaches significantly outperform the
baseline at p < 0.01 (randomization test). The
low recall in baseline is predictable, since it
only considers meronymy bridging and compound
noun anaphors whose head is prenominally mod-
ified by the antecedent head. (e.g. the state?
state gasoline taxes). Under the same features,
the learning-based approach (mlSystem ruleFeats)
performs slightly worse than the rule-based sys-
tem (ruleSystem) with regard to the F-score.
R P F
baseline 2.9 13.3 4.8
ruleSystem 11.9 42.9 18.6
mlSystem ruleFeats 12.1 35.0 18.0
mlSystem ruleFeats+atomFeats 16.7 21.2 18.7
mlSystem atomFeats 20.5 10.1 13.5
Table 7: Experimental results for the baseline, the
rule-based system and the learning-based systems.
Surprisingly, incorporating rich features
into the learning-based approach (mlSys-
tem ruleFeats+atomFeats) does not yield much
improvement over the rule-based system (with an
2090
Figure 1: Vieria & Poesio?s (2000) original algorithm for processing definite NPs. We further divide
their lenient bridging category into two subcategories: 2.1 Different-head coreference and 2.2 Bridging.
F-score of 18.7 in mlSystem ruleFeats+atomFeats
compared to 18.6 in ruleSystem). We suppose that
the learning-based system generalizes poorly with
only atomic features in Table 4, Table 5 and Table
6. Results on mlSystem atomFeats support our
assumption: the F-score drops considerably after
removing the rule features. Although ISNotes is
a reasonably sized corpus for bridging compared
to previous work, diverse bridging relations,
especially lots of context specific relations such
as pachinko-devotees or palms-the thieves, lead
to relatively small-scale training data for each
type of relation. Therefore it is difficult for the
learning-based approach to learn effective rules to
predict bridging links.
However, all learning-based systems tend to
have higher recall but lower precision compared
to the rule-based system. This suggests that the
learning-based systems are ?greedy? to predict
bridging links. A close look at these links in
mlSystem atomFeats indicates that the learning-
based system predicts more correct bridging
anaphors but fails to find the correct antecedents.
In fact, lots of those ?half? correct links sound
reasonable without the specific context, such as
the story-readers (gold bridging link: this novel-
readers) or the executive director?s office-the
desks (gold bridging link: the fund?s building-the
desks).
5 Conclusions
We proposed a rule-based approach for un-
restricted bridging resolution where bridging
anaphora are not limited to definite NPs and the
relations between anaphor and antecedent are not
restricted to meronymic relations. We designed
eight rules to resolve bridging based on linguis-
tic intuition. Our rule-based system performs bet-
ter than a learning-based approach which has ac-
cess to the same knowledge resources as the rule-
based system. Particularly, the learning-based sys-
tem enriched with more features does not yield
much improvement over the rule-based system.
We speculate that the learning-based system could
benefit from more training data. Furthermore, bet-
ter methods to model the semantics of the specific
context need to be explored in the future.
This work is ? to our knowledge ? the first
bridging resolution system that handles the unre-
stricted phenomenon in a realistic setting.
Acknowledgements
We thank Renata Vieira for excavating part of her
source code for us. We also thank the reviewers
for their helpful comments. Yufang Hou is funded
by a PhD scholarship from the Research Training
Group Coherence in Language Processing at Hei-
delberg University. This work has been partially
funded by the Klaus Tschira Foundation.
2091
References
Nicholas Asher and Alex Lascarides. 1998. Bridging.
Journal of Semantics, 15:83?113.
Razvan Bunescu. 2003. Associative anaphora resolu-
tion: A Web-based approach. In Proceedings of the
EACL 2003 Workshop on The Computational Treat-
ment of Anaphora, Budapest, Hungary, 14 April,
2003, pages 47?52.
Aoife Cahill and Arndt Riester. 2012. Automati-
cally acquiring fine-grained information status dis-
tinctions in German. In Proceedings of the SIGdial
2012 Conference: The 13th Annual Meeting of the
Special Interest Group on Discourse and Dialogue,
Seoul, Korea, 5?6 July 2012, pages 232?236.
Philipp Cimiano. 2006. Ingredients of a first-order ac-
count of bridging. In Proceedings of the 5th Inter-
national Workshop on Inference in Computational
Semantics, Buxton, U.K., 20?21 April 2006, pages
139?144.
Herbert H. Clark. 1975. Bridging. In Proceedings
of the Conference on Theoretical Issues in Natu-
ral Language Processing, Cambridge, Mass., June
1975, pages 169?174.
Jeanette K. Gundel, Nancy Hedberg, and Ron
Zacharski. 1993. Cognitive status and the form
of referring expressions in discourse. Language,
69:274?307.
John A. Hawkins. 1978. Definiteness and indefinite-
ness: A study in reference and grammaticality pre-
diction. Humanities Press, Atlantic Highlands, N.J.
Yufang Hou, Katja Markert, and Michael Strube.
2013a. Cascading collective classification for bridg-
ing anaphora recognition using a rich linguistic fea-
ture set. In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Process-
ing, Seattle, Wash., 18?21 October 2013, pages 814?
820.
Yufang Hou, Katja Markert, and Michael Strube.
2013b. Global inference for bridging anaphora res-
olution. In Proceedings of the 2013 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Atlanta, Georgia, 9?14 June 2013, pages
907?917.
Egoitz Laparra and German Rigau. 2013. ImpAr: A
deterministic algorithm for implicit semantic role la-
belling. In Proceedings of the 51st Annual Meet-
ing of the Association for Computational Linguis-
tics, Sofia, Bulgaria, 4?9 August 2013, pages 1180?
1189.
Emmanuel Lassalle and Pascal Denis. 2011. Leverag-
ing different meronym discovery methods for bridg-
ing resolution in French. In Proceedings of the 8th
Discourse Anaphora and Anaphor Resolution Col-
loquium (DAARC 2011), Faro, Algarve, Portugal, 6?
7 October 2011, pages 35?46.
Sebastian L?obner. 1985. Definites. Journal of Seman-
tics, 4:279?326.
Sebastian L?obner. 1998. Definite associative
anaphora. Unpublished Manuscript, Heinrich-
Heine-Universit?at D?usseldorf.
Katja Markert, Malvina Nissim, and Natalia N. Mod-
jeska. 2003. Using the web for nominal anaphora
resolution. In Proceedings of the EACL Workshop
on the Computational Treatment of Anaphora. Bu-
dapest, Hungary, 14 April 2003, pages 39?46.
Katja Markert, Yufang Hou, and Michael Strube. 2012.
Collective classification for fine-grained information
status. In Proceedings of the 50th Annual Meeting of
the Association for Computational Linguistics, Jeju
Island, Korea, 8?14 July 2012, pages 795?804.
Adam Meyers, Ruth Reeves, Catherine Macleod,
Rachel Szekely, Veronika Zielinska, Brian Young,
and Ralph Grishaman. 2004. Annotating noun ar-
gument structure for NomBank. In Proceedings of
the 4th International Conference on Language Re-
sources and Evaluation, Lisbon, Portugal, 26?28
May 2004, pages 803?806.
Natalia M. Modjeska, Katja Markert, and Malvina Nis-
sim. 2003. Using the web in machine learning
for other-anaphora resolution. In Proceedings of the
2003 Conference on Empirical Methods in Natural
Language Processing, Sapporo, Japan, 11?12 July
2003, pages 176?183.
Vincent Ng and Claire Cardie. 2002. Improving ma-
chine learning approaches to coreference resolution.
In Proceedings of the 40th Annual Meeting of the As-
sociation for Computational Linguistics, Philadel-
phia, Penn., 7?12 July 2002, pages 104?111.
Robert Parker, David Graff, Junbo Kong, Ke Chen, and
Kazuaki Maeda. 2011. English Gigaword Fifth Edi-
tion. LDC2011T07.
Massimo Poesio and Renata Vieira. 1998. A corpus-
based investigation of definite description use. Com-
putational Linguistics, 24(2):183?216.
Massimo Poesio, Renata Vieira, and Simone Teufel.
1997. Resolving bridging references in unrestricted
text. In Proceedings of the ACL Workshop on Oper-
ational Factors in Practical, Robust Anaphora Res-
olution for Unrestricted Text, Madrid, Spain, July
1997, pages 1?6.
Massimo Poesio, Rahul Mehta, Axel Maroudas, and
Janet Hitzeman. 2004. Learning to resolve bridg-
ing references. In Proceedings of the 42nd Annual
Meeting of the Association for Computational Lin-
guistics, Barcelona, Spain, 21?26 July 2004, pages
143?150.
Ellen F. Prince. 1981. Towards a taxonomy of given-
new information. In P. Cole, editor, Radical Prag-
matics, pages 223?255. Academic Press, New York,
N.Y.
2092
Ellen F. Prince. 1992. The ZPG letter: Subjects, defi-
niteness, and information-status. In W.C. Mann and
S.A. Thompson, editors, Discourse Description. Di-
verse Linguistic Analyses of a Fund-Raising Text,
pages 295?325. John Benjamins, Amsterdam.
Altaf Rahman and Vincent Ng. 2012. Learning the
fine-grained information status of discourse entities.
In Proceedings of the 13th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics, Avignon, France, 23?27 April 2012,
pages 798?807.
Ina R?osiger and Simone Teufel. 2014. Resolving
coreference and associative noun phrases in scien-
tific text. In Proceedings of the Student Research
Workshop at the 14th Conference of the European
Chapter of the Association for Computational Lin-
guistics, Gothenburg, Sweden, 26?30 April 2014,
pages 44?55.
Wee Meng Soon, Hwee Tou Ng, and Daniel
Chung Yong Lim. 2001. A machine learning ap-
proach to coreference resolution of noun phrases.
Computational Linguistics, 27(4):521?544.
Philip J. Stone, Dexter C. Dunphy, Marshall S. Smith,
Daniel M. Ogilvie, and Cambridge Computer Asso-
ciates. 1966. General Inquirer: A Computer Ap-
proach to Content Analysis. MIT Press, Cambridge,
Mass.
Renata Vieira and Massimo Poesio. 2000. An
empirically-based system for processing definite de-
scriptions. Computational Linguistics, 26(4):539?
593.
Ralph Weischedel, Martha Palmer, Mitchell Marcus,
Eduard Hovy, Sameer Pradhan, Lance Ramshaw,
Nianwen Xue, Ann Taylor, Jeff Kaufman, Michelle
Franchini, Mohammed El-Bachouti, Robert Belvin,
and Ann Houston. 2011. OntoNotes release 4.0.
LDC2011T03, Philadelphia, Penn.: Linguistic Data
Consortium.
2093
Proceedings of NAACL-HLT 2013, pages 907?917,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Global Inference for Bridging Anaphora Resolution
Yufang Hou1, Katja Markert2, Michael Strube1
1 Heidelberg Institute for Theoretical Studies gGmbH, Heidelberg, Germany
(yufang.hou|michael.strube)@h-its.org
2School of Computing, University of Leeds, UK
scskm@leeds.ac.uk
Abstract
We present the first work on antecedent se-
lection for bridging resolution without restric-
tions on anaphor or relation types. Our model
integrates global constraints on top of a rich
local feature set in the framework of Markov
logic networks. The global model improves
over the local one and both strongly outper-
form a reimplementation of prior work.
1 Introduction
Identity coreference is a relatively well understood
and well-studied instance of entity coherence. How-
ever, entity coherence can rely on more complex,
lexico-semantic, frame or encyclopedic relations
than identity. Anaphora linking distinct entities or
events this way are called bridging or associative
anaphora and have been widely discussed in the lin-
guistic literature (Clark, 1975; Prince, 1981; Gundel
et al, 1993).1 In Example 1, the phrases the win-
dows, the carpets and walls can be felicitously used
because they are semantically related via a part-of
relation to their antecedent the Polish center.2
(1) . . . as much as possible of the Polish center will
be made from aluminum, steel and glass recycled
from Warsaw?s abundant rubble. . . . The windows
will open. The carpets won?t be glued down and
walls will be coated with non-toxic finishes.
1Poesio and Vieira (1998) include cases where antecedent
and anaphor are coreferent but do not share the same head noun.
We restrict bridging to non-coreferential cases. We also exclude
comparative anaphora (Modjeska et al, 2003)
2Examples are from OntoNotes (Weischedel et al, 2011).
Bridging anaphora are typed in boldface; antecedents in italics.
Bridging is frequent amounting to between 5%
(Gardent and Manue?lian, 2005) and 20% (Caselli
and Prodanof, 2006) of definite descriptions (both
studies limited to NPs starting with the or non-
English equivalents). Bridging resolution is needed
to fill gaps in entity grids based on coreference only
(Barzilay and Lapata, 2008). Example 1 does not ex-
hibit any coreferential entity coherence. Coherence
can only be established when the bridging anaphora
are resolved. Bridging resolution may also be im-
portant for textual entailment (Mirkin et al, 2010).
Bridging resolution can be divided into two tasks,
recognizing that a bridging anaphor is present and
finding the correct antecedent among a list of candi-
dates. These two tasks have frequently been handled
in a pipeline with most research concentrating on an-
tecedent selection only. We also handle only the task
of antecedent selection.
Previous work on antecedent selection for bridg-
ing anaphora is restricted. It makes strong untested
assumptions about bridging anaphora types or rela-
tions, limiting it to definite NPs (Poesio and Vieira,
1998; Poesio et al, 2004; Lassalle and Denis, 2011)
or to part-of relations between anaphor and an-
tecedent (Poesio et al, 2004; Markert et al, 2003;
Lassalle and Denis, 2011). We break new ground
by considering all relations and anaphora/antecedent
types and show that the variety of bridging anaphora
is much higher than reported previously.
Following work on coreference resolution, we ap-
ply a local pairwise model (Soon et al, 2001) for an-
tecedent selection. We then develop novel semantic,
syntactic and salience features for this task, show-
ing strong improvements over one of the best known
907
prior models (Poesio et al, 2004).
However, this local model classifies each
anaphor-antecedent candidate pair in isolation.
Thus, it neglects that bridging anaphora referring to
a single antecedent often occur in clusters (see Ex-
ample 1). It also neglects that once an entity is an
antecedent for a bridging anaphor it is more likely to
be used again as antecedent. In addition, such local
models construct the list of possible antecedent can-
didates normally relying on a window size constraint
to restrict the set of candidates: is the window too
small, we miss too many correct antecedents; is it
too large, we include so many incorrect antecedents
as to lead to severe data imbalance in learning.
To remedy these flaws we change to a global
Markov logic model that allows us to:
? model constraints that certain anaphora are
likely to share the same antecedent;
? model the global semantic connectivity of a
salient potential antecedent to all anaphora in a
text;
? consider the union of potential antecedents for
all anaphora instead of a static window-sized
constraint.
We show that this global model with the same lo-
cal features but enhanced with global constraints im-
proves significantly over the local model.
2 Related Work
Prior corpus-linguistic studies on bridging are be-
set by three main problems. First, reliability is not
measured or low (Fraurud, 1990; Poesio, 2003; Gar-
dent and Manue?lian, 2005; Riester et al, 2010).3
Second, annotated corpora are small (Poesio et al,
2004; Korzen and Buch-Kromann, 2011). Third,
they are often based on strong untested assumptions
about bridging anaphora types, antecedent types or
relations, such as limiting it to definite NP anaphora
(Poesio and Vieira, 1998; Poesio et al, 2004; Gar-
dent and Manue?lian, 2005; Caselli and Prodanof,
2006; Riester et al, 2010; Lassalle and Denis,
2011), to NP antecedents (all prior work) or to part-
3Although the overall information status scheme in Riester
et al (2010) achieved high agreement, their confusion matrix
shows that the anaphoric bridging category (BRI) is frequently
confused with other categories so that the two annotators agreed
on only less than a third of bridging anaphors.
of relations between anaphor and antecedent (Mark-
ert et al, 2003; Poesio et al, 2004). In our own
work (Markert et al, 2012) we established a corpus
that circumvents these problems, i.e. human bridg-
ing recognition was reliable, it contains a medium
number of bridging cases that allows generalisable
statistics and we did not limit bridging anaphora or
antecedents according to their syntactic type or re-
lations between them. However, we only discussed
human agreement on bridging recognition in Mark-
ert et al (2012), disregarding antecedent annotation.
We also did not discuss the different types of bridg-
ing in the corpus. We will remedy this in Section 3.
Automatic work on bridging distinguishes be-
tween recognition (Vieira and Poesio, 2000; Rah-
man and Ng, 2012; Cahill and Riester, 2012; Mark-
ert et al, 2012) and antecedent selection. Work on
antecedent selection suffers from focusing on sub-
problems, e.g. only part-of bridging (Poesio et al,
2004; Markert et al, 2003) or definite NP anaphora
(Lassalle and Denis, 2011). Most relevant for us is
Lassalle and Denis (2011) who restrict anaphora to
definite descriptions but have no other restrictions
on relations or antecedent NPs (in a French corpus)
with an accuracy of 23%. Also the evaluation set-
up is sometimes not clear: The high results in Poe-
sio et al (2004) cannot be used for comparison as
they test unrealistically: they distinguish only be-
tween the correct antecedent and one or three false
candidates (baseline of 50% for the former). They
also restrict the phenomenon to part-of relations.
There is a partial overlap between bridging and
implicit noun roles (Ruppenhofer et al, 2010).
However, work on implicit noun roles is mostly
focused on few predicates (e.g. Gerber and Chai
(2012)). We consider all bridging anaphors in run-
ning text. The closest work to ours interpreting im-
plicit role filling as anaphora resolution is Silberer
and Frank (2012).
3 Corpus for Bridging: An Overview
We use the dataset we created in Markert et al
(2012) with almost 11,000 NPs annotated for infor-
mation status including 663 bridging NPs and their
antecedents in 50 texts taken from the WSJ portion
of the OntoNotes corpus (Weischedel et al, 2011).
Bridging anaphora can be any noun phrase. They
908
are not limited to definite NPs as in previous work.
In contrast to Nissim et al (2004), antecedents are
annotated and can be noun phrases, verb phrases or
even clauses. Our bridging annotation is also not
limited with regards to semantic relations between
anaphor and antecedent.
In Markert et al (2012) we achieved high agree-
ment for the overall information status annotation
scheme between three annotators (? between 75 and
80, dependent on annotator pairs) as well as for all
subcategories, including bridging (? over 60 for all
annotator pairings, over 70 for two expert annota-
tors). Here, we add the following new results:
? Agreement for selecting bridging antecedents
was around 80% for all annotator pairings.
? Surprisingly, only 255 of the 663 (38%) bridg-
ing anaphors are definite NPs, which calls into
question the strategy of prior approaches to limit
themselves to these types of bridging.
? NPs are the most frequent antecedents by far
with only 42 of 663 (6%) bridging anaphora hav-
ing a non-NP antecedent (mostly verb phrases).
? Bridging is a relatively local phenomenon with
71% of NP antecedents occurring in the same or
up to 2 sentences prior to the anaphor. However,
farther away antecedents are common when the
antecedent is the global focus of a document.
? The semantic relations between anaphor and an-
tecedent are extremely diverse with only 92 of
663 (14%) anaphors having a part-of/attribute-
of antecedent (see Example 1) and only 45 (7%)
anaphors standing in a set relationship to the an-
tecedent (see Example 2). This contrasts with
Gardent and Manue?lian?s (2005) finding that
52% of bridging cases had meronymic relations.
We find many different types of relations in our
corpus, including encyclopedic relations such as
restaurant ? the waiter as well as, frequently,
relational person nouns as bridging anaphors
such as friend, husband, president.
? There are only a few cases of bridging where
surface cues may indicate the antecedent. First,
some bridging anaphors are modified by a small
number of adjectives that have more than one
role filler, with the bridging relation often being
temporal or spatial sequence between two enti-
ties of the same semantic type as in Example 3
(see also Lassalle and Denis (2011) for a dis-
cussion of such cases). Second, some anaphors
are compounds where the nominal premodifier
matches the antecedent head as in Example 4.
(2) Still employees do occasionally try to smuggle
out a gem or two. One man wrapped several dia-
monds in the knot of his tie. Another poked a hole
in the heel of his shoe. None made it past the body
searches . . .
(3) His truck is parked across the field . . . The
farmer at the next truck shouts . . .
(4) . . . it doesn?t make the equipment needed to
produce those chips. And IBM worries that the
Japanese will take over that equipment market.
4 Models for Bridging Resolution
4.1 Pairwise mention-entity model
The pairwise model is widely used in coreference
resolution (Soon et al, 2001). We adapt it for bridg-
ing resolution4: Given an anaphor mention m and
the set of antecedent candidate entities Em which
appear before m, we create a pairwise instance
(m, e) for every e ? Em. A binary decision whether
m is bridged to e is made for each instance (m, e)
separately. A post-processing step to choose one an-
tecedent is necessary (closest first or best first are
common strategies). This model causes three prob-
lems for bridging resolution: First, the ratio between
positive and negative instances is 1 to 17 even if only
antecedent candidates from the current and the im-
mediately preceding two sentences are considered.
The ratio will be even worse with a larger win-
dow size. Therefore, usually a fixed window size is
used restricting the set of candidates. This, however,
causes a second problem: antecedents which are be-
yond the window cannot be found. In our data, only
81% of NP antecedents appear within the previous 5
sentences, and only 71% of NP antecedents appear
within the previous 2 sentences. The third problem
is a shortcoming of the pairwise model itself: deci-
sions are made for each instance separately, ignoring
4Different from coreference, we treat an anaphor as a men-
tion and an antecedent as an entity. The anaphor is the first
mention of the corresponding entity in the document.
909
relations between instances. We resolve these prob-
lems by employing a global model based on Markov
logic networks.
4.2 Markov Logic Networks
Bridging can be considered a document global phe-
nomenon, where globally salient entities are pre-
ferred as antecedents and two or more anaphors hav-
ing the same antecedent should be related or similar.
Motivated by this observation, we explore Markov
logic networks (Domingos and Lowd, 2009, MLNs)
to model bridging resolution on the global discourse
level.
MLNs are a powerful representation for joint
inference with uncertainty. An MLN consists
of a set of pairs (Fi, wi), where Fi is a formula
in first-order logic and wi is its associated real
numbered weight. It can be viewed as a template for
constructing Markov networks. Given different sets
of constants, an MLN will produce different ground
Markov networks which may vary in size but have
the same structure and parameters. For a ground
Markov network, the probability distribution over
possible worlds x is given by
P (X = x) = 1Z exp
(
?
i
wini(x)
)
(1)
where ni(x) is the number of true groundings of Fi
in x. The normalization factor Z is the partition
function.
MLNs have been applied to many NLP tasks and
achieved good performance by leveraging rich re-
lations among objects (Poon and Domingos, 2008;
Meza-Ruiz and Riedel, 2009; Fahrni and Strube,
2012, inter alia). We use thebeast5 to learn weights
for the formulas and to perform inference. thebeast
employs cutting plane inference (Riedel, 2008) to
improve the accuracy and efficiency of MAP infer-
ence for Markov logic.
With MLNs, we model bridging resolution glob-
ally on the discourse level: given the set M of all
anaphors and sets of local antecedent candidates Em
for each anaphor m ? M , we select antecedents for
all anaphors from E =?m?M Em at the same time.
Table 1 shows the hidden predicates and formulas
used. Each formula is associated with a weight. The
5http://code.google.com/p/thebeast
polarity of the weights is indicated by the leading
+ or ?. The weight value (except for hard con-
straints) is learned from training data. For some for-
mulas the final weight consists of a learned weight
w multiplied by a score d (e.g. inverse distance be-
tween antecedent and anaphor). In these cases the
final weight for a formula in a ground Markov net-
work does not just depend on the respective formula,
but also on the specific constants. We indicate such
combined weights by the term w ? d.
We tackle the previously mentioned problems of
the pairwise model: (1) We construct hard con-
straints to specify that each anaphor has at most
one antecedent entity (Table 1: f1) and that the an-
tecedent must precede the anaphor (f2). This elim-
inates the need for the post-processing step in the
pairwise model. (2) We select the antecedent en-
tity for each anaphor from the antecedent candidate
entities pool E which alleviates the missing true
antecedent problem in the pairwise model. Based
on (1) and (2), MLNs allow us to express relations
between anaphor-anaphor and anaphor-antecedent
pairs ((m,n) or (m,e)) on the global discourse level
improving accuracy by performing joint inference.
5 Features
5.1 Local features
5.1.1 Poesio et al?s feature set
Table 2 shows the feature set proposed by Poesio et
al. (2004) for part-of bridging. Google distance is
the inverse value of Google hit counts for the ofPat-
tern query (e.g. the windows of the center). Word-
Net distance is the inverse value of the shortest path
length between an anaphor and an antecedent candi-
date among all synset combinations. These features
are supposed to capture the meronymy relation be-
tween anaphor and antecedent. The other ones mea-
sure the salience of the antecedent candidate.
Group Feature Value
lexical Google distance numeric
WordNet distance numeric
salience utterance distance numeric
local first mention boolean
global first mention boolean
Table 2: Poesio et al?s feature set
910
Hidden predicates
p1 isBridging(m, e)
p2 hasSameAntecedent (m,n)
Formulas
Hard constraints
f1 ?m ? M : |e ? E : isBridging(m, e)| ? 1
f2 ?m ? M?e ? E : hasPairDistance(e,m, d) ? d < 0 ? ?isBridging(m, e)
f3 ?m,n ? M : m 6= n ? hasSameAntecedent (m,n)
? hasSameAntecedent (n,m)
f4 ?m,n, l ? M : m 6= n ?m 6= l ? n 6= l ? hasSameAntecedent (m,n)
? hasSameAntecedent (n, l) ? hasSameAntecedent (m, l)
f5 ?m,n ? M?e ? E : m 6= n ? hasSameAntecedent (m,n) ? isBridging(m, e)
? isBridging(n, e)
f6 ?m,n ? M?e ? E : m 6= n ? isBridging(m, e) ? isBridging(n, e)
? hasSameAntecedent (m,n)
Discourse level formulas
f7 + (w) ?m ? M?e ? E : predictedGlobalAnte(e) ? hasPairDistance(e,m, d)
? d > 0 ? isBridging(m, e)
f8 + (w) ?m,n ? M conjunction(m,n) ? hasSameAntecedent (m,n)
f9 + (w) ?m,n ? M sameHead(m,n) ? hasSameAntecedent (m,n)
f10 + (w) ?m,n ? M similarTo(m,n) ? hasSameAntecedent (m,n)
f11 + (w) ?m ? M?e ? E : hasSemanticClass (m, ?rolePerson?)
? hasSemanticClass(e, ?org|gpe?) ? hasPairDistance(e,m, d) ? d > 0
? isBridging(m, e)
f12 + (w ? d) ?m ? M?e ? E : hasSemanticClass (m, ?relativePerson?)
? hasSemanticClass(e, ?otherPerson?) ? hasPairDistanceInverse(e,m, d)
? isBridging(m, e)
f13 + (w ? d) ?m ? M?e ? E : hasSemanticClass (m, ?date?)
? hasSemanticClass(e, ?date?) ? hasPairDistanceInverse(e,m, d)
? isBridging(m, e)
Local formulas
f14 + (w) ?m ? M ?e ? Em : isTopRelativeRankPrepPattern (m, e) ? isBridging(m, e)
f15 + (w) ?m ? M ?e ? Em : isTopRelativeRankVerbPattern(m, e) ? isBridging(m, e)
f16 + (w ? d) ?m ? M ?e ? Em : isPartOf (m, e) ? hasPairDistanceInverse(e,m, d)
? isBridging(m, e)
f17 + (w) ?m ? M ?e ? Em : isTopRelativeRankDocSpan (m, e) ? isBridging(m, e)
f18 ? (w) ?m ? M ?e ? Em : isSameHead(m, e) ? isBridging(m, e)
f19 + (w) ?m ? M ?e ? Em : isPremodOverlap(m, e) ? isBridging(m, e)
f20 ? (w) ?m ? M ?e ? Em : isCoArgument(m, e) ? isBridging(m, e)
Table 1: Hidden predicates and formulas used for bridging resolution (m,n, l represent mentions, M the set of bridging
anaphora mentions in the whole document, e the antecedent candidate entity, Em the set of local antecedent candidate
entities for m, and E =
?
m?M Em )
911
5.1.2 Other features
Since Poesio et al (2004) deal exclusively with
meronymy bridging, we have to extend the fea-
ture set to capture more diverse relations between
anaphor and antecedent. All numeric features in Ta-
ble 3 are normalized among all antecedent candi-
dates of one anaphor. For anaphor mi and its an-
tecedent candidates Emi (eij ? Emi), the numeric
score for pair {mi, eik} is Sik. Then the value
NormSik for this pair is normalized (set to values
between 0 and 1) as below:
NormSik =
Sik ?minj Sij
maxj Sij ?minj Sij
(2)
A second variant of numeric features tells whether
the score of an anaphor-antecedent candidate pair is
the highest among all pairs for this anaphor.
Group Feature Value
semantic feat1 preposition pattern numeric
feat2 verb pattern numeric
feat3 WordNet partOf boolean
feat4 semantic class nominal
salience feat5 document span numeric
surface feat6 isSameHead boolean
feat7 isPremodOverlap boolean
syntactic feat8 isCoArgument boolean
Table 3: Local features we developed
Preposition pattern (feat1). The ofPattern pro-
posed by Poesio et al (2004) is useful for part-of
and attribute-of relations but cannot cover all bridg-
ing relations (such as sanctions against a country).
We extend the ofPattern to a generalised preposition
pattern by using the Gigaword (Parker et al, 2011)
and the Tipster (Harman and Liberman, 1993) cor-
pora (both automatically POS tagged and NP chun-
ked for improving query match precision).
First, we extract the three most highly associ-
ated prepositions for each anaphor. Then for each
anaphor-antecedent candidate pair, we use their head
words to create the query ?anaphor preposition an-
tecedent?. To improve recall, we take lowercase,
uppercase, singular and plural forms of the head
word into account, and replace proper names by
fine-grained named entity types (using a gazetteer).
All raw hit counts are converted into the Dunning
Root Loglikelihood association measure,6 then nor-
malized using Formula 2 within all antecedent can-
didates of one anaphor.
Verb pattern (feat2). A set-membership rela-
tion between anaphor and antecedent is often hard
to capture by the preposition pattern because the
anaphor often has no common noun head (see Ex-
ample 2 in Section 3). Hence, we measure the com-
patibility of the antecedent candidates with the verb
the anaphor depends on.
First, we hypothesise that anaphors whose lexi-
cal head is a pronoun or a number are potential set
bridging cases and then extract the verb the anaphor
depends on. In example 2, for the set anaphor An-
other, poked is the verb. Then for each antecedent
candidate, subject-verb or verb-object queries are
applied to the Web 1T 5-gram corpus (Brants and
Franz, 2006). In this case, employees poked and di-
amonds poked are example queries. The hit counts
are transformed into PMI and all pairs for one
anaphor are normalized as described in Formula 2.
WordNet partOf relation (feat3). To capture
part-of bridging, we extract whether the anaphor is
part of the antecedent candidate in WordNet. To im-
prove recall, we use hyponym information of the
antecedent. If an antecedent e is a hypernym of x
and an anaphor m is a meronym of x, then m is a
meronym of e.
Semantic class (feat4). The anaphor and the an-
tecedent candidate are assigned one of 16 coarse-
grained semantic classes, e.g. location, organiza-
tion, GPE, roleperson, relativePerson, otherPerson7,
product, language, NORP (nationalities, religious
or political groups) and several classes for numbers
(such as date, money or percent).
Salience feature (feat5). Salient entities are pre-
ferred as antecedents. We capture salience super-
ficially by computing the ?antecedent document
span? of an antecedent candidate. We compute the
6http://tdunning.blogspot.de/2008/03/
surprise-and-coincidence.html
7We use WordNet to extract lists for rolePerson (persons like
president or teacher playing a role in an organization) and rela-
tivePerson (persons like father or son indicating that they have
a relation with another person). Persons not in these two lists
are counted as otherPerson.
912
span of text (measured in sentences) in which the
antecedent candidate entity is mentioned. This is di-
vided by the number of sentences in the whole doc-
ument. This score is normalized using Formula 2 for
all antecedent candidates of one anaphor.
Surface features (feat6-feat7). isSameHead
(feat6) checks whether antecedent candidates have
the same head as the anaphor: this is rarely the
case in bridging anaphora (except in some cases
of set bridging and spatial/temporal sequence, see
Example 3) and can therefore be used to exclude
antecedent candidates. isPremodOverlap (feat7)
determines the antecedent for compound noun
anaphors whose head is prenominally modified by
the antecedent head (see Example 4).
Syntactic feature (feat8) The isCoArgument fea-
ture is based on the intuition that the subject can-
not be the bridging antecedent of the object in
the same clause. This feature excludes (some)
close antecedent candidates. In Example 4, the an-
tecedent candidate the Japanese isCoArgument with
the anaphor that equipment market.
5.2 Global features for MLNs
f1-f13 in Table 1 are discourse level constraints.
All antecedent candidates come from the antecedent
candidates pool E in the whole document.
Global salience (Table 1: f3-f10). The salience
feature in the pairwise model only measures the
salience for candidates within the local window.
However, globally salient antecedents are preferred
even if they are far away from the anaphor. We
model this from two perspectives:
f7 models the preference for globally salient an-
tecedents, which we derive for each document. For
m ? M and e ? E, let score(m, e) be the prepo-
sition pattern score for pair (m,e). Calculate pattern
semantic salience score esal for each e ? E as
esal =
?
m?M
score(m, e) (3)
If e appears in the title and also has the highest
pattern semantic salience score esal among all e in
E, then e is the predicted globally salient antecedent
for this document. Note that global salience here is
based on semantic connectivity to all anaphors in the
document and that not every document has a glob-
ally salient antecedent.
f3-f6 and f8-f10 model that similar or related
anaphors in one document are likely to have the
same antecedent. To make the ground Markov net-
work more sparse for more efficient inference, we
add the hidden predicate (p2) and hard constraints
(f3-f6) specifying relations among similar/related
anaphors m, n and l (reflexivity and transitivity).
Formulas f8-f10 explore three different ways (syn-
tactic and semantic) to compute the similarity be-
tween two anaphors. In f10, we use SVMlight (simi-
larity scores from WordNet plus sentence distance as
features) to predict whether two anaphors not shar-
ing the same head are similar or not.
Frequent bridging relations (Table 1: f11-f13).
Three common bridging relations are restricted by
semantic class of anaphor and antecedent (see also
Section 3). It is worth noting that in formula f11
(modeling that a role person mention like presi-
dent or chairman prefers organization or GPE an-
tecedents), we do not penalize the antecedents far
away from the anaphor. In formula f12 (modeling
that a relativePerson mention such as mother or hus-
band prefers close person antecedents) and f13, we
prefer close antecedents by including the distance
between antecedent and anaphor into the weights.
MLN formulation of local features (Table 1: f14-
f20). Corresponding to features of the pairwise
model (Table 3) ? we exclude only semantic class
as this is modelled globally via features f11-f13.
These local features are only used for an anaphor m
and its local antecedent candidate e from Em.
6 Experiments and Results
6.1 Experimental setup
We perform experiments on our gold standard cor-
pus via 10-fold cross-validation on documents. We
use gold standard mentions, true coreference infor-
mation, and the OntoNotes named entity and syntac-
tic annotation layers for feature extraction.
6.2 Improved baseline
We reimplement the algorithm from Poesio et al
(2004) as baseline. Since they did not explain
913
whether they used the mention-mention or mention-
entity model, we assume they treated antecedents as
entities and use a 2 and 5 sentence window for can-
didates8. Since the GoogleAPI is not available any
more, we use the Web 1T 5-gram corpus (Brants and
Franz, 2006) to extract the Google distance feature.
We improve it by taking all information about en-
tities via coreference into account as well as by re-
placing proper names. All other features (Table 2
in Section 5.1.1) are extracted as Poesio et al did.
A Naive Bayes classifier with standard settings in
WEKA (Witten and Frank, 2005) is used. In order
to evaluate their model in the more realistic setting
of our experiment, we apply the best first strategy to
select the antecedent for each anaphor.
6.3 Pairwise models
Pairwise model I: We use the preposition pattern
feature (feat1) plus Poesio et al?s salience features
(Table 2). We use a 2 sentence window as it per-
formed on a par with the 5 sentence window in the
baseline. We replace Naive Bayes with SVMlight
because it can deal better with imbalanced data9.
Pairwise model II: Based on Pairwise model I.
Local features feat2-feat8 from Table 2 are added.
Pairwise model III: Based on Pairwise model II.
We apply a more advanced antecedent candidate se-
lection strategy, which allows to include 77% of NP
antecedents compared to 71% in Pairwise model II.
For each anaphor, we add the top k salient enti-
ties measured through the length of the coreference
chains (k is set to 10%) as additional antecedent can-
didates. For potential set anaphors (as automatically
determined by pronoun or number heads), singu-
lar antecedent candidates are filtered out. We com-
piled a small set of adjectives (using FrameNet and
thesauri) that indicate spatial or temporal sequences
(see Example 3). For anaphors modified by such ad-
jectives we consider only antecedent candidates that
have the same semantic class as the anaphor.
8They use a 5 sentence window, because all antecedents in
their corpus are within the previous 5 sentences.
9The SVMlight parameter is set according to the ratio be-
tween positive and negative instances in the training set.
6.4 MLN models
MLN model I: MLN system using local formu-
las f1-f2 and f14-f20. The same strategy as in
Pairwise model III is used to select local antecedent
candidates Em for each anaphor m.
MLN model II: Based on MLN model I, all for-
mulas in Table 1 are used.
6.5 Results
Table 4 shows the comparison of our models to base-
lines. Significance tests are conducted using McNe-
mar?s test on overall accuracy at the level of 1%.
acc
improved baseline 2 sent. + NB 18.85
5 sent. + NB 18.40
pairwise model pairwise model I 29.11
pairwise model II 33.94
pairwise model III 36.35
MLN model MLN model I 35.60
MLN model II 41.32
Table 4: Results for MLN models compared to pairwise
models and baselines.
MLN model II, which is inspired by the linguis-
tic observation that globally salient entities are pre-
ferred as antecedents, performs significantly better
than all other systems. The gains come from three
aspects. First, by selecting the antecedent for each
anaphor from the antecedent candidate pool E in the
whole document 91% of NP antecedents are acces-
sible compared to 77% in pairwise model III. Sec-
ond, we leverage semantics and salience by using
local formulas and discourse level formulas. Lo-
cal formulas are used to capture semantic relations
for bridging pairs as well as surface and syntactic
constraints. Global formulas resolve several bridg-
ing anaphors together, often to a globally salient an-
tecedent beyond the local window. Third, the model
allows us to express specific relations among bridg-
ing anaphors and their antecedents (f11-f13).
However, our pairwise model I already outper-
forms improved baselines by about 10%, which sug-
gests that our preposition pattern feature can capture
more diverse semantic relations. The continuous im-
provements shown in pairwise model II and pair-
wise model III verify the contribution of our other
914
features and advanced antecedent candidate selec-
tion strategy. pairwise model III would become too
complex if we tried to integrate discourse level for-
mulas f7, f11-f13 into antecedent candidate selec-
tion. MLN model II solves this task elegantly.
6.6 Discussion and error analysis
We analyse our best model (MLN model II) and
compare it to the best local one (pairwise model III).
Anaphors with long distance antecedents are
harder to resolve. Table 5 shows the compari-
son of correctly resolved anaphors with regard to
anaphor-antecedent distance. We can see that the
global model is equal or better to the local model
for all anaphor types but that the difference is espe-
cially large for anaphora with antecedents that are
3 or more sentences away due to the use of global
salience and accessibility of possible antecedents
beyond a fixed window-size.
# pairs MLN II pairwise III
sent. distance
0 175 48.57 45.14
1 260 34.62 35
2 90 47.78 43.33
?3 158 35.44 16.46
Table 5: Comparison of the percentage of correctly re-
solved anaphors with regard to anaphor-antecedent dis-
tance. Significance tests are conducted using McNemar?s
test at the level of 1%.
We now distinguish between ?sibling anaphors?
(anaphors that share an antecedent with other bridg-
ing anaphors) and ?non-siblings? (anaphors that do
not share an antecedent with any other anaphor).
The performance of our MLN model II is 54%
on sibling anaphors but only 24% on non-sibling
anaphors. This shows that our use of global salience
and links between related anaphors does indeed help
to capture the behaviour of sibling anaphors.
However, our global model is good at predicting
the right antecedent for sibling anaphors where the
antecedent is globally salient but not as good for sib-
ling anaphors where the (shared) antecedent is a lo-
cally salient subtopic. Thus, in the future we need
to model equivalent constraints for local salience
of antecedents, taking into account topic segmen-
tation/shifts to improve over the 54% for sibling
anaphors.
The semantic knowledge we employ is still in-
sufficient. Typical cases where we have problems
are: (i) cases with very context-specific bridging re-
lations. For example, in one text about the stealing
of Sago Palms in California we found the thieves
as a bridging anaphor with the antecedent palms,
which is not a very usual semantic link. (ii) more
frequently, we have cases where several good an-
tecedents from a semantic perspective can be found.
For example, two laws are discussed and a later
anaphor the veto could be the veto of either bills.
Integration of the wider context apart from the two
NPs is necessary in these cases. This includes the se-
mantics of modification, whereas we currently con-
sider only head noun knowledge. An example is that
the anaphor the local council would preferably be
interpreted as the council of a village instead of the
council of a state due to the occurrence of local.
Finally, 6% of the anaphors in our corpus have a
non-NP antecedent. These cases are not correctly
resolved in our current model as we only extract NP
phrases as potential candidate antecedents.
7 Conclusions
We provide the first reasonably sized and reliably
annotated English corpus for bridging resolution. It
covers a diverse set of relations between anaphor and
antecedent as well as all anaphor/antecedent types.
We developed novel semantic, syntactic and salience
features based on linguistic intuition. Inspired by
the observation that salient entities are preferred as
antecedents, we implemented a global model for an-
tecedent selection within the framework of Markov
logic networks. We show that our global model sig-
nificantly outperforms other local models and base-
lines. This work is ? to our knowledge ? the first
bridging resolution algorithm that tackles the unre-
stricted phenomenon in a real setting.
Acknowledgements. Yufang Hou is funded by a PhD
scholarship from the Research Training Group Coher-
ence in Language Processing at Heidelberg University.
Katja Markert receives a Fellowship for Experienced Re-
searchers by the Alexander-von-Humboldt Foundation.
We thank HITS gGmbH for hosting Katja Markert and
funding the annotation. We thank our colleague Angela
Fahrni for advice on using Markov logic networks.
915
References
Regina Barzilay and Mirella Lapata. 2008. Modeling
local coherence: An entity-based approach. Computa-
tional Linguistics, 34(1):1?34.
Thorsten Brants and Alex Franz. 2006. Web 1t 5-gram
version 1. LDC2006T13, Philadelphia, Penn.: Lin-
guistic Data Consortium.
Aoife Cahill and Arndt Riester. 2012. Automatically ac-
quiring fine-grained information status distinctions in
German. In Proceedings of the SIGdial 2012 Confer-
ence: The 13th Annual Meeting of the Special Interest
Group on Discourse and Dialogue, Seoul, Korea, 5?6
July 2012, pages 232?236.
Tommaso Caselli and Irina Prodanof. 2006. Annotat-
ing bridging anaphors in Italian: In search of reliabil-
ity. In Proceedings of the 5th International Conference
on Language Resources and Evaluation, Genoa, Italy,
22?28 May 2006.
Herbert H. Clark. 1975. Bridging. In Proceedings of the
Conference on Theoretical Issues in Natural Language
Processing, Cambridge, Mass., June 1975, pages 169?
174.
Pedro Domingos and Daniel Lowd. 2009. Markov
Logic: An Interface Layer for Artificial Intelligence.
Morgan Claypool Publishers.
Angela Fahrni and Michael Strube. 2012. Jointly
disambiguating and clustering concepts and entities
with Markov logic. In Proceedings of the 24th In-
ternational Conference on Computational Linguistics,
Mumbai, India, 8?15 December 2012, pages 815?832.
Kari Fraurud. 1990. Definiteness and the processing of
noun phrases in natural discourse. Journal of Seman-
tics, 7:395?433.
Claire Gardent and He?le`ne Manue?lian. 2005. Cre?ation
d?un corpus annote? pour le traitement des descrip-
tions de?finies. Traitement Automatique des Langues,
46(1):115?140.
Matthew Gerber and Joyce Chai. 2012. Semantic role
labeling of implicit arguments for nominal predicates.
Computational Linguistics, 38(4):756?798.
Jeanette K. Gundel, Nancy Hedberg, and Ron Zacharski.
1993. Cognitive status and the form of referring ex-
pressions in discourse. Language, 69:274?307.
Donna Harman and Mark Liberman. 1993. TIPSTER
Complete. LDC93T3A, Philadelphia, Penn.: Linguis-
tic Data Consortium.
Iorn Korzen and Matthias Buch-Kromann. 2011.
Anaphoric relations in the Copenhagen dependency
treebanks. In S. Dipper and H. Zinsmeister, edi-
tors, Corpus-based Investigations of Pragmatic and
Discourse Phenomena, volume 3 of Bochumer Lin-
guistische Arbeitsberichte, pages 83?98. University of
Bochum, Bochum, Germany.
Emmanuel Lassalle and Pascal Denis. 2011. Leverag-
ing different meronym discovery methods for bridging
resolution in French. In Proceedings of the 8th Dis-
course Anaphora and Anaphor Resolution Colloquium
(DAARC 2011), Faro, Algarve, Portugal, 6?7 October
2011, pages 35?46.
Katja Markert, Malvina Nissim, and Natalia N. Mod-
jeska. 2003. Using the web for nominal anaphora
resolution. In Proceedings of the EACL Workshop on
the Computational Treatment of Anaphora. Budapest,
Hungary, 14 April 2003, pages 39?46.
Katja Markert, Yufang Hou, and Michael Strube. 2012.
Collective classification for fine-grained information
status. In Proceedings of the 50th Annual Meeting of
the Association for Computational Linguistics, Jeju Is-
land, Korea, 8?14 July 2012, pages 795?804.
Ivan Meza-Ruiz and Sebastian Riedel. 2009. Jointly
identifying predicates, arguments and senses using
Markov logic. In Proceedings of Human Language
Technologies 2009: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, Boulder, Col., 31 May ? 5 June
2009, pages 155?163.
Shachar Mirkin, Ido Dagan, and Sebastian Pado?. 2010.
Assessing the role of discourse references in entail-
ment inference. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, Uppsala, Sweden, 11?16 July 2010, pages 1209?
1219.
Natalia M. Modjeska, Katja Markert, and Malvina Nis-
sim. 2003. Using the web in machine learning for
other-anaphora resolution. In Proceedings of the 2003
Conference on Empirical Methods in Natural Lan-
guage Processing, Sapporo, Japan, 11?12 July 2003,
pages 176?183.
Malvina Nissim, Shipara Dingare, Jean Carletta, and
Mark Steedman. 2004. An annotation scheme for in-
formation status in dialogue. In Proceedings of the 4th
International Conference on Language Resources and
Evaluation, Lisbon, Portugal, 26?28 May 2004, pages
1023?1026.
Robert Parker, David Graff, Junbo Kong, Ke Chen, and
Kazuaki Maeda. 2011. English Gigaword Fifth Edi-
tion. LDC2011T07.
Massimo Poesio and Renata Vieira. 1998. A corpus-
based investigation of definite description use. Com-
putational Linguistics, 24(2):183?216.
Massimo Poesio, Rahul Mehta, Axel Maroudas, and
Janet Hitzeman. 2004. Learning to resolve bridging
references. In Proceedings of the 42nd Annual Meet-
ing of the Association for Computational Linguistics,
Barcelona, Spain, 21?26 July 2004, pages 143?150.
Massimo Poesio. 2003. Associate descriptions and
salience: A preliminary investigation. In Proceedings
916
of the EACL Workshop on the Computational Treat-
ment of Anaphora. Budapest, Hungary, 14 April 2003,
pages 31?38.
Hoifung Poon and Pedro Domingos. 2008. Joint un-
supervised coreference resolution with Markov Logic.
In Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, Waikiki,
Honolulu, Hawaii, 25?27 October 2008, pages 650?
659.
Ellen F. Prince. 1981. Towards a taxonomy of given-new
information. In P. Cole, editor, Radical Pragmatics,
pages 223?255. Academic Press, New York, N.Y.
Altaf Rahman and Vincent Ng. 2012. Learning the fine-
grained information status of discourse entities. In
Proceedings of the 13th Conference of the European
Chapter of the Association for Computational Linguis-
tics, Avignon, France, 23?27 April 2012, pages 798?
807.
Sebastian Riedel. 2008. Improving the accuracy and ef-
ficiency of MAP inference for Markov logic. In Pro-
ceedings of the 24th Conference on Uncertainty in Ar-
tificial Intelligence, Helsinki, Finland, 9?12 July 2008,
pages 468?475.
Arndt Riester, David Lorenz, and Nina Seemann. 2010.
A recursive annotation scheme for referential informa-
tion status. In Proceedings of the 7th International
Conference on Language Resources and Evaluation,
La Valetta, Malta, 17?23 May 2010, pages 717?722.
Josef Ruppenhofer, Caroline Sporleder, Roser Morante,
Collin Baker, and Martha Palmer. 2010. SemEval-
2010 Task 10: Linking events and their participants
in discourse. In Proceedings of the 5th International
Workshop on Semantic Evaluations (SemEval-2), Up-
psala, Sweden, 15?16 July 2010, pages 45?50.
Carina Silberer and Anette Frank. 2012. Casting
implicit role linking as an anaphora resolution task.
In Proceedings of STARSEM 2012: The First Joint
Conference on Lexical and Computational Semantics,
Montre?al, Que?bec, Canada, 7?8 June 2012, pages 1?
10.
Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong
Lim. 2001. A machine learning approach to corefer-
ence resolution of noun phrases. Computational Lin-
guistics, 27(4):521?544.
Renata Vieira and Massimo Poesio. 2000. An
empirically-based system for processing definite de-
scriptions. Computational Linguistics, 26(4):539?
593.
Ralph Weischedel, Martha Palmer, Mitchell Marcus, Ed-
uard Hovy, Sameer Pradhan, Lance Ramshaw, Ni-
anwen Xue, Ann Taylor, Jeff Kaufman, Michelle
Franchini, Mohammed El-Bachouti, Robert Belvin,
and Ann Houston. 2011. OntoNotes release 4.0.
LDC2011T03, Philadelphia, Penn.: Linguistic Data
Consortium.
Ian H. Witten and Eibe Frank. 2005. Data Mining: Prac-
tical Machine Learning Tools and Techniques. Mor-
gan Kaufmann, San Francisco, Cal., 2nd edition.
917
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 795?804,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Collective Classification for Fine-grained Information Status
Katja Markert1,2, Yufang Hou2, Michael Strube2
1 School of Computing, University of Leeds, UK, scskm@leeds.ac.uk
2 Heidelberg Institute for Theoretical Studies gGmbH, Heidelberg, Germany
(yufang.hou|michael.strube)@h-its.org
Abstract
Previous work on classifying information sta-
tus (Nissim, 2006; Rahman and Ng, 2011)
is restricted to coarse-grained classification
and focuses on conversational dialogue. We
here introduce the task of classifying fine-
grained information status and work on writ-
ten text. We add a fine-grained information
status layer to the Wall Street Journal portion
of the OntoNotes corpus. We claim that the
information status of a mention depends not
only on the mention itself but also on other
mentions in the vicinity and solve the task by
collectively classifying the information status
of all mentions. Our approach strongly outper-
forms reimplementations of previous work.
1 Introduction
Speakers present already known and yet to be es-
tablished information according to principles re-
ferred to as information structure (Prince, 1981;
Lambrecht, 1994; Kruijff-Korbayova? and Steedman,
2003, inter alia). While information structure af-
fects all kinds of constituents in a sentence, we here
adopt the more restricted notion of information sta-
tus which concerns only discourse entities realized
as noun phrases, i.e. mentions1. Information status
(IS henceforth) describes the degree to which a dis-
course entity is available to the hearer with regard to
the speaker?s assumptions about the hearer?s knowl-
edge and beliefs (Nissim et al, 2004). Old men-
tions are known to the hearer and have been referred
1Since not all noun phrases are referential, we call noun
phrases which carry information status mentions.
to previously. Mediated mentions have not been
mentioned before but are also not autonomous, i.e.,
they can only be correctly interpreted by reference
to another mention or to prior world knowledge. All
other mentions are new.
IS can be beneficial for a number of NLP tasks,
though the results have been mixed. Nenkova et
al. (2007) used IS as a feature for generating pitch
accent in conversational speech. As IS is restricted
to noun phrases, while pitch accent can be assigned
to any word in an utterance, the experiments were
not conclusive. For determining constituent order of
German sentences, Cahill and Riester (2009) incor-
porate features modeling IS to good effect. Rahman
and Ng (2011) showed that IS is a useful feature for
coreference resolution.
Previous work on learning IS (Nissim, 2006; Rah-
man and Ng, 2011) is restricted in several ways.
It deals with conversational dialogue, in particular
with the corpus annotated by Nissim et al (2004).
However, many applications that can profit from IS
concentrate on written texts, such as summariza-
tion. For example, Siddharthan et al (2011) show
that solving the IS subproblem of whether a per-
son proper name is already known to the reader im-
proves automatic summarization of news. There-
fore, we here model IS in written text, creating a
new dataset which adds an IS layer to the already
existing comprehensive annotation in the OntoNotes
corpus (Weischedel et al, 2011). We also report
the first results on fine-grained IS classification by
modelling further distinctions within the category
of mediated mentions, such as comparative and
bridging anaphora (see Examples 1 and 2, re-
795
spectively).2 Fine-grained IS is a prerequisite to
full bridging/comparative anaphora resolution, and
therefore necessary to fill gaps in entity grids (Barzi-
lay and Lapata, 2008) based on coreference only.
Thus, Examples 1 and 2 do not exhibit any corefer-
ential entity coherence but coherence can be estab-
lished when the comparative anaphor others is re-
solved to others than freeway survivor Buck Helm,
and the bridging anaphor the streets is resolved to
the streets of Oranjemund, respectively.
(1) the condition of freeway survivor Buck
Helm . . . , improved, hospital officials said.
Rescue crews, however, gave up hope that
others would be found.
(2) Oranjemund, the mine headquarters, is a
lonely corporate oasis of 9,000 residents.
Jackals roam the streets at night . . .
We approach the challenge of modeling IS via
collective classification, using several novel linguis-
tically motivated features. We reimplement Nissim?s
(2006) and Rahman and Ng?s (2011) approaches as
baselines and show that our approach outperforms
these by a large margin for both coarse- and fine-
grained IS classification.
2 Related Work
IS annotation schemes and corpora. We en-
hance the approach in Nissim et al (2004) in two
major ways (see also Section 3.1). First, compar-
ative anaphora are not specifically handled in Nis-
sim et al (2004) (and follow-on work such as Ritz
et al (2008) and Riester et al (2010)), although
some of them might be included in their respective
bridging subcategories. Second, we apply the
annotation scheme reliably to a new genre, namely
news. This is a non-trivial extension: Ritz et al
(2008) applied a variation of the Nissim et al (2004)
scheme to a small set of 220 NPs in a German
news/commentary corpus but found that reliability
then dropped significantly to the range of ? = 0.55
to 0.60. They attributed this to the higher syntac-
tic complexity and semantic vagueness in the com-
mentary corpus. Riester et al (2010) annotated a
2All examples in this paper are from the OntoNotes cor-
pus. The mention in question is typed in boldface; antecedents,
where applicable, are displayed in italics.
German news corpus marginally reliable (? = 0.66)
for their overall scheme but their confusion ma-
trix shows even lower reliability for several subcate-
gories, most importantly deixis and bridging.
While standard coreference corpora do not con-
tain IS annotation, some corpora annotated for
bridging are emerging (Poesio, 2004; Korzen and
Buch-Kromann, 2011) but they are (i) not annotated
for comparative anaphora or other IS categories, (ii)
often not tested for reliability or reach only low reli-
ability, (iii) often very small (Poesio, 2004).
To the best of our knowledge, we therefore
present the first English corpus reliably annotated
for a wide range of IS categories as well as full
anaphoric information for three main anaphora types
(coreference, bridging, comparative).
Automatic recognition of IS. Vieira and Poesio
(2000) describe heuristics for processing definite de-
scriptions in news text. As their approach is re-
stricted to definites, they only analyse a subset of
the mentions we consider carrying IS. Siddharthan
et al (2011) also concentrate on a subproblem of IS
only, namely the hearer-old/hearer-new distinctions
for person proper names.
Nissim (2006) and Rahman and Ng (2011) both
present algorithms for IS detection on Nissim et
al.?s (2004) Switchboard corpus. Both papers treat
IS classification as a local classification problem
whereas we look at dependencies between the IS
status of different mentions, leading to collective
classification. In addition, they only distinguish the
three main categories old, mediated and new.
Finally, we work on news corpora which poses dif-
ferent problems from dialogue.
Anaphoricity determination (Ng, 2009; Zhou and
Kong, 2009) identifies many or most old men-
tions. However, no distinction between mediated
and new mentions is made. Most approaches to
bridging resolution (Meyer and Dale, 2002; Poe-
sio et al, 2004) or comparative anaphora (Mod-
jeska et al, 2003; Markert and Nissim, 2005)
address only the selection of the antecedent for
the bridging/comparative anaphor, not its recogni-
tion. Sasano and Kurohashi (2009) do also tackle
bridging recognition, but they depend on language-
specific non-transferrable features for Japanese.
796
3 Corpus Creation
3.1 Annotation Scheme
Our scheme follows Nissim et al (2004) in dis-
tinguishing three major IS categories old, new
and mediated. A mention is old if it is ei-
ther coreferential with an already introduced entity
or a generic or deictic pronoun. We follow the
OntoNotes (Weischedel et al, 2011) definition of
coreference to be able to integrate our annotations
with it. This definition includes coreference with
noun phrase as well as verb phrase antecedents3 .
Mediated refers to entities which have not yet
been introduced in the text but are inferrable via
other mentions or are known via world knowl-
edge. We distinguish the following six subcate-
gories: The category mediated/comparative
comprises mentions compared via either a contrast
or similarity to another one (see Example 1). This
category is novel in our scheme. We also in-
clude a category mediated/bridging (see Ex-
amples 2, 3 and 4). Bridging anaphora can be
any noun phrase and are not limited to definite NPs
as in Poesio et al (2004), Gardent and Manue?lian
(2005), Riester et al (2010). In contrast to Nissim
et al (2004), antecedents for both comparative and
bridging categories are annotated and can be noun
phrases, verb phrases or even clauses. The category
mediated/knowledge is inspired by the hearer-
old distinction introduced by Prince (1992) and cov-
ers entities generally known to the hearer. It includes
many proper names, such as Poland.4 Mentions that
are syntactically linked via a possessive relation or a
PP modification to other, old or mediated men-
tions fall into the type mediated/synt (see Ex-
amples 5 and 6).5 With no change to Nissim et al?s
scheme, coordinated mentions where at least one el-
ement in the conjunction is old or mediated are
covered by the category mediated/aggregate,
and mentions referring to a value of a previously
mentioned function by the type mediated/func.
All other mentions are annotated as new, includ-
3In contrast to Nissim et al (2004), but in accordance with
OntoNotes, we do not consider generics for coreference.
4This class corresponds roughly to Nissim et al?s (2004)
mediated/general.
5This class expands Nissim et al?s (2004) poss category
that only considers possessives but not PP modification.
ing most generics as well as newly introduced, spe-
cific mentions such as Example 7.
(3) Initial steps were taken at Poland?s first en-
vironmental conference, which I attended
last month. . . . it was no accident that par-
ticipants urged the free flow of information
(4) The Bakersfield supermarket went out of
business last May. The reason was . . .
(5) One Washington couple sold their liquor
store
(6) the main artery into San Francisco
(7) the owner was murdered by robbers
3.2 Agreement Study
We carried out an agreement study with 3 annota-
tors, of which Annotator A was the scheme devel-
oper and first author of this paper. All texts used
were from the Wall Street Journal (WSJ) portion of
OntoNotes. There were no restrictions on which
texts to include apart from (i) exclusion of letters
to the editor as they contain cross-document links
and (ii) a preference for longer texts with potentially
richer discourse structure.
Mentions were automatically preselected for the
annotators using the gold-standard syntactic annota-
tion.6 The existing coreference annotation was auto-
matically carried over to the IS task by marking all
mentions in a coreference chain (apart from the first
mention in the chain) as old. The annotation task
consisted of marking all mentions for their IS (old,
mediated or new) as well as marking mediated
subcategories (see Section 3.1) and the antecedents
for comparative and bridging anaphora.
The scheme was developed on 9 texts, which were
also used for training the annotators. Inter-annotator
agreement was measured on 26 new texts, which in-
cluded 5905 pre-marked potential mentions. The an-
notations of 1499 of these were carried over from
OntoNotes, leaving 4406 potential mentions for an-
notation and agreement measurement. In addition to
6Some non-mentions such as idioms could not be filtered
out via the syntactic annotation and had to be excluded during
human annotation.
797
A-B A-C B-C
Overall Percentage coarse 87.5 86.3 86.5
Overall ? coarse 77.3 75.2 74.7
Overall Percentage fine 86.6 85.3 85.7
Overall ? fine 80.1 77.7 77.3
Table 1: Agreement Results
A-B A-C B-C
? Non-mention 81.5 78.9 86.0
? Old 80.5 83.2 79.3
? New 76.6 74.0 74.3
? Mediated/Knowledge 82.1 78.4 74.1
? Mediated/Synt 88.4 87.8 87.6
? Mediated/Aggregate 87.0 85.4 86.0
? Mediated/Func 6.0 83.2 6.9
? Mediated/Comp 81.8 78.3 81.2
? Mediated/Bridging 70.8 60.6 62.3
Table 2: Agreement Results for individual categories
percentage agreement, we measured Cohen?s ? (Art-
stein and Poesio, 2008) between all 3 possible anno-
tator pairings. We also report single-category agree-
ment for each category, where all categories but one
are merged and then ? is computed as usual. Table 1
shows agreement results for the overall scheme at
the coarse-grained (4 categories: non-mention, old,
new, mediated) and the fine-grained level (9 cate-
gories: non-mention, old, new and the 6 mediated
subtypes). The results show that the scheme is over-
all reliable, with not too many differences between
the different annotator pairings.7
Table 2 shows the individual category agreement
for all 9 categories. We achieve high reliability for
most categories.8 Particularly interesting is the fact
that hearer-old entities (mediated/knowledge)
can be identified reliably although all annotators had
substantially different backgrounds. The reliabil-
ity of the category bridging is more annotator-
dependent, although still higher, sometimes con-
siderably, than other previous attempts at bridg-
7Often, annotation is considered highly reliable when ? ex-
ceeds 0.80 and marginally reliable when between 0.67 and 0.80
(Carletta, 1996). However, the interpretation of ? is still under
discussion (Artstein and Poesio, 2008).
8The low reliability of the rare category func, when involv-
ing Annotator B, was explained by Annotator B forgetting about
this category after having used it once. Pair A-C achieved high
reliability (? 83.2 for pair A-C).
ing annotation (Poesio et al, 2004; Gardent and
Manue?lian, 2005; Riester et al, 2010).
3.3 Gold Standard
Our final gold standard corpus consists of 50 texts
from the WSJ portion of the OntoNotes corpus-
The corpus will be made publically available as
OntoNotes annotation layer via http://www.
h-its.org/nlp/download.
Disagreements in the 35 texts used for annota-
tor training (9 texts) and testing (26 texts) were re-
solved via discussion between the annotators. An
additional 15 texts were annotated by Annotator A.
Finally, Annotator A carried out consistency checks
over all texts. ? The gold standard includes 10,980
true mentions (see Table 3).
Texts 50
Mentions 10,980
old 3237
coref 3,143
generic deictic pr 94
mediated 3,708
world knowledge 924
syntactic 1,592
aggregate 211
func 65
comparative 253
bridging 663
new 4,035
Table 3: Gold Standard Distribution
4 Features
In this Section, we describe both the local as well as
the relational features we use.
4.1 Features for Local Classification
We use the following local features, including the
features in Nissim (2006) and Rahman and Ng
(2011) to be able to gauge how their systems fare on
our corpus and as a comparison point for our novel
collective classification approach.
The features developed by Nissim (2006) are
shown in Table 4. Nissim shows clearly that
these features are useful for IS classification.
Thus, subjects are more likely to be old as as-
sumed by, e.g., centering theory (Grosz et al,
798
Feature Value
full prev mention {yes, no, NA}9
mention time {first, second, more}
partial prev mention {yes, no, NA}
determiner {bare, def, dem, indef, poss, NA}
NP type {pronoun, common, proper, other}
NP length numeric
grammatical role {subject, subjpass, pp, other}
Table 4: Nissim?s (2006) feature set
1995). Also, previously unmentioned proper names
are more likely to be hearer-old and therefore
mediated/knowledge, although their exact sta-
tus will depend on how well known a particular
proper name is.
Rahman and Ng (2011) add all unigrams appear-
ing in any mention in the training set as features.
They also integrated (via a convolution tree-kernel
SVM (Collins and Duffy, 2001)) partial parse trees
that capture the generalised syntactic context of a
mention e and include the mention?s parent and sib-
ling nodes without lexical leaves. However, they use
no structure underneath the mention node e itself,
assuming that ?any NP-internal information has pre-
sumably been captured by the flat features?.
To these feature sets, we add a small set of other
local features otherlocal. These track partial previ-
ous mentions by also counting partial previous men-
tion time as well as the previous mention of con-
tent words only. We also add a mention?s number as
one of singular, plural or unknown, and whether the
mention is modified by an adjective. Another feature
encapsulates whether the mention is modified by a
comparative marker, using a small set of 10 markers
such as another, such, similar . . . and the presence
of adjectives or adverbs in the comparative. Finally,
we include the mention?s semantic class as one of 12
coarse-grained classes, including location, organisa-
tion, person and several classes for numbers (such as
date, money or percent).
4.2 Relations for Collective Classification
Both Nissim (2006) and Rahman and Ng (2011)
classify each mention individually in a standard su-
pervised ML setting, not considering potential de-
pendencies between the IS categories of different
9We changed the value of ?full prev mention? from ?nu-
meric? to {yes, no, NA}.
mentions. However, collective or joint classifica-
tion has made substantial impact in other NLP tasks,
such as opinion mining (Pang and Lee, 2004; Soma-
sundaran et al, 2009), text categorization (Yang et
al., 2002; Taskar et al, 2002) and the related task of
coreference resolution (Denis and Baldridge, 2007).
We investigate two types of relations between men-
tions that might impact on IS classification.
Syntactic parent-child relations. Two media-
ted subcategories account for accessibility via syn-
tactic links to another old or mediated men-
tion: mediated/synt is used when at least one
child of a mention is mediated or old, with child
relations restricted to pre- or postnominal posses-
sives as well as PP children in our scheme (see Sec-
tion 3.1). mediated/aggregate is for coordi-
nations in which at least one of the children is old
or mediated. In these two cases, a mention?s
IS depends directly on the IS of its children. We
therefore link a mention m1 to a mention m2 via a
hasChild relation if (i) m2 is a possessive or prepo-
sitional modification ofm1, or (ii)m1 is a coordina-
tion and m2 is one of its children.
Using such a relational feature catches two birds
with one stone: firstly, it integrates the internal struc-
ture of a mention into the algorithm, which Rah-
man and Ng (2011) ignore; secondly, it captures de-
pendencies between parent and child classification,
which would not be possible if we integrated the in-
ternal structure via flat features or additional tree
kernels. We hypothesise that the higher syntactic
complexity of our news genre (14.5% of all men-
tions are mediated/synt) will make this feature
highly effective in distinguishing between new and
mediated categories.
Syntactic precedence relations. IS is said to in-
fluence word order (Birner and Ward, 1998; Cahill
and Riester, 2009) and this fact has been exploited
in work on generation (Prevost, 1996; Filippova and
Strube, 2007; Cahill and Riester, 2009). Therefore,
we integrate dependencies between the IS classifica-
tion of mentions in precedence relations.
m1 precedes m2 if (i) m1 and m2 are in the same
clause, allowing for trace subjects in gerund and in-
finitive constructions, (ii) m1 and m2 are dependent
on the same verb or noun, allowing for interven-
ing nodes via modal, auxiliary, gerund and infinitive
799
constructions, (iii) m1 is neither a child nor a parent
of m2, and (iv) m1 occurs before m2.
For Example 8 (slightly simplified) we extract the
precedence relations shown in Table 5.
(8) She was sent by her mother to a white
woman?s house to do chores in exchange for
meals and a place to sleep.
(She)old >p (her mother)med/synt
(She)old >p (a white-woman?s house)new
(She)old >p (chores)new
(She)old >p (exchange .....sleep)new
(her mother)med/synt >p (a white woman?s house)new
(chores)new >p (exchange . . . sleep)new
(meals)new >p (a place to sleep)new
Table 5: Precedence Relations for Example 8. She is a
trace subject for do.
Proper names behave differently from common
nouns. For example, they can occur at many differ-
ent places in the clause when functioning as spatial
or temporal scene-setting elements, such as In New
York. We therefore exclude all precedence relations
where one element of the pair is a proper name.
We extract 2855 precedence relations. Table 6
shows the statistics on precedence with the first men-
tion in a pair in rows and the second in columns. Me-
diated and new mentions indeed rarely precede old
mentions, so that precedence should improve sepa-
rating of old vs other mentions.
old mediated new
old 136 387 519
mediated 88 357 379
new 85 291 613
Table 6: Precedence relations in our corpus
5 Experiments
5.1 Experimental Setup
We use our gold standard corpus (see Section 3.3)
via 10-fold cross-validation on documents for all ex-
periments. Following Nissim (2006) and Rahman
and Ng (2011), we perform all experiments on gold
standard mentions and use the human WSJ syntac-
tic annotation for feature extraction, when neces-
sary. For the extraction of semantic class, we use
OntoNotes entity type annotation for proper names
and an automatic assignment of semantic class via
WordNet hypernyms for common nouns.
Coarse-grained versions of all algorithms distin-
guish only between the three old, mediated,
new categories. Fine-grained versions distinguish
between the categories old, the six mediated
subtypes, and new. We report overall accuracy as
well as precision, recall and F-measure per category.
Significance tests are conducted using McNemar?s
test on overall algorithm accuracy, at the level of 1%.
5.2 Local Classifiers
We reimplemented the algorithms in Nissim (2006)
and Rahman and Ng (2011) as comparison base-
lines, using their feature and algorithm choices. Al-
gorithm Nissim is therefore a decision tree J48 with
standard settings in WEKA with the features in Ta-
ble 4. Algorithm RahmanNg is an SVM with a com-
posite kernel and one-vs-all training/testing (toolkit
SVMLight). They use the features in Table 4 plus
unigram and tree kernel features, described in Sec-
tion 4.1. We add our additional set of otherlocal
features to both baseline algorithms (yielding Nis-
sim+ol and RahmanNg+ol) as they aim specifically
at improving fine-grained classification.
5.3 Collective Classification
For incorporating our inter-mention links, we use a
variant of Iterative Collective classification (ICA),
which has shown good performance over a variety
of tasks (Lu and Getoor, 2003) and has been used
in NLP for example for opinion mining (Somasun-
daran et al, 2009). ICA is normally faster than
Gibbs sampling and ? in initial experiments ? did
not yield significantly different results from it.
ICA initializes each mention with its most likely
IS, according to the local classifier and features. It
then iterates a relational classifier, which uses both
local and relational features (our hasChild and pre-
cedes features) taking IS assignments to neighbour-
ing mentions into account. We use the exist aggre-
gator to define the dependence between mentions.
We use NetKit (Macskassy and Provost, 2007)
with its standard ICA settings for collective infer-
ence, as it allows direct comparison between local
and collective classification. The relational classi-
fiers are always exactly the same classifiers as the
800
local collective
Nissim+ol Nissim+olNissim Nissim+ol
+hasChild +hasChild+precedes
R P F R P F R P F R P F
Coarse
old 82.2 86.4 84.2 81.2 88.6 84.8 81.7 88.6 85.0 80.9 89.1 84.8
mediated 51.9 60.2 55.7 57.8 64.6 61.0 68.4 77.4 72.6 68.8 76.9 72.6
new 74.2 63.6 68.5 78.4 67.3 72.4 87.7 75.1 80.9 87.9 75.0 80.9
acc 69.0 72.3 79.4 79.4
Fine
old 84.0 83.3 83.6 85.0 83.9 84.5 84.3 84.7 84.5 84.1 85.2 84.6
med/knowledge 61.3 60.0 60.6 61.0 69.5 65.0 62.3 70.0 65.9 60.6 70.0 65.0
med/synt 37.2 59.7 45.8 44.7 60.0 51.3 76.8 81.4 79.0 75.7 80.1 77.9
med/agg 26.0 42.0 32.2 20.4 38.4 26.6 42.6 55.9 48.4 43.1 55.8 48.7
med/func 0.0 NA NA 32.3 65.6 43.3 33.8 53.7 41.5 35.4 53.5 48.7
med/comp 0.4 7.70 0.7 79.0 82.6 80.0 80.6 82.9 81.8 81.4 82.0 81.7
med/bridging 6.6 26.2 10.6 8.9 30.9 13.8 9.6 34.4 15.1 12.2 41.7 18.9
new 82.6 61.0 70.2 82.7 65.1 72.8 88.0 74.0 80.4 87.7 73.3 79.8
acc 66.6 70.0 77.0 76.8
Table 7: Collective classification compared to Nissim?s local classifier. Best performing algorithms are bolded.
local ones with the relational features added: thus, if
the local classifier is a tree kernel SVM so is the rela-
tional one. One problem when using the SVM Tree
kernel as relational classifier is that it allows only for
binary classification so that we need to train several
binary networks in a one-vs-all paradigm (see also
(Rahman and Ng, 2011)), which will not be able to
use the multiclass dependencies of the relational fea-
tures to optimum effect.
5.4 Results
Table 7 shows the comparison of collective classifi-
cation to local classification, using Nissim?s frame-
work and features, and Table 8 the equivalent table
for Rahman and Ng?s approach.
The improvements using the additional local fea-
tures over the original local classifiers are sta-
tistically significant in all cases. In particu-
lar, the inclusion of semantic classes improves
mediated/knowledge and mediated/func,
and comparative anaphora are recognised highly re-
liably via a small set of comparative markers.
The hasChild relation leads to significant im-
provement in accuracy over local classification in
all cases, showing the value of collective clas-
sification. The improvement here is centered
on the categories of mediated/synt (for both
cases) and mediated/aggregate (for Nis-
sim+ol+hasChild) as well as their distinction from
new.10 It is also interesting that collective clas-
sification with a concise feature set and a sim-
ple decision tree as used in Nissim+ol+hasChild,
performs equally well as RahmanNg+ol+hasChild,
which uses thousands of unigram and tree features
and a more sophisticated local classifier. It also
shows more consistent improvements over all fine-
grained classes.
The precedes relation does not lead to any fur-
ther improvement. We investigated several varia-
tions of the precedence link, such as restricting it
to certain grammatical relations, taking into account
definiteness or NP type but none of them led to
any improvement. We think there are two reasons
for this lack of success. First, the precedence of
mediated vs. new mentions does not follow a
clear order and is therefore not a very predictive fea-
ture (see Table 6). At first, this seems to contradict
studies such as Cahill and Riester (2009) that find
a variety of precedences according to information
status. However, many of the clearest precedences
they find are more specific variants of the old >p
mediated or old >p new precedence or they
are preferences at an even finer level than the one we
annotate, including for example the identification of
generics. Second, the clear old >p mediated
10For RhamanNg+ol+hasChild, the aggregate class suf-
fers from collective classification. We hypothesise that this is
an artefact of the one-vs-all training/testing for rare categories.
801
local collective
RahmanNg+ol RahmanNg+olRahmanNg RahmanNg+ol
+hasChild +hasChild+precedes
R P F R P F R P F R P F
Coarse
old 81.3 90.1 85.5 82.6 91.4 86.8 83.5 87.8 85.6 82.9 87.2 85.0
mediated 61.4 68.6 64.8 61.5 71.9 66.3 66.7 79.5 72.6 64.8 76.7 70.3
new 82.1 69.9 75.5 84.9 70.1 76.8 89.0 74.9 81.3 86.9 73.5 79.6
acc 74.9 76.3 79.8 78.3
Fine
old 85.1 87.0 86.0 85.6 87.9 86.7 85.3 87.4 86.3 85.8 87.5 86.4
med/knowledge 65.8 67.2 66.5 64.8 72.6 68.5 67.1 69.6 68.3 64.7 73.2 68.7
med/synt 55.8 72.1 62.9 55.8 72.6 63.1 79.8 78.1 78.9 79.8 78.1 78.9
med/agg 29.9 75.9 42.9 29.9 75.9 42.9 17.1 53.7 25.9 14.2 49.2 22.1
med/func 27.7 38.3 32.1 38.5 69.4 49.5 40.0 44.1 42.0 40.0 40.0 40.0
med/comp 25.3 86.5 39.1 76.7 82.2 79.3 74.3 62.7 68.0 74.3 62.7 68.0
med/bridging 10.6 44.6 17.1 9.0 47.2 15.2 1.0 15.2 2.0 1.0 13.7 1.9
new 87.3 66.3 75.4 89.0 67.8 77.0 89.2 74.6 81.2 89.2 74.6 81.2
acc 72.6 74.6 77.5 77.4
Table 8: Collective classification compared to Rahman and Ng?s local classifier. Best performing algorithms are
bolded.
and old >p new preferences are partially already
captured by the local features, especially the gram-
matical role, as, for example, subjects are often both
old as well as early on in a sentence.
With regard to fine-grained classification, many
categories including comparative anaphora, are
identified quite reliably, especially in the multiclass
classification setting (Nissim+ol+hasChild). Bridg-
ing seems to be the by far most difficult category
to identify with final best F-measures still very low.
Most bridging mentions do not have any clear inter-
nal structure or external syntactic contexts that sig-
nal their presence. Instead, they rely more on lexi-
cal and world knowledge for recognition. Unigrams
could potentially encapsulate some of this lexical
knowledge but ? without generalization ? are too
sparse for a relatively rare category such as bridg-
ing (6% of all mentions) to perform well. The diffi-
culty of bridging recognition is an important insight
of this paper as it casts doubt on the strategy in pre-
vious research to concentrate almost exclusively on
antecedent selection (see Section 2).
6 Conclusions
We presented a new approach to information sta-
tus classification in written text, for which we also
provide the first reliably annotated English language
corpus. Based on linguistic intuition, we define fea-
tures for classifying mentions collectively. We show
that our collective classification approach outper-
forms the state-of-the-art in coarse-grained IS classi-
fication by about 10% (Nissim, 2006) and 5% (Rah-
man and Ng, 2011) accuracy. The gain is almost
entirely due to improvements in distinguishing be-
tween new and mediatedmentions. For the latter,
we also report the ? to our knowledge ? first fine-
grained IS classification results.
Since the work reported in this paper relied ? fol-
lowing Nissim (2006) and Rahman and Ng (2011)
? on gold standard mentions and syntactic anno-
tations, we plan to perform experiments with pre-
dicted mentions as well. We also have to im-
prove the recognition of bridging, ideally combining
recognition and antecedent selection for a complete
resolution component. In addition, we plan to inte-
grate IS resolution with our coreference resolution
system (Cai et al, 2011) to provide us with a more
comprehensive discourse processing system.
Acknowledgements. Katja Markert received a Fel-
lowship for Experienced Researchers by the Alexander-
von-Humboldt Foundation and Yufang Hou is funded by
a PhD scholarship from the Research Training GroupCo-
herence in Language Processing at Heidelberg Univer-
sity. We thank the Heidelberg Institute for Theoretical
Studies for hosting Katja Markert and funding the anno-
tation study, and the annotators for their diligent work.
802
References
Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. Computa-
tional Linguistics, 34(4):555?596.
Regina Barzilay and Mirella Lapata. 2008. Modeling
local coherence: An entity-based approach. Computa-
tional Linguistics, 34(1):1?34.
Betty J. Birner and Gregory Ward. 1998. Information
Status and NoncanonicalWord Order in English. John
Benjamins, Amsterdam, The Netherlands.
Aoife Cahill and Arndt Riester. 2009. Incorporating in-
formation status into generation ranking. In Proceed-
ings of the Joint Conference of the 47th Annual Meet-
ing of the Association for Computational Linguistics
and the 4th International Joint Conference on Natural
Language Processing, Singapore, 2?7 August 2009,
pages 817?825.
Jie Cai, ?Eva Mu?jdricza-Maydt, and Michael Strube.
2011. Unrestricted coreference resolution via global
hypergraph partitioning. In Proceedings of the Shared
Task of the 15th Conference on Computational Natu-
ral Language Learning, Portland, Oreg., 23?24 June
2011, pages 56?60.
Jean Carletta. 1996. Assessing agreement on classifi-
cation tasks: The kappa statistic. Computational Lin-
guistics, 22(2):249?254.
Michael Collins and Nigel Duffy. 2001. Convolution
kernels for natural language. In Advances in Neural
Information Processing Systems 14, Vancouver, B.C.,
Canada, 3?8 December, 2001, pages 625?632, Cam-
bridge, Mass. MIT Press.
Pascal Denis and Jason Baldridge. 2007. Joint determi-
nation of anaphoricity and coreference resolution us-
ing integer programming. In Proceedings of Human
Language Technologies 2007: The Conference of the
North American Chapter of the Association for Com-
putational Linguistics, Rochester, N.Y., 22?27 April
2007, pages 236?243.
Katja Filippova and Michael Strube. 2007. Generat-
ing constituent order in German clauses. In Proceed-
ings of the 45th Annual Meeting of the Association for
Computational Linguistics, Prague, Czech Republic,
23?30 June 2007, pages 320?327.
Claire Gardent and He?le`ne Manue?lian. 2005. Cre?ation
d?un corpus annote? pour le traitement des descrip-
tions de?finies. Traitement Automatique des Langues,
46(1):115?140.
Barbara J. Grosz, Aravind K. Joshi, and Scott Weinstein.
1995. Centering: A framework for modeling the lo-
cal coherence of discourse. Computational Linguis-
tics, 21(2):203?225.
Iorn Korzen and Matthias Buch-Kromann. 2011.
Anaphoric relations in the Copenhagen dependency
treebanks. In S. Dipper and H. Zinsmeister, edi-
tors, Corpus-based Investigations of Pragmatic and
Discourse Phenomena, volume 3 of Bochumer Lin-
guistische Arbeitsberichte, pages 83?98. University of
Bochum, Bochum, Germany.
Ivana Kruijff-Korbayova? and Mark Steedman. 2003.
Discourse and information structure. Journal of Logic,
Language and Information. Special Issue on Dis-
cource and Information Structure, 12(3):149?259.
Knud Lambrecht. 1994. Information Structure and Sen-
tence Form. Cambridge, U.K.: Cambridge University
Press.
Qing Lu and Lise Getoor. 2003. Link-based classifica-
tion. In Proceedings of the 20th International Confer-
ence on Machine Learning, Washington, D.C., 21?24
August 2003, pages 496?503.
Sofus A. Macskassy and Foster Provost. 2007. Classi-
fication in networked data: A toolkit and a univariate
case study. Journal of Machine Learning Research,
8:935?983.
Katja Markert and Malvina Nissim. 2005. Comparing
knowledge sources for nominal anaphora resolution.
Computational Linguistics, 31(3):367?401.
Josef Meyer and Robert Dale. 2002. Mining a corpus to
support associative anaphora resolution. In Proceed-
ings of the 4th International Conference on Discourse
Anaphora and Anaphor Resolution, Lisbon, Portugal,
18?20 September, 2002.
Natalia M. Modjeska, Katja Markert, and Malvina Nis-
sim. 2003. Using the web in machine learning for
other-anaphora resolution. In Proceedings of the 2003
Conference on Empirical Methods in Natural Lan-
guage Processing, Sapporo, Japan, 11?12 July 2003,
pages 176?183.
Ani Nenkova, Jason Brenier, Anubha Kothari, Sasha Cal-
houn, LauraWhitton, David Beaver, and Dan Jurafsky.
2007. To memorize or to predict: Prominence labeling
in conversational speech. In Proceedings of Human
Language Technologies 2007: The Conference of the
North American Chapter of the Association for Com-
putational Linguistics, Rochester, N.Y., 22?27 April
2007, pages 9?16.
Vincent Ng. 2009. Graph-cut-based anaphoricity deter-
mination for coreference resolution. In Proceedings of
Human Language Technologies 2009: The Conference
of the North American Chapter of the Association for
Computational Linguistics, Boulder, Col., 31 May ? 5
June 2009, pages 575?583.
Malvina Nissim, Shipara Dingare, Jean Carletta, and
Mark Steedman. 2004. An annotation scheme for in-
formation status in dialogue. In Proceedings of the 4th
International Conference on Language Resources and
Evaluation, Lisbon, Portugal, 26?28 May 2004, pages
1023?1026.
803
Malvina Nissim. 2006. Learning information status of
discourse entities. In Proceedings of the 2006 Con-
ference on Empirical Methods in Natural Language
Processing, Sydney, Australia, 22?23 July 2006, pages
94?012.
Bo Pang and Lillian Lee. 2004. A sentimental education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. In Proceedings of the 42nd
Annual Meeting of the Association for Computational
Linguistics, Barcelona, Spain, 21?26 July 2004, pages
272?279.
Massimo Poesio, Rahul Mehta, Axel Maroudas, and
Janet Hitzeman. 2004. Learning to resolve bridging
references. In Proceedings of the 42nd Annual Meet-
ing of the Association for Computational Linguistics,
Barcelona, Spain, 21?26 July 2004, pages 143?150.
Massimo Poesio. 2004. The MATE/GNOME proposals
for anaphoric annotation, revisited. In Proceedings of
the 5th SIGdial Workshop on Discourse and Dialogue,
Cambridge, Mass., 30 April ? 1 May 2004, pages 154?
162.
Scott Prevost. 1996. An information structural approach
to spoken language generation. In Proceedings of the
34th Annual Meeting of the Association for Computa-
tional Linguistics, Santa Cruz, Cal., 24?27 June 1996,
pages 294?301.
Ellen F. Prince. 1981. Towards a taxonomy of given-new
information. In P. Cole, editor, Radical Pragmatics,
pages 223?255. Academic Press, New York, N.Y.
Ellen F. Prince. 1992. The ZPG letter: Subjects,
definiteness, and information-status. In W.C. Mann
and S.A. Thompson, editors, Discourse Description.
Diverse Linguistic Analyses of a Fund-Raising Text,
pages 295?325. John Benjamins, Amsterdam.
Altaf Rahman and Vincent Ng. 2011. Learning the in-
formation status of noun phrases in spoken dialogues.
In Proceedings of the 2011 Conference on Empirical
Methods in Natural Language Processing, Edinburgh,
Scotland, U.K., 27?29 July 2011, pages 1069?1080.
Arndt Riester, David Lorenz, and Nina Seemann. 2010.
A recursive annotation scheme for referential informa-
tion status. In Proceedings of the 7th International
Conference on Language Resources and Evaluation,
La Valetta, Malta, 17?23 May 2010, pages 717?722.
Julia Ritz, Stefanie Dipper, and Michael Go?tze. 2008.
Annotation of information structure: An evaluation
across different types of texts. In Proceedings of the
6th International Conference on Language Resources
and Evaluation, Marrakech, Morocco, 26 May ? 1
June 2008, pages 2137?2142.
Ryohei Sasano and Sadao Kurohashi. 2009. A prob-
abilistic model for associative anaphora resolution.
In Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing, Singapore,
6?7 August 2009, pages 1455?1464.
Advaith Siddharthan, Ani Nenkova, and Kathleen McK-
eown. 2011. Information status distinctions and re-
ferring expressions: An empirical study of references
to people in news summaries. Computational Linguis-
tics, 37(4):811?842.
Swapna Somasundaran, Galileo Namata, Janyce Wiebe,
and Lise Getoor. 2009. Supervised and unsupervised
methods in employing discourse relations for improv-
ing opinion polarity classification. In Proceedings of
the 2009 Conference on Empirical Methods in Natural
Language Processing, Singapore, 6?7 August 2009.
Ben Taskar, Pieter Abbeel, and Daphne Koller. 2002.
Discriminative probabilistic models for relational data.
In Proceedings of the 18th Conference on Uncertainty
in Artificial Intelligence, Edmonton, Alberta, Canada,
1-4 August 2002, pages 485?492.
Renata Vieira and Massimo Poesio. 2000. An
empirically-based system for processing definite de-
scriptions. Computational Linguistics, 26(4):539?
593.
Ralph Weischedel, Martha Palmer, Mitchell Marcus, Ed-
uard Hovy, Sameer Pradhan, Lance Ramshaw, Ni-
anwen Xue, Ann Taylor, Jeff Kaufman, Michelle
Franchini, Mohammed El-Bachouti, Robert Belvin,
and Ann Houston. 2011. OntoNotes release 4.0.
LDC2011T03, Philadelphia, Penn.: Linguistic Data
Consortium.
Yiming Yang, Sea?n Slattery, and Rayid Ghani. 2002. A
study of approaches to hypertext categorization. Jour-
nal of Intelligent Information Systems, 18(2-3):219?
241.
Guodong Zhou and Fang Kong. 2009. Global learning of
noun phrase anaphoricity in coreference resolution via
label propagation. In Proceedings of the 2009 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, Singapore, 6?7 August 2009, pages 978?986.
804
