Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 161?168,
Sydney, July 2006. c?2006 Association for Computational Linguistics
PCFGs with Syntactic and Prosodic Indicators of Speech Repairs
John Halea Izhak Shafranb Lisa Yungc
Bonnie Dorrd Mary Harperde Anna Krasnyanskayaf Matthew Leaseg
Yang Liuh Brian Roarki Matthew Snoverd Robin Stewartj
a Michigan State University; b,c Johns Hopkins University; d University of Maryland, College Park; e Purdue University
f UCLA; g Brown University; h University of Texas at Dallas; i Oregon Health & Sciences University; j Williams College
Abstract
A grammatical method of combining two
kinds of speech repair cues is presented.
One cue, prosodic disjuncture, is detected
by a decision tree-based ensemble clas-
sifier that uses acoustic cues to identify
where normal prosody seems to be inter-
rupted (Lickley, 1996). The other cue,
syntactic parallelism, codifies the expec-
tation that repairs continue a syntactic
category that was left unfinished in the
reparandum (Levelt, 1983). The two cues
are combined in a Treebank PCFG whose
states are split using a few simple tree
transformations. Parsing performance on
the Switchboard and Fisher corpora sug-
gests that these two cues help to locate
speech repairs in a synergistic way.
1 Introduction
Speech repairs, as in example (1), are one kind
of disfluent element that complicates any sort
of syntax-sensitive processing of conversational
speech.
(1) and [ the first kind of invasion of ] the first
type of privacy seemed invaded to me
The problem is that the bracketed reparan-
dum region (following the terminology of Shriberg
(1994)) is approximately repeated as the speaker
The authors are very grateful for Eugene Charniak?s help
adapting his parser. We also thank the Center for Language
and Speech processing at Johns Hopkins for hosting the sum-
mer workshop where much of this work was done. This
material is based upon work supported by the National Sci-
ence Foundation (NSF) under Grant No. 0121285. Any opin-
ions, findings and conclusions or recommendations expressed
in this material are those of the authors and do not necessarily
reflect the views of the NSF.
?repairs? what he or she has already uttered.
This extra material renders the entire utterance
ungrammatical?the string would not be gener-
ated by a correct grammar of fluent English. In
particular, attractive tools for natural language
understanding systems, such as Treebank gram-
mars for written corpora, naturally lack appropri-
ate rules for analyzing these constructions.
One possible response to this mismatch be-
tween grammatical resources and the brute facts
of disfluent speech is to make one look more
like the other, for the purpose of parsing. In
this separate-processing approach, reparanda are
located through a variety of acoustic, lexical or
string-based techniques, then excised before sub-
mission to a parser (Stolcke and Shriberg, 1996;
Heeman and Allen, 1999; Spilker et al, 2000;
Johnson and Charniak, 2004). The resulting
parse tree then has the reparandum re-attached in
a standardized way (Charniak and Johnson, 2001).
An alternative strategy, adopted in this paper, is
to use the same grammar to model fluent speech,
disfluent speech, and their interleaving.
Such an integrated approach can use syntac-
tic properties of the reparandum itself. For in-
stance, in example (1) the reparandum is an
unfinished noun phrase, the repair a finished
noun phrase. This sort of phrasal correspon-
dence, while not absolute, is strong in conver-
sational speech, and cannot be exploited on the
separate-processing approach. Section 3 applies
metarules (Weischedel and Sondheimer, 1983;
McKelvie, 1998a; Core and Schubert, 1999) in
recognizing these correspondences using standard
context-free grammars.
At the same time as it defies parsing, con-
versational speech offers the possibility of lever-
aging prosodic cues to speech repairs. Sec-
161
Figure 1: The pause between two or s and the glottalization at the end of the first makes it easy for a
listener to identify the repair.
tion 2 describes a classifier that learns to label
prosodic breaks suggesting upcoming disfluency.
These marks can be propagated up into parse
trees and used in a probabilistic context-free gram-
mar (PCFG) whose states are systematically split
to encode the additional information.
Section 4 reports results on Switchboard (God-
frey et al, 1992) and Fisher EARS RT04F data,
suggesting these two features can bring about in-
dependent improvements in speech repair detec-
tion. Section 5 suggests underlying linguistic and
statistical reasons for these improvements. Sec-
tion 6 compares the proposed grammatical method
to other related work, including state of the art
separate-processing approaches. Section 7 con-
cludes by indicating a way that string- and tree-
based approaches to reparandum identification
could be combined.
2 Prosodic disjuncture
Everyday experience as well as acoustic anal-
ysis suggests that the syntactic interruption in
speech repairs is typically accompanied by a
change in prosody (Nakatani and Hirschberg,
1994; Shriberg, 1994). For instance, the spectro-
gram corresponding to example (2), shown in Fig-
ure 1,
(2) the jehovah?s witness or [ or ] mormons or
someone
reveals a noticeable pause between the occurrence
of the two ors, and an unexpected glottalization at
the end of the first one. Both kinds of cues have
been advanced as explanations for human listen-
ers? ability to identify the reparandum even before
the repair occurs.
Retaining only the second explanation, Lickley
(1996) proposes that there is no ?edit signal? per se
but that repair is cued by the absence of smooth
formant transitions and lack of normal juncture
phenomena.
One way to capture this notion in the syntax
is to enhance the input with a special disjunc-
ture symbol. This symbol can then be propa-
gated in the grammar, as illustrated in Figure 2.
This work uses a suffix ?+ to encode the percep-
tion of abnormal prosody after a word, along with
phrasal -BRK tags to decorate the path upwards to
reparandum constituents labeled EDITED. Such
NP
NP EDITED CC NP
NP NNP CC?BRK or NNPS
DT NNP POS witness
the jehovah ?s
or~+ mormons
Figure 2: Propagating BRK, the evidence of dis-
fluent juncture, from acoustics to syntax.
disjuncture symbols are identified in the ToBI la-
beling scheme as break indices (Price et al, 1991;
Silverman et al, 1992).
The availability of a corpus annotated with
ToBI labels makes it possible to design a break
index classifier via supervised training. The cor-
pus is a subset of the Switchboard corpus, con-
sisting of sixty-four telephone conversations man-
ually annotated by an experienced linguist accord-
ing to a simplified ToBI labeling scheme (Osten-
dorf et al, 2001). In ToBI, degree of disjuncture
is indicated by integer values from 0 to 4, where
a value of 0 corresponds to clitic and 4 to a major
phrase break. In addition, a suffix p denotes per-
ceptually disfluent events reflecting, for example,
162
hesitation or planning. In conversational speech
the intermediate levels occur infrequently and the
break indices can be broadly categorized into three
groups, namely, 1, 4 and p as in Wong et al
(2005).
A classifier was developed to predict three
break indices at each word boundary based on
variations in pitch, duration and energy asso-
ciated with word, syllable or sub-syllabic con-
stituents (Shriberg et al, 2005; Sonmez et al,
1998). To compute these features, phone-level
time-alignments were obtained from an automatic
speech recognition system. The duration of these
phonological constituents were derived from the
ASR alignment, while energy and pitch were com-
puted every 10ms with snack, a public-domain
sound toolkit (Sjlander, 2001). The duration, en-
ergy, and pitch were post-processed according to
stylization procedures outlined in Sonmez et al
(1998) and normalized to account for variability
across speakers.
Since the input vector can have missing val-
ues such as the absence of pitch during unvoiced
sound, only decision tree based classifiers were
investigated. Decision trees can handle missing
features gracefully. By choosing different com-
binations of splitting and stopping criteria, an
ensemble of decision trees was built using the
publicly-available IND package (Buntine, 1992).
These individual classifiers were then combined
into ensemble-based classifiers.
Several classifiers were investigated for detect-
ing break indices. On ten-fold cross-validation,
a bagging-based classifier (Breiman, 1996) pre-
dicted prosodic breaks with an accuracy of 83.12%
while chance was 67.66%. This compares favor-
ably with the performance of the supervised classi-
fiers on a similar task in Wong et al (2005). Ran-
dom forests and hidden Markov models provide
marginal improvements at considerable computa-
tional cost (Harper et al, 2005).
For speech repair, the focus is on detecting dis-
fluent breaks. The precision and recall trade-off
on its detection can be adjusted using a thresh-
old on the posterior probability of predicting ?p?,
as shown in Figure 3.
In essence, the large number of acoustic and
prosodic features related to disfluency are encoded
via the ToBI label ?p?, and provided as additional
observations to the PCFG. This is unlike previous
work on incorporating prosodic information (Gre-
00.10.20.30.40.50.6 0
0.1
0.2
0.3
0.4
0.5
0.6
Probability of Miss
Probab
ility of 
False 
Alarm
Figure 3: DET curve for detecting disfluent breaks
from acoustics.
gory et al, 2004; Lease et al, 2005; Kahn et al,
2005) as described further in Section 6.
3 Syntactic parallelism
The other striking property of speech repairs is
their parallel character: subsequent repair regions
?line up? with preceding reparandum regions. This
property can be harnessed to better estimate the
length of the reparandum by considering paral-
lelism from the perspective of syntax. For in-
stance, in Figure 4(a) the unfinished reparandum
noun phrase is repaired by another noun phrase ?
the syntactic categories are parallel.
3.1 Levelt?s WFR and Conjunction
The idea that the reparandum is syntactically par-
allel to the repair can be traced back to Levelt
(1983). Examining a corpus of Dutch picture de-
scriptions, Levelt proposes a bi-conditional well-
formedness rule for repairs (WFR) that relates the
structure of repairs to the structure of conjunc-
tions. The WFR conceptualizes repairs as the con-
junction of an unfinished reparandum string (?)
with a properly finished repair (?). Its original
formulation, repeated here, ignores optional inter-
regna like ?er? or ?I mean.?
Well-formedness rule for repairs (WFR) A re-
pair ???? is well-formed if and only if there
is a string ? such that the string ??? and? ??
is well-formed, where ? is a completion of
the constituent directly dominating the last
element of ?. (and is to be deleted if that
last element is itself a sentence connective)
In other words, the string ? is a prefix of a phrase
whose completion, ??if it were present?would
163
render the whole phrase ?? grammatically con-
joinable with the repair ?. In example (1) ? is the
string ?the first kind of invasion of?, ? is ?the first
type of privacy? and ? is probably the single word
?privacy.?
This kind of conjoinability typically requires
the syntactic categories of the conjuncts to be the
same (Chomsky, 1957, 36). That is, a rule schema
such as (2) where X is a syntactic category, is pre-
ferred over one where X is not constrained to be
the same on either side of the conjunction.
X ? X Conj X (2)
If, as schema (2) suggests, conjunction does fa-
vor like-categories, and, as Levelt suggests, well-
formed repairs are conjoinable with finished ver-
sions of their reparanda, then the syntactic cate-
gories of repairs ought to match the syntactic cat-
egories of (finished versions of) reparanda.
3.2 A WFR for grammars
Levelt?s WFR imposes two requirements on a
grammar
? distinguishing a separate category of ?unfin-
ished? phrases
? identifying a syntactic category for reparanda
Both requirements can be met by adapting Tree-
bank grammars to mirror the analysis of McK-
elvie1 (1998a; 1998b). McKelvie derives phrase
structure rules for speech repairs from fluent rules
by adding a new feature called abort that can
take values true and false. For a given gram-
mar rule of the form
A ? B C
a metarule creates other rules of the form
A [abort = Q] ?
B [abort = false] C [abort = Q]
where Q is a propositional variable. These rules
say, in effect, that the constituent A is aborted just
in case the last daughter C is aborted. Rules that
don?t involve a constant value for Q ensure that the
same value appears on parents and children. The
1McKelvie?s metarule approach declaratively expresses
Hindle?s (1983) Stack Editor and Category Copy Editor rules.
This classic work effectively states the WFR as a program for
the Fidditch deterministic parser.
WFR is then implemented by rule schemas such
as (3)
X ? X [abort = true] (AFF) X (3)
that permit the optional interregnum AFF to con-
join an unfinished X-phrase (the reparandum) with
a finished X-phrase (the repair) that comes after it.
3.3 A WFR for Treebanks
McKelvie?s formulation of Levelt?s WFR can be
applied to Treebanks by systematically recoding
the annotations to indicate which phrases are un-
finished and to distinguish matching from non-
matching repairs.
3.3.1 Unfinished phrases
Some Treebanks already mark unfinished
phrases. For instance, the Penn Treebank pol-
icy (Marcus et al, 1993; Marcus et al, 1994) is
to annotate the lowest node that is unfinished with
an -UNF tag as in Figure 4(a).
It is straightforward to propagate this mark up-
wards in the tree from wherever it is annotated to
the nearest enclosing EDITED node, just as -BRK
is propagated upwards from disjuncture marks on
individual words. This percolation simulates the
action of McKelvie?s [abort = true]. The re-
sulting PCFG is one in which distributions on
phrase structure rules with ?missing? daughters are
segregated from distributions on ?complete? rules.
3.4 Reparanda categories
The other key element of Levelt?s WFR is the
idea of conjunction of elements that are in some
sense the same. In the Penn Treebank annota-
tion scheme, reparanda always receive the label
EDITED. This means that the syntactic category
of the reparandum is hidden from any rule which
could favor matching it with that of the repair.
Adding an additional mark on this EDITED node
(a kind of daughter annotation) rectifies the situ-
ation, as depicted in Figure 4(b), which adds the
notation -childNP to a tree in which the unfin-
ished tags have been propagated upwards. This
allows a Treebank PCFG to represent the general-
ization that speech repairs tend to respect syntactic
category.
4 Results
Three kinds of experiments examined the effec-
tiveness of syntactic and prosodic indicators of
164
SCC EDITED NP
and NP NP
NP PP
DT JJ NN IN NP
the first kind of NP PP?UNF
NN IN
invasion of
DT JJ NN
the first type
(a) The lowest unfinished node is given.
S
CC EDITED?childNP NP
and NP?UNF NP
NP PP?UNF
DT JJ NN IN NP?UNF
the first kind of NP PP?UNF
NN IN
invasion of
DT JJ NN
the first type
(b) -UNF propagated, daughter-annotated Switchboard tree
Figure 4: Input (a) and output (b) of tree transformations.
speech repairs. The first two use the CYK algo-
rithm to find the most likely parse tree on a gram-
mar read-off from example trees annotated as in
Figures 2 and 4. The third experiment measures
the benefit from syntactic indicators alone in Char-
niak?s lexicalized parser (Charniak, 2000). The ta-
bles in subsections 4.1, 4.2, and 4.3 summarize
the accuracy of output parse trees on two mea-
sures. One is the standard Parseval F-measure,
which tracks the precision and recall for all labeled
constituents as compared to a gold-standard parse.
The other measure, EDIT-finding F, restricts con-
sideration to just constituents that are reparanda. It
measures the per-word performance identifying a
word as dominated by EDITED or not. As in pre-
vious studies, reference transcripts were used in all
cases. A check (
?
) indicates an experiment where
prosodic breaks where automatically inferred by
the classifier described in section 2, whereas in the
(?) rows no prosodic information was used.
4.1 CYK on Fisher
Table 1 summarizes the accuracy of a stan-
dard CYK parser on the newly-treebanked
Fisher corpus (LDC2005E15) of phone conver-
sations, collected as part of the DARPA EARS
program. The parser was trained on the entire
Switchboard corpus (ca. 107K utterances) then
tested on the 5368-utterance ?dev2? subset of the
Fisher data. This test set was tagged using MX-
POST (Ratnaparkhi, 1996) which was itself trained
on Switchboard. Finally, as described in section 2
these tags were augmented with a special prosodic
break symbol if the decision tree rated the proba-
bility a ToBI ?p? symbol higher than the threshold
value of 0.75.
A
nn
ot
at
io
n
Br
ea
k
in
de
x
Pa
rs
ev
a
lF
ED
IT
F
none
? 66.54 22.9?
66.08 26.1
daughter annotation ? 66.41 29.4? 65.81 31.6
-UNF propagation ? 67.06 31.5? 66.45 34.8
both ? 69.21 40.2? 67.02 40.6
Table 1: Improvement on Fisher, MXPOSTed tags.
The Fisher results in Table 1 show that syntac-
tic and prosodic indicators provide different kinds
of benefits that combine in an additive way. Pre-
sumably because of state-splitting, improvement
in EDIT-finding comes at the cost of a small decre-
ment in overall parsing performance.
4.2 CYK on Switchboard
Table 2 presents the results of similar experi-
ments on the Switchboard corpus following the
165
train/dev/test partition of Charniak and Johnson
(2001). In these experiments, the parser was given
correct part-of-speech tags as input.
A
nn
ot
at
io
n
Br
ea
k
in
de
x
Pa
rs
ev
a
lF
ED
IT
F
none
? 70.92 18.2?
69.98 22.5
daughter annotation ? 71.13 25.0? 70.06 25.5
-UNF propagation ? 71.71 31.1? 70.36 30.0
both ? 71.16 41.7? 71.05 36.2
Table 2: Improvement on Switchboard, gold tags.
The Switchboard results demonstrate independent
improvement from the syntactic annotations. The
prosodic annotation helps on its own and in com-
bination with the daughter annotation that imple-
ments Levelt?s WFR.
4.3 Lexicalized parser
Finally, Table 3 reports the performance of Char-
niak?s non-reranking, lexicalized parser on the
Switchboard corpus, using the same test/dev/train
partition.
Annotation Parseval F EDIT F
baseline 83.86 57.6
daughter annotation 80.85 67.2
-UNF propagation 81.68 64.7
both 80.16 70.0
flattened EDITED 82.13 64.4
Table 3: Charniak as an improved EDIT-finder.
Since Charniak?s parser does its own tagging,
this experiment did not examine the utility of
prosodic disjuncture marks. However, the com-
bination of daughter annotation and -UNF prop-
agation does lead to a better grammar-based
reparandum-finder than parsers trained on flat-
tened EDITED regions. More broadly, the re-
sults suggest that Levelt?s WFR is synergistic with
the kind of head-to-head lexical dependencies that
Charniak?s parser uses.
5 Discussion
The pattern of improvement in tables 1, 2, and
3 from none or baseline rows where no syntac-
tic parallelism or break index information is used,
to subsequent rows where it is used, suggest why
these techniques work. Unfinished-category an-
notation improves performance by preventing the
grammar of unfinished constituents from being
polluted by the grammar of finished constituents.
Such purification is independent of the fact that
rules with daughters labeled EDITED-childXP
tend to also mention categories labeled XP fur-
ther to the right (or NP and VP, when XP starts
with S). This preference for syntactic parallelism
can be triggered either by externally-suggested
ToBI break indices or grammar rules annotated
with -UNF. The prediction of a disfluent break
could be further improved by POS features and N-
gram language model scores (Spilker et al, 2001;
Liu, 2004).
6 Related Work
There have been relatively few attempts to harness
prosodic cues in parsing. In a spoken language
system for VERBMOBIL task, Batliner and col-
leagues (2001) utilize prosodic cues to dramati-
cally reduce lexical analyses of disfluencies in a
end-to-end real-time system. They tackle speech
repair by a cascade of two stages ? identification of
potential interruption points using prosodic cues
with 90% recall and many false alarms, and the
lexical analyses of their neighborhood. Their ap-
proach, however, does not exploit the synergy be-
tween prosodic and syntactic features in speech re-
pair. In Gregory et al (2004), over 100 real-valued
acoustic and prosodic features were quantized into
a heuristically selected set of discrete symbols,
which were then treated as pseudo-punctuation in
a PCFG, assuming that prosodic cues function like
punctuation. The resulting grammar suffered from
data sparsity and failed to provide any benefits.
Maximum entropy based models have been more
successful in utilizing prosodic cues. For instance,
in Lease et al (2005), interruption point probabil-
ities, predicted by prosodic classifiers, were quan-
tized and introduced as features into a speech re-
pair model along with a variety of TAG and PCFG
features. Towards a clearer picture of the inter-
action with syntax and prosody, this work uses
ToBI to capture prosodic cues. Such a method is
analogous to Kahn et al (2005) but in a genera-
tive framework.
The TAG-based model of Johnson and Charniak
(2004) is a separate-processing approach that rep-
166
resents the state of the art in reparandum-finding.
Johnson and Charniak explicitly model the
crossed dependencies between individual words
in the reparandum and repair regions, intersect-
ing this sequence model with a parser-derived lan-
guage model for fluent speech. This second step
improves on Stolcke and Shriberg (1996) and Hee-
man and Allen (1999) and outperforms the specific
grammar-based reparandum-finders tested in sec-
tion 4. However, because of separate-processing
the TAG channel model?s analyses do not reflect
the syntactic structure of the sentence being ana-
lyzed, and thus that particular TAG-based model
cannot make use of properties that depend on the
phrase structure of the reparandum region. This
includes the syntactic category parallelism dis-
cussed in section 3 but also predicate-argument
structure. If edit hypotheses were augmented to
mention particular tree nodes where the reparan-
dum should be attached, such syntactic paral-
lelism constraints could be exploited in the rerank-
ing framework of Johnson et al (2004).
The approach in section 3 is more closely re-
lated to that of Core and Schubert (1999) who
also use metarules to allow a parser to switch from
speaker to speaker as users interrupt one another.
They describe their metarule facility as a modi-
fication of chart parsing that involves copying of
specific arcs just in case specific conditions arise.
That approach uses a combination of longest-first
heuristics and thresholds rather than a complete
probabilistic model such as a PCFG.
Section 3?s PCFG approach can also be viewed
as a declarative generalization of Roark?s (2004)
EDIT-CHILD function. This function helps an
incremental parser decide upon particular tree-
drawing actions in syntactically-parallel contexts
like speech repairs. Whereas Roark conditions the
expansion of the first constituent of the repair upon
the corresponding first constituent of the reparan-
dum, in the PCFG approach there exists a separate
rule (and thus a separate probability) for each al-
ternative sequence of reparandum constituents.
7 Conclusion
Conventional PCFGs can improve their detection
of speech repairs by incorporating Lickley?s hy-
pothesis about interrupted prosody and by im-
plementing Levelt?s well-formedness rule. These
benefits are additive.
The strengths of these simple tree-based tech-
niques should be combinable with sophisticated
string-based (Johnson and Charniak, 2004; Liu,
2004; Zhang and Weng, 2005) approaches by
applying the methods of Wieling et al (2005)
for constraining parses by externally-suggested
brackets.
References
L. Breiman. 1996. Bagging predictors. Machine
Learning, 24(2):123?140.
W. Buntine. 1992. Tree classication software. In Tech-
nology 2002: The Third National Technology Trans-
fer Conference and Exposition, Baltimore.
E. Charniak and M. Johnson. 2001. Edit detection
and parsing for transcribed speech. In Proceedings
of the 2nd Meeting of the North American Chap-
ter of the Association for Computational Linguistics,
pages 118?126.
E. Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of NAACL-00, pages 132?
139.
N. Chomsky. 1957. Syntactic Structures. Anua Lin-
guarum Series Minor 4, Series Volume 4. Mouton
de Gruyter, The Hague.
M. G. Core and L. K. Schubert. 1999. A syntactic
framework for speech repairs and other disruptions.
In Proceedings of the 37th Annual Meeting of the As-
sociation for Computational Linguistics, pages 413?
420.
J. J. Godfrey, E. C. Holliman, and J. McDaniel. 1992.
SWITCHBOARD: Telephone speech corpus for re-
search and development. In Proceedings of ICASSP,
volume I, pages 517?520, San Francisco.
M. Gregory, M. Johnson, and E. Charniak. 2004.
Sentence-internal prosody does not help parsing the
way punctuation does. In Proceedings of North
American Association for Computational Linguis-
tics.
M. Harper, B. Dorr, J. Hale, B. Roark, I. Shafran,
M. Lease, Y. Liu, M. Snover, and L. Yung. 2005.
Parsing and spoken structural event detection. In
2005 Johns Hopkins Summer Workshop Final Re-
port.
P. A. Heeman and J. F. Allen. 1999. Speech repairs,
intonational phrases and discourse markers: model-
ing speakers? utterances in spoken dialog. Compu-
tational Linguistics, 25(4):527?571.
D. Hindle. 1983. Deterministic parsing of syntactic
non-fluencies. In Proceedings of the ACL.
M. Johnson and E. Charniak. 2004. A TAG-based
noisy channel model of speech repairs. In Proceed-
ings of ACL, pages 33?39.
167
M. Johnson, E. Charniak, and M. Lease. 2004. An im-
proved model for recognizing disfluencies in conver-
sational speech. In Proceedings of Rich Transcrip-
tion Workshop.
J. G. Kahn, M. Lease, E. Charniak, M. Johnson, and
M. Ostendorf. 2005. Effective use of prosody in
parsing conversational speech. In Proceedings of
Human Language Technology Conference and Con-
ference on Empirical Methods in Natural Language
Processing, pages 233?240.
M. Lease, E. Charniak, and M. Johnson. 2005. Pars-
ing and its applications for conversational speech. In
Proceedings of ICASSP.
W. J. M. Levelt. 1983. Monitoring and self-repair in
speech. Cognitive Science, 14:41?104.
R. J. Lickley. 1996. Juncture cues to disfluency. In
Proceedings the International Conference on Speech
and Language Processing.
Y. Liu. 2004. Structural Event Detection for Rich
Transcription of Speech. Ph.D. thesis, Purdue Uni-
versity.
M. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguis-
tics, 19(2):313?330.
M. Marcus, G. Kim, M. A. Marcinkiewicz, R. MacIn-
tyre, A. Bies, M. Ferguson, K. Katz, and B. Schas-
berger. 1994. The Penn Treebank: Annotating
Predicate Argument Structure. In Proceedings of
the 1994 ARPA Human Language Technology Work-
shop.
D. McKelvie. 1998a. SDP ? Spoken Dialog Parser.
ESRC project on Robust Parsing and Part-of-speech
Tagging of Transcribed Speech Corpora, May.
D. McKelvie. 1998b. The syntax of disfluency in spon-
taneous spoken language. ESRC project on Robust
Parsing and Part-of-speech Tagging of Transcribed
Speech Corpora, May.
C. Nakatani and J. Hirschberg. 1994. A corpus-based
study of repair cues in spontaneous speech. Journal
of the Acoustical Society of America, 95(3):1603?
1616, March.
M. Ostendorf, I. Shafran, S. Shattuck-Hufnagel,
L. Carmichael, and W. Byrne. 2001. A prosodically
labelled database of spontaneous speech. In Proc.
ISCA Tutorial and Research Workshop on Prosody
in Speech Recognition and Understanding, pages
119?121.
P. Price, M. Ostendorf, S. Shattuck-Hufnagel, and
C. Fong. 1991. The use of prosody in syntactic
disambiguation. Journal of the Acoustic Society of
America, 90:2956?2970.
A. Ratnaparkhi. 1996. A maximum entropy part-of-
speech tagger. In Proceedings of Empirical Methods
in Natural Language Processing Conference, pages
133?141.
B. Roark. 2004. Robust garden path parsing. Natural
Language Engineering, 10(1):1?24.
E. Shriberg, L. Ferrer, S. Kajarekar, A. Venkataraman,
and A. Stolcke. 2005. Modeling prosodic feature
sequences for speaker recognition. Speech Commu-
nication, 46(3-4):455?472.
E. Shriberg. 1994. Preliminaries to a Theory of Speech
Disfluencies. Ph.D. thesis, UC Berkeley.
H. F. Silverman, M. Beckman, J. Pitrelli, M. Ostendorf,
C. Wightman, P. Price, J. Pierrehumbert, and J. Hir-
shberg. 1992. ToBI: A standard for labeling English
prosody. In Proceedings of ICSLP, volume 2, pages
867?870.
K. Sjlander, 2001. The Snack sound visualization mod-
ule. Royal Institute of Technology in Stockholm.
http://www.speech.kth.se/SNACK.
K. Sonmez, E. Shriberg, L. Heck, and M. Weintraub.
1998. Modeling dynamic prosodic variation for
speaker verification. In Proceedings of ICSLP, vol-
ume 7, pages 3189?3192.
Jo?rg Spilker, Martin Klarner, and Gu?nther Go?rz. 2000.
Processing self-corrections in a speech-to-speech
system. In Wolfgang Wahlster, editor, Verbmobil:
Foundations of speech-to-speech translation, pages
131?140. Springer-Verlag, Berlin.
J. Spilker, A. Batliner, and E. No?th. 2001. How to
repair speech repairs in an end-to-end system. In
R. Lickley and L. Shriberg, editors, Proc. of ISCA
Workshop on Disfluency in Spontaneous Speech,
pages 73?76.
A. Stolcke and E. Shriberg. 1996. Statistical language
modeling for speech disfluencies. In Proceedings
of the IEEE International Conference on Acoustics,
Speech and Signal Processing, pages 405?408, At-
lanta, GA.
R. M. Weischedel and N. K. Sondheimer. 1983.
Meta-rules as a basis for processing ill-formed in-
put. American Journal of Computational Linguis-
tics, 9(3-4):161?177.
M. Wieling, M-J. Nederhof, and G. van Noord. 2005.
Parsing partially bracketed input. Talk presented at
Computational Linguistics in the Netherlands.
D. Wong, M. Ostendorf, and J. G. Kahn. 2005. Us-
ing weakly supervised learning to improve prosody
labeling. Technical Report UWEETR-2005-0003,
University of Washington Electrical Engineering
Dept.
Q. Zhang and F. Weng. 2005. Exploring features for
identifying edited regions in disfluent sentences. In
Proceedings of the Nineth International Workshop
on Parsing Technologies, pages 179?185.
168
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 390?398,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Corrective Models for Speech Recognition of Inflected Languages
Izhak Shafran and Keith Hall
Center for Language and Speech Processing
Johns Hopkins University
Baltimore, MD 21218
{zakshafran,keith hall}@jhu.edu
Abstract
This paper presents a corrective model
for speech recognition of inflected lan-
guages. The model, based on a discrim-
inative framework, incorporates word n-
grams features as well as factored mor-
phological features, providing error reduc-
tion over the model based solely on word
n-gram features. Experiments on a large
vocabulary task, namely the Czech portion
of the MALACH corpus, demonstrate per-
formance gain of about 1.1?1.5% absolute
in word error rate, wherein morphologi-
cal features contribute about a third of the
improvement. A simple feature selection
mechanism based on ?2 statistics is shown
to be effective in reducing the number of
features by about 70% without any loss in
performance, making it feasible to explore
yet larger feature spaces.
1 Introduction
N -gram models have long been the stronghold of
statistical language modeling approaches. Within
the n-gram paradigm, straightforward approaches
for increasing accuracy include using larger train-
ing sets and augmenting the contextual informa-
tion within the n-gram window. Incorporating
syntactic features into the context has been at the
forefront of recent research (Collins et al, 2005;
Rosenfeld et al, 2001; Chelba and Jelinek, 2000;
Hall and Johnson, 2004). However, much of the
previous work has focused on English language
syntax. This paper addresses syntax as captured
by the inflectional morphology of highly inflected
language.
High inflection in a language is generally cor-
related with some level of word-order flexibil-
ity. Morphological features either directly identify
or help disambiguate the syntactic participants of
a sentence. Inflectional morphology works as a
proxy for structured syntax in a language. Model-
ing morphological features in these languages not
only provides an additional source of information
but can also alleviate data sparsity problems.
Czech speech recognition needs to deal with
two sources of errors which are absent in En-
glish, namely, the inflectional morphology and the
differences in the formal (written) and colloquial
(spoken) forms. Table 1 presents an example out-
put of our speech recognizer on an utterance from
a Holocaust survivor, who is recounting General
Romel?s desert campaign during the Second World
War. In this example, the feminine past-tense
form of the Czech verb for to be is chosen mis-
takenly, which is followed by a sequence of in-
correct words chosen primarily to maintain agree-
ment with the feminine form of the verb. This is
an example of what we refer to as the morpho-
logical grouping effect. When the acoustic model
prefers a word with an incorrect inflection, the lan-
guage model effectively propagates the error to
later words. A language model based on word-
forms prefers sequences observed in the training
data, which will implicitly force an agreement
with the inflections of preceding words, making it
difficult to stop propagating errors. Although this
analysis is anecdotal in nature, the grouping effect
appears to be prevalent in the Czech dataset used
in this work. The proposed corrective model with
morphological features is expected to alleviate the
grouping effect as well as to improve the recogni-
tion of inflected languages in general.
In the following section, we present a brief
review of related work on morphological lan-
guage modeling and discriminative language mod-
390
REF no Jez???s? to uz? byl Romel hnedle pr?ed Alexandri??
gloss well Jesus by that time already was Romel just in front of Alexandria
translation oh Jesus, Romel was already just in front of Alexandria by that time
HYP no Jez???s? to uz? byla sama hned leps??? Alexandrie
gloss well Jesus by that time already (she) was herself just better Alexandria
translation oh Jesus, she was herself just better Alexandria by that time
Table 1: An example of the grouping effect. The incorrect form of the verb to be begins a group of
incorrect words in the hypothesis, but these words agree in their morphological inflection.
els. We begin the description of our work in sec-
tion 3 with the type of morphological features
modeled as well as their computation from the out-
put word-lattices of a speech recognizer. Section 4
presents the corrective model and the training ap-
proach explored in the current work. A simple and
effective feature selection mechanism is described
in section 5. In section 6, the proposed framework
is evaluated on a large vocabulary Czech speech
recognition task. Results show that the morpho-
logical features provide a significant improvement
over models lacking these features; subsequently,
two different analyses are provided to understand
the contribution of different morphological fea-
tures.
2 Related Work
It has long been assumed that incorporating mor-
phological features into a language models should
help improve the performance of speech recogni-
tion systems. Early models for German showed
little improvements over bigram language mod-
els and almost no improvement over trigram mod-
els (Geutner, 1995). More recently, morphology-
based models have been shown to help reduce er-
ror rate for out-of-vocabulary words (Carki et al,
2000; Podvesky and Machek, 2005).
Much of the early work on morphological lan-
guage modeling was focused on utilizing compos-
ite morphological tags, largely due to the difficulty
in teasing apart the intricate interdependencies of
the morphological features. Apart from a few ex-
ceptions, there has been little work done in explor-
ing the morphological systems of highly inflected
languages.
Kirchhoff and colleagues (2004) successfully
incorporated morphological features for Arabic
using a factored language model. In their ap-
proach, morphological inflections are modeled in
a generative framework, and the space of factored
morphological tags is explored using a genetic al-
gorithm.
Adopting a different tactic, Choueiter and
colleagues (2006) exploited morphological con-
straints to prune illegal morpheme sequences from
ASR output. They noticed that the gains obtained
from the application of such constraints in Arabic
depends on the size of the vocabulary ? an absolute
gain of 2.4% in word error rate (WER) reduced
to 0.2% when the size was increased from 64k to
800k.
Our approach to modeling morphology differs
from that of Vergyri et al (2004) and Choueiter et
al. (2006). By choosing a discriminative frame-
work and maximum entropy based estimation, we
allow arbitrary features or constraints and their
combinations without the need for explicit elab-
oration of the factored space and its backoff ar-
chitecture. Thus, morphological features can be
incorporated in the absence of knowledge about
their interdependencies.
Several researchers have investigated tech-
niques for improving automatic speech recogni-
tion (ASR) results by modeling the errors (Collins
et al, 2005; Shafran and Byrne, 2004). Collins
et al (2005) present a corrective language model
based on a discriminative framework. Initially, a
set of hypotheses is generated by a baseline de-
coder with standard acoustic and language models.
A corrective model is estimated such that it scores
desired or oracle hypotheses higher than compet-
ing hypotheses. The parameters are learned via
the perceptron algorithm which shifts weight away
from features associated with poor hypotheses and
towards those associated with better hypotheses.
By the appropriate choice of desired hypotheses,
the model parameters can be estimated to mini-
mize WER in speech recognition. During decod-
ing, the model can then be used to rerank a set
of hypotheses, and hence, it is also known as a
reranking framework. This paradigm allows mod-
eling arbitrary input features, even syntactic fea-
tures obtained from a parser. We adopt a vari-
ant of this framework where the corrective model
is based on a conditional model estimated by the
maximum entropy procedure (Charniak and John-
391
son, 2005) and we investigate its effectiveness in
modeling morphological features for highly in-
flected languages, in particular, Czech.
3 Inflectional Morphology
Inflectional abundance in a language generally
corresponds to some flexibility in word order. In
a free word-order language, the order of senten-
tial participants is relatively unconstrained. This
does not mean a speaker of the language can ar-
bitrarily choose an order. Word-order choice may
change the semantic and/or pragmatic interpreta-
tion of an utterance. Czech is known as a free
word-order language allowing for subject, object,
and verbal components to come in any order. Mor-
phological inflection in these languages must in-
clude a syntactic case marker to allow the determi-
nation of which participants are subjects (nomina-
tive case), objects (accusative or dative) and other
such entities. Additionally, morphological inflec-
tion encodes features such as gender and number.
The agreement of these features between senten-
tial components (adjectives with nouns, subjects
with verbs, etc.) may further disambiguate the tar-
get of a modifier (e.g., identifying the noun that is
modified by a particular adjective).
The increased flexibility in word order aggra-
vates the data sparsity of standard n-gram lan-
guage model for two reasons: first, the number of
valid configurations of a group of words increases
with the free order; and second, lexical items are
decorated with the inflectional morphemes, multi-
plying the number of word-forms that appear.
In addition to modeling sequences of word-
forms, we model sequences of morphologically
reduced lemmas, sequence of morphological tags
and sequences of various factored representations
of the morphological tags. Factoring a word
into the semantics-bearing lemma and syntax-
bearing morphological tag alleviates the data spar-
sity problem to some extent. However, the number
of possible factorizations of n-grams is large. The
approach adopted in this work is to provide a rich
class of features and defer the modeling of their
interaction to the learning procedure.
3.1 Extracting Morphological Features
The extraction of reliable morphological features
critically effects further morphological modeling.
Here, we first select the most likely morphologi-
cal analysis for each word using a morphological
Label Description # Values
lemma Reduced lexeme < |vocab|
POS Coarse part-of-speech 12
D-POS Detailed part-of-speech 65
gen Grammatical Gender 10
num Grammatical Number 5
case Grammatical Case 8
Table 2: Czech morphological features used in the
current work. The # Values field indicates the size
of the closed set of possible values. Not all values
are used in the annotated data.
tagger. In particular, we use the Czech feature-
based tagger distributed with the Prague Depen-
dency Treebank (Hajic? et al, 2005). The tagger is
based on a morphological analyzer which uses a
lexicon and a rule-based tag guesser for words not
found in the lexicon. Trained by the maximum en-
tropy procedure, the tagger uses left and right con-
textual features from the input string. Currently,
this is the best available Czech-language tagger.
See Hajic? and Vidova?-Hladka? (1998) for further
details on the tagger.
A disadvantage of such an approach is that
the tagger works on strings rather than the word-
lattices that we expect from an ASR system.
Therefore, we must extract a set of strings from the
lattices prior to tagging. An alternative approach is
to hypothesize all morphological analyses for each
word in the lattice, thereby considering the entire
set of analyses as features in the model. In the cur-
rent implementation we have chosen to use a tag-
ger to reduce the complexity of the model by lim-
iting the number of active features while still ob-
taining relatively reliable features. Moreover, sys-
tematic errors in tagging can be potentially com-
pensated by the corrective model.
The initial stage of feature extraction begins
with an analysis of the data on which we train and
test our models. The process follows:
1. Extract the n-best hypotheses according to a
baseline model, where n varies from 50 to
1000 in the current work.
2. Tag each of the hypotheses with the morpho-
logical tagger.
3. Re-encode the original word strings along
with their tagged morphological analysis in
a weighted finite state transducer to allow
392
Word-form to obdob?? bylo pome?rne? kra?tke?
gloss that period was relatively short
lemma ten obdob?? by?t pome?rne? kra?tky?
tag PDNS1 NNNS1 VpNS- Dg? AAFS2
Table 3: A morphological analysis of Czech. This analyses was generated by the Hajic? tagger.
form to obdob?? bylo pome?rne? kra?tke?
to obdob?? obdob?? bylo bylo pome?rne? pome?rne? kra?tke?
lemma ten obdob?? by?t pome?rne? kra?tky?
ten obdob?? obdob?? by?t by?t pome?rne? pome?rne? kra?tky?
tag PDNS1 NNNS1 VpNS- Dg? AAFS2
PDNS1 NNNS1 NNNS1 VpNS- VpNS- Dg? Dg? AAFS2
POS P N V D A
P N N V V D D A
. . . . . .
case 1 1 - - 2
1 1 1 - - 0 - 2
num/case S1 S1 S- ? S2
S1 S1 S1 S- S- ? ? S2
. . . . . .
Table 4: Examples of the n-grams extracted from the Czech sentence To obdob?? bylo pome?rne? kra?tke?. A
subset of the feature classes is presented here. The morphological feature values are those assigned by
the Hajic? tagger.
an efficient means of projecting the hypothe-
ses from word-form to morphology and vice
versa.
4. Extract appropriately factored n-gram fea-
tures for each hypothesis as described below.
Each word state in the original lattice has an
associated lemma/tag from which a variety of n-
gram features can be extracted.
From the morphological features assigned by
the tagger, we chose to retain only a subset and dis-
card the less reliable features which are semantic
in nature. The basic morphological features used
are detailed in Table 2. In the tag-based model, a
string of 5 characters representing the 5 morpho-
logical fields is used as a unique identifier. The
derived features include n-grams of POS, D-POS,
gender (gen), number (num), and case features as
well as their combinations.
POS, D-POS Captures the sub-categorization of
the part-of-speech tags.
gen, num Captures complex gender-number
agreement features.
num, case Captures number agreement between
specific case markers.
POS, case Captures associated POS/Case fea-
tures (e.g., adjectives associated with nomi-
native elements).
The paired features allow for complex inflec-
tional interactions and are less sparse than the
composite 5-component morphological tags. Ad-
ditionally, the morphologically reduced lemma
and n-grams of lemmas are used as features in the
models.
Table 3 presents a morphological analysis of the
Czech sentence To obdob?? bylo pome?rne? kra?tke?.
The encoded tags represent the first 5 fields of the
Prague Dependency Treebank morphological en-
coding and correspond to the last 5 rows of Ta-
ble 2. Features for this sentence include the word-
form, lemma, and composite tag features as well
as the components of each tag and the above men-
tioned concatenation of tag fields. Additionally,
n-grams of each of these features are included. Bi-
gram features extracted from an example sentence
are illustrated in Table 4.
The following section describes how the fea-
393
tures extracted above are modeled in a discrimi-
native framework to reduce word error rate.
4 Corrective Model and Estimation
In this work, we adopt the reranking framework
of Charniak and Johnson (2005) for incorporating
morphological features. The model scores each
test hypothesis y using a linear function, v?(y), of
features extracted from the hypothesis fj(y) and
model parameters ?j , i.e., v?(y) =
?
j ?jfj(y).
The hypothesis with the highest score is then cho-
sen as the output.
The model parameters, ?, are learned from a
training set by maximum entropy estimation of the
following conditional model:
?
s
?
yi?Ys:g(yi)=maxjg(yj)
P?(yi|Ys)
Here, Ys = {yj} is the set of hypotheses for each
training utterance s and the function g returns an
extrinsic evaluation score, which in our case is
the WER of the hypothesis. P?(yi|Ys) is modeled
by a maximum entropy distribution of the form,
P?(yi|Ys) = exp v?(yi)/
?
j exp v?(yj). This
choice simplifies the numerical estimation proce-
dure since the gradient of the log-likelihood with
respect to a parameter, say ?j , reduces to differ-
ence in expected counts of the associated feature,
E?[fj |Ys]?E?[fj |yi ? Ys : g(yi) = maxjg(yj)].
To allow good generalization properties, a Gaus-
sian regularization term is also included in the cost
function.
A set of hypotheses Ys is generated for each
training utterance using a baseline ASR system.
Care is taken to reduce the bias in decoding the
training set by following a jack-knife procedure.
The training set is divided into 20 subsets and each
subset is decoded after excluding the transcripts
of that subset from the language model of the de-
coder.
The model allows the exploration of a large fea-
ture space, including n-grams of words, morpho-
logical tags, and factored tags. In a large vocab-
ulary system, this could be an enormous space.
However, in a discriminative maximum entropy
framework, only the observed features are consid-
ered. Among the observed features, those associ-
ated with words that are correct in all hypotheses
do not provide any additional discrimination ca-
pability. Mathematically, the gradient of the log-
likelihood with respect to the parameters of these
features tends to zero and they may be discarded.
Additionally, the parameters associated with fea-
tures that are rarely observed in the training set are
difficult to learn reliably and may be discarded.
To avoid redundant features, we focus on words
which are frequently incorrect; this is the error re-
gion we aim to model. In the training utterance,
the error regions of a hypothesis are identified us-
ing the alignment corresponding to the minimum
edit distance from the reference, akin to comput-
ing word error rate. To mark all the error regions in
an ASR lattice, the minimum edit distance align-
ment is obtained using equivalent finite state ma-
chine operations (Mohri, 2002). From amongst all
the error regions in the training lattices, the most
frequent 12k words in error are shortlisted. Fea-
tures are computed in the corrective model only if
they involve words for the shortlist. The parame-
ters, ?, are estimated by numerical optimization as
in (Charniak and Johnson, 2005).
5 Feature Selection
The space of features spanned by the cross-
product space of words, lemmas, tags, factored-
tags and their n-gram can potentially be over-
whelming. However, not all of these features
are equally important and many of the features
may not have a significant impact on the word
error rate. The maximum entropy framework af-
fords the luxury of discarding such irrelevant fea-
tures without much bookkeeping, unlike maxi-
mum likelihood models. In the context of mod-
eling morphological features, we investigate the
efficacy of simple feature selection based on the
?2 statistics, which has been shown to effective
in certain text categorization problems. e.g. (Yang
and Pedersen, 1997).
The ?2 statistics measures the lack of indepen-
dence by computing the deviation of the observed
counts Oi from the expected counts Ei.
?2 =
?
i
(Oi ? Ei)
2/Ei
In our case, there are two classes ? oracle hy-
potheses c and competing hypotheses c?. The
expected count is the count marginalized over
classes.
?2(f, c) =
(P (f, c)? P (f))2
P (f)
+
(P (f, c?)? P (f))2
P (f)
+
(P (f? , c)? P (f?))2
P (f?)
+
(P (f? , c?)? P (f?))2
P (f?)
394
This can be simplified using a two-way contin-
gency table of feature and class, where A is the
number of times f and c co-occur, B is the num-
ber of times f occurs without c, C is the number
of times c occurs without f , and D is the number
of times neither f nor c occurs, and N is the total
number of examples. Then, the ?2 is defined to
be:
?2(f, c) =
N ? (AD ? CB)2
(A+ C)? (B +D)? (A+B)? (C +D)
The ?2 statistics are computed for all the fea-
tures and the features with larger value are re-
tained. Alternatives feature selection mechanisms
such as those based on mutual information and in-
formation gain are less reliable than ?2 statistics
for heavy-tailed distributions. More complex fea-
ture selection mechanism would entail computing
higher order interaction between features which is
computationally expensive and so is not explored
in this work.
6 Empirical Evaluation
The corrective model presented in this work is
evaluated on a large vocabulary task consisting
of spontaneous spoken testimonies in Czech lan-
guage, which is a subset of the multilingual
MALACH corpus (Psutka et al, 2003).
6.1 Task
For acoustic model training, transcripts are avail-
able for about 62 hours of speech from 336 speak-
ers, amounting to 507k spoken words from a vo-
cabulary of 79k. A portion of this data containing
speech from 44 speakers, about 21k words in all
is treated as development set (dev). The test set
(eval) consists of about 2 hours of speech from 10
new speakers and contains about 15k words.
6.2 Baseline ASR System
The baseline ASR system uses perceptual linear
prediction (PLP) features which is computed on
44KHz input speech at the rate of 10 frames per
second, and is normalized to have zero mean and
unit variance per speaker. The acoustic models are
made of 3-state HMM triphones, whose observa-
tion distributions are clustered into about 4500 al-
lophonic (triphone) states. Each state is modeled
by a 16 component Gaussian mixture with diag-
onal covariances. The parameters of the acoustic
models are initially estimated by maximum likeli-
hood and then refined by five iterations of maxi-
mum mutual information estimation (MMI).
Unlike other comparable corpora, this corpus
contains a relatively high percentage of colloquial
words ? about 9% of the vocabulary and 7% of the
tokens. For the sake of downstream application,
the colloquial variants are subsumed in the lexi-
con. As a result, common words contain several
pronunciation variants, and a few have as many as
14 variants.
For the first pass decoding, a language model
was created by interpolating the in-domain model
(weight=0.75), estimated from 600k words of
transcripts with an out-of-domain model, esti-
mated from 15M words of Czech National Cor-
pus (Psutka et al, 2003). Both models are param-
eterized by a trigram language model with Katz
back-off. The decoding graph was built by com-
posing the language model, the lexical transducer
and the context-dependent transducer (phones to
triphones) into a single compact finite state ma-
chine.
The baseline ASR system decodes test utter-
ance in two passes. A first pass decoding is per-
formed with MMIE acoustic models, whose out-
put transcripts are bootstrapped to estimate two
maximum likelihood linear regression transforms
for each speaker using five iterations. A second
pass decoding is then performed with the new
speaker adapted acoustic models. The resulting
performance is given in Table 5. The performance
reflects the difficulty of transcribing spontaneous
speech from the elderly speakers whose speech is
also heavily accented and emotional in this corpus.
1-best 1000-best
Dev 29.9 21.5
Eval 35.9 22.4
Table 5: The performance of the baseline ASR
system is reported, showing the word error rate
of 1-best MAP hypothesis and the oracle in 1000-
best hypotheses for dev and eval sets.
6.3 Experiments With Morphology
We present a set of contrastive experiments to
gauge the performance of the corrective models
and the contribution of morphological features.
For training the corrective models, 50 best hy-
potheses are generated for each utterance using the
395
 
28.6
 
28.8 29
 
29.2
 
29.4
 
29.6
 
29.8 30  
0
 
0.2
 
0.4
 
0.6
 
0.8
 
1
WER
Fract
ion of
 featu
res us
ed
?base
line?
word 
n-gram
+ m
orph n
-gram
 
34.2
 
34.4
 
34.6
 
34.8 35
 
35.2
 
35.4
 
35.6
 
35.8 36  
0
 
0.2
 
0.4
 
0.6
 
0.8
 
1
WER
Fract
ion of
 featu
res us
ed
?base
line?
word 
n-gram
+ m
orph n
-gram
(a)Devel (b)Eval
Figure 1: Feature selection via ?2 statistics helps reduce the number of parameters by 70% without any
loss in performance, as observed in dev (a) and eval (b) sets.
jack-knife procedure mentioned earlier. For each
hypothesis, bigram and unigram features are com-
puted which consist of word-forms, lemmas, mor-
phologoical tags, factored morphological tags, and
the likelihood from the baseline ASR system. For
testing, the baseline ASR system is used to gener-
ate 1000 best hypotheses for each utterance. These
are then evaluated using the corrective models and
the best scored hypothesis is chosen as the output.
Table 6 summarizes the results on two test sets
? the dev and the eval set. A corrective model with
word bigram features improve the word error rate
by about an absolute 1% over the baseline. Mor-
phological features provide a further gain on both
the test sets consistently.
Features Dev Eval
Baseline 29.9 35.9
Word bigram 29.0 34.8
+ Morph bigram 28.7 34.4
Table 6: The word error rate of the corrective
model is compared with that of the baseline ASR
system, illustrating the improvement in perfor-
mance with morphological features.
The gains on the dev set are significant at the
level of p < 0.001 for three standard NIST tests,
namely, matched pair sentence segment, signed
pair comparison, and Wilcoxon signed rank tests.
For the smaller eval set the significant levels were
lower for morphological features. The relative
gains observed are consistent over a variety of con-
ditions that we have tested including the ones re-
ported below.
Subsequently, we investigated the impact of re-
ducing the number of features using ?2 statistics,
as described in section 5. The experiments with
bigram features of word-forms and morphology
were repeated using reduced feature sets, and the
performance was measured at 10%, 30% and 60%
of their original features. The results, as illustrated
in Figure 1, show that the word error rate does not
change significantly even after the number of fea-
tures are reduced by 70%. We have also observed
that most of the gain can be achieved by evalu-
ating 200 best hypotheses from the baseline ASR
system, which could further reduce the computa-
tional cost for time-sensitive applications.
6.4 Analysis of Feature Classes
The impact of feature classes can be analyzed by
excluding all features from a particular class and
evaluating the performance of the resulting model
without re-estimation. Figure 2 illustrates the ef-
fectiveness of different features class. The y-axis
shows the gain in F-score, which is monotonic
with the word error rate, on the entire develop-
ment dataset. In this analysis, the likelihood score
from the baseline ASR system was omitted since
our interest is in understanding the effectiveness
of categorical features such as words, lemmas and
tags.
The most independently influential feature class
is the factored tag features. This corresponds with
396
-0.00100.001
0.0020.0030.004
0.005
TNG#1 TNG#2 LNG#2 FNG#2 TFAC#1 LNG#1 FNG#1 TFAC#2
Figure 2: Analysis of features classes for a bigram
form, lemma, tag, and factored tag model. Y -axis
is the contribution of this feature if added to an
otherwise complete model. Feature classes are la-
beled: TNG ? tag n-gram, LNG ? lemma n-gram,
FNG ? form n-gram and TFAC ? factored tag n-
grams. The number following the # represents the
order of the n-gram.
our belief that modeling morphological features
requires detailed models of the morphology; in
this model the composite morphological tag n-
gram features (TNG) offer little contribution in the
presence of the factored features.
Analysis of feature reduction by the ?2 statistics
reveals a similar story. When features are ranked
according to their ?2 statistics, about 57% of the
factored tag n-grams occur in the top 10% while
only 7% of the word n-grams make it. The lemma
and composite tag n-grams give about 6.2% and
19.2% respectively. Once again, the factored tag
is the most influential feature class.
7 Conclusion
We have proposed a corrective modeling frame-
work for incorporating inflectional morphology
into a discriminative language model. Empirical
results on a difficult Czech speech recognition task
support our claim that morphology can help im-
prove speech recognition results for these types of
languages. Additionally, we present a feature se-
lection method that effectively reduces the model
size by about 70% while having little or no im-
pact on recognition accuracy. Model size reduc-
tion greatly reduces training time which can often
be prohibitively expensive for maximum entropy
training.
Analysis of the models learned on our task show
that factored morphological tags along with word-
forms provide most of the discriminative power;
and, in the presence of these features, composite
morphological tags are of little use.
The corrective model outlined here operates on
the word lattices produced by an ASR system. The
morphological tags are inferred from the word se-
quences in the lattice. Alternatively, by employ-
ing an ASR system that models the morphological
constraints in the acoustics as in (Chung and Sen-
eff, 1999), the corrective model could be applied
directly to a lattice with morphological tags.
When dealing with ASR word lattices, the ef-
ficacy of the proposed feature selection mecha-
nism can be exploited to eliminate the intermedi-
ate tagger, a potential source of errors. Instead of
considering the best morphological analysis, the
model could consider all possible analyses of the
words. Further, the feature space could be en-
riched with syntactic features which are known to
be useful (Collins et al, 2005). The task of mod-
eling is then tackled by feature selection and the
maximum entropy training procedure.
8 Acknowledgements
The authors would like to thank William Byrne for
discussions on modeling aspects, and Jan Hajic?,
Petr Ne?mec, and Vaclav Nova?k for discussions
regarding Czech morphology and tagging. This
work was supported by the NSF (U.S.A) under the
Information Technology Research (ITR) program,
NSF IIS Award No. 0122466.
References
Kenan Carki, Petra Geutner, and Tanja Schultz. 2000.
Turkish LVCSR: towards better speech recognition
for agglutinative languages. In Proceedings of the
2000 IEEE International Conference on Acoustics,
Speech, and Signal Processing, pages 3688?3691.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and MaxEnt discriminative
reranking. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguis-
tics.
Ciprian Chelba and Frederick Jelinek. 2000. Struc-
tured language modeling. Computer Speech and
Language, 14(4):283?332.
Ghinwa Choueiter, Daniel Povey, Stanley Chen, and
Geoffrey Zweig. 2006. Morpheme-based language
modeling for Arabic LVCSR. In Proceedings of the
2006 IEEE International Conference on Acoustics,
Speech, and Signal Processing, Toulouse, France.
Grace Chung and Stephanie Seneff. 1999. A hierar-
chical duration model for speech recognition based
397
on the ANGIE framework. Speech Communication,
27:113?134.
Michael Collins, Brian Roark, and Murat Saraclar.
2005. Discriminative syntactic language modeling
for speech recognition. In Proceedings of the 43rd
Annual Meeting of the Association for Computa-
tional Linguistics (ACL?05), pages 507?514, Ann
Arbor, Michigan, June. Association for Computa-
tional Linguistics.
Petra Geutner. 1995. Using morphology towards bet-
ter large-vocabulary speech recognition systems. In
Proceedings of the 1995 IEEE International Confer-
ence on Acoustics, Speech, and Signal Processing,
pages 445?448, Detroit, MI.
Jan Hajic? and Barbora Vidova?-Hladka?. 1998. Tagging
inflective languages: Prediction of morphological
categories for a rich, structured tagset. In Proceed-
ings of the COLING-ACL Conference, pages 483?
490, Montreal, Canada.
Jan Hajic?, Eva Hajic?ova?, Petr Pajas, Jarmila
Panevova?, Petr Sgall, and Barbora Vidova? Hladka?.
2005. The prague dependency treebank 2.0.
http://ufal.mff.cuni.cz/pdt2.0.
Keith Hall and Mark Johnson. 2004. Attention shifting
for parsing speech. In Proceedings of the 42nd An-
nual Meeting of the Association for Computational
Linguistics, pages 41?47, Barcelona.
Mehryar Mohri. 2002. Edit-distance of weighted
automata. In Proceedings of the 7th Interna-
tional Conference on Implementation and Applica-
tion of Automata, Jean-Marc Champarnaud and De-
nis Maurel, Eds.
Petr Podvesky and Pavel Machek. 2005. Speech
recognition of Czech?inclusion of rare words
helps. In Proceedings of the ACL Student Research
Workshop, pages 121?126, Ann Arbor, Michigan,
June. Association for Computational Linguistics.
Josef Psutka, Pavel Ircing, Josef V. Psutka, Vlasta
Radovic, William Byrne, Jan Hajic?, Jiri Mirovsky,
and Samuel Gustman. 2003. Large vocabulary ASR
for spontaneous Czech in the MALACH project.
In Proceedings of the 8th European Conference on
Speech Communication and Technology, Geneva,
Switzerland.
Roni Rosenfeld, Stanley F. Chen, and Xiaojin Zhu.
2001. Whole-sentence exponential language mod-
els: a vehicle for linguistic-statistical integration.
Computers Speech and Language, 15(1).
Izhak Shafran and William Byrne. 2004. Task-specific
minimum Bayes-risk decoding using learned edit
distance. In Proceedings of the 7th International
Conference on Spoken Language Processing, vol-
ume 3, pages 1945?48, Jeju Islands, Korea.
Dimitra Vergyri, Katrin Kirchhoff, Kevin Duh, and An-
dreas Stolcke. 2004. Morphology-based language
modeling for arabic speech recognition. In Proceed-
ings of the International Conference on Spoken Lan-
guage Processing (ICSLP/Interspeech 2004).
Yiming Yang and Jan 0. Pedersen. 1997. A compara-
tive study on feature selection in text categorization.
In Proceedings of the 14th International Conference
on Machine Learning, pages 412 ? 420, San Fran-
cisco, CA, USA.
398
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 112?119,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Hello, Who is Calling?: Can Words Reveal the Social Nature of
Conversations?
Anthony Stark, Izhak Shafran and Jeffrey Kaye
Center for Spoken Language Understanding, OHSU, Portland USA.
{starkan,shafrani,kaye}@ohsu.edu
Abstract
This study aims to infer the social nature
of conversations from their content automat-
ically. To place this work in context, our moti-
vation stems from the need to understand how
social disengagement affects cognitive decline
or depression among older adults. For this
purpose, we collected a comprehensive and
naturalistic corpus comprising of all the in-
coming and outgoing telephone calls from 10
subjects over the duration of a year. As a
first step, we learned a binary classifier to fil-
ter out business related conversation, achiev-
ing an accuracy of about 85%. This clas-
sification task provides a convenient tool to
probe the nature of telephone conversations.
We evaluated the utility of openings and clos-
ing in differentiating personal calls, and find
that empirical results on a large corpus do
not support the hypotheses by Schegloff and
Sacks that personal conversations are marked
by unique closing structures. For classifying
different types of social relationships such as
family vs other, we investigated features re-
lated to language use (entropy), hand-crafted
dictionary (LIWC) and topics learned using
unsupervised latent Dirichlet models (LDA).
Our results show that the posteriors over top-
ics from LDA provide consistently higher ac-
curacy (60-81%) compared to LIWC or lan-
guage use features in distinguishing different
types of conversations.
1 Introduction
In recent years, there has been a growing interest
in analyzing text in informal interactions such as
in Internet chat, newsgroups and twitter. The em-
phasis of most such research has been in estimating
network structure (Kwak et al, 2010) and detecting
trending topics (Ritter et al, 2010), sentiments (Pak
and Paroubek, 2010) and first stories (Petrovic? et al,
2010). The focus has been on aggregating informa-
tion from large number of users to analyze popula-
tion level statistics.
The study reported in this paper, in contrast, fo-
cuses on understanding the social interactions of an
individual over long periods of time. Our motiva-
tion stems from the need to understand the factors of
social engagement that ameliorate the rate of cogni-
tive decline and depression in older adults. Since the
early work of Glass (1997) and colleagues, several
studies on large cohorts over extended duration have
confirmed that older adults with few social relation-
ships are at an increased risk of suffering depression
and dementia. The limited information available in
currently used coarse measures, often based on self-
reports, have hindered epidemiologists from probing
the nature of this association further.
While social engagement is typically multi-
faceted, older adults, who are often less mobile, rely
on telephone conversations to maintain their social
relationships. This is reflected in a recent survey
by Pew Research Center which reported that among
adults 65 years and older, nine in ten talk with family
or friends every day and more than 95% use land-
line telephones for all or most of their calls (Tay-
lor et al, June 29 2009). Conveniently for us, tele-
phone conversations present several advantages for
analysis. Unlike many other forms of communica-
tion, the interaction is restricted solely to an audio
112
channel, without recourse to gestures or facial ex-
pressions. While we do not discount the importance
of multi-modal communication, having a communi-
cation channel restricted to a unimodal format does
significantly simplify both collection and analysis.
Furthermore, the use of a handset affords the oppor-
tunity to capture naturalistic speech samples at rela-
tively high signal-to-noise ratio. Lastly, automatic
speech recognition (ASR) systems can now tran-
scribe telephone conversations with sufficient accu-
racy for useful automated analysis.
Given the above premise, we focus our attention
on studying social interactions of older adults over
land-line telephones. To facilitate such a study, we
collected telephone conversations from several older
adults for approximately one year. Note that our
corpus is unlike the publicly available Switchboard
and Fisher corpora, which contain conversations be-
tween unfamiliar speakers discussing a topic from a
pre-determined list such as music, crime, air pollu-
tion (Godfrey et al, 1992). In contrast, the conversa-
tions in our corpus are completely natural, covering
a wide range of topics, conversational partners and
types of interactions. Our corpus is also compre-
hensive in that it includes all the outgoing/incoming
calls from subjects? homes during the observation
period.
As a step toward understanding social networks
and associated relationships, our first task was to
classify social and non-social (business) conversa-
tions. While reverse listing was useful to a certain
extent, we were unable to find listing on up to 50%
of the calls in our corpus due to lack of caller ID in-
formation on many calls as well as unlisted numbers.
Moreover, we cannot preclude the possibility that a
social conversation may occur on a business num-
ber (e.g., a friend or a relative working in a business
establishment) and vice versa. Using the subset of
calls for which we have reliable listing, we learned
a supervised classifier and then employed the classi-
fier to label the remaining calls for further analysis.
The focus of this study was not so much on learn-
ing a binary classifier, but using the resulting classi-
fier as a tool to probe the nature of telephone con-
versations as well as to test whether the scores ob-
tained from it can serve as a proxy for degree of
social familiarity. The classifier also affords us an
opportunity to re-examine hypotheses proposed by
Schegloff and Sacks (1974; 1968; 1973) about the
structure of openings and closing in business and
personal conversations. Within social conversation,
we investigated the accuracy of identifying conver-
sations with close friends and relatives from others.
The rest of this paper is arranged as follows. After
describing the corpus and ASR system in Sections 2
and 3, we probe the nature of telephone conversa-
tions in Section 4. We present direct binary classifi-
cation experiments in Section 5 and lastly, we close
with a few remarks in Section 6.
2 Corpus: Everyday Telephone
Conversations Spanning a Year
Our corpus consists of 12,067 digitized land-line
telephone conversations. Recordings were taken
from 10 volunteers, 79 years or older, over a pe-
riod of approximately 12 months. Subjects were all
native English speakers recruited from the USA. In
addition to the conversations, our corpus includes
a rich set of meta-data, such as call direction (in-
coming vs outgoing), time of call, duration and
DTMF/caller ID when available. At the end of the
data collection, for each subject, twenty telephone
numbers were identified corresponding to top ten
most frequent calls and top ten longest calls. Sub-
jects were asked to identify their relationship with
the speakers at these numbers as immediate family,
near relatives, close friends, casual friends, strangers
and business.
For this initial study, we discard conversations
with less than 30 automatically transcribed words.
This was done primarily to get rid of spurious and/or
noisy recordings related to device failure as well
as incorrectly dialed telephone numbers. Moreover,
short conversations are less likely to provide enough
social context to be useful.
Of the 8,558 available conversations, 2,728 were
identified as residential conversations and 1,095
were identified as business conversations using re-
verse listings from multiple sources; e.g. phone
directory lookup, exit interviews, internet lookup.
This left 4,395 unlabeled records, for which the re-
verse listing was either inconclusive or for which the
phone number information was missing and/or im-
properly recorded.
113
3 Automatic Speech Recognition
Conversations in our corpus were automatically
transcribed using an ASR system. Our ASR sys-
tem is structured after IBM?s conversation telephony
system which gave the top performance in the most
recent evaluation of speech recognition technology
for telephony by National Institute of Standards and
Technology (Soltau et al, 2005). The acoustic mod-
els were trained on about 2000 hours of telephone
speech from Switchboard and Fisher corpora (God-
frey et al, 1992). The system has a vocabulary of
47K and uses a trigram language model with about
10M n-grams, estimated from a mix of transcripts
and web-harvested data. Decoding is performed
in three stages using speaker-independent models,
vocal-tract normalized models and speaker-adapted
models. The three sets of models are similar in
complexity with 4000 clustered pentaphone states
and 150K Gaussians with diagonal covariances. Our
system does not include discriminative training and
performs at a word error rate of about 24% on NIST
RT Dev04 which is comparable to state of the art
performance for such systems. The privacy require-
ments in place for our corpus prohibit human lis-
tening ? precluding the transcriptions needed report-
ing recognition accuracy. However, while our cor-
pus differs from Switchboard, we expect the perfor-
mance of the 2000 hour recognizer to be relatively
close to results on NIST benchmark.
4 Nature of Telephone Conversations
4.1 Classification Experiments
As mentioned earlier, we first learned a baseline bi-
nary classifier to filter out business calls from res-
idential calls. Apart from using this as a tool to
probe the characteristics of social calls, it also helps
us to classify unlabeled calls and thus avoid discard
half the corpus from subsequent analysis of social
network and relationships. Recall, the labels for
the calls were obtained using reverse lookup from
multiple sources. We assume that the majority of
our training set reflect the true nature of the con-
versations and expect to employ the classifier sub-
sequently for correcting the errors arising when per-
sonal conversations occur on business lines and vice
versa.
We learned a baseline SVM classifier using a bal-
anced training set. From the labeled records we cre-
ated a balanced verification set containing 164,115
words over 328 conversations. The remainder was
used to create a balanced training set consisting of
866,696 words over 1,862 conversations. The SVM
was trained on 20-fold cross validation and evalu-
ated on the verification set. After experimenting
with different kernels, we found an RBF kernel to
be most effective, achieving an accuracy of 87.50%
on the verification data.
4.2 Can the Scores of the Binary Classifier
Differentiate Types of Social Relationship?
Since the SVM score has utility in measuring a con-
versation on the social-business axis, we now exam-
ine its usefulness in differentiating social ties. To
test this, we computed SVM score statistics for all
conversations with family and friends. For compar-
ison, we also computed the statistics for all conver-
sations automatically tagged as residential as well as
all conversations in the data. Table 1 shows the av-
erage family score is unambiguously higher than the
average residential conversation (independent sam-
ple t-test, p < 0.001). This is an interesting re-
sult since distinction of family conversations (from
general social calls) never factored into the SVM.
Rather, it appears to arise naturally as an extrap-
olation from the more general residential/business
discriminator. The friend sub-population exhibited
statistics much closer to the general residential pop-
ulation and its differences were not significant to any
degree. The overlap between scores for conversa-
tions with family and friends overlap significantly.
Notably, the conversations with family have a sig-
nificantly higher mean and a tighter variance than
with other social ties.
Table 1: SVM scores for phone number sub-categories.
Category # Calls Mean score STD
Family 1162 1.12 0.50
Friends 532 0.95 0.51
Residential 2728 0.93 0.63
Business 1095 -1.16 0.70
Global 8558 0.46 0.96
114
4.3 How Informative are Openings and
Closings in Differentiating Telephone
Conversations?
Schegloff and Sacks assert openings (beginnings)
and closings (ends) of telephone conversations have
certain identifiable structures (Sacks et al, 1974).
For example, the structure of openings facilitate es-
tablishing identity of the conversants and the pur-
pose of their call (Schegloff, 1968). Closings in
personal conversations are likely to include a pre-
closing signal that allows either party to mention
any unmentioned mentionables before conversation
ends (Schegloff and Sacks, 1973).
Given the above assertions, we expect openings
and closings to be informative about the type of con-
versations. Using our classifier, we compare the ac-
curacy of predicting the type from openings, clos-
ings and random segments of the conversations. For
different lengths of the three types of segments, the
observed performance of the classifier is plotted in
Figure 1. The results for the random segment were
computed by averaging over 100 trials. Several im-
portant results are immediately apparent. Openings
possess much higher utility than closings. This is
consistent with general intuition that the opening ex-
change is expected to clarify the nature and topic
of the call. Closings were found to be only as in-
formative as random segments from the conversa-
tions. This is contrary to what one might expect
from Schegloff and Sack?s assertion that pre-closing
differ significantly in personal telephone calls (Sche-
gloff and Sacks, 1973). Less intuitive is the fact that
increasing the length of the opening segment does
not improve performance. Surprisingly, a 30-word
segment from the opening appears to be sufficient to
achieve high classification accuracy (87.20%).
4.4 Data Sparsity or Inherent Ambiguity: Why
are Short Conversations difficult to
Classify?
Sparsity often has a deleterious effect on classifica-
tion performance. In our experiments, we noticed
that shorter conversations suffer from poor classifi-
cation. However, the results from the above section
appear to contradict this assertion, as a 30-word win-
dow can give very good performance. This seems to
suggest short conversations suffer poor recognition
30 50 100 250 500 10000
5
10
15
20
25
30
Number of words sampled
Re
s/bi
z cl
ass
ifica
tion
 err
ror 
(%)
 
 Word sample from startWord sample from endWord sample randomly taken
Figure 1: Comparison of classification accuracy in pre-
dicting the type of conversation from openings, closings
and random segments. Error bars are one standard devia-
tion.
due to properties beyond the obvious sparsity effect.
To test this, we investigated the differences in short
and long conversations in greater detail. We sepa-
rate calls into quintile groups based on word counts.
However, we now calculate all features from a 30-
word opening ? eliminating effects directly related
to size. The results in Table 2 show that the abil-
Table 2: Accuracy in predicting the type of conversation
when they are truncated to 30-words of openings based
on conversation length quintiles. The column, Res / Biz,
split gives the label distributions for the quintiles.
Orig. Word Counts Split Accuracy
Quintile #Words Res. / Biz.
0-20 30-87 62.12 / 37.88 78.6
20-40 88-167 48.48 / 51.52 82.8
40-60 168-295 39.39 / 60.61 91.4
60-80 296-740 40.91 / 59.09 87.8
80-100 741+ 59.38 / 40.62 93.4
ity to predict the type of conversation does not de-
grade when long conversations are truncated. Mean-
while, the accuracy of classification drops for (orig-
inally) short conversations. There is a surprisingly
small performance loss due to the artificial trunca-
tion. These observations suggest that the long and
short conversations are inherently different in na-
ture, at least in their openings.
We should point out that spurious recordings
in our corpus are concentrated in the low word
count group ? undoubtedly dropping their accura-
cies. However, the trend of improving accuracy per-
sists well into the high word count ranges where spu-
115
rious records are rare. Given this fact, it appears that
individuals in our corpus are more careful in enun-
ciating the reasons for calling if an extended phone
conversation is anticipated.
4.5 Can Openings Help Predict Relative
Lengths of Conversations?
From the results presented so far, we know that
openings are good predictors of the type of conver-
sations yet to unfold. We also know that there are in-
herent language differences between short and long
conversations. So, it is natural to ask whether open-
ings can predict relative lengths of conversations.
To test this hypothesis, we bin conversations into
5 groups or ranks based on their percentile lengths
(word counts) ? very short, short, moderate, long
and very long durations, as in Table 2. Using in-
dependent features from the 30-word opening, we
attempt to predict the relative rank of two conver-
sations by learning a rank SVM (Joachims, 2006).
We found the ranker to give 27% error rate, signifi-
cantly lower (independent sample t-test, d.f. ? 1M,
p<0.01) than the random chance of 40%. Chance
baseline was determined using Monte Carlo simula-
tion (1M random rankings) in conjunction with the
rank SVM evaluation (Joachims, 2006).
Features from very short conversations may con-
tain both openings and closings, i.e., both a hello and
a goodbye, making them easier to rank. To avoid this
confounding factor, we also compute performance
after discarding the shortest grouping of conversa-
tions (< 88 words) to ensure closings are avoided in
the 30-word window. The resulting classifier over
short, medium, long, very long conversations ranked
30% of the pairs erroneously, somewhat better than
chance at 37%. Though the performance gain over
the random ranker has shrunk considerably, there is
still some utility in using the opening of a conversa-
tion to determine its ultimate duration. However, it
is clear predicting duration via conversation opening
is a much more difficult task overall.
5 Supervised Classification of Types of
Social Relationships
While the scores of the binary classifier provided
statistically significant differences between calls to
different types of social relationships, they are not
particularly useful in classifying the calls with high
accuracy. In this section, we investigate the perfor-
mance of classifiers to differentiate the following bi-
nary classes.
? Residential vs business
? Family vs all other
? Family vs other residential
? Familiar vs non-familiar
Familiar denotes calls to those numbers with whom
subject has conversed more than 5 times. Recall
that the numbers corresponding to family members
were identified by the subjects in a post-collection
interview. We learned binary classifier for the four
cases, a few of which were reported in our early
work (Stark et al, 2011). We investigated a vari-
ety of features in these tasks. A breakdown of the
corpus is give in Table 3. Not all categories are mu-
tually exclusive. For example the majority of family
conversations also fall into the familiar and residen-
tial categories.
Table 3: Number of conversations per category.
Category Instances
Biz. 1095
Residential 2728
Family 1111
Res. non-family 1462
Familiar 3010
All 8558
5.1 Lexical Statistics
Speakers who share close social ties are likely to
engage in conversations on a wide variety of top-
ics and this is likely to reflect in the entropy of their
language use. We capture this aspect of language
use by computing language entropy over the uni-
gram word distribution for each conversation, i.e;
H(d) = ?
?
w p(w|d) log p(w|d), where p(w|d) is
the probability of word w given conversation d. We
also included two other lexical statistics namely the
speaking rate and the word count (in log domain).
Table 4 lists the utility of these language proper-
ties for differentiating the four binary classes men-
tioned earlier, where the p-value is computed using
two tailed independent sample t-test.
116
Table 4: T-statistics for different context groups. Labels:
a) Log-word count, b) speaking rate, c) language entropy.
Asterisk denotes significance at p<0.0001. Sample sizes
(n) may be found in Table 3.
Task d.f. a) b) c)
Res. v. biz. 7646 1.9 10.1? -1.9
Family v. other 8556 16.3? 9.0? 13.4?
Family v. other res. 2571 12.9? 5.1? 11.3?
Familiar v. other 8556 10.4? 6.4? 9.3?
For the most part, the significance tests conform
with preconceived ideas of language use over the
telephone. It is shown that people talk longer,
more rapidly and have wider range of language use
when conversing with a familiar contact and/or fam-
ily member. Surprisingly, only the speaking rate
showed significant differences among the residen-
tial/business categories, with business conversations
being conducted at a slower pace at least for the el-
derly demographic in our corpus.
5.2 Linguistic inquiry and Word Count
We investigated a hand-crafted dictionary of salient
words, called Linguistic Inquiry and Word Count
(LIWC), employed in social psychology stud-
ies (Pennebaker et al, 2003). This dictionary group
words into 64 categories such as pronouns, activ-
ity words, positive emotion and health. The cate-
gories have significant overlap and a given word can
map to zero or more categories. The clear benefit
of LIWC is that the word categories have very clear
and pre-labeled meanings. They suffer from the ob-
vious drawback that the words are labeled in isola-
tion without taking their context into account. The
tags are not chosen under any mathematical criteria
and so there are no guarantees the resultant feature
will be useful or optimal for classifying utterances.
Table 5 lists the LIWC categories significant (p<
0.001) to the different classes. The listed terms are
sorted according to their t-statistic, with early and
later terms more indicative of first and second class
labels respectively.
5.3 Latent Dirichlet alocation
Unsupervised clustering and feature selection can
make use of data for which we have no labels. For
example, in the case of business and residential la-
bels, unlabeled data amounts to about 50% of our
corpus. Motivated by this consideration, we exam-
ined unsupervised clustering using Latent Dirichlet
Allocation (LDA) (Blei et al, 2003).
LDA models a conversation as a bag of words.
The model generates a conversation by: (a) sam-
pling a topic distribution ? for the conversation using
a per-conversation Dirichlet topic distribution with a
hyper-parameter ?, (b) sampling a topic z for each
word in the conversation using a multinomial distri-
bution using the topic mixture ?, and (c) sampling
the word from a per-topic multinomial word distri-
bution with a hyper-parameter ? (Blei et al, 2003).
The number of topics are assumed to be given. The
per-conversation topic distribution and the per-topic
word distribution can be automatically estimated to
maximize the likelihood of training data. The spar-
sity of these two distributions can be controlled by
tweaking ? and ?; lower values increase sparsity.
For our experiments, we estimated a maximum
likelihood 30-topic LDA model from the corpus.
Experimentally, we found best cross-validation re-
sults were obtained when ? and ? were set to 0.01
and 0.1 respectively.
When peering into the topics learned by the LDA
method, it did appear that topics were approximately
separated into contextual categories. Most interest-
ing, when the number of clusters are reduced to two,
the LDA model managed to segment residential and
business conversations with relatively high accuracy
(80%). This suggests the LDA model was able to
approximately learn these classes in an unsupervised
manner.
Table 6 lists words strongly associated with the
two topics and clearly the unsupervised cluster-
ing appears to have automatically differentiated the
business-oriented calls from the rest. On closer ex-
amination, we found that most of the probability
was distributed in a limited number of words in the
business-oriented topic. On the contrary, the proba-
bility was more widely distributed among words in
the other cluster, reflecting the diversity of content
in personal calls.
5.4 Classifying Types of Social Relationships
Though t-tests are useful for ruling out insignificant
relationships, they are insufficient for quantifying
the degree of separability ? and thus, ultimately their
117
Table 5: LIWC categories found to be significant in classifying relationships, ranked according to their t-statistic.
Relationship Categories
Res. v. biz. I, Past, Self, Motion, Other, Insight, Eating, Pronoun, Down, Physcal, Excl, Space, Cogmech, Home,
Sleep, Tentat, Assent, / Article, Optim, Fillers, Senses, Hear, We, Feel, Inhib, Incl, You, School, Money,
Occup, Job, Number
Family v. all Other, Past, Assent, Sleep, Insight, I, Pronoun, Cogmech, Tentat, Motion, Self / Affect, Optim, Certain,
Future, School, Comm, Job, We, Preps, Incl, Occup, You, Number
Family v. res. Other, Past, Sleep, Pronoun, Tentat, Cogmech, Insight, Humans / Comm, We, Incl, You, Preps, Number
Familiar v. other Other, Assent, Past, I, Leisure, Self, Insight / Fillers, Certain, Social, Posemo, We, Future, Affect, Incl,
Comm, Achieve, School, You, Optim, Job, Occup
Table 6: Per-topic word distribution learned using unsu-
pervised clustering with LDA. Words are sorted accord-
ing to their posterior topic distribution. Words with iden-
tical distributions are sorted alphabetically.
Topic 1 Topic 2
Invalid, helpline, eligibility,
transactions, promo-
tional, representative,
mastercard, touchtone,
activation, nominating,
receiver, voicemail, digit,
representatives, Chrysler,
ballots, staggering, refills,
resented, classics, metro,
represented, administer,
transfers, reselling, recom-
mendations, explanation,
floral, exclusive, submit.
Adorable, aeroplanes,
Arlene, Astoria, baked,
biscuits, bitches, blisters,
bluegrass, bracelet, brains,
bushes, calorie, casinos,
Charlene, cheeses, chit,
Chris, clam, clientele,
cock, cookie, copying,
crab, Davenport, debating,
dementia, dictionary, dime,
Disneyland, eek, Eileen,
fascinated, follies, fry,
gained.
utility in discrimination. To directly test discrimi-
nation performance, we use support vector machine
classifiers. Before performing classification, we pro-
duce balanced datasets that have equal numbers of
conversations for each category. Our primary moti-
vation for artificially balancing the label distribution
in each experiment is to provide a consistent base-
line over which each classifier may be compared.
We learn SVM classifiers with an RBF kernel us-
ing 85% of data for development. SVM parameters
are tuned with 20-fold cross-validation on the dev-
set. The accuracies of the classifiers, measured on a
held out set, are reported in Table 7.
We tested four feature vectors: 1) unigram fre-
quencies, 2) surface language features (log word
count, speaking rate, entropy), 3) the 64 dimension
LIWC frequency vector and 4) a 30-dimension vec-
tor of LDA topic posterior log-probabilities.
Table 7: SVM performance for the language features. La-
bels: a) unigram vector, b) lexical statistics, c) LIWC and
d) LDA topic posterior log-probabilities
Task 1-grams L.Stats LIWC LDA
Res. v. biz. 84.95 67.61 78.70 81.03
Family v. all 78.03 61.16 72.77 74.75
Family v. res. 76.13 62.92 71.06 72.37
Familiarity 69.17 60.92 64.20 69.56
Overall, the plain unigram frequency vector pro-
vided the best discrimination performance. How-
ever, this comes at significant training costs as
the unigram feature vector has a dimensionality
of approximately 20,000. While the surface fea-
tures did possess a degree of classification utility,
there are clearly outclassed by the content-based
features. Furthermore, their integration into the
content-features yielded only insignificant improve-
ments to accuracy. Finally, it is of interest to note
that the 30-topic LDA feature trained with ML cri-
terion outperformed the 64-topic LIWC vector in all
cases.
6 Conclusions
This paper studies a unique corpus of conversational
telephone speech, a comprehensive and naturalis-
tic sample of all the incoming and outgoing tele-
phone calls from 10 older adults over the duration
of one year. Through empirical experiments we
show that the business calls can be separated from
social calls with accuracies as high as 85% using
standard techniques. Subgroups such as family can
also be differentiated automatically with accuracies
above 74%. When compared to language use (en-
tropy) and hand-crafted dictionaries (LIWC), poste-
118
riors over topics computed using a latent Dirichlet
model provide superior performance.
For the elderly demographic, openings of conver-
sations were found to be more informative in clas-
sifying conversation than closings or random seg-
ments, when using automated transcripts. The high
accuracy in classifying business from personal con-
versations suggests potential applications in design-
ing context user interface for smartphones to offer
icons related to work email, work calendar or Face-
book apps. In future work, we plan to examine
subject specific language use, turn taking and af-
fect to further improve the classification of social
calls (Shafran et al, 2003).
7 Acknowledgements
This research was supported in part by NIH Grants
5K25AG033723-02 and P30 AG024978-05 and
NSF Awards 1027834, 0958585 and 0905095. Any
opinions, findings, conclusions or recommendations
expressed in this publication are those of the authors
and do not reflect the views of the NIH or NSF. We
thank Brian Kingsbury and IBM for making their
ASR software tools available to us. We are also
grateful to Nicole Larimer, Maider Lehr and Kather-
ine Wild for their contributions to data collection.
References
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. J. Mach. Learn.
Res., 3:993?1022, March.
T A Glass, C F Mendes de Leon, T E Seeman, and L F
Berkman. 1997. Beyond single indicators of social
networks: a lisrel analysis of social ties among the el-
derly. Soc Sci Med, 44(10):1503?1517.
J. Godfrey, E. Holliman, and J. McDaniel. 1992.
SWITCHBOARD: Telephone speech corpus for re-
search and development. In IEEE International Con-
ference on Acoustics, Speech, and Signal Processing,
pages 517?520.
T. Joachims. 2006. Training linear svms in linear
time. In ACM Conference on Knowledge Discovery
and Data Mining.
Haewoon Kwak, Changhyun Lee, Hosung Park, and Sue
Moon. 2010. What is twitter, a social network or
a news media? In Proceedings of the 19th inter-
national conference on World wide web, WWW ?10,
pages 591?600, New York, NY, USA. ACM.
Alexander Pak and Patrick Paroubek. 2010. Twit-
ter as a corpus for sentiment analysis and opinion
mining. In Nicoletta Calzolari (Conference Chair),
Khalid Choukri, Bente Maegaard, Joseph Mariani,
Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel
Tapias, editors, Proceedings of the Seventh Interna-
tional Conference on Language Resources and Evalu-
ation (LREC?10), Valletta, Malta, may. European Lan-
guage Resources Association (ELRA).
J. W. Pennebaker, M. R. Mehl, and K. G. Niederhoffer.
2003. Psychological aspects of natural language use:
Our words, our selves. Annual Review of Psychology,
54(1):547?577.
Sas?a Petrovic?, Miles Osborne, and Victor Lavrenko.
2010. Streaming first story detection with applica-
tion to twitter. In Human Language Technologies: The
2010 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
HLT ?10, pages 181?189, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Unsu-
pervised modeling of twitter conversations. In Human
Language Technologies: The 2010 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics, HLT ?10, pages 172?180,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Harvey Sacks, Emanuel A. Schegloff, and Gail Jeffer-
son. 1974. A simplest systematics for the organization
of turn-taking for conversation language. Language,
50(4(1)):696?735.
Emanuel A. Schegloff and Harvey Sacks. 1973. Opening
up closings. Semiotica, 8:289?327.
Emanuel A. Schegloff. 1968. Sequencing in con-
versational openings. American Anthropologist,
70(6):1075?1095.
Izhak Shafran, Michael Riley, and Mehryar Mohri. 2003.
Voice signatures. In Proc. IEEE Automatic Speech
Recognition and Understanding Workshop.
H. Soltau, B. Kingsbury, L. Mangu, D. Povey, G. Saon,
and G. Zweig. 2005. The IBM 2004 conversational
telephony system for rich transcription. In IEEE Inter-
national Conference on Acoustics, Speech, and Signal
Processing, volume 1, pages 205?208.
Anthony Stark, Izhak Shafran, and Jeffrey Kaye. 2011.
Supervised and unsupervised feature selection for in-
ferring social nature of telephone conversations from
their content. In Proc. IEEE Automatic Speech Recog-
nition and Understanding Workshop.
Paul Taylor, Rich Morin, Kim Parker, D?Vera Cohn,
and Wendy Wang. June 29, 2009. Grow-
ing old in America: Expectations vs. reality.
http://pewsocialtrends.org/files/2010/10/Getting-Old-
in-America.pdf.
119
Proceedings of NAACL-HLT 2013, pages 211?220,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Discriminative Joint Modeling of Lexical Variation and Acoustic Confusion
for Automated Narrative Retelling Assessment
Maider Lehr?, Izhak Shafran?, Emily Prud?hommeaux? and Brian Roark?
?Center for Spoken Language Understanding, Oregon Health & Science University
?Center for Language Sciences, University of Rochester
{maiderlehr,zakshafran,emilpx,roarkbr}@gmail.com
Abstract
Automatically assessing the fidelity of a
retelling to the original narrative ? a task of
growing clinical importance ? is challenging,
given extensive paraphrasing during retelling
along with cascading automatic speech recog-
nition (ASR) errors. We present a word tag-
ging approach using conditional random fields
(CRFs) that allows a diversity of features
to be considered during inference, including
some capturing acoustic confusions encoded
in word confusion networks. We evaluate the
approach under several scenarios, including
both supervised and unsupervised training, the
latter achieved by training on the output of
a baseline automatic word-alignment model.
We also adapt the ASR models to the domain,
and evaluate the impact of error rate on per-
formance. We find strong robustness to ASR
errors, even using just the 1-best system out-
put. A hybrid approach making use of both au-
tomatic alignment and CRFs trained tagging
models achieves the best performance, yield-
ing strong improvements over using either ap-
proach alone.
1 Introduction
Narrative production tasks are an essential compo-
nent of many standard neuropsychological test bat-
teries. For example, narration of a wordless pic-
ture book is part of the Autism Diagnostic Obser-
vation Schedule (ADOS) (Lord et al, 2002) and
retelling of previously narrated stories is part of both
the Developmental Neuropsychological Assessment
(NEPSY) (Korkman et al, 1998) and the Wech-
sler Logical Memory (WLM) test (Wechsler, 1997).
Such tests also arise in reading comprehension, sec-
ond language learning and other computer-based tu-
toring systems (Xie et al, 2012; Zhang et al, 2008).
The accuracy of automated scoring of a narrative
retelling depends on correctly identifying which of
the source narrative?s propositions or events (what
we will call ?story elements?) have been included
in the retelling. Speakers may choose to relate
these elements using diverse words or phrases, and
an automated method of identifying these elements
needs to model the permissible variants and para-
phrasings. In previous work (Lehr et al, 2012;
Prud?hommeaux and Roark, 2012; Prud?hommeaux
and Roark, 2011), we developed models based on
automatic word-alignment methods, as described
briefly in Section 3. Such alignments are learned
in an unsupervised manner from a parallel corpus of
manual or ASR transcripts of retellings and the orig-
inal source narrative, much as in machine translation
training.
Relying on manual transcripts to train the align-
ment models limits the ability of these methods to
handle ASR errors. By instead training on ASR
transcripts, these methods can automatically capture
some regularities of lexical variants and their com-
mon realizations by the recognizer. Additionally, ev-
idence of acoustic confusability is available in word
lattice output from the recognizer, which can be ex-
ploited to yield more robust automatic scoring, par-
ticularly in high error-rate scenarios.
In this paper, we present and evaluate the use of
word tagging models for this task, in contrast to
just using automatic (unsupervised) word-alignment
methods. The approach is general enough to al-
211
low tagging of word confusion networks derived
from lattices, thus allowing us to explore the utility
of such representations to achieve robustness. We
present results under a range of experimental condi-
tions, including: variously adapting the ASR mod-
els to the domain; using maximum entropy models
rather than CRFs; differing tagsets (BIO versus IO);
and with varying degrees of supervision. Finally,
we demonstrate improved utility in terms of using
the automatic scores to classify elderly individuals
as having Mild Cognitive Impairment. Ultimately
we find that hybrid approaches, making use of both
word-alignment and tagging models, yield strong
improvements over either used independently.
2 Wechsler Logical Memory (WLM) task
The Wechsler Logical Memory (WLM) task (Wech-
sler, 1997), a widely used subtest of a battery of neu-
ropsychological tests used to assess memory func-
tion in adults, has been shown to be a good indicator
of Mild Cognitive Impairment (MCI) (Storandt and
Hill, 1989; Petersen et al, 1999; Wang and Zhou,
2002; Nordlund et al, 2005), the stage of cogni-
tive decline that is often a precursor to dementia of
the Alzheimer?s type. In the WLM, the subject lis-
tens to the examiner read a brief narrative and then
retells the narrative twice: immediately upon hear-
ing it and after about 20 minutes. The examiner
grades the subject?s response by counting how many
of the story elements the subject recalled.
An excerpt of the text read by the clinician while
administering the WLM task is shown in Figure 1.
The story elements in the text are delineated using
slashes, 25 elements in all. An example retelling
is shown in Figure 2 to illustrate how the retellings
are scored. The clinical evaluation guidelines spec-
ify what lexical substitutions, if any, are allowed
for each element. Some elements, such as cafeteria
and Thompson, must be recalled verbatim. In other
cases, subjects are given credit for variants, such as
Annie for Anna, or paraphrasing of concepts such as
sympathetic for touched by the woman?s story. The
example retelling received a score of 12, with one
point for each of the recalled story elements: Anna,
Boston, employed, as a cook, and robbed of, she had
four, small children, reported, station, touched by
the woman?s story, took up a collection and for her.
Anna / Thompson / of South / Boston / em-
ployed / as a cook / in a school / cafeteria /
reported / at the police / station / that she had
been held up / on State Street / the night be-
fore / and robbed / . . . / police / touched by the
woman?s story / took up a collection / for her.
Figure 1: Reference text and the set of story elements.
Ann Taylor worked in Boston as a cook. And
she was robbed of sixty-seven dollars. Is
that right? And she had four children and
reported at the some kind of station. The fel-
low sympathetic and made a collection for her
so that she can feed the children.
Figure 2: An example retelling with 12 recalled story elements.
3 Unsupervised generative automated
scoring with word alignment
In previous work (Lehr et al, 2012; Prud?hommeaux
and Roark, 2012; Prud?hommeaux and Roark,
2011), we developed a pipeline for automatically
scoring narrative retellings for the WLM task. The
utterances corresponding to a retelling were rec-
ognized using an ASR system. The story ele-
ments were identified from the 1-best ASR transcript
using word alignments produced by the Berkeley
aligner (Liang et al, 2006), an EM-based word
alignment package developed to align parallel texts
for machine translation. The word alignment model
was estimated in an unsupervised manner from a
parallel corpus consisting of source narrative and
manual transcripts of retellings from a small set of
training subjects, and from a pairwise parallel cor-
pus of manual retelling transcripts.
During inference or test, the ASR transcripts of
the retellings were aligned using the estimated align-
ment model to the source narrative text. If a word
in the retelling was mapped by the alignment model
to a content word in the source narrative, the ele-
ment associated with that content word was counted
as correctly recalled in that retelling. Recall that
the models were trained on unsupervised data so the
aligned words may not always be permissible vari-
ants of the target elements. To alleviate such extra-
neous as well as unaligned words, the alignments
below a threshold of posterior probability are dis-
carded while decoding.
212
4 Supervised discriminative automated
scoring with log-linear models
In this work, we frame the task of detecting story
elements as a tagging task. Thus, our problem re-
duces to assigning a tag to each word position in the
retelling, the tag indicating the story element that the
word is associated with. In its simplest form, we
have 26 tags: one for each of the 25 story elements
indicating the word is ?in? that element (e.g., I15);
and one for ?outside? of any story element (?O?). By
tagging word positions, we are framing the problem
in a general enough way to allow tagging of word
confusion networks (Mangu et al, 2000), which en-
code word confusions that may provide additional
robustness, particularly in high word-error rate sce-
narios. We make use of log-linear models, which
have been used for tagging confusion networks (Ku-
rata et al, 2012), and which allow very flexible fea-
ture vector definition and discriminative optimiza-
tion.
The model allows us to experiment with three
types of inputs as illustrated in the Figure 3 ? the
manual transcript, the 1-best ASR transcript, and the
word confusion network. To create supervised train-
ing data, we force-align ASR transcripts to manual
transcripts and transfer manually annotated story el-
ement tags from the reference transcripts to word po-
sitions in the confusion network or 1-best ASR out-
put using the word-level time marks. Our unsuper-
vised training scenario instead derives story element
tags from a baseline word-alignment based model.
Figure 3: Feature vectors at each word position includes lexi-
cal variants and acoustic confusions.
Markov order 0 Markov order 1
(MaxEnt) (CRF)
Context yi yi?1yi
independent (CI) yixi yi?1yixi
Context yixi?1 yi?1yixi?1
dependent (CD) yixi+1 yi?1yixi+1
Table 1: Feature templates either using or not using neighbor-
ing tag yi?1 (MaxEnt vs. CRF); and for using or not using
neighboring words xi?1, xi+1 (CI vs. CD).
4.1 Features
Given a sequence of word positions x = x1 . . . xn,
the tagger assigns a sequence of labels y = y1 . . . yn
from a tag lexicon. For each word xi in the se-
quence, we can define features in the log-linear
model based on word and tag identities. Table 1
presents several sets of features, defined over words
and tags at various positions relative to the current
word xi and tag yi and compound features are de-
noted as concatenated symbols.
Features that rely only on the current tag yi are
used in a Markov order 0 model, i.e., one for which
each tag is labeled independently. A maximum en-
tropy classifier (see Section 4.2) is used with these
feature sets. Features that include prior tags en-
code dependencies between adjacent tags, and are
used within conditional random fields models (see
Section 4.3). To examine the utility of surrounding
words xi?1 and xi+1, we distinguish between mod-
els trained with context independent features (just
xi) and context dependent features. Note that mod-
els including context dependent feature sets also in-
clude the context independent features, and Markov
order 1 models also include Markov order 0 features.
Two other details about our use of the feature tem-
plates are worth noting. First, when tagging confu-
sion networks, each word in the network at position
i results in a feature instance. Thus, if there are five
confusable words at position i, then there will be
five different xi values being used to instantiate the
features in Table 1. Second, following Kurata et al
(2012), we multiply the feature counts for the con-
text dependent features by a weight to control their
influence on the model. In this paper, the scaling
weight of the context-dependent features was 0.3.
We investigate two different tagsets for this task,
as presented in Table 2. The simpler tagset (IO) sim-
ply identifies words that are in a story element; the
213
Tagging anna rent was due
IO-tags I1 I19 I19 I19
BIO-tags B1 B19 I19 I19
Table 2: Two possible tagsets for labeling.
larger tagset (BIO) differentiates among positions in
a story element chunk. The latter tagset is only of
utility for models with Markov order greater than
zero, and hence are only used with CRF models.
4.2 MaxEnt-based multiclass classifier
Our baseline model is a Maximum Entropy (Max-
Ent) classifier where each position i from the
retelling x gets assigned one of the IO output tags
yi corresponding to the set of 25 story elements and
a null (?O?) symbol. The output tag is modeled as
the conditional probability p(yi | xi) given the word
xi at position i in the retelling.
p(yi | xi) =
exp
(
d?
k=1
?k?k(xi, yi)
)
Z(xi)
where Z(xi) is a normalization factor. The feature
functions ?(xi, yi) are the Markov order 0 features
as defined as in the previous section. The parame-
ters ? ? <d are estimated by optimizing the above
conditional probability, with L2 regularization. We
use the MALLET Toolkit (McCallum, 2002) with
default regularization parameters.
4.3 CRF-based sequence labeling model
The MaxEnt models assign a tag to each position
from the input retelling independently. However,
there are a few reasons why reframing the task as
a sequence modeling problem may improve tagging
performance. First, some of the story elements are
multiword sequences, such as she had been held up
or on State Street. Second, even if a retelling orders
recalled elements differently than the original narra-
tive, there is a tendency for story elements to occur
in certain orders.
The parameters of the CRF model, ? ? <d are
estimated by optimizing the following conditional
probability:
P (y | x) =
exp
(
d?
k=1
?k?k(x, y)
)
Z(x)
where ?(x, y) aggregates features across the entire
sequence, and Z(x) is a global normalization con-
stant over the sequence, rather than local for a partic-
ular position as with MaxEnt. Features for the CRF
model are Markov order 1 features, and as with the
MaxEnt training, we use default (L2) regularization
parameters within the MALLET toolkit.
5 Combining tagging and alignment
This paper contrasts a discriminatively trained tag-
ging approach with an unsupervised alignment-
based approach, but there are several ways in which
the two approaches can be combined. First, the
alignment model is unsupervised and can provide
its output as training data to the tagging approach,
resulting in an unsupervised discriminative model.
Second, the alignment model can provide features to
the log-linear tagging model in the supervised condi-
tion. We explore both methods of combination here.
5.1 Unsupervised discriminative tagger
The tagging task based on log-linear models pro-
vides an appropriate framework to easily incorpo-
rate diverse features and discriminatively estimate
the parameters of the model. However, this ap-
proach requires supervised tagged training data, in
this case manual labels indicating the correspon-
dence of phrases in the retellings with story elements
in the original narrative. These manual annotations
are used to derive sequences of story element tags
labeling the words of the retelling. Manually la-
beling the retellings is costly, and the scoring (thus
labeling) scheme is very specific to the test being
analyzed. To avoid manual labeling and provide a
general framework that can easily be adopted in any
retelling based assessment task, we experiment here
with an unsupervised discriminative approach.
In this unsupervised approach, the labeled train-
ing data required by the log-linear model is provided
by the automatic word alignments trained without
supervision. The resulting tag sequences replace the
manual tag sequences used in the standard super-
vised approach.
5.2 Word-alignment derived features
When training discriminative models it is a common
practice to incorporate into the feature space the out-
put from a generative model, since it is a good esti-
214
mator. Here we augment the feature space of the
log-linear models with the tags generated by the au-
tomatic word alignments. In addition to the features
defined in Section 4.1, we include new features that
match predicted labels zi from the word-alignment
model with possible labels in the tagger yi. Our fea-
tures include the current tagger label with (1) the
current predicted word-alignment label; (2) the pre-
vious predicted label; and (3) the next predicted la-
bel. Thus, the new features were yizi, yizi?1 and
yizi+1.
6 Experimental evaluations
Corpus: Our models were trained on immediate and
delayed retellings from 144 subjects with a mean
age of 85.4, of whom 36 were clinically diagnosed
with MCI (training set). We evaluated our models
on a set of retellings from 70 non-overlapping sub-
jects with a mean age of 88.5, half of whom had
received a diagnosis of MCI (test set). In contrast
to the unsupervised word-alignment based method,
the method outlined here required manual story el-
ement labels of the retellings. The training and
test sets from this paper are therefore different from
the sets used in previous work (Lehr et al, 2012;
Prud?hommeaux and Roark, 2012; Prud?hommeaux
and Roark, 2011), and the results are not directly
comparable.
The recordings were sometimes made in an infor-
mal setting, such as the subject?s home or a senior
center. For this reason, there are often extraneous
noises in the recordings such as music, footsteps,
and clocks striking the hour. Although this presents
a challenge for ASR, part of the goal of our work
is to demonstrate the robustness of our methods to
noisy audio.
6.1 Automatic transcription
The baseline ASR system used in the current work
is a Broadcast News system which is modeled af-
ter Kingsbury et al (2011). Briefly, the acoustics
of speech are modeled by 4000 clustered allophone
states defined over a pentaphone context, where
states are represented by Gaussian mixture models
with a total of 150K mixture components. The ob-
servation vectors consist of PLP features, stacked
from 10 neighboring frames and projected to a 50-
1-best oracle oracle
System (%) WCN(%) lat(%)
Baseline 47.2 39.7 27.7
AM adaptation 38.2 35.5 21.2
LM adaptation 28.3 30.7 19.9
AM+LM adaptation 25.6 26.5 16.5
Table 3: Improvement in ASR word error-rate by adapting the
Broadcast News models to the domain of narrative retelling.
dimension space using linear discriminant analysis
(LDA). The acoustic models were trained on 430
hours of transcribed speech from Broadcast News
corpus (LDC97S44, LDC98S71). The language
model is defined over an 84K vocabulary and con-
sists of about 1.8M, 1M and and 331K bigrams, tri-
grams and 4-grams, estimated from standard Broad-
cast news corpus. The decoding is performed in sev-
eral stages using successively refined acoustic mod-
els ? a context-dependent model, a vocal-tract nor-
malized model, a speaker-adapted maximum likeli-
hood linear regression (MLLR) model, and finally
a discriminatively trained model with the boosted
MMI criteria (Povey et al, 2008). The system gives
a word error rate of 13.1% on the 2004 Rich Tran-
scription benchmark by NIST (Fiscus et al, 2007),
which is comparable to state-of-the-art for equiva-
lent amounts of acoustic training data. On the WLM
corpus, the recognition word error rate was signifi-
cantly higher at 47.2% due to a mismatch in domain
and the skewed demographics (age) of the speakers.
We improved the performance of the above
Broadcast News models by adapting to the domain
of the WLM retellings. The acoustic models were
adapted using standard MLLR, where linear trans-
forms were estimated in an unsupervised manner
to maximize the likelihood over the transcripts of
the retellings. The transcripts were generated from
the baseline system after the final stage of decod-
ing with the discriminative model. The language
models were adapted by interpolating the in-domain
model (weight=0.7) with the out-of-domain model.
The gains from these adaptations are reported in
the Table 3. As expected, we find substantial gains
from both acoustic model (AM) and language model
(LM) adaptation. Furthermore, we find benefit in
employing them simultaneously. We also include
the oracle word error rate (WER) of the WCNs and
lattices for each ASR configuration.
215
One thing to note is that the oracle WER of the
WCNs is worse than the 1-best WER when adapting
the language models. We speculate that this is due
to bias introduced by the language model adapted
to the story retellings, resulting in word candidates
in the bins that are not truly acoustically confusable
candidates. This is one potential reason for the lack
of utility of WCNs in low WER conditions.
6.2 Evaluating retelling scoring
We analyzed the performance of the retelling scor-
ing methods under five different input conditions for
producing transcripts: (1) the out-of-domain Broad-
cast News recognizer with no adaptation; (2) do-
main adapted acoustic model; (3) domain adapted
language model; (4) domain adapted acoustic and
language models; and (5) manual (reference) tran-
scripts. Each story element is automatically labeled
by the systems as either having been recalled or not,
and this is compared with manual scores to derive an
F-score accuracy, by calculating precision and recall
of recalled story elements. Derived word alignments
or tag sequences are converted to binary story ele-
ment indicators by simply setting the element to 1
if any open-class word is tagged for (or aligned to)
that story element.
6.2.1 Word alignment based scoring
We evaluate the word alignment approach only on
1-best ASR transcripts and manual transcripts, not
WCNs. The first row of Table 4 reports the story ele-
ment F-scores for a range of ASR adaptation scenar-
ios. The performance of the model improves signifi-
cantly as the WER reduces with adaptation. With the
fully adapted ASR the F-score improves more than
13%, and it is only 3.4% worse than with the man-
ual transcripts. The alignments produced in each of
these scenarios are used as training data in the unsu-
pervised condition evaluated below.
6.2.2 Log-linear based automated scoring
Context-independent features Table 4 summa-
rizes the performance of the log-linear models us-
ing context independent features (CI) in supervised
(section 4), unsupervised (section 5.1) and hybrid
(section 5.2) training scenarios for different inputs
(reference transcript, ASR 1-best, and word confu-
sion network ASR output) and four different ASR
configurations.
The results show a few clear trends. Both in
the supervised and unsupervised training scenarios
the CRF model provides substantial improvements
over the MaxEnt classifier. The F-scores obtained
in the unsupervised training scenario are slightly
worse than with supervision, though they are compa-
rable to supervised results and an improvement over
just using the word alignment approach, particularly
in high WER scenarios. The hybrid training sce-
nario ? supervised learning with word alignment de-
rived features ? leads to reduced differences between
MaxEnt and CRF training compared to the other two
training scenarios. In fact, in high WER scenarios,
the MaxEnt slightly outperforms the CRF.
As expected the best performance is obtained with
manual transcripts and the worst with 1-best tran-
scripts generated by the out-of-domain ASR with
relatively high word error rate. For this ASR con-
figuration, using WCNs provide some gain, though
the gain is insignificant for the hybrid approach. In
the hybrid approach, the output labels of the word
alignment are already good indicators of the output
tag and incorporating the confusable words from the
Table 4: Story element F-score achieved by baseline word-alignment model and log-linear models (MaxEnt and CRF) using
context independent features (CI) under 3 different scenarios, with 3 different inputs (1-best ASR, word confusion network, and
manual transcripts) and different ASR models (baseline out-of-domain, AM adapted, LM adapted and AM+LM adapted).
Training Transcripts: 1-best WCN manual
Scenario ASR: baseline AM LM AM+LM baseline AM LM AM+LM N/A
Baseline word-alignment: 71.9 77.3 84.3 85.4 N/A 88.8
Supervised MaxEnt-CI 76.0 81.7 84.6 85.6 78.9 83.4 84.0 84.7 86.4
CRF-CI 80.3 87.3 89.7 91.4 83.7 88.8 88.2 90.8 94.4
Unsupervised MaxEnt-CI 72.1 79.3 82.7 84.2 77.5 81.2 83.4 83.2 84.8
CRF-CI 79.4 85.4 86.8 88.0 81.2 85.8 86.2 87.2 90.5
Hybrid MaxEnt-CI 88.1 89.4 89.2 89.6 87.6 89.2 88.8 89.5 91.8
CRF-CI 87.0 90.9 91.5 92.1 87.4 91.5 90.1 92.4 94.6
216
Training Transcripts: 1-best WCN manual
Scenario ASR: baseline AM LM AM+LM baseline AM LM AM+LM N/A
Supervised MaxEnt-CD 80.1 87.3 90.0 91.1 83.5 88.6 88.2 90.3 93.3
CRF-CD-IO 80.6 88.0 89.9 91.2 84.2 89.6 88.8 90.5 94.7
CRF-CD-BIO 81.1 87.9 90.6 91.7 84.5 89.5 88.8 90.8 94.7
Un- MaxEnt-CD 77.1 83.1 86.5 89.0 80.2 85.0 86.2 87.6 90.7
supervised CRF-CD-IO 79.1 85.3 87.1 88.3 81.0 85.9 86.4 87.5 90.3
CRF-CD-BIO 79.1 85.6 87.2 88.4 81.3 85.9 86.2 87.3 90.6
Hybrid MaxEnt-CD 88.4 90.2 90.7 91.6 88.6 90.5 90.4 91.4 93.5
CRF-CD-IO 87.9 91.3 91.6 92.5 88.3 91.7 90.7 92.1 94.8
CRF-BIO 87.8 91.9 91.8 93.0 88.7 92.0 90.7 92.3 94.7
Table 5: Story element F-score achieved by log-linear models (MaxEnt and CRF) when adding context dependent features (CD)
and BIO tags for the CRF models, under 3 different scenarios, with 3 different inputs (1-best ASR, word confusion network, and
manual transcripts) and different ASR models (baseline out-of-domain, AM adapted, LM adapted and AM+LM adapted).
WCN into the feature vector apparently mainly adds
noise.
When the transcripts are generated with the
adapted models, the word confidence score of the 1-
best is higher and the WCN bins have fewer acous-
tically confusable words. Still, the WCN input is
helpful in the AM-adapted ASR system. When
the transcripts are generated with LM adapted mod-
els, the performance is better with 1-best than with
WCNs. As mentioned earlier, adapting the lan-
guage models may introduce a bias due to the rel-
atively low LM perplexity for this domain. In the
lowest WER scenarios, the best performing systems
achieve over 90% F-score, within two percent of the
performance achieved with manual transcripts.
Context-dependent features Exercising the flex-
ibility of log-linear models, we investigated the im-
pact of using context-dependent (CD) features in-
stead of the CI features used in the previous exper-
iments. Our CD features take into account the two
immediately neighboring word positions. As men-
tioned earlier, following Kurata et al (2012), the
counts from the neighboring word positions were
weighted (? = 0.3) to avoid data sparsity. This re-
duces the sensitivity of the model to time alignment
errors between the tag and feature vector sequences
without increasing the dimensions. In Table 5, we
report the F-scores for the different ASR configu-
rations, inputs, and log-linear models with context
dependent features, using the standard IO tagset as
in Table 4.
Although there are some exceptions, adding con-
text information from the input features improves
the performance of the models. In particular, the
MaxEnt models benefit from incorporating this ex-
tra information. The MaxEnt models improve their
performance substantially for all three training sce-
narios, while the gains for the CRF models are more
modest, especially for the unsupervised approach
where the performance degrades or does not change
much, since some context information is already
captured by the Markov order 1 features.
BIO tagset As detailed in Section 4.1, story el-
ements sometimes span multiple words, so for the
CRF models we investigated two different schemes
for tagging, following typical practice in named en-
tity extraction (Ratinov and Roth, 2009) and syn-
tactic chunking (Sha and Pereira, 2003). The BIO
tagging scheme makes the distinction between the
tokens from the story elements that are in the be-
ginning from the ones that are not. The O tag is
assigned to the tokens that do not belong to any of
the story elements. The IO tagging uses a single tag
for the tokens that fall in the same story element,
which is the approach we have followed so far. In
addition to presenting results using context depen-
dent features, Table 5 presents results with the BIO
tagset.
For the supervised and hybrid approaches, the
BIO tagging provides insignificant but consistent
gains for most of the scenarios. The unsupervised
approach provides mixed results. This may be due to
the way in which the word alignment model scores
the retellings. It tags only those words from the
retelling that are aligned with a content word in the
source narrative, which may result in the loss of the
217
Training Transcripts: 1-best WCN manual
Scenario ASR: baseline AM LM AM+LM baseline AM LM AM+LM N/A
Baseline word-alignment: 0.65 0.67 0.74 0.76 N/A 0.79
Supervised MaxEnt-CD 0.65 0.73 0.76 0.77 0.70 0.73 0.77 0.77 0.81
CRF-CD-BIO 0.69 0.76 0.77 0.76 0.73 0.76 0.77 0.78 0.82
Un- MaxEnt-CD 0.65 0.72 0.75 0.76 0.70 0.75 0.75 0.76 0.80
supervised CRF-CD-BIO 0.74 0.75 0.78 0.78 0.71 0.74 0.77 0.76 0.81
Hybrid MaxEnt-CD 0.72 0.76 0.77 0.78 0.74 0.76 0.77 0.77 0.82
CRF-CD-BIO 0.72 0.76 0.78 0.78 0.76 0.77 0.78 0.79 0.81
Table 6: Classification performance (AUC) for the baseline word-alignment model and the best performing log-linear models of
both types (MaxEnt and CRF) under 3 different scenarios with 3 types of input and 4 types of ASR models.
structure of some multiwords story elements that we
are trying to capture with the BIO scheme.
6.3 Evaluating MCI classification
Each of the individuals producing retellings in our
corpus underwent a battery of neuropsychological
tests, and were assigned a Clinical Dementia Rating
(CDR) (Morris, 1993), which is a composite score
derived from measures of cognitive function in six
domains, including memory. Importantly, it is as-
signed independently of the Wechsler Logical Mem-
ory test we are analyzing in this paper, which allows
us to evaluate the utility of our WLM analyses in
an unbiased manner. MCI is defined as a CDR of
0.5 (Ritchie and Touchon, 2000), and subjects in this
study have either a CDR of 0 (no impairment) or 0.5
(MCI).
In previous work, we found that the features
extracted from the retellings are useful in dis-
tinguishing subjects with MCI from neurotyp-
ical age-matched controls (Lehr et al, 2012;
Prud?hommeaux and Roark, 2012; Prud?hommeaux
and Roark, 2011). From each retellings, we extract
Boolean features for each story element, for a total
of 50 features for classification. Each feature indi-
cates whether the retelling contained that story ele-
ment.
In this paper, we carry out similar classification
experiments to investigate the impact of using log-
linear models on the extraction of features for classi-
fication. We build a support vector machine (SVM)
using the LibSVM (Chang and Lin, 2011) exten-
sion to the WEKA data mining Java API (Hall et al,
2009). This allows recollection of different elements
to be weighted differently. This is unlike the manual
scoring of WLM based on clinical guidelines where
all elements are weighted equally irrespective of the
difficulty. The SVM was trained on manually ex-
tracted story element feature vectors. We compared
the performance of the MCI classification for three
types of input and four ASR configurations under
the supervised, unsupervised, and hybrid scenarios.
For each scenario we chose the best scoring system
from among the automated systems reported in Ta-
bles 4 and 5. Classification results, evaluated as area
under the curve (AUC), are reported in Table 6, both
for the log-linear trained tagging models and for the
baseline word-alignment based method. For refer-
ence (not shown in the table), the SVM classifier
performed at 0.83 when features values are manu-
ally populated.
The results show that the AUC improves steadily
as the quality of the transcription is improved, go-
ing from the baseline system to the adapted mod-
els. This is consistent with the improvements seen in
the F-score for detecting story elements. The differ-
ent approaches for detecting the story elements from
the transcriptions did not ultimately show significant
differences in MCI classification results. Overall,
the best classification values are given by the hy-
brid approach, which performs slightly better than
the other two approaches. The best AUC in the
hybrid scenario (0.79, very close to the AUC=0.81
achieved with manual transcripts) is obtained with
a CRF trained with WCNs from the fully adapted
ASR model and with context dependent features and
BIO tags.
Comparing WCN versus 1-best as inputs, using
WCN as input improves classification performance
when the 1-best transcripts are poor, as in the case
of out-of-domain ASR. The adapted recognizer im-
proves the performance of the 1-best significantly
making it unnecessary to resort to WCN as inputs.
Comparing the MaxEnt model with CRF model
218
for extracting story elements, we see that the average
F-scores for the MaxEnt models trained on CD fea-
tures are nearly as good as and sometimes slightly
better than those produced using the CRF models.
The CRF extracted story elements, however, tend to
yield classifiers that perform slightly better, espe-
cially in the unsupervised approach with 1-best in-
puts.
7 Summary and discussion
This paper examines the task of automatically scor-
ing narrative retellings in terms of their fidelity to
the original narrative content, using discriminatively
trained log-linear tagging models. Fully automatic
scoring must account for both lexical variation and
acoustic confusion from ASR errors. Lexical vari-
ation ? due to extensive paraphrasing on the part
of the individuals retelling the narrative ? can be
modeled effectively using word-alignment models
such as those employed in machine translation sys-
tems (Lehr et al, 2012; Prud?hommeaux and Roark,
2011). This paper focuses on an alternative ap-
proach, where both lexical variation and ASR con-
fusions are modeled using log-linear models. In ad-
dition to very flexible feature definitions, the log-
linear models bring the advantage of a discrimina-
tive model to the task. We see improvements in
story element F-score using these models over unsu-
pervised word-alignment models. Further, the fea-
ture definition flexibility allows us to incorporate the
unsupervised word-alignment labels into these mod-
els, resulting either in fully unsupervised approaches
that perform competitively with the supervised mod-
els or in hybrid (supervised) approaches that provide
the best performing systems in this study.
Our tagging models are able to process word con-
fusion networks as inputs and thus improve perfor-
mance over using 1-best ASR transcripts in scenar-
ios where the speech recognition error rate is high.
These improvements carry through to the MCI clas-
sification task, making use of features computed
from the automatic scoring of narrative retelling.
One advantage of the word-alignment model is
that such approaches do not require manual anno-
tation of the story elements, which is more labor in-
tensive than typical manual transcription of speech.
Thus, the word-alignment model can exploit large
numbers of retellings in an unsupervised manner
when trained on ASR transcripts of the retellings.
Controlled experiments here with relatively limited
training sets demonstrate that semi-supervised ap-
proaches on larger untranscribed sets are likely to
be successful.
Finally, experiments with different amounts of
ASR adaptation show that both acoustic and lan-
guage model adaptations in this domain are effec-
tive, yielding scenarios that are competitive with
manual transcription both for detecting story ele-
ments as well as for subsequent classification. With
full model adaptation to the domain, the 1-best
transcripts improved significantly, and their perfor-
mance was found to be at par with WCNs.
In future work, we would like to investigate two
questions left open by these results. First, word-
alignment models can be extended to process ASR
lattices or word confusion networks as part of the
unsupervised alignment learning algorithm, and in-
corporated into our approach. Second, the con-
textual features can be refined (e.g., concatenated
features instead of smoothed features) when large
amounts of training data is available.
It is noteworthy to mention that the lexical vari-
ants and paraphrasing learned from the data using
automated method may be useful in refining the clin-
ical guidelines for scoring (e.g., allowing additional
lexical variants and paraphrasings, or assigning un-
equal credits for different story elements to reflect
the difficulty of recollecting them) or to create the
guidelines for new languages or stories.
Acknowledgments
This research was supported in part by NIH awards
5K25AG033723-02 and P30 AG024978-05 and
NSF awards 1027834, 0958585, 0905095, 0964102
and 0826654. Any opinions, findings, conclusions
or recommendations expressed in this publication
are those of the authors and do not reflect the views
of the NIH or NSF. We thank Brian Kingsbury and
IBM for the use of their ASR software tools; Jeffrey
Kaye and Diane Howeison for their valuable input;
and the clinicians at the Oregon Center for Aging
and Technology for their care in collecting the data.
219
References
Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM:
A library for support vector machines. ACM Transac-
tions on Intelligent Systems and Technology, 2(27):1?
27.
Jonathan Fiscus, John Garofolo, Audrey Le, Alvin Mar-
tin, greg Sanders, Mark Przybocki, and David Pallett.
2007. 2004 spring nist rich transcription (rt-04s)
evaluation data. http://www.ldc.upenn.edu/
Catalog/catalogEntry.jsp?catalogId=
LDC2007S12.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: An update.
SIGKDD Explorations, 11(1).
Brian Kingsbury, Hagen Soltau, George Saon,
Stephen M. Chu, Hong-Kwang Kuo, Lidia Mangu,
Suman V. Ravuri, Nelson Morgan, and Adam Janin.
2011. The IBM 2009 GALE Arabic speech tran-
scription system. In Proceedings of ICASSP, pages
4672?4675.
Marit Korkman, Ursula Kirk, and Sally Kemp. 1998.
NEPSY: A developmental neuropsychological assess-
ment. The Psychological Corporation, San Antonio.
Gakuto Kurata, Nobuyasu Itoh, Masafumi Nishimura,
Abhinav Sethy, and Bhuvana Ramabhadran. 2012.
Leveraging word confusion networks for named entity
modeling and detection from conversational telephone
speech. Speech Communication, 54(3):491?502.
Maider Lehr, Emily Prud?hommeaux, Izhak Shafran, and
Brian Roark. 2012. Fully automated neuropsycho-
logical assessment for detecting mild cognitive impair-
ment. In Interspeech.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of HLT-NAACL.
Catherine Lord, Michael Rutter, Pamela DiLavore, and
Susan Risi. 2002. Autism Diagnostic Observation
Schedule (ADOS). Western Psychological Services,
Los Angeles.
Lidia Mangu, Eric Brill, and Andreas Stolcke. 2000.
Finding consensus in speech recognition: Word error
minimization and other applications of confusion net-
works. Computer Speech and Language, 14(4):373?
400.
Andrew Kachites McCallum. 2002. Mallet: A machine
learning for language toolkit. http://mallet.
cs.umass.edu.
John Morris. 1993. The clinical dementia rating
(CDR): Current version and scoring rules. Neurology,
43:2412?2414.
A. Nordlund, S. Rolstad, P. Hellstrom, M. Sjogren,
S. Hansen, and A. Wallin. 2005. The Goteborg MCI
study: Mild cognitive impairment is a heterogeneous
condition. Journal of Neurology, Neurosurgery and
Psychiatry, 76(11):1485?1490.
Ronald Petersen, Glenn Smith, Stephen Waring, Robert
Ivnik, Eric Tangalos, and Emre Kokmen. 1999. Mild
cognitive impairment: Clinical characterizations and
outcomes. Archives of Neurology, 56:303?308.
Daniel Povey, Dimitri Kanevsky, Brian Kingsbury,
Bhuvana Ramabhadran, George Saon, and Karthik
Visweswariah. 2008. Boosted mmi for model and fea-
ture space discriminative training. In Proceedings of
ICASSP.
Emily Prud?hommeaux and Brian Roark. 2011. Align-
ment of spoken narratives for automated neuropsycho-
logical assessment. In Proceedings of ASRU.
Emily Prud?hommeaux and Brian Roark. 2012. Graph-
based alignment of narratives for automated neuropsy-
chological assessment. In Proceedings of the NAACL
2012 Workshop on Biomedical Natural Language Pro-
cessing (BioNLP).
Lev Ratinov and Dan Roth. 2009. Design challenges
and misconceptions in named entity recognition. In
EMNLP.
Karen Ritchie and Jacques Touchon. 2000. Mild cogni-
tive impairment: Conceptual basis and current noso-
logical status. Lancet, 355:225?228.
Fei Sha and Fernando Pereira. 2003. Shallow pars-
ing with conditional random fields. In Proceedings of
HLT-NAACL.
Martha Storandt and Robert Hill. 1989. Very mild senile
dementia of the Alzheimer?s type: II Psychometric test
performance. Archives of Neurology, 46:383?386.
Qing-Song Wang and Jiang-Ning Zhou. 2002. Retrieval
and encoding of episodic memory in normal aging and
patients with mild cognitive impairment. Brain Re-
search, 924:113?115.
David Wechsler. 1997. Wechsler Memory Scale - Third
Edition. The Psychological Corporation, San Antonio.
Shasha Xie, Keelan Evanini, and Klaus Zechner. 2012.
Exploring content features for automated speech scor-
ing. In Proceedings of HLT-NAACL.
Xiaonan Zhang, Xiaonan Zhang, Jack Mostow, Jack
Mostow, Nell Duke, Christina Trotochaud, Joseph Va-
leri, and Al Corbett. 2008. Mining free-form spoken
responses to tutor prompts. In Proceedings of the First
International Conference on Educational Data Min-
ing, pages 234?241.
220
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 1?5,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Lexicographic Semirings for Exact Automata Encoding of Sequence Models
Brian Roark, Richard Sproat, and Izhak Shafran
{roark,rws,zak}@cslu.ogi.edu
Abstract
In this paper we introduce a novel use of the
lexicographic semiring and motivate its use
for speech and language processing tasks. We
prove that the semiring allows for exact en-
coding of backoff models with epsilon tran-
sitions. This allows for off-line optimization
of exact models represented as large weighted
finite-state transducers in contrast to implicit
(on-line) failure transition representations. We
present preliminary empirical results demon-
strating that, even in simple intersection sce-
narios amenable to the use of failure transi-
tions, the use of the more powerful lexico-
graphic semiring is competitive in terms of
time of intersection.
1 Introduction and Motivation
Representing smoothed n-gram language models as
weighted finite-state transducers (WFST) is most
naturally done with a failure transition, which re-
flects the semantics of the ?otherwise? formulation
of smoothing (Allauzen et al, 2003). For example,
the typical backoff formulation of the probability of
a word w given a history h is as follows
P(w | h) =
{
P(w | h) if c(hw) > 0
?hP(w | h?) otherwise
(1)
where P is an empirical estimate of the probabil-
ity that reserves small finite probability for unseen
n-grams; ?h is a backoff weight that ensures nor-
malization; and h? is a backoff history typically
achieved by excising the earliest word in the his-
tory h. The principle benefit of encoding the WFST
in this way is that it only requires explicitly storing
n-gram transitions for observed n-grams, i.e., count
greater than zero, as opposed to all possible n-grams
of the given order which would be infeasible in for
example large vocabulary speech recognition. This
is a massive space savings, and such an approach is
also used for non-probabilistic stochastic language
models, such as those trained with the perceptron
algorithm (Roark et al, 2007), as the means to ac-
cess all and exactly those features that should fire
for a particular sequence in a deterministic automa-
ton. Similar issues hold for other finite-state se-
quence processing problems, e.g., tagging, bracket-
ing or segmenting.
Failure transitions, however, are an implicit
method for representing a much larger explicit au-
tomaton ? in the case of n-gram models, all pos-
sible n-grams for that order. During composition
with the model, the failure transition must be inter-
preted on the fly, keeping track of those symbols
that have already been found leaving the original
state, and only allowing failure transition traversal
for symbols that have not been found (the semantics
of ?otherwise?). This compact implicit representa-
tion cannot generally be preserved when composing
with other models, e.g., when combining a language
model with a pronunciation lexicon as in widely-
used FST approaches to speech recognition (Mohri
et al, 2002). Moving from implicit to explicit repre-
sentation when performing such a composition leads
to an explosion in the size of the resulting trans-
ducer, frequently making the approach intractable.
In practice, an off-line approximation to the model
is made, typically by treating the failure transitions
as epsilon transitions (Mohri et al, 2002; Allauzen
et al, 2003), allowing large transducers to be com-
posed and optimized off-line. These complex ap-
proximate transducers are then used during first-pass
decoding, and the resulting pruned search graphs
(e.g., word lattices) can be rescored with exact lan-
guage models encoded with failure transitions.
Similar problems arise when building, say, POS-
taggers as WFST: not every pos-tag sequence will
have been observed during training, hence failure
transitions will achieve great savings in the size of
models. Yet discriminative models may include
complex features that combine both input stream
(word) and output stream (tag) sequences in a single
feature, yielding complicated transducer topologies
for which effective use of failure transitions may not
1
be possible. An exact encoding using other mecha-
nisms is required in such cases to allow for off-line
representation and optimization.
In this paper, we introduce a novel use of a semir-
ing ? the lexicographic semiring (Golan, 1999) ?
which permits an exact encoding of these sorts of
models with the same compact topology as with fail-
ure transitions, but using epsilon transitions. Unlike
the standard epsilon approximation, this semiring al-
lows for an exact representation, while also allow-
ing (unlike failure transition approaches) for off-line
composition with other transducers, with all the op-
timizations that such representations provide.
In the next section, we introduce the semiring, fol-
lowed by a proof that its use yields exact represen-
tations. We then conclude with a brief evaluation of
the cost of intersection relative to failure transitions
in comparable situations.
2 The Lexicographic Semiring
Weighted automata are automata in which the tran-
sitions carry weight elements of a semiring (Kuich
and Salomaa, 1986). A semiring is a ring that may
lack negation, with two associative operations? and
? and their respective identity elements 0 and 1. A
common semiring in speech and language process-
ing, and one that we will be using in this paper, is
the tropical semiring (R? {?},min,+,?, 0), i.e.,
min is the ? of the semiring (with identity?) and
+ is the ? of the semiring (with identity 0). This is
appropriate for performing Viterbi search using neg-
ative log probabilities ? we add negative logs along
a path and take the min between paths.
A ?W1,W2 . . .Wn?-lexicographic weight is a tu-
ple of weights where each of the weight classes
W1,W2 . . .Wn, must observe the path property
(Mohri, 2002). The path property of a semiring K
is defined in terms of the natural order on K such
that: a <K b iff a ? b = a. The tropical semiring
mentioned above is a common example of a semir-
ing that observes the path property, since:
w1 ? w2 = min{w1, w2}
w1 ? w2 = w1 + w2
The discussion in this paper will be restricted to
lexicographic weights consisting of a pair of tropi-
cal weights ? henceforth the ?T, T ?-lexicographic
semiring. For this semiring the operations ? and ?
are defined as follows (Golan, 1999, pp. 223?224):
?w1, w2? ? ?w3, w4? =
?
????
????
if w1 < w3 or
?w1, w2? (w1 = w3 &
w2 < w4)
?w3, w4? otherwise
?w1, w2? ? ?w3, w4? = ?w1 + w3, w2 + w4?
The term ?lexicographic? is an apt term for this
semiring since the comparison for ? is like the lexi-
cographic comparison of strings, comparing the first
elements, then the second, and so forth.
3 Language model encoding
3.1 Standard encoding
For language model encoding, we will differentiate
between two classes of transitions: backoff arcs (la-
beled with a ? for failure, or with  using our new
semiring); and n-gram arcs (everything else, labeled
with the word whose probability is assigned). Each
state in the automaton represents an n-gram history
string h and each n-gram arc is weighted with the
(negative log) conditional probability of the word w
labeling the arc given the history h. For a given his-
tory h and n-gram arc labeled with a word w, the
destination of the arc is the state associated with the
longest suffix of the string hw that is a history in the
model. This will depend on the Markov order of the
n-gram model. For example, consider the trigram
model schematic shown in Figure 1, in which only
history sequences of length 2 are kept in the model.
Thus, from history hi = wi?2wi?1, the word wi
transitions to hi+1 = wi?1wi, which is the longest
suffix of hiwi in the model.
As detailed in the ?otherwise? semantics of equa-
tion 1, backoff arcs transition from state h to a state
h?, typically the suffix of h of length |h| ? 1, with
weight (? log?h). We call the destination state a
backoff state. This recursive backoff topology ter-
minates at the unigram state, i.e., h = , no history.
Backoff states of order k may be traversed either
via ?-arcs from the higher order n-gram of order k+
1 or via an n-gram arc from a lower order n-gram of
order k?1. This means that no n-gram arc can enter
the zeroeth order state (final backoff), and full-order
states ? history strings of length n? 1 for a model
of order n ? may have n-gram arcs entering from
other full-order states as well as from backoff states
of history size n? 2.
3.2 Encoding with lexicographic semiring
For an LM machineM on the tropical semiring with
failure transitions, which is deterministic and has the
2
h i =wi-2wi-1 hi+1 =wi-1wiwi /-logP(wi |h i)
wi-1
?/-log ?hi
wi
?/-log ?h i+1
wi /-logP(wi|wi-1)
?/-log ?w i-1 wi /-logP(wi)
Figure 1: Deterministic finite-state representation of n-gram
models with negative log probabilities (tropical semiring). The
symbol ? labels backoff transitions. Modified from Roark and
Sproat (2007), Figure 6.1.
path property, we can simulate ?-arcs in a standard
LM topology by a topologically equivalent machine
M ? on the lexicographic ?T, T ? semiring, where ?
has been replaced with epsilon, as follows. For every
n-gram arc with label w and weight c, source state
si and destination state sj , construct an n-gram arc
with label w, weight ?0, c?, source state s?i, and des-
tination state s?j . The exit cost of each state is con-
structed as follows. If the state is non-final, ??,??.
Otherwise if it final with exit cost c it will be ?0, c?.
Let n be the length of the longest history string in
the model. For every ?-arc with (backoff) weight
c, source state si, and destination state sj repre-
senting a history of length k, construct an -arc
with source state s?i, destination state s
?
j , and weight
???(n?k), c?, where ? > 0 and ??(n?k) takes ? to
the (n ? k)th power with the ? operation. In the
tropical semiring, ? is +, so ??(n?k) = (n ? k)?.
For example, in a trigram model, if we are backing
off from a bigram state h (history length = 1) to a
unigram state, n ? k = 2 ? 0 = 2, so we set the
backoff weight to ?2?,? log?h) for some ? > 0.
In order to combine the model with another au-
tomaton or transducer, we would need to also con-
vert those models to the ?T, T ? semiring. For these
automata, we simply use a default transformation
such that every transition with weight c is assigned
weight ?0, c?. For example, given a word lattice
L, we convert the lattice to L? in the lexicographic
semiring using this default transformation, and then
perform the intersection L? ?M ?. By removing ep-
silon transitions and determinizing the result, the
low cost path for any given string will be retained
in the result, which will correspond to the path
achieved with ?-arcs. Finally we project the second
dimension of the ?T, T ? weights to produce a lattice
in the tropical semiring, which is equivalent to the
result of L ?M , i.e.,
C2(det(eps-rem(L? ?M ?))) = L ?M
where C2 denotes projecting the second-dimension
of the ?T, T ? weights, det(?) denotes determiniza-
tion, and eps-rem(?) denotes -removal.
4 Proof
We wish to prove that for any machine N ,
ShortestPath(M ? ? N ?) passes through the equiv-
alent states in M ? to those passed through in M for
ShortestPath(M ? N). Therefore determinization
of the resulting intersection after -removal yields
the same topology as intersection with the equiva-
lent ? machine. Intuitively, since the first dimension
of the ?T, T ? weights is 0 for n-gram arcs and > 0
for backoff arcs, the shortest path will traverse the
fewest possible backoff arcs; further, since higher-
order backoff arcs cost less in the first dimension of
the ?T, T ? weights in M ?, the shortest path will in-
clude n-gram arcs at their earliest possible point.
We prove this by induction on the state-sequence
of the path p/p? up to a given state si/s?i in the respec-
tive machines M/M ?.
Base case: If p/p? is of length 0, and therefore the
states si/s?i are the initial states of the respective ma-
chines, the proposition clearly holds.
Inductive step: Now suppose that p/p? visits
s0...si/s?0...s
?
i and we have therefore reached si/s
?
i
in the respective machines. Suppose the cumulated
weights of p/p? are W and ??,W ?, respectively. We
wish to show that whichever sj is next visited on p
(i.e., the path becomes s0...sisj) the equivalent state
s? is visited on p? (i.e., the path becomes s?0...s
?
is
?
j).
Let w be the next symbol to be matched leaving
states si and s?i. There are four cases to consider:
(1) there is an n-gram arc leaving states si and s?i la-
beled with w, but no backoff arc leaving the state;
(2) there is no n-gram arc labeled with w leaving the
states, but there is a backoff arc; (3) there is no n-
gram arc labeled with w and no backoff arc leaving
the states; and (4) there is both an n-gram arc labeled
with w and a backoff arc leaving the states. In cases
(1) and (2), there is only one possible transition to
take in either M or M ?, and based on the algorithm
for construction of M ? given in Section 3.2, these
transitions will point to sj and s?j respectively. Case
(3) leads to failure of intersection with either ma-
chine. This leaves case (4) to consider. In M , since
there is a transition leaving state si labeled with w,
3
the backoff arc, which is a failure transition, can-
not be traversed, hence the destination of the n-gram
arc sj will be the next state in p. However, in M ?,
both the n-gram transition labeled with w and the
backoff transition, now labeled with , can be tra-
versed. What we will now prove is that the shortest
path through M ? cannot include taking the backoff
arc in this case.
In order to emit w by taking the backoff arc out
of state s?i, one or more backoff () transitions must
be taken, followed by an n-gram arc labeled with
w. Let k be the order of the history represented
by state s?i, hence the cost of the first backoff arc
is ?(n? k)?,? log(?s?i)? in our semiring. If we
traverse m backoff arcs prior to emitting the w,
the first dimension of our accumulated cost will be
m(n? k+ m?12 )?, based on our algorithm for con-
struction of M ? given in Section 3.2. Let s?l be the
destination state after traversing m backoff arcs fol-
lowed by an n-gram arc labeled with w. Note that,
by definition, m ? k, and k ? m + 1 is the or-
der of state s?l. Based on the construction algo-
rithm, the state s?l is also reachable by first emit-
ting w from state s?i to reach state s
?
j followed by
some number of backoff transitions. The order of
state s?j is either k (if k is the highest order in the
model) or k + 1 (by extending the history of state
s?i by one word). If it is of order k, then it will re-
quire m? 1 backoff arcs to reach state s?l, one fewer
than the path to state s?l that begins with a back-
off arc, for a total cost of (m? 1)(n? k + m?12 )?
which is less than m(n? k + m?12 )?. If state
s?j is of order k + 1, there will be m backoff
arcs to reach state s?l, but with a total cost of
m(n? (k + 1) + m?12 )? = m(n? k +
m?3
2 )?
which is also less than m(n? k + m?12 )?. Hence
the state s?l can always be reached from s
?
i with a
lower cost through state s?j than by first taking the
backoff arc from s?i. Therefore the shortest path on
M ? must follow s?0...s
?
is
?
j . 2
This completes the proof.
5 Experimental Comparison of , ? and
?T, T ? encoded language models
For our experiments we used lattices derived from a
very large vocabulary continuous speech recognition
system, which was built for the 2007 GALE Ara-
bic speech recognition task, and used in the work
reported in Lehr and Shafran (2011). The lexico-
graphic semiring was evaluated on the development
set (2.6 hours of broadcast news and conversations;
18K words). The 888 word lattices for the develop-
ment set were generated using a competitive base-
line system with acoustic models trained on about
1000 hrs of Arabic broadcast data and a 4-gram lan-
guage model. The language model consisting of
122M n-grams was estimated by interpolation of 14
components. The vocabulary is relatively large at
737K and the associated dictionary has only single
pronunciations.
The language model was converted to the automa-
ton topology described earlier, and represented in
three ways: first as an approximation of a failure
machine using epsilons instead of failure arcs; sec-
ond as a correct failure machine; and third using the
lexicographic construction derived in this paper.
The three versions of the LM were evaluated by
intersecting them with the 888 lattices of the de-
velopment set. The overall error rate for the sys-
tems was 24.8%?comparable to the state-of-the-
art on this task1. For the shortest paths, the failure
and lexicographic machines always produced iden-
tical lattices (as determined by FST equivalence);
in contrast, 81% of the shortest paths from the ep-
silon approximation are different, at least in terms
of weights, from the shortest paths using the failure
LM. For full lattices, 42 (4.7%) of the lexicographic
outputs differ from the failure LM outputs, due to
small floating point rounding issues; 863 (97%) of
the epsilon approximation outputs differ.
In terms of size, the failure LM, with 5.7 mil-
lion arcs requires 97 Mb. The equivalent ?T, T ?-
lexicographic LM requires 120 Mb, due to the dou-
bling of the size of the weights.2 To measure speed,
we performed the intersections 1000 times for each
of our 888 lattices on a 2993 MHz Intel R? Xeon R?
CPU, and took the mean times for each of our meth-
ods. The 888 lattices were processed with a mean
of 1.62 seconds in total (1.8 msec per lattice) us-
ing the failure LM; using the ?T, T ?-lexicographic
LM required 1.8 seconds (2.0 msec per lattice), and
is thus about 11% slower. Epsilon approximation,
where the failure arcs are approximated with epsilon
arcs took 1.17 seconds (1.3 msec per lattice). The
1The error rate is a couple of points higher than in Lehr and
Shafran (2011) since we discarded non-lexical words, which are
absent in maximum likelihood estimated language model and
are typically augmented to the unigram backoff state with an
arbitrary cost, fine-tuned to optimize performance for a given
task.
2If size became an issue, the first dimension of the ?T, T ?-
weight can be represented by a single byte.
4
slightly slower speeds for the exact method using the
failure LM, and ?T, T ? can be related to the over-
head of computing the failure function at runtime,
and determinization, respectively.
6 Conclusion
In this paper we have introduced a novel applica-
tion of the lexicographic semiring, proved that it
can be used to provide an exact encoding of lan-
guage model topologies with failure arcs, and pro-
vided experimental results that demonstrate its ef-
ficiency. Since the ?T, T ?-lexicographic semiring
is both left- and right-distributive, other optimiza-
tions such as minimization are possible. The par-
ticular ?T, T ?-lexicographic semiring we have used
here is but one of many possible lexicographic en-
codings. We are currently exploring the use of a
lexicographic semiring that involves different semir-
ings in the various dimensions, for the integration of
part-of-speech taggers into language models.
An implementation of the lexicographic semir-
ing by the second author is already available as
part of the OpenFst package (Allauzen et al, 2007).
The methods described here are part of the NGram
language-model-training toolkit, soon to be released
at opengrm.org.
Acknowledgments
This research was supported in part by NSF Grant
#IIS-0811745 and DARPA grant #HR0011-09-1-
0041. Any opinions, findings, conclusions or recom-
mendations expressed in this publication are those of
the authors and do not necessarily reflect the views
of the NSF or DARPA. We thank Maider Lehr for
help in preparing the test data. We also thank the
ACL reviewers for valuable comments.
References
Cyril Allauzen, Mehryar Mohri, and Brian Roark. 2003.
Generalized algorithms for constructing statistical lan-
guage models. In Proceedings of the 41st Annual
Meeting of the Association for Computational Linguis-
tics, pages 40?47.
Cyril Allauzen, Michael Riley, Johan Schalkwyk, Woj-
ciech Skut, and Mehryar Mohri. 2007. OpenFst: A
general and efficient weighted finite-state transducer
library. In Proceedings of the Twelfth International
Conference on Implementation and Application of Au-
tomata (CIAA 2007), Lecture Notes in Computer Sci-
ence, volume 4793, pages 11?23, Prague, Czech Re-
public. Springer.
Jonathan Golan. 1999. Semirings and their Applications.
Kluwer Academic Publishers, Dordrecht.
Werner Kuich and Arto Salomaa. 1986. Semirings,
Automata, Languages. Number 5 in EATCS Mono-
graphs on Theoretical Computer Science. Springer-
Verlag, Berlin, Germany.
Maider Lehr and Izhak Shafran. 2011. Learning a dis-
criminative weighted finite-state transducer for speech
recognition. IEEE Transactions on Audio, Speech, and
Language Processing, July.
Mehryar Mohri, Fernando C. N. Pereira, and Michael
Riley. 2002. Weighted finite-state transducers in
speech recognition. Computer Speech and Language,
16(1):69?88.
Mehryar Mohri. 2002. Semiring framework and algo-
rithms for shortest-distance problems. Journal of Au-
tomata, Languages and Combinatorics, 7(3):321?350.
Brian Roark and Richard Sproat. 2007. Computational
Approaches to Morphology and Syntax. Oxford Uni-
versity Press, Oxford.
Brian Roark, Murat Saraclar, and Michael Collins. 2007.
Discriminative n-gram language modeling. Computer
Speech and Language, 21(2):373?392.
5
Proceedings of the 2014 Workshop on Biomedical Natural Language Processing (BioNLP 2014), pages 38?44,
Baltimore, Maryland USA, June 26-27 2014.
c?2014 Association for Computational Linguistics
Detecting Health Related Discussions in Everyday Telephone
Conversations for Studying Medical Events in the Lives of Older Adults
Golnar Sheikhshab, Izhak Shafran, Jeffrey Kaye
Oregon Health & Science University
sheikhsh,shafrani,kaye@ohsu.edu
Abstract
We apply semi-supervised topic modeling
techniques to detect health-related discus-
sions in everyday telephone conversations,
which has applications in large-scale epi-
demiological studies and for clinical in-
terventions for older adults. The privacy
requirements associated with utilizing ev-
eryday telephone conversations preclude
manual annotations; hence, we explore
semi-supervised methods in this task. We
adopt a semi-supervised version of Latent
Dirichlet Allocation (LDA) to guide the
learning process. Within this framework,
we investigate a strategy to discard irrel-
evant words in the topic distribution and
demonstrate that this strategy improves
the average F-score on the in-domain task
and an out-of-domain task (Fisher corpus).
Our results show that the increase in dis-
cussion of health related conversations is
statistically associated with actual medi-
cal events obtained through weekly self-
reports.
1 Introduction
There has been considerable interest in under-
standing, promoting, and monitoring healthy
lifestyles among older adults while minimizing the
frequency of clinical visits. Longitudinal studies
on large cohorts are necessary, for example, to un-
derstand the association between social networks,
depression, dementia, and general health. In this
context, detecting discussions of health are impor-
tant as indicators of under-reported health events
in daily lives as well as for studying healthy so-
cial support networks. The detection of medical
events such as higher levels of pain or discom-
fort may also be useful in providing timely clin-
ical intervention for managing chronic illness and
thus promoting healthy independent living among
older adults.
Motivated by this larger goal, we develop and
investigate techniques for identifying conversa-
tions containing any health related discussion. We
are interested in detecting discussions about med-
ication with doctors, as well as conversations with
others, where among all different topics being dis-
cussed, subjects may also be complaining about
pain or changes in health status.
The privacy concerns of recording and analyz-
ing everyday telephone conversation prevents us
from manually transcribing and annotating con-
versations. So, we automatically transcribe the
conversations using an automatic speech recog-
nition system and look-up the telephone number
corresponding to each conversation as a heuristic
means of deriving labels. This technique is suit-
able for labeling a small subset of the conversa-
tions that are only sufficient for developing semi-
supervised algorithms and for evaluating the meth-
ods for analysis.
Before delving into our approach, we discuss
a few relevant and related studies in Section 2
and describe our unique naturalistic corpus in Sec-
tion 3. Given the restrictive nature of our labeled
in-domain data set, we are interested in a clas-
sifier that generalizes to the unlabeled data. We
evaluate the generalizability of the classifiers us-
ing an out-of-domain corpus. We adopt a semi-
supervised topic modeling approach to address
our task, and develop an iterative feature selec-
tion method to improve our classifier, as described
in Section 4. We evaluate the efficacy of our ap-
proach empirically, on the in-domain as well as an
out-of-domain corpus, and report results in Sec-
tion 5.
2 Related Work
The task of identifying conversations where health
is mentioned differs from many other tasks in topic
38
modeling because in this task we are interested in
one particular topic. A similar study is the work of
Prier and colleagues (Prier et al., 2011). They use
a set of predefined seed words as queries to gather
tweets related to tobacco or marijuana usage, and
then use LDA to discover related subtopics. Thus,
their method is sensitive to the seed words chosen.
One way to reduce the sensitivity to the manu-
ally specified seed words is to expand the set us-
ing WordNet. Researchers have investigated this
approach in sentiment analysis (Kim and Hovy,
2004; Yu and Hatzivassiloglou, 2003). However,
when expanding the seed word set using WordNet,
we need to be careful to avoid antonyms and words
that have high degree of linkage with many words
in the vocabulary. Furthermore, we can not ap-
ply such an approach for languages with poor re-
sources, where manually curated knowledge is un-
available. The other drawback of this approach is
that we can not use characteristics of the end task,
in our case health-related conversation retrieval,
to select the words. As an alternative method,
Han and colleagues developed an interactive sys-
tem where users selected the most relevant words
from a set, proposed by an automated system (Han
et al., 2009).
Another idea for expanding the seed words is
using the statistical information. Among statis-
tical methods, the simplest approach is to com-
pute pairwise co-occurrence with the seed words.
Li and Yamanishi ranked the words co-occurring
with the seed words according to information the-
oretic costs, and used the highest ranked words as
the expanded set (Li and Yamanishi, 2003). This
idea can be more effective when the co-occurrence
is performed over subsets instead, as in Hisamitsu
and Niwa?s work (Hisamitsu and Niwa, 2001).
However, it is computationally expensive to search
over subsets of words. Depending on the language
and task, heuristics might be applicable. An ex-
ample of this kind of approach is Zagibalov and
Carroll?s work on sentiment analysis in Chinese
(Zagibalov and Carroll, 2008).
Alternatively, we can treat the task of identify-
ing words associated with seed words as a cluster-
ing problem with the intuition that the seed words
are in the same cluster. An effective strategy to
cluster words into topics, is Latent Dirichlet Allo-
cation (LDA) (Blei et al., 2003) . However, LDA
is an unsupervised algorithm and the clustered top-
ics are not guaranteed to include the topic of inter-
est. The Seeded LDA, a variant of LDA, attempts
to address this problem by incorporating the seed
words as priors over the topics (Jagarlamudi et
al., 2012). However, the estimation procedure is
more complicated. Alternatively, in Topic LDA
(TLDA), a clever extension to LDA, Andrzejewski
and Zhu address this problem by fixing the mem-
bership of the words to valid topics (Andrzejewski
and Zhu, 2009). When the focus is on detecting
just one topic, as in our task, we can expand the
seed words more selectively using the small set of
labeled data and that is the approach adopted in
this paper.
3 Data
One interesting aspect of our study is the unique-
ness of our corpus, which is both naturalistic and
exhaustive. We recorded about 41,000 land-line
everyday telephone conversations from 56 volun-
teers, 65 years or older, over a period of approxi-
mately 6 to 12 months. Since these everyday tele-
phone conversations are private conversations, and
might include private information such as names,
telephone numbers, or banking information, we
assured the subjects that no one would listen to the
recorded conversations. Thus, we couldn?t manu-
ally transcribe the conversations; instead, we used
an Automatic Speech Recognition (ASR) system
that we describe here.
Automatic Speech Recognition System Con-
versations in our corpus were automatically tran-
scribed using an ASR system, which is structured
after IBM?s conversation telephony system (Soltau
et al., 2005). The acoustic models were trained
on about 2000 hours of telephone speech from
Switchboard and Fisher corpora (Godfrey et al.,
1992). The system has a vocabulary of 47K
and uses a trigram language model with about
10M n-grams, estimated from a mix of transcripts
and web-harvested data. Decoding is performed
in three stages using speaker-independent mod-
els, vocal-tract normalized models and speaker-
adapted models. The three sets of models are sim-
ilar in complexity with 4000 clustered pentaphone
states and 150K Gaussians with diagonal covari-
ances. Our system does not include discriminative
training and performs at a word error rate of about
24% on NIST RT Dev04 which is comparable to
state of the art performance for such systems. We
are unable to measure the performance of this rec-
ognizer on our corpus due to the stringent privacy
39
requirements mentioned earlier. Since both cor-
pora are conversational telephone speech and the
training data contains large number of conversa-
tions (2000 hours), we expect the performance of
our recognizer to be relatively close to results on
NIST benchmark.
Heuristically labeling a small subset of conver-
sations For training and evaluation purposes, we
need a labeled set of conversations; that is, a set
of conversations where we know whether or not
they contain health-related discussions. Since the
privacy concerns do not allow for manually label-
ing the conversations, we used reverse look-up ser-
vice in www.whitepages.com. We sent the
phone number corresponding to each conversation
(when available) to this website to obtain informa-
tion about the other end of the conversation. Based
of the information we got back from this web-
site, we labeled a small subset of the conversations
which fell into unambiguous business categories.
For example, we labeled the calls to ?hospital? and
?pharmacy? as health-related, and those to ?car re-
pair? and ?real estate? as non-health-related.
The limitations of the labeled set The labeled
set we obtained is small and restricted in type of
conversations. Since phone numbers are not avail-
able for many of the conversations we recorded,
and also because www.whitepages.com does
not return unambiguous information for many of
available phone numbers, we managed to label
only 681 conversations ? 275 health-related and
406 non-health-related. This labeled set has an-
other limitation: it contains conversations to busi-
ness numbers only. In reality however, we are in-
terested in the much larger set of conversations
between friends, relatives, and other members of
subjects? social support network. Thus, the gener-
alizability of the classifier we train is very impor-
tant.
Fisher Corpus To explicitly test the generaliz-
ability of our classifier, we use a second evaluation
set from Fisher corpus (Cieri et al., 2004). Fisher
corpus contains telephone conversations with pre-
assigned topics. There are 40 topics and only
one of them, illness, is health-related. We identi-
fied 338 conversations on illness, and sampled 702
conversations from the other 39 non-health topics.
Since we do not train on Fisher corpus, we call
it the out-of-domain task to apply our method on
Fisher corpus; as opposed to the in-domain task
which is to apply our method on the everyday tele-
phone conversations.
Extra information on subjects? health In the
everyday telephone conversations corpus, we also
have access to the subjects? weekly self-reports
on their medical status during the week indicating
medical events such as injury or going to emer-
gency room. We will use these pieces of infor-
mation to relate the health-related conversations to
actual medical events in the subjects? lives.
4 Method
4.1 Overview
As we explained in Section 3, we can label a small
set of conversations in the everyday telephone con-
versations corpus as health-related vs. non-health
related. Using this labeled set we can train a sup-
port vector machine (SVM) to classify the con-
versations. In absence of feature selection, the
conversations are represented by a vector of tf-idf
scores for every word in the vocabulary where tf-
idf is a score for measuring the importance of a
word in one document of a corpus. As we see in
Section 5, such a classifier doesn?t generalize to
the out-of-domain Fisher task (i.e. when we test
the classifier on Fisher data set, we do not get good
precision and recalls). Generalizability is impor-
tant in our case, especially because the data we use
for training is limited in number and the nature of
conversations.
One way to improve generalization is to per-
form feature selection. That is, instead of using
tf-idf scores for the whole vocabulary, we would
like to rely only on features relevant to detecting
the health topic. We propose a new way for feature
selection for retrieving documents containing in-
formation about a specific topic when there is only
a limited set of labeled documents available. The
idea is to pick a few words highly related to the
topic of interest as seed words and to use TLDA
(Andrzejewski and Zhu, 2009) to force those seed
words into one (for example, the first) topic. In our
task, the topic of interest is health. So, we choose
doctor, medicine, and pain ? often used while dis-
cussing health ? as our seed words. Topics in LDA
based methods such as TLDA are usually repre-
sented using the n most probable words; where n
is an arbitrary number. So, the first candidate sets
for expanding our seed words are the sets of 50
most probable words in the topic of health in dif-
40
ferent runs of TLDA. As our experiments reveal,
these candidate sets contain many words that are
unrelated to health . To solve this problem, we use
the small labeled set of conversations to filter out
the unrelated words.
Figure 1 shows the proposed iterative algo-
rithm. The algorithm starts with initializing the
seed words to doctor, medicine, and pain. Then,
in each iteration, TLDA performs semi-supervised
topic modeling and returns the 50 most probable
candidate words in the health topic. We select a
subset of these candidate words which, if added
to the seed words, would maximize the average of
precision and recall on the train set for a simple
classifier. This simple classifier marks a conver-
sion as health related if, and only if, it contains at
least one of the seed words. The algorithm termi-
nates when the subset selection is unable to add
a new word contributing to the average of preci-
sion and recall. The tf-idf vector for the expanded
set represents the conversations in the classifica-
tion process.
It is worth mentioning that we train TLDA using
all 41000 unlabeled conversations, and chose the
number of topics, K, to be 20.
5 Experiments
In all of our experiments, we trained SVM
classifiers, with different features, to detect the
conversations on health using the popular lib-
SVM (Chang and Lin, 2011) implementation. We
chose the parameters of the SVM using a 30-fold
cross-validated (CV) grid search over the training
data. We also used a 4-fold cross validation over
the labeled set of conversations to maximize the
use of the relatively small labeled set. That is, we
trained the feature selection algorithm on 3-folds
and tested the resulting SVM tested on the fourth.
In in-domain task we always report the average
performance across the folds.
Table 1 shows the results of our experiments
using different input features. We report on re-
call, precision and F-measure in in-domain and
out-of-domain (Fisher) task as well as on average
F-measure of the two. The justification for consid-
ering the average F-measure is that we want our
algorithm to work well on both in-domain corpus
and Fisher corpus since we need to make sure that
our classifier is generalizable (i.e. it works well on
Fisher) and it works well on the private and natu-
ral telephone conversations (i.e. the ones similar
Figure 1: Expanding the set of seed words: in each
iteration, the current seed words are forced into
the topic of health to guide TLDA towards finding
more health related words. The candidate set con-
sists of the 50 most probable words of the topic of
health in TLDA. We investigate the gain of adding
each word of the candidate set to the seed words by
temporarily adding it to the seed words and look-
ing at the average of precision and recall on the
training set for a classifier that classifies a conver-
sation as health-related if and only if it contains at
least one of the seed words. We select the words
that maximize this objective and add them to the
seed words until no other words contributes to the
average precision and recall.
to the in-domain corpus)
When using the full vocabulary, the in-domain
performance (the performance on the everyday
telephone conversations data) is relatively good
with 75.1% recall and 83.5% precision. But the
out-of-domain recall (recall on the Fisher data set)
is considerably low at 2.8%. Ideally, we want
a classifier that performs well in both domains.
Rows 2 to 5 can be seen as steps to get to such
a classifier.
The second row shows the performance of the
other extreme end of feature selection: the fea-
tures include the manually chosen words doctor,
medicine, and pain only. While this leads to very
good out-of-domain performance, the in-domain
recall has dropped considerably. We trained
TLDA 30 times, and selected the 50 most probable
words in the health topic. The third row in Table 1
shows the average performance of SVM when us-
ing the tf-idf of these sets of words as the feature
vector on in-domain and out-of-domain tasks. Us-
ing the 50 most probable words in health topic sig-
nificantly improves average F-score (71%) across
41
Recall Precision F-measure
Feature Words In-Domain Fisher In-Domain Fisher In-Domain Fisher Average
Full vocabulary
(no feature selection) 75.2 2.8 83.5 91.1 79.1 5.4 42.3
Initial words
(doctor, medicine, pain ) 45.1 69.2 94.8 94.5 61.1 79.9 70.5
50 most probable words in health
(average over 30 runs) 58.4 57.4 86.3 97.5 69.7 72.3 71.0
Words selected by our method
(average over 30 runs) 56.1 66.5 91.0 95.5 69.4 78.4 73.9
Union of all selected words
(across 30 runs) 67.7 69.4 87.8 95.1 76.5 80.2 78.3
Table 1: Performance of SVM classifiers using different feature selection methods. The In-Domain task
involves the everyday telephone conversations corpus. We call Fisher corpus out of domain, because no
example of this corpus was used in training.
both tasks over using the full vocabulary (42.3%)
but it is clear that this is only due to improvement
in out-of-domain task. Table 2 shows one set of
the 50 most probable words in health topic,the re-
sult of one run of TLDA. Evidently, these words
contain many irrelevant words. This is the motiva-
tion for our iterative algorithm.
Next, we evaluate the performance of our iter-
ative algorithm. The fourth row in Table 1 shows
the average performance of SVM using expanded
seed words that our algorithm suggested in 30
runs. Our algorithm improves the average F-score
by 3% comparing to the standard TLDA. This is
due to a 5% improvement in out-of-domain task
as opposed to a 0.3% performance decrease in in-
domain task.
Since our algorithm has a probabilistic topic
modeling component (i.e. TLDA), different runs
lead to different sets of expanded seed words. We
extract a union of all the words chosen over 30
runs and evaluate the performance of SVM using
this union set. This improves the performance of
our method further to achieve the best average F-
score of 78.3%, which is an 85% improvement
over using the SVM with full vocabulary. It is im-
portant to notice that the in-domain performance is
still lower than the full-vocabulary baseline by less
than 3% while the out-of-domain performance is
the best obtained. Once again, we are more inter-
ested in the average F-measure because we need
our algorithm to generalize well (work well on
out-of-domain corpus) and to work well on natural
private conversations (on the conversations similar
to the on-domain corpus).
Our last experiment tests statistical associa-
tion between health-related discussions in every-
day telephone conversations, and actual medical
pain, medicine, appointment, medical, doc-
tors, emergency, prescription, contact, med-
ication, dial, insurance, pharmacy, schedule,
moment, reached, questions, services, surgery,
telephone, record, appointments, options, ad-
dress, patient, advice, quality, tuesday, posi-
tion, answered, records, wednesday, therapy,
healthy, correct, department, ensure, numbers,
act, doctor, personal, test, senior, nurse, plan,
kaiser
Table 2: 50 most probable words in the topic of
health returned by one run of TLDA. The bold
words are the ones are hand-picked.
events in older adults. As mentioned in Section 3,
we have access to weekly self-reports on medical
events for subjects? in everyday telephone conver-
sations corpus. We used our best classifier, the
SVM with union of expanded seed words, to clas-
sify all the conversations in our corpus into health-
containing and health-free conversations. We then
mark each conversation as temporally near a med-
ical event if a reported medical event occurred
within a 3-week time window. We chose a 3-week
window to allow for one report before and after
the event.
Table 3 shows the number of conversations in
different categories. At first glance it might seem
like the number of false positives or false nega-
tives is quite large but we should notice that be-
ing near a medical event is not the ground truth
here. We just want to see if there is any associa-
tion between occurrence of health-related conver-
sations and occurrence of an actual medical event
in lives of our subjects. We can see that 90.9%
42
of the conversations are classified as health-related
but this percentage is slightly different for con-
versations near medical events(91.5%) vs. for the
other conversations (89.1). This slight difference
is significant according to ?
2
test of independence
(?
2
(df = 1, N = 47288) = 61.17, p < 0.001).
near a Classified as
medical event health-related non-health-related
yes 1348 11067
no 2964 31909
Table 3: Number of telephone conversations in
different categories. Each conversation is consid-
ered near a medical even if and only if there is
at least one self-report in a window of 3 weeks
around its date. Being near a medical event does
not reveal the true nature of the conversation and
thus is not the ground truth. So, there are no false
positive, true positive, etc. in this table.
6 Conclusions
In this paper, we investigated the problem of iden-
tifying conversations with any mention of health.
The private nature of our everyday telephone con-
versations corpus poses constraints on manual
transcription and annotation. Looking up phone
numbers associated with business calls, we labeled
a small set of conversations when the other end
was a business clearly related or unrelated to the
health industry. However, the labeled set is not
large enough for training a robust classifier. We
developed a semi-supervised iterative method for
selecting features, where we learn a distribution
of words on health topic using TLDA, and sub-
sequently filter irrelevant words iteratively. We
demonstrate that our method generalizes well and
improves the average F-score on in-domain and
out-of-domain tasks over two baselines, using full
vocabulary without feature selection or feature se-
lection using TLDA alone. In our task, the gener-
alization of the classifier is important since we are
interested in detecting not only conversations on
health with business (the annotated examples) but
also with others in subjects? social network. Using
our classifier, we find a significant statistical as-
sociation between the occurrence of conversations
about health and the occurrence of self-reported
medical events.
Acknowledgments
This research was supported in part by NIH Grants
1K25AG033723, and P30 AG008017, as well as
by NSF Grants 1027834, and 0964102. Any opin-
ions, findings, conclusions or recommendations
expressed in this publication are those of the au-
thors and do not necessarily reflect the views of the
NIH. We thank Nicole Larimer for help in collect-
ing the data, Maider Lehr for testing the data col-
lection devices and Katherine Wild for early dis-
cussions on this project. We are grateful to Brian
Kingsbury and his colleagues for providing us ac-
cess to IBM?s attila software tools.
References
David Andrzejewski and Xiaojin Zhu. 2009. La-
tent dirichlet allocation with topic-in-set knowledge.
In Proceedings of the NAACL HLT 2009 Workshop
on Semi-Supervised Learning for Natural Language
Processing, SemiSupLearn ?09, pages 43?48.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet allocation. Journal of Ma-
chine Learning Research, 3:993?1022.
C.-C. Chang and C.-J. Lin. 2011. Libsvm : a library
for support vector machines. ACM Transactions on
Intelligent Systems and Technology, 2.
Christopher Cieri, David Miller, and Kevin Walker.
2004. The fisher corpus: a resource for the next
generations of speech-to-text. In LREC, volume 4,
pages 69?71.
John J Godfrey, Edward C Holliman, and Jane Mc-
Daniel. 1992. Switchboard: Telephone speech cor-
pus for research and development. In Acoustics,
Speech, and Signal Processing, 1992. ICASSP-92.,
1992 IEEE International Conference on, volume 1,
pages 517?520. IEEE.
Hong-qi Han, Dong-Hua Zhu, and Xue-feng Wang.
2009. Semi-supervised text classification from un-
labeled documents using class associated words. In
Computers & Industrial Engineering, 2009. CIE
2009. International Conference on, pages 1255?
1260. IEEE.
Toru Hisamitsu and Yoshiki Niwa. 2001. Topic-word
selection based on combinatorial probability. In NL-
PRS, volume 1, page 289.
Jagadeesh Jagarlamudi, Hal Daum?e III, and Raghaven-
dra Udupa. 2012. Incorporating lexical priors into
topic models. In Proceedings of the 13th Confer-
ence of the European Chapter of the Association for
Computational Linguistics, pages 204?213. Associ-
ation for Computational Linguistics.
43
Soo-Min Kim and Eduard Hovy. 2004. Determin-
ing the sentiment of opinions. In Proceedings of
the 20th international conference on Computational
Linguistics, page 1367. Association for Computa-
tional Linguistics.
Hang Li and Kenji Yamanishi. 2003. Topic analysis
using a finite mixture model. Information process-
ing & management, 39(4):521?541.
Kyle W. Prier, Matthew S. Smith, Christophe Giraud-
Carrier, and Carl L. Hanson. 2011. Identifying
health-related topics on twitter: an exploration of
tobacco-related tweets as a test topic. In Proceed-
ings of the 4th international conference on Social
computing, behavioral-cultural modeling and pre-
diction, pages 18?25.
Hagen Soltau, Brian Kingsbury, Lidia Mangu, Daniel
Povey, George Saon, and Geoffrey Zweig. 2005.
The ibm 2004 conversational telephony system for
rich transcription. In Proc. ICASSP, volume 1,
pages 205?208.
Hong Yu and Vasileios Hatzivassiloglou. 2003. To-
wards answering opinion questions: Separating facts
from opinions and identifying the polarity of opinion
sentences. In Proceedings of the 2003 conference on
Empirical methods in natural language processing,
pages 129?136. Association for Computational Lin-
guistics.
Taras Zagibalov and John Carroll. 2008. Automatic
seed word selection for unsupervised sentiment clas-
sification of chinese text. In Proceedings of the
22nd International Conference on Computational
Linguistics-Volume 1, pages 1073?1080. Associa-
tion for Computational Linguistics.
44
