 Location Normalization for Information Extraction* 
 
 
Huifeng Li, Rohini K. Srihari, Cheng Niu, and Wei Li 
Cymfony Inc. 
600 Essjay Road, Williamsville, NY 14221, USA 
(hli, rohini, cniu, wei)@cymfony.com 
 
 
Abstract  
 
Ambiguity is very high for location names. For 
example, there are 23 cities named ?Buffalo? in 
the U.S.  Country names such as ?Canada?, 
?Brazil? and ?China? are also city names in the 
USA. Almost every city has a Main Street or 
Broadway. Such ambiguity needs to be handled 
before we can refer to location names for 
visualization of related extracted events. This 
paper presents a hybrid approach for location 
normalization which combines (i) lexical 
grammar driven by local context constraints, (ii) 
graph search for maximum spanning tree and 
(iii) integration of semi-automatically derived 
default senses. The focus is on resolving 
ambiguities for the following types of location 
names: island, town, city, province, and country. 
The results are promising with 93.8% accuracy 
on our test collections. 
 
1 Introduction 
The task of location normalization is to identify 
the correct sense of a possibly ambiguous 
location Named Entity (NE). Ambiguity is very 
serious for location NEs. For example, there are 
23 cities named ?Buffalo?, including the city in 
New York State and in Alabama State. Even 
country names such as ?Canada?, ?Brazil?, and 
?China? are also city names in the USA. Almost 
every city has a Main Street or Broadway. Such 
ambiguity needs to be properly handled before 
converting location names into some normal 
form to support entity profile construction, event 
merging and visualization of extracted events on 
 
*This work was partly supported by a grant from the 
Air Force Research Laboratory?s Information 
Directorate (AFRL/IF), Rome, NY, under contract 
F30602-00-C-0090.  
a map for an Information Extraction (IE) System.  
Location normalization is a special 
application of word sense disambiguation 
(WSD). There is considerable research on WSD. 
Knowledge-based work, such as (Hirst, 1987; 
McRoy, 1992; Ng and Lee, 1996) used 
hand-coded rules or supervised machine learning 
based on annotated corpus to perform WSD. 
Recent work emphasizes corpus-based 
unsupervised approach (Dagon and Itai, 1994; 
Yarowsky, 1992; Yarowsky, 1995) that avoids 
the need for costly truthed training data. Location 
normalization is different from general WSD in 
that the selection restriction often used for WSD 
in many cases is not sufficient to distinguish the 
correct sense from the other candidates.  
For example, in the sentence ?The White 
House is located in Washington?, the selection 
restriction from the collocation ?located in? can 
only determine that ?Washington? should be a 
location name, but is not sufficient to decide the 
actual sense of this location. Location 
normalization depends heavily on co-occurrence 
constraints of geographically related location 
entities mentioned in the same discourse. For 
example, if ?Buffalo?, ?Albany? and ?Rochester? 
are mentioned in the same document, the most 
probable senses of ?Buffalo?, ?Albany? and 
?Rochester? should refer to the cities in New 
York State. There are certain fixed 
keyword-driven patterns from the local context, 
which decide the sense of location NEs. These 
patterns use keywords such as ?city?, ?town?, 
?province?, ?on?, ?in? or other location names. For 
example, the pattern ?X + city? can determine 
sense tags for cases like ?New York city?; and the 
pattern ?City + comma + State? can disambiguate 
cases such as ?Albany, New York? and 
?Shanghai, Illinois?. In the absence of these 
patterns, co-occurring location NEs in the same 
discourse can be good evidence for predicting the 
most probable sense of a location name.  
  
Unrestricted
text Tokenizer
POS Tagging
Shallow Parsing
Coreference
Semantic Parsing
Pragmatic Filter
NE Tagging
LocNZ
Profile
Event
Question 
Answering
O
ut
pu
t(I
E 
D
at
ab
as
e)
Intelligent
Browsing
Summari-
zationVisualization
Kernel IE Modules Linguistic Modules
Note: NE: name entity tagging; LocNZ: location normalization
Application Modules
O
ut
pu
t(I
E 
D
at
ab
as
e)
 
Figure 1. InfoXtract system architecture 
 
 
Event type: Job change
Keyword: hired
Company : Microsoft
Person in: Mary
Position: sales person
Location: Beijing
Date: January 1
Event type: Job change
Keyword: replaced
Company: Microsoft
Person out : he(Dick)
Position: sales person
Location: Beijing
Date: Yesterday
Event 1 Event 2
Event type: Job change
Keyword: hired
Keyword: replaced
Company: Microsoft
Person in: Mary
Person out : he(Dick)
Position: salesperson
Location: <LocationProfile101>
Date: 2000-01-01
Figure 2. Location verification in Event 
merging. 
For choosing the best matching sense set 
within a document, we simply construct a graph 
where each node represents a sense of a location 
NE, and each edge represents the relationship 
between two location name senses. A graph 
spanning algorithm can be used to select the best 
senses from the graph. If there exist nodes that 
cannot be resolved in this step, we will apply 
default location senses that were extracted 
semi-automatically by statistical processing. The 
location normalization module, or ?LocNZ?, is 
applied after the NE tagging module in our 
InfoXtract IE system as shown in Figure 1. 
This paper focuses on how to resolve 
ambiguity for the names of island, town, city, 
province, and country. Three applications of 
LocNZ in Information Extraction are illustrated 
in Section 2. Section 3 presents location sense 
identification using local context; Section 4 
describes disambiguation process using 
information within a document through graph 
processing; Section 5 shows how to 
semi-automatically collect default senses of 
locations from a corpus; Section 6 presents an 
algorithm for location normalization with 
experimental results. The summary and 
conclusions are given in Section 7. Sample text 
and the results of location tagging are given in the 
Appendix. 
 
2   Applications of Location Normalization 
Several applications are enabled through location  
normalization. 
? Event extraction and merging 
Event extraction is an advanced IE task. 
Extracted events can be merged to provide key 
content in a document. The merging process 
consists of several steps including checking 
information compatibility such as checking 
synonyms, name aliases and co-reference of 
anaphors, time and location normalization. Two 
events cannot be merged if there is a conflicting 
condition such as time and location. Figure 2 
shows an example of event merging where the 
events occurred in Microsoft at Beijing, not in 
Seattle. 
? Event visua lization 
Visualization applications can illustrate where an 
event occurred with support of location 
normalization. Figure 3 demonstrates a 
visualized event on a map based on the 
normalized location names associated with the 
events. The input to visualization consists of 
extracted events from a news story pertaining to 
Julian Hill?s life. The arrow points to the city 
where the event occurred. 
? Entity profile construction 
An entity profile is an information object for 
entities such as person, organization and location.  
It is defined as an Attribute Value Matrix (AVM)  
to represent key aspects of information about 
entities, including their relationships with other 
entities. Each attribute slot embodies some 
  
Event type: <Die: Event 200>
Who:       <Julian Werver Hill: PersonProfile 001>
When:     1996-01-0 7
Where :     <Loca t ionPro f i l e103>
Preceding_event:  <hospitalize: Event 260>
Subsequent_event:  <bury:  Event  250>
Event Visualization
;  ; 
; ; 
Predicate: Die
Who: Julian Werner Hill
When:
Where: <LocationProfile 103>
Hockessin, Delaware, USA,
19707,75.688873,39.77604
1996-01-07
Figure 3. Event visulization with location. 
information about the entity in one aspect. Each 
relationship is represented by an attribute slot in 
the Profile AVM.  Sample Profile AVMs 
involving the reference of locations are 
illustrated below. 
<PersonProfile 001> :: 
Name:   Julian Werner Hill  
Position: Research chemist 
Age:        91  
Birth-place: <LocationProfile100> 
Affiliation:  Du Pont Co.  
Education:  MIT  
 
<LocationProfile 100> :: 
Name:   St. Louis  
State:    Missouri 
Country: United States of America  
Zipcode:  63101 
Lattitude : 90.191313  
Longitude:  38.634616 
Related_profiles: <PersonProfile 001>  
 
Several other applications such as question 
answering and classifying documents by location 
areas can also be enabled through LocNZ. 
3 Lexical Grammar Processing in 
Local Context 
Named Entity tagging systems (Krupka and 
Hausman, 1998; Srihari et al, 2000) attempt to 
tag information such as names of people, 
organizations, locations, time, etc. in running 
text.  In InfoXtract, we combine Maximum 
Entropy Model (MaxEnt) and Hidden Markov 
Model for NE tagging (Shrihari et al,, 2000). The 
Maximum Entropy Models incorporate local 
contextual evidence in handling ambiguity of 
information from a location gazetteer. In the 
Tipster Location gazetteer used by InfoXtract, 
there are a lot of common words, such as I, A, 
June, Friendship , etc. Also, there is large overlap 
between person names and location names, such 
as Clinton, Jordan, etc. Using MaxEnt, systems 
learn under what situation a word is a location 
name, but it is very difficult to determine the 
correct sense of an ambiguous location name. If a 
word can represent a city or state at the same 
time, such as New York or Washington, it is 
difficult to decide if it refers to city or state. The 
NE tagger in InfoXtract only assigns the location 
super-type tag NeLOC to the identified location 
words and leaves the task of location sub-type 
tagging such as NeCITY or NeSTATE and its 
normalization to the subsequent module LocNZ. 
For representation of LocNZ results, we add 
an unique zip code and position information 
that is longitude and latitude for the cities for 
event visualization. 
The first step of LocNZ is to use local context 
that is the co-occurring words around a location 
name. Local context can be a reliable source in 
deciding the sense of a location. The following 
are most commonly used patterns for this 
purpose.  
 
(1) location+comma+NP(headed by ?city?)  
e.g. Chicago, an old city  
(2) ?city of? +location1+comma+location2 
e.g. city of Albany, New York 
(3) ?city of? +location 
(4) ?state of?+location  
(5) location1+{,}+location2+{,}+location3 
e.g. (i) Williamsville, New York, USA 
       (ii) New York, Buffalo,USA 
     (6) {on, in}+location 
 e.g. on Strawberry ? NeIsland 
 in Key West ? NeCity 
Patterns (1) , (3), (4) and (6) can be used to decide 
if the location is a city, a state or an island, while 
patterns (2) and (5) can be used to determine both 
the sub-tag and its sense. These patterns are 
implemented in  our finite state transducer 
formalism. 
 4 Maximum Spanning Tree 
Calculation with Global Information 
Although local context can be reliable evidence 
for disambiguating location senses, there are still 
many cases which cannot be captured by the 
above patterns. Information in the entire 
document (i.e. discourse information) should be 
considered. Since all location names in a 
document have meaning relationships among 
them, a way to represent the best sense 
combination within the document is needed.  
The LocNZ process constructs a weighted 
graph where each node represents a location 
sense, and each edge represents similarity weight 
between location names. Apparently there will be 
no links among the different senses of a location 
name, so the graph will be partially complete. We 
calculate the maximum weight spanning tree 
(MaxST) using Kruskal?s MinST algorithm 
(Cormen et al 1990). The nodes on the 
resulting MaxST are the most promising senses 
of the location names.  
We define three criteria for similarity weight 
assignment between two nodes:  
(1) More weight will be given to the edge 
between a city and the province (or the 
country) to which it belongs.  
(2) Distance between location names mentioned 
in the document is taken into consideration. 
The shorter the distance, the more we assign  
the weight between the nodes.  
(3) The number of word occurrences affects the 
weight calculation. For multiple mentions of 
a location name, only one node will be 
represented in the graph. We assume that all 
the same location mentions have the same 
meaning in a document following one sense 
per discourse principle (Gale, Church, and 
Yarowsky, 1992).  
When calculating the weight between two 
location names, the predefined similarity values 
shown in Table 1, the number of location name 
occurrences and the distance between them in a 
text are taken into consideration. After selecting 
each edge, the senses that are connected will be 
chosen, and other senses of the same location 
name will be discarded so that they will not be 
considered again in the MaxST calculation. A 
weight value is calculated with equation (1), 
where sij indicate the jth sense of wordi, a reflects 
the number of location name occurrences in a 
text, and b refers to the distance between the two 
location names. Figure 4 shows the graph for 
calculating MaxST. Dots in a circle mean the 
number of senses of a location name. 
Table 1. Similarity value sim(si,si) between 
location sense pairs. 
Loc1 Loc2 Relationship  Sim(si,si) 
C1 P1 P1 includes C1  5 
IL Ctr1 Ctr1 includes IL 5 
C1 Ctr1 Ctr1 is direct parent 5 
C1 C2 C1 and C2 in same 
province/state 
3 
C1 C2 C1 and C2 in same 
country 
2 
C1 P1 C1 and P1 are in same 
country but C1 is not 
in P1 
2 
C1 Ctr1 Ctr1 is not a direct 
parent of C1  
3 
P1 Ctr1 P1 is in Ctr1 1 
P1 P2 P1 and P2 in same 
country 
1 
Loc1 Loc2 Loc1 and Loc2 are two 
sense nodes of the 
same location name 
-? 
Loc1 Loc2 Other cases 0 
Note: Ci: city; Pi: province/state; IL: island; Ctri: 
country; Loci: location. 
),(),(
/))()((),(
/),(),(),(),(
jijkij
jijkij
jkijjkijjkijjkij
wwdistss
numAllwnumwnumss
numAllsssssssimssScore
=
+=
-+=
b
a
ba
                          (1) 
5 Default Sense Extraction 
In our experiments, we found that the system 
performance suffers greatly from the lack of 
lexical information on default senses. For 
example, people refer to ?Los Angeles? as the 
city at California more than the city in 
Philippines, Chile, Puerto Rico, or the city in 
Texas in the USA. This problem becomes a 
bottleneck in the system performance. As 
mentioned before, a location name usually has a 
dozen senses that need sufficient evidence in a 
document for selecting one sense among them. 
 Canada
{Kansas,
Kentucky,
Country}
Vancouver
{British Columbia
Washington
port in USA
Port in Canada}
New York 
{Prov in USA,
New York City,
?}
Toronto
(Ontorio ,
New South Wales,
Illinois,
?}
Charlottetown
{Prov in USA,
New York City,
?}
Prince Edward Island
{Island in Canada,
Island in South Africa,
Province in Canada}
Quebec
(city in Quebec ,
Quebec Prov,
Connecticut,
?}
3*4 l ines
2*3 lines
4*11 lines
11*10 lines
3*10 lines
8*3 l ines
2*8 lines
2*43*11
3*10
3*33*8
2*10
2*3
8*4
8*11
8*10
8*4
10*4
3*11
 
Figure 4. Graph for calculating maximum weight 
spanning tree. 
But in many cases there is no explicit clue in a 
document, so the system has to choose the default 
senses that most people may refer to under 
common sense.  
The Tipster Gazetteer (http://crl.nmsu.edu/ 
cgi-bin/Tools/CLR/clrcat) used in our system has 
171,039 location entries with 237,916 total 
senses that cover most location names all over the 
world. Each location in the gazetteer may have 
several senses. Among them 30,711 location 
names have more than one sense. Although it has 
ranking tags on some location entries, a lot of 
them have no tags attached or the same rank is 
assigned to the entries of the same name. 
Manually calculating the default senses for over 
30,000 location names will be difficult and it is 
subject to inconsistency due to the different 
knowledge background of the human taggers. To 
solve this problem in calculating the default 
senses of location names, we propose to extract 
the knowledge from a corpus using statistical 
processing method.  
With the TREC-8 (Text Retrieval Conference) 
corpus, we can only extract default senses for 
1687 location names, which cannot satisfy our 
requirement. This result shows that the general 
corpus is not sufficient to suit our purpose due to 
the serious ?data sparseness? problem. Through a 
series of experiments, we found that we could 
download highly useful information from Web 
search engines such as Google, Yahoo, and 
Northern Light by searching ambiguous location 
names in the Gazetteer. Web search engines can 
provide the closest content by their built-in 
ranking mechanisms. Among those engines, we 
found that the Yahoo search engine is the best 
one for our purpose.  We wrote a script to 
download web-pages from Yahoo! using each 
ambiguous location name as a search string.   
In order to derive default senses automatically 
from the downloaded web-pages, we use the 
similarity features and scoring values between 
location-sense pairs described in Section 3. For 
example, if ?Los Angeles? co-occurs with 
?California? in the same web-page, then its sense 
will be most probably set to the city in California 
by the system.   Suppose a location word w has 
several city senses si: Sense(w) indicates the 
default sense of w; sim(wi,xjk) means the 
similarity value between two senses of the  word 
w and the j th co-occuring word xj; num(w) is the   
number of w in the document, and NumAll is the 
total number of locations.  a  is a parameter that 
reflects the importance of the co-occurring 
location names and is determined empirically. 
The default sense of w is wi that maximizes the 
similarity value with all co-occurring location 
names. The maximum similarity should be larger 
than a threshold to keep meaningful default 
senses. The threshold can be determined 
empirically through experimentation. 
)))(/())((*                
*),((maxmax)(
1 11
wnumNumAllxnum
xssimwSense
j
n
j
jkipkmi
-
= ?
= ????
a
(2) 
 
For each of 30,282 ambiguous location names, 
we used the name itself as search term in Yahoo 
to download its corresponding web-page. The 
system produced default senses for 18,446 
location names. At the same time, it discarded the 
remaining location names because the 
corresponding web-pages do not contain 
sufficient evidence to reach the threshold. We  
observed that the results reflect the correct senses 
in most cases, and found that the discarded 
location names have low references in the search 
results of other Web search engines. This means 
they will not appear frequently in text, hence 
minimal impact on system performance. We 
manually modified some of the default sense 
results based on the ranking tags in the Tipster 
Gazetteer and some additional information on 
population of the locations in order to consolidate 
the default senses.  
 6 Algorithm and Experiment 
With the information from local context, 
discourse context and the knowledge of default 
senses, the location normalization process turned 
out to be very efficient and precise. The 
processing flow is divided into 5 steps: 
Step 1. Look up the location gazetteer to 
associate candidate senses for each location NE; 
Step 2. Call the pattern matching sub-module to 
resolve the ambiguity of the NEs involved in 
local patterns like ?Williamsville, New York, 
USA? to retain only one sense for the NE as early 
as possible; 
Step 3. Apply the ?one sense per discourse? 
principle for each disambiguated location name 
to propagate the selected sense to its other 
occurrences within a document; 
Step 4. Call the global sub-module, which is a 
graph search algorithm, to resolve the remaining 
ambiguities; 
Step 5. If the decision score for a location name is 
lower than a threshold, we choose a default sense 
of that name as a result. 
For evaluating the system performance, 53 
documents from a travel site 
(http://www.worldtravelguide.net/navigate/region/na
m.asp), CNN News and New York Times are 
used. Table 2 shows some sample results from 
our test collections. For results shown in Column 
4, we first applied default senses of location 
names available from the Tipster Gazetteer in 
accordance with the rules specified in the 
gazetteer document. If there is no ranking value 
tagged for a location name, we select the first 
sense in the gazetteer as its default. This 
experiment showed accuracy of 42%. For 
Column 5, we tagged the corpus with default 
senses we derived with the method described in 
section 5, and found that it can resolve 78% 
location name ambiguity. Column 6 in Table 2 is 
the result of our LocNZ system using the 
algorithm described above as well as default 
senses we derived. The system showed promising 
results with 93.8% accuracy.  
7 Conclusion 
This paper presents a method of location 
normalization for information extraction with 
experimental results and its applications. In 
future work, we will integrate a expanded 
location gazetteer including names of landmarks, 
mountains and lakes such as Holland Tunnel (in 
New York, not in Holland) and Hoover Dam (in 
Arizona, not in Alabama), to enlarge the system 
coverage, and adjust the scoring weight given in 
Table 1 for better normalization results. Using 
context information other than location names 
can be a subtask for determining specific location 
names such as bridge or area names. 
Table 2. Experimental evaluation for location name normalization. 
 
Correctly tagged locations Document Type No. of 
Ambigu-
ous Loc 
Names 
No. of 
Ambigu-
ous 
senses  
With Tipster 
Gazetteer 
default sense 
and rule only 
With LocNZ 
default senses 
only 
LocNZ 
Precision 
(%) of 
LocNZ  
California Intro. 26 326 13 18 25 96 
Canada Intro. 14 75 13 13 14 100 
Florida Intro 22 221 10 18 20 90 
Texas Intro. 13 153 9 11 12 93 
CNN News 1 27 486 10 23 25 92 
CNN News 2 26 360 10 22 24 92 
CNN News 3 16 113 4 10 14 87.5 
New York Times 1 8 140 1 7 8 100 
New York Times 2 10 119 2 7 10 100 
New York Times 3 18 218 5 13 17 94 
Total 180 2211 77 (42%) 142 (78%) 169 (93.8%)  93.8 
 
 8 Acknowledgement 
The authors wish to thank Carrie Pine of AFRL 
for supporting this work.  Other members of 
Cymfony?s R&D team, including Sargur N. 
Srihari, have also contributed in various ways. 
References 
Cormen, Thomas H., Charles E. Leiserson, and 
Ronald L. Rivest. 1990. Introduction to 
Algorithm. The MIT Press, pp. 504-505. 
Dagon, Ido and Alon Itai. 1994. Word Sense 
Disambiguation Using a Second Language 
Monolingual Corpus. Computational 
Linguistics, Vol.20, pp. 563-596. 
Gale, W.A., K.W. Church, and D. Yarowsky. 
1992. One Sense Per Discourse. In 
Proceedings of the 4th DARPA Speech and 
Natural Language Workshop. pp. 233-237. 
Hirst, Graeme. 1987. Semantic Interpretation 
and the Resolution of Ambiguity. Cambridge 
University Press, Cambridge. 
Krupka, G.R. and K. Hausman.  1998.  IsoQuest 
Inc.: Description of the NetOwl (TM) Extractor 
System as Used for MUC-7.  Proceedings of 
MUC.  
McRoy, Susan W. 1992. Using Multiple 
Knowledge Sources for Word Sense 
Discrimination. Computational Linguistics, 
18(1): 1-30. 
Ng, Hwee Tou and Hian Beng Lee. 1996. 
Integrating Multiple Knowledge Sources to 
Disambiguate Word Sense: an Exemplar-based 
Approach. In Proceedings of 34th Annual 
Meeting of the Association for Computational 
Linguistics, pp. 40-47, California. 
Srihari, Rohini, Cheng Niu, and Wei Li. 2000. A 
Hybrid Approach for Named Entity and 
Sub-Type Tagging. In Proceedings of ANLP 
2000, Seattle. 
Yarowsky, David. 1992. Word-sense 
Disambiguation Using Statistical Models of 
Roget?s Categories Trained on Large Corpora. 
In Proceedings of the 14 th Internaional 
Conference on Computational Linguistics 
(COLING-92), pp. 454-460, Nates, France. 
Yarowsky, David. 1995. Unsupervised Word 
Sense Disambiguation Rivaling Supervised 
Methods. In Proceedings of the 33rd Annual 
Meeting of the Association for Computational 
Linguistics, Cambridge, Massachusetts. 
Appendix: Sample text and tagged result 
Few countries in the world offer as many choices to 
the world traveler as Canada. Whether your passion is 
skiing, sailing, museum-combing or indulging in 
exceptional cuisine, Canada has it all.  
Western Canada is renowned for its stunningly 
beautiful countryside. Stroll through Vancouver's 
Park, overlooking the blue waters of English Bay or 
ski the slopes of world-famous Whistler-Blackcomb, 
surrounded by thousands of hectares of pristine 
forestland. For a cultural experience, you can take an 
Aboriginal nature hike to learn about Canada's First 
Nations' history and cuisine, while outdoorsmen can 
river-raft, hike or heli-ski the thousands of kilometers 
of Canada's backcountry, where the memories of gold 
prospectors and pioneers still flourish today.  
By contrast, Canada mixes the flavor and charm of 
Europe with the bustle of trendy New York. Toronto 
boasts an irresistible array of ethnic restaurants, 
bakeries and shops to tempt the palate, while 
Charlottetown, Canada's birthplace, is located amidst 
the rolling fields and sandy Atlantic beaches of Prince 
Edward Island. Between the two, ancient Quebec City 
is a world unto itself: the oldest standing citadel in 
North America and the heart of Quebec hospitality.  
 
 
Location City Province Country 
Canada - - Canada 
Vancouver Vancouver British 
Columbia 
Canada 
New York New York New York USA 
Toronto Toronto Ontario Canada 
Charlotte- 
town 
Charlotte- 
town 
Prince 
Edward 
Island 
Canada 
Prince 
Edward 
Island 
- Prince 
Edward 
Island 
Canada 
Quebec Quebec Quebec Canada 
 
InfoXtract location normalization: a hybrid approach to geographic 
references in information extraction ? 
 
 
Huifeng Li, Rohini K. Srihari, Cheng Niu, and Wei Li 
Cymfony Inc. 
600 Essjay Road, Williamsville, NY 14221, USA 
(hli, rohini, cniu, wei)@cymfony.com 
 
 
                                                     
? This work was partly supported by a grant from the Air Force Research Laboratory?s Information Directorate (AFRL/IF), Rome, 
NY, under contract F30602-01-C-0035. The authors wish to thank Carrie Pine of AFRL for supporting and commenting this work. 
Abstract  
Ambiguity is very high for location names. For 
example, there are 23 cities named ?Buffalo? in the 
U.S.  Based on our previous work, this paper presents 
a refined hybrid approach to geographic references 
using our information extraction engine InfoXtract. 
The InfoXtract location normalization module 
consists of local pattern matching and discourse 
co-occurrence analysis as well as default senses.  
Multiple knowledge sources are used in a number of 
ways: (i) pattern matching driven by local context, 
(ii) maximum spanning tree search for discourse 
analysis, and (iii) applying default sense heuristics 
and extracting default senses from the web. The 
results are benchmarked with 96% accuracy on our 
test collections that consist of both news articles and 
tourist guides. The performance contribution for each 
component of the module is also benchmarked and 
discussed. 
 
1 Introduction 
The task of location normalization is to decode 
geographic references for extracted location  Named 
Entities (NE). Ambiguity is a very serious problem for 
location NEs. For example, there are 23 cities named 
?Buffalo?, including the city in New York State and in 
the state of Alabama. Country names such as 
?Canada?, ?Brazil?, and ?China? are also city names in 
the USA. Such ambiguity needs to be properly 
handled before converting location names into normal 
form to support Entity Profile (EP) construction, 
information merging/consolidation as well as 
visualization of location-stamped extracted events on 
a map.  
Location normalization is a special application of 
word sense disambiguation (WSD). There is 
considerable research on WSD. Knowledge-based 
work, such as [Hirst 1987; McRoy 1992; Ng and 
Lee 1996] used hand-coded rules or supervised 
machine learning based on an annotated corpus to 
perform WSD. Recent work emphasizes a 
corpus-based unsupervised approach [Dagon and 
Itai 1994; Yarowsky 1992; Yarowsky 1995] that 
avoids the need for costly truthed training data.  
Location normalization is different from general 
WSD in that the selection restriction often used for 
WSD in many cases is not sufficient to distinguish 
the correct sense from the other candidates. For 
example, in the sentence ?The White House is 
located in Washington?, the selection restriction 
from the collocation ?located in? can only 
determine that ?Washington? should be a location 
name, but is not sufficient to decide the actual sense 
of this location.  
In terms of local context, we found that there are 
certain fairly predictable keyword-driven patterns 
which can decide the senses of location NEs. These 
patterns use keywords such as ?city?, ?town?, 
?province?, ?on?, ?in? or candidate location subtypes 
that can be assigned from a location gazetteer. For 
example, the pattern ?X + city? can determine sense 
tags for cases like ?New York City?; and the pattern 
?Candidate-city-name + comma + 
Candidate-state-name? can disambiguate cases 
such as ?Albany, New York? and ?Shanghai, 
Illinois?.  
In the absence of these patterns, co-occurring 
location NEs in the same discourse provide 
evidence for predicting the most probable sense of a 
location name. More specifically, location 
normalization depends on co-occurrence 
constraints of geographically related location entities 
mentioned in the same document. For example, if 
?Buffalo?, ?Albany? and ?Rochester? are mentioned in 
the same  document, the most probable senses of 
?Buffalo?, ?Albany? and ?Rochester? should refer to 
the cities in New York State.   
For choosing the best matching sense set within a 
document, we simply construct a graph where each 
node represents a sense of a location NE, and each 
edge represents the relationship between two location 
name senses. A graph  spanning algorithm can be used 
to select the best senses from the graph.  
Last but not least, proper assignment of default 
senses is found to play a significant role in the 
performance of a location normalizer. This involves 
two issues: (i) determining default senses using 
heuristics and/or other methods, such as statistical 
processing for semi-automatic default sense extraction 
from the web [Li et al 2002]; and (ii) setting the 
conditions/thresholds and the proper levels when 
assigning default senses, to coordinate with local and 
discourse evidence for enhanced performance. The 
second issue can be resolved through experimentation. 
In the light of the above overview, this paper 
presents an effective hybrid location normalization 
approach which consists of local pattern matching and 
discourse co-occurrence analysis as well as default 
senses. Multiple knowledge sources are used in a 
number of ways: (i) pattern matching driven by local 
context, (ii) maximum spanning tree search for 
discourse analysis, and (iii) applying heuristics-based 
default senses and web-extracted default senses in 
proper stages.  
In the remaining text, Section 2 introduces the 
background for this research. Section 3 describes our 
previous work in this area and Section 4 presents the 
modified algorithm to address the issues with the 
previous method. Experiment and benchmarks are 
described in Section 5. Section 6 is the conclusion. 
 
2 Background 
The design and implementation of the location 
normalization module is an integrated part of 
Cymfony?s core information extraction (IE) engine 
InfoXtract. InfoXtract extracts and normalizes entities, 
relationships and events from natural language text.  
Figure 1 shows the overall system architecture of 
InfoXtract, involving multiple modules in a pipeline 
structure.  
InfoXtract involves a spectrum of linguistic 
processing and relationship/event extraction. This 
engine, in its current state, involves over 100 levels of 
processing and 12 major components. Some 
components are based on hand-crafted pattern 
matching rules, some are statistical models or 
procedures, and others are hybrid (e.g. NE, 
Co-reference, Location Normalization). The basic 
information extraction task is NE tagging [Krupka 
and Hausman 1998; Srihari et al 2000].  The NE 
tagger identifies and classifies proper names of type 
PERSON, ORGANIZATION, PRODUCT, 
NAMED-EVENTS, LOCATION (LOC) as well as 
numerical expressions such as MEASUREMENT 
(e.g. MONEY, LENGTH, WEIGHT, etc) and time 
expressions (TIME, DATE, MONTH, etc.). 
Parallel to location normalization, InfoXtract also 
involves time normalization and measurement 
normalization.  
   
Document Processor
Knowledge Resources
Lexicon
Resources
Grammars
Process
Manager
Tokenlist
Legend
Output
Manager
Source
Document
Linguistic Processor(s)Tokenizer
Tokenlist
Lexicon Lookup
 POS Tagging
NE Tagging
Shallow
Parsing
Relationship
Extraction
Document
pool
NE
CE
EP
SVO
Time
Normalization
Profile/Event
Consolidation
Event
Extraction
Abbreviations
NE = Named Entity
CE = Correlated Entity
EP = Entity Profile
SVO = Subject-Verb-Object
GE = General Event
PE = Predefined Event
Rule-based
Pattern Matching
Procedure or
Statistical Model
Hybrid
Module
GE
Statistical
Models
PE
IE
Repository
Deep Parsing
Coreference
Location
Normalization
Measurement
Normalization
 Figure 1:  System Architecture of InfoXtract 
 
InfoXtract combines the Maximum Entropy 
Model (MaxEnt) and Hidden Markov Model for 
NE tagging [Srihari et al 2000]. Maximum 
Entropy Models incorporate local contextual 
evidence to handle ambiguity of information from a 
location gazetteer. In the Tipster Location 
Gazetteer used by InfoXtract, there are many 
common words, such as I, A, June, Friendship, etc. 
Also, there is large overlap between person names 
and location names, such as Clinton, Jordan, etc. 
Using MaxEnt, systems learn under what situation 
a word is a location name, but it is very difficult to 
determine the correct sense of an ambiguous 
location name. The NE tagger in InfoXtract only 
assigns the location super-type tag LOC to the 
identified location words and leaves the task of 
location sub-type tagging such as CITY or STATE 
and its disambiguation to the subsequent module 
Location Normalization.  
Beyond NE, the major information objects 
extracted by InfoXtract are Correlated Entity (CE) 
relationships (e.g. AFFILIATION and POSITION), 
Entity Profile (EP) that is a collection of extracted 
entity-centric information, Subject-Verb-Object 
(SVO) which refers to dependency links between 
logical subject/object and its verb governor, General 
Event (GE) on who did what when and where and 
Predefined Event (PE) such as Management 
Succession and Company Acquisition.  
It is believed that these information objects capture 
the key content of the processed text. When 
normalized location, time and measurement NEs are 
associated with information objects (events, in 
particular) based on parsing, co-reference and/or 
discourse propagation, these events are stamped. The 
processing results are stored in IE Repository, a 
dynamic knowledge warehouse used to support 
cross-document consolidation, text mining for hidden 
patterns and IE applications. For example, 
location-stamped events can support information 
visualization on maps (Figure 2); time-stamped 
information objects can support visualization along a 
timeline; measurement-stamped objects will allow 
advanced retrieval such as find all Company 
Acquisition events that involve money amount greater 
than 2 million US dollars. 
 
Event type: <Die: Event 200>
Who:       <Julian Werver Hill: PersonProfile 001>
When:     1996-01-07
Where:    <LocationProfile103>
Preceding_event: <hospitalize: Event 260>
Subsequent_event: <bury: Event 250>
Event Visualization
;  ; 
; ; 
Predicate: DieWho: Julian Werner Hill
When:
Where: <LocationProfile 103>
Hockessin, Delaware, USA,
19707,75.688873,39.77604
1996-01-07
Figure 2:  Location-stamped Information 
Visualization 
 
3 Previous Work and Issues 
This paper is follow-up research based on our previous 
work [Li et al 2002]. Some efficiency and 
performance issues are identified and addressed by the 
modified approach.  
The previous algorithm [Li et al 2002] for location 
normalization consisted of five steps. 
 
Step 1. Look up location names in the 
gazetteer to associate candidate senses for 
each location NE; 
Step 2. Call the pattern matching sub-module 
to resolve the ambiguity of the NEs involved 
in local patterns like ?Williamsville, New 
York, USA? to retain only one sense for the 
NE as early as possible; 
Step 3. Apply the ?one sense per discourse? 
principle [Gale et al1992] for each 
disambiguated location name to propagate 
the selected sense to its other mentions 
within a document; 
Step 4. Call the discourse sub-module, 
which is a graph search algorithm 
(Kruskal?s algorithm), to resolve the 
remaining ambiguities; 
Step 5. If the decision score for a location 
name is lower than a threshold, we choose a 
default sense of that name as a result. 
In this algorithm, Step 2, Step 4, and Step 5 
complement each other, and help produce better 
overall performance.  
Step 2 uses local context that is the co-occurring 
words around a location name. Local context can be 
a reliable source in deciding the sense of a location. 
The following are the most commonly used 
patterns for this purpose.  
 
(1) LOC + ?,? + NP (headed by ?city?)  
e.g. Chicago, an old city  
(2) ?city of? + LOC1 + ?,? + LOC2 
e.g. city of Albany, New York 
(3) ?city of? + LOC 
(4) ?state of? + LOC  
(5) LOC1+ ?,? + LOC2 + ?,? + LOC3 
e.g. (i) Williamsville, New York, USA 
       (ii) New York, Buffalo, USA 
     (6) ?on?/ ?in? + LOC 
 e.g. on Strawberry  ISLAND 
 in Key West  CITY 
 
Patterns (1) , (3), (4) and (6) can be used to decide if 
the location is a city, a state or an island, while 
patterns (2) and (5) can be used to determine both 
the sub-tag and its sense. 
Step 4 constructs a weighted graph where each 
node represents a location sense, and each edge 
represents similarity weight between location 
names. The graph is partially complete since there 
are no links among the different senses of a location 
name. The maximum weight spanning tree (MST) 
is calculated using Kruskal?s MinST algorithm 
[Cormen et al 1990]. The nodes on the resulting 
MST are the most promising senses of the location 
names.  
Figure 3 and Figure 4 show the graphs for 
calculating MST. Dots in a circle mean the number 
of senses of a location name. 
Through experiments, we found an efficiency 
problem in Step 4 which adopted Kruskal?s 
algorithm for MST search to capture the impact of 
location co-occurrence in a discourse. While this 
algorithm works fairly well for short documents (e.g. 
most news articles), there is a serious time complexity 
issue when numerous location names are contained in 
long documents. A weighted graph is constructed by 
linking sense nodes for each location with the sense 
nodes for other locations. In addition, there is also an 
associated performance issue: the value weighting for 
the calculated edges using the previous method is not 
distinctive enough.  We observe that the number of 
location mentions and the distance between the 
location names impact the selection of location senses, 
but the previous method could not reflect these factors 
in distinguishing the weights of candidate senses. 
 
Canada
{Kansas,
Kentucky,
Country}
Vancouver
{British Columbia
Washington
port in USA
Port in Canada}
New York 
{Prov in USA,
New York City,
?}
Toronto
(Ontorio,
New South Wales,
Illinois,
?}
Charlottetown
{Prov in USA,
New York City,
?}
Prince Edward Island
{Island in Canada,
Island in South Africa,
Province in Canada}
Quebec
(city in Quebec,
Quebec Prov,
Connecticut,
?}
3*4 lines
2*3 lines
4*11 lines
11*10 lines
3*10 lines8*3 lines
2*8 lines
2*43*11
3*10
3*33*8
2*10
2*3
8*4
8*11
8*10
8*4
10*4
3*11
 
Figure 3:  Graph and its Spanning Tree 
 
Canada
{Kansas,
Kentucky,
Country}
Vancouver
{British Columbia
Washington
port in USA
Port in Canada}
New York 
{Prov in USA,
New York City,
?}
Toronto
(Ontorio,
New South Wales,
Illinois,
?}
Charlottetown
{city in New York
Port in canada}
Prince Edward Island
{Island in Canada,
Island in South Africa,
Province in Canada}
Quebec
(city in Quebec,
Prov in Canada,
Connecticut,
?}
3.6
3.6
3.66
3.6
3.6
0
1
2
3
4
5
6
7
 
Figure 4:  Max Spanning Tree 
 
Finally, our research shows that default senses play 
a significant role in location normalization. For 
example, people refer to ?Los Angeles? as the city in 
California more than the city in the Philippines, Chile, 
Puerto Rico, or the city in Texas in the USA. 
Unfortunately, the available Tipster Gazetteer 
(http://crl.nmsu.edu/cgi-bin/Tools/CLR/clrcat) does 
not mark default senses for most entries. It has 
171,039 location entries with 237,916  senses, among 
which 30,711 location names are ambiguous. 
Manually tagging the default senses for over 30,000 
location names is difficult; moreover, it is also subject 
to inconsistency due to the different knowledge 
backgrounds of the human taggers. This problem 
was solved by developing a procedure to 
automatically extract default senses from web 
pages using the Yahoo! search engine [Li et al 
2002]. Such a procedure has the advantage of 
enabling ?re-training? of default senses when 
necessary. If the web pages obtained through Yahoo! 
represent a typical North American ?view? of what 
default sense should be assigned to location names, 
it may be desirable to re-train the default senses of 
location names  using other views (e.g. an Asian 
view or African view) when the system needs to 
handle overseas documents that contain many 
foreign location names.  
In addition to the above automatic default sense 
extraction, we later found that a few simple default 
sense heuristics, when used at proper levels, can 
further enhance performance. This finding is 
incorporated in our modified approach described in 
Section 3 below.  
 
4 Modified Hybrid Approach 
To address the issues identified in Section 2, we 
adopt Prim?s algorithm, which traverses each node 
of a graph to choose the most promising senses. 
This algorithm has much less search space and 
shows the advantage of being able to reflect the 
number of location mentions and their distances in 
a document.  
The following is the description of our adapted 
Prim?s algorithm for the weight calculation.  
The weight of each sense of a node is calculated 
by considering the effect of linked senses of other 
location nodes based on a predefined weight table 
(Table 1) for the sense categories of co-occurring 
location names. For example, when a location name 
with a potential city sense co-occurs with a location 
name with a potential state/province sense and the 
city is in the state/province, the impact weight of 
the state/province name on the city name is fairly 
high, with the weight set to 3 as shown in the 3rd 
row of Table 1.   
Table 1. Impact weight of Sense2 on Sense1 
Sense1 Sense2 Condition  Weight 
City City in same state 2 
 City in same country 1 
 State in same state 3 
 Country in country without 
state (e.g. in Europe) 
4 
 
Let W(Si) be the calculated weight of a sense Sj of 
a location; weight(Sj->Si) means the weight of Si 
influenced by sense Sj; Num(Loci) is the number of 
location mentions; and ?/dist(Loci, Locj) is the 
measure of distance between two locations.  The final 
sense of a location is the one that has maximum 
weight. A location name may be mentioned a number 
of times in a document.  For each location name, we 
only count the location mention that has the maximum 
sense weight summation in equation (1) and 
eventually propagate the selected sense of this 
location mention to all its other mentions based on one 
sense per discourse principle.  Equation (2) refers to 
the sense with the maximum weight for Loci. 
 
(1)
( )

=
?
=
m
j
jijij
i
LocLocdistLocNumSSweight
SW
0
),(/*)(*)(
)(
?
 
(2) ))(()( maxarg j
j
i SWLocS =   
wj ??0  
Through experiments, we also found that it is 
beneficial to select default senses when candidate 
location senses in the discourse analysis turn out to be 
of the same weight. We included two kinds of default 
senses: heuristics-based default senses and the default 
senses extracted semi-automatically from the web 
using Yahoo. For the first category of default senses, 
we observe that if a name has a country sense and 
other senses, such as ?China? and ?Canada?, the 
country senses are dominant in most cases. The 
situation is the same for a name with province sense 
and for a name with country capital sense (e.g. London, 
Beijing). The updated algorithm for location 
normalization is as follows. 
 
Step 1. Look up the location gazetteer to 
associate candidate senses for each location 
NE; 
Step 2. If a location has sense of country, then 
select that sense as the default sense of that 
location (heuristics); 
Step 3. Call the pattern matching sub-module 
for local patterns like ?Williamsville, New 
York, USA?; 
Step 4. Apply the ?one sense per discourse? 
principle for each disambiguated location 
name to propagate the selected sense to its 
other mentions within a document; 
Step 5. Apply default sense heuristics for a 
location with province or capital senses; 
Step 6. Call Prim?s algorithm in the 
discourse sub-module to resolve the 
remaining ambiguities (Figure 5); 
Step 7. If the difference between the sense 
with the maximum weight and the sense 
with next largest weight is equal to or lower 
than a threshold, choose the default sense of 
that name from lexicon.  Otherwise, choose 
the sense with the maximum weight as 
output. 
Canada
{Kansas,
Kentucky,
Country}
Vancouver
{British Columbia
Washington
port in USA
Port in Canada}
New York 
{Prov in USA,
New York City,
?}
Toronto
(Ontorio,
New South Wales,
Illinois,
?}
Charlottetown
{Prov in USA,
New York City,
?}
Prince Edward Island
{Island in Canada,
Island in South Africa,
Province in Canada}
Quebec
(city in Quebec,
Quebec Prov,
Connecticut,
?}
 
 Figure 5:  Weight assigned to Sense Nodes 
 
5 Experiment and Benchmark 
With the information from local context, discourse 
context and the knowledge of default senses, the 
location normalization process is  efficient and 
precise.  
The testing documents were randomly selected 
from CNN news and from travel guide web pages. 
Table 2 shows the preliminary testing results using 
different configurations.  
As shown, local patterns (Column 4) alone 
contribute 12% to the overall performance while 
proper use of defaults senses and the heuristics 
(Column 5) can achieve close to 90%. In terms of 
discourse co-occurrence evidence, the new method 
using Prim?s algorithm (Column 7) is clearly better 
than the previous method using Kruskal?s 
algorithm (Column 6), with 13% enhancement 
(from 73.8% to 86.6%). But both methods cannot 
outperform default senses.  Finally, when using all 
three types of evidence, the new hybrid method 
presented in this paper shows significant 
performance enhancement (96% in Column 9) over 
the previous method (81.9% in Column 8), in 
addition to a satisfactory solution to the efficiency 
problem.  
Table 2. Experimental evaluation for location normalization 
File # of 
ambiguous 
location 
names 
# of 
mentions 
Pattern 
hits 
Def-
senses 
 
Kruskal 
Algo. 
only 
Prim 
Algo 
only  
Kruskal 
+Pattern 
+Def 
(previous) 
Prim 
+Pattern 
+Def 
(new) 
Cnn1 26 39 4 20 21 24 26  26 
Cnn2 12 20 5 11 7 10 11 11 
Cnn3 14 29 0 12 10 12 10 14 
Cnn4 8 14 2 8 4 4 4 8 
Cnn5 11 26 1 9 5 8 5 9 
Cnn6 19 35 6 16 11 16 13 18 
Cnn7 11 27 0 11 4 7 6 10 
Calif. 16 30 0 16 16 16 16 16 
Florida 19 28 0 19 19 19 18 19 
Texas 13 13 0 12 13 13 13 12 
Total 149 261 12% 89.9% 73.8% 86.6% 81.9% 96% 
 
We observed that if a file contains more 
concentrated locations, such as the state introductions 
in the travel guides for California, Florida and Texas, 
the accuracy is higher than the relatively short news 
articles from CNN.  
 
6 Conclusion and Future Work 
This paper presented an effective hybrid method of 
location normalization for information extraction with 
promising experimental results. In the future, we will 
integrate an expanded location gazetteer including 
names of landmarks, mountains and lakes such 
as Holland Tunnel (in New York, not in Holland) and 
Hoover Dam (in Arizona, not in Alabama), to enlarge 
the system coverage. Meanwhile, more extensive 
benchmarking is currently being planned in order to 
conduct a detailed analysis of different evidence 
sources and their interaction and contribution to 
system performance.  
References 
Cormen, Thomas H., Charles E. Leiserson, and 
Ronald L. Rivest. 1990. Introduction to Algorithm. 
The MIT Press, 504-505. 
Dagon, Ido and Alon Itai. 1994. Word Sense 
Disambiguation Using a Second Language 
Monolingual Corpus. Computational Linguistics, 
Vol.20, 563-596. 
Gale, W.A., K.W. Church, and D. Yarowsky. 1992. 
One Sense Per Discourse. Proceedings of the 4th 
DARPA Speech and Natural Language Workshop. 
233-237. 
Hirst, Graeme. 1987. Semantic Interpretation and the 
Resolution of Ambiguity. Cambridge University 
Press, Cambridge. 
Huifeng Li, Rohini K. Srihari, Cheng Niu, Wei Li. 
2002. Location Normalization for Information 
Extraction, COLING 2002, Taipei, Taiwan. 
Krupka, G.R. and K. Hausman.  1998.  IsoQuest 
Inc.: Description of the NetOwl (TM) Extractor 
System as Used for MUC-7.  Proceedings of 
MUC.  
McRoy, Susan W. 1992. Using Multiple 
Knowledge Sources for Word Sense 
Discrimination. Computational Linguistics, 
18(1): 1-30. 
Ng, Hwee Tou and Hian Beng Lee. 1996. 
Integrating Multiple Knowledge Sources to 
Disambiguate Word Sense: an Exemplar-based 
Approach. ACL 1996, 40-47, California. 
Srihari, Rohini, Cheng Niu, and Wei Li. 2000. A 
Hybrid Approach for Named Entity and 
Sub-Type Tagging. ANLP 2000, Seattle. 
Yarowsky, David. 1992. Word-sense 
Disambiguation Using Statistical Models of 
Roget?s Categories Trained on Large Corpora. 
COLING 1992, 454-460, Nantes, France. 
Yarowsky, David. 1995. Unsupervised Word Sense 
Disambiguation Rivaling Supervised Methods. 
ACL 1995, Cambridge, Massachusetts. 
Context Clustering for Word Sense Disambiguation Based on  
Modeling Pairwise Context Similarities 
Cheng Niu, Wei Li, Rohini K. Srihari, Huifeng Li, Laurie Crist 
Cymfony Inc. 
600 Essjay Road, Williamsville, NY 14221. USA. 
{cniu, wei, rohini, hli, lcrist}@cymfony.com 
 
Abstract 
Traditionally, word sense disambiguation 
(WSD) involves a different context model for 
each individual word. This paper presents a 
new approach to WSD using weakly 
supervised learning. Statistical models are not 
trained for the contexts of each individual 
word, but for the similarities between context 
pairs at category level. The insight is that the 
correlation regularity between the sense 
distinction and the context distinction can be 
captured at category level, independent of 
individual words. This approach only requires 
a limited amount of existing annotated training 
corpus in order to disambiguate the entire 
vocabulary. A context clustering scheme is 
developed within the Bayesian framework. A 
maximum entropy model is then trained to 
represent the generative probability 
distribution of context similarities based on 
heterogeneous features, including trigger 
words and parsing structures. Statistical 
annealing is applied to derive the final context 
clusters by globally fitting the pairwise 
context similarity distribution. Benchmarking 
shows that this new approach significantly 
outperforms the existing WSD systems in the 
unsupervised category, and rivals supervised 
WSD systems. 
1 Introduction 
Word Sense Disambiguation (WSD) is one of the 
central problems in Natural Language Processing. 
The difficulty of this task lies in the fact that 
context features and the corresponding statistical 
distribution are different for each individual word. 
Traditionally, WSD involves modeling the 
contexts for each word.  [Gale et al 1992] uses the 
Na?ve Bayes method for context modeling which 
requires a manually truthed corpus for each 
ambiguous word. This causes a serious Knowledge 
Bottleneck. The situation is worse when 
considering the domain dependency of word 
senses. To avoid the Knowledge Bottleneck, 
unsupervised or weakly supervised learning 
approaches have been proposed. These include the 
bootstrapping approach [Yarowsky 1995] and the 
context clustering approach [Schutze 1998]. 
Although the above unsupervised or weakly 
supervised learning approaches are less subject to 
the Knowledge Bottleneck, some weakness exists: 
i) for each individual keyword, the sense number 
has to be provided and in the bootstrapping case, 
seeds for each sense are also required; ii) the 
modeling usually assumes some form of evidence 
independency, e.g. the vector space model used in 
[Schutze 1998] and [Niu et al 2003]: this limits the 
performance and its potential enhancement; iii) 
most WSD systems either use selectional 
restriction in parsing relations, and/or  trigger 
words which co-occur within a window size of the 
ambiguous word. We previously at-tempted 
combining both types of evidence but only 
achieved limited improvement due to the lack of a 
proper modeling of information over-lapping [Niu 
et al 2003]. 
This paper presents a new algorithm that 
addresses these problems. A novel context 
clustering scheme based on modeling the 
similarities between pairwise contexts at category 
level is presented in the Bayesian framework. A 
generative maximum entropy model is then trained 
to represent the generative probability distribution 
of pairwise context similarities based on 
heterogeneous features that cover both co-
occurring words and parsing structures. Statistical 
annealing is used to derive the final context 
clusters by globally fitting the pairwise context 
similarities. 
This new algorithm only requires a limited 
amount of existing annotated corpus to train the 
generative maximum entropy model for the entire 
vocabulary. This capability is based on the 
observation that a system does not necessarily 
require training data for word A in order to 
disambiguate A.  The insight is that the correlation 
regularity between the sense distinction and the 
context distinction can be captured at category 
level, independent of individual words. 
In what follows, Section 2 formulates WSD as a 
context clustering task based on the pairwise 
                                             Association for Computational Linguistics
                        for the Semantic Analysis of Text, Barcelona, Spain, July 2004
                 SENSEVAL-3: Third International Workshop on the Evaluation of Systems
context similarity model. The context clustering 
algorithm is described in Sections 3 and 4, 
corresponding to the two key aspects of the 
algorithm, i.e. the generative maximum entropy 
modeling and the annealing-based optimization. 
Section 5 describes benchmarks and conclusion. 
2 Task Definition and Algorithm Design 
Given n  mentions of a key word, we first 
introduce the following symbols. iC  refers to the 
i -th context.  iS  refers to the sense of the i -th 
context. jiCS ,  refers to the context similarity 
between the i -th context and the j -th context, 
which is a subset of the predefined context 
similarity features. ?f  refers to the ? -th 
predefined context similarity feature. So jiCS ,  
takes the form of { }?f . 
The WSD task is defined as the hard clustering 
of multiple contexts of the key word. Its final 
solution is represented as { }MK ,  where K refers 
to the number of distinct senses, and M represents 
the many-to-one mapping (from contexts to a 
cluster) such that ( ) K]. [1,j n],[1,i j,iM ??=  
For any given context pair, a set of context 
similarity features are defined. With n mentions of 
the same key word, 2
)1( ?nn  context similarities 
[ ] [ )( )ijniCS ji ,1,,1 , ??  are computed. The WSD task 
is formulated as searching for { }MK ,  which 
maximizes the following conditional probability: 
{ }( ) [ ] [ )( )ijniCSMK ji ,1,,1       }{,Pr , ??  
Based on Bayesian Equity, this is equivalent to 
maximizing the joint probability in Eq. (1), which 
contains a prior probability distribution of WSD, 
{ }( )MK ,Pr .  
 
{ }( ) [ ] [ )( )
{ }( ) { }( )
{ }( ) { }( )MKMKCS
MKMKCS
ijniCSMK
ij
Ni
ji
ji
ji
,Pr,Pr
,Pr,}{Pr
,1,,1       }{,,Pr
1,1
,1
,
,
,
?
?=
=
=
=
??
 (1) 
 
Because there is no prior knowledge available 
about what solution is preferred, it is reasonable to 
take an equal distribution as the prior probability 
distribution. So WSD is equivalent to searching for 
{ }MK ,  which maximizes Expression (2). 
 { }( )?
?=
=
1,1
,1
, ,Pr
ij
Ni
ji MKCS    (2) 
where 
{ }( ) ( ) ( ) ( )( )





?
==
= otherwise ,Pr
jMiM if ,Pr,Pr
,
,
,
jiji
jiji
ji SSCS
SSCSMKCS   
     (3) 
 
To learn the conditional probabilities ( )jiji SSCS =|Pr ,  and ( )jiji SSCS ?|Pr ,  in Eq. (3), a 
maximum entropy model is trained. There are two 
major advantages of this maximum entropy model: 
i) the model is independent of individual words; ii) 
the model takes no information independence 
assumption about the data, and hence is powerful 
enough to utilize heterogeneous features. With the 
learned conditional probabilities in Eq. (3), for a 
given { }MK ,  candidate, we can compute the 
conditional probability of Expression (2).  In the 
final step, optimization is performed to search for 
{ }MK ,  that maximizes the value of Expression 
(2). 
3 Maximum Entropy Modeling 
This section presents the definition of context 
similarity features, and how to estimate the 
generative probabilities of context similarity ( )jiji SSCS =,Pr  and ( )jiji SSCS ?,Pr  using 
maximum entropy modeling. 
Using the Senseval-2 training corpus,1 we have 
constructed Corpus I and Corpus II for each Part-
of-speech (POS) tag. Corpus I is constructed using 
context pairs involving the same sense of a word.  
Corpus II is constructed using context pairs that 
refer to different senses of a word. Each corpus 
contains about 18,000 context pairs. The instances 
in the corpora are represented as pairwise context 
similarities, taking the form of { }?f . The two 
conditional probabilities ( )jiji SSCS =,Pr  and ( )jiji SSCS ?,Pr  can be represented as 
( )}{Pr maxEntI ?f  and ( )}{Pr maxEntII ?f  which are 
generative probabilities by maximum entropy for 
Corpus I and Corpus II. 
We now present how to compute the context 
similarities. Each context contains the following 
two categories of features: 
i) Trigger words centering around the key word 
within a predefined window size equal to 50 
tokens to both sides of the key word. Trigger 
words are learned using the same technique as 
in [Niu et al 2003]. 
ii) Parsing relationships associated with the key 
word automatically decoded by our parser 
                                                     
1 Note that the words that appear in the Senseval-3 
lexical sample evaluation are removed in the corpus 
construction process. 
InfoXtract [Srihari et al 2003]. The 
relationships being utilized are listed below.  
 
Noun: subject-of, object-of, complement-of, 
has-adjective-modifier, has-noun-
modifier, modifier-of, possess, 
possessed-by, appositive-of 
 
Verb: has-subject, has-object, has-
complement, has-adverb-modifier, 
has-prepositional-modifier 
 
Adjective: modifier-of, has-adverb-modifier 
 
Based on the above context features, the 
following three categories of context similarity 
features are defined: 
(1) Context similarity based on a vector space 
model using co-occurring trigger words: the 
trigger words centering around the key word 
are represented as a vector, and the tf*idf 
scheme is used to weigh each trigger word. 
The cosine of the angle between two resulting 
vectors is used as a context similarity 
measure. 
(2) Context similarity based on Latent 
semantic analysis (LSA) using trigger words: 
LSA [Deerwester et al 1990] is a technique 
used to uncover the underlying semantics 
based on co-occurrence data. Using LSA, 
each word is represented as a vector in the 
semantic space. The trigger words are 
represented as a vector summation. Then the 
cosine of the angle between the two resulting 
vector summations is computed, and used as a 
context similarity measure. 
(3) LSA-based Parsing Structure Similarity: 
each relationship is in the form of )(wR? . 
Using LSA, each word w  is represented as 
semantic vector ( )wV . Then, the similarity 
between )( 1wR? and )( 2wR?  is represented as 
the cosine of angle between ( )1wV  and ( )2wV . 
Two special values are assigned to two 
exceptional cases: i) when  no relationship 
?R  is decoded in both contexts; ii) when the 
relationship ?R is decoded only for one 
context. 
To facilitate the maximum entropy modeling in 
the later stage, the resulting similarity measure is 
discretized into 10 integer values. Now the 
pairwise context similarity is a set of similarity 
features, e.g. 
 
{VSM-Similairty-equal-to-2, LSA-Trigger-
Words-Similarity-equal-to-1, LSA-Subject-
Similarity-equal-to-2}. 
 
In addition to the three categories of basic 
context similarity features defined above, we also 
define induced context similarity features by 
combining basic context similarity features using 
the logical AND operator. With induced features, 
the context similarity vector in the previous 
example is represented as 
 
{VSM-Similairty-equal-to-2, LSA- Trigger-
Words-Similarity-equal-to-1, LSA-Subject-
Similarity-equal-to-2,  
[VSM-Similairty-equal-to-2 and LSA-Trigger -
Words-Similarity-equal-to-1], [VSM-Similairty-
equal-to-2 and LSA-Subject-Similarity-equal-to-
2],  
???, 
[VSM-Similairty-equal-to-2 and LSA-Trigger -
Words-Similarity-equal-to-1 and LSA-Subject-
Similarity-equal-to-2]}. 
 
The induced features provide direct and fine-
grained information, but suffer from less sampling 
space. To make the computation feasible, we 
regulate 3 as the maximum number of logical AND 
in the induced features. Combining basic features 
and induced features under a smoothing scheme, 
maximum entropy modeling may achieve optimal 
performance. 
Now the maximum entropy modeling can be 
formulated as follows: given a pairwise context 
similarity }{ ?f , the generative probability of 
}{ ?f in Corpus I or Corpus II is given as 
 
( )
{ }
?
?
=
?
?
ff
fwZf
1}{Pr maxEnt         (4) 
 
where Z is the normalization factor, fw  is the 
weight associated with feature f . The Iterative 
Scaling algorithm combined with Monte Carlo 
simulation [Pietra, Pietra, & Lafferty 1995] is used 
to train the weights in this generative model. 
Unlike the commonly used conditional maximum 
entropy modeling which approximates the feature 
configuration space as the training corpus 
[Ratnaparkhi 1998], Monte Carlo techniques are 
required in the generative modeling to simulate the 
possible feature configurations. The exponential 
prior smoothing scheme [Goodman 2003] is 
adopted. The same training procedure is performed 
using Corpus I and Corpus II to estimate 
( )}{Pr maxEntI if  and ( )}{Pr maxEntII if  respectively. 
4 Statistical Annealing 
With the maximum entropy modeling presented 
above, the WSD task is performed as follows: i) 
for a given set of contexts, the pairwise context 
similarity measures are computed; ii) for each 
context similarity }{ if , the two generative 
probabilities ( )}{Pr maxEntI if  and ( )}{Pr maxEntII if  are 
computed; iii) for a given WSD candidate 
solution{ }MK , , the conditional probability (2) can 
be computed. Optimization based on statistical 
annealing (Neal 1993) is used to search for { }MK ,  
which maximizes Expression (2). 
The optimization process consists of two steps. 
First, a local optimal solution{ }0, MK is computed 
by a greedy algorithm. Then by setting { }0, MK as 
the initial state, statistical annealing is applied to 
search for the global optimal solution. To reduce 
the search time, we set the maximum value of K  
to 5. 
5 Benchmarking and Conclusion 
To enter the Senseval-3 evaluation, we 
implemented the following procedure to map the 
context clusters to Senseval-3 standards: i) process 
the Senseval-3 training corpus and testing corpus 
using our parser; ii) for each word to be 
benchmarked, retrieve the related contexts from 
the corpora and cluster them; iii) Based on 10% of 
the sense tags in the Senseval-3 training corpus 
(10% data correspond roughly to an average of 2-3 
instances for each sense), the context cluster is 
mapped onto the most frequent WSD sense 
associated with the cluster members. By design, 
the context clusters correspond to distinct senses, 
therefore, we do not allow multiple context clusters 
to be mapped onto one sense. In case multiple 
clusters correspond to one sense, only the largest 
cluster is retained; iv), each instance in the testing 
corpus is tagged with the same sense as the one to 
which its context cluster corresponds.  
    We are not able to compare our performance 
with other systems in Senseval-3 because at the 
time of writing, the Senseval-3 evaluation results 
are not publicly available. As a note, compared 
with the Senseval-2 English Lexical Sample 
evaluation, the benchmarks of our new algorithm 
(Table 1) are significantly above the performance 
of the WSD systems in the unsupervised category, 
and rival the performance of the supervised WSD 
systems. 
 
Table 1. Senseval-3 Lexical Sample Evaluation  
Accuracy  
Category Fine grain (%) Coarse grain (%) 
Adjective (5) 49.1 64.8 
Noun (20) 57.9 66.6 
Verb (32) 55.3 66.3 
Average 56.3% 66.4% 
 
6 Acknowledgements 
This work was supported by the Navy SBIR 
program under contract N00178-03-C-1047. 
References  
Gale, W., K. Church, and D. Yarowsky. 1992. A 
Method for Disambiguating Word Senses in a 
Large Corpus. Computers and the Humanities, 
26. 
Yarowsky, D. 1995. Unsupervised Word Sense 
Disambiguation Rivaling Supervised Methods. 
In Proceedings of ACL 1995. 
Schutze, H. 1998. Automatic Word Sense 
Disambiguation. Computational Linguistics, 23. 
C. Niu, Zhaohui Zheng, R. Srihari, H. Li, and W. 
Li 2003. Unsupervised Learning for Verb Sense 
Disambiguation Using Both trigger Words and 
Parsing Relations. In Proceeding of PACLING 
2003, Halifax, Canada. 
Deerwester, S., S. T. Dumais, G. W. Furnas, T. K. 
Landauer, and R. Harshman. 1990. Indexing by 
Latent Semantic Analysis. In Journal of the 
American Society of Information Science 
Goodman, J. 2003. Exponential Priors for 
Maximum Entropy Models. 
Neal, R.M. 1993. Probabilistic Inference Using 
Markov Chain Monte Carlo Methods. Technical 
Report, Univ. of Toronto. 
Pietra, S. D., V. D. Pietra, and J. Lafferty. 1995. 
Inducing Features Of Random Fields. In IEEE 
Transactions on Pattern Analysis and Machine 
Intelligence.  
Adwait Ratnaparkhi. (1998). Maximum Entropy 
Models for Natural Language Ambiguity 
Resolution. Ph.D. Dissertation. University of 
Pennsylvania. 
Srihari, R., W. Li, C. Niu and T. Cornell. 2003. 
InfoXtract: A Customizable Intermediate Level 
Information Extraction Engine. In Proceedings 
of HLT/NAACL 2003 Workshop on SEALTS. 
Edmonton, Canada. 
Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),
pages 33?39, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
 
Abstract 
Traditionally, word sense disambiguation 
(WSD) involves a different context classifi-
cation model for each individual word. This 
paper presents a weakly supervised learning 
approach to WSD based on learning a word 
independent context pair classification 
model. Statistical models are not trained for 
classifying the word contexts, but for classi-
fying a pair of contexts, i.e. determining if a 
pair of contexts of the same ambiguous word 
refers to the same or different senses. Using 
this approach, annotated corpus of a target 
word A can be explored to disambiguate 
senses of a different word B. Hence, only a 
limited amount of existing annotated corpus 
is required in order to disambiguate the entire 
vocabulary. In this research, maximum en-
tropy modeling is used to train the word in-
dependent context pair classification model. 
Then based on the context pair classification 
results, clustering is performed on word men-
tions extracted from a large raw corpus. The 
resulting context clusters are mapped onto 
the external thesaurus WordNet. This ap-
proach shows great flexibility to efficiently 
integrate heterogeneous knowledge sources, 
e.g. trigger words and parsing structures. 
Based on Senseval-3 Lexical Sample stan-
dards, this approach achieves state-of-the-art 
performance in the unsupervised learning 
category, and performs comparably with the 
supervised Na?ve Bayes system. 
1 Introduction 
Word Sense Disambiguation (WSD) is one of the 
central problems in Natural Language Processing. 
The difficulty of this task lies in the fact that con-
text features and the corresponding statistical dis-
tribution are different for each individual word. 
Traditionally, WSD involves training the context 
classification models for each ambiguous word.  
(Gale et al 1992) uses the Na?ve Bayes method for 
context classification which requires a manually 
annotated corpus for each ambiguous word. This 
causes a serious Knowledge Bottleneck. The bot-
tleneck is particularly serious when considering the 
domain dependency of word senses. To overcome 
the Knowledge Bottleneck, unsupervised or weakly 
supervised learning approaches have been pro-
posed. These include the bootstrapping approach 
(Yarowsky 1995) and the context clustering ap-
proach (Sch?tze 1998). 
The above unsupervised or weakly supervised 
learning approaches are less subject to the Knowl-
edge Bottleneck. For example, (Yarowsky 1995) 
only requires sense number and a few seeds for 
each sense of an ambiguous word (hereafter called 
keyword). (Sch?tze 1998) may only need minimal 
annotation to map the resulting context clusters 
onto external thesaurus for benchmarking and ap-
plication-related purposes. Both methods are based 
on trigger words only. 
This paper presents a novel approach based on 
learning word-independent context pair classifica-
tion model. This idea may be traced back to 
(Sch?tze 1998) where context clusters based on 
generic Euclidean distance are regarded as distinct 
word senses. Different from (Sch?tze 1998), we 
observe that generic context clusters may not al-
ways correspond to distinct word senses. There-
fore, we used supervised machine learning to 
model the relationships between the context dis-
tinctness and the sense distinctness. 
Although supervised machine learning is used 
for the context pair classification model, our over-
all system belongs to the weakly supervised cate-
gory because the learned context pair classification 
Word Independent Context Pair Classification Model for Word 
Sense Disambiguation 
 
Cheng Niu, Wei Li, Rohini K. Srihari, and Huifeng Li 
Cymfony Inc. 
600 Essjay Road, Williamsville, NY 14221, USA. 
{cniu, wei, rohini,hli}@cymfony.com 
33
model is independent of the keyword for disam-
biguation. Our system does not need human-
annotated instances for each target ambiguous 
word. The weak supervision is performed by using 
a limited amount of existing annotated corpus 
which does not need to include the target word set.   
The insight is that the correlation regularity be-
tween the sense distinction and the context distinc-
tion can be captured at Part-of-Speech category 
level, independent of individual words or word 
senses. Since context determines the sense of a 
word, a reasonable hypothesis is that there is some 
mechanism in the human comprehension process 
that will decide when two contexts are similar (or 
dissimilar) enough to trigger our interpretation of a 
word in the contexts as one meaning (or as two 
different meanings). We can model this mecha-
nism by capturing the sense distinction regularity 
at category level.  
In the light of this, a maximum entropy model is 
trained to determine if a pair of contexts of the 
same keyword refers to the same or different word 
senses. The maximum entropy modeling is based 
on heterogeneous context features that involve 
both trigger words and parsing structures. To en-
sure the resulting model?s independency of indi-
vidual words, the keywords used in training are 
different from the keywords used in benchmarking. 
For any target keyword, a collection of contexts is 
retrieved from a large raw document pool. Context 
clustering is performed to derive the optimal con-
text clusters which globally fit the local context 
pair classification results. Here statistical annealing 
is used for its optimal performance. In benchmark-
ing, a mapping procedure is required to correlate 
the context clusters with external ontology senses. 
In what follows, Section 2 formulates the maxi-
mum entropy model for context pair classification. 
The context clustering algorithm, including the 
object function of the clustering and the statistical 
annealing-based optimization, is described in Sec-
tion 3. Section 4 presents and discusses bench-
marks, followed by conclusion in Section 5. 
2 Maximum Entropy Modeling for Con-
text Pair Classification 
Given n  mentions of a keyword, we first introduce 
the following symbols. iC  refers to the i -th con-
text.  iS  refers to the sense of the i -th context. 
jiCS ,  refers to the context similarity between the 
i -th context and the j -th context, which is a subset 
of the predefined context similarity features. ?f  
refers to the ? -th predefined context similarity 
feature. So jiCS ,  takes the form of { }?f . 
In this section, we study the context pair classi-
fication task, i.e. given a pair of contexts iC and 
jC  of the same target word, are they referring to 
the same sense? This task is formulated as compar-
ing the following conditional probabilities: ( )jiji CSSS ,Pr =  and ( )jiji CSSS ,Pr ? . Unlike 
traditional context classification for WSD where 
statistical model is trained for each individual 
word, our context pair classification model is 
trained for each Part-of-speech (POS) category. 
The reason for choosing POS as the appropriate 
category for learning the context similarity is that 
the parsing structures, hence the context represen-
tation, are different for different POS categories. 
The training corpora are constructed using the 
Senseval-2 English Lexical Sample training cor-
pus. To ensure the resulting model?s independency 
of individual words, the target words used for 
benchmarking (which will be the ambiguous words 
used in Senseval-3 English Lexicon Sample task) 
are carefully removed in the corpus construction 
process. For each POS category, positive and nega-
tive instances are constructed as follows.  
Positive instances are constructed using context 
pairs referring to the same sense of a word.  Nega-
tive instances are constructed using context pairs 
that refer to different senses of a word.  
For each POS category, we have constructed 
about 36,000 instances, half positive and half nega-
tive. The instances are represented as pairwise con-
text similarities, taking the form of { }?f . 
Before presenting the context similarity features 
we used, we first introduce the two categories of 
the involved context features: 
 
i) Co-occurring trigger words within a prede-
fined window size equal to 50 words to both 
sides of the keyword. The trigger words are 
learned from a TIPSTER document pool con-
taining ~170 million words of AP and WSJ 
news articles. Following (Sch?tze 1998), ?2 is 
used to measure the cohesion between the 
keyword and a co-occurring word.  In our ex-
34
periment, all the words are first sorted based 
on its ?2 with the keyword, and then the top 
2,000 words are selected as trigger words. 
 
ii) Parsing relationships associated with the 
keyword automatically decoded by a broad-
coverage parser, with F-measure (i.e. the pre-
cision-recall combined score) at about 85% 
(reference temporarily omitted for the sake of 
blind review). The logical dependency rela-
tionships being utilized are listed below. 
 
Noun:  subject-of,  
object-of, 
complement-of,  
has-adjective-modifier,  
has-noun-modifier,  
modifier-of,  
possess, 
 possessed-by,  
appositive-of 
 
Verb:   has-subject,  
has-object, 
 has-complement,  
has-adverb-modifier,  
has-prepositional-phrase-modifier 
 
Adjective: modifier-of,  
has-adverb-modifier 
 
Based on the above context features, the follow-
ing three categories of context similarity features 
are defined: 
 
(1)  VSM-based (Vector Space Model based) 
trigger word similarity: the trigger words 
around the keyword are represented as a vec-
tor, and the word i in context j is weighted as 
follows: 
)(log*),(),( idf
Djitfjiweight =  
where ),( jitf  is the frequency of word i in 
the j-th context; D is the number of docu-
ments in the pool; and )(idf  is the number of 
documents containing the word i. D and 
)(idf are estimated using the document pool 
introduced above. The cosine of the angle be-
tween two resulting vectors is used as the 
context similarity measure. 
 
(2)  LSA-based (Latent Semantic Analysis based) 
trigger word similarity: LSA (Deerwester et 
al. 1990) is a technique used to uncover the 
underlying semantics based on co-occurrence 
data. The first step of LSA is to construct 
word-vs.-document co-occurrence matrix. 
Then singular value decomposition (SVD) is 
performed on this co-occurring matrix. The 
key idea of LSA is to reduce noise or insig-
nificant association patterns by filtering the 
insignificant components uncovered by SVD. 
This is done by keeping only the top k singu-
lar values. By using the resulting word-vs.-
document co-occurrence matrix after the fil-
tering, each word can be represented as a vec-
tor in the semantic space. 
  
In our experiment, we constructed the original 
word-vs.-document co-occurring matrix as 
follows: 100,000 documents from the 
TIPSTER corpus were used to construct the 
co-occurring matrix. We processed these 
documents using our POS tagger, and se-
lected the top n most frequently mentioned 
words from each POS category as base 
words: 
 
top 20,000 common nouns 
top 40,000 proper names 
top 10,000 verbs 
top 10,000 adjectives 
top 2,000 adverbs 
 
In performing SVD, we set k (i.e. the number 
of nonzero singular values) as 200, following 
the practice reported in (Deerwester et al 
1990) and (Landauer & Dumais, 1997). 
 
Using the LSA scheme described above, each 
word is represented as a vector in the seman-
tic space. The co-occurring trigger words are 
represented as a vector summation. Then the 
cosine of the angle between the two resulting 
vector summations is computed, and used as 
the context similarity measure. 
 
(3) LSA-based parsing relationship similarity: 
each relationship is in the form of )(wR? . 
Using LSA, each word w  is represented as a 
35
semantic vector ( )wV . The similarity between 
)( 1wR? and )( 2wR?  is represented as the co-
sine of the angle between ( )1wV  and ( )2wV . 
Two special values are assigned to two excep-
tional cases: (i) when no relationship ?R  is 
decoded in both contexts; (ii) when the rela-
tionship ?R is decoded only for one context. 
 
In matching parsing relationships in a context 
pair, if only exact node match counts, very few 
cases can be covered, hence significantly reducing 
the effect of the parser in this task. To solve this 
problem, LSA is used as a type of synonym expan-
sion in matching.  For example, using LSA, the 
following word similarity values are generated: 
 
similarity(good, good)   1.00 
similarity(good, pretty) 0.79 
similarity(good, great) 0.72 
?? 
 
Given a context pair of a noun keyword, suppose 
the first context involves a relationship has-
adjective-modifier whose value is good, and the 
second context involves the same relationship has-
adjective-modifier with the value pretty, then the 
system assigns 0.79 as the similarity value for this 
relationship pair. 
 
To facilitate the maximum entropy modeling in 
the later stage, all the three categories of the result-
ing similarity values are discretized into 10 inte-
gers. Now the pairwise context similarity is 
represented as a set of similarity features, e.g. 
 
{VSM-Trigger-Words-Similairty-equal-to-2,  
  LSA-Trigger-Words-Similarity-equal-to-1,      
  LSA-Subject-Similarity-equal-to-2}. 
 
In addition to the three categories of basic con-
text similarity features defined above, we also de-
fine induced context similarity features by 
combining basic context similarity features using 
the logical and operator. With induced features, the 
context similarity vector in the previous example is 
represented as 
 
{VSM-Trigger-Word-Similairty-equal-to-2,  
  LSA- Trigger-Word-Similarity-equal-to-1,  
  LSA-Subject-Similarity-equal-to-2,  
  [VSM-Similairty-equal-to-2 and  
   LSA-Trigger-Word-Similarity-equal-to-1],    
  [VSM-Similairty-equal-to-2 and  
   LSA-Subject-Similarity-equal-to-2],  
  ??? 
  [VSM-Trigger-Word-Similairty-equal-to-2   
and LSA-Trigger-Word-Similarity-equal-to-1 
and LSA-Subject-Similarity-equal-to-2] 
} 
 
The induced features provide direct and fine-
grained information, but suffer from less sampling 
space. Combining basic features and induced fea-
tures under a smoothing scheme, maximum en-
tropy modeling may achieve optimal performance. 
Using the context similarity features defined 
above, the training corpora for the context pair 
classification model is in the following format: 
 
Instance_0 tag=?positive? {VSM-Trigger-Word-
Similairty-equal-to-2, ?} 
Instance_1 tag=?negative? {VSM-Trigger-Word-
Similairty-equal-to-0, ?} 
????? 
where positive tag denotes a context pair associ-
ated with same sense, and negative tag denotes a 
context pair associated with different senses.  
 
The maximum entropy modeling is used to com-
pute the conditional probabilities ( )jiji CSSS ,Pr =  and ( )jiji CSSS ,Pr ? : once the 
context pair jiCS ,  is represented as }{ ?f , the con-
ditional probability is given as 
 
( )
{ }
?
?
=
?
?
ff
ftwZft ,
1}{Pr         (1) 
where { }jiji SSSSt ?=? , , Z is the normaliza-
tion factor, ftw ,  is the weight associated with tag t 
and feature f . Using the training corpora con-
structed above, the weights can be computed based 
on Iterative Scaling algorithm (Pietra etc. 1995) 
The exponential prior smoothing scheme (Good-
man 2003) is adopted in the training.  
36
3 Context Clustering based on Context 
Pair Classification Results 
Given n  mentions { }iC of a keyword, we use the 
following context clustering scheme. The discov-
ered context clusters correspond to distinct word 
senses.  
For any given context pair, the context similarity 
features defined in Section 2 are computed. With n 
mentions of the same keyword, 2
)1( ?nn  context 
similarities [ ] [ )( )ijniCS ji ,1,,1 , ??  are computed. 
Using the context pair classification model, each 
pair is associated with two scores ( )( )jijiji CSSSsc ,0, Prlog ==  and 
( )( )jijiji CSSSsc ,1, Prlog ==  which correspond to 
the probabilities of two situations: the pair refers to 
the same or different word senses. 
Now we introduce the symbol { }MK ,  which re-
fers to the final context cluster configuration, 
where K refers to the number of distinct sense, and 
M represents the many-to-one mapping (from con-
texts to a sense) such that 
( ) K]. [1,j n],[1,i j,iM ??= Based on the pairwise 
scores { } 0, jisc and  { } 1, jisc , WSD is formulated as 
searching for { }MK , which maximizes the follow-
ing global scores: 
 
{ }( ) ( )
[ ][ )
 MK,c
,1
,n1,i
,
,

?
?
=
ij
jik
jiscs  (2) 
where ( ) ( ) ( )



=
= otherwise
jMiMifjik      ,1
 ,0,  
 
Similar clustering scheme has been used success-
fully for the task of co-reference in (Luo etc. 
2004), (Zelenko, Aone and Tibbetts, 2004a) and 
(Zelenko, Aone and Tibbetts, 2004b). 
In this paper, statistical annealing-based optimi-
zation (Neal 1993) is used to search for { }MK ,  
which maximizes Expression (2). 
The optimization process consists of two steps. 
First, an intermediate solution { }0, MK  is com-
puted by a greedy algorithm. Then by setting 
{ }0, MK as the initial state, statistical annealing is 
applied to search for the global optimal solution. 
The optimization algorithm is as follows. 
1. Set the initial state { }MK , as nK = , and 
[ ]n1,i  ,)( ?= iiM ; 
2. Select a cluster pair for merging that 
maximally increases 
{ }( ) ( )
[ ][ )
 MK,c
,1
,n1,i
,
,

?
?
=
ij
jik
jiscs  
3. If no cluster pair can be merged to in-
crease { }( ) ( )
[ ][ )
 MK,c
,1
,n1,i
,
,

?
?
=
ij
jik
jiscs , output 
{ }MK , as the intermediate solution; 
otherwise, update { }MK ,  by the merge 
and go to step 2. 
 
Using the intermediate solution { }0, MK of the 
greedy algorithm as the initial state, the statistical 
annealing is implemented using the following 
pseudo-code:  
       Set { } { }0,, MKMK = ; 
       for( 1.01?*;?? ;?? final0 =<= ) 
{ 
    iterate pre-defined number of times 
    { 
          set { } { }MKMK ,, 1 = ; 
     update { }1, MK  by randomly changing 
cluster number and cluster contents;  
            set { }( ){ }( )MK,c
MK,c 1
s
sx =  
           if(x>=1) 
           { 
             set { } { }1,, MKMK =  
           } 
          else 
          { 
             set { } { }1,, MKMK =  with probability  
              ?x . 
          } 
         if { }( ) { }( )0MK,cMK,c ss >  
         then set { } { }MKMK ,, 0 =  
     } 
  } 
  output { }0, MK  as the optimal state. 
 
37
4 Benchmarking 
Corpus-driven context clusters need to map to a 
word sense standard to facilitate performance 
benchmark. Using Senseval-3 evaluation stan-
dards, we implemented the following procedure to 
map the context clusters:  
 
i) Process TIPSTER corpus and the origi-
nal unlabeled Senseval-3 corpora (in-
cluding the training corpus and the 
testing corpus) by our parser, and save 
all the parsing results into a repository.  
 
ii) For each keyword, all related contexts in 
Senseval-3 corpora and up-to-1,000 re-
lated contexts in TIPSTER corpus are 
retrieved from the repository.  
 
iii) All the retrieved contexts are clustered 
based on the context clustering algo-
rithm presented in Sect. 2 and 3. 
 
iv) For each keyword sense, three annotated 
contexts from Senseval-3 training cor-
pus are used for the sense mapping. The 
context cluster is mapped onto the most 
frequent word sense associated with the 
cluster members. By design, the context 
clusters correspond to distinct senses, 
therefore, we do not allow multiple con-
text clusters to be mapped onto one 
sense. In case multiple clusters corre-
spond to one sense, only the largest 
cluster is retained.  
 
v) Each context in the testing corpus is 
tagged with the sense to which its con-
text cluster corresponds to. 
 
As mentioned above, Sensval-2 English lexical 
sample training corpora is used to train the context 
pair classification model. And Sensval-3 English 
lexical sample testing corpora is used here for 
benchmarking. There are several keyword occur-
ring in both Senseval-2 and Senseval-3 corpora. 
The sense tags associated with these keywords are 
not used in the context pair classification training 
process.  
In order to gauge the performance of this new 
weakly supervised learning algorithm, we have 
also implemented a supervised Na?ve Bayes sys-
tem following (Gale et al 1992).  This system is 
trained based on the Senseval-3 English Lexical 
Sample training corpus.  In addition, for the pur-
pose of quantifying the contribution from the pars-
ing structures in WSD, we have run our new 
system with two configurations: (i) using only 
trigger words; (ii) using both trigger words and 
parsing relationships. All the benchmarking is per-
formed using the Senseval-3 English Lexical Sam-
ple testing corpus and standards.  
The performance benchmarks for the two sys-
tems in three runs are shown in Table 1, Table 2 
and Table 3. When using only trigger words, this 
algorithm has 8 percentage degradation from the 
supervised Na?ve Bayes system (see Table 1 vs. 
Table 2). When adding parsing structures, per-
formance degradation is reduced, with about 5 per-
centage drop (see Table 3 vs. Table 2). Comparing 
Table 1 with Table 3, we observe about 3% en-
hancement due to the contribution from the parsing 
support in WSD. The benchmark of our algorithm 
using both trigger words and parsing relationships 
is one of the best in unsupervised category of the 
Senseval-3 Lexical Sample evaluation. 
 
Table 1. New Algorithm Using Only Trigger Words  
Accuracy  
Category Fine grain (%) Coarse grain (%) 
Adjective (5) 46.3 60.8 
Noun (20) 54.6 62.8 
Verb (32) 54.1 64.2 
Overall  54.0 63.4 
 
Table 2. Supervised Na?ve Bayes System 
Accuracy  
Category Fine grain (%) Coarse grain (%) 
Adjective (5) 44.7 56.6 
Noun (20) 66.3 74.5 
Verb (32) 58.6 70.0 
Overall 61.6 71.5 
 
Table 3. New Algorithm Using Both Trigger Words and 
Parsing  
Accuracy  
Category Fine grain (%) Coarse grain (%) 
Adjective (5) 49.1 64.8 
Noun (20) 57.9 66.6 
Verb (32) 55.3 66.3 
Overall 56.3 66.4 
38
 
It is noted that Na?ve Bayes algorithm has many 
variation, and its performance has been greatly 
enhanced during recent research. Based on Sen-
seval-3 results, the best Na?ve Bayse system out-
perform our version (which is implemented based 
on Gale et al 1992) by 8%~10%. So the best su-
pervised WSD systems output-perform our weakly 
supervised WSD system by 13%~15% in accuracy. 
5 Conclusion 
We have presented a weakly supervised learning 
approach to WSD. Statistical models are not 
trained for the contexts of each individual word, 
but for context pair classification. This approach 
overcomes the knowledge bottleneck that chal-
lenges supervised WSD systems which need la-
beled data for each individual word. It captures the 
correlation regularity between the sense distinction 
and the context distinction at Part-of-Speech cate-
gory level, independent of individual words and 
senses. Hence, it only requires a limited amount of 
existing annotated corpus in order to disambiguate 
the full target set of ambiguous words, in particu-
lar, the target words that do not appear in the train-
ing corpus.   
The weakly supervised learning scheme can 
combine trigger words and parsing structures in 
supporting WSD. Using Senseval-3 English Lexi-
cal Sample benchmarking, this new approach 
reaches one of the best scores in the unsupervised 
category of English Lexical Sample evaluation. 
This performance is close to the performance for 
the supervised Na?ve Bayes system. 
In the future, we will implement a new scheme 
to map context clusters onto WordNet senses by 
exploring WordNet glosses and sample sentences. 
Based on the new sense mapping scheme, we will 
benchmark our system performance using Senseval 
English all-words corpora.  
 
References  
Deerwester, S., S. T. Dumais, G. W. Furnas, T. K. 
Landauer, and R. Harshman. 1990. Indexing by 
Latent Semantic Analysis. In Journal of the 
American Society of Information Science 
Gale, W., K. Church, and D. Yarowsky. 1992. A 
Method for Disambiguating Word Senses in a 
Large Corpus. Computers and the Humanities, 
26. 
Goodman, J. 2003. Exponential Priors for Maxi-
mum Entropy Models. In Proceedings of HLT-
NAACL 2004. 
Landauer, T. K., & Dumais, S. T. 1997. A solution 
to Plato's problem: The Latent Semantic Analy-
sis theory of the acquisition, induction, and rep-
resentation of knowledge. Psychological 
Review, 104, 211-240, 1997. 
Luo, X., A. Ittycheriah, H. Jing, N. Kambhatla and 
S. Roukos. A Mention-Synchronous Corefer-
ence Resolution Algorithm Based on the Bell 
Tree. In The Proceedings of ACL 2004. 
Neal, R.M. 1993. Probabilistic Inference Using 
Markov Chain Monte Carlo Methods. Technical 
Report, Univ. of Toronto. 
Pietra, S. D., V. D. Pietra, and J. Lafferty. 1995. 
Inducing Features Of Random Fields. In IEEE 
Transactions on Pattern Analysis and Machine 
Intelligence. 
Sch?tze, H. 1998. Automatic Word Sense Disam-
biguation. Computational Linguistics, 23. 
Yarowsky, D. 1995. Unsupervised Word Sense 
Disambiguation Rivaling Supervised Methods. 
In Proceedings of ACL 1995.  
Zelenko, D., C. Aone and J. 2004. Tibbetts. 
Coreference Resolution for Information Extrac-
tion. In Proceedings of ACL 2004 Workshop on 
Reference Resolution and its Application.  
Zelenko, D., C. Aone and J. 2004. Tibbetts. Binary 
Integer Programming for Information Extrac-
tion. In Proceedings of ACE 2004 Evaluation 
Workshop.  
 
39
