Proceedings of the 12th Conference of the European Chapter of the ACL, pages 229?237,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Cognitively Motivated Features for Readability Assessment 
 
 
 Lijun Feng  No?mie Elhadad  Matt Huenerfauth 
 The City University of New York,  Columbia University  The City University of New York, 
 Graduate Center  New York, NY, USA  Queens College & Graduate Center 
 New York, NY, USA  noemie@dbmi.columbia.edu New York, NY, USA 
 lijun7.feng@gmail.com  matt@cs.qc.cuny.edu 
 
  
 
Abstract 
We investigate linguistic features that correlate 
with the readability of texts for adults with in-
tellectual disabilities (ID).  Based on a corpus 
of texts (including some experimentally meas-
ured for comprehension by adults with ID), we 
analyze the significance of novel discourse-
level features related to the cognitive factors 
underlying our users? literacy challenges.  We 
develop and evaluate a tool for automatically 
rating the readability of texts for these users.  
Our experiments show that our discourse-
level, cognitively-motivated features improve 
automatic readability assessment. 
1 Introduction 
Assessing the degree of readability of a text has 
been a field of research as early as the 1920's. 
Dale and Chall define readability as ?the sum 
total (including all the interactions) of all those 
elements within a given piece of printed material 
that affect the success a group of readers have 
with it. The success is the extent to which they 
understand it, read it at optimal speed, and find it 
interesting? (Dale and Chall, 1949). It has long 
been acknowledged that readability is a function 
of text characteristics, but also of the readers 
themselves.  The literacy skills of the readers, 
their motivations, background knowledge, and 
other internal characteristics play an important 
role in determining whether a text is readable for 
a particular group of people. In our work, we 
investigate how to assess the readability of a text 
for people with intellectual disabilities (ID). 
Previous work in automatic readability as-
sessment has focused on generic features of a 
text at the lexical and syntactic level.  While such 
features are essential, we argue that audience-
specific features that model the cognitive charac-
teristics of a user group can improve the accura-
cy of a readability assessment tool.  The contri-
butions of this paper are: (1) we present a corpus 
of texts with readability judgments from adults 
with ID; (2) we propose a set of cognitively-
motivated features which operate at the discourse 
level; (3) we evaluate the utility of these features 
in predicting readability for adults with ID. 
Our framework is to create tools that benefit 
people with intellectual disabilities (ID), specifi-
cally those classified in the ?mild level? of men-
tal retardation, IQ scores 55-70.  About 3% of 
the U.S. population has intelligence test scores of 
70 or lower (U.S. Census Bureau, 2000).  People 
with ID face challenges in reading literacy.  They 
are better at decoding words (sounding them out) 
than at comprehending their meaning (Drew & 
Hardman, 2004), and most read below their men-
tal age-level (Katims, 2000).  Our research ad-
dresses two literacy impairments that distinguish 
people with ID from other low-literacy adults: 
limitations in (1) working memory and (2) dis-
course representation.  People with ID have 
problems remembering and inferring information 
from text (Fowler, 1998).  They have a slower 
speed of semantic encoding and thus units are 
lost from the working memory before they are 
processed (Perfetti & Lesgold, 1977; Hickson-
Bilsky, 1985).  People with ID also have trouble 
building cohesive representations of discourse 
(Hickson-Bilsky, 1985).  As less information is 
integrated into the mental representation of the 
current discourse, less is comprehended.   
Adults with ID are limited in their choice of 
reading material.  Most texts that they can readi-
ly understand are targeted at the level of reada-
bility of children.  However, the topics of these 
texts often fail to match their interests since they 
are meant for younger readers.  Because of the 
mismatch between their literacy and their inter-
ests, users may not read for pleasure and there-
fore miss valuable reading-skills practice time.  
In a feasibility study we conducted with adults 
229
with ID, we asked participants what they enjoyed 
learning or reading about.  The majority of our 
subjects mentioned enjoying watching the news, 
in particular local news.  Many mentioned they 
were interested in information that would be re-
levant to their daily lives.  While for some ge-
nres, human editors can prepare texts for these 
users, this is not practical for news sources that 
are frequently updated and specific to a limited 
geographic area (like local news). Our goal is to 
create an automatic metric to predict the reada-
bility of local news articles for adults with ID.  
Because of the low levels of written literacy 
among our target users, we intend to focus on 
comprehension of texts displayed on a computer 
screen and read aloud by text-to-speech software; 
although some users may depend on the text-to-
speech software, we use the term readability. 
This paper is organized as follows.  Section 2 
presents related work on readability assessment. 
Section 3 states our research hypotheses and de-
scribes our methodology.  Section 4 focuses on 
the data sets used in our experiments, while sec-
tion 5 describes the feature set we used for rea-
dability assessment along with a corpus-based 
analysis of each feature.  Section 6 describes a 
readability assessment tool and reports on evalu-
ation.  Section 7 discusses the implications of the 
work and proposes direction for future work. 
2 Related Work on Readability Metrics 
Many readability metrics have been established 
as a function of shallow features of texts, such as 
the number of syllables per word and number of 
words per sentence (Flesch, 1948; McLaughlin, 
1969; Kincaid et al, 1975). These so-called tra-
ditional readability metrics are still used today in 
many settings and domains, in part because they 
are very easy to compute. Their results, however, 
are not always representative of the complexity 
of a text (Davison and Kantor, 1982). They can 
easily misrepresent the complexity of technical 
texts, or reveal themselves un-adapted to a set of 
readers with particular reading difficulties. Other 
formulas rely on lexical information; e.g., the 
New Dale-Chall readability formula consults a 
static, manually-built list of ?easy? words to de-
termine whether a text contains unfamiliar words 
(Chall and Dale, 1995).  
Researchers in computational linguistics have 
investigated the use of statistical language mod-
els (unigram in particular) to capture the range of 
vocabulary from one grade level to another (Si 
and Callan, 2001; Collins-Thompson and Callan, 
2004). These metrics predicted readability better 
than traditional formulas when tested against a 
corpus of web pages. The use of syntactic fea-
tures was also investigated (Schwarm and Osten-
dorf, 2005; Heilman et al, 2007; Petersen and 
Ostendorf, 2009) in the assessment of text reada-
bility for English as a Second Language readers. 
While lexical features alone outperform syntactic 
features in classifying texts according to their 
reading levels, combining the lexical and syntac-
tic features yields the best results. 
Several elegant metrics that focus solely on 
the syntax of a text have also been developed.  
The Yngve (1960) measure, for instance, focuses 
on the depth of embedding of nodes in the parse 
tree; others use the ratio of terminal to non-
terminal nodes in the parse tree of a sentence 
(Miller and Chomsky, 1963; Frazier, 1985).  
These metrics have been used to analyze the 
writing of potential Alzheimer's patients to detect 
mild cognitive impairments (Roark, Mitchell, 
and Hollingshead, 2007), thereby indicating that 
cognitively motivated features of text are valua-
ble when creating tools for specific populations. 
Barzilay and Lapata (2008) presented early 
work in investigating the use of discourse to dis-
tinguish abridged from original encyclopedia 
articles.  Their focus, however, is on style detec-
tion rather than readability assessment per se.  
Coh-Metrix is a tool for automatically calculat-
ing text coherence based on features such as re-
petition of lexical items across sentences and 
latent semantic analysis (McNamara et al, 
2006).  The tool is based on comprehension data 
collected from children and college students. 
Our research differs from related work in that 
we seek to produce an automatic readability me-
tric that is tailored to the literacy skills of adults 
with ID.  Because of the specific cognitive cha-
racteristics of these users, it is an open question 
whether existing readability metrics and features 
are useful for assessing readability for adults 
with ID.  Many of these earlier metrics have fo-
cused on the task of assigning texts to particular 
elementary school grade levels.  Traditional 
grade levels may not be the ideal way to score 
texts to indicate how readable they are for adults 
with ID.  Other related work has used models of 
vocabulary (Collins-Thompson and Callan, 
2004).  Since we would like to use our tool to 
give adults with ID access to local news stories, 
we choose to keep our metric topic-independent. 
Another difference between our approach and 
previous approaches is that we have designed the 
features used by our readability metric based on 
230
the cognitive aspects of our target users.  For ex-
ample, these users are better at decoding words 
than at comprehending text meaning (Drew & 
Hardman, 2004); so, shallow features like ?sylla-
ble count per word? or unigram models of word 
frequency (based on texts designed for children) 
may be less important indicators of reading diffi-
culty.  A critical challenge for our users is to 
create a cohesive representation of discourse.  
Due to their impairments in semantic encoding 
speed, our users may have particular difficulty 
with texts that place a significant burden on 
working memory (items fall out of memory be-
fore they can be semantically encoded).   
While we focus on readability of texts, other 
projects have automatically generated texts for 
people with aphasia (Carroll et al, 1999) or low 
reading skills (Williams and Reiter, 2005). 
3 Research Hypothesis and Methods 
We hypothesize that the complexity of a text for 
adults with ID is related to the number of entities 
referred to in the text overall.  If a paragraph or a 
text refers to too many entities at once, the reader 
has to work harder at mapping each entity to a 
semantic representation and deciding how each 
entity is related to others.  On the other hand, 
when a text refers to few entities, less work is 
required both for semantic encoding and for in-
tegrating the entities into a cohesive mental re-
presentation.  Section 5.2 discusses some novel 
discourse-level features (based on the ?entity 
density? of a text) that we believe will correlate 
to comprehension by adults with ID.   
To test our hypothesis, we used the following 
methodology.  We collected four corpora (as de-
scribed in Section 4).  Three of them (Britannica, 
LiteracyNet and WeeklyReader) have been ex-
amined in previous work on readability.  The 
fourth (LocalNews) is novel and results from a 
user study we conducted with adults with ID.  
We then analyzed how significant each feature is 
on our Britannica and LiteracyNet corpora.  Fi-
nally, we combined the significant features into a 
linear regression model and experimented with 
several feature combinations. We evaluated our 
model on the WeeklyReader and LocalNews 
corpora. 
4 Corpora and Readability Judgments  
To study how certain linguistic features indicate 
the readability of a text, we collected a corpus of 
English text at different levels of readability.  An 
ideal corpus for our research would contain texts 
that have been written specifically for our au-
dience of adults with intellectual disabilities ? in 
particular if such texts were paired with alternate 
versions of each text written for a general au-
dience.  We are not aware of such texts available 
electronically, and so we have instead mostly 
collected texts written for an audience of child-
ren.  The texts come from online and commercial 
sources, and some have been analyzed previous-
ly by text simplification researchers (Petersen 
and Ostendorf, 2009).  Our corpus also contains 
some novel texts produced as part of an experi-
mental study involving adults with ID. 
4.1 Paired and Graded Generic Corpora: 
Britannica, LiteracyNet, and Weekly 
Reader 
The first section of our corpus (which we refer to 
as Britannica) has 228 articles from the Encyclo-
pedia Britannica, originally collected by (Barzi-
lay and Elhadad, 2003).  This consists of 114 
articles in two forms: original articles written for 
adults and corresponding articles rewritten for an 
audience of children.  While the texts are paired, 
the content of the texts is not identical: some de-
tails are omitted from the child version, and addi-
tional background is sometimes inserted.  The 
resulting corpus is comparable in content. 
Because we are particularly interested in mak-
ing local news articles accessible to adults with 
ID, we collected a second paired corpus, which 
we refer to as LiteracyNet, consisting of 115 
news articles made available through (West-
ern/Pacific Literacy Network / LiteracyNet, 
2008).  The collection of local CNN stories is 
available in an original and simplified/abridged 
form (230 total news articles) designed for use in 
literacy education. 
The third corpus we collected (Weekly Reader) 
was obtained from the Weekly Reader corpora-
tion (Weekly Reader, 2008).  It contains articles 
for students in elementary school.  Each text is 
labeled with its target grade level (grade 2: 174 
articles, grade 3: 289 articles, grade 4: 428 ar-
ticles, grade 5: 542 articles).  Overall, the corpus 
has 1433 articles. (U.S. elementary school grades 
2 to 5 generally are for children ages 7 to 10.) 
The corpora discussed above are similar to 
those used by Petersen and Ostendorf (2009).  
While the focus of our research is adults with ID, 
most of the texts discussed in this section have 
been simplified or written by human authors to 
be readable for children.  Despite the texts being 
intended for a different audience than the focus 
of our research, we still believe these texts to be 
231
of value.  It is rare to encounter electronically 
available corpora in which an original and a sim-
plified version of a text is paired (as in the Bri-
tannica and LiteracyNet corpora) or texts labeled 
as being at specific levels of readability (as in the 
Weekly Reader corpus). 
4.2 Readability-Specific Corpus: LocalNews 
The final section of our corpus contains local 
news articles that are labeled with comprehen-
sion scores.  These texts were produced for a fea-
sibility study involving adults with ID.  Each text 
was read by adults with ID, who then answered 
comprehension questions to measure their under-
standing of the texts.  Unlike the previous corpo-
ra, LocalNews is novel and was not investigated 
by previous research in readability. 
After obtaining university approval for our ex-
perimental protocol and informed consent 
process, we conducted a study with 14 adults 
with mild intellectual disabilities who participate 
in daytime educational programs in the New 
York area.  Participants were presented with ten 
articles collected from various local New York 
based news websites.  Some subjects saw the 
original form of an article and others saw a sim-
plified form (edited by a human author); no sub-
ject saw both versions.  The texts were presented 
in random order using software that displayed 
the text on the screen, read it aloud using text-to-
speech software, and highlighted each word as it 
was read.  Afterward, subjects were asked aloud 
multiple-choice comprehension questions. We 
defined the readability score of a story as the 
percentage of correct answers averaged across 
the subjects who read that particular story. 
A human editor performed the text simplifica-
tion with the goal of making the text more reada-
ble for adults with mild ID.  The editor made the 
following types of changes to the original news 
stories: breaking apart complex sentences, un-
embedding information in complex prepositional 
phrases and reintegrating it as separate sentences, 
replacing infrequent vocabulary items with more 
common/colloquial equivalents, omitting sen-
tences and phrases from the story that mention 
entities and phrases extraneous to the main 
theme of the article.  For instance, the original 
sentence ?They?re installing an induction loop 
system in cabs that would allow passengers with 
hearing aids to tune in specifically to the driver?s 
voice.? was transformed into ?They?re installing 
a system in cabs. It would allow passengers with 
hearing aids to listen to the driver?s voice.? 
This corpus of local news articles that have 
been human edited and scored for comprehen-
sion by adults with ID is small in size (20 news 
articles), but we consider it a valuable resource.  
Unlike the texts that have been simplified for 
children (the rest of our corpus), these texts have 
been rated for readability by actual adults with 
ID.  Furthermore, comprehension scores are de-
rived from actual reader comprehension tests, 
rather than self-perceived comprehension.  Be-
cause of the small size of this part of our corpus, 
however, we primarily use it for evaluation pur-
poses (not for training the readability models). 
5 Linguistic Features and Readability  
We now describe the set of features we investi-
gated for assessing readability automatically.  
Table 1 contains a list of the features ? including 
a short code name for each feature which may be 
used throughout this paper.  We have begun by 
implementing the simple features used by the 
Flesh-Kincaid and FOG metrics: average number 
of words per sentence, average number of syl-
lables per word, and percentage of words in the 
document with 3+ syllables. 
5.1 Basic Features Used in Earlier Work 
We have also implemented features inspired by 
earlier research on readability.  Petersen and Os-
tendorf (2009) included features calculated from 
parsing the sentences in their corpus using the 
Charniak parser (Charniak, 2000): average parse 
tree height, average number of noun phrases per 
sentence, average number of verb phrases per 
sentence, and average number of SBARs per sen-
tence. We have implemented versions of most of 
these parse-tree-related features for our project.  
We also parse the sentences in our corpus using 
Charniak?s parser and calculate the following 
features listed in Table 1: aNP, aN, aVP, aAdj, 
aSBr, aPP, nNP, nN, nVP, nAdj, nSBr, and nPP.   
5.2 Novel Cognitively-Motivated Features  
Because of the special reading characteristics of 
our target users, we have designed a set of cogni-
tively motivated features to predict readability of 
texts for adults with ID.  We have discussed how 
working memory limits the semantic encoding of 
new information by these users; so, our features 
indicate the number of entities in a text that the 
reader must keep in mind while reading each 
sentence and throughout the entire document.  It 
is our hypothesis that this ?entity density? of a 
232
text plays an important role in the difficulty of 
that text for readers with intellectual disabilities. 
The first set of features incorporates the Ling-
Pipe named entity detection software (Alias-i, 
2008), which detects three types of entities: per-
son, location, and organization.  We also use the 
part-of-speech tagger in LingPipe to identify the 
common nouns in the document, and we find the 
union of the common nouns and the named entity 
noun phrases in the text.  The union of these two 
sets is our definition of ?entity? for this set of 
features.  We count both the total number of 
?entity mentions? in a text (each token appear-
ance of an entity) and the total number of unique 
entities (exact-string-match duplicates only 
counted once).  Table 1 lists these features: nEM, 
nUE, aEM, and aUE.  We count the totals per 
document to capture how many entities the read-
er must keep track of while reading the docu-
ment.  We also expect sentences with more enti-
ties to be more difficult for our users to semanti-
cally encode due to working memory limitations; 
so, we also count the averages per sentence to 
capture how many entities the reader must keep 
in mind to understand each sentence.   
To measure the working memory burden of a 
text, we?d like to capture the number of dis-
course entities that a reader must keep in mind.  
However, the ?unique entities? identified by the 
named entity recognition tool may not be a per-
fect representation of this ? several unique enti-
ties may actually refer to the same real-world 
entity under discussion.  To better model how 
multiple noun phrases in a text refer to the same 
entity or concept, we have also built features us-
ing lexical chains (Galley and McKeown, 2003).  
Lexical chains link nouns in a document con-
nected by relations like synonymy or hyponomy; 
chains can indicate concepts that recur through-
out a text.  A lexical chain has both a length 
(number of noun phrases it includes) and a span 
(number of words in the document between the 
first noun phrase at the beginning of the chain 
and the last noun phrase that is part of the chain).  
We calculate the number of lexical chains in the 
document (nLC) and those with a span greater 
than half the document length (nLC2).  We be-
lieve these features may indicate the number of 
entities/concepts that a reader must keep in mind 
during a document and the subset of very impor-
tant entities/concepts that are the main topic of 
the document.  The average length and average 
span of the lexical chains in a document (aLCL 
and aLCS) may also indicate how many of the 
chains in the document are short-lived, which 
may mean that they are ancillary enti-
ties/concepts, not the main topics. 
The final two features in Table 1 (aLCw and 
aLCe) use the concept of an ?active? chain.  At a 
particular location in a text, we define a lexical 
chain to be ?active? if the span (between the first 
and last noun in the lexical chain) includes the 
current location.  We expect these features may 
indicate the total number of concepts that the 
reader needs to keep in mind during a specific 
moment in time when reading a text.  Measuring 
the average number of concepts that the reader of 
a text must keep in mind may suggest the work-
ing memory burden of the text over time.  We 
were unsure if individual words or individual 
noun-phrases in the document should be used as 
the basic unit of ?time? for the purpose of aver-
aging the number of active lexical chains; so, we 
included both features. 
5.3 Testing the Significance of Features 
To select which features to include in our auto-
matic readability assessment tool (in Section 6), 
Code Feature
aWPS average number of words per sentence
aSPW average number of syllables per word
%3+S % of words in document with 3+ syllables
aNP avg. num. NPs per sentence
aN avg. num. common+proper nouns per sentence
aVP avg. num. VPs per sentence
aAdj avg. num. Adjectives per sentence
aSBr avg. num. SBARs per sentence
aPP avg. num. prepositional phrases per sentence
nNP total number of NPs per sentence
nN total num. of common+proper nouns in document
nVP total number of VPs in the document
nAdj total number of Adjectives in the document
nSBr total number of SBARs in the document
nPP total num. of prepositional phrases in document
nEM number of entity mentions in document
nUE number of unique entit ies in document
aEM avg. num. entity mentions per sentence
aUE avg. num. unique entit ies per sentence
nLC number of lexical chains in document
nLC2 num. lex. chains, span > half document length
aLCL average lexical chain length
aLCS average lexical chain span
aLCw avg. num. lexical chains active at  each word
aLCn avg. num. lexical chains active at  each NP
Table 1: Implemented Features
233
we analyzed the documents in our paired corpora 
(Britannica and LiteracyNet).  Because they con-
tain a complex and a simplified version of each 
article, we can examine differences in readability 
while holding the topic and genre constant.  We 
calculated the value of each feature for each doc-
ument, and we used a paired t-test to determine if 
the difference between the complex and simple 
documents was significant for that corpus. 
Table 2 contains the results of this feature se-
lection process; the columns in the table indicate 
the values for the following corpora: Britannica 
complex, Britannica simple, LiteracyNet com-
plex, and LiteracyNet simple.  An asterisk ap-
pears in the ?Sig? column if the difference be-
tween the feature values for the complex vs. 
simple documents is statistically significant for 
that corpus (significance level: p<0.00001).   
The only two features which did not show a 
significant difference (p>0.01) between the com-
plex and simple versions of the articles were: 
average lexical chain length (aLCL) and number 
of lexical chains with span greater than half the 
document length (nLC2).  The lack of signific-
ance for aLCL may be explained by the vast ma-
jority of lexical chains containing few members; 
complex articles contained more of these chains 
? but their chains did not contain more members.  
In the case of nLC2, over 80% of the articles in 
each category contained no lexical chains whose 
span was greater than half the document length.  
The rarity of a lexical chain spanning the majori-
ty of a document may have led to there being no 
significant difference between complex/simple. 
6 A Readability Assessment Tool 
After testing the significance of features using 
paired corpora, we used linear regression and our 
graded corpus (Weekly Reader) to build a reada-
bility assessment tool.  To evaluate the tool?s 
usefulness for adults with ID, we test the correla-
tion of its scores with the LocalNews corpus. 
6.1 Versions of Our Model 
We began our evaluation by implementing three 
versions of our automatic readability assessment 
tool.  The first version uses only those features 
studied by previous researchers (aWPS, aSPW, 
%3+S, aNP, aN, aVP, aAdj, aSBr, aPP, nNP, nN, 
nVP, nAdj, nSBr, nPP).  The second version uses 
only our novel cognitively motivated features 
(section 5.2).  The third version uses the union of 
both sets of features.  By building three versions 
of the tool, we can compare the relative impact 
of our novel cognitively-motivated features.  For 
all versions, we have only included those fea-
tures that showed a significant difference be-
tween the complex and simple articles in our 
paired corpora (as discussed in section 5.3). 
6.2 Learning Technique and Training Data 
Early work on automatic readability analysis 
framed the problem as a classification task: 
creating multiple classifiers for labeling a text as 
being one of several elementary school grade 
levels (Collins-Thompson and Callan, 2004).  
Because we are focusing on a unique user group 
with special reading challenges, we do not know 
a priori what level of text difficulty is ideal for 
our users.  We would not know where to draw 
category boundaries for classification.  We also 
prefer that our assessment tool assign numerical 
difficulty scores to texts.  Thus, after creating 
this tool, we can conduct further reading com-
prehension experiments with adults with ID to 
determine what threshold (for readability scores 
assigned by our tool) is appropriate for our users. 
Feature
Brit. 
Com.
Brit. 
Simp. Sig
LitN. 
Com.
LitN. 
Simp. Sig
aWPS 20.13 14.37 * 17.97 12.95 *
aSPW 1.708 1.655 * 1.501 1.455 *
%3+S 0.196 0.177 * 0.12 0.101 *
aNP 8.363 6.018 * 6.519 4.691 *
aN 7.024 5.215 * 5.319 3.929 *
aVP 2.334 1.868 * 3.806 2.964 *
aAdj 1.95 1.281 * 1.214 0.876 *
aSBr 0.266 0.205 * 0.793 0.523 *
aPP 2.858 1.936 * 1.791 1.22 *
nNP 798 219.2 * 150.2 102.9 *
nN 668.4 190.4 * 121.4 85.75 *
nVP 242.8 69.19 * 88.2 65.52 *
nAdj 205 47.32 * 28.11 19.04 *
nSBr 31.33 7.623 * 18.16 11.43 *
nPP 284.7 70.75 * 41.06 26.79 *
nEM 624.2 172.7 * 115.2 82.83 *
nUE 355 117 * 81.56 54.94 *
aEM 6.441 4.745 * 5.035 3.789 *
aUE 4.579 3.305 * 3.581 2.55 *
nLC 59.21 17.57 * 12.43 8.617 *
nLC2 0.175 0.211 0.191 0.226
aLCL 3.009 3.022 2.817 2.847
aLCS 357 246.1 * 271.9 202.9 *
aLCw 1.803 1.358 * 1.407 1.091 *
aLCn 1.852 1.42 * 1.53 1.201 *
Table 2: Feature Values of Paired Corpora
234
To select features for our model, we used our 
paired corpora (Britannica and LiteracyNet) to 
measure the significance of each feature.  Now 
that we are training a model, we make use of our 
graded corpus (articles from Weekly Reader).  
This corpus contains articles that have each been 
labeled with an elementary school grade level for 
which it was written.  We divide this corpus ? 
using 80% of articles as training data and 20% as 
testing data.  We model the grade level of the 
articles using linear regression; our model is im-
plemented using R (R Development Core Team, 
2008).  
6.3 Evaluation of Our Readability Tool 
We conducted two rounds of training and evalua-
tion of our three regression models.  We also 
compare our models to a baseline readability as-
sessment tool: the popular Flesh-Kincaid Grade 
Level index (Kincaid et al, 1975).  
In the first round of evaluation, we trained and 
tested our regression models on the Weekly 
Reader corpus.  This round of evaluation helped 
to determine whether our feature-set and regres-
sion technique were successfully modeling those 
aspects of the texts that were relevant to their 
grade level.  Our results from this round of eval-
uation are presented in the form of average error 
scores.  (For each article in the Weekly Reader 
testing data, we calculate the difference between 
the output score of the model and the correct 
grade-level for that article.)  Table 3 presents the 
average error results for the baseline system and 
our three regression models.  We can see that the 
model trained on the shallow and parse-related 
features out-performs the model trained only on 
our novel features; however, the best model 
overall is the one is trained on all of the features.  
This model predicts the grade level of Weekly 
Reader articles to within roughly 0.565 grade 
levels on average.   
 
Readability Model (or baseline) Average Error 
Baseline: Flesh-Kincaid Index 2.569 
Basic Features Only 0.6032 
Cognitively Motivated Features Only 0.6110 
Basic + Cognitively-Motiv. Features 0.5650 
Table 3: Predicting Grade Level of Weekly Reader 
 
In our second round of evaluation, we trained 
the regression model on the Weekly Reader cor-
pus, but we tested it against the LocalNews cor-
pus.  We measured the correlation between our 
regression models? output and the comprehen-
sion scores of adults with ID on each text.  For 
this reason, we do not calculate the ?average er-
ror?; instead, we simply measure the correlation 
between the models? output and the comprehen-
sion scores. (We expect negative correlations 
because comprehension scores should increase as 
the predicted grade level of the text goes down.)  
Table 4 presents the correlations for our three 
models and the baseline system in the form of 
Pearson?s R-values.  We see a surprising result: 
the model trained only on the cognitively-
motivated features is more tightly correlated with 
the comprehension scores of the adults with ID.  
While the model trained on all features was bet-
ter at assigning grade levels to Weekly Reader 
articles, when we tested it on the local news ar-
ticles from our user-study, it was not the top-
performing model.  This result suggests that the 
shallow and parse-related features of texts de-
signed for children (the Weekly Reader articles, 
our training data) are not the best predictors of 
text readability for adults with ID.   
 
Readability Model (or baseline) Pearson?s R 
Baseline: Flesh-Kincaid Index -0.270 
Basic Features Only -0.283 
Cognitively Motivated Features Only -0.352 
Basic + Cognitively-Motiv. Features -0.342 
Table 4: Correlation to User-Study Comprehension 
7 Discussion 
Based on the cognitive and literacy skills of 
adults with ID, we designed novel features that 
were useful in assessing the readability of texts 
for these users.  The results of our study have 
supported our hypothesis that the complexity of a 
text for adults with ID is related to the number of 
entities referred to in the text.  These ?entity den-
sity? features enabled us to build models that 
were better at predicting text readability for 
adults with intellectual disabilities.  
This study has also demonstrated the value of 
collecting readability judgments from target us-
ers when designing a readability assessment tool.  
The results in Table 4 suggest that models 
trained on corpora containing texts designed for 
children may not always lead to accurate models 
of the readability of texts for other groups of 
low-literacy users.  Using features targeting spe-
cific aspects of literacy impairment have allowed 
us to make better use of children?s texts when 
designing a model for adults with ID. 
7.1 Future Work 
In order to study more features and models of 
readability, we will require more testing data for 
tracking progress of our readability regression 
235
models.  Our current study has illustrated the 
usefulness of texts that have been evaluated by 
adults with ID, and we therefore plan to increase 
the size of this corpus in future work.   In addi-
tion to using this corpus for evaluation, we may 
want to use it to train our regression models.  For 
this study, we trained on Weekly Reader text 
labeled with elementary school grade levels, but 
this is not ideal.  Texts designed for children may 
differ from those that are best for adults with ID, 
and ?grade levels? may not be the best way to 
rank/rate text readability for these users.  While 
our user-study comprehension-test corpus is cur-
rently too small for training, we intend to grow 
the size of this corpus in future work.   
We also plan on refining our cognitively moti-
vated features for measuring the difficulty of a 
text for our users.  Currently, we use lexical 
chain software to link noun phrases in a docu-
ment that may refer to similar entities/concepts.  
In future work, we plan to use co-reference reso-
lution software to model how multiple ?entity 
mentions? may refer to a single discourse entity.  
For comparison purposes, we plan to imple-
ment other features that have been used in earlier 
readability assessment systems.  For example, 
Petersen and Ostendorf (2009) created lists of the 
most common words from the Weekly Reader 
articles, and they used the percentage of words in 
a document not on this list as a feature.   
The overall goal of our research is to develop 
a software system that can automatically simplify 
the reading level of local news articles and 
present them in an accessible way to adults with 
ID.  Our automatic readability assessment tool 
will be a component in this future text simplifica-
tion system.  We have therefore preferred to in-
clude features in our tool that focus on aspects of 
the text that can be modified during a simplifica-
tion process.  In future work, we will study how 
to use our readability assessment tool to guide 
how a text revision system decides to modify a 
text to increase its readability for these users. 
7.2 Summary of Contributions 
We have contributed to research on automatic 
readability assessment by designing a new me-
thod for assessing the complexity of a text at the 
level of discourse.  Our novel ?entity density? 
features are based on named entity and lexical 
chain software, and they are inspired by the cog-
nitive underpinnings of the literacy challenges of 
adults with ID ? specifically, the role of slow 
semantic encoding and working memory limita-
tions.  We have demonstrated the usefulness of 
these novel features in modeling the grade level 
of elementary school texts and in correlating to 
readability judgments from adults with ID.   
Another contribution of our work is the collec-
tion of an initial corpus of texts of local news 
stories that have been manually simplified by a 
human editor.  Both the original and the simpli-
fied versions of these stories have been evaluated 
by adults with intellectual disabilities.  We have 
used these comprehension scores in the evalua-
tion phase of this study, and we have suggested 
how constructing a larger corpus of such articles 
could be useful for training readability tools. 
More broadly, this project has demonstrated 
how focusing on a specific user population, ana-
lyzing their cognitive skills, and involving them 
in a user-study has led to new insights in model-
ing text readability.  As Dale and Chall?s defini-
tion (1949) originally argued, characteristics of 
the reader are central to the issue of readability.  
We believe our user-focused research paradigm 
may be used to drive further advances in reada-
bility assessment for other groups of users. 
Acknowledgements 
We thank the Weekly Reader Corporation for 
making its corpus available for our research.  We 
are grateful to Martin Jansche for his assistance 
with the statistical data analysis and regression. 
References  
Alias-i. 2008. LingPipe 3.6.0. http://alias-
i.com/lingpipe (accessed October 1, 2008) 
Barzilay, R., Elhadad, N., 2003. Sentence alignment 
for monolingual comparable corpora. In Proc 
EMNLP, pp. 25-32. 
Barzilay R., Lapata, M., 2008. Modeling Local Cohe-
rence: An Entity-based Approach. Computational 
Linguistics. 34(1):1-34. 
Carroll, J., Minnen, G., Pearce, D., Canning, Y., Dev-
lin, S., Tait, J. 1999. Simplifying text for language-
impaired readers. In Proc. EACL Poster, p. 269. 
Chall, J.S., Dale, E., 1995. Readability Revisited: The 
New Dale-Chall Readability Formula. Brookline 
Books, Cambridge, MA. 
Charniak, E. 2000. A maximum-entropy-inspired 
parser.  In Proc. NAACL, pp. 132-139. 
Collins-Thompson, K., and Callan, J.  2004.  A lan-
guage modeling approach to predicting reading dif-
ficulty.  In Proc. NAACL, pp. 193-200. 
Dale, E. and J. S. Chall.  1949.  The concept of reada-
bility.  Elementary English 26(23). 
236
Davison, A., and Kantor, R.  1982.  On the failure of 
readability formulas to define readable texts: A case 
study from adaptations.  Reading Research Quar-
terly, 17(2):187-209. 
Drew, C.J., and Hardman, M.L. 2004.  Mental retar-
dation: A lifespan approach to people with intellec-
tual disabilities (8th ed.).  Columbus, OH: Merrill. 
Flesch, R.  1948.  A new readability yardstick.  Jour-
nal of Applied Psychology, 32:221-233. 
Fowler, A.E.  1998.  Language in mental retardation.  
In Burack, Hodapp, and Zigler (Eds.), Handbook of 
Mental Retardation and Development.  Cambridge, 
UK: Cambridge Univ. Press, pp. 290-333. 
Frazier, L.  1985.  Natural Language Parsing: Psy-
chological, Computational, and Theoretical Pers-
pectives, chapter Syntactic complexity, pp. 129-
189.  Cambridge University Press. 
Galley, M., McKeown, K. 2003. Improving Word 
Sense Disambiguation in Lexical Chaining. In 
Proc. IJCAI, pp. 1486-1488.  
Gunning, R. 1952.  The Technique of Clear Writing. 
McGraw-Hill. 
Heilman, M., Collins-Thompson, K., Callan, J., and 
Eskenazi, M.  2007.  Combining lexical and gram-
matical features to improve readability measures for 
first and second language texts.  In Proc. NAACL, 
pp. 460-467. 
Hickson-Bilsky, L.  1985.  Comprehension and men-
tal retardation.  International Review of Research in 
Mental Retardation, 13: 215-246. 
Katims, D.S.  2000.  Literacy instruction for people 
with mental retardation: Historical highlights and 
contemporary analysis.  Education and Training in 
Mental Retardation and Developmental Disabili-
ties, 35(1): 3-15. 
Kincaid, J. P., Fishburne, R. P., Rogers, R. L., and 
Chissom, B. S.  1975.  Derivation of new readabili-
ty formulas for Navy enlisted personnel, Research 
Branch Report 8-75, Millington, TN. 
Kincaid, J., Fishburne, R., Rodgers, R., and Chisson, 
B.  1975.  Derivation of new readability formulas 
for navy enlisted personnel.  Technical report, Re-
search Branch Report 8-75, U.S.  Naval Air Station. 
McLaughlin, G.H.  1969.  SMOG grading - a new 
readability formula.  Journal of Reading, 
12(8):639-646. 
McNamara, D.S., Ozuru, Y., Graesser, A.C., & Lou-
werse, M. (2006) Validating Coh-Metrix., In Proc. 
Conference of the Cognitive Science Society, pp. 
573.   
Miller, G., and Chomsky, N.  1963.  Handbook of 
Mathematical Psychology, chapter Finatary models 
of language users, pp. 419-491.  Wiley. 
Perfetti, C., and Lesgold, A.  1977.  Cognitive 
Processes in Comprehension, chapter Discourse 
Comprehension and sources of individual differ-
ences.  Erlbaum. 
Petersen, S.E., Ostendorf, M. 2009. A machine learn-
ing approach to reading level assessment. Computer 
Speech and Language, 23: 89-106. 
R Development Core Team. 2008. R: A Language 
and Environment for Statistical Computing. Vienna, 
Austria: R Foundation for Statistical Computing. 
http://www.R-project.org 
Roark, B., Mitchell, M., and Hollingshead, K.  2007.  
Syntactic complexity measures for detecting mild 
cognitive impairment.  In Proc. ACL Workshop on 
Biological, Translational, and Clinical Language 
Processing (BioNLP'07), pp. 1-8. 
Schwarm, S., and Ostendorf, M.  2005.  Reading level 
assessment using support vector machines and sta-
tistical language models.  In Proc. ACL, pp. 523-
530. 
Si, L., and Callan, J.  2001.  A statistical model for 
scientific readability.  In Proc. CIKM, pp. 574-576. 
Stenner, A.J. 1996. Measuring reading comprehension 
with the Lexile framework.  4th North American 
Conference on Adolescent/Adult Literacy. 
U.S. Census Bureau.  2000.  Projections of the total 
resident population by five-year age groups and 
sex, with special age categories: Middle series 
2025-2045.  Washington: U.S. Census Bureau, Po-
pulations Projections Program, Population Division. 
Weekly Reader, 2008. http://www.weeklyreader.com 
(Accessed Oct., 2008). 
Western/Pacific Literacy Network / Literacyworks, 
2008. CNN SF learning resources. 
http://literacynet.org/cnnsf/ (Accessed Oct., 2008). 
Williams, S., Reiter, E. 2005. Generating readable 
texts for readers with low basic skills. In Proc. Eu-
ropean Workshop on Natural Language Genera-
tion, pp. 140-147. 
Yngve, V.  1960.  A model and a hypothesis for lan-
guage structure.  American Philosophical Society, 
104: 446-466. 
237
Coling 2010: Poster Volume, pages 276?284,
Beijing, August 2010
A Comparison of Features for Automatic Readability Assessment
Lijun Feng
City University of New York
lijun7.feng@gmail.com
Martin Jansche
Google, Inc.
jansche@acm.org
Matt Huenerfauth
City University of New York
matt@cs.qc.cuny.edu
Noe?mie Elhadad
Columbia University
noemie@dbmi.columbia.edu
Abstract
Several sets of explanatory variables ? in-
cluding shallow, language modeling, POS,
syntactic, and discourse features ? are com-
pared and evaluated in terms of their im-
pact on predicting the grade level of read-
ing material for primary school students.
We find that features based on in-domain
language models have the highest predic-
tive power. Entity-density (a discourse fea-
ture) and POS-features, in particular nouns,
are individually very useful but highly cor-
related. Average sentence length (a shal-
low feature) is more useful ? and less ex-
pensive to compute ? than individual syn-
tactic features. A judicious combination
of features examined here results in a sig-
nificant improvement over the state of the
art.
1 Introduction
1.1 Motivation and Method
Readability Assessment quantifies the difficulty
with which a reader understands a text. Automatic
readability assessment enables the selection of ap-
propriate reading material for readers of varying
proficiency. Besides modeling and understanding
the linguistic components involved in readability, a
readability-prediction algorithm can be leveraged
for the task of automatic text simplification: as sim-
plification operators are applied to a text, the read-
ability is assessed to determine whether more sim-
plification is needed or a particular reading level
was reached.
Identifying text properties that are strongly cor-
related with text complexity is itself complex. In
this paper, we explore a broad range of text proper-
ties at various linguistic levels, ranging from dis-
course features to language modeling features, part-
of-speech-based grammatical features, parsed syn-
tactic features and well studied shallow features,
many of which are inspired by previous work.
We use grade levels, which indicate the number
of years of education required to completely under-
stand a text, as a proxy for reading difficulty. The
corpus in our study consists of texts labeled with
grade levels ranging from grade 2 to 5. We treat
readability assessment as a classification task and
evaluate trained classifiers in terms of their predic-
tion accuracy. To investigate the contributions of
various sets of features, we build prediction models
and examine how the choice of features influences
the model performance.
1.2 Related Work
Many traditional readability metrics are linear mod-
els with a few (often two or three) predictor vari-
ables based on superficial properties of words, sen-
tences, and documents. These shallow features
include the average number of syllables per word,
the number of words per sentence, or binned word
frequency. For example, the Flesch-Kincaid Grade
Level formula uses the average number of words
per sentence and the average number of syllables
per word to predict the grade level (Flesch, 1979).
The Gunning FOG index (Gunning, 1952) uses av-
erage sentence length and the percentage of words
with at least three syllables. These traditional met-
rics are easy to compute and use, but they are not
reliable, as demonstrated by several recent stud-
ies in the field (Si and Callan, 2001; Petersen and
Ostendorf, 2006; Feng et al, 2009).
276
With the advancement of natural language pro-
cessing tools, a wide range of more complex text
properties have been explored at various linguis-
tic levels. Si and Callan (2001) used unigram
language models to capture content information
from scientific web pages. Collins-Thompson and
Callan (2004) adopted a similar approach and used
a smoothed unigram model to predict the grade lev-
els of short passages and web documents. Heilman
et al (2007) continued using language modeling
to predict readability for first and second language
texts. Furthermore, they experimented with vari-
ous statistical models to test their effectiveness at
predicting reading difficulty (Heilman et al, 2008).
Schwarm/Petersen and Ostendorf (Schwarm and
Ostendorf, 2005; Petersen and Ostendorf, 2006)
used support vector machines to combine features
from traditional reading level measures, statistical
language models and automatic parsers to assess
reading levels. In addition to lexical and syntactic
features, several researchers started to explore dis-
course level features and examine their usefulness
in predicting text readability. Pitler and Nenkova
(2008) used the Penn Discourse Treebank (Prasad
et al, 2008) to examine discourse relations. We
previously used a lexical-chaining tool to extract
entities that are connected by certain semantic re-
lations (Feng et al, 2009).
In this study, we systematically evaluate all
above-mentioned types of features, as well as a
few extensions and variations. A detailed descrip-
tion of the features appears in Section 3. Section
4 discusses results of experiments with classifiers
trained on these features. We begin with a descrip-
tion of our data in the following section.
2 Corpus
We contacted the Weekly Reader1 corporation, an
on-line publisher producing magazines for elemen-
tary and high school students, and were granted
access in October 2008 to an archive of their ar-
ticles. Among the articles retrieved, only those
for elementary school students are labeled with
grade levels, which range from 2 to 5. We selected
only this portion of articles (1629 in total) for the
1http://www.weeklyreader.com
Table 1: Statistics for the Weekly Reader Corpus
Grade docs. words/document words/sentence
mean std. dev. mean std. dev.
2 174 128.27 106.03 9.54 2.32
3 289 171.96 106.05 11.39 2.42
4 428 278.03 187.58 13.67 2.65
5 542 335.56 230.25 15.28 3.21
study.2 These articles are intended to build chil-
dren?s general knowledge and help them practice
reading skills. While pre-processing the texts, we
found that many articles, especially those for lower
grade levels, consist of only puzzles and quizzes,
often in the form of simple multiple-choice ques-
tions. We discarded such texts and kept only 1433
full articles. Some distributional statistics of the
final corpus are listed in Table 1.
3 Features
3.1 Discourse Features
We implement four subsets of discourse fea-
tures: entity-density features, lexical-chain fea-
tures, coreference inference features and entity grid
features. The coreference inference features are
novel and have not been studied before. We pre-
viously studied entity-density features and lexical-
chain features for readers with intellectual disabili-
ties (Feng et al, 2009). Entity-grid features have
been studied by Barzilay and Lapata (2008) in a
stylistic classification task. Pitler and Nenkova
(2008) used the same features to evaluate how well
a text is written. We replicate this set of features
for grade level prediction task.
3.1.1 Entity-Density Features
Conceptual information is often introduced in a
text by entities, which consist of general nouns
and named entities, e.g. people?s names, locations,
organizations, etc. These are important in text
comprehension, because established entities form
basic components of concepts and propositions, on
which higher level discourse processing is based.
Our prior work illustrated the importance of en-
tities in text comprehension (Feng et al, 2009).
2A corpus of Weekly Reader articles was previously used
in work by Schwarm and Ostendorf (2005). However, the two
corpora are not identical in size nor content.
277
Table 2: New Entity-Density Features
1 percentage of named entities per document
2 percentage of named entities per sentences
3 percentage of overlapping nouns removed
4 average number of remaining nouns per sentence
5 percentage of named entities in total entities
6 percentage of remaining nouns in total entities
We hypothesized that the number of entities in-
troduced in a text relates to the working memory
burden on their targeted readers ? individuals with
intellectual disabilities. We defined entities as a
union of named entities and general nouns (nouns
and proper nouns) contained in a text, with over-
lapping general nouns removed. Based on this, we
implemented four kinds of entity-density features:
total number of entity mentions per document, total
number of unique entity mentions per document,
average number of entity mentions per sentence,
and average number of unique entity mentions per
sentence.
We believe entity-density features may also re-
late to the readability of a text for a general au-
dience. In this paper, we conduct a more re-
fined analysis of general nouns and named entities.
To collect entities for each document, we used
OpenNLP?s3 name-finding tool to extract named
entities; general nouns are extracted from the out-
put of Charniak?s Parser (see Section 3.3). Based
on the set of entities collected for each document,
we implement 12 new features. We list several of
these features in in Table 2.
3.1.2 Lexical Chain Features
During reading, a more challenging task with enti-
ties is not just to keep track of them, but to resolve
the semantic relations among them, so that infor-
mation can be processed, organized and stored in
a structured way for comprehension and later re-
trieval. In earlier work (Feng et al, 2009), we
used a lexical-chaining tool developed by Galley
and McKeown (2003) to annotate six semantic re-
lations among entities, e.g. synonym, hypernym,
hyponym, etc. Entities that are connected by these
semantic relations were linked through the text to
form lexical chains. Based on these chains, we
implemented six features, listed in Table 3, which
3http://opennlp.sourceforge.net/
Table 3: Lexical Chain Features
1 total number of lexical chains per document
2 avg. lexical chain length
3 avg. lexical chain span
4 num. of lex. chains with span ? half doc. length
5 num. of active chains per word
6 num. of active chains per entity
Table 4: Coreference Chain Features
1 total number of coreference chains per document
2 avg. num. of coreferences per chain
3 avg. chain span
4 num. of coref. chains with span ? half doc. length
5 avg. inference distance per chain
6 num. of active coreference chains per word
7 num. of active coreference chains per entity
we use in our current study. The length of a chain
is the number of entities contained in the chain,
the span of chain is the distance between the index
of the first and last entity in a chain. A chain is
defined to be active for a word or an entity if this
chain passes through its current location.
3.1.3 Coreference Inference Features
Relations among concepts and propositions are of-
ten not stated explicitly in a text. Automatically re-
solving implicit discourse relations is a hard prob-
lem. Therefore, we focus on one particular type,
referential relations, which are often established
through anaphoric devices, e.g. pronominal refer-
ences. The ability to resolve referential relations is
important for text comprehension.
We use OpenNLP to resolve coreferences. En-
tities and pronominal references that occur across
the text and refer to the same person or object
are extracted and formed into a coreference chain.
Based on the chains extracted, we implement seven
features as listed in Table 4. The chain length,
chain span and active chains are defined in a sim-
ilar way to the lexical chain features. Inference
distance is the difference between the index of the
referent and that of its pronominal reference. If the
same referent occurs more than once in a chain,
the index of the closest occurrence is used when
computing the inference distance.
3.1.4 Entity Grid Features
Coherent texts are easier to read. Several computa-
tional models have been developed to represent and
278
measure discourse coherence (Lapata and Barzilay,
2005; Soricut and Marcu, 2006; Elsner et al, 2007;
Barzilay and Lapata, 2008) for NLP tasks such as
text ordering and text generation. Although these
models are not intended directly for readability re-
search, Barzilay and Lapata (2008) have reported
that distributional properties of local entities gen-
erated by their grid models are useful in detecting
original texts from their simplified versions when
combined with well studied lexical and syntactic
features. This approach was subsequently pursued
by Pitler and Nenkova (2008) in their readability
study. Barzilay and Lapata?s entity grid model is
based on the assumption that the distribution of
entities in locally coherent texts exhibits certain
regularities. Each text is abstracted into a grid
that captures the distribution of entity patterns at
the level of sentence-to-sentence transitions. The
entity grid is a two-dimensional array, with one di-
mension corresponding to the salient entities in the
text, and the other corresponding to each sentence
of the text. Each grid cell contains the grammatical
role of the specified entity in the specified sentence:
whether it is a subject (S), object (O), neither of
the two (X), or absent from the sentence (-).
We use the Brown Coherence Toolkit (v0.2) (El-
sner et al, 2007), based on (Lapata and Barzilay,
2005), to generate an entity grid for each text in
our corpus. The distribution patterns of entities
are traced between each pair of adjacent sentences,
resulting in 16 entity transition patterns4. We then
compute the distribution probability of each entity
transition pattern within a text to form 16 entity-
grid-based features.
3.2 Language Modeling Features
Our language-modeling-based features are inspired
by Schwarm and Ostendorf?s (2005) work, a study
that is closely related to ours. They used data
from the same data ? the Weekly Reader ? for
their study. They trained three language mod-
els (unigram, bigram and trigram) on two paired
complex/simplified corpora (Britannica and Litera-
cyNet) using an approach in which words with high
information gain are kept and the remaining words
4These 16 transition patterns are: ?SS?, ?SO?, ?SX?, ?S-?,
?OS?, ?OO?, ?OX?, ?O-?, ?XS?, ?XO?, ?XX?, ?X-?, ?-S?,
?-O?, ?-X?, ?- -?.
are replaced with their parts of speech. These lan-
guage models were then used to score each text
in the Weekly Reader corpus by perplexity. They
reported that this approach was more successful
than training LMs on text sequences of word la-
bels alone, though without providing supporting
statistics.
It?s worth pointing out that their LMs were not
trained on the Weekly Reader data, but rather on
two unrelated paired corpora (Britannica and Lit-
eracyNet). This seems counter-intuitive, because
training LMs directly on the Weekly Reader data
would provide more class-specific information for
the classifiers. They justified this choice by stating
that splitting limited Weekly Reader data for train-
ing and testing purposes resulted in unsuccessful
performance.
We overcome this problem by using a hold-
one-out approach to train LMs directly on our
Weekly Reader corpus, which contains texts rang-
ing from Grade 2 to 5. We use grade levels to
divide the whole corpus into four smaller subsets.
In addition to implementing Schwarm and Osten-
dorf?s information-gain approach, we also built
LMs based on three other types of text sequences
for comparison purposes. These included: word-
token-only sequence (i.e., the original text), POS-
only sequence, and paired word-POS sequence.
For each grade level, we use the SRI Language
Modeling Toolkit5 (with Good-Turing discounting
and Katz backoff for smoothing) to train 5 lan-
guage models (1- to 5-gram) using each of the four
text sequences, resulting in 4?5?4= 80 perplex-
ity features for each text tested.
3.3 Parsed Syntactic Features
Schwarm and Ostendorf (2005) studied four parse
tree features (average parse tree height, average
number of SBARs, noun phrases, and verb phrases
per sentences). We implemented these and addi-
tional features, using the Charniak parser (Char-
niak, 2000). Our parsed syntactic features focus on
clauses (SBAR), noun phrases (NP), verb phrases
(VP) and prepositional phrases (PP). For each
phrase, we implement four features: total num-
ber of the phrases per document, average number
of phrases per sentence, and average phrase length
5http://www.speech.sri.com/projects/srilm/
279
measured by number of words and characters re-
spectively. In addition to average tree height, we
implement two non-terminal-node-based features:
average number of non-terminal nodes per parse
tree, and average number of non-terminal nodes
per word (terminal node).
3.4 POS-based Features
Part-of-speech-based grammatical features were
shown to be useful in readability prediction (Heil-
man et al, 2007; Leroy et al, 2008). To extend
prior work, we systematically studied a number of
common categories of words and investigated to
what extent they are related to a text?s complex-
ity. We focus primarily on five classes of words
(nouns, verbs, adjectives, adverbs, and preposi-
tions) and two broad categories (content words,
function words). Content words include nouns,
verbs, numerals, adjectives, and adverbs; the re-
maining types are function words. The part of
speech of each word is obtained from examining
the leaf node based on the output of Charniak?s
parser, where each leaf node consists of a word and
its part of speech. We group words based on their
POS labels. For each class of words, we imple-
ment five features. For example, for the adjective
class, we implemented the following five features:
percent of adjectives (tokens) per document, per-
cent of unique adjectives (types) per document,
ratio of unique adjectives per total unique words
in a document, average number of adjectives per
sentence and average number of unique adjectives
per sentence.
3.5 Shallow Features
Shallow features refer to those used by traditional
readability metrics, such as Flesch-Kincaid Grade
Level (Flesch, 1979), SMOG (McLaughlin, 1969),
Gunning FOG (Gunning, 1952), etc. Although
recent readability studies have strived to take ad-
vantage of NLP techniques, little has been revealed
about the predictive power of shallow features.
Shallow features, which are limited to superficial
text properties, are computationally much less ex-
pensive than syntactic or discourse features. To en-
able a comparison against more advanced features,
we implement 8 frequently used shallow features
as listed in Table 5.
Table 5: Shallow Features
1 average number of syllables per word
2 percentage of poly-syll. words per doc.
3 average number of poly-syll. words per sent.
4 average number of characters per word
5 Chall-Dale difficult words rate per doc.
6 average number of words per sentence
7 Flesch-Kincaid score
8 total number of words per document
3.6 Other Features
For comparison, we replicated 6 out-of-vocabulary
features described in Schwarm and Ostendorf
(2005). For each text in the Weekly Reader corpus,
these 6 features are computed using the most com-
mon 100, 200 and 500 word tokens and types based
on texts from Grade 2. We also replicated the 12
perplexity features implemented by Schwarm and
Ostendorf (2005) (see Section 3.2).
4 Experiments and Discussion
Previous studies on reading difficulty explored vari-
ous statistical models, e.g. regression vs. classifica-
tion, with varying assumptions about the measure-
ment of reading difficulty, e.g. whether labels are
ordered or unrelated, to test the predictive power
of models (Heilman et al, 2008; Petersen and Os-
tendorf, 2009; Aluisio et al, 2010). In our re-
search, we have used various models, including
linear regression; standard classification (Logis-
tic Regression and SVM), which assumes no rela-
tion between grade levels; and ordinal regression/
classification (provided by Weka, with Logistic
Regression and SMO as base function), which as-
sumes that the grade levels are ordered. Our exper-
iments show that, measured by mean squared error
and classification accuracy, linear regression mod-
els perform considerably poorer than classification
models. Measured by accuracy and F-measure,
ordinal classifiers perform comparable or worse
than standard classifiers. In this paper, we present
the best results, which are obtained by standard
classifiers. We use two machine learning packages
known for efficient high-quality multi-class classi-
fication: LIBSVM (Chang and Lin, 2001) and the
Weka machine learning toolkit (Hall et al, 2009),
from which we choose Logistic Regression as clas-
sifiers. We train and evaluate various prediction
280
Table 6: Comparison of discourse features
Feature Set LIBSVM Logistic Regress.
Entity-Density 59.63?0.632 57.59?0.375
Lexical Chain 45.86?0.815 42.58?0.241
Coref. Infer. 40.93?0.839 42.19?0.238
Entity Grid 45.92?1.155 42.14?0.457
all combined 60.50?0.990 58.79?0.703
models using the features described in Section 3.
We evaluate classification accuracy using repeated
10-fold cross-validation on the Weekly Reader cor-
pus. Classification accuracy is defined as the per-
centage of texts predicted with correct grade levels.
We repeat each experiment 10 times and report the
mean accuracy and its standard deviation.
4.1 Discourse Features
We first discuss the improvement made by extend-
ing our earlier entity-density features (Feng et al,
2009). We used LIBSVM to train and test mod-
els on the Weekly Reader corpus with our earlier
features and our new features respectively. With
earlier features only, the model achieves 53.66%
accuracy. With our new features added, the model
performance is 59.63%.
Table 6 presents the classification accuracy of
models trained with discourse features. We see
that, among four subsets of discourse features,
entity-density features perform significantly better
than the other three feature sets and generate the
highest classification accuracy (LIBSVM: 59.63%,
Logistic Regression: 57.59%). While Logistic Re-
gression results show that there is not much perfor-
mance difference among lexical chain, coreference
inference, and entity grid features, classification
accuracy of LIBSVM models indicates that lexical
chain features and entity grid features are better
in predicting text readability than coreference in-
ference features. Combining all discourse features
together does not significantly improve accuracy
compared with models trained only with entity-
density features.
4.2 Language Modeling Features
Table 7 compares the performance of models gen-
erated using our approach and our replication of
Schwarm and Ostendorf?s (2005) approach. In our
approach, features were obtained from language
Table 7: Comparison of lang. modeling features
Feature Set LIBSVM Logistic Regress.
IG 62.52?1.202 62.14?0.510
Text-only 60.17?1.206 60.31?0.559
POS-only 56.21?2.354 57.64?0.391
Word/POS pair 60.38?0.820 59.00?0.367
all combined 68.38?0.929 66.82?0.448
IG by Schwarm 52.21?0.832 51.89?0.405
Table 8: Comparison of parsed syntactic features
Feature Set # Feat. LIBSVM
Original features 4 50.68?0.812
Expanded features 21 57.79?1.023
models trained on the Weekly Reader corpus. Not
surprisingly, these are more effective than LMs
trained on the Britannica and LiteracyNet corpora,
in Schwarm and Ostendorf?s approach. Our results
support their claim that LMs trained with infor-
mation gain outperform LMs trained with POS la-
bels. However, we also notice that training LMs on
word labels alone or paired word/POS sequences
achieved similar classification accuracy to the IG
approach, while avoiding the complicated feature
selection of the IG approach.
4.3 Parsed Syntactic Features
Table 8 compares a classifier trained on the four
parse features of Schwarm and Ostendorf (2005) to
a classifier trained on our expanded set of parse fea-
tures. The LIBSVM classifier with the expanded
feature set scored 7 points higher than the one
trained on only the original four features, improv-
ing from 50.68% to 57.79%. Table 9 shows a
detailed comparison of particular parsed syntactic
features. The two non-terminal-node-based fea-
tures (average number of non-terminal nodes per
tree and average number of non-terminal nodes
per word) have higher discriminative power than
average tree height. Among SBARs, NPs, VPs and
PPs, our experiments show that VPs and NPs are
the best predictors.
4.4 POS-based Features
The classification accuracy generated by models
trained with various POS features is presented
in Table 10. We find that, among the five word
classes investigated, noun-based features gener-
281
Table 9: Detailed comp. of syntactic features
Feature Set LIBSVM Logistic Regress.
Non-term.-node ratios 53.02?0.571 51.80?0.171
Average tree height 44.26?0.914 43.45?0.269
SBARs 44.42?1.074 43.50?0.386
NPs 51.56?1.054 48.14?0.408
VPs 53.07?0.597 48.67?0.484
PPs 49.36?1.277 46.47?0.374
all combined 57.79?1.023 54.11?0.473
Table 10: Comparison of POS features
Feature Set LIBSVM Logistic Regress.
Nouns 58.15?0.862 57.01?0.256
Verbs 54.40?1.029 55.10?0.291
Adjectives 53.87?1.128 52.75?0.427
Adverbs 52.66?0.970 50.54?0.327
Prepositions 56.77?1.278 54.13?0.312
Content words 56.84?1.072 56.18?0.213
Function words 52.19?1.494 50.95?0.298
all combined 59.82?1.235 57.86?0.547
ate the highest classification accuracy, which is
consistent with what we have observed earlier
about entity-density features. Another notable ob-
servation is that prepositions demonstrate higher
discriminative power than adjectives and adverbs.
Models trained with preposition-based features per-
form close to those trained with noun-based fea-
tures. Among the two broader categories, content
words (which include nouns) demonstrate higher
predictive power than function words (which in-
clude prepositions).
4.5 Shallow Features
We present some notable findings on shallow fea-
tures in Table 11. Experimental results generated
by models trained with Logistic Regression show
that average sentence length has dominating predic-
tive power over all other shallow features. Features
based on syllable counting perform much worse.
The Flesch-Kincaid Grade Level score uses a fixed
linear combination of average words per sentence
and average syllables per word. Combining those
two features (without fixed coefficients) results in
the best overall accuracy, while using the Flesch-
Kincaid score as a single feature is significantly
worse.
Table 11: Comparison of shallow features
Feature Set Logistic Regress.
Avg. words per sent. 52.17?0.193
Avg. syll. per word 42.51?0.264
above two combined 53.04?0.514
Flesch-Kincaid score 50.83?0.144
Avg. poly-syll. words per sent. 45.70?0.306
all 8 features combined 52.34?0.242
4.6 Comparison with Previous Studies
A trivial baseline of predicting the most frequent
grade level (grade 5) predicts 542 out of 1433 texts
(or 37.8%) correctly. With this in mind, we first
compare our study with the widely-used Flesch-
Kincaid Grade Level formula, which is a linear
function of average words per sentence and average
syllables per word that aims to predict the grade
level of a text directly. Since this is a fixed formula
with known coefficients, we evaluated it directly
on our entire Weekly Reader corpus without cross-
validation. We obtain the predicted grade level
of a text by rounding the Flesch-Kincaid score
to the nearest integer. For only 20 out of 1433
texts the predicted and labeled grade levels agree,
resulting in a poor accuracy of 1.4%. By contrast,
using the Flesch-Kincaid score as a feature of a
simple logistic regression model achieves above
50% accuracy, as discussed in Section 4.5.
The most closely related previous study is the
work of Schwarm and Ostendorf (2005). How-
ever, because their experiment design (85/15 train-
ing/test data split) and machine learning tool
(SV Mlight) differ from ours, their results are not
directly comparable to ours. To make a compar-
ison, we replicated all the features used in their
study and then use LIBSVM and Weka?s Logistic
Regression to train two models with the replicated
features and evaluate them on our Weekly Reader
corpus using 10-fold cross-validation.
Using the same experiment design, we train clas-
sifiers with three combinations of our features as
listed in Table 12. ?All features? refers to a naive
combination of all features. ?AddOneBest? refers
to a subset of features selected by a group-wise
add-one-best greedy feature selection. ?WekaFS?
refers to a subset of features chosen by Weka?s
feature selection filter.
?WekaFS? consists of 28 features selected au-
282
Table 12: Comparison with previous work
baseline accuracy (majority class) 37.8
Flesch-Kincaid Grade Level 1.4
Feature Set # Feat. LIBSVM Logistic Reg.
Schwarm 25 63.18?1.664 60.50?0.477
All features 273 72.21?0.821 63.71?0.576
AddOneBest 122 74.01?0.847 69.22?0.411
WekaFS 28 70.06?0.777 65.46?0.336
tomatically by Weka?s feature selection filter us-
ing a best-first search method. The 28 features
include language modeling features, syntactic fea-
tures, POS features, shallow features and out-of-
vocabulary features. Aside from 4 shallow features
and 5 out-of-vocabulary features, the other 19 fea-
tures are novel features we have implemented for
this paper.
As Table 12 shows, a naive combination of all
features results in classification accuracy of 72%,
which is much higher than the current state of the
art (63%). This is not very surprising, since we are
considering a greater variety of features than any
previous individual study. Our WekaFS classifier
uses roughly the same number of features as the
best published result, yet it has a higher accuracy
(70.06%). Our best results were obtained by group-
wise add-one-best feature selection, resulting in
74% classification accuracy, a big improvement
over the state of the art.
5 Conclusions
We examined the usefulness of features at various
linguistic levels for predicting text readability in
terms of assigning texts to elementary school grade
levels. We implemented a set of discourse features,
enriched previous work by creating several new
features, and systematically tested and analyzed
the impact of these features.
We observed that POS features, in particular
nouns, have significant predictive power. The high
discriminative power of nouns in turn explains the
good performance of entity-density features, based
primarily on nouns. In general, our selected POS
features appear to be more correlated to text com-
plexity than syntactic features, shallow features
and most discourse features.
For parsed syntactic features, we found that verb
phrases appear to be more closely correlated with
text complexity than other types of phrases. While
SBARs are commonly perceived as good predic-
tors for syntactic complexity, they did not prove
very useful for predicting grade levels of texts in
this study. In future work, we plan to examine this
result in more detail.
Among the 8 shallow features, which are used
in various traditional readability formulas, we iden-
tified that average sentence length has dominating
predictive power over all other lexical or syllable-
based features.
Not surprisingly, among language modeling
features, combined features obtained from LMs
trained directly on the Weekly Reader corpus show
high discriminative power, compared with features
from LMs trained on unrelated corpora.
Discourse features do not seem to be very use-
ful in building an accurate readability metric. The
reason could lie in the fact that the texts in the cor-
pus we studied exhibit relatively low complexity,
since they are aimed at primary-school students. In
future work, we plan to investigate whether these
discourse features exhibit different discriminative
power for texts at higher grade levels.
A judicious combination of features examined
here results in a significant improvement over the
state of the art.
References
Sandra Aluisio, Lucia Specia, Caroline Gasperin,
and Carolina Scarton. 2010. Readability assess-
ment for text simplification. In NAACL-HLT
2010: The 5th Workshop on Innovative Use of
NLP for Building Educational Applications.
Regina Barzilay and Mirella Lapata. 2008. Model-
ing local coherence: An entity-based approach.
Computational Linguistics, 34(1):1?34.
Chih-Chung Chang and Chih-Jen Lin. 2001. LIB-
SVM: A Library for Support Vector Machines.
Software available at http://www.csie.ntu.
edu.tw/~cjlin/libsvm.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of the 1st Con-
ference of the North American Chapter of the
ACL, pages 132?139.
283
Kevyn Collins-Thompson and Jamie Callan. 2004.
A language modeling approach to predicting
reading difficulty. In Proceedings of the Hu-
man Language Technology Conference of the
North American Chapter of the Association for
Computational Linguistics (HLT-NAACL 2004).
Micha Elsner, Joseph Austerweil, and Eugene
Charniak. 2007. A unified local and global
model for discourse coherence. In Proceed-
ings of the Conference on Human Language
Technology and North American chapter of the
Association for Computational Linguistics (HLT-
NAACL 2007).
Lijun Feng, Noe?mie Elhadad, and Matt Huener-
fauth. 2009. Cognitively motivated features for
readability assessment. In The 12th Conference
of the European Chapter of the Association for
Computational Linguistics (EACL 2009).
Rudolf Flesch. 1979. How to write plain English.
Harper and Brothers, New York.
Michel Galley and Kathleen McKeown. 2003. Im-
proving word sense disambiguation in lexical
chaining. In Proceedings of the 18th Inter-
national Joint Conference on Artificial Intelli-
gence.
Robert Gunning. 1952. The Technique of Clear
Writing. McGraw-Hill.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bern-
hard Pfahringer, Peter Reutemann, and Ian H.
Witten. 2009. The WEKA data mining software:
An update. SIGKDD Explorations, 11(1):10?18.
Michael J. Heilman, Kevyn Collins-Thompson,
Jamie Callan, and Maxine Eskenazi. 2007. Com-
bining lexical and grammatical features to im-
prove readability measures for first and second
language texts. In Human Language Technolo-
gies 2007: The Conference of the North Amer-
ican Chapter of the Association for Computa-
tional Linguistics.
Michael J. Heilman, Kevyn Collins-Thompson,
and Maxine Eskenazi. 2008. An analysis of sta-
tistical models and features for reading difficulty
prediction. In ACL 2008: The 3rd Workshop on
Innovative Use of NLP for Building Educational
Applications.
Mirella Lapata and Regina Barzilay. 2005. Auto-
matic evaluation of text coherence: Models and
representations. In Proceedings of the Interna-
tional Joint Conference on Artificial Intelligence
(IJCAI?05), pages 1085?1090.
Gondy Leroy, Stephen Helmreich, James R. Cowie,
Trudi Miller, and Wei Zheng. 2008. Evaluating
online health information: Beyond readability
formulas. In AMIA 2008 Symposium Proceed-
ings.
G. Harry McLaughlin. 1969. Smog grading a
new readability formula. Journal of Reading,
12(8):639?646.
Sarah E. Petersen and Mari Ostendorf. 2006. A
machine learning approach to reading level as-
sessment. Technical report, University of Wash-
ington CSE Technical Report.
Sarah E. Petersen and Mari Ostendorf. 2009. A ma-
chine learning approach to reading level assess-
ment. Computer Speech and Language, 23:89?
106.
Emily Pitler and Ani Nenkova. 2008. Revisiting
readability: A unified framework for predict-
ing text quality. In Proceedings of the 2008
Conference on Empirical Methods in Natural
Language Processing.
R. Prasad, N. Dinesh, A. Lee, E. Miltsakaki,
L. Robaldo, A. Joshi, and B. Webber. 2008. The
Penn discourse treebank. In The Sixth Interna-
tional Conference on Language Resources and
Evaluation (LREC?08).
Sarah E. Schwarm and Mari Ostendorf. 2005.
Reading level assessment using support vector
machines and statistical language models. In
Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics.
Luo Si and Jamie Callan. 2001. A statistical model
for scientific readability. In Proceedings of the
Tenth International Conference on Information
and Knowledge Management.
Radu Soricut and Daniel Marcu. 2006. Discourse
generation using utility-trained coherence mod-
els. In Proceedings of the 21st International
Conference on Computational Linguistics and
44th Annual Meeting of the Association for Com-
putational Linguistics.
284
