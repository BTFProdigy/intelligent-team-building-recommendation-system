Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 299?307,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Unsupervised Constraint Driven Learning For Transliteration Discovery
Ming-Wei Chang Dan Goldwasser Dan Roth Yuancheng Tu
University of Illinois at Urbana Champaign
Urbana, IL 61801
{mchang21,goldwas1,danr,ytu}@uiuc.edu
Abstract
This paper introduces a novel unsupervised
constraint-driven learning algorithm for iden-
tifying named-entity (NE) transliterations in
bilingual corpora. The proposed method does
not require any annotated data or aligned cor-
pora. Instead, it is bootstrapped using a simple
resource ? a romanization table. We show that
this resource, when used in conjunction with
constraints, can efficiently identify translitera-
tion pairs. We evaluate the proposed method
on transliterating English NEs to three differ-
ent languages - Chinese, Russian and Hebrew.
Our experiments show that constraint driven
learning can significantly outperform existing
unsupervised models and achieve competitive
results to existing supervised models.
1 Introduction
Named entity (NE) transliteration is the process of
transcribing a NE from a source language to some
target language while preserving its pronunciation in
the original language. Automatic NE transliteration
is an important component in many cross-language
applications, such as Cross-Lingual Information Re-
trieval (CLIR) and Machine Translation(MT) (Her-
mjakob et al, 2008; Klementiev and Roth, 2006a;
Meng et al, 2001; Knight and Graehl, 1998).
It might initially seem that transliteration is an
easy task, requiring only finding a phonetic mapping
between character sets. However simply matching
every source language character to its target lan-
guage counterpart is not likely to work well as in
practice this mapping depends on the context the
characters appear in and on transliteration conven-
tions which may change across domains. As a result,
current approaches employ machine learning meth-
ods which, given enough labeled training data learn
how to determine whether a pair of words consti-
tute a transliteration pair. These methods typically
require training data and language-specific expertise
which may not exist for many languages. In this pa-
per we try to overcome these difficulties and show
that when the problem is modeled correctly, a sim-
ple character level mapping is a sufficient resource.
In our experiments, English was used as the
source language, allowing us to use romanization ta-
bles, a resource commonly-available for many lan-
guages1. These tables contain an incomplete map-
ping between character sets, mapping every charac-
ter to its most common counterpart.
Our transliteration model takes a discriminative
approach. Given a word pair, the model determines
if one word is a transliteration of the other. The
features used by this model are character n-gram
matches across the two strings. For example, Fig-
ure 1 describes the decomposition of a word pair into
unigram features as a bipartite graph in which each
edge represents an active feature.
We enhance the initial model with constraints, by
framing the feature extraction process as a struc-
tured prediction problem - given a word pair, the set
of possible active features is defined as a set of latent
binary variables. The contextual dependency be-
1The romanization tables available at the Library of
Congress website (http://www.loc.gov/catdir/cpso/roman.html)
cover more than 150 languages written in various non-Roman
scripts
299
Figure 1: Top: The space of all possible features that can be
generated given the word pair. Bottom: A pruned features rep-
resentation generated by the inference process.
tween features is encoded as a set of constraints over
these variables. Features are extracted by finding
an assignment that maximizes the similarity score
between the two strings and conforms to the con-
straints. The model is bootstrapped using a roman-
ization table and uses a discriminatively self-trained
classifier as a way to improve over several training
iterations. Furthermore, when specific knowledge
about the source and target languages exists, it can
be directly injected into the model as constraints.
We tested our approach on three very differ-
ent languages - Russian, a Slavic language, He-
brew a Semitic language, and Chinese, a Sino-
Tibetan language. In all languages, using this sim-
ple resource in conjunction with constraints pro-
vided us with a robust transliteration system which
significantly outperforms existing unsupervised ap-
proaches and achieves comparable performance to
supervised methods.
The rest of the paper is organized as follows.
Sec. 2 briefly examines more related work. Sec. 3
explains our model and Sec. 4 provide a linguistic
intuition for it. Sec. 5 describes our experiments and
evaluates our results followed by sec. 6 which con-
cludes our paper.
2 Related Works
Transliteration methods typically fall into two cate-
gories: generative approaches (Li et al, 2004; Jung
et al, 2000; Knight and Graehl, 1998) that try to
produce the target transliteration given a source lan-
guage NE, and discriminative approaches (Gold-
wasser and Roth, 2008b; Bergsma and Kondrak,
2007; Sproat et al, 2006; Klementiev and Roth,
2006a), that try to identify the correct translitera-
tion for a word in the source language given several
candidates in the target language. Generative meth-
ods encounter the Out-Of-Vocabulary (OOV) prob-
lem and require substantial amounts of training data
and knowledge of the source and target languages.
Discriminative approaches, when used to for dis-
covering NE in a bilingual corpora avoid the OOV
problem by choosing the transliteration candidates
from the corpora. These methods typically make
very little assumptions about the source and target
languages and require considerably less data to con-
verge. Training the transliteration model is typi-
cally done under supervised settings (Bergsma and
Kondrak, 2007; Goldwasser and Roth, 2008b), or
weakly supervised settings with additional tempo-
ral information (Sproat et al, 2006; Klementiev and
Roth, 2006a). Our work differs from these works
in that it is completely unsupervised and makes no
assumptions about the training data.
Incorporating knowledge encoded as constraints
into learning problems has attracted a lot of atten-
tion in the NLP community recently. This has been
shown both in supervised settings (Roth and Yih,
2004; Riedel and Clarke, 2006) and unsupervised
settings (Haghighi and Klein, 2006; Chang et al,
2007) in which constraints are used to bootstrap the
model. (Chang et al, 2007) describes an unsuper-
vised training of a Constrained Conditional Model
(CCM), a general framework for combining statisti-
cal models with declarative constraints. We extend
this work to include constraints over possible assign-
ments to latent variables which, in turn, define the
underlying representation for the learning problem.
In the transliteration community there are sev-
eral works (Ristad and Yianilos, 1998; Bergsma and
Kondrak, 2007; Goldwasser and Roth, 2008b) that
show how the feature representation of a word pair
can be restricted to facilitate learning a string sim-
ilarity model. We follow the approach discussed
in (Goldwasser and Roth, 2008b), which considers
the feature representation as a structured prediction
problem and finds the set of optimal assignments (or
feature activations), under a set of legitimacy con-
straints. This approach stresses the importance of
interaction between learning and inference, as the
model iteratively uses inference to improve the sam-
ple representation for the learning problem and uses
the learned model to improve the accuracy of the in-
300
ference process. We adapt this approach to unsu-
pervised settings, where iterating over the data im-
proves the model in both of these dimensions.
3 Unsupervised Constraint Driven
Learning
In this section we present our Unsupervised Con-
straint Driven Learning (UCDL) model for discov-
ering transliteration pairs. Our task is in essence a
ranking task. Given a NE in the source language and
a list of candidate transliterations in the target lan-
guage, the model is supposed to rank the candidates
and output the one with the highest score. The model
is bootstrapped using two linguistic resources: a ro-
manization table and a set of general and linguistic
constraints. We use several iterations of self training
to learn the model. The details of the procedure are
explained in Algorithm 1.
In our model features are character pairs (cs, ct),
where cs ? Cs is a source word character and
ct ? Ct is a target word character. The feature
representation of a word pair vs, vt is denoted by
F (vs, vt). Each feature (cs, ct) is assigned a weight
W (cs, ct) ? R. In step 1 of the algorithm we initial-
ize the weights vector using the romanization table.
Given a pair (vs, vt), a feature extraction process
is used to determine the feature based representation
of the pair. Once features are extracted to represent
a pair, the sum of the weights of the extracted fea-
tures is the score assigned to the target translitera-
tion candidate. Unlike traditional feature extraction
approaches, our feature representation function does
not produce a fixed feature representation. In step
2.1, we formalize the feature extraction process as a
constrained optimization problem that captures the
interdependencies between the features used to rep-
resent the sample. That is, obtaining F (vs, vt) re-
quires solving an optimization problem. The techni-
cal details are described in Sec. 3.1. The constraints
we use are described in Sec. 3.2.
In step 2.2 the different candidates for every
source NE are ranked according to the similarity
score associated with their chosen representation.
This ranking is used to ?label? examples for a dis-
criminative learning process that learns increasingly
better weights, and thus improve the representation
of the pair: each source NE paired with its top
ranked transliteration is labeled as a positive exam-
ples (step 2.3) and the rest of the samples are consid-
ered as negative samples. In order to focus the learn-
ing process, we removed from the training set al
negative examples ruled-out by the constraints (step
2.4). As the learning process progresses, the initial
weights are replaced by weights which are discrimi-
natively learned (step 2.5). This process is repeated
several times until the model converges, and repeats
the same ranking over several iterations.
Input: Romanization table T : Cs ? Ct, Constraints
C, Source NEs: Vs, Target words: Vt
1. Initialize Model
Let W : Cs ? Ct ? R be a weight vector.
Initialize W using T by the following procedure
?(cs, ct), (cs, ct) ? T ? W(cs, ct) = 0,
?(cs, ct),?((cs, ct) ? T ) ?W(cs, ct) = ?1,
?cs,W(cs, ) = ?1, ?ct,W( , ct) = ?1.
2. Constraints driven unsupervised training
while not converged do
1. ?vs ? Vs, vt ? Vt, use C and W
to generate a representation F (vs, vt)
2. ?vs ? Vs, find the top ranking transliteration
pair (vs, v?t ) by solving
v?t = argmaxvt score(F (vs, vt)).
3. D = {(+, F (vs, v?t )) | ?vs ? Vs}.
4. ?vs ? Vs, vt ? Vt, if vt 6= v?t and
score(F (vs, vt)) 6= ??, then
D = D ? {(?, F (vs, vt))}.
5. W ? train(D)
end
Algorithm 1: UCDL Transliteration Framework.
In the rest of this section we explain this process
in detail. We define the feature extraction inference
process in Sec. 3.1, the constraints used in Sec. 3.2
and the inference algorithm in Sec. 3.3. The linguis-
tic intuition for our model is described in Sec. 4.
3.1 Finding Feature Representation as
Constrained Optimization
We use the formulation of Constrainted Conditional
Models (CCMs) (Roth and Yih, 2004; Roth and Yih,
2007; Chang et al, 2008). Previous work on CCM
models dependencies between different decisions in
structured prediction problems. Transliteration dis-
covery is a binary classification problem, however,
301
the underlying representation of each sample can be
modeled as a CCM, defined as a set of latent vari-
ables corresponding to the set of all possible features
for a given sample. The dependencies between the
features are captured using constraints.
Given a word pair, the set of all possible features
consists of all character mappings from the source
word to the target word. Since in many cases the
size of the words differ we augment each of the
words with a blank character (denoted as ? ?). We
model character omission by mapping the character
to the blank character. This process is formally de-
fined as an operator mapping a transliteration can-
didate pair to a set of binary variables, denoted as
All-Features (AF ).
AF = {(cs, ct)|cs ? vs ? { }, ct ? vt ? { }}
This representation is depicted at the top of Figure 1.
The initial sample representation (AF ) gener-
ates features by coupling substrings from the two
terms without considering the dependencies be-
tween the possible combinations. This representa-
tion is clearly noisy and in order to improve it we
select a subset F ? AF of the possible features.
The selection process is formulated as a linear op-
timization problem over binary variables encoding
feature activations in AF . Variables assigned 1 are
selected to be in F , and those assigned 0 are not.
The objective function maximized is a linear func-
tion over the variables in AF , each with its weight as
a coefficient, as in the left part of Equation 1 below.
We seek to maximize this linear sum subject to a set
of constraints. These represent the dependencies be-
tween selections and prior knowledge about possible
legitimate character mappings and correspond to the
right side of Equation 1. In our settings only hard
constraints are used and therefore the penalty (?) for
violating any of the constraints is set to ?. The spe-
cific constraints used are discussed in Sec. 3.2. The
score of the mapping F (vs, vt) can be written as fol-
lows:
1
|vt|
(W ? F (vs, vt)?
?
ci?C
?ci(F (vs, vt)) (1)
We normalize this score by dividing it by the size of
the target word, since the size of candidates varies,
normalization improved the ranking of candidates.
The result of the optimization process is a set F of
active features, defined in Equation 2. The result of
this process is described at the bottom of Figure 1.
F ?(vs, vt) = argmaxF?AF (vs,vt)score(F ). (2)
The ranking process done by our model can now be
naturally defined - given a source word vs, and a
set of candidates target words v0t , . . . , vnt , find the
candidate whose optimal representation maximizes
Equation 1. This process is defined in Equation 3.
v?t = argmaxvit
score(F (vs, vit)). (3)
3.2 Incorporating Mapping Constraints
We consider two types of constraints: language spe-
cific and general constraints that apply to all lan-
guages. Language specific constraints typically im-
pose a local restriction such as individually forcing
some of the possible character mapping decisions.
The linguistic intuition behind these constraints is
discussed in Section 4. General constraints encode
global restrictions, capturing the dependencies be-
tween different mapping decisions.
General constraints: To facilitate readability we
denote the feature mapping the i-th source word
character to the j-th target word character as a
Boolean variable aij that is 1 if that feature is active
and 0 otherwise.
? Coverage - Every character must be mapped
only to a single character or to the blank char-
acter. We can formulate this as: ?j aij = 1
and ?i aij = 1.
? No Crossing - Every character mapping, except
mapping to blank character, should preserve the
order of appearance in the source and target
words, or formally - ?i, j s.t. aij = 1 ? ?l <
i, ?k > j, alk = 0. Another constraint is ?i, j
s.t. aij = 1 ? ?l > i, ?k < j, alk = 0.
Language specific constraints
? Restricted Mapping: These constraints restrict
the possible local mappings between source
and target language characters. We maintain a
set of possible mappings {cs ? ?cs}, where
?cs ? Ct and {ct ? ?ct}, where ?ct ? Cs.
Any feature (cs, ct) such that cs /? ?ct or
ct /? ?cs is penalized in our model.
302
? Length restriction: An additional constraint
restricts the size difference between the two
words. We formulate this as follows: ?vs ?
Vs,?vt ? Vt, if ?|vt| > |vs| and ?|vs| > |vt|,
score(F (vs, vt)) = ??. Although ? can
take different values for different languages, we
simply set ? to 2 in this paper.
In addition to biasing the model to choose the
right candidate, the constraints also provide a com-
putational advantage: a given a word pair is elimi-
nated from consideration when the length restriction
is not satisfied or there is no way to satisfy the re-
stricted mapping constraints.
3.3 Inference
The optimization problem defined in Equation 2 is
an integer linear program (ILP). However, given
the structure of the problem it is possible to de-
velop an efficient dynamic programming algorithm
for it, based on the algorithm for finding the mini-
mal edit distance of two strings. The complexity of
finding the optimal set of features is only quadratic
in the size of the input pair, a clear improvement
over the ILP exponential time algorithm. The al-
gorithm minimizes the weighted edit distance be-
tween the strings, and produces a character align-
ment that satisfies the general constraints (Sec. 3.2).
Our modifications are only concerned with incorpo-
rating the language-specific constraints into the al-
gorithm. This can be done simply by assigning a
negative infinity score to any alignment decision not
satisfying these constraints.
4 Bootstrapping with Linguistic
Information
Our model is bootstrapped using two resources - a
romanization table and mapping constraints. Both
resources capture the same information - character
mapping between languages. The distinction be-
tween the two represents the difference in the con-
fidence we have in these resources - the romaniza-
tion table is a noisy mapping covering the character
set and is therefore better suited as a feature. Con-
straints, represented by pervasive, correct character
mapping, indicate the sound mapping tendency be-
tween source and target languages. For example,
certain n-gram phonemic mappings, such as r ? l
from English to Chinese, are language specific and
can be captured by language specific sound change
patterns.
Phonemes Constraints
Vowel i ? y; u ? w; a ? a
Nasal m ? m; m,n ? m
Approximant
r ? l; l, r ? l
l ? l; w ? h,w, f
h, o, u, v ? w; y ? y
Fricative v ? w, b, fs ? s, x, z; s, c ? s
Plosive
p ? b, p; p ? p
b ? b; t ? t
t, d ? d; q ? k
Table 1: All language specific constraints used in our English
to Chinese transliteration (see Sec. 3.2 for more details). Con-
straints in boldface apply to all positions, the rest apply only to
characters appearing in initial position.
These patterns have been used by other systems
as features or pseudofeatures (Yoon et al, 2007).
However, in our system these language specific rule-
of-thumbs are systematically used as constraints to
exclude impossible alignments and therefore gener-
ate better features for learning. We listed in Table 1
all 20 language specific constraints we used for Chi-
nese. There is a total of 24 constraints for Hebrew
and 17 for Russian.
The constraints in Table 1 indicate a systematic
sound mapping between English and Chinese un-
igram character mappings. Arranged by manners
of articulation each row of the table indicates the
sound change tendency among vowels, nasals, ap-
proximants (retroflex and glides), fricatives and plo-
sives. For example, voiceless plosive sounds such as
p, t in English, tend to map to both voiced (such as b,
d) and voiceless sounds in Chinese. However, if the
sound is voiceless in Chinese, its backtrack English
sound must be voiceless. This voice-voiceless sound
change tendency is captured by our constraints such
as p ? b, p and p ? p; t ? t.
5 Experiments and Analysis
In this section, we demonstrate the effectiveness
of constraint driven learning empirically. We start
by describing the datasets and experimental settings
and then proceed to describe the results. We eval-
uated our method on three very different target lan-
303
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  2  4  6  8  10  12  14  16  18  20
A
C
C
Number of Rounds
[KR 06] + temporal information[KR 06] All Cons. + unsupervsied learning
Figure 2: Comparison between our models and weakly su-
pervised learning methods (Klementiev and Roth, 2006b).
Note that one of the models proposed in (Klementiev and Roth,
2006b) takes advantage of the temporal information. Our best
model, the unsupervised learning with all constraints, outper-
forms both models in (Klementiev and Roth, 2006b), even
though we do not use any temporal information.
guages: Russian, Chinese, and Hebrew, and com-
pared our results to previously published results.
5.1 Experimental Settings
In our experiments the system is evaluated on its
ability to correctly identify the gold transliteration
for each source word. We evaluated the system?s
performance using two measures adopted in many
transliteration works. The first one is Mean Recip-
rocal Rank (MRR), used in (Tao et al, 2006; Sproat
et al, 2006), which is the average of the multiplica-
tive inverse of the rank of the correct answer. For-
mally, Let n be the number of source NEs. Let Gol-
dRank(i) be the rank the algorithm assigns to the
correct transliteration. Then, MRR is defined by:
MRR = 1n
n?
i=1
1
goldRank(i) .
Another measure is Accuracy (ACC) used in (Kle-
mentiev and Roth, 2006a; Goldwasser and Roth,
2008a), which is the percentage of the top rank can-
didates being the gold transliteration. In our im-
plementation we used the support vector machine
(SVM) learning algorithm with linear kernel as our
underlying learning algorithm (mentioned in part
2.5 of Algorithm 1) . We used the package LIB-
LINEAR (Hsieh et al, 2008) in our experiments.
Through all of our experiments, we used the 2-norm
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 1.1
 0  2  4  6  8  10  12  14  16  18  20
M
R
R
Number of Rounds
[GR 08] 250 labeled ex. with cons[GR 08] 250 labeled ex. w/o consGeneral cons + unsupervised learningAll cons. + unsupervised learning
Figure 3: Comparison between our works and supervised
models in (Goldwasser and Roth, 2008b). We show the learn-
ing curves for Hebrew under two different settings: unsuper-
vised learning with general and all constraints. The results of
two supervised models (Goldwasser and Roth, 2008b) are also
included here. Note that our unsupervised model with all con-
straints is competitive to the supervised model with 250 labeled
examples. See the text for more comparisons and details.
hinge loss as our loss function and fixed the regular-
ization parameter C to be 0.5.
5.2 Datasets
We experimented using three different target lan-
guages Russian, Chinese, and Hebrew. We used En-
glish as the source language in all these experiments.
The Russian data set2, originally introduced in
(Klementiev and Roth, 2006b), is comprised of tem-
porally aligned news articles. The dataset contains
727 single word English NEs with a correspond-
ing set of 50,648 potential Russian candidate words
which include not only name entities, but also other
words appearing in the news articles.
The Chinese dataset is taken directly from an
English-Chinese transliteration dictionary, derived
from LDC Gigaword corpus3. The entire dictionary
consists of 74,396 pairs of English-Chinese NEs,
where Chinese NEs are written in Pinyin, a roman-
ized spelling system of Chinese. In (Tao et al, 2006)
a dataset which contains about 600 English NEs and
700 Chinese candidates is used. Since the dataset
is not publicly available, we created a dataset in a
similar way. We randomly selected approximately
600 NE pairs and then added about 100 candidates
which do not correspond to any of the English NE
2The corpus is available http://L2R.cs.uiuc.edu/?cogcomp.
3http://www.ldc.upenn.edu
304
Language UCDL Prev. works
Rus. (ACC) 73 63 (41) (KR?06)
Heb. (MRR) 0.899 0.894 (GR?08)
Table 2: Comparison to previously published results. UCDL
is our method, KR?06 is described in (Klementiev and Roth,
2006b) and GR?08 in (Goldwasser and Roth, 2008b). Note that
our results for Hebrew are comparable with a supervised sys-
tem.
previously selected.
The Hebrew dataset, originally introduced in
(Goldwasser and Roth, 2008a), consists of 300
English-Hebrew pairs extracted from Wikipedia.
5.3 Results
We begin by comparing our model to previously
published models tested over the same data, in two
different languages, Russian and Hebrew. For Rus-
sian, we compare to the model presented in (Kle-
mentiev and Roth, 2006b), a weakly supervised al-
gorithm that uses both phonetic information and
temporal information. The model is bootstrapped
using a set of 20 labeled examples. In their setting
the candidates are ranked by combining two scores,
one obtained using the transliteration model and a
second by comparing the relative occurrence fre-
quency of terms over time in both languages. Due
to computational tractability reasons we slightly
changed Algorithm 1 to use only a small subset of
the possible negative examples.
For Hebrew, we compare to the model presented
in (Goldwasser and Roth, 2008b), a supervised
model trained using 250 labeled examples. This
model uses a bigram model to represent the translit-
eration samples (i.e., features are generated by pair-
ing character unigrams and bigrams). The model
also uses constraints to restrict the feature extrac-
tion process, which are equivalent to the coverage
constraint we described in Sec. 3.2.
The results of these experiments are reported us-
ing the evaluation measures used in the original pa-
pers and are summarized in Table 2. The results
show a significant improvement over the Russian
data set and comparable performance to the super-
vised method used for Hebrew.
Figure 2 describes the learning curve of our
method over the Russian dataset. We compared our
algorithm to two models described in (Klementiev
and Roth, 2006b) - one uses only phonetic simi-
larity and the second also considers temporal co-
occurrence similarity when ranking the translitera-
tion candidates. Both models converge after 50 it-
erations. When comparing our model to their mod-
els, we found that even though our model ignores
the temporal information it achieves better results
and converges after fewer iterations. Their results
report a significant improvement when using tempo-
ral information - improving an ACC score of 41%
without temporal information to 63% when using
it. Since the temporal information is orthogonal to
the transliteration model, our model should similarly
benefit from incorporating the temporal information.
Figure 3 compares the learning curve of our
method to an existing supervised method over the
Hebrew data and shows we get comparable results.
Unfortunately, we could not find a published Chi-
nese dataset. However, our system achieved similar
results to other systems, over a different dataset with
similar number of training examples. For example,
(Sproat et al, 2006) presents a supervised system
that achieves a MRR score of 0.89, when evaluated
over a dataset consisting of 400 English NE and 627
Chinese words. Our results for a different dataset of
similar size are reported in Table 3.
5.4 Analysis
The resources used in our framework consist of
- a romanization table, general and language spe-
cific transliteration constraints. To reveal the impact
of each component we experimented with different
combination of the components, resulting in three
different testing configurations.
Romanization Table: We initialized the weight
vector using a romanization table and did not use any
constraints. To generate features we use a modified
version of our AF operator (see Sec. 3), which gen-
erates features by coupling characters in close posi-
tions in the source and target words. This configura-
tion is equivalent to the model used in (Klementiev
and Roth, 2006b).
+General Constraints: This configuration uses the
romanization table for initializing the weight vector
and general transliteration constraints (see Sec. 3.2)
for feature extraction.
+All Constraints: This configuration uses lan-
guage specific constraints in addition to the gen-
305
Settings Chinese Russian Hebrew
Romanization table 0.019 (0.5) 0.034 (1.0) 0.046 (1.7)
Romanization table +learning 0.020 (0.3) 0.048 (1.3) 0.028 (0.7)
+Gen Constraints 0.746 (67.1) 0.809 (74.3) 0.533 (45.0)
+Gen Constraints +learning 0.867 (82.2) 0.906 (86.7) 0.834 (76.0)
+All Constraints 0.801 (73.4) 0.849 (79.3) 0.743 (66.0)
+All Constraints +learning 0.889 (84.7) 0.931 (90.0) 0.899 (85.0)
Table 3: Results of an ablation study of unsupervised method for three target languages. Results for ACC are inside parentheses,
and for MRR outside. When the learning algorithm is used, the results after 20 rounds of constraint driven learning are reported.
Note that using linguistic constraints has a significant impact in the English-Hebrew experiments. Our results show that a small
amount of constraints can go a long way, and better constraints lead to better learning performance.
eral transliteration constraints to generate the feature
representation. (see Sec. 4).
+Learning: Indicates that after initializing the
weight vector, we update the weight using Algo-
rithm 1. In all of the experiments, we report the
results after 20 training iterations.
The results are summarized in Table 3. Due to the
size of the Russian dataset, we used a subset consist-
ing of 300 English NEs and their matching Russian
transliterations for the analysis presented here. Af-
ter observing the results, we discovered the follow-
ing regularities for all three languages. Using the
romanization table directly without constraints re-
sults in very poor performance, even after learning.
This can be used as an indication of the difficulty of
the transliteration problem and the difficulties ear-
lier works have had when using only romanization
tables, however, when used in conjunction with con-
straints results improve dramatically. For example,
in the English-Chinese data set, we improve MRR
from 0.02 to 0.746 and for the English-Russian data
set we improve 0.03 to 0.8. Interestingly, the results
for the English-Hebrew data set are lower than for
other languages - we achieve 0.53 MRR in this set-
ting. We attribute the difference to the quality of
the mapping in the romanization table for that lan-
guage. Indeed, the weights learned after 20 train-
ing iterations improve the results to 0.83. This im-
provement is consistent across all languages, after
learning we are able to achieve a MRR score of 0.87
for the English-Chinese data set and 0.91 for the
English-Russian data set. These results show that
romanization table contains enough information to
bootstrap the model when used in conjunction with
constraints. We are able to achieve results compa-
rable to supervised methods that use a similar set of
constraints and labeled examples.
Bootstrapping the weight vector using language
specific constraints can further improve the results.
They provide several advantages: a better starting
point, an improved learning rate and a better final
model. This is clear in all three languages, for exam-
ple results for the Russian and Chinese bootstrapped
models improve by 5%, and by over 20% for He-
brew. After training the difference is smaller- only
3% for the first two and 6% for Hebrew. Figure 3 de-
scribes the learning curve for models with and with-
out language specific constraints for the English-
Hebrew data set, it can be observed that using these
constraints the model converges faster and achieves
better results.
6 Conclusion
In this paper we develop a constraints driven ap-
proach to named entity transliteration. In doing it
we show that romanization tables are a very useful
resource for transliteration discovery if proper con-
straints are included. Our framework does not need
labeled data and does not assume that bilingual cor-
pus are temporally aligned. Even without using any
labeled data, our model is competitive to existing
supervised models and outperforms existing weakly
supervised models.
7 Acknowledgments
We wish to thank the reviewers for their insightful
comments. This work is partly supported by NSF
grant SoD-HCER-0613885 and DARPA funding un-
der the Bootstrap Learning Program.
306
References
S. Bergsma and G. Kondrak. 2007. Alignment-based
discriminative string similarity. In Proc. of the Annual
Meeting of the Association of Computational Linguis-
tics (ACL), pages 656?663, Prague, Czech Republic,
June. Association for Computational Linguistics.
M. Chang, L. Ratinov, and D. Roth. 2007. Guiding semi-
supervision with constraint-driven learning. In Proc.
of the Annual Meeting of the Association of Compu-
tational Linguistics (ACL), pages 280?287, Prague,
Czech Republic, Jun. Association for Computational
Linguistics.
M. Chang, L. Ratinov, N. Rizzolo, and D. Roth. 2008.
Learning and inference with constraints. In Proc.
of the National Conference on Artificial Intelligence
(AAAI), July.
D. Goldwasser and D. Roth. 2008a. Active sample se-
lection for named entity transliteration. In Proc. of the
Annual Meeting of the Association of Computational
Linguistics (ACL), June.
D. Goldwasser and D. Roth. 2008b. Transliteration as
constrained optimization. In Proc. of the Conference
on Empirical Methods for Natural Language Process-
ing (EMNLP), pages 353?362, Oct.
A. Haghighi and D. Klein. 2006. Prototype-driven learn-
ing for sequence models. In Proc. of the Annual Meet-
ing of the North American Association of Computa-
tional Linguistics (NAACL).
U. Hermjakob, K. Knight, and H. Daume? III. 2008.
Name translation in statistical machine translation -
learning when to transliterate. In Proc. of the Annual
Meeting of the Association of Computational Linguis-
tics (ACL), pages 389?397, Columbus, Ohio, June.
Cho-Jui Hsieh, Kai-Wei Chang, Chih-Jen Lin, S. Sathiya
Keerthi, and S. Sundararajan. 2008. A dual coordinate
descent method for large-scale linear svm. In ICML
?08: Proceedings of the 25th international conference
on Machine learning, pages 408?415, New York, NY,
USA. ACM.
S. Jung, S. Hong, and E. Paek. 2000. An english to
korean transliteration model of extended markov win-
dow. In Proc. the International Conference on Com-
putational Linguistics (COLING), pages 383?389.
A. Klementiev and D. Roth. 2006a. Named entity
transliteration and discovery from multilingual com-
parable corpora. In Proc. of the Annual Meeting of the
North American Association of Computational Lin-
guistics (NAACL), pages 82?88, June.
A. Klementiev and D. Roth. 2006b. Weakly supervised
named entity transliteration and discovery from mul-
tilingual comparable corpora. In Proc. of the Annual
Meeting of the Association of Computational Linguis-
tics (ACL), pages USS,TL,ADAPT, July.
K. Knight and J. Graehl. 1998. Machine transliteration.
Computational Linguistics, pages 599?612.
H. Li, M. Zhang, and J. Su. 2004. A joint source-channel
model for machine transliteration. In Proc. of the An-
nual Meeting of the Association of Computational Lin-
guistics (ACL), pages 159?166, Barcelona, Spain, July.
H. Meng, W. Lo, B. Chen, and K. Tang. 2001.
Generating phonetic cognates to handle named en-
tities in english-chinese cross-langauge spoken doc-
ument retreival. In Proceedings of the Automatic
Speech Recognition and Understanding Workshop,
pages 389?397.
S. Riedel and J. Clarke. 2006. Incremental integer linear
programming for non-projective dependency parsing.
In Proc. of the Conference on Empirical Methods for
Natural Language Processing (EMNLP), pages 129?
137, Sydney, Australia.
E. S. Ristad and P. N. Yianilos. 1998. Learning string
edit distance. IEEE Transactions on Pattern Recogni-
tion and Machine Intelligence, 20(5):522?532, May.
D. Roth and W. Yih. 2004. A linear programming formu-
lation for global inference in natural language tasks.
pages 1?8. Association for Computational Linguistics.
D. Roth and W. Yih. 2007. Global inference for en-
tity and relation identification via a linear program-
ming formulation. In Lise Getoor and Ben Taskar, ed-
itors, Introduction to Statistical Relational Learning.
MIT Press.
R. Sproat, T. Tao, and C. Zhai. 2006. Named entity
transliteration with comparable corpora. In Proc. of
the Annual Meeting of the Association of Computa-
tional Linguistics (ACL), pages 73?80, Sydney, Aus-
tralia, July.
T. Tao, S. Yoon, A. Fister, R. Sproat, and C. Zhai. 2006.
Unsupervised named entitly transliteration using tem-
poral and phonetic correlation. In Proc. of the Con-
ference on Empirical Methods for Natural Language
Processing (EMNLP), pages 250?257.
S. Yoon, K. Kim, and R. Sproat. 2007. Multilingual
transliteration using feature based phonetic method.
In Proc. of the Annual Meeting of the Association
of Computational Linguistics (ACL), pages 112?119,
Prague, Czech Republic, June.
307
PhraseNet:
Towards Context Sensitive Lexical Semantics?
Xin Li?, Dan Roth?, Yuancheng Tu?
Dept. of Computer Science?
Dept. of Linguistics?
University of Illinois at Urbana-Champaign
{xli1,danr,ytu}@uiuc.edu
Abstract
This paper introduces PhraseNet, a context-
sensitive lexical semantic knowledge base sys-
tem. Based on the supposition that seman-
tic proximity is not simply a relation between
two words in isolation, but rather a relation
between them in their context, English nouns
and verbs, along with contexts they appear in,
are organized in PhraseNet into Consets; Con-
sets capture the underlying lexical concept, and
are connected with several semantic relations
that respect contextually sensitive lexical infor-
mation. PhraseNet makes use of WordNet as
an important knowledge source. It enhances
a WordNet synset with its contextual informa-
tion and refines its relational structure by main-
taining only those relations that respect con-
textual constraints. The contextual informa-
tion allows for supporting more functionali-
ties compared with those of WordNet. Nat-
ural language researchers as well as linguists
and language learners can gain from accessing
PhraseNet with a word token and its context, to
retrieve relevant semantic information.
We describe the design and construction of
PhraseNet and give preliminary experimental
evidence to its usefulness for NLP researches.
1 Introduction
Progress in natural language understanding research ne-
cessitates significant progress in lexical semantics and
the development of lexical semantics resources. In
a broad range of natural language applications, from
?Research supported by NSF grants IIS-99-84168,
ITR-IIS-00-85836 and an ONR MURI award.
Names of authors are listed alphabetically.
prepositional phrase attachment (Pantel and Lin, 2000;
Stetina and Nagao, 1997), co-reference resolution (Ng
and Cardie, 2002) to text summarization (Saggion and
Lapalme, 2002), semantic information is a necessary
component in the inference, by providing a level of ab-
straction that is necessary for robust decisions.
Inducing that the prepositional phrase in ?They ate
a cake with a fork? has the same grammatical
function as that in ?They ate a cake with a
spoon?, for example, depends on the knowledge that
?cutlery? and ?tableware? are the hypernyms of both
?fork? and ?spoon?. However, the noun ?fork? has five
senses listed in WordNet and each of them has several
different hypernyms. Choosing the correct one is a con-
text sensitive decision.
WordNet (Fellbaum, 1998), a manually constructed
lexical reference system provides a lexical database along
with semantic relations among the lexemes of English
and is widely used in NLP tasks today. However, Word-
Net is organized at the word level, and at this level, En-
glish suffers ambiguities. Stand-alone words may have
several meanings and take on relations (e.g., hypernyms,
hyponyms) that depend on their meanings. Consequently,
there are very few success stories of automatically us-
ing WordNet in natural language applications. In many
cases, reported (and unreported) problems are due to the
fact that WordNet enumerates all the senses of polyse-
mous words; attempts to use this resource automatically
often result in noisy and non-uniform information (Brill
and Resnik, 1994; Krymolowski and Roth, 1998).
PhraseNet is designed based on the assumption that,
by and large, semantic ambiguity in English disappears
when local context of words is taken into account. It
makes use of WordNet as an important knowledge source
and is generated automatically using WordNet and ma-
chine learning based processing of large English corpora.
It enhances a WordNet synset with its contextual informa-
tion and refines its relational structure, including relations
such as hypernym, hyponym, antonym and synonym, by
maintaining only those links that respect contextual con-
straints. However, PhraseNet is not just a functional ex-
tension of WordNet. It is an independent lexical semantic
system allied with proper user interfaces and access func-
tions that will allow researchers and practitioners to use
it in applications.
As stated before, PhraseNet, is built on the assumption
that linguistic context is an indispensable factor affecting
the perception of a semantic proximity between words.
In its current design, PhraseNet defines ?context? hierar-
chically with three abstraction levels: abstract syntactic
skeletons, such as
[(S)? (V )? (DO)? (IO)? (P )? (N)]
which stands for Subject, Verb, Direct Object, Indi-
rect Object, Preposition and Noun(Object) of the Prepo-
sition, respectively; syntactic skeletons whose compo-
nents are enhanced by semantic abstraction, such as
[Peop ? send ? Peop ? gift ? on ? Day] and fi-
nally concrete syntactic skeletons from real sentences as
[they ? send?mom? gift? on? Christmas].
Intuitively, while ?candle? and ?cigarette? would score
poorly on semantic similarity without any contextual in-
formation, their occurrence in sentences such as ?John
tried to light a candle/cigarette? may
highlight their connection with the process of burning.
PhraseNet captures such constraints from the contextual
structures extracted automatically from natural language
corpora and enumerates word lists with their hierarchical
contextual information. Several abstractions are made in
the process of extracting the context in order to prevent
superfluous information and support generalization.
The basic unit in PhraseNet is a conset, a word in its
context, together with all relations associated with it. In
the lexical database, consets are chained together via their
similar or hierarchical contexts. By listing every context
extracted from large corpora and all the generalized con-
texts based on those attested sentences, PhraseNet will
have much more consets than synsets in WordNet. How-
ever, the organization of PhraseNet respects the syntactic
structure together with the distinction of senses of each
word in its corresponding contexts.
For example, rather than linking all hypernyms of a
polysemous word to a single word token, PhraseNet con-
nects the hypernym of each sense to the target word in
every context that instantiates that sense. While in Word-
Net every word has an average of 5.4 hypernyms, in
PhraseNet, the average number of hypernyms of a word
in a conset is 1.51.
In addition to querying WordNet semantic relations
to disambiguate consets, PhraseNet alo maintains fre-
1The statistics is taken over 200, 000 words from a mixed
corpus of American English.
quency records of each word in its context to help dif-
ferentiate consets and makes use of defined similarity be-
tween contexts in this process 2.
Several access functions are built into PhraseNet that
allow retrieving information relevant to a word and its
context. When accessed with words and their contextual
information, the system tends to output more relevant se-
mantic information due to the constraint set by their syn-
tactic contexts.
While still in preliminary stages of development and
experimentation and with a lot of functionalities still
missing, we believe that PhraseNet is an important effort
towards building a contextually sensitive lexical semantic
resource, that will be of much value to NLP researchers
as well as linguists and language learners.
The rest of this paper is organized as follows. Sec. 2
presents the design principles of PhraseNet. Sec. 3 de-
scribes the construction of PhraseNet and the current
stage of the implementation. An application that pro-
vides a preliminary experimental evaluation is described
in Sec. 4. Sec. 5 discuses some related work on lexical se-
mantics resources and Sec. 6 discusses future directions
within PhraseNet.
2 The Design of PhraseNet
Context is one important notion in PhraseNet. While the
context may mean different things in natural language,
many previous work in statistically natural language pro-
cessing defined ?context? as an n-word window around
the target word (Gale et al, 1992; Brown et al, 1991;
Roth, 1998). In PhraseNet, ?context? has a more precise
definition that depends on the grammatical structure of a
sentence rather than simply counting surrounding words.
We define ?context? to be the syntactic structure of the
sentence in which the word of interest occurs. Specif-
ically, we define this notion at three abstraction levels.
The highest level is the abstract syntactic skeleton of the
sentence. That is, it is in the form of the different combi-
nations of six syntactic components. Some components
may be missing as long as the structure is from a legit-
imate English sentence. The most complete form of the
abstract syntactic skeleton is:
[(S)? (V )? (DO)? (IO)? (P )? (N)] (1)
which captures all of the six syntactic components such
as Subject, Verb, Direct Object, Indirect Object, Prepo-
sition and Noun(Object) of Preposition, respectively, in
the sentence. And all components are assumed to be
arranged to obey the word order in English. The low-
est level of contexts is the concrete instantiation of the
stated syntactic skeleton, such as [Mary(S)?give(V )?
John(DO) ? gift(IO) ? on(P ) ? birthday(N)] and
2See details in Sec. 3
[I(S)? eat(V )? bread(DO)? with(P )? hand(N)]
which are extracted directly from corpora with grammat-
ical lemmatization done during the process. Therefore,
all word tokens are in their lemma format. The middle
layer(s) consists of generalized formats of the syntactic
skeleton. For example, the first example given above can
be generalized as [Peop(S)?give(V )?Peop(DO)?
Possession(IO) ? on(P ) ?Day(N)] by replacing
some of its components with more abstract semantic con-
cepts.
PhraseNet organizes nouns and verbs into ?consets?
and a ?conset? is defined as a context with all its
corresponding pointers (edges) to other consets. The
context that forms a conset can be either directly ex-
tracted from the corpus, or at a certain level of ab-
straction. For example, both [Mary(S) ? eat(V ) ?
cake(DO) ? on(P ) ? birthday(N), {p1, p2, . . . , pn}]
and [Peop(S) ? eat(V ) ? Food(DO) ? on(P ) ?
Day(N), {p1, p2, . . . , pn}] are consets.
Two types of relational pointers are defined currently
in PhraseNet: Equal and Hyper. Both of these two re-
lations are based on the context of each conset. Equal
is defined among consets with same number of compo-
nents and same syntactic ordering, i.e, some contexts
under the same abstract syntactic structure (the highest
level of context as defined in this paper). It is defined
that the Equal relation exists among consets whose con-
texts are with same abstract syntactic skeleton, if there is
only one component at the same position that is differ-
ent. For example, [Mary(S)?give(V )?John(DO)?
gift(IO)?on(P )?birthday(N), {p1, p2, . . . , pn}] and
[Mary(S) ? send(V ) ? John(DO) ? gift(IO) ?
on(P ) ? birthday(N), {p1, p2, . . . , pk}] are equal be-
cause the syntactic skeleton each of them has is the
same, i.e., [(S) ? (V ) ? (DO) ? (IO) ? (P ) ? (N)]
and except one word in the verb position that is differ-
ent, i.e., ?give? and ?send?, all other five components
at the corresponding same position are the same. The
Equal relation is transitive only with regard to a spe-
cific component in the same position. For example,
to be transitive to the above two example consets, the
Equal conset should be also different from them only
by its verb. The Hyper relation is also defined for con-
sets with same abstract syntactic structure. For conset
A and conset B, if they have the same syntactic struc-
ture, and if there is at least one component of the con-
text in A that is the hypernym of the component in that
of B at the corresponding same position, and all other
components are the same respectively, A is the Hyper
conset of B. For example, both [Molly(S) ? hit(V ) ?
Body(DO), {p1, p2, . . . , pj}] and [Peop(S)?hit(V )?
Body(DO), {p1, p2, . . . , pn}] are Hyper consets of
[Molly(S)?hit(V )?nose(DO), {p1, p2, . . . , pk}]. The
intuition behind these two relations is that the Equal rela-
Figure 1: The basic organization of PhraseNet: The upward
arrow denotes the Hyper relation and the dotted two-way arrow
with a V above denotes the Equal relation that is transitive with
regard to the V component.
tion can cluster a list of words which occur in exactly the
same contextual structure and if the extreme case occurs,
namely when the same context in all these equal consets
with regard to a specific syntactic component groups vir-
tually any nouns or verbs, the Hyper relation can be used
here for further disambiguation.
To summarize, PhraseNet can be thought of as a graph
on consets. Each node is a context and edges between
nodes are relations defined by the context of each node.
They are either Equal or Hyper. Equal relation can be
derived by matching consets and it is easy to implement
while building the Hyper relation requires the assistance
of WordNet and the defined Equal relation. Semantic re-
lations among words can be generated using the two types
of defined edges. For example, it is likely that the target
words in all equal consets with transitivity have similar
meaning. If this is not true at the lowest lower of contexts,
it is more likely to be true at higher, i.e., more generalized
level. Figure 1 shows a simple example reflecting the pre-
liminary design of PhraseNet.
After we get the similar meaning lists based on their
contexts, we can build interaction from this word list to
WordNet and inherit other semantic relations from Word-
Net. However, each member of a word list can help to dis-
ambiguate other members in this list. Therefore, it is ex-
pected that with the pruning assisted by list members, i.e.,
the disambiguation by truncating semantic relations asso-
ciated with each synset in WordNet, the extract meaning
in the context together with all other semantic relations
such as hypernyms, holonyms, troponyms, antonyms can
be derived from WordNet.
In the next two sections we describe our current im-
plementation of these operations and preliminary experi-
ments we have done with them.
2.1 Accessing PhraseNet
Retrieval of information from PhraseNet is done via sev-
eral access functions that we describe below. PhraseNet
is designed to be accessed via multiple functions with
flexible input modes set by the user. These functions
may allow users to exploit several different functionali-
ties of PhraseNet, depending on their goal and amount of
resources they have.
An access function in PhraseNet has two components.
The first component is the input, which can vary from
a single word token to a word with its complete con-
text. The second component is the functionality, which
ranges over simple retrieval and several relational func-
tions, modelled after WordNet relations.
The most basic and simplest way to query PhraseNet
is with a single word. In this case, the system outputs all
contexts the word can occur in, and its related words in
each context.
PhraseNet can also be accessed with input that consists
of a single word token along with its context information.
Context information refers to any of the elements in the
syntactic skeleton defined in Eq. 1, namely, Subject(S),
Verb(V), Direct Object(DO), Indirect Object(IO), Prepo-
sition(P) and Noun(Object) of the Preposition(N). The
contextual roles S, V, DO, IO, P or N or any subset of
them, can be specified by the user or derived by an appli-
cation making use of a shallow or full parser. The more
information the user provides, the more specific the re-
trieved information is.
To ease the requirements from the user, say, in case
no information of this form is available to the user,
PhraseNet will, in the future, have functions that allow a
user to supply a word token and some context, where the
functionality of the word in the context is not specified.
See Sec. 6 for a discussion.
Function Name Input Variables Output
PN WL Word [, Context] Word List
PN RL Word [, Context] WordNet relations
PN SN Word [, Context] Sense
PN ST Context Sentence
Table 1: PhraseNet Access Functions: PhraseNet access
functions along with their input and output. [i] denotes optional
input. PN RL is a family of functions, modelled after WordNet
relations.
Table 1 lists the functionality of the access functions in
PhraseNet. If the user only input a word token without
any context, all those designed functions will return each
context the input word occurs together with the wordlist
in these contexts. Otherwise, the output is constrained by
the input context. The functions are described below:
PN WL takes the optional contextual skeleton and one
specified word in that context as inputs and returns
the corresponding wordlist occurring in that context
or a higher level of context. A parameter to this
function specifies if we want to get the complete
wordlist or those words in the list that satisfy a spe-
cific pruning criterion. (This is the function used in
the experiments in Sec. 4.)
PN RL is modelled after the WordNet access functions.
It will return all words in those contexts that are
linked in PhraseNet by their Equal or Hyper rela-
tion. Those words can help to access WordNet to
derive all lexical relations stored there.
PN SN is modelled after the semantic concordance
in (Landes et al, 1998). It takes a word token and
an optional context as input, and returns the sense
of the word in that context. Similarly to PN RL this
function is implemented by appealing to WordNet
senses and pruning the possible sense based on the
wordlist determined for the given context.
PN ST is not implemented at this point, but is designed
to output a sentence that has same structure as the
input context, but use different words. It is inspired
by the work on reformulation, e.g., (Barzilay and
McKeown, 2001).
We can envision many ways users of PhraseNet can
make use of the retrieved information. At this point in the
life of PhraseNet we focus mostly on using PhraseNet as
a way to acquire semantic features to aid learning based
natural language applications. This determines our prior-
ities in the implementation that we describe next.
3 Constructing PhraseNet
Constructing PhraseNet involves three main stages: (1)
extracting syntactic skeletons from corpora, (2) con-
structing the core element in PhraseNet: consets, and (3)
developing access functions.
The first stage makes use of fully parsed data. In
constructing the current version of PhraseNet we used
two corpora. The first, relatively small corpus of the
1.1 million-word Penn-State Treebank which consists
of American English news articles (WSJ), and is fully
parsed. The second corpus has about 5 million sentences
of the TREC-11 (Voorhees, 2002), also containing mostly
American English news articles (NYT, 1998) and parsed
with Dekang Lin?s minipar parser (Lin, 1998a).
In the near future we are planning to construct a much
larger version of PhraseNet, using Trec-10 and Trec-11
data sets, which cover about 8 GB of text. We believe that
the size is very important here, and will add significant
robustness to our results.
To reduce ostensibly different contexts, two important
abstractions take place at this stage. (1) Syntactic lemma-
tization to get the lemma for both nouns and verbs in
the context defined in Eq. 1. For data parsed via Lin?s
minipar, the lexeme of each word is already included
in the parser. (2) Sematic categorization to unify pro-
nouns, proper names of people, locations and organiza-
tion as well as numbers. This semantic abstraction cap-
tures the underlying semantic proximity by categorizing
multitudinous surface-form proper names into one repre-
senting symbol.
While the first abstraction is simple the second is not.
At this point we use an NE tagger we developed our-
selves based on the approach to phrase identification de-
veloped in (Punyakanok and Roth, 2001). Note that this
abstraction handles multiword phrases. While the accu-
racy of the NE tagger is around 90%, we have yet to ex-
periment with the implication of this additional noise on
PhraseNet.
At the end of this stage, each sentence in the original
corpora is transformed into a single context either at
the lowest level or a more generalized instantiation
(with name entity tagged). For example, ?For six
years, T. Marshall Hahn Jr. has made
corporate acquisitions in the George
Bush mode: kind and gentle.?, changes to:
[Peop?make? acquisition? in?mode].
The second stage of constructing PhraseNet concen-
trates on constructing the core element in PhraseNet:
consets.
To do that, for each context, we collect wordlists that
contain those words that we determine to be admissible in
the context(or contexts share the equal relation). The first
step in constructing the wordlists in PhraseNet is to fol-
low the most strict definition ? include those words that
actually occur in the same context in the corpus. This in-
volves all Equal consets with the transitive property to
a specific syntactic component. We then apply to the
wordlists three types of pruning operations that are based
on (1) frequency of word occurrences in identical or simi-
lar contexts; (2) categorization of words in wordlist based
on clustering all contexts they occur in, and (3) pruning
via the relational structure inherited from WordNet - we
prune from the wordlist outliers in terms of this relational
structure. Some of these operations are parameterized
and determining the optimal setting is an experimental
issue.
1. Every word in a conset wordlist has a frequency
record associated with it, which records the fre-
quency of the word in its exact context. We prune
words with a frequency below k (with the current
corpus we choose k = 3). A disadvantage of
this pruning method is that it might filter out some
appropriate words with a low frequency in reality.
For example, for the partial context [strategy ?
involve? ? ? ? ? ?], we have:
[strategy - involve - * - * - *, < DO : advertisement
4, abuse 1, campaign 2, compromise 1, everything 1,
fumigation 1, item 1, membership 1, option 3, stock-
option 1> ]
In this case,?strategy? is the subject and ?involve?
is the predicate and all words in the list serve as the
direct object. The number in the parentheses is the
frequency of the token. With k = 3 we actually get
as a wordlist only: < advertisment, option >.
2. There are several ways to prune wordlists based on
the different contexts words may occur in. This in-
volves a definition of similar contexts and threshold-
ing based on the number of such contexts a word oc-
curs in. At this point, we implement the construction
of PhraseNet using a clustering of contexts, as done
in (Pantel and Lin, 2002). An exhaustive PhraseNet
list is intersected with word lists generated based on
clustered contexts given by (Pantel and Lin, 2002).
3. We prune from the wordlist outliers in terms of the
relational structure inherited from WordNet. Cur-
rently, this is implemented only using the hypernym
relation. The hypernym shared by the highest num-
ber of words in the wordlist is kept in the database.
For example, by searching ?option? in WordNet, we
get its three senses. Then we collect the hypernyms
of ?option? from all the senses as follows:
05319492(a financial instrument whose value is
based on another security)
04869064(the cognitive process of reaching a deci-
sion)
00026065(something done)
We do this for every word in the original list and find
out the hypernym(s) shared by the highest number of
words in the original wordlist. The final pick in this
case is the synset 05319492 which is shared by both
?option? and ?stock option? as their hypernym.
The third stage is to develop the access functions. As
mentioned before, while we envision many ways users
of PhraseNet can use the retrieved information, at this
preliminary stage of PhraseNet we focus mostly on us-
ing PhraseNet as a way to supply abstract semantic fea-
tures that learning based natural language applications
can benefit from.
For this purpose, so far we have only used and evalu-
ated the function PN WL. PN WL takes as input as
specific word and (optionally) its context and returns a
lists of words which are semantically related to the target
word in the given context. For example,
PN WL ( V= protest, [peop - legislation - * - * - * ])=
[protest, resist, dissent, veto, blackball, negative, for-
bid, prohibit, interdict, proscribe, disallow ].
This function can be implemented via any of the three
pruning methods discussed earlier (see Sec. 4). This
wordlists that this function outputs, can be used to aug-
ment feature based representations for other, learning
based, NLP tasks. Other access functions of PhraseNet
can serve in other ways, e.g., expansions in information
retrieval, but we have not experimented with it yet.
With the experiments we are doing right now,
PhraseNet only takes inputs with the context information
in the format of Eq. 1. Semantic categorization and syn-
tactic lemmatization of the context is required in order to
get matched in the database. However, PhraseNet will,
in the future, have functions that allow a user to supply a
word token and more flexible contexts.
4 Evaluation and Application
In this section we provide a first evaluation of PhraseNet.
We do that in the context of a learning task.
Learning tasks in NLP are typically modelled as clas-
sification tasks, where one seeks a mapping g : X ?
c1, ..., ck, that maps an instance x ? X (e.g., a sentence)
to one of c1, ..., ck ? representing some properties of the
instance (e.g., a part-of-speech tag of a word in the con-
text of the sentence). Typically, the raw representation
? sentence or document ? are first mapped to some fea-
ture based representation, and then a learning algorithm
is applied to learn a mapping from this representation to
the desired property (Roth, 1998). It is clear that in most
cases representing the mapping g in terms of the raw rep-
resentation of the input instance ? words and their order
? is very complex. Functionally simple representations
of this mapping can only be formed if we augment the
information that is readily available in the input instance
with additional, more abstract information. For exam-
ple, it is common to augment sentence representations
with syntactic categories ? part-of-speech (POS), under
the assumption that the sought-after property, for which
we seek the classifier, depends on the syntactic role of a
word in the sentence rather than the specific word. Sim-
ilar logic can be applied to semantic categories. In many
cases, the property seems not to depend on the specific
word used in the sentence ? that could be replaced with-
out affecting this property ? but rather on its ?meaning?.
In this section we show the benefit of using PhraseNet
in doing that in the context of Question Classification.
Question classification (QC) is the task of determining
the semantic class of the answer of a given question.
For example, given the question: ?What Cuban
dictator did Fidel Castro force out
of power in 1958?? we would like to determine
that its answer should be a name of a person. Our
approach to QC follows that of (Li and Roth, 2002).
The question classifier used is a multi-class classifier
which can classify a question into one of 50 fine-grained
classes.
The baseline classifier makes use of syntactic features
like the standard POS information and information ex-
tracted by a shallow parser in addition to the words in
the sentence. The classifier is then augmented with stan-
dard WordNet or with PhraseNet information as follows.
In all cases, words in the sentence are augmented with
additional words that are supposed to be semantically re-
lated to them. The intuition, as described above, is that
this provides a level of abstract ? we could have poten-
tially seen an equivalent question, where other ?equiva-
lent? words occur.
For WordNet, for each word in a question, all its hyper-
nyms are added to its feature based representation (in ad-
dition to the syntactic features). For PhraseNet, for each
word in a question, all the words in the corresponding
conset wordlist are added (where the context is supplied
by the question).
Our experiments compare the three pruning operations
described above. Training is done on a data set of 21,500
questions. Performance is evaluated by the precision of
classifying 1,000 test questions, defined as follows:
Precison = # of correct predictions# of predictions (2)
Table 2 presents the classification precision before and
after incorporating WordNet and PhraseNet information
into the classifier. By augmenting the question classi-
fier with PhraseNet information, even in this preliminary
stage, the error rate of the classifier can be reduced by
12%, while an equivalent use of WordNet information re-
duces the error by only 5.7%.
Information Used Precision Err Reduction
Baseline 84.2% 0%
WordNet 85.1% 5.7%
PN: Freq. based Pruning 84.4% 1.3%
PN: Categ. based Pruning 85% 5.1%
PN: Relation based Pruning 86.1% 12%
Table 2: Question Classification with PhraseNet Informa-
tion Question classification precision and error rate reduction
compared with the baseline error rate(15.8%) by incorporat-
ing WordNet and PhraseNet(PN) information. ?Baseline? is
the classifier that uses only syntactic features. The classifier
is trained over 21,500 questions and tested over 1000 TREC 10
and 11 questions.
5 Related Work
In this section we point to some of the related work
on syntax, semantics interaction and lexical semantic re-
sources in computational linguistics and natural language
processing. Many current syntactic theories make the
common assumption that various aspects of syntactic al-
ternation are predicable via the meaning of the predi-
cate in the sentence (Fillmore, 1968; Jackendoff, 1990;
Levin, 1993). With the resurgence of lexical seman-
tics and corpus linguistics during the past two decades,
this so-called linking regularity triggers a broad interest
of using syntactic representations illustrated in corpora
to classify lexical meaning (Baker et al, 1998; Levin,
1993; Dorr and Jones, 1996; Lapata and Brew, 1999; Lin,
1998b; Pantel and Lin, 2002).
FrameNet (Baker et al, 1998) produces a seman-
tic dictionary that documents combinatorial properties
of English lexical items in semantic and syntactic terms
based on attestations in a very large corpus. In FrameNet,
a frame is an intuitive structure that formalizes the links
between semantics and syntax in the results of lexical
analysis. (Fillmore et al, 2001) However, instead of de-
rived via attested sentences from corpora automatically,
each conceptual frame together with all its frame ele-
ments has to be constructed via slow and labor-intensive
manual work. FrameNet is not constructed automatically
based on observed syntactic alternations. Though deep
semantic analysis is built for each frame, lack of auto-
matic derivation of the semantic roles from large corpora3
confines the usage of this network drastically.
Levin?s classes (Levin, 1993) of verbs are based on the
assumption that the semantics of a verb and its syntactic
behavior are predictably related. She defines 191 verb
classes by grouping 4183 verbs which pattern together
with respect to their diathesis alternations, namely alter-
nations in the expressions of arguments. In Levin?s clas-
sification, it is the syntactic skeletons (such as np-v-np-
pp)to classify verbs directly. Levin?s classification is val-
idated via experiments done by (Dorr and Jones, 1996)
and some counter-arguments are in (Baker and Ruppen-
hofer, 2002). Her work provides a a small knowledge
source that needs further expansion.
Lin?s work (Lin, 1998b; Pantel and Lin, 2002) makes
use of distributional syntactic contextual information to
define semantic proximity. Dekang Lin?s grouping of
similar words is a combination of the abstract syntactic
skeleton and concrete word tokens. Lin uses syntactic de-
pendencies such as ?Subj-people?, ?Modifier-red?, which
combine both abstract syntactic notations and their con-
crete word token representations. He applies this method
to classifying not only verbs, but also nouns and adjec-
tives. While no evaluation has ever been done to deter-
mine if concrete word tokens are necessary when the syn-
tactic phrase types are already presented, Lin?s work in-
directly shows that the concrete lexical representation is
effective.
WordNet (Fellbaum, 1998) by far is the most widely
used semantic database. However, this database does not
3The attempt to label these semantic roles automatically in
(Gildea and Jurafsky, 2002) assumes knowledge of the frame
and covers only 20% of them.
always work as successfully as researchers have expected
(Krymolowski and Roth, 1998; Montemagni and Pirelli,
1998). This seems to be due to lack of topical context
(Harabagiu et al, 1999; Agirre et al, 2001) as well as
local context (Fellbaum, 1998). By adding contextual in-
formation, many researchers, (e.g., (Green et al, 2001;
Lapata and Brew, 1999; Landes et al, 1998)), have al-
ready made some improvements over it.
The work on the importance of connecting syntax and
semantics in developing lexical semantic resources shows
the importance of contextual information as a step to-
wards deeper level of processing. With hierarchical sen-
tential local contexts embedded and used to categorize
word classes automatically, we believe that PhraseNet
provides the right direction for building useful lexical se-
mantic database.
6 Discussion and Further Work
We believe that progress in semantics and in develop-
ing lexical resources is a prerequisite to any signifi-
cant progress in natural language understanding. This
work makes a step in this direction by introducing a
context-sensitive lexical semantic knowledge base sys-
tem, PhraseNet. We have argued that while cur-
rent lexical resources like WordNet are invaluable, we
should move towards contextually sensitive resources.
PhraseNet is designed to fill this gap, and our preliminary
experiments with it are promising.
PhraseNet is an ongoing project and is still in its pre-
liminary stage. There are several key issues that we are
currently exploring. First, given that PhraseNet draws
part of it power from corpora, we are planning to en-
large the corpus used. We believe that the data size
is very important and will add significant robustness to
our current results. At the same time, since construct-
ing PhraseNet relies on machine learning techniques, we
need to study extensively the effect of tuning these on
the reliability of PhraseNet. Second, there are several
functionalities and access functions that we are planning
to augment PhraseNet with. Among those is the ability
of allowing a user to query PhraseNet even without ex-
plicitly specifying the role of words in the context. This
would reduce the requirement for users and applications
using PhraseNet. Finally, current PhraseNet has no lexi-
cal information about adjectives and adverbs, which may
contain important distributional information about their
modified nouns or verbs. We would like to take this in-
formation into consideration in the near future.
References
E. Agirre, O. Ansa, D. Martinez, and E. Hovy. 2001. Enriching
wordnet concepts with topic signatures.
C. Baker and J. Ruppenhofer. 2002. Framenet?s frames vs.
levin?s verb classes. In Proceedings of the 28th Annual Meet-
ing of the Berkeley Linguistics Society.
C. Baker, C. Fillmore, and J. Lowe. 1998. The Berkeley
FrameNet project. In Christian Boitet and Pete Whitelock,
editors, Proceedings of the Thirty-Sixth Annual Meeting of
the Association for Computational Linguistics and Seven-
teenth International Conference on Computational Linguis-
tics, pages 86?90, San Francisco, California. Association for
Computational Linguistics, Morgan Kaufmann Publishers.
R. Barzilay and K. R. McKeown. 2001. Extracting paraphrases
from a parallel corpus. In Proceeding of the 10th Conference
of the European Chapter of ACL.
E. Brill and P. Resnik. 1994. A rule-based approach to prepo-
sitional phrase attachment disambiguation. In Proc. of COL-
ING.
P. F. Brown, S. A. D. Pietra, V. J. D. Pietra, and R. L. Mercer.
1991. Word sense disambiguation using statistical methods.
In Proceedings of ACL-1991.
B. Dorr and D. Jones. 1996. Role of word-sense disambigua-
tion in lexical acquisition.
C. Fellbaum. 1998. In C. Fellbaum, editor, WordNet: An Elec-
tronic Lexical Database. The MIT Press.
C. J. Fillmore, C. Wooters, and C. F. Baker. 2001. Building
a large lexical databank which provides deep semantics. In
Proceedings of the Pacific Asian Conference on Language,
Information and Computation, HongKong.
C. J. Fillmore. 1968. The case for case. In Bach and Harms,
editors, Universals in Linguistic Theory, pages 1?88. Holt,
Rinehart, and Winston, New York.
W. A. Gale, K. W. Church, and D. Jarowsky. 1992. A method
for disambiguation word senses in large corpora. Computers
and the Humanities, 26(5-6):415?439.
D. Gildea and D. Jurafsky. 2002. Automatic labeling of se-
mantic roles. Computational Linguistics, 28(3):245?288,
September.
R. Green, L. Pearl, B. J. Dorr, and P. Resnik. 2001. Lexical re-
source integration across the syntax-semantics interface. In
Proceedings of WordNet and Other Lexical Resources Work-
shop, NAACL, Pittsburg, June.
S. M. Harabagiu, G. A. Miller, and D. I. Moldovan. 1999.
Wordnet2 - a morphologically and semantically enhanced re-
sources. In Proceedings of ACL-SIGLEX99: Standardizing
Lexical Resources, pages 1?8, Maryland.
R. Jackendoff. 1990. Semantic Structures. MIT Press, Cam-
bridge, MA.
Y. Krymolowski and D. Roth. 1998. Incorporating knowledge
in natural language learning: A case study. In COLING-
ACL?98 workshop on the Usage of WordNet in Natural Lan-
guage Processing Systems.
S. Landes, C. Leacock, and R. I. Tengi. 1998. Building seman-
tic concordances. In C. Fellbaum, editor, WordNet: an Elec-
toronic Lexical Database, pages 199?216. The MIT Press.
M. Lapata and C. Brew. 1999. Using subcategorization to re-
solve verb class ambiguity. In Proceedings of EMNLP, pages
266?274.
B. Levin. 1993. English Verb Classes and Alternations:
A Preliminary Investigation. University of Chicago Press,
Chicago, IL.
X. Li and D. Roth. 2002. Learning question classifiers. In
Proceedings of COLING.
D. Lin. 1998a. Dependency-based evaluation of minipar. In
In Workshop on the Evaluation of Parsing Systems Granada
Spain.
D. Lin. 1998b. An information-theoretic definition of similar-
ity. In Proc. 15th International Conf. on Machine Learning,
pages 296?304. Morgan Kaufmann, San Francisco, CA.
S. Montemagni and V. Pirelli. 1998. Augmenting WordNet-
like lexical resources with distributional evidence. an
application-oriented perspective. In S. Harabagiu, editor,
Use of WordNet in Natural Language Processing Systems:
Proceedings of the Conference, pages 87?93. Association for
Computational Linguistics.
V. Ng and C. Cardie. 2002. Improving machine learning ap-
proaches to coreference resolution. In Proceedings of 40th
Annual Meeting of the ACL, TaiPei.
P. Pantel and D. Lin. 2000. An unsupervised approach to
prepositional phrase attachment using contextually similar
words. In Proceedings of Association for Computational
Linguistics, Hongkong.
P. Pantel and D. Lin. 2002. Discovering word senses from text.
In The Eighth ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining.
V. Punyakanok and D. Roth. 2001. The use of classifiers in
sequential inference. In NIPS-13; The 2000 Conference on
Advances in Neural Information Processing Systems, pages
995?1001. MIT Press.
D. Roth. 1998. Learning to resolve natural language ambigu-
ities: A unified approach. In Proc. National Conference on
Artificial Intelligence, pages 806?813.
H. Saggion and G. Lapalme. 2002. Generating indicative-
informative summaries with sumum. Computational Lin-
guistics, 28(4):497?526.
J. Stetina and M. Nagao. 1997. Corpus based pp attachment
ambiguity rosolution with a semantic dictionary. In Proceed-
ings of the 5th Workshop on Very Large Corpora, Beijing and
Hongkong.
E. Voorhees. 2002. Overview of the TREC-2002 question an-
swering track. In The Eleventh TREC Conference, pages
115?123.
Semantic Role Labeling Via Generalized Inference Over Classifiers
Vasin Punyakanok, Dan Roth, Wen-tau Yih, Dav Zimak Yuancheng Tu
Department of Computer Science Department of Linguistics
University of Illinois at Urbana-Champaign
{punyakan,danr,yih,davzimak,ytu}@uiuc.edu
Abstract
We present a system submitted to the CoNLL-
2004 shared task for semantic role labeling.
The system is composed of a set of classifiers
and an inference procedure used both to clean
the classification results and to ensure struc-
tural integrity of the final role labeling. Lin-
guistic information is used to generate features
during classification and constraints for the in-
ference process.
1 Introduction
Semantic role labeling is a complex task to discover pat-
terns within sentences corresponding to semantic mean-
ing. We believe it is hopeless to expect high levels of per-
formance from either purely manual classifiers or purely
learned classifiers. Rather, supplemental linguistic infor-
mation must be used to support and correct a learning
system. The system we present here is composed of two
phases.
First, a set of phrase candidates is produced using two
learned classifiers?one to discover beginning positions
and one to discover end positions for each argument type.
Hopefully, this phase discovers a small superset of all
phrases in the sentence (for each verb).
In the second phase, the final prediction is made. First,
candidate phrases from the first phase are re-scored using
a classifier designed to determine argument type, given
a candidate phrase. Because phrases are considered as a
whole, global properties of the candidates can be used to
discover how likely it is that a phrase is of a given ar-
gument type. However, the set of possible role-labelings
is restricted by structural and linguistic constraints. We
encode these constraints using linear functions and use
integer programming to ensure the final prediction is con-
sistent (see Section 4).
2 SNoW Learning Architecture
The learning algorithm used is a variation of the Winnow
update rule incorporated in SNoW (Roth, 1998; Roth and
Yih, 2002), a multi-class classifier that is specifically tai-
lored for large scale learning tasks. SNoW learns a sparse
network of linear functions, in which the targets (phrase
border predictions or argument type predictions, in this
case) are represented as linear functions over a common
feature space. It incorporates several improvements over
the basic Winnow update rule. In particular, a regular-
ization term is added, which has the affect of trying to
separate the data with a think separator (Grove and Roth,
2001; Hang et al, 2002). In the work presented here we
use this regularization with a fixed parameter.
Experimental evidence has shown that SNoW activa-
tions are monotonic with the confidence in the prediction
Therefore, it can provide a good source of probability es-
timation. We use softmax (Bishop, 1995) over the raw ac-
tivation values as conditional probabilities. Specifically,
suppose the number of classes is n, and the raw activa-
tion values of class i is acti. The posterior estimation for
class i is derived by the following equation.
score(i) = pi =
eacti
?
1?j?n eactj
3 First Phase: Find Argument Candidates
The first phase is to predict the phrases of a given sen-
tence that correspond to some argument (given the verb).
Unfortunately, it turns out that it is difficult to predict the
exact phrases accurately. Therefore, the goal of the first
phase is to output a superset of the correct phrases by fil-
tering out unlikely candidates.
Specifically, we learn two classifiers, one to detect
beginning phrase locations and a second to detect end
phrase locations. Each multi-class classifier makes pre-
dictions over forty-three classes ? thirty-two argument
types, ten continuous argument types, one class to detect
not begging and one class to detect not end. The follow-
ing features are used:
? Word feature includes the current word, two words
before and two words after.
? Part-of-speech tag (POS) feature includes the POS
tags of the current word, two words before and after.
? Chunk feature includes the BIO tags for chunks of
the current word, two words before and after.
? Predicate lemma & POS tag show the lemma form
and POS tag of the active predicate.
? Voice feature indicates the voice (active/passive) of
the current predicate. This is extracted with a simple
rule: a verb is identified as passive if it follows a to-
be verb in the same phrase chuck and its POS tag
is VBN(past participle) or it immediately follows a
noun phrase.
? Position feature describes if the current word is be-
fore of after the predicate.
? Chunk pattern feature encodes the sequence of
chunks from the current words to the predicate.
? Clause tag indicates the boundary of clauses.
? Clause path feature is a path formed from a semi-
parsed tree containing only clauses and chunks.
Each clause is named with the chunk immediately
preceding it. The clause path is the path from predi-
cate to target word in the semi-parsed tree.
? Clause position feature is the position of the tar-
get word relative to the predicate in the semi-parsed
tree containing only clauses. Specifically, there
are four configurations?target word and predicate
share same parent, parent of target word is ancestor
of predicate, parent of predicate is ancestor of target
word, or otherwise.
Because each phrase consists of a single beginning and
a single ending, these classifiers can be used to construct
a set of potential phrases (by combining each predicted
begin with each predicted end after it of the same type).
Although the outputs of this phase are potential ar-
gument candidates, along with their types, the second
phase re-scores the arguments using all possible types.
After eliminating the types from consideration, the first
phase achieves 98.96% and 88.65% recall (overall, with-
out verb) on the training and the development set, respec-
tively. Because these are the only candidates that are
passed to the second phase, 88.65% is an upper bound
of the recall for our overall system.
4 Second Phase: Phrase Classification
The second phase of our system assigns the final argu-
ment classes to (a subset) of the phrases supplied from the
first phase. This task is accomplished in two steps. First,
a multi-class classifier is used to supply confidence scores
corresponding to how likely individual phrases are to
have specific argument types. Then we look for the most
likely solution over the whole sentence, given the matrix
of confidences and linguistic information that serves as a
set of global constraints over the solution space.
Again, the SNoW learning architecture is used to train
a multi-class classifier to label each phrase to one of
the argument types, plus a special class ? no argument.
Training examples are created from the phrase candidates
supplied from the first phase using the following features:
? Predicate lemma & POS tag, voice, position,
clause Path, clause position, chunk pattern Same
features as the first phase.
? Word & POS tag from the phrase, including the
first/last word and tag, and the head word1.
? Named entity feature tells if the target phrase is,
embeds, overlaps, or is embedded in a named entity.
? Chunk features are the same as named entity (but
with chunks, e.g. noun phrases).
? Length of the target phrase, in the numbers of words
and chunks.
? Verb class feature is the class of the active predicate
described in the frame files.
? Phrase type uses simple heuristics to identify the
target phrase like VP, PP, or NP.
? Sub-categorization describes the phrase structure
around the predicate. We separate the clause where
the predicate is in into three part ? the predicate
chunk, segments before and after the predicate. The
sequence of the phrase types of these three segments
is our feature.
? Baseline follows the rule of identifying AM-NEG
and AM-MOD and uses them as features.
? Clause coverage describes how much of local
clause (from the predicate) is covered by the target
phrase.
? Chunk pattern length feature counts the number of
patterns in the phrase.
? Conjunctions join every pair of the above features
as new features.
? Boundary words & POS tags include one or two
words/tags before and after the target phrase.
1We use simple rules to first decide if a candidate phrase
type is VP, NP, or PP. The headword of an NP phrase is the
right-most noun. Similarly, the left-most verb/proposition of a
VP/PP phrase is extracted as the headword
? Bigrams are pairs of words/tags in the window from
two words before the target to the first word of the
target, and also from the last word to two words after
the phrase.
? Sparse colocation picks one word/tag from the two
words before the phrase, the first word/tag, the last
word/tag of the phrase, and one word/tag from the
two words after the phrase to join as features.
Alternately, we could have derived a scoring function
from the first phase confidences of the open and closed
predictors for each argument type. This method has
proved useful in the literature for shallow parsing (Pun-
yakanok and Roth, 2001). However, it is hoped that ad-
ditional global features of the phrase would be necessary
due to the variety and complexity of the argument types.
See Table 1 for a comparison.
Formally (but very briefly), the phrase classifier is at-
tempting to assign labels to a set of phrases, S1:M , in-
dexed from 1 to M . Each phrase Si can take any label
from a set of phrase labels, P , and the indexed set of
phrases can take a set of labels, s1:M ? PM . If we as-
sume that the classifier returns a score, score(Si = si),
corresponding to the likelihood of seeing label si for
phrase Si, then, given a sentence, the unaltered inference
task that is solved by our system maximizes the score of
the phrase, score(S1:M = s1:M ),
s?1:M = argmax
s1:M?PM
score(S1:M = s1:M )
= argmax
s1:M?PM
M
?
i=1
score(Si = si).
(1)
The second step for phrase identification is eliminating
labelings using global constraints derived from linguistic
information and structural considerations. Specifically,
we limit the solution space through the used of a filter
function, F , that eliminates many phrase labelings from
consideration. It is interesting to contrast this with previ-
ous work that filters individual phrases (see (Carreras and
Ma`rquez, 2003)). Here, we are concerned with global
constraints as well as constraints on the phrases. There-
fore, the final labeling becomes
s?1:M = argmax
s1:M?F(PM)
M
?
i=1
score(Si = si) (2)
The filter function used considers the following con-
straints:
1. Arguments cannot cover the predicate except those
that contain only the verb or the verb and the follow-
ing word.
2. Arguments cannot overlap with the clauses (they can
be embedded in one another).
3. If a predicate is outside a clause, its arguments can-
not be embedded in that clause.
4. No overlapping or embedding phrases.
5. No duplicate argument classes for A0-A5,V.
6. Exactly one V argument per sentence.
7. If there is C-V, then there has to be a V-A1-CV pat-
tern.
8. If there is a R-XXX argument, then there has to be a
XXX argument.
9. If there is a C-XXX argument, then there has to be
a XXX argument; in addition, the C-XXX argument
must occur after XXX.
10. Given the predicate, some argument classes are ille-
gal (e.g. predicate ?stalk? can take only A0 or A1).
Constraint 1 is valid because all the arguments of a pred-
icate must lie outside the predicate. The exception is for
the boundary of the predicate itself. Constraint 1 through
constraint 3 are actually constraints that can be evaluated
on a per-phrase basis and thus can be applied to the indi-
vidual phrases at any time. For efficiency sake, we elimi-
nate these even before the second phase scoring is begun.
Constraints 5, 8, and 9 are valid for only a subset of the
arguments.
These constraints are easy to transform into linear con-
straints (for example, for each class c, constraint 5 be-
comes
?M
i=1[Si = c] ? 1) 2. Then the optimum solution
of the cost function given in Equation 2 can be found by
integer linear programming3. A similar method was used
for entity/relation recognition (Roth and Yih, 2004).
Almost all previous work on shallow parsing and
phrase classification has used Constraint 4 to ensure that
there are no overlapping phrases. By considering addi-
tional constraints, we show improved performance (see
Table 1).
5 Results
In this section, we present results. For the second phase,
we evaluate the quality of the phrase predictor. The re-
sult first evaluates the phrase classifier, given the perfect
phrase locations without using inference (i.e. F(PM ) =
PM ). The second, adds inference to the phrase classifica-
tion over the perfect classifiers (see Table 2). We evaluate
the overall performance of our system (without assum-
ing perfect phrases) by training and evaluating the phrase
classifier on the output from the first phase (see Table 3).
Finally,since this is a tagging task, we compare this
system with the basic tagger that we have, the CLCL
2where [x] is 1 if x is true and 0 otherwise
3(Xpress-MP, 2003) was used in all experiments to solve in-
teger linear programming.
Precision Recall F1
1st Phase, non-Overlap 70.54% 61.50% 65.71
1st Phase, All Const. 70.97% 60.74% 65.46
2nd Phase, non-Overlap 69.69% 64.75% 67.13
2nd Phase, All Const. 71.96% 64.93% 68.26
Table 1: Summary of experiments on the development set.
The phrase scoring is choosen from either the first phase or the
second phase and each is evaluated by considering simply non-
overlapping constraints or the full set of linguistic constraints.
To make a fair comparison, parameters were set seperately to
optimize performance when using the first phase results. All
results are for overall performance.
Precision Recall F1
Without Inference 86.95% 87.24% 87.10
With Inference 88.03% 88.23% 88.13
Table 2: Results of second phase phrase prediction and in-
ference assuming perfect boundary detection in the first phase.
Inference improves performance by restricting label sequences
rather than restricting structural properties since the correct
boundaries are given. All results are for overall performance
on the development set.
shallow parser from (Punyakanok and Roth, 2001), which
is equivalent to using the scoring function from the first
phase with only the non-overlapping constraints. Table 1
shows how how additional constraints over the standard
non-overlapping constraints improve performance on the
development set4.
6 Conclusion
We show that linguistic information is useful for semantic
role labeling used both to derive features and to derive
hard constraints on the output. We show that it is possible
to use integer linear programming to perform inference
that incorporates a wide variety of hard constraints that
would be difficult to incorporate using existing methods.
In addition, we provide further evidence supporting the
use of scoring phrases over scoring phrase boundaries for
complex tasks.
Acknowledgments This research is supported by
NSF grants ITR-IIS-0085836, ITR-IIS-0085980 and IIS-
9984168, EIA-0224453 and an ONR MURI Award. We
also thank AMD for their equipment donation and Dash
Optimization for free academic use of their Xpress-MP
software.
References
C. Bishop, 1995. Neural Networks for Pattern Recognition,
chapter 6.4: Modelling conditional distributions, page 215.
Oxford University Press.
4The test set was not publicly available to evaluate these re-
sults.
Precision Recall F?=1
Overall 70.07% 63.07% 66.39
A0 81.13% 77.70% 79.38
A1 74.21% 63.02% 68.16
A2 54.16% 41.04% 46.69
A3 47.06% 26.67% 34.04
A4 71.43% 60.00% 65.22
A5 0.00% 0.00% 0.00
AM-ADV 39.36% 36.16% 37.69
AM-CAU 45.95% 34.69% 39.53
AM-DIR 42.50% 34.00% 37.78
AM-DIS 52.00% 67.14% 58.61
AM-EXT 46.67% 50.00% 48.28
AM-LOC 33.47% 34.65% 34.05
AM-MNR 45.19% 36.86% 40.60
AM-MOD 92.49% 94.96% 93.70
AM-NEG 85.92% 96.06% 90.71
AM-PNC 32.79% 23.53% 27.40
AM-PRD 0.00% 0.00% 0.00
AM-TMP 59.77% 56.89% 58.30
R-A0 81.33% 76.73% 78.96
R-A1 58.82% 57.14% 57.97
R-A2 100.00% 22.22% 36.36
R-A3 0.00% 0.00% 0.00
R-AM-LOC 0.00% 0.00% 0.00
R-AM-MNR 0.00% 0.00% 0.00
R-AM-PNC 0.00% 0.00% 0.00
R-AM-TMP 54.55% 42.86% 48.00
V 98.37% 98.37% 98.37
Table 3: Results on the test set.
X. Carreras and L. Ma`rquez. 2003. Phrase recognition by filter-
ing and ranking with perceptrons. In Proceedings of RANLP-
2003.
A. Grove and D. Roth. 2001. Linear concepts and hidden vari-
ables. Machine Learning, 42(1/2):123?141.
T. Hang, F. Damerau, , and D. Johnson. 2002. Text chunking
based on a generalization of winnow. Journal of Machine
Learning Research, 2:615?637.
V. Punyakanok and D. Roth. 2001. The use of classifiers in
sequential inference. In NIPS-13; The 2000 Conference on
Advances in Neural Information Processing Systems, pages
995?1001. MIT Press.
D. Roth and W. Yih. 2002. Probabilistic reasoning for entity
& relation recognition. In COLING 2002, The 19th Interna-
tional Conference on Computational Linguistics, pages 835?
841.
D. Roth and W. Yih. 2004. A linear programming formulation
for global inference in natural language tasks. In Proc. of
CoNLL-2004.
D. Roth. 1998. Learning to resolve natural language ambigui-
ties: A unified approach. In Proc. of AAAI, pages 806?813.
Xpress-MP. 2003. Dash Optimization. Xpress-MP.
http://www.dashoptimization.com/products.html.
Coling 2010: Poster Volume, pages 1265?1273,
Beijing, August 2010
Citation Author Topic Model in Expert Search
Yuancheng Tu, Nikhil Johri, Dan Roth, Julia Hockenmaier
University of Illinois at Urbana-Champaign
{ytu,njohri2,danr,juliahmr}@illinois.edu
Abstract
This paper proposes a novel topic model,
Citation-Author-Topic (CAT) model that
addresses a semantic search task we define
as expert search ? given a research area as
a query, it returns names of experts in this
area. For example, Michael Collins would
be one of the top names retrieved given the
query Syntactic Parsing.
Our contribution in this paper is two-fold.
First, we model the cited author informa-
tion together with words and paper au-
thors. Such extra contextual information
directly models linkage among authors
and enhances the author-topic association,
thus produces more coherent author-topic
distribution. Second, we provide a prelim-
inary solution to the task of expert search
when the learning repository contains ex-
clusively research related documents au-
thored by the experts. When compared
with a previous proposed model (Johri
et al, 2010), the proposed model pro-
duces high quality author topic linkage
and achieves over 33% error reduction
evaluated by the standard MAP measure-
ment.
1 Introduction
This paper addresses the problem of searching for
people with similar interests and expertise, given
their field of expertise as the query. Many existing
people search engines need people?s names to do a
?keyword? style search, using a person?s name as
a query. However, in many situations, such infor-
mation is insufficient or impossible to know be-
forehand. Imagine a scenario where the statistics
department of a university invited a world-wide
known expert in Bayesian statistics and machine
learning to give a keynote speech; how can the
organizer notify all the people on campus who
are interested without spamming those who are
not? Our paper proposes a solution to the afore-
mentioned scenario by providing a search engine
which goes beyond ?keyword? search and can re-
trieve such information semantically. The orga-
nizer would only need to input the research do-
main of the keynote speaker, i.e. Bayesian statis-
tics, machine learning, and all professors and stu-
dents who are interested in this topic will be re-
trieved and an email agent will send out the infor-
mation automatically.
Specifically, we propose a Citation-Author-
Topic (CAT) model which extracts academic re-
search topics and discovers different research
communities by clustering experts with similar in-
terests and expertise. CAT assumes three steps of
a hierarchical generative process when producing
a document: first, an author is generated, then that
author generates topics which ultimately generate
the words and cited authors. This model links
authors to observed words and cited authors via
latent topics and captures the intuition that when
writing a paper, authors always first have topics
in their mind, based on which, they choose words
and cite related works.
Corpus linguists or forensic linguists usually
1265
identify authorship of disputed texts based on
stylistic features, such as vocabulary size, sen-
tence length, word usage that characterize a spe-
cific author and the general semantic content is
usually ignored (Diederich et al, 2003). On the
other hand, graph-based and network based mod-
els ignore the content information of documents
and only focus on network connectivity (Zhang
et al, 2007; Jurczyk and Agichtein, 2007). In
contrast, the model we propose in this paper fully
utilizes the content words of the documents and
combines them with the stylistic flavor contex-
tual information to link authors and documents to-
gether to not only identify the authorship, but also
to be used in many other applications such as pa-
per reviewer recommendation, research commu-
nity identification as well as academic social net-
work search.
The novelty of the work presented in this pa-
per lies in the proposal of jointly modeling the
cited author information and using a discrimi-
native multinomial distribution to model the co-
author information instead of an artificial uni-
form distribution. In addition, we apply and eval-
uate our model in a semantic search scenario.
While current search engines cannot support in-
teractive and exploratory search effectively, our
model supports search that can answer a range of
exploratory queries. This is done by semantically
linking the interests of authors to the topics of the
collection, and ultimately to the distribution of the
words in the documents.
In the rest of this paper, we first present some
related work on author topic modeling and expert
search in Sec. 2. Then our model is described in
Sec. 3. Sec. 4 introduces our expert search system
and Sec. 5 presents our experiments and the evalu-
ation. We conclude this paper in Sec. 6 with some
discussion and several further developments.
2 Related Work
Author topic modeling, originally proposed
in (Steyvers et al, 2004; Rosen-Zvi et al, 2004),
is an extension of Latent Dirichlet Allocation
(LDA) (Blei et al, 2003), a probabilistic genera-
tive model that can be used to estimate the proper-
ties of multinomial observations via unsupervised
learning. LDA represents each document as a
mixture of probabilistic topics and each topic as
a multinomial distribution over words. The Au-
thor topic model adds an author layer over LDA
and assumes that the topic proportion of a given
document is generated by the chosen author.
Author topic analysis has attracted much atten-
tion recently due to its broad applications in ma-
chine learning, text mining and information re-
trieval. For example, it has been used to pre-
dict authors for new documents (Steyvers et al,
2004), to recommend paper reviewers (Rosen-Zvi
et al, 2004), to model message data (Mccallum et
al., 2004), to conduct temporal author topic anal-
ysis (Mei and Zhai, 2006), to disambiguate proper
names (Song et al, 2007), to search academic so-
cial networks (Tang et al, 2008) and to generate
meeting status analyses for group decision mak-
ing (Broniatowski, 2009).
In addition, there are many related works on
expert search at the TREC enterprise track from
2005 to 2007, which focus on enterprise scale
search and discovering relationships between enti-
ties. In that setting, the task is to find the experts,
given a web domain, a list of candidate experts
and a set of topics 1. The task defined in our paper
is different in the sense that our topics are hid-
den and our document repositories are more ho-
mogeneous since our documents are all research
papers authored by the experts. Within this set-
ting, we can explore in depth the influence of the
hidden topics and contents to the ranking of our
experts. Similar to (Johri et al, 2010), in this pa-
per we apply CAT in a semantic retrieval scenario,
where searching people is associated with a set of
hidden semantically meaningful topics instead of
their personal names.
In recent literature, there are three main lines of
work that extend author topic analyses. One line
of work is to relax the model?s ?bag-of-words?
assumption by automatically discovering multi-
word phrases and adding them into the original
model (Johri et al, 2010). Similar work has also
been proposed for other topic models such as
Ngram topic models (Wallach, 2006; Wang and
McCallum, 2005; Wang et al, 2007; Griffiths et
al., 2007).
1http://trec.nist.gov/pubs.html
1266
Another line of work models authors informa-
tion as a general contextual information (Mei and
Zhai, 2006) or associates documents with network
structure analysis (Mei et al, 2008; Serdyukov et
al., 2008; Sun et al, 2009). This line of work
aims to propose a general framework to deal with
collections of texts with an associated networks
structure. However, it is based on a different topic
model than ours; for example, Mei?s works (Mei
and Zhai, 2006; Mei et al, 2008) extend proba-
bilistic latent semantic analysis (PLSA), and do
not have cited author information explicitly.
Our proposal follows the last line of work
which extends author topic modeling with spe-
cific contextual information and directly captures
the association between authors and topics to-
gether with this contextual information (Tang et
al., 2008; Mccallum et al, 2004). For exam-
ple, in (Tang et al, 2008), publication venue is
added as one extra piece of contextual informa-
tion and in (Mccallum et al, 2004), email recip-
ients, which are treated as extra contextual infor-
mation, are paired with email authors to model an
email message corpus. In our proposed method,
the extra contextual information consists of the
cited authors in each documents. Such contextual
information directly captures linkage among au-
thors and cited authors, enhances author-topic as-
sociations, and therefore produces more coherent
author-topic distributions.
3 The Citation-Author-Topic (CAT)
Model
CAT extends previously proposed author topic
models by explicitly modelling the cited author
information during the generative process. Com-
pared with these models (Rosen-Zvi et al, 2004;
Johri et al, 2010), whose plate notation is shown
in Fig. 1, CAT (shown in Fig. 2) adds cited au-
thor information and generates authors according
to the observed author distribution.
Four plates in Fig. 1 represent topic (T ), au-
thor (A), document (D) and words in each doc-
ument (Nd) respectively. CAT (Fig. 2) has one
more plate, cited-author topic plate, in which each
topic is represented as a multinomial distribution
over all cited authors (?c).
Within CAT, each author is associated with a
 
D
A
N d
Figure 1: Plate notation of the previously pro-
posed author topic models (Rosen-Zvi et al,
2004; Johri et al, 2010).

D
A
N d

Figure 2: Plate notation of our current model:
CAT generates words W and cited authors C in-
dependently given the topic.
multinomial distribution over all topics, ~?a, and
each topic is a multinomial distribution over all
words, ~?t, as well as a multinomial distribution
over all cited authors ~?c. Three symmetric Dirich-
let conjugate priors, ?, ? and ?, are defined for
each of these three multinomial distributions in
CAT as shown in Fig. 2.
The generative process of CAT is formally de-
fined in Algorithm 1. The model first samples
the word-topic, cited author-topic and the author-
topic distributions according to the three Dirich-
let hyperparameters. Then for each word in each
document, first the author k is drawn from the
observed multinomial distribution and that author
chooses the topic zi, based on which word wi and
cited author ci are generated independently.
CAT differs from previously proposed MAT
(Multiword-enhanced Author Topic) model (Johri
et al, 2010) in two aspects. First of all, CAT uses
1267
Algorithm 1: CAT: A, T ,D,N are four
plates as shown in Fig. 2. The generative pro-
cess of CAT modeling.
Data: A, T ,D,N
for each topic t ? T do
draw a distribution over words:
~?t ? DirN (?) ;
draw a distribution over cited authors:
~?c ? DirC(?) ;
for each author a ? A do
draw a distribution over topics:
~?a ? DirT (?) ;
for each document d ? D and k authors ? d
do
for each word w ? d do
choose an author
k ? Multinomial(Ad) ;
assign a topic i given the author:
zk,i|k ? Multinomial(?a) ;
draw a word from the chosen topic:
wd,k,i|zk,i ? Multinomial(?zk,i) ;
draw a cited author from the topic:
cd,k,i|zk,i ? Multinomial(?zk,i)
cited author information to enhance the model
and assumes independence between generating
the words and cited authors given the topic. Sec-
ondly, instead of an artificial uniform distribution
over all authors and co-authors, CAT uses the ob-
served discriminative multinomial distribution to
generate authors.
3.1 Parameter Estimation
CAT includes three sets of parameters. The T
topic distribution over words, ?t which is similar
to that in LDA. The author-topic distribution ?a as
well as the cited author-topic distribution ?c. Al-
though CAT is a relatively simple model, finding
its posterior distribution over these hidden vari-
ables is still intractable due to their high dimen-
sionality. Many efficient approximate inference
algorithms have been used to solve this problem
including Gibbs sampling (Griffiths and Steyvers,
2004; Steyvers and Griffiths, 2007; Griffiths et al,
2007) and mean-field variational methods (Blei et
al., 2003). Gibbs sampling is a special case of
Markov-Chain Monte Carlo (MCMC) sampling
and often yields relatively simple algorithms for
approximate inference in high dimensional mod-
els.
In our CAT modeling, we use a collapsed Gibbs
sampler for our parameter estimation. In this
Gibbs sampler, we integrated out the hidden vari-
ables ?, ? and ? using the Dirichlet delta func-
tion (Heinrich, 2009). The Dirichlet delta func-
tion with an M dimensional symmetric Dirichlet
prior ? is defined as:
?M (?) =
?
(
?M
)
? (M?)
Based on the independence assumptions de-
fined in Fig. 2, the joint distribution of topics,
words and cited authors given all hyperparame-
ters which originally represented by integrals can
be transformed into the delta function format and
formally derived in Equation 1.
P (~z, ~w,~c|?, ?, ?) (1)
= P (~z|?, ?, ?)P (~w,~c|~z, ?, ?, ?)
= P (~z)P (~w|~z)P (~c|~z)
=
A?
a=1
?(nA+?)
?(?)
T?
z=1
?(nZw+?)
?(?)
T?
z=1
?(nZc+?)
?(?)
The updating equation from which the Gibbs
sampler draws the hidden variable for the current
state j, i.e., the conditional probability of drawing
the kth author Kkj , the ith topic Zij , and the cth
cited author Ccj tuple, given all the hyperparame-
ters and all the observed documents and authors,
cited authors except the current assignment (the
exception is denoted by the symbol ??j), is de-
fined in Equation 2.
P (Zij ,Kkj , Ccj |Wwj ,??j, Ad, ?, ?, ?) (2)
? ?(nZ+?)?(nZ,?j+?)
?(nK+?)
?(nK,?j+?)
?(nC+?)
?(nC,?j+?)
= n
w
i,?j+?w
V
P
w=1
nwi,?j+V ?w
nik,?j+?i
T
P
i=1
nik,?j+T?i
nci,?j+?c
C
P
c=1
nci,?j+C?c
The parameter sets ? and ?, ? can be interpreted
as sufficient statistics on the state variables of
the Markov Chain due to the Dirichlet conjugate
priors we used for the multinomial distributions.
1268
These three sets of parameters are estimated based
on Equations 3 , 4 and 5 respectively, in which nwi
is defined as the number of times the word w is
generated by topic i; nik is defined as the number
of times that topic i is generated by author k and
nic is defined as the number of times that the cited
author c is generated by topic i. The vocabulary
size is V , the number of topics is T and the cited-
author size is C.
?w,i =
nwi + ?w
V?
w=1
nwi + V ?w
(3)
?k,i =
nik + ?i
T?
i=1
nik + T?i
(4)
?c,i =
nci + ?c
C?
c=1
nci + C?c
(5)
The Gibbs sampler used in our experiments is
adapted from the Matlab Topic Modeling Tool-
box 2.
4 Expert Search
In this section, we describe a preliminary re-
trieval system that supports expert search, which
is intended to identify groups of research experts
with similar research interests and expertise by in-
putting only general domain key words. For ex-
ample, we can retrieve Michael Collins via search
for natural language parsing.
Our setting is different from the standard TREC
expert search in that we do not have a pre-defined
list of experts and topics, and our documents are
all research papers authored by experts. Within
this setting, we do not need to identify the status of
our experts, i.e., a real expert or a communicator,
as in TREC expert search. All of our authors and
cited authors are experts and the task amounts to
ranking the experts according to different topics
given samples of their research papers.
The ranking function of this retrieval model is
derived through the CAT parameters. The search
2http://psiexp.ss.uci.edu/research/programs data/
aims to link research topics with authors to by-
pass the proper names of these authors. Our re-
trieval function ranks the joint probability of the
query words (W ) and the target author (a), i.e.,
P (W,a). This probability is marginalized over all
topics, and the probability that an author is cited
given the topic is used as an extra weight in our
ranking function. The intuition is that an author
who is cited frequently should be more prominent
and ranked higher. Formally, we define the rank-
ing function of our retrieval system in Equation 6.
ca denotes when the author is one of the cited au-
thors in our corpus. CAT assumes that words and
authors, and cited authors are conditionally inde-
pendent given the topic, i.e., wi ? a ? ca.
P (W,a) =
?
wi
?i
?
t
P (wi, a|t, ca)P (t, ca)
=
?
wi
?i
?
t
P (wi|t)P (a|t)P (ca|t)P (t)
(6)
W is the input query, which may contain one or
more words. If a multiword is detected within the
query, it is added into the query. The final score
is the sum of all words in this query weighted by
their inverse document frequency ?i.
In our experiments, we chose ten queries which
cover several popular research areas in computa-
tional linguistics and natural language processing
and run the retrieval system based on three mod-
els: the original author topic model (Rosen-Zvi
et al, 2004), the MAT model (Johri et al, 2010)
and the CAT model. In the original author topic
model, query words are treated token by token.
Both MAT and CAT expand the query terms with
multiwords if they are detected inside the original
query. For each query, top 10 authors are returned
from the system. We manually label the relevance
of these 10 authors based on the papers collected
in our corpus.
Two standard evaluation metrics are used to
measure the retrieving results. First we evaluate
the precision at a given cut-off rank, namely pre-
cision at rank k with k ranging from 1 to 10. We
then calculate the average precision (AP) for each
query and the mean average precision (MAP) for
1269
the queries. Unlike precision at k, MAP is sensi-
tive to the ranking and captures recall information
since it assumes the precision of the non-retrieved
documents to be zero. It is formally defined as
the average of precisions computed at the point of
each of the relevant documents in the ranked list
as shown in Equation 7.
AP =
?n
r=1(Precision(r)? rel(r))
| relevant documents | (7)
To evaluate the recall of our system, we col-
lected a pool of authors for six of our queries re-
turned from an academic search engine, Arnet-
Miner (Tang et al, 2008)3 as our reference author
pool and evaluate our recall based on the number
of authors we retrieved from that pool.
5 Experiments and Analysis
In this section, we describe the empirical evalua-
tion of our model qualitatively and quantitatively
by applying our model to the expert search we de-
fined in Sec. 4. We compare the retrieving results
with two other models: Multiword- enhanced Au-
thor Topic (MAT) model (Johri et al, 2010) and
the original author topic model (Rosen-Zvi et al,
2004).
5.1 Data set and Pre-processing
We crawled the ACL anthology website and col-
lected papers from ACL, EMNLP and CONLL
over a period of seven years. The ACL anthol-
ogy website explicitly lists each paper together
with its title and author information. Therefore,
the author information of each paper can be ob-
tained accurately without extracting it from the
original paper. However, many author names are
not represented consistently. For example, the
same author may have his/her middle name listed
in some papers, but not in others. We therefore
normalized all author names by eliminating mid-
dle names from all authors.
Cited authors of each paper are extracted from
the reference section and automatically identified
by a named entity recognizer tuned for citation ex-
traction (Ratinov and Roth, 2009). Similar to reg-
ular authors, all cited authors are also normalized
3http://www.arnetminer.org
Conf. Year Paper Author uni. Vocab.
ACL 03-09 1,326 2,084 34,012 205,260
EMNLP 93-09 912 1,453 40,785 219,496
CONLL 97-09 495 833 27,312 123,176
Total 93-09 2,733 2,911 62,958 366,565
Table 1: Statistics about our data set. Uni. denotes
unigram words and Vocab. denotes all unigrams
and multiword phrases discovered in the data set.
with their first name initial and their full last name.
We extracted about 20,000 cited authors from our
corpus. However, for the sake of efficiency, we
only keep those cited authors whose occurrence
frequency in our corpus is above a certain thresh-
old. We experimented with thresholds of 5, 10 and
20 and retained the total number of 2,996, 1,771
and 956 cited authors respectively.
We applied the same strategy to extract mul-
tiwords from our corpus and added them into
our vocabulary to implement the model described
in (Johri et al, 2010). Some basic statistics about
our data set are summarized in Table 1 4.
5.2 Qualitative Coherence Analysis
As shown by other previous works (Wallach,
2006; Griffiths et al, 2007; Johri et al, 2010),
our model also demonstrates that embedding mul-
tiword tokens into the model can achieve more co-
hesive and better interpretable topics. We list the
top 10 words from two topics of CAT and compare
them with those from the unigram model in Ta-
ble 2. Unigram topics contain more general words
which can occur in every topic and are usually less
discriminative among topics.
Our experiments also show that CAT achieves
better retrieval quality by modeling cited authors
jointly with authors and words. The rank of an
author is boosted if that author is cited more fre-
quently. We present in Table 3 the ranking of one
of our ten query terms to demostrate the high qual-
ity of our proposed model. When compared to the
model without cited author information, CAT not
only retrieves more comprehensive expert list, its
ranking is also more reasonable than the model
without cited author information.
Another observation in our experiments is that
4Download the data and the software package at:
http://L2R.cs.uiuc.edu/?cogcomp/software.php.
1270
Query term: parsing
Proposed CAT Model Model without cited authors
Rank Author Prob. Author Prob.
1 J. Nivre 0.125229 J. Nivre 0.033200
2 C. Manning 0.111252 R. Barzilay 0.023863
3 M. Johnson 0.101342 M. Johnson 0.023781
4 J. Eisner 0.063528 D. Klein 0.018937
5 M. Collins 0.047347 R. McDonald 0.017353
6 G. Satta 0.042081 L. Marquez 0.016003
7 R. McDonald 0.041372 A. Moschitti 0.015781
8 D. Klein 0.041149 N. Smith 0.014792
9 K. Toutanova 0.024946 C. Manning 0.014040
10 E. Charniak 0.020843 K. Sagae 0.013384
Table 3: Ranking for the query term: parsing. CAT achieves more comprehensive and reasonable rank
list than the model without cited author information.
CAT Uni. AT Model
TOPIC 49 Topic 27
pronoun resolution anaphor
antecedent antecedents
coreference resolution anaphoricity
network anphoric
resolution is
anaphor anaphora
pronouns soon
anaphor antecedent determination
semantic knowledge pronominal
proper names salience
TOPIC 14 Topic 95
translation quality hypernym
translation systems seeds
source sentence taxonomy
word alignments facts
paraphrases hyponym
decoder walk
parallel corpora hypernyms
translation system page
parallel corpus logs
translation models extractions
Table 2: CAT with embedded multiword com-
ponents achieves more interpretable topics com-
pared with the unigram Author Topic (AT) model.
some experts who published many papers, but on
heterogeneous topics, may not be ranked at the
very top by models without cited author infor-
mation. However, with cited author information,
those authors are ranked higher. Intuitively this
makes sense since many of these authors are also
the most cited ones.
5.3 Quantitative retrieval results
One annotator labeled the relevance of the re-
trieval results from our expert search system. The
annotator was also given all the paper titles of each
Precision@K
K CAT Model Model w/o Cited Authors
1 0.80 0.80
2 0.80 0.70
3 0.73 0.60
4 0.70 0.50
5 0.68 0.48
6 0.70 0.47
7 0.69 0.40
8 0.68 0.45
9 0.73 0.44
10 0.70 0.44
Table 4: Precision at K evaluation of our proposed
model and the model without cited author infor-
mation.
corresponding retrieved author to help make this
binary judgment. We experiment with ten queries
and retrieve the top ten authors for each query.
We first used the precision at k for evaluation.
We calculate the precision at k for both our pro-
posed CAT model and the MAT model, which
does not have the cited author information. The
results are listed in Table 4. It can be observed
that at every rank position, our CAT model works
better. In order to focus more on relevant retrieval
results, we also calculated the mean average pre-
cision (MAP) for both models. For the given ten
queries, the MAP score for the CAT model is 0.78,
while the MAT model without cited author infor-
mation has a MAP score of 0.67. The CAT model
with cited author information achieves about 33%
error reduction in this experiment.
1271
Query ID Query Term
1 parsing
2 machine translation
3 dependency parsing
4 transliteration
5 semantic role labeling
6 coreference resolution
7 language model
8 Unsupervised Learning
9 Supervised Learning
10 Hidden Markov Model
Table 5: Queries and their corresponding ids we
used in our experiments.
Recall for each query
Query ID CAT Model Model w/o Cite
1 0.53 0.20
2 0.13 0.20
3 0.27 0.13
4 0.13 0.2
5 0.27 0.20
6 0.13 0.26
Average 0.24 0.20
Table 6: Recall comparison between our proposed
model and the model without cited author infor-
mation.
Since we do not have a gold standard experts
pool for our queries, to evaluate recall, we col-
lected a pool of authors returned from an aca-
demic search engine, ArnetMiner (Tang et al,
2008) as our reference author pool and evaluated
our recall based on the number of authors we re-
trieved from that pool. Specifically, we get the
top 15 returned persons from that website for each
query and treat them as the whole set of relevant
experts for that query and our preliminary recall
results are shown in Table 6.
In most cases, the CAT recall is better than that
of the compared model, and the average recall is
better as well. All the queries we used in our ex-
periments are listed in Table 5. And the average
recall value is based on six of the queries which
have at least one overlap author with those in our
reference recall pool.
6 Conclusion and Further Development
This paper proposed a novel author topic model,
CAT, which extends the existing author topic
model with additional cited author information.
We applied it to the domain of expert retrieval
and demonstrated the effectiveness of our model
in improving coherence in topic clustering and au-
thor topic association. The proposed model also
provides an effective solution to the problem of
community mining as shown by the promising re-
trieval results derived in our expert search system.
One immediate improvement would result from
extending our corpus. For example, we can ap-
ply our model to the ACL ARC corpus (Bird et
al., 2008) to check the model?s robustness and en-
hance the ranking by learning from more data. We
can also apply our model to data sets with rich
linkage structure, such as the TREC benchmark
data set or ACL Anthology Network (Radev et al,
2009) and try to enhance our model with the ap-
propriate network analysis.
Acknowledgments
The authors would like to thank Lev Ratinov for
his help with the use of the NER package and the
three anonymous reviewers for their helpful com-
ments and suggestions. The research in this pa-
per was supported by the Multimodal Information
Access & Synthesis Center at UIUC, part of CCI-
CADA, a DHS Science and Technology Center of
Excellence.
References
Bird, S., R. Dale, B. Dorr, B. Gibson, M. Joseph,
M. Kan, D. Lee, B Powley, D. Radev, and Y. Tan.
2008. The acl anthology reference corpus: A refer-
ence dataset for bibliographic research in computa-
tional linguistics. In Proceedings of LREC?08.
Blei, D., A. Ng, and M. Jordan. 2003. Latent dirichlet
allocation. Journal of Machine Learning Research.
Broniatowski, D. 2009. Generating status hierar-
chies from meeting transcripts using the author-
topic model. In In Proceedings of the Workshop:
Applications for Topic Models: Text and Beyond.
Diederich, J., J. Kindermann, E. Leopold, and
G. Paass. 2003. Authorship attribution with support
vector machines. Applied Intelligence, 19:109?123.
1272
Griffiths, T. and M. Steyvers. 2004. Finding scientific
topic. In Proceedings of the National Academy of
Science.
Griffiths, T., M. Steyvers, and J. Tenenbaum. 2007.
Topics in semantic representation. Psychological
Review.
Heinrich, G. 2009. Parameter estimation for text anal-
ysis. Technical report, Fraunhofer IGD.
Johri, N., D. Roth, and Y. Tu. 2010. Experts? retrieval
with multiword-enhanced author topic model. In
Proceedings of NAACL-10 Semantic Search Work-
shop.
Jurczyk, P. and E. Agichtein. 2007. Discovering au-
thorities in question answer communities by using
link analysis. In Proceedings of CIKM?07.
Mccallum, A., A. Corrada-emmanuel, and X. Wang.
2004. The author-recipient-topic model for topic
and role discovery in social networks: Experiments
with enron and academic email. Technical report,
University of Massachusetts Amherst.
Mei, Q. and C. Zhai. 2006. A mixture model for con-
textual text mining. In Proceedings of KDD-2006,
pages 649?655.
Mei, Q., D. Cai, D. Zhang, and C. Zhai. 2008. Topic
modeling with network regularization. In Proceed-
ing of WWW-08:, pages 101?110.
Radev, D., M. Joseph, B. Gibson, and P. Muthukrish-
nan. 2009. A Bibliometric and Network Analysis
of the field of Computational Linguistics. Journal
of the American Society for Information Science and
Technology.
Ratinov, L. and D. Roth. 2009. Design challenges
and misconceptions in named entity recognition. In
Proc. of the Annual Conference on Computational
Natural Language Learning (CoNLL).
Rosen-Zvi, M., T. Griffiths, M. Steyvers, and P. Smyth.
2004. the author-topic model for authors and docu-
ments. In Proceedings of UAI.
Serdyukov, P., H. Rode, and D. Hiemstra. 2008. Mod-
eling multi-step relevance propagation for expert
finding. In Proceedings of CIKM?08.
Song, Y., J. Huang, and I. Councill. 2007. Efficient
topic-based unsupervised name disambiguation. In
Proceedings of JCDL-2007, pages 342?351.
Steyvers, M. and T. Griffiths. 2007. Probabilistic topic
models. In Handbook of Latent Semantic Analysis.
Lawrence Erlbaum Associates.
Steyvers, M., P. Smyth, and T. Griffiths. 2004. Proba-
bilistic author-topic models for information discov-
ery. In Proceedings of KDD.
Sun, Y., J. Han, J. Gao, and Y. Yu. 2009. itopicmodel:
Information network-integrated topic modeling. In
Proceedings of ICDM-2009.
Tang, J., J. Zhang, L. Yao, J. Li, L. Zhang, and Z. Su.
2008. Arnetminer: Extraction and mining of aca-
demic social networks. In Proceedings of KDD-
2008, pages 990?998.
Wallach, H. 2006. Topic modeling; beyond bag of
words. In International Conference on Machine
Learning.
Wang, X. and A. McCallum. 2005. A note on topi-
cal n-grams. Technical report, University of Mas-
sachusetts.
Wang, X., A. McCallum, and X. Wei. 2007. Topical
n-grams: Phrase and topic discoery with an appli-
cation to information retrieval. In Proceedings of
ICDM.
Zhang, J., M. Ackerman, and L. Adamic. 2007. Ex-
pertise networks in online communities: Structure
and algorithms. In Proceedings of WWW 2007.
1273
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 65?69,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Sorting out the Most Confusing English Phrasal Verbs
Yuancheng Tu
Department of Linguistics
University of Illinois
ytu@illinois.edu
Dan Roth
Department of Computer Science
University of Illinois
danr@illinois.edu
Abstract
In this paper, we investigate a full-fledged
supervised machine learning framework for
identifying English phrasal verbs in a given
context. We concentrate on those that we de-
fine as the most confusing phrasal verbs, in the
sense that they are the most commonly used
ones whose occurrence may correspond either
to a true phrasal verb or an alignment of a sim-
ple verb with a preposition.
We construct a benchmark dataset1 with 1,348
sentences from BNC, annotated via an Inter-
net crowdsourcing platform. This dataset is
further split into two groups, more idiomatic
group which consists of those that tend to be
used as a true phrasal verb and more compo-
sitional group which tends to be used either
way. We build a discriminative classifier with
easily available lexical and syntactic features
and test it over the datasets. The classifier
overall achieves 79.4% accuracy, 41.1% er-
ror deduction compared to the corpus major-
ity baseline 65%. However, it is even more
interesting to discover that the classifier learns
more from the more compositional examples
than those idiomatic ones.
1 Introduction
Phrasal verbs in English, are syntactically defined
as combinations of verbs and prepositions or parti-
cles, but semantically their meanings are generally
not the direct sum of their parts. For example, give
in means submit, yield in the sentence, Adam?s say-
ing it?s important to stand firm , not give in to ter-
rorists. Adam was not giving anything and he was
1http://cogcomp.cs.illinois.edu/page/resources/PVC Data
not in anywhere either. (Kolln and Funk, 1998) uses
the test of meaning to detect English phrasal verbs,
i.e., each phrasal verb could be replaced by a single
verb with the same general meaning, for example,
using yield to replace give in in the aforementioned
sentence. To confuse the issue even further, some
phrasal verbs, for example, give in in the follow-
ing two sentences, are used either as a true phrasal
verb (the first sentence) or not (the second sentence)
though their surface forms look cosmetically similar.
1. How many Englishmen gave in to their emo-
tions like that ?
2. It is just this denial of anything beyond what is
directly given in experience that marks Berke-
ley out as an empiricist .
This paper is targeting to build an automatic learner
which can recognize a true phrasal verb from its
orthographically identical construction with a verb
and a prepositional phrase. Similar to other types
of MultiWord Expressions (MWEs) (Sag et al,
2002), the syntactic complexity and semantic id-
iosyncrasies of phrasal verbs pose many particular
challenges in empirical Natural Language Process-
ing (NLP). Even though a few of previous works
have explored this identification problem empiri-
cally (Li et al, 2003; Kim and Baldwin, 2009) and
theoretically (Jackendoff, 2002), we argue in this pa-
per that this context sensitive identification problem
is not so easy as conceivably shown before, espe-
cially when it is used to handle those more com-
positional phrasal verbs which are empirically used
either way in the corpus as a true phrasal verb or
a simplex verb with a preposition combination. In
addition, there is still a lack of adequate resources
or benchmark datasets to identify and treat phrasal
65
verbs within a given context. This research is also
an attempt to bridge this gap by constructing a pub-
licly available dataset which focuses on some of the
most commonly used phrasal verbs within their most
confusing contexts.
Our study in this paper focuses on six of the most
frequently used verbs, take, make, have, get, do
and give and their combination with nineteen com-
mon prepositions or particles, such as on, in, up
etc. We categorize these phrasal verbs according to
their continuum of compositionality, splitting them
into two groups based on the biggest gap within
this scale, and build a discriminative learner which
uses easily available syntactic and lexical features to
analyze them comparatively. This learner achieves
79.4% overall accuracy for the whole dataset and
learns the most from the more compositional data
with 51.2% error reduction over its 46.6% baseline.
2 Related Work
Phrasal verbs in English were observed as one kind
of composition that is used frequently and consti-
tutes the greatest difficulty for language learners
more than two hundred and fifty years ago in Samuel
Johnson?s Dictionary of English Language2. They
have also been well-studied in modern linguistics
since early days (Bolinger, 1971; Kolln and Funk,
1998; Jackendoff, 2002). Careful linguistic descrip-
tions and investigations reveal a wide range of En-
glish phrasal verbs that are syntactically uniform,
but diverge largely in semantics, argument struc-
ture and lexical status. The complexity and idiosyn-
crasies of English phrasal verbs also pose a spe-
cial challenge to computational linguistics and at-
tract considerable amount of interest and investi-
gation for their extraction, disambiguation as well
as identification. Recent computational research on
English phrasal verbs have been focused on increas-
ing the coverage and scalability of phrasal verbs by
either extracting unlisted phrasal verbs from large
corpora (Villavicencio, 2003; Villavicencio, 2006),
or constructing productive lexical rules to gener-
ate new cases (Villanvicencio and Copestake, 2003).
Some other researchers follow the semantic regular-
ities of the particles associated with these phrasal
verbs and concentrate on disambiguation of phrasal
2It is written in the Preface of that dictionary.
verb semantics, such as the investigation of the most
common particle up by (Cook and Stevenson, 2006).
Research on token identification of phrasal verbs
is much less compared to the extraction. (Li et
al., 2003) describes a regular expression based sim-
ple system. Regular expression based method re-
quires human constructed regular patterns and can-
not make predictions for Out-Of-Vocabulary phrasal
verbs. Thus, it is hard to be adapted to other NLP
applications directly. (Kim and Baldwin, 2009) pro-
poses a memory-based system with post-processed
linguistic features such as selectional preferences.
Their system assumes the perfect outputs of a parser
and requires laborious human corrections to them.
The research presented in this paper differs from
these previous identification works mainly in two
aspects. First of all, our learning system is fully
automatic in the sense that no human intervention
is needed, no need to construct regular patterns or
to correct parser mistakes. Secondly, we focus our
attention on the comparison of the two groups of
phrasal verbs, the more idiomatic group and the
more compositional group. We argue that while
more idiomatic phrasal verbs may be easier to iden-
tify and can have above 90% accuracy, there is still
much room to learn for those more compostional
phrasal verbs which tend to be used either positively
or negatively depending on the given context.
3 Identification of English Phrasal Verbs
We formulate the context sensitive English phrasal
verb identification task as a supervised binary clas-
sification problem. For each target candidate within
a sentence, the classifier decides if it is a true phrasal
verb or a simplex verb with a preposition. Formally,
given a set of n labeled examples {xi, yi}ni=1, we
learn a function f : X ? Y where Y ? {?1, 1}.
The learning algorithm we use is the soft-margin
SVM with L2-loss. The learning package we use
is LIBLINEAR (Chang and Lin, 2001)3.
Three types of features are used in this discrimi-
native model. (1)Words: given the window size from
the one before to the one after the target phrase,
Words feature consists of every surface string of
all shallow chunks within that window. It can be
an n-word chunk or a single word depending on
3http://www.csie.ntu.edu.tw/?cjlin/liblinear/
66
the the chunk?s bracketing. (2)ChunkLabel: the
chunk name with the given window size, such as VP,
PP, etc. (3)ParserBigram: the bi-gram of the non-
terminal label of the parents of both the verb and
the particle. For example, from this partial tree (VP
(VB get)(PP (IN through)(NP (DT the)(NN day))),
the parent label for the verb get is VP and the par-
ent node label for the particle through is PP. Thus,
this feature value is VP-PP. Our feature extractor
is implemented in Java through a publicly available
NLP library4 via the tool called Curator (Clarke et
al., 2012). The shallow parser is publicly avail-
able (Punyakanok and Roth, 2001)5 and the parser
we use is from (Charniak and Johnson, 2005).
3.1 Data Preparation and Annotation
All sentences in our dataset are extracted from BNC
(XML Edition), a balanced synchronic corpus con-
taining 100 million words collected from various
sources of British English. We first construct a list of
phrasal verbs for the six verbs that we are interested
in from two resources, WN3.0 (Fellbaum, 1998)
and DIRECT6. Since these targeted verbs are also
commonly used in English Light Verb Constructions
(LVCs), we filter out LVCs in our list using a pub-
licly available LVC corpus (Tu and Roth, 2011). The
result list consists of a total of 245 phrasal verbs.
We then search over BNC and find sentences for all
of them. We choose the frequency threshold to be
25 and generate a list of 122 phrasal verbs. Finally
we manually pick out 23 of these phrasal verbs and
sample randomly 10% extracted sentences for each
of them for annotation.
The annotation is done through a crowdsourcing
platform7. The annotators are asked to identify true
phrasal verbs within a sentence. The reported inner-
annotator agreement is 84.5% and the gold aver-
age accuracy is 88%. These numbers indicate the
good quality of the annotation. The final corpus
consists of 1,348 sentences among which, 65% with
a true phrasal verb and 35% with a simplex verb-
preposition combination.
4http://cogcomp.cs.illinois.edu/software/edison/
5http://cogcomp.cs.illinois.edu/page/software view/Chunker
6http://u.cs.biu.ac.il/?nlp/downloads/DIRECT.html
7crowdflower.com
3.2 Dataset Splitting
Table 1 lists all verbs in the dataset. Total is the to-
tal number of sentences annotated for that phrasal
verb and Positive indicated the number of examples
which are annotated as containing the true phrasal
verb usage. In this table, the decreasing percent-
age of the true phrasal verb usage within the dataset
indicates the increasing compositionality of these
phrasal verbs. The natural division line with this
scale is the biggest percentage gap (about 10%) be-
tween make out and get at. Hence, two groups are
split over that gap. The more idiomatic group con-
sists of the first 11 verbs with 554 sentences and 91%
of these sentences include true phrasal verb usage.
This data group is more biased toward the positive
examples. The more compositional data group has
12 verbs with 794 examples and only 46.6% of them
contain true phrasal verb usage. Therefore, this data
group is more balanced with respective to positive
and negative usage of the phrase verbs.
Verb Total Positive Percent(%)
get onto 6 6 1.00
get through 61 60 0.98
get together 28 27 0.96
get on with 70 67 0.96
get down to 17 16 0.94
get by 11 10 0.91
get off 51 45 0.88
get behind 7 6 0.86
take on 212 181 0.85
get over 34 29 0.85
make out 57 48 0.84
get at 35 26 0.74
get on 142 103 0.73
take after 10 7 0.70
do up 13 8 0.62
get out 206 118 0.57
do good 8 4 0.50
make for 140 65 0.46
get it on 9 3 0.33
get about 20 6 0.30
make over 12 3 0.25
give in 118 27 0.23
have on 81 13 0.16
Total: 23 1348 878 0.65
Table 1: The top group consists of the more idiomatic
phrasal verbs with 91% of their occurrence within the
dataset to be a true phrasal verb. The second group con-
sists of those more compositional ones with only 46.6%
of their usage in the dataset to be a true phrasal verb.
67
3.3 Experimental Results and Discussion
Our results are computed via 5-cross validation. We
plot the classifier performance with respect to the
overall dataset, the more compositional group and
the more idiomatic group in Figure 1. The clas-
sifier only improves 0.6% when evaluated on the
idiomatic group. Phrasal verbs in this dataset are
more biased toward behaving like an idiom regard-
less of their contexts, thus are more likely to be cap-
tured by rules or patterns. We assume this may ex-
plain some high numbers reported in some previ-
ous works. However, our classifier is more effec-
tive over the more compositional group and reaches
73.9% accuracy, a 51.1% error deduction comparing
to its majority baseline. Phrasal verbs in this set tend
to be used equally likely as a true phrasal verb and
as a simplex verb-preposition combination, depend-
ing on their context. We argue phrasal verbs such as
these pose a real challenge for building an automatic
context sensitive phrasal verb classifier. The overall
accuracy of our preliminary classifier is about 79.4%
when it is evaluated over all examples from these
two groups.
 0
 0.2
 0.4
 0.6
 0.8
 1
 1.2
Overall Compositional Idiomatic
Ac
cu
ra
cy
Data Groups
Classifier Accuracy for Different Data Groups
Comparison against their Majority Baselines Respectively
Majority Baseline
Classifier Accuracy
Figure 1: Classifier Accuracy of each data group, com-
paring with their baseline respectively. Classifier learns
the most from the more compositional group, indicated
by its biggest histogram gap.
Finally, we conduct an ablation analysis to ex-
plore the contributions of the three types of features
in our model and their accuracies with respect to
each data group are listed in Table 2 with the bold-
faced best performance. Each type of features is
used individually in the classifier. The feature type
Words is the most effective feature with respect to
the idiomatic group and the overall dataset. And the
chunk feature is more effective towards the compo-
sitional group, which may explain the linguistic in-
tuition that negative phrasal verbs usually do not be-
long to the same syntactic chunk.
Datasets
Overall Compositional Idiom.
Baseline 65.0% 46.6% 91%
Words 78.6% 70.2% 91.4%
Chunk 65.6% 70.7% 89.4%
ParserBi 64.4% 67.2% 89.4%
Table 2: Accuracies achieved by the classifier when
tested on different data groups. Features are used indi-
vidually to evaluate the effectiveness of each type.
4 Conclusion
In this paper, we build a discriminative learner to
identify English phrasal verbs in a given context.
Our contributions in this paper are threefold. We
construct a publicly available context sensitive En-
glish phrasal verb dataset with 1,348 sentences from
BNC. We split the dataset into two groups according
to their tendency toward idiosyncrasy and compo-
sitionality, and build a discriminative learner which
uses easily available syntactic and lexical features to
analyze them comparatively. We demonstrate em-
pirically that high accuracy achieved by models may
be due to the stronger idiomatic tendency of these
phrasal verbs. For many of the more ambiguous
cases, a classifier learns more from the composi-
tional examples and these phrasal verbs are shown
to be more challenging.
Acknowledgments
The authors would like to thank four annonymous
reviewers for their valuable comments. The research
in this paper was supported by the Multimodal Infor-
mation Access & Synthesis Center at UIUC, part of
CCICADA, a DHS Science and Technology Center
of Excellence and the Defense Advanced Research
Projects Agency (DARPA) Machine Reading Pro-
gram under Air Force Research Laboratory (AFRL)
prime contract no. FA8750-09-C-0181. Any opin-
ions and findings expressed in this material are those
of the authors and do not necessarily reflect the view
of DHS, DARPA, AFRL, or the US government.
68
References
D. Bolinger. 1971. The Phrasal Verb in English. Har-
vard University Press.
C. Chang and C. Lin, 2001. LIBSVM: a library
for support vector machines. Software available at
http://www.csie.ntu.edu.tw/?cjlin/libsvm.
E. Charniak and M. Johnson. 2005. Coarse-to-fine n-
best parsing and maxent discriminative reranking. In
Proceedings of ACL-2005.
J. Clarke, V. Srikumar, M. Sammons, and D. Roth. 2012.
An NLP curator: How I learned to stop worrying and
love NLP pipelines. In Proceedings of LREC-2012.
P. Cook and S. Stevenson. 2006. Classifying particle
semantics in English verb-particle constructions. In
Proceedings of the Workshop on Multiword Expres-
sions: Identifying and Exploiting Underlying Proper-
ties, pages 45?53, Sydney, Australia.
C. Fellbaum, editor. 1998. WordNet: An Electronic Lex-
ical Database. MIT Press.
R. Jackendoff. 2002. English particle constructions, the
lexicon, and the autonomy of syntax. In N. Dehe?,
R. Jackendoff, A. McIntyre, and S. Urban, editors,
Verb-Particle Explorations, pages 67?94. Mouton de
Gruyter.
S Kim and T. Baldwin. 2009. How to pick out token
instances of English verb-particle constructions. Jour-
nal of Language Resources and Evaluation.
M. Kolln and R. Funk. 1998. Understanding English
Grammar. Allyn and Bacon.
W. Li, X. Zhang, C. Niu, Y. Jiang, and R. Srihari. 2003.
An expert lexicon approach to identifying English
phrasal verbs. In Proceedings of the 41st Annual Meet-
ing of ACL, pages 513?520.
V. Punyakanok and D. Roth. 2001. The use of classifiers
in sequential inference. In NIPS, pages 995?1001.
I. Sag, T. Baldwin, F. Bond, and A. Copestake. 2002.
Multiword expressions: A pain in the neck for NLP.
In Proc. of the 3rd International Conference on Intel-
ligent Text Processing and Computational Linguistics
(CICLing-2002), pages 1?15.
Y. Tu and D. Roth. 2011. Learning english light verb
constructions: Contextual or statistica. In Proceedings
of the ACL Workshop on Multiword Expressions: from
Parsing and Generation to the Real World.
A. Villanvicencio and A. Copestake. 2003. Verb-particle
constructions in a computational grammar of English.
In Proceedings of the 9th International Conference on
HPSG, pages 357?371.
A. Villavicencio. 2003. Verb-particle constructions and
lexical resources. In Proceedings of the ACL 2003
Workshop on Multiword Expressions: Analysis, Acqui-
sition and Treatment, pages 57?64.
A. Villavicencio, 2006. Computational Linguistics Di-
mensions of the Syntax and Semantics of Prepositions,
chapter Verb-Particel Constructions in the World Wide
Web. Springer.
69
Proceedings of the NAACL HLT 2010 Workshop on Semantic Search, pages 10?18,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Experts? Retrieval with Multiword-Enhanced Author Topic Model
Nikhil Johri Dan Roth Yuancheng Tu
Dept. of Computer Science Dept. of Linguistics
University of Illinois at Urbana-Champaign
{njohri2,danr,ytu}@illinois.edu
Abstract
In this paper, we propose a multiword-
enhanced author topic model that clusters au-
thors with similar interests and expertise, and
apply it to an information retrieval system that
returns a ranked list of authors related to a key-
word. For example, we can retrieve Eugene
Charniak via search for statistical parsing.
The existing works on author topic model-
ing assume a ?bag-of-words? representation.
However, many semantic atomic concepts are
represented by multiwords in text documents.
This paper presents a pre-computation step as
a way to discover these multiwords in the cor-
pus automatically and tags them in the term-
document matrix. The key advantage of this
method is that it retains the simplicity and
the computational efficiency of the unigram
model. In addition to a qualitative evaluation,
we evaluate the results by using the topic mod-
els as a component in a search engine. We ex-
hibit improved retrieval scores when the docu-
ments are represented via sets of latent topics
and authors.
1 Introduction
This paper addresses the problem of searching peo-
ple with similar interests and expertise without in-
putting personal names as queries. Many existing
people search engines need people?s names to do a
?keyword? style search, using a person?s name as a
query. However, in many situations, such informa-
tion is impossible to know beforehand. Imagine a
scenario where the statistics department of a univer-
sity invited a world-wide known expert in Bayesian
statistics and machine learning to give a keynote
speech; how can the department head notify all the
people on campus who are interested without spam-
ming those who are not? Our paper proposes a solu-
tion to the aforementioned scenario by providing a
search engine which goes beyond ?keyword? search
and can retrieve such information semantically. The
department head would only need to input the do-
main keyword of the keynote speaker, i.e. Bayesian
statistics, machine learning, and all professors and
students who are interested in this topic will be
retrieved. Specifically, we propose a Multiword-
enhanced Author-Topic Model (MATM), a proba-
bilistic generative model which assumes two steps
of generation process when producing a document.
Statistical topical modeling (Blei and Lafferty,
2009a) has attracted much attention recently due to
its broad applications in machine learning, text min-
ing and information retrieval. In these models, se-
mantic topics are represented by multinomial distri-
bution over words. Typically, the content of each
topic is visualized by simply listing the words in or-
der of decreasing probability and the ?meaning? of
each topic is reflected by the top 10 to 20 words in
that list. The Author-Topic Model (ATM) (Steyvers
et al, 2004; Rosen-Zvi et al, 2004) extends the ba-
sic topical models to include author information in
which topics and authors are modeled jointly. Each
author is a multinomial distribution over topics and
each topic is a multinomial distribution over words.
Our contribution to this paper is two-fold. First
of all, our model, MATM, extends the original ATM
by adding semantically coherent multiwords into the
term-document matrix to relax the model?s ?bag-of-
10
words? assumption. Each multiword is discovered
via statistical measurement and filtered by its part of
speech pattern via an off-line way. One key advan-
tage of tagging these semantic atomic units off-line,
is the retention of the flexibility and computational
efficiency in using the simpler word exchangeable
model, while providing better interpretation of the
topics author distribution.
Secondly, to the best of our knowledge, this is
the first proposal to apply the enhanced author topic
modeling in a semantic retrieval scenario, where
searching people is associated with a set of hid-
den semantically meaningful topics instead of their
names. While current search engines cannot sup-
port interactive and exploratory search effectively,
search based on our model serves very well to an-
swer a range of exploratory queries about the doc-
ument collections by semantically linking the inter-
ests of the authors to the topics of the collection, and
ultimately to the distribution of the words in the doc-
uments.
The rest of the paper is organized as follows. We
present some related work on topic modeling, the
original author-topic model and automatic phrase
discovery methods in Sec. 2. Then our model is de-
scribed in Sec. 3. Sec. 4 presents our experiments
and the evaluation of our method on expert search.
We conclude this paper in Sec. 5 with some discus-
sion and several further developments.
2 Related Work
Author topic modeling, originally proposed
in (Steyvers et al, 2004; Rosen-Zvi et al, 2004), is
an extension of another popular topic model, Latent
Dirichlet Allocation (LDA) (Blei et al, 2003), a
probabilistic generative model that can be used to
estimate the properties of multinomial observations
via unsupervised learning. LDA represents each
document as a mixture of probabilistic topics and
each topic as a multinomial distribution over words.
The Author topic model adds an author layer over
LDA and assumes that the topic proportion of a
given document is generated by the chosen author.
Both LDA and the author topic model assume
bag-of-words representation. As shown by many
previous works (Blei et al, 2003; Steyvers et al,
2004), even such unrealistic assumption can actu-
ally lead to a reasonable topic distribution with rel-
atively simple and computationally efficient infer-
ence algorithm. However, this unigram represen-
tation also poses major handicap when interpreting
and applying the hidden topic distributions. The
proposed MATM is an effort to try to leverage this
problem in author topic modeling. There have been
some works on Ngram topic modeling over the orig-
inal LDA model (Wallach, 2006; Wang and McCal-
lum, 2005; Wang et al, 2007; Griffiths et al, 2007).
However, to the best of our knowledge, this paper
is the first to embed multiword expressions into the
author topic model.
Many of these Ngram topic models (Wang and
McCallum, 2005; Wang et al, 2007; Griffiths et
al., 2007) improves the base model by adding a new
indicator variable xi to signify if a bigram should
be generated. If xi = 1, the word wi is gener-
ated from a distribution that depends only on the
previous word to form an Ngram. Otherwise, it is
generated from a distribution only on the topic pro-
portion (Griffiths et al, 2007) or both the previous
words and the latent topic (Wang and McCallum,
2005; Wang et al, 2007). However, these complex
models not only increase the parameter size to V
times larger than the size of the original LDA model
parameters (V is the size of the vocabulary of the
document collection) 1, it also faces the problem of
choosing which word to be the topic of the potential
Ngram. In many text retrieval tasks, the humongous
size of data may prevent us using such complicated
computation on-line. However, our model retains
the computational efficiency by adding a simple tag-
ging process via pre-computation.
Another effort in the current literature to interpret
the meaning of the topics is to label the topics via
a post-processing way (Mei et al, 2007; Blei and
Lafferty, 2009b; Magatti et al, 2009). For example,
Probabilistic topic labeling (Mei et al, 2007) first
extracts a set of candidate label phrases from a refer-
ence collection and represents each candidate label-
ing phrase with a multinomial distribution of words.
Then KL divergence is used to rank the most prob-
able labels for a given topic. This method needs not
only extra reference text collection, but also facing
1LDA collocation models and topic Ngram models also have
parameters for the binomial distribution of the indicator variable
xi for each word in the vocabulary.
11
the problem of finding discriminative and high cov-
erage candidate labels. Blei and Lafferty (Blei and
Lafferty, 2009b) proposed a method to annotate each
word of the corpus by its posterior word topic distri-
bution and then cast a statistical co-occurrence anal-
ysis to extract the most significant Ngrams for each
topic and visualize the topic with these Ngrams.
However, they only applied their method to basic
LDA model.
In this paper, we applied our multiword extension
to the author topic modeling and no extra reference
corpora are needed. The MATM, with an extra pre-
computing step to add meaningful multiwords into
the term-document matrix, enables us to retain the
flexibility and computational efficiency to use the
simpler word exchangeable model, while providing
better interpretation of the topics and author distri-
bution.
3 Multiword-enhanced Author-Topic
Model
The MATM is an extension of the original ATM
(Rosen-Zvi et al, 2004; Steyvers et al, 2004) by
semantically tagging collocations or multiword ex-
pressions, which represent atomic concepts in doc-
uments in the term-document matrix of the model.
Such tagging procedure enables us to retain compu-
tational efficiency of the word-level exchangeabil-
ity of the orginal ATM while provides more sensi-
ble topic distributions and better author topic coher-
ence. The details of our model are presented in Al-
gorithm 1.
3.1 Beyond Bag-of-Words Tagging
The first for loop in Algorithm 1 is the procedure
of our multiword tagging. Commonly used ngrams,
or statistically short phrases in text retrieval, or
so-called collocations in natural language process-
ing have long been studied by linguistics in vari-
ous ways. Traditional collocation discovery meth-
ods range from frequency to mean and variance,
from statistical hypothesis testing, to mutual infor-
mation (Manning and Schtze, 1999). In this pa-
per, we use a simple statistical hypothesis testing
method, namely Pearson?s chi-square test imple-
mented in Ngram Statistic Package (Banerjee and
Pedersen, 2003), enhanced by passing the candidate
phrases through some pre-defined part of speech
patterns that are likely to be true phrases. This
very simple heuristic has been shown to improve the
counting based methods significantly (Justenson and
Katz, 1995).
The ?2 test is chosen since it does not assume any
normally distributed probabilities and the essence
of this test is to compare the observed frequencies
with the frequencies expected for independence. We
choose this simple statistic method since in many
text retrieval tasks the volume of data we see al-
ways makes it impractical to use very sophisticated
statistical computations. We also focus on nominal
phrases, such as bigram and trigram noun phrases
since they are most likely to function as semantic
atomic unit to directly represent the concepts in text
documents.
3.2 Author Topic Modeling
The last three generative procedures described in Al-
gorithm 1 jointly model the author and topic infor-
mation. This generative model is adapted directly
from (Steyvers et al, 2004). Graphically, it can be
visualized as shown in Figure 1.
Figure 1: Plate notation of our model: MATM
The four plates in Fiture 1 represent topic (T), au-
thor (A), document (D) and Words in each document
(Nd) respectively. Each author is associated with a
multinomial distribution over all topics, ~?a and each
topic is a multinomial distribution over all words, ~?t.
Each of these distribution has a symmetric Dirichlet
prior over it, ~? and ~? respectively. When generat-
ing a document, an author k is first chosen according
to a uniform distribution. Then this author chooses
the topic from his/her associated multinomial distri-
bution over topics and then generates a word from
the multinomial distribution of that topic over the
12
words.
Algorithm 1: MATM: A,T ,D,N are four
plates as shown in Fig. 1. The first for loop is the
off-line process of multiword expressions. The
rest of the algorithm is the generative process of
the author topic modeling.
Data: A,T ,D,N
for all documents d ? D do
Part-of-Speech tagging ;
Bigram extraction ;
Part-of Speech Pattern Filtering ;
Add discovered bigrams into N ;
for each author a ? A do
draw a distribution over topics:
~?a ? DirT (~?) ;
for each topic t ? T do
draw a distribution over words:
~?t ? DirN (~?) ;
for each document d ? D and k authors ? d do
for each word w ? d do
choose an author k ? uniformly;
draw a topic assignment i given the
author: zk,i|k ? Multinomial(?a) ;
draw a word from the chosen topic:
wd,k,i|zk,i ? Multinomial(?zk,i) ;
MATM includes two sets of parameters. The T
topic distribution over words, ?t which is similar to
that in LDA. However, instead of a document-topic
distribution, author topic modeling has the author-
topic distribution, ?a. Using a matrix factorization
interpretation, similar to what Steyvers, Griffiths and
Hofmann have pointed out for LDA (Steyvers and
Griffiths, 2007) and PLSI (Hofmann, 1999), a word-
author co-occurrence matrix in author topic model
can be split into two parts: a word-topic matrix ?
and a topic-author matrix ?. And the hidden topic
serves as the low dimensional representation for the
content of the document.
Although the MATM is a relatively simple model,
finding its posterior distribution over these hidden
variables is still intractable. Many efficient ap-
proximate inference algorithms have been used to
solve this problem including Gibbs sampling (Grif-
fiths and Steyvers, 2004; Steyvers and Griffiths,
2007; Griffiths et al, 2007) and mean-field vari-
ational methods (Blei et al, 2003). Gibbs sam-
pling is a special case of Markov-Chain Monte Carlo
(MCMC) sampling and often yields relatively sim-
ple algorithms for approximate inference in high di-
mensional models.
In our MATM, we use a collapsed Gibbs sam-
pler for our parameter estimation. In this Gibbs
sampler, we integrated out the hidden variables ?
and ? as shown by the delta function in equation 2.
This Dirichlet delta function with a M dimentional
symmetric Dirichlet prior is defined in Equation 1.
For the current state j, the conditional probability
of drawing the kth author Kkj and the ith topic Zij
pair, given all the hyperparameters and all the obe-
served documents and authors except the current as-
signment (the exception is denoted by the symbol
?j), is defined in Equation 2.
?M (?) =
?
(
?M
)
? (M?) (1)
P (Zij ,Kkj |Wj = w,Z?j ,K?j ,W?j , Ad, ~?, ~?)
?
?(nZ+~?)
?(nZ,?j+~?)
?(nK+~?)
?(nK,?j+~?)
= n
w
i,?j+ ~?w
?V
w=1 nwi,?j+V ~?w
nik,?j+~?i
?T
i=1 nik,?j+T ~?i
(2)
And the parameter sets ? and ? can be interpreted
as sufficient statistics on the state variables of the
Markov Chain due to the Dirichlet conjugate priors
we used for the multinomial distributions. The two
formulars are shown in Equation 3 and Equation 4 in
which nwi is defined as the number of times that the
word w is generated by topic i and nik is defined as
the number of times that topic i is generated by au-
thor k. The Gibbs sampler used in our experiments
is from the Matlab Topic Modeling Toolbox 2.
?w,i =
nwi + ~?w
?V
w=1 nwi + V ~?w
(3)
?k,i =
nik + ~?i
?T
i=1 nik + T ~?i
(4)
2http://psiexp.ss.uci.edu/research/programs data/toolbox.htm
13
4 Experiments and Analysis
In this section, we describe the empirical evaluation
of our model qualitatively and quantitatively by ap-
plying our model to a text retrieval system we call
Expert Search. This search engine is intended to re-
trieve groups of experts with similar interests and ex-
pertise by inputting only general domain key words,
such as syntactic parsing, information retrieval.
We first describe the data set, the retrieval system
and the evaluation metrics. Then we present the em-
pirical results both qualitatively and quantitatively.
4.1 Data
We crawled from ACL anthology website and col-
lected seven years of annual ACL conference papers
as our corpus. The reference section is deleted from
each paper to reduce some noisy vocabulary, such
as idiosyncratic proper names, and some coding er-
rors caused during the file format conversion pro-
cess. We applied a part of speech tagger3 to tag
the files and retain in our vocabulary only content
words, i.e., nouns, verbs, adjectives and adverbs.
The ACL anthology website explicitly lists each
paper together with its title and author information.
Therefore, the author information of each paper can
be obtained accurately without extracting from the
original paper. We transformed all pdf files to text
files and normalized all author names by eliminating
their middle name initials if they are present in the
listed names. There is a total of 1,326 papers in the
collected corpus with 2, 084 authors. Then multi-
words (in our current experiments, the bigram collo-
cations) are discovered via the ?2 statistics and part
of speech pattern filtering. These multiwords are
then added into the vocabulary to build our model.
Some basic statistics about this corpus is summa-
rized in Table 1.
Two sets of results are evaluated use the retrieval
system in our experiments: one set is based on un-
igram vocabulary and the other with the vocabulary
expanded by the multiwords.
4.2 Evaluation on Expert Search
We designed a preliminary retrieval system to eval-
uate our model. The functionality of this search is
3The tagger is from:
http://l2r.cs.uiuc.edu/?cogcomp/software.php
ACL Corpus Statistics
Year range 2003-2009
Total number of papers 1,326
Total number of authors 2,084
Total unigrams 34,012
Total unigram and multiwords 205,260
Table 1: Description of the ACL seven-year collection in
our experiments
to associate words with individual authors, i.e., we
rank the joint probability of the query words and the
target author P (W,a). This probability is marginal-
ized over all topics in the model to rank all authors
in our corpus. In addition, the model assumes that
the word and the author is conditionally indepen-
dent given the topic. Formally, we define the ranking
function of our retrieval system in Equation 5:
P (W,a) =
?
wi
?i
?
t
P (wi, a|t)P (t)
=
?
wi
?i
?
t
P (wi|t)P (a|t)P (t) (5)
W is the input query, which may contain one or
more words. If a multiword is detected within the
query, it is added into the query. The final score is
the sum of all words in this query weighted by their
inverse document frequency ?i The inverse docu-
ment frequency is defined as Equation 6.
?i =
1
DF (wi)
(6)
In our experiments, we chose ten queries which
covers several most popular research areas in com-
putational linguistics and natural language process-
ing. In our unigram model, query words are treated
token by token. However, in our multiword model,
if the query contains a multiword inside our vocabu-
lary, it is treated as an additional token to expand the
query. For each query, top 10 authors are returned
from the system. We manually label the relevance
of these 10 authors based on the papers they submit-
ted to these seven-year ACL conferences collected
in our corpus. Two evaluation metrics are used to
measure the precision of the retrieving results. First
we evaluate the precision at a given cut-off rank,
namely precision at K with K ranging from 1 to 10.
14
We also calculate the average precision (AP) for
each query and the mean average precision (MAP)
for all the 10 queries. Average precision not only
takes ranking as consideration but also emphasizes
ranking relevant documents higher. Different from
precision at K, it is sensitive to the ranking and cap-
tures some recall information since it assumes the
precision of the non-retrieved documents to be zero.
It is defined as the average of precisions computed
at the point of each of the relevant documents in the
ranked list as shown in equation 7.
AP =
?n
r=1(Precision(r)? rel(r))
?
relevant documents
(7)
Currently in our experiments, we do not have a
pool of labeled authors to do a good evaluation of
recall of our system. However, as in the web brows-
ing activity, many users only care about the first sev-
eral hits of the retrieving results and precision at K
and MAP measurements are robust measurements
for this purpose.
4.3 Results and Analysis
In this section, we first examine the qualitative re-
sults from our model and then report the evaluation
on the external expert search.
4.3.1 Qualitative Coherence Analysis
As have shown by other works on Ngram topic
modeling (Wallach, 2006; Wang et al, 2007; Grif-
fiths et al, 2007), our model also demonstrated that
embedding multiword tokens into the simple author
topic model can always achieve more coherent and
better interpretable topics. We list top 15 words
from two topics of the multiword model and uni-
gram model respectively in Table 2. Unigram topics
contain more general words which can occur in ev-
ery topic and are usually less discriminative among
topics.
Our experiments also show that embedding the
multiword tokens into the model achieves better
clustering of the authors and the coherence between
authors and topics. We demonstrate this qualita-
tively by listing two examples respectively from the
multiword models and the unigram model in Table 3.
For example, for the topic on dependency pars-
ing, unigram model missed Ryan-McDonald and the
ranking of the authors are also questionable. Further
MultiWord Model Unigram Model
TOPIC 4 Topic 51
coreference-resolution resolution
antecedent antecedent
treesubstitution-grammars pronoun
completely pronouns
pronoun is
resolution information
angry antecedents
candidate anaphor
extracted syntactic
feature semantic
pronouns coreference
model anaphora
perceptual-cooccurrence definite
certain-time model
anaphora-resolution only
TOPIC 49 Topic 95
sense sense
senses senses
word-sense disambiguation
target-word word
word-senses context
sense-disambiguation ontext
nouns ambiguous
automatically accuracy
semantic-relatedness nouns
disambiguation unsupervised
provided target
ambiguous-word predominant
concepts sample
lexical-sample automatically
nouns-verbs meaning
Table 2: Comparison of the topic interpretation from the
multiword-enhanced and the unigram models. Qualita-
tively, topics with multiwords are more interpretable.
quantitative measurement is listed in our quantita-
tive evaluation section. However, qualitatively, mul-
tiword model seems less problematic.
Some of the unfamiliar author may not be easy to
make a relevance judgment. However, if we trace
all the papers the author wrote in our collected cor-
pus, many of the authors are coherently related to the
topic. We list all the papers in our corpus for three
authors from the machine translation topic derived
from the multiword model in Table 4 to demonstrate
the coherence between the author and the related
topic. However, it is also obvious that our model
missed some real experts in the corresponding field.
15
MultiWord Model Unigram Model
Topic 63 Topic 145 Topic 23 Topic 78
Word Word Word Word
translation dependency-parsing translation dependency
machine-translation dependency-tree translations head
language-model dependency-trees bilingual dependencies
statistical-machine dependency pairs structure
translations dependency-structures language structures
phrases dependency-graph machine dependent
translation-model dependency-relation parallel order
decoding dependency-relations translated word
score order monolingual left
decoder does quality does
Author Author Author Author
Shouxun-Lin Joakim-Nivre Hua-Wu Christopher-Manning
David-Chiang Jens-Nilsson Philipp-Koehn Hisami-Suzuk
Qun-Liu David-Temperley Ming-Zhou Kenji-Sagae
Philipp-Koehn Wei-He Shouxun-Lin Jens-Nilsson
Chi-Ho-Li Elijah-Mayfield David-Chiang Jinxi-Xu
Christoph-Tillmann Valentin-Jijkoun Yajuan-Lu Joakim-Nivre
Chris-Dyer Christopher-Manning Haifeng-Wang Valentin-Jijkoun
G-Haffari Jiri-Havelka Aiti-Aw Elijah-Mayfield
Taro-Watanabe Ryan-McDonald Chris-Callison-Burch David-Temperley
Aiti-Aw Andre-Martins Franz-Och Julia-Hockenmaier
Table 3: Two examples for topic and author coherece from multiword-enhanced model and unigram model. Top 10
words and authors are listed accordingly for each model.
For example, we did not get Kevin Knight for the
machine translation topic. This may be due to the
limitation of our corpus since we only collected pa-
pers from one conference in a limited time, or be-
cause usually these experts write more divergent on
various topics.
Another observation in our experiment is that
some experts with many papers may not be ranked
at the very top by our system. However, they have
pretty high probability to associate with several top-
ics. Intuitively this makes sense, since many of these
famous experts write papers with their students in
various topics. Their scores may therefore not be as
high as authors who have fewer papers in the corpus
which are concentrated in one topic.
4.3.2 Results from Expert Search
One annotator labeled the relevance of the re-
trieval results from our expert search system. The
annotator was also given all the paper titles of each
corresponding retrieved author to help make the bi-
nary judgment. We experimented with ten queries
and retrieved the top ten authors for each query.
We first used the precision at K for evaluation. we
calculate the precision at K for both of our multi-
word model and the unigram model and the results
are listed in Table 5. It is obvious that at every rank
position, the multiword model works better than the
unigram model. In order to focus more on relevant
retrieval results, we then calculate the average preci-
sion for each query and mean average precision for
both models. The results are in Table 6.
When only comparing the mean average precision
(MAP), the multiword model works better. How-
ever, when examining the average precision of each
query within these two models, the unigram model
also works pretty well with some queries. How the
query words may interact with our model deserves
further investigation.
5 Discussion and Further Development
In this paper, we extended the existing author topic
model with multiword term-document input and ap-
plied it to the domain of expert retrieval. Although
our study is preliminary, our experiments do return
16
Author Papers from ACL(03-09)
Shouxun-Lin
Log-linear Models for Word Alignment
Maximum Entropy Based Phrase Reordering Model for Statistical Machine Translation
Tree-to-String Alignment Template for Statistical Machine Translation
Forest-to-String Statistical Translation Rules
Partial Matching Strategy for Phrase-based Statistical Machine Translation
David-Chiang
A Hierarchical Phrase-Based Model for Statistical Machine Translation
Word Sense Disambiguation Improves Statistical Machine Translation
Forest Rescoring: Faster Decoding with Integrated Language Models
Fast Consensus Decoding over Translation Forests
Philipp-Koehn
Feature-Rich Statistical Translation of Noun Phrases
Clause Restructuring for Statistical Machine Translation
Moses: Open Source Toolkit for Statistical Machine Translation
Enriching Morphologically Poor Languages for Statistical Machine Translation
A Web-Based Interactive Computer Aided Translation Tool
Topics in Statistical Machine Translation
Table 4: Papers in our ACL corpus for three authors related to the ?machine translation? topic in Table 3.
Precision@K
K Multiword Model Unigram Model
1 0.90 0.80
2 0.80 0.80
3 0.73 0.67
4 0.70 0.65
5 0.70 0.64
6 0.72 0.65
7 0.71 0.64
8 0.71 0.66
9 0.71 0.66
10 0.70 0.64
Table 5: Precision at K evaluation of the multiword-
enhanced model and the unigram model.
promising results, demonstrating the effectiveness
of our model in improving coherence in topic clus-
ters. In addition, the use of the MATM for expert
retrieval returned some useful preliminary results,
which can be further improved in a number of ways.
One immediate improvement would be an exten-
sion of our corpus. In our experiments, we consid-
ered only ACL papers from the last 7 years. If we
extend our data to cover papers from additional con-
ferences, we will be able to strengthen author-topic
associations for authors who submit papers on the
same topics to different conferences. This will also
allow more prominent authors to come to the fore-
front in our search application. Such a modifica-
Average Precision (AP)
Query Multi. Mod. Uni. Mod.
Language Model 0.79 0.58
Unsupervised Learning 1.0 0.78
Supervised Learning 0.84 0.74
Machine Translation 0.95 1.0
Semantic Role Labeling 0.81 0.57
Coreference Resolution 0.59 0.72
Hidden Markov Model 0.93 0.37
Dependency Parsing 0.75 0.94
Parsing 0.81 0.98
Transliteration 0.62 0.85
MAP: 0.81 0.75
Table 6: Average Precision (AP) for each query and Mean
Average Precision (MAP) of the multiword-enhanced
model and the unigram model.
tion would require us to further increase the model?s
computational efficiency to handle huge volumes of
data encountered in real retrieval systems.
Another further development of this paper is the
addition of citation information to the model as a
layer of supervision for the retrieval system. For in-
stance, an author who is cited frequently could have
a higher weight in our system than one who isn?t,
and could occur more prominently in query results.
Finally, we can provide a better evaluation of our
system through a measure of recall and a simple
baseline system founded on keyword search of pa-
per titles. Recall can be computed via comparison to
a set of expected prominent authors for each query.
17
Acknowledgments
The research in this paper was supported by the Mul-
timodal Information Access & Synthesis Center at
UIUC, part of CCICADA, a DHS Science and Tech-
nology Center of Excellence.
References
S. Banerjee and T. Pedersen. 2003. The design, im-
plementation, and use of the Ngram Statistic Package.
In Proceedings of the Fourth International Conference
on Intelligent Text Processing and Computational Lin-
guistics, pages 370?381.
D. Blei and J. Lafferty. 2009a. Topic models. In A. Sri-
vastava and M. Sahami, editors, Text Mining: Theory
and Applications. Taylor and Francis.
D. Blei and J. Lafferty. 2009b. Visualiz-
ing topics with multi-word expressions. In
http://arxiv.org/abs/0907.1013.
D. Blei, A. Ng, and M. Jordan. 2003. Latent dirichlet
allocation. Journal of Machine Learning Research.
T. Griffiths and M. Steyvers. 2004. Finding scientific
topic. In Proceedings of the National Academy of Sci-
ence.
T. Griffiths, M. Steyvers, and J. Tenenbaum. 2007. Top-
ics in semantic representation. Psychological Review.
T. Hofmann. 1999. Probabilistic latent semantic index-
ing. In Proceedings of SIGIR.
J. Justenson and S. Katz. 1995. Technical terminology:
some linguistic properties and an algorithm for inden-
tification in text. Natural Language Engineering.
D. Magatti, S. Calegari, D. Ciucci, and F. Stella. 2009.
Automatic labeling of topics. In ISDA, pages 1227?
1232.
Christopher D. Manning and Hinrich Schtze. 1999.
Foundations of Statistical Natural Language Process-
ing. Cambridge, Massachusetts.
Q. Mei, X. Shen, and C. Zhai. 2007. Automatic la-
beling of multinomial topic models. In Proceedings
of the 13th ACM SIGKDD international conference
on Knowledge discovery and data mining, pages 490?
499.
M. Rosen-Zvi, T. Griffiths, M. Steyvers, and P. Smyth.
2004. the author-topic model for authors and docu-
ments. In Proceedings of UAI.
M. Steyvers and T. Griffiths. 2007. Probabilistic topic
models. In Handbook of Latent Semantic Analysis.
Lawrence Erlbaum Associates.
M. Steyvers, P. Smyth, and T. Griffiths. 2004. Proba-
bilistic author-topic models for information discovery.
In Proceedings of KDD.
H. Wallach. 2006. Topic modeling; beyond bag
of words. In International Conference on Machine
Learning.
X. Wang and A. McCallum. 2005. A note on topical n-
grams. Technical report, University of Massachusetts.
X. Wang, A. McCallum, and X. Wei. 2007. Topical n-
grams: Phrase and topic discoery with an application
to information retrieval. In Proceedings of ICDM.
18
Proceedings of the Workshop on Multiword Expressions: from Parsing and Generation to the Real World (MWE 2011), pages 31?39,
Portland, Oregon, USA, 23 June 2011. c?2011 Association for Computational Linguistics
Learning English Light Verb Constructions: Contextual or Statistical
Yuancheng Tu
Department of Linguistics
University of Illinois
ytu@illinois.edu
Dan Roth
Department of Computer Science
University of Illinois
danr@illinois.edu
Abstract
In this paper, we investigate a supervised ma-
chine learning framework for automatically
learning of English Light Verb Constructions
(LVCs). Our system achieves an 86.3% accu-
racy with a baseline (chance) performance of
52.2% when trained with groups of either con-
textual or statistical features. In addition, we
present an in-depth analysis of these contex-
tual and statistical features and show that the
system trained by these two types of cosmet-
ically different features reaches similar per-
formance empirically. However, in the situa-
tion where the surface structures of candidate
LVCs are identical, the system trained with
contextual features which contain information
on surrounding words performs 16.7% better.
In this study, we also construct a balanced
benchmark dataset with 2,162 sentences from
BNC for English LVCs. And this data set is
publicly available and is also a useful com-
putational resource for research on MWEs in
general.
1 Introduction
Multi-Word Expressions (MWEs) refer to various
types of linguistic units or expressions, including
idioms, noun compounds, named entities, complex
verb phrases and any other habitual collocations.
MWEs pose a particular challenge in empirical Nat-
ural Language Processing (NLP) because they al-
ways have idiosyncratic interpretations which can-
not be formulated by directly aggregating the se-
mantics of their constituents (Sag et al, 2002).
The study in this paper focuses on one special
type of MWEs, i.e., the Light Verb Constructions
(LVCs), formed from a commonly used verb and
usually a noun phrase (NP) in its direct object po-
sition, such as have a look and make an offer in
English. These complex verb predicates do not fall
clearly into the discrete binary distinction of com-
positional or non-compositional expressions. In-
stead, they stand somewhat in between and are typ-
ically semi-compositional. For example, consider
the following three candidate LVCs: take a wallet,
take a walk and take a while. These three complex
verb predicates are cosmetically very similar. But
a closer look at their semantics reveals significant
differences and each of them represents a different
class of MWEs. The first expression, take a wallet
is a literal combination of a verb and its object noun.
The last expression take a while is an idiom and its
meaning cost a long time to do something, cannot
be derived by direct integration of the literal mean-
ing of its components. Only the second expression,
take a walk is an LVC whose meaning mainly de-
rives from one of its components, namely its noun
object (walk) while the meaning of its main verb is
somewhat bleached (Butt, 2003; Kearns, 2002) and
therefore light (Jespersen, 1965).
LVCs have already been identified as one of the
major sources of problems in various NLP applica-
tions, such as automatic word alignment (Samardz?ic?
and Merlo, 2010) and semantic annotation transfer-
ence (Burchardt et al, 2009), and machine transla-
tion. These problems provide empirical grounds for
distinguishing between the bleached and full mean-
ing of a verb within a given sentence, a task that is
often difficult on the basis of surface structures since
they always exhibit identical surface properties. For
example, consider the following sentences:
31
1. He had a look of childish bewilderment on his
face.
2. I?ve arranged for you to have a look at his file
in our library.
In sentence 1, the verb have in the phrase have a
look has its full fledged meaning ?possess, own? and
therefore it is literal instead of light. However, in
sentence 2, have a look only means look and the
meaning of the verb have is impoverished and is thus
light.
In this paper, we propose an in-depth case study
on LVC recognition, in which we investigate ma-
chine learning techniques for automatically identi-
fying the impoverished meaning of a verb given a
sentence. Unlike the earlier work that has viewed all
verbs as possible light verbs (Tan et al, 2006), We
focus on a half dozen of broadly documented and
most frequently used English light verbs among the
small set of them in English.
We construct a token-based data set with a total
of 2, 162 sentences extracted from British National
Corpus (BNC)1 and build a learner with L2-loss
SVM. Our system achieves a 86.3% accuracy with
a baseline (chance) performance of 52.2%. We also
extract automatically two groups of features, statis-
tical and contextual features and present a detailed
ablation analysis of the interaction of these features.
Interestingly, the results show that the system per-
forms similarly when trained independently with ei-
ther groups of these features. And the integration
of these two types of features does not improve the
performance. However, when tested with all sen-
tences with the candidate LVCs whose surface struc-
tures are identical in both negative and positive ex-
amples, for example, the aforementioned sentence 1
(negative) and 2 (positive) with the candidate LVC
?have a look?, the system trained with contextual
features which include information on surrounding
words performs more robust and significantly better.
This analysis contributes significantly to the under-
standing of the functionality of both contextual and
statistical features and provides empirical evidence
to guide the usage of them in NLP applications.
In the rest of the paper, we first present some re-
lated work on LVCs in Sec. 2. Then we describe our
1http://www.natcorp.ox.ac.uk/XMLedition/
model including the learning algorithm and statisti-
cal and contextual features in Sec. 3. We present our
experiments and analysis in Sec. 4 and conclude our
paper in Sec. 5.
2 Related Work
LVCs have been well-studied in linguistics since
early days (Jespersen, 1965; Butt, 2003; Kearns,
2002). Recent computational research on LVCs
mainly focuses on type-based classification, i.e., sta-
tistically aggregated properties of LVCs. For exam-
ple, many works are about direct measuring of the
compositionality (Venkatapathy and Joshi, 2005),
compatibility (Barrett and Davis, 2003), acceptabil-
ity (North, 2005) and productivity (Stevenson et al,
2004) of LVCs. Other works, if related to token-
based identification, i.e., identifying idiomatic ex-
pressions within context, only consider LVCs as one
small subtype of other idiomatic expressions (Cook
et al, 2007; Fazly and Stevenson, 2006).
Previous computational works on token-based
identification differs from our work in one key as-
pect. Our work builds a learning system which sys-
tematically incorporates both informative statistical
measures and specific local contexts and does in-
depth analysis on both of them while many previ-
ous works, either totally rely on or only emphasize
on one of them. For example, the method used
in (Katz and Giesbrecht, 2006) relies primarily on
local co-occurrence lexicon to construct feature vec-
tors for each target token. On the other hand, some
other works (Fazly and Stevenson, 2007; Fazly and
Stevenson, 2006; Stevenson et al, 2004), argue that
linguistic properties, such as canonical syntactic pat-
terns of specific types of idioms, are more informa-
tive than local context.
Tan et.al. (Tan et al, 2006) propose a learning ap-
proach to identify token-based LVCs. The method is
only similar to ours in that it is a supervised frame-
work. Our model uses a different data set annotated
from BNC and the data set is larger and more bal-
anced compared to the previous data set from WSJ.
In addition, previous work assumes all verbs as po-
tential LVCs while we intentionally exclude those
verbs which linguistically never tested as light verbs,
such as buy and sell in English and only focus on
a half dozen of broadly documented English light
32
verbs, such as have, take, give, do, get and make.
The lack of common benchmark data sets for
evaluation in MWE research unfortunately makes
many works incomparable with the earlier ones. The
data set we construct in this study hopefully can
serve as a common test bed for research in LVCs
or MWEs in general.
3 Learning English LVCs
In this study, we formulate the context sensitive En-
glish LVC identification task as a supervised binary
classification problem. For each target LVC candi-
date within a sentence, the classifier decides if it is
a true LVC. Formally, given a set of n labeled ex-
amples {xi, yi}ni=1, we learn a function f : X ? Y
where Y ? {?1, 1}. The learning algorithm we use
is the classic soft-margin SVM with L2-loss which
is among the best ?off-the-shelf? supervised learn-
ing algorithms and in our experiments the algorithm
indeed gives us the best performance with the short-
est training time. The algorithm is implemented us-
ing a modeling language called Learning Based Java
(LBJ) (Rizzolo and Roth, 2010) via the LIBSVM
Java API (Chang and Lin, 2001).
Previous research has suggested that both local
contextual and statistical measures are informative
in determining the class of an MWE token. How-
ever, it is not clear to what degree these two types
of information overlap or interact. Do they contain
similar knowledge or the knowledge they provide
for LVC learning is different? Formulating a clas-
sification framework for identification enables us to
integrate all contextual and statistical measures eas-
ily through features and test their effectiveness and
interaction systematically.
We focus on two types of features: contextual and
statistical features, and analyze in-depth their inter-
action and effectiveness within the learning frame-
work. Statistical features in this study are numerical
features which are computed globally via other big
corpora rather than the training and testing data used
in the system. For example, the Cpmi and Deverbal
v/n Ratio (details in sec. 3.1) are generated from the
statistics of Google n-gram and BNC corpus respec-
tively. Since the phrase size feature is numerical and
the selection of the candidate LVCs in the data set
uses the canonical length information2, we include
it into the statistical category. Contextual features
are defined in a broader sense and consist of all local
features which are generated directly from the input
sentences, such as word features within or around
the candidate phrases. We describe the details of the
used contextual features in sec. 3.2.
Our experiments show that arbitrarily combining
statistic features within our current learning system
does not improve the performance. Instead, we pro-
vide systematic analysis for these features and ex-
plore some interesting empirical observations about
them within our learning framework.
3.1 Statistical Features
Cpmi: Collocational point-wise mutual information
is calculated from Google n-gram dataset whose n-
gram counts are generated from approximately one
trillion words of text from publicly accessible Web
pages. We use this big data set to overcome the data
sparseness problem.
Previous works (Stevenson et al, 2004; Cook et
al., 2007) show that one canonical surface syntac-
tic structure for LVCs is V + a/an Noun. For ex-
ample, in the LVC take a walk, ?take? is the verb
(V) and ?walk? is the deverbal noun. The typical
determiner in between is the indefinite article ?a?.
It is also observed that when the indefinite article
changes to definite, such as ?the?, ?this? or ?that?,
a phrase is less acceptable to be a true LVC. There-
fore, the direct collocational pmi between the verb
and the noun is derived to incorporate this intuition
as shown in the following3:
Cpmi = 2I(v, aN) ? I(v, theN)
Within this formula, I(v, aN) is the point-wise mu-
tual information between ?v?, the verb, and ?aN?,
the phrase such as ?a walk? in the aforementioned
example. Similar definition applies to I(v, theN).
PMI of a pair of elements is calculated as (Church et
al., 1991):
I(x, y) = log Nx+yf(x, y)f(x, ?)f(?, y)
2We set an empirical length constraint to the maximal length
of the noun phrase object when generating the candidates from
BNC corpus.
3The formula is directly from (Stevenson et al, 2004).
33
Nx+y is the total number of verb and a/the noun
pairs in the corpus. In our case, all trigram counts
with this pattern in N-gram data set. f(x, y) is the
frequency of x and y co-occurring as a v-a/theN pair
where f(x, ?) and f(?, y) are the frequency when
either of x and y occurs independent of each other
in the corpus. Notice these counts are not easily
available directly from search engines since many
search engines treat articles such as ?a? or ?the? as
stop words and remove them from the search query4.
Deverbal v/n Ratio: the second statistical feature
we use is related to the verb and noun usage ratio of
the noun object within a candidate LVC. The intu-
ition here is that the noun object of a candidate LVC
has a strong tendency to be used as a verb or related
to a verb via derivational morphology. For exam-
ple, in the candidate phrase ?have a look?, ?look?
can directly be used as a verb while in the phrase
?make a transmission?, ?transmission? is derivation-
ally related to the verb ?transmit?. We use fre-
quency counts gathered from British National Cor-
pus (BNC) and then calculate the ratio since BNC
encodes the lexeme for each word and is also tagged
with parts of speech. In addition, it is a large corpus
with 100 million words, thus, an ideal corpus to cal-
culate the verb-noun usage for each candidate word
in the object position.
Two other lexical resources, WordNet (Fellbaum,
1998) and NomLex (Meyers et al, 1998), are used
to identify words which can directly be used as a
noun and a verb and those that are derivational re-
lated. Specifically, WordNet is used to identify the
words which can be used as both a noun and a verb
and NomLex is used to recognize those derivation-
ally related words. And the verb usage counts of
these nouns are the frequencies of their correspond-
ing derivational verbs. For example, for the word
?transmission?, its verb usage frequency is the count
in BNC with its derivationally related verb ?trans-
mit?.
Phrase Size: the third statistical feature is the ac-
tual size of the candidate LVC phrase. Many modi-
fiers can be inserted inside the candidate phrases to
generate new candidates. For example, ?take a look?
can be expanded to ?take a close look?, ?take an ex-
4Some search engines accept ?quotation strategy? to retain
stop words in the query.
tremely close look? and the expansion is in theory
infinite. The hypothesis behind this feature is that
regular usage of LVCs tends to be short. For exam-
ple, it is observed that the canonical length in En-
glish is from 2 to 6.
3.2 Contextual Features
All features generated directly from the input sen-
tences are categorized into this group. They con-
sists of features derived directly from the candidate
phrases themselves as well as their surrounding con-
texts.
Noun Object: this is the noun head of the object
noun phrase within the candidate LVC phrase. For
example, for a verb phrase ?take a quick look?, its
noun head ?look? is the active Noun Object feature.
In our data set, there are 777 distinctive such nouns.
LV-NounObj: this is the bigram of the light verb
and the head of the noun phrase. This feature en-
codes the collocation information between the can-
didate light verb and the head noun of its object.
Levin?s Class: it is observed that members within
certain groups of verb classes are legitimate candi-
dates to form acceptable LVCs (Fazly et al, 2005).
For example, many sound emission verbs accord-
ing to Levin (Levin, 1993), such as clap, whis-
tle, and plop, can be used to generate legitimate
LVCs. Phrases such as make a clap/plop/whistle are
all highly acceptable LVCs by humans even though
some of them, such as make a plop rarely occur
within corpora. We formulate a vector for all the
256 Levin?s verb classes and turn the correspond-
ing class-bits on when the verb usage of the head
noun in a candidate LVC belongs to these classes.
We add one extra class, other, to be mapped to those
verbs which are not included in any one of these 256
Levin?s verb classes.
Other Features: we construct other local con-
textual features, for example, the part of speech of
the word immediately before the light verb (titled
posBefore) and after the whole phrase (posAfter).
We also encode the determiner within all candidate
LVCs as another lexical feature (Determiner). We
examine many other combinations of these contex-
tual features. However, only those features that con-
tribute positively to achieve the highest performance
of the classifier are listed for detailed analysis in the
next section.
34
4 Experiments and Analysis
In this section, we report in detail our experimental
settings and provide in-depth analysis on the inter-
actions among features. First, we present our mo-
tivation and methodology to generate the new data
set. Then we describe our experimental results and
analysis.
4.1 Data Preparation and Annotation
The data set is generated from BNC, a balanced syn-
chronic corpus containing 100 million words col-
lected from various sources of British English. We
begin our sentence selection process with the ex-
amination of a handful of previously investigated
verbs (Fazly and Stevenson, 2007; Butt, 2003).
Among them, we pick the 6 most frequently used
English light verbs: do, get, give, have, make and
take.
To identify potential LVCs within sentences, we
first extract all sentences where one or more of the
six verbs occur from BNC (XML Edition) and then
parse these sentences with Charniak?s parser (Char-
niak and Johnson, 2005). We focus on the ?verb
+ noun object? pattern and choose all the sentences
which have a direct NP object for the target verbs.
We then collect a total of 207, 789 sentences.
We observe that within all these chosen sentences,
the distribution of true LVCs is still low. We there-
fore use three resources to filter out trivial nega-
tive examples. Firstly, We use WordNet (Fellbaum,
1998) to identify the head noun in the object position
which can be used as both a noun and a verb. Then,
we use frequency counts gathered from BNC to fil-
ter out candidates whose verb usage is smaller than
their noun usage. Finally, we use NomLex (Meyers
et al, 1998) to recognize those head words in the
object position whose noun forms and verb forms
are derivationally related, such as transmission and
transmit. We keep all candidates whose object head
nouns are derivationlly related to a verb according
to a gold-standard word list we extract from Nom-
Lex5. With this pipeline method, we filter out ap-
proximately 55% potential negative examples. This
leaves us with 92, 415 sentences which we sample
about 4% randomly to present to annotators. This
filtering method successfully improves the recall of
5We do not count those nouns ending with er and ist
the positive examples and ensures us a corpus with
balanced examples.
A website6 is set up for annotators to annotate the
data. Each potential LVC is presented to the anno-
tator in a sentence. The annotator is asked to decide
whether this phrase within the given sentence is an
LVC and to choose an answer from one of these four
options: Yes, No, Not Sure, and Idiom.
Detailed annotation instructions and LVC exam-
ples are given on the annotation website. When fac-
ing difficult examples, the annotators are instructed
to follow a general ?replacing? principle, i.e, if the
candidate light verb within the sentence can be re-
placed by the verb usage of its direct object noun
and the meaning of the sentence does not change,
that verb is regarded as a light verb and the candidate
is an LVC. Each example is annotated by two anno-
tators and We only accept examples where both an-
notators agree on positive or negative. We generate a
total of 1, 039 positive examples and 1, 123 negative
examples. Among all these positive examples, there
are 760 distinctive LVC phrases and 911 distinctive
verb phrases with the pattern ?verb + noun object?
among negative examples. The generated data set
therefore gives the classifier the 52.2% chance base-
line if the classifier always votes the majority class
in the data set.
4.2 Evaluation Metrics
For each experiment, we evaluate the performance
with three sets of metrics. We first report the stan-
dard accuracy on the test data set. Since accuracy
is argued not to be a sufficient measure of the eval-
uation of a binary classifier (Fazly et al, 2009) and
some previous works also report F1 values for the
positive classes, we therefore choose to report the
precision, recall and F1 value for both positive and
negative classes.
True Class
+ -
Predicted Class + tp fp
- fn tn
Table 1: Confusion matrix to define true positive (tp),
true negative (tn), false positive (fp) and false negative
(fn).
6http://cogcomp.cs.illinois.edu/?ytu/test/LVCmain.html
35
Based on the classic confusion matrix as shown in
Table 1, we calculate the precision and recall for the
positive class in equation 1:
P+ = tptp + fp R
+ = tptp + fn (1)
And similarly, we use equation 2 for negative class.
And the F1 value is the harmonic mean of the preci-
sion and recall of each class.
P? = tntn + fn R
? = tntn + fp (2)
4.3 Experiments with Contextual Features
In our experiments, We aim to build a high perfor-
mance LVC classifier as well as to analyze the in-
teraction between contextual and statistical features.
We randomly sample 90% sentences for training and
the rest for testing. Our chance baseline is 52.2%,
which is the percentage of our majority class in the
data set. As shown in Table 2, the classifier reaches
an 86.3% accuracy using all contextual features de-
scribed in previous section 3.2. Interestingly, we ob-
serve that adding other statistical features actually
hurts the performance. The classifier can effectively
learn when trained with discrete contextual features.
Label Precision Recall F1
+ 86.486 84.211 85.333
- 86.154 88.189 87.160
Accuracy 86.307
Chance Baseline 52.2
Table 2: By using all our contextual features, our classi-
fier achieves overall 86.307% accuracy.
In order to examine the effectiveness of each indi-
vidual feature, we conduct an ablation analysis and
experiment to use only one of them each time. It is
shown in Table 3 that LV-NounObj is found to be the
most effective contextural feature since it boosts the
baseline system up the most, an significant increase
of 31.6%.
We then start from this most effective feature, LV-
NounObj and add one feature each step to observe
the change of the system accuracy. The results are
listed in Table 4. Other significant features are fea-
tures within the candidate LVCs themselves such as
Determiner, Noun Object and Levin?s Class related
Features Accuracy Diff(%)Baseline (chance) 52.2
LV-NounObj 83.817 +31.6
Noun Object 79.253 +27.1
Determiner 72.614 +20.4
Levin?s Class 69.295 +17.1
posBefore 53.112 +0.9
posAfter 51.037 -1.1
Table 3: Using only one feature each time. LV-NounObj
is the most effective feature. Performance gain is associ-
ated with a plus sign and otherwise a negative sign.
to the object noun. This observation agrees with pre-
vious research that the acceptance of LVCs is closely
correlated to the linguistic properties of their compo-
nents. The part of speech of the word after the phrase
seems to have negative effect on the performance.
However, experiments show that without this fea-
ture, the overall performance decreases.
Features Accuracy Diff(%)Baseline (chance) 52.2
+ LV-NounObj 83.817 +31.6
+ Noun Object 84.232 +0.4
+ Levin?s Class 84.647 +0.4
+ posBefore 84.647 0.0
+ posAfter 83.817 -0.8
+ Determiner 86.307 +2.5
Table 4: Ablation analysis for contextual features. Each
feature is added incrementally at each step. Performance
gain is associated with a plus sign otherwise a negative
sign.
4.4 Experiments with Statistical Features
When using statistical features, instead of directly
using the value, we discretize each value to a binary
feature. On the one hand, our experiments show that
this way of transformation achieves the best perfor-
mance. On the other hand, the transformation plays
an analogical role as a kernel function which maps
one dimensional non-linear separable examples into
an infinite or high dimensional space to render the
data linearly separable.
In these experiments, we use only numerical fea-
tures described in section 3.1. And it is interesting
to observe that those features achieve very similar
36
Label Precision Recall F1
+ 86.481 85.088 86.463
- 86.719 87.402 87.059
Accuracy 86.307
Table 5: Best performance achieved with statistical fea-
tures. Comparing to Table 2, the performance is similar
to that trained with all contextual features.
performance as the contextual features as shown in
Table 5.
To validate that the similar performance is not
incidental. We then separate our data into 10-fold
training and testing sets and learn independently
from each fold of these ten split. Figure 1, which
shows the comparison of accuracies for each data
fold, indicates the comparable results for each fold
of the data. Therefore, we conclude that the similar
effect achieved by training with these two groups of
features is not accidental.
 50
 60
 70
 80
 90
 100
0 1 2 3 4 5 6 7 8 9
Ac
cu
ra
cy
Ten folds in the Data Set
Accuracy of each fold using statistic or contextual features
Contextual Features
Statistic Features
Figure 1: Classifier Accuracy of each fold of all 10 fold
testing data, trained with groups of statistical features and
contextual features separately. The similar height of each
histogram indicates the similar performance over each
data separation and the similarity is not incidental.
We also conduct an ablation analysis with statis-
tical features. Similar to the ablation analyses for
contextual features, we first find that the most ef-
fective statistical feature is Cpmi, the collocational
based point-wise mutual information. Then we add
one feature at each step and show the increasing
performance in Table 6. Cpmi is shown to be a
good indicator for LVCs and this observation agrees
with many previous works on the effectiveness of
Features Accuracy Diff(%)BaseLine (chance) 52.2
+ Cpmi 83.402 +31.2
+ Deverbal v/n Ratio 85.892 +2.5
+ Phrase Size 86.307 +0.4
Table 6: Ablation analysis for statistical features. Each
feature is added incrementally at each step. Performance
gain is associated with a plus sign.
point-wise mutual information in MWE identifica-
tion tasks.
4.5 Interaction between Contextual and
Statistical Features
Experiments from our previous sections show that
two types of features which are cosmetically differ-
ent actually achieve similar performance. In the ex-
periments described in this section, we intend to do
further analysis to identify further the relations be-
tween them.
4.5.1 Situation when they are similar
Our ablation analysis shows that Cpmi and LV-
NounObj features are the most two effective features
since they boost the baseline performance up more
than 30%. We then train the classifier with them to-
gether and observe that the classifier exhibits sim-
ilar performance as the one trained with them in-
dependently as shown in Table 7. This result indi-
cates that these two types of features actually pro-
vide similar knowledge to the system and therefore
combining them together does not provide any addi-
tional new information. This observation also agrees
with the intuition that point-wise mutual informa-
tion basically provides information on word collo-
cations (Church and Hanks, 1990).
Feature Accuracy F1+ F1-
LV-NounObj 83.817 82.028 85.283
Cpmi 83.402 81.481 84.962
Cpmi+LV-NounObj 83.817 82.028 85.283
Table 7: The classifier achieves similar performance
trained jointly with Cpmi and LV-NounObj features, com-
paring with the performance trained independently.
37
4.5.2 Situation when they are different
Token-based LVC identification is a difficult task
on the basis of surface structures since they always
exhibit identical surface properties. However, can-
didate LVCs with identical surface structures in both
positive and negative examples provide an ideal test
bed for the functionality of local contextual features.
For example, consider again these two aforemen-
tioned sentences which are repeated here for refer-
ence:
1. He had a look of childish bewilderment on his
face.
2. I?ve arranged for you to have a look at his file
in our library.
The system trained only with statistic features can-
not distinguish these two examples since their type-
based statistical features are exactly the same. How-
ever, the classifier trained with local contextual fea-
tures is expected to perform better since it contains
feature information from surrounding words. To
verify our hypothesis, we extract all examples in
our data set which have this property and then se-
lect same number of positive and negative examples
from them to formulate our test set. We then train
out classifier with the rest of the data, independently
with contextual features and statistical features. As
shown in Table 8, the experiment results validate
our hypothesis and show that the classifier trained
with contextual features performs significantly bet-
ter than the one trained with statistical features. The
overall lower system results also indicate that indeed
the test set with all ambiguous examples is a much
harder test set.
One final observation is the extremely low F1
value for negative class and relatively good perfor-
mance for positive class when trained with only sta-
tistical features. This may be explained by the fact
that statistical features have stronger bias toward
predicting examples as positive and can be used as
an unsupervised metric to acquire real LVCs in cor-
pora.
5 Conclusion and Further Research
In this paper, we propose an in-depth case study on
LVC recognition, in which we build a supervised
learning system for automatically identifying LVCs
Classifier Accuracy F1+ F1-
Contextual 68.519 75.362 56.410
Statistical 51.852 88.976 27.778
Diff (%) +16.7 -13.6 +28.3
Table 8: Classifier trained with local contextual features
is more robust and significantly better than the one trained
with statistical features when the test data set consists of
all ambiguous examples.
in context. Our learning system achieves an 86.3%
accuracy with a baseline (chance) performance of
52.2% when trained with groups of either contex-
tual or statistical features. In addition, we exploit in
detail the interaction of these two groups of contex-
tual and statistical features and show that the system
trained with these two types of cosmetically differ-
ent features actually reaches similar performance in
our learning framework. However, when it comes to
the situation where the surface structures of candi-
date LVCs are identical, the system trained with con-
textual features which include information on sur-
rounding words provides better and more robust per-
formance.
In this study, we also construct a balanced bench-
mark dataset with 2,162 sentences from BNC for
token-based classification of English LVCs. And
this data set is publicly available and is also a use-
ful computational resource for research on MWEs in
general.
There are many aspects for further research of the
current study. One direction for further improve-
ment would be to include more long-distance fea-
tures, such as parse tree path, to test the sensitivity of
the LVC classifier to those features and to examine
more extensively the combination of the contextual
and statistical features. Another direction would be
to adapt our system to other MWE types and to test
if the analysis on contextual and statistical features
in this study also applies to other MWEs.
Acknowledgments
The authors would like to thank all annotators who
annotated the data via the web interface and four
annonymous reviewers for their valuable comments.
The research in this paper was supported by the Mul-
timodal Information Access & Synthesis Center at
38
UIUC, part of CCICADA, a DHS Science and Tech-
nology Center of Excellence.
References
L. Barrett and A. Davis. 2003. Diagnostics for determing
compatibility in english support verb nominalization
pairs. In Proceedings of CICLing-2003, pages 85?90.
A. Burchardt, K. Erk, A. Frank, A. Kowalski, S. Pado,
and M. Pinkal. 2009. Using framenet for seman-
tic analysis of german: annotation, representation
and automation. In Hans Boas, editor, Multilingual
FrameNets in Computational Lexicography: methods
and applications, pages 209?244. Mouton de Gruyter.
M. Butt. 2003. The light verb jungle. In Harvard Work-
ing Paper in Linguistics, volume 9, pages 1?49.
C. Chang and C. Lin, 2001. LIBSVM: a library
for support vector machines. Software available at
http://www.csie.ntu.edu.tw/?cjlin/libsvm.
E. Charniak and M. Johnson. 2005. Coarse-to-fine n-
best parsing and maxent discriminative reranking. In
Proceedings of ACL-2005.
K. Church and P. Hanks. 1990. Word association norms,
mutual information, and lexicography. Computational
Linguistics, 16(1), March.
K. Church, W. Gale, P. Hanks, and D. Hindle. 1991. Us-
ing statistics in lexical analysis. In Lexical Acquisi-
tion: Exploiting On-Line Resources to Build a Lexi-
con, pages 115?164. Erlbaum.
P. Cook, A. Fazly, and S. Stevenson. 2007. Pulling their
weight: Exploiting syntactic forms for the automatic
identification of idiomatic expressions in context. In
Proceedings of the Workshop on A Broader Perspec-
tive on Multiword Expressions, pages 41?48, Prague,
Czech Republic, June. Association for Computational
Linguistics.
A. Fazly and S. Stevenson. 2006. Automatically con-
structing a lexicon of verb phrase idiomatic combina-
tions. In Proceedings of EACL-2006.
A. Fazly and S. Stevenson. 2007. Distinguishing sub-
types of multiword expressions using linguistically-
motivated statistical measures. In Proceedings of the
Workshop on A Broader Perspective on Multiword Ex-
pressions, pages 9?16, Prague, Czech Republic, June.
A. Fazly, R. North, and S. Stevenson. 2005. Auto-
matically distinguishing literal and figurative usages
of highly polysemous verbs. In Proceedings of the
ACL-SIGLEX Workshop on Deep Lexical Acquisition,
pages 38?47, Ann Arbor, Michigan, June. Association
for Computational Linguistics.
A. Fazly, P. Cook, and S. Stevenson. 2009. Unsupervised
type and token identification of idiomatic expression.
Comutational Linguistics.
C. Fellbaum, editor. 1998. WordNet: An Electronic Lex-
ical Database. MIT Press.
O. Jespersen. 1965. A Modern English Grammar on His-
torical Principles, Part VI, Morphology. Aeorge Allen
and Unwin Ltd.
G. Katz and E. Giesbrecht. 2006. Automatic identi-
fication of non-compositional multi-word expressions
using latent semantic analysis. In Proceedings of the
Workshop on Multiword Expressions: Identifying and
Exploiting Underlying Properties, pages 12?19.
K. Kearns. 2002. Light verbs in english. In
http://www.ling.canterbury.ac.nz/documents.
B. Levin. 1993. English Verb Classes and Alternations,
A Preliminary Investigation. University of Chicago
Press.
A. Meyers, C. Macleod, R. Yangarber, R. Grishman,
L. Barrett, and R. Reeves. 1998. Using nomlex to
produce nominalization patterns for information ex-
traction. In Proceedings of COLING-ACL98 Work-
shop:the Computational Treatment of Nominals.
R. North. 2005. Computational measures of the ac-
ceptability of light verb constructions. University of
Toronto, Master Thesis.
N. Rizzolo and D. Roth. 2010. Learning based java for
rapid development of nlp systems. In Proceedings of
the International Conference on Language Resources
and Evaluation (LREC).
I. Sag, T. Baldwin, F. Bond, and A. Copestake. 2002.
Multiword expressions: A pain in the neck for nlp. In
In Proc. of the 3rd International Conference on Intel-
ligent Text Processing and Computational Linguistics
(CICLing-2002, pages 1?15.
T. Samardz?ic? and P. Merlo. 2010. Cross-lingual vari-
ation of light verb constructions: Using parallel cor-
pora and automatic alignment for linguistic research.
In Proceedings of the 2010 Workshop on NLP and Lin-
guistics: Finding the Common Ground, pages 52?60,
Uppsala, Sweden, July.
S. Stevenson, A. Fazly, and R. North. 2004. Statistical
measures of the semi-productivity of light verb con-
structions. In Proceedings of ACL-04 workshop on
Multiword Expressions: Integrating Processing, pages
1?8.
Y. Tan, M. Kan, and H. Cui. 2006. Extending corpus-
based identification of light verb constructions using
a supervised learning framework. In Proceedings of
EACL-06 workshop on Multi-word-expressions in a
multilingual context, pages 49?56.
S. Venkatapathy and A. Joshi. 2005. Measuring the rel-
ative compositionality of verb-noun (v-n) collocations
by integrating features. In Proceedings of HLT and
EMNLP05, pages 899?906.
39
